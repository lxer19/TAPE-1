URL: http://www.cs.toronto.edu/~frey/ed-swed.ps
Refering-URL: http://www.cs.toronto.edu/~frey/index.html
Root-URL: 
Email: frey@cs.utoronto.ca  frank@comm.utoronto.ca and gulak@eecg.utoronto.ca  
Title: Early Detection and Trellis Splicing: Reduced-Complexity Soft Iterative Decoding  
Author: Brendan J. Frey Frank R. Kschischang and P. Glenn Gulak 
Date: August 12, 1996  
Address: 6 King's College Road, Toronto, Canada M5S 3H5  10 King's College Road, Toronto, Canada M5S 3G4  
Affiliation: Department of Computer Science, University of Toronto  Department of Electrical and Computer Engineering, University of Toronto  
Abstract: The excellent bit error rate performance of new soft iterative decoding algorithms (cf. turbo-codes) is achieved at the expense of a computa-tionally burdensome iterative decoding procedure. In this paper, we present a new method called early detection that can be used to reduce the computational complexity of a variety of soft iterative decoding methods. Using a confidence criterion, some information symbols are detected early on in the iterative decoding procedure, leading to a reduction in the computational complexity of further processing. We derive an instance of this algorithm that can be applied to turbo-codes. The resulting trellis splicing algorithm is easy to implement and leads to a reduction in computational complexity of over a factor of four relative to conventional turbo-decoding in a specific operating regime. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Berrou, A. Glavieux, and P. Thitima jshima, </author> <title> "Near Shannon limit error-correcting coding and decoding: Turbo codes," </title> <booktitle> in Proceedings of the IEEE International Conference on Communications, </booktitle> <year> 1993. </year>
Reference-contexts: 1 Introduction The impressive bit error rate performance of the turbo-decoding algorithm introduced by Berrou et al. <ref> [1] </ref> has led to an explosion of interest in the theory and application of a more general class of decoding algorithms called soft iterative decoding. Soft iterative decoding is an error-correction decoding method that makes use of two or more constituent codes. <p> The decoder operates on the received word in cycles. During each cycle, the decoder processes a signal constituent code using the output of the previous cycle to bias the result. The special case of soft iterative decoding introduced by Berrou et al. <ref> [1] </ref> uses two or more convolutional constituent codes that differ only in that each is derived from a different permutation of the information symbols. <p> Because probability propagation is only approximate in this case, the entire propagation procedure must be iterated until (hopefully) convergence. As an example of a graphical model that is not singly-connected, consider the convolutional turbo-code of Berrou et al. <ref> [1] </ref>, which has a trellis corresponding to each of two convolutional constituent codes. Two adjacent sections in one trellis are connected directly in that trellis as well as indirectly via the information symbols through the other trellis. Probability propagation between constituent codes works as follows. <p> eg., 1 This distribution is not the true product form a posteriori distribution, since the latter must properly take into account all constituent codes simultaneously. 2 Begin i 0 Produce information symbol soft decisions using constituent code m (i) i i+1 Decoding complete? No Detect all information symbols End in <ref> [1] </ref> each constituent code consists of a convolutional code produced using a different permutation of the information symbols. m (i) = i mod J . <p> However, for higher thresholds very few bits are early-detected. Evidently, a tradeoff must be made between complexity reduction and the final BER. 3.2 Termination Criterion A termination criterion is used to determine when to interrupt the iterative procedure and detect the remaining information symbols. Berrou et al. <ref> [1] </ref> simply execute a predetermined number of cycles. In their case, this criterion implies that a fixed number of computations are performed for each information symbol sequence. When early detection is used, however, the number of computations per cycle varies from sequence to sequence and also during decoding.
Reference: [2] <author> N. Wiberg, H.-A. Loeliger, and R. Kotter, </author> <title> "Codes and iterative decoding on general graphs," </title> <journal> European Transactions on Telecommunications, </journal> <volume> vol. 6, </volume> <pages> pp. 513-525, </pages> <year> 1995. </year>
Reference-contexts: The special case of soft iterative decoding introduced by Berrou et al. [1] uses two or more convolutional constituent codes that differ only in that each is derived from a different permutation of the information symbols. However, as pointed out by Wiberg et al. <ref> [2] </ref> and MacKay and Neal [3], the key ideas of soft iterative decoding are present in Gallager's 1963 work on low-density parity-check codes [4]. In 1981, Tanner [5] developed Gallager's work further and derived some lower bounds on rate and minimum distance for "recursive" codes. <p> In 1981, Tanner [5] developed Gallager's work further and derived some lower bounds on rate and minimum distance for "recursive" codes. A large body of work that ranges from the very practical to the very theoretical has recently emerged <ref> [2, 3, 6-14] </ref>. In this paper we make the observation that for soft iterative decoding, very often the final decoding decision affecting a particular information symbol is determined after only a relatively small number of cycles.
Reference: [3] <author> D. J. C. MacKay and R. M. Neal, </author> <title> "Good codes based on very sparse matrices," in Cryptography and Coding. </title> <booktitle> 5th IMA Conference (C. </booktitle> <editor> Boyd, ed.), </editor> <volume> no. </volume> <booktitle> 1025 in Lecture Notes in Computer Science, </booktitle> <pages> pp. 100-111, </pages> <address> Berlin Germany: </address> <publisher> Springer, </publisher> <year> 1995. </year>
Reference-contexts: The special case of soft iterative decoding introduced by Berrou et al. [1] uses two or more convolutional constituent codes that differ only in that each is derived from a different permutation of the information symbols. However, as pointed out by Wiberg et al. [2] and MacKay and Neal <ref> [3] </ref>, the key ideas of soft iterative decoding are present in Gallager's 1963 work on low-density parity-check codes [4]. In 1981, Tanner [5] developed Gallager's work further and derived some lower bounds on rate and minimum distance for "recursive" codes. <p> In 1981, Tanner [5] developed Gallager's work further and derived some lower bounds on rate and minimum distance for "recursive" codes. A large body of work that ranges from the very practical to the very theoretical has recently emerged <ref> [2, 3, 6-14] </ref>. In this paper we make the observation that for soft iterative decoding, very often the final decoding decision affecting a particular information symbol is determined after only a relatively small number of cycles.
Reference: [4] <author> R. G. Gallager, </author> <title> Low-Density Parity-Check Codes. </title> <address> Cambridge MA.: </address> <publisher> MIT Press, </publisher> <year> 1963. </year>
Reference-contexts: However, as pointed out by Wiberg et al. [2] and MacKay and Neal [3], the key ideas of soft iterative decoding are present in Gallager's 1963 work on low-density parity-check codes <ref> [4] </ref>. In 1981, Tanner [5] developed Gallager's work further and derived some lower bounds on rate and minimum distance for "recursive" codes. A large body of work that ranges from the very practical to the very theoretical has recently emerged [2, 3, 6-14]. <p> For example, soft iterative decoding is fundamentally related to the method of probability propagation [20] which is an algorithm used for computing conditional probabilities using graphical models (see textbook references [21,22]). Gallager <ref> [4] </ref>, Tanner [5] and Pearl [20] have shown that probability propagation is an exact algorithm for computing conditional probabilities when the graphical model is singly-connected. A singly-connected graph is one that has only a single path connecting any two nodes.
Reference: [5] <author> R. M. Tanner, </author> <title> "A recursive approach to low complexity codes," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> vol. 27, </volume> <pages> pp. 533-547, </pages> <year> 1981. </year>
Reference-contexts: However, as pointed out by Wiberg et al. [2] and MacKay and Neal [3], the key ideas of soft iterative decoding are present in Gallager's 1963 work on low-density parity-check codes [4]. In 1981, Tanner <ref> [5] </ref> developed Gallager's work further and derived some lower bounds on rate and minimum distance for "recursive" codes. A large body of work that ranges from the very practical to the very theoretical has recently emerged [2, 3, 6-14]. <p> For example, soft iterative decoding is fundamentally related to the method of probability propagation [20] which is an algorithm used for computing conditional probabilities using graphical models (see textbook references [21,22]). Gallager [4], Tanner <ref> [5] </ref> and Pearl [20] have shown that probability propagation is an exact algorithm for computing conditional probabilities when the graphical model is singly-connected. A singly-connected graph is one that has only a single path connecting any two nodes.
Reference: [6] <author> G. Battail, C. Berrou, and A. Glavieux, </author> <title> "Pseudo-random recursive convolutional coding for near-capacity performance," </title> <booktitle> in Proceedings of GLOBECOM'93, </booktitle> <year> 1993. </year>
Reference: [7] <author> J. Lodge, R. Young, P. Hoeher, and J. Ha genauer, </author> <title> "Separable MAP `filters' for the decoding of product and concatenated codes," </title> <booktitle> in Proceedings of IEEE International Conference on Communications, </booktitle> <pages> pp. 1740-1745, </pages> <year> 1993. </year>
Reference: [8] <author> J. D. Anderson, </author> <title> "The TURBO-coding scheme," </title> <booktitle> in Proceedings of ISIT'94, </booktitle> <year> 1994. </year>
Reference: [9] <author> J. E. M. Nillson and R. Kotter, </author> <title> "Iterative decoding of product code constructions," </title> <booktitle> in Proceedings of ISITA'94, </booktitle> <year> 1994. </year>
Reference: [10] <author> P. Jung and M. Nasshan, </author> <title> "Dependence of the error performance of turbo-codes on the in-terleaver structure in short frame transmission systems," </title> <journal> Electronics Letters, </journal> <volume> vol. 30, </volume> <pages> pp. 287-288, </pages> <year> 1994. </year>
Reference: [11] <author> D. Divsalar and F. Pollara, </author> <title> "Turbo-codes for PCS applications," </title> <booktitle> in Proceedings of ICC'95, </booktitle> <pages> pp. 54-59, </pages> <year> 1995. </year>
Reference: [12] <author> J. Hagenauer, E. Offer, and L. Papke, </author> <title> "Iter ative decoding of binary block and convolutional codes," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> vol. 42, </volume> <pages> pp. 429-445, </pages> <year> 1996. </year>
Reference-contexts: The computational cost of each section in the forward-backward algorithm thus consists of the time spent computing the ffs and fis for each state, as well as the time spent computing the a posteriori odds. Although there are various useful techniques and approximations for decreasing this cost <ref> [12, 14] </ref>, we will define it as our basic computational unit, and refer to it as a trellis section operation. Suppose that according to some early detection criterion, we decide that the value of information bit x k+1 is 1.
Reference: [13] <author> S. Benedetto and G. Montorsi, "Unveiling turbo-codes: </author> <title> Some results on parallel concatenated coding schemes," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> vol. 42, </volume> <pages> pp. 409-428, </pages> <year> 1996. </year>
Reference: [14] <author> S. Benedetto, D. Divsalar, G. Montorsi, and F. Pollara, </author> <title> "Soft-ouput decoding algorithms in iterative decoding of parallel concatenated 9 convolutional codes." </title> <note> Submitted to IEEE International Conference on Communications, </note> <year> 1996. </year>
Reference-contexts: The computational cost of each section in the forward-backward algorithm thus consists of the time spent computing the ffs and fis for each state, as well as the time spent computing the a posteriori odds. Although there are various useful techniques and approximations for decreasing this cost <ref> [12, 14] </ref>, we will define it as our basic computational unit, and refer to it as a trellis section operation. Suppose that according to some early detection criterion, we decide that the value of information bit x k+1 is 1.
Reference: [15] <author> N. Alon, J. Bruck, J. Naor, M. Naor, and R. M. Roth, </author> <title> "Construction of asymptotically good low-rate error-correcting codes through pseudo-random graphs," </title> <journal> IEEE Transactions on Information Theory, </journal> <month> March </month> <year> 1992. </year>
Reference: [16] <author> M. Sipser and D. A. Spielman, </author> <title> "Expander codes." </title> <note> To appear in IEEE Transactions on Information Theory, </note> <year> 1996. </year>
Reference: [17] <author> D. A. Spielman, </author> <title> "Linear-time encodable and decodable error-correcting codes." </title> <note> To appear in IEEE Transactions on Information Theory, </note> <year> 1996. </year>
Reference: [18] <author> F. R. Kschischang and V. Sorokine, </author> <title> "On the trellis structure of block codes," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> vol. 41, </volume> <pages> pp. 1924-1937, </pages> <year> 1995. </year>
Reference-contexts: Many codes are very naturally described using graphical models [2-5, 15-17], and in fact every block code can be represented graphically <ref> [18, 19] </ref>. A graphical model consists of a graph which depicts dependencies between information symbols, state variables and codeword symbols, in addition to local probabilistic information (eg., signal likelihoods).
Reference: [19] <author> R. J. </author> <title> McEliece, "On the BJCR trellis for lin ear block codes," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> vol. 42, </volume> <year> 1996. </year>
Reference-contexts: Many codes are very naturally described using graphical models [2-5, 15-17], and in fact every block code can be represented graphically <ref> [18, 19] </ref>. A graphical model consists of a graph which depicts dependencies between information symbols, state variables and codeword symbols, in addition to local probabilistic information (eg., signal likelihoods). <p> For turbo-codes, each constituent code is a convolutional code that is processed using the forward-backward algorithm [23,24] (see Section 2) in order to obtain a posteriori information bit probabilities given a priori information bit probabilities. The forward-backward algorithm can be viewed simply as a combination of probabilistic "flows" <ref> [19] </ref> computed in the forward direction and in the backward direction. Consider the simple two-state trellis shown in Fig. 6a, that corresponds, say, to the jth constituent code.
Reference: [20] <author> J. Pearl, </author> <title> "Fusion, propagation, and structur ing in belief networks," </title> <journal> Artificial Intelligence, </journal> <volume> vol. 29, </volume> <pages> pp. 241-288, </pages> <year> 1986. </year>
Reference-contexts: For example, soft iterative decoding is fundamentally related to the method of probability propagation <ref> [20] </ref> which is an algorithm used for computing conditional probabilities using graphical models (see textbook references [21,22]). Gallager [4], Tanner [5] and Pearl [20] have shown that probability propagation is an exact algorithm for computing conditional probabilities when the graphical model is singly-connected. <p> For example, soft iterative decoding is fundamentally related to the method of probability propagation <ref> [20] </ref> which is an algorithm used for computing conditional probabilities using graphical models (see textbook references [21,22]). Gallager [4], Tanner [5] and Pearl [20] have shown that probability propagation is an exact algorithm for computing conditional probabilities when the graphical model is singly-connected. A singly-connected graph is one that has only a single path connecting any two nodes.
Reference: [21] <author> J. Pearl, </author> <title> Probabilistic Reasoning in Intelli gent Systems. </title> <address> San Mateo CA.: </address> <publisher> Morgan Kauf-mann, </publisher> <year> 1988. </year>
Reference: [22] <author> R. E. </author> <title> Neapolitan, Probabilistic Reasoning in Expert Systems. </title> <address> New York NY.: </address> <publisher> John Wiley & Sons, </publisher> <year> 1990. </year>
Reference: [23] <author> L. E. Baum and T. Petrie, </author> <title> "Statistical in ference for probabilistic functions of finite state markov chains," </title> <journal> Annals of Mathematical Statistics, </journal> <volume> vol. 37, </volume> <pages> pp. 1559-1563, </pages> <year> 1966. </year>
Reference-contexts: An algorithm for computing P (X k = x k jy) on such a graphical model was developed by Baum and Petrie <ref> [23] </ref> as well as by Bahl et al. [24] (who cite [23]). The former authors called their procedure the forward-backward algorithm. <p> An algorithm for computing P (X k = x k jy) on such a graphical model was developed by Baum and Petrie <ref> [23] </ref> as well as by Bahl et al. [24] (who cite [23]). The former authors called their procedure the forward-backward algorithm. Although there has recently been a tendency to refer to this algorithm as the "BCJR" algorithm (after the initials of the latter authors), we prefer to use the original name since it is more descriptive.
Reference: [24] <author> L. R. Bahl, J. Cocke, F. Jelinek, and J. Ra viv, </author> <title> "Optimal decoding of linear codes for minimizing symbol error rate," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> vol. 20, </volume> <pages> pp. 284-287, </pages> <year> 1974. </year>
Reference-contexts: An algorithm for computing P (X k = x k jy) on such a graphical model was developed by Baum and Petrie [23] as well as by Bahl et al. <ref> [24] </ref> (who cite [23]). The former authors called their procedure the forward-backward algorithm. Although there has recently been a tendency to refer to this algorithm as the "BCJR" algorithm (after the initials of the latter authors), we prefer to use the original name since it is more descriptive.
Reference: [25] <author> T. M. Cover and J. A. Thomas, </author> <title> Elements of Information Theory. </title> <address> New York NY.: </address> <publisher> John Wiley & Sons, </publisher> <year> 1991. </year> <month> 10 </month>
Reference-contexts: We are currently exploring a more sophisticated termination criterion that depends on the log-5 likelihood of the overall codeword sequence, given the current estimate of the information symbol sequence. Because of the asymptotic equiparti-tion property <ref> [25] </ref>, for reasonably long sequences this log-likelihood gives an accurate measure of the quality of the current information symbol sequence estimate.
References-found: 25

