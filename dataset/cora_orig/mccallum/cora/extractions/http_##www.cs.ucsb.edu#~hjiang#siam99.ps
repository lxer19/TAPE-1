URL: http://www.cs.ucsb.edu/~hjiang/siam99.ps
Refering-URL: http://www.cs.ucsb.edu/~hjiang/
Root-URL: http://www.cs.ucsb.edu
Email: hjiang@cs.ucsb.edu  joy@cs.ucsb.edu  kshen@cs.ucsb.edu  tyang@cs.ucsb.edu  
Title: Efficient Sparse LU Factorization with Lazy Space Allocation  
Author: Bin Jiang Steven Richman kai Shen Tao Yang 
Address: Santa Barbara, CA 93106.  Santa Barbara, CA 93106.  Santa Barbara, CA 93106.  Santa Barbara, CA 93106.  
Affiliation: Department of Computer Science, University of California  Department of Computer Science, University of California  Department of Computer Science, University of California  Department of Computer Science, University of California  
Abstract: Static symbolic factorization coupled with 2D supernode partitioning and asynchronous computation scheduling can achieve high gigaflop rates for sparse LU with pivoting on distributed memory machines. This paper studies the space requirement of this approach and proposes an optimization strategy called lazy space allocation which acquires memory on-the-fly only when it is necessary. This strategy can effectively control memory usage, especially when static symbolic factorization overestimates fill-ins excessively. Our experiments show that the new LU code, which combines this strategy with elimination-forest guided partitioning and scheduling, has sequential time and space cost competitive with SuperLU, is space scalable for solving problems of large sizes on multiple processors, and can deliver up to 10 GFLOPS on 128 Cray 450Mhz T3E nodes. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Ashcraft, R. Grimes, J. Lewis, B. Peyton, and H. Simon. </author> <title> Progress in Sparse Matrix Methods for Large Sparse Linear Systems on Vector Supercomputers. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 1 </volume> <pages> 10-30, </pages> <year> 1987. </year>
Reference-contexts: Our design for LU task scheduling using the above forest concept is different from the ones for Cholesky <ref> [1, 12] </ref> because pivoting and row interchanges complicate the flow control in LU. Using Theorem 6, we are able to exploit some parallelism among F actor () tasks.
Reference: [2] <author> T. Davis and I. S. Duff. </author> <title> An Unsymmetric-pattern Multifrontal Method for Sparse LU factorization. </title> <journal> SIAM Matrix Analysis & Applications, </journal> <month> January </month> <year> 1997. </year>
Reference-contexts: We have compared our sequential code with SuperLU, but not UMFPACK <ref> [2] </ref> because SuperLU has been shown competitive to UMFPACK [3]. The following benchmark matrices are used from various application domains: af23560, e40r0100, fidap011, godwin, jpwh991, lns3937, lnsp3937, memplus, orsreg1, raefsky4, saylr4, sherman3, sherman5, TIa, TIb, TId and wang3. All matrices are ordered using the minimum degree algorithm.

Reference: [4] <author> J. Demmel, J. Gilbert, and X. Li. </author> <title> An Asynchronous Parallel Supernodal Algorithm for Sparse Gaussian Elimination. </title> <type> Technical Report CSD-97-943, </type> <institution> EECS Department, UC Berkeley, </institution> <month> February </month> <year> 1997. </year> <note> To appear in SIAM J. Matrix Anal. Appl. </note>
Reference-contexts: When pivoting is required to maintain numerical stability in using direct methods for solving non-symmetric linear systems, it is hard to develop high performance parallel code because dynamic partial pivoting causes severe caching miss and load imbalance on modern architectures with memory hierarchies. The previous work such as SuperLU <ref> [4] </ref> has addressed parallelization using shared memory platforms.
Reference: [5] <author> I. S. Duff. </author> <title> On Algorithms for Obtaining a Maximum Transversal. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 7(3) </volume> <pages> 315-330, </pages> <month> September </month> <year> 1981. </year>
Reference-contexts: Notice that for any nonsingular matrix which does not have a zero-free diagonal, it is always possible to permute the rows of the matrix so that the permuted matrix has a zero-free diagonal <ref> [5] </ref>. We will use the following notations in the rest of this section. We will still call the matrix after symbolic factorization as A since this paper assumes the symbolic factorization is conducted first.
Reference: [6] <author> C. Fu, X. Jiao, and T. Yang. </author> <title> Efficient Sparse LU Factorization with Partial Pivoting on Distributed Memory Architectures. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 9(2) </volume> <pages> 109-125, </pages> <month> February </month> <year> 1998. </year>
Reference-contexts: The dynamic factorization, which is used in the sequential and share-memory versions of SuperLU [11], provides more accurate data structure prediction on the fly, but it is challenging to parallelize SuperLU with low runtime control overhead on distributed memory machines. In <ref> [6, 7] </ref>, we show that static factorization does not produce too many fill-ins for most of the tested matrices, even for large matrices using a simple matrix ordering strategy (minimum degree ordering).
Reference: [7] <author> C. Fu and T. Yang. </author> <title> Sparse LU Factorization with Partial Pivoting on Distributed Memory Machines. </title> <booktitle> In Proceedings of ACM/IEEE Supercomputing, </booktitle> <address> Pittsburgh, </address> <month> November </month> <year> 1996. </year>
Reference-contexts: The previous work such as SuperLU [4] has addressed parallelization using shared memory platforms. For distributed memory machines, in <ref> [7] </ref> we proposed an approach that adopts a static symbolic factorization scheme [9] to avoid data structure variation, identifies data regularity to maximize the use of BLAS-3 operations, and utilizes graph scheduling techniques and efficient run-time support [8] to exploit irregular parallelism. <p> The dynamic factorization, which is used in the sequential and share-memory versions of SuperLU [11], provides more accurate data structure prediction on the fly, but it is challenging to parallelize SuperLU with low runtime control overhead on distributed memory machines. In <ref> [6, 7] </ref>, we show that static factorization does not produce too many fill-ins for most of the tested matrices, even for large matrices using a simple matrix ordering strategy (minimum degree ordering). <p> We do not need to check any constraint on U because as long as a child-parent pair (i; i 1) satisfies jL i j = jL i1 j 1, we can show that jU i j = jU i1 j 1 based on Theorem 1 in <ref> [7] </ref> and hence the structures of U i and U i1 are identical. For most of the tested sparse matrices in our experiments, the average supernode size after the above partitioning strategy is very small, about 1.5 to 2 columns. This leads to relatively fine grained computation.
Reference: [8] <author> C. Fu and T. Yang. </author> <title> Space and Time Efficient Execution of Parallel Irregular Computations. </title> <booktitle> In Proceedings of ACM Symposium on Principles & Practice of Parallel Programming, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: For distributed memory machines, in [7] we proposed an approach that adopts a static symbolic factorization scheme [9] to avoid data structure variation, identifies data regularity to maximize the use of BLAS-3 operations, and utilizes graph scheduling techniques and efficient run-time support <ref> [8] </ref> to exploit irregular parallelism. Recently [13] we have further studied the properties of elimination forests to guide supernode partitioning/amalgamation and execution scheduling.
Reference: [9] <author> A. George and E. Ng. </author> <title> Parallel Sparse Gaussian Elimination with Partial Pivoting. </title> <journal> Annals of Operations Research, </journal> <volume> 22 </volume> <pages> 219-240, </pages> <year> 1990. </year>
Reference-contexts: The previous work such as SuperLU [4] has addressed parallelization using shared memory platforms. For distributed memory machines, in [7] we proposed an approach that adopts a static symbolic factorization scheme <ref> [9] </ref> to avoid data structure variation, identifies data regularity to maximize the use of BLAS-3 operations, and utilizes graph scheduling techniques and efficient run-time support [8] to exploit irregular parallelism. Recently [13] we have further studied the properties of elimination forests to guide supernode partitioning/amalgamation and execution scheduling. <p> Section 5 presents the experimental results on Cray T3E. Section 6 concludes the paper. 2 Background 2.1 Static Symbolic Factorization Static symbolic factorization is proposed in <ref> [9] </ref> to identify the worst case nonzero patterns without knowing numerical values of elements. The basic idea is to statically consider all the possible pivoting choices at each elimination step and the space is allocated for all the possible nonzero entries.
Reference: [10] <author> X. Jiao. </author> <title> Parallel Sparse Gaussian Elimination with Partial Pivoting and 2-D Data Mapping. </title> <type> Master's thesis, </type> <institution> Dept. of Computer Science, University of California at Santa Barbara, </institution> <month> August </month> <year> 1997. </year>
Reference-contexts: The basic idea is to statically consider all the possible pivoting choices at each elimination step and the space is allocated for all the possible nonzero entries. Using an efficient 1 implementation of the symbolic factorization algorithm <ref> [10] </ref>, this preprocessing step can be very fast. For example, it costs less than one second for most of our tested matrices, at worst it costs 2 seconds on a single node of Cray T3E, and the memory requirement is relatively small.
Reference: [11] <author> X. Li. </author> <title> Sparse Gaussian Elimination on High Performance Computers. </title> <type> PhD thesis, </type> <institution> Computer Science Division, EECS, UC Berkeley, </institution> <year> 1996. </year>
Reference-contexts: For example, it costs less than one second for most of our tested matrices, at worst it costs 2 seconds on a single node of Cray T3E, and the memory requirement is relatively small. The dynamic factorization, which is used in the sequential and share-memory versions of SuperLU <ref> [11] </ref>, provides more accurate data structure prediction on the fly, but it is challenging to parallelize SuperLU with low runtime control overhead on distributed memory machines.
Reference: [12] <author> E. Rothberg. </author> <title> Exploiting the Memory Hierarchy in Sequential and Parallel Sparse Cholesky Factorization. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Stanford, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: Our design for LU task scheduling using the above forest concept is different from the ones for Cholesky <ref> [1, 12] </ref> because pivoting and row interchanges complicate the flow control in LU. Using Theorem 6, we are able to exploit some parallelism among F actor () tasks.
Reference: [13] <author> K. Shen, X. Jiao, and T. Yang. </author> <title> Elimination Forest Guided 2D Sparse LU Factorization. </title> <booktitle> In Proc. of ACM Symp. on Parallel Architectures and Algorithms(SPAA'98), </booktitle> <month> June </month> <year> 1998. </year> <month> 11 </month>
Reference-contexts: For distributed memory machines, in [7] we proposed an approach that adopts a static symbolic factorization scheme [9] to avoid data structure variation, identifies data regularity to maximize the use of BLAS-3 operations, and utilizes graph scheduling techniques and efficient run-time support [8] to exploit irregular parallelism. Recently <ref> [13] </ref> we have further studied the properties of elimination forests to guide supernode partitioning/amalgamation and execution scheduling.
References-found: 12

