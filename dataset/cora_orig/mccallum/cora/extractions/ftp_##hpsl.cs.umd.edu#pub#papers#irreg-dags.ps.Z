URL: ftp://hpsl.cs.umd.edu/pub/papers/irreg-dags.ps.Z
Refering-URL: http://www.cs.umd.edu/projects/hpsl/papers.brandnew/LocalResources/tech-10-23.htm
Root-URL: 
Email: ftchong@ai.mit.edu  shamik@cs.umd.edu  brewer@lcs.mit.edu  saltz@cs.umd.edu  
Title: To appear in Toward Teraflop Computing and New Grand Challenge Applications. Multiprocessor Runtime Support for
Author: Eds. Rajiv K. Frederic T. Chong Shamik D. Sharma Eric A. Brewer Joel Saltz 
Affiliation: Massachusetts Institute of Technology University of Maryland  
Note: Kalia and Priya Vashishta. Nova Science Publishers, Inc. New York. 1995.  
Abstract: We examine multiprocessor runtime support for fine-grained, irregular directed acyclic graphs (DAGs) such as those that arise from sparse-matrix triangular solves. We conduct our experiments on the CM-5, whose lower latencies and active-message support allow us to achieve unprecedented speedups for a general multiprocessor. Where as previous implementations have maximum speedups of less than 4 on even simple banded matrices, we are able to obtain scalable performance on extremely small and irregular problems. On a matrix with only 5300 rows, we are able to achieve scalable performance with a speedup of 34 for 128 processors, resulting in an absolute performance of over 33 million double-precision floating point operations per second. We achieve these speedups with non-matrix-specific methods which are applicable to any DAG. We compare a range of run-time preprocessed and dynamic approaches on matrices from the Harwell-Boeing benchmark set. Although precomputed data distributions and execution schedules produce the best performance, we find that it is challenging to keep their cost low enough to make them worthwhile on small, fine-grained problems. Additionally, we find that a policy of frequent network polling can reduce communication overhead by a factor of three over the standard CM-5 policies. We present a detailed study of runtime overheads and demonstrate that send and receive processor overhead still dominate these applications on the CM-5. We conclude that these applications would highly benefit from architectural support for low-overhead communication.
Abstract-found: 1
Intro-found: 1
Reference: [ACJ + 91] <author> Anant Agarwal, David Chaiken, Kirk Johnson, David Kranz, John Kubiatowicz, Kiyoshi Kuri-hara, Beng-Hong Lim, Gino Maa, and Dan Nussbaum. </author> <title> The MIT Alewife machine: a large-scale distributed-memory multiprocessor. MIT/LCS/TM 454, </title> <publisher> MIT, </publisher> <address> 545 Technology Sq., Cambridge, MA 02139, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Newer systems promise to deliver much lower overheads which directly translates to much higher performance on our applications. The T3D [T3D93], for example, can send a message in 4 cycles and receive a message with similar overhead. The Alewife <ref> [ACJ + 91] </ref> shared-memory multiprocessor has a memory controller which reduces processor overhead to similar levels. The StarT [Bec93] architecture has an on-processor message interface and can send 256 bits in one clock cycle and can receive 128 bits in one cycle.
Reference: [ACM88] <author> Arvind, David E. Culler, and Gino K. Maa. </author> <title> Assessing the benefits of fine-grained parallelism in dataflow programs. </title> <booktitle> In Supercomputing `88. IEEE, </booktitle> <year> 1988. </year>
Reference-contexts: ARPA (NAG-1-1485), ONR (SC292-1-22913), and NSF (ASC 9213821). do i = 1, n do j = low (i), high (i) x (i) = x (i) - t (j)*x (column (j)) end do Our methods depend heavily upon active messages [E + 92] and are closely related to the dataflow paradigm <ref> [ACM88] </ref>. Active messages greatly reduce overhead by providing user-level communications. We also take full advantage of active message handlers to decrease synchronization and data-copying costs. These mechanisms allow us to take a data-driven approach with fine-grained synchronization. This approach, coupled with reasonable hardware support, facilitates much of our speedups. <p> In order to ensure such synchronization, we use presence counters <ref> [ACM88] </ref>. A presence counter is used 5 to count the number of items that have arrived. If the computation to be performed on incoming data elements is non-commutative, the presence counter must be placed at the beginning of each task to check for the arrival of all data before computation.
Reference: [AS92] <author> Fernando Alvarado and Robert Schreiber. </author> <title> Optimal parallel solution of sparse triangular systems. </title> <journal> SIAM Journal of Scientific and Statistical Computation, </journal> <note> 1992. to appear. </note>
Reference-contexts: We would like to note that in applications where the solve is repeated several times, it may be more efficient to invert the triangular matrix, converting the solve into an easily parallelizable matrix vector multiplication <ref> [AS92] </ref>. It is possible to eliminate fill-in during the inversion [PA92].
Reference: [BB94] <author> Eric A. Brewer and Robert D. Blumofe. Strata: </author> <title> A high-performance communications library. </title> <note> Technical Report (to appear). </note> <institution> MIT Laboratory for Computer Science, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: It also has a control network for global operations. To keep our study architecturally general, we do not use the vector units, nor do we use the control network. 3.2 Strata For efficient communication and accurate timing, we made extensive use of the Strata communications library <ref> [BB94] </ref>. Strata is a CMMD-compatible [Thi93b] package with high-performance communication and extensive support for timing and debugging. Section 6 discusses the impact of using Strata's communication primitives. The standard CMMD timers on the CM-5 require kernel calls for each timing event, which cost hundreds of cycles.
Reference: [Bec93] <author> Michael J. Beckerle. </author> <title> Overview of the START(*T) multithreaded computer. </title> <booktitle> In Proceedings of IEEE COMPCON, </booktitle> <month> February </month> <year> 1993. </year>
Reference-contexts: The T3D [T3D93], for example, can send a message in 4 cycles and receive a message with similar overhead. The Alewife [ACJ + 91] shared-memory multiprocessor has a memory controller which reduces processor overhead to similar levels. The StarT <ref> [Bec93] </ref> architecture has an on-processor message interface and can send 256 bits in one clock cycle and can receive 128 bits in one cycle.
Reference: [BK94] <author> Eric A. Brewer and Bradley C. Kuszmaul. </author> <title> How to get good performance from the CM-5 data network. </title> <booktitle> In International Parallel Processing Symposium, </booktitle> <address> Cancun, Mexico, </address> <month> April </month> <year> 1994. </year> <note> To Appear. </note>
Reference-contexts: Section 7 covers this technique in more detail. In general, we timed events that lasted at most one-tenth of the 16.6 milliseconds between timer interrupts. This ensures that the cycle counts are rarely affected by the interrupts, time-slicing, or other processes. <ref> [BK94] </ref> covers CM-5 timing methodology in more detail. 4 Runtime Methods We adopt a data-driven approach to DAG computation. Active messages provide an efficient mechanism to communicate the data and synchronize the computation. At any given time, several tasks in the DAG may be ready to execute. <p> In general, the redistribution of data can be considered a global permutation. Global permutations on the CM-5 can be performed at roughly 2.5 Mbytes per second per processing node <ref> [BK94] </ref>. The upper triangular bcspwr10 matrix, for example, has 5300 rows, each represented by approximately 24 bytes. There is also a symmetric lower triangular matrix which we must store separately. In the 8 processor case, this gives each node approximately 32 Kbytes to contribute to the permutation. <p> Strata reduces communications overhead in our application up to a factor of three relative to the CMAML rpc version. The change from CMAML to Strata increases application performance by 25 to 30 percent. The importance of polling and the resulting improvements are consistent with the results in <ref> [BK94] </ref>. Given the limited capacity of the network, it is critical to use both sides of the network, which provides twice as much buffering. Secondarily, it is critical to poll on every injection to avoid stalling other processors.
Reference: [CY93] <author> Soumen Chakrabarti and Katherine Yelick. </author> <title> Implementing an irregular application on a distributed memory architecture. </title> <booktitle> In Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practices of Parallel Programming, </booktitle> <pages> pages 169-178, </pages> <address> San Diego, California, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Rubin [Rub92] has studied regular, fine-grained DAGs arising from a conjugate gradient problem. While Rubin performed his study on the Monsoon dataflow machine, Yeung and Agar-wal [YA93] have studied the same problem on Alewife, a shared-memory, SPARC-based multiprocessor. Chakrabarti and Yelick <ref> [CY93] </ref> have studied the Groebner-basis problem, which results in an irregular DAG. However, this computation is not as fine-grained as the applications we examine. The Groebner study used consumer-driven techniques, using multithreading to hide the latency of fetching data.
Reference: [D + 92] <author> William J. Dally et al. </author> <title> The message-driven processor: A multicomputer processing node with efficient mechanisms. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 23-39, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: The Alewife [ACJ + 91] shared-memory multiprocessor has a memory controller which reduces processor overhead to similar levels. The StarT [Bec93] architecture has an on-processor message interface and can send 256 bits in one clock cycle and can receive 128 bits in one cycle. The J-Machine <ref> [D + 92] </ref> first demonstrated such processor-interface integration, but was not implemented to support the floating point operations 11 necessary scientific applications such as our benchmarks. With low-overhead architectures, we can expect send and receive overhead of only 3 cycles per 24-byte message in our application.
Reference: [DGL92] <author> Ian S. Duff, Roger G. Grimes, and John G. Lewis. </author> <title> User's guide for the Harwell-Boeing sparse matrix collection. </title> <type> Technical Report TR/PA/92/86, CERFACS, 42 Ave G. Coriolis, </type> <address> 31057 Toulouse Cedex, France, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: Howeever, this scheme does not follow the DSC ordering strictly. When the next row in the schedule is not ready, we allow other rows to computed if they are ready for execution. 4.5 Benchmarks We chose four power grid matrices from the Harwell-Boeing <ref> [DGL92] </ref> benchmark set for our experiments. These sparse matrices come from the BCSPWR set, and represent parts of the Western US Power Network and the Eastern US Power Network. Our matrices were incompletely factored, with the sparsity pattern being identical to that of the original matrix.
Reference: [E + 92] <author> Thorsten von Eicken et al. </author> <title> Active messages: a mechanism for integrated communication and computation. </title> <booktitle> In Proceedings of the 19th Annual Symposium on Computer Architecture, </booktitle> <address> Queensland, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: IBM. x Joel Saltz is supported in part by EPRI (RP3103-06), ARPA (NAG-1-1485), ONR (SC292-1-22913), and NSF (ASC 9213821). do i = 1, n do j = low (i), high (i) x (i) = x (i) - t (j)*x (column (j)) end do Our methods depend heavily upon active messages <ref> [E + 92] </ref> and are closely related to the dataflow paradigm [ACM88]. Active messages greatly reduce overhead by providing user-level communications. We also take full advantage of active message handlers to decrease synchronization and data-copying costs. These mechanisms allow us to take a data-driven approach with fine-grained synchronization.
Reference: [ETM93] <institution> Analysis of performance accelerator running ETMSP. </institution> <type> Technical Report TR-102856, </type> <institution> Performance Processors, Inc., </institution> <address> Palo Alto, California 94301, </address> <month> October </month> <year> 1993. </year> <note> Research Project 8010-31. </note>
Reference-contexts: For example, the time spent performing solves is equal to or double that of the time spent factoring in the sequential execution of the ETMSP power grid code <ref> [ETM93] </ref>. We would like to note that in applications where the solve is repeated several times, it may be more efficient to invert the triangular matrix, converting the solve into an easily parallelizable matrix vector multiplication [AS92]. It is possible to eliminate fill-in during the inversion [PA92].
Reference: [GY92] <author> Apostolos Gerasoulis and Tao Yang. </author> <title> A comparison of clustering heuristics for scheduling directed acyclic graphs on multiprocessors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 16 </volume> <pages> 276-291, </pages> <year> 1992. </year>
Reference-contexts: We have examined different preprocessing schemes and show the performance impacts of each on our application. 4 4.1 Task Distribution For task distribution, we have considered three schemes block distribution, cyclic distribution and Dominant Sequence Clustering (DSC) <ref> [GY92] </ref>. We considered block and cyclic distributions of nodes across processors because of their simplicity. While these distributions do not optimize either communication or load balance, their computational preprocessing costs are neglibile.
Reference: [HR93] <author> Michael T. Heath and Padma Raghavan. </author> <title> Distributed solution of sparse linear systems. </title> <type> Technical Report UIUCDCS-R-93-1793, </type> <institution> Department of Computer Science, University of Illinois, Urbana, </institution> <address> IL 61801, </address> <month> February </month> <year> 1993. </year>
Reference-contexts: This problem is due to the extremely small grain of the computation. Each row depends only on a few other rows, and each dependency involves a single multiply-accumulate (generally double-precision floating point). Speedups are rarely reported for sparse triangular solves. Heath and Raghavan <ref> [HR93] </ref> report good results for factorization on the Intel iPSC/860, but their solves exhibit constant or decreasing performance as the number of processors increases. They obtain speedups of no more than 3 or 4 regardless of the number of processors, even for banded matrices.
Reference: [L + 92] <author> Charles E. Leiserson et al. </author> <title> The network architecture of the connection machine CM-5. </title> <booktitle> In Symposium on Parallel Architectures and Algorithms, </booktitle> <pages> pages 272-285, </pages> <address> San Diego, California, </address> <month> June </month> <year> 1992. </year> <note> ACM. </note>
Reference-contexts: Each node of the CM-5 has a 33 MHz SPARC processor as well as four vector units. The CM-5 has a fat-tree-based network <ref> [L + 92] </ref> which provides fairly uniform communications delays and sustainable bandwidth. It also has a control network for global operations.
Reference: [LBT87] <author> Robert F. Lucas, Tom Blank, and Jerome J. Tiemann. </author> <title> A parallel solution method for large sparse systems of equations. </title> <journal> IEEE Transactions on Computer-Aided Design, </journal> <volume> CAD-6(6):981-991, </volume> <month> November </month> <year> 1987. </year>
Reference-contexts: They obtain speedups of no more than 3 or 4 regardless of the number of processors, even for banded matrices. They downplay the importance of the solve by noting that the running time of the factorization phase dominates that of the solve phase. Lucas, Blank, and Tiemann <ref> [LBT87] </ref> report similar results. While the time spent in a single triangular solve is much less than the time spent in factorization, there are many applications where the solve time is important. Often there are tens to hundreds of solves performed using a single factorization.
Reference: [Moy91] <author> Steven A. Moyer. </author> <title> Performance of the iPSC/860 node architecture. </title> <type> Technical Report IPC-TR-91-007, </type> <institution> Institute for Parallel Computation, School of Engineering and Applied Science, University of Virginia, </institution> <address> Charlottesville, VA 22903, </address> <month> May </month> <year> 1991. </year> <month> 13 </month>
Reference-contexts: It is possible to eliminate fill-in during the inversion [PA92]. We have not investigated this technique in this paper. 0 We estimated speedups assuming a sequential solve speed similar to that of uncached DAXPY on a single iPSC/860 node <ref> [Moy91] </ref>. 3 3 Experimental Platform In this section, we describe the multiprocessor hardware and communications software that we use in our experiments. 3.1 CM-5 We perform our experiments on the CM-5 [Thi93a]. Each node of the CM-5 has a 33 MHz SPARC processor as well as four vector units.
Reference: [PA92] <author> Alex Pothen and Fernando Alvarado. </author> <title> A fast rerdering algorithm for parallel sparse triangular solution. </title> <journal> SIAM Journal of Scientific and Statistical Computation, </journal> <volume> 13 </volume> <pages> 645-653, </pages> <year> 1992. </year>
Reference-contexts: We would like to note that in applications where the solve is repeated several times, it may be more efficient to invert the triangular matrix, converting the solve into an easily parallelizable matrix vector multiplication [AS92]. It is possible to eliminate fill-in during the inversion <ref> [PA92] </ref>.
Reference: [Rub92] <author> Norman Rubin. </author> <title> Data flow computing and the conjugate gradient method. </title> <type> Technical Report MCRC-TR-25, </type> <institution> Motorola Cambridge Research Center, </institution> <year> 1992. </year>
Reference-contexts: These mechanisms allow us to take a data-driven approach with fine-grained synchronization. This approach, coupled with reasonable hardware support, facilitates much of our speedups. Surprisingly, there have been few studies of irregular, fine-grained DAGs in the dataflow community. Rubin <ref> [Rub92] </ref> has studied regular, fine-grained DAGs arising from a conjugate gradient problem. While Rubin performed his study on the Monsoon dataflow machine, Yeung and Agar-wal [YA93] have studied the same problem on Alewife, a shared-memory, SPARC-based multiprocessor.
Reference: [Sal90] <author> Joel H. Saltz. </author> <title> Aggregation methods for solving sparse triangular systems on multiprocessors. </title> <journal> SIAM Journal of Scientific and Statistical Computing, </journal> <volume> 11(1) </volume> <pages> 123-144, </pages> <month> Jan </month> <year> 1990. </year>
Reference-contexts: The data driven scheme automatically ensures that all dependencies are respected. Instead of adopting a data-driven method, we can obtain an ordering via preprocessing. For example, global level scheduling, as in <ref> [Sal90] </ref>, partitions the nodes of the DAG into levels L 1 : : : L n , such that the nodes in level L i depend only on nodes in L 1 : : : L i1 . Each processor orders its local nodes based on their levels.
Reference: [Sar89] <author> Vivek Sarkar. </author> <title> Partitioning and Scheduling Parallel Programs for Execution on Multiprocessors. </title> <publisher> MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: At the opposite end of the spectrum there are computation-intensive task clustering schemes which look at the entire DAG in order to minimize the critical path length of the DAG <ref> [Sar89] </ref> [Yan93]. For example, Dominant Sequence Clustering, obtains an allocation by incrementally reducing the critical path length through successive clustering steps. Comparisons show that DSC outperforms most other static-task allocation scheme in both processing time and quality of computation schedules.
Reference: [SMC91] <author> Joel H. Saltz, Ravi Mirchandaney, and Kay Crowley. </author> <title> Run-time parallelization and scheduling of loops. </title> <journal> IEEE Transactions on Computers, </journal> <pages> pages 603-611, </pages> <year> 1991. </year>
Reference-contexts: 1 Introduction Our goal is to execute fine-grained, irregular directed acyclic graphs (DAGs) efficiently on general-purpose, distributed-memory multiprocessors. We explore a range of runtime methods, some of which involve runtime preprocessing, an approach shown to be effective with iterative computations <ref> [SMC91] </ref>. We test our methods on an important real-world application, sparse-matrix triangular solves. <p> Active messages provide an efficient mechanism to communicate the data and synchronize the computation. At any given time, several tasks in the DAG may be ready to execute. The next task may be selected dynamically at compute-time. However, we may also adopt preprocessed schemes, commonly known as inspector-executor strategies <ref> [SMC91] </ref>. A preprocessing step (called an inspector) is used to optimize the execution of the actual computation step (called the executor). The runtime cost of the one-time preprocessing step is amortized over all executions of the executor.
Reference: [T3D93] <institution> CRAY T3D system architecture overview. </institution> <type> Technical report, </type> <institution> Cray Research, Inc., 900 Lowater Road, Chippewa Falls, WI 54729, </institution> <month> September </month> <year> 1993. </year>
Reference-contexts: Newer systems promise to deliver much lower overheads which directly translates to much higher performance on our applications. The T3D <ref> [T3D93] </ref>, for example, can send a message in 4 cycles and receive a message with similar overhead. The Alewife [ACJ + 91] shared-memory multiprocessor has a memory controller which reduces processor overhead to similar levels.
Reference: [Thi93a] <institution> Thinking Machines Corporation, Cambridge, MA. </institution> <type> CM-5 Technical Summary, </type> <month> November </month> <year> 1993. </year>
Reference-contexts: We estimated speedups assuming a sequential solve speed similar to that of uncached DAXPY on a single iPSC/860 node [Moy91]. 3 3 Experimental Platform In this section, we describe the multiprocessor hardware and communications software that we use in our experiments. 3.1 CM-5 We perform our experiments on the CM-5 <ref> [Thi93a] </ref>. Each node of the CM-5 has a 33 MHz SPARC processor as well as four vector units. The CM-5 has a fat-tree-based network [L + 92] which provides fairly uniform communications delays and sustainable bandwidth. It also has a control network for global operations.
Reference: [Thi93b] <institution> Thinking Machines Corporation, </institution> <address> Cambridge, MA. </address> <note> CMMD Reference Manual (Version 3.0), </note> <month> May </month> <year> 1993. </year>
Reference-contexts: To keep our study architecturally general, we do not use the vector units, nor do we use the control network. 3.2 Strata For efficient communication and accurate timing, we made extensive use of the Strata communications library [BB94]. Strata is a CMMD-compatible <ref> [Thi93b] </ref> package with high-performance communication and extensive support for timing and debugging. Section 6 discusses the impact of using Strata's communication primitives. The standard CMMD timers on the CM-5 require kernel calls for each timing event, which cost hundreds of cycles.
Reference: [YA93] <author> Donald Yeung and Anant Agarwal. </author> <title> Experience with fine-grain synchronization in MIMD machines for preconditioned conjugate gradient. </title> <booktitle> In Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practices of Parallel Programming, </booktitle> <pages> pages 187-197, </pages> <address> San Diego, California, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Surprisingly, there have been few studies of irregular, fine-grained DAGs in the dataflow community. Rubin [Rub92] has studied regular, fine-grained DAGs arising from a conjugate gradient problem. While Rubin performed his study on the Monsoon dataflow machine, Yeung and Agar-wal <ref> [YA93] </ref> have studied the same problem on Alewife, a shared-memory, SPARC-based multiprocessor. Chakrabarti and Yelick [CY93] have studied the Groebner-basis problem, which results in an irregular DAG. However, this computation is not as fine-grained as the applications we examine.
Reference: [Yan93] <author> Tao Yang. </author> <title> Scheduling and code generation for parallel architectures. </title> <type> Technical Report DCS-TR-299, </type> <institution> Rutgers, </institution> <month> May </month> <year> 1993. </year> <month> 14 </month>
Reference-contexts: At the opposite end of the spectrum there are computation-intensive task clustering schemes which look at the entire DAG in order to minimize the critical path length of the DAG [Sar89] <ref> [Yan93] </ref>. For example, Dominant Sequence Clustering, obtains an allocation by incrementally reducing the critical path length through successive clustering steps. Comparisons show that DSC outperforms most other static-task allocation scheme in both processing time and quality of computation schedules. <p> Currently, however, we perform level scheduling sequentially. While level scheduling is a reasonable heuristic, it does not try to schedule rows in an order that can overlap computation with communication. For example, the PYRROS package <ref> [Yan93] </ref>, which generates the DSC clusters, also provides a task ordering based on the Ready Critical Path heuristic. This heuristic tries to schedule those tasks on the critical path first so that dependent nodes can be executed earlier.
References-found: 26

