URL: ftp://ftp.cs.washington.edu/tr/1994/09/UW-CSE-94-09-12.PS.Z
Refering-URL: http://www.cs.washington.edu/homes/levy/opal/opalpapers.html
Root-URL: 
Email: ffeeley,chase,nara,levyg@cs.washington.edu  
Title: Integrating Coherency and Recoverability in Distributed Systems  
Author: Michael J. Feeley, Jeffrey S. Chase, Vivek R. Narasayya, and Henry M. Levy 
Date: November 1994.  
Note: First published in the "Proceedings of the First Symposium on Operating Systems Design and Implementation," Usenix Association,  
Address: Seattle, WA 98195  
Affiliation: Department of Computer Science and Engineering, FR-35 University of Washington  
Abstract: We propose a technique for maintaining coherency of a transactional distributed shared memory, used by applications accessing a shared persistent store. Our goal is to improve support for fine-grained distributed data sharing in collaborative design applications, such as CAD systems and software development environments. In contrast, traditional research in distributed shared memory has focused on supporting parallel programs; in this paper, we show how distributed programs can benefit from this shared-memory abstraction as well. Our approach, called log-based coherency, integrates coherency support with a standard mechanism for ensuring recoverability of persistent data. In our system, transaction logs are the basis of both recoverabil-ity and coherency. We have prototyped log-based coherency as a set of extensions to RVM [Satyanarayanan et al. 94], a runtime package supporting recoverable virtual memory. Our prototype adds coherency support to RVM in a simple way that does not require changes to existing RVM applications. We report on our prototype and its performance, and discuss its relationship to other DSM systems. 
Abstract-found: 1
Intro-found: 1
Reference: [Butterworth et al. 92] <author> Butterworth, P., Otis, A., and Stein, J. </author> <title> The GemStone object database management system. </title> <journal> Communications of the ACM, </journal> <volume> 34(10) </volume> <pages> 64-77, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: A number of persistent stores have been built; some are research systems [Cockshott et al. 84, Moss 90, Carey et al. 94] and others are commercial object-oriented database (OODB) products (e.g., <ref> [Butterworth et al. 92, Lamb et al. 91, O. Deux 92] </ref>), augmented with database features such as query processors, schema languages, and indexing facilities.
Reference: [Carey et al. 93] <author> Carey, M. J., Dewitt, D. J., and Naughton, J. F. </author> <title> The OO7 benchmark. </title> <booktitle> 1993 ACM SIGMOD. International Conference on Management of Data, </booktitle> <volume> 22(2) </volume> <pages> 12-21, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: While most DSM systems have used parallel programs for evaluation, our application domain | collaborative design applications accessing a distributed persistent store | requires a different benchmark. For this reason, we have chosen to use OO7 <ref> [Carey et al. 93] </ref>, a standard object-oriented database benchmark. OO7 consists of a number of different traversals, updates, and queries of a synthetic object-oriented database. The database and the traversal tests are intended to be suggestive of typical engineering database applications.
Reference: [Carey et al. 94] <author> Carey, M. J., Dewitt, D. J., and Franklin, M. J. </author> <title> Shoring up persistent applications. </title> <booktitle> 1994 ACM SIGMOD. International Conference on Management of Data, </booktitle> <month> May </month> <year> 1994. </year> <note> [Carter et al. 91] Carter, </note> <author> J., Bennet, J., and Zwaenepoel, W. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems and Principles, </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Client programs navigate through the stored data by following pointers directly in virtual memory, with the sys tem moving data between memory and the persistent store as needed. A number of persistent stores have been built; some are research systems <ref> [Cockshott et al. 84, Moss 90, Carey et al. 94] </ref> and others are commercial object-oriented database (OODB) products (e.g., [Butterworth et al. 92, Lamb et al. 91, O. Deux 92]), augmented with database features such as query processors, schema languages, and indexing facilities.
Reference: [Cockshott et al. 84] <author> Cockshott, W., Atkinson, M., Chisholm, K., Bailey, P., and Morrison, R. </author> <title> Persistent object management system. </title> <journal> Software Practice and Experience, </journal> <volume> 14(1), </volume> <month> Jan-uary </month> <year> 1984. </year>
Reference-contexts: Client programs navigate through the stored data by following pointers directly in virtual memory, with the sys tem moving data between memory and the persistent store as needed. A number of persistent stores have been built; some are research systems <ref> [Cockshott et al. 84, Moss 90, Carey et al. 94] </ref> and others are commercial object-oriented database (OODB) products (e.g., [Butterworth et al. 92, Lamb et al. 91, O. Deux 92]), augmented with database features such as query processors, schema languages, and indexing facilities.
Reference: [Delis & Roussopoulos 92] <author> Delis, A. and Roussopou-los, N. </author> <title> Performance and scalability of client-server database architectures. </title> <booktitle> In Proceedings of the 18th International Conference on Very Large Databases, </booktitle> <pages> pages 610-623, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Their purpose is to allow checkpointing to take place in the standby, off-line, without interfering with clients executing on the primary copy of the database. Delis and Roussopoulos conducted a simulation study of client-server relational databases using a log-based approach for updating client caches <ref> [Delis & Roussopoulos 92] </ref>. Updates are centralized at the server; the server maintains a recovery log and a separate update log.
Reference: [Dig 92] <institution> Digital Equipment Corporation, Maynard, MA. Alpha Architecture Handbook, </institution> <year> 1992. </year>
Reference-contexts: We report timings and related overheads for several OO7 traversals using our prototype. These experiments were conducted using two Digital 3000-400 Alpha APX (133 Mhz, 74 SPECints) workstations with 8-Kbyte pages and separate 512-Kbyte direct-mapped instruction and data caches <ref> [Dig 92] </ref>. The machines are connected by a 100 Mbit/s AN1 network, an experimental switch-based network capable of sending message packets of up to 64 Kbytes [Rodeheffer & Schroeder 91]. Elapsed time measurements were taken using the Alpha cycle counter.
Reference: [Eppinger et al. 91] <author> Eppinger, J., Mummert, L., and Spector, A. </author> <title> Camelot and Avalon. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: We refer to it as a cached persistent store since the ideas generalize to persistent virtual memory systems that do not provide full database features. Note that cached persistent stores are distinct from transactional systems for reliable distributed programming (e.g., Argus [Liskov 88] and Camelot <ref> [Eppinger et al. 91] </ref>) in that the database itself is not distributed; each transaction updates a cached image of a centralized database in virtual memory on a single node. <p> In the event of failure, a recovery procedure restores the database to a consistent state by replaying the committed log records into the permanent database file. Many recoverable systems use write-ahead redo logging; for example, it is fundamental to the pin/update/log commit protocol used by Camelot <ref> [Eppinger et al. 91] </ref>. When multiple clients are accessing the store through a network, the log records are written to a logically centralized storage service that also holds the permanent database file. A client failure aborts all uncommitted transactions executing in that client.
Reference: [Feeley & Levy 92] <author> Feeley, M. J. and Levy, H. M. </author> <title> Distributed shared memory with versioned objects. </title> <booktitle> In Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, </booktitle> <month> Octo-ber </month> <year> 1992. </year>
Reference-contexts: In this scheme, pending log records must be buffered in the recipient until they can be applied. We have used a similar version-based consistency model in the past for a range of parallel applications <ref> [Feeley & Levy 92] </ref>. 2.2 Propagating Log Records Our prototype uses a simple eager policy for propagating updates.
Reference: [Gharachorloo et al. 90] <author> Gharachorloo, K., Lenoski, D., Laudon, J., Gibbons, P., Gupta, A., and Hennessy, J. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proc. 17th Annual Symposium on Computer Architecture, Computer Architecture News, </booktitle> <pages> pages 15-26. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1990. </year>
Reference-contexts: In these systems, page-grain locking and page-grain coherency led to performance problems caused by false sharing. Newer high-performance DSM systems based on release consistency <ref> [Gharachorloo et al. 90] </ref> (e.g., Munin), lazy release consistency [Keleher et al. 92] (TreadMarks) and entry consistency (Midway) have reduced this problem by supplying synchronization primitives that function independently of the coherency protocol, and drive the propagation of updates.
Reference: [Hagmann 86] <author> Hagmann, R. B. </author> <title> A crash recovery scheme for a memory-resident database system. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-35(9):839-843, </volume> <month> September </month> <year> 1986. </year>
Reference-contexts: This is important in part because optimizations using non-volatile RAM can be used to eliminate synchronous disk writes from the commit critical path <ref> [Hagmann 86] </ref>. The figures also depict estimated lower bounds for alternative DSM implementations that use page access faults to capture updates. For log-based coherency, updates are captured by calls to the RVM set range procedure.
Reference: [Hornick & Zdonik 87] <author> Hornick, M. F. and Zdonik, S. B. </author> <title> A shared, segmented memory system for an object-oriented database. </title> <journal> A ACM Transactions on Office Informations Systems, </journal> <volume> 5(1), </volume> <month> January </month> <year> 1987. </year>
Reference-contexts: We believe that alternative synchronization models can be implemented with a modest effort, by replacing the synchronization primitives and the calls they make to underlying logging and coherency routines. Several relaxed models have been proposed by researchers arguing that strict serializability is inappropriate for design transactions (e.g., <ref> [Hornick & Zdonik 87] </ref>). We are exploring a read/write model that permits readers to operate on a previous consistent version of the data while an update is in progress elsewhere; readers use an accept primitive to explicitly signal their willingness to move forward to a newer consistent version.
Reference: [Hosking & Moss 93] <author> Hosking, A. L. and Moss, J. E. B. </author> <title> Protection traps and alternatives for memory managemnt of an object-oriented language. </title> <booktitle> In Proceedings of the 14th ACM Symposium on Operating Systems Principles, </booktitle> <month> December </month> <year> 1993. </year>
Reference: [Janssens & Fuchs 93] <author> Janssens, B. and Fuchs, W. K. </author> <title> Relaxing consistency in recoverable distributed shared memory. </title> <booktitle> In Proceedings of the Twenty-Third Annual International Symposium on Fault-Tolerant Computing: Digest of Papers, </booktitle> <pages> pages 155-163, </pages> <month> June </month> <year> 1993. </year> <note> [Keleher et al. 92] Keleher, </note> <author> P., Cox, A., and Zwaenepoel, W. </author> <title> Lazy release consistency for software distributed shared memory. </title> <booktitle> In Proceedings of the 19th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: A node writes its volatile log to disk before transferring a modified page to another node. Janssens and Fuchs added checkpointing to relaxed-consistency DSM <ref> [Janssens & Fuchs 93] </ref>. Instead of requiring checkpointing or other recoverability actions each time an application gains access to a page, their system checkpoints only when a node releases or acquires a lock.
Reference: [Keleher et al. 94] <author> Keleher, P., Cox, A. L., Dwarkadas, S., and Zwaenepoel, W. TreadMarks: </author> <title> Distributed shared memory on standard workstations and operating systems. </title> <booktitle> In Proceedings of the Winder 1994 USENIX Conference, </booktitle> <pages> pages 115-132, </pages> <month> January </month> <year> 1994. </year>
Reference: [Lamb et al. 91] <author> Lamb, C., Landis, G., Orenstein, J., and Weinreb, D. </author> <title> The ObjectStore database system. </title> <journal> Communications of the ACM, </journal> <volume> 34(10) </volume> <pages> 50-63, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: A number of persistent stores have been built; some are research systems [Cockshott et al. 84, Moss 90, Carey et al. 94] and others are commercial object-oriented database (OODB) products (e.g., <ref> [Butterworth et al. 92, Lamb et al. 91, O. Deux 92] </ref>), augmented with database features such as query processors, schema languages, and indexing facilities.
Reference: [Li & Hudak 89] <author> Li, K. and Hudak, P. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: 1 Introduction Existing distributed shared memory (DSM) systems support parallel programming on distributed-memory multicomputers and workstation networks. Examples of such systems include IVY <ref> [Li & Hudak 89] </ref>, Munin [Carter et al. 91], TreadMarks [Keleher et al. This work is supported in part by the National Science Foundation (Grants No. CDA-9123308 and CCR-9200832), the Washington Technology Center, Digital Equipment Corporation, Boeing Computer Services, Intel Corporation, Hewlett-Packard Corporation, and Apple Computer. <p> The following subsections discuss underlying concepts, the design choices we made, and some alternative possibilities. 2.1 The Role of Synchronization In DSM systems, the method and timing of update propagation is closely tied to synchronization events. The first DSM systems (Monads [Rosenberg & Abram-son 85] and IVY <ref> [Li & Hudak 89] </ref>) used virtual page protections and reference traps to capture updates and synchronize access to shared pages. In these systems, page-grain locking and page-grain coherency led to performance problems caused by false sharing.
Reference: [Li & Naughton 88] <author> Li, L. and Naughton, J. F. </author> <title> Multiprocessor main memory transaction processing. </title> <booktitle> In Proceedings of the International Symposium on Databases in Parallel and Distributed Systems, </booktitle> <pages> pages 177-187, </pages> <month> De-cember </month> <year> 1988. </year>
Reference-contexts: Harp [Liskov et al. 92] file servers log received updates to peer servers in order to remove stable storage writes from the commit path. Similarly, Naughton and Li have used log propagation to keep a hot standby of a main-memory database <ref> [Li & Naughton 88] </ref>. The standby keeps a complete copy of the database in its memory and receives updates from the primary in the form of log records sent at commit points.
Reference: [Liskov 88] <author> Liskov, B. </author> <title> Distributed programming in Argus. </title> <journal> Communications of the ACM, </journal> <volume> 31(3) </volume> <pages> 300-312, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: We refer to it as a cached persistent store since the ideas generalize to persistent virtual memory systems that do not provide full database features. Note that cached persistent stores are distinct from transactional systems for reliable distributed programming (e.g., Argus <ref> [Liskov 88] </ref> and Camelot [Eppinger et al. 91]) in that the database itself is not distributed; each transaction updates a cached image of a centralized database in virtual memory on a single node.
Reference: [Liskov et al. 92] <author> Liskov, B., Ghemawat, S., Gruber, R., Johnson, P., and Shrira, L. </author> <title> Replication in the Harp file system. </title> <booktitle> In Proceedings of the Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 226-238, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: Servers cooperate to ensure that log records are applied in a consistent order at all locations. In our system, the replicas are caches that keep only enough data to meet the local client's needs; update propagation may be delayed until a client requests the new data. Harp <ref> [Liskov et al. 92] </ref> file servers log received updates to peer servers in order to remove stable storage writes from the commit path. Similarly, Naughton and Li have used log propagation to keep a hot standby of a main-memory database [Li & Naughton 88].
Reference: [Moss 90] <author> Moss, J. E. B. </author> <title> Design of the Mneme persistent object store. </title> <journal> ACM Transactions on Information Systems, </journal> <volume> 8(2) </volume> <pages> 103-139, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: Client programs navigate through the stored data by following pointers directly in virtual memory, with the sys tem moving data between memory and the persistent store as needed. A number of persistent stores have been built; some are research systems <ref> [Cockshott et al. 84, Moss 90, Carey et al. 94] </ref> and others are commercial object-oriented database (OODB) products (e.g., [Butterworth et al. 92, Lamb et al. 91, O. Deux 92]), augmented with database features such as query processors, schema languages, and indexing facilities.
Reference: [Mueller 93] <author> Mueller, F. </author> <title> A library implementation of POSIX threads under Unix. </title> <booktitle> In Proceedings of the Winter 1993 USENIX Conference, </booktitle> <pages> pages 29-41, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: We modified the RVM commit procedure to broadcast the same new-value information that is written to disk. Coherency data is broadcast using TCP/IP by issuing a writev system call for each node that has the current region mapped. We use the OSF/1 PThreads facility <ref> [Mueller 93] </ref> to create receiver threads for each communication channel that connects a node to its peers.
Reference: [Neves et al. 94] <author> Neves, N., Castro, M., and Guedes, P. </author> <title> A checkpoint protocol for an entry consistent shared memory system. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Principles of Distributed Computing, </booktitle> <month> August </month> <year> 1994. </year>
Reference-contexts: Instead of requiring checkpointing or other recoverability actions each time an application gains access to a page, their system checkpoints only when a node releases or acquires a lock. Neves et al. added checkpointing to an entry-consistent DSM system similar to Midway, using object-grain locking <ref> [Neves et al. 94] </ref>. Their system tolerates single node failures by keeping old versions of modified objects in the volatile memory of the nodes that modify them. Each node checkpoints independently; a failed node recovers by replaying its execution starting with its most recent checkpoint.
Reference: [O. Deux 92] <author> O. </author> <title> Deux. The O 2 system. </title> <journal> Communications of the ACM, </journal> <volume> 34(10) </volume> <pages> 34-48, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: A number of persistent stores have been built; some are research systems [Cockshott et al. 84, Moss 90, Carey et al. 94] and others are commercial object-oriented database (OODB) products (e.g., <ref> [Butterworth et al. 92, Lamb et al. 91, O. Deux 92] </ref>), augmented with database features such as query processors, schema languages, and indexing facilities.
Reference: [O'Toole et al. 93] <author> O'Toole, J., Nettles, S., and Gif-ford, D. </author> <title> Concurrent compacting garbage collection of a persistent heap. </title> <booktitle> In Proceedings of the Fourteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 161-174, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: A call to this function indicates an intent to update a particular range of bytes. We expect that this range corresponds to an object, and that the call is made by code generated explicitly by the language compiler (such as the ML compiler that has been used with RVM <ref> [O'Toole et al. 93] </ref>). In contrast, most DSM systems use virtual page access faults to captue updates, though software-based write detection is also used in Midway [Zekauskas et al. 94]. This issue is discussed in more detail in Section 4. <p> For log-based coherency, updates are captured by calls to the RVM set range procedure. These calls are coded explicitly in our OO7 benchmarks, although other RVM applications have used compiler-generated set range calls <ref> [O'Toole et al. 93] </ref>. In contrast, most DSM systems use page access faults to capture updates without involvement from the compiler or application. <p> This validates our assertion that there is a high degree of overlap between the mechanisms for recoverability and coherency. 5 Related Work In the context of RVM, transaction logs have been used in a similar fashion to propagate updates between data spaces in a system with concurrent replicating garbage collection <ref> [O'Toole et al. 93] </ref>. Our contribution is to use this idea for maintaining coherency of client database caches. Log propagation has been used to maintain the consistency of replicas in replicated database systems.
Reference: [Richard & Singhal 93] <author> Richard, III, G. G. and Sing-hal, M. </author> <title> Using logging and asynchronous checkpointing to implement recoverable distributed shared memory. </title> <booktitle> In Proceedings of the 12th Symposium on Reliable Distributed Systems, </booktitle> <pages> pages 58-67, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: Stumm and Zhou describe a system that tolerates the failure of a single node by ensuring that every page resides in the caches of at least two nodes [Stumm & Zhou 90]. Richard and Singhal use page logging <ref> [Richard & Singhal 93] </ref>; a copy of a page is written to a local volatile log each time it is acquire for reading or writing. A node writes its volatile log to disk before transferring a modified page to another node.
Reference: [Rodeheffer & Schroeder 91] <author> Rodeheffer, T. and Schroeder, M. D. </author> <title> Automatic reconfiguration in autonet. </title> <booktitle> In Proceedings of the Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 183-197, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: The machines are connected by a 100 Mbit/s AN1 network, an experimental switch-based network capable of sending message packets of up to 64 Kbytes <ref> [Rodeheffer & Schroeder 91] </ref>. Elapsed time measurements were taken using the Alpha cycle counter. For these experiments, we disabled RVM disk logging so that we could isolate the costs associated with coherency from the synchronous disk writes needed to support recovery.
Reference: [Rosenberg & Abramson 85] <author> Rosenberg, J. and Abramson, D. A. MONADS-PC: </author> <title> A capability-based workstation to support software engineering. </title> <booktitle> In Proceedings of the 18th Hawaii International Conference on System Sciences, </booktitle> <year> 1985. </year>
Reference: [Satyanarayanan et al. 94] <author> Satyanarayanan, M., Mashburn, H. H., Kumar, P., Steere, D. C., and J.Kistler., J. </author> <title> Lightweight recoverable virtual memory. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 12(4) </volume> <pages> 33-57, </pages> <month> Febru-ary </month> <year> 1994. </year>
Reference-contexts: The locking routines can collect and maintain information about which peers must receive a given set of updates, and when. 3 Prototype Implementation To experiment with our approach, we prototyped a simple implementation of log-based coherency by adding distribution support to CMU's Recoverable Virtual Memory (RVM) package <ref> [Satyanarayanan et al. 94] </ref>. RVM is a logging facility that supports transactional update and recovery of virtual-memory resident data structures. <p> Online trimming could be implemented using the merging procedure by coordinating checkpointing; one node would checkpoint at a time, broadcasting to other nodes when done to inform them of their new log head. An improved log-trimming scheme for RVM is described in <ref> [Satyanarayanan et al. 94] </ref>. In this scheme, nodes checkpoint a page at a time by writing the current version of a page to the checkpoint file. Log records for updates made to a page before it was check-pointed can be discarded.
Reference: [Stumm & Zhou 90] <author> Stumm, M. and Zhou, S. </author> <title> Fault tolerant distributed shared memory algorithms. </title> <booktitle> In Proceedings of the Second IEEE Symposium on Parallel and Distributed Processing, </booktitle> <pages> pages 719-724, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: The first such system is due to Wu and Fuchs [Wu & Fuchs 90]. Stumm and Zhou describe a system that tolerates the failure of a single node by ensuring that every page resides in the caches of at least two nodes <ref> [Stumm & Zhou 90] </ref>. Richard and Singhal use page logging [Richard & Singhal 93]; a copy of a page is written to a local volatile log each time it is acquire for reading or writing.
Reference: [Thekkath & Levy 94] <author> Thekkath, C. and Levy, H. </author> <title> Hardware and software support for efficient exception handling. </title> <booktitle> In Proceedings of the 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: For example, using Figures 5 and 7, we can determine that if there are 1000 updates per transaction, log-based coherency performs better when there are 45 or fewer updates per page (55 if the updates are ordered). Recent work <ref> [Thekkath & Levy 94] </ref> has shown an order-of-magnitude reduction in exception-handling cost, which would make hardware-based write detection more attractive.
Reference: [Wu & Fuchs 90] <author> Wu, K.-L. and Fuchs, W. K. </author> <title> Recoverable disributed shared virtual memory. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 39(4) </volume> <pages> 460-469, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: This allows non-transactional applications such as parallel programs to be made recoverable. The main issue is to coordinate individual node checkpoints to attain a consistent global checkpoint, using a combination of dependency tracking, message logging, and replication. The first such system is due to Wu and Fuchs <ref> [Wu & Fuchs 90] </ref>. Stumm and Zhou describe a system that tolerates the failure of a single node by ensuring that every page resides in the caches of at least two nodes [Stumm & Zhou 90].
Reference: [Zekauskas et al. 94] <author> Zekauskas, M. J., Sawdon, W. A., and Bershad, B. N. </author> <title> Software write detection for distributed shared memory. </title> <booktitle> In Proceedings of the First Symposium on Operating Systems Design and Implementation, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: CDA-9123308 and CCR-9200832), the Washington Technology Center, Digital Equipment Corporation, Boeing Computer Services, Intel Corporation, Hewlett-Packard Corporation, and Apple Computer. Both Feeley and Chase have been supported by Intel Foundation Graduate Fellowships. Chase's present address is: Department of Computer Science, Duke University, Durham, NC 27706 (chase@cs.duke.edu). 94], and Midway <ref> [Zekauskas et al. 94] </ref>. These DSM systems maintain the illusion of a single shared memory by synchronizing data access and moving data between nodes when required, transparently to the application. DSM is useful in this context, because it simplifies programming of these distributed-parallel programs. <p> In contrast, most DSM systems use virtual page access faults to captue updates, though software-based write detection is also used in Midway <ref> [Zekauskas et al. 94] </ref>. This issue is discussed in more detail in Section 4. RVM coalesces modified ranges that are adjacent or overlapping in order to avoid writing redundant bytes to the disk log.
References-found: 32

