URL: http://www.pdos.lcs.mit.edu/~kerr/final-thesis.ps
Refering-URL: http://www.pdos.lcs.mit.edu/~kerr/papers.html
Root-URL: 
Title: High-Performance Application-Specific Networking  
Author: by Deborah Anne Wallach M. Frans Kaashoek Arthur C. Smith 
Degree: (1992) Submitted to the Department of Electrical Engineering and Computer Science in partial fulfillment of the requirements for the degree of Doctor of Philosophy in Computer Science at the  All rights reserved. Author  Certified by  Associate Professor of Computer Science and Engineering Thesis Supervisor Accepted by  Chairman, Departmental Committee on Graduate Students  
Date: January 1997  31 January 1997  
Address: (1990)  
Affiliation: S.B., Massachusetts Institute of Technology  S.M., Massachusetts Institute of Technology  MASSACHUSETTS INSTITUTE OF TECHNOLOGY  c Massachusetts Institute of Technology 1997.  Department of Electrical Engineering and Computer Science  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> M.B. Abbott and L.L. Peterson. </author> <title> Increasing network throughput by integrating protocol layers. </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> 1(5) </volume> <pages> 600-610, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: There is also quite a bit of work on protocol composition [6, 27, 28, 58, 59]. The first system to provide an automatic modular mechanism for ILP is Abbott and Peter-son <ref> [1] </ref>. They describe an ILP system that composes macros into integrated loops at compile time, eliminating multiple data traversals. Each macro is written with initialization and finalization code and a main body that takes in word-sized input and emits word-sized output. <p> Therefore, we designed the ASH system to support dynamic ILP. ILP can be dynamically provided through the use of pipes, which were first proposed by Abbott and Peterson <ref> [1] </ref> for use in static composition. A pipe is a computation written to act on streaming data, taking several bytes of data as input and producing several bytes of output while performing only a tiny computation (such as a byteswap, or an accumulation for a checksum). <p> To address this bottleneck, applications must be able to direct message placement, and to exploit ILP during copying. We examine each below. Avoiding message copies Message copies cripple networking performance <ref> [1, 13, 56] </ref>. However, most network systems make little provision for application-directed data transfer. This results in needless data copies as incoming messages are copied from network buffers to intermediate buffers (e.g., BSD's mbufs [33]) and then copied to their eventual destination.
Reference: [2] <author> T.E. Anderson, M.D. Dahlin, J.M. Neefe, D.A. Patterson, D.S. Roselli, and R.Y. Wang. </author> <title> Serverless network file systems. </title> <booktitle> In Proceedings of the 15th Symposium on Operating Systems Principles, </booktitle> <pages> pages 109-126, </pages> <address> Copper Mountain Resort, CO, USA, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: For example, the last few years have seen a proliferation of distributed shared memory systems [30, 32, 35], real-time video and voice applications [63], parallel applications [14, 47], and tightly-coupled distributed systems <ref> [2, 60, 56] </ref>. Unfortunately, although raw CPU and networking hardware speeds have increased, this increase is not reaching applications: networking software and memory subsystem performance already limit applications and will only do so more in the future [13, 16, 56].
Reference: [3] <author> T.E. Anderson, S.S. Owicki, J.B. Saxe, </author> <title> and C.P. Thacker. High speed switch scheduling for local area networks. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(4) </volume> <pages> 319-352, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: The 240 has separate direct-mapped write-through 64-KByte caches for instructions and data. The I/O devices are accessed over a 25-MHz TURBOchannel bus. On the 125, the I/O devices are accessed over a 12.5-MHz TURBOchannel bus. The four DECstations are connected with an AN2 switch <ref> [3] </ref>. 6.1.2 Methodology While collecting the numbers reported in this thesis, we had a fair number of problems with cache conflicts (similar to problems reported by others [42]), because the DECstations have direct-mapped caches.
Reference: [4] <author> M.L. Bailey, B. Gopal, M.A. Pagels, L.L. Peterson, and P. Sarkar. PATHFINDER: </author> <title> A pattern-based packet classifier. </title> <booktitle> In Proceedings of the First Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 115-123, </pages> <address> Monterey, CA, USA, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: DPF is an order of magnitude faster than the highest performance packet filter engines (MPF [67] and PATHFINDER <ref> [4] </ref>) in the literature. 5.3.2 AN2 interface Similarly to other systems [18, 47, 60], the AN2 device is securely exported by using the ATM connection identifier to demultiplex packets. Before communicating, processes bind to a virtual circuit identifier (VCI).
Reference: [5] <author> B.N. Bershad, S. Savage, P. Pardyak, E.G. Sirer, M. Fiuczynski, D. Becker, S. Eggers, and C. Chambers. </author> <title> Extensibility, safety and performance in the SPIN operating system. </title> <booktitle> In Proceedings of the Fifteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 267-284, </pages> <address> Copper Mountain Resort, CO, USA, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: In some sense this work can be viewed as a natural extension of the same philosophical foundation that inspired the packet filter: we have provided a framework that allows applications outside of the operating system to install new functionality without kernel modifications. The SPIN project <ref> [5] </ref> is concurrently investigating the use of downloading code into the kernel. SPIN's Plexus network system runs user code fragments in the interrupt handler [25] or as a kernel thread. Plexus guarantees safety by requiring that these code fragments are written in a type-safe language, Modula-3.
Reference: [6] <author> N.T. Bhatti and R.D. Schlichting. </author> <title> A system for constructing configurable high-level protocols. </title> <booktitle> In ACM SIGCOMM '95, </booktitle> <pages> pages 138-150, </pages> <address> Cambridge, MA, USA, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: There is also quite a bit of work on protocol composition <ref> [6, 27, 28, 58, 59] </ref>. The first system to provide an automatic modular mechanism for ILP is Abbott and Peter-son [1]. They describe an ILP system that composes macros into integrated loops at compile time, eliminating multiple data traversals. <p> This organization has a direct impact on efficiency: since untrusted software cannot augment these operations, any integration that was not anticipated by the network architects is penalized. Given the richness of possible operations, such mismatches happen quite easily. Furthermore, many systems compose protocols at runtime <ref> [6, 27, 28, 58, 59] </ref>, making static ILP infeasible.
Reference: [7] <editor> R. Braden, D. Borman, and C. Partridge. </editor> <title> Computing the Internet checksum. </title> <type> RFC 1071. </type>
Reference-contexts: The VCODE interface is that of an extended RISC machine: instructions are low-level register-to-register operations. A sample pipe to compute the Internet checksum <ref> [7] </ref> is provided in Figure 3-3. Each pipe is allocated in the context of a pipe list (pl in the figure) and given a pipe identifier that is used to name it.
Reference: [8] <author> T. Braun and C. Diot. </author> <title> Protocol implementation using integrated layer processing. </title> <booktitle> In ACM SIGCOMM '95, </booktitle> <pages> pages 151-161, </pages> <address> Cambridge, MA, USA, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: Because TCP is important, well documented, and widely used, we illustrate the benefits of ASHs using TCP. Also, as pointed out by Braun and Diot <ref> [8] </ref>, it is important to evaluate ILP in a complete protocol environment, and TCP can benefit from the use of ILP. Our TCP implementation lowers the cost of data transfer by placing the common-case fast path in a handler which can be run either as an ASH or an upcall.
Reference: [9] <author> G. Buzzard, D. Jacobson, M. Mackey, S. Marovich, and J. Wilkes. </author> <title> An implementation of the Hamlyn sender-managed interface architecture. </title> <booktitle> In Proceedings of the Second Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 245-259, </pages> <address> Seattle, WA, USA, </address> <month> October </month> <year> 1996. </year>
Reference-contexts: For example, the roundtrip time for a short message (up to 40 bytes) in U-Net increases from 65 microseconds to 125 microseconds if the applications performing the communication are not scheduled when the message arrives [60]; in Hamlyn the time increases from 28 microseconds to 78 microseconds <ref> [9] </ref>. Both U-Net and Hamlyn are implemented on operating systems which will suspend the currently running application and immediately reschedule the application an arriving message is for, if the application is suspended waiting for the message. <p> This is not true of all devices. In particular, network interfaces which have been specifically designed for user-level communication may not need to generate an interrupt at all upon message arrival (e.g., Hamlyn <ref> [9] </ref>). For those types of systems, handlers are not useful if the application is running during message arrival.
Reference: [10] <author> J.B. Carter. </author> <title> Efficient Distributed Shared Memory Based On Multi-Protocol Release Consistency. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> August </month> <year> 1993. </year> <month> 107 </month>
Reference-contexts: Functionality is impaired because communication code running at the user level is not integrated with the operating system. For example, some software DSM systems are integrated with virtual memory <ref> [10, 32, 35] </ref>. Because virtual memory is normally implemented in the operating system, it can be awkward and slow for user-level code to query or modify it. The performance of systems built with user-level communication can suffer in two ways.
Reference: [11] <author> D.D. Clark. </author> <title> The structuring of systems using upcalls. </title> <booktitle> In Proceedings of the 10th Sympo--sium on Operating Systems Principles, </booktitle> <pages> pages 171-180, </pages> <address> Orcas Island, WA, USA, </address> <month> December </month> <year> 1985. </year>
Reference-contexts: Finally, the issue of handlers and how they affect the scheduling of the system as a whole is also important to consider. 2.1 Computational model ASHs can been viewed as a restricted form of Clark's upcalls <ref> [11] </ref>. Upcalls were proposed by Clark as an alternative way of structuring systems.
Reference: [12] <author> D.D. Clark, V. Jacobson, J. Romkey, and H. Salwen. </author> <title> An analysis of TCP processing overhead. </title> <journal> IEEE Communications Magazine, </journal> <volume> 27(6) </volume> <pages> 23-29, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: 25 methodology can be viewed as an extension of active messages to a general purpose environment in a way that still guarantees small latencies while also providing strong protection guarantees. 2.3.3 ILP and protocol composition There have been many instances of ad hoc ILP, for example, in many networking kernels <ref> [12] </ref>. There is also quite a bit of work on protocol composition [6, 27, 28, 58, 59]. The first system to provide an automatic modular mechanism for ILP is Abbott and Peter-son [1].
Reference: [13] <author> D.D. Clark and D.L. Tennenhouse. </author> <title> Architectural considerations for a new generation of protocols. </title> <booktitle> In ACM Communication Architectures, Protocols, and Applications (SIG-COMM) 1990, </booktitle> <pages> pages 200-208, </pages> <address> Philadelphia, PA, USA, </address> <month> September </month> <year> 1990. </year>
Reference-contexts: Unfortunately, although raw CPU and networking hardware speeds have increased, this increase is not reaching applications: networking software and memory subsystem performance already limit applications and will only do so more in the future <ref> [13, 16, 56] </ref>. This thesis addresses the important problem of delivering hardware-level network performance to applications by introducing application-specific safe message handlers (ASHs), which are user-written handlers that are safely and efficiently executed in the kernel in response to a message arrival. <p> Therefore, the negotiation of protocol layers can require multiple costly memory traversals, stressing a weak link in high-performance networking: the memory subsystems of the endpoint nodes. As argued by Clark and Tennenhouse <ref> [13] </ref>, an integrated approach, where these application-specific operations are combined into a single memory traversal, can greatly improve the latency and throughput of a system. The ASH system integrates data manipulations such as checksumming or conversions into the data transfer engine itself, automatically and dynamically performing integrated layering processing (ILP). <p> In addition to simple data copying, many systems perform multiple traversals of message data as every layer of the networking software performs its operations (e.g., checksumming, encryption, conversion). At an operational level, these multiple data manipulations are as bad as multiple copies. To remove this overhead, Clark and Tennenhouse <ref> [13] </ref> propose integrated layer processing (ILP), where the manipulations of each layer are compressed into a single operation. To the best of our knowledge, all systems based on ILP are static, in that all integration must be hard coded into the networking system. <p> Unfortunately, while network throughput and CPU performance have improved significantly in the last decade, workstation memory subsystems have not. As a result, the crucial bottleneck in bulk data transfer occurs during the movement of data from the network buffer to its final destination in application space <ref> [13, 16] </ref>. To address this bottleneck, applications must be able to direct message placement, and to exploit ILP during copying. We examine each below. Avoiding message copies Message copies cripple networking performance [1, 13, 56]. However, most network systems make little provision for application-directed data transfer. <p> To address this bottleneck, applications must be able to direct message placement, and to exploit ILP during copying. We examine each below. Avoiding message copies Message copies cripple networking performance <ref> [1, 13, 56] </ref>. However, most network systems make little provision for application-directed data transfer. This results in needless data copies as incoming messages are copied from network buffers to intermediate buffers (e.g., BSD's mbufs [33]) and then copied to their eventual destination.
Reference: [14] <author> D.E. Culler, A. Dusseau, S.C. Goldstein, A. Krishnamurthy, S. Lumetta, T. von Eicken, and K. Yelick. </author> <title> Parallel programming in Split-C. </title> <booktitle> In Supercomputing, </booktitle> <pages> pages 262-273, </pages> <address> Portland, OR, USA, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: For example, the last few years have seen a proliferation of distributed shared memory systems [30, 32, 35], real-time video and voice applications [63], parallel applications <ref> [14, 47] </ref>, and tightly-coupled distributed systems [2, 60, 56]. Unfortunately, although raw CPU and networking hardware speeds have increased, this increase is not reaching applications: networking software and memory subsystem performance already limit applications and will only do so more in the future [13, 16, 56].
Reference: [15] <author> P. Deutsch and C.A. Grant. </author> <title> A flexible measurement tool for software systems. </title> <booktitle> Information Processing 71, </booktitle> <year> 1971. </year>
Reference-contexts: Those that can tolerate more latency can use the flexibility of the upcall; those that cannot will be confined to ASHs. 2.2 Safe code importation There are a number of clear antecedents to our work on making ASHs safe: Deutsch's seminal paper <ref> [15] </ref> and Wahbe et al.'s modern revisitation of safe code importation [62] influenced our ideas strongly, as did Mogul's original packet filter paper [41]. <p> In this chapter, we discuss these two issues first for ASHs and then for upcalls. This chapter presents a variety of software and hardware techniques that go beyond the ones originally proposed by Deutsch and Grant <ref> [15] </ref> and Wahbe et al. [62]. <p> Wild memory references and jumps are prevented using a combination of address-space fire-walls and sandboxing [62]. The ASH system for the MIPS can bound execution time using a framework inspired by Deutsch <ref> [15] </ref>. We examine each technique in further detail below. Preventing exceptions Exceptions are prevented either through runtime or download-time checks. Runtime checks are used to prevent divide-by-zero errors; unaligned exceptions are prevented by forcing pointers to be aligned to the requirements of the base machine 2 .
Reference: [16] <author> P. Druschel, M.B. Abbott, M.A. Pagels, and L.L. Peterson. </author> <title> Network subsystem design. </title> <journal> IEEE Network, </journal> <volume> 7(4) </volume> <pages> 8-17, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: Unfortunately, although raw CPU and networking hardware speeds have increased, this increase is not reaching applications: networking software and memory subsystem performance already limit applications and will only do so more in the future <ref> [13, 16, 56] </ref>. This thesis addresses the important problem of delivering hardware-level network performance to applications by introducing application-specific safe message handlers (ASHs), which are user-written handlers that are safely and efficiently executed in the kernel in response to a message arrival. <p> Unfortunately, while network throughput and CPU performance have improved significantly in the last decade, workstation memory subsystems have not. As a result, the crucial bottleneck in bulk data transfer occurs during the movement of data from the network buffer to its final destination in application space <ref> [13, 16] </ref>. To address this bottleneck, applications must be able to direct message placement, and to exploit ILP during copying. We examine each below. Avoiding message copies Message copies cripple networking performance [1, 13, 56]. However, most network systems make little provision for application-directed data transfer.
Reference: [17] <author> P. Druschel and L.L. Peterson. Fbufs: </author> <title> A high-bandwidth cross-domain transfer facility. </title> <booktitle> In Proceedings of the 14th Symposium on Operating Systems Principles, </booktitle> <pages> pages 175-189, </pages> <address> Asheville, NC, USA, </address> <month> December </month> <year> 1993. </year>
Reference-contexts: promising direction to making ASHs run faster (i.e., by reducing the sandboxing overhead). 24 2.3 ASH benefits The particular abilities that ASHs provide have been provided in part by other networking systems, though not all together. 2.3.1 Message vectoring Message vectoring has been a popular focus of the networking community <ref> [17, 18, 20, 60, 46] </ref>. The main difference between our work and previous work is that ASHs can perform application-specific computation at message arrival. By using application-state and domain knowledge these handlers can perform operations difficult in the context of static protocol specifications.
Reference: [18] <author> P. Druschel, L.L. Peterson, and B.S. Davie. </author> <title> Experiences with a high-speed network adaptor: A software perspective. </title> <booktitle> In ACM Communication Architectures, Protocols, and Applications (SIGCOMM) 1994, </booktitle> <pages> pages 2-13, </pages> <address> London, UK, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: promising direction to making ASHs run faster (i.e., by reducing the sandboxing overhead). 24 2.3 ASH benefits The particular abilities that ASHs provide have been provided in part by other networking systems, though not all together. 2.3.1 Message vectoring Message vectoring has been a popular focus of the networking community <ref> [17, 18, 20, 60, 46] </ref>. The main difference between our work and previous work is that ASHs can perform application-specific computation at message arrival. By using application-state and domain knowledge these handlers can perform operations difficult in the context of static protocol specifications. <p> The main difference between our work and previous work is that ASHs can perform application-specific computation at message arrival. By using application-state and domain knowledge these handlers can perform operations difficult in the context of static protocol specifications. Application Device Channels (ADCs) <ref> [18] </ref> are a different approach to eliminating protection domain boundaries from the common communication path. In this approach, much of the network device driver is linked with the application, and many messages can be handled without OS intervention. <p> DPF is an order of magnitude faster than the highest performance packet filter engines (MPF [67] and PATHFINDER [4]) in the literature. 5.3.2 AN2 interface Similarly to other systems <ref> [18, 47, 60] </ref>, the AN2 device is securely exported by using the ATM connection identifier to demultiplex packets. Before communicating, processes bind to a virtual circuit identifier (VCI). Two kinds of connections are available on our system: receiving from a single host and receiving from multiple hosts. <p> Our final application is a Web server. This application vividly demonstrates what happens to system performance when there is a large number of aborts. 55 6.1 Experimental environment This section reports on the base performance of our system (i.e., without the use of ASHs or upcalls). Like other systems <ref> [18, 20, 37, 60, 57] </ref>, all the protocols are implemented in user space. The main point to take from the results in this section is that our implementation performs well and is competitive with the best systems reported in the literature. <p> Direct comparisons with other high-performance systems such as Osiris <ref> [18] </ref> and Afterburner [20] are difficult since they run on 57 different networks and have special purpose network cards, but our implementation appears to be competitive. 6.1.5 User-level internet protocols On top of the raw interface we have implemented several network protocols (ARP/RARP, IP, UDP, TCP, HTTP, and NFS) as user-level <p> In summary, the base performance of our system for UDP and TCP is about the same or is better than most high-performance user-level and in-kernel implementations <ref> [18, 20, 25, 37, 57] </ref>, as long as the applications are scheduled when the messages arrive. 6.2 Sandboxing overhead Before we examine any ASH results, it is instructive to understand what kind of sandboxing overhead will be incurred by applications.
Reference: [19] <author> Peter Druschel and Gaurav Banga. </author> <title> Lazy receiver processing (lrp): A network subsystem architecture for server systems. </title> <booktitle> In Proceedings of the Second Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 261-275, </pages> <address> Seattle, WA, USA, </address> <month> October </month> <year> 1996. </year>
Reference-contexts: have been recently executed on a per process basis, and refuse to execute any more for processes receiving more than their share of messages, instead falling back to the normal mechanism. 26 A similar approach to fairly and stably dealing with high communication loads is described in Druschel and Banga <ref> [19] </ref>. The two key techniques described in their paper are lazy protocol processing at the receiver's priority, and early demultiplexing, are both used in the Aegis operating system that our work was performed on.
Reference: [20] <author> A. Edwards, G. Watson, J. Lumley, D. Banks, C. Clamvokis, and C. Dalton. </author> <title> User-space protocols deliver high performance to applications on a low-cost Gb/s LAN. </title> <booktitle> In ACM Communication Architectures, Protocols, and Applications (SIGCOMM) 1994, </booktitle> <pages> pages 14-24, </pages> <address> London, UK, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: promising direction to making ASHs run faster (i.e., by reducing the sandboxing overhead). 24 2.3 ASH benefits The particular abilities that ASHs provide have been provided in part by other networking systems, though not all together. 2.3.1 Message vectoring Message vectoring has been a popular focus of the networking community <ref> [17, 18, 20, 60, 46] </ref>. The main difference between our work and previous work is that ASHs can perform application-specific computation at message arrival. By using application-state and domain knowledge these handlers can perform operations difficult in the context of static protocol specifications. <p> We would expect this approach to be slower than ASHs when the application is not running, because the entire application/driver process must be scheduled to handle the message. The most similar work to the ASH system is Edwards et al. <ref> [20] </ref>, who import simple scripts using the Unix ioctl system call to copy messages to their destination. The main differences are the expressiveness of the two implementations. Their system supplies only rudimentary operations (e.g., copy and allocate), limiting the flexibility with which applications can manipulate data transfer. <p> Our final application is a Web server. This application vividly demonstrates what happens to system performance when there is a large number of aborts. 55 6.1 Experimental environment This section reports on the base performance of our system (i.e., without the use of ASHs or upcalls). Like other systems <ref> [18, 20, 37, 60, 57] </ref>, all the protocols are implemented in user space. The main point to take from the results in this section is that our implementation performs well and is competitive with the best systems reported in the literature. <p> Direct comparisons with other high-performance systems such as Osiris [18] and Afterburner <ref> [20] </ref> are difficult since they run on 57 different networks and have special purpose network cards, but our implementation appears to be competitive. 6.1.5 User-level internet protocols On top of the raw interface we have implemented several network protocols (ARP/RARP, IP, UDP, TCP, HTTP, and NFS) as user-level libraries, which are <p> The general structure is similar to other implementations of user-level protocols <ref> [20, 60] </ref>. The UDP implementation is a straightforward implementation of the UDP protocol as specified in RFC768. Similarly, the TCP implementation is a library-based implementation of RFC793. <p> In summary, the base performance of our system for UDP and TCP is about the same or is better than most high-performance user-level and in-kernel implementations <ref> [18, 20, 25, 37, 57] </ref>, as long as the applications are scheduled when the messages arrive. 6.2 Sandboxing overhead Before we examine any ASH results, it is instructive to understand what kind of sandboxing overhead will be incurred by applications.
Reference: [21] <author> D.R. Engler. </author> <title> VCODE: a retargetable, extensible, very fast dynamic code generation system. </title> <booktitle> In Proceedings of the SIGPLAN '96 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 160-170, </pages> <address> Philadelphia, PA, USA, </address> <month> May </month> <year> 1996. </year> <note> http://www.pdos.lcs.mit.edu/engler/ vcode.html. </note>
Reference-contexts: First, the composition is completely dynamic: any pipe can be composed with any other at runtime. Second, it is modular: the ASH system converts between gauge sizes and prevents name conflicts by binding the context inside the pipe itself. The pipes for ASHs are written in VCODE <ref> [21] </ref>, which is a set of C macros that provide a low-level extension language for dynamic code generation. VCODE is designed to be simple to implement and efficient both in terms of the cost of code generation and in terms of the computational performance of its generated code.
Reference: [22] <author> D.R. Engler and M.F. Kaashoek. DPF: </author> <title> fast, flexible message demultiplexing using dynamic code generation. </title> <booktitle> In ACM Communication Architectures, Protocols, and Applications (SIGCOMM '96), </booktitle> <pages> pages 53-59, </pages> <address> Stanford, CA, USA, </address> <month> August </month> <year> 1996. </year> <month> 108 </month>
Reference-contexts: Packet filters can be implemented efficiently <ref> [22] </ref>. Another technique sometimes used by user-level communication systems implemented on top of ATM networks is to use the virtual circuit index of the incoming message as an application identifier [60]. <p> The Aegis implementation of the packet filter engine, DPF <ref> [22] </ref>, uses dynamic code generation. DPF exploits dynamic code generation in two ways: by using it to eliminate interpretation overhead by compiling packet filters to executable code when they are installed into the kernel, and by using 46 application. <p> In order to implement dynamic integrated layer processing, we added 250 lines of interface code plus the VCODE system. VCODE is an independent, released code package of about 3000 lines of code; its use is amortized over multiple OS functions in our environment (including dynamic packet filters <ref> [22] </ref>). Given that VCODE is a stand-alone package, we find providing DILP to be worthwhile, as DILP greatly simplifies writing efficient integrated loops. 5.9 Summary This chapter described the specific subsystems that we used to provide efficient user-level communication, ASHs, and upcalls.
Reference: [23] <author> D.R. Engler, M.F. Kaashoek, and J. O'Toole Jr. Exokernel: </author> <title> an operating system architec-ture for application-specific resource management. </title> <booktitle> In Proceedings of the Fifteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 251-266, </pages> <address> Copper Mountain Resort, Colorado, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: The next chapter presents the implementation specifics of the entire system: networking, the operating system, and the mechanism to provide fast message handling. 44 Chapter 5 Implementation Specifics We have implemented a system for ASHs in Aegis, an exokernel operating system <ref> [23] </ref>. This chapter describes the implementation and testbed environment used in our experiments. 5.1 Overview As shown in Figure 5-1, Aegis is the operating system we are using as our platform. <p> Finally, FTP will receive incoming messages as ASHs, running in the kernel. The rest of this chapter discusses the various subsystems necessary to support these three communication models. 5.2 Aegis Aegis is an exokernel operating system for MIPS-based DECstations. The idea of an exoker-nel <ref> [23] </ref> operating system is to allow applications to manage the physical resources of a machine as much as is possible. The abstractions and policies normally expected in an operating system (e.g., virtual memory or interprocess communication) are consigned to user-level library operating systems, which execute outside the kernel. <p> In fact, because Aegis was designed for flexibility and performance from its inception, it is not the ideal platform. As reported in <ref> [23] </ref>, it has extremely fast kernel crossing times (about a factor of 10 faster than Ultrix), and context switches are quite inexpensive (about a factor of 5 faster than Ultrix). <p> Just as applications are given the entire message to process, after a demultiplexing step based on virtual circuit identifier, so are ASHs. No higher level demultiplexing is done; no higher-level protocol (e.g., IP) is forced upon ASHs. Similarly, for the Ethernet implementation reported in <ref> [23] </ref>, demultiplexing of a message to an ASH was done through DPF (see Section 6.1); again, no more functionality is required in the kernel than is needed to demultiplex the messages to the correct process in the first place. <p> our experiments underestimate the benefits of running ASHs in any other kernel because kernel crossings in Aegis have been highly optimized: Aegis kernel's crossings are five times better than the best reported numbers in the literature and are an order of magnitude better than a run-of-the-mill UNIX system like Ultrix <ref> [23] </ref>. <p> example, on the DECstation 5000/240 the advantage of running an ASH in the Aegis exokernel versus running an upcall in user space is approximately 35 microseconds; under Ultrix4.2 this difference would be more like 95 microseconds (the approximate cost of an exception plus the system call back into the kernel) <ref> [23] </ref>. 6.3.1 High throughput High data transfer rates are required by bulk data transfer operations. Unfortunately, while network throughput and CPU performance have improved significantly in the last decade, workstation memory subsystems have not.
Reference: [24] <author> D.R. Engler, D.A. Wallach, and M.F. Kaashoek. </author> <title> Design and implementation of a modular, flexible, and fast system for dynamic protocol composition. </title> <type> Technical Memorandum TM-552, </type> <institution> Massachusetts Institute of Technology Laboratory for Computer Science, </institution> <month> May </month> <year> 1996. </year>
Reference-contexts: this function is responsible for setting up the initial state of the accumulator register, then later reading it in, and folding it to 16 bits. 3.2.3 ASHs with dynamic protocol composition In addition to dynamic ILP, ASH programmers can use the dynamic protocol composition extensions provided by the ASH system <ref> [24] </ref>.
Reference: [25] <author> M.E. Fiuczynski and B.N. Bershad. </author> <title> An extensible protocol architecture for application-specific networking. </title> <booktitle> In Proceedings of USENIX, </booktitle> <pages> pages 55-64, </pages> <address> San Diego, CA, USA, </address> <month> January </month> <year> 1996. </year>
Reference-contexts: The SPIN project [5] is concurrently investigating the use of downloading code into the kernel. SPIN's Plexus network system runs user code fragments in the interrupt handler <ref> [25] </ref> or as a kernel thread. Plexus guarantees safety by requiring that these code fragments are written in a type-safe language, Modula-3. Plexus simplifies protocol composition, but unlike ASHs, does not provide direct support for dynamic ILP. <p> In summary, the base performance of our system for UDP and TCP is about the same or is better than most high-performance user-level and in-kernel implementations <ref> [18, 20, 25, 37, 57] </ref>, as long as the applications are scheduled when the messages arrive. 6.2 Sandboxing overhead Before we examine any ASH results, it is instructive to understand what kind of sandboxing overhead will be incurred by applications.
Reference: [26] <author> J. Gosling. </author> <title> Java intermediate bytecodes. </title> <booktitle> In ACM SIGPLAN Workshop on Intermediate Representations (IR'95), </booktitle> <pages> pages 111-118, </pages> <address> San Francisco, CA, USA, </address> <month> March </month> <year> 1995. </year>
Reference-contexts: Preliminary Plexus numbers for in-kernel UDP on Ethernet and ATM look promising but are slower than our user-level implementation of UDP. No numbers are reported yet for TCP. With the advent of HotJava and Java <ref> [26] </ref>, code importation in the form of mobile code has received a lot of press. Recently Tennenhouse and Wetherall have proposed to use mobile code to build Active Networks [53]; in an active network, protocols are replaced by programs, which are safely executed on routers on message arrival.
Reference: [27] <author> R. Harper and P. Lee. </author> <title> Advanced languages for systems software: The Fox project in 1994. </title> <type> Technical Report CMU-SC-94-104, </type> <institution> Carnegie Mellon University, </institution> <address> Pittsburgh, PA 15213, </address> <month> January </month> <year> 1994. </year>
Reference-contexts: There is also quite a bit of work on protocol composition <ref> [6, 27, 28, 58, 59] </ref>. The first system to provide an automatic modular mechanism for ILP is Abbott and Peter-son [1]. They describe an ILP system that composes macros into integrated loops at compile time, eliminating multiple data traversals. <p> This organization has a direct impact on efficiency: since untrusted software cannot augment these operations, any integration that was not anticipated by the network architects is penalized. Given the richness of possible operations, such mismatches happen quite easily. Furthermore, many systems compose protocols at runtime <ref> [6, 27, 28, 58, 59] </ref>, making static ILP infeasible.
Reference: [28] <author> N.C. Hutchinson and L.L. Peterson. </author> <title> The x-kernel: an architecture for implementing network protocols. </title> <journal> IEEE Trans. on Soft. Eng., </journal> <volume> 17(1), </volume> <month> January </month> <year> 1991. </year>
Reference-contexts: There is also quite a bit of work on protocol composition <ref> [6, 27, 28, 58, 59] </ref>. The first system to provide an automatic modular mechanism for ILP is Abbott and Peter-son [1]. They describe an ILP system that composes macros into integrated loops at compile time, eliminating multiple data traversals. <p> This organization has a direct impact on efficiency: since untrusted software cannot augment these operations, any integration that was not anticipated by the network architects is penalized. Given the richness of possible operations, such mismatches happen quite easily. Furthermore, many systems compose protocols at runtime <ref> [6, 27, 28, 58, 59] </ref>, making static ILP infeasible.
Reference: [29] <author> K.L. Johnson. </author> <title> High-Performance All-Software Distributed Shared Memory. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <month> December </month> <year> 1995. </year>
Reference-contexts: The version of the application that we used is adapted from the n-squared version from the SPLASH-2 benchmark suite [66], and is identical to that reported on in <ref> [29] </ref>. As described in [29], there is a region for each molecule and three small regions used to calculate running sums updated every iteration by each processor. The problem size that we use is 512 molecules. <p> The version of the application that we used is adapted from the n-squared version from the SPLASH-2 benchmark suite [66], and is identical to that reported on in <ref> [29] </ref>. As described in [29], there is a region for each molecule and three small regions used to calculate running sums updated every iteration by each processor. The problem size that we use is 512 molecules. <p> The time is measured in seconds. increase both application and system performance as a whole. Barnes-Hut Barnes-Hut is also taken from <ref> [29] </ref>, and also originated as a SPLASH-2 application. Barnes-Hut simulates the evolution of a system of bodies under the influence of gravitational forces [29] using hierarchical n-body techniques. As described in [29], there is a region for each of the octtree data structure elements present in the SPLASH-2 code: bodies, tree <p> The time is measured in seconds. increase both application and system performance as a whole. Barnes-Hut Barnes-Hut is also taken from <ref> [29] </ref>, and also originated as a SPLASH-2 application. Barnes-Hut simulates the evolution of a system of bodies under the influence of gravitational forces [29] using hierarchical n-body techniques. As described in [29], there is a region for each of the octtree data structure elements present in the SPLASH-2 code: bodies, tree cells, and tree leaves. <p> Barnes-Hut Barnes-Hut is also taken from <ref> [29] </ref>, and also originated as a SPLASH-2 application. Barnes-Hut simulates the evolution of a system of bodies under the influence of gravitational forces [29] using hierarchical n-body techniques. As described in [29], there is a region for each of the octtree data structure elements present in the SPLASH-2 code: bodies, tree cells, and tree leaves.
Reference: [30] <author> K.L. Johnson, M.F. Kaashoek, and D.A. Wallach. </author> <title> CRL: High-performance all-software distributed shared memory. </title> <booktitle> In Proceedings of the 15th Symposium on Operating Systems Principles, </booktitle> <pages> pages 213-228, </pages> <address> Copper Mountain Resort, CO, USA, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: For example, the last few years have seen a proliferation of distributed shared memory systems <ref> [30, 32, 35] </ref>, real-time video and voice applications [63], parallel applications [14, 47], and tightly-coupled distributed systems [2, 60, 56]. <p> Finally, when there is more work that can be done by handlers, the handler versions do even better. 6.4.2 Parallel applications using CRL Distributed shared memory (DSM) is a programming model which provides applications with a shared address space abstraction. The C Region Library <ref> [30] </ref> is an all-software distributed shared memory system developed at MIT and in use by several groups outside of MIT. Because it is entirely implemented in software, it is easy to port to new platforms, and was thus a good choice for a DSM library for Aegis.
Reference: [31] <author> M.F. Kaashoek, D.R. Engler, G.R. Ganger, and D.A. Wallach. </author> <title> Server operating systems. </title> <booktitle> In Seventh SIGOPS European Workshop: Systems Support for Worldwide Applications, </booktitle> <pages> pages 141-148, </pages> <address> Connemara, Ireland, </address> <month> September </month> <year> 1996. </year>
Reference-contexts: For example, combining the disk buffer cache with the TCP retransmission pool results in greater system performance, because data does not have to be stored in multiple locations <ref> [31] </ref>. 1.1.1 Kernel-level vs. user-level communication There are currently two communication options available to applications running in a distributed system environment: in-kernel protocols and user-level protocols. <p> The first section summarizes the results and conclusions of the thesis. The second section outlines directions for possible future work. 8.1 Summary The goal of the work in this dissertation was to provide efficient communication performance to applications. Other work (e.g., <ref> [31] </ref> and others) has shown that the flexibility of user-level communication can provide enormous performance gains to applications.
Reference: [32] <author> P. Keleher, S. Dwarkadas, A.L. Cox, and W. Zwaenepoel. TreadMarks: </author> <title> Distributed shared memory on standard workstations and operating systems. </title> <booktitle> In Proceedings of the 1994 Winter USENIX Conference, </booktitle> <pages> pages 115-132, </pages> <address> San Francisco, CA, USA, </address> <month> January </month> <year> 1994. </year>
Reference-contexts: For example, the last few years have seen a proliferation of distributed shared memory systems <ref> [30, 32, 35] </ref>, real-time video and voice applications [63], parallel applications [14, 47], and tightly-coupled distributed systems [2, 60, 56]. <p> Functionality is impaired because communication code running at the user level is not integrated with the operating system. For example, some software DSM systems are integrated with virtual memory <ref> [10, 32, 35] </ref>. Because virtual memory is normally implemented in the operating system, it can be awkward and slow for user-level code to query or modify it. The performance of systems built with user-level communication can suffer in two ways.
Reference: [33] <author> S.J. Leffler, M.K. McKusick, M.J. Karels, and J.S. Quarterman. </author> <title> The design and implementation of the 4.3BSD UNIX operating system. </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: Other operating systems are even less responsive, and 18 will not preempt a running application in response to an incoming message <ref> [51, 33] </ref>. The performance of systems using user-level communication may also suffer for technology-specific reasons. Certain network interfaces (e.g., many Ethernet cards) deliver messages only to a limited region of memory. <p> We examine each below. Avoiding message copies Message copies cripple networking performance [1, 13, 56]. However, most network systems make little provision for application-directed data transfer. This results in needless data copies as incoming messages are copied from network buffers to intermediate buffers (e.g., BSD's mbufs <ref> [33] </ref>) and then copied to their eventual destination. To solve this problem, we allow a handler to control where messages are placed in memory, eliminating all intermediate copies. Our general computational model provides two additional benefits.
Reference: [34] <author> C.E. Leiserson, Z.S. Abuhamdeh, </author> <title> D.C. </title> <type> Douglas, C.R. Feynman, </type> <institution> M.N. Ganmukhi, J.V. Hill, W.D. Hillis, B.C. Kuszmaul, M.A. St. Pierre, </institution> <note> D.S. </note> <author> Wells, M.C. Wong, S. Yang, and R. Zak. </author> <title> The network architecture of the Connection Machine CM-5. </title> <note> Early version appeared in Proceedings of SPAA '92, </note> <month> November 9, </month> <year> 1992. </year> <month> 109 </month>
Reference-contexts: We study three applications that use the CRL library: the Traveling Salesman Problem, Water, and Barnes-Hut. In order to port CRL to Aegis, we wrote a very simple active message layer with similar functionality to that provided by Thinking Machines' CM-5 <ref> [34, 61] </ref> and layered this code directly above the AN2 interface. Handlers are used to perform the protocol coherence actions. The handlers are designed to fail only if interrupts are disabled (disabling interrupts is the usual mechanism used to provide atomicity for CRL implementations).
Reference: [35] <author> K. Li. IVY: </author> <title> A shared virtual memory system for parallel computing. </title> <booktitle> In International Conference on Parallel Computing, </booktitle> <pages> pages 94-101, </pages> <address> University Park, PA, USA, </address> <month> August </month> <year> 1988. </year>
Reference-contexts: For example, the last few years have seen a proliferation of distributed shared memory systems <ref> [30, 32, 35] </ref>, real-time video and voice applications [63], parallel applications [14, 47], and tightly-coupled distributed systems [2, 60, 56]. <p> Functionality is impaired because communication code running at the user level is not integrated with the operating system. For example, some software DSM systems are integrated with virtual memory <ref> [10, 32, 35] </ref>. Because virtual memory is normally implemented in the operating system, it can be awkward and slow for user-level code to query or modify it. The performance of systems built with user-level communication can suffer in two ways.
Reference: [36] <editor> J. Liedtke. </editor> <booktitle> On -kernel construction. In Proceedings of the 15th Symposium on Operating Systems Principles, </booktitle> <pages> pages 237-250, </pages> <address> Copper Mountain Resort, CO, USA, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: a message, in which case it can immediately receive the message, or suspended waiting for a message, in which case it must be rescheduled before it can handle the message. and easy to add to an operating system; implementations of fast asynchronous ones are neither common nor trivial to implement <ref> [36] </ref>. Although a fast upcall requires a switch to user space to run the handler, a full process switch is unnecessary; this is what provides us with the speed. <p> Modern operating systems implement upcalls (or exceptions), but only synchronously (i.e., only to a running process). Liedtke's -kernel implements extremely fast, asynchronous upcalls by performing address space switches instead of full context switches <ref> [36] </ref>; it is this type of upcall that we compare ASHs to in this thesis. Because ASHs are intended primarily for simple, small-latency operations, the time they run in can be bounded, since the operating system can reason about their behavior (as well as check for safety). <p> In fact, Liedtke, a researcher who has implemented upcalls highly efficiently, believes that the only way to do so is to completely rewrite the operating system from scratch <ref> [36] </ref>. The handler code written for ASHs and upcalls themselves tends to be simpler than general-purpose applications, once a programmer figures out which are the important cases that should 52 be handled in an handler. <p> Liedtke, a researcher who has written an extremely high performance implementation of upcalls, believes that the only way to do so is to completely rewrite the operating system from scratch <ref> [36] </ref>.
Reference: [37] <author> C. Maeda and B.N. Bershad. </author> <title> Protocol service decomposition for high-performance networking. </title> <booktitle> In Proceedings of the Fourteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 244-255, </pages> <address> Asheville, NC, USA, </address> <year> 1993. </year>
Reference-contexts: Our final application is a Web server. This application vividly demonstrates what happens to system performance when there is a large number of aborts. 55 6.1 Experimental environment This section reports on the base performance of our system (i.e., without the use of ASHs or upcalls). Like other systems <ref> [18, 20, 37, 60, 57] </ref>, all the protocols are implemented in user space. The main point to take from the results in this section is that our implementation performs well and is competitive with the best systems reported in the literature. <p> In summary, the base performance of our system for UDP and TCP is about the same or is better than most high-performance user-level and in-kernel implementations <ref> [18, 20, 25, 37, 57] </ref>, as long as the applications are scheduled when the messages arrive. 6.2 Sandboxing overhead Before we examine any ASH results, it is instructive to understand what kind of sandboxing overhead will be incurred by applications.
Reference: [38] <author> R.P. Martin. HPAM: </author> <title> An Active Message layer for a network of HP workstations. </title> <booktitle> In Proceedings of Hot Interconnects II, </booktitle> <month> August </month> <year> 1994. </year>
Reference-contexts: Active messages on parallel machines do not worry about issues of software protection. Several user-level AM implementations for networks of workstations have recently become available <ref> [38, 60] </ref>. U-Net, originally designed for ATM networks, does provide protection, but only at a cost of higher latency: messages are not executed until the corresponding process happens to be scheduled by the kernel [60]. HPAM is designed for HP workstations connected via an FDDI layer.
Reference: [39] <author> J. Mogul. </author> <title> The case for persistent-connection HTTP. In Conference on Applications, </title> <booktitle> Technologies, Architectures and Protocols for Computer Communication (SIGCOMM '95), </booktitle> <pages> pages 299-313, </pages> <month> August </month> <year> 1995. </year> <note> A more comprehensive version of this paper is available on line at Digitial Equipment Corporation Western Research Laboratory, Research Report 95/4 May, </note> <year> 1995. </year>
Reference-contexts: Each new request requires a new connection to be opened. Because of the inefficiency of this system (since the same client often issues multiple requests to the same server), proposals have been made to allow connections to stay open across multiple requests <ref> [39] </ref>. We examine both single-request and multiple-request connections here. In our experimental setup, the client opens a connection and then repeatedly performs the 77 as the number of requests per connection increases. The time is measured in microseconds per page requested.
Reference: [40] <author> J.C. Mogul and K.K. Ramakrishnan. </author> <title> Eliminating receive livelock in an interrupt-driven kernel. </title> <type> Technical Report 95/8, </type> <institution> Digital Western Research Laboratory, </institution> <month> December </month> <year> 1995. </year> <note> This report is an expanded version of a paper in the Proceedings of the 1996 USENIX Technical Conference. </note>
Reference-contexts: In contrast, the ASH system allows new manipulation functions to be dynamically incorporated into the system. 2.4 Scheduling An improperly designed operating system can suffer from receive livelock when faced with a constant stream of network interrupts, as described in Mogul and Ramakrishnan <ref> [40] </ref>. Correctly adding ASHs to an operating system which has no receive livelock will not reintroduce the problem.
Reference: [41] <author> J.C. Mogul, R.F. Rashid, and M.J. Accetta. </author> <title> The packet filter: An efficient mechanism for user-level network code. </title> <booktitle> In Proceedings of the Eleventh ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 39-51, </pages> <address> Austin, TX, USA, </address> <month> November </month> <year> 1987. </year>
Reference-contexts: At worse, the operating system will not even preempt the gmake, but will instead wait for it to finish running. operating system, an act that many are loathe to do. User-level communication was proposed in order to solve these problems <ref> [41] </ref>. The idea is that all communication code is placed outside the operating system, and each application can have its own communication protocols which it can optimize to its own needs. <p> cannot will be confined to ASHs. 2.2 Safe code importation There are a number of clear antecedents to our work on making ASHs safe: Deutsch's seminal paper [15] and Wahbe et al.'s modern revisitation of safe code importation [62] influenced our ideas strongly, as did Mogul's original packet filter paper <ref> [41] </ref>. In some sense this work can be viewed as a natural extension of the same philosophical foundation that inspired the packet filter: we have provided a framework that allows applications outside of the operating system to install new functionality without kernel modifications. <p> There are a variety of standard techniques available to perform this demultiplexing. One such technique is packet filters <ref> [41] </ref>, in which applications specify patterns describing the packets that they are interested in (e.g., all IP packets sent to port 80) and an in-kernel packet filter engine examines each incoming message, determining the matching application and delivering the message to it. Packet filters can be implemented efficiently [22]. <p> We describe these software techniques here in detail. 1 David Mazieres from MIT designed and implemented the x86 version. 42 Exceptions are prevented using runtime and static checks (as is done in existing packet--filters <ref> [41, 67] </ref>). Wild memory references and jumps are prevented using a combination of address-space fire-walls and sandboxing [62]. The ASH system for the MIPS can bound execution time using a framework inspired by Deutsch [15]. We examine each technique in further detail below. <p> into the ASH itself (since they are now user-level code). 5.3 Network and operating system interfaces Aegis provides protected access to two network devices: a 10 Mbits/s Ethernet and a 155 Mbits/s AN2 (Digital's ATM network). 5.3.1 Ethernet interface The Ethernet device is securely exported by a packet filter engine <ref> [41] </ref>. The Aegis implementation of the packet filter engine, DPF [22], uses dynamic code generation. DPF exploits dynamic code generation in two ways: by using it to eliminate interpretation overhead by compiling packet filters to executable code when they are installed into the kernel, and by using 46 application.
Reference: [42] <author> D. Mosberger, L.L. Peterson, P.G. Bridges, and S. O'Malley. </author> <title> Analysis of techniques to improve protocol processing latency. </title> <type> Technical Report TR96-93, </type> <institution> University of Arizona, </institution> <year> 1996. </year>
Reference-contexts: On the 125, the I/O devices are accessed over a 12.5-MHz TURBOchannel bus. The four DECstations are connected with an AN2 switch [3]. 6.1.2 Methodology While collecting the numbers reported in this thesis, we had a fair number of problems with cache conflicts (similar to problems reported by others <ref> [42] </ref>), because the DECstations have direct-mapped caches. In order to minimize the effect these conflicts had on our experiments, we automatically linked the kernel object files in many different orders and picked a best-case timing to report, for every application.
Reference: [43] <author> G.C. Necula and P. Lee. </author> <title> Safe kernel extensions without run-time checking. </title> <booktitle> In Proceedings of the Second Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 229-243, </pages> <address> Seattle, WA, USA, </address> <month> October </month> <year> 1996. </year>
Reference-contexts: Finally, Necula and Lee are investigating Proof Carrying Code, a method where code carries with it a proof that it is safe <ref> [43] </ref>. The operating system can then check the proof at download time, and therefore there is no need for runtime checks (except as an aid to generating a proof). <p> Finally, different techniques can be used to ensure that ASHs are safe for different systems. For example, the Intel x86 provides hardware support which obviates the need to check loads and stores at runtime. Even on systems without special-purpose hardware, techniques such as proof-carrying code (PCC) <ref> [43] </ref> may provide the ability to download complex kernel code with little to no extra runtime overhead. Additionally, we believe that adding efficient upcalls to a standard operating system may be more difficult than adding ASHs (because ASHs involve less operating system mechanism), but have not actually performed this experiment. <p> Another potential improvement would be to allow ASHs to directly modify kernel data structures through the use of the Proof Carrying Code techniques of Necula and Lee <ref> [43] </ref>, instead of using the system call interface. A further exploration of dynamic protocol composition would be desirable. The problem with having the ability to specialize protocols and abstractions is that a great number of them can arise.
Reference: [44] <author> V. Padmanabhan and J Mogul. </author> <title> Improving HTTP latency. </title> <booktitle> In Proceedings of the Second International WWW Conference, </booktitle> <address> Chicago, IL, USA, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: in our experiments: the first is eight bytes, which is smaller than any page would likely be, but represents the case where the software overhead should matter the most, and the second is 2048 bytes, which was estimated to be approximately the median size for pages by Padmanabhan and Mogul <ref> [44] </ref>. The handler used for both ASHs and upcalls in this set of experiments is built on top of the one for TCP discussed in Section 6.4.1. It handles the low-level TCP protocol actions without invoking Web-server-specific code (for example, the sending of acknowledgments) to certain control messages.
Reference: [45] <author> T.A. Proebsting and S.A. Watterson. </author> <title> Filter fusion. </title> <booktitle> In Proceedings of the 23th Annual Symposium on Principles of Programming Languages, </booktitle> <pages> pages 119-130, </pages> <address> St. Petersburg Beach, FL, USA, </address> <month> January </month> <year> 1996. </year>
Reference-contexts: Given the richness of possible data manipulations, however, disallowing application-specific operations can carry a significant cost. For example, even a single re-traversal of the data can halve available bandwidth. Proebsting and Watterson describe a new algorithm for static ILP using filter fusion <ref> [45] </ref>. Static composition requires that all desired compositions be known and performed at compilation time. There are two main drawbacks to such an approach. The first is the exponential code growth inherent in it.
Reference: [46] <author> S.H. Rodrigues, T.E. Anderson, and D.E. Culler. </author> <title> High-performance local area communication with fast sockets. </title> <booktitle> In Proceedings of the USENIX 1997 Annual Technical Conference, </booktitle> <pages> pages 257-274, </pages> <address> Anaheim, CA, USA, </address> <month> January </month> <year> 1997. </year> <month> 110 </month>
Reference-contexts: promising direction to making ASHs run faster (i.e., by reducing the sandboxing overhead). 24 2.3 ASH benefits The particular abilities that ASHs provide have been provided in part by other networking systems, though not all together. 2.3.1 Message vectoring Message vectoring has been a popular focus of the networking community <ref> [17, 18, 20, 60, 46] </ref>. The main difference between our work and previous work is that ASHs can perform application-specific computation at message arrival. By using application-state and domain knowledge these handlers can perform operations difficult in the context of static protocol specifications.
Reference: [47] <author> D.J. Scales, M. Burrows, and C.A. Thekkath. </author> <title> Experience with parallel computing on the AN2 network. </title> <booktitle> In International Parallel Processing Symposium, </booktitle> <pages> pages 94-103, </pages> <address> Honolulu, HI, USA, </address> <month> April </month> <year> 1996. </year>
Reference-contexts: For example, the last few years have seen a proliferation of distributed shared memory systems [30, 32, 35], real-time video and voice applications [63], parallel applications <ref> [14, 47] </ref>, and tightly-coupled distributed systems [2, 60, 56]. Unfortunately, although raw CPU and networking hardware speeds have increased, this increase is not reaching applications: networking software and memory subsystem performance already limit applications and will only do so more in the future [13, 16, 56]. <p> DPF is an order of magnitude faster than the highest performance packet filter engines (MPF [67] and PATHFINDER [4]) in the literature. 5.3.2 AN2 interface Similarly to other systems <ref> [18, 47, 60] </ref>, the AN2 device is securely exported by using the ATM connection identifier to demultiplex packets. Before communicating, processes bind to a virtual circuit identifier (VCI). Two kinds of connections are available on our system: receiving from a single host and receiving from multiple hosts. <p> For the AN2 interface, the table also compares the user-level version to the best in-kernel version we were able to write. Since the hardware overhead for a round trip is approximately 96 microseconds <ref> [47] </ref>, the kernel software is adding only 16 microseconds of overhead. <p> For this measurement, the user-level application is sitting in a tight loop polling for a message; the other processes on the system are basically idle. of packets of different sizes from user level. The maximum achievable per-link bandwidth is about 16.8 MBytes/second (134 Mbits/second) <ref> [47] </ref>. At a 4-KByte packet size, we reach 16.11 MBytes/second. These raw numbers are competitive with other high-performance implementations that also export the network to user space. Scales et al. [47] measure about twice as much software overhead (7,600 cycles or 34 microseconds) for a null packet send using their pvm <p> The maximum achievable per-link bandwidth is about 16.8 MBytes/second (134 Mbits/second) <ref> [47] </ref>. At a 4-KByte packet size, we reach 16.11 MBytes/second. These raw numbers are competitive with other high-performance implementations that also export the network to user space. Scales et al. [47] measure about twice as much software overhead (7,600 cycles or 34 microseconds) for a null packet send using their pvm send and pvm receive interface using the same ATM board, with a substantially faster machine (a 225-MHz DEC 3000 Model 700 AlphaStation rated at 157 SPECint92).
Reference: [48] <author> M.I. Seltzer, Y. Endo, C. Small, and K.A. Smith. </author> <title> Dealing with disaster: Surviving misbehaved kernel extensions. </title> <booktitle> In Proceedings of the Second Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 213-227, </pages> <address> Seattle, WA, USA, </address> <month> October </month> <year> 1996. </year>
Reference-contexts: Small and Seltzer compare a number of approaches to safely executing untrusted code [50]. The Vino project is also investigating means for safely importing code into the kernel <ref> [48] </ref>. They consider five classes of misbehaving application kernel extensions, and introduce a transactional model to prevent these types of misbehavior. Our framework also prevents these classes of misbehavior, but instead of using a heavyweight transactional model, we carefully restrict the interface.
Reference: [49] <author> J.P. Singh, W. Weber, and A. Gupta. </author> <title> SPLASH: Stanford parallel applications for shared memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Water Water is an n-body molecular dynamics application that evaluates forces and potentials in a system of water molecules in the liquid state, as reported in the SPLASH parallel application suite description <ref> [49] </ref>. The version of the application that we used is adapted from the n-squared version from the SPLASH-2 benchmark suite [66], and is identical to that reported on in [29].
Reference: [50] <author> C. Small and M. Seltzer. </author> <title> A comparison of OS extension technologies. </title> <booktitle> In Proceedings of USENIX, </booktitle> <pages> pages 41-54, </pages> <address> San Diego, CA, USA, </address> <month> January </month> <year> 1996. </year>
Reference-contexts: Recently Tennenhouse and Wetherall have proposed to use mobile code to build Active Networks [53]; in an active network, protocols are replaced by programs, which are safely executed on routers on message arrival. Small and Seltzer compare a number of approaches to safely executing untrusted code <ref> [50] </ref>. The Vino project is also investigating means for safely importing code into the kernel [48]. They consider five classes of misbehaving application kernel extensions, and introduce a transactional model to prevent these types of misbehavior.
Reference: [51] <author> P.G. Sobalvarro. </author> <title> Demand-based Cosheduling of Parallel Jobs on Multiprogrammed Multiprocessors. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <month> January </month> <year> 1997. </year>
Reference-contexts: Other operating systems are even less responsive, and 18 will not preempt a running application in response to an incoming message <ref> [51, 33] </ref>. The performance of systems using user-level communication may also suffer for technology-specific reasons. Certain network interfaces (e.g., many Ethernet cards) deliver messages only to a limited region of memory. <p> Issues about schedulability and when and how a message handler should abort have been recently explored in Optimistic Active Messages [65]. The tradeoffs discussed there are applicable here. Patrick Sobalvarro has explored demand-based coscheduling, where processes that are communicating with one another on different machines are scheduled simultaneously <ref> [51] </ref>. This strategy greatly increases the likelihood that an application will be running when a message arrives for it if the application is communicating often with the other applications it is interested in being coscheduled with.
Reference: [52] <author> R.W. Stevens. </author> <title> TCP/IP illustrated: </title> <booktitle> the protocols, </booktitle> <volume> volume 1, chapter 18, </volume> <pages> page 237. </pages> <publisher> Addison-Wesley Pub. Co., </publisher> <year> 1994. </year>
Reference-contexts: Although a larger MSS (up to the size of the maximum buffer size of the underlying network) is often better, especially for local communication, the default non-local size is normally only 536 bytes <ref> [52] </ref>. We 68 therefore performed a throughput experiment with the MSS set to this size; this is advantageous to handlers, for there is more work to be done which is application-independent and can thus be handled by the library ASH transparently to the application.
Reference: [53] <author> D.L. Tennenhouse and D.J. Wetherall. </author> <title> Towards an active network architecture. </title> <booktitle> In Proc. Multimedia, Computing, and Networking 96, </booktitle> <month> January </month> <year> 1996. </year>
Reference-contexts: No numbers are reported yet for TCP. With the advent of HotJava and Java [26], code importation in the form of mobile code has received a lot of press. Recently Tennenhouse and Wetherall have proposed to use mobile code to build Active Networks <ref> [53] </ref>; in an active network, protocols are replaced by programs, which are safely executed on routers on message arrival. Small and Seltzer compare a number of approaches to safely executing untrusted code [50]. The Vino project is also investigating means for safely importing code into the kernel [48].
Reference: [54] <author> C.A. Thekkath and H.M. Levy. </author> <title> Limits to low-latency communication on high-speed networks. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(2) </volume> <pages> 179-203, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Using larger train sizes increases the throughput. On the Ethernet, both UDP latency and throughput are (modulo processor speed differences) about the same as the fastest implementation reported in the literature <ref> [54] </ref>. Using the AN2 interface, UDP latencies are about 43 microseconds higher than the raw user-level latencies. This difference is because the UDP library allocates send buffers, and initializes IP and UDP fields.
Reference: [55] <author> C.A. Thekkath and H.M. Levy. </author> <title> Hardware and software support for efficient exception handling. </title> <booktitle> In Sixth International Conference on Architecture Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 110-119, </pages> <address> San Francisco, CA, USA, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: The polling and interrupt time would certainly increase. We estimate those times to be equal to each other; the actual value we use is taken from Thekkath and Levy's measurement of the time to Deliver Simple Exception to Null User Handler <ref> [55] </ref> and scaled for our hardware (it was measured on a DECstation 5000/200).
Reference: [56] <author> C.A. Thekkath, H.M. Levy, and E.D. Lazowska. </author> <title> Separating data and control transfer in distributed operating systems. </title> <booktitle> In Sixth International Conference on Architecture Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 2-11, </pages> <address> San Francisco, California, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: For example, the last few years have seen a proliferation of distributed shared memory systems [30, 32, 35], real-time video and voice applications [63], parallel applications [14, 47], and tightly-coupled distributed systems <ref> [2, 60, 56] </ref>. Unfortunately, although raw CPU and networking hardware speeds have increased, this increase is not reaching applications: networking software and memory subsystem performance already limit applications and will only do so more in the future [13, 16, 56]. <p> Unfortunately, although raw CPU and networking hardware speeds have increased, this increase is not reaching applications: networking software and memory subsystem performance already limit applications and will only do so more in the future <ref> [13, 16, 56] </ref>. This thesis addresses the important problem of delivering hardware-level network performance to applications by introducing application-specific safe message handlers (ASHs), which are user-written handlers that are safely and efficiently executed in the kernel in response to a message arrival. <p> This ability allows them to perform control operations at message reception, implementing such computational actions as traditional active messages [61] or remote lock acquisition in a distributed shared memory system. Even recently, low-overhead control transfer had been considered to be infeasible to implement <ref> [56] </ref>. We have also integrated support for dynamic integrated layer processing into the ASH system. Current systems often have a number of protocol layers between the application and the network, with each layer often requiring that the entire message be touched (e.g., to compute a checksum). <p> The send part of a layer (if any) is similarly structured to the receive path, only without an abort handler. writes <ref> [56] </ref>. The ASH extracts the message destination address and length from the message (using the consume library call). It then copies the payload to the destination address, also using the consume call. The message is not passed up further. <p> We take this measurement in isolation, without the cost of communication, but with both ASHs 60 running in the kernel 1 . The remote write, modeled after that of Thekkath et al. <ref> [56] </ref>, reads the segment number, offset, and size from the message, uses address translation tables to determine the correct place to write the data to, and then writes the data (assuming the request is valid). <p> To address this bottleneck, applications must be able to direct message placement, and to exploit ILP during copying. We examine each below. Avoiding message copies Message copies cripple networking performance <ref> [1, 13, 56] </ref>. However, most network systems make little provision for application-directed data transfer. This results in needless data copies as incoming messages are copied from network buffers to intermediate buffers (e.g., BSD's mbufs [33]) and then copied to their eventual destination. <p> The cost of control transfers is sufficiently high that recently a dichotomy has been drawn between control and data transfer in the interests of constructing systems to efficiently perform just data transfer <ref> [56] </ref>. Handlers remove the restrictive cost of control transfer for those operations that can be expressed in terms of handlers. We believe that the expressiveness of handlers as we have described them in this thesis is sufficient for most operations subject to low-latency requirements.
Reference: [57] <author> C.A. Thekkath, T.D. Nguyen, E. Moy, and E. Lazowska. </author> <title> Implementing network protocols at user level. </title> <booktitle> In ACM Communication Architectures, Protocols, and Applications (SIGCOMM) 1993, </booktitle> <pages> pages 64-73, </pages> <address> San Francisco, CA, USA, </address> <month> October </month> <year> 1993. </year>
Reference-contexts: Our final application is a Web server. This application vividly demonstrates what happens to system performance when there is a large number of aborts. 55 6.1 Experimental environment This section reports on the base performance of our system (i.e., without the use of ASHs or upcalls). Like other systems <ref> [18, 20, 37, 60, 57] </ref>, all the protocols are implemented in user space. The main point to take from the results in this section is that our implementation performs well and is competitive with the best systems reported in the literature. <p> In summary, the base performance of our system for UDP and TCP is about the same or is better than most high-performance user-level and in-kernel implementations <ref> [18, 20, 25, 37, 57] </ref>, as long as the applications are scheduled when the messages arrive. 6.2 Sandboxing overhead Before we examine any ASH results, it is instructive to understand what kind of sandboxing overhead will be incurred by applications.
Reference: [58] <author> C. Tschudin. </author> <title> Flexible protocol stacks. </title> <booktitle> In Proc. SIGCOMM 1991, </booktitle> <pages> pages 197-204, </pages> <address> Zurich, Switzerland, </address> <month> September </month> <year> 1991. </year>
Reference-contexts: There is also quite a bit of work on protocol composition <ref> [6, 27, 28, 58, 59] </ref>. The first system to provide an automatic modular mechanism for ILP is Abbott and Peter-son [1]. They describe an ILP system that composes macros into integrated loops at compile time, eliminating multiple data traversals. <p> This organization has a direct impact on efficiency: since untrusted software cannot augment these operations, any integration that was not anticipated by the network architects is penalized. Given the richness of possible operations, such mismatches happen quite easily. Furthermore, many systems compose protocols at runtime <ref> [6, 27, 28, 58, 59] </ref>, making static ILP infeasible.
Reference: [59] <author> R. van Renesse, K.P. Birman, R. Friedman, M. Hayden, and D. Karr. </author> <title> A framework for protocol composition in Horus. </title> <booktitle> In Proceedings of Fourteenth ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 138-150, </pages> <address> Ottawa, Ontario, Canada, </address> <month> August </month> <year> 1995. </year> <month> 111 </month>
Reference-contexts: There is also quite a bit of work on protocol composition <ref> [6, 27, 28, 58, 59] </ref>. The first system to provide an automatic modular mechanism for ILP is Abbott and Peter-son [1]. They describe an ILP system that composes macros into integrated loops at compile time, eliminating multiple data traversals. <p> This organization has a direct impact on efficiency: since untrusted software cannot augment these operations, any integration that was not anticipated by the network architects is penalized. Given the richness of possible operations, such mismatches happen quite easily. Furthermore, many systems compose protocols at runtime <ref> [6, 27, 28, 58, 59] </ref>, making static ILP infeasible.
Reference: [60] <author> T. von Eicken, A. Basu, V. Buch, and W. Vogels. U-Net: </author> <title> A user-level network interface for parallel and distributed computing. </title> <booktitle> In Proceedings of the Fifteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 40-53, </pages> <address> Copper Mountain Resort, CO, USA, </address> <year> 1995. </year>
Reference-contexts: For example, the last few years have seen a proliferation of distributed shared memory systems [30, 32, 35], real-time video and voice applications [63], parallel applications [14, 47], and tightly-coupled distributed systems <ref> [2, 60, 56] </ref>. Unfortunately, although raw CPU and networking hardware speeds have increased, this increase is not reaching applications: networking software and memory subsystem performance already limit applications and will only do so more in the future [13, 16, 56]. <p> This is a serious problem for user-level communication systems. For example, the roundtrip time for a short message (up to 40 bytes) in U-Net increases from 65 microseconds to 125 microseconds if the applications performing the communication are not scheduled when the message arrives <ref> [60] </ref>; in Hamlyn the time increases from 28 microseconds to 78 microseconds [9]. Both U-Net and Hamlyn are implemented on operating systems which will suspend the currently running application and immediately reschedule the application an arriving message is for, if the application is suspended waiting for the message. <p> promising direction to making ASHs run faster (i.e., by reducing the sandboxing overhead). 24 2.3 ASH benefits The particular abilities that ASHs provide have been provided in part by other networking systems, though not all together. 2.3.1 Message vectoring Message vectoring has been a popular focus of the networking community <ref> [17, 18, 20, 60, 46] </ref>. The main difference between our work and previous work is that ASHs can perform application-specific computation at message arrival. By using application-state and domain knowledge these handlers can perform operations difficult in the context of static protocol specifications. <p> Active messages on parallel machines do not worry about issues of software protection. Several user-level AM implementations for networks of workstations have recently become available <ref> [38, 60] </ref>. U-Net, originally designed for ATM networks, does provide protection, but only at a cost of higher latency: messages are not executed until the corresponding process happens to be scheduled by the kernel [60]. HPAM is designed for HP workstations connected via an FDDI layer. <p> Several user-level AM implementations for networks of workstations have recently become available [38, 60]. U-Net, originally designed for ATM networks, does provide protection, but only at a cost of higher latency: messages are not executed until the corresponding process happens to be scheduled by the kernel <ref> [60] </ref>. HPAM is designed for HP workstations connected via an FDDI layer. It makes the optimistic assumption that incoming messages are intended for the currently running process; messages intended for other processes are copied multiple times. <p> Packet filters can be implemented efficiently [22]. Another technique sometimes used by user-level communication systems implemented on top of ATM networks is to use the virtual circuit index of the incoming message as an application identifier <ref> [60] </ref>. Any technique used by user-level communication to demultiplex messages may also be used by user-level communication augmented with ASHs or upcalls. <p> DPF is an order of magnitude faster than the highest performance packet filter engines (MPF [67] and PATHFINDER [4]) in the literature. 5.3.2 AN2 interface Similarly to other systems <ref> [18, 47, 60] </ref>, the AN2 device is securely exported by using the ATM connection identifier to demultiplex packets. Before communicating, processes bind to a virtual circuit identifier (VCI). Two kinds of connections are available on our system: receiving from a single host and receiving from multiple hosts. <p> Our final application is a Web server. This application vividly demonstrates what happens to system performance when there is a large number of aborts. 55 6.1 Experimental environment This section reports on the base performance of our system (i.e., without the use of ASHs or upcalls). Like other systems <ref> [18, 20, 37, 60, 57] </ref>, all the protocols are implemented in user space. The main point to take from the results in this section is that our implementation performs well and is competitive with the best systems reported in the literature. <p> U-Net (182 vs. 66 microseconds), since our experiments are taken on slower machines (40-MHz vs. 66-MHz), the AN2 hardware latency is higher than the Fore latency (96 microseconds vs. 42 microseconds), and we have not attempted to rewrite the AN2 firmware to achieve low latency, as was done for U-Net <ref> [60] </ref>. <p> The general structure is similar to other implementations of user-level protocols <ref> [20, 60] </ref>. The UDP implementation is a straightforward implementation of the UDP protocol as specified in RFC768. Similarly, the TCP implementation is a library-based implementation of RFC793. <p> Using the AN2 interface, UDP latencies are about 43 microseconds higher than the raw user-level latencies. This difference is because the UDP library allocates send buffers, and initializes IP and UDP fields. Our implementation seems to have lower overhead than U-Net <ref> [60] </ref>; the U-Net implementation adds 73 microseconds on a 66-MHz processor while our implementation adds 62 microseconds on a 40-MHz processor (even though, unlike their numbers, our checksum and memory copy are not integrated for this measurement). <p> Past systems precluded protected, low-latency control transfer, or heavily relied on user-level polling to achieve performance (e.g., in U-Net using signals to indicate the arrival of a message instead of polling adds 60 microseconds to the 65-microsecond roundtrip latency <ref> [60] </ref>). The cost of control transfers is sufficiently high that recently a dichotomy has been drawn between control and data transfer in the interests of constructing systems to efficiently perform just data transfer [56].
Reference: [61] <author> T. von Eicken, D.E. Culler, S.C. Goldstein, and K.E. Schauser. </author> <title> Active messages: a mechanism for integrated communication and computation. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <pages> pages 256-266, </pages> <address> Gold Coast, Aus-tralia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: Control initiation ASHs can perform general computation. This ability allows them to perform control operations at message reception, implementing such computational actions as traditional active messages <ref> [61] </ref> or remote lock acquisition in a distributed shared memory system. Even recently, low-overhead control transfer had been considered to be infeasible to implement [56]. We have also integrated support for dynamic integrated layer processing into the ASH system. <p> Nevertheless, their simple interface is easy to implement and tune; it remains to be seen if the expressiveness we provide is superior to it for real applications on real systems. 2.3.2 Control initiation In the parallel community the concept of active messages <ref> [61] </ref> has gained great popularity, since it dramatically decreases latency by executing the required code directly in the message handler. Active messages on parallel machines do not worry about issues of software protection. Several user-level AM implementations for networks of workstations have recently become available [38, 60]. <p> Examples include remote lock acquisition, reference counting, voting, global barriers, object location queries, and method invocations. The need for low-latency remote computation is so overwhelming that the parallel community has spawned a new paradigm of programming built around the concept of active messages <ref> [61] </ref>: an efficient, unprotected transfer of control to the application in the interrupt handler. A key benefit of both ASHs and upcalls is that because the runtime of downloaded code is bounded, they can be run in situations when performing a full context switch to an unscheduled application is impractical. <p> We study three applications that use the CRL library: the Traveling Salesman Problem, Water, and Barnes-Hut. In order to port CRL to Aegis, we wrote a very simple active message layer with similar functionality to that provided by Thinking Machines' CM-5 <ref> [34, 61] </ref> and layered this code directly above the AN2 interface. Handlers are used to perform the protocol coherence actions. The handlers are designed to fail only if interrupts are disabled (disabling interrupts is the usual mechanism used to provide atomicity for CRL implementations).
Reference: [62] <author> R. Wahbe, S. Lucco, T. Anderson, and S. Graham. </author> <title> Efficient software-based fault isolation. </title> <booktitle> In Proceedings of the Fourteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 203-216, </pages> <address> Asheville, NC, USA, </address> <month> December </month> <year> 1993. </year>
Reference-contexts: Although the concept of sandboxing is not new <ref> [62] </ref>, we introduce the concept of avoiding exceptions, discuss a variety of techniques for bounding execution time, and limit the available operating system interface in order to restrict the amount of OS changes that need to be made to support application code running in the kernel. * Experimental evaluation The thesis <p> tolerate more latency can use the flexibility of the upcall; those that cannot will be confined to ASHs. 2.2 Safe code importation There are a number of clear antecedents to our work on making ASHs safe: Deutsch's seminal paper [15] and Wahbe et al.'s modern revisitation of safe code importation <ref> [62] </ref> influenced our ideas strongly, as did Mogul's original packet filter paper [41]. <p> In this chapter, we discuss these two issues first for ASHs and then for upcalls. This chapter presents a variety of software and hardware techniques that go beyond the ones originally proposed by Deutsch and Grant [15] and Wahbe et al. <ref> [62] </ref>. <p> Wild memory references and jumps are prevented using a combination of address-space fire-walls and sandboxing <ref> [62] </ref>. The ASH system for the MIPS can bound execution time using a framework inspired by Deutsch [15]. We examine each technique in further detail below. Preventing exceptions Exceptions are prevented either through runtime or download-time checks. <p> On the MIPS architecture, code executing in kernel mode can read and write physical memory directly. To prevent this, we force all loads and stores to have user-level addresses, using the code inspection (sandboxing) techniques of Wahbe et al. <ref> [62] </ref>. Making sandboxed data copies efficient requires complex analysis of the user-supplied code. The ASH system therefore provides the capability of accessing message data through specialized trusted function calls, implemented in the kernel. These calls allow access checks to be aggregated at initiation time. <p> This thesis also provides a detailed description of the techniques necessary to run application code in the kernel safely even when the application programmer is not constrained to write in a pointer-safe language. Although the concept of sandboxing is not new <ref> [62] </ref>, we extended its implementation by avoiding the exceptions ASHs could take, bounding their execution time, and limiting the available operating system interface in order to restrict the amount of OS changes that need to be made to support application code running in the kernel.
Reference: [63] <author> I. Wakeman, A. Ghosh, J. Crowcroft, V. Jacobson, and S. Floyd. </author> <title> Implementing real time packet forwarding policies using Streams. </title> <booktitle> In Proceedings USENIX Winter 1995 Technical Conference, </booktitle> <pages> pages 71-82, </pages> <address> New Orleans, LA, USA, </address> <month> January </month> <year> 1995. </year>
Reference-contexts: For example, the last few years have seen a proliferation of distributed shared memory systems [30, 32, 35], real-time video and voice applications <ref> [63] </ref>, parallel applications [14, 47], and tightly-coupled distributed systems [2, 60, 56]. Unfortunately, although raw CPU and networking hardware speeds have increased, this increase is not reaching applications: networking software and memory subsystem performance already limit applications and will only do so more in the future [13, 16, 56].
Reference: [64] <author> D. A. Wallach, D. R. Engler, and M. F. Kaashoek. ASHs: </author> <title> Application-specific handlers for high-performance messaging. </title> <booktitle> In ACM Communication Architectures, Protocols, and Applications (SIGCOMM '96), </booktitle> <address> Stanford, California, </address> <month> August </month> <year> 1996. </year>
Reference: [65] <author> D.A. Wallach, W.C. Hsieh, K.L. Johnson, M.F. Kaashoek, and W.E. Weihl. </author> <title> Optimistic active messages: A mechanism for scheduling communication with computation. </title> <booktitle> In Proceedings of the 5th Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 217-226, </pages> <address> Santa Barbara, CA, USA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Issues about schedulability and when and how a message handler should abort have been recently explored in Optimistic Active Messages <ref> [65] </ref>. The tradeoffs discussed there are applicable here. Patrick Sobalvarro has explored demand-based coscheduling, where processes that are communicating with one another on different machines are scheduled simultaneously [51].
Reference: [66] <author> S.C. Woo, M. Ohara, E. Torrie, J.P. Singh, and A. Gupta. </author> <title> The SPLASH-2 programs: Characterization and methodological considerations. </title> <booktitle> In Proceedings of the 22nd International Symposium on Computer Architecture, </booktitle> <pages> pages 24-36, </pages> <address> Santa Margherita Ligure, Italy, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: The version of the application that we used is adapted from the n-squared version from the SPLASH-2 benchmark suite <ref> [66] </ref>, and is identical to that reported on in [29]. As described in [29], there is a region for each molecule and three small regions used to calculate running sums updated every iteration by each processor. The problem size that we use is 512 molecules.
Reference: [67] <author> M. Yuhara, B. Bershad, C. Maeda, and E. Moss. </author> <title> Efficient packet demultiplexing for multiple endpoints and large messages. </title> <booktitle> In Proceedings of the Winter 1994 USENIX Conference, </booktitle> <pages> pages 153-165, </pages> <address> San Francisco, CA, USA, </address> <month> January </month> <year> 1994. </year> <month> 112 </month>
Reference-contexts: We describe these software techniques here in detail. 1 David Mazieres from MIT designed and implemented the x86 version. 42 Exceptions are prevented using runtime and static checks (as is done in existing packet--filters <ref> [41, 67] </ref>). Wild memory references and jumps are prevented using a combination of address-space fire-walls and sandboxing [62]. The ASH system for the MIPS can bound execution time using a framework inspired by Deutsch [15]. We examine each technique in further detail below. <p> DPF is an order of magnitude faster than the highest performance packet filter engines (MPF <ref> [67] </ref> and PATHFINDER [4]) in the literature. 5.3.2 AN2 interface Similarly to other systems [18, 47, 60], the AN2 device is securely exported by using the ATM connection identifier to demultiplex packets. Before communicating, processes bind to a virtual circuit identifier (VCI).
References-found: 67

