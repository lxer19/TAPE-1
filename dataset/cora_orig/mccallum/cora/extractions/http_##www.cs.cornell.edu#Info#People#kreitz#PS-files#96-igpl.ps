URL: http://www.cs.cornell.edu/Info/People/kreitz/PS-files/96-igpl.ps
Refering-URL: http://www.cs.cornell.edu/Info/People/kreitz/Abstracts/96-igpl.html
Root-URL: 
Email: E-mail: kreitz@intellektik.informatik.th-darmstadt.de  
Title: Formal Mathematics for Verifiably Correct Program Synthesis  
Author: CHRISTOPH 
Keyword: Program Synthesis, Type Theory, Reflected Reasoning  
Address: D-64283 Darmstadt, Germany.  
Affiliation: KREITZ, Fachgebiet Intellektik, Fachbereich Informatik, Technische Hochschule Darmstadt, Alexanderstrae 10,  
Abstract: We describe a formalization of the meta-mathematics of programming in a higher-order logical calculus as a means to create verifiably correct implementations of program synthesis tools. Using reflected notions of programming concepts we can specify the actions of synthesis methods within the object language of the calculus and prove formal theorems about their behavior. The theorems serve as derived inference rules implementing the kernel of these methods in a flexible, safe, efficient and comprehensible way. We demonstrate the advantages of using formal mathematics in support of program development systems through an example in which we formalize a strategy for deriving global search algorithms from formal specifications. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Stuart F. Allen, Robert L. Constable, Douglas J. Howe, and William E. Aitken. </author> <title> The semantics of reflected proof. </title> <editor> In John C. Mitchell, editor, </editor> <booktitle> Proceedings of the Fifth Annual Symposium on Logic in Computer Science, </booktitle> <pages> pages 95-106. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1990. </year>
Reference-contexts: The concept of tactics aims at specializing general inference systems towards a particular application. Using meta-theorems and reflection as a general technique to control the application of tactics has already been investigated in <ref> [12, 13, 1] </ref>. In Isabelle [22] meta-theorems about reasoning within some object logic are used to generate derived inference rules for this logic.
Reference: [2] <author> Wolfgang Bibel. </author> <title> Syntax-directed, semantics-supported program synthesis. </title> <journal> Artificial Intelligence, </journal> <volume> 14(3) </volume> <pages> 243-261, </pages> <year> 1980. </year>
Reference-contexts: Therefore, less rigorous methods are used when developing strategies. During the last decades many approaches (see e.g. <ref> [9, 17, 18, 2, 26] </ref>) have been implemented and tested successfully for a number of examples. The KIDS system [26, 29] is believed to be close to the point where it can be used to develop small routine programs. <p> Since then we have refined our approach by elaborating a formal meta-theory of programming and begun an implementation with the NuPRL proof development system [4]. We have investigated translations between synthesis paradigms and represented synthesis strategies of the systems LOPS <ref> [2] </ref> and KIDS [27, 28, 29]. The results, which are presented in detail in the author's technical report [15], show that a rigorous approach does in fact lead to safe and efficient program synthesis systems.
Reference: [3] <author> Alan Bundy. </author> <title> Automatic guidance of program synthesis proofs. </title> <booktitle> In Proceedings of the Workshop on Automating Software Design, IJCAI-89, </booktitle> <pages> pages 57-59, </pages> <year> 1989. </year>
Reference-contexts: So far the most fruitful approach to bridge the gap between formal deduction and complex applications has been that of tactics, first introduced in Edinburgh LCF [8], and since adopted in many other systems (see e.g. <ref> [20, 4, 21, 3, 11] </ref>). Here deductive methods are written as meta-programs guiding the application of inference rules. Tactics are a means to combine the advantages of formality with those of high-level methods.
Reference: [4] <author> Robert L. Constable et. al. </author> <title> Implementing Mathematics with the NuPRL proof development system. </title> <publisher> Prentice Hall, </publisher> <year> 1986. </year>
Reference-contexts: Many formal calculi are, at least in principle, powerful enough to express all of mathematics and programming (see e.g. <ref> [16, 4, 7, 5, 24] </ref>). But there remains a problem of expressiveness in practical applications. Inference steps in these logics operate on a very low level and make program derivations long and difficult to comprehend. <p> So far the most fruitful approach to bridge the gap between formal deduction and complex applications has been that of tactics, first introduced in Edinburgh LCF [8], and since adopted in many other systems (see e.g. <ref> [20, 4, 21, 3, 11] </ref>). Here deductive methods are written as meta-programs guiding the application of inference rules. Tactics are a means to combine the advantages of formality with those of high-level methods. <p> Since then we have refined our approach by elaborating a formal meta-theory of programming and begun an implementation with the NuPRL proof development system <ref> [4] </ref>. We have investigated translations between synthesis paradigms and represented synthesis strategies of the systems LOPS [2] and KIDS [27, 28, 29]. <p> This allows one to `implement' the theory presented on paper without any modifications by using a proof system for the underlying calculus. As a basic calculus we have selected the formulation of intuitionistic type theory [16] used by the NuPRL proof development system <ref> [4] </ref>. Type theory already provides formalizations of a constructive higher order logic and low level constructs such as integers, strings, (recursive) function spaces, products, sums, lists, etc. as well as the corresponding type constructors. <p> parameters in a definition. 78 Formal Mathematics for Verifiably Correct Program Synthesis TYPES Class of first level data types = ff , 6 = ff Equality decision procedures for type ff let x=term in expr, expr where x=term abstraction over a term letrec f (x)=body (Recursive) function definition (predefined in <ref> [4, ch. 12] </ref>) dom (f)(x) x is in the domain of f (predefined in [4, ch. 12]) letrec f (x)=body in t abstraction over a recursive function fffififi, hha 1 ,..,a n ii, a.i Product type declaration, Tuple, i-th projection let hha 1 ,..,a n ii = p in expr Local <p> of first level data types = ff , 6 = ff Equality decision procedures for type ff let x=term in expr, expr where x=term abstraction over a term letrec f (x)=body (Recursive) function definition (predefined in <ref> [4, ch. 12] </ref>) dom (f)(x) x is in the domain of f (predefined in [4, ch. 12]) letrec f (x)=body in t abstraction over a recursive function fffififi, hha 1 ,..,a n ii, a.i Product type declaration, Tuple, i-th projection let hha 1 ,..,a n ii = p in expr Local assignment of projections to the a i IB, true, false Data type of boolean <p> In principle, all these aspects are inherently contained in all constructive formal theories. Meta-theorems do in fact `behave' like primitive inference rules of NuPRL`s type theory <ref> [4] </ref>. The true advantage of formulating and proving meta-theorems is, however, that one can turn an existing general inference system like NuPRL into a high level reasoning system specialized in the area of program development. <p> To realize our approach we could rely on several well-known theoretical insights and techniques. A comprehensible representation of formal theories is the main purpose of the definition mechanism of the NuPRL proof development systems <ref> [4] </ref>. The concept of tactics aims at specializing general inference systems towards a particular application. Using meta-theorems and reflection as a general technique to control the application of tactics has already been investigated in [12, 13, 1].
Reference: [5] <author> Thierry Coquand and Gerard Huet. </author> <title> The calculus of constructions. </title> <journal> Information and Computation, </journal> <volume> 76 </volume> <pages> 95-120, </pages> <year> 1988. </year>
Reference-contexts: Many formal calculi are, at least in principle, powerful enough to express all of mathematics and programming (see e.g. <ref> [16, 4, 7, 5, 24] </ref>). But there remains a problem of expressiveness in practical applications. Inference steps in these logics operate on a very low level and make program derivations long and difficult to comprehend.
Reference: [6] <author> J. </author> <title> Costas. A study of a class of detection waveforms having nearly ideal range doppler amibi-guity properties. </title> <booktitle> In Proceedings of the IEEE, </booktitle> <volume> volume 72, </volume> <pages> pages 996-1009, </pages> <year> 1984. </year>
Reference-contexts: The derivation selects the non-well-founded GS-theory gs seq over set and can be justified only via the newly introduced concept of wf-filters. Example 4.13 (Synthesis of an Algorithm enumerating Costas Arrays) In <ref> [6] </ref>, Costas introduced a class of permutations that can be used to generate radar and sonar signals with ideal ambiguity functions. Since then, many publications have investigated combinatorial properties of these permutations, now known as Costas arrays, but no general construction has been found.
Reference: [7] <author> Jean-Yves Girard, Yves Lafont, and Paul Taylor. </author> <title> Proofs and Types. </title> <publisher> Cambridge University Press, </publisher> <year> 1989. </year>
Reference-contexts: Many formal calculi are, at least in principle, powerful enough to express all of mathematics and programming (see e.g. <ref> [16, 4, 7, 5, 24] </ref>). But there remains a problem of expressiveness in practical applications. Inference steps in these logics operate on a very low level and make program derivations long and difficult to comprehend.
Reference: [8] <author> Michael J. Gordon, Robin Milner, and Christopher P. Wadsworth. </author> <title> Edinburgh LCF: A mechanized Logic of Computation. </title> <publisher> LNCS 78, Springer Verlag, </publisher> <year> 1979. </year>
Reference-contexts: Most researchers are aware of these inadequacies but shy away from the amount of labour which is necessary to overcome them. So far the most fruitful approach to bridge the gap between formal deduction and complex applications has been that of tactics, first introduced in Edinburgh LCF <ref> [8] </ref>, and since adopted in many other systems (see e.g. [20, 4, 21, 3, 11]). Here deductive methods are written as meta-programs guiding the application of inference rules. Tactics are a means to combine the advantages of formality with those of high-level methods.
Reference: [9] <author> Cordell C. Green. </author> <title> An application of theorem proving to problem solving. </title> <booktitle> In Proceedings of the 1 st International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 219-239, </pages> <year> 1969. </year>
Reference-contexts: Therefore, less rigorous methods are used when developing strategies. During the last decades many approaches (see e.g. <ref> [9, 17, 18, 2, 26] </ref>) have been implemented and tested successfully for a number of examples. The KIDS system [26, 29] is believed to be close to the point where it can be used to develop small routine programs.
Reference: [10] <author> S. Hayashi. PX: </author> <title> a system extracting programs from proofs. </title> <booktitle> In Proceedings of the IFIP Confer-eence on Formal Description of Programming Concepts, </booktitle> <pages> pages 399-424, </pages> <year> 1986. </year>
Reference-contexts: This means that using formal meta-theorems within a synthesis process can also lead to a significant efficiency improvement of the generated code compared to algorithms extracted from `pure' synthesis proofs <ref> [25, 10, 23] </ref>. Theorems like theorem 4.12 contain complex algorithms which have been introduced and verified explicitly in their formal proofs in order to support a synthesis of efficient programs without having to execute long and complex derivations.
Reference: [11] <author> M. Heisel, W. Reif, and W. Stephan. </author> <title> Tactical theorem proving in program verification. </title> <editor> In M. E. Stickel, editor, </editor> <booktitle> Proceedings of the 10 th Conference on Automated Deduction, </booktitle> <volume> LNCS 449, </volume> <pages> pages 117-131. </pages> <publisher> Springer Verlag, </publisher> <year> 1990. </year>
Reference-contexts: So far the most fruitful approach to bridge the gap between formal deduction and complex applications has been that of tactics, first introduced in Edinburgh LCF [8], and since adopted in many other systems (see e.g. <ref> [20, 4, 21, 3, 11] </ref>). Here deductive methods are written as meta-programs guiding the application of inference rules. Tactics are a means to combine the advantages of formality with those of high-level methods.
Reference: [12] <author> Todd B. Knoblock and Robert L. Constable. </author> <title> Formalized metareasoning in Type Theory. </title> <booktitle> In Proceedings of the First Annual Symposium on Logic in Computer Science. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <year> 1986. </year>
Reference-contexts: The concept of tactics aims at specializing general inference systems towards a particular application. Using meta-theorems and reflection as a general technique to control the application of tactics has already been investigated in <ref> [12, 13, 1] </ref>. In Isabelle [22] meta-theorems about reasoning within some object logic are used to generate derived inference rules for this logic.
Reference: [13] <author> Todd B. Knoblock. </author> <title> Metamathematical extensibility in Type Theory. </title> <type> PhD thesis, </type> <institution> Cornell University. Department of Computer Science, </institution> <address> Ithaca, NY, </address> <year> 1987. </year>
Reference-contexts: The concept of tactics aims at specializing general inference systems towards a particular application. Using meta-theorems and reflection as a general technique to control the application of tactics has already been investigated in <ref> [12, 13, 1] </ref>. In Isabelle [22] meta-theorems about reasoning within some object logic are used to generate derived inference rules for this logic.
Reference: [14] <author> Christoph Kreitz. </author> <title> Towards a formal theory of program construction. </title> <journal> Revue d' intelligence artificielle, </journal> <volume> 4(3) </volume> <pages> 53-79, </pages> <month> November </month> <year> 1990. </year>
Reference-contexts: Another advantage is that the mathematical language makes it possible to integrate approaches to program synthesis which currently appear to be incomparable. Furthermore, the gap between formal and `humanly comprehensible' reasoning can be bridged by expressing formal theorems in a programmer's terminology. In <ref> [14] </ref> we have begun the formalization of basic programming concepts and shown how the synthesis paradigms of proofs-as-programs and synthesis by transformations are reflected in such a framework. <p> Except for the fact that its variables must be quantified and typed a formal meta-theorem is almost identical to its informal counterpart. The following theorem, for instance, deals with the well-known relation between program synthesis and proving a `specification theorem'. Theorem 3.1 (Proofs-as-Programs (cf. <ref> [14] </ref>, Theorem 10)) 8spec = hhD, R, I, Oii:SPEC. spec is satisfiable , 8x:D. I (x) ) 9y:R.O (x,y) This theorem is a variant of the Martin Lof's constructive axiom of choice [16, p. 50] and follows from the constructive nature of the underlying calculus.
Reference: [15] <author> Christoph Kreitz. METASYNTHESIS: </author> <title> Deriving Programs that Develop Programs. </title> <type> Thesis for Habilitation, </type> <institution> Technische Hochschule Darmstadt, FG Intellektik, </institution> <year> 1993. </year>
Reference-contexts: We have investigated translations between synthesis paradigms and represented synthesis strategies of the systems LOPS [2] and KIDS [27, 28, 29]. The results, which are presented in detail in the author's technical report <ref> [15] </ref>, show that a rigorous approach does in fact lead to safe and efficient program synthesis systems. <p> Figure 1 lists and explains the extensions used in this paper. Further ones (about 120) as well as the formal definitions and a few hundred lemmata stating their essential properties can be found in full detail in the author's technical report <ref> [15] </ref>. The resulting mathematical language serves both as specification and programming language. 2 Since in general equality decision procedures for members of type ff cannot be derived from ff we have introduced a new class TYPES of first level data types. <p> Formal proofs, because of their length, will be omitted. A reader interested in details may consult <ref> [15] </ref>. 4.1 GS-theories In order to reason about global search algorithms we have represented the concept of global search theories by a class GS of all objects G consisting of a specification spec = hhD,R,I,Oii and the additional components S, J, s 0 , sat, split, ext. <p> The first three axioms are consequences of elementary laws about finite sets and sequences (see <ref> [15, Appendices A.4/A.5] </ref>) and the fourth is shown by induction. Lemma 4.3 (GS-theory for finite sequences) 8ff:TYPES. gs seq over set (ff) is a GS theory 4.2 Filters The theory gs seq over set (ff) presented above is not well-founded since the split-operation is not bounded. <p> Since it would take more than 5 pages to present the formal proof we refer the reader interested in details to <ref> [15, Appendix C.5] </ref>. 4.3 Specializing a known global search theory Theorem 4.6 can be used for synthesizing a given specification if we can provide an appropriate GS-theory which extends it. <p> Our framework can not only be used for representing synthesis strategies based on algorithm schemata (although it can most easily be explained by these) but is also applicable to strategies based on transformations (see our formalization of LOPS in <ref> [15] </ref>) or on extracting programs from constructive proofs. Because of the rigorous mathematical approach our formal framework is more general than the existing approaches to program synthesis since these depend on the peculiarities of the synthesis paradigm (i.e. proofs-as-programs, transformations, or algorithm schemata) in which they are formulated.
Reference: [16] <author> Per Martin-Lof. </author> <title> Intuitionistic Type Theory, </title> <booktitle> Studies in Proof Theory Lecture Notes. </booktitle> <publisher> Bibliopolis, </publisher> <year> 1984. </year>
Reference-contexts: Many formal calculi are, at least in principle, powerful enough to express all of mathematics and programming (see e.g. <ref> [16, 4, 7, 5, 24] </ref>). But there remains a problem of expressiveness in practical applications. Inference steps in these logics operate on a very low level and make program derivations long and difficult to comprehend. <p> This allows one to `implement' the theory presented on paper without any modifications by using a proof system for the underlying calculus. As a basic calculus we have selected the formulation of intuitionistic type theory <ref> [16] </ref> used by the NuPRL proof development system [4]. Type theory already provides formalizations of a constructive higher order logic and low level constructs such as integers, strings, (recursive) function spaces, products, sums, lists, etc. as well as the corresponding type constructors. <p> Theorem 3.1 (Proofs-as-Programs (cf. [14], Theorem 10)) 8spec = hhD, R, I, Oii:SPEC. spec is satisfiable , 8x:D. I (x) ) 9y:R.O (x,y) This theorem is a variant of the Martin Lof's constructive axiom of choice <ref> [16, p. 50] </ref> and follows from the constructive nature of the underlying calculus. Within the context of software development systems, however, it has several additional aspects which are common to all formal meta-theorems representing deductive methods: 1. Its formulation gives a completely formal representation of a derivation method.
Reference: [17] <author> Zohar Manna and Richard J. Waldinger. </author> <title> Synthesis: Dreams ) programs. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-5(4):294-328, </volume> <year> 1979. </year>
Reference-contexts: Therefore, less rigorous methods are used when developing strategies. During the last decades many approaches (see e.g. <ref> [9, 17, 18, 2, 26] </ref>) have been implemented and tested successfully for a number of examples. The KIDS system [26, 29] is believed to be close to the point where it can be used to develop small routine programs.
Reference: [18] <author> Zohar Manna and Richard J. Waldinger. </author> <title> A deductive approach to program synthesis. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 2(1) </volume> <pages> 90-121, </pages> <year> 1980. </year>
Reference-contexts: Therefore, less rigorous methods are used when developing strategies. During the last decades many approaches (see e.g. <ref> [9, 17, 18, 2, 26] </ref>) have been implemented and tested successfully for a number of examples. The KIDS system [26, 29] is believed to be close to the point where it can be used to develop small routine programs.
Reference: [19] <author> G. Neugebauer, Bertram Fronhofer, and Christoph Kreitz. </author> <title> XPRTS an implementation tool for program synthesis. </title> <editor> In D. Metzing, editor, </editor> <booktitle> Proceedings of the 13 th German Workshop on Artificial Intelligence, Informatik Fachberichte 216, </booktitle> <pages> pages 348-357. </pages> <publisher> Springer Verlag, </publisher> <year> 1989. </year>
Reference-contexts: It is not clear how the systems reflect these foundations. Instead, program synthesizers have the same problems as conventional software: only specialists are able to handle them properly, unexpected errors occur, and after a while they become difficult to maintain and modify (cf. experiences reported in <ref> [19] </ref>). Most researchers are aware of these inadequacies but shy away from the amount of labour which is necessary to overcome them.
Reference: [20] <author> Lawrence C. Paulson. </author> <title> Logic and Computation: Interactive Proof with Cambridge LCF. </title> <publisher> Cam-bridge University Press, </publisher> <year> 1987. </year> <title> 94 Formal Mathematics for Verifiably Correct Program Synthesis </title>
Reference-contexts: So far the most fruitful approach to bridge the gap between formal deduction and complex applications has been that of tactics, first introduced in Edinburgh LCF [8], and since adopted in many other systems (see e.g. <ref> [20, 4, 21, 3, 11] </ref>). Here deductive methods are written as meta-programs guiding the application of inference rules. Tactics are a means to combine the advantages of formality with those of high-level methods.
Reference: [21] <author> Lawrence C. Paulson. </author> <title> The foundation of a generic theorem prover. </title> <journal> Journal of Automated Reasoning, </journal> <volume> 5 </volume> <pages> 363-397, </pages> <year> 1989. </year>
Reference-contexts: So far the most fruitful approach to bridge the gap between formal deduction and complex applications has been that of tactics, first introduced in Edinburgh LCF [8], and since adopted in many other systems (see e.g. <ref> [20, 4, 21, 3, 11] </ref>). Here deductive methods are written as meta-programs guiding the application of inference rules. Tactics are a means to combine the advantages of formality with those of high-level methods.
Reference: [22] <author> Lawrence C. Paulson. </author> <title> Isabelle: The next 700 theorem provers. </title> <editor> In Piergiorgio Odifreddi, editor, </editor> <booktitle> Logic and Computer Science, </booktitle> <pages> pages 361-386. </pages> <publisher> Academic Press, </publisher> <year> 1990. </year>
Reference-contexts: The concept of tactics aims at specializing general inference systems towards a particular application. Using meta-theorems and reflection as a general technique to control the application of tactics has already been investigated in [12, 13, 1]. In Isabelle <ref> [22] </ref> meta-theorems about reasoning within some object logic are used to generate derived inference rules for this logic.
Reference: [23] <author> Christine Paulin-Mohring. </author> <title> Extracting F ! 's programs from proofs in the Calculus of Constructions. </title> <booktitle> In Proc. of the 16 th ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 89-104, </pages> <year> 1989. </year>
Reference-contexts: This means that using formal meta-theorems within a synthesis process can also lead to a significant efficiency improvement of the generated code compared to algorithms extracted from `pure' synthesis proofs <ref> [25, 10, 23] </ref>. Theorems like theorem 4.12 contain complex algorithms which have been introduced and verified explicitly in their formal proofs in order to support a synthesis of efficient programs without having to execute long and complex derivations.
Reference: [24] <author> Robert Pollack. </author> <title> The theory of LEGO a proof checker for the extendend calculus of constructions. </title> <type> PhD thesis, </type> <institution> University of Edinburgh, </institution> <year> 1994. </year>
Reference-contexts: Many formal calculi are, at least in principle, powerful enough to express all of mathematics and programming (see e.g. <ref> [16, 4, 7, 5, 24] </ref>). But there remains a problem of expressiveness in practical applications. Inference steps in these logics operate on a very low level and make program derivations long and difficult to comprehend.
Reference: [25] <author> James T. Sasaki. </author> <title> The Extraction and Optimization of Programs from Constructive Proofs. </title> <type> PhD thesis, </type> <institution> Cornell University. Department of Computer Science, </institution> <address> Ithaca, NY, </address> <year> 1985. </year>
Reference-contexts: This means that using formal meta-theorems within a synthesis process can also lead to a significant efficiency improvement of the generated code compared to algorithms extracted from `pure' synthesis proofs <ref> [25, 10, 23] </ref>. Theorems like theorem 4.12 contain complex algorithms which have been introduced and verified explicitly in their formal proofs in order to support a synthesis of efficient programs without having to execute long and complex derivations. <p> This leads to a significant efficiency improvement of the derivation process to be performed. Secondly, the generated code does not depend on the quality of a general mechanism for extracting programs from constructive proofs <ref> [25] </ref> but is an instantiation of the program which has been explicitly provided and verified in the proof of the theorem.
Reference: [26] <author> Douglas R. Smith and Michael R. Lowry. </author> <title> Algorithm theories and design tactics. </title> <booktitle> Science of Computer Programming, </booktitle> <address> 14(2-3):305-321, </address> <month> October </month> <year> 1990. </year>
Reference-contexts: Therefore, less rigorous methods are used when developing strategies. During the last decades many approaches (see e.g. <ref> [9, 17, 18, 2, 26] </ref>) have been implemented and tested successfully for a number of examples. The KIDS system [26, 29] is believed to be close to the point where it can be used to develop small routine programs. <p> Therefore, less rigorous methods are used when developing strategies. During the last decades many approaches (see e.g. [9, 17, 18, 2, 26]) have been implemented and tested successfully for a number of examples. The KIDS system <ref> [26, 29] </ref> is believed to be close to the point where it can be used to develop small routine programs. But while the theoretical foundations of these strategies are thoroughly investigated their implementations are created `ad hoc' rather than systematically. It is not clear how the systems reflect these foundations. <p> A design strategy can then be based on such a theorem. The idea of developing synthesis strategies on the basis of parameterized theorems about schematic solutions to a given synthesis problem is not entirely new. Many algorithm design tactics successfully used in the KIDS system <ref> [27, 28, 26, 29, 30] </ref> rely on theorems of that kind.
Reference: [27] <author> Douglas R. Smith. </author> <title> Top-down synthesis of divide-and-conquer algorithms. </title> <journal> Artificial Intelligence, </journal> <volume> 27(1) </volume> <pages> 43-96, </pages> <month> September </month> <year> 1985. </year>
Reference-contexts: Since then we have refined our approach by elaborating a formal meta-theory of programming and begun an implementation with the NuPRL proof development system [4]. We have investigated translations between synthesis paradigms and represented synthesis strategies of the systems LOPS [2] and KIDS <ref> [27, 28, 29] </ref>. The results, which are presented in detail in the author's technical report [15], show that a rigorous approach does in fact lead to safe and efficient program synthesis systems. <p> A design strategy can then be based on such a theorem. The idea of developing synthesis strategies on the basis of parameterized theorems about schematic solutions to a given synthesis problem is not entirely new. Many algorithm design tactics successfully used in the KIDS system <ref> [27, 28, 26, 29, 30] </ref> rely on theorems of that kind.
Reference: [28] <author> Douglas R. Smith. </author> <title> Structure and design of global search algorithms. </title> <type> Technical Report KES.U.87.12, </type> <institution> Kestrel Institute, </institution> <month> November </month> <year> 1987. </year> <note> Revised Version, </note> <month> July </month> <year> 1988. </year>
Reference-contexts: Since then we have refined our approach by elaborating a formal meta-theory of programming and begun an implementation with the NuPRL proof development system [4]. We have investigated translations between synthesis paradigms and represented synthesis strategies of the systems LOPS [2] and KIDS <ref> [27, 28, 29] </ref>. The results, which are presented in detail in the author's technical report [15], show that a rigorous approach does in fact lead to safe and efficient program synthesis systems. <p> A design strategy can then be based on such a theorem. The idea of developing synthesis strategies on the basis of parameterized theorems about schematic solutions to a given synthesis problem is not entirely new. Many algorithm design tactics successfully used in the KIDS system <ref> [27, 28, 26, 29, 30] </ref> rely on theorems of that kind. <p> Sets of candidates are represented by descriptors and a satisfaction predicate determines when a candidate solution is in the set denoted by a descriptor. A careful analysis of the general structure of global search algorithms and the conditions under which such algorithms meet their specifications (see <ref> [28] </ref>) has shown that besides a specification spec = hhD, R, I, Oii the following components are essential: 1. <p> A global search theory is called well-founded if it has this property. In <ref> [28] </ref> it has been shown that well-founded global search theories satisfying the four axioms lead to provably correct global search algorithms of the above kind. <p> We have overcome this problem by a rigorous mathematical formalization of the knowledge contained in the derivation strategy. This formalization showed, that a few additions and refinements of the strategy described in <ref> [28] </ref> are necessary to make meta-theorems directly applicable for designing verified global search algorithms. The well-foundedness property, for instance, would put an extreme burden on the inference mechanism if it had to be proven at derivation time. <p> Obviously any filter is a wf-filter for a GS-theory G if G is already well-founded. Otherwise a necessary wf-filter can turn G into a well-founded GS-theory through exchanging split by split . Using this insight we can prove an improved and completely formal version of Smith's main theorem <ref> [28, theorem 1] </ref>: GS-theories (which may not be well-founded themselves) together with necessary well-foundedness filters contain all the information required to construct provably correct algorithms. Theorem 4.6 (Correct Global Search Programs) 1. 8G=hhhhD,R,I,Oii, S, J, s 0 , sat, split, extii:GS. 8:Filters (G). 8F aux :DfifiS6!Set (R). <p> Proving the second part is more difficult. The proof of <ref> [28, theorem 1] </ref> uses fixed point theory and cannot be formalized straightforwardly since there are no inference rules about fixed points. <p> This can be done by reducing the specification to the specification part of the GS-theory, deriving a substitution from this reduction, and specializing the GS-theory to one extending the specification. To perform these steps we introduce a few definitions which are straightforward formalizations of the corresponding concepts in <ref> [28] </ref>. Definition 4.7 (Specification Reduction) spec reduces to spec 0 let hhD,R,I,Oii=spec and hhD',R',I',O'ii=spec 0 in RR' ^ 8x:D. I (x) ) spec reduces to spec 0 with let hhD,R,I,Oii=spec and hhD',R',I',O'ii=spec 0 in RR' ^ 8x:D. <p> This step, which results in an additional necessary filter for G , does not affect well-foundedness and can be performed heuristically. All these insights lead to a single theorem which can easily be applied within a program derivation strategy. It improves the ones given in <ref> [28] </ref> since its requirements for generating verified global search algorithms are weaker. Theorem 4.11 (Bottom-Up Synthesis of Global Search Algorithms) 8spec = hhD, R, I, Oii:SPEC. 8G=hhhhD',R',I',O'ii, S, J, s 0 , sat, split, extii:GS. 8:D6!D'. 8:Filters (G). 8:Filters (G (spec)). <p> Furthermore it gives rise to a verified implementation of a simple synthesis strategy designing a global search algorithm for a given specification spec = hhD, R, I, Oii. This strategy refines and improves the one given in <ref> [28] </ref> since it uses theorem 4.12 as its key component. 1. State the synthesis goal as a theorem ` FUNCTION F (x:D):Set (R) WHERE I (x) RETURNS fz | O (x,z)g is satisfiable 2. Instantiate the parameters D, R, I, O and apply theorem 4.12 as a top-down inference rule.
Reference: [29] <author> Douglas R. Smith. </author> <title> KIDS | a knowledge-based software development system. </title> <editor> In Michael R. Lowry and Robert D. McCartney, editors, </editor> <booktitle> Automating Software Design, </booktitle> <pages> pages 483-514, </pages> <publisher> AAAI Press / The MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: Therefore, less rigorous methods are used when developing strategies. During the last decades many approaches (see e.g. [9, 17, 18, 2, 26]) have been implemented and tested successfully for a number of examples. The KIDS system <ref> [26, 29] </ref> is believed to be close to the point where it can be used to develop small routine programs. But while the theoretical foundations of these strategies are thoroughly investigated their implementations are created `ad hoc' rather than systematically. It is not clear how the systems reflect these foundations. <p> Since then we have refined our approach by elaborating a formal meta-theory of programming and begun an implementation with the NuPRL proof development system [4]. We have investigated translations between synthesis paradigms and represented synthesis strategies of the systems LOPS [2] and KIDS <ref> [27, 28, 29] </ref>. The results, which are presented in detail in the author's technical report [15], show that a rigorous approach does in fact lead to safe and efficient program synthesis systems. <p> A design strategy can then be based on such a theorem. The idea of developing synthesis strategies on the basis of parameterized theorems about schematic solutions to a given synthesis problem is not entirely new. Many algorithm design tactics successfully used in the KIDS system <ref> [27, 28, 26, 29, 30] </ref> rely on theorems of that kind. <p> The strategy requires only very few elementary global search theories to perform its task and is able to solve most of the programming problems approached in the field of program synthesis so far. It has even been demonstrated in <ref> [29] </ref> that the strategy is capable to solve a real programming problem whose solution had not been found long before. Because of a hand-written procedural encoding, however, the implemented strategy does not check well-foundedness and derives global search algorithms even if the theorems supporting it are not applicable anymore. <p> We shall illustrate this strategy by the same real programming problem which has been used in <ref> [29] </ref> to demonstrate the capabilities of the KIDS system. The derivation selects the non-well-founded GS-theory gs seq over set and can be justified only via the newly introduced concept of wf-filters.
Reference: [30] <author> Douglas R. Smith and Eduardo A. </author> <title> Parra. Transformational approach to transportation scheduling. </title> <booktitle> In Proceedings of the 8th Knowledge-Based Software Engineering Conference, </booktitle> <pages> pages 60-68, </pages> <year> 1993. </year> <note> Received 1 October 1994. Revised 7 June 1995 </note>
Reference-contexts: A design strategy can then be based on such a theorem. The idea of developing synthesis strategies on the basis of parameterized theorems about schematic solutions to a given synthesis problem is not entirely new. Many algorithm design tactics successfully used in the KIDS system <ref> [27, 28, 26, 29, 30] </ref> rely on theorems of that kind.
References-found: 30

