URL: ftp://ftp.ai.mit.edu/pub/users/gideon/papers/AITR-1426.ps.Z
Refering-URL: http://www.ai.mit.edu/people/gideon/gideon.html
Root-URL: 
Title: Internal Camera Calibration using Rotation and Geometric Shapes  
Author: by Gideon P. Stein 
Degree: Submitted to the Department of Electrical Engineering and Computer Science in partial fulfillment of the requirements for the degree of MASTER OF SCIENCE in Electrical Engineering and Computer Science at the  All rights reserved The author hereby grants to MIT permission to reproduce and to distribute copies of this thesis document in whole or in  
Note: c flGideon P. Stein,  part.  
Date: February 1993  1993.  
Affiliation: MASSACHUSETTS INSTITUTE OF TECHNOLOGY  
Abstract: This paper describes a simple method for internal camera calibration for computer vision systems. It is intended for use with medium to wide angle camera lenses. With modification it can be used for longer focal lengths. This method is based on tracking image features through a sequence of images while the camera undergoes pure rotation. This method does not require a special calibration object. The location of the features relative to the camera or to each other need not be known. It is only required that the features can be located accurately in the image. This method can therefore be used both for laboratory calibration and for self calibration in autonomous robots working in unstructured environments. The method works when features can be located to single pixel accuracy, but subpixel accuracy should be used if available. In the basic method the camera is mounted on a rotary stage so that the angle of rotation can be measured accurately and the axis of rotation is constant. A set of image pairs is used with various angular displacements. If the internal camera parameters and axis of rotation were known one could predict where the feature points from one image will appear in the second image of the pair. If there is an error in the internal camera parameters the features in the second image will not coincide with the feature locations computed using the first image. One can then perform a 
Abstract-found: 1
Intro-found: 1
Reference: [1] <editor> Beardley, P., et al, </editor> <booktitle> "Camera Calibration Using Multiple Images" In Proceedings of the Second European Conference on Computer Vision, </booktitle> <pages> 312-320, </pages> <address> Santa Margherita Ligure, Italy, </address> <month> May </month> <year> (1992) </year>
Reference-contexts: In [3] the vanishing points of parallel lines drawn on the faces of a cube are used to locate the focal length and principal point. In <ref> [1] </ref> planar sets of parallel lines are rotated around an axis not perpendicular to the plane. <p> The focal length varied over a range of 0:5% and 0:3% for 8mm and 16mm lenses respectively. The calibration object used was a cube with parallel lines drawn on it's faces. Beardsley <ref> [1] </ref> shows that if a plane is rotated around an axis which is not perpendicular to the plane, then the vanishing point produced by a set of parallel lines on the plane, draws a conic section in the image plane.
Reference: [2] <author> Brown, </author> <note> D.C.,"Close -range Camera Calibration" Photogrammetric Engineering 37 855-866 (1971) </note>
Reference-contexts: These methods do not require knowing or finding the position of the object relative to the camera. The plumb line method <ref> [2] </ref>, which uses the images of straight lines for calibrating lens distortion, a method for finding the aspect ratio using the image of a sphere [15], and a new method which uses spheres to locate the principal point and estimate the focal length are described in detail in section 3 in <p> The advantage of this method is that neither the position nor orientation of the lines relative to the camera nor the internal camera parameters such as focal length or principal point need to be known. Brown <ref> [2] </ref> fully presents the mathematics of this method and uses plumb lines for the calibration of a high accuracy metric camera for both radial and decentering lens distortion. The camera used has a wide image format. <p> The camera used has a wide image format. The lens had a focal length of about 135 mm and viewing angle of about 60 o . The results in <ref> [2] </ref> show that the lens distortion parameters change both with the distance for which the lens is focused and with the actual object distance. <p> The equations for determining the radial distortion ffir s for a lens focused at distances can be computed from the radial distortion measured at two other focus distances, s 1 and s 2 using the equations derived in <ref> [2] </ref>. ffir s = ff s ffir s 1 + (1 ff s )ffir s 2 (29) where ff s = s 2 s 1 s f for the lens used in [2]. <p> radial distortion measured at two other focus distances, s 1 and s 2 using the equations derived in <ref> [2] </ref>. ffir s = ff s ffir s 1 + (1 ff s )ffir s 2 (29) where ff s = s 2 s 1 s f for the lens used in [2]. One can see that for distances less than 100 times the focal length the change might be significant. In our experiments the objects were no less than 50 focal lengths from the camera and so the radial distortion parameters were close to those at infinity. <p> We chose to use equation (8) which does 18 the distortion at s = 1 (ffir s 1 ) and for s in units of focal length. Theoretical curve fitted to data from <ref> [2] </ref>.(*) denotes actual data . not have a decentering term but accounts for the decentering distortion by using the point of best symmetry rather than the principal point as the center of the radial distortion. <p> Also shown in figure 4 is the correction using a single radial distortion parameter. The correction values are over 2 pixels different. The change in radial distortion as a function of object distance has an opposite sign from the results described in <ref> [2] </ref> and from the results shown in figure 2. Note that the results in figure 2 show the change in radial distortion due to a change in object distance and focus distance but other results in [2] show the same trend when focus distance is kept constant. <p> a function of object distance has an opposite sign from the results described in <ref> [2] </ref> and from the results shown in figure 2. Note that the results in figure 2 show the change in radial distortion due to a change in object distance and focus distance but other results in [2] show the same trend when focus distance is kept constant. We have no explanation for the difference between our results and those in [2]. <p> that the results in figure 2 show the change in radial distortion due to a change in object distance and focus distance but other results in <ref> [2] </ref> show the same trend when focus distance is kept constant. We have no explanation for the difference between our results and those in [2].
Reference: [3] <author> Caprile, B. and Torre, V., </author> <title> "Using Vanishing Points for Camera Calibration" International Journal of Computer Vision4, </title> <month> 127-140 </month> <year> (1990) </year>
Reference-contexts: In <ref> [3] </ref> the vanishing points of parallel lines drawn on the faces of a cube are used to locate the focal length and principal point. In [1] planar sets of parallel lines are rotated around an axis not perpendicular to the plane. <p> A.2 Methods that use geometrical properties of objects There are a variety of methods that use the vanishing points of parallel lines to determine the external and internal camera parameters. As shown in [9, pages 54-58] <ref> [3] </ref> the perspective projection of a set of parallel lines that are not parallel to the image plane is a set of lines that appear to meet at one point on the image plane. This point is called the vanishing point. <p> This point is called the vanishing point. The classic example of this phenomenon is the parallel tracks of a railway line which appear to meet at some point far off in the distance. The reader is pointed to [9] <ref> [3] </ref> for a formal discussion of the subject but a few properties of vanishing points are noted [3]: 1. The vanishing points associated with the sets of lines that are all parallel to a given plane, all lie on the same line in the image. <p> The classic example of this phenomenon is the parallel tracks of a railway line which appear to meet at some point far off in the distance. The reader is pointed to [9] <ref> [3] </ref> for a formal discussion of the subject but a few properties of vanishing points are noted [3]: 1. The vanishing points associated with the sets of lines that are all parallel to a given plane, all lie on the same line in the image. This is called the vanishing line. 2. <p> If the camera moves, the motion of the vanishing points in the image plane depends only on the camera rotation not the camera translation. The vanishing 44 points of three non coplanar sets of parallel lines fully determine the rotation matrix. Caprile and Torre <ref> [3] </ref> use these facts to determine the principal point and focal length of the camera and also to determine the camera rotation in an image sequence. The image aspect ratio must be determined by some other means. No effort was made to deal with radial distortion.
Reference: [4] <editor> Faugeras,O.D., et al., </editor> <booktitle> "Camera Self-Calibration : Theory and Experiments" In Proceedings of the Second European Conference on Computer Vision, </booktitle> <pages> 321-334, </pages> <address> Santa Margherita Ligure, Italy, </address> <month> May </month> <year> (1992) </year>
Reference-contexts: It is assumed that the camera has no radial distortion and that all the other camera parameters are known. No information is given as to the accuracy of the results. Faugeras et al. <ref> [4] </ref> develop a method where a motion sequence of a single camera moving in an unconstrained manner can be used to calculate the internal camera parameters. A camera model with no lens distortion is used. <p> It is also shown [10] that no more internal camera parameters can be deduced from the set of two images. Faugeras et al. <ref> [4] </ref> show that given a motion sequence of 4 images where the motion is unknown and unconstrained it is possible in theory to recover the internal camera parameters assuming a perfect pinhole model. The method currently suffers very badly from noise in feature location.
Reference: [5] <author> Faugeras,O.D., </author> <title> "What can be seen in three dimensions with an uncalibrated stereo rig" In Proceedings of the Second European Conference on Computer Vision, </title> <address> 563-578, Santa Margherita Ligure, Italy, </address> <month> May </month> <year> (1992) </year>
Reference-contexts: Camera calibration is important if we wish to derive metric information from the images, although qualitative information can obtained from uncalibrated cameras and a stereo camera pair. Work has been done to see how much knowledge can be obtained using uncalibrated cameras <ref> [5] </ref> [13], but the mainstream efforts in robot vision assume some means of calibration. In some cases we do not need to find the camera parameters explicitly. In other words, the transformation is defined in terms of intermediate parameters which are combinations of the camera parameters.
Reference: [6] <author> Fryer, J.G. and Mason, </author> <title> S.O.,"Rapid Lens Calibration of a Video Camera"Photogrammetric Engineering and Remote Sensing 55 437-442, </title> <year> (1989) </year>
Reference-contexts: One can see that for distances less than 100 times the focal length the change might be significant. In our experiments the objects were no less than 50 focal lengths from the camera and so the radial distortion parameters were close to those at infinity. Fryer <ref> [6] </ref> uses the plumb line method for the calibration of video cameras. Very sharp lines on a printed writing pad are used as the calibration pattern. As the calibration pattern we used black bars against a white background. <p> A straight line was fitted to each of the edges and the RMS distance of the points from line was computed. The best parameters are those parameters that reduce the RMS error to a minimum and could be found using non-linear optimization. Fryer <ref> [6] </ref> used the decentering terms in (6) and it was therefore necessary to choose a point on the image as the principal point. We chose to use equation (8) which does 18 the distortion at s = 1 (ffir s 1 ) and for s in units of focal length.
Reference: [7] <author> Ganapathy,S., </author> <title> "Decomposition of transformation matrices for robot vision" Pattern Recognition Letters2, </title> <month> 401-412 </month> <year> (1984) </year>
Reference-contexts: It is possible to calibrate stereo systems in this way. In the case where there is no lens distortion, we could find the transformation between the 3D world coordinates and image locations in the image planes of the two cameras using homogeneous matrices [18] <ref> [7] </ref>. We might never bother to actually break down the transformation matrices into internal and external parameters. We could make accurate measurements of the location of one object relative to another object in the scene. <p> Sutherland [19] shows how to obtain the matrix M in a least squares way using the known world coordinates and their corresponding image coordinates. The more difficult problem is obtaining the individual camera parameters from the matrix M. This involves solving nonlinear equations. Ganapathy <ref> [7] </ref> shows a non-iterative method for their solution and a more geometrically intuitive method is described in [18]. 43 Lenz and Tsai [21][11] and Weng et al.[22] provide methods for determining the camera parameters in the presence of lens distortion.
Reference: [8] <author> Gennery, </author> <title> D.B., </title> <booktitle> "Stereo-camera Calibration" in Proc. Image Understanding Workshop, </booktitle> <pages> 101-108, </pages> <month> Nov. </month> <year> (1979) </year>
Reference-contexts: It would be a great advantage to be able to calibrate a camera using only feature coordinates in the image plane. This cannot be done with a single image. Instead it requires camera motion. Gennery <ref> [8] </ref> proposes using nonlinear optimization to calibrate a stereo system where the unknown parameters are the focal lengths and the relative position of the 9 two cameras. It is assumed that the camera has no radial distortion and that all the other camera parameters are known.
Reference: [9] <author> Haralick, R.M. and Shapiro, </author> <title> L.G., </title> <publisher> Computer and Robot Vision Addison-Wesley Publishing Company, </publisher> <year> (1993) </year>
Reference-contexts: The results in [22] show that in simulation the errors in focal length are above 0:5%. A.2 Methods that use geometrical properties of objects There are a variety of methods that use the vanishing points of parallel lines to determine the external and internal camera parameters. As shown in <ref> [9, pages 54-58] </ref> [3] the perspective projection of a set of parallel lines that are not parallel to the image plane is a set of lines that appear to meet at one point on the image plane. This point is called the vanishing point. <p> This point is called the vanishing point. The classic example of this phenomenon is the parallel tracks of a railway line which appear to meet at some point far off in the distance. The reader is pointed to <ref> [9] </ref> [3] for a formal discussion of the subject but a few properties of vanishing points are noted [3]: 1. The vanishing points associated with the sets of lines that are all parallel to a given plane, all lie on the same line in the image. <p> If the internal camera parameters are known we can find the relative position of two cameras in a stereo pair up to a scale factor in the translation. Following <ref> [9, pages 144-147] </ref> given the feature locations of a set of image points in the two cameras (x 1i ; y 1i ) and (x 2i ; y 2i ) the following system of equations must be solved: 0 @ Y 1 A 6 0 @ y 1i 1 A fi
Reference: [10] <author> Hartley, </author> <title> R.I., </title> <booktitle> "Estimation of Relative Camera Camera Positions for Uncalibrated Cameras" In Proceedings of the Second European Conference on Computer Vision, </booktitle> <pages> 563-578, </pages> <address> Santa Margherita Ligure, Italy, </address> <month> May </month> <year> (1992) </year>
Reference-contexts: Equation (46) can be solved iteratively. Longuet-Higgens [12] provides a non-iterative solution to this problem. If the focal lengths of the cameras are unknown but all the other internal camera parameters are known then Hartley <ref> [10] </ref> shows that the focal lengths and the relative position of the cameras can be found using feature point locations alone. It is also shown [10] that no more internal camera parameters can be deduced from the set of two images. <p> If the focal lengths of the cameras are unknown but all the other internal camera parameters are known then Hartley <ref> [10] </ref> shows that the focal lengths and the relative position of the cameras can be found using feature point locations alone. It is also shown [10] that no more internal camera parameters can be deduced from the set of two images.
Reference: [11] <author> Lenz, R.K. and Tsai, R.Y., </author> <title> "Techniques for Calibration of the Scale Factor and Image Center for High accuracy 3-D Machine Vision Metrology" IEEE Trans. </title> <journal> Pattern Anal. Machine Intell. </journal> <month> 10,713-720 </month> <year> (1988) </year>
Reference-contexts: These are sometimes called control points. In laboratories, control points can be obtained using a calibration object <ref> [11] </ref> [22]. Outdoors, control points could be markings on the ground [17] or buildings whose positions can be verified from maps [18]. If the aspect ratio is unknown then a three dimensional calibration object is required [11] (i.e. the points cannot be coplanar). <p> In laboratories, control points can be obtained using a calibration object <ref> [11] </ref> [22]. Outdoors, control points could be markings on the ground [17] or buildings whose positions can be verified from maps [18]. If the aspect ratio is unknown then a three dimensional calibration object is required [11] (i.e. the points cannot be coplanar). As a 3D object one can use a planar object with feature points clearly marked, which can be moved accurately in the Z direction, perpendicular to the plane of the points. <p> The sample clock frequency is given by the manufacturer of the frame grabber system but because of the use of phase locked loops this frequency can vary <ref> [11] </ref>. 2.2 Camera model with lens distortion Off the shelf cameras and lenses which are often used for computer vision have large amounts of lens distortion. This is particularly noticeable in the case of wide angle lenses.
Reference: [12] <author> Longuet-Higgins, H.C., </author> <title> "A computer algorithm for reconstructing a scene from two projections" Nature 293,133-135 (1981) </title>
Reference-contexts: Equation (46) can be solved iteratively. Longuet-Higgens <ref> [12] </ref> provides a non-iterative solution to this problem. If the focal lengths of the cameras are unknown but all the other internal camera parameters are known then Hartley [10] shows that the focal lengths and the relative position of the cameras can be found using feature point locations alone.
Reference: [13] <author> Mohr, R. and Arbogast, E., </author> <title> "It can be done without camera calibration" Pattern Recognition Letters 12,39-43 (1991) </title>
Reference-contexts: Camera calibration is important if we wish to derive metric information from the images, although qualitative information can obtained from uncalibrated cameras and a stereo camera pair. Work has been done to see how much knowledge can be obtained using uncalibrated cameras [5] <ref> [13] </ref>, but the mainstream efforts in robot vision assume some means of calibration. In some cases we do not need to find the camera parameters explicitly. In other words, the transformation is defined in terms of intermediate parameters which are combinations of the camera parameters.
Reference: [14] <institution> More, J.J., et al., "User Guide for Minpack-1" Argonne National Laboratory, Argonne, </institution> <address> Illinois (1980) 46 </address>
Reference-contexts: An improvement on this method would be to use a higher order curve for approximating the edge rather than a straight line. 5.4 Nonlinear optimization code The camera parameters were found using a nonlinear optimization program based on the subroutine LMDIF from the software package MINPACK-1 <ref> [14] </ref>. This subroutine uses the array of residuals (e i ) to adjust the parameters to minimize the sum square error P i . This is a more powerful method than only using the sum itself. It calculates derivatives using forward differencing. 33 Sanyo 8:5mm camera.
Reference: [15] <author> Penna, M.A. </author> <title> "Camera Calibration: A quick and Easy Way to Determine the Scale Factor" IEEE Trans. </title> <journal> Pattern Anal. Machine Intell. </journal> <month> 13,1240-1245 </month> <year> (1991) </year>
Reference-contexts: These methods do not require knowing or finding the position of the object relative to the camera. The plumb line method [2], which uses the images of straight lines for calibrating lens distortion, a method for finding the aspect ratio using the image of a sphere <ref> [15] </ref>, and a new method which uses spheres to locate the principal point and estimate the focal length are described in detail in section 3 in this paper. <p> For an object 100m away, the translation and the corresponding error will be smaller by a factor of 100. 15 3 Calibration using Geometric ob jects 3.1 Finding the aspect ratio using spheres An interesting method for finding the aspect ratio was developed in <ref> [15] </ref>. We will make slight changes in the derivation mostly in order to fit in with notation used in this paper.
Reference: [16] <editor> Press, W.H. et al., </editor> <publisher> "Numerical Recipes in C" Cambridge University Press, </publisher> <year> (1988) </year>
Reference-contexts: Since each of the angles, and hence the sum of the angles, is a monotonic function of our guess of f it is simple to find the correct f iteratively using simple numerical methods such as bisection or Brent's method <ref> [16] </ref>. the focal length. The camera parameters do not stay constant as we vary our guess of f .
Reference: [17] <editor> Slama, C.C. ed, </editor> <title> Manual of Photogrammetry,4th edition, </title> <journal> American Society of Photogrammetry (1980). </journal>
Reference-contexts: These are sometimes called control points. In laboratories, control points can be obtained using a calibration object [11] [22]. Outdoors, control points could be markings on the ground <ref> [17] </ref> or buildings whose positions can be verified from maps [18]. If the aspect ratio is unknown then a three dimensional calibration object is required [11] (i.e. the points cannot be coplanar). <p> The problem also manifests itself when one tries to use subsets of the camera parameters for other tasks. The individual parameters can be found more accurately by using calibration points from a large volume of space and with a large variation in depth. One method <ref> [17] </ref> uses multiple views of the calibration object from different camera positions with camera internal parameters kept constant. <p> This is particularly noticeable in the case of wide angle lenses. The standard model for this distortion <ref> [17] </ref> takes into account both radial distortion and decentering distortion. This model gives the correction for lens distortion. In other words, it is a mapping from the distorted image coordinates, that are observable, to the undistorted image plane coordinates which are not physically measurable. <p> The calibration techniques used in machine vision are usually referred to as Analytical Methods in the photogrammetric literature <ref> [17] </ref> and are used for 'on the job' calibration, as opposed to the calibration methods used in camera and lens manufacture. The standard approach solves the nonlinear projection equations by iterative techniques. <p> This is especially a good idea if one wishes to determine the internal parameters accurately. This can be performed using Simultaneous Multi-Frame Analytical Calibration <ref> [17] </ref>. Initially, the machine vision literature did not take into account lens distortion. The imaging process can then be described using homogeneous matrices [18].
Reference: [18] <author> Strat,T.M., </author> <title> "Recovering the Camera Parameters from a Transformation Matrix" in Proc. </title> <booktitle> DARPA Image Understanding Workshop, </booktitle> <pages> 264-271, </pages> <month> Oct. </month> <year> (1984) </year>
Reference-contexts: It is possible to calibrate stereo systems in this way. In the case where there is no lens distortion, we could find the transformation between the 3D world coordinates and image locations in the image planes of the two cameras using homogeneous matrices <ref> [18] </ref> [7]. We might never bother to actually break down the transformation matrices into internal and external parameters. We could make accurate measurements of the location of one object relative to another object in the scene. <p> If on the other hand we wish to find the location of the camera relative to the objects in the scene we would be required to find the external calibration parameters explicitly. This is a more difficult problem <ref> [18] </ref>. Work on motion vision and pose estimation often use the perspective projection model and assume that the internal camera parameters are known. They also assume that the parameters that can correct for lens distortion are known. <p> These are sometimes called control points. In laboratories, control points can be obtained using a calibration object [11] [22]. Outdoors, control points could be markings on the ground [17] or buildings whose positions can be verified from maps <ref> [18] </ref>. If the aspect ratio is unknown then a three dimensional calibration object is required [11] (i.e. the points cannot be coplanar). <p> This is especially a good idea if one wishes to determine the internal parameters accurately. This can be performed using Simultaneous Multi-Frame Analytical Calibration [17]. Initially, the machine vision literature did not take into account lens distortion. The imaging process can then be described using homogeneous matrices <ref> [18] </ref>. <p> Strat <ref> [18] </ref> shows that the how to compute matrix M knowing the internal and external camera parameters. Sutherland [19] shows how to obtain the matrix M in a least squares way using the known world coordinates and their corresponding image coordinates. <p> The more difficult problem is obtaining the individual camera parameters from the matrix M. This involves solving nonlinear equations. Ganapathy [7] shows a non-iterative method for their solution and a more geometrically intuitive method is described in <ref> [18] </ref>. 43 Lenz and Tsai [21][11] and Weng et al.[22] provide methods for determining the camera parameters in the presence of lens distortion. Lenz and Tsai [21] assume only one parameter of radial distortion and no decentering distortion.
Reference: [19] <author> Sutherland, </author> <title> I.E., </title> <booktitle> "Three-Dimensional Data Input by Tablet" in Proceedings of the IEEE, </booktitle> <month> 62,453-458 </month> <year> (1974) </year>
Reference-contexts: Strat [18] shows that the how to compute matrix M knowing the internal and external camera parameters. Sutherland <ref> [19] </ref> shows how to obtain the matrix M in a least squares way using the known world coordinates and their corresponding image coordinates. The more difficult problem is obtaining the individual camera parameters from the matrix M. This involves solving nonlinear equations.
Reference: [20] <author> Thomas, </author> <title> G.B., "Calculus and Analytic Geometry" 4th Edition, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> (1969) </year>
Reference-contexts: The eccentricity of the ellipse is a function of the focal length, the distance of the center of the ellipse from the principal point and of the length of the major axis <ref> [20] </ref>. 21 Legend: |- Ruler at 1m Ruler at 0:5m . . Ruler at 0:2m ..... Laser print at 0:1m from the principal point for the Sanyo 8:5mm lens. Each line uses the correction parameters obtained from straight objects placed on planes at different distances from the image plane.
Reference: [21] <author> Tsai, R.Y. </author> <title> "A Versatile Camera Calibration Technique for High-Accuracy 3D Machine Vision Metrology Using Off-the-Shelf TV Cameras and Lenses" IEEE Journal of Robotics and AutomationRA-3 323-344 (1987) </title>
Reference-contexts: This involves solving nonlinear equations. Ganapathy [7] shows a non-iterative method for their solution and a more geometrically intuitive method is described in [18]. 43 Lenz and Tsai <ref> [21] </ref>[11] and Weng et al.[22] provide methods for determining the camera parameters in the presence of lens distortion. Lenz and Tsai [21] assume only one parameter of radial distortion and no decentering distortion. First they assume the principal point is at the center of the image buffer and find the external parameters and focal length using the radial alignment constraint [21] and then find the radial distortion parameter, K 1 . <p> Lenz and Tsai <ref> [21] </ref> assume only one parameter of radial distortion and no decentering distortion. First they assume the principal point is at the center of the image buffer and find the external parameters and focal length using the radial alignment constraint [21] and then find the radial distortion parameter, K 1 . Then, using these parameters, they compute an error term by projecting the known world coordinates onto the image plane.
Reference: [22] <author> Weng, J et al. </author> <title> "Camera Calibration with Distortion Models and Accuracy Evaluation" IEEE Trans. </title> <journal> Pattern Anal. Machine Intell. </journal> <note> 14,965-980 (1992) 47 </note>
Reference-contexts: These are sometimes called control points. In laboratories, control points can be obtained using a calibration object [11] <ref> [22] </ref>. Outdoors, control points could be markings on the ground [17] or buildings whose positions can be verified from maps [18]. If the aspect ratio is unknown then a three dimensional calibration object is required [11] (i.e. the points cannot be coplanar). <p> Despite this problem the calibration can still be used to accurately measure the position of an object in the workspace from it's image position in a stereo image pair. In <ref> [22] </ref>, a simulation of camera calibration with noisy data resulted in an error greater or equal to 0:5% in both the focal length and the external parameters. This happened even with a simulation of a simple pinhole camera model with no lens distortion. <p> This procedure is repeated till convergence. Both the techniques of Lenz and Tsai [21][11] and the techniques of Weng et al.<ref> [22] </ref> result in camera parameters which enable very accurate 3D measurements. Lenz and Tsai [21][11] do not evaluate the accuracy of the individual camera parameters. The results in [22] show that in simulation the errors in focal length are above 0:5%. A.2 Methods that use geometrical properties of objects There are a variety of methods that use the vanishing points of parallel lines to determine the external and internal camera parameters.
References-found: 22

