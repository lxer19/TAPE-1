URL: ftp://ftp.cs.utexas.edu/pub/code2/DistrExecEnvironments.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/code/code-publications/
Root-URL: 
Title: DISTRIBUTED EXECUTION ENVIRONMENTS FOR THE CODE 2.0 PARALLEL PROGRAMMING SYSTEM  
Degree: APPROVED BY SUPERVISING COMMITTEE:  
Abstract-found: 0
Intro-found: 0
Reference: [Bab92] . <author> Babaoglu, </author> <title> Paralex: An Environment for Parallel Programming in Distributed Systems, </title> <booktitle> Proc. ACM Int. Conf. on Supercomputing, </booktitle> <month> July, </month> <year> 1992. </year>
Reference-contexts: This is because Linda [S-ci92] basically is a shared memory model. The runtime execution model remains the same over all the architectures on which it runs. 6.3 Others There are a variety of other systems for parallel programming. They include Paralex <ref> [Bab92] </ref>, PPSE [Lew90] and P4 [But92]. They are discussed in detail in the literature. 70 Chapter 7. Conclusion and Future Work In this thesis, the design and implementation of a distributed execution environment in PVM for the CODE 2.0 graphical parallel programming system has been presented.
Reference: [Bar86] <author> J.Barnes and P. Hut, </author> <title> A Hierarchical O(N log N) Force-Calculation Algorithm, </title> <journal> Nature, </journal> <volume> vol. 324, </volume> <editor> p. </editor> <volume> 446, </volume> <year> 1986. </year>
Reference-contexts: These include the Block Triangular Solver [Don86],the Life [Gar70] program and the Barnes Hut <ref> [Bar86, Cha92] </ref> algorithm.These programs use different patterns of communication. <p> As can be observed from the above table, not only do CODE and hand-coded programs execute in comparable timings, they also show very similar patterns of speedup. Again, note that the granularity of the computations decreases as the number of processors increase. 5.4.3 The Barnes-Hut Algorithm The Barnes-Hut algorithm <ref> [Bar86, Cha92] </ref> computes the gravitational interactions among N particles and also computes their positions over time. Its order of execution is O (N log N).
Reference: [Beg91] <author> A. Beguelin, et al., </author> <title> Graphical Development Tools for Network Based Concurrent Supercomputing, </title> <booktitle> Proc. </booktitle> <address> Supercomuting 91, Al-buquerque, NM, </address> <pages> pp. 435-444, </pages> <year> 1991. </year>
Reference-contexts: Chapter 5 details the results of an extensive set of experiments. 1.5 Related Work There have been many attempts at providing similar high-level abstractions for parallel programming. PVM itself is an attempt at presenting a homogeneous message-passing interface to a heterogeneous network of computers. HeNCE <ref> [Beg91] </ref> is a graphical parallel programming tool which can be used to create parallel programs on a set of networked machines. Linda [Sci92] is another parallel programming language which tries to augment sequential programming through the use of primitives which are architecture-independent. <p> Related Work There have been many attempts to abstract parallel and distributed computing. Some have used graphical approaches while others have used the idea of enhancing existing languages by providing new primitives. In this chapter an example of each will be examined, HeNCE <ref> [Beg91] </ref> and Linda [S-ci92] respectively. Newton [New93a] gives a through survey of related work. Since, this recent survey is available, we give only a brief sketch of the two systems. 6.1 HeNCE HeNCE is a graphical parallel programming environment.
Reference: [Bro94] <author> James C. Browne, Jack Dongarra, Syed I. Hyder, Keith Moore, and Peter Newton, </author> <title> Visual Programming and Parallel Computing, </title> <institution> University of Tennessee Tech Report CS-94-229, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: It is very similar to CODE in terms of goal, function, philosophy as well as the look and feel. The two have similarities and dissimilarities, which are outlined below. Much of what is outlined below is from <ref> [Bro94] </ref>. Both CODE and HeNCE support graphical/textual interfaces. Both allow calls to external sequential routines, enforce type checking and perform automatic storage management. CODE provides a complete and very expressive set of firing rules.
Reference: [But92] <author> R. Butler and E. Lusk, </author> <title> Users Guide to the P4 Programming System, </title> <type> Tech. Report ANL-92/17, </type> <institution> Argonne National Laboratory, </institution> <year> 1992. </year>
Reference-contexts: This is because Linda [S-ci92] basically is a shared memory model. The runtime execution model remains the same over all the architectures on which it runs. 6.3 Others There are a variety of other systems for parallel programming. They include Paralex [Bab92], PPSE [Lew90] and P4 <ref> [But92] </ref>. They are discussed in detail in the literature. 70 Chapter 7. Conclusion and Future Work In this thesis, the design and implementation of a distributed execution environment in PVM for the CODE 2.0 graphical parallel programming system has been presented.
Reference: [Don86] <author> J.J. Dongarra and D.C. Sorenson, </author> <title> SCHEDULE: Tools for Developing and Analyzing Parallel Fortran Programs, </title> <note> Argonne National Laboratory MCSD Technical Memorandum No. 86, </note> <month> Nov. </month> <year> 1986. </year>
Reference-contexts: It uses a parallel algorithm developed by Jack Dongarra and Danny Sorenson <ref> [Don86] </ref>. Values obtained by running CODE generated programs as well as hand-coded programs on matrices of size 1200x1200 are as follows. In these experiments the number of blocks is set to the number of processors, so that the granularity of computations decreases as the number of processors increase.
Reference: [Cha92] <author> K.M. Chandy and S. Taylor, </author> <title> An Introduction to Parallel Programming, </title> <journal> pp. </journal> <pages> 145-157, </pages> <publisher> Jones and Bartlett, </publisher> <address> Boston, </address> <year> 1992. </year>
Reference-contexts: These include the Block Triangular Solver [Don86],the Life [Gar70] program and the Barnes Hut <ref> [Bar86, Cha92] </ref> algorithm.These programs use different patterns of communication. <p> As can be observed from the above table, not only do CODE and hand-coded programs execute in comparable timings, they also show very similar patterns of speedup. Again, note that the granularity of the computations decreases as the number of processors increase. 5.4.3 The Barnes-Hut Algorithm The Barnes-Hut algorithm <ref> [Bar86, Cha92] </ref> computes the gravitational interactions among N particles and also computes their positions over time. Its order of execution is O (N log N).
Reference: [Gar70] <author> M. Gardner, </author> <title> Mathematical Recreations, </title> <journal> Scientific American, </journal> <volume> vol. 223, no. 4, </volume> <pages> pp. 120-123, </pages> <year> 1970. </year>
Reference-contexts: The focus is on the relative performance of CODE generated versus hand-coded PVM programs rather than absolute performance measures. 5.2 Set Up A set of standard programs were chosen to serve as benchmarks for our distributed execution environment. These include the Block Triangular Solver [Don86],the Life <ref> [Gar70] </ref> program and the Barnes Hut [Bar86, Cha92] algorithm.These programs use different patterns of communication. <p> This is due to good placement of units of computation which ensures that some of the data do not have to be explicitly transmitted between processors. 5.4.2 The Life program The life program is based on the Game of Life <ref> [Gar70] </ref>. The Game of Life consists of a rectangular grid of cells, each surrounded by eight neighbors. Each cell can be on (alive) or off (dead) during each iteration (generation) of the program.
Reference: [Gei94] <author> A. Geist, et al., </author> <title> PVM 3 Users Guide an d Reference Manual, </title> <institution> Oak Ridge National Laboratory, Tennessee, </institution> <year> 1994. </year> <month> 73 </month>
Reference-contexts: This thesis extends these results by demonstrating that programs with efficiency comparable to hand-coded ones can be obtained for CODE 2.0 programs for a popular distributed memory programming environment, the Parallel Virtual Machine (PVM) <ref> [Gei94] </ref>. This compiler from CODE 2.0 to PVM was created by the individual efforts of the author of this thesis in a few months of work. 2 1.1 CODE 2.0 CODE 2.0 (CODE) is a graphical retargetable parallel programming system. It facilitates a compositional approach to programming. <p> That concludes the extract from the CODE 2.0 User manual. 2.2 PVM This section will provide a brief overview of the Parallel Virtual Machine (PVM) as relevant to this thesis. It contains material taken from the PVM 3 Users Guide and Reference Manual <ref> [Gei94] </ref>. PVM 3 is a software system that permits a network of heterogeneous UNIX computers to be used as a single large parallel computer. In other words, a user defined collection of serial, parallel and vector computers appear as one large distributed-memory computer. <p> pvm_recv ()- blocking receive pvm_send () - sends the data in the active message buffer pvm_spawn () - starts new PVM processes pvm_upk* () - unpack the active message buffer with arrays of prescribed data type More detailed descriptions of these primitives may be found in the PVM3 Users Manual <ref> [Gei94] </ref>. 72
Reference: [Lew90] <author> T.G. Lewis and W. Rudd, </author> <title> Architecture of the Parallel Programming Support Environment, </title> <booktitle> Proc. CompCon90, </booktitle> <address> San Francisco, CA, </address> <month> Feb. 26 - Mar 2., </month> <year> 1990. </year>
Reference-contexts: This is because Linda [S-ci92] basically is a shared memory model. The runtime execution model remains the same over all the architectures on which it runs. 6.3 Others There are a variety of other systems for parallel programming. They include Paralex [Bab92], PPSE <ref> [Lew90] </ref> and P4 [But92]. They are discussed in detail in the literature. 70 Chapter 7. Conclusion and Future Work In this thesis, the design and implementation of a distributed execution environment in PVM for the CODE 2.0 graphical parallel programming system has been presented.
Reference: [New92] <author> P. </author> <title> Newton and J.C. Browne,The CODE 2.0 Graphical Parallel Programming Language, </title> <booktitle> Proc. ACM Int. Conf. on Supercomputing, </booktitle> <month> July, </month> <year> 1992. </year>
Reference: [New93a] <author> P. </author> <title> Newton, A Graphical Retargetable Parallel Programming Environment and its Efficient Implementation, </title> <type> Ph.D. thesis, </type> <institution> University of Texas at Austin, Dept. of Comp. Sci., </institution> <year> 1993. </year>
Reference-contexts: Also, the programming model must not be biased towards either shared-memory machines or distributed-memory machines. However, it is a common belief that such generalized abstractions do not support creation of efficient runtime structures. The CODE 2.0 <ref> [New93a] </ref> abstract parallel programming system has been shown to yield programs with execution efficiency comparable to hand-coding for shared-memory multiprocessors. <p> A CODE translator already exists for the shared-memory architecture of the Sequent Symmetry. It has been shown to generate programs comparable in efficiency to hand-coded programs on the Sequent <ref> [New93a] </ref>. We show that CODE can also be used to produce an efficient executable for a distributed execution environment like PVM. 1.3 Distributed Execution Environment This thesis deals with the problem of compiling CODE graphs into an efficient executable for the PVM environment. <p> Translation of CODE programs for a target architecture is a fivestage process consisting of translation from the GUI representation to the AST, AST decoration, AST optimization, Translation and Linking with the runtime library. The first two stages are architecture-independent and are not discussed here <ref> [New93a] </ref>. Also, no AST optimizations have been made as part of this thesis. The discussion here will pertain to translation and linking only. Translating involves running the translation methods of all nodes in the AST for the PVM architecture. This produces a set of C programs which use PVM primitives. <p> As can be seen from Figure 4-3 the translate methods of various members of the AST write to the various output files. This introduces a coordination problem in which these translate methods must interleave code generation among themselves. This problem is explained in detail in Chapter 7 of <ref> [New93a] </ref>. Structured files are used to solve this problem. Structured files are also explained in detail in Chapter 7 of [New93a]. The following brief explanation of structured files is from that chapter. <p> This introduces a coordination problem in which these translate methods must interleave code generation among themselves. This problem is explained in detail in Chapter 7 of <ref> [New93a] </ref>. Structured files are used to solve this problem. Structured files are also explained in detail in Chapter 7 of [New93a]. The following brief explanation of structured files is from that chapter. A structured file defines in template form the structure of a set of UNIX files that will contain the code that make up the translated CODE 2.0 program. <p> Related Work There have been many attempts to abstract parallel and distributed computing. Some have used graphical approaches while others have used the idea of enhancing existing languages by providing new primitives. In this chapter an example of each will be examined, HeNCE [Beg91] and Linda [S-ci92] respectively. Newton <ref> [New93a] </ref> gives a through survey of related work. Since, this recent survey is available, we give only a brief sketch of the two systems. 6.1 HeNCE HeNCE is a graphical parallel programming environment.
Reference: [New93b] <author> P. Newton and S. Khedekar, </author> <title> CODE 2.0 User Manual, </title> <year> 1993. </year>
Reference: [New93c] <author> P. </author> <title> Newton, CODE 2.0 Language Refererence, </title> <year> 1993. </year>
Reference: [Sci92] <institution> Scientific Computing Associates, Inc., C-Linda Reference Manual, </institution> <address> New Haven, CT, </address> <year> 1992. </year>
Reference-contexts: PVM itself is an attempt at presenting a homogeneous message-passing interface to a heterogeneous network of computers. HeNCE [Beg91] is a graphical parallel programming tool which can be used to create parallel programs on a set of networked machines. Linda <ref> [Sci92] </ref> is another parallel programming language which tries to augment sequential programming through the use of primitives which are architecture-independent. Chapter 6 explains in greater detail some of the related work in this area. Chapter 7 presents the conclusions and indicates possible future work. 5 Chapter 2.
Reference: [Str91] <author> B. Stroustrup, </author> <title> The C++ Programming Language, </title> <address> Reading, </address> <publisher> Addi-son Wesley, </publisher> <year> 1991. </year>
Reference: [Sun91] <author> V.S. Sunderam, </author> <title> PVM: A Framework for Parallel Distributed Computing, </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 2(4) </volume> <pages> 315-339, </pages> <month> Dec., </month> <year> 1990. </year>
References-found: 17

