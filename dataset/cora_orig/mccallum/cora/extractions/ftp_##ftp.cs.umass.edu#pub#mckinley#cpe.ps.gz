URL: ftp://ftp.cs.umass.edu/pub/mckinley/cpe.ps.gz
Refering-URL: http://spa-www.cs.umass.edu/bibliography.html
Root-URL: 
Email: ken@cs.rice.edu kats@cri.ensmp.fr tseng@cs.rice.edu  
Title: Analysis and Transformation in an Interactive Parallel Programming Tool  
Author: Ken Kennedy Kathryn S. M c Kinley Chau-Wen Tseng 
Address: Houston, TX 77251-1892  
Affiliation: Department of Computer Science Rice University  
Abstract: The ParaScope Editor is a new kind of interactive parallel programming tool for developing scientific Fortran programs. It assists the knowledgeable user by displaying the results of sophisticated program analyses and by providing editing and a set of powerful interactive transformations. After an edit or parallelism-enhancing transformation, the ParaScope Editor incrementally updates both the analyses and source quickly. This paper describes the underlying implementation of the ParaScope Editor, paying particular attention to the analysis and representation of dependence information and its reconstruction after changes to the program. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> F. Allen, M. Burke, P. Charles, R. Cytron, and J. Ferrante. </author> <title> An overview of the PTRAN analysis system for multiprocessing. </title> <booktitle> In Proceedings of the First International Conference on Supercomputing. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Athens, Greece, </address> <month> June </month> <year> 1987. </year>
Reference-contexts: Parafrase-2 adds scheduling, improved program analysis, and transformations [38]. More advanced interprocedural and symbolic analysis is planned [22]. Parafrase-2 uses Faust as a front end to provide interactive parallelization and graphical displays [21]. Page 22 Ptran is also an automatic parallelizer with extensive program analysis <ref> [1] </ref>. It computes the SSA and program dependence graphs, and performs constant propagation and interprocedural analysis [19]. Ptran introduces both task and loop parallelism, but currently the only other program transformations are variable privatization and loop distribution.
Reference: [2] <author> F. Allen and J. Cocke. </author> <title> A catalogue of optimizing transformations. </title> <editor> In J. Rustin, editor, </editor> <booktitle> Design and Optimization of Compilers. </booktitle> <publisher> Prentice-Hall, </publisher> <year> 1972. </year>
Reference-contexts: Ped supports a large set of transformations that have proven useful for introducing, discovering, and exploiting parallelism. Ped also supports transformations for enhancing the use of the memory hierarchy. These transformations are described in detail in the literature <ref> [2, 3, 10, 34, 36, 48] </ref>. We classify the transformations in Ped as follows. <p> Page 18 i i After Unroll and Before Unroll and 7.5 Unroll-and-Jam Unroll-and-jam is a transformation that unrolls an outer loop in a loop nest, then jams (or fuses) the resulting inner loops <ref> [2, 11] </ref>. Unroll-and-jam can be used to convert dependences carried by the outer loop into loop independent dependences or dependences carried by some inner loop. It brings two accesses to the same memory location closer together and can significantly improve performance by enabling reuse of either registers or cache.
Reference: [3] <author> J. R. Allen and K. Kennedy. </author> <title> Automatic translation of Fortran programs to vector form. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(4) </volume> <pages> 491-542, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: Hence, the results may differ each time the program is executed. This kind of anomaly, often called a data race, precludes the parallelization of the above loop. In the literature of compilation for parallel execution, a potential data race is referred to as a loop-carried dependence <ref> [3, 34] </ref>. Without explicit synchronization, only loops with no carried dependences may be safely executed in parallel. Automatic parallelizers use this principle by constructing a dependence graph for the entire program and then parallelizing every loop that does not carry a dependence. <p> on x if and only if: 1. 9 a non-null path p, from x to y, such that y postdominates every node between x and y on p, and 2. y does not postdominate x. 2.3 Loop-Carried and Loop-Independent Dependence Dependences are also characterized as either being loop-carried or loop-independent <ref> [3] </ref>. Consider the following loop: DO I = 2, N S 2 : : : = A (I) ENDDO The true dependence S 1 ffiS 2 is loop-independent because it exists regardless of the surrounding loops. <p> Loop-carried dependences are important because they inhibit loops from executing in parallel without synchronization. When there are nested loops, the level of any carried dependence is the outermost loop on which it first arises <ref> [3] </ref>. 2.4 Dependence Testing Determining the existence of data dependence between array references is more difficult than for scalars, because the subscript expressions must be considered. The process of differentiating between two subscripted references in a loop nest is called dependence testing. <p> Direction vectors, introduced by Wolfe [48], are useful for calculating loop-carried dependences. A dependence is carried by the outermost loop for which the element in the direction vector is not an `='. Additionally, direction vectors are used to determine the safety and profitability of loop interchange <ref> [3, 48] </ref>. Distance vectors are more precise versions of direction vectors that specify the actual number of loop iterations between two accesses to the same memory location [3, 48]. <p> Additionally, direction vectors are used to determine the safety and profitability of loop interchange <ref> [3, 48] </ref>. Distance vectors are more precise versions of direction vectors that specify the actual number of loop iterations between two accesses to the same memory location [3, 48]. They are employed by transformations to exploit parallelism and the memory hierarchy. 3 Work Model Ped is designed to exploit loop-level parallelism, which comprises most of the usable parallelism in scientific codes when synchronization costs are considered [15]. <p> The current implementation of Ped can determine if event style synchronization is sufficient to protect a particular dependence. 5.8 Utilizing External Analysis To overcome gaps in the current implementation of program analysis, Ped may import dependence information from Pfc, the Rice system for automatic vectorization and parallelization <ref> [3] </ref>. Pfc's program analyzer is more mature and contains symbolic analysis, interprocedural regular sections and constants, as well as control and data dependence analysis. Pfc produces a file of dependence information that Ped converts into its own internal representation. <p> Ped supports a large set of transformations that have proven useful for introducing, discovering, and exploiting parallelism. Ped also supports transformations for enhancing the use of the memory hierarchy. These transformations are described in detail in the literature <ref> [2, 3, 10, 34, 36, 48] </ref>. We classify the transformations in Ped as follows. <p> The purpose, mechanics, and safety of these transformations are presented, followed by their profitability estimates, user advice, and incremental dependence update algorithms. 7.1 Loop Interchange Loop interchange is a key transformation that modifies the traversal order of the iteration space for the selected loop nest <ref> [3, 48] </ref>. It has been used extensively in vectorizing and parallelizing compilers to adjust the granularity of parallel loops and to expose parallelism [3, 34, 48]. Ped interchanges pairs of adjacent loops. Loop permutations may be performed as a series of pairwise interchanges. <p> It has been used extensively in vectorizing and parallelizing compilers to adjust the granularity of parallel loops and to expose parallelism <ref> [3, 34, 48] </ref>. Ped interchanges pairs of adjacent loops. Loop permutations may be performed as a series of pairwise interchanges. Ped supports interchange of triangular or skewed loops. It also interchanges complex loop nests that result after interchanging skewed loops. <p> Finally, the interchange flags are recalculated for dependences in the loop nest. 7.2 Loop Distribution Loop distribution separates independent statements inside a single loop into multiple loops with identical headers <ref> [3, 34] </ref>. It is used to expose partial parallelism by separating statements which may be parallelized from those that must be executed sequentially. Loop distribution is a cornerstone of vectorization and parallelization [3, 34]. In Ped the user can specify whether distribution is for the purpose of vectorization or parallelization. <p> 7.2 Loop Distribution Loop distribution separates independent statements inside a single loop into multiple loops with identical headers <ref> [3, 34] </ref>. It is used to expose partial parallelism by separating statements which may be parallelized from those that must be executed sequentially. Loop distribution is a cornerstone of vectorization and parallelization [3, 34]. In Ped the user can specify whether distribution is for the purpose of vectorization or parallelization. If the user specifies vectorization, then each statement is placed in a separate loop when possible. <p> Therefore, S 2 is placed in a separate loop and both loops may be made parallel, as illustrated above by the two transformed loops on the right. Loop distribution can also enable loop interchange to enhance parallelism or data locality <ref> [3, 14] </ref>. Update Updates can be performed quickly on the existing dependence graph after loop distribution. For each new loop Ped also creates new loop info structures and attaches them to the AST. Data and control dependences between statements in the same partition remain unchanged. <p> Our work on interactive parallelization bears similarities to Ptool, Sigmacs, Pat, Superb, Tiny, and Forge 90. Ped has been greatly influenced by the Rice Parallel Fortran Converter (Pfc), which has focused on the problem of automatically vectorizing and parallelizing sequential Fortran <ref> [3] </ref>. Pfc has a mature dependence analyzer which performs data dependence analysis, control dependence analysis, interprocedural constant propagation [12], interprocedural side effect analysis of scalars [17], and interprocedural array section analysis [24]. The precursor to Ped, a dependence browser named Ptool, interactively displayed Pfc's dependences to users [25].
Reference: [4] <author> B. Alpern, M. Wegman, and K. Zadeck. </author> <title> Detecting equality of variables in programs. </title> <booktitle> In Proceedings of the Fifteenth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> San Diego, CA, </address> <month> January </month> <year> 1988. </year>
Reference-contexts: The symbolic analyzer will then provide or build an appropriate shadow expression. The SSA graph for scalars, produced by scalar data-flow analysis, provides the framework for symbolic analysis in Ped. Global value numbering is performed on the SSA graph <ref> [4] </ref>. An extended version of this algorithm simplifies value numbers as they are built, using arithmetic laws and cancellation. Value numbers are then translated into shadow expressions for use by dependence testing and other analyses.
Reference: [5] <institution> Applied Parallel Research, Placerville, CA. </institution> <note> Forge 90 Distributed Memory Parallelizer: User's Guide, version 8.0 edition, </note> <year> 1992. </year>
Reference-contexts: Extensions to Tiny include precise dependence tests, array kill analysis, and automatic selection of program transformation sequences [39]. Forge 90, formerly Mimdizer, is an interactive parallelization system for MIMD shared and distributed-memory machines from Applied Parallel Research <ref> [5] </ref>. It performs data-flow and dependence analyses, and also supports loop-level transformations. Associated tools graphically display call graph, control flow, dependence, and profiling information. Forge 90 can be used to generate parallel programs for both shared and distributed-memory machines.
Reference: [6] <author> V. Balasundaram, K. Kennedy, U. Kremer, K. S. M c Kinley, and J. Subhlok. </author> <title> The ParaScope Editor: An interactive parallel programming tool. </title> <booktitle> In Proceedings of Supercomputing '89, </booktitle> <address> Reno, NV, </address> <month> November </month> <year> 1989. </year>
Reference-contexts: Clearly a tool with this much functionality is bound to be complex. Ped incorporates a complete source editor and supports dependence analysis, dependence display, and a large variety of program transformations to enhance parallelism. Previous work has described the usage and motivation of the ParaScope Editor <ref> [6, 23, 32] </ref> and the ParaScope parallel programming environment [16]. In this paper, we focus on the implementation of Ped's analysis and transformation features. <p> This functionality is needed because program analysis is necessarily conservative. Ped's user interface provides a mechanism that enables users to reject a particular dependence or a class of dependences that they feel are overly conservative <ref> [6, 16, 23, 32] </ref>. In response, Ped no longer considers these dependences during program transformations. Users can similarly correct overly conservative variable classifications by reclassifying shared variables as private. In response, Ped eliminates from further consideration any loop-carried dependences incident on these variables.
Reference: [7] <author> U. Banerjee. </author> <title> Unimodular transformations of double loops. </title> <booktitle> In Advances in Languages and Compilers for Parallel Computing, </booktitle> <address> Irvine, CA, August 1990. </address> <publisher> The MIT Press. </publisher>
Reference-contexts: It can be applied via unimodular methods using loop interchange, strip mining, and loop reversal to obtain loop-level parallelism in a loop nest <ref> [7, 47] </ref>. All of these transformations are supported in Ped. Loop skewing is applied to a pair of perfectly nested loops that both carry dependences, even after loop interchange.
Reference: [8] <author> M. Burke. </author> <title> An interval-based approach to exhaustive and incremental interprocedural data-flow analysis. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 12(3) </volume> <pages> 341-395, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: Several algorithms for incremental analysis can be found in the literature; e.g., data-flow analysis [41], interprocedural analysis <ref> [8] </ref>, interprocedural recompilation analysis [9], as well as dependence analysis [40]. However, few of these algorithms have been implemented and evaluated in an interactive environment. Rather than tackle all these problems at once, we chose a simple yet practical strategy for the current implementation of Ped.
Reference: [9] <author> M. Burke and L. Torczon. </author> <title> Interprocedural optimization: Eliminating unnecessary recompilation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <note> to appear 1993. </note>
Reference-contexts: Several algorithms for incremental analysis can be found in the literature; e.g., data-flow analysis [41], interprocedural analysis [8], interprocedural recompilation analysis <ref> [9] </ref>, as well as dependence analysis [40]. However, few of these algorithms have been implemented and evaluated in an interactive environment. Rather than tackle all these problems at once, we chose a simple yet practical strategy for the current implementation of Ped.
Reference: [10] <author> D. Callahan, S. Carr, and K. Kennedy. </author> <title> Improving register allocation for subscripted variables. </title> <booktitle> In Proceedings of the SIGPLAN '90 Conference on Program Language Design and Implementation, </booktitle> <address> White Plains, NY, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Ped supports a large set of transformations that have proven useful for introducing, discovering, and exploiting parallelism. Ped also supports transformations for enhancing the use of the memory hierarchy. These transformations are described in detail in the literature <ref> [2, 3, 10, 34, 36, 48] </ref>. We classify the transformations in Ped as follows. <p> It brings two accesses to the same memory location closer together and can significantly improve performance by enabling reuse of either registers or cache. When applied in conjunction with scalar replacement on scientific codes, unroll-and-jam has resulted in integer factor speedups, even for single processors <ref> [10] </ref>. Unroll-and-jam may also be applied to imperfectly nested loops or loops with complex iteration spaces. Figure 5 shows an example iteration space before and after unroll-and-jam of degree 1. <p> Profitability Balance describes the ratio between computation and memory access rates [11]. Unroll-and-jam is profitable if it brings the balance of a loop closer to the balance of the underlying machine. Ped automatically calculates the optimal unroll-and-jam degree for a loop nest, including loops with complex iteration spaces <ref> [10] </ref>. Update An algorithm for the incremental update of the dependence graph after unroll-and-jam is described elsewhere [10]. However, since no global data-flow or symbolic information is changed by unroll-and-jam, we chose the same strategy as for loop fusion. <p> Ped automatically calculates the optimal unroll-and-jam degree for a loop nest, including loops with complex iteration spaces <ref> [10] </ref>. Update An algorithm for the incremental update of the dependence graph after unroll-and-jam is described elsewhere [10]. However, since no global data-flow or symbolic information is changed by unroll-and-jam, we chose the same strategy as for loop fusion. <p> This feature has proven itself to be extremely valuable in practice. The analysis and transformation features of Ped are currently being used by the Fortran D compiler for distributed-memory machines [26], a source-to-source Fortran translator for improving data locality <ref> [10, 14] </ref>, and an on-the-fly data-race detection system for shared-memory machines [27]. 10 Related Work Several other research groups are also developing advanced parallel programming tools. Ped's analysis and transformation capabilities compare favorably to automatic parallelization systems such as Parafrase, Ptran, Pips and of course Pfc.
Reference: [11] <author> D. Callahan, J. Cocke, and K. Kennedy. </author> <title> Estimating interlock and improving balance for pipelined machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5(4) </volume> <pages> 334-358, </pages> <month> August </month> <year> 1988. </year> <pages> Page 24 </pages>
Reference-contexts: Page 18 i i After Unroll and Before Unroll and 7.5 Unroll-and-Jam Unroll-and-jam is a transformation that unrolls an outer loop in a loop nest, then jams (or fuses) the resulting inner loops <ref> [2, 11] </ref>. Unroll-and-jam can be used to convert dependences carried by the outer loop into loop independent dependences or dependences carried by some inner loop. It brings two accesses to the same memory location closer together and can significantly improve performance by enabling reuse of either registers or cache. <p> If any of these dependences cross between the imperfectly nested statements and the statements in the inner loop, they inhibit unroll-and-jam. Specifically, the intervening statements cannot be moved and prevent fusion of the inner loops. Profitability Balance describes the ratio between computation and memory access rates <ref> [11] </ref>. Unroll-and-jam is profitable if it brings the balance of a loop closer to the balance of the underlying machine. Ped automatically calculates the optimal unroll-and-jam degree for a loop nest, including loops with complex iteration spaces [10].
Reference: [12] <author> D. Callahan, K. Cooper, K. Kennedy, and L. Torczon. </author> <title> Interprocedural constant propagation. </title> <booktitle> In Proceedings of the SIGPLAN '86 Symposium on Compiler Construction, </booktitle> <address> Palo Alto, CA, </address> <month> June </month> <year> 1986. </year>
Reference-contexts: Interprocedural analysis is required so that worst case assumptions need not be made when calls are encountered. ParaScope performs conventional interprocedural analysis that discovers constants, aliasing, flow-insensitive side effects such as ref and mod <ref> [12, 17] </ref>. Flow-sensitive side effects such as use and kill are not currently available. Even with these analyses, improvements are limited because arrays are treated as monolithic objects, making it impossible to determine whether two references to an array actually access the same memory location. <p> Ped has been greatly influenced by the Rice Parallel Fortran Converter (Pfc), which has focused on the problem of automatically vectorizing and parallelizing sequential Fortran [3]. Pfc has a mature dependence analyzer which performs data dependence analysis, control dependence analysis, interprocedural constant propagation <ref> [12] </ref>, interprocedural side effect analysis of scalars [17], and interprocedural array section analysis [24]. The precursor to Ped, a dependence browser named Ptool, interactively displayed Pfc's dependences to users [25].
Reference: [13] <author> D. Callahan, K. Kennedy, and J. Subhlok. </author> <title> Analysis of event synchronization in a parallel programming tool. </title> <booktitle> In Proceedings of the Second ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Seattle, WA, </address> <month> March </month> <year> 1990. </year>
Reference-contexts: Establishing that the order specified by certain dependences will always be observed has been shown to be co-NP-hard, but techniques have been developed to identify dependences that are satisfied by existing synchronization under restricted circumstances <ref> [13] </ref>. The current implementation of Ped can determine if event style synchronization is sufficient to protect a particular dependence. 5.8 Utilizing External Analysis To overcome gaps in the current implementation of program analysis, Ped may import dependence information from Pfc, the Rice system for automatic vectorization and parallelization [3].
Reference: [14] <author> S. Carr, K. Kennedy, K. S. M c Kinley, and C. Tseng. </author> <title> Compiler optimizations for improving data locality. </title> <type> Technical Report TR92-195, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> November </month> <year> 1992. </year>
Reference-contexts: Therefore, S 2 is placed in a separate loop and both loops may be made parallel, as illustrated above by the two transformed loops on the right. Loop distribution can also enable loop interchange to enhance parallelism or data locality <ref> [3, 14] </ref>. Update Updates can be performed quickly on the existing dependence graph after loop distribution. For each new loop Ped also creates new loop info structures and attaches them to the AST. Data and control dependences between statements in the same partition remain unchanged. <p> Instead of breaking up a loop into multiple loops, it combines two loop nests into a single loop nest. Researchers have found loop fusion useful for improving data locality, reducing loop overhead, and enabling loop interchange <ref> [14, 31] </ref>. Ped supports fusion of adjacent loop nests where the outer loops iterate identically, but the loop index variable names need not be the same. Safety Loop fusion is safe if it does not reverse the order of execution of the source and sink of any dependence [48]. <p> After fusion, the write and the read occur on the same iteration, making A (I) much more likely to still be in cache or a register at the time of the read. Loop fusion can also improve data locality and increase parallelism by enabling loop interchange <ref> [14, 31] </ref>. Ped does not currently report these cases. Update Updates after loop fusion are straightforward and quick. The loop info structures are merged. If the loop index variables differ they are both replaced with a new unique name. <p> This feature has proven itself to be extremely valuable in practice. The analysis and transformation features of Ped are currently being used by the Fortran D compiler for distributed-memory machines [26], a source-to-source Fortran translator for improving data locality <ref> [10, 14] </ref>, and an on-the-fly data-race detection system for shared-memory machines [27]. 10 Related Work Several other research groups are also developing advanced parallel programming tools. Ped's analysis and transformation capabilities compare favorably to automatic parallelization systems such as Parafrase, Ptran, Pips and of course Pfc.
Reference: [15] <author> D. Chen, H. Su, and P. Yew. </author> <title> The impact of synchronization and granularity on parallel systems. </title> <booktitle> In Proceedings of the 17th International Symposium on Computer Architecture, </booktitle> <address> Seattle, WA, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: They are employed by transformations to exploit parallelism and the memory hierarchy. 3 Work Model Ped is designed to exploit loop-level parallelism, which comprises most of the usable parallelism in scientific codes when synchronization costs are considered <ref> [15] </ref>. In the work model best supported by Ped, the user first selects a loop for parallelization. Ped then displays all of its carried dependences. The user may sort or filter the dependences, as well as edit and delete dependences that are due to overly conservative dependence analysis.
Reference: [16] <author> K. Cooper, M. W. Hall, R. T. Hood, K. Kennedy, K. S. McKinley, J. M. Mellor-Crummey, L. Torczon, and S. K. Warren. </author> <title> The ParaScope parallel programming environment. </title> <journal> Proceedings of the IEEE, </journal> <note> To appear 1993. </note>
Reference-contexts: Ped incorporates a complete source editor and supports dependence analysis, dependence display, and a large variety of program transformations to enhance parallelism. Previous work has described the usage and motivation of the ParaScope Editor [6, 23, 32] and the ParaScope parallel programming environment <ref> [16] </ref>. In this paper, we focus on the implementation of Ped's analysis and transformation features. Particular attention is paid to the representation of dependences, the construction of the dependence graph, and how dependences are used and incrementally reconstructed for each program transformation in an efficient and flexible manner. <p> Each element of the vector can represent a dependence distance or direction. Dependence edges are organized for the user interface using a higher level data abstraction, called the edge list. The edge list provides the user a configurable method of filtering, sorting, and selecting dependences <ref> [16, 32] </ref>. 4.2.2 Level Vectors Dependence edges hold most of the dependence information for a program, but level vectors provide the glue which links them together and to the AST. Every executable statement in a loop nest involved with a dependence has a level vector. <p> This functionality is needed because program analysis is necessarily conservative. Ped's user interface provides a mechanism that enables users to reject a particular dependence or a class of dependences that they feel are overly conservative <ref> [6, 16, 23, 32] </ref>. In response, Ped no longer considers these dependences during program transformations. Users can similarly correct overly conservative variable classifications by reclassifying shared variables as private. In response, Ped eliminates from further consideration any loop-carried dependences incident on these variables. <p> Plans are in place to provide facilities in Ped for more persistent user assertions [23]. 9 Modular Design The ParaScope Editor is designed so that its analysis and transformation capabilities can be used throughout the ParaScope programming environment <ref> [16] </ref>. By separating the calculation of safety and profitability for program transformations, we have made them easily adaptable by other tools. Transformation are easily integrated into new systems by using Ped's tests for legality and substituting new profitability measures. This feature has proven itself to be extremely valuable in practice.
Reference: [17] <author> K. Cooper, K. Kennedy, and L. Torczon. </author> <title> The impact of interprocedural analysis and optimization in the IR n programming environment. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 8(4) </volume> <pages> 491-523, </pages> <month> October </month> <year> 1986. </year>
Reference-contexts: Interprocedural analysis is required so that worst case assumptions need not be made when calls are encountered. ParaScope performs conventional interprocedural analysis that discovers constants, aliasing, flow-insensitive side effects such as ref and mod <ref> [12, 17] </ref>. Flow-sensitive side effects such as use and kill are not currently available. Even with these analyses, improvements are limited because arrays are treated as monolithic objects, making it impossible to determine whether two references to an array actually access the same memory location. <p> Once constructed, regular sections may be quickly intersected during interprocedural analysis and dependence testing to determine whether dependences exist. The implementation of regular sections in ParaScope is nearing completion. We are also integrating existing ParaScope interprocedural analysis and transformations such as inlining and cloning into Ped <ref> [17] </ref>. 5.6 Dependence Testing The dependence testing phase refines the coarse dependence graph for array variables created by scalar analysis and sharpened by interprocedural analysis. Ped classifies subscripts in a pair of array references Page 11 according to two orthogonal criteria: complexity and separability. <p> Pfc has a mature dependence analyzer which performs data dependence analysis, control dependence analysis, interprocedural constant propagation [12], interprocedural side effect analysis of scalars <ref> [17] </ref>, and interprocedural array section analysis [24]. The precursor to Ped, a dependence browser named Ptool, interactively displayed Pfc's dependences to users [25]. Ped integrates and extends Pfc's analysis & transformations and Ptool's browsing capabilities, making them available to the user in an interactive environment.
Reference: [18] <author> R. Cytron, J. Ferrante, B. Rosen, M. Wegman, and K. Zadeck. </author> <title> Efficiently computing static single assignment form and the control dependence graph. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(4) </volume> <pages> 451-490, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: It also provides a framework for the later phases of the dependence analyzer. Ped first constructs the control flow graph and postdominator tree. It then computes dominance frontiers for each scalar variable and uses them to build the static single assignment (SSA) graph for each procedure <ref> [18] </ref>. Edges in the SSA graph correspond to precise true dependences for scalar variables. Next, Ped constructs a coarse dependence graph for array variables in each loop nest by connecting fDefsg with fDefs [ Usesg. These edges are later refined through dependence testing to construct dependence edges.
Reference: [19] <author> J. Ferrante, K. Ottenstein, and J. Warren. </author> <title> The program dependence graph and its use in optimization. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(3) </volume> <pages> 319-349, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: Page 2 2.2 Control Dependence Intuitively, a control dependence, S 1 ffi c S 2 , indicates that the execution of S 1 directly determines whether S 2 will be executed. The following formal definitions of control dependence and the postdominance relation are taken from the literature <ref> [19] </ref>. Def: x is postdominated by y in the control flow graph G f , if every path from x to stop contains y, where stop is the exit node of G f . <p> Parafrase-2 uses Faust as a front end to provide interactive parallelization and graphical displays [21]. Page 22 Ptran is also an automatic parallelizer with extensive program analysis [1]. It computes the SSA and program dependence graphs, and performs constant propagation and interprocedural analysis <ref> [19] </ref>. Ptran introduces both task and loop parallelism, but currently the only other program transformations are variable privatization and loop distribution. Sarkar and Thekkath propose a unified framework for applying iteration-reordering transformations to perfect loop nests [42].
Reference: [20] <author> G. Goff, K. Kennedy, and C. Tseng. </author> <title> Practical dependence testing. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Program Language Design and Implementation, </booktitle> <address> Toronto, Canada, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: When possible, this phase derives constants or simplified forms for loop bounds, loop steps, array subscript expressions, array dimensions, and control flow. Symbolic analysis significantly improves the precision of dependence testing <ref> [20, 22, 44] </ref> and interprocedural analysis [24]. Unlike automatic parallelization tools, Ped does not make transformations to the program for purposes of analysis. Accordingly, the results of symbolic analysis are stored in shadow expressions that represent Page 10 computations of interest. <p> Dependence edges are eliminated if dependence between the references can be disproved. Otherwise, dependence testing characterizes the dependences with a minimal set of hybrid distance/direction vectors. This dependence information is vital for guiding transformations. Ped's dependence tests are discussed in detail elsewhere <ref> [20] </ref>. Most of these dependence tests have been implemented in the current version of Ped. We are in the process of extending them to handle symbolic expressions, complex iteration spaces, and regular sections. 5.7 Analysis of Synchronization In a sophisticated parallel program, the user may wish to employ complex synchronization.
Reference: [21] <author> V. Guarna, D. Gannon, Y. Gaur, and D. Jablonowski. </author> <title> Faust: An environment for programming parallel scientific applications. </title> <booktitle> In Proceedings of Supercomputing '88, </booktitle> <address> Orlando, FL, </address> <month> November </month> <year> 1988. </year>
Reference-contexts: Batch analysis is performed after each transformation phase to update the dependence information for the entire program. Parafrase-2 adds scheduling, improved program analysis, and transformations [38]. More advanced interprocedural and symbolic analysis is planned [22]. Parafrase-2 uses Faust as a front end to provide interactive parallelization and graphical displays <ref> [21] </ref>. Page 22 Ptran is also an automatic parallelizer with extensive program analysis [1]. It computes the SSA and program dependence graphs, and performs constant propagation and interprocedural analysis [19]. Ptran introduces both task and loop parallelism, but currently the only other program transformations are variable privatization and loop distribution. <p> Sigmacs, a programmable interactive parallelizer in the Faust programming environment, computes and displays call graphs, process graphs, and a statement dependence graph <ref> [21, 43] </ref>. In a process graph each node represents a task or a process, which is a separate entity running in parallel. The call and process graphs may be animated dynamically at run time. Sigmacs also performs several interactive program transformations, and is working on automatic updating of dependence information.
Reference: [22] <author> M. Haghighat and C. Polychronopoulos. </author> <title> Symbolic dependence analysis for high-performance parallelizing compilers. </title> <booktitle> In Advances in Languages and Compilers for Parallel Computing, </booktitle> <address> Irvine, CA, August 1990. </address> <publisher> The MIT Press. </publisher>
Reference-contexts: When possible, this phase derives constants or simplified forms for loop bounds, loop steps, array subscript expressions, array dimensions, and control flow. Symbolic analysis significantly improves the precision of dependence testing <ref> [20, 22, 44] </ref> and interprocedural analysis [24]. Unlike automatic parallelization tools, Ped does not make transformations to the program for purposes of analysis. Accordingly, the results of symbolic analysis are stored in shadow expressions that represent Page 10 computations of interest. <p> In Parafrase, program transformations are structured in phases and are always applied where applicable. Batch analysis is performed after each transformation phase to update the dependence information for the entire program. Parafrase-2 adds scheduling, improved program analysis, and transformations [38]. More advanced interprocedural and symbolic analysis is planned <ref> [22] </ref>. Parafrase-2 uses Faust as a front end to provide interactive parallelization and graphical displays [21]. Page 22 Ptran is also an automatic parallelizer with extensive program analysis [1]. It computes the SSA and program dependence graphs, and performs constant propagation and interprocedural analysis [19].
Reference: [23] <author> M. W. Hall, T. Harvey, K. Kennedy, N. McIntosh, K. S. M c Kinley, J. D. Oldham, M. Paleczny, and G. Roth. </author> <title> Experiences using the ParaScope Editor. </title> <booktitle> In Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Clearly a tool with this much functionality is bound to be complex. Ped incorporates a complete source editor and supports dependence analysis, dependence display, and a large variety of program transformations to enhance parallelism. Previous work has described the usage and motivation of the ParaScope Editor <ref> [6, 23, 32] </ref> and the ParaScope parallel programming environment [16]. In this paper, we focus on the implementation of Ped's analysis and transformation features. <p> We are, as always, in the process of further extending Ped's transformation capabilities. An effort is underway to incorporate automatic parallelization strategies within Ped in order to provide users with further assistance in the parallelization process <ref> [23, 37] </ref>. 8 Edits This section describes strategies for updates after edits to the program text or direct modification of Ped's dependence information. <p> This functionality is needed because program analysis is necessarily conservative. Ped's user interface provides a mechanism that enables users to reject a particular dependence or a class of dependences that they feel are overly conservative <ref> [6, 16, 23, 32] </ref>. In response, Ped no longer considers these dependences during program transformations. Users can similarly correct overly conservative variable classifications by reclassifying shared variables as private. In response, Ped eliminates from further consideration any loop-carried dependences incident on these variables. <p> These user assertions are retained even after structured transformations and edits. However, if a change results in batch reanalysis, any dependence and variable assertions will be lost. Plans are in place to provide facilities in Ped for more persistent user assertions <ref> [23] </ref>. 9 Modular Design The ParaScope Editor is designed so that its analysis and transformation capabilities can be used throughout the ParaScope programming environment [16]. By separating the calculation of safety and profitability for program transformations, we have made them easily adaptable by other tools.
Reference: [24] <author> P. Havlak and K. Kennedy. </author> <title> An implementation of interprocedural bounded regular section analysis. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 350-360, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: When possible, this phase derives constants or simplified forms for loop bounds, loop steps, array subscript expressions, array dimensions, and control flow. Symbolic analysis significantly improves the precision of dependence testing [20, 22, 44] and interprocedural analysis <ref> [24] </ref>. Unlike automatic parallelization tools, Ped does not make transformations to the program for purposes of analysis. Accordingly, the results of symbolic analysis are stored in shadow expressions that represent Page 10 computations of interest. <p> To provide more precise analysis, array accesses can be summarized in terms of regular sections that describe subsections of arrays such as rows, columns, and rectangles <ref> [24] </ref>. Local symbolic analysis and interprocedural constants are required to build accurate regular sections. Once constructed, regular sections may be quickly intersected during interprocedural analysis and dependence testing to determine whether dependences exist. The implementation of regular sections in ParaScope is nearing completion. <p> Pfc has a mature dependence analyzer which performs data dependence analysis, control dependence analysis, interprocedural constant propagation [12], interprocedural side effect analysis of scalars [17], and interprocedural array section analysis <ref> [24] </ref>. The precursor to Ped, a dependence browser named Ptool, interactively displayed Pfc's dependences to users [25]. Ped integrates and extends Pfc's analysis & transformations and Ptool's browsing capabilities, making them available to the user in an interactive environment. Parafrase was the first automatic vectorizing and parallelizing compiler [34].
Reference: [25] <author> L. Henderson, R. Hiromoto, O. Lubeck, and M. Simmons. </author> <title> On the use of diagnostic dependency-analysis tools in parallel programming: Experiences using PTOOL. </title> <journal> The Journal of Supercomputing, </journal> <volume> 4 </volume> <pages> 83-96, </pages> <year> 1990. </year>
Reference-contexts: Pfc has a mature dependence analyzer which performs data dependence analysis, control dependence analysis, interprocedural constant propagation [12], interprocedural side effect analysis of scalars [17], and interprocedural array section analysis [24]. The precursor to Ped, a dependence browser named Ptool, interactively displayed Pfc's dependences to users <ref> [25] </ref>. Ped integrates and extends Pfc's analysis & transformations and Ptool's browsing capabilities, making them available to the user in an interactive environment. Parafrase was the first automatic vectorizing and parallelizing compiler [34]. It supports program analysis and performs a large number of program transformations to improve parallelism.
Reference: [26] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Transformation are easily integrated into new systems by using Ped's tests for legality and substituting new profitability measures. This feature has proven itself to be extremely valuable in practice. The analysis and transformation features of Ped are currently being used by the Fortran D compiler for distributed-memory machines <ref> [26] </ref>, a source-to-source Fortran translator for improving data locality [10, 14], and an on-the-fly data-race detection system for shared-memory machines [27]. 10 Related Work Several other research groups are also developing advanced parallel programming tools.
Reference: [27] <author> R. Hood, K. Kennedy, and J. Mellor-Crummey. </author> <title> Parallel program debugging with on-the-fly anomaly detection. </title> <booktitle> In Proceedings of Supercomputing '90, </booktitle> <address> New York, NY, </address> <month> November </month> <year> 1990. </year>
Reference-contexts: The analysis and transformation features of Ped are currently being used by the Fortran D compiler for distributed-memory machines [26], a source-to-source Fortran translator for improving data locality [10, 14], and an on-the-fly data-race detection system for shared-memory machines <ref> [27] </ref>. 10 Related Work Several other research groups are also developing advanced parallel programming tools. Ped's analysis and transformation capabilities compare favorably to automatic parallelization systems such as Parafrase, Ptran, Pips and of course Pfc.
Reference: [28] <author> F. Irigoin, P. Jouvelot, and R. Triolet. </author> <title> Semantical interprocedural parallelization: An overview of the PIPS project. </title> <booktitle> In Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Sarkar and Thekkath propose a unified framework for applying iteration-reordering transformations to perfect loop nests [42]. The framework provides rules for calculating legality and updating dependence vectors and loop bounds, but does not estimate profitability. Pips applies sophisticated interprocedural semantical analysis to parallelize programs for shared and distributed-memory machines <ref> [28] </ref>. The user can select the precision of analysis desired, inspect the resulting predicates and regions calculated by Pips, and then apply transformations such as privatization, loop distribution, and parallelization to specified procedures or the entire program.
Reference: [29] <author> F. Irigoin and R. Triolet. </author> <title> Supernode partitioning. </title> <booktitle> In Proceedings of the Fifteenth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> San Diego, CA, </address> <month> January </month> <year> 1988. </year>
Reference-contexts: This update strategy proved simple to implement and is quick in practice. 7.4 Loop Skewing Loop skewing is a transformation that changes the shape of the iteration space to expose parallelism across a wavefront <ref> [29, 48] </ref>. It can be applied via unimodular methods using loop interchange, strip mining, and loop reversal to obtain loop-level parallelism in a loop nest [7, 47]. All of these transformations are supported in Ped.
Reference: [30] <author> K. Kennedy and K. S. M c Kinley. </author> <title> Loop distribution with arbitrary control flow. </title> <booktitle> In Proceedings of Supercomputing Page 25 '90, </booktitle> <address> New York, NY, </address> <month> November </month> <year> 1990. </year>
Reference-contexts: The user may then apply or reject the distribution partition. Safety To maintain the meaning of the original loop, the partition must not put statements that are involved in recurrences into different loops <ref> [30, 34] </ref>. Recurrences are calculated by finding strongly connected regions in the subgraph composed of loop-independent dependences and dependences carried on the loop to be distributed. <p> These decisions correspond to loop-independent control dependences that cross between partitions. We use Kennedy and M c Kinley's method to insert new arrays, called execution variables, that record these "crossing" decisions <ref> [30] </ref>. Given a partition, this algorithm introduces the minimal number of execution variables necessary to effect the partition, even for loops with arbitrary control flow. Profitability Currently Ped does not change the order of statements in the loop during partitioning. <p> First, loop-independent data dependences are introduced between the definitions and uses of execution variables representing the crossing decision. A control dependence is then inserted from the test on the execution variable to the sink of the original control dependence. The update algorithm is explained more thoroughly elsewhere <ref> [30] </ref>. 7.3 Loop Fusion Loop fusion is the dual of loop distribution. Instead of breaking up a loop into multiple loops, it combines two loop nests into a single loop nest. Researchers have found loop fusion useful for improving data locality, reducing loop overhead, and enabling loop interchange [14, 31].
Reference: [31] <author> K. Kennedy and K. S. M c Kinley. </author> <title> Maximizing loop parallelism and improving data locality via loop fusion and distribution. </title> <type> Technical Report TR92-189, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> August </month> <year> 1992. </year>
Reference-contexts: In addition, when the source of a dependence lexically follows the sink, these statements will be placed in the same partition. The implementation of a partitioning algorithm that maximizes loop parallelism while producing the fewest parallel loops is underway <ref> [31] </ref>. When distributing for vectorization, statements not involved in recurrences are placed in separate loops. When distributing for parallelization, they are partitioned as follows. A statement is added to the preceding partition only if it does not cause that partition to be sequentialized. Otherwise it begins a new partition. <p> Instead of breaking up a loop into multiple loops, it combines two loop nests into a single loop nest. Researchers have found loop fusion useful for improving data locality, reducing loop overhead, and enabling loop interchange <ref> [14, 31] </ref>. Ped supports fusion of adjacent loop nests where the outer loops iterate identically, but the loop index variable names need not be the same. Safety Loop fusion is safe if it does not reverse the order of execution of the source and sink of any dependence [48]. <p> After fusion, the write and the read occur on the same iteration, making A (I) much more likely to still be in cache or a register at the time of the read. Loop fusion can also improve data locality and increase parallelism by enabling loop interchange <ref> [14, 31] </ref>. Ped does not currently report these cases. Update Updates after loop fusion are straightforward and quick. The loop info structures are merged. If the loop index variables differ they are both replaced with a new unique name.
Reference: [32] <author> K. Kennedy, K. S. M c Kinley, and C. Tseng. </author> <title> Interactive parallel programming using the ParaScope Editor. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 329-341, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Clearly a tool with this much functionality is bound to be complex. Ped incorporates a complete source editor and supports dependence analysis, dependence display, and a large variety of program transformations to enhance parallelism. Previous work has described the usage and motivation of the ParaScope Editor <ref> [6, 23, 32] </ref> and the ParaScope parallel programming environment [16]. In this paper, we focus on the implementation of Ped's analysis and transformation features. <p> Each element of the vector can represent a dependence distance or direction. Dependence edges are organized for the user interface using a higher level data abstraction, called the edge list. The edge list provides the user a configurable method of filtering, sorting, and selecting dependences <ref> [16, 32] </ref>. 4.2.2 Level Vectors Dependence edges hold most of the dependence information for a program, but level vectors provide the glue which links them together and to the AST. Every executable statement in a loop nest involved with a dependence has a level vector. <p> This functionality is needed because program analysis is necessarily conservative. Ped's user interface provides a mechanism that enables users to reject a particular dependence or a class of dependences that they feel are overly conservative <ref> [6, 16, 23, 32] </ref>. In response, Ped no longer considers these dependences during program transformations. Users can similarly correct overly conservative variable classifications by reclassifying shared variables as private. In response, Ped eliminates from further consideration any loop-carried dependences incident on these variables.
Reference: [33] <author> U. Kremer, H. Zima, H.-J. Bast, and M. Gerndt. </author> <title> Advanced tools and techniques for automatic parallelization. </title> <journal> Parallel Computing, </journal> <volume> 7 </volume> <pages> 387-393, </pages> <year> 1988. </year>
Reference-contexts: Superb provides a set of interactive program transformations, including transformations that exploit data parallelism. The user specifies a data partitioning, then node programs with the necessary send and receive operations are automatically generated. Algorithms are also described for incremental update of use-def and def-use chains following structured program transformations <ref> [33] </ref>. Tiny provides precise data dependence analysis and program transformations for a core subset of Fortran [49]. It is particularly adept at performing complex loop transformations on imperfectly nested loops. Extensions to Tiny include precise dependence tests, array kill analysis, and automatic selection of program transformation sequences [39].
Reference: [34] <author> D. Kuck, R. Kuhn, D. Padua, B. Leasure, and M. J. Wolfe. </author> <title> Dependence graphs and compiler optimizations. </title> <booktitle> In Conference Record of the Eighth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Williamsburg, VA, </address> <month> January </month> <year> 1981. </year>
Reference-contexts: Hence, the results may differ each time the program is executed. This kind of anomaly, often called a data race, precludes the parallelization of the above loop. In the literature of compilation for parallel execution, a potential data race is referred to as a loop-carried dependence <ref> [3, 34] </ref>. Without explicit synchronization, only loops with no carried dependences may be safely executed in parallel. Automatic parallelizers use this principle by constructing a dependence graph for the entire program and then parallelizing every loop that does not carry a dependence. <p> There are four types of data dependence <ref> [34] </ref>: True (flow) dependence occurs when S 1 writes a memory location that S 2 later reads. Anti dependence occurs when S 1 reads a memory location that S 2 later writes. Output dependence occurs when S 1 writes a memory location that S 2 later writes. <p> Ped supports a large set of transformations that have proven useful for introducing, discovering, and exploiting parallelism. Ped also supports transformations for enhancing the use of the memory hierarchy. These transformations are described in detail in the literature <ref> [2, 3, 10, 34, 36, 48] </ref>. We classify the transformations in Ped as follows. <p> It has been used extensively in vectorizing and parallelizing compilers to adjust the granularity of parallel loops and to expose parallelism <ref> [3, 34, 48] </ref>. Ped interchanges pairs of adjacent loops. Loop permutations may be performed as a series of pairwise interchanges. Ped supports interchange of triangular or skewed loops. It also interchanges complex loop nests that result after interchanging skewed loops. <p> Finally, the interchange flags are recalculated for dependences in the loop nest. 7.2 Loop Distribution Loop distribution separates independent statements inside a single loop into multiple loops with identical headers <ref> [3, 34] </ref>. It is used to expose partial parallelism by separating statements which may be parallelized from those that must be executed sequentially. Loop distribution is a cornerstone of vectorization and parallelization [3, 34]. In Ped the user can specify whether distribution is for the purpose of vectorization or parallelization. <p> 7.2 Loop Distribution Loop distribution separates independent statements inside a single loop into multiple loops with identical headers <ref> [3, 34] </ref>. It is used to expose partial parallelism by separating statements which may be parallelized from those that must be executed sequentially. Loop distribution is a cornerstone of vectorization and parallelization [3, 34]. In Ped the user can specify whether distribution is for the purpose of vectorization or parallelization. If the user specifies vectorization, then each statement is placed in a separate loop when possible. <p> The user may then apply or reject the distribution partition. Safety To maintain the meaning of the original loop, the partition must not put statements that are involved in recurrences into different loops <ref> [30, 34] </ref>. Recurrences are calculated by finding strongly connected regions in the subgraph composed of loop-independent dependences and dependences carried on the loop to be distributed. <p> The precursor to Ped, a dependence browser named Ptool, interactively displayed Pfc's dependences to users [25]. Ped integrates and extends Pfc's analysis & transformations and Ptool's browsing capabilities, making them available to the user in an interactive environment. Parafrase was the first automatic vectorizing and parallelizing compiler <ref> [34] </ref>. It supports program analysis and performs a large number of program transformations to improve parallelism. In Parafrase, program transformations are structured in phases and are always applied where applicable. Batch analysis is performed after each transformation phase to update the dependence information for the entire program.
Reference: [35] <author> B. Leasure, </author> <title> editor. PCF Fortran: Language Definition, version 3.1. </title> <booktitle> The Parallel Computing Forum, </booktitle> <address> Champaign, IL, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: 1 Introduction The ParaScope Editor is a tool designed to help skilled users interactively transform a sequential Fortran 77 program into a parallel program with explicit parallel constructs, such as those in PCF Fortran <ref> [35] </ref>. In a language like PCF Fortran, the principal mechanism for the introduction of parallelism is the parallel loop, which specifies that its iterations may be run in parallel according to any schedule. The fundamental problem introduced by such languages is the possibility of nondeterministic execution.
Reference: [36] <author> D. Loveman. </author> <title> Program improvement by source-to-source transformations. </title> <journal> Journal of the ACM, </journal> <volume> 17(2) </volume> <pages> 121-145, </pages> <month> January </month> <year> 1977. </year>
Reference-contexts: Ped supports a large set of transformations that have proven useful for introducing, discovering, and exploiting parallelism. Ped also supports transformations for enhancing the use of the memory hierarchy. These transformations are described in detail in the literature <ref> [2, 3, 10, 34, 36, 48] </ref>. We classify the transformations in Ped as follows.
Reference: [37] <author> K. S. McKinley. </author> <title> Automatic and Interactive Parallelization. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> April </month> <year> 1992. </year>
Reference-contexts: We are, as always, in the process of further extending Ped's transformation capabilities. An effort is underway to incorporate automatic parallelization strategies within Ped in order to provide users with further assistance in the parallelization process <ref> [23, 37] </ref>. 8 Edits This section describes strategies for updates after edits to the program text or direct modification of Ped's dependence information.
Reference: [38] <author> C. Polychronopoulos, M. Girkar, M. Haghighat, C. Lee, B. Leung, and D. Schouten. </author> <title> The structure of Parafrase-2: An advanced parallelizing compiler for C and Fortran. </title> <editor> In D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing. </booktitle> <publisher> The MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: In Parafrase, program transformations are structured in phases and are always applied where applicable. Batch analysis is performed after each transformation phase to update the dependence information for the entire program. Parafrase-2 adds scheduling, improved program analysis, and transformations <ref> [38] </ref>. More advanced interprocedural and symbolic analysis is planned [22]. Parafrase-2 uses Faust as a front end to provide interactive parallelization and graphical displays [21]. Page 22 Ptran is also an automatic parallelizer with extensive program analysis [1].
Reference: [39] <author> W. Pugh and D. Wonnacott. </author> <title> Eliminating false data dependences using the Omega test. </title> <booktitle> In Proceedings of the SIGPLAN '92 Conference on Program Language Design and Implementation, </booktitle> <address> San Francisco, CA, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: Tiny provides precise data dependence analysis and program transformations for a core subset of Fortran [49]. It is particularly adept at performing complex loop transformations on imperfectly nested loops. Extensions to Tiny include precise dependence tests, array kill analysis, and automatic selection of program transformation sequences <ref> [39] </ref>. Forge 90, formerly Mimdizer, is an interactive parallelization system for MIMD shared and distributed-memory machines from Applied Parallel Research [5]. It performs data-flow and dependence analyses, and also supports loop-level transformations. Associated tools graphically display call graph, control flow, dependence, and profiling information.
Reference: [40] <author> C. Rosene. </author> <title> Incremental Dependence Analysis. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> March </month> <year> 1990. </year>
Reference-contexts: Several algorithms for incremental analysis can be found in the literature; e.g., data-flow analysis [41], interprocedural analysis [8], interprocedural recompilation analysis [9], as well as dependence analysis <ref> [40] </ref>. However, few of these algorithms have been implemented and evaluated in an interactive environment. Rather than tackle all these problems at once, we chose a simple yet practical strategy for the current implementation of Ped. First, the scope of each program change is evaluated.
Reference: [41] <author> B. Ryder and M. Paull. </author> <title> Incremental data flow analysis algorithms. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 10(1) </volume> <pages> 1-50, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: In order to calculate precise dependence information, Ped may need to incrementally update the control flow, control dependence, SSA, and call graphs, as well as recalculate live range, constant, symbolic, interprocedural, and dependence testing information. Several algorithms for incremental analysis can be found in the literature; e.g., data-flow analysis <ref> [41] </ref>, interprocedural analysis [8], interprocedural recompilation analysis [9], as well as dependence analysis [40]. However, few of these algorithms have been implemented and evaluated in an interactive environment. Rather than tackle all these problems at once, we chose a simple yet practical strategy for the current implementation of Ped.
Reference: [42] <author> V. Sarkar and R. Thekkath. </author> <title> A general framework for interation-reordering loop transformations (technical summary). </title> <booktitle> In Proceedings of the SIGPLAN '92 Conference on Program Language Design and Implementation, </booktitle> <address> San Francisco, CA, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: Ptran introduces both task and loop parallelism, but currently the only other program transformations are variable privatization and loop distribution. Sarkar and Thekkath propose a unified framework for applying iteration-reordering transformations to perfect loop nests <ref> [42] </ref>. The framework provides rules for calculating legality and updating dependence vectors and loop bounds, but does not estimate profitability. Pips applies sophisticated interprocedural semantical analysis to parallelize programs for shared and distributed-memory machines [28].
Reference: [43] <author> B. Shei and D. Gannon. SIGMACS: </author> <title> A programmable programming environment. </title> <booktitle> In Advances in Languages and Compilers for Parallel Computing, </booktitle> <address> Irvine, CA, August 1990. </address> <publisher> The MIT Press. </publisher>
Reference-contexts: Sigmacs, a programmable interactive parallelizer in the Faust programming environment, computes and displays call graphs, process graphs, and a statement dependence graph <ref> [21, 43] </ref>. In a process graph each node represents a task or a process, which is a separate entity running in parallel. The call and process graphs may be animated dynamically at run time. Sigmacs also performs several interactive program transformations, and is working on automatic updating of dependence information.
Reference: [44] <author> J. Singh and J. Hennessy. </author> <title> An empirical investigation of the effectiveness of and limitations of automatic parallelization. </title> <booktitle> In Proceedings of the International Symposium on Shared Memory Multiprocessors, </booktitle> <address> Tokyo, Japan, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: When possible, this phase derives constants or simplified forms for loop bounds, loop steps, array subscript expressions, array dimensions, and control flow. Symbolic analysis significantly improves the precision of dependence testing <ref> [20, 22, 44] </ref> and interprocedural analysis [24]. Unlike automatic parallelization tools, Ped does not make transformations to the program for purposes of analysis. Accordingly, the results of symbolic analysis are stored in shadow expressions that represent Page 10 computations of interest.
Reference: [45] <author> K. Smith and W. Appelbe. </author> <title> PAT an interactive Fortran parallelizing assistant tool. </title> <booktitle> In Proceedings of the 1988 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1988. </year>
Reference-contexts: The call and process graphs may be animated dynamically at run time. Sigmacs also performs several interactive program transformations, and is working on automatic updating of dependence information. Pat is also an interactive parallelization tool <ref> [45] </ref>. Its dependence analysis is restricted to Fortran programs where only one write occurs to each variable in a loop. Pat supports replication and alignment, insertion and deletion of assignment statements, and loop parallelization. It can also insert synchronization to protect specific dependences.
Reference: [46] <author> K. Smith, W. Appelbe, and K. Stirewalt. </author> <title> Incremental dependence analysis for interactive parallelization. </title> <booktitle> In Proceedings of the 1990 ACM International Conference on Supercomputing, </booktitle> <address> Amsterdam, The Netherlands, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: It can also insert synchronization to protect specific dependences. Pat divides analysis into scalar and dependence phases, but does not perform symbolic or interprocedural analysis. The incremental dependence update that follows transformations is simplified due to its austere analysis <ref> [46] </ref>. Superb interactively converts sequential programs into data parallel SPMD programs that can be executed on the Suprenum distributed memory multiprocessor [50]. Superb provides a set of interactive program transformations, including transformations that exploit data parallelism.
Reference: [47] <author> M. E. Wolf and M. Lam. </author> <title> A loop transformation theory and an algorithm to maximize parallelism. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 452-471, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: It can be applied via unimodular methods using loop interchange, strip mining, and loop reversal to obtain loop-level parallelism in a loop nest <ref> [7, 47] </ref>. All of these transformations are supported in Ped. Loop skewing is applied to a pair of perfectly nested loops that both carry dependences, even after loop interchange.
Reference: [48] <author> M. J. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1989. </year>
Reference-contexts: Since several different values of ff and fi may satisfy the dependence equations, a set of distance and direction vectors may be needed to completely describe the dependences arising between a pair of array references. Direction vectors, introduced by Wolfe <ref> [48] </ref>, are useful for calculating loop-carried dependences. A dependence is carried by the outermost loop for which the element in the direction vector is not an `='. Additionally, direction vectors are used to determine the safety and profitability of loop interchange [3, 48]. <p> Direction vectors, introduced by Wolfe [48], are useful for calculating loop-carried dependences. A dependence is carried by the outermost loop for which the element in the direction vector is not an `='. Additionally, direction vectors are used to determine the safety and profitability of loop interchange <ref> [3, 48] </ref>. Distance vectors are more precise versions of direction vectors that specify the actual number of loop iterations between two accesses to the same memory location [3, 48]. <p> Additionally, direction vectors are used to determine the safety and profitability of loop interchange <ref> [3, 48] </ref>. Distance vectors are more precise versions of direction vectors that specify the actual number of loop iterations between two accesses to the same memory location [3, 48]. They are employed by transformations to exploit parallelism and the memory hierarchy. 3 Work Model Ped is designed to exploit loop-level parallelism, which comprises most of the usable parallelism in scientific codes when synchronization costs are considered [15]. <p> Ped supports a large set of transformations that have proven useful for introducing, discovering, and exploiting parallelism. Ped also supports transformations for enhancing the use of the memory hierarchy. These transformations are described in detail in the literature <ref> [2, 3, 10, 34, 36, 48] </ref>. We classify the transformations in Ped as follows. <p> The purpose, mechanics, and safety of these transformations are presented, followed by their profitability estimates, user advice, and incremental dependence update algorithms. 7.1 Loop Interchange Loop interchange is a key transformation that modifies the traversal order of the iteration space for the selected loop nest <ref> [3, 48] </ref>. It has been used extensively in vectorizing and parallelizing compilers to adjust the granularity of parallel loops and to expose parallelism [3, 34, 48]. Ped interchanges pairs of adjacent loops. Loop permutations may be performed as a series of pairwise interchanges. <p> It has been used extensively in vectorizing and parallelizing compilers to adjust the granularity of parallel loops and to expose parallelism <ref> [3, 34, 48] </ref>. Ped interchanges pairs of adjacent loops. Loop permutations may be performed as a series of pairwise interchanges. Ped supports interchange of triangular or skewed loops. It also interchanges complex loop nests that result after interchanging skewed loops. <p> Ped supports fusion of adjacent loop nests where the outer loops iterate identically, but the loop index variable names need not be the same. Safety Loop fusion is safe if it does not reverse the order of execution of the source and sink of any dependence <ref> [48] </ref>. If there are no dependences between two loops, fusion is always safe. If there is a dependence, it is loop-independent. To test if fusion is safe, dependence testing is performed on the loop bodies as if they were in a single loop. <p> This update strategy proved simple to implement and is quick in practice. 7.4 Loop Skewing Loop skewing is a transformation that changes the shape of the iteration space to expose parallelism across a wavefront <ref> [29, 48] </ref>. It can be applied via unimodular methods using loop interchange, strip mining, and loop reversal to obtain loop-level parallelism in a loop nest [7, 47]. All of these transformations are supported in Ped.
Reference: [49] <author> M. J. Wolfe. </author> <title> The Tiny loop restructuring research tool. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: Algorithms are also described for incremental update of use-def and def-use chains following structured program transformations [33]. Tiny provides precise data dependence analysis and program transformations for a core subset of Fortran <ref> [49] </ref>. It is particularly adept at performing complex loop transformations on imperfectly nested loops. Extensions to Tiny include precise dependence tests, array kill analysis, and automatic selection of program transformation sequences [39].
Reference: [50] <author> H. Zima, H.-J. Bast, and M. Gerndt. </author> <title> SUPERB: A tool for semi-automatic MIMD/SIMD parallelization. </title> <journal> Parallel Computing, </journal> <volume> 6 </volume> <pages> 1-18, </pages> <year> 1988. </year> <pages> Page 26 </pages>
Reference-contexts: The incremental dependence update that follows transformations is simplified due to its austere analysis [46]. Superb interactively converts sequential programs into data parallel SPMD programs that can be executed on the Suprenum distributed memory multiprocessor <ref> [50] </ref>. Superb provides a set of interactive program transformations, including transformations that exploit data parallelism. The user specifies a data partitioning, then node programs with the necessary send and receive operations are automatically generated.
References-found: 50

