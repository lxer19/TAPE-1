URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/khaigh/www/papers/khaigh98d.ps.gz
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/khaigh/www/papers/khaigh98d.abstract.html
Root-URL: 
Email: khaigh@cs.cmu.edu  mmv@cs.cmu.edu  
Title: Learning Situation-Dependent Costs: Using Execution to Refine Planning Models  
Author: Karen Zita Haigh Manuela M. Veloso 
Address: Pittsburgh PA 15213-3891  
Affiliation: Computer Science Department, Carnegie Mellon University,  
Web: http://www.cs.cmu.edu/~khaigh  http://www.cs.cmu.edu/~mmv  
Abstract: Physical environments are so complex that it is hard to hand-tune all of the domain knowledge, especially to model the dynamics of the environment. The work presented in this paper explores machine learning techniques to autonomously identify situations in the environment that affect plan quality. We introduce the concept of situation-dependent costs, where situational features can be attached to the costs used by the path planner. These costs effectively diagnose and predict situations the robot encounters so that the planner can generate paths that are appropriate for each situation. We present an implementation of our situation-dependent learning approach in a real robotic system, Rogue. Rogue learns situation-dependent costs for arcs in a topological map of the environment; these costs are then used by the path planner to predict and avoid failures. In this article, we present the representation of the path planner and the navigation modules, and describe the execution trace. We show how training data is extracted from the execution trace. We present experimental results from a simulated, controlled environment as well as from data collected from the actual robot. Our approach effectively refines models of dynamic systems and improves the ef ficiency of generated plans.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Baroglio, A. Giordana, M. Kaiser, M. Nuttin, and R. Pi-ola. </author> <title> Learning controllers for industrial robots. </title> <journal> Machine Learning, </journal> <volume> 23 </volume> <pages> 221-249, </pages> <year> 1996. </year>
Reference-contexts: It needs to learn. Learning has been applied to robotics problems in a variety of manners. Common applications include map learn ing and localization (e.g. [9; 10; 21]), or learning operational parameters for better actuator control (e.g. <ref> [1; 3; 15] </ref>). Instead of improving low-level actuator control, our work focusses at the planning stages of the system. A few other researchers have explored this area as well, learning costs of actions, or their applicability criteria (e.g. [8; 11; 17; 14; 20]).
Reference: [2] <author> R. A. Becker, J. M. Chambers, and A. R. Wilks. </author> <title> The New S Language. </title> <address> (Pacific Grove, CA: </address> <publisher> Wadsworth & Brooks/Cole), </publisher> <year> 1988. </year> <note> Code available from http://www.- mathsoft.com/splus/. </note>
Reference-contexts: The desired output is situation-dependent knowledge in a form that can be used by the planner. We selected regression trees [4] as our learning mechanism. Other learning mechanisms may be appropriate in different robot architectures with different data representations. We selected an off-the-shelf package, namely S-PLUS <ref> [2] </ref>, as the regression tree implementation. A regression tree is fitted for each arc using binary recursive partitioning where the data is successively split until data is too sparse or nodes are pure (below a preset deviance). Splits are selected to maximize the reduction in deviance of the node.
Reference: [3] <author> S. W. Bennett and G. F. DeJong. </author> <title> Real-world robotics: Learning to plan for robust execution. </title> <journal> Machine Learning, </journal> <volume> 23 </volume> <pages> 121-161, </pages> <year> 1996. </year>
Reference-contexts: It needs to learn. Learning has been applied to robotics problems in a variety of manners. Common applications include map learn ing and localization (e.g. [9; 10; 21]), or learning operational parameters for better actuator control (e.g. <ref> [1; 3; 15] </ref>). Instead of improving low-level actuator control, our work focusses at the planning stages of the system. A few other researchers have explored this area as well, learning costs of actions, or their applicability criteria (e.g. [8; 11; 17; 14; 20]).
Reference: [4] <author> L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. </author> <title> Classification and Regression Trees. </title> <address> (Pacific Grove, CA: </address> <publisher> Wadsworth & Brooks/Cole), </publisher> <year> 1984. </year>
Reference-contexts: The input to the algorithm is the events matrix described in Section 3.3. The desired output is situation-dependent knowledge in a form that can be used by the planner. We selected regression trees <ref> [4] </ref> as our learning mechanism. Other learning mechanisms may be appropriate in different robot architectures with different data representations. We selected an off-the-shelf package, namely S-PLUS [2], as the regression tree implementation.
Reference: [5] <author> J. M. Chambers and T. Hastie. </author> <title> Statistical models in S. </title> <address> (Pacific Grove, CA: </address> <publisher> Wadsworth & Brooks/Cole), </publisher> <year> 1992. </year>
Reference-contexts: Splits are selected to maximize the reduction in deviance of the node. Deviance of a node is calculated as D = P all examples i and predicted values y i within the node. Chambers & Hastie <ref> [5] </ref> discuss the method in more detail.
Reference: [6] <author> R. Goodwin. </author> <title> Meta-Level Control for Decision-Theoretic Planners. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <year> 1996. </year> <note> Available as Technical Report CMU-CS-96-186. </note>
Reference-contexts: Learning opportunities are extracted from the execution traces created by by the navigation module. Figure 2 shows how our algorithm fits into the framework of the Xavier architecture. The path planner uses a decision-theoretic A* algorithm that operates on a topological map with metric information <ref> [6] </ref>. Navigation is done using Partially Observable Markov Decision Process Models (POMDPs) [19]. Our learning algorithm affects the arc costs of the topological map so that the planner will select plans with a higher efficiency.
Reference: [7] <author> K. Z. Haigh. </author> <title> Learning Situation-Dependent Planning Knowledge from Uncertain Robot Execution Data. </title> <type> PhD thesis, </type> <institution> Computer Science Department, Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <month> February </month> <year> 1998. </year> <note> Available as technical report CMU-CS-98-108. </note>
Reference-contexts: We have been developing a robot architecture, Rogue, which aims at equipping a real robot with the ability to learn from its own execution experiences <ref> [7] </ref>. This paper presents the continuation of the project, in which learning has been incorporated. Rogue processes uncertain navigation data to create improved domain models for its planners, successfully abstracting numeric sensor information into symbolic planner information. <p> The algorithm to calculate this sequence is known as Viterbi's algorithm [16]. However, Viterbi's algorithm was not designed for use in Markov models with additional uncertainty variables, such as time or length. We developed an improvement to Viterbi's algorithm, called Multi/Markov Viterbi <ref> [7] </ref>, which heuristically improves the estimate of the robot's trajectory for Markov models with structural changes that capture additional uncertainty. <p> This fact complicates the reconstruction of the arc path because a single Markov sequence may map to multiple arc sequences. Selecting the correct one is a challenging problem that we address with a greedy heuristic, based on expectation times <ref> [7] </ref>. a) Markov Representation b) Planner Arc Representation 3.3 Creating a Uniform Output Once arc traversal events have been identified from the execution trace, updated costs need to be calculated. The cost evaluation function, C, yields an updated arc traversal weight. <p> Additional features from the execution trace can be trivially added; this matrix was recorded for the experiments described in Section 5, while sonar readings and other features were added for experiments involving the task planner <ref> [7] </ref>. 4 Learning Algorithm In this section we present the learning mechanism we use to create the mapping from situation features (F) and events (E ) to costs (C). The input to the algorithm is the events matrix described in Section 3.3. <p> This situation-dependent knowledge can be incorporated into the planning or monitoring efforts so that tasks can be achieved with greater reliability and efficiency. Haigh <ref> [7] </ref> describes an implementation of the approach for the robot's task planner. Situation-dependent features are an effective way to capture the changing nature of a real-world environment. Beyond using situation-dependent costs for planning, possible future research directions include customizing the environment and autonomously extracting features from the execution trace.
Reference: [8] <author> L. P. Kaelbling, M. L. Littman, and A. W. Moore. </author> <title> Reinforcement learning: A survey. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4 </volume> <pages> 237-285, </pages> <year> 1996. </year>
Reference-contexts: Instead of improving low-level actuator control, our work focusses at the planning stages of the system. A few other researchers have explored this area as well, learning costs of actions, or their applicability criteria (e.g. <ref> [8; 11; 17; 14; 20] </ref>). IMPROV [14] learns action descriptions, but its performance degrades dramatically with environmental noise. Clementine [11] and CSL [20] both learn sensor utilities, including which sensor to use for what information.
Reference: [Klingspor et al., 1996] <author> Volker Klingspor, Katharina J. Morik, and Anke D. </author> <title> Rieger (1996). Learning concepts from sensor data of a mobile robot. </title> <journal> Machine Learning, </journal> <volume> 23 </volume> <pages> 305-332. </pages>
Reference-contexts: The area of possible research with perhaps with the greatest potential for improving the performance of learning systems, is to automatically decide what features to add to the data set. Klingspor et al. <ref> [Klingspor et al., 1996] </ref> have already designed techniques for learning high-level feature concepts from low-level data. It remains an open research problem to automatically incorporate those features into learning.
Reference: [9] <author> S. Koenig and R. G. Simmons. </author> <title> Passive distance learning for robot navigation. </title> <booktitle> In Machine Learning: Proceedings of the Thirteenth International Conference (ICML96), </booktitle> <pages> pages 266-274, </pages> <year> 1996. </year>
Reference-contexts: To be truly autonomous, the robot needs to be able to use accumulated experience and feedback about its performance to improve its behaviour. It needs to learn. Learning has been applied to robotics problems in a variety of manners. Common applications include map learn ing and localization (e.g. <ref> [9; 10; 21] </ref>), or learning operational parameters for better actuator control (e.g. [1; 3; 15]). Instead of improving low-level actuator control, our work focusses at the planning stages of the system.
Reference: [10] <author> D. Kortenkamp and T. Weymouth. </author> <title> Topological mapping for mobile robots using a combination of sonar and vision sensing. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI-94), </booktitle> <pages> pages 979-984, </pages> <year> 1994. </year>
Reference-contexts: To be truly autonomous, the robot needs to be able to use accumulated experience and feedback about its performance to improve its behaviour. It needs to learn. Learning has been applied to robotics problems in a variety of manners. Common applications include map learn ing and localization (e.g. <ref> [9; 10; 21] </ref>), or learning operational parameters for better actuator control (e.g. [1; 3; 15]). Instead of improving low-level actuator control, our work focusses at the planning stages of the system.
Reference: [11] <author> J. Lindner, R. R. Murphy, and E. Nitz. </author> <title> Learning the expected utility of sensors and algorithms. </title> <booktitle> In IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems, </booktitle> <pages> pages 583-590, </pages> <year> 1994. </year>
Reference-contexts: Instead of improving low-level actuator control, our work focusses at the planning stages of the system. A few other researchers have explored this area as well, learning costs of actions, or their applicability criteria (e.g. <ref> [8; 11; 17; 14; 20] </ref>). IMPROV [14] learns action descriptions, but its performance degrades dramatically with environmental noise. Clementine [11] and CSL [20] both learn sensor utilities, including which sensor to use for what information. <p> A few other researchers have explored this area as well, learning costs of actions, or their applicability criteria (e.g. [8; 11; 17; 14; 20]). IMPROV [14] learns action descriptions, but its performance degrades dramatically with environmental noise. Clementine <ref> [11] </ref> and CSL [20] both learn sensor utilities, including which sensor to use for what information. LIVE [17] learns a model of the environment, as well as the costs of applying actions in that environment.
Reference: [12] <author> T. M. Mitchell. </author> <title> Machine Learning. </title> <address> (New York, NY: Mc-Graw Hill), </address> <year> 1997. </year>
Reference: [13] <author> J. O'Sullivan, K. Z. Haigh, and G. D. Armstrong. </author> <type> Xavier. </type> <institution> Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <month> April </month> <year> 1997. </year> <note> Manual, Version 0.3, unpublished internal report. Available via http://www.cs.cmu.edu/~Xavier/. </note>
Reference-contexts: Most of these "problems" model the actual behaviour of the robot, allowing code developed on the simulator to run successfully on the robot with no modification <ref> [13] </ref>. The simulator allows us to tightly control the experiments to ensure that the learning algorithm is indeed learning appropriate situation-dependent costs. an exposition of the variety one might see at a conference. obstacles (dark boxes in corridors).
Reference: [14] <author> D. J. Pearson. </author> <title> Learning Procedural Planning Knowledge in Complex Environments. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering and Computer Science, University of Michigan, </institution> <address> Ann Arbor, MI, </address> <year> 1996. </year> <note> Available as Technical Report CSE-TR-309-96. </note>
Reference-contexts: Instead of improving low-level actuator control, our work focusses at the planning stages of the system. A few other researchers have explored this area as well, learning costs of actions, or their applicability criteria (e.g. <ref> [8; 11; 17; 14; 20] </ref>). IMPROV [14] learns action descriptions, but its performance degrades dramatically with environmental noise. Clementine [11] and CSL [20] both learn sensor utilities, including which sensor to use for what information. <p> Instead of improving low-level actuator control, our work focusses at the planning stages of the system. A few other researchers have explored this area as well, learning costs of actions, or their applicability criteria (e.g. [8; 11; 17; 14; 20]). IMPROV <ref> [14] </ref> learns action descriptions, but its performance degrades dramatically with environmental noise. Clementine [11] and CSL [20] both learn sensor utilities, including which sensor to use for what information. LIVE [17] learns a model of the environment, as well as the costs of applying actions in that environment.
Reference: [15] <author> D. A. Pomerleau. </author> <title> Neural network perception for mobile robot guidance. </title> <address> (Dordrecht, Netherlands: </address> <publisher> Kluwer Academic), </publisher> <year> 1993. </year>
Reference-contexts: It needs to learn. Learning has been applied to robotics problems in a variety of manners. Common applications include map learn ing and localization (e.g. [9; 10; 21]), or learning operational parameters for better actuator control (e.g. <ref> [1; 3; 15] </ref>). Instead of improving low-level actuator control, our work focusses at the planning stages of the system. A few other researchers have explored this area as well, learning costs of actions, or their applicability criteria (e.g. [8; 11; 17; 14; 20]).
Reference: [16] <author> L. R. Rabiner and B. H. Juang. </author> <title> An introduction to hidden Markov models. </title> <journal> IEEE ASSP Magazine, </journal> <pages> pages 4-16, </pages> <month> January </month> <year> 1986. </year>
Reference-contexts: However, the action sequence stored in the trace, together with the probability distribution, can be used to calculate the most likely sequence of Markov states that the robot passed through. The algorithm to calculate this sequence is known as Viterbi's algorithm <ref> [16] </ref>. However, Viterbi's algorithm was not designed for use in Markov models with additional uncertainty variables, such as time or length.
Reference: [17] <author> W.-M. Shen. </author> <title> Autonomous Learning from the Environment. </title> <address> (New York, NY: </address> <publisher> Computer Science Press), </publisher> <year> 1994. </year>
Reference-contexts: Instead of improving low-level actuator control, our work focusses at the planning stages of the system. A few other researchers have explored this area as well, learning costs of actions, or their applicability criteria (e.g. <ref> [8; 11; 17; 14; 20] </ref>). IMPROV [14] learns action descriptions, but its performance degrades dramatically with environmental noise. Clementine [11] and CSL [20] both learn sensor utilities, including which sensor to use for what information. <p> IMPROV [14] learns action descriptions, but its performance degrades dramatically with environmental noise. Clementine [11] and CSL [20] both learn sensor utilities, including which sensor to use for what information. LIVE <ref> [17] </ref> learns a model of the environment, as well as the costs of applying actions in that environment. In some situations, it is enough to learn that a particular action has a certain average probability or cost. However, there are times when actions may have different costs under different situations.
Reference: [18] <author> R. Simmons, R. Goodwin, K. Z. Haigh, S. Koenig, and J. O'Sullivan. </author> <title> A layered architecture for office delivery robots. </title> <booktitle> In Proceedings of the First International Conference on Autonomous Agents, </booktitle> <pages> pages 245-252, </pages> <year> 1997. </year>
Reference-contexts: These steps are summarized in Table 1. Learning occurs incrementally and off-line; each time a plan is executed, new data is collected and added to previous data, and then all data is used for creating a new set of situation-dependent rules. Rogue uses the Xavier robot <ref> [18] </ref> as its learning platform (see Figure 1). Knowledge in the path planner is represented as a topological map of the environment in which the robot navigates. The map is a graph with nodes and arcs representing office rooms, corridors, doors and lobbies. <p> In Section 5 we present some experimental results. Finally, in Section 6 we summarize the main points of the paper. 2 Architecture & Representation Xavier is a mobile robot being developed at CMU <ref> [18] </ref>. It is built on an RWI B24 base and includes bump sensors, a laser light striper, sonars, a color camera and a speech board.
Reference: [19] <author> R. Simmons and S. Koenig. </author> <title> Probabilistic robot navigation in partially observable environments. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI-95), </booktitle> <pages> pages 1080-1087, </pages> <year> 1995. </year>
Reference-contexts: Figure 2 shows how our algorithm fits into the framework of the Xavier architecture. The path planner uses a decision-theoretic A* algorithm that operates on a topological map with metric information [6]. Navigation is done using Partially Observable Markov Decision Process Models (POMDPs) <ref> [19] </ref>. Our learning algorithm affects the arc costs of the topological map so that the planner will select plans with a higher efficiency. The challenge is to create variable costs depending on high-level features and to extract this information automatically from the robot's execution data.
Reference: [20] <author> M. Tan. </author> <title> Cost-sensitive robot learning. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pitts-burgh, PA, </address> <year> 1991. </year> <note> Available as Technical Report CMU-CS-91-134. </note>
Reference-contexts: Instead of improving low-level actuator control, our work focusses at the planning stages of the system. A few other researchers have explored this area as well, learning costs of actions, or their applicability criteria (e.g. <ref> [8; 11; 17; 14; 20] </ref>). IMPROV [14] learns action descriptions, but its performance degrades dramatically with environmental noise. Clementine [11] and CSL [20] both learn sensor utilities, including which sensor to use for what information. <p> A few other researchers have explored this area as well, learning costs of actions, or their applicability criteria (e.g. [8; 11; 17; 14; 20]). IMPROV [14] learns action descriptions, but its performance degrades dramatically with environmental noise. Clementine [11] and CSL <ref> [20] </ref> both learn sensor utilities, including which sensor to use for what information. LIVE [17] learns a model of the environment, as well as the costs of applying actions in that environment.
Reference: [21] <author> S. Thrun. </author> <title> A Bayesian approach to landmark discovery and active perception for mobile robot navigation. </title> <type> Technical Report CMU-CS-96-122, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <year> 1996. </year>
Reference-contexts: To be truly autonomous, the robot needs to be able to use accumulated experience and feedback about its performance to improve its behaviour. It needs to learn. Learning has been applied to robotics problems in a variety of manners. Common applications include map learn ing and localization (e.g. <ref> [9; 10; 21] </ref>), or learning operational parameters for better actuator control (e.g. [1; 3; 15]). Instead of improving low-level actuator control, our work focusses at the planning stages of the system.
References-found: 22

