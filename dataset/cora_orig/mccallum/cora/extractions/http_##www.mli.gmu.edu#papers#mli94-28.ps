URL: http://www.mli.gmu.edu/papers/mli94-28.ps
Refering-URL: http://www.mli.gmu.edu/kpubs.html
Root-URL: 
Email: -michalski, iimam-@aic.gmu.edu  
Title: Learning Problem-Oriented Decision Structures from Decision Rules: The AQDT-2 System  
Author: Ryszard S. Michalski and Ibrahim F. Imam 
Address: Fairfax, VA. 22030  
Affiliation: Center for Artificial Intelligence George Mason University  
Abstract: A decision structure is an acyclic graph that specifies an order of tests to be applied to an object (or a situation) to arrive at a decision about that object. and serves as a simple and powerful tool for organizing a decision process. This paper proposes a methodology for learning decision structures that are oriented toward specific decision making situations. The methodology consists of two phases: 1determining and storing declarative rules describing the decision process, 2deriving online a decision structure from the rules. The first step is performed by an expert or by an AQ-based inductive learning program that learns decision rules from examples of decisions (AQ15 or AQ17). The second step transforms the decision rules to a decision structure that is most suitable for the given decision making situation. The system, AQDT-2, implementing the second step, has been applied to a problem in construction engineering. In the experiments, AQDT-2 outperformed all other programs applied to the same problem in terms of the accuracy and the simplicity of the generated decision structures. Key words: machine learning, inductive learning, decision structures, decision rules, attribute selection.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Arciszewski, T, Bloedorn, E., Michalski, R., Mustafa, M., and Wnek, J., </author> <title> "Constructive Induction in Structural Design", Report of Machine Learning and Inference Labratory, </title> <institution> MLI-92-7, Center for AI, George Mason Un., </institution> <year> 1992. </year>
Reference-contexts: New features of AQDT-2 are demonstrated in an experiment on determining a decision structure for choosing wind 3 bracings for tall buildings <ref> [1] </ref>. The results briefly illustrate how the system tailors decision structures to different decision making situations. 2 The AQDT-2 Method This section describes the AQDT-2 method for building a decision structure from decision rules.
Reference: 2. <author> Bloedorn, E., Wnek, J., Michalski, R.S., and Kaufman, K. , "AQ17: </author> <title> A Multistrategy Learning System: The Method and Users Guide, Report of Machine Learning and Inference Labratory, </title> <institution> MLI-93-12, Center for AI, George Mason Un. </institution> <year> 1993. </year>
Reference-contexts: This paper presents a new version of the system, called AQDT-2. The new system generates a goal-oriented decision structure from decision rules learned by either rule learning system AQ15 [10] or system AQ17, which has extensive constructive induction capabilities <ref> [2] </ref>.
Reference: 3. <author> Bohanec, M. and Bratko, I., </author> <title> Trading Accuracy for Simplicity in Decision Trees, </title> <journal> Machine Learning Journal, </journal> <volume> Vol. 15, No. 3, </volume> <publisher> Kluwer Academic Publishers, </publisher> <year> 1994. </year> <month> 11 </month>
Reference-contexts: less t-weight, and it has a predictive accuracy of 88%. x6 x1 C3 Complexity No. of nodes: 3 No. of leaves: 5 C4 C2 x5 C1 C2 3.5 Decision Structure Generalization Seeking a general decision in some decision making situations is often desired, however it may influence the predictive accuracy <ref> [3] </ref>. AQDT-2 generalizes the decision structure, during the process of generating the decision structure, after selecting an attribute to be a node in the structure, and before splitting the rules into subsets each corresponds to one of its values.
Reference: 4. <author> Breiman, L., Friedman, J.H., Olshen, R.A. & Stone, C.J., </author> <title> Classification and Regression Structures, </title> <type> Belmont, </type> <institution> California: Wadsworth Int. Group, </institution> <year> 1984. </year>
Reference-contexts: The essential characteristic of any such method is the attribute selection criterion used for choosing attributes to be assigned to the nodes of the decision tree being built. Such criteria include the entropy reduction [12, 13], the gini index of diversity <ref> [4] </ref>, and others (e.g., 5, 6, 11). A decision tree/decision structure can be an effective tool for describing a decision process, as long as all the required tests can be measured, and the decision making 2 situations it was designed for remain constant.
Reference: 5. <author> Cestnik, B. & Bratko, I. </author> , <title> On Estimating Probabilities in Structure Pruning, </title> <booktitle> Proceeding of EWSL 91, </booktitle> <pages> (pp. 138-150) Porto, </pages> <address> Portugal, March 6-8, </address> <year> 1991. </year>
Reference: 6. <author> Cestnik, B. & Karalic, A., </author> <title> The Estimation of Probabilities in Attribute Selection Measures for Decision Structure Induction in Proceeding of the European Summer School on Machine Learning, </title> <address> July 22-31, Priory Corsendonk, Belgium, </address> <year> 1991. </year>
Reference: 7. <author> Imam, I.F. and Michalski, </author> <title> R.S. , "Learning Decision Structures from Decision Rules: A method and initial results from a comparative study", </title> <journal> in Journal of Intelligent Information Systems JIIS, </journal> <volume> Vol. 2, No. 3, </volume> <pages> pp. 279-304, </pages> <editor> Kerschberg, L., Ras, Z., & Zemankova, M. (Eds.), </editor> <publisher> Kluwer Academic Pub., </publisher> <address> MA, </address> <year> 1993. </year>
Reference-contexts: This approach allows one to generate a decision structure that avoids evaluating an attribute that is difficult to measure or delay its evaluation. Initial ideas on this approach, and the first system implementing it, AQDT-1, have been described in <ref> [7] </ref>. This paper presents a new version of the system, called AQDT-2. The new system generates a goal-oriented decision structure from decision rules learned by either rule learning system AQ15 [10] or system AQ17, which has extensive constructive induction capabilities [2]. <p> In AQDT-2, a probabilistic class assignment is used only when there is no alternative attribute to chose. 3.4 Decision Structure Pruning 9 Having noisy rules can affect the decision making process negatively. To prune the noisy rules a proposed method followed ideas introduced in earlier work <ref> [10, 7] </ref> where decision rules of small strength are pruned (e.g. rules which cover very few examples). The default setting of AQDT-2 prunes decision rules with strength of 3% or less of the total number of t-weight for the given decision class. <p> Conclusion The system AQDT-2 determines problem-oriented decision structures from decision rules generated by an AQ-type inductive learning program. The system is quite efficient, because it is easier to generate a decision structure tailored to any given decision making situation from rules than to modify a decision structure once created <ref> [7] </ref>. The method uses an attribute ranking criterion composed of four elementary criteria: the disjointness, the importance, the value distribution, and the dominance of an attribute in the decision rules.
Reference: 8. <editor> Imam, I.F., Michalski, R.S. and Kerschberg, L., </editor> <title> Discovering Attribute Dependence in Databases by Integrating Symbolic Learning and Statistical Analysis Techniques", </title> <booktitle> Proceeding of the First International Workshop on Knowledge Discovery in Database, </booktitle> <address> Washington, D.C., </address> <month> July, </month> <pages> 11-12, </pages> <year> 1993. </year>
Reference-contexts: Selecting a test with the maximum possible disjointness produces a node of the decision structure whose children can be immediately assigned decision classes. Importance. The second elementary criterion, the importance of a test, is based on the importance score (IS), introduced in <ref> [8] </ref>. In the obtained rules, each test is assigned a score that represents the total number of training examples, which are covered by the rules involving this test. Decision rules learned by an AQ learning program are accompanied with information on their strength.
Reference: 9. <author> Michalski, </author> <title> R.S. , AQVAL/1-Computer Implementation of a Variable-Valued Logic System VL1 and Examples of its Application to Pattern Recognition, </title> <booktitle> Proceeding of the First International Joint Conference on Pattern Recognition, </booktitle> <pages> (pp. 3-17), </pages> <address> Washington, DC, </address> <month> October 30- November 1, </month> <year> 1973. </year>
Reference-contexts: For example, the condition part [x3=1 v 3]&[x4=1] is multiplied out to two rules with condition parts [x3=1]&[x4=1] and [x3=3]&[x4=1]. The above criteria can be combined into one general test ranking measure using the lexicographic evaluation functional with tolerances (LEF) <ref> [9] </ref>. LEF combines two or more elementary criteria by evaluating them one by one (in the order defined by LEF) on the given set of tests. A test passes to the next criterion only if it scores on the 5 previous criterion within the range defined by the tolerance.
Reference: 10. <author> Michalski, R.S., Mozetic, I., Hong, J. & Lavrac, N., </author> <title> The MultiPurpose Incremental Learning System AQ15 and Its Testing Application to Three Medical Domains, </title> <booktitle> Proceedings of AAAI-86, </booktitle> <pages> (pp. 1041-1045), </pages> <address> Philadelphia, PA, </address> <year> 1986. </year>
Reference-contexts: Initial ideas on this approach, and the first system implementing it, AQDT-1, have been described in [7]. This paper presents a new version of the system, called AQDT-2. The new system generates a goal-oriented decision structure from decision rules learned by either rule learning system AQ15 <ref> [10] </ref> or system AQ17, which has extensive constructive induction capabilities [2]. <p> In AQDT-2, a probabilistic class assignment is used only when there is no alternative attribute to chose. 3.4 Decision Structure Pruning 9 Having noisy rules can affect the decision making process negatively. To prune the noisy rules a proposed method followed ideas introduced in earlier work <ref> [10, 7] </ref> where decision rules of small strength are pruned (e.g. rules which cover very few examples). The default setting of AQDT-2 prunes decision rules with strength of 3% or less of the total number of t-weight for the given decision class.
Reference: 11. <author> Mingers, J., </author> <title> An Empirical Comparison of selection Measures for Decision-Structure Induction, </title> <journal> Machine Learning , Vol. </journal> <volume> 3, No. 3, </volume> <pages> (pp. 319-342), </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1989a. </year>
Reference: 12. <author> Quinlan, J.R., </author> <title> Discovering Rules By Induction from Large Collections of Examples, </title> <editor> in D. Michie (Edr), </editor> <booktitle> Expert Systems in the Microelectronic Age , Edinburgh University Press, </booktitle> <year> 1979. </year>
Reference-contexts: Decision trees are typically generated from a set of examples of decisions. The essential characteristic of any such method is the attribute selection criterion used for choosing attributes to be assigned to the nodes of the decision tree being built. Such criteria include the entropy reduction <ref> [12, 13] </ref>, the gini index of diversity [4], and others (e.g., 5, 6, 11). A decision tree/decision structure can be an effective tool for describing a decision process, as long as all the required tests can be measured, and the decision making 2 situations it was designed for remain constant.
Reference: 13. <author> Quinlan, J.R., </author> <title> Learning efficient classification procedures and their application to chess end games in R.S. </title> <editor> Michalski, J.G. Carbonell and T.M. Mitchell, (Eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach. </booktitle> <address> Los Altos: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1983. </year>
Reference-contexts: Decision trees are typically generated from a set of examples of decisions. The essential characteristic of any such method is the attribute selection criterion used for choosing attributes to be assigned to the nodes of the decision tree being built. Such criteria include the entropy reduction <ref> [12, 13] </ref>, the gini index of diversity [4], and others (e.g., 5, 6, 11). A decision tree/decision structure can be an effective tool for describing a decision process, as long as all the required tests can be measured, and the decision making 2 situations it was designed for remain constant.

References-found: 13

