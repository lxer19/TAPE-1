URL: http://www.cs.washington.edu/research/jair/volume7/ihrig97a.ps
Refering-URL: http://www.cs.washington.edu/research/jair/abstracts/ihrig97a.html
Root-URL: 
Email: ihrig@asu.edu  rao@asu.edu  
Title: Storing and Indexing Plan Derivations through Explanation-based Analysis of Retrieval Failures  
Author: Laurie H. Ihrig Subbarao Kambhampati 
Address: Tempe, AZ 85287-5406  
Affiliation: Department of Computer Science and Engineering Arizona State University  
Note: Journal of Artificial Intelligence Research 7 (1997) 161-198 Submitted 5/97; published 11/97  
Abstract: Case-Based Planning (CBP) provides a way of scaling up domain-independent planning to solve large problems in complex domains. It replaces the detailed and lengthy search for a solution with the retrieval and adaptation of previous planning experiences. In general, CBP has been demonstrated to improve performance over generative (from-scratch) planning. However, the performance improvements it provides are dependent on adequate judgements as to problem similarity. In particular, although CBP may substantially reduce planning effort overall, it is subject to a mis-retrieval problem. The success of CBP depends on these retrieval errors being relatively rare. This paper describes the design and implementation of a replay framework for the case-based planner dersnlp+ebl. der-snlp+ebl extends current CBP methodology by incorporating explanation-based learning techniques that allow it to explain and learn from the retrieval failures it encounters. These techniques are used to refine judgements about case similarity in response to feedback when a wrong decision has been made. The same failure analysis is used in building the case library, through the addition of repairing cases. Large problems are split and stored as single goal subproblems. Multi-goal problems are stored only when these smaller cases fail to be merged into a full solution. An empirical evaluation of this approach demonstrates the advantage of learning from experienced retrieval failure.
Abstract-found: 1
Intro-found: 1
Reference: <author> Barletta, R., & Mark, W. </author> <year> (1988). </year> <title> Explanation-based indexing of cases. </title> <booktitle> In Proceedings AAAI-88. </booktitle>
Reference-contexts: Ultimately, we would like to retain in the library a minimum number of cases such that all new problems are solved through the efficient retrieval and adaptation of the cases that are stored (Smyth & Keane, 1995). However, in complex domains, the planner's theory of problem similarity is incomplete <ref> (Barletta & Mark, 1988) </ref>. It does not have information about all of the relevant features of a new situation which determine if a stored case will be applicable. Sometimes a new problem will contain extra goals and/or changed initial state conditions.
Reference: <author> Barrett, A., & Weld, D. </author> <year> (1994). </year> <title> Partial order planning: evaluating possible efficiency gains. </title> <journal> Artificial Intelligence, </journal> <volume> 67, </volume> <pages> 71-112. </pages>
Reference-contexts: However, goals are not always trivially serializable for the plan-space planner (Veloso & Blythe, 1994). For example, consider the 2 D m S 1 domain <ref> (Barrett & Weld, 1994) </ref> shown in Figure 8. <p> Replay performance was tested both with and without case failure information. 2.5.1 Domains Experiments were run on problems drawn from two domains. The first was the artificial domain, 2 D m S 1 , originally described in <ref> (Barrett & Weld, 1994) </ref> and shown in Figure 8. Testing was done on problems which were randomly generated from this domain with the restriction that they always contain the goal g ff . The Logistics Transportation domain of (Veloso, 1994) was adopted for the second set of experiments.
Reference: <author> Bergmann, R., & Wilke, W. </author> <year> (1995). </year> <title> Building and refining abstract planning cases by change of representation language.. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 3, </volume> <pages> 53-118. </pages>
Reference-contexts: Like priar (Kambhampati & Hendler, 1992) and spa (Hanks & Weld, 1995), it is implemented on a partial-order planner. In this aspect it differs from state-space systems such as prodigy/analogy (Veloso & Carbonell, 1993a; Veloso, 1994) and paris <ref> (Bergmann & Wilke, 1995) </ref>. Like prodigy/analogy, it employs the case adaptation strategy, derivational replay which stores planning experience in the form of successful plan derivations. Previous decisions made in earlier planning episodes become instructions to guide the search process in solving the new problem. <p> This strategy differs from earlier approaches such as priar (Kambhampati & Hendler, 1992), prodigy/analogy (Veloso, 1994), paris <ref> (Bergmann & Wilke, 1995) </ref>, and caplan (Munoz-Avilla & Weberskirch, 1996), in that the division into goal subsets is not based on the structure of the final plan alone, but on the sequence of events making up the problem-solving episode.
Reference: <author> Carbonell, J. </author> <year> (1983). </year> <title> Learning by analogy: Formulating and generalizing plans from past experience. </title> <editor> In Michalski, R., Carbonell, J., & Mitchell, T. (Eds.), </editor> <booktitle> Machine Learning: an Artificial Intelligence approach, </booktitle> <volume> Vol. </volume> <pages> 1. </pages> <address> Palo Alto, CA: </address> <publisher> Tioga Press. </publisher>
Reference: <author> DeJong, G., & Mooney, R. </author> <year> (1986). </year> <title> Explanation-based learning: An alternative view. </title> <journal> Machine Learning, </journal> <volume> 1 (2), </volume> <pages> 145-176. </pages>
Reference: <author> Doorenbos, R. </author> <year> (1995). </year> <title> Production Matching for Large Learning Systems. </title> <type> Ph.D. thesis, </type> <institution> Computer Science Department, Carnegie Mellon University. </institution>
Reference-contexts: It is also possible to improve the performance of rule-based EBL by reducing the number of rules through the use of utility monitoring strategies (Gratch & DeJong, 1992), or by using a more sophisticated match algorithm <ref> (Doorenbos, 1995) </ref>. For example, Doorenbos (1995) employs an improved rule matcher based on the Rete algorithm. dersnlp+ebl, on the other hand, aims at alleviating the utility problem by reducing the number of times rules are matched. Similar to rule-based EBL, its learning component is employed to generate rules.
Reference: <author> Fikes, R., & Nilsson, N. </author> <year> (1971). </year> <title> A new approach to the application of theorem proving to problem solving. </title> <journal> Artificial Intelligence, </journal> <volume> 2, </volume> <pages> 189-208. </pages>
Reference-contexts: A planning problem is a 3-tuple hI; G; Ai, where I is a complete description of the initial state, G is the description of the goal state, and A is the set of operators in strips representation <ref> (Fikes & Nilsson, 1971) </ref>.
Reference: <author> Fox, S., & Leake, D. </author> <year> (1995). </year> <title> Using introspective reasoning to refine indexing. </title> <booktitle> In Proceedings IJCAI-95. 195 Ihrig & Kambhampati Francis, </booktitle> <editor> A., & Ram, S. </editor> <year> (1995). </year> <title> A comparative utility analysis of case-based reasoning and control-rule learning systems. </title> <booktitle> In Proceedings of the 8th European Conference on Machine Learning, </booktitle> <address> ECML-95. </address>
Reference: <author> Friedland, P., & Iwasaki, Y. </author> <year> (1985). </year> <title> The concept and implementation of skeletal plans. </title> <journal> Journal of Automated Reasoning, </journal> <volume> 1 (2), </volume> <pages> 161-208. </pages>
Reference-contexts: cases, the retrieval of multiple cases in preparation for solving a new problem, and finally, a replay mechanism by which the planner employs the retrieved plan derivations as a sequence of instructions to guide the new search process. dersnlp+ebl's methodology depends on aspects that it has in common with molgen <ref> (Friedland & Iwasaki, 1985) </ref> and priar (Kambhampati & Hendler, 1992). It requires an eager case adaptation strategy, so that a skeletal plan will be constructed which contains all of the constraints added on the advice of the retrieved cases, and only those constraints.
Reference: <author> Gratch, J., & DeJong, G. </author> <year> (1992). </year> <title> Composer: A probabilistic solution to the utility problem in speed-up learning. </title> <booktitle> In Proceedings AAAI-92. </booktitle>
Reference-contexts: It is also possible to improve the performance of rule-based EBL by reducing the number of rules through the use of utility monitoring strategies <ref> (Gratch & DeJong, 1992) </ref>, or by using a more sophisticated match algorithm (Doorenbos, 1995). For example, Doorenbos (1995) employs an improved rule matcher based on the Rete algorithm. dersnlp+ebl, on the other hand, aims at alleviating the utility problem by reducing the number of times rules are matched.
Reference: <author> Hammond, K. </author> <year> (1990). </year> <title> Explaining and repairing plans that fail. </title> <journal> Artificial Intelligence, </journal> <volume> 45, </volume> <pages> 173-228. </pages>
Reference-contexts: Failure explanations are automatically generated from the search process which is used in extending the case to the new problem-solving situation. These are used in building the case library through the addition of repairing cases. Although earlier systems such as chef <ref> (Hammond, 1990) </ref> have exploited EBL techniques, their use is restricted to reasoning about the correctness of the plans generated by the case-based planner. In contrast, dersnlp+ebl starts with a sound and complete plan synthesis strategy. <p> Retrieval failures are treated as an opportunity by which the planner stores a new repairing case. In this aspect it is similar to Hammond's chef <ref> (Hammond, 1990) </ref> which also learns to improve its retrieval strategy based on failures. Despite this surface similarity, there are important differences in our 191 Ihrig & Kambhampati complished by an underlying generative planner. approach. dersnlp+ebl learns from case extension failures, whereas chef concentrates on learning from execution failures.
Reference: <author> Hanks, S., & Weld, D. </author> <year> (1995). </year> <title> A domain-independent algorithm for plan adaptation. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2, </volume> <pages> 319-360. </pages>
Reference-contexts: If the planner cannot predict ahead of time that these previous choices are wrong for the current situation, it will experience a retrieval error. In this paper, we introduce dersnlp+ebl (DERivational Systematic NonLinear Planner + Explanation-Based Learning), a CBP system which like priar (Kambhampati & Hendler, 1992) and spa <ref> (Hanks & Weld, 1995) </ref> is based on a sound and complete domain-independent planner. dersnlp+ebl deals with the mis-retrieval problem by allowing the planner to learn from its planning failures so that it may anticipate future errors. <p> Section 5 relates our work to previous case-based planners, including chef and prodigy/analogy. Section 6 provides a summary. 2. Learning from Case Failure As stated earlier, dersnlp+ebl is based on a complete and correct domain-independent planning strategy. Like priar (Kambhampati & Hendler, 1992) and spa <ref> (Hanks & Weld, 1995) </ref>, it is implemented on a partial-order planner. In this aspect it differs from state-space systems such as prodigy/analogy (Veloso & Carbonell, 1993a; Veloso, 1994) and paris (Bergmann & Wilke, 1995).
Reference: <author> Hendler, J., Stoffel, K., & Mulvehill, A. </author> <year> (1996). </year> <title> High performance support for case-based planning applications. In Technological Achievements of the Arpa/Rome Laboratory Planning Initiative: Advanced Planning Technology. </title> <publisher> AAAI Press. </publisher>
Reference-contexts: For example, mpa (Ram & Francis, 1996) is built around a retrieval engine which performs asynchronous memory retrieval. caper <ref> (Hendler et al., 1996) </ref> uses a structure matching algorithm which parallelizes the process by which the plan's success conditions represented as a retrieval probe are matched with a large knowledge base of world facts.
Reference: <author> Ihrig, L., & Kambhampati, S. </author> <year> (1994a). </year> <title> Derivation replay for partial-order planning. </title> <booktitle> In Proceedings AAAI-94. </booktitle>
Reference-contexts: The skeletal plan is then further refined to achieve any goals left open. Previous work has demonstrated the effectiveness of this approach to plan-space replay as well as its advantage over state-space replay <ref> (Ihrig & Kambhampati, 1994a, 1994b) </ref>. Eager case adaptation can also be described as extension-first. The skeletal plan is first extended in the search for a solution, and, only after this extension fails, is the plan backtracked over, discarding the plan constraints which were added on the advice of previous episodes. <p> Replay may have an advantage in multi-case reuse since it allows the planner to readily merge small subplans to solve large problems. dersnlp can be contrasted to prodigy/analogy in that it employs a case fitting methodology called eager derivation replay <ref> (Ihrig & Kambhampati, 1994a, 1996) </ref>. With this replay strategy, the applicable cases are replayed in sequence before returning to from-scratch planning. Eager replay simplifies the replay process by avoiding the decision as to how to alternate replay of multiple cases. <p> With this replay strategy, the applicable cases are replayed in sequence before returning to from-scratch planning. Eager replay simplifies the replay process by avoiding the decision as to how to alternate replay of multiple cases. The effectiveness of this approach is dependent on the underlying plan-space planning strategy <ref> (Ihrig & Kambhampati, 1994a) </ref>. dersnlp's eager case adaptation strategy allows case failure to be defined in terms of the failure of a single node in the search tree. <p> As a partial-order case-based planner, dersnlp has the ability to solve large problems by retrieving multiple instances of smaller subproblems and merging these cases through sequenced replay <ref> (Ihrig & Kambhampati, 1994a) </ref>. The dersnlp+ebl framework extends this approach through the use of new EBL techniques which are employed in the construction of the case library. These techniques are used to explain a plan merging failure and to identify a set of negatively interacting goals.
Reference: <author> Ihrig, L., & Kambhampati, S. </author> <year> (1994b). </year> <title> Plan-space vs state-space planning in reuse and replay. </title> <type> Tech. rep. 94-006, </type> <institution> Department of Computer Science and Engineering. Arizona State University. </institution> <note> Also available at http://rakaposhi.eas.asu.edu/yochan.html. </note>
Reference: <author> Ihrig, L., & Kambhampati, S. </author> <year> (1996). </year> <title> Design and implementation of a replay framework based on a partial order planner. </title> <booktitle> In Proceedings AAAI-96. </booktitle>
Reference: <author> Joslin, D., & Roach, J. </author> <year> (1990). </year> <title> A theoretical analysis of conjunctive goal problems. </title> <journal> Artificial Intelligence, </journal> <volume> 41, </volume> <pages> 97-106. </pages>
Reference: <author> Kambhampati, S. </author> <year> (1992). </year> <title> Utility tradeoffs in incremental modification and reuse of plans. In Proceedings AAAI Spring Symposium on Computational Considerations in Supporting Incremental Modification and Reuse. </title>
Reference-contexts: If the planner cannot predict ahead of time that these previous choices are wrong for the current situation, it will experience a retrieval error. In this paper, we introduce dersnlp+ebl (DERivational Systematic NonLinear Planner + Explanation-Based Learning), a CBP system which like priar <ref> (Kambhampati & Hendler, 1992) </ref> and spa (Hanks & Weld, 1995) is based on a sound and complete domain-independent planner. dersnlp+ebl deals with the mis-retrieval problem by allowing the planner to learn from its planning failures so that it may anticipate future errors. <p> Section 5 relates our work to previous case-based planners, including chef and prodigy/analogy. Section 6 provides a summary. 2. Learning from Case Failure As stated earlier, dersnlp+ebl is based on a complete and correct domain-independent planning strategy. Like priar <ref> (Kambhampati & Hendler, 1992) </ref> and spa (Hanks & Weld, 1995), it is implemented on a partial-order planner. In this aspect it differs from state-space systems such as prodigy/analogy (Veloso & Carbonell, 1993a; Veloso, 1994) and paris (Bergmann & Wilke, 1995). <p> in preparation for solving a new problem, and finally, a replay mechanism by which the planner employs the retrieved plan derivations as a sequence of instructions to guide the new search process. dersnlp+ebl's methodology depends on aspects that it has in common with molgen (Friedland & Iwasaki, 1985) and priar <ref> (Kambhampati & Hendler, 1992) </ref>. It requires an eager case adaptation strategy, so that a skeletal plan will be constructed which contains all of the constraints added on the advice of the retrieved cases, and only those constraints. <p> As in earlier approaches, such as priar <ref> (Kambhampati & Hendler, 1992) </ref>, prodigy/analogy (Veloso, 1994) and caplan (Munoz-Avilla & Weberskirch, 1996), the cases that are stored cover smaller subsets of the original set of input goals achieved in the successful problem-solving episode. dersnlp+ebl differs from these earlier approaches in that the division into goal subsets is not based on <p> Prior research <ref> (Kambhampati, 1992) </ref> has analyzed their tradeoffs. The hybrid learning approach of dersnlp+ebl is designed to alleviate the drawbacks associated with both pure case-based planning, and rule-based EBL. Prior to this work, EBL has been used to construct generalized search control rules which may be applied to each new problem-solving situation. <p> Related Work and Discussion dersnlp+ebl's storage strategy relies on the capability of the case-based planner to replay multiple cases, each covering a small subset of goals, and then add step orderings to interleave their respective plans. This strategy differs from earlier approaches such as priar <ref> (Kambhampati & Hendler, 1992) </ref>, prodigy/analogy (Veloso, 1994), paris (Bergmann & Wilke, 1995), and caplan (Munoz-Avilla & Weberskirch, 1996), in that the division into goal subsets is not based on the structure of the final plan alone, but on the sequence of events making up the problem-solving episode.
Reference: <author> Kambhampati, S. </author> <year> (1994). </year> <title> Exploiting causal structure to control retrieval and refitting during plan reuse. </title> <journal> Computational Intelligence, </journal> <volume> 10. </volume>
Reference: <author> Kambhampati, S., & Chen, J. </author> <year> (1993). </year> <title> Relative utility of ebg based plan reuse in partial ordering vs total ordering planning. </title> <booktitle> In Proceedings AAAI-93, </booktitle> <pages> pp. 514-519. </pages> <address> Washington, D.C. </address>
Reference: <author> Kambhampati, S., & Hendler, J. A. </author> <year> (1992). </year> <title> A validation structure based theory of plan modification and reuse. </title> <journal> Artificial Intelligence, </journal> <volume> 55, </volume> <pages> 193-258. </pages>
Reference-contexts: If the planner cannot predict ahead of time that these previous choices are wrong for the current situation, it will experience a retrieval error. In this paper, we introduce dersnlp+ebl (DERivational Systematic NonLinear Planner + Explanation-Based Learning), a CBP system which like priar <ref> (Kambhampati & Hendler, 1992) </ref> and spa (Hanks & Weld, 1995) is based on a sound and complete domain-independent planner. dersnlp+ebl deals with the mis-retrieval problem by allowing the planner to learn from its planning failures so that it may anticipate future errors. <p> Section 5 relates our work to previous case-based planners, including chef and prodigy/analogy. Section 6 provides a summary. 2. Learning from Case Failure As stated earlier, dersnlp+ebl is based on a complete and correct domain-independent planning strategy. Like priar <ref> (Kambhampati & Hendler, 1992) </ref> and spa (Hanks & Weld, 1995), it is implemented on a partial-order planner. In this aspect it differs from state-space systems such as prodigy/analogy (Veloso & Carbonell, 1993a; Veloso, 1994) and paris (Bergmann & Wilke, 1995). <p> in preparation for solving a new problem, and finally, a replay mechanism by which the planner employs the retrieved plan derivations as a sequence of instructions to guide the new search process. dersnlp+ebl's methodology depends on aspects that it has in common with molgen (Friedland & Iwasaki, 1985) and priar <ref> (Kambhampati & Hendler, 1992) </ref>. It requires an eager case adaptation strategy, so that a skeletal plan will be constructed which contains all of the constraints added on the advice of the retrieved cases, and only those constraints. <p> As in earlier approaches, such as priar <ref> (Kambhampati & Hendler, 1992) </ref>, prodigy/analogy (Veloso, 1994) and caplan (Munoz-Avilla & Weberskirch, 1996), the cases that are stored cover smaller subsets of the original set of input goals achieved in the successful problem-solving episode. dersnlp+ebl differs from these earlier approaches in that the division into goal subsets is not based on <p> Prior research <ref> (Kambhampati, 1992) </ref> has analyzed their tradeoffs. The hybrid learning approach of dersnlp+ebl is designed to alleviate the drawbacks associated with both pure case-based planning, and rule-based EBL. Prior to this work, EBL has been used to construct generalized search control rules which may be applied to each new problem-solving situation. <p> Related Work and Discussion dersnlp+ebl's storage strategy relies on the capability of the case-based planner to replay multiple cases, each covering a small subset of goals, and then add step orderings to interleave their respective plans. This strategy differs from earlier approaches such as priar <ref> (Kambhampati & Hendler, 1992) </ref>, prodigy/analogy (Veloso, 1994), paris (Bergmann & Wilke, 1995), and caplan (Munoz-Avilla & Weberskirch, 1996), in that the division into goal subsets is not based on the structure of the final plan alone, but on the sequence of events making up the problem-solving episode.
Reference: <author> Kambhampati, S., Ihrig, L., & Srivastava, B. </author> <year> (1996a). </year> <title> A candidate set based analysis of subgoal interactions in conjunctive goal planning. </title> <booktitle> In Proceedings of the 3rd Intl. Conf. on AI Planning Systems. 196 Storing and Indexing Plan Derivations Kambhampati, </booktitle> <editor> S., Katukam, S., & Qu, Y. </editor> <year> (1996b). </year> <title> Failure driven dynamic search control for partial order planners: An explanation-based approach. </title> <journal> Artificial Intelligence, </journal> <volume> 88, </volume> <pages> 253-315. </pages>
Reference: <author> Kambhampati, S., Knoblock, C., & Yang, Q. </author> <year> (1995). </year> <title> Planning as refinement search: a unified framework for evaluating design tradeoffs in partial-order planning. </title> <journal> Artificial Intelligence, </journal> <volume> 76, </volume> <pages> 167-238. </pages>
Reference: <author> Koehler, J. </author> <year> (1994). </year> <title> Avoiding pitfalls in case-based planning. </title> <booktitle> In Proceedings of the 2nd Intl. Conf. on AI Planning Systems. </booktitle>
Reference: <author> Korf, R. </author> <year> (1987). </year> <title> Planning as search: a qualitative approach. </title> <journal> Artificial Intelligence, </journal> <volume> 33, </volume> <pages> 65-68. </pages>
Reference: <author> McAllester, D., & Rosenblitt, D. </author> <year> (1991). </year> <title> Systematic nonlinear planning. </title> <booktitle> In Proceedings AAAI-91. </booktitle>
Reference: <author> Minton, S. </author> <year> (1990a). </year> <title> Issues in the design of operator composition systems. </title> <booktitle> In Proceedings of the International conference on Machine Learning. </booktitle>
Reference-contexts: The justification for replay is strengthened to add the condition that no new linking opportunities are 5. An analogous decrease in plan quality occurs in state-space plan reuse, when sequencing macro-operators results state loops <ref> (Minton, 1990a) </ref>. 6.
Reference: <author> Minton, S. </author> <year> (1990b). </year> <title> Quantitative results concerning the utility of explanation-based learning. </title> <journal> Artificial Intelligence, </journal> <volume> 42, </volume> <pages> 363-392. </pages>
Reference: <author> Mostow, J., & Bhatnagar, N. </author> <year> (1987). </year> <title> Failsafe: A floor planner that uses ebg to learn from its failures. </title> <booktitle> In Proceedings IJCAI-87. </booktitle>
Reference: <author> Munoz-Avilla, H., & Weberskirch, F. </author> <year> (1996). </year> <title> Planning for manufacturing workpieces by storing, indexing and replaying planning decisions. </title> <booktitle> In Proceedings of the 3rd Intl. Conf. on AI Planning Systems. </booktitle> <address> AAAI-Press. </address>
Reference: <author> Munoz-Avilla, H., & Weberskirch, F. </author> <year> (1997). </year> <title> A case study on mergeability of cases with a partial-order planner. </title> <booktitle> In Proceedings of the 4th European Conf. on Planning. </booktitle>
Reference: <author> Ram, A., & Cox, M. </author> <year> (1994). </year> <title> Introspecive reasoning using meta-explanations for multistrat-egy learning. </title> <editor> In Michalski, R., & Tecuci, G. (Eds.), </editor> <booktitle> Machine Learning: A multistrategy approach Vol. IV. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Ram, S., & Francis, A. </author> <year> (1996). </year> <title> Multi-plan retrieval and adaptation in an experience-based agent. </title> <editor> In Leake, D. B. (Ed.), </editor> <title> Case-Based Reasoning: experiences, lessons, and future directions. </title> <publisher> AAAI Press/The MIT Press. </publisher>
Reference-contexts: For example, mpa <ref> (Ram & Francis, 1996) </ref> is built around a retrieval engine which performs asynchronous memory retrieval. caper (Hendler et al., 1996) uses a structure matching algorithm which parallelizes the process by which the plan's success conditions represented as a retrieval probe are matched with a large knowledge base of world facts. <p> The skeletal plan is first refined through the addition of plan constraints before undertaking any further retraction of constraints. spa, on the other hand, alternates the retraction of the plan constraints with the further addition of new constraints. mpa <ref> (Ram & Francis, 1996) </ref> extends spa's transformational strategy to accomplish multi-case retrieval and adaptation. 193 Ihrig & Kambhampati As mentioned earlier, derivational analogy is a case-based planning technique which was introduced by Carbonell (Veloso & Carbonell, 1993b).
Reference: <author> Redmond, M. </author> <year> (1990). </year> <title> Distributed cases for case-based reasoning:facilitating use of multiple cases. </title> <booktitle> In Proceedings AAAI-90. </booktitle>
Reference: <author> Smyth, B., & Keane, M. </author> <year> (1995). </year> <title> Remembering to forget: A competence-preserving deletion policy for cbr. </title> <booktitle> In Proceedings IJCAI-95. </booktitle>
Reference-contexts: Ultimately, we would like to retain in the library a minimum number of cases such that all new problems are solved through the efficient retrieval and adaptation of the cases that are stored <ref> (Smyth & Keane, 1995) </ref>. However, in complex domains, the planner's theory of problem similarity is incomplete (Barletta & Mark, 1988). It does not have information about all of the relevant features of a new situation which determine if a stored case will be applicable. <p> The aim is to eventually have in the library a minimal number of cases such that all of the problems encountered may be achieved by successfully merging multiple instances of stored cases. The approach is therefore to retain cases based on their competence as well as their performance <ref> (Smyth & Keane, 1995) </ref>. 3.2.1 An Example of dersnlp+ebl's Storage Strategy As an example of how a multi-goal problem is stored, consider the problem contained in destination location, l d .
Reference: <author> Tambe, N., Newell, A., & Rosenbloom, P. </author> <year> (1990). </year> <title> The problem of expensive chunks and its solution by restricting expressiveness. </title> <journal> Machine Learning, </journal> <volume> 5, </volume> <pages> 299-349. </pages>
Reference: <author> Tate, A. </author> <year> (1977). </year> <title> Generating project networks. </title> <booktitle> In Proceedings IJCAI-77. 197 Ihrig & Kambhampati Veloso, M. </booktitle> <year> (1994). </year> <title> Planning and learning by analogical reasoning. </title> <booktitle> Springer Verlag. Number 886 in Lecture Notes in Artificial Intelligence. </booktitle>
Reference-contexts: The priar framework (Kambhampati & Hendler, 1992; Kambhampati, 1994) is based within nonlin <ref> (Tate, 1977) </ref>. nonlin creates its plans through hierarchical task reduction. It is also a partial-order (plan-space) planner which constructs plans by protecting their underlying causal structure. Like dersnlp+ebl, it extends a case through the normal course of plan refinement defined by an underlying plan-space strategy.
Reference: <author> Veloso, M., & Blythe, J. </author> <year> (1994). </year> <title> Linkability: Examining causal link commitments in partial-order planning. </title> <booktitle> In Proceedings of the 2nd Intl. Conf. on AI Planning Systems. </booktitle>
Reference-contexts: This replay strategy can be contrasted with that of prodigy/analogy <ref> (Veloso, 1994) </ref> where replay is alternated with from-scratch planning for extra goals not covered by a case. In eager derivation replay each previous decision is eagerly adopted if justified in the current context. <p> This means that dersnlp+ebl inherits all of the properties of snlp, including soundness, completeness, and systematicity. A sample trace of snlp's decision process is shown in Figure 7. The trace corresponds to a simple problem from the logistics transportation domain of <ref> (Veloso, 1994) </ref> adapted for snlp as in Figure 4. This problem contains the goal of getting a single package, ob1, to a designated airport, l d . <p> However, goals are not always trivially serializable for the plan-space planner <ref> (Veloso & Blythe, 1994) </ref>. For example, consider the 2 D m S 1 domain (Barrett & Weld, 1994) shown in Figure 8. <p> Testing was done on problems which were randomly generated from this domain with the restriction that they always contain the goal g ff . The Logistics Transportation domain of <ref> (Veloso, 1994) </ref> was adopted for the second set of experiments. Eight packages and one airplane were randomly distributed over four cities. Problem goals represented the task of getting one or more packages to a single destination airport 4 . <p> As in earlier approaches, such as priar (Kambhampati & Hendler, 1992), prodigy/analogy <ref> (Veloso, 1994) </ref> and caplan (Munoz-Avilla & Weberskirch, 1996), the cases that are stored cover smaller subsets of the original set of input goals achieved in the successful problem-solving episode. dersnlp+ebl differs from these earlier approaches in that the division into goal subsets is not based on the structure of the final <p> Individual planning episodes which achieve this goal are represented one level lower in the net. Each is labeled by 180 Storing and Indexing Plan Derivations ob1 l d ). its relevant initial state conditions, otherwise known as the footprinted initial state <ref> (Veloso, 1994) </ref>. Together, the goal and initial state conditions make up the static success conditions on which cases are first retrieved. <p> If cases are retrieved on the basis of a partial match of the relevant initial state conditions, then retrieval errors may occur because of unmatched conditions <ref> (Veloso, 1994) </ref>. For example, just as a failure might occur in our logistics transportation example if there is an extra package off the plane's route, a similar failure will occur if a package is moved off the plane's route. <p> Our hypothesis was that performance would improve over problem solving experience as more negative interactions are discovered and stored. In addition, we predicted that dersnlp+ebl's method of storage would result in a low library size and low retrieval costs. The Logistics Transportation domain <ref> (Veloso, 1994) </ref> has become somewhat of a benchmark in the CBP literature. A scaled up version was therefore chosen for this purpose. We tested large multi-goal problems drawn from the domain shown in Figure 4 scaled up to first 6 and then 15 cities. <p> This strategy differs from earlier approaches such as priar (Kambhampati & Hendler, 1992), prodigy/analogy <ref> (Veloso, 1994) </ref>, paris (Bergmann & Wilke, 1995), and caplan (Munoz-Avilla & Weberskirch, 1996), in that the division into goal subsets is not based on the structure of the final plan alone, but on the sequence of events making up the problem-solving episode. <p> This model was developed by Veloso in prodigy/analogy <ref> (Veloso, 1994) </ref>, which employed the case fitting strategy called derivational replay. Case fitting based on replay is similar to fitting in plan reuse, in that it is based on the plan's underlying causal structure.
Reference: <author> Veloso, M., & Carbonell, J. </author> <year> (1993a). </year> <title> Derivational analogy in prodigy: Automating case acquisition, storage and utilization. </title> <journal> Machine Learning, </journal> <volume> 10, </volume> <pages> 249-278. </pages>
Reference: <author> Veloso, M., & Carbonell, J. </author> <year> (1993b). </year> <title> Toward scaling up machine learning: A case study with derivational analogy in prodigy. </title> <editor> In Minton, S. (Ed.), </editor> <title> Machine Learning methods for planning. </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: other hand, alternates the retraction of the plan constraints with the further addition of new constraints. mpa (Ram & Francis, 1996) extends spa's transformational strategy to accomplish multi-case retrieval and adaptation. 193 Ihrig & Kambhampati As mentioned earlier, derivational analogy is a case-based planning technique which was introduced by Carbonell <ref> (Veloso & Carbonell, 1993b) </ref>. This model was developed by Veloso in prodigy/analogy (Veloso, 1994), which employed the case fitting strategy called derivational replay. Case fitting based on replay is similar to fitting in plan reuse, in that it is based on the plan's underlying causal structure.
Reference: <author> Yang, Q., Nau, D., & Hendler, J. </author> <year> (1992). </year> <title> Merging separately generated plans with restricted interactions. </title> <journal> Computational Intelligence, </journal> <volume> 8 (2), </volume> <pages> 648-676. 198 </pages>
Reference-contexts: Plan merging through increasing the justification for replay accomplishes the retracting out of redundant action sequences, which may cause a planning failure. It thus deals with the action-merging interactions defined in <ref> (Yang, Nau, & Hendler, 1992) </ref>. In the next section we describe an empirical study testing the effectiveness of this merging strategy. 3.3.1 An Empirical Test of dersnlp+ebl's Plan Merging Strategy A preliminary study was conducted to test the effectiveness of dersnlp+ebl's method of plan merging through replay.
References-found: 41

