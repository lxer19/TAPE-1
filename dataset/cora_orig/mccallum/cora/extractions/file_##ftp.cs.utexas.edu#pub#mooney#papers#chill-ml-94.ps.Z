URL: file://ftp.cs.utexas.edu/pub/mooney/papers/chill-ml-94.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/ml/chillin.html
Root-URL: 
Email: zelle,mooney,konvissa@cs.utexas.edu  
Title: Combining Top-down and Bottom-up Techniques in Inductive Logic Programming  
Author: John M. Zelle, Raymond J. Mooney and Joshua B. Konvisser 
Address: Austin, TX 78712  
Affiliation: Department of Computer Sciences University of Texas  
Date: 1994  
Note: Appears in Machine Learning: Proceedings of the Eleventh International Conference pp 343-351, Morgan Kaufmann,  
Abstract: This paper describes a new method for inducing logic programs from examples which attempts to integrate the best aspects of existing ILP methods into a single coherent framework. In particular, it combines a bottom-up method similar to Golem with a top-down method similar to Foil. It also includes a method for predicate invention similar to Champ and an elegant solution to the "noisy oracle" problem which allows the system to learn recursive programs without requiring a complete set of positive examples. Systematic experimental comparisons to both Golem and Foil on a range of problems are used to clearly demonstrate the advantages of the approach.
Abstract-found: 1
Intro-found: 1
Reference: <author> Banerji, R. B. </author> <year> (1992). </year> <title> Learning theoretical terms. </title> <editor> In Muggleton, S., editor, </editor> <booktitle> Inductive Logic Programming, </booktitle> <pages> 93-110. </pages> <address> New York, NY: </address> <publisher> Academic Press. </publisher>
Reference: <author> Cameron-Jones, R. M., and Quinlan, J. R. </author> <year> (1994). </year> <title> Efficient top-down induction of logic programs. </title> <journal> SIGART Bulletin, </journal> <volume> 5(1) </volume> <pages> 33-42. </pages>
Reference: <author> Cohen, W. W. </author> <year> (1993). </year> <title> Pac-learning a resticted class of recursive logic programs. </title> <booktitle> In Proceedings of National Conference on Artificial Intelligence, </booktitle> <pages> 86-92. </pages> <address> Washington, D.C. </address>
Reference-contexts: Recursive hypotheses are evaluated by using the positive examples as a model of the predicate being learned. When the examples are incomplete, they provide a "noisy oracle" and Foil has difficulty learning even simple recursive concepts <ref> (Cohen, 1993) </ref>. This paper describes a new ILP algorithm, Chillin 1 , which combines elements of top-down and bottom 1 For Chill, INduction algorithm. Chill is a language acquisition system based on learning control-rules for logic programs (Zelle and Mooney, 1993). up induction methods.
Reference: <author> De Raedt, L., Lavrac, N., and Dzeroski, S. </author> <year> (1993). </year> <title> Multiple predicate learning. </title> <booktitle> In Proceedings of the Thirteenth International Joint conference on Artificial intelligence, </booktitle> <pages> 1037-1042. </pages> <address> Chambery, France. </address>
Reference-contexts: Ehance-ments are also needed for multi-predicate learning <ref> (De Raedt et al., 1993) </ref>, particularly for inventing predicates useful in the definition of multiple concepts. Finally, further experimental evaluation on a wider range of more realistic problems is needed.
Reference: <author> Kijsirikul, B., Numao, M., and Shimura, M. </author> <year> (1992). </year> <title> Discrimination-based constructive induction of logic programs. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> 44-49. </pages> <address> San Jose, CA. </address>
Reference-contexts: This partially completed clause is then passed to invent predicate for completion. 2.2.3 Inventing New Predicates Predicate invention is carried out in a manner analogous to Champ <ref> (Kijsirikul et al., 1992) </ref>.
Reference: <author> Lapointe, S., and Matwin, S. </author> <year> (1992). </year> <title> Sub-unification: A tool for efficient induction of recursive programs. </title> <booktitle> In Proceedings of the Ninth International Workshop on Machine Learning, </booktitle> <pages> 273-281. </pages> <address> Ab-erdeen, Scotland. </address>
Reference: <author> McClelland, J. L., and Kawamoto, A. H. </author> <year> (1986). </year> <title> Mech nisms of sentence processing: Assigning roles to constituents of sentences. </title> <editor> In Rumelhart, D. E., and McClelland, J. L., editors, </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> Vol. II, </volume> <pages> 318-362. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: For direct comparison, we performed the experiments with two versions of Chillin; the curve labeled "Chillin-" is Chillin with predicate invention turned off. The learning curves show that Chillin rapidly converges to very good definitions. 2 This data is derived from a framework for parsing sentences from <ref> (McClelland and Kawamoto, 1986) </ref> as described in (Zelle and Mooney, 1993). member multiply grandfather shift member grandfather Disabling predicate invention had only a minor impact (1%) in accuracy with smaller training sets, and no difference was detectable for larger sets.
Reference: <author> Muggleton, S. </author> <note> (to appear). Inverting implication. Ar tificial Intelligence. </note>
Reference: <author> Muggleton, S., and Buntine, W. </author> <year> (1988). </year> <title> Machine in vention of first-order predicates by inverting resolution. </title> <booktitle> In Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <pages> 339-352. </pages> <address> Ann Arbor, MI. </address>
Reference-contexts: The algorithm starts with a most specific definition (the set of positive examples) and introduces generalizations which make the definition more compact. Compactness is measured by a Cigol-like size metric <ref> (Muggleton and Buntine, 1988) </ref> which is a simple measure of the syntactic size of the program. The search for more general definitions is carried out in a hill-climbing fashion.
Reference: <author> Muggleton, S., and Feng, C. </author> <year> (1992). </year> <title> Efficient induction of logic programs. </title> <editor> In Muggleton, S., editor, </editor> <booktitle> Inductive Logic Programming, </booktitle> <pages> 281-297. </pages> <address> New York: </address> <publisher> Academic Press. </publisher>
Reference-contexts: Bottom-up methods, drawing heavily on logic programming theory, search for program clauses by considering generalizations created by inverting logical resolution or, more generally, implication. A successful representative of this class is Muggleton and Feng's Golem <ref> (Muggleton and Feng, 1992) </ref>. Top-down methods, in contrast, learn clauses by searching from general to specific in a manner analogous to traditional machine-learning approaches for inducing decision trees.
Reference: <author> Plotkin, G. D. </author> <year> (1970). </year> <title> A note on inductive general ization. </title> <editor> In Meltzer, B., and Michie, D., editors, </editor> <booktitle> Machine Intelligence (Vol. 5). </booktitle> <address> New York: </address> <publisher> Elsevier North-Holland. </publisher>
Reference-contexts: While both Golem and Foil have been successful, each of these approaches has weaknesses. Golem is based on the construction of relative least-general generalizations, rlggs <ref> (Plotkin, 1970) </ref> which forces the background knowledge to be expressed extensionally as a set of ground facts. This explicit model of background knowledge can be excessively large, and the clauses constructed from such models can grow explosively. <p> These three processes are explained in detail in the following subsections. 2.2.1 Constructing an Initial Generalization The initial generalization of the input clauses is computed by finding the least-general-generalization (LGG) of the input clauses under theta-subsumption <ref> (Plotkin, 1970) </ref>. The LGG of clauses C 1 and C 2 is the least general clause which subsumes (in the usual, non-empirical sense) both C 1 and C 2 .
Reference: <author> Quinlan, J. </author> <year> (1990). </year> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5(3) </volume> <pages> 239-266. </pages>
Reference: <author> Rouveirol, C. </author> <year> (1992). </year> <title> Extensions of inversion of resolu tion applied to theory completion. </title> <editor> In Muggleton, S., editor, </editor> <booktitle> Inductive Logic Programming, </booktitle> <pages> 63-86. </pages> <address> New York, NY: </address> <publisher> Academic Press. </publisher>
Reference: <author> Stahl, I., Tausend, B., and Wirth, R. </author> <year> (1993). </year> <title> Two methods for improving inductive logic programming systems. </title> <booktitle> In Machine Learning: ECML-93, </booktitle> <pages> 41-55. </pages> <address> Vienna. </address>
Reference-contexts: The left-hand graph of Figure 5 shows timing results for grandfather. Note that the run-time for Golem is lower here because it is not learning a definition, but rather, just memorizing the examples. 4 RELATED RESEARCH Like Chillin, Series (Wirth and O'Rorke, 1991) and, later Indico <ref> (Stahl et al., 1993) </ref> make use of LGGs of examples to construct clause heads containing functions. However, both of these systems precompute a set of clause heads for which bodies are subsequently induced. The approach taken by Chillin interleaves the bottom-up and top-down mechanisms, handling a larger class of concepts.
Reference: <author> Wirth, R. </author> <year> (1988). </year> <title> Learning by failure to prove. </title> <booktitle> In Proceedings of EWSL 88, </booktitle> <pages> 237-51. </pages> <publisher> Pitman. </publisher>
Reference: <author> Wirth, R., and O'Rorke, P. </author> <year> (1991). </year> <title> Constraints on predicate invention. </title> <booktitle> In Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <pages> 457-461. </pages> <address> Evanston, Ill. </address>
Reference: <author> Zelle, J. M., and Mooney, R. J. </author> <year> (1993). </year> <title> Learn ing semantic grammars with constructive inductive logic programming. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> 817-822. </pages> <address> Washington, D.C. </address>
Reference-contexts: This paper describes a new ILP algorithm, Chillin 1 , which combines elements of top-down and bottom 1 For Chill, INduction algorithm. Chill is a language acquisition system based on learning control-rules for logic programs <ref> (Zelle and Mooney, 1993) </ref>. up induction methods. This algorithm has been used as an element of a larger system which learns to parse natural language (Zelle and Mooney, 1993). Chillin's combination of techniques offers several advantages. <p> Chill is a language acquisition system based on learning control-rules for logic programs <ref> (Zelle and Mooney, 1993) </ref>. up induction methods. This algorithm has been used as an element of a larger system which learns to parse natural language (Zelle and Mooney, 1993). Chillin's combination of techniques offers several advantages. Chillin learns with intensionally expressed background knowledge and can handle examples containing functions without explicit constructor predicates. <p> The learning curves show that Chillin rapidly converges to very good definitions. 2 This data is derived from a framework for parsing sentences from (McClelland and Kawamoto, 1986) as described in <ref> (Zelle and Mooney, 1993) </ref>. member multiply grandfather shift member grandfather Disabling predicate invention had only a minor impact (1%) in accuracy with smaller training sets, and no difference was detectable for larger sets.
Reference: <author> Zelle, J. M., and Mooney, R. J. </author> <year> (1994). </year> <title> Inducing de terministic Prolog parsers from treebanks: A machine learning approach. </title> <booktitle> In Proceedings of National Conference on Artificial Intelligence. </booktitle> <address> Seat-tle, WA. </address>
Reference-contexts: It has also recently been used to learn natural language parsers from real text corpora requiring induction over thousands of complex, structured examples <ref> (Zelle and Mooney, 1994) </ref>. Consequently, we believe it provides an important foundation for continued progress on robust and efficient induction of complex relational and recursive concepts. Acknowledgments Thanks to Ross Quinlan and Mike Cameron-Jones for Foil, and Stephen Muggleton and Cao Feng for Golem.
References-found: 18

