URL: ftp://borneo.gmd.de/pub/as/ga/gmd_as_ga-93_04.ps
Refering-URL: http://www.ing.unlp.edu.ar/cetad/mos/memetic_home.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: e-mail: zhang@gmd.de, muehlen@gmd.de  
Title: Genetic Programming of Minimal Neural Nets Using Occam's Razor  
Author: Byoung-Tak Zhang Heinz Muhlenbein 
Address: Schloss Birlinghoven, D-5205 Sankt Augustin 1, Germany  
Affiliation: Artificial Intelligence Research Division German National Research Center for Computer Science (GMD)  
Abstract: A genetic programming method is investigated for optimizing both the architecture and the connection weights of multilayer feedforward neural networks. The genotype of each network is represented as a tree whose depth and width are dynamically adapted to the particular application by specifically defined genetic operators. The weights are trained by a next-ascent hillclimb-ing search. A new fitness function is proposed that quantifies the principle of Occam's razor. It makes an optimal trade-off between the error fitting ability and the parsimony of the network. We discuss the results for two problems of differing complexity and study the convergence and scaling properties of the algorithm.
Abstract-found: 1
Intro-found: 1
Reference: <author> Y. S. </author> <title> Abu-Mostafa (1989). The Vapnik-Chervonenkis dimension: information versus complexity in learning. </title> <journal> Neural Computation, </journal> <volume> 1(3) </volume> <pages> 312-317. </pages>
Reference: <author> F. </author> <title> Gruau (1992). Genetic synthesis of boolean neural networks with a cell rewriting developmental process. </title> <type> Tech. Rep., </type> <institution> Laboratoire de l'Informatique du Parallelisme. </institution>
Reference: <author> S. A. Harp, T. Samad, and A. </author> <title> Guha (1989). Towards the genetic synthesis of neural networks, </title> <booktitle> Proc. ICGA-89, </booktitle> <pages> 360-369. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> A. G. </author> <month> Ivakhnenko </month> <year> (1971). </year> <title> Polynomial theory of complex systems. </title> <journal> IEEE Trans. Sys. Man and Cybern., SMC-1(4):364-378. </journal>
Reference-contexts: It is interesting to notice that the global behavior of this optimization method is comparable with the group method of data handling (GMDH) in which additional terms are incrementally added to the existing polynomial approx-imator to achieve a minimal description length model of a complex system <ref> (Ivakhnenko 1971) </ref>. In general, the results are encouraging. For large size problems of some class, however, the convergence was very slow. A simple optimization method does not exist which performs better than any other optimization method for a reasonable large class of binary functions of size n.
Reference: <author> H. </author> <title> Kitano (1990). Designing neural networks using genetic algorithms with graph generation system. </title> <journal> Complex Systems, </journal> <volume> 4 </volume> <pages> 461-476. </pages>
Reference: <author> J. R. </author> <title> Koza (1990). Genetic Programming: A paradigm for genetically breeding populations of computer programs to solve problems. </title> <type> Tech. Rep. </type> <institution> STAN-CS-90-1314, Dept. of Computer Science, Stanford Univ., </institution> <address> CA. </address>
Reference: <author> D. J. C. </author> <title> MacKay (1992). Bayesian methods for adaptive models. </title> <type> Ph.D. thesis, </type> <institution> Caltech, Pasadena, </institution> <address> CA. </address>
Reference: <author> W. S. McCulloch, and W. </author> <title> Pitts (1943). A logical calculus of the ideas immanent in nervous activity. </title> <journal> Bull. Math. Biophysics, </journal> <volume> 5 </volume> <pages> 115-133. </pages>
Reference-contexts: Despite their simplicity, McCulloch-Pitts neurons are very powerful. In fact, it can be shown that any finite logical expression can be realized by them <ref> (McCulloch and Pitts 1943) </ref>. is represented as a set of m trees, each corresponding to one output unit. In the grammar, the nonterminal symbol Y is used to represent a neural unit having a threshold of and r weights. The integer r indicates the receptive field width of the unit.
Reference: <author> G. F. Miller, P. M. Todd, and S. U. </author> <title> Hegde (1989). Designing neural networks using genetic algorithms. </title> <booktitle> Proc. ICGA-89, </booktitle> <pages> 379-384. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> H. </author> <title> Muhlenbein (1993). Evolutionary algorithms: </title> <journal> theory and applications. </journal> <note> To appear in E. </note> <editor> H. L. Aarts, and J. K. Lenstra (eds.), </editor> <title> Local Search in Combinatorial Optimization, </title> <publisher> Wiley. </publisher>
Reference-contexts: Using an Occam's razor in the fitness function, the method prefers a simple network architecture to a complex one. The weights are trained not by backpropagation, but by a next-ascent hillclimbing search. The breeder genetic algorithm BGA <ref> (Muhlenbein et al. 1993) </ref> is used for evolving optimal networks. The paper is organized as follows. In Section 2, the fitness function for the genetic search of minimal complexity solutions is derived. <p> A simple optimization method does not exist which performs better than any other optimization method for a reasonable large class of binary functions of size n. To be powerful, every sophisticated optimization method has to be tuned to the application <ref> (Muhlenbein 1993) </ref>. In order to speed up the genetic search, an analysis of the fitness landscape has to be made. the number of weights, layers, and units for the best individual in each generation.
Reference: <author> H. Muhlenbein, and J. </author> <title> Kindermann (1989). The dynamics of evolution and learning|Towards genetic neural networks. </title> <editor> In R. Pfeifer et al. (eds.), </editor> <booktitle> Connectionism in Perspective, </booktitle> <pages> 173-197, </pages> <publisher> North-Holland. </publisher>
Reference: <author> H. Muhlenbein, and D. </author> <title> Schlierkamp-Voosen (1993). Predictive models for the breeder genetic algorithm I: continuous parameter optimization. </title> <booktitle> Evolutionary Computation, </booktitle> <pages> 1(1). </pages>
Reference-contexts: Using an Occam's razor in the fitness function, the method prefers a simple network architecture to a complex one. The weights are trained not by backpropagation, but by a next-ascent hillclimbing search. The breeder genetic algorithm BGA <ref> (Muhlenbein et al. 1993) </ref> is used for evolving optimal networks. The paper is organized as follows. In Section 2, the fitness function for the genetic search of minimal complexity solutions is derived. <p> A simple optimization method does not exist which performs better than any other optimization method for a reasonable large class of binary functions of size n. To be powerful, every sophisticated optimization method has to be tuned to the application <ref> (Muhlenbein 1993) </ref>. In order to speed up the genetic search, an analysis of the fitness landscape has to be made. the number of weights, layers, and units for the best individual in each generation.
Reference: <author> T. Poggio, and F. </author> <title> Girosi (1990). Networks for approximation and learning. </title> <journal> Proc. IEEE, </journal> <volume> 78(9) </volume> <pages> 1481-1497. </pages>
Reference: <author> D. E. Rumelhart, G. E. Hinton, and R. J. </author> <title> Williams (1986). Learning internal representations by error-propagation. </title> <editor> In D. E. Rumelhart, and J. L. McClelland (eds.), </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> Vol. I, </volume> <pages> 318-362. </pages> <publisher> MIT Press. </publisher>
Reference-contexts: Similar representation has also been used by Whitley et al. (1990) to prune unnecessary connections. Kitano (1990) and Gruau (1992) describe encoding schemes in which a network configuration is indirectly specified by a graph generation grammar which evolves by genetic algorithms. All these methods use the backpropagation algorithm <ref> (Rumelhart et al. 1986) </ref> to train the weights of the network. Koza (1990) provides an alternative approach to representing neural networks, under the framework of so-called genetic programming, which enables modification not only of the weights but also of the architecture for a neural network.
Reference: <author> R. </author> <title> Sorkin (1983). A quantitative Occam's razor. </title> <journal> Int. J. Theor. Phys., </journal> <volume> 22(12) </volume> <pages> 1091-1104. </pages>
Reference: <author> N. Tishby, E. Levin, and S. A. </author> <month> Solla </month> <year> (1989). </year> <title> Consistent inference of probabilities in layered networks: predictions and generalization. </title> <booktitle> Proc. Int. Joint Conf. Neural Networks, </booktitle> <volume> Vol. II, </volume> <pages> 403-409. </pages> <publisher> IEEE. </publisher>
Reference: <author> D. Whitley, T. Starkweather, and C. </author> <title> Bogart (1990). Genetic algorithms and neural networks: optimizing connections and connectivity. </title> <journal> Parallel Computing, </journal> <volume> 14 </volume> <pages> 347-361. </pages>
Reference: <author> B. T. </author> <title> Zhang (1992). Learning by Genetic Neural Evolution. (in German), </title> <address> Sankt Augustin, Infix-Verlag. </address> <note> Also available as Informatik Berichte No. 93, </note> <institution> Institut fur Informatik, Uni-versitat Bonn. </institution>
Reference: <author> B. T. Zhang, and G. </author> <month> Veenker </month> <year> (1991a). </year> <title> Focused incremental learning for improved generalization with reduced training sets. </title> <editor> In T. Kohonen et al. (eds.), </editor> <booktitle> Artificial Neural Networks: Proc. ICANN-91, </booktitle> <volume> Vol. I, </volume> <pages> 227-232. </pages> <publisher> Elsevier. </publisher>
Reference: <author> B. T. Zhang, and G. </author> <month> Veenker </month> <year> (1991b). </year> <title> Neural networks that teach themselves through genetic discovery of novel examples. </title> <booktitle> In Proc. IJCNN-91, </booktitle> <volume> Vol. I, </volume> <pages> 690-695. </pages> <publisher> IEEE. </publisher>
References-found: 20

