URL: ftp://speech.cse.ogi.edu/pub/zhihong/icassp97.ps.Z
Refering-URL: http://cslu.cse.ogi.edu/people/hu/index.html
Root-URL: http://www.cse.ogi.edu
Email: (zhihong@cse.ogi.edu)  
Title: SMOOTHNESS ANALYSIS FOR TRAJECTORY FEATURES  
Author: Zhihong Hu and Etienne Barnard 
Address: 20000 N.W. Walker Road, P.O. Box 91000, Portland, OR 97291-1000, USA,  
Affiliation: Center for Spoken Language Understanding, Oregon Graduate Institute of Science and Technology,  
Abstract: Dynamic modeling of speech is potentially a major improvement on Hidden Markov Models (HMMs). In one approach, trajectory models[1] are used to model the dynamics of the spectrum, and are used as basis for classification [1, 2]. Although some improvement has been achieved in this way, one would hope for more substantial improvements given that the independence assumption is removed. One reason why this was not achieved may be that the trajectory models are based on cepstral coefficients; we show that these tracks contain spurious oscillations. This suggests that these trajectory features might have a high within-class variance. We introduce a measure of evaluating the smoothness of trajectory-based features. This measure provides a method of selecting the best of a set of similar features. Formant trajectories prove to be significantly smoother than trajectories of mel scale cepstral coefficients (MFCC) by this measure, but this does not translate directly to improved performance. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> W. Goldenthal, </author> <title> Statistical Trajectory Models for Phonetic Recognition. </title> <type> PhD thesis, </type> <institution> M.I.T., </institution> <month> Auguest </month> <year> 1994. </year>
Reference-contexts: The similarity in dynamic movements of the articulators corresponds to the similarity of dynamics in the acoustics of each phone. Therefore, features that can capture these dynamic movements in the acoustic signal should be very useful in speech recognition. Several trajectory models <ref> [1, 3, 4, 5] </ref> have been proposed to capture the dynamic behavior of speech. In Goldenthal's work [1], for instance, the temporal behavior is modeled by templates of the dynamics of the acoustic attributes used to represent the signal. <p> Therefore, features that can capture these dynamic movements in the acoustic signal should be very useful in speech recognition. Several trajectory models [1, 3, 4, 5] have been proposed to capture the dynamic behavior of speech. In Goldenthal's work <ref> [1] </ref>, for instance, the temporal behavior is modeled by templates of the dynamics of the acoustic attributes used to represent the signal. By estimating their spatial-temporal correlation structure, trajectory models are generated for phonetic recognition. Most of Goldenthal's results are obtained using trajectories of mel scale cepstral coefficients (MFCCs). <p> Most of Goldenthal's results are obtained using trajectories of mel scale cepstral coefficients (MFCCs). In our past efforts to incorporate this dynamic information in speech recognition [6], we built a recognition system using syllable-like units and used the statistical trajectory models (as described in <ref> [1] </ref>) as the main features. Although we obtained encouraging results, we found that trajectories of the coefficients in a cepstral-based analysis oscillate through time even when the signal is changing slowly and smoothly. This is caused by the trigonometric mapping between cepstral coefficients and frequency components (Section 3). <p> This may be related to the additional information contained in the cepstral features (of which there are almost four times as many as the formant features). Note that our results are comparable to what Goldenthal reported in <ref> [1] </ref>. 4.4. Classification using more information To investigate whether additional information can be used to improved the performance of the formant-based models, features describing other attributes of the formants are added into the feature set to be investigated.
Reference: [2] <author> M. Afify, Y. Gong, and J. Haton, </author> <title> "Estimation of mixtures of stochastic dynamic trajectories: application to continuous speech recognition," </title> <booktitle> Computer Speech and Language, </booktitle> <volume> no. 10, </volume> <pages> pp. 23-36, </pages> <year> 1996. </year>
Reference: [3] <author> M. Ostendorf and S. Roukos, </author> <title> "A stochastic segment model for phoneme-based continuous speech recognition," </title> <journal> IEEE Transaction on Accoustics, Speech, and Signal Processing., </journal> <volume> vol. 37, no. 12, </volume> <pages> pp. 1857-1869, </pages> <year> 1989. </year>
Reference-contexts: The similarity in dynamic movements of the articulators corresponds to the similarity of dynamics in the acoustics of each phone. Therefore, features that can capture these dynamic movements in the acoustic signal should be very useful in speech recognition. Several trajectory models <ref> [1, 3, 4, 5] </ref> have been proposed to capture the dynamic behavior of speech. In Goldenthal's work [1], for instance, the temporal behavior is modeled by templates of the dynamics of the acoustic attributes used to represent the signal.
Reference: [4] <author> V. Digalakis, J. Rohlicek, and M. Ostendorf, </author> <title> "A dynamical system approach to continuous speech recognition," </title> <booktitle> in ICASSP-91, </booktitle> <pages> pp. 289-292, </pages> <year> 1991. </year>
Reference-contexts: The similarity in dynamic movements of the articulators corresponds to the similarity of dynamics in the acoustics of each phone. Therefore, features that can capture these dynamic movements in the acoustic signal should be very useful in speech recognition. Several trajectory models <ref> [1, 3, 4, 5] </ref> have been proposed to capture the dynamic behavior of speech. In Goldenthal's work [1], for instance, the temporal behavior is modeled by templates of the dynamics of the acoustic attributes used to represent the signal.
Reference: [5] <author> V. Digalakis, J. Rohlicek, and M. Ostendorf, </author> <title> "ML estimation of a stochastic linear system with the em algorithm and its application to speech recognition," </title> <journal> IEEE Transaction of Speech and Audio Processing., </journal> <volume> vol. 1, no. 4, </volume> <pages> pp. 431-442, </pages> <year> 1993. </year>
Reference-contexts: The similarity in dynamic movements of the articulators corresponds to the similarity of dynamics in the acoustics of each phone. Therefore, features that can capture these dynamic movements in the acoustic signal should be very useful in speech recognition. Several trajectory models <ref> [1, 3, 4, 5] </ref> have been proposed to capture the dynamic behavior of speech. In Goldenthal's work [1], for instance, the temporal behavior is modeled by templates of the dynamics of the acoustic attributes used to represent the signal.
Reference: [6] <author> Z. Hu, S. Schalkwyk, E. Barnard, and R. Cole, </author> <title> "Speech recognition using syllable-like units," </title> <booktitle> in ICSLP-96, </booktitle> <month> Oc-tober </month> <year> 1996. </year>
Reference-contexts: By estimating their spatial-temporal correlation structure, trajectory models are generated for phonetic recognition. Most of Goldenthal's results are obtained using trajectories of mel scale cepstral coefficients (MFCCs). In our past efforts to incorporate this dynamic information in speech recognition <ref> [6] </ref>, we built a recognition system using syllable-like units and used the statistical trajectory models (as described in [1]) as the main features.
Reference: [7] <author> L. Welling and H. Ney, </author> <title> "A model for efficient formant estimation," </title> <booktitle> in ICASSP-96, </booktitle> <pages> pp. 797-800, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: The data set used in the experiments. In the experiments performed, three formants are estimated for each vowel by using a formant estimation method proposed by Welling and Ney <ref> [7] </ref>. The results of the formants smoothed by a median filter of window width 5 are also presented. These are compared to 14th-order MFCCs. 4.1. Smoothness comparison Tracks with 10 states for each dimension (MFCC or formant) are computed for each segment. RPE values are calculated for each track.
Reference: [8] <author> R. Hogg and R. Tanis, </author> <title> Probability and Statistical Inference. </title> <publisher> Macmillan Publishing Company, </publisher> <year> 1993. </year> <note> General Intro : ISBN 0-02-355821-0. </note>
Reference: [9] <author> S. Chen and Y. Wang, </author> <title> "Vector quantization of pitch information in mandarin speech," </title> <journal> IEEE Transactions on Communications, </journal> <volume> vol. 38, no. 9, </volume> <pages> pp. 1317-1320, </pages> <year> 1990. </year>
Reference-contexts: The second window is the phonetic label. l-ay+ih means phoneme ay in the left context of l and right context of ih. The third window shows the formant trajectory reconstructed from the 3rd-order polynomial fitting. In this experiment, the coefficients of an orthonormal polynomial approximation (the Legendre polynomial <ref> [9] </ref>) for the trajectories are used as features in classification. The classification results are shown in Table 4. feature MFCC formant smoothed formant dimension 61 17 17 % correct 69.7% 66.7% 66.8% Table 4. Classification results using polynomial ap proximation for different trajectories.
References-found: 9

