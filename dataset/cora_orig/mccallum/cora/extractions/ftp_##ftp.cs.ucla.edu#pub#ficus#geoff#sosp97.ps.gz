URL: ftp://ftp.cs.ucla.edu/pub/ficus/geoff/sosp97.ps.gz
Refering-URL: http://ficus-www.cs.ucla.edu/project-members/geoff/pubs.html
Root-URL: http://www.cs.ucla.edu
Title: Automated Hoarding for Mobile Computers  
Author: Geoffrey H. Kuenning and Gerald J. Popek 
Abstract: A common problem facing mobile computing is disconnected operation, or computing in the absence of a network. Hoarding eases disconnected operation by selecting a subset of the user's files for local storage. We describe a hoarding system that can operate without user intervention, by observing user activity and predicting future needs. The system calculates a new measure, semantic distance, between individual files, and uses this to feed a clustering algorithm that chooses which files should be hoarded. A separate replication system manages the actual transport of data; any of a number of replication systems may be used. We discuss practical problems encountered in the real world and present usage statistics showing that our system outperforms previous approaches by factors that can exceed 10:1. y The authors are affiliated with the Computer Science Department, University of California, Los Angeles. Gerald Popek is also affiliated with Platinum technology. E-mail: geoff@fmg.cs.ucla.edu, popek@platinum.com. To appear in the 16th ACM Symposium on Operating Systems Principles, October, 1997, St. Malo, France. Copyright c fl1997 by the Association for Computing Machinery, Inc. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Publications Dept, ACM Inc., fax +1 (212) 869-0481, or permissions@acm.org. fl This work was partially supported by the Defense Advanced Research Projects Agency under contract N00174-91-C-0107.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Rafael Alonso, Daniel Barbar a, and Luis L. Cova. </author> <title> Using stashing to increase node autonomy in distributed file systems. </title> <booktitle> In Proceedings of the Ninth IEEE Symposium on Reliability in Distributed Software and Database Systems, </booktitle> <pages> pages 12-21, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: The local files can be managed and kept consistent by a replication system [7, 9, 11]. The difficult challenge is the hoarding problem of selecting which files should be stored locally. Earlier solutions have simply chosen the most recently referenced files <ref> [1, 9] </ref> or asked the user to participate at least peripherally in managing hoard contents [11, 21]. The former approach is wasteful of scarce hoard space, while the latter requires more expertise and involvement that most users are willing to offer. <p> Early systems used an LRU mechanism to load the hoard <ref> [1, 9] </ref>, or left the problem to unspecified external mechanisms [8]. Some of these systems were actually used for disconnected operation, but no data on the performance of hoarding has ever been published.
Reference: [2] <author> Rajive Bagrodia, Wesley W. Chu, Leonard Kleinrock, and Gerald Popek. </author> <title> Vision, issues, and architecture for nomadic computing. </title> <journal> IEEE Personal Communications Magazine, </journal> <volume> 2(6) </volume> <pages> 14-27, </pages> <month> December </month> <year> 1995. </year>
Reference-contexts: The system is capable of supporting disconnected operation for lengthy periods with only occasional hoard misses, giving the user the illusion that the network is still present even in the complete absence of communication. This level of automation enables the entire virtual-networking paradigm of mobile operation <ref> [2] </ref>. An especially important contribution of SEER is the freedom from manual user configuration. While previous systems required the hoard contents to be specified partially or entirely by hand, SEER is able to infer project contents and make its hoarding decisions without intruding on the user's work.
Reference: [3] <author> Benamin S. Duran and Patrick L. Odell. </author> <title> Cluster Analysis: A Survey, </title> <booktitle> volume 100 of Lecture Notes in Economics and Mathematical Systems. </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1974. </year>
Reference-contexts: No Distance Metric. Although we call semantic distance a distance measure, it is not a distance metric as required by many clustering algorithms <ref> [3] </ref>. In particular, it is asymmetric and does not satisfy the triangle inequality. Overlapping Clusters. Perhaps the most troublesome characteristic of the problem is the need for files to be members of more than one cluster.
Reference: [4] <author> James Griffioen and Randy Appleton. </author> <title> Performance measurements of automatic pre-fetching. </title> <booktitle> In Proceedings of the ISCA International Conference on Parallel and 20 Distributed Computing Systems, </booktitle> <month> September </month> <year> 1995. </year>
Reference-contexts: The method we have chosen is based on the observation that semantic locality is similar to temporal locality: files that are referenced at the same time tend to be semantically related. This observation is not original to us <ref> [4, 5, 12, 21] </ref>, but to our knowledge we are the first to formalize the notion of semantic locality and its relationship to temporal locality. This leads directly to a first definition of semantic distance (note that all of our suggested measures are asymmetric): Definition 1 Temporal semantic distance.
Reference: [5] <author> Knut Stener Grimsrud, James K. Archibald, and Brent E. Nelson. </author> <title> Multiple prefetch adaptive disk caching. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 5(1) </volume> <pages> 88-103, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: The method we have chosen is based on the observation that semantic locality is similar to temporal locality: files that are referenced at the same time tend to be semantically related. This observation is not original to us <ref> [4, 5, 12, 21] </ref>, but to our knowledge we are the first to formalize the notion of semantic locality and its relationship to temporal locality. This leads directly to a first definition of semantic distance (note that all of our suggested measures are asymmetric): Definition 1 Temporal semantic distance.
Reference: [6] <author> Michial Allen Gunter. Rumor: </author> <title> A reconciliation-based user-level optimistic replication system for mobile computers. </title> <type> Master's thesis, </type> <institution> University of California, </institution> <address> Los Angeles, Los Angeles, CA, </address> <month> June </month> <year> 1997. </year>
Reference-contexts: It also makes SEER more portable because very little is assumed about the underlying system. SEER currently runs atop the RUMOR <ref> [6, 18] </ref> user-level replication system, a custom-built master-slave replication service called CHEAP RUMOR, and CODA [11], and it could easily be used with other systems such as FICUS [7]and LITTLE WORK [9]. A feature critical to usability is that, unlike previous systems, SEER normally operates without user intervention.
Reference: [7] <author> Richard G. Guy, John S. Heidemann, Wai Mak, Thomas W. Page, Jr., Gerald J. Popek, and Dieter Rothmeier. </author> <title> Implementation of the Ficus replicated file system. </title> <booktitle> In USENIX Conference Proceedings, </booktitle> <pages> pages 63-71. </pages> <institution> University of California, Los Angeles, USENIX, </institution> <month> June </month> <year> 1990. </year>
Reference-contexts: A very attractive solution to the lack of communication is hoarding, in which non-local files are cached on the local disk prior to disconnection. The local files can be managed and kept consistent by a replication system <ref> [7, 9, 11] </ref>. The difficult challenge is the hoarding problem of selecting which files should be stored locally. Earlier solutions have simply chosen the most recently referenced files [1, 9] or asked the user to participate at least peripherally in managing hoard contents [11, 21].
Reference: [8] <author> John S. Heidemann, Thomas W. Page, Jr., Richard G. Guy, and Gerald J. Popek. </author> <title> Primarily disconnected operation: Experiences with Ficus. </title> <booktitle> In Proceedings of the Second Workshop on Management of Replicated Data, </booktitle> <pages> pages 2-5. </pages> <institution> University of California, Los An-geles, IEEE, </institution> <month> November </month> <year> 1992. </year>
Reference-contexts: Early systems used an LRU mechanism to load the hoard [1, 9], or left the problem to unspecified external mechanisms <ref> [8] </ref>. Some of these systems were actually used for disconnected operation, but no data on the performance of hoarding has ever been published. Our own experience suggests that LRU is usually an adequate approach, so that users would find these systems acceptable.
Reference: [9] <author> L. B. Huston and Peter Honeyman. </author> <title> Disconnected operation for AFS. </title> <booktitle> In Proceedings of the USENIX Symposium on Mobile and Location-Independent Computing, </booktitle> <pages> pages 1-10. </pages> <publisher> USENIX, </publisher> <year> 1993. </year>
Reference-contexts: A very attractive solution to the lack of communication is hoarding, in which non-local files are cached on the local disk prior to disconnection. The local files can be managed and kept consistent by a replication system <ref> [7, 9, 11] </ref>. The difficult challenge is the hoarding problem of selecting which files should be stored locally. Earlier solutions have simply chosen the most recently referenced files [1, 9] or asked the user to participate at least peripherally in managing hoard contents [11, 21]. <p> The local files can be managed and kept consistent by a replication system [7, 9, 11]. The difficult challenge is the hoarding problem of selecting which files should be stored locally. Earlier solutions have simply chosen the most recently referenced files <ref> [1, 9] </ref> or asked the user to participate at least peripherally in managing hoard contents [11, 21]. The former approach is wasteful of scarce hoard space, while the latter requires more expertise and involvement that most users are willing to offer. <p> SEER currently runs atop the RUMOR [6, 18] user-level replication system, a custom-built master-slave replication service called CHEAP RUMOR, and CODA [11], and it could easily be used with other systems such as FICUS [7]and LITTLE WORK <ref> [9] </ref>. A feature critical to usability is that, unlike previous systems, SEER normally operates without user intervention. There is no need to build explicit lists of important files or to instruct the system that certain activities are of interest. <p> Early systems used an LRU mechanism to load the hoard <ref> [1, 9] </ref>, or left the problem to unspecified external mechanisms [8]. Some of these systems were actually used for disconnected operation, but no data on the performance of hoarding has ever been published.
Reference: [10] <author> R. A. Jarvis and E. A. Patrick. </author> <title> Clustering using a similarity measure based on shared near neighbors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-22(11):1025-1034, </volume> <month> November </month> <year> 1973. </year>
Reference-contexts: There is no numerical measure that can be used to characterize the goodness of a particular cluster assignment, eliminating algorithms that seek to optimize such a criterion. 2 Optimal clustering is NP-hard [16]. 3.3.2 Agglomerative Algorithm The algorithm we have developed is based on one originated by Jarvis and Patrick <ref> [10] </ref>. This algorithm is bottom-up, or agglomerative, starting with each data point assigned to an individual cluster and then combining clusters according to a shared-neighbors criterion. In the original formulation, the algorithm first calculates the n nearest neighbors to each point, where n is a parameter of the algorithm.
Reference: [11] <author> James J. Kistler and Mahadev Satyanaraya-nan. </author> <title> Disconnected operation in the Coda file system. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 10(1) </volume> <pages> 3-25, </pages> <year> 1992. </year>
Reference-contexts: A very attractive solution to the lack of communication is hoarding, in which non-local files are cached on the local disk prior to disconnection. The local files can be managed and kept consistent by a replication system <ref> [7, 9, 11] </ref>. The difficult challenge is the hoarding problem of selecting which files should be stored locally. Earlier solutions have simply chosen the most recently referenced files [1, 9] or asked the user to participate at least peripherally in managing hoard contents [11, 21]. <p> The difficult challenge is the hoarding problem of selecting which files should be stored locally. Earlier solutions have simply chosen the most recently referenced files [1, 9] or asked the user to participate at least peripherally in managing hoard contents <ref> [11, 21] </ref>. The former approach is wasteful of scarce hoard space, while the latter requires more expertise and involvement that most users are willing to offer. <p> It also makes SEER more portable because very little is assumed about the underlying system. SEER currently runs atop the RUMOR [6, 18] user-level replication system, a custom-built master-slave replication service called CHEAP RUMOR, and CODA <ref> [11] </ref>, and it could easily be used with other systems such as FICUS [7]and LITTLE WORK [9]. A feature critical to usability is that, unlike previous systems, SEER normally operates without user intervention. <p> However, a few calls (on LINUX, only exec and exit) are traced before execution to capture important information that will be destroyed when the call completes. 5 Evaluating Success 5.1 Measurement Methodology As discussed in <ref> [11] </ref>, traditional measures of cache performance, such as miss rate, are inappropriate in a hoarding situation. In a traditional caching system, a miss causes a relatively minor performance penalty, and has no effect on the overall course of the computation. <p> Again, this provides evidence to suggest that hoard misses were not bothersome to our users. It is worth noting that intelligent user behavior is an important factor in the success of SEER. This same factor was previously observed with CODA <ref> [11, Section 5.2.2] </ref>. Before the advent of mobile computing, a traveling businessperson would load his briefcase with documents he expected to work on. While on an airplane, he would not attempt to work on a project that he knew was not in the briefcase. <p> This is in contrast to SEER's clustering approach, where an attention shift will quickly cause all members of a project to be loaded into the hoard. 6.2 CODA The CODA system <ref> [11] </ref> enhanced simple LRU by allowing the user to specify an offset to be applied to the LRU age of a particular file, as a means of indicating its importance. A global bound arranged that for older files, the offset controlled the hoarding decision regardless of the original reference order. <p> Mahadev Satyanarayanan has commented [19] that this approach is similar to programming in assembly language: it provides excellent control over what happens, but is tedious and requires great expertise. There are very few published results on the hoarding behavior of CODA. Although both <ref> [11] </ref> and [20] give quantitative information, the data presented relates to the size of working sets and the performance of the replication system. The only discussion of hoarding success is couched in general terms.
Reference: [12] <author> Thomas M. Kroeger and Darrell D. E. </author> <title> Long. Predicting file system actions from prior events. </title> <booktitle> In USENIX Conference Proceedings, </booktitle> <pages> pages 319-328, </pages> <address> San Diego, California, Jan-uary 1996. </address> <publisher> USENIX. </publisher>
Reference-contexts: The method we have chosen is based on the observation that semantic locality is similar to temporal locality: files that are referenced at the same time tend to be semantically related. This observation is not original to us <ref> [4, 5, 12, 21] </ref>, but to our knowledge we are the first to formalize the notion of semantic locality and its relationship to temporal locality. This leads directly to a first definition of semantic distance (note that all of our suggested measures are asymmetric): Definition 1 Temporal semantic distance.
Reference: [13] <author> Geoffrey H. Kuenning. </author> <title> The design of the SEER predictive caching system. </title> <booktitle> In Proceedings of the Workshop on Mobile Computing Systems and Applications, </booktitle> <address> Santa Cruz, CA, </address> <month> December </month> <year> 1994. </year>
Reference-contexts: These independent streams are intermixed when observed by SEER, and create incorrect and spurious file relationships if not properly handled. We had originally hypothesized <ref> [13] </ref> that the data reductions discussed in Section 3.1.2 would provide a noise-filtering mechanism that would eliminate the effects of these spurious relationships. Unfortunately, experience proved this hypothesis incorrect: although noise was reduced, it 11 was not eliminated, and the resulting spurious rela-tionships tended to cause poor hoarding decisions.
Reference: [14] <author> Geoffrey H. Kuenning, Gerald J. Popek, and Peter Reiher. </author> <title> An analysis of trace data for predictive file caching in mobile computing. </title> <booktitle> In USENIX Conference Proceedings, </booktitle> <pages> pages 291-306. </pages> <publisher> USENIX, </publisher> <month> June </month> <year> 1994. </year>
Reference-contexts: We plan to conduct further studies with the CODA user base and to port SEER to the WINDOWS environment. The latter port will make SEER available to business and management users, who often have very different behavior than computer scientists <ref> [14] </ref>. As part of this porting effort, we plan to analyze the performance of SEER in other settings and to compare this to our current data. There are also significant opportunities for further development of the underlying mechanisms.
Reference: [15] <author> Geoffrey Houston Kuenning. Seer: </author> <title> Predictive File Hoarding for Disconnected Mobile Operation. </title> <type> PhD thesis, </type> <institution> University of Cali-fornia, </institution> <address> Los Angeles, Los Angeles, CA, </address> <month> May </month> <year> 1997. </year> <note> Also available as UCLA CSD Technical Report UCLA-CSD-970015. </note>
Reference-contexts: We chose not to do this partly for efficiency, and partly to capture the phenomenon of intensive work on a single project. The various options involved in calculating se mantic distance are discussed in detail in <ref> [15] </ref>. 3 perscripts o and c indicate opens and closes respectively. This sequence is diagrammed in Figure 1, where the extent of an access is indicated by the width of the enclosing box. <p> Second, when 4 processing a new file reference, the distances up-dated are limited to those from files that are within a distance of M (currently M = 100) of the current reference. Although these heuristics can introduce a large error in pathological cases <ref> [15] </ref>, in practice they have produced acceptable results. A compensation algorithm detects and partially adjusts for larger distances by inserting M whenever a value larger than M would have occurred. <p> If this reference has a distance larger than that of the new candidate, it is chosen for replacement. Finally, if there is still no candidate, an aging system is applied that allows very old and inactive references to be replaced by newer ones; details are given elsewhere <ref> [15] </ref>. <p> Although space precludes a detailed discussion, we found it necessary to devote significant effort to searching the parameter space for the values that would produce good results for all users. The search methods and the parameters we used for our tests are detailed in <ref> [15] </ref>. 4.10 Avoiding Deadlock Since SEER issues its own system calls, deadlock can occur if these calls are themselves traced. To avoid this problem, the trace mechanism does not record calls made by the observer and correlator themselves. However, experience showed that this step was not enough. <p> The median is omitted when there are fewer than 4 samples. This table also omits rows for all severity levels that had a zero miss count, and for machines that had no misses. Such rows would merely report the disconnection-time statistics for those machines; interested readers may refer to <ref> [15] </ref> for more information. It is clear from these tables that the users of SEER did not suffer greatly due to hoard misses. <p> All told, the system comprises approximately 47,500 lines of code. The cost of running SEER is twofold: CPU and memory requirements. The CPU cost of tracking system calls is minor, about 35 s on a 133-MHz PENTIUM r processor <ref> [15] </ref>, and the system calls traced are infrequent ones such as open, making tracing inexpensive.
Reference: [16] <author> Mirko Krivanek. </author> <title> Algorithmic and Geometric Aspects of Cluster Analysis. </title> <address> Academia Praha, Prague, </address> <year> 1991. </year>
Reference-contexts: No Objective Criterion. There is no numerical measure that can be used to characterize the goodness of a particular cluster assignment, eliminating algorithms that seek to optimize such a criterion. 2 Optimal clustering is NP-hard <ref> [16] </ref>. 3.3.2 Agglomerative Algorithm The algorithm we have developed is based on one originated by Jarvis and Patrick [10]. This algorithm is bottom-up, or agglomerative, starting with each data point assigned to an individual cluster and then combining clusters according to a shared-neighbors criterion.
Reference: [17] <author> Peter Reiher, John S. Heidemann, David Rat-ner, Gregory Skinner, and Gerald J. Popek. </author> <title> Resolving file conflicts in the Ficus file system. </title> <booktitle> In USENIX Conference Proceedings, </booktitle> <pages> pages 183-195. </pages> <institution> University of California, Los Angeles, USENIX, </institution> <month> June </month> <year> 1994. </year>
Reference-contexts: SEER does not itself do the file hoarding; instead an underlying replication system performs this task. This design frees SEER from the troublesome details of moving files back and forth between computers, making sure updates are propagated to other replicas of the files, and managing conflicts <ref> [17] </ref>. It also makes SEER more portable because very little is assumed about the underlying system.
Reference: [18] <author> Peter Reiher, Jerry Popek, Michial Gunter, John Salomone, and David Ratner. </author> <title> Peer-to-peer reconciliation based replication for mobile computers. </title> <booktitle> In Proceedings of the ECOOP Workshop on Mobility and Replication, </booktitle> <month> July </month> <year> 1996. </year>
Reference-contexts: It also makes SEER more portable because very little is assumed about the underlying system. SEER currently runs atop the RUMOR <ref> [6, 18] </ref> user-level replication system, a custom-built master-slave replication service called CHEAP RUMOR, and CODA [11], and it could easily be used with other systems such as FICUS [7]and LITTLE WORK [9]. A feature critical to usability is that, unlike previous systems, SEER normally operates without user intervention.
Reference: [19] <author> Mahadev Satyanarayanan, </author> <month> January </month> <year> 1997. </year> <type> Personal communication. </type>
Reference-contexts: Mahadev Satyanarayanan has commented <ref> [19] </ref> that this approach is similar to programming in assembly language: it provides excellent control over what happens, but is tedious and requires great expertise. There are very few published results on the hoarding behavior of CODA.
Reference: [20] <author> Mahadev Satyanarayanan, James J. Kistler, Lily B. Mummert, Maria R. Ebling, Puneet 21 Kumar, and Qi Lu. </author> <title> Experience with discon-nected operation in a mobile computing environment. </title> <booktitle> In Proceedings of the USENIX Symposium on Mobile and Location-Independent Computing, </booktitle> <pages> pages 11-28, </pages> <address> Cambridge, MA, </address> <month> August </month> <year> 1993. </year> <booktitle> USENIX. </booktitle>
Reference-contexts: Instead, a hoard miss generally causes the user to stop work on the current task and switch to a secondary one. In a trace-driven simulation, a hoard miss invalidates the trace because of this taskswitching behavior. 5.1.1 Time to First Miss An alternative measure, first suggested in <ref> [20] </ref>, is the time to the first hoard miss, measured as either elapsed time or number of file references. This is attractive because it quantifies the amount of work the user was able to do before a hoard failure forced a change in activity. <p> When an attention shift occurred, users would change projects by loading a new set of priorities, called a hoard profile, for that project. According to <ref> [20] </ref>, separate hoard profiles were normally used for applications and data; a user would choose a subset of possible profiles depending on the expected activity. <p> Mahadev Satyanarayanan has commented [19] that this approach is similar to programming in assembly language: it provides excellent control over what happens, but is tedious and requires great expertise. There are very few published results on the hoarding behavior of CODA. Although both [11] and <ref> [20] </ref> give quantitative information, the data presented relates to the size of working sets and the performance of the replication system. The only discussion of hoarding success is couched in general terms. <p> Although both [11] and [20] give quantitative information, the data presented relates to the size of working sets and the performance of the replication system. The only discussion of hoarding success is couched in general terms. For example, from <ref> [20, Section 5.2.2] </ref>: Many disconnected sessions experienced by our users, including many sections of extended duration, involved no cache misses whatsoever. and When disconnected misses did occur, they often were not fatal to the session.
Reference: [21] <author> Carl D. Tait, Hui Lei, Swarup Acharya, and Henry Chang. </author> <title> Intelligent file hoarding for mobile computers. </title> <booktitle> In Proceedings of Mobi-Com '95: The First International Conference on Mobile Computing and Networking, </booktitle> <pages> pages 119-125, </pages> <address> Berkeley, CA, </address> <month> November </month> <year> 1995. </year> <month> 22 </month>
Reference-contexts: The difficult challenge is the hoarding problem of selecting which files should be stored locally. Earlier solutions have simply chosen the most recently referenced files [1, 9] or asked the user to participate at least peripherally in managing hoard contents <ref> [11, 21] </ref>. The former approach is wasteful of scarce hoard space, while the latter requires more expertise and involvement that most users are willing to offer. <p> The method we have chosen is based on the observation that semantic locality is similar to temporal locality: files that are referenced at the same time tend to be semantically related. This observation is not original to us <ref> [4, 5, 12, 21] </ref>, but to our knowledge we are the first to formalize the notion of semantic locality and its relationship to temporal locality. This leads directly to a first definition of semantic distance (note that all of our suggested measures are asymmetric): Definition 1 Temporal semantic distance. <p> To address the problem, we found it necessary to separate the reference streams on a per-process basis in a manner similar to that used by Tait et al.'s SPY UTILITY <ref> [21] </ref>. SEER maintains a separate reference-history list for each process, and calculates semantic distances on a process-local basis. The file-open test mentioned in Definition 3 is also performed on a per-process basis. <p> Indeed, it was often possible for a user to fallback on different tasks two or three times before they gave up and ter minated the session. 6.3 SPY UTILITY To date, the only other attempt to automate the hoarding process is Tait et al.'s SPY UTILITY <ref> [21] </ref>. Like SEER, this system tracks process execution trees and infers the contents of projects based on file accesses. It differs in that it restricts itself to loading unions of access trees, rather than attempting to create project clusters at a higher semantic level. This mechanism is much more limited.
References-found: 21

