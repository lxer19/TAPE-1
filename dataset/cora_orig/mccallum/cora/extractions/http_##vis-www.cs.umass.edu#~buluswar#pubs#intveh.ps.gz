URL: http://vis-www.cs.umass.edu/~buluswar/pubs/intveh.ps.gz
Refering-URL: http://vis-www.cs.umass.edu/~buluswar/
Root-URL: 
Email: buluswar@cs.umass.edu draper@cs.colostate.edu  
Title: Color machine vision for autonomous vehicles  
Author: Shashi D. Buluswar Bruce A. Draper 
Keyword: Color, Autonomous vehicles, Machine learning in computer vision.  
Note: Supported by the Advanced Research Projects Agency through Rome Labs under contract F30602-94-C-0042.  
Address: Amherst, MA, U.S.A. Ft. Collins, CO, U.S.A.  
Affiliation: Dept. of Computer Science Dept. of Computer Science University of Massachusetts Colorado State University  
Abstract: Color can be a useful feature in autonomous vehicle systems that are based on machine vision, for tasks such as obstacle detection, lane/road following, and recognition of miscellaneous scene objects. Unfortunately, few existing autonomous vehicle systems use color to its full extent, largely because color-based recognition in outdoor scenes is complicated, and existing color machine vision techniques have not been shown to be effective in realistic outdoor images. This paper presents a technique for achieving effective real-time color recognition in outdoor scenes. The technique uses Multivariate Decision Trees for piecewise linear non-parametric function approximation to learn the color of a target object from training samples, and then detects targets by classifying pixels based on the approximated function. The method has been successfully tested in several domains, such as autonomous highway navigation, off-road navigation and target detection for unmanned military vehicles, in projects such as the U.S. National Automated Highway System (AHS) and the U.S. Defense Advanced Project Agency Unmanned Ground Vehicle (DARPA-UGV). MDT-based systems have been used in stand-alone mode, as well as in conjunction with systems based on other sensor configurations. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J.R. Beveridge, D. Panda and T. Yachik, </author> <title> November 1993 Fort Carson RSTA Data Collection, </title> <institution> Colorado State University Technical Report CSS-94-118, </institution> <year> 1994. </year>
Reference-contexts: Consequently, the hyperplanes of the MDT can make fine distinctions between target color and the background. The MDT-based system was tested on the Ft. Carson data set <ref> [1] </ref> by a DARPA-sanctioned study by LGA, Inc. [49], and at UGV Demo-C. 19 images (left, targets marked with circles), binary classification (middle), targets extracted (right). are extracted from the binary classification image by using clustering target pixels and applying region-level heuristics such as (the range of) expected vehicle size (s)
Reference: [2] <author> T.E. Boult and G. Wolberg, </author> <title> "Correcting Chromatic Aberrations Using Image Warping", </title> <booktitle> DARPA Image Understanding Workshop, </booktitle> <year> 1992. </year>
Reference-contexts: The effect of nonlinear 11 response is virtually impossible to detect, except with careful calibration [34]. Another problem that has been shown to cause color skewing in calibration studies is chromatic aberration <ref> [2] </ref>. This phenomenon occurs because the focal length of a lens is a function of the wavelength of the light incident upon the lens. Hence, different colors may focus at different points with respect to the image plane and the optical axis. <p> There are two types of displacement caused by chromatic aberration, lateral and longitudinal. Lateral chromatic aberration can cause light of a certain wavelength to focus on a cell neighboring the intended cell, causing color mixing. Experiments <ref> [2, 34] </ref> indicate that this type of color mixing occurs mostly along surface boundaries, thus leaving the non-boundary pixels unaffected. Longitudinal displacement of light, i.e., along the optical axis can cause unequal blurring of different wavelengths. The same experiments [2, 34] indicate that parametric methods sensitive to small perturbations in the <p> Experiments <ref> [2, 34] </ref> indicate that this type of color mixing occurs mostly along surface boundaries, thus leaving the non-boundary pixels unaffected. Longitudinal displacement of light, i.e., along the optical axis can cause unequal blurring of different wavelengths. The same experiments [2, 34] indicate that parametric methods sensitive to small perturbations in the assumed physics-based models are far more likely to be affected by such blurring than are the empirical methods used in this study, given the relative magnitude of color shifts due to the other complicating factors. 3.6 Overall distribution in
Reference: [3] <author> L. Breiman, J.H. Friedman, R.A. Olshen and C.J. Stone, </author> <title> Classification and Regression Trees, </title> <address> Belmont, CA: </address> <publisher> Wadsworth International Group, </publisher> <year> 1984. </year>
Reference-contexts: Like other non-parametric learning techniques, decision trees are susceptible to over-training. In order to correct for over-fitting, a fully grown tree can be pruned <ref> [3, 41, 4] </ref> by determining the classification error for each non-leaf subtree, and then comparing it to the classification error resulting from replacing the subtree with a leaf-node bearing the class label of the majority of the training instances in the set.
Reference: [4] <author> C.E. Brodley and P.E. Utgoff, </author> <title> "Multivariate decision trees", </title> <booktitle> Machine Learning, </booktitle> <year> 1995. </year>
Reference-contexts: It is then shown that such distributions can be "learned" from training samples using Multivariate Decision Trees (MDT's) <ref> [4] </ref> for non-parametric approximation of decision boundaries around the training samples. Image pixels are then classified according to their location with respect to the learned decision boundaries. <p> Another way of classifying pixels is to segment the feature space and classify pixel instances based on their position in the segmented feature space. This can be done in a number of ways: by drawing explicit piecewise-linear boundaries in RGB space (decision trees <ref> [41, 4] </ref>); by learning a nonlinear function (genetic algorithms [31] and radial basis functions [39]) that maps RGB values explicitly to numerical values which are then thresholded to find decision boundaries; and by learning a mapping function that uses RGB as the input feature space but maps the input feature space <p> Multivariate Decision Trees <ref> [4] </ref> are more general, and fit hyperplanes of arbitrary orientation around the distributions. Genetic algorithms (GA's) use principles from evolutionary biology to converge on optimal parameters of a fixed-dimensional nonlinear polynomial function. Radial basis functions (RBF's) approximate a 14 function as a weighted sum of Gaussians. <p> Although NN's can be expected to perform well for RGB distributions in this application, the arbitrary nature of the hidden layer feature space makes analysis difficult; consequently, the work presented here uses Multivariate Decision Trees. 5 Multivariate Decision Trees Multivariate Decision Trees (MDT's) <ref> [4] </ref> create piecewise-linear approximations of regions in feature space by recursively dividing feature space with hyperplanes (figure 6). MDT's recursively subdivide the feature space by linear threshold units (LTU's) [33, 12]. The LTU's are binary tests, represented by linear combinations of feature values and associated weights. <p> Like other non-parametric learning techniques, decision trees are susceptible to over-training. In order to correct for over-fitting, a fully grown tree can be pruned <ref> [3, 41, 4] </ref> by determining the classification error for each non-leaf subtree, and then comparing it to the classification error resulting from replacing the subtree with a leaf-node bearing the class label of the majority of the training instances in the set.
Reference: [5] <author> T.A. Brown and J. Koplowitz, </author> <title> "The weighted nearest neighbor rule for class dependent sample sizes", </title> <journal> IEEE Transactions of Information Theory, </journal> <volume> 25 </volume> <pages> 617-619, </pages> <year> 1979. </year>
Reference-contexts: There are a number of techniques that have been used in other domains for function approximation and classification. In nearest-neighbor classification <ref> [51, 5] </ref>, given a set X n = fx 0 ; x 1 ; :::; x n g of n independent samples, a new instance x n+1 is classified according to the distances between x n+1 and each element of the set fx 0 ; x 1 ; :::; x n
Reference: [6] <author> G. Buchsbaum, </author> <title> "A Spatial Processor Model for Object Colour Perception", </title> <journal> Journal of the Franklin Institute, </journal> <volume> 310 </volume> <pages> 1-26, </pages> <year> 1980. </year>
Reference-contexts: Among the algorithms that make assumptions about the statistical distributions of surface colors in the scene, Buchsbaum <ref> [6] </ref> assumes that the average of the surface reflectances over the entire scene is gray (the gray-world assumption); Gershon [19] assumes that the average scene reflectance matches that of another known color; Vrhel [47] assumes knowledge of the general covariance structure of the illuminant, given a small set of illuminants, and
Reference: [7] <author> S. Buluswar, </author> <title> Trichromatic model of Daylight Variation, </title> <institution> University of Massachusetts Computer Science Department, </institution> <type> technical report, </type> <institution> UM-CS-1995-012. </institution>
Reference-contexts: For instance, Maloney [28] and Yuille [52] assume that the linear combination of two basis functions is sufficient. Under the assumption, the variation in surface color in a three-dimensional color space would follow a plane. Daylight, however, follows a parabolic surface in three dimensions (RGB) <ref> [7] </ref>; hence, the assumptions of these methods are true only under specifically controlled illumination. <p> The CIE daylight model [22] describes the variation in daylight color as a parabola in the CIE chromaticity space 1 (figure 4). y = 2:87x 3:0x 2 0:275; (1) where 0:25 &lt;= x &lt;= 0:38. In RGB space, the parabola stretches out into a thin paraboloid surface <ref> [7] </ref>. 1 RGB is a linear transform of the CIE chromaticity space [20] 8 3.2 Illumination geometry and viewing geometry Illumination geometry, i.e., the orientation of the surface normal with respect to the illuminant, affects the composition of the light incident upon the surface.
Reference: [8] <author> H.R. Condit and F. </author> <title> Grum, </title> <journal> "Spectral Energy Distribution of Daylight" Journal of the Optical Society of America, </journal> <volume> 54(7) </volume> <pages> 937-944, </pages> <year> 1964. </year>
Reference: [9] <author> J. Crisman and C. Thorpe, </author> <title> "Color Vision for Road Following", Vision and Navigation: </title> <publisher> The Carnegie Mellon NAVLAB, Kluwer, </publisher> <year> 1990. </year> <month> 21 </month>
Reference-contexts: 1 Introduction Machine vision techniques are increasingly being used in intelligent autonomous vehicle systems <ref> [9, 11, 24, 30, 40, 46] </ref>. <p> 1 Introduction Machine vision techniques are increasingly being used in intelligent autonomous vehicle systems [9, 11, 24, 30, 40, 46]. Most of these systems (with a few exceptions <ref> [9, 40, 48] </ref>) do not utilize color, despite the fact that color can be a useful feature for detecting objects such as lanes, obstacles and traffic signs, and even though color cameras are becoming an increasingly inexpensive part of autonomous vehicle platforms. <p> Crisman's SCARF algorithm <ref> [9] </ref> approximates an "average" road color from samples, and models the variation of the color of the road under daylight as Gaussian noise about an empirically derived "average" road color; pixels are then classified based on minimum-distance likelihood.
Reference: [10] <author> J.E. Dayhoff, </author> <title> Neural Network Architectures, </title> <publisher> Van Nostrand Reinhold, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: values explicitly to numerical values which are then thresholded to find decision boundaries; and by learning a mapping function that uses RGB as the input feature space but maps the input feature space to an intermediate feature space, so as the facilitate boundary fitting (neural networks with a hidden layer <ref> [42, 10] </ref>). (Univariate) Decision Trees [41] approximate a boundary by fitting hyperplanes around the samples, orthogonal to the axes of the feature space. Multivariate Decision Trees [4] are more general, and fit hyperplanes of arbitrary orientation around the distributions.
Reference: [11] <author> E. D. Dickmanns and B. D. Mysliwetz, </author> <title> "Recursive 3-D road and relative ego-state recognition", </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 14(2) </volume> <pages> 199-213, </pages> <year> 1992. </year>
Reference-contexts: 1 Introduction Machine vision techniques are increasingly being used in intelligent autonomous vehicle systems <ref> [9, 11, 24, 30, 40, 46] </ref>. <p> In addition, there has been work on lane-finding and obstacle-detection for autonomous vehicles in gray-scale images based on edge-detection or stereo <ref> [11, 24, 29, 30] </ref>; since this paper is concerned with the use of color information, gray-scale techniques will not be discussed in the literature review. 2.1 Computational color constancy Most of the work in computational color recognition under varying illumination has been in the area of color constancy, the goal of
Reference: [12] <author> R.O. Duda and P.E. Hart, </author> <title> Pattern Classification and Scene Analysis, </title> <address> New York: </address> <publisher> Wiley & Sons, </publisher> <year> 1973. </year>
Reference-contexts: MDT's recursively subdivide the feature space by linear threshold units (LTU's) <ref> [33, 12] </ref>. The LTU's are binary tests, represented by linear combinations of feature values and associated weights. Each division attempts to separate, in a set of known instances (the training set), target instances from non-targets.
Reference: [13] <author> G.D. Finlayson, </author> <title> "Color Constancy in Diagonal Chromaticity Space", </title> <booktitle> Proceedings of the Fifth International Conference on Computer Vision, </booktitle> <year> 1995. </year>
Reference-contexts: In a variation of this algorithm, Finlayson <ref> [13] </ref> applies a spectral sharpening transform to the sensory data in order to relax the gamut constraints. The assumptions about gamut-mapping restrict the application of CRULE to matte Mondrian surfaces under controlled illumination and fixed orientation.
Reference: [14] <author> G.D. Finlayson, B.V. Funt and K. Barnard, </author> <title> "Color Constancy Under Varying Illumination", </title> <booktitle> Proceedings of the Fifth International Conference on Computer Vision, </booktitle> <year> 1995. </year>
Reference-contexts: These methods are based on the assumption of a single point-source illuminant; this assumption is not valid for an extended or non-point-source illuminant such as daylight. In yet another approach, D'Zmura [53]and Finlayson <ref> [14] </ref> require light from multiple illumi-nants incident upon the multiple instances of a single surface in the same scene. The problem with these approaches is that they require identification of the same surface in two spatially distinct parts of the image that are subject to different illuminants.
Reference: [15] <author> D. </author> <title> Forsyth."A Novel Approach for Color Constancy", </title> <journal> International Journal of Computer Vision, </journal> <volume> 5 </volume> <pages> 5-36, </pages> <year> 1990. </year>
Reference-contexts: Daylight, however, follows a parabolic surface in three dimensions (RGB) [7]; hence, the assumptions of these methods are true only under specifically controlled illumination. Among the algorithms that make assumptions about image gamuts is Forsyth's CRULE (coefficient rule) algorithm <ref> [15] </ref>, which maps the gamut of possible image colors to another gamut of colors that is known a-priori, so that the number of possible mappings restricts the set of pos 5 sible illuminants. <p> Such assumptions, while applicable to controlled settings, are not generally applicable to unconstrained images. The assumptions made by the aforementioned algorithms are such that most of them perform only on highly restricted images (such as Mondrians), under mostly constrained lighting. Forsyth <ref> [15] </ref> aptly states, "Experimental results for [color constancy] algorithms running on real images are not easily found in the literature: : : Some work exists on the processes which can contribute to real world lightness constancy, but very little progress has been made in this area." 6 2.2 Non-parametric (sample-based) approaches
Reference: [16] <author> W. Freeman and D. Brainard, </author> <title> "Bayesian Decision Theory: the maximum local mass estimate", </title> <booktitle> Proceedings of the Fifth International Conference on Computer Vision, </booktitle> <year> 1995. </year>
Reference-contexts: that the average of the surface reflectances over the entire scene is gray (the gray-world assumption); Gershon [19] assumes that the average scene reflectance matches that of another known color; Vrhel [47] assumes knowledge of the general covariance structure of the illuminant, given a small set of illuminants, and Freeman <ref> [16] </ref> assumes that the illumination and reflection in a scene follow known probability distributions. These methods are effective when the distribution of colors within the scene follows the assumed model or distribution.
Reference: [17] <author> B.V. Funt, G.D. Finlayson, </author> <title> "The State of Computational Color Constancy", </title> <booktitle> Proceedings of the First Pan-Chromatic Conference, Inter-Society Color Council, </booktitle> <year> 1995. </year>
Reference-contexts: Depending on their assumptions and techniques, color constancy algorithms can be divided into the following six categories <ref> [17] </ref>: (1) those which make assumptions about the statistical distribution of surface colors in the scene, (2) those which make assumptions about the types of reflection and illumination, (3) those assuming a fixed image gamut, (4) those which obtain an indirect measure of the illuminant, (5) those which require multiple illuminants,
Reference: [18] <author> B.V. Funt and M.S. Drew, </author> <title> "Color Space Analysis of Mutual Illumination", </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 12 </volume> <pages> 1319-1326, </pages> <year> 1993. </year>
Reference-contexts: Another class of algorithms uses an indirect measure of the illumination. For instance, Shafer [44], Klinker [23] and Lee [26] use surface specularities (Sato [43] uses a similar principle, but not for color constancy); similarly, Funt <ref> [18] </ref> uses inter-reflections to measure the illuminant. These methods are based on the assumption of a single point-source illuminant; this assumption is not valid for an extended or non-point-source illuminant such as daylight.
Reference: [19] <author> R. Gershon, A. Jepson and J. Tsotsos, </author> <title> The Effects of Ambient Illumination on the Structure of Shadows in Chromatic Images. </title> <institution> RBCV-TR-86-9, Dept. of Computer Science, University of Toronto, </institution> <year> 1986. </year>
Reference-contexts: In outdoor images, the color of the illuminant (i.e., daylight) varies with the time-of-day, cloud cover and other atmospheric conditions [22]; the illuminant and viewing geometry vary with changes in object and camera position and orientation. In addition, shadows and inter-reflectances <ref> [19] </ref>, and certain sensor response parameters [34], all of which can be difficult to model in outdoor scenarios, may also affect the apparent color of objects. <p> Among the algorithms that make assumptions about the statistical distributions of surface colors in the scene, Buchsbaum [6] assumes that the average of the surface reflectances over the entire scene is gray (the gray-world assumption); Gershon <ref> [19] </ref> assumes that the average scene reflectance matches that of another known color; Vrhel [47] assumes knowledge of the general covariance structure of the illuminant, given a small set of illuminants, and Freeman [16] assumes that the illumination and reflection in a scene follow known probability distributions. <p> Evidently, different types of surfaces exhibit different color shifts, depending on the combination of the aforementioned factors. 3.4 Shadows and inter-reflections Inter-reflections and shadows can cause a further variation in color by altering the color of the light incident upon the surface <ref> [19] </ref>. Inter-reflections, for instance, cause light reflected off other surfaces in the scene to be incident upon the surface being examined.
Reference: [20] <institution> F.S. Hill, Computer Graphics, Macmillan, </institution> <address> New York, </address> <year> 1990. </year>
Reference-contexts: In RGB space, the parabola stretches out into a thin paraboloid surface [7]. 1 RGB is a linear transform of the CIE chromaticity space <ref> [20] </ref> 8 3.2 Illumination geometry and viewing geometry Illumination geometry, i.e., the orientation of the surface normal with respect to the illuminant, affects the composition of the light incident upon the surface.
Reference: [21] <author> B.K.P. Horn, </author> <title> Robot Vision, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1987. </year> <month> 22 </month>
Reference-contexts: The color (or rather, the apparent color) of an object depends on illuminant color, the reflectance of the object, illumination geometry (orientation of the surface normal with respect to the illuminant), viewing geometry (orientation of the surface normal with respect to the sensor), and sensor parameters <ref> [21] </ref>. In outdoor images, the color of the illuminant (i.e., daylight) varies with the time-of-day, cloud cover and other atmospheric conditions [22]; the illuminant and viewing geometry vary with changes in object and camera position and orientation. <p> The regions of the color circle representing the colors of sunlight and skylight (empirically determined) are shown. The standard model of image formation <ref> [21] </ref> describes the observed color of objects in an image as a function of (i) the color of the incident light (daylight, in the case of outdoor images), (ii) the reflectance properties of the surface of the object (iii) the illumination geometry, (iv) the viewing geometry, and (v) the imaging parameters. <p> in the assumed physics-based models are far more likely to be affected by such blurring than are the empirical methods used in this study, given the relative magnitude of color shifts due to the other complicating factors. 3.6 Overall distribution in RGB space Assuming, from the standard image formation model <ref> [21] </ref>, that apparent color is determined by the product of the incident light and the surface reflectance, and then somewhat altered by shadows, inter-reflections and imaging parameters, it can be deduced that the RGB distributions can be arbitrarily shaped, depending on the nature of the surface.
Reference: [22] <author> D. Judd, D. MacAdam and G. Wyszecki, </author> <title> "Spectral Distribution of Typical Daylight as a Function of Correlated Color Temperature", </title> <journal> Journal of the Optical Society of America, </journal> <volume> 54(8) </volume> <pages> 1031-1040, </pages> <year> 1964. </year>
Reference-contexts: In outdoor images, the color of the illuminant (i.e., daylight) varies with the time-of-day, cloud cover and other atmospheric conditions <ref> [22] </ref>; the illuminant and viewing geometry vary with changes in object and camera position and orientation. In addition, shadows and inter-reflectances [19], and certain sensor response parameters [34], all of which can be difficult to model in outdoor scenarios, may also affect the apparent color of objects. <p> These methods are effective when the distribution of colors within the scene follows the assumed model or distribution. In outdoor scenes, the CIE daylight model <ref> [22] </ref> suggests that the gray-world assumption will not be valid; at the same time, as later sections will show, no general assumptions can be made about the distribution of surface colors even if the distribution of daylight color is known. <p> The CIE daylight model <ref> [22] </ref> describes the variation in daylight color as a parabola in the CIE chromaticity space 1 (figure 4). y = 2:87x 3:0x 2 0:275; (1) where 0:25 &lt;= x &lt;= 0:38.
Reference: [23] <author> G.J. Klinker, S.A. Shafer and T. Kanade, </author> <title> "Color image analysis with an intrinsic reflection model", </title> <booktitle> Proceedings of the International Conference on Computer Vision, </booktitle> <year> 1988. </year>
Reference-contexts: By restricting the illumination, this method is applied only to synthetic or highly constrained indoor images. Another class of algorithms uses an indirect measure of the illumination. For instance, Shafer [44], Klinker <ref> [23] </ref> and Lee [26] use surface specularities (Sato [43] uses a similar principle, but not for color constancy); similarly, Funt [18] uses inter-reflections to measure the illuminant. <p> The Dichromatic reflection model (originally proposed by Shafer [44] and subsequently extended by Novak [35], Lee [26] and Klinker <ref> [23] </ref>) models the net reflection of a surface as the linear combination of the specular and Lambertian reflection components.
Reference: [24] <author> K. Kluge and C. Thorpe, </author> <title> "Representation and recovery of road geometry in YARF", Intelligent Vehicles, </title> <year> 1992. </year>
Reference-contexts: 1 Introduction Machine vision techniques are increasingly being used in intelligent autonomous vehicle systems <ref> [9, 11, 24, 30, 40, 46] </ref>. <p> In addition, there has been work on lane-finding and obstacle-detection for autonomous vehicles in gray-scale images based on edge-detection or stereo <ref> [11, 24, 29, 30] </ref>; since this paper is concerned with the use of color information, gray-scale techniques will not be discussed in the literature review. 2.1 Computational color constancy Most of the work in computational color recognition under varying illumination has been in the area of color constancy, the goal of
Reference: [25] <author> E.H. </author> <title> Land, </title> <journal> "Lightness and Retinex Theory", Scientific American, </journal> <volume> 237(6) </volume> <pages> 108-129, </pages> <month> Decem-ber </month> <year> 1977. </year>
Reference-contexts: Once again, the approaches have been shown to be effective only on Mondrian or similarly restricted images. The final group of color constancy algorithms assumes the presence of surfaces of known reflectance in the scene and then determine the illuminant. For instance, Land's Retinex algorithm <ref> [25] </ref> and its many variations require the presence of a surface of maximal (white) reflectance within the scene. Similarly, Novak's supervised color constancy algorithm [36] requires surfaces of other known reflectances. Such assumptions, while applicable to controlled settings, are not generally applicable to unconstrained images.
Reference: [26] <author> S.W. Lee, </author> <title> Understanding of Surface Reflections in Computer Vision by Color and Multiple Views, </title> <type> Ph.D. Dissertation, </type> <institution> University of Pennsylvania, </institution> <year> 1992. </year>
Reference-contexts: By restricting the illumination, this method is applied only to synthetic or highly constrained indoor images. Another class of algorithms uses an indirect measure of the illumination. For instance, Shafer [44], Klinker [23] and Lee <ref> [26] </ref> use surface specularities (Sato [43] uses a similar principle, but not for color constancy); similarly, Funt [18] uses inter-reflections to measure the illuminant. These methods are based on the assumption of a single point-source illuminant; this assumption is not valid for an extended or non-point-source illuminant such as daylight. <p> The Dichromatic reflection model (originally proposed by Shafer [44] and subsequently extended by Novak [35], Lee <ref> [26] </ref> and Klinker [23]) models the net reflection of a surface as the linear combination of the specular and Lambertian reflection components.
Reference: [27] <institution> Lockheed-Martin Corp., from DARPA UGV DEMO-C, </institution> <year> 1995. </year>
Reference-contexts: In both tests, between 50% and 95% of the on-target pixels were correctly classified, enough to form clusters approximately the expected size of the targets. In further UGV tests, the color-based system was combined with an infra-red system <ref> [27] </ref> to further improve the performance. 7 Future work and conclusions In all of the tests, there has been a large number of false positives.
Reference: [28] <author> L.T. Maloney and B.A. Wandell, </author> <title> "Color Constancy: A Method for Recovering Surface Spectral Reflectance", </title> <journal> Journal of the Optical Society of America, </journal> <volume> A3:29-33, </volume> <year> 1986. </year>
Reference-contexts: Consequently, these methods are too restrictive for all but very constrained scenes. The second set of color constancy algorithms make assumptions about the dimensionality of spectral basis functions [45] required to accurately model illumination and surface reflectance. For instance, Maloney <ref> [28] </ref> and Yuille [52] assume that the linear combination of two basis functions is sufficient. Under the assumption, the variation in surface color in a three-dimensional color space would follow a plane.
Reference: [29] <author> I. Masaki, </author> <title> Vision-based Vehicle Guidance, </title> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: In addition, there has been work on lane-finding and obstacle-detection for autonomous vehicles in gray-scale images based on edge-detection or stereo <ref> [11, 24, 29, 30] </ref>; since this paper is concerned with the use of color information, gray-scale techniques will not be discussed in the literature review. 2.1 Computational color constancy Most of the work in computational color recognition under varying illumination has been in the area of color constancy, the goal of
Reference: [30] <author> L. Matthies, A. Kelly, T. Litwin and G. Tharp, </author> <title> "Obstacle detection for unmanned ground vehicles: A progress report", Intelligent Vehicles, </title> <year> 1995. </year>
Reference-contexts: 1 Introduction Machine vision techniques are increasingly being used in intelligent autonomous vehicle systems <ref> [9, 11, 24, 30, 40, 46] </ref>. <p> In addition, there has been work on lane-finding and obstacle-detection for autonomous vehicles in gray-scale images based on edge-detection or stereo <ref> [11, 24, 29, 30] </ref>; since this paper is concerned with the use of color information, gray-scale techniques will not be discussed in the literature review. 2.1 Computational color constancy Most of the work in computational color recognition under varying illumination has been in the area of color constancy, the goal of
Reference: [31] <author> M. Mitchell, </author> <title> An Introduction to Genetic Algorithms, </title> <publisher> MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: This can be done in a number of ways: by drawing explicit piecewise-linear boundaries in RGB space (decision trees [41, 4]); by learning a nonlinear function (genetic algorithms <ref> [31] </ref> and radial basis functions [39]) that maps RGB values explicitly to numerical values which are then thresholded to find decision boundaries; and by learning a mapping function that uses RGB as the input feature space but maps the input feature space to an intermediate feature space, so as the facilitate
Reference: [32] <author> S.K. Nayar, K. Ikeuchi and T. Kanade, </author> <title> "Determining Shape and Reflectance of Hybrid Surfaces by Photometric Sampling", </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> 6 </volume> <pages> 418-431, </pages> <year> 1990. </year>
Reference-contexts: Most realistic surfaces have components of both Lambertian and specular reflection. A number of models have been applied with varying degrees of success to such surfaces, most notably Phong's shading model [38], Shafer's Dichromatic model [44], and Nayar's hybrid reflection model based on photometric sampling <ref> [32] </ref>. The Dichromatic reflection model (originally proposed by Shafer [44] and subsequently extended by Novak [35], Lee [26] and Klinker [23]) models the net reflection of a surface as the linear combination of the specular and Lambertian reflection components. <p> The Phong model has been very widely applied by the computer graphics community as an effective method of achieving shading effects in rendering grey-scale and color images. Nayar <ref> [32] </ref> describes the brightness of surface reflectance as a linear combination of the Lambertian and specular components (a concept similar to the Dichromatic Model [44]): I = IL + IS; (3) where I is the total intensity at a given point in the surface, and IL and IS the intensities of <p> IL = Acos ( s n ) (Lambert's law), where A is the constant representing the weight of the Lambertian component, and s and n are the directions of the illumination source and the surface normal. IS, modeled by the delta function <ref> [32] </ref>, is Bffi ( s 2 n ), where B is the weight of the specular component. Hence, I = Acos ( s n )+Bffi ( s 2 n ).
Reference: [33] <author> N.J. Nilsson, </author> <title> Learning Machines, </title> <address> New York: McGraw Hill, </address> <year> 1965. </year>
Reference-contexts: MDT's recursively subdivide the feature space by linear threshold units (LTU's) <ref> [33, 12] </ref>. The LTU's are binary tests, represented by linear combinations of feature values and associated weights. Each division attempts to separate, in a set of known instances (the training set), target instances from non-targets.
Reference: [34] <author> C. Novak, S. Shafer and R. Wilson, </author> <title> "Obtaining Accurate Color Images for Machine Vision Research", </title> <booktitle> Proceedings of the SPIE, v 1250, </booktitle> <year> 1990. </year> <month> 23 </month>
Reference-contexts: In outdoor images, the color of the illuminant (i.e., daylight) varies with the time-of-day, cloud cover and other atmospheric conditions [22]; the illuminant and viewing geometry vary with changes in object and camera position and orientation. In addition, shadows and inter-reflectances [19], and certain sensor response parameters <ref> [34] </ref>, all of which can be difficult to model in outdoor scenarios, may also affect the apparent color of objects. <p> Clipping is easily detected but not easily avoided, especially in outdoor images, where it is difficult for imaging hardware to adapt to the variation in the range of intensities. Any software approach to interpreting clipped pixels is bound to be ad-hoc and domain-dependent <ref> [34] </ref>; hence, until improvements in sensor design take place, machine vision methods may be forced to simply detect clipped pixels and discard those points in the image. Blooming is a related phenomenon, where sensor cells saturated due to clipping "bleed" into neighboring cells. <p> Blooming is a related phenomenon, where sensor cells saturated due to clipping "bleed" into neighboring cells. Blooming is harder to detect, except through finding clipped pixels and probabilistically tracing pixel values in the direction most likely to cause blooming <ref> [34] </ref>. On one hand, blooming is a much more serious problem than clipping because it is harder to detect; on the other, inter-cell bleeding is a simpler problem to prevent from a hardware design point of view. <p> The effect of nonlinear 11 response is virtually impossible to detect, except with careful calibration <ref> [34] </ref>. Another problem that has been shown to cause color skewing in calibration studies is chromatic aberration [2]. This phenomenon occurs because the focal length of a lens is a function of the wavelength of the light incident upon the lens. <p> There are two types of displacement caused by chromatic aberration, lateral and longitudinal. Lateral chromatic aberration can cause light of a certain wavelength to focus on a cell neighboring the intended cell, causing color mixing. Experiments <ref> [2, 34] </ref> indicate that this type of color mixing occurs mostly along surface boundaries, thus leaving the non-boundary pixels unaffected. Longitudinal displacement of light, i.e., along the optical axis can cause unequal blurring of different wavelengths. The same experiments [2, 34] indicate that parametric methods sensitive to small perturbations in the <p> Experiments <ref> [2, 34] </ref> indicate that this type of color mixing occurs mostly along surface boundaries, thus leaving the non-boundary pixels unaffected. Longitudinal displacement of light, i.e., along the optical axis can cause unequal blurring of different wavelengths. The same experiments [2, 34] indicate that parametric methods sensitive to small perturbations in the assumed physics-based models are far more likely to be affected by such blurring than are the empirical methods used in this study, given the relative magnitude of color shifts due to the other complicating factors. 3.6 Overall distribution in
Reference: [35] <author> C. Novak and S. Shafer, </author> <title> A Method for Estimating Scene Parameters from Color Histograms, </title> <institution> Carnegie Mellon University School of Computer Science, </institution> <type> technical report, </type> <institution> CMU-CS-93-177, </institution> <year> 1993. </year>
Reference-contexts: A number of models have been applied with varying degrees of success to such surfaces, most notably Phong's shading model [38], Shafer's Dichromatic model [44], and Nayar's hybrid reflection model based on photometric sampling [32]. The Dichromatic reflection model (originally proposed by Shafer [44] and subsequently extended by Novak <ref> [35] </ref>, Lee [26] and Klinker [23]) models the net reflection of a surface as the linear combination of the specular and Lambertian reflection components.
Reference: [36] <author> C. Novak, </author> <title> "Supervised Color Constancy for Machine Vision", </title> <booktitle> Proceedings of the SPIE: Conference on Visual Processing and Digital Display, </booktitle> <year> 1991. </year>
Reference-contexts: For instance, Land's Retinex algorithm [25] and its many variations require the presence of a surface of maximal (white) reflectance within the scene. Similarly, Novak's supervised color constancy algorithm <ref> [36] </ref> requires surfaces of other known reflectances. Such assumptions, while applicable to controlled settings, are not generally applicable to unconstrained images. The assumptions made by the aforementioned algorithms are such that most of them perform only on highly restricted images (such as Mondrians), under mostly constrained lighting.
Reference: [37] <author> Y. Ohta and Y. Hayashi, </author> <title> "Recovery of Illuminant and Surface Colors from Images Based on the CIE Daylight", </title> <booktitle> Proceedings of the Third European Conference on Computer Vision, </booktitle> <year> 1994. </year>
Reference-contexts: In a variation of this algorithm, Finlayson [13] applies a spectral sharpening transform to the sensory data in order to relax the gamut constraints. The assumptions about gamut-mapping restrict the application of CRULE to matte Mondrian surfaces under controlled illumination and fixed orientation. Ohta <ref> [37] </ref> assumes a known gamut of illuminants (controlled indoor lighting that lies on some points along the CIE model), and uses multi-image correspondence to determine the specific illuminant from the known set. By restricting the illumination, this method is applied only to synthetic or highly constrained indoor images.
Reference: [38] <author> B.T. Phong, </author> <title> "Illumination for Computer Generated Images", </title> <journal> Communications of the ACM, </journal> <volume> 18 </volume> <pages> 311-317. </pages>
Reference-contexts: Most realistic surfaces have components of both Lambertian and specular reflection. A number of models have been applied with varying degrees of success to such surfaces, most notably Phong's shading model <ref> [38] </ref>, Shafer's Dichromatic model [44], and Nayar's hybrid reflection model based on photometric sampling [32]. <p> Specularities in the 9 image are used to determine the weights for each component. The Phong shading model <ref> [38] </ref> approximates the falloff in brightness of specular reflection as cos n (ff), where ff is the difference between the viewing angle and the angle of maximal specular reflectance.
Reference: [39] <author> T. Poggio, and F. Girosi, </author> <title> "Regularization algorithms for learning that are equivalent to multilayer networks" Science, </title> <booktitle> 247 </booktitle> <pages> 978-982, </pages> <year> 1990. </year>
Reference-contexts: This can be done in a number of ways: by drawing explicit piecewise-linear boundaries in RGB space (decision trees [41, 4]); by learning a nonlinear function (genetic algorithms [31] and radial basis functions <ref> [39] </ref>) that maps RGB values explicitly to numerical values which are then thresholded to find decision boundaries; and by learning a mapping function that uses RGB as the input feature space but maps the input feature space to an intermediate feature space, so as the facilitate boundary fitting (neural networks with
Reference: [40] <author> D.A. Pomerleau, </author> <title> Neural Network Perception for Mobile Robot Guidance, </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, </address> <year> 1993. </year>
Reference-contexts: 1 Introduction Machine vision techniques are increasingly being used in intelligent autonomous vehicle systems <ref> [9, 11, 24, 30, 40, 46] </ref>. <p> 1 Introduction Machine vision techniques are increasingly being used in intelligent autonomous vehicle systems [9, 11, 24, 30, 40, 46]. Most of these systems (with a few exceptions <ref> [9, 40, 48] </ref>) do not utilize color, despite the fact that color can be a useful feature for detecting objects such as lanes, obstacles and traffic signs, and even though color cameras are becoming an increasingly inexpensive part of autonomous vehicle platforms. <p> For instance, in the case of the examples in figures 1 and 2, this approach would calculate an average color for each surface from the corresponding distribution, and use that average as the most likely color of the object under any set of conditions. Pomerleau's ALVINN road-follower <ref> [40] </ref> uses color images of road scenes along with user-induced steering signals to train a neural network to follow road/lane markers.
Reference: [41] <author> J.R. Quinlan, </author> <title> "Induction of Decision Trees", </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: Another way of classifying pixels is to segment the feature space and classify pixel instances based on their position in the segmented feature space. This can be done in a number of ways: by drawing explicit piecewise-linear boundaries in RGB space (decision trees <ref> [41, 4] </ref>); by learning a nonlinear function (genetic algorithms [31] and radial basis functions [39]) that maps RGB values explicitly to numerical values which are then thresholded to find decision boundaries; and by learning a mapping function that uses RGB as the input feature space but maps the input feature space <p> which are then thresholded to find decision boundaries; and by learning a mapping function that uses RGB as the input feature space but maps the input feature space to an intermediate feature space, so as the facilitate boundary fitting (neural networks with a hidden layer [42, 10]). (Univariate) Decision Trees <ref> [41] </ref> approximate a boundary by fitting hyperplanes around the samples, orthogonal to the axes of the feature space. Multivariate Decision Trees [4] are more general, and fit hyperplanes of arbitrary orientation around the distributions. <p> Like other non-parametric learning techniques, decision trees are susceptible to over-training. In order to correct for over-fitting, a fully grown tree can be pruned <ref> [3, 41, 4] </ref> by determining the classification error for each non-leaf subtree, and then comparing it to the classification error resulting from replacing the subtree with a leaf-node bearing the class label of the majority of the training instances in the set.
Reference: [42] <author> D.E. Rumelhart, G.E. Hinton and J.L. McLelland, </author> <title> "A general framework for parallel distributed processing", Parallel Distributed Processing: Explorations in the microstructures of cognition, </title> <publisher> Bradford Books/ MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: values explicitly to numerical values which are then thresholded to find decision boundaries; and by learning a mapping function that uses RGB as the input feature space but maps the input feature space to an intermediate feature space, so as the facilitate boundary fitting (neural networks with a hidden layer <ref> [42, 10] </ref>). (Univariate) Decision Trees [41] approximate a boundary by fitting hyperplanes around the samples, orthogonal to the axes of the feature space. Multivariate Decision Trees [4] are more general, and fit hyperplanes of arbitrary orientation around the distributions.
Reference: [43] <author> Y. Sato and K. </author> <title> Ikeuchi, "Reflectance analysis under solar illumination", </title> <booktitle> Proceedings of the IEEE Workshop for Physics-based Modeling in Computer Vision, </booktitle> <year> 1995. </year>
Reference-contexts: By restricting the illumination, this method is applied only to synthetic or highly constrained indoor images. Another class of algorithms uses an indirect measure of the illumination. For instance, Shafer [44], Klinker [23] and Lee [26] use surface specularities (Sato <ref> [43] </ref> uses a similar principle, but not for color constancy); similarly, Funt [18] uses inter-reflections to measure the illuminant. These methods are based on the assumption of a single point-source illuminant; this assumption is not valid for an extended or non-point-source illuminant such as daylight.
Reference: [44] <author> S.A. Shafer, </author> <title> "Using Color to Separate Reflection Components", Color Research Application, </title> <booktitle> 10 </booktitle> <pages> 210-218, </pages> <year> 1985. </year>
Reference-contexts: By restricting the illumination, this method is applied only to synthetic or highly constrained indoor images. Another class of algorithms uses an indirect measure of the illumination. For instance, Shafer <ref> [44] </ref>, Klinker [23] and Lee [26] use surface specularities (Sato [43] uses a similar principle, but not for color constancy); similarly, Funt [18] uses inter-reflections to measure the illuminant. <p> Most realistic surfaces have components of both Lambertian and specular reflection. A number of models have been applied with varying degrees of success to such surfaces, most notably Phong's shading model [38], Shafer's Dichromatic model <ref> [44] </ref>, and Nayar's hybrid reflection model based on photometric sampling [32]. The Dichromatic reflection model (originally proposed by Shafer [44] and subsequently extended by Novak [35], Lee [26] and Klinker [23]) models the net reflection of a surface as the linear combination of the specular and Lambertian reflection components. <p> A number of models have been applied with varying degrees of success to such surfaces, most notably Phong's shading model [38], Shafer's Dichromatic model <ref> [44] </ref>, and Nayar's hybrid reflection model based on photometric sampling [32]. The Dichromatic reflection model (originally proposed by Shafer [44] and subsequently extended by Novak [35], Lee [26] and Klinker [23]) models the net reflection of a surface as the linear combination of the specular and Lambertian reflection components. <p> Nayar [32] describes the brightness of surface reflectance as a linear combination of the Lambertian and specular components (a concept similar to the Dichromatic Model <ref> [44] </ref>): I = IL + IS; (3) where I is the total intensity at a given point in the surface, and IL and IS the intensities of the specular and Lambertian components.
Reference: [45] <author> J.L. Simonds, </author> <title> "Application of characteristic vector analysis to photographic and optical response data", </title> <journal> Journal of the Optical Society of America, </journal> <volume> 53(8), </volume> <year> 1963. </year> <month> 24 </month>
Reference-contexts: Consequently, these methods are too restrictive for all but very constrained scenes. The second set of color constancy algorithms make assumptions about the dimensionality of spectral basis functions <ref> [45] </ref> required to accurately model illumination and surface reflectance. For instance, Maloney [28] and Yuille [52] assume that the linear combination of two basis functions is sufficient. Under the assumption, the variation in surface color in a three-dimensional color space would follow a plane.
Reference: [46] <author> C.E. Thorpe, M. Herbert, T. Kanade and S. Shafer, </author> <title> "Vision and navigation for the Carnegie--Mellon NAVLAB", </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 10(3) </volume> <pages> 362-373. </pages>
Reference-contexts: 1 Introduction Machine vision techniques are increasingly being used in intelligent autonomous vehicle systems <ref> [9, 11, 24, 30, 40, 46] </ref>.
Reference: [47] <author> M.J. Vrhel and H.J. Trussell, </author> <title> "Filter considerations in color correction" IEEE Transactions on Image Processing, </title> <booktitle> 3 </booktitle> <pages> 147-161, </pages> <year> 1994. </year>
Reference-contexts: Among the algorithms that make assumptions about the statistical distributions of surface colors in the scene, Buchsbaum [6] assumes that the average of the surface reflectances over the entire scene is gray (the gray-world assumption); Gershon [19] assumes that the average scene reflectance matches that of another known color; Vrhel <ref> [47] </ref> assumes knowledge of the general covariance structure of the illuminant, given a small set of illuminants, and Freeman [16] assumes that the illumination and reflection in a scene follow known probability distributions.
Reference: [48] <author> A.M. Waxman, J.J. LeMoigne, L.S. Davis, B. Srinivasan, T.R. Kushner, E. Liang, T. Siddalingaiah, </author> <title> "A Visual Navigation System for Automonous Land Vehicles", </title> <journal> IEEE Transactions on Robotics and Automation A(3):124-141, </journal> <year> 1987. </year>
Reference-contexts: 1 Introduction Machine vision techniques are increasingly being used in intelligent autonomous vehicle systems [9, 11, 24, 30, 40, 46]. Most of these systems (with a few exceptions <ref> [9, 40, 48] </ref>) do not utilize color, despite the fact that color can be a useful feature for detecting objects such as lanes, obstacles and traffic signs, and even though color cameras are becoming an increasingly inexpensive part of autonomous vehicle platforms.
Reference: [49] <author> T. Yachik, </author> <title> "Status of Evaluation, </title> <booktitle> RSTA Workshop", DARPA Image Understanding Workshop, </booktitle> <year> 1995. </year>
Reference-contexts: Consequently, the hyperplanes of the MDT can make fine distinctions between target color and the background. The MDT-based system was tested on the Ft. Carson data set [1] by a DARPA-sanctioned study by LGA, Inc. <ref> [49] </ref>, and at UGV Demo-C. 19 images (left, targets marked with circles), binary classification (middle), targets extracted (right). are extracted from the binary classification image by using clustering target pixels and applying region-level heuristics such as (the range of) expected vehicle size (s) and aspect ratio.
Reference: [50] <author> P. Young, </author> <title> Recursive Estimation and Time-Series Analysis, </title> <address> New York: </address> <publisher> Springer-Verlag, </publisher> <year> 1984. </year>
Reference-contexts: Thus, each node in a decision tree is either a decision or a class. Figure 6 shows a 15 decision-tree operating in a three-dimensional feature space. Several methods exist for learning the weights in a linear threshold unit; this implementation uses the Recursive Least Squares (RLS) algorithm <ref> [50] </ref>. <p> weights, and P k = P k1 P k1 X k [1 + X T k P k1 (5) The weights are initialized randomly, and the matrix consists of 0 values everywhere except along the diagonal, which is set to a very large value: 10 6 according to Young's recommendation <ref> [50] </ref>.
Reference: [51] <author> T. Young and T. Calvert, </author> <title> Classification, Estimation and Pattern Recognition, </title> <publisher> Elsevier, </publisher> <year> 1974. </year>
Reference-contexts: There are a number of techniques that have been used in other domains for function approximation and classification. In nearest-neighbor classification <ref> [51, 5] </ref>, given a set X n = fx 0 ; x 1 ; :::; x n g of n independent samples, a new instance x n+1 is classified according to the distances between x n+1 and each element of the set fx 0 ; x 1 ; :::; x n
Reference: [52] <author> A. Yuille, </author> <title> "A method for computing spectral reflectance", </title> <journal> Biological Cybernetics, </journal> <volume> 56 </volume> <pages> 195-201, </pages> <year> 1987. </year>
Reference-contexts: Consequently, these methods are too restrictive for all but very constrained scenes. The second set of color constancy algorithms make assumptions about the dimensionality of spectral basis functions [45] required to accurately model illumination and surface reflectance. For instance, Maloney [28] and Yuille <ref> [52] </ref> assume that the linear combination of two basis functions is sufficient. Under the assumption, the variation in surface color in a three-dimensional color space would follow a plane.
Reference: [53] <author> M. D'Zmura, and G. Iverson, </author> <title> "Color Constancy: Basic theory of two stage linear recovery of spectral descriptions for lights and surfaces" Journal of the Optical Society of America, </title> <booktitle> A 10 </booktitle> <pages> 2148-2165, </pages> <year> 1993. </year>
References-found: 53

