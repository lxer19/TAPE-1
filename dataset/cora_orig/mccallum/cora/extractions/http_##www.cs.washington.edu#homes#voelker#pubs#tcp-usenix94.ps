URL: http://www.cs.washington.edu/homes/voelker/pubs/tcp-usenix94.ps
Refering-URL: http://www.cs.washington.edu/homes/voelker/vitae.html
Root-URL: http://www.cs.washington.edu
Title: Latency Analysis of TCP on an ATM Network  
Author: Alec Wolman, Geoff Voelker, and Chandramohan A. Thekkath 
Affiliation: Department of Computer Science and Engineering University of Washington  
Abstract: In this paper we characterize the latency of the BSD 4.4 alpha implementation of TCP on an ATM network. Latency reduction is a difficult task, and careful analysis is the first step towards reduction. We investigate the impact of both the network controller and the protocol implementation on latency. We find that a low latency network controller has a significant impact on the overall latency of TCP. We also characterize the impact on latency of some widely discussed improvements to TCP, such as header prediction and the combination of the checksum calculation with data copying. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Brian N. Bershad, Thomas E. Anderson, Edward D. Lazowska, and Henry M. Levy. </author> <title> Lightweight Remote Procedure Call. </title> <journal> In ACM Transactions on Computer Systems,8(1):37-55, </journal> <month> February, </month> <year> 1990. </year>
Reference-contexts: Our measurements used a range of packet sizes. Based upon previous studies of RPC and TCP traffic behavior, we chose a variety of packet lengths sized 500 bytes and smaller <ref> [1, 8] </ref>. We also measured packets of 1400 bytes (the Ethernet MTU minus protocol headers), 4000 bytes (fits on a single memory page including protocol headers), and 8000 bytes (fits on two memory pages including protocol headers, also close to our ATM MTU of 9K).
Reference: [2] <author> John B. Carter and Willy Zwaenepoel. </author> <title> Optimistic Implementation of Bulk Data Transfer. </title> <booktitle> In Proceedings of the 1989 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <month> May </month> <year> 1989, </year> <pages> pp. 61-69. </pages>
Reference-contexts: The second technique involves exploiting traffic locality to predict the next incoming packet to avoid the protocol control block (PCB) lookup cost. Others have studied using traffic locality to improve throughput for bulk data transfer protocols <ref> [2, 17] </ref>; we study its impact on latency. In the BSD implementation, the TCP input processing engine keeps a single entry cache of the most recently used PCB. If the incoming packet is from the same connection as the previous packet, the call to the PCB lookup routine is avoided.
Reference: [3] <author> Chran-Ham Chang, Dick Flower, John Forecast, Heather Gray, Bill Hawe, Ashok Nadkarni, K. K. Ramakr-ishna, Uttam Shikarpur and Kathy Wilde. </author> <title> High-Performance TCP/IP and UDP/IP Networking in DEC OSF/1 for Alpha AXP. </title> <journal> Digital Technical Journal, </journal> <volume> Vol. 5 No. 1, </volume> <month> Winter </month> <year> 1993, </year> <pages> pp. 44-61. </pages>
Reference-contexts: Prior studies have concentrated on characterizing and optimizing the throughput of TCP on substantially different hardware or networks than the ones we describe here (e.g., <ref> [3, 8] </ref>). In addition to focusing on an ATM network, we investigate how optimizations previously suggested for improving throughput affect latency. We believe that studying the latency characteristics of TCP on ATM networks is particularly interesting for two reasons. <p> We implemented a similar optimized checksum algorithm; the performance of this algorithm and the ULTRIX algorithm at user level are shown in Table 5. An optimization suggested in <ref> [3, 4] </ref> combines the checksum calculation with one of the data copies to eliminate redundant movement of data over the memory bus. In ULTRIX 4.2A, data is copied at least twice on both send and receive in addition to calculating the TCP checksum. <p> However, the details of the implementation can be quite difficult and heavily dependent on the details of the device driver. For comparison, the Digital OSF study also dicusses implementing a combined copy and checksum in the kernel <ref> [3] </ref>. It appears that their combined copy and checksum is only used on the receive side for incoming UDP packets. Also, their implementation combines the checksum with the copy from kernel to user space, rather than from device to kernel memory.
Reference: [4] <author> David D. Clark, Van Jacobson, John Romkey, and Howard Salwen. </author> <title> An Analysis of TCP Processing Overhead. </title> <journal> IEEE Communications Magazine, </journal> <month> June </month> <year> 1989, </year> <pages> 23-39. </pages>
Reference-contexts: Second, our study allows us to answer the following questions: Can we provide evidence that TCP is a viable option for a transport layer for RPC? How have the changes in technology affected the results of earlier studies (e.g., <ref> [4] </ref>)? Is latency dominated by the cost of operating system services, such as buffer management? If so, can the use of such services be reduced enough to make latency acceptable for applications that require low latency? 1.1 System Overview All of our experiments were run on a pair of DECstation 5000/200 <p> Section 2 summarizes our measurements of TCP latency on the baseline system. Sections 3 and 4 study the effect of several modifications motivated by the results in Section 2. These modifications are not new and have been suggested by others to improve throughput <ref> [4, 9] </ref>. However, our focus here is on the effect of these modifications on latency. 2 Measurement of the Baseline System The baseline system that we measured is the BSD 4.4 alpha TCP release operating on an ATM network. <p> Eliminating the checksum (discussed in Section 4.2) opens the possibility of eliminating these data copying costs given a network adapter that supports DMA. With a combined copy and TCP checksum, Clark et al. discuss a network adapter design that eliminates the need for a second copy <ref> [4] </ref>. <p> the issue of the data copy, we address the problem of reducing the remaining protocol processing time using header prediction in the next section and the problem of optimizing the checksum in a subsequent section. 3 Header Prediction Header prediction has often been suggested as a performance benefit for TCP <ref> [4] </ref>. There are two distinct kinds of optimizations that are often called header prediction. The first, involving prefilling parts of the transport header, is a known optimization for lowering latency [11, 15], and is not discussed further here. <p> We implemented a similar optimized checksum algorithm; the performance of this algorithm and the ULTRIX algorithm at user level are shown in Table 5. An optimization suggested in <ref> [3, 4] </ref> combines the checksum calculation with one of the data copies to eliminate redundant movement of data over the memory bus. In ULTRIX 4.2A, data is copied at least twice on both send and receive in addition to calculating the TCP checksum. <p> The graph in Figure 2 shows the relative performance of the three methods for calculating the TCP checksum and copying the data. We compare the performance of our implementation of the integrated checksum and copy with a user-level implementation on a Sun-3 described in <ref> [4] </ref>. Their measurements provide an interesting comparison of the scale in performance of a combined checksum and copy algorithm when changing hardware platforms. For example, with 1 KB of data they reported 130 s to perform the checksum, and 140 s to perform the memory to memory copy.
Reference: [5] <institution> Digital Equipment Corporation, Maynard, MA. Alpha Architecture Reference Manual, </institution> <year> 1992. </year>
Reference-contexts: With proper support from the host-network interface and the processor-memory subsystem, eliminating the TCP checksum can also benefit throughput oriented applications. For example, having DMA capability in the host-network interface and a snoopy cache as found in <ref> [5] </ref>, allows data to be moved at near bus bandwidth speeds to the application layer.
Reference: [6] <institution> FORE Systems. TCA-100 TURBOchannel ATM Computer Interface, </institution> <note> User's Manual, </note> <year> 1992. </year>
Reference-contexts: 1 Introduction In this paper we investigate the latency characteristics of the TCP transport protocol on an Asynchronous Transfer Mode (ATM) network <ref> [6] </ref>. The characteristics of LAN technologies have changed a great deal in the last few years. With faster network hardware, the disparity between software and hardware costs is even greater. This increases the importance of efficient protocol implementations and efficient operating system interfaces. <p> The following factors in network communication make measuring TCP performance, especially latency, interesting: * The existence of a high quality TCP software implementation: the BSD 4.4 alpha TCP code. * The availability of low latency network interfaces: e.g., the FORE TCA-100 ATM interface <ref> [6] </ref>. * The wide use of applications and subsystems (like RPC) that can benefit from reduced latency. Prior studies have concentrated on characterizing and optimizing the throughput of TCP on substantially different hardware or networks than the ones we describe here (e.g., [3, 8]).
Reference: [7] <author> Daniel H. Greene and J. Bryan Lyles. </author> <title> Reliability of Adaptation Layers Protocols for High-Speed Networks, III, </title> <publisher> Elsevier Science Publishers B.V., </publisher> <year> 1993, </year> <pages> pp. 185-200. </pages>
Reference: [8] <author> Jonathan Kay and Joseph Pasquale. </author> <title> A Performance Analysis of TCP/IP and UDP/IP Networking Software for the DECstation 5000 Tech Report, </title> <booktitle> CSL U.C. </booktitle> <address> San Diego/Sequoia, </address> <month> December </month> <year> 1992. </year>
Reference-contexts: Prior studies have concentrated on characterizing and optimizing the throughput of TCP on substantially different hardware or networks than the ones we describe here (e.g., <ref> [3, 8] </ref>). In addition to focusing on an ATM network, we investigate how optimizations previously suggested for improving throughput affect latency. We believe that studying the latency characteristics of TCP on ATM networks is particularly interesting for two reasons. <p> Our measurements used a range of packet sizes. Based upon previous studies of RPC and TCP traffic behavior, we chose a variety of packet lengths sized 500 bytes and smaller <ref> [1, 8] </ref>. We also measured packets of 1400 bytes (the Ethernet MTU minus protocol headers), 4000 bytes (fits on a single memory page including protocol headers), and 8000 bytes (fits on two memory pages including protocol headers, also close to our ATM MTU of 9K). <p> It is already common practice to eliminate the UDP checksum for local area NFS traffic. Kay and Pasquale describe a mechanism using the Alternate Checksum Option to negotiate connections that do not use the checksum <ref> [8] </ref>. We therefore restrict ourselves to an analysis of the error characteristics and the implications of eliminating the checksum for local-area ATM traffic. We define local-area traffic as packets that go from source host to destination host without passing through any IP routers.
Reference: [9] <author> Jonathan Kay and Joseph Pasquale. </author> <title> Measurement, Analysis, and Improvement of UDP/IP Throughput for the DECstation 5000 Tech Report, </title> <booktitle> CSL U.C. </booktitle> <address> San Diego/Sequoia, </address> <month> January </month> <year> 1993. </year>
Reference-contexts: Section 2 summarizes our measurements of TCP latency on the baseline system. Sections 3 and 4 study the effect of several modifications motivated by the results in Section 2. These modifications are not new and have been suggested by others to improve throughput <ref> [4, 9] </ref>. However, our focus here is on the effect of these modifications on latency. 2 Measurement of the Baseline System The baseline system that we measured is the BSD 4.4 alpha TCP release operating on an ATM network. <p> We then address the issue of eliminating the checksum for particular combinations of link types and applications. 4.1 Optimizing the Checksum Others have noted that the ULTRIX 4.2A checksum algorithm could be improved by eliminating halfword accesses and using loop unrolling <ref> [9] </ref>. We implemented a similar optimized checksum algorithm; the performance of this algorithm and the ULTRIX algorithm at user level are shown in Table 5.
Reference: [10] <editor> Van Jacobson, Robert Braden, and David Borman. </editor> <title> TCP Extensions for High Performance. </title> <type> RFC 1323, </type> <institution> LBL, USC/ISI, and Cray Research, </institution> <month> May </month> <year> 1992. </year>
Reference: [11] <author> David B. Johnson and Willy Zwaenepoel. </author> <title> The Peregrine high-performance RPC system. </title> <journal> Software Practice and Experience, </journal> <volume> 23(2) </volume> <pages> 201-221, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: There are two distinct kinds of optimizations that are often called header prediction. The first, involving prefilling parts of the transport header, is a known optimization for lowering latency <ref> [11, 15] </ref>, and is not discussed further here. The second technique involves exploiting traffic locality to predict the next incoming packet to avoid the protocol control block (PCB) lookup cost.
Reference: [12] <author> Paul E. McKenney and Ken F. Dove. </author> <title> Efficient Demultiplexing of Incoming TCP Packets. </title> <booktitle> In Proceedings of SIGCOMM '92, </booktitle> <month> August </month> <year> 1992, </year> <pages> pp. 269-79. </pages>
Reference-contexts: | 2500 | 4500 | 6500 | 8500 | 10500 | 12500 Size (bytes) 4 20 80 200 500 1400 4000 8000 Round Trip Time ( m s) Effects of Header Prediction. study alternative data structures for PCB lookup, and analyze these data structures by the expected average search length <ref> [12] </ref>. However, they do not discuss how long a search of any given length will take. While this facilitates comparisons, it is difficult to study the absolute effect of header prediction.
Reference: [13] <author> John K. Ousterhout. </author> <title> Why Aren't Operating Systems Getting Faster As Fast as Hardware? In Proceedings of the USENIX 1990 Summer Conference, </title> <month> June </month> <year> 1990, </year> <pages> pp. 247-256. </pages>
Reference: [14] <author> Jerome H. Saltzer, David P. Reed, and David D. Clark. </author> <title> End-to-end arguments in system design. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2(4) </volume> <pages> 277-288, </pages> <month> November </month> <year> 1984. </year>
Reference-contexts: The end-to-end argument, a classic principle in system design, says that the two ends of a reliable communication path should not depend on any of the intervening system components for correctness <ref> [14] </ref>. In other words, to assure the integrity of the communicated data, the communication end points must do a check independent of any checks done by intermediary components.
Reference: [15] <author> Michael D. </author> <title> Schroeder and Michael Burrows.Performance of Firefly RPC. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 8(1) </volume> <pages> 1-17, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: There are two distinct kinds of optimizations that are often called header prediction. The first, involving prefilling parts of the transport header, is a known optimization for lowering latency <ref> [11, 15] </ref>, and is not discussed further here. The second technique involves exploiting traffic locality to predict the next incoming packet to avoid the protocol control block (PCB) lookup cost. <p> This notion is similar to the RPC fast path found in high performance RPC systems such as SRC RPC <ref> [15] </ref>. A related issue is the organization of PCBs, so that lookup is efficient in the case where there is a miss in the PCB cache. The insertion algorithm for the linked list of PCBs places the most recent creation at the head of the list.
Reference: [16] <author> Michael D. Schroeder, Andrew D. Birrell, Michael Burrows, Hal Murray, Roger M. Needham, Thomas L. Rodeheffer, Edwin H. Satterthwaite, and Charles P. Thacker. Autonet: </author> <title> A high-speed, self-configuring local area network using point-to-point links. </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> 9(8) </volume> <pages> 1318-1335, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: CSE Dept. Tech. Report #93-03-02. 2 The TurboChannel card is the AN-1 controller from DEC SRC <ref> [16] </ref>. Note that we did not employ the AN-1 network in this study, only the clock on its controller.
Reference: [17] <author> Cheng Song and Lawrence Landweber. </author> <title> Optimizing Bulk Data Transfer Performance: A Packet Train Approach. </title> <booktitle> In Proceedings of SIGCOMM '88, </booktitle> <month> September </month> <year> 1988, </year> <pages> pp. 134-144. </pages>
Reference-contexts: The second technique involves exploiting traffic locality to predict the next incoming packet to avoid the protocol control block (PCB) lookup cost. Others have studied using traffic locality to improve throughput for bulk data transfer protocols <ref> [2, 17] </ref>; we study its impact on latency. In the BSD implementation, the TCP input processing engine keeps a single entry cache of the most recently used PCB. If the incoming packet is from the same connection as the previous packet, the call to the PCB lookup routine is avoided.

References-found: 17

