URL: http://www.almaden.ibm.com/cs/quest/papers/kdd96_dev.ps
Refering-URL: http://www.almaden.ibm.com/cs/quest/publications.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: A Linear Method for Deviation Detection in Large Databases  
Author: Andreas Arning Rakesh Agrawal and Prabhakar Raghavan 
Keyword: Index Terms: Data Mining, Knowledge Discovery, Deviation, Exception, Error  
Address: Germany  San Jose, California 95120, U.S.A.  
Affiliation: IBM German Software Development Laboratory Boeblingen,  IBM Almaden Research Center  
Abstract: We describe the problem of finding deviations in large data bases. Normally, explicit information outside the data, like integrity constraints or predefined patterns, is used for deviation detection. In contrast, we approach the problem from the inside of the data, using the implicit redundancy of the data. We give a formal description of the problem and present a linear algorithm for detecting deviations. Our solution simulates a mechanism familiar to human beings: after seeing a series of similar data, an element disturbing the series is considered an exception. We also present experimental results from the application of this algorithm on real-life datasets showing its effectiveness. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Agrawal, R., and Srikant, R. </author> <year> 1994. </year> <title> Fast algorithms for mining association rules. </title> <booktitle> In Proceedings of the VLDB Conference. </booktitle>
Reference: <author> Agrawal, R.; Imielinski, T.; and Swami, A. </author> <year> 1993. </year> <title> Database mining: A performance perspective. </title> <journal> IEEE Transactions on Knowledge and Data Engineering 5(6) </journal> <pages> 914-925. </pages>
Reference-contexts: Below we give a new measure for deviation detection and an algorithm for detecting deviation. The proposed algorithm has linear complexity. This is a desirable property for a data mining algorithm, as we know from <ref> (Agrawal, Imielinski, & Swami 1993) </ref> that presents a method for discovering frequent co occurrences of attribute values in large data sets.
Reference: <author> Aha, D. W.; Kibler, D.; and Albert, M. K. </author> <year> 1991. </year> <title> Instance-based learning algorithms. </title> <booktitle> Machine Learning 6(1) </booktitle> <pages> 37-66. </pages>
Reference-contexts: Deviations have been often viewed as outliers, or errors, or noise in data. There has been work in Statistics on identifying outliers (e.g. (Hoaglin, Mosteller, & Tukey 1983) (Johnson 1992)). There has been work in extending learning algorithms to cope with a small amount of noise (e.g. <ref> (Aha, Kibler, & Albert 1991) </ref>). Additionally, some work has been done to determine the impact of erroneous examples on the learning results: there is experimental work as in (Quinlan 1986) and quantitative theoretical work as in (Angluin & Laird 1988), extending the valuable work of (Valiant 1984).
Reference: <author> Angluin, D., and Laird, P. </author> <year> 1988. </year> <title> Learning from noisy examples. </title> <booktitle> Machine Learning 2(4) </booktitle> <pages> 343-370. </pages>
Reference-contexts: Additionally, some work has been done to determine the impact of erroneous examples on the learning results: there is experimental work as in (Quinlan 1986) and quantitative theoretical work as in <ref> (Angluin & Laird 1988) </ref>, extending the valuable work of (Valiant 1984). Rather than being considered outliers that need to be tolerated during the main-line processing, deviations play the leading part in this paper: the one and only purpose of our proposed method is to discover them.
Reference: <author> Arning, A. </author> <year> 1995. </year> <editor> Fehlersuche in groen Datenmen-gen unter Verwendung der in den Daten vorhandenen Redundanz. </editor> <booktitle> PhD dissertation, </booktitle> <institution> Universitat Osnabruck, Fachbereich Sprach- und Literaturwissenschaft. </institution>
Reference-contexts: For details, see <ref> (Arning 1995) </ref>. The auxiliary function M S (I j ) computes a user customizable maximum value for the elements of I j in order to find those elements that particularly increase this maximum.
Reference: <author> Chamberlin, D. </author> <year> 1996. </year> <title> Using the New DB2: IBM's Object-Relational Database System. </title> <publisher> Morgan Kauf-mann. </publisher>
Reference: <author> Fisher, D. H. </author> <year> 1987. </year> <title> Knowledge acquisition via incremental conceptual clustering. </title> <booktitle> Machine Learning 2(2) </booktitle> <pages> 139-172. </pages>
Reference-contexts: Moreover, these methods generally require the existence of a metrical distance function between data elements. Such is the case with even refined clustering algorithms as <ref> (Fisher 1987) </ref> (Hanson & Bauer 1989) (Rumelhart & Zipser 1985). Michalski (Michal-ski & Stepp 1983) replaces the usual Euclidian distance measure between data elements by enriching the measure with conceptual similarity, but still requires a symmetrical distance measure and imposes restrictive constraints on permissible representations.
Reference: <author> Garey, M., and Johnson, D. </author> <year> 1979. </year> <title> Computers and Intractability: a guide to the theory of NP-completeness. </title>
Reference-contexts: However, one cannot hope to always find an efficient ordering for examining the elements that yields the optimal solution for every C and D. Indeed, there exist C and D for which the problem is NP-hard by a reduction from Maximum Independent Set in a graph <ref> (Garey & Johnson 1979) </ref>. This motivates the definition of the sequential exception problem.
Reference: <editor> W. H. </editor> <publisher> Freeman. </publisher>
Reference: <author> Hanson, S. J., and Bauer, M. </author> <year> 1989. </year> <title> Conceptual clustering, categorization, </title> <booktitle> and polymorphy. Machine Learning 3(4) </booktitle> <pages> 343-372. </pages>
Reference-contexts: Moreover, these methods generally require the existence of a metrical distance function between data elements. Such is the case with even refined clustering algorithms as (Fisher 1987) <ref> (Hanson & Bauer 1989) </ref> (Rumelhart & Zipser 1985). Michalski (Michal-ski & Stepp 1983) replaces the usual Euclidian distance measure between data elements by enriching the measure with conceptual similarity, but still requires a symmetrical distance measure and imposes restrictive constraints on permissible representations.
Reference: <author> Hoaglin, D.; Mosteller, F.; and Tukey, J. </author> <year> 1983. </year> <title> Understanding Robust and Exploratory Data Analysis. </title> <address> New York: </address> <publisher> John Wiley. </publisher>
Reference-contexts: Introduction The importance of detecting deviations (or exceptions) in data has been recognized in the fields of Databases and Machine Learning for a long time. Deviations have been often viewed as outliers, or errors, or noise in data. There has been work in Statistics on identifying outliers (e.g. <ref> (Hoaglin, Mosteller, & Tukey 1983) </ref> (Johnson 1992)). There has been work in extending learning algorithms to cope with a small amount of noise (e.g. (Aha, Kibler, & Albert 1991)).
Reference: <author> Johnson, R. </author> <year> 1992. </year> <title> Applied Multivariate Statistical Analysis. </title> <publisher> Prentice Hall. </publisher>
Reference-contexts: Deviations have been often viewed as outliers, or errors, or noise in data. There has been work in Statistics on identifying outliers (e.g. (Hoaglin, Mosteller, & Tukey 1983) <ref> (Johnson 1992) </ref>). There has been work in extending learning algorithms to cope with a small amount of noise (e.g. (Aha, Kibler, & Albert 1991)).
Reference: <author> Li, M., and Vitanyi, P. </author> <year> 1991. </year> <title> Kolmogorov Complexity. </title> <publisher> Springer Verlag. </publisher>
Reference-contexts: We, on the other hand, only require a function that can yield the degree to which a data element causes the "dissimilarity" of the data set to increase. This function does not have to fulfill the conditions to be metrical. The literature on Kolmogorov complexity <ref> (Li & Vi-tanyi 1991) </ref> is relevant to the problem of deviation detection. We are looking for the subset of data that leads to the greatest reduction in Kolmogorov complexity for the amount of data discarded. Our problem is also related to the Minimum Description Length (MDL) principle (Rissanen 1989).
Reference: <author> Michalski, R. S., and Stepp, R. E. </author> <year> 1983. </year> <title> Learning from observation: conceptual clustering. </title> <editor> In Michalski et al. </editor> <year> (1983). </year> <pages> 331-363. </pages>
Reference: <author> Michalski, R. S.; Carbonell, J. G.; and Mitchell, T. M., eds. </author> <year> 1983. </year> <title> Machine Learning: </title> <booktitle> An Artificial Intelligence Approach, volume I. </booktitle> <address> Los Altos, California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Quinlan, J. R. </author> <year> 1986. </year> <title> Induction of decision trees. </title> <booktitle> Machine Learning 1(1) </booktitle> <pages> 81-106. </pages>
Reference-contexts: There has been work in extending learning algorithms to cope with a small amount of noise (e.g. (Aha, Kibler, & Albert 1991)). Additionally, some work has been done to determine the impact of erroneous examples on the learning results: there is experimental work as in <ref> (Quinlan 1986) </ref> and quantitative theoretical work as in (Angluin & Laird 1988), extending the valuable work of (Valiant 1984).
Reference: <author> Rissanen, J. </author> <year> 1989. </year> <title> Stochastic Complexity in Statistical Inquiry. </title> <publisher> World Scientific Publ. Co. </publisher>
Reference-contexts: We are looking for the subset of data that leads to the greatest reduction in Kolmogorov complexity for the amount of data discarded. Our problem is also related to the Minimum Description Length (MDL) principle <ref> (Rissanen 1989) </ref>. The MDL principle states that the best model for encoding data is the one that minimizes the sum of the cost of describing the data in terms of the model and the cost of describing the model.
Reference: <author> Rumelhart, D. E., and Zipser, D. </author> <year> 1985. </year> <title> Feature discovery by competitive learning. </title> <booktitle> Cognitive Science 9 </booktitle> <pages> 75-112. </pages>
Reference-contexts: Moreover, these methods generally require the existence of a metrical distance function between data elements. Such is the case with even refined clustering algorithms as (Fisher 1987) (Hanson & Bauer 1989) <ref> (Rumelhart & Zipser 1985) </ref>. Michalski (Michal-ski & Stepp 1983) replaces the usual Euclidian distance measure between data elements by enriching the measure with conceptual similarity, but still requires a symmetrical distance measure and imposes restrictive constraints on permissible representations.
Reference: <author> Shavlik, J. W., and Dietterich, T. G., eds. </author> <year> 1990. </year> <booktitle> Readings in Machine Learning, Series in Machine Learning. </booktitle> <publisher> Morgan Kaufmann. Vality Technology Inc. </publisher> <year> 1995. </year> <title> Integrity product Overview. </title>
Reference-contexts: We approach the problem from the inside of the data, using the implicit redundancy in the data to detect deviations. The problem of detecting deviations can be considered to be a special case of clustering of data into two clusters: deviations and non-deviations. However, the usual clustering methods (see <ref> (Shavlik & Dietterich 1990) </ref>) are biased toward discarding deviations as noise; we, in contrast, try to isolate small minorities. Moreover, these methods generally require the existence of a metrical distance function between data elements.
Reference: <author> Valiant, L. G. </author> <year> 1984. </year> <title> A theory of the learnable. </title> <journal> Communications of the ACM 27(11) </journal> <pages> 1134-1142. </pages>
Reference-contexts: Additionally, some work has been done to determine the impact of erroneous examples on the learning results: there is experimental work as in (Quinlan 1986) and quantitative theoretical work as in (Angluin & Laird 1988), extending the valuable work of <ref> (Valiant 1984) </ref>. Rather than being considered outliers that need to be tolerated during the main-line processing, deviations play the leading part in this paper: the one and only purpose of our proposed method is to discover them.
References-found: 20

