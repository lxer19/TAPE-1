URL: ftp://ftp.cs.rochester.edu/pub/papers/systems/94.tr550.Scheduler_conscious_synchronization.ps.gz
Refering-URL: http://www.cs.rochester.edu/u/scott/pubs.html
Root-URL: 
Email: fkthanasi,bob,scottg@cs.rochester.edu  
Title: Scheduler-Conscious Synchronization  
Author: Leonidas I. Kontothanassis, Robert W. Wisniewski, and Michael L. Scott 
Date: December 1994  
Address: Rochester, NY 14627-0226  
Affiliation: Department of Computer Science University of Rochester  
Abstract: Efficient synchronization is important for achieving good performance in parallel programs, especially on large-scale multiprocessors. Most synchronization algorithms have been designed to run on a dedicated machine, with one application process per processor, and can suffer serious performance degradation in the presence of multiprogramming. Problems arise when running processes block or, worse, busy-wait for action on the part of a process that the scheduler has chosen not to run. In this paper we describe and evaluate a set of scheduler-conscious synchronization algorithms that perform well in the presence of multiprogramming while maintaining good performance on dedicated machines. We consider both large and small machines, with a particular focus on scalability, and examine mutual-exclusion locks, reader-writer locks, and barriers. The algorithms we study fall into two classes: those that heuristically determine appropriate behavior and those that use scheduler information to guide their behavior. We show that while in some cases either method is sufficient, in general sharing information across the kernel-user interface both eases the design of synchronization algorithms and improves their performance. fl This work was supported in part by National Science Foundation grants numbers CCR-9319445 and CDA-8822724, by ONR contract number N00014-92-J-1801 (in conjunction with the ARPA Research in Information Science and Technology-High Performance Computing, Software Science and Technical program, ARPA Order no. 8930), and by ARPA research grant no. MDA972-92-J-1012. Robert Wisniewski was supported in part by an ARPA Fellowship in High Performance Computing administered by the Institute for Advanced Computer Studies, University of Maryland. Experimental results were obtained in part through use of resources at the Cornell Theory Center, which receives major funding from NSF and New York State; additional funding comes from ARPA, the NIH, IBM Corporation, and other members of the Center's Corporate Research Institute. The government has certain rights in this material. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. E. Anderson. </author> <title> The Performance of Spin Lock Alternatives for Shared-Memory Multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(1) </volume> <pages> 6-16, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: The key to good performance is to minimize active sharing. One option is to use backoff techniques <ref> [1, 30] </ref> in which a processor that attempts unsuccessfully to acquire a lock waits for a period of time before trying again. The amount of time depends on the estimated level of contention. Bounded exponential backoff works well for test and set locks. <p> Backoff proportional to the number of predecessors works well for ticket locks. A second option for scalable locks is to use distributed data structures to ensure that no two processes spin on the same location. The queue-based spin locks of Anderson <ref> [1] </ref> and of Graunke and Thakkar [9] minimize active sharing on coherently-cached machines by arranging for every waiting processor to spin on a different element of an array. Each element of the array lies in a separate, dynamically-chosen cache line, which migrates to the spinning processor. <p> Many researchers have now developed algorithms of this type <ref> [1, 9, 26, 30, 44] </ref>. The scalability of centralized algorithms may also be improved by introducing appropriate forms of back-off [1, 30]. <p> Many researchers have now developed algorithms of this type [1, 9, 26, 30, 44]. The scalability of centralized algorithms may also be improved by introducing appropriate forms of back-off <ref> [1, 30] </ref>. The scheduler-conscious test and set locks of Psyche, Symunix, or Scheduler Activations can be modified trivially to incorporate backoff, though the work of Anderson and of Mellor-Crummey and Scott suggests that the result will still produce more contention than a queue-based lock.
Reference: [2] <author> T. E. Anderson, B. N. Bershad, E. D. Lazowska, and H. M. Levy. </author> <title> Scheduler Activations: Effective Kernel Support for the User-Level Management of Parallelism. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 10(1) </volume> <pages> 53-79, </pages> <month> February </month> <year> 1992. </year> <booktitle> Originally presented at the Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <month> October </month> <year> 1991. </year>
Reference-contexts: Several groups have proposed extensions to the kernel/user interface that allow a system to avoid adverse scheduler/lock interactions while still doing scheduling in the kernel. The Scheduler Activation proposal of Anderson et al. <ref> [2] </ref> allows a parallel application to recover from untimely preemption. When a processor is taken away from an application, another processor in the same application is given a software interrupt, informing it of the preemption. <p> Our kernel extensions are enumerated below. They build upon ideas proposed by the Symunix project at NYU [8]. Similar extensions could be based on the kernel interfaces of Psyche [29] or Scheduler Activations <ref> [2] </ref>. * KE-1: For each process the kernel and user cooperate to maintain a variable that represents the process's state and that can be manipulated under certain rules by either. The variable has four possible values: preemptable, preempted, self unpreempt-able, and other unpreemptable. <p> of processors available to an application being relatively infrequent, compared to the rate at which barriers are encountered. 6 The pseudocode of the previous paper has been modified slightly here for the sake of consistency. 23 shared global_sense, barrier_count, num_blocked : integer := 0, 0, 0 shared wakeup_sems : array <ref> [2] </ref> of semaphore := -0 shared mutex : lock private local_sense : integer := 0 procedure barrier () local_sense := 1 - local_sense count : integer := fetch_and_increment (&barrier_count) if count &lt; NUM_PROCESSES - 1 for i : integer in 1..SWITCH_TIME if global_sense = local_sense return acquire_lock (mutex) if global_sense = <p> [local_sense]) else barrier_count := 0 acquire_lock (mutex) global_sense := 1 - global_sense // release spinning processes count := num_blocked num_blocked := 0 release_lock (mutex) for i in 1..count V (wakeup_sems [local_sense]) // release blocked processes 24 shared global_sense, barrier_count, num_blocked : integer := 0, 0, 0 shared wakeup_sems : array <ref> [2] </ref> of semaphore := -0 shared mutex : lock private local_sense : integer := 0 private spin_threshold : integer := SWITCH_TIME private episode_count : integer := 0 private episode_time : array [3] of integer := -SWITCH_TIME procedure barrier () local_sense := 1 - local_sense count : integer := fetch_and_increment (&barrier_count) if <p> (0, spin_threshold ADJUST) else barrier_count := 0 acquire_lock (mutex) global_sense := 1 - global_sense // release spinning processes count := num_blocked num_blocked := 0 release_lock (mutex) for i in 1..count V (wakeup_sem [local_sense]) // release blocked processes 25 shared global_sense, barrier_count : integer := 0, 0 shared wakeup_sems : array <ref> [2] </ref> of semaphore := -0 shared partition : ^partition_block shared barrier_processors : array [2] of integer := -partition-&gt;num_processors-private local_sense : integer := 0 procedure barrier () local_sense := 1 - local_sense count : integer := fetch_and_increment (&barrier_count) if count + 1 &lt; NUM_PROCESSES if count + 1 &gt;= NUM_PROCESSES - barrier_processors <p> global_sense // release spinning processes count := num_blocked num_blocked := 0 release_lock (mutex) for i in 1..count V (wakeup_sem [local_sense]) // release blocked processes 25 shared global_sense, barrier_count : integer := 0, 0 shared wakeup_sems : array <ref> [2] </ref> of semaphore := -0 shared partition : ^partition_block shared barrier_processors : array [2] of integer := -partition-&gt;num_processors-private local_sense : integer := 0 procedure barrier () local_sense := 1 - local_sense count : integer := fetch_and_increment (&barrier_count) if count + 1 &lt; NUM_PROCESSES if count + 1 &gt;= NUM_PROCESSES - barrier_processors [local_sense] repeat until global_sense = local_sense // spin else P (wakeup_sem [local_sense]) else <p> Specifically, 26 type whole_and_parts = union whole : long parts: array [4] of byte type tree_node = record have_child : whole_and_parts child_not_ready : whole_and_parts := have_child parent_flag : ^byte dummy : byte // something harmless to point at type processor_info = record barrier_count : integer := 0 wakeup_sems : array <ref> [2] </ref> of semaphore := -0 generation : integer := 0 // used to synchronize reorganization shared processors : array [MAX_PROCESSORS] of processor_info shared nodes : array [MAX_PROCESSORS] of tree_node // have_child and parent_flag fields of individual nodes are initialized // as appropriate in the inter-processor tree; see code in reorganize ()
Reference: [3] <author> N. S. Arenstorf and H. Jordan. </author> <title> Comparing Barrier Algorityhms. </title> <journal> Parallel Computing, </journal> <volume> 12 </volume> <pages> 157-170, </pages> <year> 1989. </year>
Reference-contexts: Several researchers have shown how to solve these problems by building scalable barriers, with log-depth tree- or FFT-like patterns of point-to-point notifications among processes <ref> [3, 11, 20, 25, 30, 32, 38, 45] </ref>. Unfortunately, the deterministic notification patterns of scalable barriers may require that processes run in a different order from the one chosen by the scheduler. The problem is related to, but more severe than, the preemption-while-waiting problem in FIFO locks. <p> // release blocked processes 24 shared global_sense, barrier_count, num_blocked : integer := 0, 0, 0 shared wakeup_sems : array [2] of semaphore := -0 shared mutex : lock private local_sense : integer := 0 private spin_threshold : integer := SWITCH_TIME private episode_count : integer := 0 private episode_time : array <ref> [3] </ref> of integer := -SWITCH_TIME procedure barrier () local_sense := 1 - local_sense count : integer := fetch_and_increment (&barrier_count) if count &lt; NUM_PROCESSES - 1 now : integer := get_current_time () for i in 1..spin_threshold if global_sense = local_sense goto exit_barrier acquire_lock (mutex) if global_sense = local_sense release_lock (mutex) goto exit_barrier
Reference: [4] <author> T. S. Axelrod. </author> <title> Effects of Synchronization Barriers on Multiprocessor Performance. </title> <journal> Parallel Computing, </journal> <volume> 3 </volume> <pages> 129-140, </pages> <year> 1986. </year>
Reference-contexts: To avoid this problem, they suggest (without an implementation) that blocking synchronization be used among the processes on a given processor, with a scalable busy-wait barrier among processors. (Such combination barriers were originally suggested by Axelrod <ref> [4] </ref> to minimize resource needs in barriers constructed from OS-provided locks.) The challenge for a combination barrier is to communicate partition information from the scheduler to the application, and to adapt to partitioning changes at run time. <p> Specifically, 26 type whole_and_parts = union whole : long parts: array <ref> [4] </ref> of byte type tree_node = record have_child : whole_and_parts child_not_ready : whole_and_parts := have_child parent_flag : ^byte dummy : byte // something harmless to point at type processor_info = record barrier_count : integer := 0 wakeup_sems : array [2] of semaphore := -0 generation : integer := 0 // used
Reference: [5] <author> D. L. Black. </author> <title> Scheduling Support for Concurrency and Parallelism in the Mach Operating System. </title> <journal> Computer, </journal> <volume> 23(5) </volume> <pages> 35-43, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: The second processor can then perform a context switch to the preempted process if desired, e.g. to push it through its critical section. In a similar vein, Black's work on Mach <ref> [5] </ref> allows a process to suggest to the scheduler that it be de-scheduled in favor of some specific other process, eg. the holder of a desired lock. Both of these proposals assume that process migration is relatively cheap.
Reference: [6] <author> P. Brinch Hansen. </author> <title> Distributed Processes: A Concurrent Programming Concept. </title> <journal> Communications of the ACM, </journal> <volume> 21(11) </volume> <pages> 934-941, </pages> <month> November </month> <year> 1978. </year>
Reference-contexts: This sort of organization is common in distributed systems. It can be cast as a natural interpretation of monitors (as, for example, in Brinch Hansen's Distributed Processes notation <ref> [6] </ref>), or as function shipping [24, 40] to a common destination. In recent years, several machines have been developed that provide hardware support for very fast invocation of functions on remote processors [18, 33].
Reference: [7] <author> M. Crovella, P. Das, C. Dubnicki, T. LeBlanc, and E. Markatos. </author> <title> Multiprogramming on Multiprocessors. </title> <booktitle> In Proceedings of the Third IEEE Symposium on Parallel and Distributed Processing, </booktitle> <pages> pages 590-597, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: a given processor are scheduled in a different order, and if they simply yield the processor when unable to proceed (as opposed to waiting on a kernel-provided synchronization queue), then the scheduler may need to cycle through most of the ready list several times in order to achieve a barrier <ref> [7] </ref>. The rest of the paper is organized as follows. Section 2 discusses related work. It explains in more detail why synchronization algorithms suffer under multiprogramming, why scalable synchronization algorithms are particularly susceptible to multiprogramming effects, and why previous research does not fully remedy the problem. <p> Unfortunately, bin-packing problems make it difficult to use all processors effectively in a co-scheduled system, and context switches among applications detract from computation time and lead to sub-optimal use of caches. Using simulation, modeling, and experimentation, Crovella et al. <ref> [7] </ref>, Leutenegger and Vernon [22], Tucker and Gupta [10], and Zahorjan and McCann [47] have shown that dynamically partitioning the processors of a machine among applications is preferable to either timesharing or co-scheduling. <p> Our mutual exclusion and reader-writer locks use only the context block records; the barriers use both. All of our algorithms work well in a dynamic hardware-partitioned environment|an environment widely believed to provide the best combination of throughput and fast turnaround for large-scale multiprocessors <ref> [7, 22, 41, 47] </ref>. Except for the barriers, which require partition information, all of the algorithms will also work well under ordinary time sharing.
Reference: [8] <author> J. Edler, J. Lipkis, and E. Schonberg. </author> <title> Process Management for Highly Parallel UNIX Systems. </title> <booktitle> In Proceedings of the USENIX Workshop on Unix and Supercomputers, </booktitle> <address> Pittsburgh, PA, </address> <month> September </month> <year> 1988. </year> <month> 43 </month>
Reference-contexts: Both of these proposals assume that process migration is relatively cheap. Rather than recover from untimely preemption, the Symunix system of Edler et al. <ref> [8] </ref> and the Psyche system of Marsh et al. [29] provide mechanisms to avoid or prevent it. The Symunix scheduler allows a process to request that it not be preempted during a critical section, and will honor that request, within reason. <p> Our kernel extensions are enumerated below. They build upon ideas proposed by the Symunix project at NYU <ref> [8] </ref>. Similar extensions could be based on the kernel interfaces of Psyche [29] or Scheduler Activations [2]. * KE-1: For each process the kernel and user cooperate to maintain a variable that represents the process's state and that can be manipulated under certain rules by either. <p> The generation count indicates the number of times that the partition has changed in size since the application started running. As noted above, extensions KE-1 and KE-2 are based in part on ideas developed for the Symunix kernel <ref> [8] </ref>. We have introduced additional states, and have made the state variable writable and readable by both user-level and kernel-level code [43].
Reference: [9] <author> G. Graunke and S. Thakkar. </author> <title> Synchronization Algorithms for Shared-Memory Multi--processors. </title> <journal> Computer, </journal> <volume> 23(6) </volume> <pages> 60-69, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Backoff proportional to the number of predecessors works well for ticket locks. A second option for scalable locks is to use distributed data structures to ensure that no two processes spin on the same location. The queue-based spin locks of Anderson [1] and of Graunke and Thakkar <ref> [9] </ref> minimize active sharing on coherently-cached machines by arranging for every waiting processor to spin on a different element of an array. Each element of the array lies in a separate, dynamically-chosen cache line, which migrates to the spinning processor. <p> Many researchers have now developed algorithms of this type <ref> [1, 9, 26, 30, 44] </ref>. The scalability of centralized algorithms may also be improved by introducing appropriate forms of back-off [1, 30].
Reference: [10] <author> A. Gupta, A. Tucker, and S. Urushibara. </author> <title> The Impact of Operating System Scheduling Policies and Synchronization Methods on the Performance of Parallel Applications. </title> <booktitle> In Proceedings of the 1991 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 120-132, </pages> <address> San Diego, CA, </address> <month> May </month> <year> 1991. </year>
Reference-contexts: Unfortunately, bin-packing problems make it difficult to use all processors effectively in a co-scheduled system, and context switches among applications detract from computation time and lead to sub-optimal use of caches. Using simulation, modeling, and experimentation, Crovella et al. [7], Leutenegger and Vernon [22], Tucker and Gupta <ref> [10] </ref>, and Zahorjan and McCann [47] have shown that dynamically partitioning the processors of a machine among applications is preferable to either timesharing or co-scheduling.
Reference: [11] <author> D. Hensgen, R. Finkel, and U. Manber. </author> <title> Two Algorithms for Barrier Synchronization. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 17(1) </volume> <pages> 1-17, </pages> <year> 1988. </year>
Reference-contexts: Several researchers have shown how to solve these problems by building scalable barriers, with log-depth tree- or FFT-like patterns of point-to-point notifications among processes <ref> [3, 11, 20, 25, 30, 32, 38, 45] </ref>. Unfortunately, the deterministic notification patterns of scalable barriers may require that processes run in a different order from the one chosen by the scheduler. The problem is related to, but more severe than, the preemption-while-waiting problem in FIFO locks.
Reference: [12] <author> M. Herlihy. </author> <title> Wait-Free Synchronization. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(1) </volume> <pages> 124-149, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: An alternative approach is to avoid the use of locks. There are at least two ways of doing so. Herlihy <ref> [12, 13] </ref> has led the development of lock-free and wait-free data structures. Rather than rely on locks, these data structures use special algorithms based on such universal atomic primitives as compare and store and load linked/store conditional.
Reference: [13] <author> M. Herlihy. </author> <title> A Methodology for Implementing Highly Concurrent Data Objects. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 15(5) </volume> <pages> 745-770, </pages> <month> Novem-ber </month> <year> 1993. </year>
Reference-contexts: An alternative approach is to avoid the use of locks. There are at least two ways of doing so. Herlihy <ref> [12, 13] </ref> has led the development of lock-free and wait-free data structures. Rather than rely on locks, these data structures use special algorithms based on such universal atomic primitives as compare and store and load linked/store conditional.
Reference: [14] <author> D. V. James, A. T. Laundrie, S. Gjessing, and G. S. Sohi. </author> <title> Scalable Coherent Interface. </title> <journal> Computer, </journal> <volume> 23(6) </volume> <pages> 74-77, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Examples include the queued locks of the Stanford Dash machine [21], the QOLB (queue-on-lock-bit) operation of the IEEE Scalable Coherent Interface <ref> [14] </ref>, and the near-constant-time barriers of the Thinking Machines CM-5 and the Cray Research T3D. It is not yet clear whether the advantages of such special operations over simpler read-modify-write instructions are worth the implementation cost. 3 or to minimize contention.
Reference: [15] <author> E. H. Jensen, G. W. Hagensen, and J. M. Broughton. </author> <title> A New Approach to Exclusive Data Access in Shared Memory Multiprocessors. </title> <type> Technical Report UCRL-97663, </type> <institution> Lawrence Livermore National Laboratory, </institution> <month> November </month> <year> 1987. </year>
Reference-contexts: We assume the availability of special instructions that allow a process to read, modify, and write a shared variable as a single atomic operation. 1 Examples include test and set, fetch and increment, fetch and store (swap), compare and store, and the pair load - linked and store conditional <ref> [15] </ref>, now available in the DEC Alpha and MIPS II architectures. The atomic instructions used in our algorithms can all be emulated efficiently by load-linked and store-conditional.
Reference: [16] <author> A. R. Karlin, K. Li, M. S. Manasse, and S. Owicki. </author> <title> Empirical Studies of Competitive Spinning for a Shared-Memory Multiprocessor. </title> <booktitle> In Proceedings of the Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 41-55, </pages> <address> Pacific Grove, CA, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: Karlin et al. <ref> [16] </ref> present and evaluate a richer set of spin-then-block alternatives, including competitive techniques that adjust the spin time based on past experience. 2 Their goal is to adapt to variability in 2 A competitive algorithms is one whose worst-case performance is provably within a constant factor of optimal worst-case performance. 4 <p> Experience-based heuristics can be successful to the extent that the present and future resemble the past. It forms the basis of the competitive lock algorithms of Karlin et al. <ref> [16] </ref>, and of the competitive barriers we describe at the beginning of section 4.3. In the case of locks, the goal is to block if the wait time will be longer than twice the context switch time, and to spin if it will be shorter. <p> There are several possible ways to resolve this tradeoff, ranging from always spin to always block. For a dynamically changing environment neither of the extremes provides a satisfactory solution. Inspired by the competitive spinning techniques used by Karlin et al. for locks <ref> [16] </ref>, we have developed a set of competitive techniques for barriers [17]. These techniques choose between spinning and blocking based on the amount of time spent waiting at previous barrier episodes.
Reference: [17] <author> L. Kontothanassis and R. Wisniewski. </author> <title> Using Scheduler Information to Achieve Optimal Barrier Synchronization Performance. </title> <booktitle> In Proceedings of the Fourth ACM Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: Inspired by Karlin et al., we have developed techniques to make the spin versus block decision at a centralized (small-scale) barrier <ref> [17] </ref>. We present these techniques in the beginning of section 4.3. <p> We have introduced additional states, and have made the state variable writable and readable by both user-level and kernel-level code [43]. Extension (KE-3) is a generalization of the interface described in our work on small-scale scheduler-conscious barriers <ref> [17] </ref> and resembles the "magic page" of information provided by the Psyche kernel [37]. None of the extensions requires the kernel to maintain information that it does not already have available in its internal data structures. <p> For a dynamically changing environment neither of the extremes provides a satisfactory solution. Inspired by the competitive spinning techniques used by Karlin et al. for locks [16], we have developed a set of competitive techniques for barriers <ref> [17] </ref>. These techniques choose between spinning and blocking based on the amount of time spent waiting at previous barrier episodes. <p> The second barrier gathers information from the last three barrier episodes and shortens or lengthens its spinning threshold based on the observed waiting time. Additional heuristics are explored in a previous paper <ref> [17] </ref>. 6 Competitive spinning barriers provide a simple way to achieve acceptable performance in a multiprogrammed environment. They require no kernel extensions, and work reasonably well if process arrival times are not significantly skewed.
Reference: [18] <author> D. Kranz, K. Johnson, A. Agarwal, J. Kubiatowicz, and B. Lim. </author> <title> Integrating Message-Passing and Shared-Memory: Early Experience. </title> <booktitle> In Proceedings of the Fourth ACM Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: In recent years, several machines have been developed that provide hardware support for very fast invocation of functions on remote processors <ref> [18, 33] </ref>. Even on more conventional hardware, programming techniques such as active messages [42] can make remote execution very fast. Because computation is centralized and requests are processed serially, active messages provide implicit synchronization.
Reference: [19] <author> O. Krieger, M. Stumm, and R. Unrau. </author> <title> A Fair Fast Scalable Reader-Writer Lock. </title> <booktitle> In Proceedings of the 1993 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: Markatos [27] uses a doubly-linked list in a queue-based lock to replace FIFO order with highest-priority-first, for real-time systems. Others have shown how to build queue-based 6 scalable reader-writer locks <ref> [19, 31] </ref>. Yang and Anderson [44] have shown how to use local--only spinning to improve the performance of locks that use no atomic memory operations other than loads and stores. <p> Pseudocode for our scheduler-conscious reader-writer lock appears in figures 5 through 7. It is based on a fair scalable reader-writer lock devised by Krieger et al. <ref> [19] </ref>. When a writer releases a lock for which both readers and writers are waiting, and the longest waiting unpreempted process is a reader, the code grants access to all readers that have been waiting longer than any writers. <p> RW-TAS-B-np The same as RW-TAS-B, but with avoidance of preemption in critical sections, using the Symunix kernel interface. RW-Queue A scalable reader-writer lock based on the lock by Krieger et al. <ref> [19] </ref>. RW-Smart-Q An extension to the RW-Queue lock that uses kernel extensions KE-1 and KE-2 to avoid preemption in the critical section, and to avoid passing the lock to a preempted process.
Reference: [20] <author> C. A. Lee. </author> <title> Barrier Synchronization over Multistage Interconnection Networks. </title> <booktitle> In Proceedings of the Second IEEE Symposium on Parallel and Distributed Processing, </booktitle> <pages> pages 130-133, </pages> <address> Dallas, TX, </address> <month> December </month> <year> 1990. </year> <month> 44 </month>
Reference-contexts: Several researchers have shown how to solve these problems by building scalable barriers, with log-depth tree- or FFT-like patterns of point-to-point notifications among processes <ref> [3, 11, 20, 25, 30, 32, 38, 45] </ref>. Unfortunately, the deterministic notification patterns of scalable barriers may require that processes run in a different order from the one chosen by the scheduler. The problem is related to, but more severe than, the preemption-while-waiting problem in FIFO locks.
Reference: [21] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. S. Lam. </author> <title> The Stanford Dash Multiprocessor. </title> <journal> Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: For example: a mutual exclusion lock may keep waiting processes in a FIFO queue, either for the sake of fairness 1 Some multiprocessors, especially the larger ones, provide more sophisticated hardware support for synchronization. Examples include the queued locks of the Stanford Dash machine <ref> [21] </ref>, the QOLB (queue-on-lock-bit) operation of the IEEE Scalable Coherent Interface [14], and the near-constant-time barriers of the Thinking Machines CM-5 and the Cray Research T3D.
Reference: [22] <author> S. T. Leutenegger and M. K. Vernon. </author> <title> Performance of Multiprogrammed Multiprocessor Scheduling Algorithms. </title> <booktitle> In Proceedings of the 1990 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems, </booktitle> <address> Boulder, CO, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: Unfortunately, bin-packing problems make it difficult to use all processors effectively in a co-scheduled system, and context switches among applications detract from computation time and lead to sub-optimal use of caches. Using simulation, modeling, and experimentation, Crovella et al. [7], Leutenegger and Vernon <ref> [22] </ref>, Tucker and Gupta [10], and Zahorjan and McCann [47] have shown that dynamically partitioning the processors of a machine among applications is preferable to either timesharing or co-scheduling. <p> Our mutual exclusion and reader-writer locks use only the context block records; the barriers use both. All of our algorithms work well in a dynamic hardware-partitioned environment|an environment widely believed to provide the best combination of throughput and fast turnaround for large-scale multiprocessors <ref> [7, 22, 41, 47] </ref>. Except for the barriers, which require partition information, all of the algorithms will also work well under ordinary time sharing.
Reference: [23] <author> B. Lim and A. Agarwal. </author> <title> Reactive Synchronization Algorithms for Multiprocessors. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 25-35, </pages> <address> San Jose, CA, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: Scalable synchronization algorithms reduce contention for memory and interconnect resources but usually have higher overhead than their centralized counterparts in the absence of contention. Lim and Agarwal <ref> [23] </ref> suggest synchronization algorithms that monitor the degree of contention and switch between centralized and distributed implementations based on the observed degree of competition for synchronization variables. Moving from a centralized to a queue-based lock adds one more dimension to the scheduler/synchronization problem. <p> On small-scale machines and for low-contention locks a test and set with exponential backoff or ticket lock with proportional backoff may be preferable [30]. (A hybrid lock that switches between test and set and a queue-based lock, depending on observed contention, is another possibility <ref> [23] </ref>.) With appropriate backoff, test and set and ticket locks scale equally well. They use different atomic instructions, making them usable on different machines.
Reference: [24] <author> B. G. Lindsay and others. </author> <title> Computation and Communication in R*: A Distributed Database Manager. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2(1) </volume> <pages> 24-28, </pages> <month> February </month> <year> 1984. </year>
Reference-contexts: This sort of organization is common in distributed systems. It can be cast as a natural interpretation of monitors (as, for example, in Brinch Hansen's Distributed Processes notation [6]), or as function shipping <ref> [24, 40] </ref> to a common destination. In recent years, several machines have been developed that provide hardware support for very fast invocation of functions on remote processors [18, 33]. Even on more conventional hardware, programming techniques such as active messages [42] can make remote execution very fast.
Reference: [25] <author> B. Lubachevsky. </author> <title> Synchronization Barrier and Related Tools for Shared Memory Parallel Programming. </title> <booktitle> In Proceedings of the 1989 International Conference on Parallel Processing, </booktitle> <pages> pages II:175-179, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: Several researchers have shown how to solve these problems by building scalable barriers, with log-depth tree- or FFT-like patterns of point-to-point notifications among processes <ref> [3, 11, 20, 25, 30, 32, 38, 45] </ref>. Unfortunately, the deterministic notification patterns of scalable barriers may require that processes run in a different order from the one chosen by the scheduler. The problem is related to, but more severe than, the preemption-while-waiting problem in FIFO locks.
Reference: [26] <author> P. Magnussen, A. Landin, and E. Hagersten. </author> <title> Queue Locks on Cache Coherent Multiprocessors. </title> <booktitle> In Proceedings of the Eighth International Parallel Processing Symposium, </booktitle> <address> Cancun, Mexico, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: Because it spins on a location of its own choosing, a process can arrange for that location to lie in local memory even on machines without coherent caches. Magnussen et al. <ref> [26] </ref> have shown how to modify linked-list queue-based locks to minimize interprocessor communication on a coherently-cached machine. Markatos [27] uses a doubly-linked list in a queue-based lock to replace FIFO order with highest-priority-first, for real-time systems. Others have shown how to build queue-based 6 scalable reader-writer locks [19, 31]. <p> Many researchers have now developed algorithms of this type <ref> [1, 9, 26, 30, 44] </ref>. The scalability of centralized algorithms may also be improved by introducing appropriate forms of back-off [1, 30].
Reference: [27] <author> E. P. Markatos. </author> <title> Multiprocessor Synchronization Primitives with Priorities. </title> <booktitle> In Proceedings of the Eighth IEEE Workshop on Real-Time Operating Systems and Software, </booktitle> <pages> pages 1-7, </pages> <address> Atlanta, GA, </address> <month> May </month> <year> 1991. </year> <title> Held in conjunction with the Seventeenth IFAC/IFIP Workshop on Real-Time Programming, </title> <booktitle> and published in the Newsletter of the IEEE Computer Society Technical Committee on Real-Time Systems 7:4 (Fall 1991). </booktitle>
Reference-contexts: Because it spins on a location of its own choosing, a process can arrange for that location to lie in local memory even on machines without coherent caches. Magnussen et al. [26] have shown how to modify linked-list queue-based locks to minimize interprocessor communication on a coherently-cached machine. Markatos <ref> [27] </ref> uses a doubly-linked list in a queue-based lock to replace FIFO order with highest-priority-first, for real-time systems. Others have shown how to build queue-based 6 scalable reader-writer locks [19, 31]. <p> We have considered algorithms that leave preempted processes in an explicit queue so that they only lose their turn while they are preempted. Markatos adopted a similar approach in his real-time queued lock <ref> [27] </ref>, where the emphasis was on passing access to the highest-priority waiting process.
Reference: [28] <author> E. Markatos, M. Crovella, P. Das, C. Dubnicki, and T. LeBlanc. </author> <title> The Effects of Multiprogramming on Barrier Synchronization. </title> <booktitle> In Proceedings of the Third IEEE Symposium on Parallel and Distributed Processing, </booktitle> <pages> pages 662-669, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: The problem is related to, but more severe than, the preemption-while-waiting problem in FIFO locks. With a lock the scheduler may need to cycle through the entire ready list before reaching the process that is able to make progress. With a scalable busy-wait barrier, Markatos et al. have shown <ref> [28] </ref> that the scheduler may need to cycle through the entire ready list a logarithmic number of times (spinning for as long as a quantum between context switches) in order to achieve the barrier. <p> This order may conflict with the scheduling policy on a multiprogrammed system, consequently causing an unreasonable number of context switches <ref> [28] </ref> to occur before achieving the barrier. The basic idea of our scalable scheduler-conscious barrier is to make the optimal spin versus block decision within each individual processor or cluster, and to employ a scalable log-depth barrier across processors, where context switches are not an issue.
Reference: [29] <author> B. D. Marsh, M. L. Scott, T. J. LeBlanc, and E. P. Markatos. </author> <title> First-Class User-Level Threads. </title> <booktitle> In Proceedings of the Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 110-121, </pages> <address> Pacific Grove, CA, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: Both of these proposals assume that process migration is relatively cheap. Rather than recover from untimely preemption, the Symunix system of Edler et al. [8] and the Psyche system of Marsh et al. <ref> [29] </ref> provide mechanisms to avoid or prevent it. The Symunix scheduler allows a process to request that it not be preempted during a critical section, and will honor that request, within reason. <p> Our kernel extensions are enumerated below. They build upon ideas proposed by the Symunix project at NYU [8]. Similar extensions could be based on the kernel interfaces of Psyche <ref> [29] </ref> or Scheduler Activations [2]. * KE-1: For each process the kernel and user cooperate to maintain a variable that represents the process's state and that can be manipulated under certain rules by either. The variable has four possible values: preemptable, preempted, self unpreempt-able, and other unpreemptable.
Reference: [30] <author> J. M. Mellor-Crummey and M. L. Scott. </author> <title> Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(1) </volume> <pages> 21-65, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: The key to good performance is to minimize active sharing. One option is to use backoff techniques <ref> [1, 30] </ref> in which a processor that attempts unsuccessfully to acquire a lock waits for a period of time before trying again. The amount of time depends on the estimated level of contention. Bounded exponential backoff works well for test and set locks. <p> Each element of the array lies in a separate, dynamically-chosen cache line, which migrates to the spinning processor. The queue-based spin lock of Mellor-Crummey and Scott <ref> [30] </ref> represents its queue with a distributed linked list instead of an array. Each waiting processor uses a fetch and store operation to obtain the address of the list element (if any) associated with the previous holder of the lock. <p> Several researchers have shown how to solve these problems by building scalable barriers, with log-depth tree- or FFT-like patterns of point-to-point notifications among processes <ref> [3, 11, 20, 25, 30, 32, 38, 45] </ref>. Unfortunately, the deterministic notification patterns of scalable barriers may require that processes run in a different order from the one chosen by the scheduler. The problem is related to, but more severe than, the preemption-while-waiting problem in FIFO locks. <p> Many researchers have now developed algorithms of this type <ref> [1, 9, 26, 30, 44] </ref>. The scalability of centralized algorithms may also be improved by introducing appropriate forms of back-off [1, 30]. <p> Many researchers have now developed algorithms of this type [1, 9, 26, 30, 44]. The scalability of centralized algorithms may also be improved by introducing appropriate forms of back-off <ref> [1, 30] </ref>. The scheduler-conscious test and set locks of Psyche, Symunix, or Scheduler Activations can be modified trivially to incorporate backoff, though the work of Anderson and of Mellor-Crummey and Scott suggests that the result will still produce more contention than a queue-based lock. <p> Pseudocode for the Smart Queue lock appears in figure 3. One of the problems with queue-based locks is high overhead in the absence of contention. On small-scale machines and for low-contention locks a test and set with exponential backoff or ticket lock with proportional backoff may be preferable <ref> [30] </ref>. (A hybrid lock that switches between test and set and a queue-based lock, depending on observed contention, is another possibility [23].) With appropriate backoff, test and set and ticket locks scale equally well. They use different atomic instructions, making them usable on different machines. <p> // signal children it is safe to proceed else parent_id : integer := (my_processor-1)/4 my_node-&gt;parentflag := &nodes [parent_id].childnotready.parts [(my_processor-1)%4] processors [my_processor].generation := my_generation repeat until processors [parent_id].generation = my_generation // spin 28 we combine the Scheduler Information barrier of figure 10 with the scalable tree barrier of Mellor-Crummey and Scott <ref> [30] </ref>. Processes assigned to the same processor or cluster use a Scheduler Information barrier. The last process to reach the barrier becomes the representative process for the processor/cluster. Representative processes participate in the tree barrier. <p> On the SGI Challenge, this is the native lock, augmented with backoff. TAS-B-np The same as TAS-B, but avoids preemption in critical sections by using the Symunix kernel interface. Queue A list-based queued lock with local-only spinning <ref> [30] </ref>. Queue-np An extension to the Queue lock that avoids preemption in critical sections, also using the Symunix kernel interface. This algorithm does not avoid passing the lock to a process that has been preempted while waiting in line. <p> The Native-np and TAS-B-np locks display the best results, though past work <ref> [30] </ref> suggests that they will generate more bus traffic than the scalable locks, and would interfere more with the data-access memory traffic of a real program. 9 On the KSR, the scheduling quantum is fixed at 50 ms and the ratio of critical to noncritical section lengths is 1:65. <p> resulting in a small performance improvement. 5.4.2 Scalable barriers For large scale machines we implemented and tested three barriers: 40 on the KSR using 57 processors (multiprogramming level = 1) on the KSR using 57 processors (multiprogramming level = 2) Tree - Mellor-Crummey and Scott's tree barrier with flag wakeup <ref> [30] </ref>. This algorithm associates processes with nodes (both internal and leaves) in a static 4-ary fan-in tree. After waiting for their children (if any) and signaling their parent (if any) in the arrival tree, processes spin on locally-cached copies of a single, global wakeup flag.
Reference: [31] <author> J. M. Mellor-Crummey and M. L. Scott. </author> <title> Scalable Reader-Writer Synchronization for Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the Third ACM Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 106-113, </pages> <address> Williamsburg, VA, </address> <month> April </month> <year> 1991. </year> <month> 45 </month>
Reference-contexts: Markatos [27] uses a doubly-linked list in a queue-based lock to replace FIFO order with highest-priority-first, for real-time systems. Others have shown how to build queue-based 6 scalable reader-writer locks <ref> [19, 31] </ref>. Yang and Anderson [44] have shown how to use local--only spinning to improve the performance of locks that use no atomic memory operations other than loads and stores.
Reference: [32] <author> J. M. Mellor-Crummey and M. L. Scott. </author> <title> Synchronization Without Contention. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 269-278, </pages> <address> Santa Clara, CA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Several researchers have shown how to solve these problems by building scalable barriers, with log-depth tree- or FFT-like patterns of point-to-point notifications among processes <ref> [3, 11, 20, 25, 30, 32, 38, 45] </ref>. Unfortunately, the deterministic notification patterns of scalable barriers may require that processes run in a different order from the one chosen by the scheduler. The problem is related to, but more severe than, the preemption-while-waiting problem in FIFO locks.
Reference: [33] <author> M. Noakes, D. Wallach, and W. Dally. </author> <title> The J-Machine Multicomputer: An Architectural Evaluation. </title> <booktitle> In Proceedings of the Twentieth International Symposium on Computer Architecture, </booktitle> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: In recent years, several machines have been developed that provide hardware support for very fast invocation of functions on remote processors <ref> [18, 33] </ref>. Even on more conventional hardware, programming techniques such as active messages [42] can make remote execution very fast. Because computation is centralized and requests are processed serially, active messages provide implicit synchronization.
Reference: [34] <author> J. K. Ousterhout, D. A. Scelza, and P. S. Sindhu. </author> <title> Medusa: An Experiment in Distributed Operating System Structure. </title> <journal> Communications of the ACM, </journal> <volume> 23(2) </volume> <pages> 92-105, </pages> <month> February </month> <year> 1980. </year>
Reference-contexts: To address the synchronization issue, Ousterhout et al. <ref> [34] </ref> developed the notion of co-scheduling (also known as gang scheduling), in which all of the processes of an application run at the same time.
Reference: [35] <author> J. K. Ousterhout. </author> <title> Scheduling Techniques for Concurrent Systems. </title> <booktitle> In Proceedings of the Third International Conference on Distributed Computing Systems, </booktitle> <pages> pages 22-30, </pages> <month> October </month> <year> 1982. </year>
Reference-contexts: In a fair implementation this has the side effect that readers following the writers are also denied access.) Ousterhout <ref> [35] </ref> introduced spin-then-block locks that attempt to minimize the impact of preemption (or other sources of delay) in critical sections by having a waiting process spin for a small amount of time and then, if unsuccessful, block.
Reference: [36] <author> C. D. Polychronopoulos and D. J. Kuck. </author> <title> Guided Self-scheduling: A Practical Scheduling Scheme for Parallel Supercomputers. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(12):1425-1439, </volume> <month> December </month> <year> 1987. </year>
Reference-contexts: In a similar vein, Zahorjan et al. [48] suggest that barrier-based programs use guided self-scheduling <ref> [36] </ref>, with exactly one server thread per processor. It is not clear, however, that all applications will be able to adjust their number of processes dynamically, or that programmers will wish to write in a style that allows them to do so.
Reference: [37] <author> M. L. Scott, T. J. LeBlanc, and B. D. Marsh. </author> <booktitle> Multi-Model Parallel Programming in Psyche. In Proceedings of the Second ACM Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 70-78, </pages> <address> Seattle, WA, </address> <month> March </month> <year> 1990. </year>
Reference-contexts: Extension (KE-3) is a generalization of the interface described in our work on small-scale scheduler-conscious barriers [17] and resembles the "magic page" of information provided by the Psyche kernel <ref> [37] </ref>. None of the extensions requires the kernel to maintain information that it does not already have available in its internal data structures.
Reference: [38] <author> M. L. Scott and J. M. Mellor-Crummey. </author> <title> Fast, Contention-Free Combining Tree Barriers. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 22(4) </volume> <pages> 449-481, </pages> <month> August </month> <year> 1994. </year> <note> Earlier version available as TR 429, </note> <institution> Computer Science Department, University of Rochester, </institution> <month> June </month> <year> 1992. </year>
Reference-contexts: Several researchers have shown how to solve these problems by building scalable barriers, with log-depth tree- or FFT-like patterns of point-to-point notifications among processes <ref> [3, 11, 20, 25, 30, 32, 38, 45] </ref>. Unfortunately, the deterministic notification patterns of scalable barriers may require that processes run in a different order from the one chosen by the scheduler. The problem is related to, but more severe than, the preemption-while-waiting problem in FIFO locks.
Reference: [39] <author> J. P. Singh, W. Weber, and A. Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory. </title> <journal> ACM SIGARCH Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: To verify the results obtained from the synthetic program, and to investigate the effect of memory traffic generated by data accesses, we measured the performance of a pair of real applications: the Cholesky program from the Stanford SPLASH suite <ref> [39] </ref>, and a multiprocessor version of Quicksort. These programs contain no barriers; they synchronize only with locks. Figures 17 to 20 show their completion times, in seconds, when run with a multiprogramming level of 2 using 11 processors on the SGI and 63 processors on the KSR.
Reference: [40] <author> J. W. Stamos and D. K. Gifford. </author> <title> Remote Evaluation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 12(4) </volume> <pages> 537-565, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: This sort of organization is common in distributed systems. It can be cast as a natural interpretation of monitors (as, for example, in Brinch Hansen's Distributed Processes notation [6]), or as function shipping <ref> [24, 40] </ref> to a common destination. In recent years, several machines have been developed that provide hardware support for very fast invocation of functions on remote processors [18, 33]. Even on more conventional hardware, programming techniques such as active messages [42] can make remote execution very fast.
Reference: [41] <author> A. Tucker and A. Gupta. </author> <title> Process Control and Scheduling Issues for Multiprogrammed Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the Twelfth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 159-166, </pages> <address> Litchfield Park, AZ, </address> <month> December </month> <year> 1989. </year>
Reference-contexts: 3 Barriers 3 3 Table 1: Types of performance loss encountered in different synchronization algorithms processes than processors as the main reason for remaining performance degradation and propose a scheme in which applications adapt to changes in the size of hardware partitions by changing the number of processes they use <ref> [41] </ref>. In a similar vein, Zahorjan et al. [48] suggest that barrier-based programs use guided self-scheduling [36], with exactly one server thread per processor. <p> Our mutual exclusion and reader-writer locks use only the context block records; the barriers use both. All of our algorithms work well in a dynamic hardware-partitioned environment|an environment widely believed to provide the best combination of throughput and fast turnaround for large-scale multiprocessors <ref> [7, 22, 41, 47] </ref>. Except for the barriers, which require partition information, all of the algorithms will also work well under ordinary time sharing.
Reference: [42] <author> T. von Eicken, D. E. Culler, S. C. Goldstein, and K. E. Schauser. </author> <title> Active Messages: a Mechanism for Integrated Communication and Computation. </title> <booktitle> In Proceedings of the Nineteenth International Symposium on Computer Architecture, </booktitle> <pages> pages 256-266, </pages> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: In recent years, several machines have been developed that provide hardware support for very fast invocation of functions on remote processors [18, 33]. Even on more conventional hardware, programming techniques such as active messages <ref> [42] </ref> can make remote execution very fast. Because computation is centralized and requests are processed serially, active messages provide implicit synchronization. On the other hand, they do not permit concur-rency (as do, say, reader-writer locks), and can only be used when the manager is not a bottleneck.
Reference: [43] <author> R. W. Wisniewski, L. Kontothanassis, and M. L. Scott. </author> <title> Scalable Spin Locks for Mul-tiprogrammed Systems. </title> <booktitle> In Proceedings of the Eighth International Parallel Processing 46 Symposium, </booktitle> <pages> pages 583-589, </pages> <address> Cancun, Mexico, </address> <month> April </month> <year> 1994. </year> <note> Earlier but expanded ver-sion available as TR 454, </note> <institution> Computer Science Department, University of Rochester, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: Heuristics based on past behavior suffice to obtain competitive performance, but a small extension to the kernel/user interface admits on-line optimal decisions. 4 The mutual exclusion locks originally appeared in a conference publication <ref> [43] </ref>. 7 Centralized barriers generally employ a counter that is updated by each process. Access to the counter poses two obstacles to scalable performance on large machines. First, as with locks, simultaneous attempts to update the counter can lead to unacceptable levels of contention. <p> As noted above, extensions KE-1 and KE-2 are based in part on ideas developed for the Symunix kernel [8]. We have introduced additional states, and have made the state variable writable and readable by both user-level and kernel-level code <ref> [43] </ref>. Extension (KE-3) is a generalization of the interface described in our work on small-scale scheduler-conscious barriers [17] and resembles the "magic page" of information provided by the Psyche kernel [37]. <p> We believe that as large-scale multiprocessors become more common they will inevitably be multiprogrammed, and the importance of exchanging information across the kernel-user boundary will increase. Acknowledgements Our thanks to Hiroaki Takada for discovering a subtle timing window in our scalable queue algorithm <ref> [43] </ref>, to Donna Bergmark and the Cornell Theory Center for their help with the KSR 1, and to Maged Michael for his careful reading and helpful suggestions.
Reference: [44] <author> H. Yang and J. H. Anderson. </author> <title> Fast, Scalable Synchronization with Minimal Hardware Support (extended abstract). </title> <booktitle> In Proceedings of the Twelfth ACM Symposium on Principles of Distributed Computing, </booktitle> <month> August </month> <year> 1993. </year>
Reference-contexts: Markatos [27] uses a doubly-linked list in a queue-based lock to replace FIFO order with highest-priority-first, for real-time systems. Others have shown how to build queue-based 6 scalable reader-writer locks [19, 31]. Yang and Anderson <ref> [44] </ref> have shown how to use local--only spinning to improve the performance of locks that use no atomic memory operations other than loads and stores. Scalable synchronization algorithms reduce contention for memory and interconnect resources but usually have higher overhead than their centralized counterparts in the absence of contention. <p> Many researchers have now developed algorithms of this type <ref> [1, 9, 26, 30, 44] </ref>. The scalability of centralized algorithms may also be improved by introducing appropriate forms of back-off [1, 30].
Reference: [45] <author> P. Yew, N. Tzeng, and D. H. Lawrie. </author> <title> Distributing Hot-Spot Addressing in Large-Scale Multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(4):388-395, </volume> <month> April </month> <year> 1987. </year>
Reference-contexts: Several researchers have shown how to solve these problems by building scalable barriers, with log-depth tree- or FFT-like patterns of point-to-point notifications among processes <ref> [3, 11, 20, 25, 30, 32, 38, 45] </ref>. Unfortunately, the deterministic notification patterns of scalable barriers may require that processes run in a different order from the one chosen by the scheduler. The problem is related to, but more severe than, the preemption-while-waiting problem in FIFO locks.
Reference: [46] <author> J. Zahorjan, E. D. Lazowska, and D. L. Eager. </author> <title> Spinning Versus Blocking in Parallel Systems with Uncertainty. </title> <booktitle> In Proceedings of the International Symposium on Performance of Distributed and Parallel Systems, </booktitle> <month> December </month> <year> 1988. </year>
Reference-contexts: Competitive spinning works best when the behavior of a lock does not change rapidly with time, so that past behavior is an appropriate indicator of future behavior. Zahorjan et al. <ref> [46, 48] </ref> present a formal model of spin-wait times.
Reference: [47] <author> J. Zahorjan and C. McCann. </author> <title> Processor Scheduling in Shared Memory Multiprocessors. </title> <booktitle> In Proceedings of the 1990 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 214-225, </pages> <address> Boulder, CO, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: Using simulation, modeling, and experimentation, Crovella et al. [7], Leutenegger and Vernon [22], Tucker and Gupta [10], and Zahorjan and McCann <ref> [47] </ref> have shown that dynamically partitioning the processors of a machine among applications is preferable to either timesharing or co-scheduling. <p> Our mutual exclusion and reader-writer locks use only the context block records; the barriers use both. All of our algorithms work well in a dynamic hardware-partitioned environment|an environment widely believed to provide the best combination of throughput and fast turnaround for large-scale multiprocessors <ref> [7, 22, 41, 47] </ref>. Except for the barriers, which require partition information, all of the algorithms will also work well under ordinary time sharing.
Reference: [48] <author> J. Zahorjan, E. D. Lazowska, and D. L. Eager. </author> <title> The Effect of Scheduling Discipline on Spin Overhead in Shared Memory Parallel Systems. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(2) </volume> <pages> 180-198, </pages> <month> April </month> <year> 1991. </year> <month> 47 </month>
Reference-contexts: The algorithm's performance is then vulnerable not only to preemption of the process in the critical section, but also to preemption of processes near the head of the waiting list|the algorithm may give the lock to a process that is not running <ref> [48] </ref>. Similarly, a barrier algorithm may keep processes in a tree, in order to replace O (n) serialized operations on a counter with O (log n) operations on the longest path in the tree. <p> Competitive spinning works best when the behavior of a lock does not change rapidly with time, so that past behavior is an appropriate indicator of future behavior. Zahorjan et al. <ref> [46, 48] </ref> present a formal model of spin-wait times. <p> If a process is preempted while awaiting its turn to access a shared data structure, processes later in the order cannot proceed even if the lock is released by the original owner|the lock will be passed to the preempted process instead. This problem was noted by Zahorjan et al. <ref> [48] </ref>, but no solution was suggested. <p> In a similar vein, Zahorjan et al. <ref> [48] </ref> suggest that barrier-based programs use guided self-scheduling [36], with exactly one server thread per processor. It is not clear, however, that all applications will be able to adjust their number of processes dynamically, or that programmers will wish to write in a style that allows them to do so.
References-found: 48

