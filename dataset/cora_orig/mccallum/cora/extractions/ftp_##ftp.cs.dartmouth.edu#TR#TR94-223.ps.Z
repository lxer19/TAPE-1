URL: ftp://ftp.cs.dartmouth.edu/TR/TR94-223.ps.Z
Refering-URL: http://www.cs.dartmouth.edu/reports/abstracts/TR94-223/
Root-URL: http://www.cs.dartmouth.edu
Title: Asymptotically Tight Bounds for Performing BMMC Permutations on Parallel Disk Systems 1 rank lg(M=B) parallel
Author: Thomas H. Cormen Thomas Sundquist Leonard F. Wisniewski lg(M=B) N lg(N=B) 
Keyword: bound of  
Date: 2  
Note: BD  rithm that uses at most 2N  
Address: College  
Affiliation: Department of Mathematics and Computer Science Dartmouth  
Abstract: We give asymptotically equal lower and upper bounds for the number of parallel I/O operations required to perform bit-matrix-multiply/complement (BMMC) permutations on parallel disk systems. In a BMMC permutation on N records, where N is a power of 2, each (lg N )-bit source address x maps to a corresponding (lg N)-bit target address y by the matrix equation y = A x c, where matrix multiplication is performed over GF (2). The characteristic matrix A is (lg N )fi(lg N ) and nonsingular over GF (2). Under the Vitter-Shriver parallel-disk model with N records, D disks, B records per block, and M records of memory, we show a universal lower is the lower left lg(N=B) fi lg B submatrix of the characteristic matrix. We also present an algo We introduce a new subclass of BMMC permutations, called memoryload-dispersal (MLD) permutations, which can be performed in one pass. This subclass, which is used in the BMMC algorithm, extends the catalog of one-pass permutations appearing in [4]. Although many BMMC permutations of practical interest fall into subclasses that might be explicitly invoked within the source code, we show how to detect in at most N=BD+ then avoid the general-permutation algorithm and save parallel I/Os by using our algorithm. l
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Aggarwal, A. K. Chandra, and M. Snir. </author> <title> Hierarchical memory with block transfer. </title> <booktitle> In Proceedings of the 28th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 204-216, </pages> <month> Oct. </month> <year> 1987. </year>
Reference-contexts: As [4] shows, both the standard binary-reflected Gray code and its inverse have characteristic matrices of this form, and so they are MRC permutations. 3 Johnsson and Ho [8] call BPC permutations dimension permutations, and Aggarwal, Chandra, and Snir <ref> [1] </ref> call BPC permutations without complementing rational permutations. 6 MLD permutations We define here a new BMMC permutation subclass, which we shall use in our asymptotically optimal BMMC algorithm.
Reference: [2] <author> A. Aggarwal and J. S. Vitter. </author> <title> The input/output complexity of sorting and related problems. </title> <journal> Commun. ACM, </journal> <volume> 31(9) </volume> <pages> 1116-1127, </pages> <month> Sept. </month> <year> 1988. </year>
Reference-contexts: For BMMC permutations, r is the rank of the leading lg M fi lg M submatrix of A, and the function H (N; M; B) is given by equation (1). For BPC permutations, the function (A) is defined in equation (3). Vitter <ref> [2] </ref> using a model with one disk and D independent read/write heads, which is at least as powerful as the Vitter-Shriver model. Specific classes of permutations sometimes require fewer parallel I/Os than general permu tations. <p> The BPC algorithm of [4] is asymptotically optimal as well; see [3] for details. Technique To prove Theorem 3, we rely heavily on the technique used by Aggarwal and Vitter <ref> [2] </ref> for a lower bound on I/Os in matrix transposition; their proof is based in turn on a method by Floyd [6]. We prove the lower bound for the case in which D = 1; the general case follows by dividing by D. We consider only I/Os that are simple. <p> The multiplicative and additive constants in the I/O complexity of our algorithm are small, which is especially fortunate in light of the expense of disk accesses. One can adapt the proof by Aggarwal and Vitter <ref> [2] </ref> of Lemma 6 to bound max precisely, rather than just asymptotically. In particular, it is a straightforward exercise to derive the bound max B 2 + lg (M=B) : Moreover, the potential change is at most zero for write operations, and so the potential increases only during read operations.
Reference: [3] <author> T. H. Cormen. </author> <title> Virtual Memory for Data-Parallel Computing. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, </institution> <year> 1992. </year> <note> Available as Technical Report MIT/LCS/TR-559. </note>
Reference-contexts: As one of the most basic data-movement operations, Portions of this research were performed while Tom Cormen was at the MIT Laboratory for Computer Science and appear in <ref> [3] </ref>. He was supported in part by the Defense Advanced Research Projects Agency under Grant N00014-91-J-1698. Other portions of this research were performed while at Dartmouth College and were supported in part by funds from Dartmouth College and in part by the National Science Foundation under Grant CCR-9308667. <p> Although this cost model does not account for the variation in disk access times caused by head movement and rotational latency, programmers often have no control over these factors. The number of disk accesses, however, can be minimized by carefully designed algorithms such as those in <ref> [3, 4, 7, 9, 10, 11, 13, 14] </ref> and this paper. <p> &gt; &gt; : ~ lg (M=B) + 9 if M N ; ~ lg (M=B) + 1 if N &lt; M &lt; NB ; p (1) One can adapt the lower bound proven in this paper to show that BD lg (M=B) parallel I/Os are necessary (see Section 2.8 of <ref> [3] </ref> for example), but so far it has been unknown whether the fi BD H (N; M; B) term is necessary in all cases. <p> Section 5 presents an algorithm that achieves the bound given by Theorem 3, and so this algorithm is asymptotically optimal. The BPC algorithm of [4] is asymptotically optimal as well; see <ref> [3] </ref> for details. Technique To prove Theorem 3, we rely heavily on the technique used by Aggarwal and Vitter [2] for a lower bound on I/Os in matrix transposition; their proof is based in turn on a method by Floyd [6]. <p> We make the trailing (n m) fi (n m) submatrix nonsingular by adding columns in into those in ffi. Consider ffi as a set of n m columns and as a set of m columns. We use Gaussian 21 elimination as described in <ref> [3] </ref> to determine a maximal set V of linearly independent columns in ffi and a set W of n m rank ffi columns in that, along with V , comprise a set of n m linearly independent columns. <p> It is particularly satisfying that the tight bound was achieved not by raising the lower bound proven here and in <ref> [3] </ref>, but by decreasing the upper bound in [4]. The multiplicative and additive constants in the I/O complexity of our algorithm are small, which is especially fortunate in light of the expense of disk accesses. <p> Detection is inexpensive and, when successful, permits the execution of our BMMC algorithm or possibly a faster algorithm for a more restricted permutation class. What other permutations can be performed quickly? Several O (1)-pass permutation classes appear in <ref> [3] </ref>, and this paper has added one more (MLD permutations in Section 3). One can also show that the inverse of any one-pass permutation is a one-pass permutation. Moreover, the composition of an MLD permutation with the inverse of an MLD permutation is a one-pass permutation.
Reference: [4] <author> T. H. Cormen. </author> <title> Fast permuting in disk arrays. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 17(1-2):41-57, </volume> <month> Jan. and Feb. </month> <year> 1993. </year>
Reference-contexts: Although this cost model does not account for the variation in disk access times caused by head movement and rotational latency, programmers often have no control over these factors. The number of disk accesses, however, can be minimized by carefully designed algorithms such as those in <ref> [3, 4, 7, 9, 10, 11, 13, 14] </ref> and this paper. <p> (bit-matrix-multiply/ complement) nonsingular matrix A 2 ~ lg (M=B) + H (N; M; B) (bit-permute/ complement) permutation matrix A 2 ~ lg (M=B) + 1 (memory rearrangement/ complement) m n m nonsingular arbitrary 0 nonsingular n m Table 1: Classes of permutations, their characteristic matrices, and upper bounds shown in <ref> [4] </ref> on the number of passes needed to perform them. A pass consists of reading and writing each record exactly once and therefore uses exactly 2N=BD parallel I/Os. For MRC permutations, submatrix dimensions are shown on matrix borders. <p> Specific classes of permutations sometimes require fewer parallel I/Os than general permu tations. Vitter and Shriver showed how to transpose an R fi S matrix (N = RS) with only fi N lg min (B;R;S;N=B) parallel I/Os. Subsequently, Cormen <ref> [4] </ref> studied several classes of bit-defined permutations that include matrix transposition as a special case. Table 1 shows some of the classes of permutations examined and the corresponding upper bounds derived in [4]. BMMC permutations The most general class considered in [4] is bit-matrix-multiply/complement, or BMMC, permutations. 1 In a BMMC <p> Subsequently, Cormen <ref> [4] </ref> studied several classes of bit-defined permutations that include matrix transposition as a special case. Table 1 shows some of the classes of permutations examined and the corresponding upper bounds derived in [4]. BMMC permutations The most general class considered in [4] is bit-matrix-multiply/complement, or BMMC, permutations. 1 In a BMMC permutation, we have an n fi n characteristic matrix A = (a ij ) whose entries are drawn from f0; 1g and is nonsingular (i.e., invertible) over GF (2), 2 and we <p> Subsequently, Cormen <ref> [4] </ref> studied several classes of bit-defined permutations that include matrix transposition as a special case. Table 1 shows some of the classes of permutations examined and the corresponding upper bounds derived in [4]. BMMC permutations The most general class considered in [4] is bit-matrix-multiply/complement, or BMMC, permutations. 1 In a BMMC permutation, we have an n fi n characteristic matrix A = (a ij ) whose entries are drawn from f0; 1g and is nonsingular (i.e., invertible) over GF (2), 2 and we have a complement vector c = (c 0 ; <p> That is, we perform the permutations characterized by the factors of a matrix from right to left. Proof: The proof is a simple induction, using Lemma 1. The BMMC algorithm in <ref> [4] </ref> exploits Corollary 2 to factor a characteristic matrix into a product of other characteristic matrices, performing the permutations given by the factors right to left. <p> The class of BPC permutations includes many common permutations such as matrix transposition, bit-reversal permutations (used in performing FFTs), vector-reversal permutations, hypercube permutations, and matrix reblocking. Previous work <ref> [4] </ref> expressed the I/O complexity of BPC permutations in terms of cross-ranks. <p> The cross-rank of A is the maximum of the band m-cross-ranks: (A) = max ( b (A); m (A)) : (3) The BPC algorithm in <ref> [4] </ref> uses at most 2N ~ lg (M=B) + 1 parallel I/Os. One can adapt the lower bound we prove in Section 2 for BMMC permutations to show that this BPC algorithm is asymptotically optimal. <p> The BMMC algorithm in Section 5, however, is asymptotically optimal for all BMMC permutations|including those that are BPC| and it reduces the innermost factor of 2 in the above bound to a factor of 1. Not only is the BPC algorithm in <ref> [4] </ref> improved upon by the results in this paper, but the notion of cross-rank appears to be obviated as well. <p> Cormen <ref> [4] </ref> shows that any MRC permutation requires only one pass of N=BD parallel reads and N=BD parallel writes. If we partition the N records into N=M consecutive sets of M records each, we call each set a memoryload. <p> Any MRC permutation can be performed by reading in a memoryload, permuting its records in memory, and writing them out to a (possibly different) memoryload number. The class of MRC permutations includes those characterized by unit upper triangular matrices. As <ref> [4] </ref> shows, both the standard binary-reflected Gray code and its inverse have characteristic matrices of this form, and so they are MRC permutations. 3 Johnsson and Ho [8] call BPC permutations dimension permutations, and Aggarwal, Chandra, and Snir [1] call BPC permutations without complementing rational permutations. 6 MLD permutations We define <p> Section 5 presents an algorithm that achieves the bound given by Theorem 3, and so this algorithm is asymptotically optimal. The BPC algorithm of <ref> [4] </ref> is asymptotically optimal as well; see [3] for details. Technique To prove Theorem 3, we rely heavily on the technique used by Aggarwal and Vitter [2] for a lower bound on I/Os in matrix transposition; their proof is based in turn on a method by Floyd [6]. <p> These M=B target blocks are distributed evenly among the disks, with M=BD mapping to each disk. Given these properties, we can perform an MLD permutation in one pass. Like the other one-pass permutations described in <ref> [4] </ref>, we allow the permutation to map records from one set of N=BD stripes (the "source portion" of the parallel disk system) to a different set of N=BD stripes (the "target portion"). One can think of addresses as relative to the beginning of the appropriate portion. <p> In this way, we need not be concerned with overwriting source records before we get a chance to read them. Note that when we chain passes together, as in the BMMC algorithm of Section 5 and the BPC algorithm of <ref> [4] </ref>, we can avoid allocating a new target portion in each pass by reversing the roles of the source and target portions between passes. We perform an MLD permutation by processing source memoryload numbers from 0 to N=M 1. <p> For that matter, we wish to run even faster algorithms for any of the special cases of BMMC permutations (MRC, MLD, or block BMMC <ref> [4] </ref>) whenever possible as well. We must know the characteristic matrix A and complement vector c, however, to run any of these algorithms. <p> It is particularly satisfying that the tight bound was achieved not by raising the lower bound proven here and in [3], but by decreasing the upper bound in <ref> [4] </ref>. The multiplicative and additive constants in the I/O complexity of our algorithm are small, which is especially fortunate in light of the expense of disk accesses. One can adapt the proof by Aggarwal and Vitter [2] of Lemma 6 to bound max precisely, rather than just asymptotically.
Reference: [5] <author> A. Edelman, S. Heller, and S. L. Johnsson. </author> <title> Index transformation algorithms in a linear algebra framework. </title> <type> Technical Report LBL-31841, </type> <institution> Lawrence Berkeley Laboratory, </institution> <year> 1992. </year>
Reference-contexts: . . . . . . a n1;0 a n1;1 a n1;2 a n1;n1 7 7 7 5 6 6 6 4 x 1 . . . 3 7 7 7 6 6 6 4 c 1 . . . 3 7 7 7 : 1 Edelman, Heller, and Johnsson <ref> [5] </ref> call BMMC permutations affine transformations or, if there is no comple menting, linear transformations. 2 Matrix multiplication over GF (2) is like standard matrix multiplication over the reals but with all arithmetic performed modulo 2.
Reference: [6] <author> R. W. Floyd. </author> <title> Permuting information in idealized two-level storage. </title> <editor> In R. E. Miller and J. W. Thatcher, editors, </editor> <booktitle> Complexity of Computer Computations, </booktitle> <pages> pages 105-109. </pages> <publisher> Plenum Press, </publisher> <year> 1972. </year>
Reference-contexts: Technique To prove Theorem 3, we rely heavily on the technique used by Aggarwal and Vitter [2] for a lower bound on I/Os in matrix transposition; their proof is based in turn on a method by Floyd <ref> [6] </ref>. We prove the lower bound for the case in which D = 1; the general case follows by dividing by D. We consider only I/Os that are simple. An input is simple if each record read is removed from the disk and moved into an empty location in memory.
Reference: [7] <author> M. T. Goodrich, J.-J. Tsay, D. E. Vengroff, and J. S. Vitter. </author> <title> External-memory computational geometry. </title> <booktitle> In Proceedings of the 34th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 714-723, </pages> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: Although this cost model does not account for the variation in disk access times caused by head movement and rotational latency, programmers often have no control over these factors. The number of disk accesses, however, can be minimized by carefully designed algorithms such as those in <ref> [3, 4, 7, 9, 10, 11, 13, 14] </ref> and this paper.
Reference: [8] <author> S. L. Johnsson and C.-T. Ho. </author> <title> Generalized shu*e permutations on boolean cubes. </title> <type> Technical Report TR-04-91, </type> <institution> Harvard University Center for Research in Computing Technology, </institution> <month> Feb. </month> <year> 1991. </year>
Reference-contexts: The class of MRC permutations includes those characterized by unit upper triangular matrices. As [4] shows, both the standard binary-reflected Gray code and its inverse have characteristic matrices of this form, and so they are MRC permutations. 3 Johnsson and Ho <ref> [8] </ref> call BPC permutations dimension permutations, and Aggarwal, Chandra, and Snir [1] call BPC permutations without complementing rational permutations. 6 MLD permutations We define here a new BMMC permutation subclass, which we shall use in our asymptotically optimal BMMC algorithm.
Reference: [9] <author> M. H. Nodine and J. S. Vitter. </author> <title> Greed sort: An optimal external sorting algorithm for multiple disks. </title> <type> Technical Report CS-91-20, </type> <institution> Department of Computer Science, Brown University, </institution> <year> 1991. </year>
Reference-contexts: Although this cost model does not account for the variation in disk access times caused by head movement and rotational latency, programmers often have no control over these factors. The number of disk accesses, however, can be minimized by carefully designed algorithms such as those in <ref> [3, 4, 7, 9, 10, 11, 13, 14] </ref> and this paper. <p> The first term comes into play when the block size B is small, and the second term is the sorting bound fi BD lg (M=B) , which was shown by Vitter and Shriver for randomized sorting and by Nodine and Vitter <ref> [9, 10, 11] </ref> for deterministic sorting.
Reference: [10] <author> M. H. Nodine and J. S. Vitter. </author> <title> Large-scale sorting in parallel memories. </title> <booktitle> In Proceedings of the 3rd Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 29-39, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Although this cost model does not account for the variation in disk access times caused by head movement and rotational latency, programmers often have no control over these factors. The number of disk accesses, however, can be minimized by carefully designed algorithms such as those in <ref> [3, 4, 7, 9, 10, 11, 13, 14] </ref> and this paper. <p> The first term comes into play when the block size B is small, and the second term is the sorting bound fi BD lg (M=B) , which was shown by Vitter and Shriver for randomized sorting and by Nodine and Vitter <ref> [9, 10, 11] </ref> for deterministic sorting.
Reference: [11] <author> M. H. Nodine and J. S. Vitter. </author> <title> Optimal deterministic sorting on parallel disks. </title> <type> Technical Report CS-92-08, </type> <institution> Department of Computer Science, Brown University, </institution> <year> 1992. </year>
Reference-contexts: Although this cost model does not account for the variation in disk access times caused by head movement and rotational latency, programmers often have no control over these factors. The number of disk accesses, however, can be minimized by carefully designed algorithms such as those in <ref> [3, 4, 7, 9, 10, 11, 13, 14] </ref> and this paper. <p> The first term comes into play when the block size B is small, and the second term is the sorting bound fi BD lg (M=B) , which was shown by Vitter and Shriver for randomized sorting and by Nodine and Vitter <ref> [9, 10, 11] </ref> for deterministic sorting.
Reference: [12] <author> G. Strang. </author> <title> Linear Algebra and Its Applications. </title> <publisher> Harcourt Brace Jovanovich, </publisher> <address> San Diego, third edition, </address> <year> 1988. </year> <month> 28 </month>
Reference-contexts: Lemma 11 Let K and L be q-column matrices for which ker K ker L. Then row L row K. Proof: It is a well-known fact from linear algebra (see Strang <ref> [12, p. 138] </ref> for example) that the row space is the orthogonal complement of the kernel. That is, for any q-vectors u 2 ker L and v 2 row L, the inner product u v is 0.
Reference: [13] <author> J. S. Vitter and E. A. M. Shriver. </author> <title> Optimal disk I/O with parallel block transfer. </title> <booktitle> In Proceedings of the Twenty Second Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 159-169, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Moreover, the low constant factor in our algorithm makes it very practical. Model and previous results We use the parallel-disk model first proposed by Vitter and Shriver <ref> [13, 14] </ref>, who also gave asymptotically optimal algorithms for several problems including sorting and general permutations. In the Vitter-Shriver model, N records are stored on D disks D 0 ; D 1 ; : : : ; D D1 , with N=D records stored on each disk. <p> Although this cost model does not account for the variation in disk access times caused by head movement and rotational latency, programmers often have no control over these factors. The number of disk accesses, however, can be minimized by carefully designed algorithms such as those in <ref> [3, 4, 7, 9, 10, 11, 13, 14] </ref> and this paper.
Reference: [14] <author> J. S. Vitter and E. A. M. Shriver. </author> <title> Algorithms for parallel memory I: Two-level memories. </title> <type> Technical Report CS-92-04, </type> <institution> Department of Computer Science, Brown University, </institution> <month> Aug. </month> <year> 1992. </year> <note> Revised version of Technical Report CS-90-21. 29 </note>
Reference-contexts: Moreover, the low constant factor in our algorithm makes it very practical. Model and previous results We use the parallel-disk model first proposed by Vitter and Shriver <ref> [13, 14] </ref>, who also gave asymptotically optimal algorithms for several problems including sorting and general permutations. In the Vitter-Shriver model, N records are stored on D disks D 0 ; D 1 ; : : : ; D D1 , with N=D records stored on each disk. <p> Although this cost model does not account for the variation in disk access times caused by head movement and rotational latency, programmers often have no control over these factors. The number of disk accesses, however, can be minimized by carefully designed algorithms such as those in <ref> [3, 4, 7, 9, 10, 11, 13, 14] </ref> and this paper.
References-found: 14

