URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P311.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts92.htm
Root-URL: http://www.mcs.anl.gov
Title: AUTOMATIC DIFFERENTIATION FOR PDES UNSATURATED FLOW CASE STUDY  Methods for Partial Differential Equations  
Author: George F. Corliss, Christian Bischof, Thomas Robey Andreas Griewank, and Steven J. Wright VII, R. Vichnevetski, D. Knight, and G. Richter, 
Note: Argonne Preprint ANL/MCS-P311-0692 Published in Advances in Computer  Eds., IMACS, New Brunswick, pp. 150-156, 1992.  
Address: Albuquerque, NM 87102-1710 9700 S. Cass Avenue, Argonne, IL 60439  
Affiliation: SPECTRA Research Institute Mathematics and Computer Science Division 1613 University Blvd. NE Argonne National Laboratory  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> W. Baur and V. Strassen. </author> <title> The complexity of partial derivatives. </title> <journal> Theoretical Computer Science, </journal> <volume> 22 </volume> <pages> 317-330, </pages> <year> 1983. </year>
Reference-contexts: This is exactly the case for computing a gradient, which can be viewed as a Jacobian matrix with only one row. This issue is discussed in more detail in [8, 11, 12]. Wolfe observed [17], and Baur and Strassen confirmed <ref> [1] </ref>, that if care is taken in handling quantities which are common to the (rational) function and its derivatives, then the cost of evaluating a gradient with n components is a small multiple of the cost of evaluating the underlying scalar function.
Reference: [2] <author> Bruce D. Christianson. </author> <title> Automatic Hessians by reverse accumulation. </title> <type> Technical Report NOC TR228, </type> <institution> The Numerical Optimisation Center, Hatfield Polytechnic, Hatfield, U.K., </institution> <month> April </month> <year> 1990. </year>
Reference-contexts: We also note that even though we showed the computation only of first derivatives, the automatic differentiation approach can easily be generalized to the computation of univariate Taylor series or Hessians and multivariate higher-order derivatives <ref> [2, 9, 15] </ref>. This discussion is intended to demonstrate that the principles underlying automatic differentiation are not complicated: We just associate extra computations (which are entirely specified on a statement-by-statement basis) with the statements executed in the original code. <p> unsigned short Tape_Tag = 1; int Keep = 0; int degree = 1; double **Indep_X = new double*[dim]; double **Depend_Y = new double*[dim]; adouble ad_xv [dim]; adouble ad_r [dim]; int adual (adouble *, adouble *); for (j = 0; j &lt; dim; j ++) - Indep_X [j] = new double <ref> [2] </ref>; Depend_Y [j] = new double [2]; - /* Compute right hand side vector f */ f=(double *) calloc (dim,sizeof (double)); if (f==NULL) return (-1); trace_on (Tape_Tag, Keep); for (i = 0; i &lt; dim; i ++) - ad_xv [i] = x [i]; else - // Nominate independent variables ad_xv [i] <p> Keep = 0; int degree = 1; double **Indep_X = new double*[dim]; double **Depend_Y = new double*[dim]; adouble ad_xv [dim]; adouble ad_r [dim]; int adual (adouble *, adouble *); for (j = 0; j &lt; dim; j ++) - Indep_X [j] = new double <ref> [2] </ref>; Depend_Y [j] = new double [2]; - /* Compute right hand side vector f */ f=(double *) calloc (dim,sizeof (double)); if (f==NULL) return (-1); trace_on (Tape_Tag, Keep); for (i = 0; i &lt; dim; i ++) - ad_xv [i] = x [i]; else - // Nominate independent variables ad_xv [i] &lt;<= x [i]; - k =
Reference: [3] <author> T. F. Coleman and J. J. </author> <title> More. Estimation of sparse Jacobian matrices and graph coloring problems. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 20 </volume> <pages> 187-209, </pages> <year> 1984. </year>
Reference-contexts: A naive coding perturbing each component of u in turn is easy, but very expensive. Implementations usually attempt to exploit the sparsity structure which is known to be present. Matrix Coloring One technique for exploiting the sparsity structure of J is matrix coloring <ref> [3] </ref>. If two columns of J have nonzero elements only in disjoint sets of rows, then those two columns can be computed simultaniously, using either divided difference or automatic differentiation techniques.
Reference: [4] <author> George Corliss, Andreas Griewank, Tom Robey, and Steve Wright. </author> <title> Automatic differentiation applied to unsaturated flow | ADOL-C case study. </title> <type> Technical Memorandum ANL/MCS-TM-162, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, Ill., </institution> <month> April </month> <year> 1992. </year>
Reference-contexts: This is the code that formed the basis for the further explorations described below. 6 Conversion to ADOL-C In this section, we describe the steps involved in converting the original code to generate J using ADOL-C. More details of the conversion can be found in <ref> [4] </ref>. 6.1 Step 1: Convert to C++ The program unsat was first converted to run with the GNU G++ implementation of C++.
Reference: [5] <author> Lawrence C. W. Dixon. </author> <title> Use of automatic differentiation for calculating Hessians and Newton steps. </title> <editor> In Andreas Griewank and George F. Corliss, editors, </editor> <title> Automatic Differentiation of Algorithms: Theory, </title> <booktitle> Implementation, and Application, </booktitle> <pages> pages 114-125. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, Penn., </address> <year> 1991. </year>
Reference-contexts: Current tools (see [14]) achieve this by storing a record of every computation performed. An interpreter performs a backward pass on this "tape." The resulting overhead often dominates the complexity advantage of the reverse mode in an actual implementation (see <ref> [5] </ref>). We also note that even though we showed the computation only of first derivatives, the automatic differentiation approach can easily be generalized to the computation of univariate Taylor series or Hessians and multivariate higher-order derivatives [2, 9, 15].
Reference: [6] <author> Richard Ewing and Mary Wheeler. </author> <title> Computational aspects of mixed finite element methods. </title> <editor> In R. Stepleman et. al., editor, </editor> <publisher> Scientific Computing. IMACS/North-Holland Publishing Comapany, </publisher> <year> 1983. </year>
Reference-contexts: Steady state porous media flow involves an elliptic partial differential equation that contains a conductivity coeffecient <ref> [6, 16] </ref>. The coefficient is typically discontinuous across different materials and can vary greatly. For unsaturated flow, the conductivity is usually taken to be a function of pore pressure which introduces a nonlinearity to the problem.
Reference: [7] <author> Andreas Griewank. </author> <title> On solving nonlinear equations with simple singularities or nearly singular solutions. </title> <journal> SIAM Review, </journal> <volume> 27(4) </volume> <pages> 537-563, </pages> <month> December </month> <year> 1985. </year>
Reference-contexts: However, the rapid changes in conductivities can cause poor conditioning of the Jacobian or possibly rank deficiency. One can handle rank deficiency by adding some constraints to uniquely define a solution. Alternatively, one should take into account the suggestions of Griewank <ref> [7] </ref> on the behavior of Newton's method and its variation for singular systems. Two different situations must be distinguished. In the first case, there is (locally) a smooth solution manifold of dimension p, and the rank of the Jacobian drops by exactly p at the solutions.
Reference: [8] <author> Andreas Griewank. </author> <title> On automatic differentiation. </title> <editor> In M. Iri and K. Tanabe, editors, </editor> <booktitle> Mathematical Programming: Recent Developments and Applications, </booktitle> <pages> pages 83-108. </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1989. </year> <note> Also appeared as Preprint MCS-P10-1088, </note> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, Ill., </institution> <month> October </month> <year> 1988. </year>
Reference-contexts: 1 Introduction The techniques of automatic differentiation <ref> [8, 10, 15] </ref> are applied to an example partial differential equation arising from the modeling of unsaturated flow. One common paradigm for the numerical solution to some classes of 2-, 3-, or higher-dimensional partial differential equations is: 1. <p> Assume that rt contains the deriva tives of t with respect to the independent variables x, rt = i @x (1) ; @t j T We propagate these derivatives by using elementary differentiation arithmetic based on the chain rule <ref> [8, 15] </ref> for computing the derivatives of y (1) and y (2), as shown in Figure 2. <p> This is exactly the case for computing a gradient, which can be viewed as a Jacobian matrix with only one row. This issue is discussed in more detail in <ref> [8, 11, 12] </ref>.
Reference: [9] <author> Andreas Griewank. </author> <title> Automatic evaluation of first- and higher-derivative vectors. </title> <editor> In R. Seydel, F. W. Schneider, T. Kupper, and H. Troger, editors, </editor> <booktitle> Proceedings of the Conference at Wurzburg, </booktitle> <month> Aug. </month> <year> 1990, </year> <title> Bifurcation and Chaos: Analysis, Algorithms, </title> <journal> Applications, </journal> <volume> volume 97, </volume> <pages> pages 135-148. </pages> <publisher> Birkhauser Verlag, </publisher> <address> Basel, Switzerland, </address> <year> 1991. </year>
Reference-contexts: We also note that even though we showed the computation only of first derivatives, the automatic differentiation approach can easily be generalized to the computation of univariate Taylor series or Hessians and multivariate higher-order derivatives <ref> [2, 9, 15] </ref>. This discussion is intended to demonstrate that the principles underlying automatic differentiation are not complicated: We just associate extra computations (which are entirely specified on a statement-by-statement basis) with the statements executed in the original code.
Reference: [10] <author> Andreas Griewank. </author> <title> The chain rule revisited in scientific computing. </title> <journal> SIAM News, </journal> <volume> 24, </volume> <month> May & July </month> <year> 1991. </year> <note> No. 3, </note> <editor> p. </editor> <volume> 20 & No. 4, </volume> <editor> p. </editor> <volume> 8. </volume>
Reference-contexts: 1 Introduction The techniques of automatic differentiation <ref> [8, 10, 15] </ref> are applied to an example partial differential equation arising from the modeling of unsaturated flow. One common paradigm for the numerical solution to some classes of 2-, 3-, or higher-dimensional partial differential equations is: 1.
Reference: [11] <author> Andreas Griewank. </author> <title> Achieving logarithmic growth of temporal and spatial complexity in reverse automatic differentiation. Optimization Methods and Software, </title> <note> to appear. Also appeared as Preprint MCS-P228-0491, </note> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Ar-gonne, Ill., </institution> <year> 1991. </year>
Reference-contexts: This is exactly the case for computing a gradient, which can be viewed as a Jacobian matrix with only one row. This issue is discussed in more detail in <ref> [8, 11, 12] </ref>.
Reference: [12] <author> Andreas Griewank, David Juedes, Jay Srinivasan, and Charles Tyner. ADOL-C, </author> <title> a package for the automatic differentiation of algorithms written in C/C++. </title> <journal> ACM Trans. Math. Software, </journal> <note> to appear. Also appeared as Preprint MCS-P180-1190, </note> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, Ill., </institution> <year> 1990. </year>
Reference-contexts: We present a case study documenting the steps we took in analyzing a code for unsaturated flow in porous media for the purpose of computing J by automatic differentiation using ADOL-C <ref> [12] </ref>, a tool for automatic differentiation using overloaded operators in C++. <p> This is exactly the case for computing a gradient, which can be viewed as a Jacobian matrix with only one row. This issue is discussed in more detail in <ref> [8, 11, 12] </ref>. <p> The nonlinear equations are contained in the C function dual (x,f). Centered differences are used to calculate a very sparse 1989 fi 1989 Jacobian J . The resulting linear equation is solved by a bi-conjugate gradient algorithm. We approached the code hoping to demonstrate the superiority of the ADOL-C <ref> [12] </ref> implementation of automatic differentiation over the centered difference approximations used in Robey's original code. The high degree of nonlinearity was felt to be a potential cause of inaccuracy using centered differences, and we hoped that automatic differentiation could improve accuracy and speed. <p> We followed the instructions in <ref> [12] </ref> first for the forward mode of automatic differentiation.
Reference: [13] <author> Andreas Griewank and Ph. L. Toint. </author> <title> On the unconstrained opimization of partially separable functions. </title> <editor> In M. J. D. Powell, editor, </editor> <booktitle> Nonlinear Optimization 1981, NATO Conference Series II: Systems Science, </booktitle> <pages> pages 301-321. </pages> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1982. </year>
Reference-contexts: Partial Separability A function f : R n ! R is called partially separable <ref> [13] </ref> if it can be expressed as a linear combination f (u) = P f i (u). Often, each f i depends on only a few of the components of u. Partially separable functions arise frequently in optimization.
Reference: [14] <author> David Juedes. </author> <title> A taxonomy of automatic differentiation tools. </title> <editor> In Andreas Griewank and George F. Corliss, editors, </editor> <title> Automatic Differentiation of Algorithms: Theory, </title> <booktitle> Implementation, and Application, </booktitle> <pages> pages 315-329. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, Penn., </address> <year> 1991. </year> <note> Also appeared as Preprint MCS-P265-0991, </note> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, Ill., </institution> <month> October </month> <year> 1991. </year>
Reference-contexts: Despite the advantages of the reverse mode from the viewpoint of complexity, the implementation for the general case is quite complicated. It requires the ability to access in reverse order the instructions performed for the computation of f and the values of their operands and results. Current tools (see <ref> [14] </ref>) achieve this by storing a record of every computation performed. An interpreter performs a backward pass on this "tape." The resulting overhead often dominates the complexity advantage of the reverse mode in an actual implementation (see [5]). <p> As a result, a variety of implementations of automatic differentiation have been developed over the years (see <ref> [14] </ref> for a survey). 4 Unsaturated Flow Problem We study a two-dimensional unsaturated flow in a porous medium. Steady state porous media flow involves an elliptic partial differential equation that contains a conductivity coeffecient [6, 16]. The coefficient is typically discontinuous across different materials and can vary greatly.
Reference: [15] <author> Louis B. Rall. </author> <title> Automatic Differentiation: Techniques and Applications, </title> <booktitle> volume 120 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1981. </year>
Reference-contexts: 1 Introduction The techniques of automatic differentiation <ref> [8, 10, 15] </ref> are applied to an example partial differential equation arising from the modeling of unsaturated flow. One common paradigm for the numerical solution to some classes of 2-, 3-, or higher-dimensional partial differential equations is: 1. <p> Assume that rt contains the deriva tives of t with respect to the independent variables x, rt = i @x (1) ; @t j T We propagate these derivatives by using elementary differentiation arithmetic based on the chain rule <ref> [8, 15] </ref> for computing the derivatives of y (1) and y (2), as shown in Figure 2. <p> We also note that even though we showed the computation only of first derivatives, the automatic differentiation approach can easily be generalized to the computation of univariate Taylor series or Hessians and multivariate higher-order derivatives <ref> [2, 9, 15] </ref>. This discussion is intended to demonstrate that the principles underlying automatic differentiation are not complicated: We just associate extra computations (which are entirely specified on a statement-by-statement basis) with the statements executed in the original code.
Reference: [16] <author> Mary Wheeler and Ruth Gonzalez. </author> <title> Mixed finite element methods for petroleum reservoir engineering problems. </title> <editor> In R. Glowinski and J.-L. Lions, editors, </editor> <booktitle> Computing Methods in Applied Sciences and Engineering, VI. </booktitle> <publisher> Elsevier, </publisher> <year> 1984. </year>
Reference-contexts: Steady state porous media flow involves an elliptic partial differential equation that contains a conductivity coeffecient <ref> [6, 16] </ref>. The coefficient is typically discontinuous across different materials and can vary greatly. For unsaturated flow, the conductivity is usually taken to be a function of pore pressure which introduces a nonlinearity to the problem.
Reference: [17] <author> Philip Wolfe. </author> <title> Checking the calculation of gradients. </title> <journal> ACM Trans. Math. Softw., </journal> <volume> 6(4) </volume> <pages> 337-343, </pages> <year> 1982. </year>
Reference-contexts: This is exactly the case for computing a gradient, which can be viewed as a Jacobian matrix with only one row. This issue is discussed in more detail in [8, 11, 12]. Wolfe observed <ref> [17] </ref>, and Baur and Strassen confirmed [1], that if care is taken in handling quantities which are common to the (rational) function and its derivatives, then the cost of evaluating a gradient with n components is a small multiple of the cost of evaluating the underlying scalar function.
References-found: 17

