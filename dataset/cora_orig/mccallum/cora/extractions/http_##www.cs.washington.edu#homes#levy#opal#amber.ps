URL: http://www.cs.washington.edu/homes/levy/opal/amber.ps
Refering-URL: http://www.cs.washington.edu/homes/levy/opal/opalpapers.html
Root-URL: 
Title: The Amber System: Parallel Programming on a Network of Multiprocessors  
Author: Jeffrey S. Chase, Franz G. Amador, Edward D. Lazowska Henry M. Levy and Richard J. Littlefield 
Address: Seattle, WA 98195  
Affiliation: Department of Computer Science University of Washington  
Abstract: This paper describes a programming system called Amber which permits a single application program to use a homogeneous network of multiprocessors in a uniform way, making the network appear to the application as an integrated, non-uniform memory access, shared-memory multiprocessor. This simplifies the development of applications and allows compute-intensive parallel programs to effectively harness the potential of multiple nodes. Amber programs are written using an object-oriented subset of the C++ programming language, supplemented with primitives for managing concurrency and distribution. Amber provides a network-wide shared-object virtual memory in which coherence is provided by hardware means for locally-executing threads, and by software means for remote accesses. Amber runs on top of the Topaz operating system on a network of DEC SRC Firefly multiprocessor work stations.
Abstract-found: 1
Intro-found: 1
Reference: [Allchin & McKendry 83] <author> J. Allchin and M. McKendry. </author> <title> Synchronization and recovery of actions. </title> <booktitle> In Proceedings of the 2nd ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 31-44, </pages> <month> August </month> <year> 1983. </year>
Reference-contexts: The abstractions provided by object classes hide not only the representation of objects but also the internal details of their execution, synchronization, and location. In contrast to many other object-oriented systems <ref> [Allchin & McKendry 83, Almes et al. 85] </ref>, the goal of Amber is to execute a single application that performs a (parallel) computation, computes a result, and terminates.
Reference: [Almes et al. 85] <author> G. T. Almes, A. P. Black, E. D. Lazowska, and J. D. Noe. </author> <title> The Eden system: A technical review. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-11(1):43-59, </volume> <month> January </month> <year> 1985. </year>
Reference-contexts: The abstractions provided by object classes hide not only the representation of objects but also the internal details of their execution, synchronization, and location. In contrast to many other object-oriented systems <ref> [Allchin & McKendry 83, Almes et al. 85] </ref>, the goal of Amber is to execute a single application that performs a (parallel) computation, computes a result, and terminates.
Reference: [Bershad et al. 88a] <author> B. N. Bershad, E. D. Lazowska, and H. M. Levy. </author> <title> PRESTO: A system for object-oriented parallel programming. </title> <journal> Software Practice and Experience, </journal> <volume> 18(8), </volume> <month> August </month> <year> 1988. </year>
Reference-contexts: It also relies on C++ rather than a special-purpose programming language, making Amber more attractive to application programmers and eliminating the need for a compiler. * The concurrency model of Amber follows that of Presto, a C++-based system for building medium-grained parallel applications on shared-memory multiprocessors <ref> [Bershad et al. 88a] </ref>. Key properties of Presto include efficient support for threads, locking, and scheduling, and an open approach in which the application programmer can easily customize the programming model [Bershad et al. 88b].
Reference: [Bershad et al. 88b] <author> B. N. Bershad, E. D. Lazowska, H. M. Levy, and D. Wagner. </author> <title> An open environment for building parallel programming systems. </title> <booktitle> In Proceedings of the ACM SIGPLAN Symposium on Parallel Programming Environments, Applications, and Languages, </booktitle> <month> July </month> <year> 1988. </year>
Reference-contexts: Key properties of Presto include efficient support for threads, locking, and scheduling, and an open approach in which the application programmer can easily customize the programming model <ref> [Bershad et al. 88b] </ref>.
Reference: [Birrell & Nelson 84] <author> A. D. Birrell and B. J. Nelson. </author> <title> Implementing remote procedure calls. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2(1) </volume> <pages> 39-59, </pages> <month> February </month> <year> 1984. </year>
Reference-contexts: This shared virtual memory system eliminates the need for programmers to explicitly manage binding and remote communication, as they would with a conventional remote procedure call system <ref> [Birrell & Nelson 84] </ref>. From the programmer's perspective data distribution in Ivy is completely transparent because memory is guaranteed to be consistent at the byte level across all the nodes. A major difference between Amber and Ivy is that Amber takes a function-shipping rather than a data-shipping approach to coherence.
Reference: [Chang & Mergen 88] <author> A. Chang and M. F. Mergen. </author> <title> 801 storage: Architecture and programming. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 28-50, </pages> <month> February </month> <year> 1988. </year> <month> 19 </month>
Reference-contexts: For now, we are limiting ourselves to relatively small networks (e.g., tens of nodes). Microprocessors with 48- or 64-bit address spaces and inverted page tables are likely to become common in the next several years, which will eliminate this concern <ref> [Chang & Mergen 88, Mahon et al. 86] </ref>. 4.1.2 Handling Non-local References Although Amber objects are referenced by virtual address, location-independence requires the system to determine whether or not an object is local when it is invoked.
Reference: [Downey 88] <author> W. J. Downey. </author> <title> Inside the GP1000. </title> <type> Technical report, </type> <institution> BBN Advanced Computers, Inc., Cambridge, Massachusetts, </institution> <month> May </month> <year> 1988. </year>
Reference-contexts: 1 Introduction Small-scale shared-memory multiprocessors are becoming widely available in implementations rang ing from single-user workstations to mini-supercomputers <ref> [Thacker et al. 88, Fielland & Rodgers 84, Downey 88] </ref>. Two factors working together are responsible for this trend. First, microprocessor performance has increased at a remarkable rate.
Reference: [Fielland & Rodgers 84] <author> G. Fielland and D. Rodgers. </author> <title> 32-bit computer system shares load equally among up to 12 processors. </title> <booktitle> Electronic Design, </booktitle> <pages> pages 153-168, </pages> <month> September </month> <year> 1984. </year>
Reference-contexts: 1 Introduction Small-scale shared-memory multiprocessors are becoming widely available in implementations rang ing from single-user workstations to mini-supercomputers <ref> [Thacker et al. 88, Fielland & Rodgers 84, Downey 88] </ref>. Two factors working together are responsible for this trend. First, microprocessor performance has increased at a remarkable rate.
Reference: [Fowler 85] <author> R. J. Fowler. </author> <title> Decentralized Object Finding Using Forwarding Addresses. </title> <type> PhD dissertation, </type> <institution> University of Washington, </institution> <month> December </month> <year> 1985. </year> <institution> Department of Computer Science technical report 85-12-1. </institution>
Reference-contexts: When the kernel handles a trap on an invocation of a remote object it retrieves a forwarding address for the object from the object's descriptor. (The use of forwarding addresses is described in <ref> [Fowler 85] </ref>.) The forwarding address may be out of date if the object moves frequently. In this case the object's location can be determined by following a chain of forwarding addresses, since the object leaves a new forwarding address on each node through which it passes.
Reference: [Gehringer et al. 87] <author> E. F. Gehringer, D. P. Siewiorek, and Z. Segall. </author> <title> Parallel Processing: The Cm fl Experience. </title> <publisher> Digital Press, </publisher> <address> Bedford, Massachusetts, </address> <year> 1987. </year>
Reference-contexts: Section 4 discusses several important differences between these two views of distributed virtual memory. * The organization of Amber programs into closely-cooperating clusters is similar to the task force structure in the Medusa [Ousterhout et al. 80] and StarOS [Jones et al. 79] operating systems for Cm fl <ref> [Gehringer et al. 87] </ref>, and to Argus guardians [Liskov 88]. However, on Amber this clustering is determined at run time and can change dynamically as a program executes.
Reference: [Jones et al. 79] <author> A. K. Jones, R. J. Chansler, I. Durham, K. Schwans, and S. R. Vegdahl. StarOS, </author> <title> a multiprocessor operating systems for the support of task forces. </title> <booktitle> In Proceedings of the 7th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 117-127, </pages> <month> December </month> <year> 1979. </year>
Reference-contexts: Section 4 discusses several important differences between these two views of distributed virtual memory. * The organization of Amber programs into closely-cooperating clusters is similar to the task force structure in the Medusa [Ousterhout et al. 80] and StarOS <ref> [Jones et al. 79] </ref> operating systems for Cm fl [Gehringer et al. 87], and to Argus guardians [Liskov 88]. However, on Amber this clustering is determined at run time and can change dynamically as a program executes.
Reference: [Jul 88] <author> E. </author> <month> Jul. </month> <title> Object Mobilty in a Distributed Object-Oriented System. </title> <type> PhD dissertation, </type> <institution> University of Washington, </institution> <month> December </month> <year> 1988. </year> <institution> Department of Computer Science, TR-88-12-06. </institution>
Reference-contexts: This is useful because some applications will need to reorganize object locations following different computational phases of a program, although static object placement (i.e., creation-time placement without further mobility) might be sufficient for many applications. 5 Amber's mobility primitives are modeled after mobility in the Emerald system <ref> [Jul 88] </ref>. All Amber objects have a number of basic mobility operations defined for them. An object can be moved with MoveTo and its location can be determined with Locate. The programmer can also Attach an object to another object or Unattach an object which is attached.
Reference: [Jul et al. 88] <author> E. Jul, H. Levy, N. Hutchinson, and A. Black. </author> <title> Fine-grained mobility in the Emerald system. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 109-133, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: While there are many dimensions to the Amber system, its essentials are easily grasped in terms of its relationships to several other systems: * The distribution model of Amber is similar to that of Emerald <ref> [Jul et al. 88] </ref>, a distributed object-oriented programming system that includes support for fine-grained object mobility. Amber differs from Emerald in its design for concurrency and virtual memory.
Reference: [Lampson 84] <author> B. W. Lampson. </author> <title> Hints for computer system design. </title> <journal> IEEE Software, </journal> <volume> 1(1) </volume> <pages> 11-28, </pages> <month> January </month> <year> 1984. </year>
Reference-contexts: We believe that the drawbacks to these facilities are outweighed by their benefits for performance-critical parallel programming. This view of what a programming environment should do is consistent with accepted systems design goals of separating policy from mechanism and not hiding power <ref> [Levin et al. 75, Lampson 84] </ref>.
Reference: [Levin et al. 75] <author> R. Levin, E. Cohen, W. Corwin, F. Pollack, and W. Wulf. </author> <title> Policy/mechanism separation in hydra. </title> <booktitle> In Proceedings of the Fifth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 132-140, </pages> <month> November </month> <year> 1975. </year>
Reference-contexts: We believe that the drawbacks to these facilities are outweighed by their benefits for performance-critical parallel programming. This view of what a programming environment should do is consistent with accepted systems design goals of separating policy from mechanism and not hiding power <ref> [Levin et al. 75, Lampson 84] </ref>.
Reference: [Li & Hudak 86] <author> K. Li and P. Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <booktitle> In Proceedings of the 5th ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 229-239, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: Presto, however, has no facilities for distribution. * The attractiveness of the architectural model a large-scale shared-memory multiprocessor built in software from a network of workstations or small-scale multiprocessors was first suggested by Kai Li, through his work on the Ivy distributed virtual memory system <ref> [Li 86, Li & Hudak 86] </ref>. Amber can be thought of as a distributed virtual memory system in which 2 the unit of coherence is the object, while Ivy is a distributed virtual memory system in which the unit of coherence is the page. <p> Ivy also pioneered the use of a network-wide virtual memory to simplify communication through the use of shared-memory semantics. Coherence of Ivy's shared memory is maintained by memory managers on each node, which use page faults to detect shared accesses and exchange coherency messages with the other memory managers <ref> [Li & Hudak 86] </ref>. This shared virtual memory system eliminates the need for programmers to explicitly manage binding and remote communication, as they would with a conventional remote procedure call system [Birrell & Nelson 84].
Reference: [Li 86] <author> K. Li. </author> <title> Shared Virtual Memory on Loosely Coupled Multiprocessors. </title> <type> PhD dissertation, </type> <institution> Yale University, </institution> <month> September </month> <year> 1986. </year> <month> YALEU/DCS/RR-492. </month>
Reference-contexts: Presto, however, has no facilities for distribution. * The attractiveness of the architectural model a large-scale shared-memory multiprocessor built in software from a network of workstations or small-scale multiprocessors was first suggested by Kai Li, through his work on the Ivy distributed virtual memory system <ref> [Li 86, Li & Hudak 86] </ref>. Amber can be thought of as a distributed virtual memory system in which 2 the unit of coherence is the object, while Ivy is a distributed virtual memory system in which the unit of coherence is the page.
Reference: [Liskov 88] <author> B. Liskov. </author> <title> Distributed programming in Argus. </title> <journal> Communications of the ACM, </journal> <volume> 31(3) </volume> <pages> 300-312, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: between these two views of distributed virtual memory. * The organization of Amber programs into closely-cooperating clusters is similar to the task force structure in the Medusa [Ousterhout et al. 80] and StarOS [Jones et al. 79] operating systems for Cm fl [Gehringer et al. 87], and to Argus guardians <ref> [Liskov 88] </ref>. However, on Amber this clustering is determined at run time and can change dynamically as a program executes.
Reference: [Mahon et al. 86] <author> M. J. Mahon, R. B.-L. Lee, T. C. Miller, J. C. Huck, and W. R. Bryg. </author> <title> Hewlett-Packard precision architecture: The processor. </title> <journal> Hewlett-Packard Journal, </journal> <volume> 37(8), </volume> <month> August </month> <year> 1986. </year>
Reference-contexts: For now, we are limiting ourselves to relatively small networks (e.g., tens of nodes). Microprocessors with 48- or 64-bit address spaces and inverted page tables are likely to become common in the next several years, which will eliminate this concern <ref> [Chang & Mergen 88, Mahon et al. 86] </ref>. 4.1.2 Handling Non-local References Although Amber objects are referenced by virtual address, location-independence requires the system to determine whether or not an object is local when it is invoked.
Reference: [Ousterhout et al. 80] <author> J. K. Ousterhout, D. A. Scelza, and P. S. Sindhu. </author> <title> Medusa: An experiment in distributed operating system structure. </title> <journal> Communications of the ACM, </journal> <volume> 23(2) </volume> <pages> 92-105, </pages> <month> February </month> <year> 1980. </year>
Reference-contexts: Section 4 discusses several important differences between these two views of distributed virtual memory. * The organization of Amber programs into closely-cooperating clusters is similar to the task force structure in the Medusa <ref> [Ousterhout et al. 80] </ref> and StarOS [Jones et al. 79] operating systems for Cm fl [Gehringer et al. 87], and to Argus guardians [Liskov 88]. However, on Amber this clustering is determined at run time and can change dynamically as a program executes.
Reference: [Stroustrup 86] <author> B. Stroustrup. </author> <title> The C++ Programming Language. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mas-sachusetts, </address> <year> 1986. </year>
Reference-contexts: Amber is based on an object-oriented model of computation in which collections of dynamic, mobile objects distributed among nodes in a network interact through location-independent invocation. Amber programs are written using an object-oriented subset of the C++ programming language <ref> [Stroustrup 86] </ref>, to which Amber adds primitives for thread management, synchronization, and object mobility. The Amber system is composed of a preprocessor to C++ and a run-time library, referred to as the Amber kernel, which is linked with the user's code.
Reference: [Thacker et al. 88] <author> C. P. Thacker, L. C. Stewart, and E. H. Satterthwaite, Jr. Firefly: </author> <title> A multiprocessor workstation. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 37(8) </volume> <pages> 909-920, </pages> <month> August </month> <year> 1988. </year> <month> 20 </month>
Reference-contexts: 1 Introduction Small-scale shared-memory multiprocessors are becoming widely available in implementations rang ing from single-user workstations to mini-supercomputers <ref> [Thacker et al. 88, Fielland & Rodgers 84, Downey 88] </ref>. Two factors working together are responsible for this trend. First, microprocessor performance has increased at a remarkable rate. <p> The Amber system is composed of a preprocessor to C++ and a run-time library, referred to as the Amber kernel, which is linked with the user's code. Amber runs under the Topaz operating system on a network of DEC SRC Firefly multiprocessor workstations <ref> [Thacker et al. 88] </ref>.
References-found: 22

