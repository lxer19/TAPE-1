URL: http://www.isi.edu/asd/defacto/publications/raw99.defacto.ps
Refering-URL: http://www.isi.edu/asd/defacto/publications/publications_index.html
Root-URL: http://www.isi.edu
Title: DEFACTO: A Design Environment for Adaptive Computing Technology  
Author: Kiran Bondalapati Pedro Diniz Phillip Duncan John Granacki Mary Hall Rajeev Jain Heidi Ziegler 
Abstract: The lack of high-level design tools hampers the widespread adoption of adaptive computing systems. Application developers have to master a wide range of functions, from the high-level architecture design, to the timing of actual control and data signals. In this paper we describe DEFACTO, an end-to-end design environment aimed at bridging the gap in tools for adaptive computing by bringing together parallelizing compiler technology and synthesis techniques. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> D. Buell, J. Arnold and W. Kleinfelder, </author> <title> "Splash 2: </title> <booktitle> FPGAs in a Custom Computing Machine," IEEE Symposium on FPGAs for Custom Computing Machines, </booktitle> <publisher> Computer Society Press, </publisher> <address> Los Alamitos CA, </address> <year> 1996. </year>
Reference-contexts: At present, developing applications on most such systems requires low-level VHDL coding, and complex management of communication and control. While a few tools for application developer are being designed, these have been narrowly focused on a single application or a specific configurable architecture <ref> [1] </ref> or require new programming languages and computing paradigms [2]. The absence of general-purpose, high-level programming tools for adaptive computing applications has hampered the widespread adoption of this technology; currently, this area is only accessible to a very small collection of specially trained individuals.
Reference: 2. <author> M. Gokhale and J. Stone, </author> <title> "NAPA C: Compiling for a Hybrid RISC/FPGA Architecture," </title> <booktitle> IEEE Symposium on FPGAs for Custom Computing Machines, </booktitle> <publisher> Computer Society Press, </publisher> <address> Los Alamitos CA, </address> <month> April, </month> <year> 1997. </year>
Reference-contexts: While a few tools for application developer are being designed, these have been narrowly focused on a single application or a specific configurable architecture [1] or require new programming languages and computing paradigms <ref> [2] </ref>. The absence of general-purpose, high-level programming tools for adaptive computing applications has hampered the widespread adoption of this technology; currently, this area is only accessible to a very small collection of specially trained individuals.
Reference: 3. <author> Hall, M et al., </author> <title> "Maximizing Multiprocessor Performance with the SUIF Compiler," </title> <publisher> IEEE Computer, IEEE Computer Society Press, </publisher> <address> Los Alamitos CA, </address> <month> Dec. </month> <year> 1996. </year>
Reference-contexts: or pipelining in the execution of the iterations of task G1. 3.1 Identifying Configurable Computing Computations The compiler identifies parallel loops, including vector-style SIMD computations, more general parallel loops that follow multiple threads of control, and pipelined parallel loops using existing array data dependence analysis, privatization and reduction recognition techniques <ref> [3] </ref>. In addition to these parallelization analyses, the DEFACTO compiler can also exploit partial evaluation, constant folding and special arithmetic formats to generate specialized versions of a given loop body.
Reference: 4. <author> Polychronopoulos, C. and Kuck D., </author> <title> "Guided-Self-Scheduling A Practical Scheduling Scheme for Parallel Computers," </title> <journal> ACM Transactions on Computers, </journal> <volume> Vol. 12, No. 36., </volume> <pages> pp. 1425-1439, </pages> <month> Dec. </month> <year> 1987. </year>
Reference-contexts: the Example Code in Figure 2 For this example, it is possible to split the data in the b, z and a arrays by columns and privatize the variable sum, so that the iterations of the outer-most loop could be assigned to different CCUs in consecutive blocks and performed concurrently <ref> [4] </ref>. Figure 3 illustrates a possible data and computation partitioning for the code example. In this figure, we have represented each configuration C associated with a given set of hardware functions F. 3.3 Generating Control for the Partition The final program implementation cannot operate correctly without a control mechanism.
Reference: 5. <author> Anderson J. and Lam, M., </author> <title> "Global Optimizations for Parallelism and Locality on Scalable Parallel Machines," </title> <booktitle> In Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI'93), </booktitle> <pages> pp. 112-125, </pages> <publisher> ACM Press, </publisher> <address> NY, </address> <month> July </month> <year> 1993. </year> <title> This article was processed using the L a T E X macro package with LLNCS style </title>
Reference-contexts: For a multi-CCU architecture, we can use the data partitioning analysis to determine which partitions of the data can be accommodated that result in minimal communication and synchronization <ref> [5] </ref>. These data partitions are subject to the additional constraint that the corresponding tasks that access the data can be fully implemented on a single CCU.
References-found: 5

