URL: ftp://dis.cs.umass.edu/pub/or-ijhcs-naghi.ps
Refering-URL: http://dis.cs.umass.edu/research/roles.html
Root-URL: 
Email: fnagendra,lesserg@cs.umass.edu  lander@bbtech.com  
Phone: 2  
Title: Learning Organizational Roles in a Heterogeneous Multi-agent System  on Evolution and Learning in Multiagent Systems.  
Author: M. V. Nagendra Prasad Victor R. Lesser and Susan E. Lander 
Note: To appear in the International Journal of Human-Computer Studies (IJHCS), special issue  
Date: August 22, 1996  
Address: Amherst, MA 01003  401 Main Street Amherst, MA 0l002  
Affiliation: 1 Department of Computer Science University of Massachusetts  Blackboard Technology Group, Inc.  
Abstract: This paper presents studies in learning a form of organizational knowledge called organizational roles in a multi-agent agent system. It attempts to demonstrate the viability and utility of self-organization in an agent-based system involving complex interactions within the agent set. We present a multi-agent parametric design system called L-TEAM where a set of heterogeneous agents learn their organizational roles in negotiated search for mutually acceptable designs. We tested the system on a steam condenser design domain and empirically demonstrated its usefulness. L-TEAM produced better results than its non-learning predecessor, TEAM, which required elaborate knowledge engineering to hand-code organizational roles for its agent set. In addition, we discuss experiments with L-TEAM that highlight the importance of certain learning issues in multi-agent systems. fl This material is based upon work supported by the National Science Foundation under Grant Nos. IRI-9523419 and EEC-9209623. The content of this paper does not necessarily reflect the position or the policy of the Government, and no official endorsement should be inferred. 
Abstract-found: 1
Intro-found: 1
Reference: <author> A. Barto, R. S. and Watkins, C. </author> <year> (1990). </year> <title> Learning and sequential decision making. </title> <editor> In Gariel, M. and Moore, J. W., editors, </editor> <booktitle> Learning and Computational Neuroscience, </booktitle> <address> Cambridge, MA. </address> <publisher> MIT Press. </publisher>
Reference: <author> Garland, A. and Alterman, R. </author> <year> (1996). </year> <title> Multi-agent learning through collective memory. </title> <booktitle> In Proceedings of the 1996 AAAI Spring Symposium on Adaptation, Co-evolution and Learning in Multiagent Systems, </booktitle> <address> Stanford, CA. </address>
Reference-contexts: Though Weiss (Weiss, 1994) studies this system in the blocks world domain, it could represent an interesting alternative to the learning mechanism we proposed in this paper for learning organizational knowledge. Nagendra Prasad, Lesser and Lander (Nagendra Prasad et al., 1996) and Garland and Alter-man <ref> (Garland and Alterman, 1996) </ref> discuss issues in knowledge reuse in multi-agent systems.
Reference: <author> Holland, J. H. </author> <year> (1985). </year> <title> Properties of bucket brigade algorithm. </title> <booktitle> In First International Conference on Genetic Algorithms and their Applications, </booktitle> <pages> pages 1-7, </pages> <address> Pittsburgh, PA. </address>
Reference-contexts: In L-TEAM, the concept of potential leads to different organizations and better quality results and is not a just a speedup device. A related work using classifier systems for learning suitable multi-agent organizations is presented in Weiss (Weiss, 1994). Multiple agents use a variant of Holland's <ref> (Holland, 1985) </ref> bucket brigade algorithm to learn appropriate instantiations of hierarchical organizations. Though Weiss (Weiss, 1994) studies this system in the blocks world domain, it could represent an interesting alternative to the learning mechanism we proposed in this paper for learning organizational knowledge.
Reference: <author> Lander, S. E. </author> <year> (1994). </year> <title> Distributed Search in Heterogeneous and Reusable Multi-Agent Systems. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Sceince, University of Massachusetts, Amherst. </institution>
Reference-contexts: 1 Introduction Requirements like reusability of legacy systems and heterogeneity of agent representations lead to a number of challenging issues in Multi-agent Systems (MAS). Lander and Lesser <ref> (Lander and Lesser, 1994) </ref> developed the TEAM framework to examine some of these issues in heterogeneous reusable agents in the context of parametric design. TEAM is an open system assembled through minimally customized integration of a dynamically selected subset of a catalogue of existing agents. <p> The evolution of a composite solution in TEAM can be viewed as a series of state transitions as shown in Figure 1 (from <ref> (Lander, 1994) </ref>). For a composite solution in a given state, an agent can play one of a set of organizational roles (in TEAM, these roles are solution-initiator, solution-extender, or solution-critic). An organizational role represents a set of tasks an agent can perform on a composite solution. <p> This decision is complicated by the fact that an agent has to achieve this choice within its local view of the problem-solving situation <ref> (Lander, 1994) </ref>. The objective of this paper is to investigate the utility of machine learning techniques as an aid to such a decision process in situations where the set of agents involved in problem solving are not necessarily known to the designer of any single agent. <p> the commu nicated violated local constraints, which can be used to determine the potential of a sequence of roles ending in a conflict. 4 Experimental Results To demonstrate the effectiveness of the mechanisms in L-TEAM and compare them to those in TEAM, we used the same domain as in Lander <ref> (Lander, 1994) </ref> parametric design of steam condensers. The prototype multi-agent system for this domain, built on top of the TEAM framework, consists of seven agents: pump-agent, heat-exchanger-agent, motor-agent, vbelt-agent, shaft-agent, platform-agent, and frequency-critic. The problem solving process starts by placing a problem specification on a central blackboard (BB).
Reference: <author> Lander, S. E. and Lesser, V. R. </author> <year> (1994). </year> <title> Sharing meta-information to guide cooperative search among heterogeneous reusable agents. </title> <institution> Computer Science Technical Report 94-48, University of Massachusetts. </institution> <note> To appear in IEEE Transactions on Knowledge and Data Engineering, </note> <year> 1996. </year>
Reference-contexts: 1 Introduction Requirements like reusability of legacy systems and heterogeneity of agent representations lead to a number of challenging issues in Multi-agent Systems (MAS). Lander and Lesser <ref> (Lander and Lesser, 1994) </ref> developed the TEAM framework to examine some of these issues in heterogeneous reusable agents in the context of parametric design. TEAM is an open system assembled through minimally customized integration of a dynamically selected subset of a catalogue of existing agents. <p> The evolution of a composite solution in TEAM can be viewed as a series of state transitions as shown in Figure 1 (from <ref> (Lander, 1994) </ref>). For a composite solution in a given state, an agent can play one of a set of organizational roles (in TEAM, these roles are solution-initiator, solution-extender, or solution-critic). An organizational role represents a set of tasks an agent can perform on a composite solution. <p> This decision is complicated by the fact that an agent has to achieve this choice within its local view of the problem-solving situation <ref> (Lander, 1994) </ref>. The objective of this paper is to investigate the utility of machine learning techniques as an aid to such a decision process in situations where the set of agents involved in problem solving are not necessarily known to the designer of any single agent. <p> the commu nicated violated local constraints, which can be used to determine the potential of a sequence of roles ending in a conflict. 4 Experimental Results To demonstrate the effectiveness of the mechanisms in L-TEAM and compare them to those in TEAM, we used the same domain as in Lander <ref> (Lander, 1994) </ref> parametric design of steam condensers. The prototype multi-agent system for this domain, built on top of the TEAM framework, consists of seven agents: pump-agent, heat-exchanger-agent, motor-agent, vbelt-agent, shaft-agent, platform-agent, and frequency-critic. The problem solving process starts by placing a problem specification on a central blackboard (BB).
Reference: <author> Lesser, V. R. and Erman, L. D. </author> <year> (1980). </year> <title> Distributed interpretation: A model and an experiment. </title> <journal> IEEE Transactions on Computers, C-29(12):1144-1163. </journal>
Reference-contexts: On a few occasions, situation-specific-L-TEAM performed worse than non-situation-specific-L-TEAM. We attribute this observation to the phenomenon of distraction frequently observed in multi-agent systems <ref> (Lesser and Erman, 1980) </ref>. In the context of role assignments, this phenomenon maps to the ability of the agents to judge whether it is effective to work on its own designs or respond to the designs generated by the other members of the agent set in the present situation.
Reference: <author> Mataric, M. J. </author> <year> (1994). </year> <title> Reward functions for accelerated learning. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <address> San Francisco, CA. </address>
Reference-contexts: We believe that importance of concepts like potential become more apparent in such domains. Mataric <ref> (Mataric, 1994) </ref> discusses the concept of progress estimators akin to the idea of potential. Potential differs from progress estimators in that the later was primarily used as a method of speeding up reinforcement learning whereas the former plays a more complex role.
Reference: <author> Nagendra Prasad, M. V., Lesser, V. R., and Lander, S. E. </author> <year> (1996). </year> <title> Retrieval and reasoning in distributed case bases. Journal of Visual Communication and Image Representation, </title> <journal> Special Issue on Digital Libraries, </journal> <volume> 7(1) </volume> <pages> 74-87. </pages>
Reference-contexts: Though Weiss (Weiss, 1994) studies this system in the blocks world domain, it could represent an interesting alternative to the learning mechanism we proposed in this paper for learning organizational knowledge. Nagendra Prasad, Lesser and Lander <ref> (Nagendra Prasad et al., 1996) </ref> and Garland and Alter-man (Garland and Alterman, 1996) discuss issues in knowledge reuse in multi-agent systems.
Reference: <author> Sandholm, T. and Crites, R. </author> <year> (1995). </year> <title> Multi-agent reinforcement learning in the repeated prisoner's dilemma. </title> <note> to appear in Biosystems. </note>
Reference-contexts: Identifying such actions and rewarding the learning system for them can lead to an enhanced performance. 5 Related Work Previous work related to learning in multi-agent systems is limited. Tan (Tan, 1993), Sandholm and Nagendra Prasad (Sandholm and NagendraPrasad, 1993), Sandholm and Crites <ref> (Sandholm and Crites, 1995) </ref>, and Sen and Sekaran (Sen et al., 1994) discuss multi-agent reinforcement learning systems. All these systems rely on reinforcement learning methods (A. Barto and Watkins, 1990; Sutton, 1988).
Reference: <author> Sandholm, T. and NagendraPrasad, M. V. </author> <year> (1993). </year> <title> Muscle: Multi-agent system for coordinated learning experiments. </title> <note> Unpublished working paper. 16 Sen, </note> <author> S., Sekaran, M., and Hale, J. </author> <year> (1994). </year> <title> Learning to coordinate without sharing information. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pages 426-431, </pages> <address> Seattle, WA. </address> <publisher> AAAI. </publisher>
Reference-contexts: Identifying such actions and rewarding the learning system for them can lead to an enhanced performance. 5 Related Work Previous work related to learning in multi-agent systems is limited. Tan (Tan, 1993), Sandholm and Nagendra Prasad <ref> (Sandholm and NagendraPrasad, 1993) </ref>, Sandholm and Crites (Sandholm and Crites, 1995), and Sen and Sekaran (Sen et al., 1994) discuss multi-agent reinforcement learning systems. All these systems rely on reinforcement learning methods (A. Barto and Watkins, 1990; Sutton, 1988).
Reference: <author> Sian, S. S. </author> <year> (1991). </year> <title> Extending learning to multiple agents: issues and a model for multi-agent machine learning. </title> <booktitle> In Proceedings of Machine Learning - EWSL 91, </booktitle> <pages> pages 440-456, </pages> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Certain multi-agent learning systems in the literature deal with a different task from that presented in this paper. Systems like ILS (Silver et al., 1990) and MALE <ref> (Sian, 1991) </ref> use multi-agent techniques to build hybrid learners from multiple learning agents. On the other hand, L-TEAM learns problem-solving control for multi-agent systems. 6 Implications and Conclusion Previous work in self-organization for efficient distributed search control has, for the most part, involved simple agents with simple interaction patterns.
Reference: <author> Silver, B., Frawely, W., Iba, G., Vittal, J., and Bradford, K. </author> <year> (1990). </year> <title> A framework for multi-paradigmatic learning. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <pages> pages 348-358. </pages>
Reference-contexts: Certain multi-agent learning systems in the literature deal with a different task from that presented in this paper. Systems like ILS <ref> (Silver et al., 1990) </ref> and MALE (Sian, 1991) use multi-agent techniques to build hybrid learners from multiple learning agents.
Reference: <author> Sugawara, T. and Lesser, V. R. </author> <year> (1993). </year> <title> On-line learning of coordination plans. </title> <booktitle> In Proceedings of the Twelfth International Workshop on Distributed AI, </booktitle> <address> Hidden Valley, Pa. </address>
Reference-contexts: Nagendra Prasad, Lesser and Lander (Nagendra Prasad et al., 1996) and Garland and Alter-man (Garland and Alterman, 1996) discuss issues in knowledge reuse in multi-agent systems. Sugawra and Lesser <ref> (Sugawara and Lesser, 1993) </ref> discuss a distributed networking system where each local segment of the network has an intelligent diagnosis agent called LODES that monitors traffic on the network and uses an explanation-based learning technique to develop coordination rules for the LODES agents.
Reference: <author> Sutton, R. </author> <year> (1988). </year> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44. </pages>
Reference-contexts: Once the learning is done, an agent chooses the role with maximum rating in a given situation. This implies that after the learning phase, each agent organizes itself to play a fixed role in a given situation 1 . We use the supervised-learning approach to prediction learning (see <ref> (Sutton, 1988) </ref>) to learn estimates for the UPC vectors for each of the situations. The agents collectively explore the space of possible role assignments to identify good role assignments in each of the situations. <p> in a state n that can be classified as situation j will lead to a final state, accumulated after p problem solving instances. 2 Note that the supervised learning approach to prediction learning is different from reinforcement learning which assigns credit by means of the differences between temporally successive predictions <ref> (Sutton, 1988) </ref>. In this paper we are primarily concerned with showing the benefits and characteristics of learning in multi-agent systems rather than with the merits of a particular learning method over others.
Reference: <author> Tan, M. </author> <year> (1993). </year> <title> Multi-agent reinforcement learning: Independent vs. </title> <booktitle> cooperative agents. In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pages 330-337. </pages>
Reference-contexts: Identifying such actions and rewarding the learning system for them can lead to an enhanced performance. 5 Related Work Previous work related to learning in multi-agent systems is limited. Tan <ref> (Tan, 1993) </ref>, Sandholm and Nagendra Prasad (Sandholm and NagendraPrasad, 1993), Sandholm and Crites (Sandholm and Crites, 1995), and Sen and Sekaran (Sen et al., 1994) discuss multi-agent reinforcement learning systems. All these systems rely on reinforcement learning methods (A. Barto and Watkins, 1990; Sutton, 1988).
Reference: <author> Weiss, G. </author> <year> (1994). </year> <title> Some studies in distributed machine learning and organizational design. </title> <type> Technical Report FKI-189-94, </type> <institution> Institut fur Informatik, TU Munchen. </institution>
Reference-contexts: In L-TEAM, the concept of potential leads to different organizations and better quality results and is not a just a speedup device. A related work using classifier systems for learning suitable multi-agent organizations is presented in Weiss <ref> (Weiss, 1994) </ref>. Multiple agents use a variant of Holland's (Holland, 1985) bucket brigade algorithm to learn appropriate instantiations of hierarchical organizations. Though Weiss (Weiss, 1994) studies this system in the blocks world domain, it could represent an interesting alternative to the learning mechanism we proposed in this paper for learning organizational <p> A related work using classifier systems for learning suitable multi-agent organizations is presented in Weiss <ref> (Weiss, 1994) </ref>. Multiple agents use a variant of Holland's (Holland, 1985) bucket brigade algorithm to learn appropriate instantiations of hierarchical organizations. Though Weiss (Weiss, 1994) studies this system in the blocks world domain, it could represent an interesting alternative to the learning mechanism we proposed in this paper for learning organizational knowledge.
Reference: <author> Whitehair, R. and Lesser, V. R. </author> <year> (1993). </year> <title> A framework for the analysis of sophisticated control in interpretation systems. </title> <institution> Computer Science Technical Report 93-53, University of Mas-sachusetts. </institution> <month> 17 </month>
Reference-contexts: In our case, situation-specific organizational roles led to better performance, especially in harder problems. The rest of the paper is organized as follows. Section 2 discusses the characteristics of a distributed search space and Section 3 presents our use of the UPC formalism <ref> (Whitehair and Lesser, 1993) </ref> as a basis for learning organizational knowledge. The following section discusses an implementation of L-TEAM based on this algorithm and presents the results of our empirical explorations. <p> The formal basis for learning role assignments is derived from the UPC formalism for search control (see Whitehair and Lesser <ref> (Whitehair and Lesser, 1993) </ref>) that relies on the calculation and use of the Utility, Probability and Cost (UPC) values associated with each 4 hstate; R; f inal statei tuple.
References-found: 17

