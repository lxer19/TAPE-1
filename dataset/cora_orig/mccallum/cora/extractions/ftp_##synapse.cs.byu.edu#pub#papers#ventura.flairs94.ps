URL: ftp://synapse.cs.byu.edu/pub/papers/ventura.flairs94.ps
Refering-URL: ftp://synapse.cs.byu.edu/pub/papers/details.html
Root-URL: 
Email: e-mail: dan@axon.cs.byu.edu, martinez@cs.byu.edu  
Title: BRACE: A Paradigm For the Discretization of Continuously Valued Data  
Author: Dan Ventura Tony R. Martinez 
Address: Provo, Utah 84602  
Affiliation: Computer Science Department, Brigham Young University,  
Date: 117-21, 1994  
Note: Proceedings of the Seventh Florida Artificial Intelligence Research Symposium, pp.  
Abstract: Discretization of continuously valued data is a useful and necessary tool because many learning paradigms assume nominal data. A list of objectives for efficient and effective discretization is presented. A paradigm called BRACE (Boundary Ranking And Classification Evaluation) that attempts to meet the objectives is presented along with an algorithm that follows the paradigm. The paradigm meets many of the objectives, with potential for extension to meet the remainder. Empirical results have been promising. For these reasons BRACE has potential as an effective and efficient method for discretization of continuously valued data. A further advantage of BRACE is that it is general enough to be extended to other types of clustering/unsupervised learning. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Catlett, J., </author> <title> "On Changing Continuous Attributes Into Ordered Discrete Attributes", </title> <booktitle> Lecture Notes in Artificial Intelligence , Ed. </booktitle> <editor> J. Siekmann, </editor> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1991, </year> <pages> pp. 164-78. </pages>
Reference-contexts: Discretizing continuously valued data produces inherent generalization by grouping data into several ranges, representing it in a more general way. It has been shown that for some learning algorithms, efficient discretization as preprocessing resulted in significant speedup <ref> [1] </ref>. Further, discretization has some psychological plausibility since in many cases humans apparently perform a similar preprocessing step ___________________ representing temperature, weather, speed, etc., as nominal values.
Reference: [2] <author> Fisher, Douglas H., </author> <title> "Knowledge Acquisition Via Incremental Conceptual Clustering", Readings in Machine Learning , eds. </title> <editor> Jude W. Shavlik and Thomas G. Dietterich, </editor> <publisher> Morgan Kaufman Publishers, Inc., </publisher> <address> San Mateo, California, </address> <year> 1990, </year> <pages> pp. 267-283. </pages>
Reference-contexts: 1 INTRODUCTION Many machine learning and neurally inspired algorithms are limited, at least in their pure form, to working with nominal data. Examples include ID3 [8], AQ * [6], COBWEB <ref> [2] </ref>, and ASOCS [5]. For many real-world problems, some provision must be made to support processing of continuously valued data. The main effort of this research is to develop an effective and efficient method for discretization of continuously valued data as a preprocessor for supervised learning systems.
Reference: [3] <author> Jarvis, R. A. and Patrick, Edward A., </author> <title> "Clustering Using a Similarity Measure Based On Shared Nearest Neighbors", Nearest Neighbor Norms: NN Pattern Classification T e ch ni qu e s, </title> <editor> Ed. Belur Dasarathy, </editor> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> California, </address> <year> 1991, </year> <pages> pp. 388-97. </pages>
Reference: [4] <author> Kerber, Randy, "ChiMerge: </author> <title> Discretization of Numeric Attributes", </title> <booktitle> Proceedings of the 10th National Conference on Artificial Intelligence, </booktitle> <year> 1992, </year> <pages> pp. 123-7. </pages>
Reference-contexts: Several methods specifically for discretization also exist including the equal-width-intervals method, the equal-frequency-intervals method, and ChiMerge <ref> [4] </ref>. These generally outperform the methods mentioned above since they are designed specifically for the task of discretization. However, these too have inherent weaknesses that limit their usefulness. For example, the two former methods are very simple and arbitrary.
Reference: [5] <author> Martinez, Tony R., </author> <title> "Adaptive Self-Organizing Concurrent Systems", </title> <booktitle> Progress in Neural Networks, </booktitle> <volume> vol. 1, ch. 5, </volume> <editor> ed. O. Omidvar, </editor> <publisher> Ablex Publishing, </publisher> <year> 1990, </year> <pages> pp. 105-26. </pages>
Reference-contexts: 1 INTRODUCTION Many machine learning and neurally inspired algorithms are limited, at least in their pure form, to working with nominal data. Examples include ID3 [8], AQ * [6], COBWEB [2], and ASOCS <ref> [5] </ref>. For many real-world problems, some provision must be made to support processing of continuously valued data. The main effort of this research is to develop an effective and efficient method for discretization of continuously valued data as a preprocessor for supervised learning systems.
Reference: [6] <author> Michalski, Ryszard S., </author> <title> "A Theory and Methodology of Inductive Learning", Readings in Machine Learning , eds. </title> <editor> Jude W. Shavlik and Thomas G. Dietterich, </editor> <publisher> Morgan Kaufman Publishers, Inc., </publisher> <address> San Mateo, California, </address> <year> 1990, </year> <pages> pp. 70-95. </pages>
Reference-contexts: 1 INTRODUCTION Many machine learning and neurally inspired algorithms are limited, at least in their pure form, to working with nominal data. Examples include ID3 [8], AQ * <ref> [6] </ref>, COBWEB [2], and ASOCS [5]. For many real-world problems, some provision must be made to support processing of continuously valued data. The main effort of this research is to develop an effective and efficient method for discretization of continuously valued data as a preprocessor for supervised learning systems.
Reference: [7] <author> Murphy, P. M. and Aha, D. W., </author> <title> U C I Repository of machine learning databases, </title> <address> Irvine, CA: </address> <institution> University of California, Department of Information and Computer Science, </institution> <year> 1992. </year>
Reference-contexts: They are not limited to the binary case. 5 EMPIRICAL RESULTS VALLEY was tested on various data sets from the UC Irvine machine learning database <ref> [7] </ref>. One example is the hepatitis data set, which has a number of continuously valued variables with widely varying distributions.
Reference: [8] <author> Quinlan, J. R., </author> <title> "Induction of Decision Trees", Readings in Machine Learning , eds. </title> <editor> Jude W. Shavlik and Thomas G. Dietterich, </editor> <publisher> Morgan Kaufman Publishers, Inc., </publisher> <address> San Mateo, California, </address> <year> 1990, </year> <pages> pp. 57-69. </pages>
Reference-contexts: 1 INTRODUCTION Many machine learning and neurally inspired algorithms are limited, at least in their pure form, to working with nominal data. Examples include ID3 <ref> [8] </ref>, AQ * [6], COBWEB [2], and ASOCS [5]. For many real-world problems, some provision must be made to support processing of continuously valued data.
Reference: [9] <author> Tou, J. T. and Gonzalez, R. C., </author> <title> P a t t e r n Recognition Principles , Addison-Wesley Publishing Company, </title> <address> Reading, Massachusetts, </address> <year> 1974. </year>
References-found: 9

