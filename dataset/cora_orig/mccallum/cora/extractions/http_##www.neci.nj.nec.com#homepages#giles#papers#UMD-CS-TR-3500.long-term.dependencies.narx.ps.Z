URL: http://www.neci.nj.nec.com/homepages/giles/papers/UMD-CS-TR-3500.long-term.dependencies.narx.ps.Z
Refering-URL: http://www.neci.nj.nec.com/homepages/giles/papers/
Root-URL: 
Phone: 2  3  4  
Title: Learning long-term dependencies is not as difficult with NARX recurrent neural networks  
Author: Tsungnan Lin ; Bill G. Horne Peter Tino ; and C. Lee Giles ; 
Address: 4 Independence Way, Princeton, NJ 08540  Princeton, NJ 08540  Ilkovicova 3, 812 19 Bratislava, Slovakia  College Park, MD 20742  
Affiliation: 1 NEC Research Institute,  Department of Electrical Engineering, Princeton University,  Dept. of Computer Science and Engineering, Slovak Technical University,  UMIACS, University of Maryland,  
Abstract: It has recently been shown that gradient descent learning algorithms for recurrent neural networks can perform poorly on tasks that involve long-term dependencies, i.e. those problems for which the desired output depends on inputs presented at times far in the past. In this paper we explore the long-term dependencies problem for a class of architectures called NARX recurrent neural networks, which have powerful representational capabilities. We have previously reported that gradient descent learning is more effective in NARX networks than in recurrent neural network architectures that have "hidden states" on problems including grammatical inference and nonlinear system identification. Typically, the network converges much faster and generalizes better than other networks. The results in this paper are an attempt to explain this phenomenon. We present some experimental results which show that NARX networks can often retain information for two to three times as long as conventional recurrent neural networks. We show that although NARX networks do not circumvent the problem of long-term dependencies, they can greatly improve performance on long-term dependency problems . We also describe in detail some of the assumption regarding what it means to latch infor mation robustly and suggest possible ways to loosen these assumptions.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A.D. Back and A.C. Tsoi. </author> <title> FIR and IIR synapses, a new neural network architecture for time series modeling. </title> <journal> Neural Computation, </journal> <volume> 3(3) </volume> <pages> 375-385, </pages> <year> 1991. </year>
Reference-contexts: We also presented two experimental problems which show that NARX networks can outperform networks with single delays on some simple problems involving long-term dependencies. We speculate that similar results could be obtained for other networks. In particular we hypothesize that any network that uses tapped delay feedback <ref> [1, 14] </ref> would demonstrate improved performance on problems involving long-term dependencies. It may also be possible to obtain similar results for the architectures proposed in [6, 9, 20, 34]. We have also described in detail some of the assumptions presented in [2] regarding what it means to latch information robustly.
Reference: [2] <author> Y. Bengio, P. Simard, and P. Frasconi. </author> <title> Learning long-term dependencies with gradient is difficult. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 5(2) </volume> <pages> 157-166, </pages> <year> 1994. </year> <month> 20 </month>
Reference-contexts: This was noted by Mozer who reported that RNNs were able to learn short term musical structure using gradient based methods [17], but had difficulty capturing global behavior. These ideas were recently formalized by Bengio et al. <ref> [2] </ref>, who showed that if a system is to robustly latch information, then the fraction of the gradient due to information n time steps in the past approaches zero as n becomes large. Several approaches have been suggested to circumvent the problem of vanishing gradients. <p> Several approaches have been suggested to circumvent the problem of vanishing gradients. For example, gradient-based methods can be abandoned completely in favor of alternative optimization methods <ref> [2, 21] </ref>. However, the algorithms investigated so far either perform just as poorly on problems involving long-term dependencies, or, when they are better, require far more computational resources [2]. <p> For example, gradient-based methods can be abandoned completely in favor of alternative optimization methods [2, 21]. However, the algorithms investigated so far either perform just as poorly on problems involving long-term dependencies, or, when they are better, require far more computational resources <ref> [2] </ref>. Another possibility is to modify conventional gradient descent by more heavily weighing the fraction of the gradient due to information far in the past, but there is no guarantee that such a modified algorithm would converge to a minima of the error surface being searched [2]. <p> far more computational resources <ref> [2] </ref>. Another possibility is to modify conventional gradient descent by more heavily weighing the fraction of the gradient due to information far in the past, but there is no guarantee that such a modified algorithm would converge to a minima of the error surface being searched [2]. As an alternative to using different learning algorithms, one suggestion has been to alter the input data so that it represents a reduced description that makes global features more explicit and more readily detectable [17, 26, 25]. <p> Typically, these networks converge much faster and generalize better than other networks. The results in this paper give an explanation of this phenomenon. 2 Vanishing gradients and long-term dependencies Bengio et al. <ref> [2] </ref> have analytically explained why learning problems with long-term dependencies is difficult. They argue that for many practical applications the goal of the network must be to robustly latch information, i.e. the network must be able to store information for a long period of time in the presence of noise. <p> In Section 6, we discuss this definition of robustness in more detail and describe how some of the assumptions associated with it might be loosened. In this section we briefly describe some of the key aspects of the results in <ref> [2] </ref>. A recurrent neural network can be written in the form x (t + 1) = f (x (t); u (t); w) (1) where x, u, y and w are column vectors representing the state, input, output and weights of the network respectively. <p> Bengio et al. <ref> [2] </ref> showed that if the network satisfies their definition of robustly latching information, i.e. if the Jacobian at each time step has all of its eigenvalues inside the unit circle, then J x (T; n) is an exponentially decreasing function of n, so that lim n!1 J x (T; n) = <p> It is possible to derive analytical results for some simple toy problems to show that NARX networks are indeed less sensitive to long-term dependencies. Here we give one such example, which is based upon the latching problem described in <ref> [2] </ref>. Consider the simple one node autonomous recurrent network described by, x (t) = tanh (wx (t 1)) where w = 1:25, which has two stable fixed points at 0:710 and one unstable fixed point at zero. <p> We tried two different problems: the latching problem and the parity problem. 5.1 The latching problem We explored a slight modification on the latching problem described in <ref> [2] </ref>. This problem is a minimal task designed as a test that must necessarily be passed in order for a network to robustly latch information. Bengio et al. describe the task as one in which the input values are to be learned. <p> had 11, 8, and 6 hidden nodes respectively, giving 56, 57, and 55 weights. dependencies decreases as the number of output delays increases. 6 A closer look at robust information latching In this section we make a critical examination of the definition of robust latching given by Bengio et al. <ref> [2] </ref>. Specifically, they assume that if a network is to be robust to noise, then the states must always be in the reduced attracting set of the hyperbolic attractor. While such a condition is sufficient to robustly latch information, it is not necessary. <p> It may also be possible to obtain similar results for the architectures proposed in [6, 9, 20, 34]. We have also described in detail some of the assumptions presented in <ref> [2] </ref> regarding what it means to latch information robustly. Based on shadowing lemma from dynamical systems' theory we have shown that information can potentially be robustly latched to an attractor X in every state x of the basin of attraction fi (X) of X.
Reference: [3] <author> S. Chen, S.A. Billings, and P.M. Grant. </author> <title> Non-linear system identification using neural networks. </title> <journal> International Journal of Control, </journal> <volume> 51(6) </volume> <pages> 1191-1214, </pages> <year> 1990. </year>
Reference-contexts: In this paper, we also propose an architectural approach to deal with long-term dependencies. We focus on a class of architectures based upon Nonlinear AutoRegressive models with eXogenous inputs (NARX models), and are therefore called NARX recurrent neural networks <ref> [3, 18] </ref>. This is a powerful class of models which has recently been shown to be computationally equivalent to Turing machines [28]. <p> This is a powerful class of models which has recently been shown to be computationally equivalent to Turing machines [28]. It has been demonstrated that they are well suited for modeling nonlinear systems such as heat exchangers <ref> [3] </ref>, waste water treatment plants [31, 32], catalytic reforming systems in a petroleum refinery [32], nonlinear oscillations associated with multi-legged locomotion in biological systems [33], time series [4], and various artificial nonlinear systems [3, 18, 22]. <p> It has been demonstrated that they are well suited for modeling nonlinear systems such as heat exchangers [3], waste water treatment plants [31, 32], catalytic reforming systems in a petroleum refinery [32], nonlinear oscillations associated with multi-legged locomotion in biological systems [33], time series [4], and various artificial nonlinear systems <ref> [3, 18, 22] </ref>. Furthermore, we 2 have previously reported that gradient descent learning is more effective in NARX networks than in recurrent neural network architectures with "hidden states" when applied to problems including grammatical inference and nonlinear system identification [11, 13]. <p> descent methods are not sufficiently powerful to discover a relationship between target outputs and inputs that occur at a much earlier time, which they term the problem of long-term dependencies. 3 NARX networks An important class of discrete-time nonlinear systems is the Nonlinear AutoRegressive with eXoge nous inputs (NARX) model <ref> [3, 15, 16, 31, 32] </ref>: y (t) = f u (t D u ); : : : ; u (t 1); u (t); y (t D y ); : : : ; y (t 1) ; (3) where u (t) and y (t) represent input and output of the network at <p> When the function f can be approximated by a Multilayer Perceptron, the resulting system is called a NARX recurrent neural network <ref> [3, 18] </ref>. 5 In this paper we shall consider NARX networks with zero input order and a one dimensional output, i.e. those networks which have feedback from the output only. However there is no reason why our results could not be extended to networks with higher input orders.
Reference: [4] <author> J. Connor, L.E. Atlas, and D.R. Martin. </author> <title> Recurrent networks and NARMA modeling. </title> <editor> In J.E. Moody, S.J. Hanson, and R.P. Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 301-308, </pages> <year> 1992. </year>
Reference-contexts: It has been demonstrated that they are well suited for modeling nonlinear systems such as heat exchangers [3], waste water treatment plants [31, 32], catalytic reforming systems in a petroleum refinery [32], nonlinear oscillations associated with multi-legged locomotion in biological systems [33], time series <ref> [4] </ref>, and various artificial nonlinear systems [3, 18, 22]. Furthermore, we 2 have previously reported that gradient descent learning is more effective in NARX networks than in recurrent neural network architectures with "hidden states" when applied to problems including grammatical inference and nonlinear system identification [11, 13].
Reference: [5] <author> E. Coven, I. Kan, and J.Yorke. </author> <title> Pseudo-orbit shadowing in the family of tent maps. </title> <journal> Transactions AMS, </journal> <volume> 308 </volume> <pages> 227-241, </pages> <year> 1988. </year>
Reference-contexts: A useful formalization of this idea in dynamical systems' theory is stated in terms of the shadowing lemma <ref> [5, 10] </ref>. Given a number b &gt; 0, a b-pseudo-orbit of the system S A is a sequence f ~ x (t)g such that kM ( ~ x (t)) ~ x (t + 1)k &lt; b, for all t 0.
Reference: [6] <author> B. de Vries and J.C. Principe. </author> <title> The gamma model | A new neural model for temporal processing. </title> <booktitle> Neural Networks, </booktitle> <volume> 5 </volume> <pages> 565-576, </pages> <year> 1992. </year>
Reference-contexts: We speculate that similar results could be obtained for other networks. In particular we hypothesize that any network that uses tapped delay feedback [1, 14] would demonstrate improved performance on problems involving long-term dependencies. It may also be possible to obtain similar results for the architectures proposed in <ref> [6, 9, 20, 34] </ref>. We have also described in detail some of the assumptions presented in [2] regarding what it means to latch information robustly.
Reference: [7] <author> J.L. Elman. </author> <title> Finding structure in time. </title> <journal> Cognitive Science, </journal> <volume> 14 </volume> <pages> 179-211, </pages> <year> 1990. </year>
Reference-contexts: By increasing the length of the strings to be learned, we will be able to create a problem with long term dependencies, in which the output will depend on input values far in the past. In this experiment we compared Elman's Simple Recurrent Network <ref> [7] </ref> against NARX networks. Each network had six hidden nodes. Since the output if each hidden node in an Elman network is fed back, there were six delay elements (states) in the network. The NARX network had six feedback delays from the output node.
Reference: [8] <author> P. Frasconi, M. Gori, M. Maggini, and G. </author> <title> Soda. Unified integration of explicit rules and learning by example in recurrent networks. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 7(2) </volume> <pages> 340-346, </pages> <year> 1995. </year>
Reference-contexts: In our simulation, we fixed the recurrent feedback weight to w = 1:25, which gives the autonomous network two stable fixed points at 0:710 and one unstable fixed point at zero, as described in Section 4. It can be shown <ref> [8] </ref> that the network is robust to perturbations in the range [0:155; 0:155]. 12 strings, for different number of output delays (D = 1, D = 3 and D = 6). Thus, the uniform noise in e (t) was restricted to this range.
Reference: [9] <author> P. Frasconi, M. Gori, and G. </author> <title> Soda. Local feedback multilayered networks. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 120-130, </pages> <year> 1992. </year>
Reference-contexts: This effect is called the problem of vanishing gradient, or forgetting behavior <ref> [9] </ref>. <p> We speculate that similar results could be obtained for other networks. In particular we hypothesize that any network that uses tapped delay feedback [1, 14] would demonstrate improved performance on problems involving long-term dependencies. It may also be possible to obtain similar results for the architectures proposed in <ref> [6, 9, 20, 34] </ref>. We have also described in detail some of the assumptions presented in [2] regarding what it means to latch information robustly.
Reference: [10] <author> M. Garzon and F. Botelho. </author> <title> Observability of neural network behavior. </title> <editor> In J.D. Cowen, G. Tesauro, and J. Alspector, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> pages 455-462. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: A useful formalization of this idea in dynamical systems' theory is stated in terms of the shadowing lemma <ref> [5, 10] </ref>. Given a number b &gt; 0, a b-pseudo-orbit of the system S A is a sequence f ~ x (t)g such that kM ( ~ x (t)) ~ x (t + 1)k &lt; b, for all t 0. <p> It is proved in <ref> [10] </ref> that except possibly for small exceptional sets, discrete-time analog neural 18 networks do have the shadowing property. In particular, they show that that the shadowing property holds for networks with sigmoidal (i.e. strictly increasing, bounded form above and below, and continuously differentiable) activation functions.
Reference: [11] <author> C.L. Giles and B.G. </author> <title> Horne. Representation and learning in recurrent neural network architectures. </title> <booktitle> In Proceedings of the Eigth Yale Workshop on Adaptive and Learning Systems, </booktitle> <pages> pages 128-134, </pages> <year> 1994. </year>
Reference-contexts: Furthermore, we 2 have previously reported that gradient descent learning is more effective in NARX networks than in recurrent neural network architectures with "hidden states" when applied to problems including grammatical inference and nonlinear system identification <ref> [11, 13] </ref>. Typically, these networks converge much faster and generalize better than other networks. The results in this paper give an explanation of this phenomenon. 2 Vanishing gradients and long-term dependencies Bengio et al. [2] have analytically explained why learning problems with long-term dependencies is difficult. <p> This has been observed previously, in the sense that gradient descent learning appeared to be more effective in NARX networks than in recurrent neural network architectures that have "hidden states" on problems including grammatical inference and nonlinear system identification <ref> [11, 13] </ref>. The intuitive explanation for this behavior is that the output delays are manifested as jump-ahead connections in the unfolded network that is often used to describe algorithms like Backprop 19 agation Through Time.
Reference: [12] <author> M. Gori, M. Maggini, and G. </author> <title> Soda. Scheduling of modular architectures for inductive inference of regular grammars. </title> <booktitle> In ECAI'94 Workshop on Combining Symbolic and Connectionist Processing, Amsterdam, </booktitle> <pages> pages 78-87. </pages> <publisher> Wiley, </publisher> <month> August </month> <year> 1994. </year>
Reference-contexts: However, this approach may fail if short term dependencies are equally as important. Finally, it has been suggested that a network architecture that operates on multiple time scales might be useful for approaching this problem <ref> [12] </ref>. In this paper, we also propose an architectural approach to deal with long-term dependencies. We focus on a class of architectures based upon Nonlinear AutoRegressive models with eXogenous inputs (NARX models), and are therefore called NARX recurrent neural networks [3, 18].
Reference: [13] <author> B.G. Horne and C.L. Giles. </author> <title> An experimental comparison of recurrent neural networks. </title> <booktitle> In Advances in Neural Information Processing Systems 7, </booktitle> <year> 1995. </year> <note> To appear. </note>
Reference-contexts: Furthermore, we 2 have previously reported that gradient descent learning is more effective in NARX networks than in recurrent neural network architectures with "hidden states" when applied to problems including grammatical inference and nonlinear system identification <ref> [11, 13] </ref>. Typically, these networks converge much faster and generalize better than other networks. The results in this paper give an explanation of this phenomenon. 2 Vanishing gradients and long-term dependencies Bengio et al. [2] have analytically explained why learning problems with long-term dependencies is difficult. <p> This has been observed previously, in the sense that gradient descent learning appeared to be more effective in NARX networks than in recurrent neural network architectures that have "hidden states" on problems including grammatical inference and nonlinear system identification <ref> [11, 13] </ref>. The intuitive explanation for this behavior is that the output delays are manifested as jump-ahead connections in the unfolded network that is often used to describe algorithms like Backprop 19 agation Through Time.
Reference: [14] <author> R.R. Leighton and B.C. Conrath. </author> <title> The autoregressive backpropagation algorithm. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks, </booktitle> <volume> volume 2, </volume> <pages> pages 369-377, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: We also presented two experimental problems which show that NARX networks can outperform networks with single delays on some simple problems involving long-term dependencies. We speculate that similar results could be obtained for other networks. In particular we hypothesize that any network that uses tapped delay feedback <ref> [1, 14] </ref> would demonstrate improved performance on problems involving long-term dependencies. It may also be possible to obtain similar results for the architectures proposed in [6, 9, 20, 34]. We have also described in detail some of the assumptions presented in [2] regarding what it means to latch information robustly.
Reference: [15] <author> I.J. Leontaritis and S.A. Billings. </author> <title> Input-output parametric models for non-linear systems: Part I: deterministic non-linear systems. </title> <journal> International Journal of Control, </journal> <volume> 41(2) </volume> <pages> 303-328, </pages> <year> 1985. </year>
Reference-contexts: descent methods are not sufficiently powerful to discover a relationship between target outputs and inputs that occur at a much earlier time, which they term the problem of long-term dependencies. 3 NARX networks An important class of discrete-time nonlinear systems is the Nonlinear AutoRegressive with eXoge nous inputs (NARX) model <ref> [3, 15, 16, 31, 32] </ref>: y (t) = f u (t D u ); : : : ; u (t 1); u (t); y (t D y ); : : : ; y (t 1) ; (3) where u (t) and y (t) represent input and output of the network at
Reference: [16] <author> L. Ljung. </author> <title> System identification : Theory for the user. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1987. </year>
Reference-contexts: descent methods are not sufficiently powerful to discover a relationship between target outputs and inputs that occur at a much earlier time, which they term the problem of long-term dependencies. 3 NARX networks An important class of discrete-time nonlinear systems is the Nonlinear AutoRegressive with eXoge nous inputs (NARX) model <ref> [3, 15, 16, 31, 32] </ref>: y (t) = f u (t D u ); : : : ; u (t 1); u (t); y (t D y ); : : : ; y (t 1) ; (3) where u (t) and y (t) represent input and output of the network at
Reference: [17] <author> M. C. Mozer. </author> <title> Induction of multiscale temporal structure. </title> <editor> In J.E. Moody, S. J. Hanson, and R.P. Lippmann, editors, </editor> <booktitle> Neural Information Processing Systems 4, </booktitle> <pages> pages 275-282. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year> <month> 21 </month>
Reference-contexts: This was noted by Mozer who reported that RNNs were able to learn short term musical structure using gradient based methods <ref> [17] </ref>, but had difficulty capturing global behavior. These ideas were recently formalized by Bengio et al. [2], who showed that if a system is to robustly latch information, then the fraction of the gradient due to information n time steps in the past approaches zero as n becomes large. <p> As an alternative to using different learning algorithms, one suggestion has been to alter the input data so that it represents a reduced description that makes global features more explicit and more readily detectable <ref> [17, 26, 25] </ref>. However, this approach may fail if short term dependencies are equally as important. Finally, it has been suggested that a network architecture that operates on multiple time scales might be useful for approaching this problem [12].
Reference: [18] <author> K.S. Narendra and K. Parthasarathy. </author> <title> Identification and control of dynamical systems using neural networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 1 </volume> <pages> 4-27, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: In this paper, we also propose an architectural approach to deal with long-term dependencies. We focus on a class of architectures based upon Nonlinear AutoRegressive models with eXogenous inputs (NARX models), and are therefore called NARX recurrent neural networks <ref> [3, 18] </ref>. This is a powerful class of models which has recently been shown to be computationally equivalent to Turing machines [28]. <p> It has been demonstrated that they are well suited for modeling nonlinear systems such as heat exchangers [3], waste water treatment plants [31, 32], catalytic reforming systems in a petroleum refinery [32], nonlinear oscillations associated with multi-legged locomotion in biological systems [33], time series [4], and various artificial nonlinear systems <ref> [3, 18, 22] </ref>. Furthermore, we 2 have previously reported that gradient descent learning is more effective in NARX networks than in recurrent neural network architectures with "hidden states" when applied to problems including grammatical inference and nonlinear system identification [11, 13]. <p> When the function f can be approximated by a Multilayer Perceptron, the resulting system is called a NARX recurrent neural network <ref> [3, 18] </ref>. 5 In this paper we shall consider NARX networks with zero input order and a one dimensional output, i.e. those networks which have feedback from the output only. However there is no reason why our results could not be extended to networks with higher input orders.
Reference: [19] <author> O. Nerrand, P. Roussel-Ragot, L. Personnaz, G. Dreyfus, and S. Marcos. </author> <title> Neural networks and nonlinear adaptive filtering: Unifying concepts and new algorithms. </title> <journal> Neural Computation, </journal> <volume> 5(2) </volume> <pages> 165-199, </pages> <year> 1993. </year>
Reference-contexts: Almost any recurrent neural network architecture can be expressed in this form <ref> [19] </ref>, where f and g depend on the specific architecture. For example, in simple first-order recurrent neural networks f would be a sigmoid of a weighted sum of the values x (t) and u (t) and 3 g would simply select one of the states as output.
Reference: [20] <author> P. Poddar and K.P. Unnikrishnan. </author> <title> Non-linear prediction of speech signals using memory neuron networks. In B.H. Juang, S.Y. Kung, and C.A. Kamm, editors, Neural Networks for Signal Processing: </title> <booktitle> Proceedings of the 1991 IEEE Workshop, </booktitle> <pages> pages 1-10. </pages> <publisher> IEEE Press, </publisher> <year> 1991. </year>
Reference-contexts: We speculate that similar results could be obtained for other networks. In particular we hypothesize that any network that uses tapped delay feedback [1, 14] would demonstrate improved performance on problems involving long-term dependencies. It may also be possible to obtain similar results for the architectures proposed in <ref> [6, 9, 20, 34] </ref>. We have also described in detail some of the assumptions presented in [2] regarding what it means to latch information robustly.
Reference: [21] <author> G.V. Puskorius and L.A. Feldkamp. </author> <title> Recurrent network training with the decoupled extended Kalman filter. </title> <booktitle> In Proceedings of the 1992 SPIE Conference on the Science of Artificial Neural Networks, </booktitle> <address> Orlando, Florida, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: Several approaches have been suggested to circumvent the problem of vanishing gradients. For example, gradient-based methods can be abandoned completely in favor of alternative optimization methods <ref> [2, 21] </ref>. However, the algorithms investigated so far either perform just as poorly on problems involving long-term dependencies, or, when they are better, require far more computational resources [2].
Reference: [22] <author> S.-Z. Qin, H.-T. Su, and T.J. McAvoy. </author> <title> Comparison of four neural net learning methods for dynamic system identification. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 3(1) </volume> <pages> 122-130, </pages> <year> 1992. </year>
Reference-contexts: It has been demonstrated that they are well suited for modeling nonlinear systems such as heat exchangers [3], waste water treatment plants [31, 32], catalytic reforming systems in a petroleum refinery [32], nonlinear oscillations associated with multi-legged locomotion in biological systems [33], time series [4], and various artificial nonlinear systems <ref> [3, 18, 22] </ref>. Furthermore, we 2 have previously reported that gradient descent learning is more effective in NARX networks than in recurrent neural network architectures with "hidden states" when applied to problems including grammatical inference and nonlinear system identification [11, 13].
Reference: [23] <author> R.J.Williams and D. Zipser. </author> <title> Gradient-based learning algorithms for recurrent networks and their computational complexity. </title> <editor> In Y. Chauvin and D. E. Rumelhart, editors, Back-propagation: </editor> <booktitle> Theory, Architectures and Applications, chapter 13, </booktitle> <pages> pages 433-486. </pages> <publisher> Lawrence Erlbaum Publishers, </publisher> <address> Hillsdale, N.J., </address> <year> 1995. </year>
Reference-contexts: gradient can be expanded r w C = p T We can expand this further by assuming that the weights at different time indices are independent and computing the partial gradient with respect to these weights, which is the trick used to derive algorithms such as Backpropagation Through Time (BPTT) <ref> [24, 23] </ref>. The total gradient is then 1 We deal only with problems in which the target output is presented at the end of the sequence. 4 equal to the sum of these partial gradients.
Reference: [24] <author> D.E. Rumelhart, G.E. Hinton, and R.J. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In D.E. Rumelhart and J.L. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, </booktitle> <pages> pages 318-362. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: gradient can be expanded r w C = p T We can expand this further by assuming that the weights at different time indices are independent and computing the partial gradient with respect to these weights, which is the trick used to derive algorithms such as Backpropagation Through Time (BPTT) <ref> [24, 23] </ref>. The total gradient is then 1 We deal only with problems in which the target output is presented at the end of the sequence. 4 equal to the sum of these partial gradients.
Reference: [25] <author> J. Schmidhuber. </author> <title> Learning complex, extended sequences using the principle of history compression. </title> <journal> Neural Computation, </journal> <volume> 4(2) </volume> <pages> 234-242, </pages> <year> 1992. </year>
Reference-contexts: As an alternative to using different learning algorithms, one suggestion has been to alter the input data so that it represents a reduced description that makes global features more explicit and more readily detectable <ref> [17, 26, 25] </ref>. However, this approach may fail if short term dependencies are equally as important. Finally, it has been suggested that a network architecture that operates on multiple time scales might be useful for approaching this problem [12].
Reference: [26] <author> J. Schmidhuber. </author> <title> Learning unambiguous reduced sequence descriptions. </title> <editor> In J. E. Moody, S. J. Hanson, and R. P. Lippman, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 291-298. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: As an alternative to using different learning algorithms, one suggestion has been to alter the input data so that it represents a reduced description that makes global features more explicit and more readily detectable <ref> [17, 26, 25] </ref>. However, this approach may fail if short term dependencies are equally as important. Finally, it has been suggested that a network architecture that operates on multiple time scales might be useful for approaching this problem [12].
Reference: [27] <author> D.R. Seidl and D. Lorenz. </author> <title> A structure by which a recurrent neural network can approximate a nonlinear dynamic system. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks, </booktitle> <volume> volume 2, </volume> <pages> pages 709-714, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Recurrent Neural Networks (RNNs) are capable of representing arbitrary nonlinear dynamical sys tems <ref> [27, 29, 30] </ref>. However, learning simple behavior can be quite difficult using gradient descent.
Reference: [28] <author> H.T. Siegelmann, B.G. Horne, and C.L. Giles. </author> <title> Computational capabilities of NARX neural networks. </title> <institution> Technical Report UMIACS-TR-95-12 and CS-TR-3408, Institute for Advanced Computer Studies, University of Maryland, </institution> <year> 1995. </year>
Reference-contexts: We focus on a class of architectures based upon Nonlinear AutoRegressive models with eXogenous inputs (NARX models), and are therefore called NARX recurrent neural networks [3, 18]. This is a powerful class of models which has recently been shown to be computationally equivalent to Turing machines <ref> [28] </ref>.
Reference: [29] <author> H.T. Siegelmann and E.D. Sontag. </author> <title> On the computational power of neural networks. </title> <journal> Journal of Computer and System Science, </journal> <volume> 50(1) </volume> <pages> 132-150, </pages> <year> 1995. </year>
Reference-contexts: 1 Introduction Recurrent Neural Networks (RNNs) are capable of representing arbitrary nonlinear dynamical sys tems <ref> [27, 29, 30] </ref>. However, learning simple behavior can be quite difficult using gradient descent.
Reference: [30] <author> E.D. Sontag. </author> <title> Systems combining linearity and saturations and relations to neural networks. </title> <type> Technical Report SYCON-92-01, </type> <institution> Rutgers Center for Systems and Control, </institution> <year> 1992. </year>
Reference-contexts: 1 Introduction Recurrent Neural Networks (RNNs) are capable of representing arbitrary nonlinear dynamical sys tems <ref> [27, 29, 30] </ref>. However, learning simple behavior can be quite difficult using gradient descent.
Reference: [31] <author> H.-T. Su and T.J. McAvoy. </author> <title> Identification of chemical processes using recurrent networks. </title> <booktitle> In Proceedings of the American Controls Conference, </booktitle> <volume> volume 3, </volume> <pages> pages 2314-2319, </pages> <year> 1991. </year> <month> 22 </month>
Reference-contexts: This is a powerful class of models which has recently been shown to be computationally equivalent to Turing machines [28]. It has been demonstrated that they are well suited for modeling nonlinear systems such as heat exchangers [3], waste water treatment plants <ref> [31, 32] </ref>, catalytic reforming systems in a petroleum refinery [32], nonlinear oscillations associated with multi-legged locomotion in biological systems [33], time series [4], and various artificial nonlinear systems [3, 18, 22]. <p> descent methods are not sufficiently powerful to discover a relationship between target outputs and inputs that occur at a much earlier time, which they term the problem of long-term dependencies. 3 NARX networks An important class of discrete-time nonlinear systems is the Nonlinear AutoRegressive with eXoge nous inputs (NARX) model <ref> [3, 15, 16, 31, 32] </ref>: y (t) = f u (t D u ); : : : ; u (t 1); u (t); y (t D y ); : : : ; y (t 1) ; (3) where u (t) and y (t) represent input and output of the network at
Reference: [32] <author> H.-T. Su, T.J. McAvoy, and P. Werbos. </author> <title> Long-term predictions of chemical processes using recurrent neural networks: A parallel training approach. </title> <journal> Industrial Engineering and Chemical Research, </journal> <volume> 31 </volume> <pages> 1338-1352, </pages> <year> 1992. </year>
Reference-contexts: This is a powerful class of models which has recently been shown to be computationally equivalent to Turing machines [28]. It has been demonstrated that they are well suited for modeling nonlinear systems such as heat exchangers [3], waste water treatment plants <ref> [31, 32] </ref>, catalytic reforming systems in a petroleum refinery [32], nonlinear oscillations associated with multi-legged locomotion in biological systems [33], time series [4], and various artificial nonlinear systems [3, 18, 22]. <p> It has been demonstrated that they are well suited for modeling nonlinear systems such as heat exchangers [3], waste water treatment plants [31, 32], catalytic reforming systems in a petroleum refinery <ref> [32] </ref>, nonlinear oscillations associated with multi-legged locomotion in biological systems [33], time series [4], and various artificial nonlinear systems [3, 18, 22]. <p> descent methods are not sufficiently powerful to discover a relationship between target outputs and inputs that occur at a much earlier time, which they term the problem of long-term dependencies. 3 NARX networks An important class of discrete-time nonlinear systems is the Nonlinear AutoRegressive with eXoge nous inputs (NARX) model <ref> [3, 15, 16, 31, 32] </ref>: y (t) = f u (t D u ); : : : ; u (t 1); u (t); y (t D y ); : : : ; y (t 1) ; (3) where u (t) and y (t) represent input and output of the network at
Reference: [33] <author> S.T. Venkataraman. </author> <title> On encoding nonlinear oscillations in neural networks for locomotion. </title> <booktitle> In Proceedings of the Eigth Yale Workshop on Adaptive and Learning Systems, </booktitle> <pages> pages 14-20, </pages> <year> 1994. </year>
Reference-contexts: It has been demonstrated that they are well suited for modeling nonlinear systems such as heat exchangers [3], waste water treatment plants [31, 32], catalytic reforming systems in a petroleum refinery [32], nonlinear oscillations associated with multi-legged locomotion in biological systems <ref> [33] </ref>, time series [4], and various artificial nonlinear systems [3, 18, 22]. Furthermore, we 2 have previously reported that gradient descent learning is more effective in NARX networks than in recurrent neural network architectures with "hidden states" when applied to problems including grammatical inference and nonlinear system identification [11, 13].
Reference: [34] <author> E.A. Wan. </author> <title> Time series prediction by using a connectionist network with internal delay lines. In A.S. </title> <editor> Weigend and N.A. Gershenfeld, editors, </editor> <booktitle> Time Series Prediction, </booktitle> <pages> pages 195-217. </pages> <publisher> Addison-Wesley, </publisher> <year> 1994. </year> <month> 23 </month>
Reference-contexts: We speculate that similar results could be obtained for other networks. In particular we hypothesize that any network that uses tapped delay feedback [1, 14] would demonstrate improved performance on problems involving long-term dependencies. It may also be possible to obtain similar results for the architectures proposed in <ref> [6, 9, 20, 34] </ref>. We have also described in detail some of the assumptions presented in [2] regarding what it means to latch information robustly.
References-found: 34

