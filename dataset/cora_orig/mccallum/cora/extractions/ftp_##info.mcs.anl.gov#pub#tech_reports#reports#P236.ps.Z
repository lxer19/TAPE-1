URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P236.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts91.htm
Root-URL: http://www.mcs.anl.gov
Email: bischof@mcs.anl.gov  
Title: LAPACK: Linear Algebra Software for Supercomputers 1  
Author: Christian H. Bischof 
Date: 101-120, 1991.  
Note: Argonne Preprint MCS-P236-0491 Appeared in Proc. 2nd ODIN Symposium, A. Schreiner and W. Ewinger, Eds., pp.  
Address: Argonne, IL 60439-4801  
Affiliation: Mathematics and Computer Science Division Argonne National Laboratory  
Abstract: This paper presents an overview of the LAPACK library, a portable, public-domain library to solve the most common linear algebra problems. This library provides a uniformly designed set of subroutines for solving systems of simultaneous linear equations, least-squares problems, and eigenvalue problems for dense and banded matrices. We elaborate on the design methodologies incorporated to make the LAPACK codes efficient on today's high-performance architectures. In particular, we discuss the use of block algorithms and the reliance on the Basic Linear Algebra Subprograms (BLAS). We present performance results that show the suitability of the LAPACK approach for vector uniprocessors and shared-memory multiprocessors. We also address some issues that have to be dealt with in tuning LAPACK for specific architectures. Lastly, we present results that show that the LAPACK software can be adapted with little effort to we distributed-memory environments, and discuss future efforts resulting from this project. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Anderson, A. Benzoni, J. Dongarra, S. Moulton, S. Ostrouchov, B. Tourancheau, and R. van de Geijn. </author> <title> Basic Linear Algebra Communication Subprograms. </title> <booktitle> In Proceedings of the Sixth Distributed-Memory Computing Conference. </booktitle> <publisher> ACM Press, </publisher> <year> 1991. </year>
Reference-contexts: Ed Anderson, Annamaria Benzoni, Jack Dongarra, Steve Moulton, Susan Ostrouchov, Bernard Tourancheau, and Robert van de Geijn of the University of Tennessee have been defining a standard communication library for dense linear algebra computations on distributed-memory machines, the so-called BLACS (Basic Linear Algebra Communication Subprograms) <ref> [1] </ref>. Using a prototype implementation of the BLACS, they have implemented the QR, LU, Cholesky, and symmetric indefinite factorizations, as well as the reductions to Hessenberg and tridiagonal form [18, 2]. Performance results on a 128-node Intel iPSC/860 distributed-memory multiprocessor are shown in Figure 10.
Reference: [2] <author> E. Anderson, A. Benzoni, J. Dongarra, S. Moulton, S. Ostrouchov, B. Tourancheau, and R. van de Geijn. </author> <title> Basic Linear Algebra Communication Subprograms. </title> <booktitle> In Proceedings of the 14 Fifth Siam Conference for Parallel Processing in Scientific Computing, </booktitle> <address> Philadelphia, </address> <year> 1991. </year> <note> SIAM. </note>
Reference-contexts: Using a prototype implementation of the BLACS, they have implemented the QR, LU, Cholesky, and symmetric indefinite factorizations, as well as the reductions to Hessenberg and tridiagonal form <ref> [18, 2] </ref>. Performance results on a 128-node Intel iPSC/860 distributed-memory multiprocessor are shown in Figure 10.
Reference: [3] <author> E. Anderson, C. Bischof, J. Demmel, J. Dongarra, J. DuCroz, S. Hammarling, and W. Kahan. </author> <title> Prospectus for an extension to LAPACK: A portable linear algebra library for high-performance computers. </title> <note> Technical Report LAPACK Working Note #26, </note> <institution> CS-90-118, Computer Science Department, The University of Tennessee, </institution> <year> 1990. </year>
Reference-contexts: Of course, during this effort, a variety of worthwhile 12 0 20 40 60 QR factorization on IBM RISC/6000-550 order of matrix MFLOPS NB= 1 NB= 16 NB= 32 avenues opened up, some of which we aim to pursue in a successor project to LAPACK <ref> [3] </ref>.
Reference: [4] <author> Edward Anderson, Zhaojun Bai, Christian Bischof, James Demmel, Jack Dongarra, Jeremy DuCroz, Anne Greenbaum, Sven Hammarling, Alan McKenney, and Danny Sorensen. LA-PACK: </author> <title> A portable linear algebra library for high-performance computers. </title> <editor> In Joanne Martin, editor, </editor> <booktitle> SUPERCOMPUTING '90, </booktitle> <pages> pages 2-10, </pages> <address> New York, </address> <year> 1990. </year> <note> ACM Press. Also LAPACK Working Note #20, </note> <institution> CS-90-105, Computer Science Department, The University of Tennessee. </institution>
Reference-contexts: These issues are discussed in more detail in <ref> [4] </ref>.
Reference: [5] <author> Edward Anderson and Jack Dongarra. </author> <title> Installing and testing the initial release of LAPACK | Unix and non-Unix versions. </title> <type> Technical Report MCS-TM-130, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: In all areas, similar functionality will be provided for real and complex matrices. The LAPACK package also includes test and timing routines to verify the installation of the LAPACK codes on a particular architecture and to assess their performance (see <ref> [5] </ref>). As a result, LAPACK serves many purposes. * It is an easy-to-use library that solves linear algebra problems efficiently and reliably.
Reference: [6] <author> Edward Anderson and Jack Dongarra. </author> <title> Evaluating block algorithm variants in LAPACK. </title> <type> Technical Report CS-90-103, </type> <institution> Computer Science Department, The University of Tennessee, </institution> <month> April </month> <year> 1990. </year>
Reference-contexts: We also see that, depending on the architecture chosen, the differences between the various versions can be quite substantial. For a discussion of these issues for other decompositions, see <ref> [6] </ref>. For the LAPACK release, we tried to choose the variant that provides the best "average" performance over the range of target machines. We also note that there may be quite a difference between the variant that overwrites the upper or lower triangle, because of the different memory access patterns.
Reference: [7] <author> Zhajoun Bai and James Demmel. </author> <title> On a block implementation of Hessenberg multishift QR iteration. </title> <journal> International Journal of High-Speed Computing, </journal> <volume> 1(1) </volume> <pages> 97-112, </pages> <year> 1989. </year> <note> Also LAPACK Working Note #8, </note> <institution> ANL-MCS-TM-127, Mathematics and Computer Science Division, Argonne National Laboratory. </institution>
Reference-contexts: As a result, efforts are under way [15, 28] to achieve this effect by means of compiler transformations. There are other block algorithms, however (for example, the block multishift algorithm of Bai and Demmel <ref> [7] </ref> and the QR factorization algorithm for rank-deficient matrices of Bischof [11]), where new algorithms have been invented in order to be able to exploit the power of matrix-matrix kernels. For example, the performance of the QR algorithm for rank-deficient matrices on the CRAY Y-MP is shown in Figure 7.
Reference: [8] <author> Christian Bischof, James Demmel, Jack Dongarra, Jeremy Du Croz, Anne Greenbaum, Sven Hammarling, and Danny Sorensen. </author> <note> LAPACK Working Note #5: Provisional contents. Technical Report ANL-88-38, </note> <institution> Argonne National Laboratory, Mathematics and Computer Science Division, </institution> <month> September </month> <year> 1988. </year>
Reference-contexts: Machine-specific optimizations are limited to those kernels, and the user interface is uniform across machines. A detailed description of the LAPACK package is given in <ref> [8] </ref>. 2 The Basic Linear Algebra Subprograms (BLAS) The Basic Linear Algebra Subprograms (BLAS) provide an interface for the elementary matrix and vector operations.
Reference: [9] <author> Christian H. Bischof. </author> <title> Adaptive blocking in the QR factorization. </title> <journal> The Journal of Supercomputing, </journal> <volume> 3(3) </volume> <pages> 193-208, </pages> <year> 1989. </year>
Reference-contexts: In <ref> [9] </ref>, we suggested a methodology called adaptive blocking for finding "good" block sizes for pipelined factorization algorithms on distributed-memory multiprocessors.
Reference: [10] <author> Christian H. Bischof. </author> <title> A block QR factorization algorithm using restricted pivoting. </title> <booktitle> In Proceedings SUPERCOMPUTING '89, </booktitle> <pages> pages 248-256, </pages> <address> Baltimore, Md., 1989. </address> <publisher> ACM Press. </publisher>
Reference-contexts: For an m fi n matrix partitioned into blocks of size nb, one has to compute O (m n nb) extra floating-point operations in the blocked algorithm, compared to the unblocked algorithm <ref> [10, 14, 29] </ref>. These extra operations are offset by the higher speed of the Level 3 BLAS kernels (compared to the Level 2 BLAS kernels) that can now be employed. But obviously there is a tradeoff.
Reference: [11] <author> Christian H. Bischof. </author> <title> A block QR factorization algorithm for rank-deficient matrices. </title> <editor> In Jack J. Dongarra, Paul Messina, Danny C. Sorensen, and Robert G. Voigt, editors, </editor> <booktitle> Parallel Processing for Scientific Computing, </booktitle> <pages> pages 9-14, </pages> <address> Philadelphia, </address> <year> 1990. </year> <note> SIAM. </note>
Reference-contexts: As a result, efforts are under way [15, 28] to achieve this effect by means of compiler transformations. There are other block algorithms, however (for example, the block multishift algorithm of Bai and Demmel [7] and the QR factorization algorithm for rank-deficient matrices of Bischof <ref> [11] </ref>), where new algorithms have been invented in order to be able to exploit the power of matrix-matrix kernels. For example, the performance of the QR algorithm for rank-deficient matrices on the CRAY Y-MP is shown in Figure 7.
Reference: [12] <author> Christian H. Bischof. </author> <title> Fundamental Linear Algebra Computations on High-Performance Computers, </title> <booktitle> volume 250 of Informatik Fachberichte, </booktitle> <pages> pages 167-182. </pages> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1990. </year>
Reference-contexts: To exploit the Level 3 BLAS, one usually must express the algorithm at the top level in terms of operations on submatrices (the so-called blocks) as compared to vector- or scalar-oriented operations. Many references to block algorithms can be found in <ref> [12, 21, 22] </ref>. As an example of a block algorithm, we consider algorithms for computing the Cholesky decomposition A = LL T ; where A is a symmetric positive definite matrix and L is an lower triangular matrix. <p> For example, the performance of the QR algorithm for rank-deficient matrices on the CRAY Y-MP is shown in Figure 7. We here compare the LINPACK codes, the traditional algorithm implemented with the Level 2 BLAS, and the new block algorithm. Many references to block algorithms can be found in <ref> [12, 21, 22] </ref>. We also note that the LAPACK codes are, in many respects, significantly more reliable than other available software products of comparable scope.
Reference: [13] <author> Christian H. Bischof and Philippe G. Lacroute. </author> <title> An adaptive blocking strategy for matrix factorizations. </title> <editor> In H. Burkhart, editor, </editor> <booktitle> Lecture Notes in Computer Science 457, </booktitle> <pages> pages 210-221, </pages> <address> New York, 1990. </address> <publisher> Springer Verlag. </publisher> <pages> 15 </pages>
Reference-contexts: In [9], we suggested a methodology called adaptive blocking for finding "good" block sizes for pipelined factorization algorithms on distributed-memory multiprocessors. This technique was refined in <ref> [13] </ref>, where we developed a recursion formula for the optimal blocking strategy. 5 Outlook The last test release of the LAPACK package will be sent out in May of 1991, and the package will be formally released in the summer of 1991.
Reference: [14] <author> Christian H. Bischof and Charles F. Van Loan. </author> <title> The WY representation for products of House--holder matrices. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 8:s2-s13, </volume> <year> 1987. </year>
Reference-contexts: For an m fi n matrix partitioned into blocks of size nb, one has to compute O (m n nb) extra floating-point operations in the blocked algorithm, compared to the unblocked algorithm <ref> [10, 14, 29] </ref>. These extra operations are offset by the higher speed of the Level 3 BLAS kernels (compared to the Level 2 BLAS kernels) that can now be employed. But obviously there is a tradeoff.
Reference: [15] <author> Steven Carr and Ken Kennedy. </author> <title> Blocking linear algebra codes for memory hierarchies. </title> <booktitle> In Proceedings of the Fourth SIAM Conference on Parallel Processing for Scientific Computing. </booktitle> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1990. </year> <note> To appear. </note>
Reference-contexts: At any rate, however, the new codes perform substantially better than the EISPACK codes. To achieve block algorithms for the Cholesky and LU factorizations is relatively simple: it is essentially a restructuring of loops. As a result, efforts are under way <ref> [15, 28] </ref> to achieve this effect by means of compiler transformations.
Reference: [16] <author> J. J. Dongarra, J. R. Bunch, C. B. Moler, and G. W. Stewart. </author> <title> LINPACK Users' Guide. </title> <publisher> SIAM Press, </publisher> <address> Philadelphia, </address> <year> 1979. </year>
Reference-contexts: The new library will extend the successful EISPACK [23, 30] and LINPACK <ref> [16] </ref> libraries, integrating the two sets of algorithms into a unified, systematic library. A great deal of effort has also been expended to incorporate design methodologies that make the LAPACK algorithms more appropriate for today's high-performance architectures.
Reference: [17] <author> Jack Dongarra, Jeremy Du Croz, Iain Duff, and Sven Hammarling. </author> <title> A set of Level 3 Basic Linear Algebra Subprograms. </title> <type> Preprint MCS-P1-0888, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <month> August </month> <year> 1988. </year>
Reference-contexts: While we do not hope for LAPACK codes to be optimal for all architectures, we expect high performance over a wide range of machines. By relying on the Basic Linear Algebra Subprograms (BLAS) <ref> [17, 19, 27] </ref> the codes can be "tuned" to a given architecture by efficient|and, in all likelihood machine-dependent| implementations of these kernels. Machine-specific optimizations are limited to those kernels, and the user interface is uniform across machines. <p> For this reason it is imperative to reuse data as much as possible to cut down on data movement overhead. This goal can be achieved by expressing a computation in terms of matrix-matrix operations. The Level 3 BLAS <ref> [17] </ref> provide the matrix-matrix operations needed for linear algebra. Together with the Level 1 and 2 BLAS, they provide a well-defined interface for the elementary matrix and vector operations and add to the portability, modularity, and ease of maintenance of the software.
Reference: [18] <author> Jack Dongarra and Susan Ostrouchov. </author> <title> LAPACK block factorization algorithms on the Intel iPSc/860. </title> <note> Technical Report LAPACK Working Note #24, TR CS-90-115, </note> <institution> Computer Science Department, The University of Tennessee, </institution> <month> October </month> <year> 1990. </year>
Reference-contexts: Using a prototype implementation of the BLACS, they have implemented the QR, LU, Cholesky, and symmetric indefinite factorizations, as well as the reductions to Hessenberg and tridiagonal form <ref> [18, 2] </ref>. Performance results on a 128-node Intel iPSC/860 distributed-memory multiprocessor are shown in Figure 10.
Reference: [19] <author> Jack J. Dongarra, Jeremy Du Croz, Sven Hammarling, and Richard J. Hanson. </author> <title> An extended set of Fortran Basic Linear Algebra Subprograms. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 14(1) </volume> <pages> 1-17, </pages> <year> 1988. </year>
Reference-contexts: While we do not hope for LAPACK codes to be optimal for all architectures, we expect high performance over a wide range of machines. By relying on the Basic Linear Algebra Subprograms (BLAS) <ref> [17, 19, 27] </ref> the codes can be "tuned" to a given architecture by efficient|and, in all likelihood machine-dependent| implementations of these kernels. Machine-specific optimizations are limited to those kernels, and the user interface is uniform across machines. <p> The first BLAS [27], which we call Level 1 BLAS, implement common vector-vector operations such as a dot product, or a "saxpy," y y + ffx; where x and y are vectors and ff is a scalar. The Level 2 BLAS <ref> [19] </ref> provide matrix-vector operations such as matrix-vector multiplication and rank-one updates. The development of the Level 2 BLAS was motivated by vector-processing machines.
Reference: [20] <author> Jack J. Dongarra and Iain S. Duff. </author> <title> Advanced computer architectures. </title> <type> Technical Report ANL-MCS-TM-57, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1989. </year> <note> Revision 2. </note>
Reference-contexts: Unfortunately, this approach is often not well suited to computers with a memory hierarchy (such as global memory, cache or local memory, and vector registers) and parallel-processing computers. (For a description of many advanced-computer architectures, see <ref> [20, 26, 31] </ref>.) Data at low levels of the memory hierarchy can be accessed immediately, whereas data at higher levels is available only after some delay and (because of memory bank conflicts) may not be available at a rate fast enough to feed the arithmetic units.
Reference: [21] <author> Jack J. Dongarra, Iain S. Duff, Danny C. Sorensen, and Henk A. Van der Vorst. </author> <title> Solving Linear Systems on Vector and Shared-Memory Computers. </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1991. </year>
Reference-contexts: To exploit the Level 3 BLAS, one usually must express the algorithm at the top level in terms of operations on submatrices (the so-called blocks) as compared to vector- or scalar-oriented operations. Many references to block algorithms can be found in <ref> [12, 21, 22] </ref>. As an example of a block algorithm, we consider algorithms for computing the Cholesky decomposition A = LL T ; where A is a symmetric positive definite matrix and L is an lower triangular matrix. <p> For example, the performance of the QR algorithm for rank-deficient matrices on the CRAY Y-MP is shown in Figure 7. We here compare the LINPACK codes, the traditional algorithm implemented with the Level 2 BLAS, and the new block algorithm. Many references to block algorithms can be found in <ref> [12, 21, 22] </ref>. We also note that the LAPACK codes are, in many respects, significantly more reliable than other available software products of comparable scope.
Reference: [22] <author> Kyle Gallivan, Robert Plemmons, and Ahmed Sameh. </author> <title> Parallel algorithms for dense linear algebra computations. </title> <journal> SIAM Review, </journal> <volume> 32(1) </volume> <pages> 54-135, </pages> <year> 1990. </year>
Reference-contexts: To exploit the Level 3 BLAS, one usually must express the algorithm at the top level in terms of operations on submatrices (the so-called blocks) as compared to vector- or scalar-oriented operations. Many references to block algorithms can be found in <ref> [12, 21, 22] </ref>. As an example of a block algorithm, we consider algorithms for computing the Cholesky decomposition A = LL T ; where A is a symmetric positive definite matrix and L is an lower triangular matrix. <p> For example, the performance of the QR algorithm for rank-deficient matrices on the CRAY Y-MP is shown in Figure 7. We here compare the LINPACK codes, the traditional algorithm implemented with the Level 2 BLAS, and the new block algorithm. Many references to block algorithms can be found in <ref> [12, 21, 22] </ref>. We also note that the LAPACK codes are, in many respects, significantly more reliable than other available software products of comparable scope.
Reference: [23] <author> B. Garbow, J. Boyle, J. Dongarra, and C. Moler. </author> <title> Matrix Eigensystem Routines | EISPACK Guide Extension, </title> <booktitle> volume 51 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer Verlag, </publisher> <address> New York, </address> <year> 1977. </year>
Reference-contexts: Because of their considerable size, the codes are also good compiler and floating-point arithmetic test suites. * Lastly, the accuracy and robustness of the LAPACK codes provide a standard against which competing implementations and algorithms can be measured. The new library will extend the successful EISPACK <ref> [23, 30] </ref> and LINPACK [16] libraries, integrating the two sets of algorithms into a unified, systematic library. A great deal of effort has also been expended to incorporate design methodologies that make the LAPACK algorithms more appropriate for today's high-performance architectures.
Reference: [24] <author> Nikolaus Geers. </author> <title> Optimization of Level 2 BLAS for Siemens VP systems. </title> <type> Technical Report 37.39, </type> <institution> University of Karlsruhe, Computer Center, </institution> <year> 1989. </year>
Reference-contexts: It should be noted that the performance numbers for the Siemens S600/10 are preliminary, since the BLAS employed are simply the Fortran BLAS that had been optimized for the Siemens VP series <ref> [24, 25] </ref>. For all machines a matrix-matrix multiply is preferable to a series of matrix-vector multiplies. The reason is that the memory bandwidth of high-performance supercomputers is significantly lower than the speed of the processors.
Reference: [25] <author> Helke Grasemann. </author> <title> Optimization of Level 3 BLAS for Siemens VP systems. </title> <type> Technical Report 38.89, </type> <institution> University of Karlsruhe, Computer Center, </institution> <year> 1989. </year>
Reference-contexts: It should be noted that the performance numbers for the Siemens S600/10 are preliminary, since the BLAS employed are simply the Fortran BLAS that had been optimized for the Siemens VP series <ref> [24, 25] </ref>. For all machines a matrix-matrix multiply is preferable to a series of matrix-vector multiplies. The reason is that the memory bandwidth of high-performance supercomputers is significantly lower than the speed of the processors.
Reference: [26] <author> Kai Hwang and Faye A. Briggs. </author> <title> Computer Architecture and Parallel Processing. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1984. </year>
Reference-contexts: Unfortunately, this approach is often not well suited to computers with a memory hierarchy (such as global memory, cache or local memory, and vector registers) and parallel-processing computers. (For a description of many advanced-computer architectures, see <ref> [20, 26, 31] </ref>.) Data at low levels of the memory hierarchy can be accessed immediately, whereas data at higher levels is available only after some delay and (because of memory bank conflicts) may not be available at a rate fast enough to feed the arithmetic units. <p> The left-looking version requires the greatest total number of memory references, but fewer writes than the right-looking version. This latter feature may be advantageous in shared-memory multiprocessors where cache consistency is guaranteed by the use of "write-through" caches <ref> [26] </ref>. On those architectures, read accesses to cached data can be satisfied in one cycle, but write accesses are immediately flushed to memory; as a result, write accesses can be much slower than read accesses. The performance of those variants on an eight-processor CRAY Y-MP is shown in Figure 3.
Reference: [27] <author> C. L. Lawson, R. J. Hanson, R. J. Kincaid, and F. T. Krogh. </author> <title> Basic Linear Algebra Subprograms for Fortran usage. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 5(3) </volume> <pages> 308-323, </pages> <month> September </month> <year> 1979. </year> <month> 16 </month>
Reference-contexts: While we do not hope for LAPACK codes to be optimal for all architectures, we expect high performance over a wide range of machines. By relying on the Basic Linear Algebra Subprograms (BLAS) <ref> [17, 19, 27] </ref> the codes can be "tuned" to a given architecture by efficient|and, in all likelihood machine-dependent| implementations of these kernels. Machine-specific optimizations are limited to those kernels, and the user interface is uniform across machines. <p> A detailed description of the LAPACK package is given in [8]. 2 The Basic Linear Algebra Subprograms (BLAS) The Basic Linear Algebra Subprograms (BLAS) provide an interface for the elementary matrix and vector operations. The first BLAS <ref> [27] </ref>, which we call Level 1 BLAS, implement common vector-vector operations such as a dot product, or a "saxpy," y y + ffx; where x and y are vectors and ff is a scalar. The Level 2 BLAS [19] provide matrix-vector operations such as matrix-vector multiplication and rank-one updates.
Reference: [28] <author> Robert Schreiber and Jack Dongarra. </author> <title> Automatic blocking of nested loops. </title> <type> Technical Report CS-90-108, </type> <institution> Computer Science Department, The University of Tennessee, </institution> <year> 1990. </year>
Reference-contexts: At any rate, however, the new codes perform substantially better than the EISPACK codes. To achieve block algorithms for the Cholesky and LU factorizations is relatively simple: it is essentially a restructuring of loops. As a result, efforts are under way <ref> [15, 28] </ref> to achieve this effect by means of compiler transformations.
Reference: [29] <author> Robert Schreiber and Charles Van Loan. </author> <title> A storage efficient WY representation for products of Householder transformations. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 10(1) </volume> <pages> 53-57, </pages> <year> 1989. </year>
Reference-contexts: For an m fi n matrix partitioned into blocks of size nb, one has to compute O (m n nb) extra floating-point operations in the blocked algorithm, compared to the unblocked algorithm <ref> [10, 14, 29] </ref>. These extra operations are offset by the higher speed of the Level 3 BLAS kernels (compared to the Level 2 BLAS kernels) that can now be employed. But obviously there is a tradeoff.
Reference: [30] <author> B. Smith, J. Boyle, J. Dongarra, B. Garbow, Y. Ikebe, V. Klema, and C. B. Moler. </author> <title> Matrix Eigensystem Routines | EISPACK Guide. </title> <publisher> Springer Verlag, </publisher> <address> New York, 2nd edition, </address> <year> 1976. </year>
Reference-contexts: Because of their considerable size, the codes are also good compiler and floating-point arithmetic test suites. * Lastly, the accuracy and robustness of the LAPACK codes provide a standard against which competing implementations and algorithms can be measured. The new library will extend the successful EISPACK <ref> [23, 30] </ref> and LINPACK [16] libraries, integrating the two sets of algorithms into a unified, systematic library. A great deal of effort has also been expended to incorporate design methodologies that make the LAPACK algorithms more appropriate for today's high-performance architectures.
Reference: [31] <author> Harold Stone. </author> <title> High-Performance Computer Architecture. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachus-setts, </address> <year> 1987. </year> <month> 17 </month>
Reference-contexts: Unfortunately, this approach is often not well suited to computers with a memory hierarchy (such as global memory, cache or local memory, and vector registers) and parallel-processing computers. (For a description of many advanced-computer architectures, see <ref> [20, 26, 31] </ref>.) Data at low levels of the memory hierarchy can be accessed immediately, whereas data at higher levels is available only after some delay and (because of memory bank conflicts) may not be available at a rate fast enough to feed the arithmetic units.
References-found: 31

