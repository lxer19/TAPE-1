URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-373.ps.Z
Refering-URL: http://www-white.media.mit.edu/cgi-bin/tr_pagemaker/
Root-URL: http://www.media.mit.edu
Email: fsbasu,mkc,billg,ali,sandyg@media.mit.edu  
Title: Audio for Interactive Environments  
Author: Sumit Basu, Michael Casey, William Gardner, Ali Azarbayejani, and Alex Pentland 
Address: 20 Ames St., Cambridge, MA 02139 USA  
Affiliation: The MIT Media Laboratory,  
Note: Vision-Steered  
Pubnum: Perceptual Computing Section,  
Abstract: M.I.T Media Laboratory Perceptual Computing Section Technical Report No. 373 Appears: Proceedings of IMAGE'COM '96, Bordeaux, France, May 1996 Abstract We present novel techniques for obtaining and producing audio information in an interactive virtual environment using vision information. These techniques are free of mechanisms that would encumber the user, such as clip-on microphones, headphones, etc. Methods are described for both extracting sound from a given position in space and for rendering an "auditory scene," i.e., given a user location, producing sounds that appear to the user to be coming from an arbitrary point in 3-D space. In both cases, vision information about user position is used to guide the algorithms, resulting in solutions to problems that are difficult and often impossible to robustly solve in the auditory domain alone.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> ANSI. </author> <title> S3.5-1969,American National Standard Methods for the Calculation of the Articulation Index. </title> <institution> American National Standards Institute, </institution> <address> New York, </address> <year> 1969. </year>
Reference-contexts: One such metric is the intelligibility-weighted directivity index [18] in which the directivity index is weighted by a set of frequency-dependent coefficients provided by the ANSI standard for the speech articulation index <ref> [1] </ref>. This metric weights the directivity index in fourteen one-third-octave bands spanning 180 to 4500 Hz [18]. 3.3 Designing the Array An important first consideration is the choice of array geometry. Two possible architectures were considered; endfire (not shown) and broadside Figure 3.
Reference: [2] <author> Sumit Basu, Irfan Essa, and Alex Pentland. </author> <title> "Motion Regularization for Model-Based Head Tracking". M.I.T. Media Laboratory Perceptual Computing Technical Report No. </title> <type> 362. </type>
Reference-contexts: While this is more than accurate enough for the adaptive beamforming algorithm, it is not sufficient for high-quality transaural rendering: the detailed orientation of the head is also necessary. To attain this additional information, we can use the 6 DOF rigid motion head-tracking algorithm described in <ref> [2] </ref>. This method models the head as a rigid ellipsoid and projects the frame to frame motion onto the possible rigid motions of the model. Plots of the orientation tracking are shown for a calibrated sequence in Figure 8.
Reference: [3] <author> Durand R. Begault. </author> <title> 3-D Sound for Virtual Reality and Multimedia. </title> <publisher> Academic Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: For a more detailed discussion and a description of the system used in our virtual environment, please refer to [4]. 3 4.2 Audio Synthesis Principles As described above, a binaural spatializer simulates the auditory experience of one or more sound sources arbitrarily located around a listener <ref> [3] </ref>. The basic idea is to reproduce the acoustical signals at the two ears that would occur in a normal listening situation.
Reference: [4] <author> Michael A. Casey, William G. Gardner, and Sumit Basu. </author> <title> "Vision Steered Beam-forming and Transaural Rendering for the Artificial Life Interactive Virtual Environment (ALIVE)". </title> <journal> In Proc. Audio Eng. Soc. </journal> <volume> Conv., </volume> <year> 1995. </year>
Reference-contexts: Further details for the system we have implemented can be found in <ref> [4] </ref>; further details on beamforming in general can be found in [11]. The geometry of the microphone array is represented by the set of vectors r n which describe the position of each microphone n relative to some reference point (e.g., the center of the array), see Figure 3. <p> Elements steered at 15, 45, and 75 degrees. Note that the reference point of the broadside array geometry (Figure 3) should be aligned with the center of each polar plot A detailed examination of the response patterns with the different array geometries and element responses is developed in <ref> [4] </ref>. Through this study, it was found that four microphones in endfire arrange would provide a very directional beam, but would produce a symmetric lobe at . <p> The incorporation of a second array, on the other side of the ba*e, gives the angles zero to -90 degrees. A detailed account of this proposed setup is in <ref> [4] </ref>. 4 Producing Audio Information We have only presented half of the story so far; we have yet to show how we return audio information to the user. <p> The basics of the theory behind these techniques is presented below. We first demonstrate the spatialization process with headphones and then extend this to the free-field situation with transaural rendering. For a more detailed discussion and a description of the system used in our virtual environment, please refer to <ref> [4] </ref>. 3 4.2 Audio Synthesis Principles As described above, a binaural spatializer simulates the auditory experience of one or more sound sources arbitrarily located around a listener [3]. The basic idea is to reproduce the acoustical signals at the two ears that would occur in a normal listening situation. <p> The direction of the source ( = azimuth, = elevation) determines which pair of HRTFs to use, and the distance (r) determines the gain. A multiple source spatializer then adds a constant level of reverberation to enhance distance perception (see <ref> [4] </ref>). The simplest implementation of a binaural spatializer uses the measured HRTFs directly as finite impulse response (FIR) filters. Because the head response persists for several milliseconds, HRTFs can be more than 100 samples long at typical audio sampling rates.
Reference: [5] <author> Duane H. Cooper and Jerald L. Bauck. </author> <title> "Prospects for Transaural Recording". </title> <journal> J. Audio Eng. Soc., </journal> 37(1/2):3-19, 1989. 
Reference-contexts: The basic idea is to filter the binaural signal such that the subsequent stereo presentation produces the binaural signal at the ears of the listener. The technique was first put into practice by Schroeder and Atal [16, 15] and later refined by Cooper and Bauck <ref> [5] </ref>, who referred to it as "transaural audio". The stereo listening situation is shown in Figure 6, where ^x L and ^x R are the signals sent to the speakers, and y L and y R are the signals at the listener's ears. <p> * The ipsilateral response is taken to be unity, and the con tralateral response is modeled as a delay and attenuation [15]. * Same as above, but the contralateral response is modeled as a delay, attenuation, and lowpass filter 2 . * The head is modeled as a rigid sphere <ref> [5] </ref>. * The head is modeled as a generic human head without pinna. At high frequencies, where pinna response becomes important (&gt; 8 kHz), the head effectively blocks the crosstalk between channels. Furthermore, the variation in head response for different people is greatest at high frequencies [14].
Reference: [6] <author> H. Cox. </author> <title> "Robust Adaptive Beamforming" IEEE Transactions on Acoustics, </title> <booktitle> Speech and Signal Processing, </booktitle> <volume> 35(10) </volume> <pages> 1365-1376, </pages> <year> 1987. </year>
Reference-contexts: As a result, we have the advantages of a static beamforming solution that is adaptive through the use of vision information. Beamforming is a relatively old techique; it was developed in the 1950's for radar applications. In addition, its use in microphone arrays has been widely studied <ref> [6, 9, 17, 18] </ref>. We certainly do not claim to have developed the "optimal" beam-forming strategy for an interactive environment: we leave that task to the audio engineering community. In fact, our approach to beamforming is among the simplest possible.
Reference: [7] <author> Stephen Handel. </author> <title> Listening: An Introduction to the Perception of Auditory Events. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1989. </year>
Reference-contexts: We perceive the location of a sound not only on the basis of the magnitude difference between the two ears (i.e., balance), but also on the basis of the phase and timing difference between the ears (see p.99 of <ref> [7] </ref>). Though this latter difference may seem to be small, human listeners can detect interaural time differences as short as 0.01 msec, which corresponds to a difference in sound source orientations of roughly one degree [7]. <p> on the basis of the phase and timing difference between the ears (see p.99 of <ref> [7] </ref>). Though this latter difference may seem to be small, human listeners can detect interaural time differences as short as 0.01 msec, which corresponds to a difference in sound source orientations of roughly one degree [7]. It has been shown that we use both magnitude and phase information to perform the subtle discrimination tasks we are capable of, such as being able to discern the words of one person from those of an adjacent person (the canonical "cocktail party" problem).
Reference: [8] <author> J. M. Jot, Veronique Larcher, and Olivier Warusfel. </author> <title> "Digital signal processing issues in the context of binaural and transaural stereophony". </title> <journal> In Proc. Audio Eng. Soc. </journal> <volume> Conv., </volume> <year> 1995. </year>
Reference-contexts: The interaural delay can be included in the filter responses directly as leading zero coefficients, or can be factored out in an effort to shorten the filter lengths. It is also possible to use mimimum phase filters derived from the HRTFs <ref> [8] </ref>, since these will in general be shorter than the original HRTFs. This is somewhat risky because the resulting interaural phase may be completely distorted.
Reference: [9] <author> F. Khalil, J.P. Jullien, and A. Gilloire. </author> <title> "Microphone Array for Sound Pickup in Teleconference Systems". </title> <journal> Journal of the Audio Engineering Society, </journal> <volume> 42(9) </volume> <pages> 691-699, </pages> <year> 1994. </year>
Reference-contexts: As a result, we have the advantages of a static beamforming solution that is adaptive through the use of vision information. Beamforming is a relatively old techique; it was developed in the 1950's for radar applications. In addition, its use in microphone arrays has been widely studied <ref> [6, 9, 17, 18] </ref>. We certainly do not claim to have developed the "optimal" beam-forming strategy for an interactive environment: we leave that task to the audio engineering community. In fact, our approach to beamforming is among the simplest possible. <p> This is the well-known principle of pattern multiplication <ref> [9] </ref> [18]. For omnidirectional microphones, the gain patterns for the two layouts are identical but for a rotation. In our implementation, cardioid microphones were used and were placed in a broadside arrangement due to space constraints (see Figure 2).
Reference: [10] <author> P. Maes, T. Darrell, B. Blumberg, and A. Pentland. </author> <title> "The ALIVE System: Full-body Interaction with Autonomous Agents". </title> <booktitle> Proceedings of the Computer Animation Conference, </booktitle> <address> Switzerland, </address> <publisher> IEEE Press, </publisher> <year> 1995. </year>
Reference-contexts: 2 Overview of the Vision System In order to frame our discussion, we first present a brief overview of Pfinder (Person finder), a real-time vision system for tracking and interpretation of people used in our interactive environment (for a more detailed account of the system, please refer to [20] and <ref> [10] </ref>). Pfinder has the capability to accurately determine the 3-D locations of the user's head and other features in real-time at a frame rate of 10Hz and an accuracy of 10cm. With two cameras (stereo Pfinder), the accuracy virtual environment can be refined to 1.5cm.
Reference: [11] <author> R.J. Mailloux. </author> <title> Phased Array Antenna Handbook. </title> <publisher> Artech House, </publisher> <address> Boston, </address> <year> 1994. </year>
Reference-contexts: Further details for the system we have implemented can be found in [4]; further details on beamforming in general can be found in <ref> [11] </ref>. The geometry of the microphone array is represented by the set of vectors r n which describe the position of each microphone n relative to some reference point (e.g., the center of the array), see Figure 3.
Reference: [12] <author> Keith D. Martin. </author> <title> A computational model of spatial hearing. </title> <type> Master's thesis, </type> <institution> MIT Dept. of Elec. Eng., </institution> <year> 1995. </year>
Reference-contexts: This is somewhat risky because the resulting interaural phase may be completely distorted. It would appear, however, that interaural amplitudes as a function of frequency encode more useful directional information than in-teraural phase <ref> [12] </ref>. 4.3 Principles of transaural audio Transaural audio is a method used to deliver binaural signals to the ears of a listener using stereo loudspeakers. The basic idea is to filter the binaural signal such that the subsequent stereo presentation produces the binaural signal at the ears of the listener.
Reference: [13] <author> Henrik Moller, Dorte Hammershoi, Clemen Boje Jensen, and Michael Fris Sorensen. </author> <title> "Transfer Characteristics of Headphones Measured on Human Ears". </title> <journal> J. Audio Eng. Soc., </journal> <volume> 43(4) </volume> <pages> 203-217, </pages> <year> 1995. </year>
Reference-contexts: Usually, the HRTFs are equalized to compensate for the headphone to ear frequency response <ref> [19, 13] </ref>. A schematic diagram of a single source system is shown in Figure 4.2. The direction of the source ( = azimuth, = elevation) determines which pair of HRTFs to use, and the distance (r) determines the gain.
Reference: [14] <author> Henrik Moller, Michael Fris Sorensen, Dorte Hammershoi, and Clemen Boje Jensen. </author> <title> "Head-Related Transfer Functions of Human Subjects". </title> <journal> J. Audio Eng. Soc., </journal> <volume> 43(5) </volume> <pages> 300-321, </pages> <year> 1995. </year>
Reference-contexts: At high frequencies, where pinna response becomes important (&gt; 8 kHz), the head effectively blocks the crosstalk between channels. Furthermore, the variation in head response for different people is greatest at high frequencies <ref> [14] </ref>.
Reference: [15] <author> M. R. Schroeder. </author> <title> "Digital simulation of sound transmission in reverberant spaces". </title> <journal> J. Acoust. Soc. Am., </journal> <volume> 47(2) </volume> <pages> 424-431, </pages> <year> 1970. </year>
Reference-contexts: The basic idea is to filter the binaural signal such that the subsequent stereo presentation produces the binaural signal at the ears of the listener. The technique was first put into practice by Schroeder and Atal <ref> [16, 15] </ref> and later refined by Cooper and Bauck [5], who referred to it as "transaural audio". <p> Here we list a few possible models in order of increasing complexity: * The ipsilateral response is taken to be unity, and the con tralateral response is modeled as a delay and attenuation <ref> [15] </ref>. * Same as above, but the contralateral response is modeled as a delay, attenuation, and lowpass filter 2 . * The head is modeled as a rigid sphere [5]. * The head is modeled as a generic human head without pinna. <p> The general form of the transaural filter (equation 6) may be used instead, but at much greater computational cost. It may be better to abandon the simplified IIR model and use an FIR implementation based on a more realistic head model <ref> [15] </ref>. Using the static, symmetrical transaural system described earlier, the head tracking information was also used to update the positions of 3-D sounds so that the auditory scene remained fixed as the listener's head rotated.
Reference: [16] <author> M. R. Schroeder and B. S. Atal. </author> <title> "Computer simulation of sound transmission in rooms". </title> <journal> IEEE Conv. Record, </journal> <volume> 7 </volume> <pages> 150-155, </pages> <year> 1963. </year>
Reference-contexts: The basic idea is to filter the binaural signal such that the subsequent stereo presentation produces the binaural signal at the ears of the listener. The technique was first put into practice by Schroeder and Atal <ref> [16, 15] </ref> and later refined by Cooper and Bauck [5], who referred to it as "transaural audio".
Reference: [17] <author> W. Soede, A.J. Berkhout, and F.A. Bilsen. </author> <title> "Development of a Directional Hearing Instrument Based on Array Technology". </title> <journal> Journal of the Aoustical Society of America, </journal> <volume> 94(2) </volume> <pages> 785-798, </pages> <year> 1993. </year>
Reference-contexts: As a result, we have the advantages of a static beamforming solution that is adaptive through the use of vision information. Beamforming is a relatively old techique; it was developed in the 1950's for radar applications. In addition, its use in microphone arrays has been widely studied <ref> [6, 9, 17, 18] </ref>. We certainly do not claim to have developed the "optimal" beam-forming strategy for an interactive environment: we leave that task to the audio engineering community. In fact, our approach to beamforming is among the simplest possible.
Reference: [18] <author> R.W. Stadler and W.M. Rabinowitz. </author> <title> "On the Potential of Fixed Arrays for Hearing Aids". </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 94(3) </volume> <pages> 1332-1342, </pages> <year> 1993. </year>
Reference-contexts: As a result, we have the advantages of a static beamforming solution that is adaptive through the use of vision information. Beamforming is a relatively old techique; it was developed in the 1950's for radar applications. In addition, its use in microphone arrays has been widely studied <ref> [6, 9, 17, 18] </ref>. We certainly do not claim to have developed the "optimal" beam-forming strategy for an interactive environment: we leave that task to the audio engineering community. In fact, our approach to beamforming is among the simplest possible. <p> Note that there are a variety of ways of optimizing the ja n j values in W. The standard performance metric for the directionality of a fixed array is the directivity index which is shown in Equation 3 <ref> [18] </ref>. <p> In order to assess an array for use in speech enhancement a broad-band performance metric must be used. One such metric is the intelligibility-weighted directivity index <ref> [18] </ref> in which the directivity index is weighted by a set of frequency-dependent coefficients provided by the ANSI standard for the speech articulation index [1]. This metric weights the directivity index in fourteen one-third-octave bands spanning 180 to 4500 Hz [18]. 3.3 Designing the Array An important first consideration is the <p> One such metric is the intelligibility-weighted directivity index <ref> [18] </ref> in which the directivity index is weighted by a set of frequency-dependent coefficients provided by the ANSI standard for the speech articulation index [1]. This metric weights the directivity index in fourteen one-third-octave bands spanning 180 to 4500 Hz [18]. 3.3 Designing the Array An important first consideration is the choice of array geometry. Two possible architectures were considered; endfire (not shown) and broadside Figure 3. A second factor is the choice of microphone gain pattern for the individual microphone elements, F (). <p> This is the well-known principle of pattern multiplication [9] <ref> [18] </ref>. For omnidirectional microphones, the gain patterns for the two layouts are identical but for a rotation. In our implementation, cardioid microphones were used and were placed in a broadside arrangement due to space constraints (see Figure 2). The polar response patterns for this arrangement are shown in Figure 4.
Reference: [19] <author> F. L. Wightman and D. J. Kistler. </author> <title> "Headphone simulation of free-field listening". </title> <journal> J. Acoust. Soc. Am., </journal> <volume> 85 </volume> <pages> 858-878, </pages> <year> 1989. </year>
Reference-contexts: Usually, the HRTFs are equalized to compensate for the headphone to ear frequency response <ref> [19, 13] </ref>. A schematic diagram of a single source system is shown in Figure 4.2. The direction of the source ( = azimuth, = elevation) determines which pair of HRTFs to use, and the distance (r) determines the gain.
Reference: [20] <author> Christopher Wren, Ali Azarbayejani, Trevor Darrell, and Alex Pentland. "Pfinder: </author> <title> Real-Time Tracking of the Human Body". </title> <booktitle> SPIE Photonics East, </booktitle> <volume> 2615 </volume> <pages> 89-98, </pages> <year> 1995. </year> <month> 6 </month>
Reference-contexts: transaural rendering. 2 Overview of the Vision System In order to frame our discussion, we first present a brief overview of Pfinder (Person finder), a real-time vision system for tracking and interpretation of people used in our interactive environment (for a more detailed account of the system, please refer to <ref> [20] </ref> and [10]). Pfinder has the capability to accurately determine the 3-D locations of the user's head and other features in real-time at a frame rate of 10Hz and an accuracy of 10cm. With two cameras (stereo Pfinder), the accuracy virtual environment can be refined to 1.5cm.
References-found: 20

