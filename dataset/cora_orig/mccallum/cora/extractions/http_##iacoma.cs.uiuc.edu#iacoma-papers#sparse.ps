URL: http://iacoma.cs.uiuc.edu/iacoma-papers/sparse.ps
Refering-URL: http://iacoma.cs.uiuc.edu/papers.html
Root-URL: http://www.cs.uiuc.edu
Email: -zhang2,torrella@cs.uiuc.edu  rwerger@cs.tamu.edu  
Title: Hardware for Speculative Run-Time Parallelization in Distributed Shared-Memory Multiprocessors 1  
Author: Ye Zhang Lawrence Rauchwerger and Josep Torrellas 
Web: http://iacoma.cs.uiuc.edu/iacoma/  
Address: IL 61801  Texas A&M University, College Station, TX 77843  
Affiliation: Computer Science Department University of Illinois at Urbana-Champaign,  Computer Science Department  
Abstract: Run-time parallelization is often the only way to execute the code in parallel when data dependence information is incomplete at compile time. This situation is common in many important applications. Unfortunately, known techniques for run-time parallelization are often computationally expensive or not general enough. To address this problem, we propose new hardware support for efficient run-time parallelization in distributed shared-memory (DSM) multiprocessors. The idea is to execute the code in parallel speculatively and use extensions to the cache coherence protocol hardware to detect any dependence violations. As soon as a dependence is detected, execution stops, the state is restored, and the code is re-executed serially. This scheme, which we apply to loops, allows iterations to execute and complete in potentially any order. This scheme requires hardware extensions to the cache coherence protocol and memory hierarchy of a DSM. It has low overhead. In this paper, we present the algorithms and a hardware design of the scheme. Overall, the scheme delivers average loop speedups of 7.3 for 16 processors and is 50% faster than a related software-only method. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> U. Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer Academic Publishers, Norwell, </publisher> <address> MA, </address> <year> 1988. </year>
Reference-contexts: To determine whether or not the order of the iterations affects the semantics of the loop, we need to analyze the data dependences across iterations (or cross-iteration dependences) <ref> [1] </ref>. There are three types of data dependences, namely flow (read after write), anti (write after read), and output (write after write). If there are no anti, output, or flow dependences across iterations, the loop can be executed in parallel. Such a loop is called a doall loop.
Reference: [2] <author> M. Berry et al. </author> <title> The Perfect Club Benchmarks: Effective Performance Evaluation of Supercomputers. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 3(3) </volume> <pages> 5-40, </pages> <month> Fall </month> <year> 1989. </year>
Reference-contexts: Finally, the time it takes to broadcast a cross-processor interrupt to all processors when a processor detects a dependence violation and the parallel execution must be aborted is 30 microseconds. 5.2 Loops To evaluate the proposed scheme, we use loops from applications in the Perfect Club set <ref> [2] </ref> and one application from the National Center for Supercomputing Applications. These applications are very large and cannot be run to completion with our limited simulation resources. Consequently, we test our scheme on individual loops within these applications.
Reference: [3] <author> W. Blume, R. Doallo, R. Eigenmann, J. Grout, J. Hoe-flinger, T. Lawrence, J. Lee, D. Padua, Y. Paek, B. Pot-tenger, L. Rauchwerger, and P. Tu. </author> <title> Advanced Program Restructuring for High-Performance Computers with Po-laris. </title> <journal> IEEE Computer, </journal> <volume> 29(12) </volume> <pages> 78-82, </pages> <month> December </month> <year> 1996. </year>
Reference-contexts: 1 Introduction While there has been much work on automatic extraction of parallelism at compile time for multiprocessors <ref> [3, 5, 8] </ref>, current parallelizing compilers often have only limited success. One of the reasons for this is that access patterns sometimes depend on the input data and, therefore, the information available at compile time is incomplete. This is common in applications with irregular domains or interactions. <p> The multiprocessor modeled has the hardware sup port described in Section 4.2 to implement the proposed hard-ware scheme. In addition, since we also evaluate the software scheme of Section 2.2, the simulator is interfaced directly to the output of the Polaris parallelizing compiler <ref> [3] </ref>. The architecture modeled has 200-MHz simple RISC processors. Each processor has a 32-Kbyte on-chip primary cache and a 512-Kbyte off-chip secondary cache. Both caches are direct-mapped and have 64-byte lines. The write buffers are 4-entry deep. <p> Consequently, we test our scheme on individual loops within these applications. The loops chosen must both account for a large fraction of the execution time and not be analyzable by Polaris. Polaris is one of the most advanced parallelizing compilers that currently exist <ref> [3] </ref>. It is equipped with leading-edge dependence analysis techniques. While examining individual loops is not an ideal situation, we feel that it gives a representative notion of what the proposed system can do.
Reference: [4] <author> D. K. Chen, J. Torrellas, and P. C. Yew. </author> <title> An Efficient Algorithm for the Run-Time Parallelization of Do-Across Loops. </title> <booktitle> In Supercomputing '94, </booktitle> <pages> pages 518-527, </pages> <month> November </month> <year> 1994. </year>
Reference: [5] <author> K. D. Cooper, M. W. Hall, R. T. Hood, K. Kennedy, K. S. M. Kinley, J. M. Mellor-Crummey, L. Torczon, and S. K. Warren. </author> <title> The ParaScope Parallel Programming Environment. </title> <journal> Proc. IEEE, </journal> <volume> 81(2) </volume> <pages> 244-263, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: 1 Introduction While there has been much work on automatic extraction of parallelism at compile time for multiprocessors <ref> [3, 5, 8] </ref>, current parallelizing compilers often have only limited success. One of the reasons for this is that access patterns sometimes depend on the input data and, therefore, the information available at compile time is incomplete. This is common in applications with irregular domains or interactions.
Reference: [6] <author> S. Goldschmidt. </author> <title> Simulation of Multiprocessors: Accuracy and Performance. </title> <type> Ph.D. Thesis, </type> <institution> Stanford University, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: In this section, we present our evaluation methodology and, in the next one, the results. 5.1 Simulation Environment Our evaluation is based on execution-driven simulations of a CC-NUMA shared-memory multiprocessor using Tango-lite <ref> [6] </ref>. The multiprocessor modeled has the hardware sup port described in Section 4.2 to implement the proposed hard-ware scheme. In addition, since we also evaluate the software scheme of Section 2.2, the simulator is interfaced directly to the output of the Polaris parallelizing compiler [3].
Reference: [7] <author> S. Gopal, T. N. Vijaykumar, J. E. Smith, and G. S. Sohi. </author> <title> Speculative Versioning Cache. </title> <booktitle> In Proceedings of the 4th International Symposium on High-Performance Computer Architecture, </booktitle> <month> February </month> <year> 1998. </year>
Reference-contexts: We show that the scheme delivers average loop speedups of 7.3 for 16 processors and is 50% faster than the software-only method described above. The scheme proposed is related to several concurrently-proposed schemes for speculative parallelization inside a multiprocessor chip <ref> [7, 11, 14] </ref>. While all schemes have in common that they add hardware extensions to the cache coherence protocol, ours aims at larger machines. <p> The hardware scheme is less sensitive to this problem. 7 Related Work The work most related is three schemes that have been proposed concurrently with ours <ref> [7, 11, 14] </ref> and which support speculative parallelization inside a multiprocessor chip. These schemes, proposed by Oplinger et al [11], Steffan and Mowry [14], and Gopal et al [7] are very similar to each other. <p> These schemes, proposed by Oplinger et al [11], Steffan and Mowry [14], and Gopal et al <ref> [7] </ref> are very similar to each other. The cache coherence protocol inside a chip is extended with versions or time stamps similar to ours. Parallelism is exploited by running one task (for example one loop iteration) on each of the processors on chip.
Reference: [8] <author> M. Hall, J. Anderson, S. Amarasinghe, B. Murphy, S.- W. Liao, E. Bugnion, and M. Lam. </author> <title> Maximizing Multiprocessor Performance with the SUIF Compiler. </title> <journal> IEEE Computer, </journal> <volume> 29(12) </volume> <pages> 84-89, </pages> <month> December </month> <year> 1996. </year>
Reference-contexts: 1 Introduction While there has been much work on automatic extraction of parallelism at compile time for multiprocessors <ref> [3, 5, 8] </ref>, current parallelizing compilers often have only limited success. One of the reasons for this is that access patterns sometimes depend on the input data and, therefore, the information available at compile time is incomplete. This is common in applications with irregular domains or interactions.
Reference: [9] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> The Directory-Based Cache Coherence Protocol for the DASH Multiprocessor. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Both caches are direct-mapped and have 64-byte lines. The write buffers are 4-entry deep. We selected such small caches because the only loops that we can run have smaller working sets than real-life ones. The caches are kept coherent with a DASH-like cache coherence protocol <ref> [9] </ref>. Each node has part of the global memory and the corresponding section of the directory. We model contention in the whole system except in the global network, which is given a fixed latency.
Reference: [10] <author> S.-T. Leung and J. Zahorjan. </author> <title> Improving the Performance of Runtime Parallelization. </title> <booktitle> In 4th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 83-91, </pages> <month> May </month> <year> 1993. </year>
Reference: [11] <author> J. Oplinger, D. Heine, S.-W. Liao, B. A. Nayfeh, M. S. Lam, and K. Olukotun. </author> <title> Software and Hardware for Exploiting Speculative Parallelism with a Multiprocessor. </title> <type> Technical Report CSL-TR-97-715, </type> <institution> Stanford University Computer Systems Laboratory, </institution> <month> February </month> <year> 1997. </year>
Reference-contexts: We show that the scheme delivers average loop speedups of 7.3 for 16 processors and is 50% faster than the software-only method described above. The scheme proposed is related to several concurrently-proposed schemes for speculative parallelization inside a multiprocessor chip <ref> [7, 11, 14] </ref>. While all schemes have in common that they add hardware extensions to the cache coherence protocol, ours aims at larger machines. <p> The hardware scheme is less sensitive to this problem. 7 Related Work The work most related is three schemes that have been proposed concurrently with ours <ref> [7, 11, 14] </ref> and which support speculative parallelization inside a multiprocessor chip. These schemes, proposed by Oplinger et al [11], Steffan and Mowry [14], and Gopal et al [7] are very similar to each other. <p> The hardware scheme is less sensitive to this problem. 7 Related Work The work most related is three schemes that have been proposed concurrently with ours [7, 11, 14] and which support speculative parallelization inside a multiprocessor chip. These schemes, proposed by Oplinger et al <ref> [11] </ref>, Steffan and Mowry [14], and Gopal et al [7] are very similar to each other. The cache coherence protocol inside a chip is extended with versions or time stamps similar to ours.
Reference: [12] <author> L. Rauchwerger. </author> <title> Run-Time Parallelization: A Framework for Parallel Computation. </title> <type> Ph.D. Thesis, </type> <institution> University of Illinois at Urbana-Champaign, Center for Supercomputing Research and Development, </institution> <month> September </month> <year> 1995. </year>
Reference-contexts: To parallelize these loops, the algorithm needs to use an extra shadow array (A w min (1 : s)) similar to the write shadow array <ref> [12] </ref>. parallelized by extending the algorithm. Under privatization, another issue is the ability to copy data selectively between the shared array under test and its private copies, and vice-versa.
Reference: [13] <author> L. Rauchwerger and D. Padua. </author> <title> The LRPD Test: Speculative Run-Time Parallelization of Loops with Privatiza-tion and Reduction Parallelization. </title> <booktitle> In Proceedings of the SIGPLAN 1995 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 218-232, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: This inspector-executor method is also applied to fully-parallel loops. Unfortunately, in general, the inspector may be both computationally expensive and have side-effects. Consequently, it can be argued that the inspector-executor approach is not a generally applicable method. Recently, we have introduced a framework for software run-time parallelization for multiprocessors <ref> [13] </ref>. It has two main characteristics. First, instead of finding a valid parallel execution schedule for the loop, it focuses on simply deciding whether or not the loop is fully parallel. Second, instead of distributing the loop into inspector and executor, it executes the loop speculatively as a doall. <p> 3 presents our hardware scheme, Section 4 shows an implementation of it, Section 5 discusses how we evaluate it, and Section 6 evaluates it. 2 Speculative Run-Time Paral- lelization in Software We have recently proposed the LRPD test, a new algorithm that uses speculation to parallelize loops at run time <ref> [13] </ref>. <p> However, if the pattern of access is sparse, it is better to save individual elements into a sparse storage data structure like a hash table just before they are modified. This is done in software <ref> [13] </ref>. Note that the compiler only needs to save modifiable shared arrays not read-only or privatized arrays. Finally, it is also possible to reduce the amount of backup requirements by identifying and checkpointing a point of minimum state in the program prior to the loop. <p> This is necessary to implement the marking phase efficiently <ref> [13] </ref>. There fore, if we want to support loops of up to 2 16 iterations, for example, we need 2 bytes per element for each shadow array. operations update the appropriate shadow arrays. The shadow arrays are shown in (c). <p> The shadow arrays are shown in (c). In the example, the test fails. 2.2.3 Implementation and Compiler Integration The implementation of this algorithm is described in <ref> [13] </ref>. In a DSM system, each processor allocates a private copy of the shadow arrays in its local memory. The marking phase is performed locally on the private copy. Then, as part of the analysis phase, the contents of the private shadow arrays are merged in parallel. <p> These copy operations are called read-ins. Furthermore, a relatively small fraction of loops requires that some elements from the private arrays be copied to the shared array after the loop finishes. These copies are called copy-outs <ref> [13] </ref>. Finally, the algorithm as presented detects cross-iteration dependences. We refer to it as the iteration-wise test. Sometimes, a processor-wise test, which tests only for cross-processor dependences, delivers higher performance. Checking only for cross-processor dependences does not require any algorithmic modifications. <p> This approach has two advantages. First, a loop that is not fully parallel passes the processor-wise test when data-dependent iterations are assigned to the same processor. Second, each entry in a shadow array now only needs to be one bit <ref> [13] </ref>. Consequently, shadow arrays are now accessed with bitmap operations, resulting in significant space savings. The disadvantage of the processor-wise test is that it requires blocked static iteration scheduling. This is necessary to insure that each processor gets a block of contiguous iterations. <p> We showed that the scheme delivers average loop speedups of 7.3 for 16 processors and is 50% faster than a related software-only scheme. Our hardware scheme can be easily integrated with reduction parallelization in the same way as the software scheme has been <ref> [13] </ref>. We have not presented the algorithm due to lack of space. The scheme can also be designed so that the data produced is regularly checkpointed to minimize the cost of failure. We are currently extending our work in two areas.
Reference: [14] <author> J. G. Steffan and T. C. Mowry. </author> <title> The Potential for Thread-Level Data Speculation in Tightly-Coupled Multiprocessors. </title> <type> Technical Report CSRI-TR-350, </type> <institution> Computer Science Research Institute, University of Toronto, </institution> <month> February </month> <year> 1997. </year>
Reference-contexts: We show that the scheme delivers average loop speedups of 7.3 for 16 processors and is 50% faster than the software-only method described above. The scheme proposed is related to several concurrently-proposed schemes for speculative parallelization inside a multiprocessor chip <ref> [7, 11, 14] </ref>. While all schemes have in common that they add hardware extensions to the cache coherence protocol, ours aims at larger machines. <p> The hardware scheme is less sensitive to this problem. 7 Related Work The work most related is three schemes that have been proposed concurrently with ours <ref> [7, 11, 14] </ref> and which support speculative parallelization inside a multiprocessor chip. These schemes, proposed by Oplinger et al [11], Steffan and Mowry [14], and Gopal et al [7] are very similar to each other. <p> The hardware scheme is less sensitive to this problem. 7 Related Work The work most related is three schemes that have been proposed concurrently with ours [7, 11, 14] and which support speculative parallelization inside a multiprocessor chip. These schemes, proposed by Oplinger et al [11], Steffan and Mowry <ref> [14] </ref>, and Gopal et al [7] are very similar to each other. The cache coherence protocol inside a chip is extended with versions or time stamps similar to ours. Parallelism is exploited by running one task (for example one loop iteration) on each of the processors on chip.
Reference: [15] <author> J. Wu, J. Saltz, S. Hiranandani, and H. Berryman. </author> <title> Run-time Compilation Methods for Multicomputers. </title> <editor> In D. H. Schwetman, editor, </editor> <booktitle> Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <pages> pages 26-30. </pages> <publisher> CRC Press, Inc., </publisher> <year> 1991. </year> <title> Vol. </title> <booktitle> II Software. </booktitle>
Reference: [16] <author> Y. Zhang, L. Rauchwerger, and J. Torrellas. </author> <title> Hardware for Speculative Run-Time Parallelization in Distributed Shared-Memory Multiprocessors. </title> <type> Technical Report 1523, </type> <institution> University of Illinois at Urbana-Champaign, Center for Supercomputing Research and Development, </institution> <month> July </month> <year> 1997. </year>
Reference-contexts: We can now include in the algorithms of Figure 4 the changes in the state of the cache tags. Because of lack of space, Figure 6 shows only the write transactions. The read transactions are shown in <ref> [16] </ref>. In the figure, the state in the directory and in the cache tags is denoted with the prefixes dir and tag respectively. In the home node, directory and memory are accessed at the same time. All algorithms assume in-order delivery of messages. <p> In the latter directory, the current iteration number is compared to MaxR1st. If the former is lower, the parallelization fails; otherwise, MinW is set to the minimum of its current value and the current iteration number. More details can be found in <ref> [16] </ref>. Since the private copies of the array under test start-off uninitialized, every time that a processor accesses a memory line for the first time, it performs a read-in. This is done by the protocol controller in the directory of the private array.
References-found: 16

