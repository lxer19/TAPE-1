URL: http://www.iro.umontreal.ca/labs/neuro/pointeurs/icpr-word.ps
Refering-URL: http://www.iro.umontreal.ca/labs/neuro/proc.html
Root-URL: http://www.iro.umontreal.ca
Title: Word-Level Training of a Handritten Word Recognizer based on Convolutional Neural Networks  
Author: Yann Le Cun Yoshua Bengio 
Keyword: Pattern Recognition and Neural Networks  
Note: Submitted to 12-ICPR, Jerusalem (Israel):  An applications paper (on-line handwriting recognition)  
Date: January 29, 1994  
Address: Holmdel, NJ 07733  U.S.A. Universite de Montreal Montreal, Qc H3C-3J7, Canada  
Affiliation: AT&T Bell Laboratories Dept. Informatique et  Recherche Operationnelle  
Abstract: We introduce a new approach for on-line recognition of handwritten words written in unconstrained mixed style. Words are represented by low resolution "annotated images" where each pixel contains information about trajectory direction and curvature. The recognizer is a convolution network which can be spatially replicated. From the network output, a hidden Markov model produces word scores. The entire system is globally trained to minimize word-level errors. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bengio, Y., R. De Mori and G. Flammia and R. Kompe. </author> <year> 1992. </year> <title> Global Optimization of a Neural Network-Hidden Markov Model Hybrid. </title> <journal> IEEE Transactions on Neural Networks v.3, nb.2, pp.252-259. </journal>
Reference-contexts: Experiments described in the next section have shown important reductions in error rates when training with this word-level criterion instead of just training the network separately for each character. Similar combinations of neural networks with HMMs or dynamic programming have been proposed in the past, for speech recognition problems <ref> (Bengio et al 92) </ref>. 6 Experimental Results In a first set of experiments, we evaluated the generalization ability of the neural network classifier coupled with the word normalization preprocessing and AMAP input representation. All results are in writer independent mode (different writers in training and testing).
Reference: <author> Bengio, Y. and Le Cun Y. </author> <year> 1994. </year> <title> Word Normalization For On-Line Handwritten Word Recog 8 nition. </title> <note> Submitted to ICPR'94, Jerusalem. </note>
Reference-contexts: We propose a new word normalization scheme, based on fitting a geometrical model of the word structure. Our model has four "flexible" lines representing respectively the ascenders line, the core line, the base line and the descenders line. See the companion paper <ref> (Bengio & Le Cun 94) </ref> for details of this model. Variables that associate each vertical extremum with one of the curves are taken as hidden variables of the EM algorithm.
Reference: <author> Burges, C., O. Matan, Y. Le Cun, J. Denker, L. Jackel, C. Stenard, C. Nohl and J. Ben. </author> <year> 1992. </year> <title> Shortest Path Segmentation: A Method for Training a Neural Network to Recognize character Strings. </title> <booktitle> Proc. IJCNN'92 (Baltimore), </booktitle> <pages> pp. 165-172, </pages> <month> v.3. </month>
Reference-contexts: In addition to recognizing characters, the system must also correctly segment the characters within the words. One approach, that we call INSEG, is to recognize a large number of heuristically segmented candidate characters and combine them optimally with a postprocessor <ref> (Burges et al 92, Schenkel et al 93) </ref>. Another approach, that we call OUTSEG, is to delay all segmentation decisions until after the recognition, as is often done in speech recognition.
Reference: <author> Guyon, I., Albrecht, P., Le Cun, Y., Denker, J. S., and Weissman, H. </author> <title> 1991 design of a neural network character recognizer for a touch terminal. </title> <journal> Pattern Recognition, </journal> <volume> 24(2) </volume> <pages> 105-119. </pages>
Reference-contexts: Trajectories are normalized, and local geometrical or dynamical features are sometimes extracted. The recognition is performed using curve matching (Tap-pert 90), or other classification techniques such as Neural Networks <ref> (Guyon et al 91) </ref>. While, as stated earlier, these representations have several advantages, their dependence on stroke ordering and individual writing styles makes them difficult to use in high accuracy, writer independent systems that integrate the segmentation with the recognition.
Reference: <author> Le Cun, Y. </author> <year> 1986. </year> <title> Learning Processes in an Asymmetric Threshold Network. </title> <editor> In Bienen-stock, E., Fogelman-Soulie, F., and Weisbuch, G., editors, </editor> <booktitle> Disordered systems and biological organization, </booktitle> <pages> pages 233-240, </pages> <address> Les Houches, France. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: MLCNNs are feed-forward neural networks whose architectures are tailored for minimizing the sensitivity to translations, rotations, or distortions of the input image. They are trained with a variation of the Back-Propagation algorithm <ref> (Rumelhart et al 86, Le Cun 86) </ref>. The units in MCLNNs are only connected to a local neighborhood in the previous layer. Each unit can be seen as a local feature detector whose function is determined by the learning procedure.
Reference: <author> Le Cun, Y. </author> <year> 1989. </year> <title> Generalization and Network Design Strategies. </title> <editor> In Pfeifer, R., Schreter, Z., Fogelman, F., and Steels, L., editors, </editor> <booktitle> Connectionism in Perspective, </booktitle> <address> Zurich, Switzerland. Elsevier. </address> <note> an extended version was published as a technical report of the University of Toronto. </note>
Reference-contexts: Unlike many other representations (such as global features), AMAPs can be computed for complete words without requiring segmentation. 4 Convolutional Neural Networks Image-like representations such as AMAPs are particularly well suited for use in combination with Multi-Layer Convolutional Neural Networks (MLCNN) <ref> (Le Cun 89, Le Cun et al 90) </ref>. MLCNNs are feed-forward neural networks whose architectures are tailored for minimizing the sensitivity to translations, rotations, or distortions of the input image. They are trained with a variation of the Back-Propagation algorithm (Rumelhart et al 86, Le Cun 86).
Reference: <author> Le Cun, Y., Matan, O., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., Jackel, L. D., and Baird, H. S. </author> <year> 1990. </year> <title> Handwritten Zip Code Recognition with Multilayer Networks. </title> <editor> In IAPR, editor, </editor> <booktitle> Proc. of the International Conference on Pattern Recognition, </booktitle> <address> Atlantic City. </address> <publisher> IEEE. </publisher>
Reference: <author> Matan, O., Burges, C. J. C., LeCun, Y., and Denker, J. S. </author> <year> 1992. </year> <title> Multi-Digit Recognition Using a Space Displacement Neural Network. </title> <editor> In Moody, J. M., Hanson, S. J., and Lipp-man, R. P., editors, </editor> <booktitle> Neural Information Processing Systems, </booktitle> <volume> volume 4. </volume> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: Instead of producing a single output vector, they produce a series of output vectors. The outputs detects and recognize characters at different (and overlapping) locations on the input. These multiple-input, multiple-output MLCNN are called Space Displacement Neural Networks (SDNN) <ref> (Matan et al 92) </ref> (see Figure 2).
Reference: <author> Rumelhart, D. E., Hinton, G. E., and Williams, R. J. </author> <year> 1986. </year> <title> Learning internal representations by error propagation. </title> <booktitle> In Parallel distributed processing: Explorations in the microstructure of cognition, </booktitle> <volume> volume I, </volume> <pages> pages 318-362. </pages> <publisher> Bradford Books, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: MLCNNs are feed-forward neural networks whose architectures are tailored for minimizing the sensitivity to translations, rotations, or distortions of the input image. They are trained with a variation of the Back-Propagation algorithm <ref> (Rumelhart et al 86, Le Cun 86) </ref>. The units in MCLNNs are only connected to a local neighborhood in the previous layer. Each unit can be seen as a local feature detector whose function is determined by the learning procedure.
Reference: <author> Schenkel, M., Guyon, I., Weissman, H., and Nohl, C. </author> <year> 1993. </year> <title> TDNN Solutions for Recognizing On-Line Natural Handwriting. </title> <booktitle> In Advances in Neural Information Processing Systems 5. </booktitle> <publisher> Morgan Kaufman. </publisher>
Reference: <author> Tappert, C., Suen, C., and Wakahara, T. </author> <year> 1990. </year> <title> The state of the art in on-line handwriting recognition. </title> <journal> IEEE Trans. PAMI, </journal> <volume> 12(8). </volume> <pages> 9 </pages>
References-found: 11

