URL: http://www.cs.purdue.edu/homes/spa/papers/scs.ps
Refering-URL: http://www.cs.purdue.edu/homes/spa/publications.html
Root-URL: http://www.cs.purdue.edu
Email: af1p@andrew.cmu.edu spa@cs.purdue.edu  
Title: GREEDY ALGORITHMS FOR THE SHORTEST COMMON SUPERSTRING THAT ARE ASYMPTOTICALLY OPTIMAL June 17, 1997  
Author: Alan Frieze Wojciech Szpankowski 
Address: Pittsburgh, PA 15213 W. Lafayette, IN 47907 U.S.A. U.S.A.  
Affiliation: Dept. of Mathematics Dept. of Computer Science Carnegie Mellon University Purdue University  
Abstract: There has recently been a resurgence of interest in the shortest common superstring problem due to its important applications in molecular biology (e.g., recombination of DNA) and data compression. The problem is NP-hard, but it has been known for some time that greedy algorithms work well for this problem. More precisely, it was proved in a recent sequence of papers that in the worst case a greedy algorithm produces a superstring that is at most fi times (2 fi 4) worse than optimal. We analyze the problem in a probabilistic framework, and consider the optimal total overlap O opt n and the overlap O gr n produced by various greedy algorithms. These turn out to be asymptotically equivalent. We show that with high probability lim n!1 O opt n O gr n H where n is the number of original strings, and H is the entropy of the underlying alphabet. Our results hold under a condition that the lengths of all strings are not too short. fl This work was supported by NSF Grant CCR-9225008. y This research was supported in part by NSF Grants CCR-9201078, NCR-9206315 and NCR-9415491, and in part by NATO Collaborative Grant CGR.950060. The author also thanks INRIA, Sophia Antipolis, project MISTRAL for hospitality and support during the summer of 1996 when this paper was completed. 1 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. Alexander, </author> <title> Shortest Common Superstring of Random Strings, </title> <journal> J. Appl. Probab., </journal> <volume> 33, </volume> <pages> 1112-1126, </pages> <year> 1996. </year>
Reference-contexts: More precisely, let n be the number of (long) strings. We assume that the lengths of all strings are (log n) (see below for a more precise formulation and relaxation of this assumption; cf. also <ref> [1] </ref>). Let also O opt n denote the optimal total overlap and let O gr n be that produced by various greedy algorithms. <p> The literature on worst-case analysis of SCS is impressive (cf. [3, 6, 8, 14, 17, 19, 28, 29]) 2 but probabilistic analysis of SCS is very scarce. Only recently, did Alexander <ref> [1] </ref> prove that the average optimal overlap in the Bernoulli model EO opt n ~ 1 H n log n. <p> The assumption regarding equal length strings is not relevant as long as there are enough long strings satisfying (4). A precise formulation of the proportion of short and long strings such that Theorem 1 still holds can be found in Alexander <ref> [1] </ref>. (iii) Markovian Model. <p> Lemma 1 (ii)) (b) jfi : M n (i) (1 + * 2 ) 1 H log ngj n 1* 2 =2 (cf. Lemma 1 (i)) (c) O opt H n log n. (cf. <ref> [1] </ref>) It follows from (29) that whp 1 * 2 n log n n 1* 2 =2 K log n + n MG H 1 * log n: Indeed, the RHS of the above bounds the total overlap if (a), (b) and (c) hold.
Reference: [2] <author> C.Armen and C.Stein, </author> <title> Short Superstrings and the Structure of Overlapping Strings, </title> <journal> Journal of Computational Biology, </journal> <note> to appear. </note>
Reference: [3] <author> C.Armen and C.Stein, </author> <title> A 2-2/3 Approximation Algorithm for the Shortest Superstring Problem, </title> <booktitle> Proc. Combinatorial Pattern Matching, </booktitle> <year> 1996. </year>
Reference-contexts: Thus constructing a good approximation to SCS is of prime interest. It has been shown recently, that a greedy algorithm can compute in O (n log n) time a superstring that in the worst case is only fi times (where 2 fi 4) longer than the shortest common superstring <ref> [3, 6, 8, 14, 17, 19, 28, 29] </ref>; see also [13]. Our results are also about greedy approximations of the shortest common superstring but in a probabilistic framework. <p> The mixing model includes the Bernoulli model, as well the Markovian model and the hidden Markov model (cf. [23, 27]). The literature on worst-case analysis of SCS is impressive (cf. <ref> [3, 6, 8, 14, 17, 19, 28, 29] </ref>) 2 but probabilistic analysis of SCS is very scarce. Only recently, did Alexander [1] prove that the average optimal overlap in the Bernoulli model EO opt n ~ 1 H n log n.
Reference: [4] <author> W. Bains and G. Smith, </author> <title> A Novel Method for Nucleic Acid Sequence Determination, </title> <journal> J. Theor. Biol., </journal> <volume> 135, </volume> <pages> 303-307, </pages> <year> 1988. </year>
Reference-contexts: 1 Introduction Various versions of the shortest common superstring (in short: SCS) problem play important roles in data compression and DNA sequencing. In fact, in laboratories DNA sequencing (cf. <ref> [4, 9, 18, 22] </ref>) is routinely done by sequencing large numbers of relatively short fragments, and then heuristically finding a short common superstring.
Reference: [5] <author> P. Billingsley, </author> <title> Ergodic Theory and Information, </title> <publisher> John Wiley & Sons, </publisher> <address> New York (1965). </address>
Reference-contexts: Thus, let us consider a generic string x (from the set x 1 ; : : : ; x n of strings), and let us assume that is generated by a stationary ergodic source. Then, it is well known that the entropy H can be defined as (cf. <ref> [5] </ref>) k!1 E log P (x k k k!1 log P (x k k Furthermore, we restrict somewhat the dependency among symbols of x, that is, we define the mixing model. Let x j i denote the substring x i x i+1 x j of x. <p> Let us start with (22) and (23). From the Shannon-McMillan-Breiman theorem for the relative frequency (cf. <ref> [5] </ref>), we know that almost surely (k) ! p t for any 1 t M . This would immediately imply that M (k; *) = O (n) where ! 0 as k ! 1, which is enough for our results to hold.
Reference: [6] <author> A. Blum, T. Jiang, M. Li, J. Tromp, M. Yannakakis, </author> <title> Linear Approximation of Shortest Superstring, </title> <journal> J. the ACM, </journal> <volume> 41, </volume> <pages> 630-647, </pages> <note> 1994; also STOC, 328-336, </note> <year> 1991. </year>
Reference-contexts: Thus constructing a good approximation to SCS is of prime interest. It has been shown recently, that a greedy algorithm can compute in O (n log n) time a superstring that in the worst case is only fi times (where 2 fi 4) longer than the shortest common superstring <ref> [3, 6, 8, 14, 17, 19, 28, 29] </ref>; see also [13]. Our results are also about greedy approximations of the shortest common superstring but in a probabilistic framework. <p> The mixing model includes the Bernoulli model, as well the Markovian model and the hidden Markov model (cf. [23, 27]). The literature on worst-case analysis of SCS is impressive (cf. <ref> [3, 6, 8, 14, 17, 19, 28, 29] </ref>) 2 but probabilistic analysis of SCS is very scarce. Only recently, did Alexander [1] prove that the average optimal overlap in the Bernoulli model EO opt n ~ 1 H n log n. <p> I (I n fx; yg) [ fzg; n O gr 6. until jIj = 1 We consider three variants: GREEDY: In Step 3, choose x 6= y in order to maximise o (x; y) (cf. <ref> [6] </ref>). RGREEDY: In Step 3, x is the string z produced in the previous iteration, while y is chosen in order to maximise o (x; y) = o (z; y). Our initial choice for x is x 1 . <p> As previously mentioned, Yang and Zhang [31] gave an analysis of MGREEDY. Our proof is much shorter, relying on Lemmas 3 and 4 and the following proposition: 14 Proposition 1 (Blum et. al. <ref> [6] </ref>) The cycles C 1 ; C 2 ; : : : ; C t are a maximum weight cycle cover and so w (C 1 ) + w (C 2 ) + + w (C t ) O opt One can also view GREEDY and MGREEDY as algorithms for finding
Reference: [7] <author> T.M. Cover and J.A. Thomas, </author> <title> Elements of Information Theory, </title> <publisher> John Wiley&Sons, </publisher> <address> New York (1991). </address>
Reference-contexts: We observe that h 2 is related to the so called Renyi second order entropy (cf. <ref> [7, 20] </ref>). Now, we are ready to formulate our generalization of Theorem 1. Theorem 2 Consider the Shortest Common Superstring problem under the mixing model (M). <p> Indeed, instead of storing all strings of total length n` we can store the Shortest Common Superstring and n pointers indicating the beginning of an original string (plus lengths of all strings). But, this does not provide optimal compression (which is known to be the entropy H <ref> [7] </ref>). To see this, let us compute the compression ratio C n which is defined as the ratio of the number of bits needed to transmit the compression code to the length of the original set of strings (i.e., n`). <p> The rate of convergence is O (1= p and the convergence also holds in moments. Proof. We first present a simple proof of (12). We observe that by Shannon-McMillan-Breiman <ref> [7] </ref> for any stationary and ergodic sequence the state space k of all sequences of length k can be partition into a set of "good states" G k and "bad states" B k such that for any " and large enough k we have P (B k ) " and for
Reference: [8] <author> A.Czumaj, L.Gasienic, M.Piotrow and W.Rytter, </author> <title> Parallel and Sequential Approximations of Shortest Superstrings, </title> <booktitle> Proceedings of the Fourth Scandinavian Workshop on Algorithm Theory, </booktitle> <pages> 95-106, </pages> <year> 1994. </year>
Reference-contexts: Thus constructing a good approximation to SCS is of prime interest. It has been shown recently, that a greedy algorithm can compute in O (n log n) time a superstring that in the worst case is only fi times (where 2 fi 4) longer than the shortest common superstring <ref> [3, 6, 8, 14, 17, 19, 28, 29] </ref>; see also [13]. Our results are also about greedy approximations of the shortest common superstring but in a probabilistic framework. <p> The mixing model includes the Bernoulli model, as well the Markovian model and the hidden Markov model (cf. [23, 27]). The literature on worst-case analysis of SCS is impressive (cf. <ref> [3, 6, 8, 14, 17, 19, 28, 29] </ref>) 2 but probabilistic analysis of SCS is very scarce. Only recently, did Alexander [1] prove that the average optimal overlap in the Bernoulli model EO opt n ~ 1 H n log n.
Reference: [9] <author> R. Drmanac and C. Crkvenjakov, </author> <title> Sequencing by Hybridization (SBH) with Oligonu-cloide Probes as an Integral Approach for the Analysis of Complex Genome, </title> <journal> Int. J. genomic Research, </journal> <volume> 1, </volume> <pages> 59-79, </pages> <year> 1992. </year> <month> 18 </month>
Reference-contexts: 1 Introduction Various versions of the shortest common superstring (in short: SCS) problem play important roles in data compression and DNA sequencing. In fact, in laboratories DNA sequencing (cf. <ref> [4, 9, 18, 22] </ref>) is routinely done by sequencing large numbers of relatively short fragments, and then heuristically finding a short common superstring.
Reference: [10] <author> P. Flajolet and R. Sedgewick, </author> <title> Mellin Transforms and Asymptotics: Finite Differences and Rice's Integrals, </title> <journal> Theoretical Computer Science, </journal> <volume> 144, </volume> <pages> 101-124, </pages> <year> 1995. </year>
Reference-contexts: To deal efficiently with such sums we use a Mellin-like approach (cf. <ref> [10, 15, 25] </ref>).
Reference: [11] <author> J.Gallant, D.Maier and J.A.Storer, </author> <title> On Finding Minimal Length Superstrings, </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 20, </volume> <pages> 50-58, </pages> <year> 1980. </year>
Reference-contexts: We call it an approximate SCS and one asks for a superstring that contains approximately (e.g., in the Hamming distance sense) the original strings x 1 ; x 2 ; : : : ; x n as substrings. It is known that computing the shortest common superstring is NP-hard, <ref> [11] </ref>. Thus constructing a good approximation to SCS is of prime interest.
Reference: [12] <author> P. Jacquet and W. Szpankowski, </author> <title> Analysis of Digital Tries with Markovian Dependency, </title> <journal> IEEE Trans. on Information Theory, </journal> <volume> 37, </volume> <pages> 1470-1475, </pages> <year> 1991. </year>
Reference-contexts: The following lemma summarizes our knowledge of M n as well as the height H n , and suffices to prove an upper bound on O opt n . We point out that M n has been studied before in several papers devoted to tries (e.g., <ref> [12, 15, 23] </ref>), while H n is distributed as the height of a trie built from x 1 ; : : : ; x n (cf. [23, 26, 27]). <p> Now, we proceed to prove part (iii) for the Bernoulli model, however, one can extend these results to the Markovian model (cf. <ref> [12] </ref>). For simplicity of presentation, we now work on a binary alphabet with p 1 = p and p 2 = q = 1 p. <p> Details can be found in <ref> [12] </ref>. 3.2 Lower Bounds on O gr n in the Bernoulli Model In this subsection we prove lower bounds on O gr n only for the Bernoulli model (i.e., we complete the proof of Theorem 1).
Reference: [13] <author> T. Jiang and M. Li, </author> <title> Approximating Shortest Superstring with Constraints, </title> <booktitle> WADS, </booktitle> <pages> 385-396, </pages> <address> Montreal 1993. </address>
Reference-contexts: It has been shown recently, that a greedy algorithm can compute in O (n log n) time a superstring that in the worst case is only fi times (where 2 fi 4) longer than the shortest common superstring [3, 6, 8, 14, 17, 19, 28, 29]; see also <ref> [13] </ref>. Our results are also about greedy approximations of the shortest common superstring but in a probabilistic framework.
Reference: [14] <author> T.Jiang, Z.Jiang and D.Breslauer, </author> <title> Rotation of Periodic Strings and Short Superstrings, </title> <booktitle> Proceedings of the Third South American Conference on String Processing, to appear. </booktitle>
Reference-contexts: Thus constructing a good approximation to SCS is of prime interest. It has been shown recently, that a greedy algorithm can compute in O (n log n) time a superstring that in the worst case is only fi times (where 2 fi 4) longer than the shortest common superstring <ref> [3, 6, 8, 14, 17, 19, 28, 29] </ref>; see also [13]. Our results are also about greedy approximations of the shortest common superstring but in a probabilistic framework. <p> The mixing model includes the Bernoulli model, as well the Markovian model and the hidden Markov model (cf. [23, 27]). The literature on worst-case analysis of SCS is impressive (cf. <ref> [3, 6, 8, 14, 17, 19, 28, 29] </ref>) 2 but probabilistic analysis of SCS is very scarce. Only recently, did Alexander [1] prove that the average optimal overlap in the Bernoulli model EO opt n ~ 1 H n log n.
Reference: [15] <author> D. E. Knuth, </author> <title> The Art of Computer Programming. Sorting and Searching, </title> <publisher> Addison-Wesley 1973. </publisher>
Reference-contexts: The following lemma summarizes our knowledge of M n as well as the height H n , and suffices to prove an upper bound on O opt n . We point out that M n has been studied before in several papers devoted to tries (e.g., <ref> [12, 15, 23] </ref>), while H n is distributed as the height of a trie built from x 1 ; : : : ; x n (cf. [23, 26, 27]). <p> To deal efficiently with such sums we use a Mellin-like approach (cf. <ref> [10, 15, 25] </ref>). <p> Then, (15) and (16) are direct consequences of the above and the Cauchy residue theorem. The limiting distribution part (i.e., (17)) follows from the above and Goncharov's theorem (cf. <ref> [15] </ref>) which states that M n are normally distributed if for a complex lim e n = n G n (e = n ) = e 2 2 where n = EM n and n = p VarM n .
Reference: [16] <author> D. E. Knuth, Motwani, and B. Pittel, </author> <title> Stable Husbands, Random Structures and Algorithms, </title> <booktitle> 1, </booktitle> <pages> 1-14, </pages> <year> 1990. </year>
Reference-contexts: Indeed we could delay generating b t = (x t ) until after x t has been chosen as y in Step 3. This idea has been labelled the method of deferred decisions by Knuth, Motwani and Pittel <ref> [16] </ref>. Thus at the end of an execution of an iteration of RGREEDY: Lemma 2 (z) is random and independent of the previous history of the algorithm. We continue by examining the likely shape of the strings (x 1 ); : : : ; (x n ).
Reference: [17] <author> S.R.Kosaraju, J.K.Park and C.Stein, </author> <title> Long Tours and Short Superstrings, </title> <booktitle> Proceedings of the 35th Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> 166-177, </pages> <year> 1994. </year>
Reference-contexts: Thus constructing a good approximation to SCS is of prime interest. It has been shown recently, that a greedy algorithm can compute in O (n log n) time a superstring that in the worst case is only fi times (where 2 fi 4) longer than the shortest common superstring <ref> [3, 6, 8, 14, 17, 19, 28, 29] </ref>; see also [13]. Our results are also about greedy approximations of the shortest common superstring but in a probabilistic framework. <p> The mixing model includes the Bernoulli model, as well the Markovian model and the hidden Markov model (cf. [23, 27]). The literature on worst-case analysis of SCS is impressive (cf. <ref> [3, 6, 8, 14, 17, 19, 28, 29] </ref>) 2 but probabilistic analysis of SCS is very scarce. Only recently, did Alexander [1] prove that the average optimal overlap in the Bernoulli model EO opt n ~ 1 H n log n.
Reference: [18] <author> A. Lesek (Ed.), </author> <title> Computational Molecular Biology, Sources and Methods for Sequence Analysis, </title> <publisher> Oxford University Press, </publisher> <year> 1988. </year>
Reference-contexts: 1 Introduction Various versions of the shortest common superstring (in short: SCS) problem play important roles in data compression and DNA sequencing. In fact, in laboratories DNA sequencing (cf. <ref> [4, 9, 18, 22] </ref>) is routinely done by sequencing large numbers of relatively short fragments, and then heuristically finding a short common superstring.
Reference: [19] <author> Ming Li, </author> <title> Towards a DNA Sequencing Theory, </title> <booktitle> Proc. of 31st IEEE Symp. on Foundation of Computer Science, </booktitle> <month> 125-134 </month> <year> 1990. </year>
Reference-contexts: Thus constructing a good approximation to SCS is of prime interest. It has been shown recently, that a greedy algorithm can compute in O (n log n) time a superstring that in the worst case is only fi times (where 2 fi 4) longer than the shortest common superstring <ref> [3, 6, 8, 14, 17, 19, 28, 29] </ref>; see also [13]. Our results are also about greedy approximations of the shortest common superstring but in a probabilistic framework. <p> The mixing model includes the Bernoulli model, as well the Markovian model and the hidden Markov model (cf. [23, 27]). The literature on worst-case analysis of SCS is impressive (cf. <ref> [3, 6, 8, 14, 17, 19, 28, 29] </ref>) 2 but probabilistic analysis of SCS is very scarce. Only recently, did Alexander [1] prove that the average optimal overlap in the Bernoulli model EO opt n ~ 1 H n log n.
Reference: [20] <author> T. Luczak and W. Szpankowski, </author> <title> A Lossy Data Compression Based on an Approximate Pattern Matching, </title> <journal> IEEE Trans. Information Theory, </journal> <note> to appear. </note>
Reference-contexts: We observe that h 2 is related to the so called Renyi second order entropy (cf. <ref> [7, 20] </ref>). Now, we are ready to formulate our generalization of Theorem 1. Theorem 2 Consider the Shortest Common Superstring problem under the mixing model (M). <p> Yang and Zhang [31] proved that this constant is the reverse of the so called lower mutual information, provided the lengths of the strings are not too short (i.e., ` &gt; 4 r 1 (D) log n, where r 1 (D) the so called second generalized Renyi's entropy defined in <ref> [20] </ref>). (vi) Limiting Distribution ?. Theorem 2 presents only a convergence in probability, and might insufficient for some applications. We, therefore, conjecture that a stronger result is also true, namely, the central limit theorem.
Reference: [21] <author> K. Marton and P. Shields, </author> <title> The Positive-Divergence and Blowing-up Properties, </title> <journal> Israel J. Math, </journal> <volume> 80, </volume> <month> 331-348 </month> <year> (1994). </year>
Reference-contexts: This would immediately imply that M (k; *) = O (n) where ! 0 as k ! 1, which is enough for our results to hold. For general, stationary ergodic sequences the probability can decay to zero quite slowly. However, Marton and Shields <ref> [21] </ref> have proved recently that (k)=k converges exponentially to p t for processes satisfying the so called blowing-up property which can be stated as follows (cf. [21]: a stationary and ergodic process has the blowing-up property if for any " &gt; 0 there exists a ffi &gt; 0 and integer N <p> For general, stationary ergodic sequences the probability can decay to zero quite slowly. However, Marton and Shields <ref> [21] </ref> have proved recently that (k)=k converges exponentially to p t for processes satisfying the so called blowing-up property which can be stated as follows (cf. [21]: a stationary and ergodic process has the blowing-up property if for any " &gt; 0 there exists a ffi &gt; 0 and integer N such 17 that for any n N and any B n PfBg e nffi =) Pf [B] " g 1 " where [B] " is a
Reference: [22] <author> P. Pevzner, </author> <title> l-tuple DNA Sequencing: </title> <journal> Computer Analysis, J. Biomolecular Structure and Dynamics, </journal> <volume> 7, </volume> <pages> 63-73, </pages> <year> 1989. </year>
Reference-contexts: 1 Introduction Various versions of the shortest common superstring (in short: SCS) problem play important roles in data compression and DNA sequencing. In fact, in laboratories DNA sequencing (cf. <ref> [4, 9, 18, 22] </ref>) is routinely done by sequencing large numbers of relatively short fragments, and then heuristically finding a short common superstring.
Reference: [23] <author> B. Pittel, </author> <title> Asymptotic Growth of a Class of Random Trees, </title> <journal> Ann. Probab., </journal> <volume> 18, 414 - 427, </volume> <year> 1985. </year>
Reference-contexts: Later, we extend the main results to the so called mixing model in which the dependency among symbols decays rapidly as the symbols are further away of each others. The mixing model includes the Bernoulli model, as well the Markovian model and the hidden Markov model (cf. <ref> [23, 27] </ref>). The literature on worst-case analysis of SCS is impressive (cf. [3, 6, 8, 14, 17, 19, 28, 29]) 2 but probabilistic analysis of SCS is very scarce. <p> (A)P (B) P (AB) (1 + ff (g))P (A)P (B): (6) In such a model, we introduce a new parameter h 2 defined as h 2 = lim log (EfP (x k k k!1 P 1 2 k P 2 (x k (7) which can be proved to exists (cf. <ref> [23, 27] </ref>). We observe that h 2 is related to the so called Renyi second order entropy (cf. [7, 20]). Now, we are ready to formulate our generalization of Theorem 1. Theorem 2 Consider the Shortest Common Superstring problem under the mixing model (M). <p> It is also well known that the entropy H can be computed as H = P M i;j=1 i p i;j log p i;j where i is the stationary distribution of the Markov chain. The quantity h 2 is a little harder to compute, as already pointed out in <ref> [23, 27] </ref>. It turns out that h 2 = log where is the largest eigenvalue of the Schur product of the transition matrix of the underlying Markov chain with itself (that is, element-wise product). (iv) SCS Does Not Compress Optimally. The SCS can be used to compress strings. <p> The following lemma summarizes our knowledge of M n as well as the height H n , and suffices to prove an upper bound on O opt n . We point out that M n has been studied before in several papers devoted to tries (e.g., <ref> [12, 15, 23] </ref>), while H n is distributed as the height of a trie built from x 1 ; : : : ; x n (cf. [23, 26, 27]). <p> We point out that M n has been studied before in several papers devoted to tries (e.g., [12, 15, 23]), while H n is distributed as the height of a trie built from x 1 ; : : : ; x n (cf. <ref> [23, 26, 27] </ref>). For the proof of the upper bound of Theorem 2, we need only part (i) of the lemma below, while part (ii) is used in subsection 3.3 to establish a restriction on the string lengths. <p> The proof of part (ii) is not much harder, and can be found in <ref> [23, 26] </ref>: For an upper bound, one derives: P (H n &gt; k) n 2 w k 2 k where w k 2 k denotes a fixed string of length k.
Reference: [24] <author> P. Shields, </author> <title> Entropy and Prefixes, </title> <journal> Ann. Probab., </journal> <volume> 20, </volume> <pages> 403-409, </pages> <year> 1992. </year>
Reference: [25] <author> W. Szpankowski, </author> <title> The Evaluation of an Alternating Sum with Applications to the Analysis of Some Data Structures, </title> <journal> Information Processing Letters, </journal> <volume> 28, </volume> <pages> 13-19, </pages> <year> 1988. </year>
Reference-contexts: To deal efficiently with such sums we use a Mellin-like approach (cf. <ref> [10, 15, 25] </ref>).
Reference: [26] <author> W. Szpankowski, </author> <title> On the Height of Digital Trees and Related Problems, </title> <journal> Algorithmica, </journal> <volume> 6, </volume> <month> 256-277 </month> <year> (1991). </year>
Reference-contexts: We point out that M n has been studied before in several papers devoted to tries (e.g., [12, 15, 23]), while H n is distributed as the height of a trie built from x 1 ; : : : ; x n (cf. <ref> [23, 26, 27] </ref>). For the proof of the upper bound of Theorem 2, we need only part (i) of the lemma below, while part (ii) is used in subsection 3.3 to establish a restriction on the string lengths. <p> The proof of part (ii) is not much harder, and can be found in <ref> [23, 26] </ref>: For an upper bound, one derives: P (H n &gt; k) n 2 w k 2 k where w k 2 k denotes a fixed string of length k. <p> Then, P (H n &gt; k) = P @ i;j=1 1 P 2 i;j P (A ij ) + i;j6=l;m P (A ij " A lm ) where the last inequality follows from the second moment inequality (see for example <ref> [26] </ref>). The above probabilities are easy to evaluate, and the reader is referred to [26, 27] for details (in fact, for the results of this paper, we only need an upper bound on H n ). <p> The above probabilities are easy to evaluate, and the reader is referred to <ref> [26, 27] </ref> for details (in fact, for the results of this paper, we only need an upper bound on H n ). Now, we proceed to prove part (iii) for the Bernoulli model, however, one can extend these results to the Markovian model (cf. [12]).
Reference: [27] <author> W. Szpankowski, </author> <title> A Generalized Suffix Tree and its (Un)Expected Asymptotic Behav--iors, </title> <journal> SIAM J. Computing, </journal> <volume> 22, </volume> <pages> pp. 1176-1198, </pages> <year> 1993. </year>
Reference-contexts: Later, we extend the main results to the so called mixing model in which the dependency among symbols decays rapidly as the symbols are further away of each others. The mixing model includes the Bernoulli model, as well the Markovian model and the hidden Markov model (cf. <ref> [23, 27] </ref>). The literature on worst-case analysis of SCS is impressive (cf. [3, 6, 8, 14, 17, 19, 28, 29]) 2 but probabilistic analysis of SCS is very scarce. <p> (A)P (B) P (AB) (1 + ff (g))P (A)P (B): (6) In such a model, we introduce a new parameter h 2 defined as h 2 = lim log (EfP (x k k k!1 P 1 2 k P 2 (x k (7) which can be proved to exists (cf. <ref> [23, 27] </ref>). We observe that h 2 is related to the so called Renyi second order entropy (cf. [7, 20]). Now, we are ready to formulate our generalization of Theorem 1. Theorem 2 Consider the Shortest Common Superstring problem under the mixing model (M). <p> It is also well known that the entropy H can be computed as H = P M i;j=1 i p i;j log p i;j where i is the stationary distribution of the Markov chain. The quantity h 2 is a little harder to compute, as already pointed out in <ref> [23, 27] </ref>. It turns out that h 2 = log where is the largest eigenvalue of the Schur product of the transition matrix of the underlying Markov chain with itself (that is, element-wise product). (iv) SCS Does Not Compress Optimally. The SCS can be used to compress strings. <p> We point out that M n has been studied before in several papers devoted to tries (e.g., [12, 15, 23]), while H n is distributed as the height of a trie built from x 1 ; : : : ; x n (cf. <ref> [23, 26, 27] </ref>). For the proof of the upper bound of Theorem 2, we need only part (i) of the lemma below, while part (ii) is used in subsection 3.3 to establish a restriction on the string lengths. <p> The above probabilities are easy to evaluate, and the reader is referred to <ref> [26, 27] </ref> for details (in fact, for the results of this paper, we only need an upper bound on H n ). Now, we proceed to prove part (iii) for the Bernoulli model, however, one can extend these results to the Markovian model (cf. [12]).
Reference: [28] <author> S. Teng and F. Yao, </author> <title> Approximating Shortest Superstring, </title> <booktitle> Proc. FOCS, </booktitle> <pages> 158-165, </pages> <year> 1993. </year>
Reference-contexts: Thus constructing a good approximation to SCS is of prime interest. It has been shown recently, that a greedy algorithm can compute in O (n log n) time a superstring that in the worst case is only fi times (where 2 fi 4) longer than the shortest common superstring <ref> [3, 6, 8, 14, 17, 19, 28, 29] </ref>; see also [13]. Our results are also about greedy approximations of the shortest common superstring but in a probabilistic framework. <p> The mixing model includes the Bernoulli model, as well the Markovian model and the hidden Markov model (cf. [23, 27]). The literature on worst-case analysis of SCS is impressive (cf. <ref> [3, 6, 8, 14, 17, 19, 28, 29] </ref>) 2 but probabilistic analysis of SCS is very scarce. Only recently, did Alexander [1] prove that the average optimal overlap in the Bernoulli model EO opt n ~ 1 H n log n.
Reference: [29] <author> E. Ukkonen, </author> <title> A Linear-Time Algorithm for Finding Approximate Shortest Common Superstrings, </title> <journal> Algorithmica, </journal> <volume> 5, </volume> <pages> 313-323, </pages> <year> 1990. </year>
Reference-contexts: Thus constructing a good approximation to SCS is of prime interest. It has been shown recently, that a greedy algorithm can compute in O (n log n) time a superstring that in the worst case is only fi times (where 2 fi 4) longer than the shortest common superstring <ref> [3, 6, 8, 14, 17, 19, 28, 29] </ref>; see also [13]. Our results are also about greedy approximations of the shortest common superstring but in a probabilistic framework. <p> The mixing model includes the Bernoulli model, as well the Markovian model and the hidden Markov model (cf. [23, 27]). The literature on worst-case analysis of SCS is impressive (cf. <ref> [3, 6, 8, 14, 17, 19, 28, 29] </ref>) 2 but probabilistic analysis of SCS is very scarce. Only recently, did Alexander [1] prove that the average optimal overlap in the Bernoulli model EO opt n ~ 1 H n log n.
Reference: [30] <author> E. Ukkonen, </author> <title> Approximate String-Matching over Suffix Trees, </title> <booktitle> Proc. Combinatorial Pattern Matching, </booktitle> <pages> 228-242, </pages> <address> Padova, </address> <year> 1993. </year>
Reference: [31] <author> E-H. Yang and Z. Zhang, </author> <title> The Shortest Common Superstring Problem: Average Case Analysis for Both Exact Matching and Approximate Matching, </title> <type> preprint. 20 </type>
Reference-contexts: Only recently, did Alexander [1] prove that the average optimal overlap in the Bernoulli model EO opt n ~ 1 H n log n. After a preliminary version of this paper was published as a technical report, Yang and Zhang <ref> [31] </ref> extended some of our results, and subsequently in this paper we provide a shorter proof for some of [31] results as well as extend some other results of [31] (cf. Remark (i) in Section 2). <p> After a preliminary version of this paper was published as a technical report, Yang and Zhang <ref> [31] </ref> extended some of our results, and subsequently in this paper we provide a shorter proof for some of [31] results as well as extend some other results of [31] (cf. Remark (i) in Section 2). This paper is organized as follows: In the next section we present our main results: First, we discuss only the Bernoulli model which is later extended to the mixing model. <p> After a preliminary version of this paper was published as a technical report, Yang and Zhang <ref> [31] </ref> extended some of our results, and subsequently in this paper we provide a shorter proof for some of [31] results as well as extend some other results of [31] (cf. Remark (i) in Section 2). This paper is organized as follows: In the next section we present our main results: First, we discuss only the Bernoulli model which is later extended to the mixing model. The proof is delayed till Section 3. <p> Here, C is a set of strings, and we see later that C corresponds to a set of cycles in an associated digraph. On termination we add the final string left in I to C (cf. <ref> [31] </ref>). In GREEDY and RGREEDY the output is the final string left in the set I. In MGREEDY the output is an arbitrary catenation of the strings in C. We will assume that the input strings are independently generated. <p> Remarks and Extensions (i) In the original version of this paper we proved Theorem 1 for the algorithm RGREEDY. Subsequently, Yang and Zhang <ref> [31] </ref> extended it to include MGREEDY. In this paper we give a shorter proof of this along with a proof for GREEDY as well. (ii) Not Equal Length Strings. The assumption regarding equal length strings is not relevant as long as there are enough long strings satisfying (4). <p> Thus, for not too large D, we conjecture that also for the Approximate SCS the optimal and greedy overlaps are asymptotically equivalent. However, the constant in front of n log n is not any longer the entropy H. Recently, Yang and Zhang <ref> [31] </ref> proved that this constant is the reverse of the so called lower mutual information, provided the lengths of the strings are not too short (i.e., ` &gt; 4 r 1 (D) log n, where r 1 (D) the so called second generalized Renyi's entropy defined in [20]). (vi) Limiting Distribution <p> Let P j = C j f j . The catenation of paths P 1 ; P 2 ; : : : ; P t define a superstring of the input. As previously mentioned, Yang and Zhang <ref> [31] </ref> gave an analysis of MGREEDY.
References-found: 31

