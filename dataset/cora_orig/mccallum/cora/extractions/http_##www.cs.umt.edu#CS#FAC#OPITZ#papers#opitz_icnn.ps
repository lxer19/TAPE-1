URL: http://www.cs.umt.edu/CS/FAC/OPITZ/papers/opitz_icnn.ps
Refering-URL: http://www.cs.umt.edu/CS/FAC/OPITZ/
Root-URL: 
Email: opitz@d.umn.edu  craven@cs.cmu.edu  shavlik@cs.wisc.edu  
Title: Using Neural Networks to Automatically Refine Expert System Knowledge Bases: Experiments in the NYNEX MAX Domain  
Author: David W. Opitz Mark W. Craven mark Jude W. Shavlik 
Address: Missoula, MT 59812  5000 Forbes Avenue Pittsburgh, PA 15213  1210 W. Dayton St. Madison, WI 53706  
Affiliation: Department of Computer Science University of Montana  School of Computer Science Carnegie Mellon University  Computer Sciences Department University of Wisconsin  
Abstract: In this paper we describe our study of applying knowledge-based neural networks to the problem of diagnosing faults in local telephone loops. Currently, NYNEX uses an expert system called MAX to aid human experts in diagnosing these faults; however, having an effective learning algorithm in place of MAX would allow easy portability between different maintenance centers, and easy updating when the phone equipment changes. We find that (i) machine learning algorithms have better accuracy than MAX, (ii) neural networks perform better than decision trees, (iii) neural network ensembles perform better than standard neural networks, (iv) knowledge-based neural networks perform better than standard neural networks, and (v) an ensemble of knowledge-based neural networks performs the best. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Craven and J. Shavlik. </author> <title> Extracting tree-structured representations of trained networks. </title> <editor> In D. Touretsky, M. Mozer, and M. Hasselmo, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, volume 8, </booktitle> <address> Cambridge, MA, 1996. </address> <publisher> MIT Press. </publisher>
Reference-contexts: The ADDEMUP algorithm extends KBANN by using genetic search techniques to generate an ensemble of knowledge-based neural networks. An ensemble is a set of separately trained classifiers whose predictions are combined to classify test cases. Finally, we also applied the TREPAN algorithm <ref> [1] </ref> to the ensembles produced by ADDEMUP. TREPAN is an algorithm for extracting comprehensible decision trees from hard-to-understand classifiers, such as the ensembles produced by ADDEMUP.
Reference: [2] <author> P. M. Murphy and M. J. Pazzani. ID2-of-3: </author> <title> Constructive induction of M-of-N concepts for discriminators in decision trees. </title> <booktitle> In Proceedings of the Eighth International Machine Learning Workshop, </booktitle> <pages> pages 183-187, </pages> <address> Evanston, IL, 1991. </address> <publisher> Mor-gan Kaufmann. </publisher>
Reference-contexts: Decision-tree complexity. algorithm internal nodes leaves symbols C4.5 44.3 27.8 47.5 28.9 44.3 27.8 TREPAN 5.8 0.9 6.8 0.9 20.0 4.7 trees than does C4.5. Whereas each split in a C4.5 tree tests only a single attribute, TREPAN's trees can use m-of-n splits <ref> [2] </ref>. An m-of-n split is a Boolean expression that is true if at least m out of a specified set of n conditions are true.
Reference: [3] <author> D. Opitz and J. Shavlik. </author> <title> Generating accurate and diverse members of a neural-network ensemble. </title> <editor> In D. Touretsky, M. Mozer, and M. Hasselmo, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, volume 8, </booktitle> <address> Cambridge, MA, 1996. </address> <publisher> MIT Press. </publisher>
Reference-contexts: The first neural-network learning algorithm we used was backpropagation [7] applied to feed-forward networks with sigmoidal hidden units. The networks we used have a single layer of 25 hidden units, and are fully connected between layers. We also used two knowledge-based network algorithms: KBANN [8] and ADDEMUP <ref> [3] </ref>. In a knowledge-based network, the topology and initial weights of the network are specified by a domain theory consisting of symbolic inference rules. The domain theory used in our experiments was adapted by us from MAX's knowledge base of 75 ART rules.
Reference: [4] <author> F. Provost and A. Danyluk. </author> <title> Learning from bad data. </title> <booktitle> In Workshop on Applying Machine Learning in Practice, held at the Twelfth International Conference on Machine Learning, </booktitle> <address> Tahoe City, CA, </address> <year> 1995. </year>
Reference-contexts: Our results indicate that knowledge-based neural networks are able to provide a small, but statistically significant improvement in generalization performance over ordinary neural networks and C4.5. This is important since even a small improvement in MAX's accuracy can be extremely valuable <ref> [4] </ref>. Additionally, we report on the application of an algorithm we have developed, called TREPAN, that extracts decision trees from trained networks so that the concepts learned by the networks are more readily understood. <p> Section 5.2 discusses these experiments. 2 The Task MAX is an expert system that was designed by NYNEX to diagnose the location of customer-reported telephone problems. Figure 1 illustrates MAX's task (refer to Rabinowitz et al. [6] and Provost and Danyluk <ref> [4] </ref> for more information). When a customer calls with a phone-service problem, a representative invokes a mechanized loop test (MLT) to create an electronic profile of the voltages and resistances in the loop between the customer's telephone and the central office. <p> In fact, for every percentage point reduction in dispatch error rate, $3 million annually is saved by the company <ref> [4] </ref>. 3 Algorithms We applied five different machine-learning algorithms to this telephone-loop diagnosis task. The first algorithm we used was Quinlan's C4.5 [5]. C4.5 is a system for inducing decision-trees from training examples. <p> where the target is provided by the repair technician, and (ii) one where the target is provided by domain experts hired to hand-analyze the data. 5.1 Experimental Results: Matching Repair Technicians The first data set we use is a cleaned-up version of dispatching customer problems used by Provost and Dany-luk <ref> [4] </ref>. In this set, the target dispatch for an example is the trouble reported by the repair technician. Any instance where the correct dispatch was deemed as being highly questionable was removed from a larger data set, and the remaining examples make up this data set.
Reference: [5] <author> J. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: In fact, for every percentage point reduction in dispatch error rate, $3 million annually is saved by the company [4]. 3 Algorithms We applied five different machine-learning algorithms to this telephone-loop diagnosis task. The first algorithm we used was Quinlan's C4.5 <ref> [5] </ref>. C4.5 is a system for inducing decision-trees from training examples. C4.5 also includes a facility for converting decision trees into inference rules, although we did not use it in our experiments. Whereas C4.5 is a symbolic learning system, the other algorithms we used are based on neural-network learning methods.
Reference: [6] <author> H. Rabinowitz, J. Flamholz, E. Wolin, and J. Euchner. NYNEX MAX: </author> <title> A telephone trouble screening expert. </title> <booktitle> In Innovative Applications of Artificial Intelligence 3, </booktitle> <pages> pages 213-230, </pages> <address> Menlo Park, CA, </address> <year> 1991. </year>
Reference-contexts: In this paper, we describe our experiments in applying KNNs to the problem of diagnosing faults in local telephone loops. The purpose of this study is to determine the suitability of using neural-network approaches to learn to perform the task currently handled by NYNEX's MAX system <ref> [6] </ref>. Having an effective learning algorithm in place of MAX would allow (i) easy portability between different maintenance centers, and (ii) easy updating when the phone equipment changes (a common occurrence). <p> Section 5.2 discusses these experiments. 2 The Task MAX is an expert system that was designed by NYNEX to diagnose the location of customer-reported telephone problems. Figure 1 illustrates MAX's task (refer to Rabinowitz et al. <ref> [6] </ref> and Provost and Danyluk [4] for more information). When a customer calls with a phone-service problem, a representative invokes a mechanized loop test (MLT) to create an electronic profile of the voltages and resistances in the loop between the customer's telephone and the central office.
Reference: [7] <author> D. Rumelhart, G. Hinton, and R. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In D. Rumelhart and J. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the microstructure of cognition. Volume 1: Foundations, </booktitle> <pages> pages 318-363. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: C4.5 also includes a facility for converting decision trees into inference rules, although we did not use it in our experiments. Whereas C4.5 is a symbolic learning system, the other algorithms we used are based on neural-network learning methods. The first neural-network learning algorithm we used was backpropagation <ref> [7] </ref> applied to feed-forward networks with sigmoidal hidden units. The networks we used have a single layer of 25 hidden units, and are fully connected between layers. We also used two knowledge-based network algorithms: KBANN [8] and ADDEMUP [3].
Reference: [8] <author> G. Towell and J. Shavlik. </author> <booktitle> Knowledge-based artificial neural networks. Artificial Intelligence, </booktitle> <address> 70(1,2):119-165, </address> <year> 1994. </year>
Reference-contexts: 1 Introduction Knowledge-based neural networks <ref> [8] </ref> (KNNs) are networks that are derived from a set of rules describing what is currently known about a task. KNNs have been shown to classify novel instances better than a wide variety of machine learning algorithms, including ordinary artificial neural networks (ANNs). <p> The first neural-network learning algorithm we used was backpropagation [7] applied to feed-forward networks with sigmoidal hidden units. The networks we used have a single layer of 25 hidden units, and are fully connected between layers. We also used two knowledge-based network algorithms: KBANN <ref> [8] </ref> and ADDEMUP [3]. In a knowledge-based network, the topology and initial weights of the network are specified by a domain theory consisting of symbolic inference rules. The domain theory used in our experiments was adapted by us from MAX's knowledge base of 75 ART rules.
References-found: 8

