URL: http://www.cs.duke.edu/~magda/soda98.ps
Refering-URL: http://www.cs.duke.edu/~magda/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Exact and Approximation Algortihms for Clustering (Extended Abstract)  
Author: Pankaj K. Agarwal Cecilia M. Procopiuc 
Abstract: In this paper we present an n O(k 11=d ) time algorithm for solving the k-center problem in R d , under L 1 and L 2 met-rics. The algorithm extends to other metrics, and to the discrete k-center problem. We also describe a simple (1 + *)- approximation algorithm for the k-center problem, with running time O(n log k) + (k=*) O(k 11=d ) . Finally, we present a n O(k 11=d ) time algorithm for solving the L-capacitated k-center problem, provided that L = (n=k 11=d ) or L = O(1). We conclude with a simple approximation algorithm for the L-capacitated k-center problem. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. K. Agarwal and M. Sharir, </author> <title> Efficient algorithms for geometric optimization, </title> <journal> ACM Comput. Surveys, </journal> <note> (to appear). </note>
Reference-contexts: Given a capacity L, the L-capacitated k-center problem is to compute a k-clustering of the smallest size so that each cluster has at most L points. Previous results. There is a vast literature on clustering problems, see, for example, the books [3, 10, 17], the survey paper <ref> [1] </ref>, and the references therein. Even the simplest clustering problems are known to be NP-Hard, including the Euclidean k-center problem in the plane [12, 23]. In fact, it is NP-Hard to approximate the 2-dimensional k-center problem within a factor smaller than 2 even under the L 1 -metric [11]. <p> The running time was improved by Feder and Greene [11] to O (n log k) for any L p -metric. Several efficient algorithms have been developed for Euclidean and rectilinear k-center problems when k is small. See the survey by Agarwal and Sharir <ref> [1] </ref> for a summary of such results.
Reference: [2] <author> R. Agrawal, A. Ghosh, T. Imielinski, B. Iyer, and A. Swami, </author> <title> An interval classifier for database mining applications, </title> <booktitle> Proc. of the 18th Conf. on Very Large Databases, </booktitle> <month> August </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Clustering a set of points into a few groups is frequently used for statistical analysis and classification in numerous applications, including information retrieval [6, 7, 8, 25], facility location [10, 29], data mining <ref> [2, 27] </ref>, spatial data bases [9, 19, 24], data compression [22], image processing [18, 26], astrophysics [21], and scientific computing [5]. Because of such a diversity of applications, several variants of clustering problems have been proposed and widely studied.
Reference: [3] <author> M. R. Anderberg, </author> <title> Cluster Analysis for Applications, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: Given a capacity L, the L-capacitated k-center problem is to compute a k-clustering of the smallest size so that each cluster has at most L points. Previous results. There is a vast literature on clustering problems, see, for example, the books <ref> [3, 10, 17] </ref>, the survey paper [1], and the references therein. Even the simplest clustering problems are known to be NP-Hard, including the Euclidean k-center problem in the plane [12, 23].
Reference: [4] <author> J. Bar-Ilan, G. Kortsarz, and D. Peleg, </author> <title> How to allocate network centers, </title> <journal> J. Algorithms, </journal> <volume> 15 (1993), </volume> <pages> 385-415. </pages>
Reference-contexts: In another paper, Hwang et al. [15] gave an n O ( p k) time algorithm which also works for the Euclidean discrete k-center problem. Not much is known about the capacitated k-center problem. Bar-Ilan et al. <ref> [4] </ref> gave the first polynomial time approximation algorithm for the capacitated k-center problem, achieving an approximation factor of 10. They also proposed an approximation algorithm for the capacitated problem with fixed cluster size. The algorithm approximates the minimum number of clusters by a factor of dln ne.
Reference: [5] <author> M. Berger and I. Rigoutsos, </author> <title> An algorithm for point clustering and grid generation, </title> <journal> IEEE Trans. Systems, Man and Cybernetics., </journal> <volume> 21 (1991), </volume> <pages> 1278-1286. </pages>
Reference-contexts: of points into a few groups is frequently used for statistical analysis and classification in numerous applications, including information retrieval [6, 7, 8, 25], facility location [10, 29], data mining [2, 27], spatial data bases [9, 19, 24], data compression [22], image processing [18, 26], astrophysics [21], and scientific computing <ref> [5] </ref>. Because of such a diversity of applications, several variants of clustering problems have been proposed and widely studied. However, they all require a partition of a given set of points into clusters that optimizes a given objective function. In this paper we present efficient algorithms for several clustering problems.
Reference: [6] <author> M. Charikar, C. Chekuri, T. Feder, and R. Motwani, </author> <title> Incremental clustering and dynamic information retrieval, </title> <booktitle> Proc. of the 29th Annu. ACM Sympos. Theory Comput., </booktitle> <year> 1997, </year> <pages> pp. 626-634. </pages>
Reference-contexts: 1 Introduction Clustering a set of points into a few groups is frequently used for statistical analysis and classification in numerous applications, including information retrieval <ref> [6, 7, 8, 25] </ref>, facility location [10, 29], data mining [2, 27], spatial data bases [9, 19, 24], data compression [22], image processing [18, 26], astrophysics [21], and scientific computing [5]. Because of such a diversity of applications, several variants of clustering problems have been proposed and widely studied.
Reference: [7] <author> D. R. Cutting, D. R. Karger, and J. O. Pedersen, </author> <title> Constant interaction-time scatter/gather browsing of very large document collections, </title> <booktitle> Proc. of the 16th Annu. International ACM SIGIR Conf. on Research and Development in Information Retrieval, </booktitle> <year> 1993, </year> <pages> pp. 126-134. </pages>
Reference-contexts: 1 Introduction Clustering a set of points into a few groups is frequently used for statistical analysis and classification in numerous applications, including information retrieval <ref> [6, 7, 8, 25] </ref>, facility location [10, 29], data mining [2, 27], spatial data bases [9, 19, 24], data compression [22], image processing [18, 26], astrophysics [21], and scientific computing [5]. Because of such a diversity of applications, several variants of clustering problems have been proposed and widely studied.
Reference: [8] <author> D. R. Cutting, D. R. Karger, J. O. Pedersen, and J. W. Tukey, Scatter/gather: </author> <title> A cluster-based approach to browsing large document collections, </title> <booktitle> Proc. of the 15th Annu. International ACM SIGIR Conf. on Research and Development in Information Retrieval, </booktitle> <year> 1992, </year> <pages> pp. 318-329. </pages>
Reference-contexts: 1 Introduction Clustering a set of points into a few groups is frequently used for statistical analysis and classification in numerous applications, including information retrieval <ref> [6, 7, 8, 25] </ref>, facility location [10, 29], data mining [2, 27], spatial data bases [9, 19, 24], data compression [22], image processing [18, 26], astrophysics [21], and scientific computing [5]. Because of such a diversity of applications, several variants of clustering problems have been proposed and widely studied.
Reference: [9] <author> A. A. Diwan, S. Rane, S. Seshadri, and S. Sudar-shan, </author> <title> Clustering techniques for minimizing external path length., </title> <booktitle> Proc. of the International Conf. on Very Large Databases, </booktitle> <year> 1996. </year>
Reference-contexts: 1 Introduction Clustering a set of points into a few groups is frequently used for statistical analysis and classification in numerous applications, including information retrieval [6, 7, 8, 25], facility location [10, 29], data mining [2, 27], spatial data bases <ref> [9, 19, 24] </ref>, data compression [22], image processing [18, 26], astrophysics [21], and scientific computing [5]. Because of such a diversity of applications, several variants of clustering problems have been proposed and widely studied.
Reference: [10] <author> Z. Drezner, ed., </author> <title> Facility Location, </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1995. </year>
Reference-contexts: 1 Introduction Clustering a set of points into a few groups is frequently used for statistical analysis and classification in numerous applications, including information retrieval [6, 7, 8, 25], facility location <ref> [10, 29] </ref>, data mining [2, 27], spatial data bases [9, 19, 24], data compression [22], image processing [18, 26], astrophysics [21], and scientific computing [5]. Because of such a diversity of applications, several variants of clustering problems have been proposed and widely studied. <p> If the centers of the clusters are required to be a subset of the input points, the problem is called the discrete k-center problem. In some applications (e.g., facility location <ref> [10, 29] </ref>, astrophysics [21]), not only the size of a cluster is important but also the number of points in the cluster. We can generalize the notion of k-clustering as follows. <p> Given a capacity L, the L-capacitated k-center problem is to compute a k-clustering of the smallest size so that each cluster has at most L points. Previous results. There is a vast literature on clustering problems, see, for example, the books <ref> [3, 10, 17] </ref>, the survey paper [1], and the references therein. Even the simplest clustering problems are known to be NP-Hard, including the Euclidean k-center problem in the plane [12, 23].
Reference: [11] <author> T. Feder and D. H. Greene, </author> <title> Optimal algorithms for approximate clustering, </title> <booktitle> Proc. 20th Annu. ACM Sympos. Theory Comput., </booktitle> <year> 1988, </year> <pages> pp. 434-444. </pages>
Reference-contexts: Even the simplest clustering problems are known to be NP-Hard, including the Euclidean k-center problem in the plane [12, 23]. In fact, it is NP-Hard to approximate the 2-dimensional k-center problem within a factor smaller than 2 even under the L 1 -metric <ref> [11] </ref>. The greedy algorithm by Gonzalez [13] gives a 2-approximation algorithm for the k-center problem in any metric space. This algorithm requires O (kn) distance computations. The running time was improved by Feder and Greene [11] to O (n log k) for any L p -metric. <p> 2-dimensional k-center problem within a factor smaller than 2 even under the L 1 -metric <ref> [11] </ref>. The greedy algorithm by Gonzalez [13] gives a 2-approximation algorithm for the k-center problem in any metric space. This algorithm requires O (kn) distance computations. The running time was improved by Feder and Greene [11] to O (n log k) for any L p -metric. Several efficient algorithms have been developed for Euclidean and rectilinear k-center problems when k is small. See the survey by Agarwal and Sharir [1] for a summary of such results. <p> We present the algorithm for the L 1 metric in the plane. It can also be adapted for any L p -metric and for higher dimensions. First, compute a real number 0 , so that fl 0 2 fl , using the algorithm by Feder and Greene <ref> [11] </ref> in time O (n log k). Let Z be a grid of size ffi = 0 *=3, i.e. Z = f (iffi; jffi) j i; j 2 Zg.
Reference: [12] <author> R. J. Fowler, M. S. Paterson, and S. L. Tanimoto, </author> <title> Optimal packing and covering in the plane are NP-complete, </title> <journal> Inform. Process. Lett., </journal> <volume> 12 (1981), </volume> <pages> 133-137. </pages>
Reference-contexts: Previous results. There is a vast literature on clustering problems, see, for example, the books [3, 10, 17], the survey paper [1], and the references therein. Even the simplest clustering problems are known to be NP-Hard, including the Euclidean k-center problem in the plane <ref> [12, 23] </ref>. In fact, it is NP-Hard to approximate the 2-dimensional k-center problem within a factor smaller than 2 even under the L 1 -metric [11]. The greedy algorithm by Gonzalez [13] gives a 2-approximation algorithm for the k-center problem in any metric space.
Reference: [13] <author> T. Gonzalez, </author> <title> Clustering to minimize the maximum in-tercluster distance, </title> <type> Theoret. </type> <institution> Comput. Sci., </institution> <month> 38 </month> <year> (1985), </year> <pages> 293-306. </pages>
Reference-contexts: Even the simplest clustering problems are known to be NP-Hard, including the Euclidean k-center problem in the plane [12, 23]. In fact, it is NP-Hard to approximate the 2-dimensional k-center problem within a factor smaller than 2 even under the L 1 -metric [11]. The greedy algorithm by Gonzalez <ref> [13] </ref> gives a 2-approximation algorithm for the k-center problem in any metric space. This algorithm requires O (kn) distance computations. The running time was improved by Feder and Greene [11] to O (n log k) for any L p -metric. <p> Moreover, our algorithm is straightforward to extend to higher dimensions. In Subsection 2.3 we describe a simple (1 + *)- approximation algorithm for the k-center problem, whose running time is O (n log k) + (k=*) O (k 11=d ) . It combines the greedy algorithm by Gonzalez <ref> [13] </ref> with our algorithm for computing an optimal k-center. Finally, we study the capacitated k-center problem in Section 3.
Reference: [14] <author> T. Gonzalez, </author> <title> Covering a set of points in multidimensional space, </title> <journal> Inform. Process. Lett., </journal> <volume> 40 (1991), </volume> <pages> 181-188. </pages>
Reference-contexts: Several efficient algorithms have been developed for Euclidean and rectilinear k-center problems when k is small. See the survey by Agarwal and Sharir [1] for a summary of such results. The algorithm described by Gonzalez <ref> [14] </ref> can solve the planar k-center problem under the L 1 metric in n O (l) time, when the points lie in a horizontal strip of height l; it can be extended to higher dimensions. 1 Another commonly used definition of the size of a cluster S i is the maximum <p> Using this observation and the algorithm by Gonzalez <ref> [14] </ref>, we develop a dynamic-programming based algorithm for finding an optimal solution. Our approach also yields an n O (k 11=d ) -time algorithm for the discrete k-center problem in R d . <p> We denote by the set of all anchored unit squares. Following an argument similar to the one in <ref> [14] </ref>, we can prove the following lemma. Lemma 2.2. The optimal cover of S is an anchored cover. <p> We show that the lines can be chosen out of a finite set of lines, and we use dynamic programming. To compute the optimal cover on a thin strip, we use a modified version of the algorithm by Gonzalez <ref> [14] </ref>, which we refer to as the Strip Algorithm. We present the main ideas of Gonzalez's algorithm below. Strip Algorithm. Let S be a set of n points lying in a horizontal strip of height l. <p> Gonzalez describes a sweepline algorithm for computing a cover of S by the minimum number of squares. His algorithm relies on the following lemma, whose proof follows from a simple packing argument. Lemma 2.3. <ref> [14] </ref> Let S be a set of n points lying in a horizontal strip of height l. Every vertical line intersects at most 2l 1 squares of the optimal cover of S. <p> Then C 0 2 F. Condition (ii) and Lemma 2.3 imply u = O (n 4l2 ). Whenever the sweep line passes through a point of S, the algorithm updates the set F so that conditions (i) and (ii) are satisfied. See the original paper <ref> [14] </ref> for details. We can easily modify this algorithm to compute the optimal cover of S (see Definition 2.1). Handling thick strips. <p> If two signatures j ; k are identical, we delete the larger cover from F. A similar step is required in the original Strip Algorithm <ref> [14] </ref> (there, the algorithm checks whether any two subcovers in F have the same set of squares intersected by the vertical line) and we refer the reader to that paper for an efficient implementation of this step.
Reference: [15] <author> R. Z. Hwang, R. C. Chang, and R. C. T. Lee, </author> <title> The generalized searching over separators strategy to solve some NP-Hard problems in subexponential time, </title> <journal> Algo-rithmica, </journal> <volume> 9 (1993), </volume> <pages> 398-423. </pages>
Reference-contexts: By doing a binary search, they compute an optimal k-clustering. This approach can be extended to the L 1 -metric as well [28]. In another paper, Hwang et al. <ref> [15] </ref> gave an n O ( p k) time algorithm which also works for the Euclidean discrete k-center problem. Not much is known about the capacitated k-center problem. Bar-Ilan et al. [4] gave the first polynomial time approximation algorithm for the capacitated k-center problem, achieving an approximation factor of 10.
Reference: [16] <author> R. Z. Hwang, R. C. T. Lee, and R. C. Chang, </author> <title> The slab dividing approach to solve the Euclidean p-center problem, </title> <journal> Algorithmica, </journal> <volume> 9 (1993), </volume> <pages> 1-22. </pages>
Reference-contexts: Such a k-clustering is called a pairwise k-clustering. The size of a k clustering is the maximum size of a cluster in . 1 Hwang et al. <ref> [16] </ref> gave an n O ( p k) -time algorithm for the Euclidean k-center problem in the plane. <p> Using this observation and the algorithm by Gonzalez [14], we develop a dynamic-programming based algorithm for finding an optimal solution. Our approach also yields an n O (k 11=d ) -time algorithm for the discrete k-center problem in R d . Recall that the algorithm by Hwang et al. <ref> [16] </ref> also runs in time n O ( p k) for d = 2. Our algorithm is however not only considerably simpler than theirs, but the constant hidden in the big-Oh notation is also smaller. Moreover, our algorithm is straightforward to extend to higher dimensions.
Reference: [17] <author> A. K. Jain and R. C. Dubes, </author> <title> Algorithms for Clustering Data, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1988. </year>
Reference-contexts: Given a capacity L, the L-capacitated k-center problem is to compute a k-clustering of the smallest size so that each cluster has at most L points. Previous results. There is a vast literature on clustering problems, see, for example, the books <ref> [3, 10, 17] </ref>, the survey paper [1], and the references therein. Even the simplest clustering problems are known to be NP-Hard, including the Euclidean k-center problem in the plane [12, 23].
Reference: [18] <author> J. Jolion, P. Meer, and S. Batauche, </author> <title> Robust clustering with applications in computer vision, </title> <journal> IEEE Trans. Pattern Analysis Mach. Intell., </journal> <volume> 13 (1991), </volume> <pages> 791-802. </pages>
Reference-contexts: 1 Introduction Clustering a set of points into a few groups is frequently used for statistical analysis and classification in numerous applications, including information retrieval [6, 7, 8, 25], facility location [10, 29], data mining [2, 27], spatial data bases [9, 19, 24], data compression [22], image processing <ref> [18, 26] </ref>, astrophysics [21], and scientific computing [5]. Because of such a diversity of applications, several variants of clustering problems have been proposed and widely studied. However, they all require a partition of a given set of points into clusters that optimizes a given objective function.
Reference: [19] <author> L. Kaufman and P. J. Rousseeuw, </author> <title> Finding Groups in Data: An Introduction to Cluster Analysis, </title> <publisher> John Wiley & Sons, </publisher> <year> 1990. </year>
Reference-contexts: 1 Introduction Clustering a set of points into a few groups is frequently used for statistical analysis and classification in numerous applications, including information retrieval [6, 7, 8, 25], facility location [10, 29], data mining [2, 27], spatial data bases <ref> [9, 19, 24] </ref>, data compression [22], image processing [18, 26], astrophysics [21], and scientific computing [5]. Because of such a diversity of applications, several variants of clustering problems have been proposed and widely studied.
Reference: [20] <author> S. Khuller and Y. J. Sussmann, </author> <title> The capacitated k-center problem, </title> <booktitle> Proc. of the 4th Annu. European Sym-pos. on Algorithms, </booktitle> <year> 1996, </year> <pages> pp. 152-166. </pages>
Reference-contexts: They also proposed an approximation algorithm for the capacitated problem with fixed cluster size. The algorithm approximates the minimum number of clusters by a factor of dln ne. Khuller and Sussmann <ref> [20] </ref> improved the approximation factor for capacitated k-center to 6 (5 for a slightly different version of the problem). Recently Shmoys et al. [29] presented approximation algorithms for some generalizations of the capaci-tated k-center problem, using relaxation techniques. Our results. <p> Khuller and Sussmann <ref> [20] </ref> gave a polynomial algorithm for computing a ((2=c)k; cL; 2 fl ) cover, for any c &gt; 1.
Reference: [21] <author> R. Lupton, F. M. Maley, and N. Young, </author> <title> Data collection for the sloan digital sky survey: A network-flow heuristic, </title> <booktitle> Proc. of the 7th Annu. ACM-SIAM Sympos. on Discrete Algorithms, </booktitle> <month> January </month> <year> 1996, </year> <pages> pp. 296-303. </pages>
Reference-contexts: Introduction Clustering a set of points into a few groups is frequently used for statistical analysis and classification in numerous applications, including information retrieval [6, 7, 8, 25], facility location [10, 29], data mining [2, 27], spatial data bases [9, 19, 24], data compression [22], image processing [18, 26], astrophysics <ref> [21] </ref>, and scientific computing [5]. Because of such a diversity of applications, several variants of clustering problems have been proposed and widely studied. However, they all require a partition of a given set of points into clusters that optimizes a given objective function. <p> If the centers of the clusters are required to be a subset of the input points, the problem is called the discrete k-center problem. In some applications (e.g., facility location [10, 29], astrophysics <ref> [21] </ref>), not only the size of a cluster is important but also the number of points in the cluster. We can generalize the notion of k-clustering as follows.
Reference: [22] <author> J. Makhoul, S. Roucos, and H. Gish, </author> <title> Vector quantization in speech coding, </title> <booktitle> Proc. of the IEEE, 73 (1985), </booktitle> <pages> 1551-1588. </pages>
Reference-contexts: 1 Introduction Clustering a set of points into a few groups is frequently used for statistical analysis and classification in numerous applications, including information retrieval [6, 7, 8, 25], facility location [10, 29], data mining [2, 27], spatial data bases [9, 19, 24], data compression <ref> [22] </ref>, image processing [18, 26], astrophysics [21], and scientific computing [5]. Because of such a diversity of applications, several variants of clustering problems have been proposed and widely studied. However, they all require a partition of a given set of points into clusters that optimizes a given objective function.
Reference: [23] <author> N. Megiddo and K. J. Supowit, </author> <title> On the complexity of some common geometric location problems, </title> <journal> SIAM J. Comput., </journal> <volume> 13 (1984), </volume> <pages> 182-196. </pages>
Reference-contexts: Previous results. There is a vast literature on clustering problems, see, for example, the books [3, 10, 17], the survey paper [1], and the references therein. Even the simplest clustering problems are known to be NP-Hard, including the Euclidean k-center problem in the plane <ref> [12, 23] </ref>. In fact, it is NP-Hard to approximate the 2-dimensional k-center problem within a factor smaller than 2 even under the L 1 -metric [11]. The greedy algorithm by Gonzalez [13] gives a 2-approximation algorithm for the k-center problem in any metric space.
Reference: [24] <author> R. T. Ng and J. Han, </author> <title> Efficient and Effective Clustering Methods for Spatial Data Mining, </title> <booktitle> Proc. of the 20th International Conf. on Very Large Databases, </booktitle> <year> 1994, </year> <pages> pp. 144-155. </pages>
Reference-contexts: 1 Introduction Clustering a set of points into a few groups is frequently used for statistical analysis and classification in numerous applications, including information retrieval [6, 7, 8, 25], facility location [10, 29], data mining [2, 27], spatial data bases <ref> [9, 19, 24] </ref>, data compression [22], image processing [18, 26], astrophysics [21], and scientific computing [5]. Because of such a diversity of applications, several variants of clustering problems have been proposed and widely studied.
Reference: [25] <author> P. Raghavan, </author> <title> Information retrieval algorithms: A survey, </title> <booktitle> Proc. of the 8th Annu. ACM-SIAM Sympos. on Discrete Algorithms, </booktitle> <month> January </month> <year> 1997, </year> <pages> pp. 11-18. </pages>
Reference-contexts: 1 Introduction Clustering a set of points into a few groups is frequently used for statistical analysis and classification in numerous applications, including information retrieval <ref> [6, 7, 8, 25] </ref>, facility location [10, 29], data mining [2, 27], spatial data bases [9, 19, 24], data compression [22], image processing [18, 26], astrophysics [21], and scientific computing [5]. Because of such a diversity of applications, several variants of clustering problems have been proposed and widely studied.
Reference: [26] <author> P. Schroeter and J. Bigun, </author> <title> Hierarchical image segmentation by multi-dimensional clustering and orientation-adaptive boundary refinement, </title> <journal> Pattern Recognition., </journal> <volume> 28 (1995), </volume> <pages> 695-709. </pages>
Reference-contexts: 1 Introduction Clustering a set of points into a few groups is frequently used for statistical analysis and classification in numerous applications, including information retrieval [6, 7, 8, 25], facility location [10, 29], data mining [2, 27], spatial data bases [9, 19, 24], data compression [22], image processing <ref> [18, 26] </ref>, astrophysics [21], and scientific computing [5]. Because of such a diversity of applications, several variants of clustering problems have been proposed and widely studied. However, they all require a partition of a given set of points into clusters that optimizes a given objective function.
Reference: [27] <author> J. Shafer, R. Agrawal, and M. Mehta, Sprint: </author> <title> A scalable parallel classifier for data mining., </title> <booktitle> Proc. of the International Conf. on Very Large Databases, </booktitle> <year> 1996. </year>
Reference-contexts: 1 Introduction Clustering a set of points into a few groups is frequently used for statistical analysis and classification in numerous applications, including information retrieval [6, 7, 8, 25], facility location [10, 29], data mining <ref> [2, 27] </ref>, spatial data bases [9, 19, 24], data compression [22], image processing [18, 26], astrophysics [21], and scientific computing [5]. Because of such a diversity of applications, several variants of clustering problems have been proposed and widely studied.
Reference: [28] <author> M. Sharir, </author> <title> Private communication, </title> <year> (1997). </year>
Reference-contexts: By doing a binary search, they compute an optimal k-clustering. This approach can be extended to the L 1 -metric as well <ref> [28] </ref>. In another paper, Hwang et al. [15] gave an n O ( p k) time algorithm which also works for the Euclidean discrete k-center problem. Not much is known about the capacitated k-center problem.
Reference: [29] <author> D. Shmoys, E. Tardos, and K. Aardal, </author> <title> Approximation algorithms for facility location problems, </title> <booktitle> Proc. of the 29th Annu. ACM Sympos. Theory Comput., </booktitle> <year> 1997, </year> <pages> pp. 265-274. </pages>
Reference-contexts: 1 Introduction Clustering a set of points into a few groups is frequently used for statistical analysis and classification in numerous applications, including information retrieval [6, 7, 8, 25], facility location <ref> [10, 29] </ref>, data mining [2, 27], spatial data bases [9, 19, 24], data compression [22], image processing [18, 26], astrophysics [21], and scientific computing [5]. Because of such a diversity of applications, several variants of clustering problems have been proposed and widely studied. <p> If the centers of the clusters are required to be a subset of the input points, the problem is called the discrete k-center problem. In some applications (e.g., facility location <ref> [10, 29] </ref>, astrophysics [21]), not only the size of a cluster is important but also the number of points in the cluster. We can generalize the notion of k-clustering as follows. <p> The algorithm approximates the minimum number of clusters by a factor of dln ne. Khuller and Sussmann [20] improved the approximation factor for capacitated k-center to 6 (5 for a slightly different version of the problem). Recently Shmoys et al. <ref> [29] </ref> presented approximation algorithms for some generalizations of the capaci-tated k-center problem, using relaxation techniques. Our results. One of the main results of this paper is an n O (k 11=d ) -time algorithm for the k-center problem in R d , under any L p -metric (Subsection 2.1).
References-found: 29

