URL: http://www.eecis.udel.edu/~pollock/papers/jpl_carroll.ps
Refering-URL: http://www.cis.udel.edu/~jochen/passages/pubs.htm
Root-URL: http://www.cis.udel.edu
Email: carroll@cis.udel.edu  pollock@cis.udel.edu  
Phone: (302) 831-1953, fax (302) 831-8458  
Title: Design and Implementation of a Compiler and Runtime System for Composite Tree Parallelism  
Author: Mark C. Chu-Carroll and Lori L. Pollock 
Keyword: parallel programming, data parallelism, cluster computing.  
Address: 19716  
Affiliation: Department of Computer and Information Sciences University of Delaware Newark, DE  
Abstract: In the field of scientific computing, the use of parallelism has led to widespread improvements in the performance of complex computations. Unfortunately, outside of the domain of computational science, the promise of parallelism has been largely unfulfilled. Workstation and PC networks have become common in most offices, and cluster computing is receiving much attention, but the lack of programming support tools directed towards non-scientific programmers has inhibited the use of parallelism to increase the performance of general purpose applications such as spreadsheets, word processors, compilers, and databases. We have developed a new model of parallel programming for general purpose applications, called composite tree parallelism, which was designed by making a small number of simple alterations to the basic data parallel programming model. Composite tree parallelism allows programmers to write coarse grained tree-based parallel data structures, in a form that retains the power and simplicity of data parallelism. In this paper, we briefly present the composite tree parallel model, and then describe the design and implementation of the compiler and runtime system for effectively supporting composite parallel programming on workstation clusters.
Abstract-found: 1
Intro-found: 1
Reference: [AGG + 93] <author> Joshua Auerbach, Arthur Goldberg, Germ an Goldszmidt, Ajei Gopal, Mark Kennedy, and James Russel. </author> <title> Concert/C Manual: A Programmers Guide to a Language for Distributed C Programming. </title> <institution> IBM T.J. Watson Research Center, </institution> <month> July </month> <year> 1993. </year>
Reference-contexts: Data marshalling is the process of translating an object stored as a pointer graph in memory into a form which can be transmitted across a communication link. Marshalling in many languages is a complex process which requires extensive annotation of data structure declarations (for an example, see <ref> [AGG + 93] </ref>). However, these problems are caused primarily by the use of pointers to untagged data types. In Scheme, the fact that every value is tagged with information that identifies its data type makes marshalling quite simple. The Cabal marshaller is closely based on the interpretive marshaller of Concert/C.
Reference: [Agh86] <author> G. Agha. </author> <title> Actors: A model of concurrent computation in distributed systems. </title> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: These systems can be categorized into actor based systems, coarse grained data parallel systems, and parallel functional programming systems. Actor systems are based on the actor model first proposed by Hewitt [HB77] and later refined by Agha <ref> [Agh86] </ref>. The actor programming model is based on having a computation performed by multiple simultaneously active computational entities, which communicate through message passing in order to cooperatively perform a single task. Actor systems attain massive parallelism by nearly eliminating any serialization in computations.
Reference: [AH92] <author> G. Agha and C. Houck. Hal: </author> <title> A high level actor language and its distributed implementation. </title> <booktitle> In Proceedings of the 21st International Conference on Parallel Processing, </booktitle> <pages> pages 158-165, </pages> <year> 1992. </year>
Reference-contexts: Actor systems attain massive parallelism by nearly eliminating any serialization in computations. Actor systems for workstation clusters have been proposed in various publications, including <ref> [AH92] </ref> and [GKSK94]. However, Actor programs are very communication intensive, which makes them difficult to execute efficiently on architectures like workstation clusters that have low communication bandwidth. Chien [Chi92] proposed an extension to the actor model, called Concurrent Aggregates (CA).
Reference: [App92] <author> A. Appel. </author> <title> Compiling with Continuations. </title> <publisher> Cambridge University Press, </publisher> <year> 1992. </year>
Reference-contexts: A complete introduction to the history and 15 usage of CPS based intermediate representations in compilers can be found in <ref> [App92] </ref>. The usefulness of the CPS form is examined in great theoretical detail in [?, ?, ?]. <p> Programs written in direct style (the term used in CPS literature for programs before the CPS translation), can be translated into CPS using a simple recursive procedure <ref> [App92, chapter 5] </ref>.
Reference: [ASU86] <author> Alfred Aho, Ravi Sethi, and Jeffrey Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison Wesley, </publisher> <year> 1986. </year>
Reference-contexts: Each type of primitive node is translated into an invocation of the appropriate primitive operation. 8. Switch nodes are translated into case expressions. 5.2 Static Analysis The compiler for a composite programming language will make extensive use of dataflow analysis <ref> [ASU86] </ref> in order to safely optimize programs. The exact set of analyses will vary slightly depending on the nature of the base language. In the case of the compiler for Cabal, the compiler performs dataflow analysis to determine the following kinds of information: Reaching Definitions. <p> In order to generate a precise control flow graph, it is highly desirable to know the precise type of program variables. This is done by performing a dataflow analysis closely modeled on constant propagation <ref> [ASU86, Section 10.11] </ref>. 23 Instead of assignments to a variable being treated as assignments of either constant or non-constant values, they are treated as assignments of a given type, which may be precise (if the exact type is known), or imprecise. <p> be propagated through the control flow graph, a set of transfer functions from L to L representing the flow of information through basic blocks in the control flow graph, and a binary meet operator on L to represent the confluence of flow at join points in the control flow graph <ref> [ASU86] </ref>. In type analysis, the lattice L is the set of all mappings from variables in the program to pairs of the form (t,precision), where t is either a type or unknown, and precision is either precise (p), imprecise (i), or undetermined (u). <p> In this case, the communications from the two par statements can be combined. These transformations and their detection are straightforward. 6 Runtime Support for Composites In most languages, the runtime support includes such things as runtime data representations, memory allocation, process and resource management, and other similar features <ref> [ASU86, chapter 7] </ref>. In addition to these standard runtime services, composite programs require several additional services in order to support the distributed execution of composite parallel programs.
Reference: [BC90] <author> G. Blelloch and S. Chatterjee. </author> <title> Vcode: A data parallel intermediate language. </title> <booktitle> In Proceedings of the 3rd Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 471-480, </pages> <year> 1990. </year>
Reference-contexts: This allows programmers to write programs that operate on recursive data structures, and to take advantage of nested parallelism. NESL compiles its programs by translating into an intermediate language called Vcode <ref> [BC90] </ref>, and performing optimizations on this intermediate form. The Vcode translation reduces all tree-based parallel structures to a pair of single dimensional arrays. Optimizations are performed on this array representation by translating parallel calls into two forms: data and task parallel.
Reference: [BCT92] <author> Preston Briggs, Keith D. Cooper, and Linda Torczon. </author> <title> Rematerialization. </title> <booktitle> In Proceedings of the ACM SIGPLAN 92 Conference on Programming Lang uage Design and Implementation. ACM, </booktitle> <month> June </month> <year> 1992. </year>
Reference-contexts: If the minimal coloring has more colors than there are available registers, the compiler must insert spill code (i.e., memory loads and stores) in order to reduce the number of required registers. There are many algorithms for register allocation based on graph coloring <ref> [CAC + 81, CH84, BCT92] </ref>. Variations on the standard graph coloring algorithms include hierarchical graph coloring [CK91], coloring using clique separators [GSS89], and register allocation over the program dependence graph [NP94].
Reference: [Ble90] <author> G. Blelloch. Nesl: </author> <title> A nested data parallel language. </title> <type> Technical Report CMU-CS-92-103, CMU, </type> <year> 1990. </year>
Reference-contexts: However, it does not provide compiler support for analyzing and optimizing those structures, but requires the programmers to optimize their code manually. A group at CMU has built a data parallel language called NESL <ref> [Ble90] </ref>, which allows programmers to implement functional programs in terms of a nested data parallel programming model. This allows programmers to write programs that operate on recursive data structures, and to take advantage of nested parallelism.
Reference: [BMT92] <author> D. Berry, R. Milner, and D. Turner. </author> <title> A semantics for ML concurrency primitives. </title> <booktitle> In Proceedings, 19th Annual Symposium on Principles of Programming Languages, </booktitle> <year> 1992. </year>
Reference-contexts: However, recently, many people in the functional programming language community have decided that functional languages need explicit support for parallelism so that functional programmers can write portable, effective parallel programs. This has led to the development of a variety of different functional parallel languages <ref> [BMT92, Nik93, FCO90] </ref>. Most of these, like their imperative counterparts, are oriented towards the implementation of scientific or numeric programs. One of these projects in particular seems to have potential in the area of building systems for implementing general purpose parallel programs.
Reference: [Bra90] <author> W. Brainerd. </author> <title> Programmer's Guide to Fortran 90. </title> <publisher> McGraw-Hill Book Co., </publisher> <year> 1990. </year>
Reference-contexts: In addition, since the object oriented capabilities of inheritance and polymorphism are not strictly required; any language which provides support for abstract data types, such as CLU [Lis81] or Fortran-90 <ref> [Bra90] </ref>, can also be used as a suitable base language for a composite implementation. This section presents Cabal 1 , which is an extension of an object oriented Scheme for composites.
Reference: [CAC + 81] <author> Gregory Chaitin, Marc Auslander, Ashok Chandra, John Cocke, Martin Hopkins, and Peter Markstein. </author> <title> Register allocation via coloring. </title> <booktitle> Computer Languages, </booktitle> <pages> pages 47-57, </pages> <month> January </month> <year> 1981. </year>
Reference-contexts: Register allocation is a phase of an optimizing compiler that attempts to assign registers to variables in a segment of code in order to effectively utilize the machine's register set. The standard technique for register allocation builds an interference graph for the program, based on variable liveness <ref> [CAC + 81] </ref>. In an interference graph, each live range in the code segment is represented by a node, where a live range is defined as the region between the first assignment to a variable, and the last use of that variable in a section of the program. <p> If the minimal coloring has more colors than there are available registers, the compiler must insert spill code (i.e., memory loads and stores) in order to reduce the number of required registers. There are many algorithms for register allocation based on graph coloring <ref> [CAC + 81, CH84, BCT92] </ref>. Variations on the standard graph coloring algorithms include hierarchical graph coloring [CK91], coloring using clique separators [GSS89], and register allocation over the program dependence graph [NP94].
Reference: [CC96] <author> Mark C. Chu-Carroll. </author> <title> Programming Language and Compiler Support for General Purpose Parallelism. </title> <type> PhD thesis, </type> <institution> Department of Computer and Information Sciences, University of Delaware, </institution> <month> August </month> <year> 1996. </year>
Reference-contexts: This section presents Cabal 1 , which is an extension of an object oriented Scheme for composites. We have developed language extensions for composites with Java <ref> [CC96] </ref>, but our implementation is based on Scheme due to the lack of a stable Java environment at the time this work was started. <p> The translation to PCG is O (n) in both time and space, where n is the number of expressions in the original Scheme program. We refer the reader to the dissertation for the proof <ref> [CC96] </ref>. 5.1.5 Translating the PCG to SPMD Message Passing Code Once the optimization of a program represented by the PCG is complete, it can be translated into SPMD code in Scheme very easily. <p> The iterative dataflow algorithm will terminate on this dataflow problem. This type analysis is also a distributive problem. We refer the reader to the dissertation for the detailed proofs <ref> [CC96] </ref>. The fact that type analysis is distributive means that the results are precise, meaning that no statically available information is lost by the transfer functions used by the analysis.
Reference: [CD95] <author> A. Chien and J. Dolby. </author> <title> Parallel programming using c++. In Chapter ICC++: A C++ Dialect for High Performance Parallel Computing. </title> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference: [CH84] <author> Frederick Chow and John Hennessy. </author> <title> Register allocation by priority-based coloring. </title> <booktitle> In Proceedings of the ACM SIGPLAN 84 Symposium on Compiler Construct ion. ACM, </booktitle> <year> 1984. </year>
Reference-contexts: If the minimal coloring has more colors than there are available registers, the compiler must insert spill code (i.e., memory loads and stores) in order to reduce the number of required registers. There are many algorithms for register allocation based on graph coloring <ref> [CAC + 81, CH84, BCT92] </ref>. Variations on the standard graph coloring algorithms include hierarchical graph coloring [CK91], coloring using clique separators [GSS89], and register allocation over the program dependence graph [NP94].
Reference: [Chi92] <author> Andrew Chien. </author> <title> Concurrent Aggregates: Supporting Modularity in Massively Parallel Programs. </title> <publisher> MIT Press, </publisher> <address> 2nd edition, </address> <year> 1992. </year>
Reference-contexts: Actor systems for workstation clusters have been proposed in various publications, including [AH92] and [GKSK94]. However, Actor programs are very communication intensive, which makes them difficult to execute efficiently on architectures like workstation clusters that have low communication bandwidth. Chien <ref> [Chi92] </ref> proposed an extension to the actor model, called Concurrent Aggregates (CA). CA allows a collection of agents to work cooperatively as if they were a single special actor called an Aggregate.
Reference: [CK91] <author> David Callahan and Brian Koblenz. </author> <title> Register allocation via heirarchical graph coloring. </title> <booktitle> In Proceeedings of the ACM SIGPLAN '91 Conference on Programming La nguage Design and Implementation. ACM, </booktitle> <year> 1991. </year>
Reference-contexts: There are many algorithms for register allocation based on graph coloring [CAC + 81, CH84, BCT92]. Variations on the standard graph coloring algorithms include hierarchical graph coloring <ref> [CK91] </ref>, coloring using clique separators [GSS89], and register allocation over the program dependence graph [NP94]. This phase of compilation tends to be very time consuming, and so is an excellent candidate for parallelization, provided an algorithm can be found which provides opportunities for parallel execution.
Reference: [CP94] <author> M. Carroll and L. Pollock. </author> <title> Composites: trees for data parallel programming. </title> <booktitle> In Proceedings of the 1994 International Conference on Computer Languages, </booktitle> <pages> pages 43-54. </pages> <publisher> IEEE, </publisher> <year> 1994. </year>
Reference-contexts: In particular, we have designed a programming construct called a composite tree <ref> [CP94] </ref>, which allows programmers to implement parallel data structures in terms of a hierarchical data parallel programming abstraction. Data parallelism is a programming model that achieves massive parallelism through the lock-step execution of individual operations simultaneously on all members of a parallel aggregate.
Reference: [DGC94] <author> J. Dean, D. Grove, and C. Chambers. </author> <title> Optimizationn of object oriented programs using static class hierarchy analysis. </title> <type> Technical Report UW-CSTR94-12-01, </type> <institution> University of Washington, </institution> <year> 1994. </year>
Reference-contexts: Variable liveness is used by the Cabal compiler during the extended footprinting optimization, described in section 5.3.1. 5.2.1 Type Analysis in Cabal In addition to the standard dataflow analyses, the Cabal compiler performs a dataflow based type analysis, based on a simplified form of type hierarchy analysis <ref> [DGC94] </ref>. This analysis attempts to derive precise types for objects in the program. This analysis is needed, because variables in an object oriented Scheme program do not have declared types (except for arguments to methods).
Reference: [FCO90] <author> J. T. Feo, D. C. Cann, and R. R. Oldehoeft. </author> <title> A report on the sisal language project. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 10 </volume> <pages> 349-366, </pages> <year> 1990. </year> <month> 44 </month>
Reference-contexts: However, recently, many people in the functional programming language community have decided that functional languages need explicit support for parallelism so that functional programmers can write portable, effective parallel programs. This has led to the development of a variety of different functional parallel languages <ref> [BMT92, Nik93, FCO90] </ref>. Most of these, like their imperative counterparts, are oriented towards the implementation of scientific or numeric programs. One of these projects in particular seems to have potential in the area of building systems for implementing general purpose parallel programs.
Reference: [GKSK94] <author> A. Gursoy, S. Krishnan, A. Sinha, and L. Kale. </author> <title> The Charm(4.3) Programming Language Manual. </title> <institution> University of Illinois, </institution> <year> 1994. </year>
Reference-contexts: Actor systems attain massive parallelism by nearly eliminating any serialization in computations. Actor systems for workstation clusters have been proposed in various publications, including [AH92] and <ref> [GKSK94] </ref>. However, Actor programs are very communication intensive, which makes them difficult to execute efficiently on architectures like workstation clusters that have low communication bandwidth. Chien [Chi92] proposed an extension to the actor model, called Concurrent Aggregates (CA).
Reference: [GL92] <author> Dennis Gannon and Jeng-Kuen Lee. </author> <title> Object-oriented parallel programming: experiments and results. </title> <booktitle> In Proceedings of Supercomputing '91. IEEE, </booktitle> <year> 1992. </year>
Reference-contexts: A group at Indiana University developed an object-based extension of Concurrent Aggregates, called Distributed Collection Model, and implemented the model in a language called pC++ based on C++ <ref> [GL92] </ref>. pC++ programs run very effectively on high performance parallel architectures such as the CM-5. pC++ programs could be compiled to execute effectively on workstation networks, but the language model currently is not capable of implementing recursive data structures or nested parallelism.
Reference: [Gri93a] <author> A. Grimshaw. </author> <title> Easy to use object-oriented parallel programming with mentat. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 39-51, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Grimshaw at the University of Virginia has developed a programming language called Mentat <ref> [Gri93a, Gri93b] </ref>, which is C++ extended by an actors variant called coarse grained dataflow. Mentat programs consist of a sequential program which makes calls on special objects called Mentat Objects. Mentat objects are essentially actors, which operate in parallel.
Reference: [Gri93b] <author> Andrew S. Grimshaw. </author> <title> The mentat computation model: data-driven support for object-oriented parallel processing. </title> <type> Technical Report CS-93-30, </type> <institution> University of Virginia, </institution> <year> 1993. </year>
Reference-contexts: Grimshaw at the University of Virginia has developed a programming language called Mentat <ref> [Gri93a, Gri93b] </ref>, which is C++ extended by an actors variant called coarse grained dataflow. Mentat programs consist of a sequential program which makes calls on special objects called Mentat Objects. Mentat objects are essentially actors, which operate in parallel.
Reference: [GSS89] <author> Rajiv Gupta, Mary Lou Soffa, and Tim Steele. </author> <title> Register allocation via clique separators. </title> <booktitle> In Proceedings of the ACM SIGPLAN '89 Conference on Programming Lan guage Design and Implementation. ACM, </booktitle> <year> 1989. </year>
Reference-contexts: There are many algorithms for register allocation based on graph coloring [CAC + 81, CH84, BCT92]. Variations on the standard graph coloring algorithms include hierarchical graph coloring [CK91], coloring using clique separators <ref> [GSS89] </ref>, and register allocation over the program dependence graph [NP94]. This phase of compilation tends to be very time consuming, and so is an excellent candidate for parallelization, provided an algorithm can be found which provides opportunities for parallel execution.
Reference: [HB77] <author> C. Hewitt and H. Baker. </author> <title> Actors and continuous functionals. </title> <booktitle> In Proceedings IFIP Working Conference on Formal Description of Programming Concepts, </booktitle> <pages> pages 367-387, </pages> <year> 1977. </year>
Reference-contexts: These systems can be categorized into actor based systems, coarse grained data parallel systems, and parallel functional programming systems. Actor systems are based on the actor model first proposed by Hewitt <ref> [HB77] </ref> and later refined by Agha [Agh86]. The actor programming model is based on having a computation performed by multiple simultaneously active computational entities, which communicate through message passing in order to cooperatively perform a single task. Actor systems attain massive parallelism by nearly eliminating any serialization in computations.
Reference: [HQ92] <author> P. Hatcher and M. Quinn. </author> <title> Data parallel programming on MIMD computers. </title> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: These include dpSather [Sch92], Dino [RSW91], DataParallel C <ref> [HQ92] </ref>, C** [LRV92] and many others. These systems all restrict the use of parallel data structures to arrays, and lack support for general purpose computing.
Reference: [Hud92] <author> P. Hudak. </author> <title> Mutable abstract datatypes -or- how to have your state and munge it too. </title> <type> Technical Report YALEU/DCS/RR-914, </type> <institution> Yale University, </institution> <year> 1992. </year>
Reference-contexts: One of these projects in particular seems to have potential in the area of building systems for implementing general purpose parallel programs. Hudak designed a construct for functional programming languages called a Mutable 5 Abstract Datatype (MAD) <ref> [Hud92] </ref>, based on a functional construct called a Monad. This group has also developed a parallel programming model based on these constructs that allows explicitly parallel programs to be written that operate on these functional mutable data types [JH93].
Reference: [JH93] <author> M. Jones and P. Hudak. </author> <title> Implicit and explicit parallel programming in haskell. </title> <type> Technical Report YALEU/DCS/RR-982, </type> <institution> Yale University, </institution> <year> 1993. </year>
Reference-contexts: This group has also developed a parallel programming model based on these constructs that allows explicitly parallel programs to be written that operate on these functional mutable data types <ref> [JH93] </ref>. This model uses a programming structure that is equivalent to our intermediate form to describe parallel computations in a referentially transparent, easily analyzable form. The drawback of this approach is that it requires the programmer to understand the underlying monadic formalism in order to write effective programs.
Reference: [Kic91] <author> Gregor Kiczales. </author> <title> The art of the metaobject protocol. </title> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: Cabal is based on a variant of Scheme which 1 A Cabal is a scheme with multiple participants. 9 has been extended with a variant of the Common Lisp Object System [Ste84], called Tiny-CLOS <ref> [Kic91] </ref>. 4.1 Cabal Composite Declarations Cabal provides a new type definition syntax in the form of the define-composite construct. This is a new definition which looks nearly identical to the standard define-class form.
Reference: [LA94] <author> K. Landry and J. Arthur. </author> <title> Achieving asynchronous speedup while preserving synchronous semantics: an implementation of instruction footprinting in linda. </title> <booktitle> In Proceedings of the 1994 International Conference on Computer Languages, </booktitle> <pages> pages 55-63. </pages> <publisher> IEEE, </publisher> <year> 1994. </year>
Reference-contexts: The Cabal compiler uses two methods to increase the performance of the program: communication optimization, to reduce the overall quantity of communication within the program, and extended footprinting, to reduce the effective cost of the communications that are performed. 27 5.3.1 Extended Footprinting Footprinting <ref> [LA94] </ref> is an optimization that reduces the effective cost of communication in parallel programs by overlapping computation with communication. This is done by separating a single communication event into two distinct events: an initiation and a completion.
Reference: [Lis81] <author> Barbera Liskov. </author> <title> CLU Reference Manual. </title> <publisher> Springer-Verlag, </publisher> <year> 1981. </year>
Reference-contexts: In addition, since the object oriented capabilities of inheritance and polymorphism are not strictly required; any language which provides support for abstract data types, such as CLU <ref> [Lis81] </ref> or Fortran-90 [Bra90], can also be used as a suitable base language for a composite implementation. This section presents Cabal 1 , which is an extension of an object oriented Scheme for composites.
Reference: [LRV92] <author> J. Larus, B. Richards, and G. Viwsanathan. </author> <title> C**: A large grain, object-oriented, data-parallel programming language. </title> <type> Technical report, </type> <institution> University of Wisconsin, Madison, </institution> <year> 1992. </year>
Reference-contexts: These include dpSather [Sch92], Dino [RSW91], DataParallel C [HQ92], C** <ref> [LRV92] </ref> and many others. These systems all restrict the use of parallel data structures to arrays, and lack support for general purpose computing.
Reference: [MFL93] <author> S. Murer, J. Feldman, and C. Lim. psather: </author> <title> Layered extensions to an object-oriented language for efficient parallel computation. </title> <type> Technical report, ICSI, </type> <institution> University of California at Berkeley, </institution> <year> 1993. </year>
Reference-contexts: These include dpSather [Sch92], Dino [RSW91], DataParallel C [HQ92], C** [LRV92] and many others. These systems all restrict the use of parallel data structures to arrays, and lack support for general purpose computing. PSather <ref> [MFL93] </ref> provides support for parallelism in a fashion that resembles data parallelism, and can be used to build nested, data parallel data structures in terms of a non-uniform memory access distributed shared memory.
Reference: [MP95] <author> Christine Makowski and Lori Pollock. </author> <title> Efficient register allocation via parallel graph coloring. </title> <booktitle> In Proceedings of the ACM Symposium on Applied Computing, Programming Languages Track, </booktitle> <month> February </month> <year> 1995. </year>
Reference-contexts: In the study done by Zobel, 33 out of 34 example programs were translatable to interval graphs. The parallel algorithm used for this example was proposed in <ref> [MP95] </ref>. This technique uses clique separators to divide a register interference graph into two subgraphs, which can be colored independently. Once the two subgraphs are colored, their colorings can be combined using simple color renaming.
Reference: [Nik93] <author> R. S. Nikhil. </author> <title> An overview of the parallel language id. </title> <type> Technical report, </type> <institution> DEC, Cambridge Research Lab, </institution> <year> 1993. </year>
Reference-contexts: However, recently, many people in the functional programming language community have decided that functional languages need explicit support for parallelism so that functional programmers can write portable, effective parallel programs. This has led to the development of a variety of different functional parallel languages <ref> [BMT92, Nik93, FCO90] </ref>. Most of these, like their imperative counterparts, are oriented towards the implementation of scientific or numeric programs. One of these projects in particular seems to have potential in the area of building systems for implementing general purpose parallel programs.
Reference: [NP94] <author> Cindy Norris and Lori Pollock. </author> <title> Register allocation over the program dependence graph. </title> <booktitle> In Proceedings of the ACM SIGPLAN '94 Conference on Programming Lan guage Design and Implementation, </booktitle> <year> 1994. </year>
Reference-contexts: There are many algorithms for register allocation based on graph coloring [CAC + 81, CH84, BCT92]. Variations on the standard graph coloring algorithms include hierarchical graph coloring [CK91], coloring using clique separators [GSS89], and register allocation over the program dependence graph <ref> [NP94] </ref>. This phase of compilation tends to be very time consuming, and so is an excellent candidate for parallelization, provided an algorithm can be found which provides opportunities for parallel execution. Further, there is an efficient divide and conquer algorithm for performing graph coloring over straight line code segments.
Reference: [RSW91] <author> M. Rosing, R. Schnabel, and R. Weaver. </author> <title> The dino parallel programming language. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13(1) </volume> <pages> 30-42, </pages> <month> Sept </month> <year> 1991. </year>
Reference-contexts: These include dpSather [Sch92], Dino <ref> [RSW91] </ref>, DataParallel C [HQ92], C** [LRV92] and many others. These systems all restrict the use of parallel data structures to arrays, and lack support for general purpose computing.
Reference: [Sch92] <author> H. Schmidt. </author> <title> Data-parallel object-oriented programming. </title> <booktitle> In Proceedings of the Fifth Australian Supercomputing Conference, </booktitle> <pages> pages 263-272, </pages> <year> 1992. </year> <month> 45 </month>
Reference-contexts: These include dpSather <ref> [Sch92] </ref>, Dino [RSW91], DataParallel C [HQ92], C** [LRV92] and many others. These systems all restrict the use of parallel data structures to arrays, and lack support for general purpose computing.
Reference: [Ste84] <author> Guy Steele. </author> <title> Common Lisp: the Language. </title> <publisher> Digital Press, </publisher> <year> 1984. </year>
Reference-contexts: Cabal is based on a variant of Scheme which 1 A Cabal is a scheme with multiple participants. 9 has been extended with a variant of the Common Lisp Object System <ref> [Ste84] </ref>, called Tiny-CLOS [Kic91]. 4.1 Cabal Composite Declarations Cabal provides a new type definition syntax in the form of the define-composite construct. This is a new definition which looks nearly identical to the standard define-class form.
Reference: [Tar85] <author> R. E. Tarjan. </author> <title> Decomposition by clique separators. </title> <journal> Discrete Mathematics, </journal> (55):221-231, 1985. 
Reference-contexts: This is based on a theorem by Tarjan which states that if two subgraphs connected by a clique separator can each be colored in n colors, then the original graph can also be colored in n colors by combining the colors of the subgraphs <ref> [Tar85] </ref>. Parallel and sequential versions of the code for this algorithm are presented in figure 3. The two versions are virtually identical, differing only in the body of the splitting_color method. Within this function, the differences are also quite trivial.
Reference: [TMC90] <institution> C* Reference Manual. Thinking Machines Corp., </institution> <year> 1990. </year>
Reference-contexts: Data parallel programming originated with the sequence based operators of languages like APL and SETL. These were adapted into a parallel programming model, which was implemented in a variety of data parallel languages, including C* <ref> [TMC90] </ref> and High Performance Fortran [Zos93], and a huge number of others.
Reference: [Zob92a] <author> Angelika Zobel. </author> <title> Program structure as a basis for parallelizing global register alloc ation. </title> <booktitle> In Proceedings of the International Conference on Computer Language. IEEE, </booktitle> <year> 1992. </year>
Reference-contexts: (sg = -&gt;splitting color (depth - 1) ) f 15 recolor from (sg); 16 g 18 g (b) Parallel version 13 line code segments may seem limiting, but Zobel showed that it is often possible to transform a register interference graph into an interval graph corresponding to straight line code <ref> [Zob92a, Zob92b] </ref>. In the study done by Zobel, 33 out of 34 example programs were translatable to interval graphs. The parallel algorithm used for this example was proposed in [MP95]. This technique uses clique separators to divide a register interference graph into two subgraphs, which can be colored independently.
Reference: [Zob92b] <author> Angelika Zobel. </author> <title> Program structure as a basis for the parallelization of global compi ler optimizations. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Carnegie Mellon University, </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: (sg = -&gt;splitting color (depth - 1) ) f 15 recolor from (sg); 16 g 18 g (b) Parallel version 13 line code segments may seem limiting, but Zobel showed that it is often possible to transform a register interference graph into an interval graph corresponding to straight line code <ref> [Zob92a, Zob92b] </ref>. In the study done by Zobel, 33 out of 34 example programs were translatable to interval graphs. The parallel algorithm used for this example was proposed in [MP95]. This technique uses clique separators to divide a register interference graph into two subgraphs, which can be colored independently.
Reference: [Zos93] <author> M. Zosel. </author> <title> High performance fortran: an overview. </title> <booktitle> In Proceedings, IEEE COMPCON '93, </booktitle> <pages> pages 132-136. </pages> <publisher> IEEE, </publisher> <year> 1993. </year> <month> 46 </month>
Reference-contexts: Data parallel programming originated with the sequence based operators of languages like APL and SETL. These were adapted into a parallel programming model, which was implemented in a variety of data parallel languages, including C* [TMC90] and High Performance Fortran <ref> [Zos93] </ref>, and a huge number of others.
References-found: 44

