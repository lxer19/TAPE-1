URL: http://www.cs.gmu.edu:80/research/gag/papers/TAI93.ps
Refering-URL: http://www.cs.gmu.edu:80/research/gag/pubs.html
Root-URL: 
Title: #1 Robust Feature Selection Algorithms  
Author: Haleh Vafaie and Kenneth De Jong 
Address: Fairfax, VA 22030  
Affiliation: Center for Artificial Intelligence George Mason University  
Abstract: Selecting a set of features which is optimal for a given task is a problem which plays an important role in a wide variety of contexts including pattern recognition, adaptive control, and machine learning. Our experience with traditional feature selection algorithms in the domain of machine learning lead to an appreciation for their computational efficiency and a concern for their brittleness. This paper describes an alternate approach to feature selection which uses genetic algorithms as the primary search component. Results are presented which suggest that genetic algorithms can be used to increase the robustness of feature selection algorithms without a significant decrease in computational efficiency. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Brodatz, P. </author> <title> A Photographic Album for Arts and Design, </title> <publisher> Dover Publishing Co., </publisher> <address> Toronto, Canada, </address> <year> 1966. </year>
Reference-contexts: In the following section we describe two classes of experiments that support this view. 5.2 Realistic Problem Sets The first example is based on texture images that were randomly selected from Brodatz album of textures <ref> [1] </ref>. These images are depicted in Figure 7.
Reference: [2] <author> De Jong, K. </author> <title> Analysis of the behavior of a class of genetic adaptive systems, </title> <type> Ph.D. Thesis, </type> <institution> Department of Computer and Communications Sciences, University of Michigan, </institution> <address> Ann Arbor, MI., </address> <year> 1975. </year>
Reference-contexts: It acts as a population perturbation operator and is a means for inserting new information into the population. This operator prevents any stagnation that might occur during the search process. Genetic algorithms have demonstrated substantial improvement over a variety of random and local search methods <ref> [2] </ref>. This is accomplished by their ability to exploit accumulating information about an initially unknown search space in order to bias subsequent search into promising subspaces. <p> Experimental Results In performing the experiments reported here, the SBS algorithm was used as described above. For the GA approach, GENESIS [6], a general purpose genetic algorithm program, was used with the standard parameter settings recommended in <ref> [2] </ref>: a population size=50, a mutation rate= 0.001, and a crossover rate=0.6.
Reference: [3] <author> De Jong, K. </author> <title> Learning with Genetic Algorithms : An overview, </title> <journal> Machine Learning Vol. </journal> <volume> 3, </volume> <publisher> Kluwer Academic publishers, </publisher> <year> 1988. </year>
Reference-contexts: Since GAs are basically a domain independent search technique, they are ideal for applications where domain knowledge and theory is difficult or impossible to provide <ref> [3] </ref>. The main issues in applying GAs to any problem are selecting an appropriate representation and an adequate evaluation function. Since GAs require the same kind of evaluation function as the SBS algorithm, it can be used without modification.
Reference: [4] <author> Devijver, P., and Kittler, J. </author> <title> PATTERN RECOGNITION: A STATISTICAL APPROACH, </title> <publisher> Prentice Hall, </publisher> <year> 1982. </year>
Reference-contexts: Sources of Brittleness The SBS algorithm is a search procedure that starts with the complete set of features, and discards one feature at a time until the desired number of features have been reached <ref> [4] </ref>. At a particular stage, there are m features remaining. To determine which (if any) feature to remove next, each of the m features are evaluated as a candidate for removal by temporarily removing it and computing the effects via the criterion function. <p> Since this can be a computationally expensive process for large data sets, a heuristic evaluation function taken from the feature selection literature was used instead. The heuristic involves selecting feature subsets which maximally separate classes using a Euclidean distance measure <ref> [4] </ref>.
Reference: [5] <author> Dom, B., Niblack, W., and Sheinvald, J. </author> <title> Feature selection with stochastic complexity, </title> <booktitle> Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <address> Rosemont, IL., </address> <year> 1989. </year>
Reference-contexts: There are two main approaches to avoiding the combinatorial explosion as the number of candidate features grows. The first involves developing problem specific strategies (heuristics) which use domain knowledge to prune the feature space to a manageable size <ref> [5] </ref>. The second approach is to use generic heuristics (primarily hill-climbing algorithms) when domain knowledge is costly to exploit or unavailable [8].
Reference: [6] <author> Grefenstette, John J. </author> <type> Technical Report CS-83-11, </type> <institution> Computer Science Dept., Vanderbilt Univ., </institution> <year> 1984. </year>
Reference-contexts: This eliminates the need for designing new genetic operators, or making any other changes to the standard form of genetic algorithms. 5. Experimental Results In performing the experiments reported here, the SBS algorithm was used as described above. For the GA approach, GENESIS <ref> [6] </ref>, a general purpose genetic algorithm program, was used with the standard parameter settings recommended in [2]: a population size=50, a mutation rate= 0.001, and a crossover rate=0.6.
Reference: [7] <author> Holland, J. H.. </author> <title> Adaptation in Natural and Artificial Systems, </title> <publisher> University of Michigan Press, </publisher> <address> Ann Arbor, MI., </address> <year> 1975. </year>
Reference-contexts: In the next section this approach is described in more detail. 4. Fe atur e Sel e cti on Usi ng G e neti c Algorithms Genetic algorithms (GAs), a form of inductive learning strategy, are adaptive search techniques initially introduced by Holland <ref> [7] </ref>.
Reference: [8] <author> Kittler, J. </author> <title> Feature set search algorithms, in Pattern Recognition and Signal Processing, </title> <editor> C.H. Chen, Ed., Sijthoff and Noordhoff, </editor> <address> The Netherlands, </address> <year> 1978. </year>
Reference-contexts: The first involves developing problem specific strategies (heuristics) which use domain knowledge to prune the feature space to a manageable size [5]. The second approach is to use generic heuristics (primarily hill-climbing algorithms) when domain knowledge is costly to exploit or unavailable <ref> [8] </ref>. In the case of texture recognition, we found ourselves in the second camp: lots of possible features, but little in the way of domain knowledge to assist the search process. We adopted a feature selection algorithm from the literature which involved the basic components illustrated in Figure 1.
Reference: [9] <author> Vafaie, H., and De Jong, K.A., </author> <title> Improving the performance of a Rule Induction System Using Genetic Algorithms, </title> <booktitle> Proceedings of the First International Workshop on MULTISTRATEGY LEARNING, </booktitle> <address> Harpers Ferry, </address> <publisher> W. </publisher> <address> Virginia, USA, </address> <year> 1991. </year>
Reference-contexts: features were then randomly extracted from an arbitrary selected area of 30 by 30 pixels from each of the chosen textures.The goal of this experiment was to find an optimal set of features to be used by AQ15 in order to induce texture classification rules (for a detailed description, see <ref> [9] </ref>). In order to obtain precise measurements of the classification accuracy associated with a particular feature set, AQ15 had to be run to produce the rules, and then the rules had to be tested for accuracy.
References-found: 9

