URL: ftp://ftp.cs.caltech.edu/tr/cs-tr-96-12.ps.Z
Refering-URL: ftp://ftp.cs.caltech.edu/tr/INDEX.html
Root-URL: http://www.cs.caltech.edu
Title: A Parallel Programming Model with Sequential Semantics  
Degree: Thesis by John Thornley In Partial Fulfillment of the Requirements for the Degree of Doctor of Philosophy  
Note: (Submitted May 20th, 1996)  
Date: 1996  
Address: Pasadena, California  
Affiliation: California Institute of Technology  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> W. B. Ackerman and J. B. Dennis. </author> <title> VAL|a value-oriented algorithmic language: Preliminary reference manual. </title> <type> Technical Report TR-218, </type> <institution> MIT Laboratory for Computer Science, Cambridge, Massachusetts, </institution> <month> June </month> <year> 1979. </year>
Reference: [2] <author> William B. Ackerman. </author> <title> Data flow languages. </title> <journal> IEEE Computer, </journal> <volume> 15(2) </volume> <pages> 15-25, </pages> <month> February </month> <year> 1982. </year>
Reference-contexts: Since the mid-1970s, single-assignment variables have been used for run-time synchronization in parallel dataflow languages such as Id [9][94], Val [1][88], and Sisal [38][87]. A review of the principles and early development of parallel dataflow programming is given by Ackerman <ref> [2] </ref>. Since the early 1980s, single-assignment variables have been used for run-time synchronization in parallel logic programming languages such as Concurrent Pro-log [109][110], Parlog [27][28], and Strand [43]. A review of the principles and history of parallel logic programming is given by Shapiro [113].
Reference: [3] <author> Ada 95 Reference Manual. </author> <title> International Organization for Standardization, </title> <month> January </month> <year> 1995. </year> <note> International Standard ANSI/ISO/IEC-8652:1995. </note>
Reference-contexts: The updated Ada language, Ada 95 <ref> [3] </ref>, is a superset of Ada 83 that adds: (xi) additional support for object-oriented programming, (xii) relaxation of some restrictions on access types, (xiii) support for extensible and hierarchical library packages, (xiv) additional communication and synchronization constructs, (xv) additional standardized libraries, and (xvi) additional support for systems programming, real-time systems, distributed <p> As an example, we outline the integration of our parallel programming model with the standard Ada tasking model. 8.5.1 The Ada Tasking Model The Ada tasking model <ref> [3, section 9] </ref> is a shared-memory parallel programming model with a powerful set of structured constructs for task creation and termination, communication and synchronization between tasks, and control of task timing and scheduling. <p> Data distribution and process mapping are an integral part of the design of efficient algorithms for distributed-memory computer systems. Methods for data distribution include: (i) distribution specifications separate from the program, e.g., Ada 95 <ref> [3, Annex E] </ref>, (ii) annotations to data declarations, e.g., HPF [40][74], and (iii) dynamic memory allocation on different nodes, e.g., PCN [24][42], CC++ [22], Fortran M [45], PVM [119], and MPI [34][115].
Reference: [4] <author> Eugene Albert, Joan D. Lukas, and Guy L. Steele, Jr. </author> <title> Data parallel computers and the FORALL statement. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13(2) </volume> <pages> 185-192, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: The Occam replicator construct can be either sequential or parallel, depending on whether the seq or par keyword is used. Many data-parallel notations [58] incorporate more restricted (usually synchronous) forms of parallel loops, such as the FORALL construct <ref> [4] </ref> in some Fortran dialects. The INDEPENDENT directive of HPF (High Performance Fortran) [40][74] is an assertion that the iterations of a loop are independent and can be executed in parallel.
Reference: [5] <author> George S. Almasi and Allan Gottlieb. </author> <title> Highly Parallel Computing. </title> <address> Benjamin/Cum-mings, Redwood City, California, </address> <note> second edition, </note> <year> 1994. </year>
Reference-contexts: Performance is affected both by the cost of the invalidation operation and by the cost of any subsequent read of the data item by the second processor. Cache coherence mechanisms for shared-memory multiprocessors are surveyed by Lilja [83]. The key issues are summarized by Almasi and Gottlieb <ref> [5, section 10.3.2] </ref>. 62 In our parallel programming model, it is erroneous for threads to share mutable data in the manner described above, between synchronization operations. Since synchronization operations are required to be relatively infrequent for reasons of efficiency, the cost of cache invalidation is insignificant in most practical programs. <p> Examples include the family of parallel functional languages based on Lisp [50][54][68], the family of concurrent logic programming languages based on Prolog [28][110][113], and dataflow languages such as Id [94], Val [88], and Sisal [38]. A review of parallel declarative languages is given by Almasi and Gottlieb <ref> [5, Section 5.3] </ref> and a collection of papers describing individual parallel functional and dataflow languages was edited by Szymanski [120]. A compiler for a parallel declarative language transforms a declarative source program into an equivalent parallel object program for a given target architecture.
Reference: [6] <author> American National Standards Institute, Inc. </author> <title> The Programming Language Ada Reference Manual. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, Germany, </address> <year> 1983. </year> <month> ANSI/MIL-STD-1815A. </month>
Reference-contexts: Single-assignment types, parallel composition of statements, and a parallel for-loop statement are integrated with the types and statements of a small sequential subset of Ada <ref> [6] </ref>. Parallel Declarative Ada programs can be developed to be identical to equivalent sequential Ada programs. A recent parallel programming language, CC++ [21][22], integrates single-assignment types, parallel composition of statements, a parallel for-loop statement, and other extensions with the full C++ [117] language. <p> In Chapter 8, we consider the integration of our model with the standard Ada tasking model. Ada is an established and internationally standardized programming language that supports constructs typical of modern imperative languages. The original Ada language, Ada 83 <ref> [6] </ref>, incorporates: (i) integer, floating-point, fixed-point, enumeration, character, boolean, array, record, and access (i.e., pointer) data types, (ii) assignment, block, if-then-else, case, while-loop, for-loop, loop-exit, function-return, and procedure-call statements, (iii) procedures and functions with in, in out, and out mode parameters, (iv) subprogram and operator overloading, (v) packages for data abstraction,
Reference: [7] <author> Christiana Amza, Alan L. Cox, Sandhya Dwarkadas, Pete Keleher, Honghui Lu, Ra-makrishnan Rajamony, Weimin Yu, and Willy Zwaenepoel. Treadmarks: </author> <title> Shared memory on networks of workstations. </title> <journal> IEEE Computer, </journal> <volume> 29(2) </volume> <pages> 18-28, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: Distributed shared memory is implemented in hardware by machines such as the Stanford DASH and FLASH and the Kendall Square Research KSR-1 and KSR-2. Distributed shared memory is implemented in software by packages such as Ivy [82] and Treadmarks <ref> [7] </ref> that are designed to run on top of networks of workstations and other distributed-memory computer systems. Most distributed shared-memory systems implement a model with both non-shared local memory and shared global memory.
Reference: [8] <author> Gregory R. Andrews. </author> <title> Concurrent Programming: </title> <booktitle> Principles and Practice. </booktitle> <address> Ben-jamin/Cummings, Redwood City, California, </address> <year> 1991. </year>
Reference-contexts: This approach to parallel programming is supported by languages such as Ada [3][6] and Modula-3 [93], and by libraries such as p4 [18], Pthreads [97], PVM [119], and MPI [34][115]. Comprehensive reviews and discussions of programming models and notations with explicit parallel semantics are given by Andrews <ref> [8] </ref>, Bal et al. [11], and Pancake [96]. Explicit parallel programming gives direct control of execution performance because of the close correlation between the programming model and the operation of a multiprocessor computer system.
Reference: [9] <author> Arvind, K. P. Gostelow, and W. Plouffe. </author> <title> An asynchronous programming language and computing machine. </title> <type> Technical Report TR-114a, </type> <institution> Department of Information and Computer Science, University of California, Irvine, </institution> <month> December </month> <year> 1978. </year> <month> 229 </month>
Reference: [10] <author> John Backus. </author> <title> Can programming be liberated from the von Neumann style? a functional style and its algebra of programs. </title> <journal> Communications of the ACM, </journal> <volume> 21(8) </volume> <pages> 613-641, </pages> <month> August </month> <year> 1978. </year> <note> 1977 ACM Turing Award Lecture. </note>
Reference: [11] <author> Henri. E. Bal, Jennifer. G. Steiner, and Andrew. S. Tanenbaum. </author> <title> Programming languages for distributed computing systems. </title> <journal> ACM Computing Surveys, </journal> <volume> 21(3) </volume> <pages> 261-322, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: Comprehensive reviews and discussions of programming models and notations with explicit parallel semantics are given by Andrews [8], Bal et al. <ref> [11] </ref>, and Pancake [96]. Explicit parallel programming gives direct control of execution performance because of the close correlation between the programming model and the operation of a multiprocessor computer system.
Reference: [12] <author> Utpal Banerjee, Rudolf Eigenmann, Alexandru Nicolau, and David A. Padua. </author> <title> Automatic program parallelization. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 81(2) </volume> <pages> 211-243, </pages> <month> February </month> <year> 1993. </year>
Reference: [13] <author> Forest Baskett and Alan Jay Smith. </author> <title> Interference in multiprocessor systems with interleaved memory. </title> <journal> Communications of the ACM, </journal> <volume> 19(6) </volume> <pages> 327-334, </pages> <month> June </month> <year> 1976. </year>
Reference: [14] <author> Guy E. Blelloch. </author> <title> Programming parallel algorithms. </title> <journal> Communications of the ACM, </journal> <volume> 39(6) </volume> <pages> 85-97, </pages> <month> March </month> <year> 1996. </year>
Reference: [15] <author> Guy E. Blelloch, Jonathan C. Hardwick, Jay Sipelstein, Marco Zagha, and Siddhartha Chatterjee. </author> <title> Implementation of a portable nested data-parallel language. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 21(1) </volume> <pages> 4-14, </pages> <month> April </month> <year> 1994. </year>
Reference: [16] <author> William Blume, Rudolf Eigenmann, Jay Hoeflinger, David Padua, Paul Petersen, and Lawrence Rauchweger. </author> <title> Automatic detection of parallelism: A grand challenge for high-performance computing. </title> <journal> IEEE Parallel and Distributed Technology, </journal> <volume> 2(3) </volume> <pages> 37-47, </pages> <month> Fall </month> <year> 1994. </year>
Reference: [17] <author> F. W. Burton. </author> <title> Functional programming for concurrent and distributed computing. </title> <journal> The Computer Journal, </journal> <volume> 30(5) </volume> <pages> 437-450, </pages> <month> October </month> <year> 1987. </year>
Reference: [18] <author> Ralph M. Butler and Ewing L. Lusk. </author> <title> Monitors, messages, and clusters: The p4 parallel programming system. </title> <journal> Parallel Computing, </journal> <volume> 20(4) </volume> <pages> 547-564, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: The semantics of parallel execution is usually defined to be equivalent to an interleaving of the actions of the parallel processes. This approach to parallel programming is supported by languages such as Ada [3][6] and Modula-3 [93], and by libraries such as p4 <ref> [18] </ref>, Pthreads [97], PVM [119], and MPI [34][115]. Comprehensive reviews and discussions of programming models and notations with explicit parallel semantics are given by Andrews [8], Bal et al. [11], and Pancake [96].
Reference: [19] <author> David Cann. </author> <title> Retire Fortran? A debate rekindled. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 81-89, </pages> <month> August </month> <year> 1992. </year>
Reference: [20] <author> K. Mani Chandy and Ian Foster. </author> <title> A notation for deterministic cooperating processes. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 6(8) </volume> <pages> 863-871, </pages> <month> August </month> <year> 1995. </year> <month> 230 </month>
Reference: [21] <author> K. Mani Chandy and Carl Kesselman. </author> <title> CC++: A declarative concurrent object oriented programming language. </title> <type> Technical Report CS-TR-92-01, </type> <institution> Computer Science Department, California Institute of Technology, </institution> <year> 1992. </year>
Reference: [22] <author> K. Mani Chandy and Carl Kesselman. </author> <title> CC++: A declarative concurrent object-oriented programming notation. </title> <editor> In Gul Agha, Peter Wegner, and Akinori Yonezawa, editors, </editor> <booktitle> Research Directions in Concurrent Object Oriented Programming, </booktitle> <pages> pages 281-313. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1993. </year>
Reference-contexts: Methods for data distribution include: (i) distribution specifications separate from the program, e.g., Ada 95 [3, Annex E], (ii) annotations to data declarations, e.g., HPF [40][74], and (iii) dynamic memory allocation on different nodes, e.g., PCN [24][42], CC++ <ref> [22] </ref>, Fortran M [45], PVM [119], and MPI [34][115]. Methods for process mapping include: (i) mapping specifications separate from the program, e.g., Ada 95, (ii) annotations and arguments to process creation, e.g., PCN, CC++, PVM and MPI, and (iii) dynamic process migration, e.g., the Concurrent Graph Library [122].
Reference: [23] <author> K. Mani Chandy and Stephen Taylor. </author> <title> A primer for Program Composition Notation. </title> <type> Technical Report CS-TR-90-10, </type> <institution> Computer Science Department, California Institute of Technology, </institution> <year> 1990. </year>
Reference: [24] <author> K. Mani Chandy and Stephen Taylor. </author> <title> An Introduction to Parallel Programming. </title> <publisher> Jones and Bartlett, </publisher> <address> Boston, Massachusetts, </address> <year> 1992. </year>
Reference-contexts: Designed in the late 1980s, PCN [23][24][42] is the first implemented language that we know of to incorporate single-assignment and mutable types and parallel and sequential composition of statements. A formal operational semantics and proof rules have been developed for PCN <ref> [24, Part III] </ref>. Much of the structure of PCN is derived from parallel logic programming. In particular, neither loops nor functions are supported, and single-assignment and mutable types belong to separate type systems with completely different compatibility rules.
Reference: [25] <author> Barbara Chapman, Hans Zima, and Piyush Mechrota. </author> <title> Extending HPF for advanced data-parallel applications. </title> <journal> IEEE Parallel and Distributed Technology, </journal> <volume> 2(3) </volume> <pages> 59-70, </pages> <month> Fall </month> <year> 1994. </year>
Reference: [26] <author> A. Church and J. B. Rosser. </author> <title> Some properties of conversions. </title> <journal> Transactions of the American Mathematical Society, </journal> <volume> 39 </volume> <pages> 472-482, </pages> <year> 1936. </year>
Reference: [27] <author> Keith Clark and Steve Gregory. </author> <title> PARLOG: Parallel programming in logic. </title> <type> Technical Report DOC 84/4, </type> <institution> Department of Computing, Imperial College, </institution> <address> London, </address> <month> April </month> <year> 1983. </year>
Reference: [28] <author> Keith Clark and Steve Gregory. </author> <title> PARLOG: Parallel programming in logic. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 8(1) </volume> <pages> 1-49, </pages> <month> January </month> <year> 1986. </year>
Reference: [29] <author> Mark J. Clement and Michael J. Quinn. </author> <title> Overlapping computations, communications and I/O in parallel sorting. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 28(2) </volume> <pages> 162-172, </pages> <month> August </month> <year> 1995. </year>
Reference: [30] <author> W. F. Clocksin and C. S. Mellish. </author> <title> Programming in Prolog. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, Germany, third edition, </address> <year> 1984. </year>
Reference: [31] <author> Keith D. Cooper, Mary Hall, Ken Kennedy, and Linda Torczon. </author> <title> Interprocedural analysis and optimization. </title> <journal> Communications of Pure and Applied Mathematics, 48(9-10):947-1003, September-October 1995. </journal> <volume> 231 </volume>
Reference: [32] <author> Keith D. Cooper, Mary W. Hall, Robert T. Hood, Ken Kennedy, Kathryn S. McKin-ley, John M. Mellorcrummey, Linda Torczon, and Scott K. Warren. </author> <title> The Parascope parallel programming environment. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 81(2) </volume> <pages> 244-263, </pages> <month> February </month> <year> 1993. </year>
Reference: [33] <author> E. W. Dijkstra. </author> <title> Co-operating sequential processes. </title> <editor> In F. Genuys, editor, </editor> <booktitle> Programming Languages, </booktitle> <pages> pages 43-112. </pages> <publisher> Academic Press, Inc., </publisher> <address> New York, New York, </address> <year> 1968. </year>
Reference-contexts: Wirth suggested the use of and in place of ";" between statements. In 1968, Dijkstra <ref> [33] </ref> proposed the parbegin-parend notation for parallel composition of statements. The first major language to support parallel composition of statements was Algol 68 [133], which incorporated the parbegin-parend notation. Other early notations to support parallel composition of statements include CSP [60][61] and Occam [86] (a programming language derived from CSP).
Reference: [34] <author> Jack Dongarra, David Walker, et al. </author> <title> Special issue - MPI a message-passing interface standard. </title> <journal> International Journal of Supercomputer Applications and High Performance Computing, </journal> <pages> 8(3-4), </pages> <month> Fall-Winter </month> <year> 1994. </year>
Reference: [35] <author> R. Kent Dybvig. </author> <title> The SCHEME Programming Language. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1987. </year>
Reference: [36] <author> D. J. Evans and N. Y. Yousif. </author> <title> Analysis of the performance of the parallel quicksort method. </title> <journal> BIT, </journal> <volume> 25 </volume> <pages> 106-112, </pages> <year> 1985. </year>
Reference: [37] <author> J. T. Feo, </author> <title> editor. A Comparative Study of Parallel Programming Languages: The Salishan Problems, </title> <booktitle> volume 6 of Special Topics in Supercomputing. </booktitle> <publisher> North-Holland, </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1992. </year>
Reference-contexts: The Paraffins problem is discussed by Turner [131] and is one of the Salishan problems <ref> [37] </ref>. The Salishan problems are a set of problems proposed at the 1988 Salishan High-Speed Computing Conference as a standard by which to compare parallel programming notations.
Reference: [38] <author> John T. Feo, David C. Cann, and Rodney R. Oldehoeft. </author> <title> A report on the Sisal language project. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 10(4) </volume> <pages> 349-366, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: Examples include the family of parallel functional languages based on Lisp [50][54][68], the family of concurrent logic programming languages based on Prolog [28][110][113], and dataflow languages such as Id [94], Val [88], and Sisal <ref> [38] </ref>. A review of parallel declarative languages is given by Almasi and Gottlieb [5, Section 5.3] and a collection of papers describing individual parallel functional and dataflow languages was edited by Szymanski [120].
Reference: [39] <author> Robert W. Floyd. </author> <title> Assigning meanings to programs. </title> <booktitle> In Proceedings of a Symposium in Applied Mathematics of the American Mathematical Society, </booktitle> <pages> pages 19-32. </pages> <publisher> American Mathematical Society, </publisher> <year> 1967. </year>
Reference-contexts: The specification says nothing about the behavior of the program if the precondition does not hold in the initial state. Specifications of this form were introduced by Hoare [59] (based on earlier work by Floyd <ref> [39] </ref>), and are described in more detail by Gries [51]. 4.1.2 Equivalence of Programs In this thesis, where we say that two programs, P and Q, are equivalent, we mean that for any given specification, P satisfies the specification if and only if Q satisfies the specification. 8 Pre, Post :
Reference: [40] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran language specification/journal of development. </title> <booktitle> Scientific Programming, </booktitle> <pages> 2(1-2), </pages> <note> Spring and Summer 1993. </note>
Reference: [41] <author> Ian Foster. </author> <title> Task parallelism and high-performance languages. </title> <journal> IEEE Parallel and Distributed Technology, </journal> <volume> 2(3) </volume> <pages> 27-36, </pages> <month> Fall </month> <year> 1994. </year>
Reference: [42] <author> Ian Foster, Robert Olson, and Steven Tuecke. </author> <title> Productive parallel programming: The PCN approach. </title> <journal> Scientific Programming, </journal> <volume> 1(1) </volume> <pages> 51-66, </pages> <month> Fall </month> <year> 1992. </year> <month> 232 </month>
Reference: [43] <author> Ian Foster and Stephen Taylor. Strand: </author> <title> New Concepts in Parallel Programming. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1990. </year>
Reference-contexts: A review of the principles and early development of parallel dataflow programming is given by Ackerman [2]. Since the early 1980s, single-assignment variables have been used for run-time synchronization in parallel logic programming languages such as Concurrent Pro-log [109][110], Parlog [27][28], and Strand <ref> [43] </ref>. A review of the principles and history of parallel logic programming is given by Shapiro [113]. Shapiro also edited a collection of seminal papers on parallel logic programming [111][112].
Reference: [44] <author> Ian Foster and Stephen Taylor. </author> <title> A compiler approach to scalable concurrent-program design. </title> <journal> ACM Transactions of Programming Languages and Systems, </journal> <volume> 16(3) </volume> <pages> 577-604, </pages> <month> May </month> <year> 1994. </year>
Reference: [45] <author> Ian T. Foster and K. Mani Chandy. </author> <title> Fortran M: A language for modular parallel programming. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 26(1) </volume> <pages> 24-35, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: Methods for data distribution include: (i) distribution specifications separate from the program, e.g., Ada 95 [3, Annex E], (ii) annotations to data declarations, e.g., HPF [40][74], and (iii) dynamic memory allocation on different nodes, e.g., PCN [24][42], CC++ [22], Fortran M <ref> [45] </ref>, PVM [119], and MPI [34][115]. Methods for process mapping include: (i) mapping specifications separate from the program, e.g., Ada 95, (ii) annotations and arguments to process creation, e.g., PCN, CC++, PVM and MPI, and (iii) dynamic process migration, e.g., the Concurrent Graph Library [122].
Reference: [46] <author> Christine Fricker. </author> <title> On memory contention problems in vector multiprocessors. </title> <journal> IEEE Transactions of Computers, </journal> <volume> 44(1) </volume> <pages> 92-105, </pages> <month> January </month> <year> 1995. </year>
Reference: [47] <author> Mike Galles and Eric Williams. </author> <title> Performance optimizations, implementation, and verification of the SGI Challenge multiprocessor. </title> <booktitle> In Proceedings of the 27th Annual IEEE Conference on Systems Science, Architecture Volume, </booktitle> <pages> pages 134-144, </pages> <address> Wailea, Hawaii, </address> <month> January 4-7 </month> <year> 1994. </year>
Reference-contexts: Main memory is interleaved to allow multiple concurrent memory accesses. The hardware is entirely responsible for the transfer of data between main memory and cache memory, and for maintaining cache coherence between the processors. The SGI Challenge architecture is described by Galles and Williams <ref> [47] </ref> and in SGI technical documentation [108]. <p> The major problem with symmetric multiprocessors is limited scalability. Although caching and memory interleaving can overcome this problem for a moderate number of processors, memory contention eventually becomes a performance bottleneck for a large number of processors. Our own experiments and those of others <ref> [47] </ref> indicate that bus-based symmetric multiprocessors can scale to at least many tens of processors with current technology, but scalability to many hundreds or thousands of processors is unlikely. 9.3 Distributed Memory A distributed-memory computer system consists of a group of processing nodes, each with its own local memory, that can
Reference: [48] <author> Kourosh Gharachorloo, Daniel Lenoski, James Laudon, Phillip Gibbons, Anoop Gupta, and John Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Distributed shared memory can be implemented in hardware through scalable cache coherence protocols that use distributed cache directories. For increased performance, these protocols generally support weak memory consistency models such as the release consistency model <ref> [48] </ref> instead of the stronger sequential memory consistency model [76] supported by most bus-based shared-memory multiprocessors. Release consistency is equivalent to sequential consistency for programs that do not violate certain restrictions on access to shared variables between synchronization points.
Reference: [49] <author> E. W. Giering, Frank Mueller, and T. P. Baker. </author> <title> Features of the GNU Ada runtime library. </title> <booktitle> In Proceedings of ACM TRI-Ada '94, </booktitle> <pages> pages 93-103, </pages> <address> Baltimore, Maryland, </address> <month> November 6-11 </month> <year> 1994. </year>
Reference-contexts: However, this inefficiency does not affect the validity of our results, since sequential and parallel programs are slowed down equally. GNAT compiles Ada tasking constructs into calls to the SGI-Irix implementation of the Pthreads (POSIX threads) library [97], as described by Giering, Muller, and Baker <ref> [49] </ref>. At run time, threads are dynamically scheduled across a user-specified number of sprocs (system processes), and sprocs are dynamically scheduled across processors. Sprocs share the pool of processors with the other processes running concurrently on the system.
Reference: [50] <author> Ron Goldman and Richard P. Gabriel. </author> <title> Qlisp: </title> <booktitle> Parallel processing in Lisp. IEEE Software, </booktitle> <pages> pages 51-59, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: Single-assignment variables are in many ways similar to the concept of futures incor 14 porated in some parallel functional programming notations. In the mid-1980s, futures were incorporated in parallel functional programming languages such as Multilisp [54] and Qlisp <ref> [50] </ref>.
Reference: [51] <editor> David Gries. </editor> <booktitle> The Science of Programming. </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York, New York, </address> <year> 1981. </year>
Reference-contexts: The specification says nothing about the behavior of the program if the precondition does not hold in the initial state. Specifications of this form were introduced by Hoare [59] (based on earlier work by Floyd [39]), and are described in more detail by Gries <ref> [51] </ref>. 4.1.2 Equivalence of Programs In this thesis, where we say that two programs, P and Q, are equivalent, we mean that for any given specification, P satisfies the specification if and only if Q satisfies the specification. 8 Pre, Post : fPreg P fPostg , fPreg Q fPostg In other
Reference: [52] <author> Thomas Gross, David R. O'Hallaron, and Jaspal Subhlok. </author> <title> Task parallelism in a high performance fortran framework. </title> <journal> IEEE Parallel and Distributed Technology, </journal> <volume> 2(3) </volume> <pages> 16-26, </pages> <month> Fall </month> <year> 1994. </year>
Reference: [53] <author> Mary W. Hall, Saman P. Amarasinghe, Brian R. Murphy, Shih-Wei Liao, and Mon-ica S. Lam. </author> <title> Detecting coarse-grain parallelism using an interprocedural parallelizing compiler. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <address> San Diego, </address> <month> December 3-8 </month> <year> 1995. </year> <month> 233 </month>
Reference: [54] <author> Robert H. Halstead, Jr. </author> <title> Multilisp: A language for concurrent symbolic computation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(4) </volume> <pages> 501-538, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: Shapiro also edited a collection of seminal papers on parallel logic programming [111][112]. Single-assignment variables are in many ways similar to the concept of futures incor 14 porated in some parallel functional programming notations. In the mid-1980s, futures were incorporated in parallel functional programming languages such as Multilisp <ref> [54] </ref> and Qlisp [50].
Reference: [55] <author> Philip J. Hatcher and Michael J. Quinn. </author> <title> Data-Parallel Programming on MIMD Computers. Scientific and Engineering Computation. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mas-sachusetts, </address> <year> 1991. </year>
Reference-contexts: Subsequent work <ref> [55] </ref>[56][100] describes techniques for the implementation of data-parallel programs on MIMD (Multiple Instruction stream, Multiple Data stream) computers. Examples of data-parallel languages include C* [102], Dataparallel C [55], pC++ [80], CM Fortran, Fortran 90 [90], HPF (High Performance Fortran) [40][74], and NESL [14][15].
Reference: [56] <author> Philip J. Hatcher, Michael J. Quinn, Anthony J. Lapadula, Bradley K. Seevers, Ray J. Anderson, and Robert R. Jones. </author> <title> Data-parallel programming on MIMD computers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 377-383, </pages> <month> July </month> <year> 1991. </year>
Reference: [57] <author> W. Daniel Hillis. </author> <title> The Connection Machine. </title> <publisher> ACM Distinguished Dissertation. MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1985. </year>
Reference-contexts: Data-parallel programming was originally described [58] in the context of programming SIMD (Single Instruction stream, Multiple Data stream) computers such as the Connection Machine <ref> [57] </ref>. Subsequent work [55][56][100] describes techniques for the implementation of data-parallel programs on MIMD (Multiple Instruction stream, Multiple Data stream) computers. Examples of data-parallel languages include C* [102], Dataparallel C [55], pC++ [80], CM Fortran, Fortran 90 [90], HPF (High Performance Fortran) [40][74], and NESL [14][15].
Reference: [58] <author> W. Daniel Hillis and Guy L. Steele, Jr. </author> <title> Data parallel programming. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1170-1183, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: Occam is the earliest implemented language that we know of to integrate quantified parallel composition of statements with the syntax for sequential iteration. The Occam replicator construct can be either sequential or parallel, depending on whether the seq or par keyword is used. Many data-parallel notations <ref> [58] </ref> incorporate more restricted (usually synchronous) forms of parallel loops, such as the FORALL construct [4] in some Fortran dialects. The INDEPENDENT directive of HPF (High Performance Fortran) [40][74] is an assertion that the iterations of a loop are independent and can be executed in parallel. <p> Data-parallel programming was originally described <ref> [58] </ref> in the context of programming SIMD (Single Instruction stream, Multiple Data stream) computers such as the Connection Machine [57]. Subsequent work [55][56][100] describes techniques for the implementation of data-parallel programs on MIMD (Multiple Instruction stream, Multiple Data stream) computers.
Reference: [59] <author> C. A. R. Hoare. </author> <title> An axiomatic basis for computer programming. </title> <journal> Communications of the ACM, </journal> <volume> 12(10) </volume> <pages> 576-583, </pages> <month> October </month> <year> 1969. </year>
Reference-contexts: The specification says nothing about the behavior of the program if the precondition does not hold in the initial state. Specifications of this form were introduced by Hoare <ref> [59] </ref> (based on earlier work by Floyd [39]), and are described in more detail by Gries [51]. 4.1.2 Equivalence of Programs In this thesis, where we say that two programs, P and Q, are equivalent, we mean that for any given specification, P satisfies the specification if and only if Q
Reference: [60] <author> C. A. R. Hoare. </author> <title> Communicating sequential processes. </title> <journal> Communications of the ACM, </journal> <volume> 21(8) </volume> <pages> 666-677, </pages> <month> August </month> <year> 1978. </year>
Reference: [61] <author> C. A. R. Hoare. </author> <title> Communicating Sequential Processes. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1985. </year>
Reference: [62] <author> Ellis Horowitz and Sartaj Sahni. </author> <title> Computing partitions with applications to the knapsack problem. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 21(2) </volume> <pages> 277-292, </pages> <month> April </month> <year> 1974. </year>
Reference-contexts: Branch-and-bound algorithms are typically used for NP-hard problems in which the search space is exponentially large with respect to the size of the problem. Examples of problems for which branch-and-bound algorithms provide efficient solutions include the 0-1 Knapsack problem <ref> [62] </ref>, the Traveling Salesman problem [84], and Integer Programming [77]. The size and shape of the search space that is traversed by a branch-and-bound algorithm cannot be predicted in advance.
Reference: [63] <author> Paul Hudak. </author> <title> Para-functional programming. </title> <journal> IEEE Computer, </journal> <volume> 19(8) </volume> <pages> 60-71, </pages> <month> August </month> <year> 1986. </year>
Reference: [64] <author> Paul Hudak. </author> <title> Conception, evolution, and application of functional programming languages. </title> <journal> ACM Computing Surveys, </journal> <volume> 21(3) </volume> <pages> 359-411, </pages> <month> September </month> <year> 1989. </year>
Reference: [65] <author> Paul Hudak. </author> <title> Para-functional programming in Haskell. </title> <editor> In Boleslaw K. Szymanski, editor, </editor> <booktitle> Parallel Functional Languages and Compilers, </booktitle> <pages> pages 159-196. </pages> <publisher> ACM Press, </publisher> <address> New York, New York, </address> <year> 1991. </year> <month> 234 </month>
Reference: [66] <editor> Paul Hudak, Simon Peyton Jones, Philip Wadler, et al. </editor> <title> Report on the programming language Haskell: A non-strict, purely functional language. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 27(5), </volume> <month> May </month> <year> 1992. </year>
Reference: [67] <author> J. Hughes. </author> <title> Why functional programming matters. </title> <journal> The Computer Journal, </journal> <volume> 32(2) </volume> <pages> 98-107, </pages> <month> April </month> <year> 1989. </year>
Reference: [68] <editor> T. Ito and R. H. Halstead, Jr., editors. </editor> <booktitle> Parallel Lisp: Languages and Systems, volume 441 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, Germany, </address> <year> 1990. </year> <booktitle> Proceedings of US/Japan Workshop on Parallel Lisp. </booktitle>
Reference-contexts: In the mid-1980s, futures were incorporated in parallel functional programming languages such as Multilisp [54] and Qlisp [50]. A collection of papers relating to parallel functional programming using futures was edited by Ito and Halstead <ref> [68] </ref>. 2.4 Integration with Sequential Imperative Programming In 1977, Kessels [71] described a conceptual framework that integrated single-assignment and mutable types, parallel and sequential composition of statements, and parallel and sequential for-loop statements.
Reference: [69] <author> Joseph JaJa and Pearl Y. Wang, </author> <title> editors. </title> <journal> Special issue on data parallel algorithms and programming. Journal of Parallel and Distributed Computing, </journal> <volume> 21(1), </volume> <month> April </month> <year> 1994. </year>
Reference-contexts: The major disadvantage of data-parallel programming is the limited range of algorithms that can be expressed. Although, efficient data-parallel algorithms have been demonstrated for wide range of applications <ref> [69] </ref>, many efficient parallel algorithms involve multiple threads of control executing different instruction streams. Some multithreaded algorithms can be expressed by extending the data-parallel programming model to allow nested data-parallel constructs, as in NESL.
Reference: [70] <author> S. L. Peyton Jones. </author> <title> Parallel implementation of functional programming. </title> <journal> The Computer Journal, </journal> <volume> 32(2) </volume> <pages> 175-186, </pages> <month> April </month> <year> 1989. </year>
Reference: [71] <author> J. L. W. Kessels. </author> <title> A conceptual framework for a nonprocedural programming language. </title> <journal> Communications of the ACM, </journal> 20(12) 906-913, December 1977. 
Reference-contexts: In the mid-1980s, futures were incorporated in parallel functional programming languages such as Multilisp [54] and Qlisp [50]. A collection of papers relating to parallel functional programming using futures was edited by Ito and Halstead [68]. 2.4 Integration with Sequential Imperative Programming In 1977, Kessels <ref> [71] </ref> described a conceptual framework that integrated single-assignment and mutable types, parallel and sequential composition of statements, and parallel and sequential for-loop statements. However, we are not aware of this framework directly leading to the design and implementation of any actual programming language.
Reference: [72] <author> Donald E. Knuth. </author> <title> The Art of Computer Programming, Volume 1: Fundamental Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <note> second edition, </note> <year> 1973. </year>
Reference-contexts: 1. a bond-centered paraffin: two radicals of size exactly k=2 bonded together, or 2. a carbon-centered paraffin: a carbon atom bonded to four radicals, each of size less than k=2, with combined size k 1. and uniqueness of this representation of paraffins follows from Knuth's theorems regarding enumeration of trees <ref> [72, section 2.3.4.4] </ref>. The Paraffins problem is discussed by Turner [131] and is one of the Salishan problems [37]. The Salishan problems are a set of problems proposed at the 1988 Salishan High-Speed Computing Conference as a standard by which to compare parallel programming notations.
Reference: [73] <author> Donald E. Knuth. </author> <title> The Art of Computer Programming, Volume 3: Sorting and Searching. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1973. </year>
Reference-contexts: Program 6.1: Specification of one-deep parallel mergesort. 6.2.2 Traditional Parallel Mergesort Algorithm The standard sequential mergesort algorithm <ref> [73, section 5.2.4] </ref>[107, chapter 12] is one of the most well-known methods of sorting and is a canonical divide-and-conquer algorithm. Sequential mergesort of an array operates as follows: 1.
Reference: [74] <author> C. Koelbel, D. Loveman, R. Schrieber, G. Steele, Jr., and M. Zosel. </author> <title> The High Performance Fortran Handbook. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1994. </year>
Reference: [75] <author> Robert Kowalski. </author> <title> Algorithm = logic + control. </title> <journal> Communications of the ACM, </journal> <volume> 22(7) </volume> <pages> 424-436, </pages> <month> July </month> <year> 1979. </year>
Reference: [76] <author> Leslie Lamport. </author> <title> How to make a multiprocessor that correctly executes multiprocess programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9):690-691, </volume> <month> September </month> <year> 1979. </year>
Reference-contexts: Distributed shared memory can be implemented in hardware through scalable cache coherence protocols that use distributed cache directories. For increased performance, these protocols generally support weak memory consistency models such as the release consistency model [48] instead of the stronger sequential memory consistency model <ref> [76] </ref> supported by most bus-based shared-memory multiprocessors. Release consistency is equivalent to sequential consistency for programs that do not violate certain restrictions on access to shared variables between synchronization points. These restrictions are part of most parallel programming models, including our model.
Reference: [77] <author> A. H. Land and A. Doig. </author> <title> An automatic method of solving discrete programming problems. </title> <journal> Econometrica, </journal> <volume> 28 </volume> <pages> 495-520, </pages> <year> 1960. </year>
Reference-contexts: Branch-and-bound algorithms are typically used for NP-hard problems in which the search space is exponentially large with respect to the size of the problem. Examples of problems for which branch-and-bound algorithms provide efficient solutions include the 0-1 Knapsack problem [62], the Traveling Salesman problem [84], and Integer Programming <ref> [77] </ref>. The size and shape of the search space that is traversed by a branch-and-bound algorithm cannot be predicted in advance. Therefore, in a parallel branch-and-bound algorithm, it is not possible to efficiently partition the search space among the parallel threads in any predetermined manner.
Reference: [78] <author> Per S. Laursen. </author> <title> Simple approaches to parallel branch and bound. </title> <journal> Parallel Computing, </journal> <volume> 19(2) </volume> <pages> 143-152, </pages> <month> February </month> <year> 1993. </year> <month> 235 </month>
Reference: [79] <author> E. L. Lawler and D. E. Woods. </author> <title> Branch-and-bound methods: A survey. </title> <journal> Operations Research, </journal> <volume> 14(4) </volume> <pages> 699-719, </pages> <month> July-August </month> <year> 1966. </year>
Reference: [80] <author> Jenq Kuen Lee and Dennis Gannon. </author> <title> Object oriented parallel programming experiments and results. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <pages> pages 273-282, </pages> <address> Albuquerque, New Mexico, </address> <month> November 18-22 </month> <year> 1991. </year>
Reference-contexts: Subsequent work [55][56][100] describes techniques for the implementation of data-parallel programs on MIMD (Multiple Instruction stream, Multiple Data stream) computers. Examples of data-parallel languages include C* [102], Dataparallel C [55], pC++ <ref> [80] </ref>, CM Fortran, Fortran 90 [90], HPF (High Performance Fortran) [40][74], and NESL [14][15].
Reference: [81] <author> Daniel E. Lenoski and Wolf-Dietrich Weber. </author> <title> Scalable Shared-Memory Multiprocessing. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, California, </address> <year> 1995. </year>
Reference-contexts: Examples of distributed-memory computer systems include the Caltech Cosmic Cube, the IBM SP-1 and SP-2, the Intel Delta and Paragon, the SGI Power Challenge Array, and networks of workstations. An excellent review and comparison of shared-memory and distributed-memory computer systems is given by Lenoski and Weber <ref> [81, chapter 1] </ref>. A significant difference between a distributed-memory computer system and a shared-memory computer system is that a processing node in a distributed-memory computer 142 system cannot directly access the entire memory space. <p> These restrictions are part of most parallel programming models, including our model. A review of approaches to the implementation of distributed shared memory in hardware (and of the design of the Stanford DASH architecture) is given by Lenoski and Weber <ref> [81] </ref>. Distributed shared memory can be implemented in software through adaptation of paging mechanisms to automatically trap accesses to nonresident remote memory pages and copy those pages to local memory. Coherence of replicated memory pages is maintained using protocols similar to those used to maintain cache coherence in hardware.
Reference: [82] <author> Kai Li and Paul Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: Distributed shared memory is implemented in hardware by machines such as the Stanford DASH and FLASH and the Kendall Square Research KSR-1 and KSR-2. Distributed shared memory is implemented in software by packages such as Ivy <ref> [82] </ref> and Treadmarks [7] that are designed to run on top of networks of workstations and other distributed-memory computer systems. Most distributed shared-memory systems implement a model with both non-shared local memory and shared global memory.
Reference: [83] <author> David J. Lilja. </author> <title> Cache coherence in large-scale shared-memory multiprocessors: Issues and comparisons. </title> <journal> ACM Computing Surveys, </journal> <volume> 25(3) </volume> <pages> 303-338, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: Performance is affected both by the cost of the invalidation operation and by the cost of any subsequent read of the data item by the second processor. Cache coherence mechanisms for shared-memory multiprocessors are surveyed by Lilja <ref> [83] </ref>. The key issues are summarized by Almasi and Gottlieb [5, section 10.3.2]. 62 In our parallel programming model, it is erroneous for threads to share mutable data in the manner described above, between synchronization operations.
Reference: [84] <author> John D. C. Little, Kalta G. Murty, Dura W. Sweeney, and Caroline Karel. </author> <title> An algorithm for the traveling salesman problem. </title> <journal> Operations Research, </journal> <volume> 11(6) </volume> <pages> 972-989, </pages> <month> November-December </month> <year> 1963. </year>
Reference-contexts: Branch-and-bound algorithms are typically used for NP-hard problems in which the search space is exponentially large with respect to the size of the problem. Examples of problems for which branch-and-bound algorithms provide efficient solutions include the 0-1 Knapsack problem [62], the Traveling Salesman problem <ref> [84] </ref>, and Integer Programming [77]. The size and shape of the search space that is traversed by a branch-and-bound algorithm cannot be predicted in advance. Therefore, in a parallel branch-and-bound algorithm, it is not possible to efficiently partition the search space among the parallel threads in any predetermined manner.
Reference: [85] <author> W. Loots and T. H. C. Smith. </author> <title> A parallel algorithm for the 0-1 knapsack problem. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 21(5) </volume> <pages> 349-362, </pages> <month> October </month> <year> 1992. </year>
Reference: [86] <author> David May. </author> <title> Occam. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 17(4) </volume> <pages> 69-79, </pages> <month> April </month> <year> 1983. </year>
Reference-contexts: In 1968, Dijkstra [33] proposed the parbegin-parend notation for parallel composition of statements. The first major language to support parallel composition of statements was Algol 68 [133], which incorporated the parbegin-parend notation. Other early notations to support parallel composition of statements include CSP [60][61] and Occam <ref> [86] </ref> (a programming language derived from CSP). Many subsequent parallel programming notations express parallel 13 execution using some form of parallel composition of statements. 2.2 Parallel For-Loop Statement Our parallelizable for-loop statement pragma is based on a general parallel for-loop statement.
Reference: [87] <author> J. R. McGraw, S. Allan, J. Glauert, and I. Dobes. </author> <title> SISAL: Streams and iteration in a single-assignment language, language reference manual. </title> <type> Technical Report M-146, </type> <institution> Lawrence Livermore National Laboratory, </institution> <year> 1983. </year>
Reference: [88] <author> James R. McGraw. </author> <title> The VAL language: Description and analysis. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 4(1) </volume> <pages> 44-82, </pages> <month> January </month> <year> 1982. </year>
Reference-contexts: Parallel declarative languages are designed to express declarative programs that are intended to be executed as parallel programs. Examples include the family of parallel functional languages based on Lisp [50][54][68], the family of concurrent logic programming languages based on Prolog [28][110][113], and dataflow languages such as Id [94], Val <ref> [88] </ref>, and Sisal [38]. A review of parallel declarative languages is given by Almasi and Gottlieb [5, Section 5.3] and a collection of papers describing individual parallel functional and dataflow languages was edited by Szymanski [120].
Reference: [89] <author> G. P. McKeown, V. J. Rayward-Smith, and S. A. </author> <title> Rush. Parallel branch-and-bound. </title> <editor> In Lydia Kronsjo and Dean Shumsheruddin, editors, </editor> <booktitle> Advances in Parallel Algorithms, chapter 5, </booktitle> <pages> pages 111-150. </pages> <publisher> Halsted Press, John Wiley and Sons, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: The efficiency of the algorithm may be improved by presorting the objects into decreasing order of value density and branching on objects in this order. This simple branch-and-bound algorithm is discussed in more detail by McKeown et al. <ref> [89] </ref>. 130 queue of subsets. (b) The subset with the highest upper bound is removed from the head of the queue. (c) The subset is partitioned into a collection of subsets. (d) The subsets are inserted into the queue according to their upper bounds or discarded. 131 8.4.3 Synchronous Parallel Branch-and-Bound <p> Many modifications to this strategy are possible to reduce the performance hot spot of the shared queue and to asynchronously communicate updated bounds between parallel threads without creating a communication bottleneck. A comprehensive discussion of the alternatives is given by McKeown et al. <ref> [89] </ref>. Published experimental results [78][85][89][99][137] indicate that asynchronous parallel branch-and-bound algorithms can deliver good speedups for a wide range of problems and multiprocessor architectures. 8.5 Integration of Our Model with Less-Restrictive Models There are two reasons that we might want to integrate our parallel programming model with some other model that
Reference: [90] <author> Michael Metcalf and John Reid. </author> <title> Fortran 90 Explained. </title> <publisher> Oxford University Press, Oxford, </publisher> <address> Great Britain, </address> <year> 1990. </year>
Reference-contexts: Subsequent work [55][56][100] describes techniques for the implementation of data-parallel programs on MIMD (Multiple Instruction stream, Multiple Data stream) computers. Examples of data-parallel languages include C* [102], Dataparallel C [55], pC++ [80], CM Fortran, Fortran 90 <ref> [90] </ref>, HPF (High Performance Fortran) [40][74], and NESL [14][15]. Parallel operations on the elements of data sets are specified using synchronous parallel loops such as the FORALL loop of HPF and other Fortran dialects, and using parallel operators on composite data structures such as the Fortran 90 array intrinsic functions.
Reference: [91] <author> Robin Milner, Mads Tofte, and Robin Harper. </author> <title> The Definition of Standard ML. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1990. </year> <month> 236 </month>
Reference: [92] <author> L. G. Mitten. </author> <title> Branch-and-bound methods: General formulation and properties. </title> <journal> Operations Research, </journal> <volume> 18(1) </volume> <pages> 24-34, </pages> <month> January-February </month> <year> 1970. </year>
Reference: [93] <author> Greg Nelson, </author> <title> editor. Systems Programming with Modula-3. Innovative Technology. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1991. </year>
Reference-contexts: The semantics of parallel execution is usually defined to be equivalent to an interleaving of the actions of the parallel processes. This approach to parallel programming is supported by languages such as Ada [3][6] and Modula-3 <ref> [93] </ref>, and by libraries such as p4 [18], Pthreads [97], PVM [119], and MPI [34][115]. Comprehensive reviews and discussions of programming models and notations with explicit parallel semantics are given by Andrews [8], Bal et al. [11], and Pancake [96].
Reference: [94] <author> Ridhiyur S. Nikhil and Arvind. </author> <title> Id: a language with implicit parallelism. </title> <editor> In J. T. Feo, editor, </editor> <title> A Comparative Study of Parallel Programming Languages: The Salishan Problems, </title> <booktitle> volume 6 of Special Topics in Supercomputing, </booktitle> <pages> pages 169-215. </pages> <publisher> North-Holland, </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1992. </year>
Reference-contexts: Parallel declarative languages are designed to express declarative programs that are intended to be executed as parallel programs. Examples include the family of parallel functional languages based on Lisp [50][54][68], the family of concurrent logic programming languages based on Prolog [28][110][113], and dataflow languages such as Id <ref> [94] </ref>, Val [88], and Sisal [38]. A review of parallel declarative languages is given by Almasi and Gottlieb [5, Section 5.3] and a collection of papers describing individual parallel functional and dataflow languages was edited by Szymanski [120].
Reference: [95] <author> David A. Padua and Michael J. Wolfe. </author> <title> Advanced compiler optimizations for supercomputers. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1184-1201, </pages> <month> December </month> <year> 1986. </year>
Reference: [96] <author> Cherri M. Pancake. </author> <title> Multithreaded languages for scientific and technical computing. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 81(2) </volume> <pages> 288-304, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: Comprehensive reviews and discussions of programming models and notations with explicit parallel semantics are given by Andrews [8], Bal et al. [11], and Pancake <ref> [96] </ref>. Explicit parallel programming gives direct control of execution performance because of the close correlation between the programming model and the operation of a multiprocessor computer system.
Reference: [97] <institution> Draft Standard for Information Technology Portable Operating Systems Interface (POSIX). IEEE, </institution> <month> September </month> <year> 1994. </year> <month> P1003.4a/D10. </month>
Reference-contexts: The semantics of parallel execution is usually defined to be equivalent to an interleaving of the actions of the parallel processes. This approach to parallel programming is supported by languages such as Ada [3][6] and Modula-3 [93], and by libraries such as p4 [18], Pthreads <ref> [97] </ref>, PVM [119], and MPI [34][115]. Comprehensive reviews and discussions of programming models and notations with explicit parallel semantics are given by Andrews [8], Bal et al. [11], and Pancake [96]. <p> However, this inefficiency does not affect the validity of our results, since sequential and parallel programs are slowed down equally. GNAT compiles Ada tasking constructs into calls to the SGI-Irix implementation of the Pthreads (POSIX threads) library <ref> [97] </ref>, as described by Giering, Muller, and Baker [49]. At run time, threads are dynamically scheduled across a user-specified number of sprocs (system processes), and sprocs are dynamically scheduled across processors. Sprocs share the pool of processors with the other processes running concurrently on the system.
Reference: [98] <author> William H. Press, Saul A Teukolsky, William T. Vetterling, and Brian P. Flannery, </author> <title> editors. Numerical Recipes in C. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, Great Britain, </address> <note> second edition, </note> <year> 1992. </year>
Reference-contexts: However, more general applicability has not been demonstrated convincingly. The essential difficulty is that an efficient sequential algorithm is not necessarily a good basis for an efficient parallel algorithm. For example, quicksort is widely accepted as the most efficient general-purpose sequential sorting algorithm <ref> [98, section 8.2] </ref>[107, Chapter 9], yet the parallelism in quicksort does not scale to more than a few processors [29][36]. Usually, there is too little information in the text of a sequential program to allow automatic transformation into an equivalent parallel program with entirely different algorithms and data structures. <p> The LU Factorize procedure in Program 1.1 computes the unit lower triangular and upper triangular factors of an input matrix, A, using Crout's method without pivoting <ref> [98, Section 2.3] </ref>. The computed factors are overlaid in the output matrix, LU. An example is shown in Figure 1.1. In the interest of brevity, the procedure specification ignores the imprecision of floating-point arithmetic and the possibility of division by zero due to the absence of pivoting. <p> The sequential quicksort algorithm that is used for sorting the subarrays and speedup comparison is adapted from the program given by Press et al. <ref> [98, section 8.2] </ref>, which uses many of the optimizations described by Sedgewick [106]. The measurements show that one-deep parallel mergesort is an effective parallel sorting algorithm. The algorithm is only marginally slower than standard quicksort when executed on one processor and produces increasing speedup with increasing numbers of processors.
Reference: [99] <author> Michael J. Quinn. </author> <title> Analysis and implementation of branch-and-bound algorithms on a hypercube multicomputer. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 39(3) </volume> <pages> 384-387, </pages> <month> March </month> <year> 1990. </year>
Reference: [100] <author> Michael J. Quinn and Philip J. Hatcher. </author> <title> Data-parallel programming on multicom-puters. </title> <journal> IEEE Software, </journal> <pages> pages 69-76, </pages> <month> September </month> <year> 1990. </year>
Reference: [101] <author> Martin C. Rinard, Daniel J. Scales, and Monica S. Lam. </author> <title> Jade: A high-level, machine-independent language for parallel programming. </title> <journal> IEEE Computer, </journal> <volume> 26(6) </volume> <pages> 28-38, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: An alternative approach is to perform at least part of analysis at run-time when more information is available [103]. Jade <ref> [101] </ref> is an example of a run-time parallelization system based on program annotations. A Jade program is a standard sequential program with annotations that specify the decomposition of the program into tasks and the data objects accessed by the tasks.
Reference: [102] <author> John R. Rose and Guy L. Steele, Jr. </author> <title> C*: An extended C language for data parallel programming. </title> <booktitle> In Proceedings of the Second International Conference on Supercomputing, </booktitle> <volume> volume 2, </volume> <pages> pages 2-16, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: Subsequent work [55][56][100] describes techniques for the implementation of data-parallel programs on MIMD (Multiple Instruction stream, Multiple Data stream) computers. Examples of data-parallel languages include C* <ref> [102] </ref>, Dataparallel C [55], pC++ [80], CM Fortran, Fortran 90 [90], HPF (High Performance Fortran) [40][74], and NESL [14][15].
Reference: [103] <author> Joel Saltz, Harry Berryman, and Janet Wu. </author> <title> Multiprocessors and run-time compilation. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 3(6) </volume> <pages> 573-592, </pages> <month> December </month> <year> 1991. </year> <month> 237 </month>
Reference-contexts: An alternative approach is to perform at least part of analysis at run-time when more information is available <ref> [103] </ref>. Jade [101] is an example of a run-time parallelization system based on program annotations. A Jade program is a standard sequential program with annotations that specify the decomposition of the program into tasks and the data objects accessed by the tasks.
Reference: [104] <author> Daniel J. Scales and Monica S. Lam. </author> <title> The design and evaluation of a shared object system for distributed memory machines. </title> <booktitle> In Proceedings of the First Symposium on Operating Systems Design and Implementation, </booktitle> <address> Monterey, California, </address> <month> November 14-17 </month> <year> 1994. </year>
Reference-contexts: A review of page-based algorithms for the implementation of distributed shared memory in software is given by Stumm and Zhou [118]. A different approach, based on caching at the level of data objects rather than pages and using single-assignment values for synchronization, is used in the SAM system <ref> [104] </ref>. 144 Our parallel programming model could be implemented on top of any system that implements distributed shared memory in hardware or software. Such an implementation would inherit the strengths and weaknesses of the underlying distributed shared-memory system. There is no clear consensus regarding the limits of this developing technology.
Reference: [105] <author> Edmond Schonberg and Bernard Banner. </author> <title> The GNAT project: A GNU-Ada 9X compiler. </title> <booktitle> In Proceedings of ACM TRI-Ada '94, </booktitle> <pages> pages 48-57, </pages> <address> Baltimore, Maryland, </address> <month> November 6-11 </month> <year> 1994. </year>
Reference-contexts: Details of these transformations are presented in Appendix A. They are also discussed in earlier work by the author of this thesis [126][129]. The programs are compiled with the SGI-Irix release of the GNAT (GNU-NYU Ada Translator) compiler <ref> [105] </ref>. GNAT is an Ada 95 front-end and run-time system for the GCC 53 (GNU C Compiler) family of compilers [116]. The efficiency of sequential code produced by GNAT is comparable to that produced by GCC for C/C++ programs (except that dynamically-sized arrays are implemented inefficiently in the current release).
Reference: [106] <author> Robert Sedgewick. </author> <title> Implementing quicksort programs. </title> <journal> Communications of the ACM, </journal> <volume> 21(10) </volume> <pages> 847-857, </pages> <month> October </month> <year> 1978. </year>
Reference-contexts: The sequential quicksort algorithm that is used for sorting the subarrays and speedup comparison is adapted from the program given by Press et al. [98, section 8.2], which uses many of the optimizations described by Sedgewick <ref> [106] </ref>. The measurements show that one-deep parallel mergesort is an effective parallel sorting algorithm. The algorithm is only marginally slower than standard quicksort when executed on one processor and produces increasing speedup with increasing numbers of processors.
Reference: [107] <author> Robert Sedgewick. </author> <title> Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <note> second edition, </note> <year> 1988. </year>
Reference: [108] <institution> Symmetric multiprocessing. </institution> <type> Technical report, </type> <institution> Silicon Graphics, Inc., </institution> <year> 1994. </year>
Reference-contexts: The hardware is entirely responsible for the transfer of data between main memory and cache memory, and for maintaining cache coherence between the processors. The SGI Challenge architecture is described by Galles and Williams [47] and in SGI technical documentation <ref> [108] </ref>.
Reference: [109] <author> Ehud Shapiro. </author> <title> A subset of Concurrent Prolog and its interpreter. </title> <type> Technical Report TR-003, </type> <institution> ICOT, Institute for New Generation Computer Technology, </institution> <address> Tokyo, Japan, </address> <year> 1983. </year>
Reference: [110] <author> Ehud Shapiro. </author> <title> Concurrent Prolog: A progress report. </title> <journal> IEEE Computer, </journal> <volume> 19(8) </volume> <pages> 44-58, </pages> <month> August </month> <year> 1986. </year>
Reference: [111] <author> Ehud Shapiro, </author> <title> editor. Concurrent Prolog: </title> <booktitle> Collected Papers, </booktitle> <volume> volume 1. </volume> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1987. </year>
Reference: [112] <author> Ehud Shapiro, </author> <title> editor. Concurrent Prolog: </title> <booktitle> Collected Papers, </booktitle> <volume> volume 2. </volume> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1987. </year>
Reference: [113] <author> Ehud Shapiro. </author> <title> The family of concurrent logic programming languages. </title> <journal> ACM Computing Surveys, </journal> <volume> 21(3) </volume> <pages> 413-510, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: Since the early 1980s, single-assignment variables have been used for run-time synchronization in parallel logic programming languages such as Concurrent Pro-log [109][110], Parlog [27][28], and Strand [43]. A review of the principles and history of parallel logic programming is given by Shapiro <ref> [113] </ref>. Shapiro also edited a collection of seminal papers on parallel logic programming [111][112]. Single-assignment variables are in many ways similar to the concept of futures incor 14 porated in some parallel functional programming notations.
Reference: [114] <author> Hanmao Shi and Jonathon Schaeffer. </author> <title> Parallel sorting by regular sampling. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 14(4) </volume> <pages> 361-372, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: This analysis indicates that algorithm overheads (e.g., uneven data partitioning and increasing computation costs) rather than system-specific factors (e.g., memory contention) and implementation overheads (e.g., thread creation and synchronization costs) are the primary limitations to speedup. Our performance measurements are consistent with those of Shi and Schaeffer <ref> [114] </ref>, who show that the one-deep parallel mergesort algorithm is among the most efficient parallel sorting algorithms currently known, for both shared-memory and distributed-memory machines. <p> The parallel algorithm delivers good speedup for up to four processors and very little additional speedup for more than four processors. Speedups increase very little with increasing data length. This performance pattern is consistent with Equation 6.1 and with other reported results <ref> [114] </ref>. 7.2.5 Parallel Mergesort with Single-Assignment Links The parallel mergesort algorithm for linked lists with single-assignment links is presented in Program 7.5. The complete text of the program is given in Appendix B.3.5.
Reference: [115] <author> Marc Snir, Steve W. Otto, Steven Huss-Lederman, David W. Walker, and Jack Don-garra. </author> <title> MPI: The Complete Reference. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1995. </year>
Reference: [116] <author> Richard Stallman. </author> <title> Using and Porting GNU GCC. Free Software Foundation, </title> <address> Cam-bridge, Massachusetts, </address> <year> 1994. </year> <month> 238 </month>
Reference-contexts: They are also discussed in earlier work by the author of this thesis [126][129]. The programs are compiled with the SGI-Irix release of the GNAT (GNU-NYU Ada Translator) compiler [105]. GNAT is an Ada 95 front-end and run-time system for the GCC 53 (GNU C Compiler) family of compilers <ref> [116] </ref>. The efficiency of sequential code produced by GNAT is comparable to that produced by GCC for C/C++ programs (except that dynamically-sized arrays are implemented inefficiently in the current release).
Reference: [117] <author> Bjarne Stroustrup. </author> <title> The C++ Programming Language. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <note> second edition, </note> <year> 1991. </year>
Reference-contexts: Parallel Declarative Ada programs can be developed to be identical to equivalent sequential Ada programs. A recent parallel programming language, CC++ [21][22], integrates single-assignment types, parallel composition of statements, a parallel for-loop statement, and other extensions with the full C++ <ref> [117] </ref> language.
Reference: [118] <author> Michael Stumm and Songnian Zhou. </author> <title> Algorithms implementing distributed shared memory. </title> <journal> IEEE Computer, </journal> <volume> 23(5) </volume> <pages> 54-64, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: However, the cost of cache misses and false sharing is greater, because memory pages are generally much larger than cache lines. A review of page-based algorithms for the implementation of distributed shared memory in software is given by Stumm and Zhou <ref> [118] </ref>.
Reference: [119] <author> V. S. Sunderam, G. A. Geist, J. Dongarra, and R. Manchek. </author> <title> The PVM concurrent computing system: Evolution, experiences, and trends. </title> <journal> Parallel Computing, </journal> <volume> 20(4) </volume> <pages> 531-545, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: The semantics of parallel execution is usually defined to be equivalent to an interleaving of the actions of the parallel processes. This approach to parallel programming is supported by languages such as Ada [3][6] and Modula-3 [93], and by libraries such as p4 [18], Pthreads [97], PVM <ref> [119] </ref>, and MPI [34][115]. Comprehensive reviews and discussions of programming models and notations with explicit parallel semantics are given by Andrews [8], Bal et al. [11], and Pancake [96]. <p> Methods for data distribution include: (i) distribution specifications separate from the program, e.g., Ada 95 [3, Annex E], (ii) annotations to data declarations, e.g., HPF [40][74], and (iii) dynamic memory allocation on different nodes, e.g., PCN [24][42], CC++ [22], Fortran M [45], PVM <ref> [119] </ref>, and MPI [34][115]. Methods for process mapping include: (i) mapping specifications separate from the program, e.g., Ada 95, (ii) annotations and arguments to process creation, e.g., PCN, CC++, PVM and MPI, and (iii) dynamic process migration, e.g., the Concurrent Graph Library [122].
Reference: [120] <editor> Boleslaw K. Szymanski, editor. </editor> <booktitle> Parallel Functional Languages and Compilers. </booktitle> <publisher> ACM Press, </publisher> <address> New York, New York, </address> <year> 1991. </year>
Reference-contexts: A review of parallel declarative languages is given by Almasi and Gottlieb [5, Section 5.3] and a collection of papers describing individual parallel functional and dataflow languages was edited by Szymanski <ref> [120] </ref>. A compiler for a parallel declarative language transforms a declarative source program into an equivalent parallel object program for a given target architecture. The advantages of this approach to parallel programming are: 1. Declarative programming languages express algorithms at a high level of abstraction. 2.
Reference: [121] <author> Stephen Taylor. </author> <title> Parallel Logic Programming Techniques. </title> <publisher> Prentice Hall, </publisher> <address> Engelwood Cliffs, New Jersey, </address> <year> 1989. </year>
Reference: [122] <author> Stephen Taylor, Jerrell Watts, Marc Rieffel, and Michael Palmer. </author> <title> The concurrent graph: Basic technology for irregular problems. </title> <booktitle> IEEE Parallel and Distributed Technology, </booktitle> <year> 1996. </year>
Reference-contexts: Methods for process mapping include: (i) mapping specifications separate from the program, e.g., Ada 95, (ii) annotations and arguments to process creation, e.g., PCN, CC++, PVM and MPI, and (iii) dynamic process migration, e.g., the Concurrent Graph Library <ref> [122] </ref>. In some systems, data distribution and process mapping can be specified in terms of virtual topologies [41][44][121] that are separately mapped onto actual machine configurations.
Reference: [123] <author> L. G. Tesler and H. J. Enea. </author> <title> A language design for concurrent processes. </title> <booktitle> In Proceedings of the 1968 AFIPS Spring Joint Computer Conference, </booktitle> <pages> pages 403-408, </pages> <address> Atlantic City, New Jersey, </address> <month> April 30-May 2 </month> <year> 1968. </year>
Reference-contexts: The INDEPENDENT directive of HPF (High Performance Fortran) [40][74] is an assertion that the iterations of a loop are independent and can be executed in parallel. Unlike our parallelizable for-loop statement pragma, no interaction is permitted between the iterations. 2.3 Single-Assignment Variables In 1968, Tesler and Enea <ref> [123] </ref> described the use of single-assignment variables as a sequencing mechanism in their parallel programming notation, Compel. In Compel, the single-assignment restriction enables automatic compile-time scheduling of the concurrent execution of statements.
Reference: [124] <author> John Thornley. </author> <title> Parallel programming with Declarative Ada. </title> <type> Technical Report CS-TR-93-03, </type> <institution> Computer Science Department, California Institute of Technology, </institution> <year> 1993. </year>
Reference: [125] <author> John Thornley. </author> <title> Integrating functional and imperative parallel programming: CC++ solutions to the Salishan problems. </title> <booktitle> In Proceedings of the 8th IEEE International Parallel Processing Symposium (IPPS '94), </booktitle> <pages> pages 61-67, </pages> <address> Cancun, Mexico, </address> <month> April 26-29 </month> <year> 1994. </year>
Reference-contexts: The original solutions were presented in Ada, C fl , Haskell, Id, Occam, 78 carbon-centered paraffin of size 6. 79 PCN, Sisal, and Scheme. Solutions to the Salishan problems in CC++ were given in earlier work by the author of this thesis <ref> [125] </ref>. 6.3.2 Program Specification The specification of a program to solve the Paraffins problem is given in Program 6.4.
Reference: [126] <author> John Thornley. </author> <title> Integrating parallel dataflow programming with the Ada tasking model. </title> <booktitle> In Proceedings of ACM TRI-Ada '94, </booktitle> <pages> pages 417-428, </pages> <address> Baltimore, Maryland, </address> <month> November 6-11 </month> <year> 1994. </year>
Reference: [127] <author> John Thornley. </author> <title> Declarative Ada: Parallel dataflow programming in a familiar context. </title> <booktitle> In Proceedings of the 23rd Annual ACM Computer Science Conference (CSC '95), </booktitle> <pages> pages 73-80, </pages> <address> Nashville, Tennessee, </address> <month> February 28-March 2 </month> <year> 1995. </year> <month> 239 </month>
Reference: [128] <author> John Thornley. </author> <title> Performance of a class of highly-parallel divide-and-conquer algorithms. </title> <type> Technical Report CS-TR-95-10, </type> <institution> Computer Science Department, California Institute of Technology, </institution> <year> 1995. </year>
Reference-contexts: We compare the measured performance of the one-deep parallel mergesort program to the performance limits of the traditional approach to parallelizing mergesort. One-deep parallel mergesort is an example of the class of one-deep parallel divide-and-conquer algorithms <ref> [128] </ref>. 6.2.1 Program Specification The specification of the one-deep parallel mergesort program is given in Program 6.1. The Parallel Mergesort procedure takes the Data array as input and returns the Result array as output. <p> A detailed breakdown and analysis of performance is contained in earlier work by the author of this thesis <ref> [128] </ref>. This analysis indicates that algorithm overheads (e.g., uneven data partitioning and increasing computation costs) rather than system-specific factors (e.g., memory contention) and implementation overheads (e.g., thread creation and synchronization costs) are the primary limitations to speedup. <p> Many other divide-and-conquer algorithms are likely to be amenable to efficient paralleliza-tion using the one-deep parallel divide-and-conquer strategy. Splitting a problem into small parts and independently solving those parts in parallel leads to good cache behavior and low synchronization costs. In other work by the author of this thesis <ref> [128] </ref>, we present a one-deep parallel quicksort algorithm with very similar performance to the one-deep parallel mergesort algorithm.
Reference: [129] <author> John Thornley. </author> <title> Performance of a high-level parallel programming layer defined on top of the Ada tasking model. </title> <booktitle> In Proceedings of TRI-Ada '95, </booktitle> <pages> pages 252-262, </pages> <address> Anaheim, California, </address> <month> November 5-10 </month> <year> 1995. </year>
Reference: [130] <author> Josep Torrellas, Monica S. Lam, and John L. Hennessy. </author> <title> False sharing and spatial locality in multiprocessor caches. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 43(6) </volume> <pages> 651-663, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Since synchronization operations are required to be relatively infrequent for reasons of efficiency, the cost of cache invalidation is insignificant in most practical programs. However, "false sharing" of data can result in situations where cache invalidation degrades parallel performance <ref> [130] </ref>. For example, consider Program 5.2, in which there is no sharing of variables between the two parallel threads.
Reference: [131] <author> D. A. Turner. </author> <title> The semantic elegance of applicative languages. </title> <booktitle> In Proceedings of the ACM Conference on Functional Programming Languages and Computer Architecture, </booktitle> <pages> pages 85-92, </pages> <address> Portsmouth, New Hampshire, </address> <month> October </month> <year> 1981. </year>
Reference-contexts: The Paraffins problem is discussed by Turner <ref> [131] </ref> and is one of the Salishan problems [37]. The Salishan problems are a set of problems proposed at the 1988 Salishan High-Speed Computing Conference as a standard by which to compare parallel programming notations.
Reference: [132] <author> Eric F. Van de Velde. </author> <title> Concurrent Scientific Computing. </title> <publisher> Springer-Verlag, </publisher> <address> New York, New York, </address> <year> 1994. </year>
Reference-contexts: Data distribution: the distribution of data across the distributed memory space. 2. Process mapping: the mapping of parallel processes or threads to processors. 140 These issues are an integral part of the design of an efficient algorithm for a distributed-memory computer system, as described by Van de Velde <ref> [132, Chapter 12] </ref>. In addition, even with a distributed shared-memory system, parallel performance is often improved by explicit copying of remote data into local memory.
Reference: [133] <author> A. van Wijngaarden, B. J. Mailloux, J. E. L. Peck, C. H. A. Koster, M. Sintzoft, C. H. Lindsey, L. G. L. T. Meertens, and R. G. Fisker. </author> <title> Revised report on the algorithmic language ALGOL 68. </title> <journal> Acta Informatica, </journal> <volume> 5(1-3):1-236, </volume> <year> 1975. </year>
Reference-contexts: Wirth suggested the use of and in place of ";" between statements. In 1968, Dijkstra [33] proposed the parbegin-parend notation for parallel composition of statements. The first major language to support parallel composition of statements was Algol 68 <ref> [133] </ref>, which incorporated the parbegin-parend notation. Other early notations to support parallel composition of statements include CSP [60][61] and Occam [86] (a programming language derived from CSP).
Reference: [134] <author> Paul G. Whiting and Robert S. V. Pascoe. </author> <title> A history of data-flow languages. </title> <journal> IEEE Annals of the History of Computing, </journal> <volume> 16(4) </volume> <pages> 38-59, </pages> <month> Winter </month> <year> 1994. </year>
Reference: [135] <author> Niklaus Wirth. </author> <title> A note on "Program Structures for Parallel Processing". </title> <journal> Communications of the ACM, </journal> <volume> 9(5) </volume> <pages> 320-321, </pages> <month> May </month> <year> 1966. </year>
Reference-contexts: In Chapter 10, we compare our programming model to other approaches to reducing the difficulty of reasoning about explicit parallelism. 2.1 Parallel Composition of Statements Our parallelizable sequence of statements pragma is based on the parallel composition of statements construct. In 1966, Wirth <ref> [135] </ref> pointed out the difference between: (i) parallelism for multiprocessor performance, and (ii) parallelism to represent concurrency in the problem specification.
Reference: [136] <author> Michael Wolfe. </author> <title> High Performance Compilers for Parallel Computing. </title> <publisher> Addison-Wesley, </publisher> <address> Redwood City, California, </address> <year> 1996. </year>
Reference-contexts: For example, loop restructuring transformations often could be used to reduce the cost of thread creation associated with our parallelizable for-loop statement. A thorough discussion of techniques for generating efficient parallel code from both sequential and parallel notations is given by Wolfe <ref> [136] </ref>. In addition, our pragmas could be used as program annotations to aid compile-time analysis by an automatic paral-lelizing compiler. 148 10.3 Runtime Parallelization Systems An automatic parallelizing compiler analyzes a sequential source program (possibly with annotations) prior to execution and generates an equivalent parallel object program.
Reference: [137] <author> Myung K. Yang and Chita R. Das. </author> <title> Evaluation of a parallel branch-and-bound algorithm on a class of multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 5(1) </volume> <pages> 74-86, </pages> <month> January </month> <year> 1994. </year>
References-found: 137

