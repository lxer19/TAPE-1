URL: http://www.isi.edu/isd/VET/wecc98-distribution.ps
Refering-URL: http://www.isi.edu/isd/VET/vet-body.html
Root-URL: http://www.isi.edu
Email: rickel@isi.edu, johnson@isi.edu  
Title: Task-Oriented Dialogs with Animated Agents in Virtual Reality  
Author: Jeff Rickel and W. Lewis Johnson 
Web: http://www.isi.edu/isd/VET/vet.html  
Date: October 1998  
Address: Tahoe City, CA,  4676 Admiralty Way, Marina del Rey, CA 90292-6695  
Affiliation: Characters,  Information Sciences Institute Computer Science Department University of Southern California  
Note: Appears in Proceedings of the First Workshop on Embodied Conversational  
Abstract: We are working towards animated agents that can carry on tutorial, task-oriented dialogs with human students. The agent's objective is to help students learn to perform physical, procedural tasks, such as operating and maintaining equipment. Although most research on such dialogs has focused on verbal communication, nonverbal communication can play many important roles as well. To allow a wide variety of interactions, the student and our agent cohabit a three-dimensional, interactive, simulated mock-up of the student's work environment. The agent, Steve, can generate and recognize speech, demonstrate actions, use gaze and gestures, answer questions, adapt domain procedures to unexpected events, and remember past actions. This paper focuses on Steve's methods for generating multi-modal behavior, contrasting our work with prior work in task-oriented dialogs, multimodal explanation generation, and animated conversational characters. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Allen, J. F.; Miller, B. W.; Ringger, E. K.; and Siko-rski, T. </author> <year> 1996. </year> <title> Robust understanding in a dialogue system. </title> <booktitle> In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> 62-70. </pages>
Reference: <author> Andre, E.; Rist, T.; and Mueller, J. </author> <year> 1998. </year> <title> Employing AI methods to control the behavior of animated interface agents. </title> <journal> Applied Artificial Intelligence. Forthcoming. </journal>
Reference-contexts: Steve's communicative suites are similar to the schemata approach to explanation generation pioneered by McKeown (McKeown 1985). In contrast, Andre et al. <ref> (Andre, Rist, & Mueller 1998) </ref> employ a standard top-down discourse planning approach to generating the communicative behavior of their animated agent, and they compile the resulting plans into finite state machines for efficient execution. The tradeoffs between these two approaches to discourse generation are well known (Moore 1995).
Reference: <author> Badler, N. I.; Phillips, C. B.; and Webber, B. L. </author> <year> 1993. </year> <title> Simulating Humans. </title> <address> New York: </address> <publisher> Oxford University Press. </publisher>
Reference-contexts: Because TRAINS and Steve carry on similar types of dialogs with users, yet focus on different aspects of such conversations, a combination of the two systems seems promising. Our work also complements research on sophisticated control of human figures <ref> (Badler, Phillips, & Webber 1993) </ref>. Such work targets more generality in human figure motion. Our human figure control is efficient and predictable, and it results in smooth anima tion.
Reference: <author> Billinghurst, M., and Savage, J. </author> <year> 1996. </year> <title> Adding intelligence to the interface. </title> <booktitle> In Proceedings of the IEEE Virtual Reality Annual International Symposium (VRAIS '96), </booktitle> <pages> 168-175. </pages> <address> Los Alamitos, CA: </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: Our work has focused more on multi-modal behavior generation than multi-modal input. To model face-to-face communication, we must extend the range of nonverbal communicative acts that students can use. To handle multi-modal input in virtual reality, the techniques of Billinghurst and Savage <ref> (Billinghurst & Savage 1996) </ref> would nicely complement Steve's current capabilities. Their agent, which is designed to train medical students how to perform sinus surgery, combines natural language understanding and simple gesture recognition.
Reference: <author> Cassell, J., and Thorisson, K. R. </author> <year> 1998. </year> <title> The power of a nod and a glance: Envelope vs. emotion in animated conversational agents. </title> <journal> Applied Artificial Intelligence. Forthcoming. </journal>
Reference: <author> Cassell, J.; Pelachaud, C.; Badler, N.; Steedman, M.; Achorn, B.; Becket, T.; Douville, B.; Prevost, S.; and Stone, M. </author> <year> 1994. </year> <title> Animated conversation: Rule-based generation of facial expression, gesture and spoken intonation for multiple conversational agents. </title> <booktitle> In Proceedings of ACM SIGGRAPH '94. </booktitle>
Reference-contexts: The agent can demonstrate how to perform actions (Rickel & Johnson 1997a). It can use locomotion, gaze, and deictic gestures to focus the student's attention (Lester et al. 1998; Noma & Badler 1997; Rickel & Johnson 1997a). It can use gaze to regulate turn-taking in a mixed-initiative dialog <ref> (Cassell et al. 1994) </ref>. Head nods and facial expressions can provide unobtrusive feedback on the student's utterances and actions without unnecessarily disrupting the student's train of thought. All of these nonverbal devices are a natural component of human dialogs. <p> Like Steve, most current animated characters are incapable of such precise timing (Andre, Rist, & Mueller 1998; Lester et al. 1998; Stone & Lester 1996). One exception is the work of Cassell and her colleagues <ref> (Cassell et al. 1994) </ref>. However, they achieve their synchronization through a multi-pass algorithm that generates an animation file for two synthetic, conversational agents. Achieving a similar degree of synchronization during a real-time dialog with a human is a more challenging problem that will require further research.
Reference: <author> Cassell, J. </author> <title> forthcoming. Embodied conversation: Integrating face and gesture into automatic spoken dialogue systems. </title> <editor> In Luperfoy, S., ed., </editor> <title> Automatic Spoken Dialogue Systems. </title> <publisher> MIT Press. </publisher>
Reference: <author> Cormen, T. H.; Leiserson, C. E.; and Rivest, R. L. </author> <year> 1989. </year> <title> Introduction to Algorithms. </title> <address> New York: </address> <publisher> McGraw-Hill. </publisher>
Reference-contexts: The sensorimotor component uses this graph to plan Steve's locomotion: given a motor command to move to a new object, Steve uses Dijkstra's shortest path algorithm <ref> (Cormen, Leiserson, & Rivest 1989) </ref> to identify a collision-free path. Although more detailed geometric knowledge of the virtual world would allow Steve to interact with it more precisely, the above knowledge is simple to provide and maintain, and it supports the critical functionality for task-oriented dialog.
Reference: <author> Deutsch, B. G. </author> <year> 1974. </year> <title> The structure of task oriented dialogs. </title> <booktitle> In Proceedings of the IEEE Speech Symposium. </booktitle> <address> Pittsburgh, PA: </address> <institution> Carnegie-Mellon University. </institution> <note> Also available as Stanford Research Institute Technical Note 90. </note>
Reference-contexts: Thus, like most earlier research on task-oriented dialogs, the agent (computer) serves as an expert that can provide guidance to a human novice. Research on such dialogs dates back more than twenty years <ref> (Deutsch 1974) </ref>, and the subject remains an active research area (Allen et al. 1996; Lochbaum 1994; Walker 1996). However, the vast majority of that research has focused solely on verbal dialogs, even though the earliest studies clearly showed the ubiquity of nonverbal communication in human task-oriented dialogs (Deutsch 1974). <p> than twenty years <ref> (Deutsch 1974) </ref>, and the subject remains an active research area (Allen et al. 1996; Lochbaum 1994; Walker 1996). However, the vast majority of that research has focused solely on verbal dialogs, even though the earliest studies clearly showed the ubiquity of nonverbal communication in human task-oriented dialogs (Deutsch 1974). To allow a wider variety of interactions among agents and human students, we use virtual reality; agents and students cohabit a three-dimensional, interactive, simulated mock-up of the student's work environment. Virtual reality offers a rich environment for multimodal interaction among agents and humans.
Reference: <author> Douville, B.; Levison, L.; and Badler, N. I. </author> <year> 1996. </year> <title> Task-level object grasping for simulated agents. </title> <type> Presence 5(4) </type> <pages> 416-430. </pages>
Reference-contexts: Our work also complements research on sophisticated control of human figures (Badler, Phillips, & Webber 1993). Such work targets more generality in human figure motion. Our human figure control is efficient and predictable, and it results in smooth anima tion. However, it does not provide human-like object manipulation <ref> (Douville, Levison, & Badler 1996) </ref>, and it would not suffice for movements such as reaching around objects or through tight spaces.
Reference: <author> Elliott, C.; Rickel, J.; and Lester, J. C. </author> <year> 1997. </year> <title> Integrating affective computing into animated tutoring agents. </title> <booktitle> In Proceedings of the IJCAI Workshop on Animated Interface Agents: Making Them Intelligent, </booktitle> <pages> 113-121. </pages>
Reference-contexts: Fi 4 Steve does not currently reproach the student for such interruptions, although we are considering such an extension <ref> (Elliott, Rickel, & Lester 1997) </ref>. nally, to support collision-free locomotion through the virtual world, Steve requires an adjacency graph: each node in the graph represents an object, and there is an edge between two nodes if there is a collision-free path directly between them.
Reference: <author> Ferguson, G.; Allen, J.; and Miller, B. </author> <year> 1996. </year> <title> TRAINS-95: Towards a mixed-initiative planning assistant. </title> <booktitle> In Proceedings of the Third Conference on AI Planning Systems. </booktitle>
Reference: <author> Grosz, B. J., and Sidner, C. L. </author> <year> 1986. </year> <title> Attention, intentions, and the structure of discourse. </title> <booktitle> Computational Liguistics 12(3) </booktitle> <pages> 175-204. </pages>
Reference-contexts: When Steve has the task initiative, its role is to demonstrate how to perform the task. In this role, it follows its plan for completing the task, demonstrating each step. Because its plan only provides a partial order over task steps, Steve uses a discourse focus stack <ref> (Grosz & Sidner 1986) </ref> to ensure a global coherence to the demonstration. The focus stack also allows Steve to recognize digressions and resume the prior demonstration when unexpected events require a temporary deviation from the usual order of task steps.
Reference: <author> Johnson, W. L.; Rickel, J.; Stiles, R.; and Munro, A. </author> <year> 1998. </year> <title> Integrating pedagogical agents into virtual environments. </title> <type> Presence 7(6). </type>
Reference-contexts: For additional technical details on this and other aspects of Steve's capabilities, see (Rickel & Johnson 1998a). For additional motivation behind this research, as well as a discussion of the related software components (e.g., the virtual reality software and the simulator) see <ref> (Johnson et al. 1998) </ref>. For a description of Steve's use in team training, where multiple students and agents can practice tasks that require coordinated action by multiple team members, see (Rickel & John-son 1998b).
Reference: <author> Laird, J. E.; Newell, A.; and Rosenbloom, P. S. </author> <year> 1987. </year> <title> Soar: An architecture for general intelligence. </title> <booktitle> Artificial Intelligence 33(1) </booktitle> <pages> 1-64. </pages>
Reference: <author> Lester, J. C.; Converse, S. A.; Kahler, S. E.; Barlow, S. T.; Stone, B. A.; and Bhogal, R. S. </author> <year> 1997. </year> <title> The persona effect: Affective impact of animated pedagogical agents. </title> <booktitle> In Proceedings of CHI '97, </booktitle> <pages> 359-366. </pages>
Reference-contexts: Fi 4 Steve does not currently reproach the student for such interruptions, although we are considering such an extension <ref> (Elliott, Rickel, & Lester 1997) </ref>. nally, to support collision-free locomotion through the virtual world, Steve requires an adjacency graph: each node in the graph represents an object, and there is an edge between two nodes if there is a collision-free path directly between them.
Reference: <author> Lester, J. C.; Voerman, J. L.; Towns, S. G.; and Call-away, C. B. </author> <year> 1998. </year> <title> Deictic believability: Coordinating gesture, locomotion, and speech in lifelike pedagogical agents. </title> <journal> Applied Artificial Intelligence. Forthcoming. </journal>
Reference: <author> Lochbaum, K. E. </author> <year> 1994. </year> <title> Using Collaborative Plans to Model the Intentional Structure of Discourse. </title> <type> Ph.D. Dissertation, </type> <institution> Harvard University. </institution> <note> Technical Report TR-25-94, Center for Research in Computing Technology. </note>
Reference: <author> Maybury, M. T., ed. </author> <year> 1993. </year> <title> Intelligent Multimedia Interfaces. </title> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Although more detailed geometric knowledge of the virtual world would allow Steve to interact with it more precisely, the above knowledge is simple to provide and maintain, and it supports the critical functionality for task-oriented dialog. Discussion In contrast to prior work in multi-modal explanation generation <ref> (Maybury 1993) </ref>, which focused mainly on combining text and graphics, the issue of media allocation seems less an issue for animated agents. The decision between conveying information in text or graphics is particularly difficult because graphics can be used in many ways.
Reference: <author> McKeown, K. R. </author> <year> 1985. </year> <title> Text Generation. </title> <publisher> Cambridge University Press. </publisher>
Reference-contexts: Steve's communicative suites are similar to the schemata approach to explanation generation pioneered by McKeown <ref> (McKeown 1985) </ref>. In contrast, Andre et al. (Andre, Rist, & Mueller 1998) employ a standard top-down discourse planning approach to generating the communicative behavior of their animated agent, and they compile the resulting plans into finite state machines for efficient execution.
Reference: <author> Moore, J. D. </author> <year> 1993. </year> <booktitle> What makes human explanations effective? In Proceedings of the 15th Annual Conference of the Cognitive Science Society, </booktitle> <pages> 131-136. </pages>
Reference: <author> Moore, J. D. </author> <year> 1995. </year> <title> Participating in Explanatory Dialogues. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: The tradeoffs between these two approaches to discourse generation are well known <ref> (Moore 1995) </ref>. The individual nodes in an ATN serve as domain-independent building blocks for Steve's behavior. Each results in a set of motor commands sent from Steve's cognitive component to the sensorimotor component.
Reference: <author> Newell, A. </author> <year> 1990. </year> <title> Unified Theories of Cognition. </title> <address> Cam-bridge, MA: </address> <publisher> Harvard University Press. </publisher>
Reference: <author> Noma, T., and Badler, N. I. </author> <year> 1997. </year> <title> A virtual human presenter. </title> <booktitle> In Proceedings of the IJCAI Workshop on Animated Interface Agents: Making Them Intelligent, </booktitle> <pages> 45-51. </pages>
Reference: <author> Rickel, J., and Johnson, W. L. </author> <year> 1997a. </year> <title> Integrating pedagogical capabilities in a virtual environment agent. </title> <booktitle> In Proceedings of the First International Conference on Autonomous Agents. </booktitle> <publisher> ACM Press. </publisher>
Reference-contexts: Although practically ignored until recently, nonverbal communication can play many important roles in task-oriented tutorial dialogs. The agent can demonstrate how to perform actions <ref> (Rickel & Johnson 1997a) </ref>. It can use locomotion, gaze, and deictic gestures to focus the student's attention (Lester et al. 1998; Noma & Badler 1997; Rickel & Johnson 1997a). It can use gaze to regulate turn-taking in a mixed-initiative dialog (Cassell et al. 1994).
Reference: <author> Rickel, J., and Johnson, W. L. </author> <year> 1997b. </year> <title> Intelligent tutoring in virtual reality: A preliminary report. </title> <booktitle> In Proceedings of the Eighth World Conference on Artificial Intelligence in Education, </booktitle> <pages> 294-301. </pages> <publisher> IOS Press. </publisher>
Reference: <author> Rickel, J., and Johnson, W. L. </author> <year> 1998a. </year> <title> Animated agents for procedural training in virtual reality: </title> <journal> Perception, cognition, and motor control. Applied Artificial Intelligence. Forthcoming. </journal>
Reference-contexts: The remainder of the paper focuses on Steve's methods for generating multi-modal communicative acts. For additional technical details on this and other aspects of Steve's capabilities, see <ref> (Rickel & Johnson 1998a) </ref>. For additional motivation behind this research, as well as a discussion of the related software components (e.g., the virtual reality software and the simulator) see (Johnson et al. 1998).
Reference: <author> Rickel, J., and Johnson, W. L. </author> <year> 1998b. </year> <title> Animated pedagogical agents for team training. </title> <booktitle> In Proceedings of the ITS Workshop on Pedagogical Agents, </booktitle> <pages> 75-77. </pages>
Reference-contexts: For a description of Steve's use in team training, where multiple students and agents can practice tasks that require coordinated action by multiple team members, see <ref> (Rickel & John-son 1998b) </ref>. Generating Multi-Modal Behavior Like many other autonomous agents that deal with a real or simulated world, Steve consists of two components: the first, implemented in Soar (Laird, Newell, & Rosenbloom 1987; Newell 1990), handles high-level cognitive processing, and the second handles sensori-motor processing.
Reference: <author> Rickel, J. </author> <year> 1988. </year> <title> An intelligent tutoring framework for task-oriented domains. </title> <booktitle> In Proceedings of the International Conference on Intelligent Tutoring Systems. </booktitle>
Reference-contexts: This involves describing the step while pointing to the object to be manipulated. To describe the step, Steve outputs a speech speci fication with three pieces of information: 3 In the future, we plan to use the approach used in TOTS <ref> (Rickel 1988) </ref> to allow Steve to initiate shifts in task initiative based on a model of the student's knowledge. * the name of the step this will be used to retrieve the associated text fragment * whether Steve has already demonstrated this step this allows Steve to acknowledge the repetition, as
Reference: <author> Sacerdoti, E. </author> <year> 1977. </year> <title> A Structure for Plans and Behavior. </title> <address> New York: </address> <publisher> Elsevier North-Holland. </publisher>
Reference: <author> Stone, B. A., and Lester, J. C. </author> <year> 1996. </year> <title> Dynamically sequencing an animated pedagogical agent. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence (AAAI-96), </booktitle> <pages> 424-431. </pages> <address> Menlo Park, CA: </address> <publisher> AAAI Press/MIT Press. </publisher>
Reference: <author> Thorisson, K. R. </author> <year> 1996. </year> <note> Communicative Humanoids: </note>
References-found: 32

