URL: ftp://ftp.cs.wisc.edu/machine-learning/shavlik-group/cherkauer.kdd96.ps
Refering-URL: http://www.cs.wisc.edu/~shavlik/abstracts/cherkauer.kdd96.ps.abstract.html
Root-URL: 
Email: cherkauer@cs.wisc.edu, shavlik@cs.wisc.edu  
Title: Growing Simpler Decision Trees to Facilitate Knowledge Discovery  
Author: Kevin J. Cherkauer Jude W. Shavlik 
Address: 1210 West Dayton Street Madison, WI 53706, USA  
Affiliation: Department of Computer Sciences University of Wisconsin  
Note: Appears in Proceedings, Second International Conference on Knowledge Discovery and Data Mining, Portland, OR: AAAI Press (1996). c 1996 AAAI.  
Abstract: When using machine learning techniques for knowledge discovery, output that is comprehensible to a human is as important as predictive accuracy. We introduce a new algorithm, SET-Gen, that improves the comprehensibility of decision trees grown by standard C4.5 without reducing accuracy. It does this by using genetic search to select the set of input features C4.5 is allowed to use to build its tree. We test SET-Gen on a wide variety of real-world datasets and show that SET-Gen trees are significantly smaller and reference significantly fewer features than trees grown by C4.5 without using SET-Gen. Statistical significance tests show that the accuracies of SET-Gen's trees are either not distinguishable from or are more accurate than those of the original C4.5 trees on all ten datasets tested. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Burl, M.; Fayyad, U.; Perona, P.; Smyth, P.; and Burl, M. </author> <year> 1994. </year> <title> Automating the hunt for volcanoes on Venus. </title> <booktitle> In IEEE Comp Soc Conf Comp Vision & Pat Rec: Proc. </booktitle> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: The datasets are summarized in Table 2. The Magellan-SAR data consists of features derived from small patches of radar images of the planet Venus, and the task is to determine if a patch contains a volcano <ref> (Burl et al. 1994) </ref>. Promoter, Ribosome Binding, and Splice Junction are all problems of detecting different types of biologically significant sites on strands of DNA. Most of the DBs are publicly available through Murphy and Aha (1994).
Reference: <author> Cherkauer, K., and Shavlik, J. </author> <year> 1996. </year> <title> Rapid quality estimation of neural network input representations. </title> <booktitle> In Advances in Neural Info Proc Sys 8. </booktitle> <publisher> MIT Press. </publisher>
Reference: <author> Forrest, S., and Mitchell, M. </author> <year> 1993. </year> <title> What makes a problem hard for a genetic algorithm? some anomalous results and their explanation. </title> <booktitle> Machine Learning 13 </booktitle> <pages> 285-319. </pages>
Reference-contexts: An indicator bit vector with one entry per input feature would be more traditional. Our justification for SET-Gen's genome style is twofold. First, the fact that features can appear multiple times potentially slows the loss of diversity that tends to occur during genetic search <ref> (Forrest & Mitchell 1993) </ref> and allows better features to proliferate. Second, unlike the bit-vector genome, SET-Gen's genome length does not Table 1: SET-Gen pseudocode.
Reference: <author> Goldberg, D. </author> <year> 1989. </year> <title> Genetic Algorithms in Search, Optimization, </title> <booktitle> and Machine Learning. </booktitle> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: This is only an intuition; we have not yet compared the performance of this crossover to a one-point crossover. SET-Gen's low crossover rate hopefully results in low disruption across generations of high-order schemata involving many features <ref> (cf. Goldberg 1989) </ref>. However, a one-point crossover might take better advantage of lower-level "building blocks" (Goldberg 1989) as individual features could assemble themselves in spatially adjacent fashion to increase their chances of being exchanged as a unit. SET-Gen's Mutate operator uses one parent. <p> SET-Gen's low crossover rate hopefully results in low disruption across generations of high-order schemata involving many features (cf. Goldberg 1989). However, a one-point crossover might take better advantage of lower-level "building blocks" <ref> (Goldberg 1989) </ref> as individual features could assemble themselves in spatially adjacent fashion to increase their chances of being exchanged as a unit. SET-Gen's Mutate operator uses one parent. Each entry of the child is copied from the parent with probability 1 P m . <p> The initial population members are created by Mutate with a temporary mutation rate of 1.00. From then on, each new feature subset is produced by applying one of the genetic operators, chosen equiprobably, to parent (s) picked randomly from the current population proportional to their fitness <ref> (Goldberg 1989) </ref>.
Reference: <author> John, G.; Kohavi, R.; and Pfleger, K. </author> <year> 1994. </year> <title> Irrelevant features and the subset selection problem. In Mach Learn: </title> <booktitle> Proc 11th Intl Conf, </booktitle> <pages> 121-129. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The SET-Gen Algorithm SET-Gen performs feature-subset selection for decision-tree induction. Table 1 gives pseudocode for the algorithm. SET-Gen applies a genetic algorithm (GA; Goldberg 1989) with a wrapper-style evaluation function <ref> (John, Kohavi, & Pfleger 1994) </ref> to search many candidate feature subsets. It uses ten-fold cross validation on the training examples to estimate the quality, or fitness, of each candidate.
Reference: <author> Murphy, P., and Aha, D. </author> <year> 1994. </year> <title> Univ. California Irvine repository of machine learning databases. </title> <note> At http://www.ics.uci.edu/~mlearn/MLRepository.html. </note>
Reference-contexts: Cancer a 32 3 56 ( 56, 0) Lymphography a 148 4 18 ( 15, 3) Magellan-SAR 611 2 137 ( 0, 137) Promoters 468 2 57 ( 57, 0) Ribosome Binding 1,877 2 49 ( 49, 0) Splice Junctions a 3,190 3 60 ( 60, 0) a Available publicly <ref> (Murphy & Aha 1994) </ref>. (1 S+F 2 ). We weight the accuracy term more heavily to encourage SET-Gen to maintain the original accuracy level. All coefficients in the fitness function were chosen prior to running any experiments. Note that A and F (normalized) vary in the range [0, 1].
Reference: <author> Quinlan, J. </author> <year> 1993. </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: To address the issue of human comprehensibility, we introduce SET-Gen, a new algorithmic approach to knowledge discovery that improves the comprehensibility of decision trees grown by a state-of-the-art tree induction algorithm, C4.5 <ref> (Quinlan 1993) </ref>, without reducing tree accuracy. SET-Gen takes a DB of labeled examples (vectors of feature-value pairs) and selects a subset of the available features for training C4.5. <p> For each trial, both SET-Gen and C4.5 chose the pruning level by doing an initial, internal ten-fold cross-validation of standard C4.5 using only the training examples. The pruning level was chosen from among ten equally spaced confidence levels: 5%, 15%, 25%, ..., 95% <ref> (Quinlan 1993) </ref>, and the one yielding the most accurate trees on the validation sets was then used to train on the entire training set for the remainder of the trial. (Thus, choosing the pruning level is part of SET-Gen and C4.5 training.
Reference: <author> Skalak, D. </author> <year> 1994. </year> <title> Prototype and feature selection by sampling and random mutation hill climbing algorithms. In Mach Learn: </title> <booktitle> Proc 11th Intl Conf, </booktitle> <pages> 293-301. </pages> <publisher> Morgan Kauf-mann. </publisher>
References-found: 8

