URL: http://www.cs.purdue.edu/research/cse/pythia/papers/conf-journal/cse96.ps.gz
Refering-URL: http://www.cs.purdue.edu/research/cse/pythia/pythia-papers.html
Root-URL: http://www.cs.purdue.edu
Email: Email: fjoshi,saw,ramakris,enh,jrrg@cs.purdue.edu  
Phone: Phone: (317)-494-7821, Fax: (317)-494-0739  
Title: Neural and Neuro-Fuzzy Approaches to Support "Intelligent" Scientific Problem Solving  
Author: Anupam Joshi, Sanjiva Weerawarana, Narendran Ramakrishnan, Elias N. Houstis, John R. Rice 
Date: June 14, 1995  
Address: West Lafayette, IN 47907-1398, USA.  
Affiliation: Department of Computer Sciences Purdue University  
Abstract: Scientific computing uses computers, especially High Performance Computing (HPC) systems, to solve complex mathematical equations which model physical phenomena. Using these systems now requires expert knowledge in a variety of fields of computer science, such as parallel computing and numerical methods. This often makes application scientists, who have the domain expertise to devise the mathematical models, unable to use the power of HPC systems. The object of problem solving environments (PSEs) is to create software systems that hide the details and complexity of the system from the users, and to allow them to deal with a high level, abstract entity that understands the application domain "language". This requires approximate reasoning techniques to automate much of numerical and parallel computing, as well as to interpret the users input. Over the past several years, we have developed PYTHIA, an "intelligent" computational assistant to achieve this goal. In this paper, we describe the connectionist techniques used in developing PYTHIA. Specifically, we discuss backpropagation based systems, as well as hybrid neuro-fuzzy systems which we have developed and used. We also compare the performance of these alternative approaches with each other, as well as with naive classifiers.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. Braun and M. Riedmiller. </author> <title> Rprop : A Fast and Robust Backpropagation Learning Strategy. </title> <booktitle> In Proceedings of the ACNN, </booktitle> <year> 1993. </year>
Reference-contexts: Table 1 shows some of the characteristics that we use to characterize a PDE problem and its solution. Each characteristic is also associated with a value ff, where ff* <ref> [0; 1] </ref>. (ff = 0 means pure absence of that property while ff = 1 means pure presence.) For logical characteristics (for example, whether the boundary conditions 3 Problem #28 (w u x ) x + (w u y ) y = 1; where w = 1; otherwise: Domain [1; 1] <p> ff* [0; 1]. (ff = 0 means pure absence of that property while ff = 1 means pure presence.) For logical characteristics (for example, whether the boundary conditions 3 Problem #28 (w u x ) x + (w u y ) y = 1; where w = 1; otherwise: Domain <ref> [1; 1] </ref> fi [1; 1] BC u = 0 True unknown Operator Self-adjoint, discontinuous coefficients Right Side Constant Boundary Conditions Dirichlet, homogeneous Solution Approximate solutions given for ff = 1; 10; 100: Strong wave fronts for ff 1: Parameter ff adjusts size of discontinuity in operator coefficients which introduces large, sharp <p> (ff = 0 means pure absence of that property while ff = 1 means pure presence.) For logical characteristics (for example, whether the boundary conditions 3 Problem #28 (w u x ) x + (w u y ) y = 1; where w = 1; otherwise: Domain <ref> [1; 1] </ref> fi [1; 1] BC u = 0 True unknown Operator Self-adjoint, discontinuous coefficients Right Side Constant Boundary Conditions Dirichlet, homogeneous Solution Approximate solutions given for ff = 1; 10; 100: Strong wave fronts for ff 1: Parameter ff adjusts size of discontinuity in operator coefficients which introduces large, sharp jumps in solution. <p> Consider the k th ordered pair fA k ; d k g from the training set. Let the desired output for the k th pattern be <ref> [1; 1; 0; 0; :::; 0] </ref>. <p> Let the desired output for the k th pattern be [1; 1; 0; 0; :::; 0]. The algorithm in fig. 2 considers this as two ordered pairs containing the same pattern A k but with two pattern classes as training outputs - d k1 = <ref> [1; 0; 0; 0; :::; 0] </ref> and d k2 = [0; 1; 0; 0; :::; 0] respectively. In other words, the pattern is associated with both class 1 and class 2. This will cause hyperboxes of both classes 1 and 2 to completely contain the pattern A k . <p> The algorithm in fig. 2 considers this as two ordered pairs containing the same pattern A k but with two pattern classes as training outputs - d k1 = [1; 0; 0; 0; :::; 0] and d k2 = <ref> [0; 1; 0; 0; :::; 0] </ref> respectively. In other words, the pattern is associated with both class 1 and class 2. This will cause hyperboxes of both classes 1 and 2 to completely contain the pattern A k . <p> Thus, the above procedure results in the pattern having equal degrees of membership in both the hyperboxes but is not completely contained in either of them. Assume that the network is first trained with the desired output as d k1 = <ref> [1; 0; 0; 0; :::; 0] </ref>. This results in the k th pattern A k having complete containment in a hyperbox of class 1 (because the 1st bit is set to 1). <p> This results in the k th pattern A k having complete containment in a hyperbox of class 1 (because the 1st bit is set to 1). Then when we train the same pattern with <ref> [0; 1; 0; 0; :::; 0] </ref>, a hyperbox of class 2 will be created/expanded to include the k th pattern. This will result in hyperbox overlap. <p> Let the desired output for the k th pattern be <ref> [1; 1; 0; 0; :::; 0] </ref>. We consider this as two ordered pairs containing the same pattern A k but with two pattern classes as training outputs - d k1 = [1; 0; 0; 0; :::; 0] and d k2 = [0; 1; 0; 0; :::; 0] respectively. <p> Let the desired output for the k th pattern be [1; 1; 0; 0; :::; 0]. We consider this as two ordered pairs containing the same pattern A k but with two pattern classes as training outputs - d k1 = <ref> [1; 0; 0; 0; :::; 0] </ref> and d k2 = [0; 1; 0; 0; :::; 0] respectively. <p> We consider this as two ordered pairs containing the same pattern A k but with two pattern classes as training outputs - d k1 = [1; 0; 0; 0; :::; 0] and d k2 = <ref> [0; 1; 0; 0; :::; 0] </ref> respectively.
Reference: [2] <author> Zell et.al. </author> <title> Snns : Stuttgart neural network simulator. </title> <type> Technical Report 3/93, </type> <institution> Institute for Parallel and Distributed High Performance Systems, University of Stuutgart, Fed Rep. of Germany, </institution> <year> 1993. </year>
Reference-contexts: Thus there are only 32 fi 10 + 10 fi 5 = 370 connections in the network, a relatively small number. The second algorithm we consider modifies backpropagation by adding a fraction (the momentum parameter, ff) of the previous weight change during the computation of the new weight change <ref> [2] </ref>. This simple artifice helps moderate changes in the search direction, reduce the notorious oscillation problems common with gradient descent. To take care of the "plateaus", a "flat spot elimination constant" is added to the derivative of f . <p> Again, as mentioned in the previous subsection, both the larger training set and the smaller set were used to separately train the network. Below we detail how each paradigm was evaluated. All the simulations were performed using the Stuttgart Neural Network Simulator <ref> [2] </ref>. As the only "free" parameter in the simple backpropagation paradigm was the learning rate , it was varied in the range [0:1 : : :0:9]. It was observed that the best performance was achieved at a value of = 0:9.
Reference: [3] <author> S.E. Fahlman. </author> <title> Faster-learning Variations on Backpropagation : An Empirical Study. </title> <editor> In T.J. Sejnowski, G.E. Hinton, and D.S. Touretzky, editors, </editor> <booktitle> in 1988 Connectionist Models Summer School. </booktitle> <publisher> Morgan Kauf-mann, </publisher> <year> 1988. </year>
Reference: [4] <author> E. Gallopoulos, E. Houstis, and J.R. Rice. </author> <title> Computer as Thinker/Doer: Problem-Solving Environments for Computational Science. </title> <journal> IEEE Computational Science and Enginerring,, </journal> <volume> vol.1(2):pp.11-23, </volume> <year> 1994. </year>
Reference-contexts: A Problem Solving Environment (PSE) is a computer system that provides all the computational facilities necessary to solve a target class of problems <ref> [4] </ref>. These features include advanced solution methods, automatic or semiautomatic selection of solution methods, and ways to easily incorporate novel solution methods.
Reference: [5] <author> E. N. Houstis, J. R. Rice, N. P. Chrisochoides, H. C. Karathanasis, P. N. Papachiou, M. K. Samartzis, E. A. Vavalis, Ko-Yang Wang, and S. Weerawarana. </author> <title> //ELLPACK: A numerical simulation programming environment for parallel MIMD machines. </title> <editor> In J. Sopka, editor, </editor> <booktitle> Proceedings of Supercomputing '90, </booktitle> <pages> pages 96-107. </pages> <publisher> ACM Press, </publisher> <year> 1990. </year>
Reference-contexts: While the techniques involved are general, our current implementation of PYTHIA operates in conjuction with systems which solve (elliptic) partial differential equations (//ELLPACK <ref> [5] </ref>). In the rest of this paper, whenever we refer to a "problem" in the context of implementation and testing, we mean a PDE problem. PYTHIA accepts as input the description of a problem, and produces the method (s) appropriate to solve it.
Reference: [6] <author> Anupam Joshi. </author> <title> To Learn or Not to Learn ... In Proc. </title> <booktitle> IJCAI'95 Workshop on Adaptation and Learning in Multiagent Systems, </booktitle> <year> 1995. </year> <note> (to appear). </note>
Reference-contexts: This is because, in the PYTHIA environment, we expect the system to constantly update its database with the new problems it has seen. 5 Collaborative PYTHIA Recently, we have begun to move towards making PYTHIA a collaborative multiagent system <ref> [6] </ref>. This is, as we shall illustrate, a more natural implementation. PDEs can be widely varying. Most application scientists tend to solve only a limited kind, and hence any PYTHIA agent they are running is likely to be able to answer questions effectively only about a limited range of problems.
Reference: [7] <author> T. Kohonen, J. Kangas, J. Laaksoonen, and K. Torkolla. </author> <title> LVQ-PAK Learning Vector Quantization Program Package. </title> <type> Technical Report 2C, </type> <institution> Laboratory of Computer and Information Science, Rakenta-janaukio, </institution> <year> 1992. </year>
Reference-contexts: The accuracy and time needed for learning depend on an appropriately chosen set of codebook vectors and the exact algorithm that modifies the codebook vectors. We detail four different implementations of the LVQ algorithm - LVQ1, OLVQ1, LVQ2 and LVQ3. LVQ PAK <ref> [7] </ref>, a LVQ program training package was used in the simulation. Let x be the input to the LVQ program and let m c denote the `codebook' vector closest to x.
Reference: [8] <author> D.P. O'Leary. </author> <title> Parallel Computing : Emerging from a Time Warp. </title> <journal> IEEE Computational Science and Enginerring,, </journal> <volume> vol.1(3), </volume> <year> 1994. </year>
Reference-contexts: The capabilites provided by HPC power have made scientific computing a rapidly growing field. Yet, for all its potential payoffs, the state-of-the-art in HPC does not match that of its workstation/PC cousin in terms of ease of use. In fact, Diane O'Leary in a recent article <ref> [8] </ref> went so far as to compare parallel computing of today to the "prehistory" of computing, where computers were used by a select few who understood the details of the architecture and operating system, where programming was complex, and debugging required reading hexadecimal dumps.
Reference: [9] <author> John R. Rice, Elias N. Houstis, and Wayne R. Dyksen. </author> <title> A population of linear, second order, elliptic partial differential equations on rectangular domains, part I. </title> <journal> Mathematics of Computation, </journal> <volume> 36 </volume> <pages> 475-484, </pages> <year> 1981. </year>
Reference-contexts: The success of our approach relies heavily on having available a reasonably large population of PDE problems whose characteristics span most of the space of all characteristic vectors. For the class of linear second order elliptic PDEs, PYTHIA uses the population defined in <ref> [9] </ref>. It consists of fifty-six linear, two-dimensional elliptic PDEs defined on rectangular domains. Forty-two of the problems are parameterized which leads to an actual problem space of more than two-hundred and fifty problems.
Reference: [10] <author> D.E. Rumelhart, G.E. Hinton, and R.J. Williams. </author> <title> Learning Internal Representations by Error Propagation. </title> <editor> In D.E. Rumelhart and McClelland J.L., editors, </editor> <booktitle> in Parallel Distributed Processing : Explorations in the Microstructure of Cognition, volume I. </booktitle> <publisher> The MIT Press, </publisher> <year> 1986. </year> <month> 19 </month>
Reference-contexts: This is essentially using gradient descent on the error surface with respect to the weight values. For more details, see the classic text by Rumelhart & McClelland <ref> [10] </ref>. Since the input and output of the network are fixed by the problem, the only layer whose size had to be determined is the hidden layer. We arbitrarily chose this to have 10 elements.
Reference: [11] <author> P.K. Simpson. </author> <title> Fuzzy Min-Max Neural Networks-Part 1: Classification. </title> <journal> IEEE Transactions on Neural Networks,, </journal> <volume> vol.3(5):pp.776-786, </volume> <year> 1992. </year> <month> 20 </month>
Reference-contexts: Typical values of * are 0:1 : : :0:5. The optimal value of * is found to decrease as the window size increases. 3.4 Fuzzy Neural Networks We have developed a new neuro-fuzzy classification scheme suited for this problem. It is based on an algorithm proposed by Simpson <ref> [11] </ref>. The basic idea is to use fuzzy sets to describe pattern classes. These fuzzy sets are, in turn, represented by the fuzzy union of several n-dimensional hyperboxes. Such hyperboxes 7 define a region in n-dimensional pattern space that contain patterns with full-class membership. <p> It is not reasonable to assume that one parameter is sufficient to tune the entire system. Moreover, as mentioned in <ref> [11] </ref>, the effect of on classification accuracy is not completely understood. In this section, we develop an enhanced scheme that operates with such overlapping and non-exclusive classes. In this process, we introduce another parameter ffi to tune the system. <p> Increasing the L 2 error threshold value did not serve to improve the accuracy. Finally, our neuro-fuzzy method, which is a variant of that proposed by Simpson <ref> [11] </ref> was observed to perform quite well. In fact, it performed almost as well as RProp both in terms of % accuracy (95:20%), mean error (0:05534) and median error (0:000001). Like RProp, increasing the L 2 error threshold did not significantly alter the performance.
References-found: 11

