URL: http://www.cs.princeton.edu/~jps/papers/latency-occupancy.ps
Refering-URL: http://www.cs.princeton.edu/~jps/papers/appls-arch.html
Root-URL: http://www.cs.princeton.edu
Title: THE EFFECTS OF LATENCY, OCCUPANCY, AND BANDWIDTH IN DISTRIBUTED SHARED MEMORY MULTIPROCESSORS  
Author: Chris Holt, Mark Heinrich, Jaswinder Pal Singh, Edward Rothberg, and John Hennessy 
Note: This research has been supported by ARPA contract DABT63-94-C-0054.  
Date: January 1995  
Pubnum: Technical Report No. CSL-TR-95-660  
Abstract-found: 0
Intro-found: 1
Reference: [ACD+91] <author> Anant Agarwal et al. </author> <title> The MIT Alewife Machine: A Large-Scale Distributed-Memory Multiprocessor. </title> <institution> MIT/LCS Memo TM-454, Massachusetts Institute of Technology, </institution> <year> 1991. </year>
Reference-contexts: L2, L4, L8 L16 L32 General-purpose Co-processor on Memory Bus o=112 pclocks, High Occupancy L1 L2, L4, L8 L16 L32 General-purpose Co-processor on I/O Bus o=224 pclocks, Very high Occupancy L1 L2, L4, L8 L16 L32 4 Small values of occupancy represent communication controllers which are tightly-integrated, hardwired state machines <ref> [ACD+91, KSR92, LLG+92] </ref>. As o increases the controller becomes less hardwired and more general-purpose, from specialized co-processors that execute protocol code sequences [KOH+94], through inexpensive off-the-shelf processors on the memory bus [RLW94], to a controller on the I/O bus of the main processor [BLA+94, SSA+95].
Reference: [BLA+94] <author> M. Blumrich et al. </author> <title> A Virtual Memory Mapped Network Interface for the SHRIMP Multicomputer. </title> <booktitle> In Proceedings of the 21st Annual Symposium on Computer Architecture, </booktitle> <pages> pages 142-153, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: As o increases the controller becomes less hardwired and more general-purpose, from specialized co-processors that execute protocol code sequences [KOH+94], through inexpensive off-the-shelf processors on the memory bus [RLW94], to a controller on the I/O bus of the main processor <ref> [BLA+94, SSA+95] </ref>. The entries in Table 2.1 correspond to specific values in our range of latencies and occupancies, which like all cycle times in this paper are expressed in 200 MHz processor clocks (pclocks).
Reference: [CKP+93] <author> David Culler et al. </author> <title> LogP: Toward a realistic model of parallel computation. </title> <booktitle> In Proceedings of the Principles and Practice of Parallel Processing, </booktitle> <pages> pages 1-12, </pages> <year> 1993. </year>
Reference-contexts: We characterize the communication architectures of DSM multiprocessors by a few key parameters that are similar to those in the logP model <ref> [CKP+93] </ref>. More detailed design decisions and machine parameters, which are crucial to establishing an architectural context, are held constant in our evaluation. We describe this architectural context in the next section. Section 3 describes the framework and methodology we use to study the effectiveness of different types of DSM architectures.
Reference: [Golds93] <author> Stephen Goldschmidt. </author> <title> Simulation of Multiprocessors: Accuracy and Performance. </title> <type> Ph.D. Thesis, </type> <institution> Stanford University, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: applications where applicable: one that tries to hide read latency with software-controlled prefetches, which we insert by hand, and the other that does not. 3.2 Simulation Environment The results presented in this paper are gathered from a detailed multi-threaded memory simulator that interfaces to the Tango Lite event-driven reference generator <ref> [Golds93] </ref>. The simulator models contention in detail within the communication controller, between the controller and its external interfaces, at main memory, and at the system bus.
Reference: [HKO+94] <author> Mark Heinrich et al. </author> <title> The Performance Impact of Flexibility in the Stanford FLASH Multiprocessor. </title> <booktitle> In Proceedings of the 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 274-285, </pages> <address> San Jose, CA, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: When the communication controller is the home of a network request it incurs occupancy 2o, because it has to retrieve data from memory and/or manipulate coherence state information <ref> [HKO+94] </ref>. In this case we assume the memory access happens in parallel with the operation of the controller. If the state lookup at the home reveals that the requested line is dirty in the home nodes cache, the communication controller incurs an extra fixed occupancy C. <p> For the communication controllers internal queue sizes and fixed interface delays, we use the values from the Stanford FLASH multiprocessor design as being representative given current technology <ref> [HKO+94] </ref>. The input and output queue sizes in the controllers processor and network interfaces are uniformly set at 16 entries. We assume processor interface delays of 2 pclocks inbound and 8 pclocks outbound, and network interface delays of 16 pclocks inbound and 8 pclocks outbound.
Reference: [HS94] <author> Chris Holt and Jaswinder Pal Singh. </author> <title> Hierarchical N-Body Methods on Shared Address Space Multiprocessors. </title> <booktitle> SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <month> February </month> <year> 1995, </year> <note> to appear. </note>
Reference-contexts: Finally, Radix is a widely used sorting algorithm. Descriptions of the applications can be found in: Barnes <ref> [HS94] </ref>; Radix and Ocean [WSH94]; Water [SWG+95]; FFT and LU [RSG93]. The applications are quite highly optimized to improve communication performance, and particularly to reduce spurious hot-spotting or contention effects that adversely impact controller occupancy.
Reference: [Katz89] <author> Jacob Katzenelson. </author> <title> Computational Structure of the N-body Problem. </title> <journal> SIAM Journal of Scientific and Statistical Computing, </journal> <pages> pages 787-815, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: Since the computation-to-communication ratio also depends on the distribution of particles, predicting the problem size required for 60% efficiency is difficult. However, similar hierarchical N-body applications have an expected computation-to-communication ratio that is linear in the problem size <ref> [Katz89] </ref>. This suggests that scaling hierarchical N-body applications to retain a desired efficiency should be relatively easy, if communication is the primary bottleneck.
Reference: [KOH+94] <author> Jeffrey Kuskin et al. </author> <title> The Stanford FLASH Multiprocessor. </title> <booktitle> In Proceedings of the 21st International Symposium on Computer Architecture, </booktitle> <pages> pages 302-313, </pages> <address> Chicago, IL, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: As o increases the controller becomes less hardwired and more general-purpose, from specialized co-processors that execute protocol code sequences <ref> [KOH+94] </ref>, through inexpensive off-the-shelf processors on the memory bus [RLW94], to a controller on the I/O bus of the main processor [BLA+94, SSA+95].
Reference: [KSR92] <institution> Kendall Square Research. </institution> <type> KSR1 Technical Summary. </type> <address> Waltham, MA, </address> <year> 1992. </year>
Reference-contexts: L2, L4, L8 L16 L32 General-purpose Co-processor on Memory Bus o=112 pclocks, High Occupancy L1 L2, L4, L8 L16 L32 General-purpose Co-processor on I/O Bus o=224 pclocks, Very high Occupancy L1 L2, L4, L8 L16 L32 4 Small values of occupancy represent communication controllers which are tightly-integrated, hardwired state machines <ref> [ACD+91, KSR92, LLG+92] </ref>. As o increases the controller becomes less hardwired and more general-purpose, from specialized co-processors that execute protocol code sequences [KOH+94], through inexpensive off-the-shelf processors on the memory bus [RLW94], to a controller on the I/O bus of the main processor [BLA+94, SSA+95].
Reference: [LLG+92] <author> Daniel Lenoski et al. </author> <title> The Stanford DASH Multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: L2, L4, L8 L16 L32 General-purpose Co-processor on Memory Bus o=112 pclocks, High Occupancy L1 L2, L4, L8 L16 L32 General-purpose Co-processor on I/O Bus o=224 pclocks, Very high Occupancy L1 L2, L4, L8 L16 L32 4 Small values of occupancy represent communication controllers which are tightly-integrated, hardwired state machines <ref> [ACD+91, KSR92, LLG+92] </ref>. As o increases the controller becomes less hardwired and more general-purpose, from specialized co-processors that execute protocol code sequences [KOH+94], through inexpensive off-the-shelf processors on the memory bus [RLW94], to a controller on the I/O bus of the main processor [BLA+94, SSA+95].
Reference: [RLW94] <author> Steven K. Reinhardt, James R. Larus, and David A. Wood. Tempest and Typhoon: </author> <title> User-Level Shared Memory. </title> <booktitle> In Proceedings of the 21st International Symposium on Computer Architecture, </booktitle> <pages> pages 325-336, </pages> <address> Chicago, IL, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: As o increases the controller becomes less hardwired and more general-purpose, from specialized co-processors that execute protocol code sequences [KOH+94], through inexpensive off-the-shelf processors on the memory bus <ref> [RLW94] </ref>, to a controller on the I/O bus of the main processor [BLA+94, SSA+95]. The entries in Table 2.1 correspond to specific values in our range of latencies and occupancies, which like all cycle times in this paper are expressed in 200 MHz processor clocks (pclocks).
Reference: [Roth93] <author> Edward Rothberg. </author> <title> Exploiting the Memory Hierarchy in Sequential and Parallel Sparse Cholesky Factorization. </title> <type> Ph.D. Thesis, </type> <institution> Stanford University, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: We use an optimized, high-radix version of FFT. The most common need for large dense LU factorization is in radar cross-section problems; however, for our purposes dense LU factorization is very similar to more widely used sparse matrix factorization techniques (such as blocked Cholesky factorization <ref> [Roth93] </ref>), and of various other matrix factorization and eigenvalue methods. Finally, Radix is a widely used sorting algorithm. Descriptions of the applications can be found in: Barnes [HS94]; Radix and Ocean [WSH94]; Water [SWG+95]; FFT and LU [RSG93].
Reference: [RSG93] <author> Edward Rothberg, Jaswinder Pal Singh and Anoop Gupta. </author> <title> Working Sets, Cache Sizes, and Node Granularity for Large-Scale Multiprocessors. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 14-25, </pages> <address> San Diego, CA, </address> <year> 1993. </year>
Reference-contexts: Finally, Radix is a widely used sorting algorithm. Descriptions of the applications can be found in: Barnes [HS94]; Radix and Ocean [WSH94]; Water [SWG+95]; FFT and LU <ref> [RSG93] </ref>. The applications are quite highly optimized to improve communication performance, and particularly to reduce spurious hot-spotting or contention effects that adversely impact controller occupancy.
Reference: [Salmon90] <author> John K. Salmon. </author> <title> Parallel Hierarchical N-body Methods. </title> <type> Ph.D. Thesis, </type> <institution> California Institute of Technology, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: Like FFT, the effect of contention is greater in the prefetched version of the code. Barnes: Unlike the previous applications, Barnes does not have separate phases of communication and computation (though there are more structured versions of the application, written for message passing machines, that do <ref> [Salmon90] </ref>). As problem size increases, more computation is done between communications, so contention decreases. Since the computation-to-communication ratio also depends on the distribution of particles, predicting the problem size required for 60% efficiency is difficult.
Reference: [SFL+94] <author> Ioannis Schoinas et al. </author> <title> Fine-grain Access Control for Distributed Shared Memory. </title> <booktitle> In Proceedings of the 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 297-306, </pages> <address> San Jose, CA, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: We assume that cache misses that access local memory and do not generate any communication do not invoke the controller and thus incur no occu 3 pancy <ref> [SFL+94] </ref>. We also assume that the state lookup that determines if a cache miss needs to invoke the controller is free. These assumptions minimize the burden on the communication controller and hence expose more fundamental limitations.
Reference: [SHG93] <author> Jaswinder Pal Singh, John L. Hennessy, and Anoop Gupta. </author> <title> Scaling parallel programs for multiprocessors: methodology and examples. </title> <booktitle> IEEE Computer, </booktitle> <month> July </month> <year> 1993. </year>
Reference-contexts: In most real scientific applications the execution time grows more rapidly than the data set size for a variety of reasons <ref> [SHG93] </ref>, and we shall comment on this as well. <p> This is because increasing data set size also requires scaling other parameters (such as the accuracy used in the multigrid solver and the number of times-steps), which increase execution time further <ref> [SHG93] </ref>. In fact, the numbers for data set size in Table 5.3 are themselves optimistic, since a larger number of grid points causes more time to be spent in the multigrid equation solver, which has the highest communication to computation ratio and the worst load imbalance in the application.
Reference: [SSA+95] <author> Craig B. Stunkel et al. </author> <title> The SP2 High-Performance Switch. </title> <journal> IBM Systems Journal, </journal> <volume> vol. 34, no. 2, </volume> <year> 1995. </year>
Reference-contexts: As o increases the controller becomes less hardwired and more general-purpose, from specialized co-processors that execute protocol code sequences [KOH+94], through inexpensive off-the-shelf processors on the memory bus [RLW94], to a controller on the I/O bus of the main processor <ref> [BLA+94, SSA+95] </ref>. The entries in Table 2.1 correspond to specific values in our range of latencies and occupancies, which like all cycle times in this paper are expressed in 200 MHz processor clocks (pclocks).
Reference: [SWG+95] <author> Jaswinder Pal Singh et al. </author> <title> The SPLASH-2 Suite of Parallel Applications, </title> <note> Technical Report to appear, </note> <institution> Stanford University. </institution>
Reference-contexts: Finally, Radix is a widely used sorting algorithm. Descriptions of the applications can be found in: Barnes [HS94]; Radix and Ocean [WSH94]; Water <ref> [SWG+95] </ref>; FFT and LU [RSG93]. The applications are quite highly optimized to improve communication performance, and particularly to reduce spurious hot-spotting or contention effects that adversely impact controller occupancy. The codes for the applications are taken from the SPLASH-2 application suite [SWG+95], although Radix was modified to use a tree data <p> found in: Barnes [HS94]; Radix and Ocean [WSH94]; Water <ref> [SWG+95] </ref>; FFT and LU [RSG93]. The applications are quite highly optimized to improve communication performance, and particularly to reduce spurious hot-spotting or contention effects that adversely impact controller occupancy. The codes for the applications are taken from the SPLASH-2 application suite [SWG+95], although Radix was modified to use a tree data structure (rather than a linear key chain) to communicate ranks and densities efficiently. 4 Results for a Fixed Problem Size Despite the approach to understanding performance chosen in Section 3.1, it is useful to first examine how the parallel efficiency of
Reference: [Valiant90] <author> Leslie G. Valiant. </author> <title> A Bridging Model for Parallel Computation. </title> <journal> In Communications of the ACM, </journal> <volume> 33(8) </volume> <pages> 103-111, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: In many structured applications, communication is isolated in different phases from local computation. This separation is reected in models of structured parallel programming such as the Bulk Synchronous Processing (BSP) model <ref> [Valiant90] </ref>. As a result, although the overall computation-to-communication ratio over the whole application increases with problem size, within the communication phases the ratio remains constant as problem size grows.
Reference: [WSH94] <author> Steven Cameron Woo, Jaswinder Pal Singh, and John L. Hennessy. </author> <title> The Performance Advantages of Integrating Block Data Transfer in Cache-Coherent Multiprocessors. </title> <booktitle> In Proceedings of the 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 219-229, </pages> <address> San Jose, CA, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: Finally, Radix is a widely used sorting algorithm. Descriptions of the applications can be found in: Barnes [HS94]; Radix and Ocean <ref> [WSH94] </ref>; Water [SWG+95]; FFT and LU [RSG93]. The applications are quite highly optimized to improve communication performance, and particularly to reduce spurious hot-spotting or contention effects that adversely impact controller occupancy.
References-found: 20

