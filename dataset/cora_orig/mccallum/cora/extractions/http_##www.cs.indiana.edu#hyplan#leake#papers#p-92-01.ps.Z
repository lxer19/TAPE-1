URL: http://www.cs.indiana.edu/hyplan/leake/papers/p-92-01.ps.Z
Refering-URL: http://www.cs.indiana.edu/hyplan/leake/papers/INDEX.html
Root-URL: 
Email: leake@cs.indiana.edu  
Title: Constructive Similarity Assessment: Using Stored Cases to Define New Situations  
Author: David B. Leake 
Address: Bloomington, IN 47405  
Affiliation: Department of Computer Science Indiana University  
Note: Proceedings of the Fourteenth Annual Conference of the Cognitive Science Society, 1992, pp. 313-318  
Abstract: A fundamental issue in case-based reasoning is similarity assessment: determining similarities and differences between new and retrieved cases. Many methods have been developed for comparing input case descriptions to the cases already in memory. However, the success of such methods depends on the input case description being sufficiently complete to reflect the important features of the new situation, which is not assured. In case-based explanation of anomalous events during story understanding, the anomaly arises because the current situation is incompletely understood; consequently, similarity assessment based on matches between known current features and old cases is likely to fail because of gaps in the current case's description. Our solution to the problem of gaps in a new case's description is an approach that we call constructive similarity assessment. Constructive similarity assessment treats similarity assessment not as a simple comparison between fixed new and old cases, but as a process for deciding which types of features should be investigated in the new situation and, if the features are borne out by other knowledge, added to the description of the current case. Constructive similarity assessment does not merely compare new cases to old: using prior cases as its guide, it dynamically carves augmented descriptions of new cases out of memory. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Ashley, K. & Rissland, E. </author> <year> (1987). </year> <title> Compare and contrast, a test of expertise. </title> <booktitle> In Proceedings of the Sixth Annual National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 273-284 Palo Alto. </address> <publisher> AAAI, Mor-gan Kaufmann, Inc. </publisher>
Reference-contexts: Likewise, in legal domains, input cases are routinely described in legal briefs that include all relevant features of the situation under consideration; they provide all the information that needs to be considered by CBR systems such as HYPO <ref> (Ashley & Rissland, 1987) </ref> and GREBE (Branting & Porter, 1991) as those systems identify similar cases. In such domains, for which input cases are guaranteed to include sufficient relevant features, traditional similarity assessment| comparison of the new case's features with features of a stored case|is appropriate.
Reference: <author> Bareiss, R. </author> <year> (1989). </year> <title> Exemplar-Based Knowledge Acquisition: A Unified Approach to Concept Representation, Classification, and Learning. </title> <publisher> Academic Press, Inc., </publisher> <address> San Diego. </address>
Reference-contexts: The decisions of whether a retrieved case applies, and of where to adapt it if it fails to apply, depend on similarity judgements; consequently, similarity criteria have been the subject of considerable study. Many approaches have resulted (see <ref> (Bareiss & King, 1989) </ref> for a sampling), but they share a common property: they compare some subset of the features provided by the input case to features of cases stored in memory. <p> Both GREBE (Branting & Porter, 1991) and PROTOS <ref> (Bareiss, 1989) </ref> also go beyond requiring literal feature matches, using explanation of more abstract relevance of features to decide similarity (e.g., PROTOS can match the legs of one chair to the pedestal of another, because both serve as a seat support).
Reference: <author> Bareiss, R. (Ed.)., </author> <title> Bareiss (1991). </title> <booktitle> Proceedings of the Case-Based Reasoning Workshop, </booktitle> <address> Palo Alto. </address> <publisher> DARPA, Morgan Kaufmann, Inc. </publisher>
Reference-contexts: Introduction Case-based reasoning (CBR) systems facilitate processing of new cases by retrieving stored information about similar prior episodes, and adapting solutions from the prior episodes to fit the new situation (for a selection of current CBR approaches, see <ref> (Bareiss, 1991) </ref>). A fundamental issue in applying the CBR process is similarity assessment: how to judge the similarity between new cases and those retrieved from memory.
Reference: <author> Bareiss, R. & King, J. </author> <year> (1989). </year> <title> Similarity assessment in case-based reasoning. </title> <editor> In Hammond, K. (Ed.), </editor> <booktitle> Proceedings of the Case-Based Reasoning Workshop, </booktitle> <pages> pp. </pages> <address> 67-71 San Mateo. </address> <publisher> DARPA, Morgan Kaufmann, Inc. </publisher>
Reference-contexts: The decisions of whether a retrieved case applies, and of where to adapt it if it fails to apply, depend on similarity judgements; consequently, similarity criteria have been the subject of considerable study. Many approaches have resulted (see <ref> (Bareiss & King, 1989) </ref> for a sampling), but they share a common property: they compare some subset of the features provided by the input case to features of cases stored in memory. <p> Both GREBE (Branting & Porter, 1991) and PROTOS <ref> (Bareiss, 1989) </ref> also go beyond requiring literal feature matches, using explanation of more abstract relevance of features to decide similarity (e.g., PROTOS can match the legs of one chair to the pedestal of another, because both serve as a seat support).
Reference: <author> Branting, K. & Porter, B. </author> <year> (1991). </year> <title> Rules and precedents as complementary warrants. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 3-9 Anaheim, CA. </address> <publisher> AAAI. </publisher>
Reference-contexts: Likewise, in legal domains, input cases are routinely described in legal briefs that include all relevant features of the situation under consideration; they provide all the information that needs to be considered by CBR systems such as HYPO (Ashley & Rissland, 1987) and GREBE <ref> (Branting & Porter, 1991) </ref> as those systems identify similar cases. In such domains, for which input cases are guaranteed to include sufficient relevant features, traditional similarity assessment| comparison of the new case's features with features of a stored case|is appropriate. However, input information is not always complete. <p> Both GREBE <ref> (Branting & Porter, 1991) </ref> and PROTOS (Bareiss, 1989) also go beyond requiring literal feature matches, using explanation of more abstract relevance of features to decide similarity (e.g., PROTOS can match the legs of one chair to the pedestal of another, because both serve as a seat support).
Reference: <author> Hammond, K. </author> <year> (1989). </year> <title> Case-Based Planning: Viewing Planning as a Memory Task. </title> <publisher> Academic Press, </publisher> <address> San Diego. </address>
Reference: <author> Kass, A. </author> <year> (1986). </year> <title> Modifying explanations to understand stories. </title> <booktitle> In Proceedings of the Eighth Annual Conference of the Cognitive Science Society Amherst, </booktitle> <address> MA. </address> <publisher> Cognitive Science Society. </publisher>
Reference: <author> Kolodner, J. </author> <year> (1987). </year> <title> Extending problem solving capabilities through case-based inference. </title> <booktitle> In Proceedings of the Fourth International Workshop on Machine Learning Irvine, CA. Machine Learning, </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: For example, the standard input to a planner or problem-solver is a set of goals and constraints. Descriptions of those goals and constraints provide the essential information that CBR systems such as CHEF (Ham-mond, 1989) and JULIA <ref> (Kolodner, 1987) </ref> need in order to judge the similarity of prior cases stored in their memories.
Reference: <author> Kolodner, J. </author> <year> (1989). </year> <title> Selecting the best case for a case-based reasoner. </title> <booktitle> In Proceedings of the Eleventh Annual Conference of the Cognitive Science Society, </booktitle> <pages> pp. </pages> <address> 155-162 Ann Arbor, MI. </address> <publisher> Cognitive Science Society. </publisher>
Reference: <author> Leake, D. </author> <year> (1991). </year> <title> ACCEPTER: a program for dynamic similarity assessment in case-based explanation. In Bareiss, </title> <editor> R. (Ed.), </editor> <booktitle> Proceedings of the Case-Based Reasoning Workshop, </booktitle> <pages> pp. </pages> <address> 51-62 San Mateo. </address> <publisher> DARPA, Morgan Kaufmann, Inc. </publisher>
Reference: <author> Leake, D. </author> <year> (1992). </year> <title> Evaluating Explanations: A Content Theory. </title> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hills-dale, NJ. </address>
Reference-contexts: Programs for Constructive Similarity Assessment As a more concrete illustration, we consider the constructive similarity assessment process investigated in the case-based explanation framework of SWALE (Kass, 1986; Leake & Owens, 1986; Schank & Leake, 1989) and of ACCEPTER, a system which began as the case evaluation component of SWALE. <ref> (Leake, 1992) </ref>. Both SWALE and ACCEPTER are story understanding systems that use case-based reasoning to explain anomalous events in news stories.
Reference: <author> Leake, D. & Owens, C. </author> <year> (1986). </year> <title> Organizing memory for explanation. </title> <booktitle> In Proceedings of the Eighth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pp. </pages> <address> 710-715 Amherst, MA. </address> <publisher> Cognitive Science Society. </publisher>
Reference: <author> Owens, C. </author> <year> (1988). </year> <title> Domain-independent prototype cases for planning. </title> <editor> In Kolodner, J. (Ed.), </editor> <booktitle> Proceedings of a Workshop on Case-Based Reasoning, </booktitle> <pages> pp. </pages> <address> 302-311 Palo Alto. </address> <publisher> DARPA, Morgan Kaufmann, Inc. </publisher>
Reference: <author> Porter, J. (Ed.). </author> <year> (1973). </year> <title> The Family Car. </title> <publisher> Time-Life Books, </publisher> <address> New York. </address>
Reference-contexts: She parked at the store, went inside and bought a pint of vanilla; the car started perfectly. She drove around the block a few times, parked and bought peppermint. When she came out, the engine would not start. <ref> (Porter, 1973, pp. 253-254) </ref> In this example, the flavor was implicated in the explanation: vanilla was sufficiently popular to be prepackaged, while peppermint was hand packed. Hand packing caused the purchase to take a few minutes more, allowing fuel from the carburetor to percolate into the engine and flood it.
Reference: <author> Rieger, C. </author> <year> (1975). </year> <title> Conceptual memory and inference. In Conceptual Information Processing. </title> <publisher> North-Holland, Amsterdam. </publisher>
Reference-contexts: Stories are incomplete, and in principle, any of the many possible inferences from the text of a story could be relevant to explaining an anomaly, but forming all those connections is an overwhelming task <ref> (Rieger, 1975) </ref>. For example, suppose an understanding system attempts to explain the breakdown of a car during a routine shopping trip.
Reference: <author> Schank, R. </author> <year> (1982). </year> <title> Dynamic Memory: A Theory of Learning in Computers and People. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, England. </address>
Reference-contexts: Conclusion Similarity assessment processes traditionally assume that all relevant information about a new 1 In this respect, the flavor of this model is very similar to that of Schank's dynamic memory theory <ref> (Schank, 1982) </ref>. case is available at the time of case retrieval, so that similarity assessment is simply matching of given features. We have shown that this assumption does not hold in real-world explanation; nor will a priori methods work for elaborating the important features to consider in a new case.
Reference: <author> Schank, R. & Leake, D. </author> <year> (1989). </year> <title> Creativity and learning in a case-based explainer. </title> <journal> Artificial Intelligence, </journal> <volume> 40 (1-3), </volume> <pages> 353-385. </pages> <note> Also in Carbonell, </note> <editor> J., editor, </editor> <title> Machine Learning: Paradigms and Methods, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
References-found: 17

