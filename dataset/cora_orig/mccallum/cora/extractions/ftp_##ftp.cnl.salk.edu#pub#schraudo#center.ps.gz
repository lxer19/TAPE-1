URL: ftp://ftp.cnl.salk.edu/pub/schraudo/center.ps.gz
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00330.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: nic@idsia.ch  
Title: On Centering Neural Network Weight Updates  
Author: Nicol N. Schraudolph 
Web: http://www.idsia.ch/  
Address: Corso Elvezia 36 6900 Lugano, Switzerland  
Affiliation: IDSIA,  
Date: 19, 1997 revised October 2, 1997  
Note: April  
Abstract: Technical Report IDSIA-19-97 Abstract. It has long been known that neural networks can learn faster when their input and hidden unit activities are centered about zero; recently we have extended this approach to also encompass the centering of error signals (Schraudolph and Sejnowski, 1996). Here we generalize this notion to all factors involved in the weight update, leading us to propose centering the slope of hidden unit activation functions as well. Slope centering removes the linear component of backpropagated error; this improves credit assignment in networks with shortcut connections. Benchmark results show that this can speed up learning significantly without adversely affecting the trained network's generalization ability.
Abstract-found: 1
Intro-found: 1
Reference: <editor> Anderson, J., and Rosenfeld, E. (Eds.). </editor> <booktitle> (1988). Neurocomputing: Foundations of Research. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge. </address>
Reference: <author> Battiti, R. </author> <year> (1992). </year> <title> First- and second-order methods for learning: Between steepest descent and Newton's method. </title> <journal> Neural Computation, </journal> <volume> 4 (2), </volume> <pages> 141-166. </pages>
Reference: <author> Battiti, R. </author> <year> (1989). </year> <title> Accelerated back-propagation learning: Two optimization methods. </title> <journal> Complex Systems, </journal> <volume> 3, </volume> <pages> 331-342. </pages> <note> 15 Bienenstock, </note> <author> E., Cooper, L., and Munro, P. </author> <year> (1982). </year> <title> Theory for the development of neuron selec-tivity: Orientation specificity and binocular interaction in visual cortex. </title> <journal> Journal of Neuro-science, </journal> <volume> 2. </volume> <editor> Reprinted in (Anderson and Rosenfeld, </editor> <year> 1988). </year>
Reference: <author> Deterding, D. H. </author> <year> (1989). </year> <title> Speaker Normalisation for Automatic Speech Recognition. </title> <type> Ph.D. thesis, </type> <institution> University of Cambridge. </institution>
Reference: <author> Finke, M., and Muller, K.-R. </author> <year> (1994). </year> <title> Estimating a-posteriori probabilities using stochastic network models. </title> <editor> In Mozer, M. C., Smolensky, P., Touretzky, D. S., Elman, J. L., and Weigend, A. S. (Eds.), </editor> <booktitle> Proceedings of the 1993 Connectionist Models Summer School, </booktitle> <address> Boulder, </address> <publisher> CO. Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ. </address>
Reference: <author> Hastie, T. J., and Tibshirani, R. J. </author> <year> (1996). </year> <title> Discriminant adaptive nearest neighbor classification. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 18 (6), </volume> <pages> 607-616. </pages>
Reference: <author> Herrmann, M. </author> <year> (1997). </year> <title> On the merits of topography in neural maps. </title> <editor> In Kohonen, T. (Ed.), </editor> <booktitle> Proceedings of the Workshop on Self-Organizing Maps, </booktitle> <pages> pp. 112-117. </pages> <institution> Helsinki University of Technology. </institution>
Reference-contexts: Even better performance can be obtained by using speaker sex/identity information (Turney, 1993a, 1993b) or by training a separate model for each vowel <ref> (Herrmann, 1997) </ref>. Combining these approaches, Tenenbaum and Freeman (1997) have reached a test set error of 23%, the lowest we are aware of to date. Training and testing. We trained fully connected feedforward networks with 10 inputs, 22 hidden units, and 11 logistic output units by minimization of cross-entropy loss.
Reference: <author> Hochreiter, S., and Schmidhuber, J. </author> <year> (1997). </year> <title> Lococode. </title> <type> Tech. rep. </type> <institution> FKI-222-97, Fakultat fur Informatik, Technische Universitat Munchen. </institution>
Reference: <author> Intrator, N. </author> <year> (1992). </year> <title> Feature extraction using an unsupervised neural network. </title> <journal> Neural Computation, </journal> <volume> 4 (1), </volume> <pages> 98-107. </pages>
Reference: <author> Lapedes, A., and Farber, R. </author> <year> (1986). </year> <title> A self-optimizing, nonsymmetrical neural net for content addressable memory and pattern recognition. </title> <journal> Physica, </journal> <volume> D 22, </volume> <pages> 247-259. </pages>
Reference: <author> LeCun, Y., Kanter, I., and Solla, S. A. </author> <year> (1991). </year> <title> Eigenvalues of covariance matrices: Application to neural-network learning. </title> <journal> Physical Review Letters, </journal> <volume> 66 (18), </volume> <pages> 2396-2399. </pages>
Reference-contexts: For instance, the hyperbolic tangent (tanh) function with its symmetric range from -1 to 1 will typically produce better-centered output than the commonly used logistic sigmoid f (y) = 1=(1 + e y ) ranging from 0 to 1, and is therefore the preferred activation function for hidden units <ref> (LeCun et al., 1991) </ref>. Similarly, the input representation can (and should) be chosen such that inputs will be roughly centered. When using shortcuts, one may even choose a priori to subtract a constant (say, half their maximum) from hidden unit slopes to improve their centering.
Reference: <editor> Orr, G. B., and Muller, K.-R. (Eds.). </editor> <year> (1998). </year> <title> Tricks of the Trade: How to Make Neural Networks Really Work (working title). </title> <booktitle> To appear in Lecture Notes in Computer Science. </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin. </address>
Reference-contexts: The benefits of centering error signals in multi-layer networks were thus reported only recently (Schraudolph and Sejnowski, 1996); here we finally suggest centering as a general methodology, and present backpropagation equations in which all factors are centered. ? Preprint | to appear in <ref> (Orr and Muller, 1998) </ref> Independence of architecture. Although centering is introduced here in the context of feed--forward networks with sigmoid activation functions, the approach itself has a far wider reach.
Reference: <author> Robinson, A. J. </author> <year> (1989). </year> <title> Dynamic Error Propagation Networks. </title> <type> Ph.D. thesis, </type> <institution> University of Cam-bridge. </institution>
Reference: <author> Schraudolph, N. N., and Sejnowski, T. J. </author> <year> (1993). </year> <title> Unsupervised discrimination of clustered data via optimization of binary information gain. </title> <editor> In Hanson, S. J., Cowan, J. D., and Giles, C. L. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> Vol. 5, </volume> <pages> pp. 499-506. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: This includes algorithms such as BCM learning (Bienenstock, Cooper, and Munro, 1982; Intrator, 1992) and binary information gain optimization <ref> (Schraudolph and Sejnowski, 1993) </ref>. 4 Empirical Results While activity centering has long been part of backpropagation lore, and empirical results for error centering have been reported by Schraudolph and Sejnowski (1996), slope centering is being proposed for the first time here.
Reference: <author> Schraudolph, N. N., and Sejnowski, T. J. </author> <year> (1996). </year> <title> Tempering backpropagation networks: Not all weights are created equal. </title> <editor> In Touretzky, D. S., Mozer, M. C., and Hasselmo, M. E. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> Vol. 8, </volume> <pages> pp. 563-569. </pages> <publisher> The MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Although Sejnowski (1977) proposed a variant of Hebbian learning in which both the pre- and postsynaptic factors of the weight update are centered, the idea was not taken up when backpropagation became popular. The benefits of centering error signals in multi-layer networks were thus reported only recently <ref> (Schraudolph and Sejnowski, 1996) </ref>; here we finally suggest centering as a general methodology, and present backpropagation equations in which all factors are centered. ? Preprint | to appear in (Orr and Muller, 1998) Independence of architecture. <p> With a local step size ij for each weight, this results in the weight update equation w ij = ij ffi j x i ; where ffi j = @E=@y j : (3) Centered. We have recently proposed <ref> (Schraudolph and Sejnowski, 1996) </ref> that the error signals ffi j should be centered as well to achieve even faster convergence. <p> Since this means that the average error hffi j i is given exclusively to the bias weight w 0 j , we have previously called this technique d.c. error shunting <ref> (Schraudolph and Sejnowski, 1996) </ref>. 2.3 Error Backpropagation Conventional.
Reference: <author> Sejnowski, T. J. </author> <year> (1977). </year> <title> Storing covariance with nonlinearly interacting neurons. </title> <journal> Journal of Mathematical Biology, </journal> <volume> 4, </volume> <pages> 303-321. </pages>
Reference: <author> Shah, S., Palmieri, F., and Datum, M. </author> <year> (1992). </year> <title> Optimal filtering algorithms for fast learning in feedforward neural networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 5, </volume> <pages> 779-787. </pages>
Reference-contexts: Where not done a priori, centering was then implemented 6 with the exact two-pass batch method. In addition, we always updated the hidden-to-output weights of the network before backpropagating error through them. This is known to sometimes improve convergence behavior <ref> (Shah, Palmieri, and Datum, 1992) </ref>, and we have found it to increase stability at the large step sizes we desire. Competitive controls. The ordinary backpropagation (plain gradient descent) algorithm has many known defects, and a large number of acceleration techniques has been proposed for it.
Reference: <author> Tenenbaum, J. B., and Freeman, W. T. </author> <year> (1997). </year> <title> Separating style and content. </title> <editor> In Mozer, M. C., Jordan, M. I., and Petsche, T. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> Vol. 9, </volume> <pages> pp. 662-668. </pages> <publisher> The MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Turney, P. D. </author> <year> (1993a). </year> <title> Exploiting context when learning to classify. </title> <booktitle> In Proceedings of the European Conference on Machine Learning, </booktitle> <pages> pp. 402-407. </pages>
Reference-contexts: Even better performance can be obtained by using speaker sex/identity information <ref> (Turney, 1993a, 1993b) </ref> or by training a separate model for each vowel (Herrmann, 1997). Combining these approaches, Tenenbaum and Freeman (1997) have reached a test set error of 23%, the lowest we are aware of to date. Training and testing.
Reference: <author> Turney, P. D. </author> <year> (1993b). </year> <title> Robust classification with context-sensitive features. </title> <booktitle> In Proceedings of the Sixth International Conference on Industrial and Engineering Applications of Artificial Intelligence and Expert Systems, </booktitle> <pages> pp. 268-276. </pages> <note> 16 Vogl, </note> <author> T. P., Mangis, J. K., Rigler, A. K., Zink, W. T., and Alkon, D. L. </author> <year> (1988). </year> <title> Accelerating the convergence of the back-propagation method. </title> <journal> Biological Cybernetics, </journal> <volume> 59, </volume> <pages> 257-263. </pages>
Reference: <author> Widrow, B., McCool, J. M., Larimore, M. G., and Johnson, Jr., C. R. </author> <year> (1976). </year> <title> Stationary and nonstationary learning characteristics of the LMS adaptive filter. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 64 (8), </volume> <pages> 1151-1162. </pages>
Reference-contexts: Its basic tenet is: All pattern-dependent factors entering the weight update equation for a neural network should be centered, i.e., have their average over patterns subtracted out. Prior work. It is well-known that the inputs to an LMS adaptive filter should be centered to permit rapid yet stable adaptation <ref> (Widrow, McCool, Larimore, and Johnson, 1976) </ref>, and LeCun, Kanter, and Solla (1991) have argued that the same applies to input and hidden unit activity in a multi-layer network.
Reference: <author> Zimmermann, H. G. </author> <year> (1994). </year> <editor> Neuronale Netze als Entscheidungskalkul. In Rehkugler, H., and Zimmermann, H. G. (Eds.), Neuronale Netze in der Okonomie: </editor> <booktitle> Grundlagen und fi-nanzwirtschaftliche Anwendungen, </booktitle> <pages> pp. 1-87. </pages> <editor> Vahlen Verlag, </editor> <title> Munich. This article was processed using the L A T E X macro package with LLNCS style 17 </title>
Reference-contexts: This combination | vario- and bold driver | was then used for all experiments reported here. Thus any performance advantage for centering reported thereafter has been realized on top of a state-of-the-art accelerated gradient method as control. Vario- <ref> (Zimmermann, 1994, page 48) </ref>. This interesting technique sets the local learning rate for each weight inversely proportional to the standard deviation of its stochastic gradient.
References-found: 22

