URL: ftp://markov.utstat.toronto.edu/jeff/james.ps.Z
Refering-URL: http://www.stats.bris.ac.uk/MCMC/pages/list.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: Internet: jeff@utstat.toronto.edu  
Phone: Phone: (416) 978-4594.  
Title: Analysis of the Gibbs sampler for a model related to James-Stein estimators  
Author: by Jeffrey S. Rosenthal* 
Date: (Revised, May 18, 1995.)  
Address: Toronto, Ontario Canada M5S 1A1  
Affiliation: Department of Statistics University of Toronto  
Abstract: Summary. We analyze a hierarchical Bayes model which is related to the usual empirical Bayes formulation of James-Stein estimators. We consider running a Gibbs sampler on this model. Using previous results about convergence rates of Markov chains, we provide rigorous, numerical, reasonable bounds on the running time of the Gibbs sampler, for a suitable range of prior distributions. We apply these results to baseball data from Efron and Morris (1975). For a different range of prior distributions, we prove that the Gibbs sampler will fail to converge, and use this information to prove that in this case the associated posterior distribution is non-normalizable. Acknowledgements. I am very grateful to Jun Liu for suggesting this project, and to Neal Madras for suggesting the use of the Submartingale Convergence Theorem herein. I thank Kate Cowles and Richard Tweedie for helpful conversations, and thank the referees for useful comments. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Y. Amit and U. </author> <title> Grenander (1991), Comparing sweep strategies for stochastic relaxation. </title>
Reference: <author> J. </author> <title> Multivariate Analysis 37, </title> <journal> No. </journal> <volume> 2, </volume> <pages> 197-222. </pages>
Reference: <author> J.R. Baxter and J.S. </author> <title> Rosenthal (1995), Rates of convergence for everywhere-positive Markov chains. </title> <journal> Stat. Prob. Lett. </journal> <volume> 22, </volume> <pages> 333-338. </pages>
Reference: <author> P. </author> <title> Billingsley (1986). Probability and Measure, 2nd ed. </title> <publisher> Wiley & Sons, </publisher> <address> New York. </address>
Reference-contexts: Since the A (k) are non-negative, it follows from the Submartingale Convergence Theorem <ref> (see e.g. Billingsley, 1986, Theorem 35.4) </ref> that A (k) converges almost surely. That is, the values of A (k) actually converge to a fixed random variable (as opposed to converging in distribution).
Reference: <author> M.C. Eaton and W.D. </author> <month> Sudderth </month> <year> (1993), </year> <title> Prediction in a multivariate normal setting: coherence and incoherence. </title> <type> Tech. Rep., </type> <institution> Dept. of Statistics, University of Minnesota. </institution>
Reference-contexts: However, this theorem is interesting in that it indicates how rigorous theoretical analysis of a Gibbs sampler, associated with a given model, can imply information about the model itself. Furthermore, priors with a 1 may sometimes occur as marginals of higher-dimensional priors, such as those for estimating covariance matrices <ref> (cf. Eaton and Sudderth, 1993) </ref>. (ii) For this particular model, it is possible to verify the directly (through integration) that the posterior distribution is improper. However, this might not be clear initially, so it is useful to see how it can be inferred from the associated Gibbs sampler.
Reference: <author> B. Efron and C. </author> <title> Morris (1973), Stein's estimation rule and its competitors An empirical Bayes approach. </title> <journal> J. Amer. Stat. Assoc., </journal> <volume> Vol. 68, No. 341, 117- 130. </volume>
Reference: <author> B. Efron and C. </author> <title> Morris (1975), Data analysis using Stein's estimator and its generalizations. </title> <journal> J. Amer. Stat. Assoc., </journal> <volume> Vol. 70, No. 350, </volume> <pages> 311-319. </pages>
Reference: <author> A. Frieze, R. Kannan, and N.G. </author> <month> Polson </month> <year> (1994), </year> <title> Sampling from log-concave distributions. </title>
Reference: <author> Ann. </author> <title> Appl. </title> <journal> Prob. </journal> <volume> 4, </volume> <pages> 812-837. </pages>
Reference: <author> A. Frigessi, C.R. Hwang, S.J. Sheu, and P. </author> <title> Di Stefano (1993), Convergence rates of the Gibbs sampler, the Metropolis algorithm, and other single-site updating dynamics. </title> <journal> J. Roy. Stat. Soc. Ser. </journal> <volume> B 55, </volume> <pages> 205-220. </pages>
Reference: <author> A.E. Gelfand and A.F.M. </author> <title> Smith (1990), Sampling based approaches to calculating marginal densities. </title> <journal> J. Amer. Stat. Assoc. </journal> <volume> 85, </volume> <pages> 398-409. </pages>
Reference: <author> A.E. Gelfand, S.E. Hills, A. Racine-Poon, and A.F.M. </author> <title> Smith (1990), Illustration of Bayesian inference in normal data models using Gibbs sampling. </title> <journal> J. Amer. Stat. Soc. </journal> <volume> 85, </volume> <pages> 972-985. </pages>
Reference: <author> S. Geman and D. </author> <title> Geman (1984), Stochastic relaxation, Gibbs distributions and the Bayesian restoration of images. </title> <journal> IEEE Trans. on pattern analysis and machine intelligence 6, </journal> <pages> 721-741. </pages>
Reference: <author> W.K. </author> <title> Hastings (1970), Monte Carlo sampling methods using Markov chains and their applications. </title> <journal> Biometrika 57, </journal> <pages> 97-109. </pages>
Reference: <author> W. James and C. </author> <title> Stein (1961), Estimation with Quadratic Loss. </title> <booktitle> Proceedings of the Fourth 13 Berkeley Symposium on Mathematical Statistics and Probability, </booktitle> <volume> Vol. 1, </volume> <publisher> University of California Press, </publisher> <address> Berkeley, </address> <pages> 361-379. </pages>
Reference: <author> M. Jerrum and A. </author> <title> Sinclair (1989), Approximating the permanent. </title> <journal> SIAM J. Comput. </journal> <volume> 18, </volume> <pages> 1149-1178. </pages>
Reference: <author> J. Liu, W. Wong, and A. </author> <title> Kong (1991a), Correlation structure and the convergence of the Gibbs sampler, I. </title> <type> Tech Rep. 299, </type> <institution> Dept. of Statistics, University of Chicago. </institution> <note> Biometrika, to appear. </note>
Reference: <author> J. Liu, W. Wong, and A. </author> <title> Kong (1991b), Correlation structure and the convergence of the Gibbs sampler, II: Applications to various scans. </title> <type> Tech Rep. 304, </type> <institution> Dept. of Statistics, University of Chicago. J. Royal Stat. Sci. (B), </institution> <note> to appear. </note>
Reference: <author> R.B. Lund and R.L. </author> <month> Tweedie </month> <year> (1993), </year> <title> Geometric convergence rates for stochastically ordered Markov chains. </title> <type> Tech. Rep., </type> <institution> Dept. of Statistics, Colorado State University. </institution>
Reference: <author> K.L. Mengersen and R.L. </author> <month> Tweedie </month> <year> (1993), </year> <title> Rates of convergence of the Hastings and Metropolis algorithms. </title> <type> Tech. Rep. 93/30, </type> <institution> Dept. of Statistics, Colorado State University. </institution>
Reference: <author> N. Metropolis, A. Rosenbluth, M. Rosenbluth, A. Teller, and E. </author> <title> Teller (1953), Equations of state calculations by fast computing machines. </title> <journal> J. Chem. Phys. </journal> <volume> 21, </volume> <pages> 1087-1091. </pages>
Reference: <author> S.P. Meyn and R.L. </author> <month> Tweedie </month> <year> (1994), </year> <title> Computable bounds for convergence rates of Markov chains. </title> <journal> Ann. Appl. Prob. </journal> <volume> 4, </volume> <pages> 981-1011. </pages>
Reference: <author> C. </author> <title> Morris (1983), Parametric empirical Bayes confidence intervals. Scientific Inference, Data Analysis, </title> <booktitle> and Robustness, </booktitle> <pages> 25-50. </pages>
Reference: <author> P. Mykland, L. Tierney, and B. </author> <title> Yu (1992), Regeneration in Markov chain samplers. </title> <type> Tech. Rep. 585, </type> <institution> School of Statistics, University of Minnesota. </institution>
Reference: <author> G.O. </author> <title> Roberts (1992), Convergence diagnostics of the Gibbs sampler. In Bayesian Statistics 4 (J.M. </title> <editor> Bernardo et al., eds.), </editor> <address> 777-784. </address> <publisher> Oxford University Press. </publisher>
Reference: <author> J.S. </author> <title> Rosenthal (1993), Rates of convergence for Data Augmentation on finite sample spaces. </title>
Reference: <author> Ann. </author> <title> Appl. </title> <journal> Prob., </journal> <volume> Vol. 3, No. 3, </volume> <pages> 319-339. </pages>
Reference: <author> J.S. </author> <title> Rosenthal (1995a), Rates of convergence for Gibbs sampler for variance components 14 models. </title> <journal> Ann. Stat., </journal> <note> to appear. </note>
Reference: <author> J.S. </author> <title> Rosenthal (1995b), Minorization conditions and convergence rates for Markov chain Monte Carlo. </title> <journal> J. Amer. Stat. Assoc., </journal> <note> to appear. </note>
Reference: <author> M.J. Schervish and B.P. </author> <month> Carlin </month> <year> (1992), </year> <title> On the convergence of successive substitution sampling, </title> <journal> J. Comp. Graph. Stat. </journal> <volume> 1, </volume> <pages> 111-127. </pages>
Reference: <author> M.A. </author> <title> Tanner and W.H. Wong (1987), The calculation of posterior distributions by data augmentation (with discussion). </title> <journal> J. Amer. Stat. Assoc. </journal> <volume> 82, </volume> <pages> 528-550. </pages>
Reference: <author> L. </author> <month> Tierney </month> <year> (1991), </year> <title> Markov chains for exploring posterior distributions. </title> <type> Tech. Rep. 560, </type> <institution> School of Statistics, University of Minnesota. Ann. Stat., </institution> <note> to appear. 15 </note>
Reference-contexts: A fundamental issue regarding such techniques is their convergence properties, specifically whether or not the algorithm will converge to the correct distribution, and if so how quickly. In addition to the many general convergence results <ref> (e.g. Tierney, 1991) </ref> and convergence diagnostics (e.g.
References-found: 32

