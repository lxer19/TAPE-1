URL: http://www.cse.ogi.edu/~mak/PS/phd_thesis2.ps.gz
Refering-URL: http://www.cse.ogi.edu/~mak/brian.mak.html
Root-URL: http://www.cse.ogi.edu
Title: Towards A Compact Speech Recognizer: Subspace Distribution Clustering Hidden Markov Model  
Author: Brian Kan-Wing Mak 
Degree: A dissertation submitted to the faculty of the Oregon Graduate Institute of Science and Technology in partial fulfillment of the requirements for the degree Doctor of Philosophy in Computer Science and Engineering  
Date: 1983  1989  April 1998  
Address: Hong Kong, Hong Kong,  Santa Barbara, USA,  
Affiliation: B.Sc. (Eng.)., Electrical Engineering, University of  M.S., Computer Science, University of California,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> G. Antoniol, F. Brugnara, M. Cettolo, and M. Federico. </author> <title> "Language Model Representation for Beam-Search Decoding". </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 588-591, </pages> <year> 1995. </year>
Reference-contexts: When all the words (or phrases) are independently represented by a sequence of phonemes, the lexical structure is said to be linear. Recently, a more compact structure, the lexical (phonetic) tree <ref> [1, 59, 73] </ref> has been found to greatly improve search speed in large vocabulary speech recognition. In the lexical tree, words in the lexicon are arranged 23 in a tree structure so that those with the same prefix (in their phonetic pronunciations) share the same part of the tree. <p> Each subspace pdf, say, b ik () in stream k of state i, is tied to one of the subspace pdf prototypes of the stream, say, h kl (), 1 l L k . That is, 8 2 fl; 8 i 2 <ref> [1; N ] </ref>; 8 k 2 [1; K]; 9 l 2 [1; L k ] such that b ik () h kl (). <p> Each subspace pdf, say, b ik () in stream k of state i, is tied to one of the subspace pdf prototypes of the stream, say, h kl (), 1 l L k . That is, 8 2 fl; 8 i 2 [1; N ]; 8 k 2 <ref> [1; K] </ref>; 9 l 2 [1; L k ] such that b ik () h kl (). <p> That is, 8 2 fl; 8 i 2 [1; N ]; 8 k 2 [1; K]; 9 l 2 <ref> [1; L k ] </ref> such that b ik () h kl ().
Reference: [2] <author> E. Barnard, R.A. Cole, M. Fanty, and P. Vermeulen. </author> <title> "Real-World Speech Recognition with Neural Networks". </title> <editor> In J. Alspector, R. Goodman, and T.X. Brown, editors, </editor> <booktitle> Proceedings of the International Workshop on Applications of Neural Networks to Telecommunications 2, </booktitle> <pages> pages 186-193. </pages> <publisher> Lawrence Erlbaum Associates, Publishers, </publisher> <year> 1995. </year>
Reference-contexts: This renders SDCHMM very attractive in practical implementation 5 Since most state-of-the-art speech recognition systems are HMM-based, we thus only consider our thesis in such context. Recently artificial neural networks (ANN) have been applied to ASR with some success <ref> [2, 11, 76] </ref>, but HMMs remain the dominant technology. 6 In this thesis, the two terms, "subspace" and "stream" are used interchangeably to mean a feature space of dimension smaller than that of the full feature space. "Subspace" is clearer mathematically, but "stream" is more common in the speech recognition community.
Reference: [3] <author> L.E. Baum, T. Petrie, G. Soules, and N. Weiss. </author> <title> "A Maximization Technique Occurring in the Statistical Analysis of Probabilistic Functions of Markov Chains". </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 41 </volume> <pages> 164-171, </pages> <year> 1970. </year>
Reference-contexts: the number of arcs coming out of the source state [10, 89]. * No explicit duration modeling is performed as it is not found necessary. * Our HMMs are trained using the simpler segmental k-means algorithm [38] which has been found to work as well as the conventional Baum-Welch algorithm <ref> [3] </ref> ( [45, 71, 89]). 15 2.3 Speech Unit to Model Although it may be most natural to model each word individually, the approach is impractical in most recognition tasks, except those of small vocabularies and isolated words. In most cases, there are not sufficient training data for each word. <p> Cast as an optimization problem, the estimation is most commonly solved by the iterative Baum-Welch (BW) algorithm <ref> [3] </ref>, a specific case of the Expectation-Maximization (EM) algorithm [30] with the log likelihood as the score function.
Reference: [4] <author> J.R. Bellegarda and D. Nahamoo. </author> <title> "Tied Mixture Continuous Parameter Modeling for Speech Recognition". </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> 38(12) </volume> <pages> 2033-2045, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: In the past, the technique of parameter tying has been applied successfully at various gran-ularities: Phones (generalized biphones/triphones [48], context-independent phones [49]), states (tied-state HMM [32, 88]), observation distributions (tied-mixture/semi-continuous HMM <ref> [4, 29, 80] </ref>), and feature parameters [85] have all been tied. For example, of the two aforementioned systems, AT&T's ATIS recognizer is a tied-state system, whereas CMU's Sphinx II employs semi-continuous HMMs. <p> Young and Woodland reported a 5-fold reduction in the number of states in their TSHMM systems [88]. 17 Tied-mixture HMM/semi-continuous HMM (SCHMM): SCHMM is a generalization of the discrete HMM in which prototypes are continuous Gaussian distributions with diagonal covariances instead of just mean vectors <ref> [4, 29, 80] </ref>. Consequently, an SCHMM enjoys fast computation of state likelihoods but reduces the quantization errors of discrete HMMs, with a negligible increase in model parameters. <p> To increase the likelihood computation efficiency, fewer streams are desirable as fewer additions are then needed to compute the full-space state likelihoods. However, fewer streams will lead to poorer performance unless more prototypes are used. 4.4 Comparison with Semi-Continuous HMM At first glance, SDCHMMs may appear similar to SCHMMs <ref> [4, 29, 80] </ref>: Both methods divide the feature space into streams, and tie subspace distributions across all states of all HMMs.
Reference: [5] <author> P. Beyerlein. </author> <title> "Fast Log-Likelihood Computation for Mixture Densities in a High-Dimensional Feature Space". </title> <booktitle> In Proceedings of the International Conference on Spoken Language Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 271-274, </pages> <year> 1994. </year>
Reference-contexts: Further computation efficiency is obtained by evaluating the state likelihood due only to the most likely mixture component of the state (see also Section 4.3.2). It has been verified empirically that the technique does not result in any loss of recognition accuracy <ref> [5, 16, 59, 79] </ref>.
Reference: [6] <author> P. Beyerlein and M. Ullrich. </author> <title> "Hamming Distance Approximation for a Fast Log-Likelihood Computation for Mixture Densities". </title> <booktitle> In Proceedings of the European Conference on Speech Communication and Technology, </booktitle> <volume> volume 2, </volume> <pages> pages 1083-1086, </pages> <year> 1995. </year>
Reference-contexts: There are also techniques to speed up computation alone: for example, by simply exercising more vigorous pruning schemes, by computing state likelihoods only from a small subset of the most relevant state probability density distributions <ref> [6, 8, 41, 63, 79] </ref>, or by fast-match techniques [21]. However, these techniques are usually done at the expense of recognition accuracy; in the case of computation speedup, more memory is usually required. <p> They are most commonly used and are found to be about as accurate as the theoretically more sound variants. Their simplicity and explicit parameterization also make them amenable to a wide range of techniques that perform fast state-likelihood computation <ref> [6, 8, 41, 63, 79] </ref> and reduce parameters through parameter tying (see Section 2.4). In particular, the continuous density HMM (CDHMM) is used as generally it is found to be highly accurate.
Reference: [7] <author> E. Bocchieri. </author> <title> "A Study of the Beam-Search Algorithm for Large Vocabulary Continuous Speech Recognition and Methods for Improved Efficiency". </title> <booktitle> In Proceedings of the European Conference on Speech Communication and Technology, </booktitle> <volume> volume 3, </volume> <pages> pages 1521-1523, </pages> <year> 1993. </year> <month> 115 </month>
Reference-contexts: To reduce the otherwise immense search space, Viterbi beam search prunes those states with log likelihoods less than that of the best path (at that moment) by a preset threshold called beam-width. It has been found that the beam-width can greatly be reduced with no loss in recognition accuracy <ref> [7] </ref>; further decrease in the beam-width will trade off accuracy for speed and memory. Further computation efficiency is obtained by evaluating the state likelihood due only to the most likely mixture component of the state (see also Section 4.3.2).
Reference: [8] <author> E. Bocchieri. </author> <title> "Vector Quantization for the Efficient Computation of Continuous Density Likelihoods". </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <volume> volume 2, </volume> <pages> pages 692-695, </pages> <year> 1993. </year>
Reference-contexts: There are also techniques to speed up computation alone: for example, by simply exercising more vigorous pruning schemes, by computing state likelihoods only from a small subset of the most relevant state probability density distributions <ref> [6, 8, 41, 63, 79] </ref>, or by fast-match techniques [21]. However, these techniques are usually done at the expense of recognition accuracy; in the case of computation speedup, more memory is usually required. <p> They are most commonly used and are found to be about as accurate as the theoretically more sound variants. Their simplicity and explicit parameterization also make them amenable to a wide range of techniques that perform fast state-likelihood computation <ref> [6, 8, 41, 63, 79] </ref> and reduce parameters through parameter tying (see Section 2.4). In particular, the continuous density HMM (CDHMM) is used as generally it is found to be highly accurate.
Reference: [9] <author> E. Bocchieri and G. Riccardi. </author> <title> "State Tying of Triphone HMM's for the 1994 AT&T ARPA ATIS Recognizer". </title> <booktitle> In Proceedings of the European Conference on Speech Communication and Technology, </booktitle> <volume> volume 2, </volume> <pages> pages 1499-1502, </pages> <year> 1995. </year>
Reference-contexts: For example, let us look at two state-of-the-art recognizers which were among the top three systems in the ARPA evaluation of ATIS (Airline Travel Information System) [25] in 1994. AT&T's general-purpose ATIS recognizer <ref> [9] </ref> contains more than 6 million parameters in its acoustic models requiring more than 24MB of memory space and consequently runs at 7 times real time to obtain a WER of 5.2% on the ATIS task on an SGI O 2 machine (MIPS R10000 CPU, 195 MHZ, 2GB shared memory). <p> The average frame likelihood of the training utterances is computed to test for model convergence. All acoustic models are speaker-independent and gender-independent 4 . 3.2.4 Ensemble Merging Algorithm for State Tying Triphone states are tied by the ensemble merging algorithm <ref> [9] </ref> shown in Algorithm 2. It is a bottom-up agglomerative clustering algorithm in which two states are tied if the tying results in minimum increase in total ensemble distortion.
Reference: [10] <author> E. Bocchieri, G. Riccardi, and J. Anantharaman. </author> <title> "The 1994 AT&T ATIS CHRONUS Recognizer". </title> <booktitle> In Proceedings of ARPA Spoken Language Systems Technology Workshop, </booktitle> <pages> pages 265-268. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1995. </year>
Reference-contexts: we would also like to point out some minor issues in our implementations of HMMs: * Precise estimation of the transition probabilities is found unnecessary, and each transition arc is simply assigned an equal probability of 1/fan-out, where fan-out is the number of arcs coming out of the source state <ref> [10, 89] </ref>. * No explicit duration modeling is performed as it is not found necessary. * Our HMMs are trained using the simpler segmental k-means algorithm [38] which has been found to work as well as the conventional Baum-Welch algorithm [3] ( [45, 71, 89]). 15 2.3 Speech Unit to Model <p> Since this thesis deals only with acoustic modeling, efficacy of which is best measured by speech recognition alone (i.e. the SPREC test), we restrict ourselves here to describe only its speech-recognition component <ref> [10] </ref>. Refer [52] for its NL understanding component.
Reference: [11] <author> H. Bourlard and N. Morgan. </author> <title> "Hybrid Connectionist Models for Continuous Speech Recognition". </title> <editor> In C.H. Lee, F.K. Soong, and K.K. Paliwal, editors, </editor> <title> Automatic Speech and Speaker Recognition (Advanced Topics), </title> <booktitle> chapter 11, </booktitle> <pages> pages 259-283. </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1996. </year>
Reference-contexts: This renders SDCHMM very attractive in practical implementation 5 Since most state-of-the-art speech recognition systems are HMM-based, we thus only consider our thesis in such context. Recently artificial neural networks (ANN) have been applied to ASR with some success <ref> [2, 11, 76] </ref>, but HMMs remain the dominant technology. 6 In this thesis, the two terms, "subspace" and "stream" are used interchangeably to mean a feature space of dimension smaller than that of the full feature space. "Subspace" is clearer mathematically, but "stream" is more common in the speech recognition community. <p> the joint conditional probabilities in terms of individual conditional probabilities in bigram-constrained HMMs [83] (using observation bigrams), or using an extended logarithmic pool [40] . * Hybrid models of Artificial Neural Networks and HMMs incorporate contextual information simply by using the current feature frame together with neighboring fea ture frames <ref> [11] </ref>. * Hybrid models of Recurrent Neural Networks and HMMs explicitly estimate the posterior phone probability of the current frame conditional on both the state and all its previous frames more effectively [76]. * The state-dependent acoustic trajectory is modeled by an autoregressive process in autoregressive HMMs [37, 68], and in
Reference: [12] <author> M. Cohen, Z. Rivlin, and H. Bratt. </author> <title> "Speech Recognition in the ATIS Domain Using Multiple Knowledge Sources". </title> <booktitle> In Proceedings of ARPA Spoken Language Systems Technology Workshop, </booktitle> <pages> pages 261-264. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1995. </year>
Reference-contexts: The capabilities of some state-of-the-art recognizers over a wide range of tasks are summarized in Figure 1.1 <ref> [12, 28, 34, 54, 87] </ref>. In the figure, the difficulty of a recognition task (from the perspective of a recognizer) is measured by the perplexity of its language model, whereas the performance of a recognizer is gauged by its word error rate (WER).
Reference: [13] <editor> D. Dahl et al. </editor> <title> "Expanding the Scope of the ATIS Task: The ATIS-3 Corpus". </title> <booktitle> In Proceedings of ARPA Human Language Technology Workshop. </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1994. </year>
Reference-contexts: An ATIS system allows users to speak naturally to inquire about air travel information stored as a relational database which is derived from the Official Airline Guide. 19 20 3.1.1 ATIS Corpora Several ATIS corpora 1 known as ATIS0 [23], ATIS1 2 , ATIS2 [25], and ATIS3 <ref> [13] </ref> were collected over the years by several institutions: AT&T, BBN, CMU, MIT, NIST, SRI, and TI. To date, the corpora contain nearly 25,000 utterances with a vocabulary size of 1,536 words.
Reference: [14] <author> L. Deng. </author> <title> "A Generalized Hidden Markov Model with State-Conditioned Trend Functions of Time for the Speech Signal". </title> <booktitle> Signal Processing, </booktitle> <volume> 27(1) </volume> <pages> 65-78, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: estimate the posterior phone probability of the current frame conditional on both the state and all its previous frames more effectively [76]. * The state-dependent acoustic trajectory is modeled by an autoregressive process in autoregressive HMMs [37, 68], and in trended HMMs with a linear polynomial regression function of time <ref> [14] </ref>. 14 * Segmental HMMs such as the stochastic trajectory model [20] and stochastic segment model [62] relax both assumptions and explicitly model acoustic trajectories of variable durations as well as the duration.
Reference: [15] <author> V. Digalakis and H. Murveit. "Genones: </author> <title> Optimizing the Degree of Tying in a Large Vocabulary HMM-based Speech Recognizer". </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 537-540, </pages> <year> 1994. </year>
Reference-contexts: Perhaps the best example is the hidden Markov network of [77] which ties allophones, states, distributions and feature parameters. Alternatively, several tying schemes may also be combined or merged. For example, genones <ref> [15] </ref> or state-clustered tied-mixture HMMs [61] divide all states into classes and only tie the mixtures 2 Speech recognition systems are only concerned with the storage space for the distributions.
Reference: [16] <author> J. Fritsch, I. Rogina, T. Sloboda, and A. Waibel. </author> <title> "Speeding Up the Score Computation of HMM Speech Recognizers with the Bucket Voronoi Intersection Algorithm". </title> <booktitle> In Proceedings of the European Conference on Speech Communication and Technology, </booktitle> <volume> volume 2, </volume> <pages> pages 1091-1094, </pages> <year> 1995. </year>
Reference-contexts: Further computation efficiency is obtained by evaluating the state likelihood due only to the most likely mixture component of the state (see also Section 4.3.2). It has been verified empirically that the technique does not result in any loss of recognition accuracy <ref> [5, 16, 59, 79] </ref>.
Reference: [17] <author> K. Fukunaga. </author> <title> Introduction to Statistical Pattern Recognition. </title> <publisher> Academic Press, Inc., </publisher> <address> 2nd edition, </address> <year> 1990. </year> <month> 116 </month>
Reference-contexts: We devise a modified k-means Gaussian clustering scheme using the Bhattacharyya distance as the distance measure between Gaussian distributions <ref> [17] </ref>. Finally, two implementation methods for the SDCHMM are studied in detail: model conversion from CDHMMs and direct SDCHMM training. Throughout the thesis, the ATIS recognition task is used as the test-bed for evaluating the SDCHMMs. <p> Bhattacharyya distance, which is defined as D bhat = 1 ( 2 1 ) T 1 + 2 1 1 ln fi 1 + 2 fi fi j 1 jj 2 j where, i and i , i = 1; 2, are the means and covariances of the two Gaussians <ref> [17] </ref>. The Bhattacharyya distance has been used in several speech-related tasks [42, 54, 57], leading to good results.
Reference: [18] <author> S. Furui. </author> <title> "Speaker-Independent Isolated Word Recognition Using Dynamic Features of Speech Spectrum". </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> 34(1) </volume> <pages> 52-59, </pages> <month> February </month> <year> 1986. </year>
Reference-contexts: To alleviate the shortcomings, many modifications have been proposed recently which mainly differ in the extent to which they relax the assumptions: * The simplest way to incorporate temporal or contextual information without modifying the HMM formulation is to add dynamic features onto the input feature vec tor <ref> [18] </ref>. * Higher-order HMMs have been used to include more of the past state history.
Reference: [19] <author> Jean-Luc Gauvain and C.H. Lee. </author> <title> "Maximum a Posteriori Estimation for Multivariate Gaussian Mixture Observations of Markov Chains". </title> <journal> IEEE Transactions on Speech and Audio Processing, </journal> <volume> 2(2) </volume> <pages> 291-298, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: There are two common approaches for speaker adaptation: The Bayesian learning approach and transformation-based approach, best exemplified by the MAP <ref> [19] </ref> and the MLLR [51] techniques respectively. With either approach, when very little speaker-specific data is available, one is generally required to put the model parameters (usually only the Gaussian means) into equivalence classes to share the scarce resources.
Reference: [20] <author> Y. Gong. </author> <title> "Stochastic Trajectory Modeling and Sentence Searching for Continuous Speech Recognition". </title> <journal> IEEE Transactions on Speech and Audio Processing, </journal> <volume> 5(1) </volume> <pages> 33-44, </pages> <month> January </month> <year> 1997. </year>
Reference-contexts: both the state and all its previous frames more effectively [76]. * The state-dependent acoustic trajectory is modeled by an autoregressive process in autoregressive HMMs [37, 68], and in trended HMMs with a linear polynomial regression function of time [14]. 14 * Segmental HMMs such as the stochastic trajectory model <ref> [20] </ref> and stochastic segment model [62] relax both assumptions and explicitly model acoustic trajectories of variable durations as well as the duration.
Reference: [21] <author> P.S. Gopalakrishnan and L.R. Bahl. </author> <title> "Fast Match Techniques". </title> <editor> In C.H. Lee, F.K. Soong, and K.K. Paliwal, editors, </editor> <title> Automatic Speech and Speaker Recognition (Advanced Topics), </title> <booktitle> chapter 17, </booktitle> <pages> pages 413-428. </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1996. </year>
Reference-contexts: There are also techniques to speed up computation alone: for example, by simply exercising more vigorous pruning schemes, by computing state likelihoods only from a small subset of the most relevant state probability density distributions [6, 8, 41, 63, 79], or by fast-match techniques <ref> [21] </ref>. However, these techniques are usually done at the expense of recognition accuracy; in the case of computation speedup, more memory is usually required.
Reference: [22] <author> S. Greenberg. </author> <title> "Understanding Speech Understanding Towards a Unified Theory of Speech Perception". In W.A. </title> <editor> Ainsworth and S. Greenberg, editors, </editor> <booktitle> Proceedings of the ESCA Tutorial and Advanced Research Workshop on the Auditory Basis of Speech Perception, </booktitle> <pages> pages 1-8. </pages> <address> Keele University, UK, </address> <year> 1996. </year>
Reference-contexts: They are motivated mainly for efficient acoustic modeling and may not be associated with a phonetic meaning. 1 As a matter of fact, there is already renewed interest in using syllables as the modeling units <ref> [22, 27, 33] </ref>. 16 2.4 Parameter Tying The history of acoustic modeling is guided by the need to strike a balance between two conflicting goals for acoustic models: trainability and resolution.
Reference: [23] <author> C.T. Hemphill, J.J. Godfrey, and G.R. Doddington. </author> <title> "The ATIS Spoken Language Systems Pilot Corpus". </title> <booktitle> In Proceedings of the DARPA Speech and Natural Language Workshop. </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1990. </year>
Reference-contexts: An ATIS system allows users to speak naturally to inquire about air travel information stored as a relational database which is derived from the Official Airline Guide. 19 20 3.1.1 ATIS Corpora Several ATIS corpora 1 known as ATIS0 <ref> [23] </ref>, ATIS1 2 , ATIS2 [25], and ATIS3 [13] were collected over the years by several institutions: AT&T, BBN, CMU, MIT, NIST, SRI, and TI. To date, the corpora contain nearly 25,000 utterances with a vocabulary size of 1,536 words.
Reference: [24] <author> H. Hermansky. </author> <title> "Perceptual Linear Predictive (PLP) Analysis of Speech". </title> <journal> Journal of Acoustical Society of America, </journal> <volume> 87(4) </volume> <pages> 1738-1752, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: Due to the findings from psychoacoustical studies which show that humans do not perceive frequencies greater than 1kHz in a linear scale but instead in a logarithmic scale, the cepstral coefficients are more commonly expressed as mel-frequency [82] cepstral coefficients (MFCC) or perceptual linear predictive (PLP) coefficients <ref> [24] </ref> in the Bark scale [90]. 11 a 11 00 a 22 a denotes the transition probability from state i to state j) 2.2.2 Hidden Markov Model Most of the aforementioned speech characteristics can be captured by a probabilistic finite-state machine called the hidden Markov model (HMM).
Reference: [25] <author> L. Hirschman et al. </author> <title> "Multi-Site Data Collection and Evaluation in Spoken Language Understanding". </title> <booktitle> In Proceedings of ARPA Human Language Technology Workshop. </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1993. </year>
Reference-contexts: For example, let us look at two state-of-the-art recognizers which were among the top three systems in the ARPA evaluation of ATIS (Airline Travel Information System) <ref> [25] </ref> in 1994. <p> An ATIS system allows users to speak naturally to inquire about air travel information stored as a relational database which is derived from the Official Airline Guide. 19 20 3.1.1 ATIS Corpora Several ATIS corpora 1 known as ATIS0 [23], ATIS1 2 , ATIS2 <ref> [25] </ref>, and ATIS3 [13] were collected over the years by several institutions: AT&T, BBN, CMU, MIT, NIST, SRI, and TI. To date, the corpora contain nearly 25,000 utterances with a vocabulary size of 1,536 words.
Reference: [26] <author> E. Horowitz and S. Sahni. </author> <title> Fundamentals of Computer Algorithm. </title> <publisher> Computer Science Press, </publisher> <year> 1978. </year>
Reference-contexts: That is, for each test utterance, the decoded string from a recognizer is compared (by a string alignment software using dynamic programming algorithm <ref> [26] </ref>) with the known orthographic transcription of the utterance, and the number of substituted words, deleted words, as well as inserted words are counted. The WER is computed as, 1 The ATIS corpora are now maintained and distributed by the Linguistic Data Consortium. <p> Viterbi search is a dynamic programming algorithm <ref> [26] </ref> which finds the most likely sequence of states of HMMs for a given sequence of observations. To reduce the otherwise immense search space, Viterbi beam search prunes those states with log likelihoods less than that of the best path (at that moment) by a preset threshold called beam-width. <p> Step 4. Repeat Step 3 until all features appear in the solution list. Step 5. The feature tuples in the "solution list" are the K-stream definition. Derivation of Streams Practically, we apply a greedy algorithm <ref> [26] </ref> to obtain streams in which the features are most correlated, as depicted in Algorithm 3. It is simple to modify the algorithm in cases when the number of features D is not a multiple of the number of streams K.
Reference: [27] <author> Z. Hu, J. Schalkwyk, E. Barnard, and R. Cole. </author> <title> "Speech Recognition Using Syllable-Like Units". </title> <booktitle> In Proceedings of the International Conference on Spoken Language Processing, </booktitle> <volume> volume 2, </volume> <pages> pages 1117-1120, </pages> <year> 1996. </year>
Reference-contexts: They are motivated mainly for efficient acoustic modeling and may not be associated with a phonetic meaning. 1 As a matter of fact, there is already renewed interest in using syllables as the modeling units <ref> [22, 27, 33] </ref>. 16 2.4 Parameter Tying The history of acoustic modeling is guided by the need to strike a balance between two conflicting goals for acoustic models: trainability and resolution.
Reference: [28] <author> X. Huang et al. </author> <title> "The SPHINX-II Speech Recognition System: An Overview". </title> <journal> Journal of Computer Speech and Language, </journal> <volume> 7(2) </volume> <pages> 137-148, </pages> <month> April </month> <year> 1993. </year> <month> 117 </month>
Reference-contexts: The capabilities of some state-of-the-art recognizers over a wide range of tasks are summarized in Figure 1.1 <ref> [12, 28, 34, 54, 87] </ref>. In the figure, the difficulty of a recognition task (from the perspective of a recognizer) is measured by the perplexity of its language model, whereas the performance of a recognizer is gauged by its word error rate (WER). <p> Similarly CMU's Sphinx II large-vocabulary recognizer <ref> [28] </ref> requires more than 40MB of memory to represent its acoustic models which have over 10 million parameters, and obtains a WER of 5.1% on the same task using an 175MHz Alphastation at the speed of 9 times real time 4 . <p> #states, M = #mixtures per state, L = #codewords per stream, K = #streams and D = feature dimension) DISCRETE SEMI-CONTINUOUS CONTINUOUS SUBSPACE DISTRIBUTION HMM HMM DENSITY HMM CLUSTERING HMM SLK+LD SLK+L (2D) SM+SM (2D) SM+L (2D)+SMK For DHMMs and SCHMMs, the number of streams is usually 3 or 4 <ref> [28, 50] </ref>, and the number of prototypes is about 256-1024; while CDHMMs typically have 20-30 mixture 39 components per state density.
Reference: [29] <author> X. Huang and M.A. Jack. </author> <title> "Semi-continuous Hidden Markov Models for Speech Signals". </title> <journal> Journal of Computer Speech and Language, </journal> <volume> 3(3) </volume> <pages> 239-251, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: In the past, the technique of parameter tying has been applied successfully at various gran-ularities: Phones (generalized biphones/triphones [48], context-independent phones [49]), states (tied-state HMM [32, 88]), observation distributions (tied-mixture/semi-continuous HMM <ref> [4, 29, 80] </ref>), and feature parameters [85] have all been tied. For example, of the two aforementioned systems, AT&T's ATIS recognizer is a tied-state system, whereas CMU's Sphinx II employs semi-continuous HMMs. <p> Young and Woodland reported a 5-fold reduction in the number of states in their TSHMM systems [88]. 17 Tied-mixture HMM/semi-continuous HMM (SCHMM): SCHMM is a generalization of the discrete HMM in which prototypes are continuous Gaussian distributions with diagonal covariances instead of just mean vectors <ref> [4, 29, 80] </ref>. Consequently, an SCHMM enjoys fast computation of state likelihoods but reduces the quantization errors of discrete HMMs, with a negligible increase in model parameters. <p> To increase the likelihood computation efficiency, fewer streams are desirable as fewer additions are then needed to compute the full-space state likelihoods. However, fewer streams will lead to poorer performance unless more prototypes are used. 4.4 Comparison with Semi-Continuous HMM At first glance, SDCHMMs may appear similar to SCHMMs <ref> [4, 29, 80] </ref>: Both methods divide the feature space into streams, and tie subspace distributions across all states of all HMMs.
Reference: [30] <author> X.D. Huang, Y. Ariki, and M.A. Jack. </author> <title> "Fundamentals of Pattern Recognition". In Hidden Markov Models for Speech Recognition, </title> <booktitle> chapter 2, </booktitle> <pages> pages 10-51. </pages> <publisher> Edinburgh University Press, </publisher> <year> 1990. </year>
Reference-contexts: Cast as an optimization problem, the estimation is most commonly solved by the iterative Baum-Welch (BW) algorithm [3], a specific case of the Expectation-Maximization (EM) algorithm <ref> [30] </ref> with the log likelihood as the score function. That is, C (O; ) = log P (O j ) In each iteration of the EM algorithm, the current model parameters (; a; b) are reestimated to ^ ( ^ ; ^ a; ^ b) which maximizes the score function.
Reference: [31] <author> X.D. Huang, Y. Ariki, and M.A. Jack. </author> <title> Hidden Markov Models for Speech Recognition. </title> <publisher> Edinburgh University Press, </publisher> <year> 1990. </year>
Reference-contexts: Convert the CDHMMs to SDCHMMs by tying the subspace Gaussians in each stream as shown in Figure 5.1. Since the training of CDHMMs is well covered in the literature <ref> [31, 69] </ref>, we will not repeat it here. Instead, when we discuss the reestimation of SDCHMM parameters in Chapter 7, we will review the reestimation of CDHMM parameters as well.
Reference: [32] <author> M. Hwang. </author> <title> "Shared Distribution Hidden Markov Models for Speech Recognition". </title> <journal> IEEE Transactions on Speech and Audio Processing, </journal> <volume> 1(4) </volume> <pages> 414-420, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: In the past, the technique of parameter tying has been applied successfully at various gran-ularities: Phones (generalized biphones/triphones [48], context-independent phones [49]), states (tied-state HMM <ref> [32, 88] </ref>), observation distributions (tied-mixture/semi-continuous HMM [4, 29, 80]), and feature parameters [85] have all been tied. For example, of the two aforementioned systems, AT&T's ATIS recognizer is a tied-state system, whereas CMU's Sphinx II employs semi-continuous HMMs. <p> Therefore TSHMM clusters the corresponding HMM states of the same base phone <ref> [32, 88] </ref>. Young and Woodland reported a 5-fold reduction in the number of states in their TSHMM systems [88]. 17 Tied-mixture HMM/semi-continuous HMM (SCHMM): SCHMM is a generalization of the discrete HMM in which prototypes are continuous Gaussian distributions with diagonal covariances instead of just mean vectors [4, 29, 80].
Reference: [33] <author> R.J. Jones, S. Downey, and J. S. Mason. </author> <title> "Continuous Speech Recognition Using Syllables". </title> <booktitle> In Proceedings of the European Conference on Speech Communication and Technology, </booktitle> <volume> volume 3, </volume> <pages> pages 1171-1174, </pages> <year> 1997. </year>
Reference-contexts: They are motivated mainly for efficient acoustic modeling and may not be associated with a phonetic meaning. 1 As a matter of fact, there is already renewed interest in using syllables as the modeling units <ref> [22, 27, 33] </ref>. 16 2.4 Parameter Tying The history of acoustic modeling is guided by the need to strike a balance between two conflicting goals for acoustic models: trainability and resolution.
Reference: [34] <author> B.H. Juang, W. Chou, and C.H. Lee. </author> <title> "Minimum Classification Error Rate Methods for Speech Recognition". </title> <journal> IEEE Transactions on Speech and Audio Processing, </journal> <volume> 5(3) </volume> <pages> 257-265, </pages> <month> May </month> <year> 1997. </year>
Reference-contexts: The capabilities of some state-of-the-art recognizers over a wide range of tasks are summarized in Figure 1.1 <ref> [12, 28, 34, 54, 87] </ref>. In the figure, the difficulty of a recognition task (from the perspective of a recognizer) is measured by the perplexity of its language model, whereas the performance of a recognizer is gauged by its word error rate (WER).
Reference: [35] <author> B.H. Juang, D.Y. Gray, and A.H. Gray, Jr. </author> <title> "Distortion Performance of Vector Quantization for LPC Voice Coding". </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> 30(2) </volume> <pages> 307-309, </pages> <month> April </month> <year> 1982. </year>
Reference-contexts: In speech coding, it is known that (full) vector quantization (VQ) results in smaller quantization distortion than scalar quantization at any given bit rate <ref> [35] </ref>. However, to attain the required high quality in practical telecommunication, full VQ suffers from training, memory, and computation problems much like those of our current complex ASR systems.
Reference: [36] <author> B.H. Juang, S.E. Levinson, and M.M. Sondhi. </author> <title> "Maximum Likelihood Estimation for Multivariate Mixture Observations of Markov Chains". </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 32(2) </volume> <pages> 307-309, </pages> <month> March </month> <year> 1986. </year>
Reference-contexts: Case II: Mixture Gaussian Output Distribution That is, b i (o t ) = P M P M Since an HMM state with a mixture density is equivalent to a multi-state HMM with single-mixture densities <ref> [36] </ref>, the reestimates of b are similar to those of Case I except that the quantity fl t (i) is modified as fl t (i; m) which is the probability of being in state i and the m-th mixture component at time t, given the model and the observation sequence O.
Reference: [37] <author> B.H. Juang and L.R. Rabiner. </author> <title> "Mixture Autoregressive Hidden Markov Models for Speech Signals". </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> 33(6) </volume> <pages> 1404-1413, </pages> <month> December </month> <year> 1985. </year>
Reference-contexts: fea ture frames [11]. * Hybrid models of Recurrent Neural Networks and HMMs explicitly estimate the posterior phone probability of the current frame conditional on both the state and all its previous frames more effectively [76]. * The state-dependent acoustic trajectory is modeled by an autoregressive process in autoregressive HMMs <ref> [37, 68] </ref>, and in trended HMMs with a linear polynomial regression function of time [14]. 14 * Segmental HMMs such as the stochastic trajectory model [20] and stochastic segment model [62] relax both assumptions and explicitly model acoustic trajectories of variable durations as well as the duration.
Reference: [38] <author> B.H. Juang and L.R. Rabiner. </author> <title> "A Segmental K-means Algorithm for Estimating Parameters of Hidden Markov Models". </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> 38(9) </volume> <pages> 1639-1641, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: each transition arc is simply assigned an equal probability of 1/fan-out, where fan-out is the number of arcs coming out of the source state [10, 89]. * No explicit duration modeling is performed as it is not found necessary. * Our HMMs are trained using the simpler segmental k-means algorithm <ref> [38] </ref> which has been found to work as well as the conventional Baum-Welch algorithm [3] ( [45, 71, 89]). 15 2.3 Speech Unit to Model Although it may be most natural to model each word individually, the approach is impractical in most recognition tasks, except those of small vocabularies and isolated <p> Each phone model is a 3-state left-to-right continuous-density hidden Markov model (CDHMM), as shown in covariances. The CDHMM acoustic models are trained via the segmental k-means training algorithm <ref> [38] </ref> as described in Algorithm 1. The average frame likelihood of the training utterances is computed to test for model convergence. <p> To do that, we first re-train context-independent (CI) CDHMMs with about 4,000 ATIS training utterances using the segmental k-means training algorithm <ref> [38] </ref>. The CI CDHMMs have the same HMM configuration as the baseline system except that there are only four Gaussian mixture components per state. Twenty-stream CI SDCHMMs are then derived from the CDHMMs by the model conversion scheme as explained in Chapter 5 requiring 64 prototypes per stream.
Reference: [39] <author> S.K. Kachigan. </author> <title> Multivariate Statistical Analysis (A Conceptual Introduction). </title> <publisher> Radius Press, </publisher> <year> 1991. </year>
Reference-contexts: One way to determine the combination weights of the derived variable is to compute what are called the beta weights so as to maximize the resulting binary correlation <ref> [39] </ref>. However, in our context, a multiple correlation measure that emphasizes mutual correlations among all variables at the same time is more desirable.
Reference: [40] <author> N.S. Kim and C.K. </author> <title> Un. "Frame-Correlated Hidden Markov Model Based on Extended Logarithmic Pool". </title> <journal> IEEE Transactions on Speech and Audio Processing, </journal> <volume> 5(2) </volume> <pages> 149-160, </pages> <month> March </month> <year> 1997. </year> <month> 118 </month>
Reference-contexts: Because of the huge increase in the number of estimation parameters, approximation is used to express the joint conditional probabilities in terms of individual conditional probabilities in bigram-constrained HMMs [83] (using observation bigrams), or using an extended logarithmic pool <ref> [40] </ref> . * Hybrid models of Artificial Neural Networks and HMMs incorporate contextual information simply by using the current feature frame together with neighboring fea ture frames [11]. * Hybrid models of Recurrent Neural Networks and HMMs explicitly estimate the posterior phone probability of the current frame conditional on both the
Reference: [41] <author> Y. Komori, M. Yamada, H. Yamamoto, and Y. Ohora. </author> <title> "An Efficient Output Probability Computation for Continuous HMM Using Rough and Detail Models". </title> <booktitle> In Proceedings of the European Conference on Speech Communication and Technology, </booktitle> <volume> volume 2, </volume> <pages> pages 1087-1090, </pages> <year> 1995. </year>
Reference-contexts: There are also techniques to speed up computation alone: for example, by simply exercising more vigorous pruning schemes, by computing state likelihoods only from a small subset of the most relevant state probability density distributions <ref> [6, 8, 41, 63, 79] </ref>, or by fast-match techniques [21]. However, these techniques are usually done at the expense of recognition accuracy; in the case of computation speedup, more memory is usually required. <p> They are most commonly used and are found to be about as accurate as the theoretically more sound variants. Their simplicity and explicit parameterization also make them amenable to a wide range of techniques that perform fast state-likelihood computation <ref> [6, 8, 41, 63, 79] </ref> and reduce parameters through parameter tying (see Section 2.4). In particular, the continuous density HMM (CDHMM) is used as generally it is found to be highly accurate.
Reference: [42] <author> T. Kosaka and S. Sagayama. </author> <title> "Tree-structured Speaker Clustering for Fast Speaker Adaptation". </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 245-248, </pages> <year> 1994. </year>
Reference-contexts: The Bhattacharyya distance has been used in several speech-related tasks <ref> [42, 54, 57] </ref>, leading to good results.
Reference: [43] <author> P. Ladefoged. </author> <title> A Course in Phonetics. </title> <publisher> Harcourt Brace Jovanovich College Publishers, 3rd edition, </publisher> <year> 1993. </year>
Reference-contexts: number of matches of 4.8 and the figures shown in Table B.1, Figure 6.1, and Figure 6.2, we have the following observations: * The extent of sharing of subspace Gaussians splits along broad phonetic categories (vowels and consonants; and within consonants, along sub-categories of fricatives, 73 plosives, nasals and approximants <ref> [43] </ref>). That is, there is more sharing of subspace Gaussians between two vowels or two consonants than between a vowel and a consonant; and, within consonants, there is more sharing between two fricatives, two plosives, etc.
Reference: [44] <author> K.W. Law and C.F. Chan. </author> <title> "Split-Dimension Vector Quantization of Parcor Coefficients for Low Bit Rate Speech Coding". </title> <journal> IEEE Transactions on Speech and Audio Processing, </journal> <volume> 2(3) </volume> <pages> 443-446, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: SDCHMM combines the accuracy of CDHMM with the compactness of feature-parameter-tying HMM. In this aspect, it is interesting to compare this work with a similar approach called "split vector quantization" <ref> [44, 65] </ref> that has been successfully applied to high-quality, low-bit rate speech coding for years. In speech coding, it is known that (full) vector quantization (VQ) results in smaller quantization distortion than scalar quantization at any given bit rate [35].
Reference: [45] <author> C.H. Lee. </author> <title> "Acoustic Modeling of Subword Units for Speech Recognition". </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <volume> volume 2, </volume> <pages> pages 721-724, </pages> <year> 1990. </year>
Reference-contexts: of arcs coming out of the source state [10, 89]. * No explicit duration modeling is performed as it is not found necessary. * Our HMMs are trained using the simpler segmental k-means algorithm [38] which has been found to work as well as the conventional Baum-Welch algorithm [3] ( <ref> [45, 71, 89] </ref>). 15 2.3 Speech Unit to Model Although it may be most natural to model each word individually, the approach is impractical in most recognition tasks, except those of small vocabularies and isolated words. In most cases, there are not sufficient training data for each word.
Reference: [46] <author> C.H. Lee, C.H. Lin, and B.H. Juang. </author> <title> "A Study on Speaker Adaptation of the Parameters of Continuous Density Hidden Markov Models". </title> <journal> IEEE Transactions on Signal Processing, </journal> <volume> 39(4) </volume> <pages> 806-814, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: We further simplify our training procedures by fixing all state-transition probabilities to 0.5 as many researchers have found that in practice, an estimated state-transition matrix makes no difference in recognition performance <ref> [46] </ref>. 7.3.4 Experiment I: Effectiveness of Direct SDCHMM Training We first check, for the same amount of training data, whether SDCHMMs trained by the direct SDCHMM training algorithm achieve the same recognition performance as that of the SDCHMMs converted from CDHMMs.
Reference: [47] <author> K.F. Lee. </author> <title> "Context-Dependent Phonetic Hidden Markov Models for Speaker-Independent Continuous Speech Recognition". </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> 38(4) </volume> <pages> 599-609, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: Monophone HMM: A monophone ties all allophones of a base phone. The small number (less than 100) of monophones results in CI systems that are simple to implement and fast to run. However, the accuracy of CI systems is usually modest. Generalized context-dependent phone HMM: Generalized triphone HMMs <ref> [47] </ref> are most commonly used. The large number of triphones (about 125,000 in English, assuming all possible combinations of three phones) makes it almost impossible to model each of them equally reliably.
Reference: [48] <author> K.F. Lee, S. Hayamizu, H.W. Hon, C. Huang, J. Swartz, and R. Weide. </author> <title> "Allophone Clustering for Continuous Speech Recognition". </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <volume> volume 2, </volume> <pages> pages 749-752, </pages> <year> 1990. </year>
Reference-contexts: With the (limited) amount of training data on hand, parameter tying allows more complex acoustic models to be estimated reliably while the number of model parameters will not grow unchecked. In the past, the technique of parameter tying has been applied successfully at various gran-ularities: Phones (generalized biphones/triphones <ref> [48] </ref>, context-independent phones [49]), states (tied-state HMM [32, 88]), observation distributions (tied-mixture/semi-continuous HMM [4, 29, 80]), and feature parameters [85] have all been tied. For example, of the two aforementioned systems, AT&T's ATIS recognizer is a tied-state system, whereas CMU's Sphinx II employs semi-continuous HMMs. <p> Fortunately many contexts of the same base phone are similarly realized and may be clustered to much fewer generalized triphones. Generalized triphones derived using a phonetic decision tree have the additional benefit of capturing "unseen" triphones <ref> [48] </ref>. Tied-state HMM (TSHMM): Since co-articulatory effects are more prominent at the onset and ending of a phone than at its center, they are better categorized at local HMM states than over the whole HMM phone as in generalized triphones.
Reference: [49] <author> K.F. Lee and H.W. Hon. </author> <title> "Large-Vocabulary Speaker-Independent Continuous Speech Recognition Using HMM". </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages 123-126, </pages> <year> 1988. </year>
Reference-contexts: In the past, the technique of parameter tying has been applied successfully at various gran-ularities: Phones (generalized biphones/triphones [48], context-independent phones <ref> [49] </ref>), states (tied-state HMM [32, 88]), observation distributions (tied-mixture/semi-continuous HMM [4, 29, 80]), and feature parameters [85] have all been tied. For example, of the two aforementioned systems, AT&T's ATIS recognizer is a tied-state system, whereas CMU's Sphinx II employs semi-continuous HMMs.
Reference: [50] <author> K.F. Lee and H.W. Hon. </author> <title> "Speaker-Independent Phone Recognition Using Hidden Markov Models". </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> 37(11) </volume> <pages> 1641-1648, </pages> <month> November </month> <year> 1989. </year> <month> 119 </month>
Reference-contexts: #states, M = #mixtures per state, L = #codewords per stream, K = #streams and D = feature dimension) DISCRETE SEMI-CONTINUOUS CONTINUOUS SUBSPACE DISTRIBUTION HMM HMM DENSITY HMM CLUSTERING HMM SLK+LD SLK+L (2D) SM+SM (2D) SM+L (2D)+SMK For DHMMs and SCHMMs, the number of streams is usually 3 or 4 <ref> [28, 50] </ref>, and the number of prototypes is about 256-1024; while CDHMMs typically have 20-30 mixture 39 components per state density.
Reference: [51] <author> C.J. </author> <title> Leggetter and P.C. Woodland. "Maximum Likelihood Linear Regression for Speaker Adaptation of Continuous Density Hidden Markov Models". </title> <journal> Journal of Computer Speech and Language, </journal> <volume> 9(2) </volume> <pages> 171-185, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: There are two common approaches for speaker adaptation: The Bayesian learning approach and transformation-based approach, best exemplified by the MAP [19] and the MLLR <ref> [51] </ref> techniques respectively. With either approach, when very little speaker-specific data is available, one is generally required to put the model parameters (usually only the Gaussian means) into equivalence classes to share the scarce resources.
Reference: [52] <author> E. Levin and R. Pieraccini. "CHRONUS, </author> <title> The Next Generation". </title> <booktitle> In Proceedings of ARPA Spoken Language Systems Technology Workshop, </booktitle> <pages> pages 269-271. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1995. </year>
Reference-contexts: Since this thesis deals only with acoustic modeling, efficacy of which is best measured by speech recognition alone (i.e. the SPREC test), we restrict ourselves here to describe only its speech-recognition component [10]. Refer <ref> [52] </ref> for its NL understanding component.
Reference: [53] <author> A. Ljolje. </author> <title> "The Importance of Cepstral Parameter Correlations in Speech Recognition". </title> <journal> Journal of Computer Speech and Language, </journal> <volume> 8(3) </volume> <pages> 223-232, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: One plausible candidate is a Gaussian distribution with block-diagonal covariance. In the past, Gaussian distributions with diagonal covariance were the most popular choice because of their trainability and computational efficiency. However, it is claimed in <ref> [53] </ref> that better acoustic models are obtained with explicit modeling of correlations among cepstral parameters. Though Gaussian distributions with full covariance are able to model such correlations, they are too costly for likelihood computations, for storage, and for training.
Reference: [54] <author> P.C. </author> <title> Loizou and A.S. Spanias. "High-Performance Alphabet Recognition". </title> <journal> IEEE Transactions on Speech and Audio Processing, </journal> <volume> 4(6) </volume> <pages> 430-445, </pages> <month> November </month> <year> 1996. </year>
Reference-contexts: The capabilities of some state-of-the-art recognizers over a wide range of tasks are summarized in Figure 1.1 <ref> [12, 28, 34, 54, 87] </ref>. In the figure, the difficulty of a recognition task (from the perspective of a recognizer) is measured by the perplexity of its language model, whereas the performance of a recognizer is gauged by its word error rate (WER). <p> The Bhattacharyya distance has been used in several speech-related tasks <ref> [42, 54, 57] </ref>, leading to good results.
Reference: [55] <author> B. Lowerre and R. Reddy. </author> <title> "The Harpy Speech Understanding System". </title> <booktitle> In Trends in Speech Recognition, </booktitle> <pages> pages 340-360. </pages> <publisher> Prentice Hall, </publisher> <year> 1980. </year>
Reference-contexts: b (a) Bigram language model b c P (b) P (b|c) P (a|b) h h b e (b) VNSA implementation bigram language model 28 The values of the 's are estimated by minimizing the perplexity of the language model using a separate held-out dataset [75]. 3.2.6 Decoding Viterbi beam search <ref> [55, 56, 60] </ref> is used. Viterbi search is a dynamic programming algorithm [26] which finds the most likely sequence of states of HMMs for a given sequence of observations.
Reference: [56] <author> B.T. Lowerre. </author> <title> Dynamic Speaker Adaptation in the Harpy Speech Recognition System. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Carnegie Mellon University, </institution> <month> April </month> <year> 1976. </year>
Reference-contexts: b (a) Bigram language model b c P (b) P (b|c) P (a|b) h h b e (b) VNSA implementation bigram language model 28 The values of the 's are estimated by minimizing the perplexity of the language model using a separate held-out dataset [75]. 3.2.6 Decoding Viterbi beam search <ref> [55, 56, 60] </ref> is used. Viterbi search is a dynamic programming algorithm [26] which finds the most likely sequence of states of HMMs for a given sequence of observations.
Reference: [57] <author> B. Mak and E. Barnard. </author> <title> "Phone Clustering Using the Bhattacharyya Distance". </title> <booktitle> In Proceedings of the International Conference on Spoken Language Processing, </booktitle> <volume> volume 4, </volume> <pages> pages 2005-2008, </pages> <year> 1996. </year>
Reference-contexts: The Bhattacharyya distance has been used in several speech-related tasks <ref> [42, 54, 57] </ref>, leading to good results.
Reference: [58] <author> J.F. Mari, J.P. Haton, and A. Kriouile. </author> <title> "Automatic Word Recognition Based on Second-Order Hidden Markov Models". </title> <journal> IEEE Transactions on Speech and Audio Processing, </journal> <volume> 5(1) </volume> <pages> 22-25, </pages> <month> January </month> <year> 1997. </year>
Reference-contexts: Because of the big increase in computational complexity, only second-order HMMs have been tried, but with limited success <ref> [58] </ref>. * State-dependent correlations between successive frames are explicitly modeled by conditioning the observation probability jointly on both the current state and the preceding observation [64].
Reference: [59] <author> H. Ney, R. Haeb-Umbach, B. Tran, and M. Oerder. </author> <title> "Improvements in Beam Search for 10000-Word Continuous Speech Recognition". </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 9-12, </pages> <year> 1992. </year>
Reference-contexts: When all the words (or phrases) are independently represented by a sequence of phonemes, the lexical structure is said to be linear. Recently, a more compact structure, the lexical (phonetic) tree <ref> [1, 59, 73] </ref> has been found to greatly improve search speed in large vocabulary speech recognition. In the lexical tree, words in the lexicon are arranged 23 in a tree structure so that those with the same prefix (in their phonetic pronunciations) share the same part of the tree. <p> Further computation efficiency is obtained by evaluating the state likelihood due only to the most likely mixture component of the state (see also Section 4.3.2). It has been verified empirically that the technique does not result in any loss of recognition accuracy <ref> [5, 16, 59, 79] </ref>.
Reference: [60] <author> H. Ney, D. Mergel, A. Noll, and A. Paeseler. </author> <title> "A Data-Driven Organization of the Dynamic Programming Beam Search for Continuous Speech Recognition". </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 833-836, </pages> <year> 1987. </year>
Reference-contexts: b (a) Bigram language model b c P (b) P (b|c) P (a|b) h h b e (b) VNSA implementation bigram language model 28 The values of the 's are estimated by minimizing the perplexity of the language model using a separate held-out dataset [75]. 3.2.6 Decoding Viterbi beam search <ref> [55, 56, 60] </ref> is used. Viterbi search is a dynamic programming algorithm [26] which finds the most likely sequence of states of HMMs for a given sequence of observations.
Reference: [61] <editor> L. Nguyen et al. </editor> <booktitle> "The 1994 BBN/BYBLOS Speech Recognition System". In Proceedings of ARPA Spoken Language Systems Technology Workshop, </booktitle> <pages> pages 77-81. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1995. </year> <month> 120 </month>
Reference-contexts: Perhaps the best example is the hidden Markov network of [77] which ties allophones, states, distributions and feature parameters. Alternatively, several tying schemes may also be combined or merged. For example, genones [15] or state-clustered tied-mixture HMMs <ref> [61] </ref> divide all states into classes and only tie the mixtures 2 Speech recognition systems are only concerned with the storage space for the distributions.
Reference: [62] <author> M. Ostendorf and S. Roukos. </author> <title> "A Stochastic Segment Model for Phoneme-Based Continuous Speech Recognition". </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> 37(12) </volume> <pages> 1857-1869, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: its previous frames more effectively [76]. * The state-dependent acoustic trajectory is modeled by an autoregressive process in autoregressive HMMs [37, 68], and in trended HMMs with a linear polynomial regression function of time [14]. 14 * Segmental HMMs such as the stochastic trajectory model [20] and stochastic segment model <ref> [62] </ref> relax both assumptions and explicitly model acoustic trajectories of variable durations as well as the duration.
Reference: [63] <author> M. Padmanabhan, D. Nahamoo L.R. Bahl, and P. de Souza. </author> <title> "Decision-Tree Based Quantization of the Feature Space of a Speech Recognizer". </title> <booktitle> In Proceedings of the Eu-ropean Conference on Speech Communication and Technology, </booktitle> <pages> pages 147-150, </pages> <year> 1997. </year>
Reference-contexts: There are also techniques to speed up computation alone: for example, by simply exercising more vigorous pruning schemes, by computing state likelihoods only from a small subset of the most relevant state probability density distributions <ref> [6, 8, 41, 63, 79] </ref>, or by fast-match techniques [21]. However, these techniques are usually done at the expense of recognition accuracy; in the case of computation speedup, more memory is usually required. <p> They are most commonly used and are found to be about as accurate as the theoretically more sound variants. Their simplicity and explicit parameterization also make them amenable to a wide range of techniques that perform fast state-likelihood computation <ref> [6, 8, 41, 63, 79] </ref> and reduce parameters through parameter tying (see Section 2.4). In particular, the continuous density HMM (CDHMM) is used as generally it is found to be highly accurate.
Reference: [64] <author> K.K. Paliwal. </author> <title> "Use of Temporal Correlation Between Successive Frames in a Hidden Markov Model Based Speech Recognizer". </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <volume> volume 2, </volume> <pages> pages 215-218, </pages> <year> 1993. </year>
Reference-contexts: Because of the big increase in computational complexity, only second-order HMMs have been tried, but with limited success [58]. * State-dependent correlations between successive frames are explicitly modeled by conditioning the observation probability jointly on both the current state and the preceding observation <ref> [64] </ref>.
Reference: [65] <author> K.K. Paliwal and B.S. Atal. </author> <title> "Efficient Vector Quantization of LPC Parameters". </title> <journal> IEEE Transactions on Speech and Audio Processing, </journal> <volume> 1(1) </volume> <pages> 3-14, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: SDCHMM combines the accuracy of CDHMM with the compactness of feature-parameter-tying HMM. In this aspect, it is interesting to compare this work with a similar approach called "split vector quantization" <ref> [44, 65] </ref> that has been successfully applied to high-quality, low-bit rate speech coding for years. In speech coding, it is known that (full) vector quantization (VQ) results in smaller quantization distortion than scalar quantization at any given bit rate [35].
Reference: [66] <author> D.S. Pallett et al. </author> <title> "1994 Benchmark Tests for the ARPA Spoken Language Program". </title> <booktitle> In Proceedings of ARPA Human Language Technology Workshop, </booktitle> <pages> pages 5-36. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1995. </year>
Reference-contexts: seven p m to Kansas City.'' ``What are the prices for the flights on Wednesday evening.'' ``Please show which flight serves dinner.'' ``Show me all the morning flights from Philadelphia to Fort Worth.'' ``What does fare code M mean?'' 3.1.2 ATIS Evaluation Tests There are three types of official evaluations <ref> [66] </ref>: SPREC: SPontaneous speech RECognition evaluation tests only the capability of the speech recognizer component in terms of word error rate (WER). <p> SLS: Spoken Language underStanding evaluation tests the performance of the whole system (both the speech recognizer and the natural language understanding modules). 3.2 The Baseline Recognizer In the 1994 ARPA-ATIS evaluation <ref> [66] </ref>, AT&T's ATIS System had the best NL performance, answering 94.1% of the queries correctly, and was among the three second best systems in the SPREC test with a WER of 3.5%.
Reference: [67] <author> D.B. Paul and J.M. Baker. </author> <title> "The Design for the Wall Street Journal-based CSR Corpus". </title> <booktitle> In Proceedings of the International Conference on Spoken Language Processing, </booktitle> <volume> volume 2, </volume> <pages> pages 899-902, </pages> <year> 1992. </year>
Reference-contexts: The CI system is run on a low-end SGI machine comparable to a 166MHz Pentium 6 The WSJ corpus consists primarily of read speech with texts drawn from a machine-readable corpus of Wall Street Journal news text <ref> [67] </ref>. 29 PC. The CD system is run on a high-end SGI machine which is about three times the speed of the low-end machine and is comparable to a 200MHz Pentium Pro PC. The machines are chosen to reflect a realistic performance on currently available desktop PCs.
Reference: [68] <author> A.B. Poritz. </author> <title> "Linear Predictive Hidden Markov Models and the Speech Signal". </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <volume> volume 2, </volume> <pages> pages 1291-1294, </pages> <year> 1982. </year>
Reference-contexts: fea ture frames [11]. * Hybrid models of Recurrent Neural Networks and HMMs explicitly estimate the posterior phone probability of the current frame conditional on both the state and all its previous frames more effectively [76]. * The state-dependent acoustic trajectory is modeled by an autoregressive process in autoregressive HMMs <ref> [37, 68] </ref>, and in trended HMMs with a linear polynomial regression function of time [14]. 14 * Segmental HMMs such as the stochastic trajectory model [20] and stochastic segment model [62] relax both assumptions and explicitly model acoustic trajectories of variable durations as well as the duration.
Reference: [69] <author> L. Rabiner and B.H. Juang. </author> <title> Fundamentals of Speech Recognition. </title> <publisher> Prentice Hall, </publisher> <year> 1993. </year>
Reference-contexts: The perplexity of a language model is the average number of words that may follow another word <ref> [69] </ref>; and WER is the percentage of words which are wrongly recognized 1 . <p> Convert the CDHMMs to SDCHMMs by tying the subspace Gaussians in each stream as shown in Figure 5.1. Since the training of CDHMMs is well covered in the literature <ref> [31, 69] </ref>, we will not repeat it here. Instead, when we discuss the reestimation of SDCHMM parameters in Chapter 7, we will review the reestimation of CDHMM parameters as well. <p> The likelihood functions fl () and ~() can be efficiently computed by the forward-backward algorithm <ref> [69] </ref>. The reestimation formula of b depends on the functional form of the state observation pdf. Here, we will consider only the two cases when the state output distribution is either a single Gaussian distribution or a mixture Gaussian density.
Reference: [70] <author> L.R. Rabiner and B.H. Juang. </author> <title> "An Introduction to Hidden Markov Models". </title> <journal> IEEE ASSP Magazine, </journal> <volume> 3(1) </volume> <pages> 4-16, </pages> <month> January </month> <year> 1986. </year>
Reference-contexts: A detailed description of HMMs is outside the scope of this thesis and interested readers are referred to <ref> [70] </ref>. 2.2.3 Our Choice of HMM for Acoustic Modeling For simplicity and computational tractability, we employ the first-order HMMs with cep-stral coefficients augmented by their first- and second-order time derivatives as the speech features.
Reference: [71] <author> L.R. Rabiner, B.H. Juang, S.E. Levinson, and M.M. Sondhi. </author> <title> "Recognition of Isolated Digits Using Hidden Markov Models with Continuous Mixture Densities". </title> <journal> AT&T Technical Journal, </journal> <volume> 64(6) </volume> <pages> 1211-1233, </pages> <month> July-August </month> <year> 1985. </year>
Reference-contexts: of arcs coming out of the source state [10, 89]. * No explicit duration modeling is performed as it is not found necessary. * Our HMMs are trained using the simpler segmental k-means algorithm [38] which has been found to work as well as the conventional Baum-Welch algorithm [3] ( <ref> [45, 71, 89] </ref>). 15 2.3 Speech Unit to Model Although it may be most natural to model each word individually, the approach is impractical in most recognition tasks, except those of small vocabularies and isolated words. In most cases, there are not sufficient training data for each word.
Reference: [72] <author> L.R. Rabiner and R.W. Schafer. </author> <title> Digital Processing of Speech Signals. </title> <publisher> Prentice Hall, </publisher> <year> 1978. </year>
Reference-contexts: In practice, short-term spectral analysis is usually applied over a window of 20-30ms of speech (SC-III) at about every 10ms (SC-I). The spectrum (envelope) is then encoded succinctly by a vector of, say, 12 cepstral coefficients <ref> [72] </ref> (SC-IV).
Reference: [73] <author> M.K. Ravishankar. </author> <title> "Efficient Algorithms for Speech Recognition". </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <year> 1996. </year> <month> 121 </month>
Reference-contexts: A significant challenge is to adjust laboratory recognizers so that they may be deployed on more affordable machines of lower processing power and smaller memory size without losing accuracy. Techniques exist to reduce memory requirement alone, for example, by using simpler but less accurate models, or through data compression <ref> [73] </ref>. There are also techniques to speed up computation alone: for example, by simply exercising more vigorous pruning schemes, by computing state likelihoods only from a small subset of the most relevant state probability density distributions [6, 8, 41, 63, 79], or by fast-match techniques [21]. <p> 1.2 Proposed Solution: It Is Time to Share More! The most common approach to reducing the number of parameters in acoustic models is parameter tying: Similar structures are discovered among the acoustic models, 4 This is the recognition speed before the recent implementation of efficient search algorithms as described in <ref> [73] </ref>. The efficient search later increases the speed to 1.6 times real time. 5 and they are then tied together to share the same value. <p> When all the words (or phrases) are independently represented by a sequence of phonemes, the lexical structure is said to be linear. Recently, a more compact structure, the lexical (phonetic) tree <ref> [1, 59, 73] </ref> has been found to greatly improve search speed in large vocabulary speech recognition. In the lexical tree, words in the lexicon are arranged 23 in a tree structure so that those with the same prefix (in their phonetic pronunciations) share the same part of the tree.
Reference: [74] <author> G. Riccardi, E. Bocchieri, and R. Pieraccini. </author> <title> "Non-deterministic Stochastic Language Models for Speech Recognition". </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 237-240, </pages> <year> 1995. </year>
Reference-contexts: For instance, the string "cba" can be decoded as "cba", "c*ba", "cb*a", or "*c*b*a", etc. The string with the highest probability is chosen. 5 In ARPA's 1994 ATIS evaluation, AT&T's system employed trigrams of word sequences realized by a third-order VNSA <ref> [74] </ref>. 27 a P (b|c) b (a) Bigram language model b c P (b) P (b|c) P (a|b) h h b e (b) VNSA implementation bigram language model 28 The values of the 's are estimated by minimizing the perplexity of the language model using a separate held-out dataset [75]. 3.2.6
Reference: [75] <author> G. Riccardi, R. Pieraccini, and E. Bocchieri. </author> <title> "Stochastic Automata for Language Modeling". </title> <journal> Journal of Computer Speech and Language, </journal> <volume> 10(4) </volume> <pages> 265-293, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: The AT&T ATIS recognition system manually defines 13 word classes and about 100 compound words or verbal forms. Bigrams of these word sequences are modeled 5 using a second-order Variable N-gram Stochastic Automaton (VNSA) <ref> [75] </ref>. Modeling word classes instead of their individual words has an additional benefit of enhanced robustness due to more training examples. An example of a simple bigram language model of three tokens "a", "b", and "c", and its VNSA implementation are shown in Figure 3.1. <p> VNSA [74]. 27 a P (b|c) b (a) Bigram language model b c P (b) P (b|c) P (a|b) h h b e (b) VNSA implementation bigram language model 28 The values of the 's are estimated by minimizing the perplexity of the language model using a separate held-out dataset <ref> [75] </ref>. 3.2.6 Decoding Viterbi beam search [55, 56, 60] is used. Viterbi search is a dynamic programming algorithm [26] which finds the most likely sequence of states of HMMs for a given sequence of observations.
Reference: [76] <author> T. Robinson, M. Hochberg, and S. Renals. </author> <title> "The Use of Recurrent Neural Networks in Continuous Speech Recognition". </title> <editor> In C.H. Lee, F.K. Soong, and K.K. Paliwal, editors, </editor> <title> Automatic Speech and Speaker Recognition (Advanced Topics), </title> <booktitle> chapter 10, </booktitle> <pages> pages 233-258. </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1996. </year>
Reference-contexts: This renders SDCHMM very attractive in practical implementation 5 Since most state-of-the-art speech recognition systems are HMM-based, we thus only consider our thesis in such context. Recently artificial neural networks (ANN) have been applied to ASR with some success <ref> [2, 11, 76] </ref>, but HMMs remain the dominant technology. 6 In this thesis, the two terms, "subspace" and "stream" are used interchangeably to mean a feature space of dimension smaller than that of the full feature space. "Subspace" is clearer mathematically, but "stream" is more common in the speech recognition community. <p> and HMMs incorporate contextual information simply by using the current feature frame together with neighboring fea ture frames [11]. * Hybrid models of Recurrent Neural Networks and HMMs explicitly estimate the posterior phone probability of the current frame conditional on both the state and all its previous frames more effectively <ref> [76] </ref>. * The state-dependent acoustic trajectory is modeled by an autoregressive process in autoregressive HMMs [37, 68], and in trended HMMs with a linear polynomial regression function of time [14]. 14 * Segmental HMMs such as the stochastic trajectory model [20] and stochastic segment model [62] relax both assumptions and explicitly
Reference: [77] <author> S. Sagayama. </author> <title> "Hidden Markov Network for Precise and Robust Acoustic Modeling". </title> <editor> In C.H. Lee, F.K. Soong, and K.K. Paliwal, editors, </editor> <title> Automatic Speech and Speaker Recognition (Advanced Topics), </title> <booktitle> chapter 7, </booktitle> <pages> pages 159-184. </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1996. </year>
Reference-contexts: An object in a tying level may be shared by several objects in the previous level (to its left). Not only are the tying structures more compact to store, they also avoid evaluating the same object twice during recognition. Perhaps the best example is the hidden Markov network of <ref> [77] </ref> which ties allophones, states, distributions and feature parameters. Alternatively, several tying schemes may also be combined or merged.
Reference: [78] <author> S. Sagayama and S. Takahashi. </author> <title> "On the Use of Scalar Quantization for Fast HMM Computation". </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 213-216, </pages> <year> 1995. </year>
Reference-contexts: When combined with SQ of input features, likelihoods due to all shared scalar Gaussian prototypes can be pre-computed and stored in a look-up table. Subsequent (full-space) state observation likelihoods can then be computed more efficiently without any divisions or multiplications <ref> [78] </ref>. Several tying schemes may be cascaded together in a HMM system at the same time to achieve the best balance between model complexity and model-estimation robustness as exemplified in Figure 2.3.
Reference: [79] <author> F. Seide. </author> <title> "Fast Likelihood Computation for Continuous-Mixture Densities Using a Tree-Based Nearest Neighbor Search". </title> <booktitle> In Proceedings of the European Conference on Speech Communication and Technology, </booktitle> <volume> volume 2, </volume> <pages> pages 1079-1082, </pages> <year> 1995. </year>
Reference-contexts: There are also techniques to speed up computation alone: for example, by simply exercising more vigorous pruning schemes, by computing state likelihoods only from a small subset of the most relevant state probability density distributions <ref> [6, 8, 41, 63, 79] </ref>, or by fast-match techniques [21]. However, these techniques are usually done at the expense of recognition accuracy; in the case of computation speedup, more memory is usually required. <p> They are most commonly used and are found to be about as accurate as the theoretically more sound variants. Their simplicity and explicit parameterization also make them amenable to a wide range of techniques that perform fast state-likelihood computation <ref> [6, 8, 41, 63, 79] </ref> and reduce parameters through parameter tying (see Section 2.4). In particular, the continuous density HMM (CDHMM) is used as generally it is found to be highly accurate. <p> Further computation efficiency is obtained by evaluating the state likelihood due only to the most likely mixture component of the state (see also Section 4.3.2). It has been verified empirically that the technique does not result in any loss of recognition accuracy <ref> [5, 16, 59, 79] </ref>.
Reference: [80] <author> E. Singer and R.P. Lippmann. </author> <title> "A Speech Recognizer Using Radial Basis Function Neural Networks in an HMM Framework". </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 629-632, </pages> <year> 1992. </year>
Reference-contexts: In the past, the technique of parameter tying has been applied successfully at various gran-ularities: Phones (generalized biphones/triphones [48], context-independent phones [49]), states (tied-state HMM [32, 88]), observation distributions (tied-mixture/semi-continuous HMM <ref> [4, 29, 80] </ref>), and feature parameters [85] have all been tied. For example, of the two aforementioned systems, AT&T's ATIS recognizer is a tied-state system, whereas CMU's Sphinx II employs semi-continuous HMMs. <p> Young and Woodland reported a 5-fold reduction in the number of states in their TSHMM systems [88]. 17 Tied-mixture HMM/semi-continuous HMM (SCHMM): SCHMM is a generalization of the discrete HMM in which prototypes are continuous Gaussian distributions with diagonal covariances instead of just mean vectors <ref> [4, 29, 80] </ref>. Consequently, an SCHMM enjoys fast computation of state likelihoods but reduces the quantization errors of discrete HMMs, with a negligible increase in model parameters. <p> To increase the likelihood computation efficiency, fewer streams are desirable as fewer additions are then needed to compute the full-space state likelihoods. However, fewer streams will lead to poorer performance unless more prototypes are used. 4.4 Comparison with Semi-Continuous HMM At first glance, SDCHMMs may appear similar to SCHMMs <ref> [4, 29, 80] </ref>: Both methods divide the feature space into streams, and tie subspace distributions across all states of all HMMs.
Reference: [81] <author> StatSci, </author> <title> a Division of MathSoft, Inc. </title> <journal> S-PLUS Guide to Statistics and Mathematical Analysis, </journal> <pages> pages 6-52. </pages> <address> StatSci, Seattle, Washington, 3.2 edition, </address> <year> 1993. </year>
Reference-contexts: Superimposed on each scatter plot is a cubic B-spline fit generated by the statistical software S-PLUS <ref> [81] </ref>. The performance of the CI-SGTS-M32-n64 SDCHMMs degrades more slowly than that of the CI-SGTS-M16-n128 SDCHMMs when the amount of training data decreases. This is clearly due to the fact that there are even fewer model parameters and more sharing among the subspace Gaussians of the CI-SGTS-M32-n64 SDCHMMs.
Reference: [82] <author> S.S. Stevens and J. Volkmann. </author> <title> "The Relation of Pitch of Frequency: A Revised Scale". </title> <journal> American Journal of Psychology, </journal> <volume> 53 </volume> <pages> 329-353, </pages> <year> 1940. </year>
Reference-contexts: Due to the findings from psychoacoustical studies which show that humans do not perceive frequencies greater than 1kHz in a linear scale but instead in a logarithmic scale, the cepstral coefficients are more commonly expressed as mel-frequency <ref> [82] </ref> cepstral coefficients (MFCC) or perceptual linear predictive (PLP) coefficients [24] in the Bark scale [90]. 11 a 11 00 a 22 a denotes the transition probability from state i to state j) 2.2.2 Hidden Markov Model Most of the aforementioned speech characteristics can be captured by a probabilistic finite-state machine
Reference: [83] <author> S. Takahashi, T. Matsuoka, Y. Minami, and K. Shikano. </author> <title> "Phoneme HMMs Constrained by Frame Correlations". </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <volume> volume 2, </volume> <pages> pages 219-222, </pages> <year> 1993. </year>
Reference-contexts: Because of the huge increase in the number of estimation parameters, approximation is used to express the joint conditional probabilities in terms of individual conditional probabilities in bigram-constrained HMMs <ref> [83] </ref> (using observation bigrams), or using an extended logarithmic pool [40] . * Hybrid models of Artificial Neural Networks and HMMs incorporate contextual information simply by using the current feature frame together with neighboring fea ture frames [11]. * Hybrid models of Recurrent Neural Networks and HMMs explicitly estimate the posterior
Reference: [84] <author> S. Takahashi and S. Sagayama. </author> <title> "Effects of Variance Tying for Four-Level Tied Structure Phone Models". </title> <booktitle> In Proceedings of ASI Conference, volume 1-Q-23, </booktitle> <pages> pages 141-142, </pages> <year> 1995. </year> <month> 122 </month>
Reference-contexts: From the perspective of quantization, one may consider SDCHMM as an approximation to the highly accurate CDHMM, achieving great data compression by subspace distribution quantization. From the perspective of hidden Markov modeling, SDCHMM unifies the theory of CDHMM which employs full-space state probability density distributions and the feature-parameter-tying HMM <ref> [84, 85] </ref> which is generated by scalar quantization of the distributions. SDCHMM combines the accuracy of CDHMM with the compactness of feature-parameter-tying HMM. <p> In the same spirit, FPTHMM <ref> [84, 85] </ref> clusters Gaussian components of a continuous density HMM (CDHMM) with diagonal covariances in each dimension into very few 1-dimensional Gaussian prototypes 2 . When combined with SQ of input features, likelihoods due to all shared scalar Gaussian prototypes can be pre-computed and stored in a look-up table. <p> mixture components per state, and thus has the following advantages over SCHMM: * Fewer components mean fewer mixture weights which then take less memory space. * Fewer components are involved in state likelihood computation which then takes less CPU time. 42 4.5 Comparison with Feature-Parameter-Tying HMM The feature-parameter-tying HMM (FPTHMM) <ref> [84, 85] </ref> turns out to be a special case of our SDCHMM when the number of streams is set to the size of the feature vector; i.e. K = D. In a sense, the FPTHMM is the scalar quantization (SQ) version of our SDCHMM.
Reference: [85] <author> S. Takahashi and S. Sagayama. </author> <title> "Four-Level Tied-Structure for Efficient Representation of Acoustic Modeling". </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 520-523, </pages> <year> 1995. </year>
Reference-contexts: In the past, the technique of parameter tying has been applied successfully at various gran-ularities: Phones (generalized biphones/triphones [48], context-independent phones [49]), states (tied-state HMM [32, 88]), observation distributions (tied-mixture/semi-continuous HMM [4, 29, 80]), and feature parameters <ref> [85] </ref> have all been tied. For example, of the two aforementioned systems, AT&T's ATIS recognizer is a tied-state system, whereas CMU's Sphinx II employs semi-continuous HMMs. <p> From the perspective of quantization, one may consider SDCHMM as an approximation to the highly accurate CDHMM, achieving great data compression by subspace distribution quantization. From the perspective of hidden Markov modeling, SDCHMM unifies the theory of CDHMM which employs full-space state probability density distributions and the feature-parameter-tying HMM <ref> [84, 85] </ref> which is generated by scalar quantization of the distributions. SDCHMM combines the accuracy of CDHMM with the compactness of feature-parameter-tying HMM. <p> In the same spirit, FPTHMM <ref> [84, 85] </ref> clusters Gaussian components of a continuous density HMM (CDHMM) with diagonal covariances in each dimension into very few 1-dimensional Gaussian prototypes 2 . When combined with SQ of input features, likelihoods due to all shared scalar Gaussian prototypes can be pre-computed and stored in a look-up table. <p> mixture components per state, and thus has the following advantages over SCHMM: * Fewer components mean fewer mixture weights which then take less memory space. * Fewer components are involved in state likelihood computation which then takes less CPU time. 42 4.5 Comparison with Feature-Parameter-Tying HMM The feature-parameter-tying HMM (FPTHMM) <ref> [84, 85] </ref> turns out to be a special case of our SDCHMM when the number of streams is set to the size of the feature vector; i.e. K = D. In a sense, the FPTHMM is the scalar quantization (SQ) version of our SDCHMM.
Reference: [86] <author> A.J. </author> <title> Viterbi. "Error Bounds for Convolutional Codes and an Asymptotically Optimal Decoding Algorithm". </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 13 </volume> <pages> 260-269, </pages> <month> April </month> <year> 1967. </year>
Reference-contexts: During Viterbi decoding <ref> [86] </ref>, the state log likelihood computation of Equation (4.12) is reduced to a summation of K pre-computed subspace Gaussian log likelihoods and the mixture weight for each component of the state.
Reference: [87] <author> P.C. Woodland, C.J. Leggetter, J.J. Odell, V. Valtchev, and S.J. Young. </author> <title> "The Development of the 1994 HTK Large Vocabulary Speech Recognition System". </title> <booktitle> In Proceedings of ARPA Spoken Language Systems Technology Workshop, </booktitle> <pages> pages 104-109. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1995. </year>
Reference-contexts: The capabilities of some state-of-the-art recognizers over a wide range of tasks are summarized in Figure 1.1 <ref> [12, 28, 34, 54, 87] </ref>. In the figure, the difficulty of a recognition task (from the perspective of a recognizer) is measured by the perplexity of its language model, whereas the performance of a recognizer is gauged by its word error rate (WER).
Reference: [88] <author> S.J. Young and P.C. Woodland. </author> <title> "The Use of State Tying in Continuous Speech Recognition". </title> <booktitle> In Proceedings of the European Conference on Speech Communication and Technology, </booktitle> <volume> volume 3, </volume> <pages> pages 2203-2206, </pages> <year> 1993. </year>
Reference-contexts: In the past, the technique of parameter tying has been applied successfully at various gran-ularities: Phones (generalized biphones/triphones [48], context-independent phones [49]), states (tied-state HMM <ref> [32, 88] </ref>), observation distributions (tied-mixture/semi-continuous HMM [4, 29, 80]), and feature parameters [85] have all been tied. For example, of the two aforementioned systems, AT&T's ATIS recognizer is a tied-state system, whereas CMU's Sphinx II employs semi-continuous HMMs. <p> Therefore TSHMM clusters the corresponding HMM states of the same base phone <ref> [32, 88] </ref>. Young and Woodland reported a 5-fold reduction in the number of states in their TSHMM systems [88]. 17 Tied-mixture HMM/semi-continuous HMM (SCHMM): SCHMM is a generalization of the discrete HMM in which prototypes are continuous Gaussian distributions with diagonal covariances instead of just mean vectors [4, 29, 80]. <p> Therefore TSHMM clusters the corresponding HMM states of the same base phone [32, 88]. Young and Woodland reported a 5-fold reduction in the number of states in their TSHMM systems <ref> [88] </ref>. 17 Tied-mixture HMM/semi-continuous HMM (SCHMM): SCHMM is a generalization of the discrete HMM in which prototypes are continuous Gaussian distributions with diagonal covariances instead of just mean vectors [4, 29, 80].
Reference: [89] <author> Yunxin Zhao. </author> <title> "A Speaker-Independent Continuous Speech Recognition System Using Continuous Mixture Gaussian Density HMM of Phoneme-Sized Units". </title> <journal> IEEE Transactions on Speech and Audio Processing, </journal> <volume> 1(3) </volume> <pages> 345-361, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: we would also like to point out some minor issues in our implementations of HMMs: * Precise estimation of the transition probabilities is found unnecessary, and each transition arc is simply assigned an equal probability of 1/fan-out, where fan-out is the number of arcs coming out of the source state <ref> [10, 89] </ref>. * No explicit duration modeling is performed as it is not found necessary. * Our HMMs are trained using the simpler segmental k-means algorithm [38] which has been found to work as well as the conventional Baum-Welch algorithm [3] ( [45, 71, 89]). 15 2.3 Speech Unit to Model <p> of arcs coming out of the source state [10, 89]. * No explicit duration modeling is performed as it is not found necessary. * Our HMMs are trained using the simpler segmental k-means algorithm [38] which has been found to work as well as the conventional Baum-Welch algorithm [3] ( <ref> [45, 71, 89] </ref>). 15 2.3 Speech Unit to Model Although it may be most natural to model each word individually, the approach is impractical in most recognition tasks, except those of small vocabularies and isolated words. In most cases, there are not sufficient training data for each word. <p> The rest of SDCHMM theory then applies as before. While other pdf functionals or Gaussians with block-diagonal covariances appear in triguing, they have not been widely studied (except, e.g., <ref> [89] </ref>) in automatic speech recog nition. <p> A 13-stream block-diagonal covariance of the features with 3 features per stream will only have 3 fi 3 fi 13 = 117 parameters | only three times the number of parameters in a diagonal covariance. In fact, <ref> [89] </ref> used such a distribution with two streams: One for the static features and the other for the dynamic features. In the context of our SDCHMM, more streams are preferable, and that will also mean a much smaller increase in distribution parameters when compared with the use of diagonal covariance.
Reference: [90] <author> E. Zwicker, G. Flottorp, and S.S. Stevens. </author> <title> "Critical Bandwidth in Loudness Summation". </title> <journal> Journal of Acoustical Society of America, </journal> <volume> 29 </volume> <pages> 548-557, </pages> <year> 1957. </year>
Reference-contexts: the findings from psychoacoustical studies which show that humans do not perceive frequencies greater than 1kHz in a linear scale but instead in a logarithmic scale, the cepstral coefficients are more commonly expressed as mel-frequency [82] cepstral coefficients (MFCC) or perceptual linear predictive (PLP) coefficients [24] in the Bark scale <ref> [90] </ref>. 11 a 11 00 a 22 a denotes the transition probability from state i to state j) 2.2.2 Hidden Markov Model Most of the aforementioned speech characteristics can be captured by a probabilistic finite-state machine called the hidden Markov model (HMM).
References-found: 90

