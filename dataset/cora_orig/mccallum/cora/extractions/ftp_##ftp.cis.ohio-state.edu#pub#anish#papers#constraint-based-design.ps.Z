URL: ftp://ftp.cis.ohio-state.edu/pub/anish/papers/constraint-based-design.ps.Z
Refering-URL: http://www.cis.ohio-state.edu/~anish/pub.html
Root-URL: 
Phone: 2.  
Title: Constraint Satisfaction as a Basis for Designing Nonmasking Fault-Tolerance  
Author: Anish Arora Mohamed Gouda George Varghese 
Keyword: design, methodology, distributed constraints, nonmasking fault-tolerance, closure, convergence, stabilization.  
Address: Columbus  
Affiliation: 1. Department of Computer Science, The Ohio State University at  Department of Computer Sciences, The University of Texas at Austin 3. Laboratory for Computer Science, Massachussetts Institute of Technology  
Abstract: We present a method for the design of nonmasking fault-tolerant programs. In our method, a set of constraints is associated with each program. As long as faults do not occur, the constraints are continually satisfied under the execution of program actions. Whenever some of the constraints are violated, due to certain faults, all constraints are eventually reestablished by subsequent execution of the program actions. To design programs thus, two types of program actions are distinguished: "closure" actions and "convergence" actions. Closure actions are the actions that perform the intended computation of the program when all of the constraints are satisfied. Convergence actions are the actions that reestablish the constraints when they have been violated. Sufficient conditions for the validation of closure and convergence actions are formalized in terms of a "constraint graph". These conditions are illustrated by designing nonmasking fault-tolerant programs for diffusing computations, atomic actions, and token rings. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. J. Fischer, N. A. Lynch, and M. S. Paterson, </author> <title> "Impossibility of distributed consensus with one faulty process", </title> <journal> Journal of the ACM, </journal> <volume> 32(2) (1985), </volume> <pages> pp. 374-382. </pages>
Reference-contexts: For instance, there are situations where achieving masking fault-tolerance is * Impossible : e.g., there is no asynchronous distributed program whose processes reach consensus on a binary value and mask the effect of a process crash <ref> [1] </ref>, * Impractical : e.g., the amount of redundancy and synchronization required to implement fail-stop processors that mask the effect of byzantine faults can be prohibitively expensive [2], or * Unnecessary : e.g., a call-back telephone service that eventually establishes a connection is useful even if it does not mask its
Reference: [2] <author> R. D. Schlichting and F. B. Schneider, </author> <title> "Fail-stop processors: An approach to designing fault-tolerant computing systems", </title> <journal> ACM Transactions on Computers (1983), </journal> <pages> pp. 222-238. </pages>
Reference-contexts: e.g., there is no asynchronous distributed program whose processes reach consensus on a binary value and mask the effect of a process crash [1], * Impractical : e.g., the amount of redundancy and synchronization required to implement fail-stop processors that mask the effect of byzantine faults can be prohibitively expensive <ref> [2] </ref>, or * Unnecessary : e.g., a call-back telephone service that eventually establishes a connection is useful even if it does not mask its initial failure to establish a connection. <p> It follows that p [ q is T -tolerant for S. 2 We illustrate Theorem 2 by designing a program that implements an atomic action. The program is motivated by a method due to Schlichting and Schneider <ref> [2] </ref>, that shows how to design certain fault-tolerant programs using a sequence of "restartable" phases.
Reference: [3] <author> E. W. Dijkstra, </author> <title> A Discipline of Programming, </title> <publisher> Prentice-Hall (1976). </publisher>
Reference-contexts: In comparison to masking fault-tolerant programs, the design of nonmasking ones has received lesser study <ref> [3] </ref>. In this paper, we present a method for the design of nonmasking fault-tolerant programs. Our method is motivated by a formal definition of what it means for a program to be fault-tolerant [4, 5]. In the method, a set of constraints is associated with each program.
Reference: [4] <author> A. Arora and M. G. Gouda, </author> <title> "Closure and convergence: A foundation of fault-tolerant computing", </title> <journal> IEEE Transactions on Software Engineering 19(11) (1993), </journal> <pages> pp. 1015-1027. </pages>
Reference-contexts: In comparison to masking fault-tolerant programs, the design of nonmasking ones has received lesser study [3]. In this paper, we present a method for the design of nonmasking fault-tolerant programs. Our method is motivated by a formal definition of what it means for a program to be fault-tolerant <ref> [4, 5] </ref>. In the method, a set of constraints is associated with each program. As long as faults do not occur, the constraints are continually satisfied under the execution of program actions. <p> Examples showing how to design program invariants appear in [7]. We next observe that the input-output relation of programs in the presence of faults can also be characterized by a state predicate that is true throughout program execution <ref> [4, 5] </ref>. Such a state predicate identifies the fault-span of the program; i.e., the set of states that the program can reach in the presence of faults. (Note that the fault-span includes the fault-free states of the program.) Examples showing how to design fault-span predicates appear in [5]. <p> As a result of this view, a program fault-span identifies a set of states that is kept closed under the execution of program actions as well as fault actions. We are now ready to give a formal definition of fault-tolerance <ref> [4, 5] </ref>. <p> The definition above yields a formal classification of masking and nonmasking fault-tolerance <ref> [4, 5] </ref>. Let p be T -tolerant for S. If S = T , we say that p is masking fault tolerant. <p> For instance, recall that one of the closure actions in the stabilizing diffusing computation involves accessing the state of a node and all its children nodes in the out-tree. This action has 19 high atomicity and may therefore be unsuitable for a distributed implementation. In <ref> [4] </ref>, we present a refinement of this system that yields actions with low atomicity and preserves the property of convergence. We study refinement issues in a companion paper. Acknowledgements.
Reference: [5] <author> A. Arora, </author> <title> "A foundation of fault-tolerant computing", </title> <type> Ph.D. Dissertation, </type> <institution> The University of Texas at Austin (1992). </institution>
Reference-contexts: In comparison to masking fault-tolerant programs, the design of nonmasking ones has received lesser study [3]. In this paper, we present a method for the design of nonmasking fault-tolerant programs. Our method is motivated by a formal definition of what it means for a program to be fault-tolerant <ref> [4, 5] </ref>. In the method, a set of constraints is associated with each program. As long as faults do not occur, the constraints are continually satisfied under the execution of program actions. <p> Examples showing how to design program invariants appear in [7]. We next observe that the input-output relation of programs in the presence of faults can also be characterized by a state predicate that is true throughout program execution <ref> [4, 5] </ref>. Such a state predicate identifies the fault-span of the program; i.e., the set of states that the program can reach in the presence of faults. (Note that the fault-span includes the fault-free states of the program.) Examples showing how to design fault-span predicates appear in [5]. <p> Such a state predicate identifies the fault-span of the program; i.e., the set of states that the program can reach in the presence of faults. (Note that the fault-span includes the fault-free states of the program.) Examples showing how to design fault-span predicates appear in <ref> [5] </ref>. These examples employ the view that all classes of faults can be represented as actions that change the program state [5, 8]. <p> These examples employ the view that all classes of faults can be represented as actions that change the program state <ref> [5, 8] </ref>. As a result of this view, a program fault-span identifies a set of states that is kept closed under the execution of program actions as well as fault actions. We are now ready to give a formal definition of fault-tolerance [4, 5]. <p> As a result of this view, a program fault-span identifies a set of states that is kept closed under the execution of program actions as well as fault actions. We are now ready to give a formal definition of fault-tolerance <ref> [4, 5] </ref>. <p> The definition above yields a formal classification of masking and nonmasking fault-tolerance <ref> [4, 5] </ref>. Let p be T -tolerant for S. If S = T , we say that p is masking fault tolerant.
Reference: [6] <author> K. M. Chandy and J. Misra, </author> <title> Parallel Program Design: A Foundation, </title> <publisher> Addison-Wesley (1988). </publisher>
Reference-contexts: finite then no action in the set is enabled in the final state. 3 Designing Nonmasking Fault-Tolerant Programs To motivate our method, let us first observe that the input-output relation of (both sequential and concurrent) programs can be characterized by a state predicate that is true throughout the program execution <ref> [6] </ref>. Such an invariant predicate serves two purposes. First, it identifies the set of "fault-free" states of the program; these are the states starting from which every computation of the program is guaranteed to meet the specification of the program, i.e., the safety and progress properties required of the program.
Reference: [7] <editor> D. Gries, </editor> <booktitle> The Science of Programming, </booktitle> <publisher> Springer-Verlag (1981). </publisher>
Reference-contexts: Second, an invariant predicate constrains the design of program actions by requiring that the set of fault-free states be kept closed under the execution of program actions. Examples showing how to design program invariants appear in <ref> [7] </ref>. We next observe that the input-output relation of programs in the presence of faults can also be characterized by a state predicate that is true throughout program execution [4, 5].
Reference: [8] <author> F. Cristian, </author> <title> "A rigorous approach to fault-tolerant programming", </title> <journal> IEEE Transactions on Software Engg. </journal> <volume> 11(1), </volume> <year> (1985). </year>
Reference-contexts: These examples employ the view that all classes of faults can be represented as actions that change the program state <ref> [5, 8] </ref>. As a result of this view, a program fault-span identifies a set of states that is kept closed under the execution of program actions as well as fault actions. We are now ready to give a formal definition of fault-tolerance [4, 5].
Reference: [9] <author> E. W. Dijkstra, </author> <title> "Self-stabilizing systems in spite of distributed control", </title> <booktitle> Communications of the ACM 17(11) (1974). </booktitle>
Reference-contexts: We illustrate Theorem 1 by designing a "stabilizing" program that maintains a diffusing computation. A stabilizing program <ref> [9, 10, 11] </ref> is one that exhibits an extreme form of nonmasking fault-tolerance: regardless of the state the program is started in, execution of the program converges to a state from where S holds. <p> The program is due to Dijkstra <ref> [9] </ref>. 7.1 Token ring Cooperation between processes of a distributed system, for say resource sharing, can be achieved by passing a token around the processes of the system. In such a token passing program, the process possessing the token has the privilege to access the shared resource.
Reference: [10] <author> F. B. Bastani, I.-L. Yen, and I.-R. Chen, </author> <title> "A class of inherently fault-tolerant distributed programs", </title> <journal> IEEE Transactions on Software Engg. </journal> <volume> 14(10) (1988), </volume> <pages> pp. 1431-1442. </pages>
Reference-contexts: We illustrate Theorem 1 by designing a "stabilizing" program that maintains a diffusing computation. A stabilizing program <ref> [9, 10, 11] </ref> is one that exhibits an extreme form of nonmasking fault-tolerance: regardless of the state the program is started in, execution of the program converges to a state from where S holds.
Reference: [11] <author> J. E. Burns and J. Pachl, </author> <title> "Uniform stabilizing rings," </title> <booktitle> ACM Transactions on Programming Languages and Systems 11(2) (1989), </booktitle> <pages> pp. 330-344. </pages>
Reference-contexts: We illustrate Theorem 1 by designing a "stabilizing" program that maintains a diffusing computation. A stabilizing program <ref> [9, 10, 11] </ref> is one that exhibits an extreme form of nonmasking fault-tolerance: regardless of the state the program is started in, execution of the program converges to a state from where S holds.
Reference: [12] <author> A. Arora and M. G. Gouda, </author> <title> "Distributed reset", </title> <journal> IEEE Transactions on Computers (1994), </journal> <note> to appear. </note>
Reference-contexts: Having completely spanned the system, the computation then collapses back to the distinguished process. Below, we specify and design an abstract version of a diffusing computation, that is independent of any specific application. (The resulting program is a simplified version of a program in <ref> [12] </ref>.) Specification : Consider a finite, rooted tree. Desired is a program in which, starting from a state where all tree nodes are colored green, the root node initiates a diffusing computation. The diffusing computation then propagates from the root to the leaves, coloring the tree nodes red.
Reference: [13] <author> M. Gouda and N. Multari, </author> <title> "Stabilizing communication protocols," </title> <journal> IEEE Transactions on Computers 40(4) (1991), </journal> <pages> pp. 448-458. </pages>
Reference-contexts: In stage one, starting from any state where T holds each computation reaches a state where R holds. In stage two, starting from any state where R holds each computation reaches a state where S holds. (Gouda and Multari call this a convergence stair of height two <ref> [13] </ref>.) In this case, the constraint graph for R may be self-looping, whereas the constraint graph for T is cyclic. As a result, convergence validation may be carried out in two corresponding stages, and Theorem 2 may be used for stage two.
Reference: [14] <author> G. Varghese, </author> <title> "Self-stabilization by local checking and correction", </title> <type> Ph.D. Dissertation, </type> <institution> Mas-sachusetts Institute of Technology (1992). </institution> <month> 20 </month>
References-found: 14

