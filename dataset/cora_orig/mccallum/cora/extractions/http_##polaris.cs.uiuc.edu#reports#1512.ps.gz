URL: http://polaris.cs.uiuc.edu/reports/1512.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/polaris/rep2.html
Root-URL: http://www.cs.uiuc.edu
Email: potteng,yuanlin,padua@csrd.uiuc.edu  asenjo,eladio,ezapata@ac.uma.es  
Phone: 217-333-6578, fax: 217-244-1351  +34-(9)5-21327-91, fax: +34-(9)5-213-27-90  
Title: On the Automatic Parallelization of Sparse and Irregular Fortran Codes  
Author: Rafael Asenjo, Eladio Gutierrez, Yuan Lin, David Padua, Bill Pottenger, Emilio Zapata Complejo Tecnologico 
Date: February 28, 1997  
Address: 1308 West Main Street, Urbana, Illinois 61801-2307  Apdo. 4114 29080 MALAGA  
Affiliation: Center for Supercomputing Research and Development University of Illinois at Urbana-Champaign  Dept. Arquitectura de Computadores E.T.S. Ingenieros de Telecomunicacion  Campus de Teatinos  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Zahira Ammarguellat and Luddy Harrison. </author> <title> Automatic Recognition of Induction & Recurrence Relations by Abstract Interpretation. </title> <booktitle> Proceedings of Sigplan 1990, </booktitle> <address> Yorktown Heights, </address> <month> 25(6) </month> <pages> 283-295, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Lagged Fibonacci generators such as that implemented in su2cor take the form of a recurrence relation. Such relations can be automatically detected using pattern recognition techniques <ref> [1] </ref>. General techniques for solving linear recurrences of this type are well known [17], and closed-forms for such recurrences can be computed at compile-time thereby breaking loop-carried dependences. In cases where RNGs are not explicitly coded as linear recurrences, other techniques must be employed.
Reference: [2] <author> R. Asenjo, M. Ujaldon, and E. L. Zapata. </author> <title> SpLU Sparse LU Factorization. HPF-2. Scope of Activities and Motivating Applications. High Performance Fortran Forum, </title> <note> version 0.8 edition, </note> <month> November </month> <year> 1994. </year>
Reference-contexts: This led to the inclusion of the original C version of SpLU in the suite of HPF-2 motivating applications. The version of SpLU included in our benchmark suite is a Fortran implementation by the authors of the original HPF-2 version <ref> [2] </ref>.
Reference: [3] <author> Richard Barrett, Michael Berry, Tony F. Chan, James Demmel, June Donato, Jack Dongarra, Victor Eijkhout, Roldan Pozo, Charles Romine, and Henk van der Vorst. </author> <title> Templates for the Solution of Linear Systems: Building Blocks for Iterative Methods. </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1994. </year>
Reference-contexts: excerpt demonstrates: do j=1,a_nr r (j)=r (j)+ad (k)*q (ac (k),i) enddo enddo The matrix 1138 BUS of Harwell-Boeing collection was used as input for this benchmark. 2.6 MVPRODUCT MVPRODUCT is a set of basic sparse matrix operations including sparse matrix-vector multiplication and the product and sum of two sparse matrices <ref> [3, 12] </ref>. The representation of the sparse 4 matrices employs two different schemes: compressed row storage (CRS) and compressed column storage (CCS) [22].
Reference: [4] <author> M. Berry, D. Chen, P. Koss, D. Kuck, L. Pointer, S. Lo, Y. Pang, R. Roloff, A. Sameh, E. Clementi, S. Chin, D. Schneider, G. Fox, P. Messina, D. Walker, C. Hsiung, J. Schwarzmeier, K. Lue, S. Orszag, F. Seidl, O. Johnson, G. Swanson, R. Goodrum, and J. Martin. </author> <title> The Perfect Club Benchmarks: Effective Performance Evalution of Supercomputers. </title> <booktitle> Int'l. Journal of Supercomputer Applications, Fall 1989, </booktitle> <volume> 3(3) </volume> <pages> 5-40, </pages> <month> Fall </month> <year> 1989. </year> <month> 23 </month>
Reference-contexts: In addition, random () is called in INITIA DO2000 in the Perfect Club benchmark MDG <ref> [4] </ref>. The CHOLESKY benchmark in our suite also calls the rand () library routine in an important loop. In three of these cases, the RNG call is the only factor preventing parallelization of the loop after application of the techniques implemented in the current Polaris restructurer.
Reference: [5] <author> G.A. Bird. </author> <title> Molecular Gas Dynamics and the Direct Simulation of Gas Flows. </title> <publisher> Oxford University Press, Oxford, </publisher> <address> England, </address> <year> 1994. </year>
Reference-contexts: The Harwell-Boeing matrix BCSSTK30 was used as input for this benchmark [10]. 2.2 DSMC3D DSMC3D is a modification of the DSMC (Direct Simulation Monte Carlo) benchmark in 3 dimensions. DSMC implements a simulation of the behavior of particles of a gas in space using the Monte Carlo method <ref> [5] </ref>.
Reference: [6] <author> Graeme Bird. </author> <title> Personal communication with author, </title> <year> 1996. </year>
Reference-contexts: However, when a molecule leaves the flow, it is deleted from the list and replaced by the last molecule in the list. This creates loop-carried dependences in the do loop. However, the deletion of molecules can be deferred until after the entire list has been processed <ref> [26, 6] </ref>.
Reference: [7] <author> William Blume, Ramon Doallo, Rudolf Eigenmann, John Grout, Jay Hoeflinger, Thomas Lawrence, Jaejin Lee, David Padua, Yunheung Paek, Bill Pottenger, Lawrence Rauchwerger, and Peng Tu. </author> <title> Parallel Programming with Polaris. </title> <journal> IEEE Computer, </journal> <volume> 29(12) </volume> <pages> 78-82, </pages> <month> December </month> <year> 1996. </year>
Reference-contexts: This paper studies how well automatic parallelization techniques work on a collection of real codes with sparse and irregular access patterns. In conducting this work, we have compared existing technology in the commercial parallelizer PFA from SGI with the Polaris restructurer <ref> [7] </ref>. In cases fl This work is supported by U.S. <p> The parallelization of histogram reductions is based on a run-time technique which depends on the associativity of the operation being performed. The Polaris parallelizing restructurer recognizes and transforms histogram reductions <ref> [7] </ref>. The parallelizing transformation takes one of three forms: critical section, privatized, expanded. Each approach is discussed and exemplified below.
Reference: [8] <author> B.R. Brooks, R.E. Bruccoleri, B.D. Olafson, D.J. States, S. Swaminathan, and M. Karplus. CHARMM: </author> <title> A Program for Macromolecular Energy, Minimization, and Dynamics Calculations. </title> <journal> J. Comp. Chem., </journal> <volume> 4 </volume> <pages> 187-217, </pages> <year> 1983. </year>
Reference-contexts: The matrix BC-SSTK14 from the Harwell-Boeing collection has been used as input to this benchmark. 2.7 NBFC The calculation of non-bonded forces forms a key element of many molecular dynamics computations <ref> [8] </ref>. NBFC computes an electro-static interaction between particles where the forces acting on an atom are calculated from a list of neighboring atoms.
Reference: [9] <author> Luiz DeRose, Kyle Gallivan, Bret Marsolf, David Padua, and Stratis Gallopoulos. </author> <title> FALCON: A MATLAB Interactive Restructuring Compiler. </title> <booktitle> Proceedings of the 8th International Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Columbus, OH, pages 18.1-18.18, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: Calls to single-threaded user routines of one of the following types linear congruential generators lagged Fibonacci generators 9 3.2.1 Single-threaded library routines We have determined that calls to RNG library routines occur in two computationally important loops in a test suite used in the evaluation of the FALCON MATLAB compiler <ref> [9] </ref>. In addition, random () is called in INITIA DO2000 in the Perfect Club benchmark MDG [4]. The CHOLESKY benchmark in our suite also calls the rand () library routine in an important loop.
Reference: [10] <author> Iain Duff, Nick Gould, John Reid, Jennifer Scott, and Linda Miles. </author> <title> Harwell Subroutine Library. </title> <institution> Technical Report http://www.rl.ac.uk/departments/ccd/numerical/hsl/hsl.html, Council for the Central Laboratory of the Research Councils, Department for Computation and Information, Advanced Research Computing Division. </institution>
Reference-contexts: The Harwell-Boeing matrix BCSSTK30 was used as input for this benchmark <ref> [10] </ref>. 2.2 DSMC3D DSMC3D is a modification of the DSMC (Direct Simulation Monte Carlo) benchmark in 3 dimensions. DSMC implements a simulation of the behavior of particles of a gas in space using the Monte Carlo method [5]. <p> This algorithm is somewhat slower than the MA48 code from Harwell Subroutine Library <ref> [10] </ref>, a left-looking standard benchmark for factorization. The motivation for developing a right-looking algorithm derived from the lack of significant parallelism in MA48. This led to the inclusion of the original C version of SpLU in the suite of HPF-2 motivating applications.
Reference: [11] <author> Ian Foster, Rob Schreiber, and Paul Havlak. </author> <title> HPF-2 Scope of Activities and Motivating Applications. </title> <type> Technical Report CRPC-TR94492, </type> <institution> Rice University, </institution> <month> November </month> <year> 1994. </year>
Reference-contexts: The suite consists of a collection of sparse and irregular application programs as well as several kernels representing key computational elements present in sparse codes. Several of the benchmarks in our suite are derived from the set of motivating applications for the HPF-2 effort <ref> [11] </ref>. Exceptions include the kernels MVPRODUCT and LANCZOS which were developed as part of this project.
Reference: [12] <author> G.H. Golub and C.F. van Loan. </author> <title> Matrix Computations. </title> <publisher> The Johns Hopkins University Press, </publisher> <year> 1993. </year>
Reference-contexts: As a result, indirection occurs on the right-hand-side of the computed expressions. do nc=nintci,nintcf direc2 (nc)=bp (nc)*direc1 (nc) * -bs (nc)*direc1 (lcc (nc,1)) * -bw (nc)*direc1 (lcc (nc,4)) * -bl (nc)*direc1 (lcc (nc,5)) enddo 2.5 LANCZOS The lanczos algorithm with full reorthogonalization determines the eigenvalues of a symmetric matrix <ref> [12] </ref>. LANCZOS is an implementation of the lanczos algorithm for sparse matrices. The key computational elements are the calculation of a sparse matrix-vector product and the reorthogonal-ization of a dense work matrix. <p> excerpt demonstrates: do j=1,a_nr r (j)=r (j)+ad (k)*q (ac (k),i) enddo enddo The matrix 1138 BUS of Harwell-Boeing collection was used as input for this benchmark. 2.6 MVPRODUCT MVPRODUCT is a set of basic sparse matrix operations including sparse matrix-vector multiplication and the product and sum of two sparse matrices <ref> [3, 12] </ref>. The representation of the sparse 4 matrices employs two different schemes: compressed row storage (CRS) and compressed column storage (CCS) [22].
Reference: [13] <author> IBM. </author> <title> Parallel FORTRAN Language and Library Reference, </title> <month> March </month> <year> 1988. </year>
Reference-contexts: The Polaris parallelizing restructurer recognizes and transforms histogram reductions [7]. The parallelizing transformation takes one of three forms: critical section, privatized, expanded. Each approach is discussed and exemplified below. The language used in the examples is based on IBM's Parallel Fortran <ref> [13] </ref>. * Critical Section The first approach involves the insertion of synchronization primitives around each reduction statement, making the sum operation atomic.
Reference: [14] <author> Jee Myeong Ku. </author> <title> The Design of an Efficient and Portable Interface Between a Parallelizing Compiler and its Target Machine. </title> <type> Master's thesis, </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Supercomputing Res. & Dev., </institution> <year> 1995. </year>
Reference-contexts: This pattern occurs commonly in many codes, both sparse and non-sparse, and is termed a histogram reduction <ref> [19, 14] </ref>. In our study of the benchmark suite we have found that histogram reductions occur in key computational loops in all four of the benchmarks derived from the HPF-2 motivating suite: NBFC, CHOLESKY, DSMC3D, and EULER.
Reference: [15] <author> D.H. Lehmer. </author> <title> Mathematical Methods in Large-scale Computing Units. </title> <booktitle> In 2nd Symposium on Large-Scale Digital Calculating Machinery, </booktitle> <pages> pages 141-146, </pages> <address> Cambridge, MA, 1951. </address> <publisher> Harvard University Press. </publisher>
Reference-contexts: The Perfect Club benchmark QCD, for example, contains the routines PRANF, LADD, and LMULT which together implement a linear congruential pseudo-random number generator <ref> [15] </ref>. Similarly, the DSMC3D benchmark in our sparse/irregular suite implements a linear congruential generator based on work described in [16]. A third example occurs in the SPEC CFP95 benchmark su2cor which implements a lagged Fibonacci generator as described in the introduction to this section.
Reference: [16] <author> P.A.W. Lewis, A.S. Goodman, and J.M. Miller. </author> <title> A Pseudo-Random Number Generator for the System/360. </title> <journal> IBM Systems Journal, </journal> <volume> 8(2) </volume> <pages> 136-146, </pages> <month> May </month> <year> 1969. </year>
Reference-contexts: The Perfect Club benchmark QCD, for example, contains the routines PRANF, LADD, and LMULT which together implement a linear congruential pseudo-random number generator [15]. Similarly, the DSMC3D benchmark in our sparse/irregular suite implements a linear congruential generator based on work described in <ref> [16] </ref>. A third example occurs in the SPEC CFP95 benchmark su2cor which implements a lagged Fibonacci generator as described in the introduction to this section. Lagged Fibonacci generators such as that implemented in su2cor take the form of a recurrence relation.
Reference: [17] <author> G. Lueker. </author> <title> Some Techniques for Solving Recurrences. </title> <journal> Computing Surveys, </journal> <volume> Vol. 12, No. 4, </volume> <month> December </month> <year> 1980. </year>
Reference-contexts: Lagged Fibonacci generators such as that implemented in su2cor take the form of a recurrence relation. Such relations can be automatically detected using pattern recognition techniques [1]. General techniques for solving linear recurrences of this type are well known <ref> [17] </ref>, and closed-forms for such recurrences can be computed at compile-time thereby breaking loop-carried dependences. In cases where RNGs are not explicitly coded as linear recurrences, other techniques must be employed. Currently we plan to develop a directive interface which can be used to identify RNGs.
Reference: [18] <author> Michael Mascagni and David Bailey. </author> <title> Requirements for a Parallel Pseudorandom Number Generator. </title> <note> Technical Report http://olympic.jpl.nasa.gov/SSTWG/lolevel.msgs.html, Center for Computing Sciences, </note> <institution> I.D.A. and NASA Ames Research Center, </institution> <month> June </month> <year> 1995. </year>
Reference-contexts: Calls to routines of this type serialize a loop. Recent work by Bailey and Mascagni involves the development and standardization of robust thread-parallel pseudorandom number generators based on lagged Fibonacci series <ref> [20, 18] </ref>. RNGs such as these can be used to replace single-threaded generators.
Reference: [19] <author> Bill Pottenger and Rudolf Eigenmann. </author> <title> Idiom Recognition in the Polaris Parallelizing Compiler. </title> <booktitle> Proceedings of the 9th ACM International Conference on Supercomputing, Barcelona, Spain, </booktitle> <pages> pages 444-448, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: This pattern occurs commonly in many codes, both sparse and non-sparse, and is termed a histogram reduction <ref> [19, 14] </ref>. In our study of the benchmark suite we have found that histogram reductions occur in key computational loops in all four of the benchmarks derived from the HPF-2 motivating suite: NBFC, CHOLESKY, DSMC3D, and EULER. <p> There are two other loops which contain conditionally incremented induction variables, ENTER3 DO4 and INIT3 DO605. Together these loops account for approximately 7.5% of the sequential execution time. Both of these loops are parallelizable using techniques outlined in <ref> [19] </ref> for determining the closed-form of induction variables if the induction can be proven to be monotonically increasing. However, the conditional increment poses a problem in that monotonicity may not hold and the induction variable ranges may overlap as a result.
Reference: [20] <author> Daniel V. Pryor, Steven A. Cuccaro, Michael Mascagni, and M. L. Robinson. </author> <title> Implementation of a Portable and Reproducible Parallel Pseudorandom Number Generator. </title> <booktitle> Proceedings of Supercomputing '94, </booktitle> <month> Nov. </month> <year> 1994. </year> <month> 24 </month>
Reference-contexts: Calls to routines of this type serialize a loop. Recent work by Bailey and Mascagni involves the development and standardization of robust thread-parallel pseudorandom number generators based on lagged Fibonacci series <ref> [20, 18] </ref>. RNGs such as these can be used to replace single-threaded generators.
Reference: [21] <author> Lawrence Rauchwerger and David Padua. </author> <title> The LRPD Test: Speculative Run-Time Paralleliza--tion of Loops with Privatization and Reduction Parallelization. </title> <booktitle> Proceedings of the SIGPLAN'95 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: These problems are nonetheless important in that a significant fraction of current applications are irregular in nature. Some work has been done in the past, but very few studies have been made of complete codes <ref> [21] </ref>. This paper studies how well automatic parallelization techniques work on a collection of real codes with sparse and irregular access patterns. In conducting this work, we have compared existing technology in the commercial parallelizer PFA from SGI with the Polaris restructurer [7].
Reference: [22] <author> L.F. Romero and E.L. Zapata. </author> <title> Data Distributions for Sparse Matrix Vector Multiplication. </title> <journal> J. Parallel Computing, </journal> <volume> 21(4) </volume> <pages> 583-605, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: The representation of the sparse 4 matrices employs two different schemes: compressed row storage (CRS) and compressed column storage (CCS) <ref> [22] </ref>. The access pattern is demonstrated by the following code abstract: do i=1,a_nr do ja=ar (i),ar (i+1)-1 if (ac (ja).eq.br (jb)) THEN c (i,k)=c (i,k) endif enddo enddo enddo enddo Here indirection occurs on the right-hand-side of the computed expressions.
Reference: [23] <author> Gary Sabot. </author> <title> The Paralation Model. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1988. </year>
Reference-contexts: Although the generation of pseudo-random numbers is not an associative operation, it can be said to be associative if we consider the fact that the particular pseudo-random number returned by a generator is not important as long as it is truly "random" <ref> [23] </ref>. In this sense, the substitution of RNGs can be viewed as the replacement of a non-associative, single-threaded RNG with an associative RNG.
Reference: [24] <author> Peng Tu and David Padua. </author> <title> Automatic Array Privatization. </title> <editor> In Utpal Banerjee, David Gelernter, Alex Nicolau, and David Padua, editors, </editor> <booktitle> Proc. Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR. </address> <booktitle> Lecture Notes in Computer Science., </booktitle> <volume> volume 768, </volume> <pages> pages 500-521, </pages> <month> August 12-14, </month> <year> 1993. </year>
Reference-contexts: independent. 3.5 Copy-in and Copy-out After the identification of monotonicity in index arrays and the proof that induction variables have non-overlapping ranges, it is often necessary to break remaining loop-carried anti and output dependences by privatizing both scalar and array variables which are defined and used within a single iteration <ref> [24] </ref>. In DSMC3D COLLMR DO100, for example, variables in the /elast/ common block 14 were privatized. Similarly, several variables in DPFAC DO50 in SpLU required privatization.
Reference: [25] <author> Guhan Viswanathan and James R. Larus. </author> <title> User-defined Reductions for Efficient Communication in Data-Parallel Languages. </title> <type> Technical Report 1293, </type> <institution> Univ. of Wisconsin-Madison, Computer Sciences Department, </institution> <month> Aug. </month> <year> 1996. </year>
Reference-contexts: After exit from the parallel loop, marked elements are removed and the array a is packed. Effectively, the operation of removing and replacing elements in the list is associative, and therefore can be parallelized <ref> [25] </ref>. The combination of these techniques in the loops mentioned above contributed to the overall program speedup of 4.95 in the manually parallelized version. The speedups reported for Polaris include the histogram reduction in MOVE3 DO#3.
Reference: [26] <author> Dick Wilmoth. </author> <title> Personal communication with author, </title> <booktitle> 1996. </booktitle> <pages> 25 </pages>
Reference-contexts: However, when a molecule leaves the flow, it is deleted from the list and replaced by the last molecule in the list. This creates loop-carried dependences in the do loop. However, the deletion of molecules can be deferred until after the entire list has been processed <ref> [26, 6] </ref>.
References-found: 26

