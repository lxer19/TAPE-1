URL: http://www.cs.duke.edu/~mlittman/docs/x-lang.ps
Refering-URL: 
Root-URL: 
Title: A statistical method for language-independent representation of the topical content of text segments  
Author: Thomas K. Landauer and Michael L. Littman 
Keyword: Key-words: Interlingua, IR, information retrieval, statistical techniques, LSI, SVD, seman tics, translation, multilingual filters  
Address: 445 South St. Morristown NJ, 07960-1910  
Affiliation: Bell Communications Research  
Email: email=tkl@bellcore.com  
Phone: Tel. 908 829 4255  
Date: October 24, 1995  
Abstract: Where there are texts in more than one language, it would be desirable if users could give queries or examples in the language in which they are most competent and obtain relevant text passages in any language. We have developed and tested a prototype system that makes this possible. The system is based entirely on a statistical technique that requires no humanly constructed dictionary, thesaurus, or term bank. The language-independent representation of text has two steps. In the first, done just once for a subject area, a sample collection of parallel texts|paragraph-by-paragraph translations in two or more languages|is analyzed by a mathematical technique called Singular Value Decomposition. Each word in the sample is assigned a vector value determined by the total pattern of usage of all the words in all the sample paragraphs. In the second step, a new document or query in any of the original languages is assigned a vector value that is an average of the values of the words it contains. Tests on a French-English corpus showed that the method works well. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Landauer, and R. Harshman. </author> <title> Indexing by latent semantic analysis. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 41 </volume> <pages> 391-407, </pages> <year> 1990. </year>
Reference-contexts: With a deeper statistical analysis we can do much better. The technique we actually used was an extension of Latent Semantic Indexing <ref> [1, 3, 2] </ref>. In this method, a powerful mathematical computation is used to analyze and represent all the linear dependencies between word occurrences in paragraphs (or other text segments). <p> The first and third matrix represent terms and documents respectively as values on a smaller set of independent "basis" vectors; the second matrix contains scaling coefficients. Mathematics and computational implementation are presented in detail in Deerwester, Dumais, Furnas, Landauer and Harshman <ref> [1] </ref>. The retrieval process is the same as in certain standard vector information-retrieval or pattern recognition methods, e.g. using the cosine between a document and query vector or between two document or term vectors as the measure of similarity.
Reference: [2] <author> S. T. Dumais, G. W. Furnas, T. K. Landauer, S. Deerwester, and R. Harshman. </author> <title> Using latent semantic analysis to improve access to textual information. </title> <booktitle> In Proceedings CHI '88, </booktitle> <pages> pages 281-286, </pages> <year> 1988. </year>
Reference-contexts: With a deeper statistical analysis we can do much better. The technique we actually used was an extension of Latent Semantic Indexing <ref> [1, 3, 2] </ref>. In this method, a powerful mathematical computation is used to analyze and represent all the linear dependencies between word occurrences in paragraphs (or other text segments).
Reference: [3] <author> G. W. Furnas, S. Deerwester, S. T. Dumais, T. K. Landauer, R. A. Harshman, L. A. Streeter, and K. E. Lochbaum. </author> <title> Information retrieval using a Singular Value Decomposition model of latent semantic structure. </title> <booktitle> In Proceedings of the 11th ACM International Conference on Research and Development in Information Retrieval, </booktitle> <year> 1988. </year>
Reference-contexts: With a deeper statistical analysis we can do much better. The technique we actually used was an extension of Latent Semantic Indexing <ref> [1, 3, 2] </ref>. In this method, a powerful mathematical computation is used to analyze and represent all the linear dependencies between word occurrences in paragraphs (or other text segments).
Reference: [4] <author> D. Harman. </author> <title> An experimental study of factors important in document ranking. </title> <booktitle> In Proceedings of 9th ACM Conference on Research and Development in Information Retrieval, </booktitle> <year> 1986. </year>
Reference-contexts: The retrieval process is the same as in certain standard vector information-retrieval or pattern recognition methods, e.g. using the cosine between a document and query vector or between two document or term vectors as the measure of similarity. The principal difference between LSI and previous vector models (See Harman <ref> [4] </ref>, Salton and McGill [6]) that have been used in information retrieval is that the vectors are constructed in a space with many fewer dimensions (typically 100) than the number of original terms (typically many thousand), and that these dimensions are the subset of linearly independent basis vectors by which the
Reference: [5] <author> G. Salton. </author> <title> Automatic processing of foreign language documents. </title> <journal> Journal of the American Society for Information Sciences, </journal> <volume> 21 </volume> <pages> 187-194, </pages> <year> 1970. </year>
Reference-contexts: As a result, queries can automatically match similar text, e.g. translations, with which they share no terms. This cannot be accomplished with the usual representations used in information retrieval without manually constructed thesauri with their attendant expense and conceptual difficulties <ref> [5] </ref>. (More traditional natural language understanding systems require complex term-knowledge bases and extremely labor-intensive information entry to represent lexical relations for this purpose, and are therefore not easily applied to new domains with large vocabularies.) For the cross-language matching situation, it is especially important to understand the synonym equivalencing effect of
Reference: [6] <author> G. Salton and M. J. McGill. </author> <title> Automatic information organization and retrieval. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1983. </year> <month> 9 </month>
Reference-contexts: The principal difference between LSI and previous vector models (See Harman [4], Salton and McGill <ref> [6] </ref>) that have been used in information retrieval is that the vectors are constructed in a space with many fewer dimensions (typically 100) than the number of original terms (typically many thousand), and that these dimensions are the subset of linearly independent basis vectors by which the original term-by-document matrix can <p> synonym equivalencing property has been observed to occur often in practice, and overall recall/precision performance for information retrieval in standard test collections has varied from a few percent less good to almost 30 percent better for LSI than for the best previous methods, i.e. standard vector methods with term weighting <ref> [6] </ref>. 4 Cross-Language use of LSI To apply LSI to cross-language retrieval, we first analyze a training set of paragraphs for which translations exist in two (or more) languages.
References-found: 6

