URL: http://robotics.stanford.edu/~sbenson/thesis.ps
Refering-URL: http://robotics.stanford.edu/~sbenson/
Root-URL: http://www.cs.stanford.edu
Title: LEARNING ACTION MODELS FOR REACTIVE AUTONOMOUS AGENTS  
Author: Scott Sherwood Benson 
Degree: a dissertation submitted to the department of computer science and the committee on graduate studies of stanford university in partial fulfillment of the requirements for the degree of doctor of philosophy By  
Date: December 6, 1996  
Abstract-found: 0
Intro-found: 1
Reference: <author> Agre, P. & Chapman, D. </author> <year> (1987), </year> <title> PENGI: An implementation of a theory of activity, </title> <booktitle> in "AAAI-87: Proceedings of the Sixth National Conference on Artificial Intelligence", </booktitle> <publisher> AAAI Press / The MIT Press, </publisher> <pages> pp. 268-272. </pages>
Reference-contexts: Furthermore, the theories constructed by ILP were more understandable than those created by attribute-value methods. 5.4 An Indexical-Functional View of ILP The concept learning algorithm in TRAIL is based on applying the concept of indexical-functional variables <ref> (Agre & Chapman 1987, Schoppers & Shu 1990) </ref> to the preimage instances.
Reference: <author> Angluin, D. </author> <year> (1987), </year> <title> "Learning regular sets from queries and counterexamples", </title> <journal> Information and Computation 75(2), </journal> <volume> 87 - 106. </volume>
Reference: <author> Angluin, D. </author> <year> (1988), </year> <title> "Queries and concept learning", </title> <journal> Machine Learning 2(4), </journal> <volume> 319 - 342. </volume>
Reference: <author> Bates, J. </author> <year> (1992), </year> <title> "Edge of intention", </title> <booktitle> Presented at AAAI Arts Exhibition 1992, SigGraph 1993, Ars Electronica 1993, and the Boston Computer Museum. </booktitle>
Reference: <author> Bates, J. </author> <year> (1994), </year> <title> "The role of emotion in believable agents", </title> <journal> Communications of the ACM 37(7), </journal> <pages> 122-125. </pages>
Reference: <author> Baum, L., Petrie, T., Soules, G. & Weiss, N. </author> <year> (1970), </year> <title> "A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains", </title> <journal> Annals of Mathematical Statistics 41, </journal> <volume> 164 - 171. </volume>
Reference: <author> Bellman, R. E. </author> <year> (1962), </year> <title> Dynamic Programming, </title> <publisher> Princeton, </publisher> <address> NJ: </address> <publisher> Princeton University Press. </publisher>
Reference: <author> Benson, S. & Nilsson, N. </author> <year> (1995), </year> <title> Reacting, planning, and learning in an autonomous agent, </title> <editor> in K. Furukawa, D. Michie & S. Muggleton, eds, </editor> <booktitle> "Machine Intelligence 14", </booktitle> <publisher> Oxford: the Clarendon Press. </publisher>
Reference: <author> Bollinger, J. G. & Duffie, N. A. </author> <year> (1988), </year> <title> Computer Control of Machines and Processes, </title> <address> Reading, Massachusets: </address> <publisher> Addison-Wesley. </publisher>
Reference: <author> Bratko, I. & Muggleton, S. </author> <year> (1995), </year> <title> "Applications of inductive logic programming", </title> <journal> Communications of the ACM 38(11), </journal> <volume> 65 - 70. 181 182 BIBLIOGRAPHY Bratko, </volume> <editor> I., Urbancic, T. & Sammut, C. </editor> <year> (1995), </year> <title> Behavioural cloning: Phenomena, results, and problems, </title> <note> Unpublished. </note>
Reference: <author> Brooks, R. </author> <year> (1986), </year> <title> "A robust layered control system for a mobile robot", </title> <journal> IEEE Journal of Robotics and Automation RA-2(1), </journal> <pages> 14-23. </pages>
Reference: <author> Brooks, R. A. </author> <year> (1989a), </year> <title> Engineering approach to building complete, </title> <booktitle> intelligent beings, in "Proceedings of the SPIE The International Society for Optical Engineering", </booktitle> <volume> Vol. 1002, </volume> <pages> pp. 618 - 625. </pages>
Reference: <author> Brooks, R. A. </author> <year> (1989b), </year> <title> "A robot that walks; emergent behaviors from a carefully evolved network", </title> <booktitle> Neural Computation 1(2), </booktitle> <volume> 253 - 262. </volume>
Reference: <author> Cameron-Jones, R. & Quinlan, J. R. </author> <year> (1993), </year> <title> Avoiding pitfalls when learning recursive theories, </title> <editor> in R. Bajcsy, ed., </editor> <booktitle> "Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence", </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Cestnik, B. </author> <year> (1990), </year> <title> Estimating probabilities: A crucial task in machine learning, </title> <booktitle> in "Proceedings of the Ninth European Conference on Machine Learning", </booktitle> <pages> pp. 147 - 149. </pages>
Reference: <author> Christensen, J. </author> <year> (1990), </year> <title> A hierarchical planner that generates its own hierarchies, </title> <booktitle> in "AAAI-90: Proceedings of the Eighth National Conference on Artificial Intelligence", </booktitle> <publisher> AAAI Press / The MIT Press, </publisher> <pages> pp. 1004 - 1009. </pages>
Reference-contexts: For instance, a predicate with the meaning "Holding (?x)-operator-can-be-applied" is likely to be of use in building higher-level plans that might make use of our hierarchical Holding (?x) operator. This is similar to the idea of predicate relaxation used in the hierarchical planner PABLO <ref> (Christensen 1990) </ref>. Predicate relaxation, in brief, is a systematic method for weakening a condition so that it holds in states in which it can easily be made true as well as states in which it is already true.
Reference: <author> Christiansen, A. D. </author> <year> (1989), </year> <title> Automated acquisition of task theories for robotic manipulation, </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University. </institution>
Reference: <author> Christiansen, A. D. </author> <year> (1991), </year> <title> Manipulation planning from empirical backprojection, </title> <booktitle> in "Proceedings. 1991 IEEE International Conference on Robotics and Automation", </booktitle> <pages> pp. 762-768. </pages>
Reference: <author> Christiansen, A. D., Mason, M. T. & Mitchell, T. M. </author> <year> (1990), </year> <title> Learning reliable manipulation strategies without initial physical models, </title> <booktitle> in "Proceedings of the 1990 IEEE International Conference on Robotics and Automation", </booktitle> <volume> Vol. 2, </volume> <publisher> IEEE Computing Society Press, </publisher> <pages> pp. 1224-30. </pages>
Reference-contexts: A variety of other less related research work has also included methods of experimentation, including Mitchell's LEX system (Mitchell, Utgoff & Banerji 1983) and Christiansen's work on learning manipulation strategies for robotic graspers <ref> (Christiansen, Mason & Mitchell 1990, Christiansen 1989) </ref>. * Learning from Delayed Effects One of the main difficulties that plagued TRAIL in the flight simulator domain was an inability to reason about the delayed effects of actions.
Reference: <author> Clark, P. & Niblett, T. </author> <year> (1989), </year> <title> "The CN2 induction algorithm", Machine Learning 3(4), </title> <type> 261 - 283. BIBLIOGRAPHY 183 Connell, </type> <institution> J. </institution> <year> (1992), </year> <title> SSS: A hybrid architecture applied to robot navigation, </title> <booktitle> in "IEEE Conference on Robotics and Automation", </booktitle> <pages> pp. 2719 - 2724. </pages>
Reference-contexts: OVERVIEW OF EXISTING ILP ALGORITHMS 103 using attribute-value representations of instances. If the ILP learning problem can be transformed so that it is expressible in terms of boolean features, then standard machine learning algorithms such as C4.5 (Quinlan 1992) and CN2 <ref> (Clark & Niblett 1989) </ref> can be used to learn a set of rules that express a concept. <p> The one positive instance is most likely noise, but FOIL will have 20 (log 2 1024 + log 2 1024) bits with which to build a clause covering the instance. The attribute-value learning system CN2 <ref> (Clark & Niblett 1989) </ref> uses a covering algorithm essentially similar to FOIL, but does search using beam search rather than hill climbing. Literals are added to clauses according to an accuracy estimate known as the Laplace estimate.
Reference: <author> Dean, T. & Boddy, M. </author> <year> (1988), </year> <title> An analysis of time-dependent planning, </title> <booktitle> in "AAAI-88: Proceedings of the Seventh National Conference on Artificial Intelligence", American Association for Artificial Intelligence, </booktitle> <publisher> AAAI Press / The MIT Press, </publisher> <pages> pp. 49 - 54. </pages>
Reference-contexts: of low-level behaviors that keep the aircraft stable, even if it is not achieving any navigational goals. 2 Any autonomous agent in a real-time domain will presumably need such parallelism, although the current state may change significantly while planning is occurring, resulting in a significantly more difficult time-dependent planning problem <ref> (Dean & Boddy 1988) </ref>. The current TRAIL system, unfortunately, does not include such parallelism. The implementation of a parallel processing system for planning and acting in LISP is a considerable project, and not really related to the main aim of this thesis.
Reference: <author> DeJong, G. F. </author> <year> (1994), </year> <title> "Learning to plan in continuous domains", </title> <journal> Artificial Intelligence 65, </journal> <volume> 71 - 141. </volume>
Reference: <author> Dietterich, T. G. </author> <year> (1990), </year> <title> "Machine learning", </title> <booktitle> Annual Review of Computer Science 4, </booktitle> <volume> 255 - 306. </volume>
Reference: <author> Dolvsak, B., Bratko, I. & Jezernik, A. </author> <year> (1994), </year> <title> Finite-element mesh design: An engineering domain for ILP application, </title> <booktitle> in "Proceedings of the Fourth International Workshop on Inductive Logic Programming ILP-94", </booktitle> <address> Bad Honnef/Bonn. </address>
Reference-contexts: However, there are a number of more realistic applications where ILP may prove useful. A more detailed overview of these applications is found in Bratko and Muggleton (1995). One commonly cited application of ILP is the design of finite element meshes for analyzing stresses in physical structures <ref> (Dolvsak, Bratko & Jezernik 1994) </ref>. Given some physical structure that is to be modeled, the modeler needs to decide on how finely meshed the model will be in each part of the structure.
Reference: <author> Drescher, G. </author> <year> (1991), </year> <title> Made Up Minds: A Constructivist Approach to Artificial Intelligence, </title> <publisher> MIT Press. </publisher>
Reference-contexts: SUMMARY AND CONCLUSIONS in order to use the tree as a single operator in a high-level plan. A variety of work has been done on the subject of hierarchy learning for autonomous agents. Drescher's schema mechanism <ref> (Drescher 1991) </ref> develops complex actions for a tabula rasa autonomous agent, based on Piagetian learning theory. Ring (1991) combines actions to create operators using reinforcement learning.
Reference: <author> Dzeroski, S. </author> <year> (1995), </year> <title> Learning first-order clausal theories in the presence of noise, </title> <booktitle> in "Proceedings of the Fifth Scandinavian Conference on Artificial Intelligence", </booktitle> <pages> pp. 51 - 60. </pages>
Reference: <author> Dzeroski, S., Muggleton, S. & Russell, S. </author> <year> (1992), </year> <title> PAC learnability of determinate logic programs, </title> <booktitle> in "Proceedings of the Fifth ACM Workshop on Computational Learning Theory", </booktitle> <pages> pp. 128 - 135. </pages>
Reference-contexts: In most ILP domains, including preimage learning, a learner needs to be able to include new variables in the learned concepts, and LINUS is unable to do so. However, there is an extension of LINUS known as DINUS <ref> (Dzeroski et al. 1992) </ref> that uses the same 104 CHAPTER 5. LEARNING IN TRAIL USING ILP framework but does allow the introduction of new variables, using determinate literals with specified predicate modes.
Reference: <author> Dzeroski, S., Muggleton, S. & Russell, S. </author> <year> (1993), </year> <title> Learnability of constrained logic programs, </title> <booktitle> in "Proceedings of the European Conference on Machine Learning", </booktitle> <pages> pp. 342 - 347. </pages>
Reference-contexts: A good textbook introducing the field is Lavrac & Dzeroski (1994). Muggleton's (1992) book provides a useful collection of papers, although there have been considerable developments in the field since then, particularly in the area of theory <ref> (Frazier & Page 1993, Dzeroski, Mug-gleton & Russell 1993, Dzeroski 1995) </ref>. 5.3.1 FOIL The first widely publicized ILP learning algorithm was Quinlan's FOIL (1990).
Reference: <author> Dzeroski, S., Todorovski, L. & Urbancic, T. </author> <year> (1995), </year> <title> Handling real numbers in ILP: a step towards better behavioral clones, </title> <booktitle> in "Proceedings of the Eighth European Conference on Machine Learning", </booktitle> <pages> pp. 283 - 286. </pages>
Reference: <author> Etzioni, O. & Weld, D. </author> <year> (1994), </year> <title> "A softbot-based interface to the internet", </title> <journal> Communications of the ACM 37(7), </journal> <volume> 72 - 76. </volume>
Reference: <author> Fikes, R. E. & Nilsson, N. J. </author> <year> (1971), </year> <title> "STRIPS: A new approach to the application of theorem proving to problem solving", </title> <booktitle> Artificial Intelligence 2, </booktitle> <pages> 189-208. </pages> <note> 184 BIBLIOGRAPHY Fikes, </note> <author> R. E., Hart, P. E. & Nilsson, N. J. </author> <year> (1972), </year> <title> "Learning and executing generalized robot programs", </title> <journal> Artificial Intelligence 4, </journal> <volume> 251 - 288. </volume>
Reference: <author> Flynn, A. M. & Brooks, R. A. </author> <year> (1988), </year> <title> MIT mobile robots what's next?, </title> <booktitle> in "IEEE International Conference on Robotics and Automation", </booktitle> <pages> pp. 611 - 617. </pages>
Reference: <author> Frazier, M. & Page, C. </author> <year> (1993), </year> <title> Learnability in inductive logic programming: some basic results and techniques, </title> <booktitle> in "AAAI-93: Proceedings of the Eleventh National Conference on Artificial Intelligence", American Association for Artificial Intelligence, </booktitle> <publisher> AAAI Press / The MIT Press, </publisher> <pages> pp. 93 - 98. </pages>
Reference-contexts: A good textbook introducing the field is Lavrac & Dzeroski (1994). Muggleton's (1992) book provides a useful collection of papers, although there have been considerable developments in the field since then, particularly in the area of theory <ref> (Frazier & Page 1993, Dzeroski, Mug-gleton & Russell 1993, Dzeroski 1995) </ref>. 5.3.1 FOIL The first widely publicized ILP learning algorithm was Quinlan's FOIL (1990).
Reference: <author> Galles, D. </author> <year> (1993), </year> <title> Map building and following using teleo-reactive trees, </title> <booktitle> in "Intelligent Autonomous Systems: </booktitle> <address> IAS3", Washington: </address> <publisher> IOS Press, </publisher> <pages> pp. 390 - 398. </pages>
Reference: <author> Gil, Y. </author> <year> (1992), </year> <title> Acquiring Domain Knowledge for Planning by Experimentation, </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University. </institution>
Reference-contexts: Note that TRAIL does not have any similar mechanism for doing experimentation to correct overly specific operator preconditions. 4.5.2 Instance Generation in EXPO The example generation process in Gil's EXPO system <ref> (Gil 1992) </ref> can also be divided up into exploration, execution, and experimentation. When EXPO executes an operator, it observes the effects of the operator. If the operator has the expected set of effects, it is not changed. <p> This method is powerful enough to learn concepts such as the preimage of the copy TOP introduced in Section 5.2. However, CDL does not have any mechanism for handling noise, nor is there any obvious way of modifying it to do so. The learning algorithm in Gil's EXPO system <ref> (Gil 1992) </ref> also operates by a method of difference-finding. EXPO compares an observed negative instance to the most similar previously observed positive instance and computes the set of differences between them. It then does experimentation in the environment to determine which difference should be added to the precondition. <p> EXAMPLES AND EVALUATION repeated experimentation, but experimentation in the flight simulator has its own difficulties, as discussed below. * Experimentation in the domain is usually not very effective. In some domains, such as the delivery domain or the part-machining domain used for experimentation in EXPO <ref> (Gil 1992) </ref>, experiments can be carried out without drastically affecting the state of the system. However, in a real-time domain such as the flight simulator, the experiment itself can often have undesirable effects. <p> This is only one approach to the more general problem of intelligent exploration of unknown environments. Both LIVE (Shen 1994) and EXPO <ref> (Gil 1992) </ref> include significantly more experimentation than does TRAIL. LIVE does experimentation by applying operators that it believes to be faulty, using a new (essentially arbitrary) assignment of variables to objects, in hopes of discovering unexpected behavior.
Reference: <author> Hanks, S., Pollack, M. & Cohen, P. </author> <year> (1993), </year> <title> "Benchmarks, test beds, controlled experimentation, and the design of agent architectures", </title> <journal> AI Magazine 14(4), </journal> <pages> 17-42. </pages>
Reference: <author> Hayes-Roth, B. </author> <year> (1995), </year> <title> Agents on stage: Advancing the state of the art of AI, </title> <editor> in C. S. Mel-lish, ed., </editor> <booktitle> "Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence", </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <pages> pp. 967 - 971. </pages>
Reference: <author> Hertz, J., Krough, A. & Palmer, R. G. </author> <year> (1991), </year> <title> Introduction to the Theory of Neural Computation, </title> <institution> Santa Fe Institute Studies in the Sciences of Complexity, </institution> <address> Reading, Mas-sachusets: </address> <publisher> Addison-Wesley. </publisher>
Reference: <author> Iba, W., Wogulis, J. & Langley, P. </author> <year> (1988), </year> <title> Trading off simplicity and coverage in incremental concept learning, </title> <editor> in J. Laird, ed., </editor> <booktitle> "Proceedings of the Fifth International Conference on Machine Learning", </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 73 - 79. </pages>
Reference-contexts: Thus, our learner should be biased toward learning such preimages. There has been little work within the machine learning community on learning small disjunctions of conjunctions. HILLARY <ref> (Iba, Wogulis & Langley 1988) </ref> is an incremental algorithm that explicitly considers both simplicity and coverage in developing disjunctive concepts. HILLARY would be a promising algorithm for use in action-model learning, but we were unaware of it at the time we were developing TRAIL's learning algorithm. <p> This issue was examined in more detail in Section 5.5.4. Second, the current algorithm is non-incremental, so the preimage must be completely recomputed every time a new instance is generated. There is little existing work on incremental ILP algorithms, although the HILLARY <ref> (Iba et al. 1988) </ref> incremental learning algorithm has been applied to relational as well as attribute-value problems. A few early ILP systems such as MIS (Shapiro 1983) were incremental but relied on an oracle to answer membership queries.
Reference: <author> Jennings, N. & Wooldridge, M. </author> <year> (1996), </year> <title> "Software agents", </title> <journal> IEE Review 42(1), </journal> <volume> 17 - 20. </volume>
Reference: <author> John, G. H., Kohavi, R. & Pfleger, K. </author> <year> (1994), </year> <title> Irrelevant features and the subset selection problem, </title> <editor> in H. Hirsh & W. Cohen, eds, </editor> <booktitle> "Machine Learning: Proceedings of the Eleventh International Conference", </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 121-129. </pages> <note> BIBLIOGRAPHY 185 Kaelbling, </note> <author> L. P. & Rosenschein, S. </author> <year> (1990), </year> <title> "Action and planning in embedded agents", </title> <booktitle> Robotics and Autonomous Systems 6, </booktitle> <volume> 35 - 48. </volume>
Reference: <author> Kazakov, D., Popelinsky, L. & Stepankova, O. </author> <year> (1996), </year> <note> "Review of available ILP datasets", Available at http://www.gmd.de/ml-archive/datasets/ilp-res.html. </note>
Reference-contexts: EVALUATION METRICS FOR AUTONOMOUS LEARNING SYSTEMS 159 learning community (Murphy & Aha 1994), and more recently has begun to be done in the area of Inductive Logic Programming <ref> (Kazakov, Popelinsky & Stepankova 1996) </ref>. However, this type of testing would be misleading in at least two respects. First, the learning problems contained in the standardized databases may not be at all similar to the learning problems faced by TRAIL in computing action preimages.
Reference: <author> King, R., Sternberg, J. & Srinivasan, A. </author> <year> (1995), </year> <title> "Relating chemical activity to structure: an examination of ILP successes", </title> <journal> New Generation Computing 13(3-4), </journal> <volume> 411 - 433. </volume>
Reference: <author> Kohavi, R. & John, G. H. </author> <year> (1996), </year> <title> "Wrappers for feature subset selection", </title> <journal> Artificial Intelligence. </journal> <note> To Appear. </note>
Reference: <author> Korf, R. E. </author> <year> (1985), </year> <title> "Depth first iterative deepening: An optimal admissible tree search", </title> <booktitle> Artificial Intelligence 27(1), </booktitle> <pages> 97-109. </pages>
Reference: <author> Korf, R. E. </author> <year> (1988), </year> <title> Search: A survey of recent results, </title> <editor> in H. Shrobe, ed., </editor> <booktitle> "Exploring Artificial Intelligence", </booktitle> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kuipers, B. & Byun, Y.-T. </author> <year> (1991), </year> <title> "A robot exploration and mapping strategy based on a semantic hierarchy of spatial representations", </title> <booktitle> Robotics and Autonomous Systems 8(1-2), </booktitle> <volume> 47 - 63. </volume>
Reference: <author> Langley, P. </author> <year> (1996), </year> <title> Elements of Machine Laerning, </title> <address> San Mateo, California: </address> <publisher> Morgan Kauf-mann. </publisher>
Reference: <author> Lavrac, N. & Dzeroski, S. </author> <year> (1994), </year> <title> Inductive Logic Programming: Techniques and Applications, </title> <address> Chichester, England: </address> <publisher> Ellis Horwood. </publisher>
Reference-contexts: TRAIL can then use various concept learning techniques to estimate the preimage of the TOP. The exact format of these instances, and the methods that TRAIL uses to learn from them, are discussed beginning in Section 5.2. 5.1 Introduction to ILP Inductive Logic Programming <ref> (Lavrac & Dzeroski 1994, Muggleton 1992) </ref>, or ILP, is a specialized subarea of machine learning aimed at learning concepts in domains that are too complicated for traditional concept learning algorithms.
Reference: <author> Lavrac, N., Dzeroski, S. & Grobelnik, M. </author> <year> (1991), </year> <title> Learning nonrecursive definitions of relations with LINUS, </title> <booktitle> in "Proceedings of the Fifth European Working Session on Learning", </booktitle> <pages> pp. 265 - 281. </pages>
Reference-contexts: of deciding when to stop expanding the rlgg and begin a new one. (Note that this problem is very similar to the stopping problem in FOIL.) Therefore we elected to switch to a top-down learning algorithm for the latest version of TRAIL. 5.3.3 LINUS and DINUS The ILP system LINUS <ref> (Lavrac, Dzeroski & Grobelnik 1991) </ref> is based on the observation that there is already a large and well-developed literature on concept learning 5.3. OVERVIEW OF EXISTING ILP ALGORITHMS 103 using attribute-value representations of instances.
Reference: <author> Littman, M., Cassandra, A. & Kaelbling, L. P. </author> <year> (1995), </year> <title> Learning policies for partially observable environments: Scaling up, </title> <editor> in A. Prieditis & S. Russell, eds, </editor> <booktitle> "Machine Learning: Proceedings of the Twelfth International Conference", </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 362 - 369. </pages>
Reference: <author> Lozano-Perez, T., Mason, M. T. & Taylor, R. </author> <year> (1984), </year> <title> "Automatic synthesis of fine-motion strategies for robots", </title> <journal> International Journal of Robotics Research 3(1), </journal> <volume> 3 - 24. 186 BIBLIOGRAPHY Mahadevan, </volume> <editor> S. </editor> <year> (1992), </year> <title> Enhancing transfer in reinforcement learning by building stochastic models of robot actions, </title> <editor> in D. Sleeman & P. Edwards, eds, </editor> <booktitle> "Machine Learning: Proceedings of the Ninth International Workshop", </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 290 - 299. </pages>
Reference: <author> Mahadevan, S. & Connell, J. </author> <year> (1992), </year> <title> "Automatic programming of behavior-based robots using reinforcement learning", </title> <booktitle> Artificial Intelligence 55(2-3), </booktitle> <pages> 311-365. </pages>
Reference: <author> McCarthy, J. & Hayes, P. </author> <year> (1970), </year> <title> Some philosophical problems from the standpoint of artificial intelligence, </title> <editor> in B. Meltzer & D. Michie, eds, </editor> <booktitle> "Machine Intelligence 4", </booktitle> <publisher> Edinburgh: Edinburgh University Press, </publisher> <pages> pp. 463-502. </pages>
Reference-contexts: Figure 5.1, which shows the ILP conversion process for two of the delivery domain instances from Table 5.1, should help make the steps in this process clear. First, recall that each instance describes a state of the world. We reify this state, as in the situation calculus <ref> (McCarthy & Hayes 1970) </ref>, adding a state argument ST i to each literal in the description of instance S i . The literals describing each instance (with state arguments included) form the background knowledge for the ILP learning problem. Next, we need to introduce the predicate to be learned.
Reference: <author> Michalski, R. </author> <year> (1983), </year> <title> A theory and methodology of inductive learning, </title> <booktitle> in "Machine Learning, An Artificial Intelligence Approach, Volume I", </booktitle> <address> Palo Alto, California: </address> <publisher> Tioga Press. </publisher>
Reference-contexts: FOIL attempts to cover the positive instances in the foreground knowledge by generating a series of clauses via an approach similar to the AQ systems <ref> (Michalski 1983) </ref>. It begins with a set of labeled positive and negative instances and an empty hypothesis. In each cycle of the algorithm, a clause is generated that covers some of the positive instances and as few of the negative instances as possible. <p> TRAIL's learning mechanism is based on the covering algorithm used in AQ <ref> (Michalski 1983) </ref> and FOIL (Quinlan 1990).
Reference: <author> Minton, S. </author> <year> (1989), </year> <title> Selectively generalizing plans for problem solving, </title> <booktitle> in "Proceedings of the Ninth International Joint Conference on Artificial Intelligence", </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 596 - 599. </pages>
Reference-contexts: However, it has been observed that macro-operators can often increase rather than decrease the complexity of search by increasing the branching factor <ref> (Minton 1989) </ref>. * Predicate Invention As we noted several times within this thesis, TRAIL is very dependent on the initial set of predicates that are given to it. Every concept that it learns, and every tree that it constructs, must be in terms of these predicates.
Reference: <author> Mitchell, T. M. </author> <year> (1982), </year> <title> "Generalization as search", </title> <booktitle> Artificial Intelligence 18, </booktitle> <pages> 203-266. </pages>
Reference-contexts: OBSERVER also learns both from problem-solving traces provided by a domain expert and from its own planning and execution. Each of these processes generate instances, which are used to generate preconditions using a learning algorithm similar to version spaces <ref> (Mitchell 1982) </ref>. However, the instance generation process in OBSERVER is considerably simpler than the instance generation process in TRAIL. OBSERVER generates only positive instances from its observations of the expert. Each state in which an operator was applied by the expert corresponds to a positive instance of that operator. <p> It is also unable to deal with overly specific preconditions or noise in the effects of actions. Wang's OBSERVER system (Wang 1995b) learns preconditions by building a most general and most specific representation in a manner similar to that of the version space algorithm <ref> (Mitchell 1982) </ref>. The most specific representation is built by taking 5.8. CONCEPT LEARNING IN OTHER ACTION-MODEL LEARNERS 135 conditions which are common to all of the observed positive instances and replacing constants with variables.
Reference: <author> Mitchell, T. M., Keller, R. & Kedar-Cabelli, S. </author> <year> (1986), </year> <title> "Explanation-based generalization: A unifying view", </title> <journal> Machine Learning 1(1), </journal> <volume> 47 - 80. </volume>
Reference: <author> Mitchell, T., Utgoff, P. & Banerji, R. </author> <year> (1983), </year> <title> Learning by experimentation: acquiring and refining problem-solving heuristics, </title> <booktitle> in "Machine Learning, An Artificial Intelligence Approach, Volume I", </booktitle> <address> Palo Alto, California: </address> <publisher> Tioga Press. </publisher>
Reference-contexts: Both of these methods could potentially be added to the TRAIL architecture, and might well reduce its dependence on the teacher. A variety of other less related research work has also included methods of experimentation, including Mitchell's LEX system <ref> (Mitchell, Utgoff & Banerji 1983) </ref> and Christiansen's work on learning manipulation strategies for robotic graspers (Christiansen, Mason & Mitchell 1990, Christiansen 1989). * Learning from Delayed Effects One of the main difficulties that plagued TRAIL in the flight simulator domain was an inability to reason about the delayed effects of actions.
Reference: <author> Mooney, R. J. </author> <year> (1992), </year> <title> Batch versus incremental theory refinement, </title> <booktitle> in "Proceedings of AAAI Spring Symposium on Knowledge Acquisition". </booktitle>
Reference-contexts: A few early ILP systems such as MIS (Shapiro 1983) were incremental but relied on an oracle to answer membership queries. Some of the more recent work on theory refinement has been in the direction of incremental algorithms <ref> (Mooney 1992) </ref>. Finally, real world domains often have many more predicates than the domains described in this thesis.
Reference: <author> Moore, A. W. </author> <year> (1990), </year> <title> Acquisition of dynamic control knowledge for a robotic manipulator, </title> <editor> in B. Porter & R. Mooney, eds, </editor> <booktitle> "Proceedings of the Seventh International Conference on Machine Learning", </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 244 - 252. </pages>
Reference: <author> Moore, A. W. & Atkinson, C. G. </author> <year> (1993), </year> <title> "Prioritized sweeping: Reinforcement learning with less data and less real time", </title> <journal> Machine Learning 13(1), </journal> <volume> 103 - 130. </volume>
Reference: <author> Moravec, H. </author> <year> (1988), </year> <title> "Certainty grids for mobile robots", </title> <journal> AI Magazine 9(2), </journal> <volume> 61 - 74. BIBLIOGRAPHY 187 Muggleton, </volume> <editor> S. & Feng, C. </editor> <year> (1990), </year> <title> Efficient induction of logic programs, </title> <booktitle> in "Proceedings of the First Conference on Algorithm Learning Theory", </booktitle> <pages> pp. 368 - 381. </pages>
Reference: <editor> Muggleton, S., ed. </editor> <booktitle> (1992), Inductive Logic Programming, </booktitle> <address> San Diego, California: </address> <publisher> Academic Press. </publisher>
Reference-contexts: GOLEM was applied to this problem (which is an important unsolved problem in 5.4. AN INDEXICAL-FUNCTIONAL VIEW OF ILP 105 molecular biology) and achieved 81% success on their testing set <ref> (Muggleton, King & Sternberg 1992) </ref>, better than the previous best known result, which had used a neural network approach. A related problem is concerned with predicting the activity of molecules given their structure.
Reference: <author> Muggleton, S., King, R. & Sternberg, M. </author> <year> (1992), </year> <title> Protein secondary structure prediction using logic, </title> <booktitle> in "Proceedings of the Second International Workshop on Inductive Logic Programming". </booktitle>
Reference-contexts: GOLEM was applied to this problem (which is an important unsolved problem in 5.4. AN INDEXICAL-FUNCTIONAL VIEW OF ILP 105 molecular biology) and achieved 81% success on their testing set <ref> (Muggleton, King & Sternberg 1992) </ref>, better than the previous best known result, which had used a neural network approach. A related problem is concerned with predicting the activity of molecules given their structure.
Reference: <author> Murphy, P. M. & Aha, D. W. </author> <year> (1994), </year> <note> "UCI repository of machine learning databases", Available by anonymous ftp to ics.uci.edu in the pub/machine-learning-databases directory. </note>
Reference-contexts: EVALUATION METRICS FOR AUTONOMOUS LEARNING SYSTEMS 159 learning community <ref> (Murphy & Aha 1994) </ref>, and more recently has begun to be done in the area of Inductive Logic Programming (Kazakov, Popelinsky & Stepankova 1996). However, this type of testing would be misleading in at least two respects.
Reference: <author> Nilsson, N. J. </author> <year> (1980), </year> <booktitle> Principles of Artificial Intelligence, </booktitle> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Nilsson, N. J. </author> <year> (1984), </year> <title> Shakey the robot, </title> <type> Technical Report 323, </type> <institution> SRI International, Menlo Park, California. </institution>
Reference: <author> Nilsson, N. J. </author> <year> (1985), </year> <title> Triangle tables: A proposal for a robot programming language, </title> <type> Technical Report 347, </type> <institution> SRI International, Menlo Park, California. </institution>
Reference: <author> Nilsson, N. J. </author> <year> (1992), </year> <title> Towards agent programs with circuit semantics, </title> <type> Technical Report STAN-CS-92-1412, </type> <institution> Stanford University Computer Science Department, Stanford, Cal-ifornia. </institution>
Reference: <author> Nilsson, N. J. </author> <year> (1994), </year> <title> "Teleo-reactive programs for agent control", </title> <journal> Journal of Artificial Intelligence Research 1, </journal> <volume> 139 - 158. </volume>
Reference: <author> Peng, J. & Williams, R. J. </author> <year> (1993), </year> <title> "Efficient learning and planning within the Dyna framework", Adaptive Behavior 1(4), </title> <type> 437 - 454. </type>
Reference: <author> Plotkin, G. </author> <year> (1969), </year> <title> A note on inductive generalization, </title> <editor> in B. Meltzer & D. Michie, eds, </editor> <booktitle> "Machine Intelligence 5", </booktitle> <publisher> Edinburgh: Edinburgh University Press, </publisher> <pages> pp. 153 - 163. </pages>
Reference-contexts: This is known as a bottom-up approach to ILP, as opposed to the top-down approach of systems such as FOIL. In order to explain GOLEM, we need to explain the concept of Relative Least General Generalization (or rlgg) in more detail. The concept of lgg, or Least General Generalization <ref> (Plotkin 1969) </ref>, is well known in logic. Intuitively, the lgg of two clauses is the most specific clause such that for each clause, there is a substitution that transforms the lgg into a subset of that clause.
Reference: <author> Pollack, M. E. & Ringuette, M. </author> <year> (1990), </year> <title> Introducing the Tileworld: Experimentally evaluating agent architectures, </title> <booktitle> in "AAAI-90: Proceedings of the Eighth National Conference on Artificial Intelligence", </booktitle> <publisher> AAAI Press / The MIT Press, </publisher> <pages> pp. 183 - 189. </pages> <address> 188 BIBLIOGRAPHY Pomerleau, D. </address> <year> (1991), </year> <title> "Efficient training of artificial neural networks for autonomous navigation", </title> <booktitle> Neural Computation 3(1), </booktitle> <volume> 88 - 97. </volume>
Reference-contexts: Although a few testbeds for autonomous agents have been proposed, such as the Tileworld domain <ref> (Pollack & Ringuette 1990) </ref>, there is not yet any widespread agreement on a standard domain in which to evaluate autonomous systems. First, the goals and assumptions of various autonomous systems differ, making it difficult to construct benchmarks on which to measure them.
Reference: <author> Pomerleau, D. </author> <year> (1993), </year> <title> Neural Network Perception for Mobile Robot Guidance, </title> <address> Boston: </address> <publisher> Kluwer Academic Publishers. </publisher>
Reference: <author> Quinlan, J. R. </author> <year> (1986), </year> <title> "Induction of decision trees", </title> <booktitle> Machine Learning 1, </booktitle> <pages> 81-106. </pages>
Reference-contexts: This process repeats until the algorithm decides it has covered sufficiently many of the positive instances. TRAIL uses a similar covering algorithm, discussed in Section 5.5.1. The added clauses themselves are generated using a greedy information-based search similar to that used in other machine learning algorithms, particularly ID3 <ref> (Quinlan 1986) </ref>. The learner begins the clause search process with an empty clause, which by definition covers all of the instances.
Reference: <author> Quinlan, J. R. </author> <year> (1990), </year> <title> "Learning logical definitions from relations", </title> <journal> Machine Learning 5(3), </journal> <volume> 239 - 266. </volume>
Reference-contexts: TRAIL's learning mechanism is based on the covering algorithm used in AQ (Michalski 1983) and FOIL <ref> (Quinlan 1990) </ref>.
Reference: <author> Quinlan, J. R. </author> <year> (1991), </year> <title> "Knowledge acquisition from structured data using determinate literals to assist search", </title> <journal> IEEE Expert 6(6), </journal> <volume> 32 - 37. </volume>
Reference-contexts: This is the idea of mode declarations of predicates, as used in GOLEM (Muggleton & Feng 1990) and FOIL2.0 <ref> (Quinlan 1991) </ref>.
Reference: <author> Quinlan, J. R. </author> <year> (1992), </year> <title> C4.5: Programs for Machine Learning, </title> <address> San Mateo, California: </address> <publisher> Mor-gan Kaufmann. </publisher>
Reference-contexts: OVERVIEW OF EXISTING ILP ALGORITHMS 103 using attribute-value representations of instances. If the ILP learning problem can be transformed so that it is expressible in terms of boolean features, then standard machine learning algorithms such as C4.5 <ref> (Quinlan 1992) </ref> and CN2 (Clark & Niblett 1989) can be used to learn a set of rules that express a concept.
Reference: <author> Rabiner, L. </author> <year> (1990), </year> <title> A tutorial on hidden Markov models and selected applications in speech recognition, </title> <editor> in A. Waibel & K.-F. Lee, eds, </editor> <booktitle> "Readings in Speech Recognition", </booktitle> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Rabiner, L. R. & Juang, B. H. </author> <year> (1986), </year> <title> "An introduction to hidden Markov models", </title> <journal> IEEE Acoustics, Speech, and Signal Processing Magazine pp. </journal> <volume> 4 - 16. </volume> <month> January, </month> <year> 1986. </year>
Reference: <author> Ring, M. </author> <year> (1991), </year> <title> Incremental development of complex behaviors through automatic construction of sensory-motor hierarchies, </title> <editor> in L. A. Birnbaum & G. C. Collins, eds, </editor> <booktitle> "Machine Learning: Proceedings of the Eighth International Workshop", </booktitle> <publisher> Morgan Kauf-mann, </publisher> <pages> pp. 343 - 347. </pages>
Reference: <author> Rivest, R. & Schapire, R. </author> <year> (1993), </year> <title> "Inference of finite automata using homing sequences", </title> <journal> Information and Computation 103(2), </journal> <volume> 299 - 347. </volume>
Reference: <author> Rogers, S. O. & Laird, J. E. </author> <year> (1996), </year> <title> Symbolic performance and learning in complex environments, </title> <booktitle> in "AAAI-96: Proceedings of the Thirteenth National Conference on Artificial Intelligence", American Association for Artificial Intelligence, </booktitle> <publisher> AAAI Press / The MIT Press, </publisher> <address> p. </address> <note> 1405. abstract; longer version available. </note>
Reference: <author> Russell, S. & Norvig, P. </author> <year> (1995), </year> <title> Artificial Intelligence: A Modern Approach, </title> <publisher> Prentice Hall. BIBLIOGRAPHY 189 Sablon, G. </publisher> <year> (1994), </year> <type> Personal Communication. </type>
Reference: <author> Sablon, G. & Bruynooghe, M. </author> <year> (1994), </year> <title> Using the event calculus to integrate planning and learning in an intelligent autonomous agent, </title> <editor> in C. Backstrom & E. Sandewall, eds, </editor> <booktitle> "Current Trends in AI Planning", </booktitle> <publisher> IOS Press, </publisher> <pages> pp. 254 - 265. </pages>
Reference: <author> Sammut, C., Hurst, S., Kedzier, D. & Michie, D. </author> <year> (1992), </year> <title> Learning to fly, </title> <editor> in D. Sleeman & P. Edwards, eds, </editor> <booktitle> "Machine Learning: Proceedings of the Ninth International Workshop", </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 385 - 393. </pages>
Reference: <author> Schoppers, M. J. </author> <year> (1987), </year> <title> Universal plans for reactive robots in unpredictable environments, </title> <booktitle> in "AAAI-87: Proceedings of the Sixth National Conference on Artificial Intelligence", </booktitle> <publisher> AAAI Press / The MIT Press, </publisher> <pages> pp. 1039-1046. </pages>
Reference-contexts: Furthermore, the theories constructed by ILP were more understandable than those created by attribute-value methods. 5.4 An Indexical-Functional View of ILP The concept learning algorithm in TRAIL is based on applying the concept of indexical-functional variables <ref> (Agre & Chapman 1987, Schoppers & Shu 1990) </ref> to the preimage instances.
Reference: <author> Schoppers, M. J. & Shu, R. </author> <year> (1990), </year> <title> An implementation of indexical-functional reference for embedded execution of symbolic plans, in "DARPA Workshop on Innovative Approaches to Planning, Scheduling, and Control". </title>
Reference: <author> Shapiro, E. </author> <year> (1983), </year> <title> Algorithmic Program Debugging, </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: There is little existing work on incremental ILP algorithms, although the HILLARY (Iba et al. 1988) incremental learning algorithm has been applied to relational as well as attribute-value problems. A few early ILP systems such as MIS <ref> (Shapiro 1983) </ref> were incremental but relied on an oracle to answer membership queries. Some of the more recent work on theory refinement has been in the direction of incremental algorithms (Mooney 1992). Finally, real world domains often have many more predicates than the domains described in this thesis.
Reference: <author> Shavlik, J. W. </author> <year> (1990), </year> <title> "Acquiring recursive and iterative concepts with explanation-based learning", </title> <journal> Machine Learning 5(1), </journal> <volume> 39 - 70. </volume>
Reference: <author> Shell, P. & Carbonell, J. </author> <year> (1989), </year> <title> Towards a general framework for composing disjunctive and iterative macro-operators, </title> <editor> in N. Sridharan, ed., </editor> <booktitle> "Proceedings of the Eleventh International Joint Conference on Artificial Intelligence", </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 596 - 602. </pages>
Reference-contexts: Ring (1991) combines actions to create operators using reinforcement learning. Triangle tables (Fikes, Hart & Nilsson 1972, Nilsson 1985) are an early method of macro-operator creation, while more recent work has focused on more flexible macro-operators involving iteration and recursion <ref> (Shell & Car-bonell 1989, Shavlik 1990) </ref>.
Reference: <author> Shen, W.-M. </author> <year> (1989), </year> <title> Learning from the Environment Based on Actions and Percepts, </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University. </institution>
Reference: <author> Shen, W.-M. </author> <year> (1990), </year> <title> Complimentary discrimination learning: A duality between generalization and discrimination, </title> <booktitle> in "Proceedings of National Conference on Artificial Intelligence", </booktitle> <publisher> MIT Press, </publisher> <pages> pp. 834 - 839. </pages>
Reference-contexts: Aside from this system, most concept learning research has focused on learning decision trees or sets of rules, which do not generally translate well to action-model preimages. Shen's CDL algorithms <ref> (Shen 1990) </ref> appear to learn small preconditions effectively within his LIVE system, but do not provide any 124 CHAPTER 5. LEARNING IN TRAIL USING ILP noise handling mechanism. <p> Therefore, each of these systems used a learning algorithm which could deal in some way with structured instances. Shen's LIVE system (Shen 1994) uses his CDL algorithm <ref> (Shen 1990) </ref> to learn preconditions of operators. The CDL algorithm operates by examining each instance individually. If the current concept classifies it correctly, no learning is done. Otherwise, the algorithm finds some difference between the concept and the misclassified instance and appends this difference to the concept description.
Reference: <author> Shen, W.-M. </author> <year> (1994), </year> <title> Autonomous Learning from the Environment, </title> <publisher> Computer Science Press, W.H. Freeman and Company. 190 BIBLIOGRAPHY Shoham, </publisher> <editor> Y. & Goyal, N. </editor> <year> (1988), </year> <title> Temporal reasoning, </title> <editor> in H. Shrobe, ed., </editor> <booktitle> "Exploring Artificial Intelligence", </booktitle> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This section discusses the instance generation process in these four systems and their differences from TRAIL. 4.5.1 Instance Generation in LIVE In Shen's LIVE system <ref> (Shen 1994) </ref>, instance generation can be roughly divided into three sources: exploration, execution, and experimentation. Since the LIVE environments are generally STRIPS-like, action traces correspond exactly to the STRIPS-like execution traces discussed in Section 4.2, and learning is done based on the differences between successive states. <p> Thus, upon completion of the basic preimage learning algorithm discussed above, TRAIL will attempt to specialize the preimage condition by adding literals that refer to the TOP variables. (These literals correspond to the inner circle relations in LIVE <ref> (Shen 1994) </ref>.) However, it should only do so as long as these literals hold in all of the positive instances that are currently covered by the preimage condition. <p> Therefore, each of these systems used a learning algorithm which could deal in some way with structured instances. Shen's LIVE system <ref> (Shen 1994) </ref> uses his CDL algorithm (Shen 1990) to learn preconditions of operators. The CDL algorithm operates by examining each instance individually. If the current concept classifies it correctly, no learning is done. <p> A new predicate can be hypothesized that holds only in those states in which the action is successful. (This idea is the basis for new feature construction within LIVE <ref> (Shen 1994) </ref>.) However, unless the new feature can easily be explained in terms of past actions or world states, it can be difficult to determine whether the new feature holds in a given state, and thus difficult to use the new feature when planning or acting. <p> This is only one approach to the more general problem of intelligent exploration of unknown environments. Both LIVE <ref> (Shen 1994) </ref> and EXPO (Gil 1992) include significantly more experimentation than does TRAIL. LIVE does experimentation by applying operators that it believes to be faulty, using a new (essentially arbitrary) assignment of variables to objects, in hopes of discovering unexpected behavior.
Reference: <author> Shoykhet, A. </author> <year> (1996), </year> <title> Fuzzy t-r trees, </title> <institution> Stanford University undergraduate project report. </institution>
Reference-contexts: SUMMARY AND CONCLUSIONS Instead, we could think of the actions in the TR tree as being determined by fuzzy predicates with varying degrees of truth. In a fuzzy TR tree <ref> (Shoykhet 1996) </ref>, as the bot approaches the midline, the OnM idline node gradually becomes true.
Reference: <author> Silverstein, G. & Pazzani, M. J. </author> <year> (1991), </year> <title> Relational cliches: Constraining constructive induction during relational learning, </title> <editor> in L. A. Birnbaum & G. C. Collins, eds, </editor> <booktitle> "Machine Learning: Proceedings of the Eighth International Workshop", </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 203 - 207. </pages>
Reference-contexts: Finally, new features may be constructed from conjunctions of predicates that frequently appear together in plans. FOCL <ref> (Silverstein & Pazzani 1991) </ref> is a version of FOIL that attempts to solve the literal connectedness problem by inventing new predicates that are combinations of existing predicates. Conjunctions of predicates may have another important use in TRAIL as well, as TRAIL learns TOPs only for single postconditions.
Reference: <author> Stahl, I. </author> <year> (1993), </year> <title> Predicate invention in ILP an overview, </title> <booktitle> in "Machine Learning: EMCL-93. European Conference on Machine Learning Proceedings", </booktitle> <pages> pp. 313 - 322. </pages>
Reference-contexts: Thus, if there is a hidden variable that is needed for learning, or if the agent wishes to use higher-level predicates for hierarchical planning, it must be able to make use of some form of predicate invention. Predicate invention is already used in a number of ILP systems <ref> (Stahl 1993) </ref> although most of these methods are not directly applicable to TRAIL. One obvious source of invented predicates lies in the learned preimages of TOPs.
Reference: <editor> Tate, A., Hendler, J. & Drummond, M. </editor> <year> (1990), </year> <title> A review of AI planning techniques, </title> <editor> in J. Allen, J. Hendler & A. Tate, eds, </editor> <booktitle> "Readings in Planning", </booktitle> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Teo, P. </author> <year> (1992), </year> <note> Botworld, unpublished manual. </note>
Reference: <author> Urbancic, T. & Bratko, I. </author> <year> (1994), </year> <title> Reconstructing human skill with machine learning, </title> <booktitle> in "Proceedings of the 11th European Conference on Artificial Intelligence", </booktitle> <publisher> John Wiley & Sons, </publisher> <pages> pp. 498 - 502. </pages>
Reference: <author> Valiant, L. G. </author> <year> (1984), </year> <title> "A theory of the learnable", </title> <journal> Communications of the ACM 27, </journal> <pages> 1134-1142. </pages>
Reference: <author> Wang, X. </author> <year> (1994), </year> <title> Learning planning operators by observation and practice, </title> <editor> in K. Hammond, ed., </editor> <booktitle> "Proceedings of the Second International Conference on AI Planning Systems", </booktitle> <publisher> AAAI Press, </publisher> <pages> pp. 335-341. </pages>
Reference: <author> Wang, X. </author> <year> (1995a), </year> <type> Personal Communication. </type>
Reference: <author> Wang, X. </author> <year> (1995b), </year> <title> Learning by observation and practice: An incremental approach for planning operator acquisition, </title> <editor> in A. Prieditis & S. Russell, eds, </editor> <booktitle> "Machine Learning: Proceedings of the Twelfth International Conference", </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 549 - 557. </pages>
Reference-contexts: INSTANCE GENERATION IN TRAIL action using a twist drill bit instead. 4.5.3 Instance Generation in OBSERVER Wang's OBSERVER system <ref> (Wang 1995b) </ref> is probably the most similar to TRAIL in terms of learning methodology. OBSERVER also learns both from problem-solving traces provided by a domain expert and from its own planning and execution. <p> It is also unable to deal with overly specific preconditions or noise in the effects of actions. Wang's OBSERVER system <ref> (Wang 1995b) </ref> learns preconditions by building a most general and most specific representation in a manner similar to that of the version space algorithm (Mitchell 1982). The most specific representation is built by taking 5.8.
Reference: <author> Wang, X. & Carbonell, J. </author> <year> (1994), </year> <title> "Learning by observation and practice: Towards real applications of planning systems", AAAI Fall Symposium on Planning and Learning: On to Real Applications. BIBLIOGRAPHY 191 Watkins, </title> <address> C. </address> <year> (1989), </year> <title> Learning from Delayed Rewards, </title> <type> PhD thesis, </type> <institution> Cambridge University. Psychology Department. </institution>
Reference: <author> Whitehead, S. D. & Ballard, D. H. </author> <year> (1990), </year> <title> Active perception and reinforcement learning, </title> <editor> in B. Porter & R. Mooney, eds, </editor> <booktitle> "Proceedings of the Seventh International Conference on Machine Learning", </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 179-188. </pages>
Reference: <author> Yamauchi, B. & Langley, P. </author> <year> (1996), </year> <title> Place recognition in dynamic real-world environments, </title> <booktitle> in "Proceedings of ROBOLEARN-96: International Workshop for Learning in Autonomous Robots". </booktitle>
References-found: 108

