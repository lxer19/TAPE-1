URL: http://robotics.stanford.edu/users/jek/Papers/sfi93.ps.gz
Refering-URL: http://robotics.stanford.edu/users/jek/bio.html
Root-URL: http://www.cs.stanford.edu
Email: jek@cs.stanford.edu  
Title: Emergent Conventions and the Structure of Multi-Agent Systems  
Author: James E. Kittock 
Note: To appear in: Available as: http://robotics.stanford.edu/people/jek/Papers/sfi93.ps This research was supported in part by grants from the Advanced Research Projects Agency and the Air Force Office of Scientific Research.  
Address: Stanford University  
Affiliation: Robotics Laboratory  
Abstract: This paper examines the emergence of conventions through "co-learning" in a model multi-agent system. Agents interact through a two-player game, receiving feedback according to the game's payoff matrix. The agent model specifies how agents use this feedback to choose a strategy from the possible strategies for the game. A global structure, represented as a graph, restricts which agents may interact with one another. Results are presented from experiments with two different games and a range of global structures. We find that for a given game, the choice of global structure has a profound effect on the evolution of the system. We give some preliminary analytical results and intuitive arguments to explain why the systems behave as they do and suggest directions of further study. Finally, we briefly discuss the relationship of these systems to work in computer science, economics, and other fields. Lynn Nadel and Daniel Stein, editors, 1993 Lectures in Complex Systems: the proceedings of the 1993 Complex Systems Summer School, Santa Fe Institute Studies in the Sciences of Complexity Lecture Volume VI. Santa Fe Institute, Addison-Wesley Publishing Co., 1994. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Axelrod. </author> <title> The Evolution of Cooperation. </title> <address> New York: </address> <publisher> Basic Books, </publisher> <year> 1984. </year>
Reference-contexts: goal here is for the agents to reach a convention with strategy C, which indicates the agents are all cooperating. 1 Not to be confused with the graph-theoretic radius, which is something else altogether. 5 The prisoner's dilemma has been examined extensively, in particular by Ax--elrod:Evolution in his classic book <ref> [1] </ref>. The relationship between previous work with prisoner's dilemma and the present experiments will be briefly discussed in Section 5. 2.2.3 Strategy Selection Once the agent model and game are specified, our final step in defining the system is to determine how agents choose the strategy that they will use. <p> Readers familiar with the Prisoner's Dilemma and its treatment in game theory and economics have probably noticed that our approach is markedly different. Our emphasis on feedback-based learning techniques violates some of the basic assumptions of economic cooperation theory <ref> [1, 6] </ref>. In particular, we do not allow for any meta-reasoning by agents; that is, our agents do not have access to the payoff matrix and thus can only make decisions based on their experience. Furthermore, agents do not know the specific source of their feedback.
Reference: [2] <author> Bernardo Huberman and Tad Hogg. </author> <title> The behavior of computational ecologies. </title> <editor> In Bernardo Huberman, editor, </editor> <booktitle> The Ecology of Computation. </booktitle> <publisher> Elsevier Science Publishers B.V., </publisher> <year> 1988. </year>
Reference-contexts: There are also links to statistical mechanics, which are exploited more thoroughly in other models of multi-agent systems which have been called "computational ecologies" <ref> [2] </ref>.
Reference: [3] <author> M. Kandori, G. Mailath, and R. Rob. </author> <title> Learning, mutation and long equilibria in games. </title> <journal> Econometrica, </journal> <volume> 61 </volume> <pages> 29-56, </pages> <year> 1993. </year>
Reference-contexts: Each time two agents interact, they receive feedback as specified by the payoff matrix. It is this feedback the agents will use to select their action the next time they interact. Systems similar to this have been referred to as iterated games <ref> [3, 6, 7] </ref>. Each task has a corresponding two-person iterated game. In this paper, we will consider two different games, which represent the distinct goals of "coordination" and "cooperation." Our modification to the co-learning setting is the addition of an interaction graph which limits agent interactions. <p> We examine two games, the iterated cooperation game (ICG) and the iterated prisoner's dilemma (IPD). ICG A +1; +1 1; 1 IPD C +2; +2 6; +6 Table 1: Payoff matrices for coordination game and prisoner's dilemma. ICG is a "pure coordination" game <ref> [3] </ref>, with two possible strategies, labelled A and B. When agents with identical strategies meet, they get positive feedback, and when agents with different strategies meet, they get negative feedback. The payoff matrix is specified in Table 1. <p> In some respects, this may limit our systems, but it also allows for a more general approach to learning in a dynamic environment. The current interest in economics with "bounded rationality" has led to some work which is closer in spirit to our model <ref> [3] </ref>. The systems discussed in this paper (and multi-agent systems in general) are also related to various other dynamical systems.
Reference: [4] <author> Andrea Schaerf, Moshe Tennenholtz, and Yoav Shoham. </author> <title> Adaptive load balancing: a study in co-learning. </title> <type> Draft manuscript, </type> <year> 1993. </year>
Reference-contexts: This leads to another possibly fruitful avenue of investigation: systems of substantially more sophisticated agents. Schaerf et al 12 have used co-learning with a more sophisticated learning rule to investigate load balancing without central control in a model multi-agent system <ref> [4] </ref>. Our present framework also has ties to theoretical computer science, especially when we view either individual agents or the entire system as finite state machines. Readers familiar with the Prisoner's Dilemma and its treatment in game theory and economics have probably noticed that our approach is markedly different.
Reference: [5] <author> Yoav Shoham and Moshe Tennenholtz. </author> <title> Emergent conventions in multi-agent systems: initial experimental results and observations. </title> <booktitle> In KR-92, </booktitle> <year> 1992. </year>
Reference-contexts: Indeed, it appears that conventions are generally necessary in multi-agent systems: conventions reduce the potential for conflict and help ensure that agents can achieve their goals in an orderly, efficient manner. In <ref> [5] </ref>, Shoham and Tennenholtz introduced the notion of emergent conventions. In contrast with conventions which might be designed into agents' behavior or legislated by a central authority, emergent conventions are the result of the behavioral decisions of individual agents based on feedback from local interactions. <p> In the original study of emergent conven 2 tions, any pair of agents could interact <ref> [5] </ref>; we will restrict this by only allowing interactions between agents which are adjacent on the interaction graph. Our primary objective is to explore the effects of this global structure on the behavior of the system. <p> We will only consider the two 3 strategy case; similar systems with more than two strategy choices are discussed in <ref> [5] </ref>. The memory of agent k, M k , is of maximum size , where is the memory size, a parameter of the agent model. An agent's memory is conveniently thought of as a set, each element of which is a feedback event. <p> We introduce the concept of an interaction graph to specify which agents can interact with one another, and we use the payoff matrix of a two-player game to determine agents' feedback. 2.2.1 Interaction Graph In other, similar models, it was possible for any pair of agents to interact <ref> [5, 6] </ref>. To explore the effects of incomplete mixing of agents, we specify an interaction graph, I, which has N vertices, each representing one of the agents in the system. <p> However, for systems where the convergence over time is not generally monotonic, this measure is effectively meaningless. There are, of course, other possible measures of performance, such as probability of achieving a fixed convergence after a fixed time (used in <ref> [5, 6] </ref>) and maximum convergence achieved in a fixed amount of time.
Reference: [6] <author> Yoav Shoham and Moshe Tennenholtz. </author> <title> Co-learning and the evolution of social activity. </title> <note> Submitted for publication, </note> <year> 1993. </year>
Reference-contexts: In contrast with conventions which might be designed into agents' behavior or legislated by a central authority, emergent conventions are the result of the behavioral decisions of individual agents based on feedback from local interactions. Shoham and Tennenholtz extended this idea into a more general framework, dubbed co-learning <ref> [6] </ref>. In the co-learning paradigm, agents acquire experience through interactions with the world, and use that experience to guide their future course of action. A distinguishing characteristic of co-learning is that each agent's environment consists (at least in part) of the other agents in the system. <p> Thus, in order for agents to adapt to their environment, they must adapt to one another's behavior. Here, we describe a modification of the co-learning framework as presented in <ref> [6] </ref> and examine its effects on the emergence of conventions in a model multi-agent system. Simulation Model We assume that most tasks that an agent might undertake can only be performed in a limited number of ways; actions are thus chosen from a finite selection of strategies. <p> Each time two agents interact, they receive feedback as specified by the payoff matrix. It is this feedback the agents will use to select their action the next time they interact. Systems similar to this have been referred to as iterated games <ref> [3, 6, 7] </ref>. Each task has a corresponding two-person iterated game. In this paper, we will consider two different games, which represent the distinct goals of "coordination" and "cooperation." Our modification to the co-learning setting is the addition of an interaction graph which limits agent interactions. <p> We introduce the concept of an interaction graph to specify which agents can interact with one another, and we use the payoff matrix of a two-player game to determine agents' feedback. 2.2.1 Interaction Graph In other, similar models, it was possible for any pair of agents to interact <ref> [5, 6] </ref>. To explore the effects of incomplete mixing of agents, we specify an interaction graph, I, which has N vertices, each representing one of the agents in the system. <p> In these experiments we use a version of the Highest Current Reward (HCR) strategy update rule <ref> [6] </ref>. 2 The current reward for a strategy is the total remembered feedback for using that strategy, i.e. for strategy the current reward is the sum of f (m) for all feedback events m in the agent's memory such that s (m) = . <p> Thus, we have different notions of convergence for the two systems. We define C t , the convergence of a system at time t, as follows. For ICG, the 2 It should be noted that the present definition of memory is slightly different than that found in <ref> [6] </ref>. There, memory was assumed to record a fixed amount of "time" during which an agent might interact many, few, or no times. <p> However, for systems where the convergence over time is not generally monotonic, this measure is effectively meaningless. There are, of course, other possible measures of performance, such as probability of achieving a fixed convergence after a fixed time (used in <ref> [5, 6] </ref>) and maximum convergence achieved in a fixed amount of time. <p> For IPD, the story is different. Using the HCR rule and working with a system equivalent to our IPD system on K N , Shoham and Tennenholtz write, "[HCR] is hopeless in the cooperation setting" <ref> [6] </ref>. They had discovered what we see in the number of agents. HCR is redeemed somewhat by its performance with IPD on C N , which appears to be polynomial, and is possibly linear. <p> As a final note, Shoham and Tennenholtz have proven a general lower bound of N log N on the convergence time of systems such as these <ref> [6] </ref>, which appears to contradict our assertion that T 90% appears to be proportional to N . However, in the general case T 100% need not be proportional to T 90% , because the final stages of convergence might be much less likely to take place. <p> Readers familiar with the Prisoner's Dilemma and its treatment in game theory and economics have probably noticed that our approach is markedly different. Our emphasis on feedback-based learning techniques violates some of the basic assumptions of economic cooperation theory <ref> [1, 6] </ref>. In particular, we do not allow for any meta-reasoning by agents; that is, our agents do not have access to the payoff matrix and thus can only make decisions based on their experience. Furthermore, agents do not know the specific source of their feedback. <p> There are also links to statistical mechanics, which are exploited more thoroughly in other models of multi-agent systems which have been called "computational ecologies" [2]. For a more thorough discussion of the relationship of the present framework to other complex dynamic systems, see <ref> [6] </ref>. 6 Conclusion We have seen that the proper global structure is required if conventions are to arise successfully in our model multi-agent system, and that this optimal structure depends upon the nature of the interactions in the agent society.
Reference: [7] <author> Karl Sigmund. </author> <title> Games of Life: Explorations in Ecology, Evolution, and Be-haviour. </title> <publisher> Oxford University Press, </publisher> <year> 1993. </year>
Reference-contexts: Each time two agents interact, they receive feedback as specified by the payoff matrix. It is this feedback the agents will use to select their action the next time they interact. Systems similar to this have been referred to as iterated games <ref> [3, 6, 7] </ref>. Each task has a corresponding two-person iterated game. In this paper, we will consider two different games, which represent the distinct goals of "coordination" and "cooperation." Our modification to the co-learning setting is the addition of an interaction graph which limits agent interactions.
Reference: [8] <author> Steven Skiena. </author> <title> Implementing Discrete Mathematics. </title> <publisher> Addison-Wesley Publishing Co., </publisher> <year> 1990. </year>
Reference-contexts: For 2 ffi &lt; b N 2 c, the vertex degree is fixed at four, and a variety of diameters result (we can measure the diameter of each graph using, e.g. Dijkstra's algorithm <ref> [8] </ref>). Once we have measured the performance of ICG on each graph D N;ffi , we can plot performance against diameter, as seen in Figure 4. We see that there is a correlation between the diameter of an interaction graph and the performance of an ICG system on that graph.
References-found: 8

