URL: http://www.cs.purdue.edu/homes/li/draft/LHJ.ps.gz
Refering-URL: http://www.cs.purdue.edu/homes/li/publications.html
Root-URL: http://www.cs.purdue.edu
Email: E-mail:li@cs.purdue.edu  E-mail: huangj@cs.umn.edu  E-mail: jin@cs.rice.edu  
Title: Page-Mapping Techniques to Reduce Cache Conflicts on CC-NUMA Multiprocessors  
Author: Zhiyuan Li Jian Huang Guohua Jin 
Keyword: CC-NUMA multiprocessor, data-task affinity, page mapping, private cache  
Address: West Lafayette, IN 47907,USA  Minneapolis, MN 55455, USA  Houston, TX 77005  
Affiliation: Department of Computer Sciences Purdue University,  Department of Computer Science and Engineering University of Minnesota,  Department of Computer Science Rice University,  
Abstract: Page-coloring and bin-hopping are two well-known page mapping schemes for reducing cache conflicts. Previous work found bin-hopping to have 4% less cache miss rate than page-coloring on uniprocessor machines. Using execution-driven simulations, we find that bin-hopping significantly outperforms simplistic page-coloring on CC-NUMA multiprocessors. In certain cases, bin-hopping has 32% to 58% less execution time and over 60% fewer cache misses. By using part of the memory ID bits to hash the page color during page-mapping, we improve the performance of page-coloring to match that of bin-hopping on CC-NUMA multiprocessors. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Agarwal A et al., </author> <title> The MIT Alewife Machine: A Large-Scale Distributed-Memory Multiprocessor, </title> <type> Technical Report 454, </type> <institution> MIT/LCS, </institution> <year> 1991. </year>
Reference-contexts: 1 Introduction Cache-Coherent Non-Uniform Memory Access (CC-NUMA) multiprocessors become increasingly attractive as an architecture which provides a transparent access to local and remote memories and a good scalability. Stanford DASH [10] and FLASH [9], MIT Alewife <ref> [1] </ref>, University of Toronto NUMAchine [19], and Sun's S3.mp [13], as well as commercial products including the Sequent STiNG [12], Hewlett-Packard SPP, and Silicon Graphics Origin 2000 [16]. A CC-NUMA machine has a number of nodes connected by an interconnection network.
Reference: [2] <author> Agarwal A, Kranz D and Natarajan V, </author> <title> Automatic partitioning of parallel loops and data arrays for distributed shared memory multiprocessors, </title> <booktitle> in: Proc. International Conference on Parallel Processing, Volume I: Architecture, </booktitle> <address> St. Charles, IL, </address> <year> 1993, </year> <journal> 2-11. </journal> <volume> 16 17 18 19 20 </volume>
Reference-contexts: To reduce remote memory references in the event of cache misses, the compiler should also try to align data allocation with the tasks. These two issues have been studied extensively recently <ref> [2, 3, 7, 11, 14] </ref>. The success of such attempts by the compiler depends on the degree of data-task affinity that can be found in a given program. <p> For certain applications with regular data sets, the compiler can often find opportunities to allocate the data in such a way that the processors tend to find the required data in their respective local memories <ref> [2, 3, 7, 11, 14] </ref>. In order for such opportunities to materialize, the compiler must be able to present its MID decision to the OS and the OS must honor such a decision.
Reference: [3] <author> Anderson J and Lam M S, </author> <title> Global optimizations for parallelism and locality on scalable parallel machines, </title> <booktitle> in: Proc. ACM SIGPLAN Conf. on Prog. Lang. Design and Imp., </booktitle> <month> June </month> <year> 1993, </year> <pages> 112-125, </pages> <address> Cambridge, Massachusetts. </address>
Reference-contexts: To reduce remote memory references in the event of cache misses, the compiler should also try to align data allocation with the tasks. These two issues have been studied extensively recently <ref> [2, 3, 7, 11, 14] </ref>. The success of such attempts by the compiler depends on the degree of data-task affinity that can be found in a given program. <p> For certain applications with regular data sets, the compiler can often find opportunities to allocate the data in such a way that the processors tend to find the required data in their respective local memories <ref> [2, 3, 7, 11, 14] </ref>. In order for such opportunities to materialize, the compiler must be able to present its MID decision to the OS and the OS must honor such a decision.
Reference: [4] <author> Bugnion E, Anderson J, Mowry T, Rosenblum M and Lam M S, </author> <title> Compiler-directed page coloring for multiprocessors, </title> <booktitle> in: Proc. of the 7th Int. Sym. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1996, </year> <institution> Cambridge, Massachusetts. </institution>
Reference-contexts: Previous works examine this issue primarily for uniprocessors and multiprocessors with uniform memory access (UMA) and they show that careful page placement by the OS can reduce cache set conflicts considerably <ref> [4, 8] </ref>. This paper examines the page placement issue in the context of 2 CC-NUMA machines, a relatively new multiprocessor architecture, where the MID decision comes into play. <p> This paper examines the page placement issue in the context of 2 CC-NUMA machines, a relatively new multiprocessor architecture, where the MID decision comes into play. Various page-mapping techniques have been proposed in the past, including page-coloring, bin-hopping, best-bin, hierarchical method [8], compiler-assisted page-coloring <ref> [4] </ref> and dynamic re-mapping [15]. Page-coloring and bin-hopping are simple and hence the most popular ones. Silicon Graphics Inc., adopts page-coloring scheme in its products, while DEC ships OSF/1 with bin-hopping.
Reference: [5] <author> Chen T and Baer J L, </author> <title> Reducing Memory Latency via Non-blocking and Prefetching Caches, </title> <booktitle> in: Proc. of the Fifth Int'l Symposium on Architectural Support for Programming Languages and Operating System, </booktitle> <month> October </month> <year> 1992, </year> <institution> Boston, Massachusetts. </institution>
Reference-contexts: We simulate a CC-NUMA system of 16 nodes. Each node has two levels of non-blocking cache, the on-chip 16-KB level-one (L1) cache, and the off-chip 2-MB level-two (L2) cache. Level-two caches of different nodes are kept coherent based on a write-invalidate policy. Inclusion property <ref> [5] </ref> is maintained and all writes to the L1 cache are written through to the L2 cache at the same time. An eight-entry write-buffer is used to hold the write-contents if the corresponding write-operation is a miss in L2 cache, and program control is then returned to the CPU.
Reference: [6] <author> Gu J, Li Z and Lee G, </author> <title> Experience with efficient array data-flow analysis for array privatization, </title> <booktitle> in: Proc. of Sixth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> June </month> <year> 1997, </year> <pages> 157-167, </pages> <address> Las Vagas, Nevada. </address>
Reference-contexts: We have performed a number of experiments to examine the behavior of page-coloring and bin-hopping, as well as the effectiveness of hashing the page-color with MID bits, on a CC-NUMA machine. 3 Experimental Setup Each program in our experiments is parallelized and instrumented by the Panorama interprocedural parallelizing compiler <ref> [6] </ref>. The output program is then compiled by SGI's f77 compiler for an Origin 2000 machine running the IRIX 6.2 OS. The -O3 optimization flag is set for the compiler. The executable object code is then fed to our multi-processor simulator (NUMAsim).
Reference: [7] <author> Kennedy K and Kremer U, </author> <title> Automatic data layout for High Performance Fortran, </title> <booktitle> in: Proc. Supercomputing '95, December 1995, </booktitle> <address> San Diego, California. </address>
Reference-contexts: To reduce remote memory references in the event of cache misses, the compiler should also try to align data allocation with the tasks. These two issues have been studied extensively recently <ref> [2, 3, 7, 11, 14] </ref>. The success of such attempts by the compiler depends on the degree of data-task affinity that can be found in a given program. <p> For certain applications with regular data sets, the compiler can often find opportunities to allocate the data in such a way that the processors tend to find the required data in their respective local memories <ref> [2, 3, 7, 11, 14] </ref>. In order for such opportunities to materialize, the compiler must be able to present its MID decision to the OS and the OS must honor such a decision.
Reference: [8] <author> Kessler R E and Hill M D, </author> <title> Page Placement Algorithms for Large Real-Indexed Caches, </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 10(4), </volume> <month> November </month> <year> 1992, </year> <pages> 338-359. </pages>
Reference-contexts: Previous works examine this issue primarily for uniprocessors and multiprocessors with uniform memory access (UMA) and they show that careful page placement by the OS can reduce cache set conflicts considerably <ref> [4, 8] </ref>. This paper examines the page placement issue in the context of 2 CC-NUMA machines, a relatively new multiprocessor architecture, where the MID decision comes into play. <p> This paper examines the page placement issue in the context of 2 CC-NUMA machines, a relatively new multiprocessor architecture, where the MID decision comes into play. Various page-mapping techniques have been proposed in the past, including page-coloring, bin-hopping, best-bin, hierarchical method <ref> [8] </ref>, compiler-assisted page-coloring [4] and dynamic re-mapping [15]. Page-coloring and bin-hopping are simple and hence the most popular ones. Silicon Graphics Inc., adopts page-coloring scheme in its products, while DEC ships OSF/1 with bin-hopping. <p> Page-coloring and bin-hopping are simple and hence the most popular ones. Silicon Graphics Inc., adopts page-coloring scheme in its products, while DEC ships OSF/1 with bin-hopping. Previous experiments find these two schemes perform comparably on uniprocessor machines, with bin-hopping delivering 4% fewer cache misses than page-coloring <ref> [8] </ref>. We make two contributions studying these two schemes in this paper. First, using an execution-driven CC-NUMA machine simulator, we find that bin-hopping can outperform page-coloring scheme by over 70% in terms of miss rates and by 10 to 45% in terms of execution time. <p> These cache-sets together are called a cache-bin and the part of SI which defines the cache-bin is considered as the color, or the cache-bin ID of the page <ref> [8] </ref>. Obviously, there exists a certain number of physical pages which have the same color in a given system. If a particular color is overly used, then excessive cache-set conflicts may result. A high performance computer, especially a modern shared-memory multiprocessor, usually has a large physical memory. <p> From the data, we observe that page-coloring with zero replacing bits has an inferior performance to all other schemes. In almost all cases, bin-hopping with a global bin-ID delivers the best performance among the studied alternative schemes. Compared with previous work <ref> [8] </ref> which studies careful page mapping on uniprocessors, the difference between page-coloring and bin-hopping on CC-NUMA machines is quite striking, even though different test programs are used. On uniprocessors, the difference in the miss rates are reported to be no more than 4% for the two mapping schemes.
Reference: [9] <author> Kuskin J et al., </author> <title> The Stanford FLASH multiprocessor, </title> <booktitle> in: Proc. Int. Sym. on Computer Architecture, </booktitle> <year> 1994, </year> <pages> 302-313, </pages> <address> Chicago, Illinois. </address>
Reference-contexts: 1 Introduction Cache-Coherent Non-Uniform Memory Access (CC-NUMA) multiprocessors become increasingly attractive as an architecture which provides a transparent access to local and remote memories and a good scalability. Stanford DASH [10] and FLASH <ref> [9] </ref>, MIT Alewife [1], University of Toronto NUMAchine [19], and Sun's S3.mp [13], as well as commercial products including the Sequent STiNG [12], Hewlett-Packard SPP, and Silicon Graphics Origin 2000 [16]. A CC-NUMA machine has a number of nodes connected by an interconnection network.
Reference: [10] <author> Lenoski D et al, </author> <title> The Stanford DASH Multiprocessor, </title> <journal> Computer, </journal> <volume> 25(3), </volume> <month> March </month> <year> 1992, </year> <pages> 63-79. </pages>
Reference-contexts: 1 Introduction Cache-Coherent Non-Uniform Memory Access (CC-NUMA) multiprocessors become increasingly attractive as an architecture which provides a transparent access to local and remote memories and a good scalability. Stanford DASH <ref> [10] </ref> and FLASH [9], MIT Alewife [1], University of Toronto NUMAchine [19], and Sun's S3.mp [13], as well as commercial products including the Sequent STiNG [12], Hewlett-Packard SPP, and Silicon Graphics Origin 2000 [16]. A CC-NUMA machine has a number of nodes connected by an interconnection network.
Reference: [11] <author> Li W and Pingali K, </author> <title> Access Normalization: Loop Restructuring for NUMA Computers, </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 11(4), </volume> <month> November </month> <year> 1993. </year>
Reference-contexts: To reduce remote memory references in the event of cache misses, the compiler should also try to align data allocation with the tasks. These two issues have been studied extensively recently <ref> [2, 3, 7, 11, 14] </ref>. The success of such attempts by the compiler depends on the degree of data-task affinity that can be found in a given program. <p> For certain applications with regular data sets, the compiler can often find opportunities to allocate the data in such a way that the processors tend to find the required data in their respective local memories <ref> [2, 3, 7, 11, 14] </ref>. In order for such opportunities to materialize, the compiler must be able to present its MID decision to the OS and the OS must honor such a decision.
Reference: [12] <author> Lovette T and Clapp R, STiNG: </author> <title> A CC-NUMA computer system for the commercial marketplace, </title> <booktitle> in: Proc. Int. Sym. on Computer Architecture, </booktitle> <year> 1996, </year> <pages> 308-317, </pages> <address> Philadelphia, Pennsylvania. </address>
Reference-contexts: Stanford DASH [10] and FLASH [9], MIT Alewife [1], University of Toronto NUMAchine [19], and Sun's S3.mp [13], as well as commercial products including the Sequent STiNG <ref> [12] </ref>, Hewlett-Packard SPP, and Silicon Graphics Origin 2000 [16]. A CC-NUMA machine has a number of nodes connected by an interconnection network. Each node consists of one or a few processors, a private cache hierarchy, and a local memory module.
Reference: [13] <editor> Nowatzyk A et al., </editor> <booktitle> The S3.mp scalable shared memory multiprocessor, in: Proc. Int. Sym. on Computer Architecture, 1995, </booktitle> <address> Santa Margherita Ligure, Italy. </address>
Reference-contexts: 1 Introduction Cache-Coherent Non-Uniform Memory Access (CC-NUMA) multiprocessors become increasingly attractive as an architecture which provides a transparent access to local and remote memories and a good scalability. Stanford DASH [10] and FLASH [9], MIT Alewife [1], University of Toronto NUMAchine [19], and Sun's S3.mp <ref> [13] </ref>, as well as commercial products including the Sequent STiNG [12], Hewlett-Packard SPP, and Silicon Graphics Origin 2000 [16]. A CC-NUMA machine has a number of nodes connected by an interconnection network. Each node consists of one or a few processors, a private cache hierarchy, and a local memory module.
Reference: [14] <author> Nguyen T N and Li Z, </author> <title> Interprocedural Analysis for Loops Scheduling and Data Allocation, </title> <booktitle> Parallel Computing, Special Issue on Languages and Compilers for Parallel Computers, </booktitle> <volume> 24(3), </volume> <year> 1998, </year> <pages> pp. 477-504. </pages>
Reference-contexts: To reduce remote memory references in the event of cache misses, the compiler should also try to align data allocation with the tasks. These two issues have been studied extensively recently <ref> [2, 3, 7, 11, 14] </ref>. The success of such attempts by the compiler depends on the degree of data-task affinity that can be found in a given program. <p> For certain applications with regular data sets, the compiler can often find opportunities to allocate the data in such a way that the processors tend to find the required data in their respective local memories <ref> [2, 3, 7, 11, 14] </ref>. In order for such opportunities to materialize, the compiler must be able to present its MID decision to the OS and the OS must honor such a decision. <p> The associativity of the level-one cache is fixed as 2-way in our experiments. The Panorama compiler uses a data-task co-allocation scheme <ref> [14] </ref> to align the data with tasks. (We inspect the alignment decisions to see their qualities.) The Panorama compiler then instruments the FORTRAN source code by inserting directives to identify the starting and ending address of an array and to specify the data allocation decisions.
Reference: [15] <author> Romer T, Lee D, Bershad B and Chen J, </author> <title> Dynamic page-mapping policies for cache conflict resolution on standard hardware, </title> <booktitle> in: Proc. of the First Symposium on Operating System Design and Implementation, </booktitle> <address> Nov. 94, Monterey, California. </address>
Reference-contexts: This paper examines the page placement issue in the context of 2 CC-NUMA machines, a relatively new multiprocessor architecture, where the MID decision comes into play. Various page-mapping techniques have been proposed in the past, including page-coloring, bin-hopping, best-bin, hierarchical method [8], compiler-assisted page-coloring [4] and dynamic re-mapping <ref> [15] </ref>. Page-coloring and bin-hopping are simple and hence the most popular ones. Silicon Graphics Inc., adopts page-coloring scheme in its products, while DEC ships OSF/1 with bin-hopping. Previous experiments find these two schemes perform comparably on uniprocessor machines, with bin-hopping delivering 4% fewer cache misses than page-coloring [8].
Reference: [16] <author> Silicon Graphics, Inc., </author> <title> Origin and Onyx2 Programmer's Reference Manual. Document Number: </title> <address> 007-3410-001, </address> <year> 1996. </year>
Reference-contexts: Stanford DASH [10] and FLASH [9], MIT Alewife [1], University of Toronto NUMAchine [19], and Sun's S3.mp [13], as well as commercial products including the Sequent STiNG [12], Hewlett-Packard SPP, and Silicon Graphics Origin 2000 <ref> [16] </ref>. A CC-NUMA machine has a number of nodes connected by an interconnection network. Each node consists of one or a few processors, a private cache hierarchy, and a local memory module. <p> Other than these constraints, the MID positions in the physical address and the virtual address is quite flexible, although it must be fixed on the given hardware and the OS. Current CC-NUMA multiprocessors, such as SGI's Origin 2000 <ref> [16] </ref> and Toronto's NUMAchine [19], place the MID at the highest bits in the physical address.
Reference: [17] <institution> Standard Performance Evaluation Corporation, </institution> <type> SPEC Newsletter, Vols. </type> <pages> 1-9, 1989-1997. 21 </pages>
Reference-contexts: If a processor makes a sequence of references to these elements across the memory modules, cache conflicts can be severe. The page-coloring scheme behaves poorly in this regard. Consider an example from program Ora in the SPEC benchmarks <ref> [17] </ref>. Ora has a parallel loop at the outmost level whose iterations can be executed by multiple processors simultaneously. Several arrays are referenced in that loop with subscripted-subscripts.
Reference: [18] <author> Veenstra J E and Fowler R J, </author> <title> MINT Tutorial and User Manual, </title> <type> Technical Report 452, </type> <institution> Dep. of Computer Science, University of Rochester, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: This technique divides an n-iteration parallel loop into chunks of n/p iterations for p processors. Each chunk is then assigned to a processor. Our experiments showed that other scheduling schemes are inferior to simple scheduling for the selected benchmark programs. NUMAsim is an execution-driven multi-processor simulator based on MINT <ref> [18] </ref>. However, modifications are made to support multiple-issue, out-of-order execution and weak-ordered memory consistency. The key parameters of our processor model and component latency are summarized in Table 2.
Reference: [19] <author> Vranesic Z et al., </author> <title> The NUMAchine Multiprocessor. </title> <type> Technical Report CSRI-324, </type> <institution> Computer Systems Research Institute, University of Toronto, </institution> <year> 1995. </year> <month> 22 </month>
Reference-contexts: 1 Introduction Cache-Coherent Non-Uniform Memory Access (CC-NUMA) multiprocessors become increasingly attractive as an architecture which provides a transparent access to local and remote memories and a good scalability. Stanford DASH [10] and FLASH [9], MIT Alewife [1], University of Toronto NUMAchine <ref> [19] </ref>, and Sun's S3.mp [13], as well as commercial products including the Sequent STiNG [12], Hewlett-Packard SPP, and Silicon Graphics Origin 2000 [16]. A CC-NUMA machine has a number of nodes connected by an interconnection network. <p> Other than these constraints, the MID positions in the physical address and the virtual address is quite flexible, although it must be fixed on the given hardware and the OS. Current CC-NUMA multiprocessors, such as SGI's Origin 2000 [16] and Toronto's NUMAchine <ref> [19] </ref>, place the MID at the highest bits in the physical address.
References-found: 19

