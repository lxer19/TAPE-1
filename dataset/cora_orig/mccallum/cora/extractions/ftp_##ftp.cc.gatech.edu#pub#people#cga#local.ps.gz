URL: ftp://ftp.cc.gatech.edu/pub/people/cga/local.ps.gz
Refering-URL: http://www.cs.gatech.edu/fac/Chris.Atkeson/publications.html
Root-URL: 
Email: cga@ai.mit.edu  
Phone: 617-253-0788,  
Title: Using Local Trajectory Optimizers To Speed Up Global Optimization In Dynamic Programming  
Author: Christopher G. Atkeson 
Address: NE43-771 545 Technology Square, Cambridge, MA 02139  
Affiliation: Department of Brain and Cognitive Sciences and the Artificial Intelligence Laboratory Massachusetts Institute of Technology,  
Abstract: Dynamic programming provides a methodology to develop planners and controllers for nonlinear systems. However, general dynamic programming is computationally intractable. We have developed procedures that allow more complex planning and control problems to be solved. We use second order local trajectory optimization to generate locally optimal plans and local models of the value function and its derivatives. We maintain global consistency of the local models of the value function, guaranteeing that our locally optimal plans are actually globally optimal, up to the resolution of our search procedures. Learning to do the right thing at each instant in situations that evolve over time is difficult, as the future cost of actions chosen now may not be obvious immediately, and may only become clear with time. Value functions are a representational tool that makes the consequences of actions explicit. Value functions are difficult to learn directly, but they can be built up from learned models of the dynamics of the world and the cost function. This paper focuses on how fast optimizers that only produce locally optimal answers can play a useful role in speeding up the process of computing or learning a globally optimal value function. Consider a system with dynamics x k+1 = f(x k ; u k ) and a cost function L(x k ; u k ), where x is the state of the system and u is a vector of actions or controls. The subscript k serves as a time index, but will be dropped in the equations that follow. A goal of reinforcement learning and optimal control is to find a policy that minimizes the total cost, which is the sum of the costs for each time step. One approach to doing this is to construct an optimal 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bellman, R., </author> <title> (1957) Dynamic Programming, </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ. </address>
Reference: <author> Bertsekas, </author> <title> D.P., (1987) Dynamic Programming: Deterministic and Stochastic Models, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ. </address>
Reference: <author> Dyer, P. and S.R. McReynolds, </author> <title> (1970) The Computation and Theory of Optimal Control, </title> <publisher> Academic Press, </publisher> <address> New York, NY. </address>
Reference-contexts: In what follows we describe how to find these locally optimal trajectories and also how to find the globally optimal trajectory. 2 2 LOCAL TRAJECTORY OPTIMIZATION We base our local optimization process on dynamic programming within a tube surrounding our current best estimate of a locally optimal trajectory <ref> (Dyer and McReynolds 1970, Ja-cobson and Mayne 1970) </ref>.
Reference: <author> Jacobson, D.H. and D.Q. Mayne, </author> <title> (1970) Differential Dynamic Programming, </title> <publisher> Elsevier, </publisher> <address> New York, NY. </address>
Reference: <author> Larson, R.E., </author> <title> (1968) State Increment Dynamic Programming, </title> <publisher> Elsevier, </publisher> <address> New York, NY. </address> <month> 8 </month>
Reference-contexts: Larson's state increment dynamic programming <ref> (Larson 1968) </ref> is a good example of this type of approach.
References-found: 5

