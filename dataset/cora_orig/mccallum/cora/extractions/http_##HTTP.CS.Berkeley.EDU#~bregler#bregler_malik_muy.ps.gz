URL: http://HTTP.CS.Berkeley.EDU/~bregler/bregler_malik_muy.ps.gz
Refering-URL: http://www.cs.gatech.edu/computing/classes/cs7322_98_spring/readings.html
Root-URL: 
Email: bregler@cs.berkeley.edu, malik@cs.berkeley.edu  
Title: Video Motion Capture  
Author: Christoph Bregler Jitendra Malik 
Keyword: CR Categories: Keywords: Computer Vision, Animation, Motion Capture, Visual Tracking, Twist Kinematics, Exponential Maps, Muybridge  
Address: Berkeley, CA 94720-1776  
Affiliation: Computer Science Division University of California, Berkeley  
Web: UCB//CSD-97-973, http://www.cs.berkeley.edu/bregler/digmuy.html  
Abstract: This paper demonstrates a new vision based motion capture technique that is able to recover high degree-of-freedom articulated human body configurations in complex video sequences. It does not require any markers, body suits, or other devices attached to the subject. The only input needed is a video recording of the person whose motion is to be captured. For visual tracking we introduce the use of a novel mathematical technique, the product of exponential maps and twist motions, and its integration into a differential motion estimation. This results in solving simple linear systems, and enables us to recover robustly the kinematic degrees-of-freedom in noise and complex self occluded configurations. We demonstrate this on several image sequences of people doing articulated full body movements, and visualize the results in re-animating an artificial 3D human model. We are also able to recover and reanimate the famous movements of Eadweard Muybridge's motion studies from the last century. To the best of our knowledge, this is the first computer vision based system that is able to process such challenging footage and recover complex motions with such high accuracy. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Basu, I.A. Essa, </author> <title> and A.P. Pentland. Motion regularization for model-based head tracking. </title> <booktitle> In International Conference on Pattern Recognition, </booktitle> <year> 1996. </year>
Reference-contexts: Based on the re-warped image we compute the new image gradients (3). Repeating this process is equivalent to a Newton-Raphson style minimization. A convenient representation of the shape of an image region is a probability mask w (x; y) 2 <ref> [0; 1] </ref>. w (x; y) = 1 declares that pixel (x; y) is part of the region. <p> be represented as a rigid body transformation in &lt; 3 using homogeneous coordinates (we will use the notation from [14]): q c = G q o with G = 2 4 r 2;1 r 2;2 r 2;3 d y 0 0 0 1 7 2 UCB//CSD-97-973, http://www.cs.berkeley.edu/bregler/digmuy.html q o = <ref> [x o ; y o ; z o ; 1] </ref> T is a point in the object frame and q c = [x c ; y c ; z c ; 1] T is the corresponding point in the camera frame. <p> = G q o with G = 2 4 r 2;1 r 2;2 r 2;3 d y 0 0 0 1 7 2 UCB//CSD-97-973, http://www.cs.berkeley.edu/bregler/digmuy.html q o = [x o ; y o ; z o ; 1] T is a point in the object frame and q c = <ref> [x c ; y c ; z c ; 1] </ref> T is the corresponding point in the camera frame. <p> Euler angles are commonly used to constrain the rotation matrix to SO (3), but they suffer from singularities and don't lead to a simple formulation in the optimization procedure (for example <ref> [1] </ref> propose a 3D ellipsoidal tracker based on Euler angles). In contrast, the twist representation provides a more elegant solution [14] and leads to a very simple linear representation of the motion model.
Reference: [2] <author> J.R. Bergen, P. Anandan, K.J. Hanna, and R. Hingorani. </author> <title> Hierarchical model-based motion estimation. </title> <booktitle> In ECCV, </booktitle> <pages> pages 237252, </pages> <year> 1992. </year>
Reference-contexts: In the following we will introduce a new region based differential technique that is tailored to articulated objects modeled by kinematic chains. We will first review a commonly used motion estimation framework <ref> [2, 19] </ref>, and then show how this can be extended for our task, using the twist and product of exponential formulation [14]. 3.1 Preliminaries Assuming that changes in image intensity are only due to translation of local image intensity, a parametric image motion between consecutive time frames t and t + <p> For the case that the motion model is linear (as in the affine case), we can write the set of equations in matrix form (see <ref> [2] </ref> for details): H + ~z = ~ 0 (4) where H 2 &lt; NfiK , and ~z 2 &lt; N . The least squares solution to (3) is: T 1 T Because (4) is the first-order Taylor series linearization of (1), we linearize around the new solution and iterate.
Reference: [3] <author> M.J. Black and P. Anandan. </author> <title> The robust estimation of multiple motions: Parametric and piecewise-smooth flow fields. Computer Vision and Image Understanding, </title> <address> 63(1):75104, </address> <month> Jan </month> <year> 1996. </year>
Reference-contexts: Such a support map usually covers a larger region, including pixels from the environment. That distracts the exact motion measurement. Robust statistics would be one solution to this problem <ref> [3] </ref>. Another solution is an EM-based layered representation [6, 9]. It is beyond the scope of this paper to describe this method in detail, but we would like to outline the method briefly: We start with an initial guess of the support map (ellipsoidal projection in this case).
Reference: [4] <author> G. Cameron, A. Bustanoby, K. Cope, S. Greenberg, C. Hayes, and O. Ozoux. </author> <title> Panel on motion capture and cg character animation. </title> <booktitle> SIGGRAPH 97, </booktitle> <pages> pages 442445, </pages> <year> 1997. </year>
Reference-contexts: Motion capture occupies an important role in the creation of special effects. Its application to CG character animation has been much more controversial; SIGGRAPH 97 featured a lively panel debate <ref> [4] </ref> between its proponents and opponents. Our goal in this paper is not to address that debate. Rather we take it as a given that motion capture, like any other technology, can be correctly or incorrectly applied and we are merely extending its possibilities.
Reference: [5] <author> L. Concalves, E.D. Bernardo, E. Ursella, and P. Perona. </author> <title> Monocular tracking of the human arm in 3d. </title> <booktitle> In Proc. Int. Conf. Computer Vision, </booktitle> <year> 1995. </year>
Reference-contexts: Higher degree-of-freedom articulated hand configurations are tracked by Regh and Kanade [17], full body configurations by Gravrila and Davis [7], and arm configurations by Kakadiaris and Metaxas [11] and by Goncalves and Perona <ref> [5] </ref>. All these approaches are demonstrated in constrained environments with high contrast edge boundaries. In most cases this is achieved by uniform backgrounds, and skintight clothing of uniform color. Also, in order to estimate 3D configurations, a camera calibration is needed.
Reference: [6] <author> A.P. Dempster, N.M. Laird, and D.B. Rubin. </author> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society B, </journal> <volume> 39, </volume> <year> 1977. </year>
Reference-contexts: Such a support map usually covers a larger region, including pixels from the environment. That distracts the exact motion measurement. Robust statistics would be one solution to this problem [3]. Another solution is an EM-based layered representation <ref> [6, 9] </ref>. It is beyond the scope of this paper to describe this method in detail, but we would like to outline the method briefly: We start with an initial guess of the support map (ellipsoidal projection in this case).
Reference: [7] <author> D.M. Gavrila and L.S. Davis. </author> <title> Towards 3-d model-based tracking and recognition of human movement: a multi-view approach. </title> <booktitle> In Proc. of the Int. Workshop on Automatic Face- and Gesture-Recognition, </booktitle> <address> Zurich, </address> <year> 1995, 1995. </year>
Reference-contexts: Both systems are specialized to one degree-of-freedom walking models. Edge and line features are extracted from images and matched to a cylindrical 3D human body model. Higher degree-of-freedom articulated hand configurations are tracked by Regh and Kanade [17], full body configurations by Gravrila and Davis <ref> [7] </ref>, and arm configurations by Kakadiaris and Metaxas [11] and by Goncalves and Perona [5]. All these approaches are demonstrated in constrained environments with high contrast edge boundaries. In most cases this is achieved by uniform backgrounds, and skintight clothing of uniform color.
Reference: [8] <author> D. Hogg. </author> <title> A program to see a walking person. </title> <journal> Image Vision Computing, </journal> <volume> 5(20), </volume> <year> 1983. </year>
Reference-contexts: Marker-free visual tracking on video recordings of human bodies goes back to work by Hogg and by Rohr <ref> [8, 18] </ref>. Both systems are specialized to one degree-of-freedom walking models. Edge and line features are extracted from images and matched to a cylindrical 3D human body model.
Reference: [9] <author> A. Jepson and M.J. Black. </author> <title> Mixture models for optical flow computation. </title> <booktitle> In Proc. IEEE Conf. Computer VIsion Pattern Recognition, </booktitle> <pages> pages 760761, </pages> <address> New York, </address> <year> 1993. </year>
Reference-contexts: Such a support map usually covers a larger region, including pixels from the environment. That distracts the exact motion measurement. Robust statistics would be one solution to this problem [3]. Another solution is an EM-based layered representation <ref> [6, 9] </ref>. It is beyond the scope of this paper to describe this method in detail, but we would like to outline the method briefly: We start with an initial guess of the support map (ellipsoidal projection in this case).
Reference: [10] <author> S.X. Ju, M.J.Black, and Y.Yacoob. </author> <title> Cardboard people: A parameterized model of articulated motion. </title> <booktitle> In 2nd Int. Conf. on Automatic Face- and Gesture-Recognition, Killington, Ver-mont, </booktitle> <pages> pages 3844, </pages> <year> 1996. </year>
Reference-contexts: Also, in order to estimate 3D configurations, a camera calibration is needed. Alternatively, Weng et. al demonstrated how to track full bodies with color features [20], and Ju et. al showed motion based tracking of leg configurations <ref> [10] </ref>. No 3D kinematic chain models were used in the last two cases.
Reference: [11] <author> I.A. Kakadiaris and D. Metaxas. </author> <title> Model-based estimation of 3d human motion with occlusion based on active multi-viewpoint selection. </title> <booktitle> In CVPR, </booktitle> <year> 1996. </year>
Reference-contexts: Edge and line features are extracted from images and matched to a cylindrical 3D human body model. Higher degree-of-freedom articulated hand configurations are tracked by Regh and Kanade [17], full body configurations by Gravrila and Davis [7], and arm configurations by Kakadiaris and Metaxas <ref> [11] </ref> and by Goncalves and Perona [5]. All these approaches are demonstrated in constrained environments with high contrast edge boundaries. In most cases this is achieved by uniform backgrounds, and skintight clothing of uniform color. Also, in order to estimate 3D configurations, a camera calibration is needed.
Reference: [12] <author> B.D. Lucas and T. Kanade. </author> <title> An iterative image registration technique with an application to stereo vision. </title> <booktitle> Proc. 7th Int. Joinnt Conf. on Art. Intell., </booktitle> <year> 1981. </year>
Reference-contexts: [a 1 ; a 2 ; a 3 ; a 4 ; d x ; d y ] T is defined as u (x; y; ) = a 1 a 2 y + d x The first-order Taylor series expansion of (1) leads to the com monly used gradient formulation <ref> [12] </ref>: I t (x; y) + [I x (x; y); I y (x; y)] u (x; y; ) = 0 (3) I t (x; y) is the temporal image gradient and [I x (x; y); I y (x; y)] is the spatial image gradient at location (x; y).
Reference: [13] <author> M.P. Murray, </author> <title> A.B. Drought, and R.C. Kory. Walking patterns of normal men. </title> <journal> Journal of Bone and Joint Surgery, </journal> <volume> 46-A(2):335360, </volume> <month> March </month> <year> 1964. </year>
Reference-contexts: We can compare the estimated angular configurations with motion capture data reported in the literature. Murray, Brought, and 5 UCB//CSD-97-973, http://www.cs.berkeley.edu/bregler/digmuy.html joint is the position on the intersection of two axes. our motion tracker (right). Kory published <ref> [13] </ref> such measurements for the hip, knee, and angle joints. We compared our motion tracker measurements with the published curves and found good agreement. Figure 4.1a shows the curves for the knee and ankle reported in [13], and figure 4.1b shows our measurements. <p> Kory published <ref> [13] </ref> such measurements for the hip, knee, and angle joints. We compared our motion tracker measurements with the published curves and found good agreement. Figure 4.1a shows the curves for the knee and ankle reported in [13], and figure 4.1b shows our measurements. We also experimented with a walking sequence of a subject seen from an oblique view with a similar kinematic model. As seen in figure 4, we tracked the angular configurations and the pose successfully over the complete sequence of 45 image frames.
Reference: [14] <author> R.M. Murray, Z. Li, and S.S. Sastry. </author> <title> A Mathematical Introduction to Robotic Manipulation. </title> <publisher> CRC Press, </publisher> <year> 1994. </year> <note> 8 UCB//CSD-97-973, http://www.cs.berkeley.edu/bregler/digmuy.html </note>
Reference-contexts: We will first review a commonly used motion estimation framework [2, 19], and then show how this can be extended for our task, using the twist and product of exponential formulation <ref> [14] </ref>. 3.1 Preliminaries Assuming that changes in image intensity are only due to translation of local image intensity, a parametric image motion between consecutive time frames t and t + 1 can be described by the following equation: I (x + u x (x; y; ); y + u y (x; <p> kinematic chain and can be tracked in the same fashion as already outlined for simpler motion models. 3.2.1 3D pose The pose of an object relative to the camera frame can be represented as a rigid body transformation in &lt; 3 using homogeneous coordinates (we will use the notation from <ref> [14] </ref>): q c = G q o with G = 2 4 r 2;1 r 2;2 r 2;3 d y 0 0 0 1 7 2 UCB//CSD-97-973, http://www.cs.berkeley.edu/bregler/digmuy.html q o = [x o ; y o ; z o ; 1] T is a point in the object frame and q <p> In contrast, the twist representation provides a more elegant solution <ref> [14] </ref> and leads to a very simple linear representation of the motion model. It is based on the observation that every rigid motion can be represented as a rotation around a 3D axis and a translation along this axis. <p> The amount of rotation is specified with a scalar angle that is multiplied by the twist: ~. The v component determines the location of the rotation axis and the amount of translation along this axis. See <ref> [14] </ref> for a detailed geometric interpretation. For simplicity, we drop the constraint that ! is unit, and discard the coefficient. Therefore ~ 2 &lt; 6 . It can be shown [14] that for any arbitrary G 2 SE (3) there exists a ~ 2 &lt; 6 twist representation. <p> The v component determines the location of the rotation axis and the amount of translation along this axis. See <ref> [14] </ref> for a detailed geometric interpretation. For simplicity, we drop the constraint that ! is unit, and discard the coefficient. Therefore ~ 2 &lt; 6 . It can be shown [14] that for any arbitrary G 2 SE (3) there exists a ~ 2 &lt; 6 twist representation. <p> The velocity of a segment k can be described with a twist V k that is a linear combination of twists ~ 0 1 ; ~ 0 k and the angular velocities _ 1 ; _ 2 ; :::; _ k (see <ref> [14] </ref> for the derivations): V k = ~ 1 _ 1 + ~ 2 _ 2 + :::~ k _ k (22) 0 e ^ ~ 1 1 :::e Ad g is the adjoint transformation associated with g. 1 Given a point q c on the k'th segment of a kinematic
Reference: [15] <author> Eadweard Muybridge. </author> <title> The Human Figure In Motion. </title> <publisher> Various Publishers, latest edition by Dover Publications, </publisher> <year> 1901. </year>
Reference-contexts: This implies that one can use historical footagemotion capture Charlie Chaplin's inimitable walk, for instance. Indeed in this paper we shall go even further back historically and show motion capture results from Muybridge sequencesthe first examples of photographically recorded motion <ref> [15] </ref>. Motion capture occupies an important role in the creation of special effects. Its application to CG character animation has been much more controversial; SIGGRAPH 97 featured a lively panel debate [4] between its proponents and opponents. Our goal in this paper is not to address that debate.
Reference: [16] <author> J. O'Rourke and N. I. Badler. </author> <title> Model-based image analysis of human motion using constraint propagation. </title> <journal> IEEE Trans. Pattern Anal. Mach. Intell., </journal> <volume> 2(6):522536, </volume> <month> November </month> <year> 1980. </year>
Reference-contexts: reviews previous video tracking techniques, section 3 introduces the new motion tracking framework and its mathematical formulation, section 4 details our experiments, and we discuss the results and future directions in section 5. 2 Review The earliest computer vision attempt to recognize human movements was reported by O'Rouke and Badler <ref> [16] </ref> working on syn UCB//CSD-97-973, http://www.cs.berkeley.edu/bregler/digmuy.html thetic images using a 3D structure of rigid segments, joints, and constraints between them. Marker-free visual tracking on video recordings of human bodies goes back to work by Hogg and by Rohr [8, 18]. Both systems are specialized to one degree-of-freedom walking models.
Reference: [17] <author> J.M. Regh and T. Kanade. </author> <title> Model-based tracking of self-occluding articulated objects. </title> <booktitle> In Proc. Int. Conf. Computer Vision, </booktitle> <year> 1995. </year>
Reference-contexts: Both systems are specialized to one degree-of-freedom walking models. Edge and line features are extracted from images and matched to a cylindrical 3D human body model. Higher degree-of-freedom articulated hand configurations are tracked by Regh and Kanade <ref> [17] </ref>, full body configurations by Gravrila and Davis [7], and arm configurations by Kakadiaris and Metaxas [11] and by Goncalves and Perona [5]. All these approaches are demonstrated in constrained environments with high contrast edge boundaries.
Reference: [18] <author> K. Rohr. </author> <title> Incremental recognition of pedestrians from image sequences. </title> <journal> In Proc. IEEE Comput. Soc. Conf. Comput. Vision and Pattern Recogn., </journal> <pages> pages 813, </pages> <address> New York City, </address> <month> June, </month> <year> 1993. </year>
Reference-contexts: Marker-free visual tracking on video recordings of human bodies goes back to work by Hogg and by Rohr <ref> [8, 18] </ref>. Both systems are specialized to one degree-of-freedom walking models. Edge and line features are extracted from images and matched to a cylindrical 3D human body model.
Reference: [19] <author> J. Shi and C. Tomasi. </author> <title> Good features to track. </title> <booktitle> In CVPR, </booktitle> <year> 1994. </year>
Reference-contexts: In the following we will introduce a new region based differential technique that is tailored to articulated objects modeled by kinematic chains. We will first review a commonly used motion estimation framework <ref> [2, 19] </ref>, and then show how this can be extended for our task, using the twist and product of exponential formulation [14]. 3.1 Preliminaries Assuming that changes in image intensity are only due to translation of local image intensity, a parametric image motion between consecutive time frames t and t +
Reference: [20] <author> C. Wren, A. Azarbayejani, T. Darrell, and A. Pentland. Pfinder: </author> <title> Real-time tracking of the human body. </title> <booktitle> In SPIE Conference on Integration Issues in Large Commercial Media Delivery Systems, </booktitle> <volume> volume 2615, </volume> <year> 1995. </year> <month> 9 </month>
Reference-contexts: In most cases this is achieved by uniform backgrounds, and skintight clothing of uniform color. Also, in order to estimate 3D configurations, a camera calibration is needed. Alternatively, Weng et. al demonstrated how to track full bodies with color features <ref> [20] </ref>, and Ju et. al showed motion based tracking of leg configurations [10]. No 3D kinematic chain models were used in the last two cases.
References-found: 20

