URL: http://now.cs.berkeley.edu/clumps/nu.ps
Refering-URL: http://now.cs.berkeley.edu/clumps/index.html
Root-URL: 
Title: Non-Uniform Partitioning of Finite Difference Methods Running on SMP Clusters  
Author: Stephen J. Fink and Scott B. Baden 
Address: San Diego  
Affiliation: Department of Computer Science and Engineering University of California,  
Note: DRAFT Submitted For Publication  
Abstract: A multicomputer or workstation cluster with multiprocessor nodes introduces significant need and opportunity for overlapping communication with computation. We evaluate partitioning strategies for an important application class, finite difference methods, running on clusters of symmetric multiprocessors. Our results show that even for a regular, uniform finite difference method, a non-uniform partitioning can give the best performance by judiciously overlapping communication and computation. We present an analytic performance model and experimental results to evaluate partitioning strategies, and discuss implications for SMP cluster programming models.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. R. Woodward, </author> <title> "Perspectives on supercomputing: Three decades of change," </title> <journal> IEEE Computer, </journal> <volume> vol. 29, </volume> <pages> pp. 99-111, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: 1 Introduction Clusters of symmetric multiprocessors (SMPs) offer promising opportunities for high-performance scientific computation <ref> [1] </ref>. An SMP cluster, with several levels of locality and parallelism, presents a more complex non-uniform memory hierarchy than a cluster with uniprocessor nodes. Within an SMP node, multiple processors share a coherent address space and can exploit fast fine-grain communication through the shared memory system.
Reference: [2] <author> I. Foster, </author> <title> Designing and Building Parallel Programs. </title> <address> Menlo Park, CA: </address> <publisher> Addison-Wesley Pub. Co., </publisher> <year> 1995. </year>
Reference-contexts: This situation increases the need to reduce and tolerate inter-node communication costs. In order to deal with this issue, we must re-evaluate partitioning strategies traditionally employed in scientific computations. Parallel implementations of data-parallel applications often employ domain decomposition partitioning strategies (see, for example, <ref> [2] </ref>). These strategies break global arrays into pieces which are distributed across the nodes of a parallel computer. Each node performs the necessary computation on its local section of the global domain. <p> Second, non-uniform, run-time partitioning can deliver better performance on finite difference methods than a model restricted to uniform static decomposition. 2 Motivation As a motivating example, consider five-point Gauss-Seidel relaxation with red-black ordering to solve Poisson's equation in two dimensions (see, for example, <ref> [2] </ref>). On a distributed-memory parallel computer with p processors, it is straightforward to implement this calculation with a regular block data decomposition. For illustrative purposes, we restrict our attention to a 1D block decomposition.
Reference: [3] <author> High Performance Fortran Forum, </author> <title> High Performance Fortran Language Specification, </title> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: These strategies break global arrays into pieces which are distributed across the nodes of a parallel computer. Each node performs the necessary computation on its local section of the global domain. Many data-parallel programming languages <ref> [3, 4, 5, 6] </ref> and libraries [7, 8] provide support for well-understood regular block partitioning strategies. fl This work was supported by a DOE Computational Science Graduate Fellowship Program and NSF contract #ASC-9520372. Computer time on the NAS Power Challenge Array was provided by NASA through the HPCC Program testbed. <p> A solution to this problem lies in higher-level programming models that shield the programmer from low-level details of message-passing and thread management. Successful higher-level models targeted to data-parallel applications include HPF <ref> [3] </ref>, Vienna Fortran [4], Split-C [5], pC++ [6], and KeLP [8]. To facilitate the domain decomposition strategy described here, a programming model should support two levels, both inter-node and intra-node, of non-uniform block decompositions. <p> To our knowledge, no current high-level data-parallel programming system supports all these facilities. One possibility is to extend a language such as HPF <ref> [3] </ref> with multiple levels of Vienna Fortran [4] generalized block decompositions. In this case, the compiler would most likely analyze data dependences to schedule overlapped communication and computation at an SMP node.
Reference: [4] <author> B. Chapman, P. Mehrotra, H. Moritsch, and H. Zima, </author> <title> "Dynamic data distributions in Vienna Fortran," </title> <type> Tech. Rep. 93-92, </type> <institution> ICASE, </institution> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: These strategies break global arrays into pieces which are distributed across the nodes of a parallel computer. Each node performs the necessary computation on its local section of the global domain. Many data-parallel programming languages <ref> [3, 4, 5, 6] </ref> and libraries [7, 8] provide support for well-understood regular block partitioning strategies. fl This work was supported by a DOE Computational Science Graduate Fellowship Program and NSF contract #ASC-9520372. Computer time on the NAS Power Challenge Array was provided by NASA through the HPCC Program testbed. <p> A solution to this problem lies in higher-level programming models that shield the programmer from low-level details of message-passing and thread management. Successful higher-level models targeted to data-parallel applications include HPF [3], Vienna Fortran <ref> [4] </ref>, Split-C [5], pC++ [6], and KeLP [8]. To facilitate the domain decomposition strategy described here, a programming model should support two levels, both inter-node and intra-node, of non-uniform block decompositions. In practice, the application will likely determine the optimal decomposition based on factors in the run-time environment. <p> To our knowledge, no current high-level data-parallel programming system supports all these facilities. One possibility is to extend a language such as HPF [3] with multiple levels of Vienna Fortran <ref> [4] </ref> generalized block decompositions. In this case, the compiler would most likely analyze data dependences to schedule overlapped communication and computation at an SMP node. We are investigating an alternate approach, providing hierarchical parallel constructs via a run-time 8 DRAFT Submitted For Publication library.
Reference: [5] <author> D. Culler, A. Dusseau, S. Goldstein, A. Krishnamurthy, S. Lumetta, T. von Eicken, and K. Yelick, </author> <title> "Parallel programming in Split-C," </title> <booktitle> in Proc. Supercomputing, </booktitle> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: These strategies break global arrays into pieces which are distributed across the nodes of a parallel computer. Each node performs the necessary computation on its local section of the global domain. Many data-parallel programming languages <ref> [3, 4, 5, 6] </ref> and libraries [7, 8] provide support for well-understood regular block partitioning strategies. fl This work was supported by a DOE Computational Science Graduate Fellowship Program and NSF contract #ASC-9520372. Computer time on the NAS Power Challenge Array was provided by NASA through the HPCC Program testbed. <p> A solution to this problem lies in higher-level programming models that shield the programmer from low-level details of message-passing and thread management. Successful higher-level models targeted to data-parallel applications include HPF [3], Vienna Fortran [4], Split-C <ref> [5] </ref>, pC++ [6], and KeLP [8]. To facilitate the domain decomposition strategy described here, a programming model should support two levels, both inter-node and intra-node, of non-uniform block decompositions. In practice, the application will likely determine the optimal decomposition based on factors in the run-time environment.
Reference: [6] <author> F. Bodin, P. Beckman, D. Gannon, S. Yang, S. Kesavan, A. Malony, and B.Mohr, </author> <title> "Implementing a parallel C++ runtime system for scalable parallel systems," </title> <booktitle> in Proc. Supercomputing, </booktitle> <pages> pp. 588-597, </pages> <year> 1993. </year>
Reference-contexts: These strategies break global arrays into pieces which are distributed across the nodes of a parallel computer. Each node performs the necessary computation on its local section of the global domain. Many data-parallel programming languages <ref> [3, 4, 5, 6] </ref> and libraries [7, 8] provide support for well-understood regular block partitioning strategies. fl This work was supported by a DOE Computational Science Graduate Fellowship Program and NSF contract #ASC-9520372. Computer time on the NAS Power Challenge Array was provided by NASA through the HPCC Program testbed. <p> A solution to this problem lies in higher-level programming models that shield the programmer from low-level details of message-passing and thread management. Successful higher-level models targeted to data-parallel applications include HPF [3], Vienna Fortran [4], Split-C [5], pC++ <ref> [6] </ref>, and KeLP [8]. To facilitate the domain decomposition strategy described here, a programming model should support two levels, both inter-node and intra-node, of non-uniform block decompositions. In practice, the application will likely determine the optimal decomposition based on factors in the run-time environment.
Reference: [7] <author> G. Agrawal, A. Sussman, and J. Saltz, </author> <title> "An integrated runtime and compile-time approach for paral-lelizing structured and block structured applications," </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 6, </volume> <month> Jul. </month> <year> 1995. </year>
Reference-contexts: These strategies break global arrays into pieces which are distributed across the nodes of a parallel computer. Each node performs the necessary computation on its local section of the global domain. Many data-parallel programming languages [3, 4, 5, 6] and libraries <ref> [7, 8] </ref> provide support for well-understood regular block partitioning strategies. fl This work was supported by a DOE Computational Science Graduate Fellowship Program and NSF contract #ASC-9520372. Computer time on the NAS Power Challenge Array was provided by NASA through the HPCC Program testbed.
Reference: [8] <author> S. J. Fink, S. B. Baden, and S. R. Kohn, </author> <title> "Flexible communication mechanisms for dynamic structured applications," </title> <booktitle> in Proc. 3rd Int'l Workshop IRREGULAR '96, </booktitle> <pages> pp. 203-215, </pages> <month> Aug. </month> <year> 1996. </year>
Reference-contexts: These strategies break global arrays into pieces which are distributed across the nodes of a parallel computer. Each node performs the necessary computation on its local section of the global domain. Many data-parallel programming languages [3, 4, 5, 6] and libraries <ref> [7, 8] </ref> provide support for well-understood regular block partitioning strategies. fl This work was supported by a DOE Computational Science Graduate Fellowship Program and NSF contract #ASC-9520372. Computer time on the NAS Power Challenge Array was provided by NASA through the HPCC Program testbed. <p> In all runs, the SMP nodes were space-shared. To minimize noise from unpredictable external events, each number reported is the minimum from five runs. Each run consists of 100 iterations, with one iteration of warm-up not timed. Our applications use an extended version of the KeLP programming system <ref> [8] </ref>. KeLP, a C++ class library built on MPI [10], provides high-level abstractions to manage block-structured applications with irregular block partitionings. For finite difference calculations, previous work has shown that KeLP performance is comparable to hand-coded MPI implementations [8]. <p> Our applications use an extended version of the KeLP programming system <ref> [8] </ref>. KeLP, a C++ class library built on MPI [10], provides high-level abstractions to manage block-structured applications with irregular block partitionings. For finite difference calculations, previous work has shown that KeLP performance is comparable to hand-coded MPI implementations [8]. To manage the two levels of parallelism and locality on SMP clusters, we extended the KeLP infrastructure with lightweight thread facilities built with vendor-provided kernel threads. In our implementation, the program consists of one heavyweight MPI process on each SMP node. <p> A solution to this problem lies in higher-level programming models that shield the programmer from low-level details of message-passing and thread management. Successful higher-level models targeted to data-parallel applications include HPF [3], Vienna Fortran [4], Split-C [5], pC++ [6], and KeLP <ref> [8] </ref>. To facilitate the domain decomposition strategy described here, a programming model should support two levels, both inter-node and intra-node, of non-uniform block decompositions. In practice, the application will likely determine the optimal decomposition based on factors in the run-time environment. <p> In this case, the compiler would most likely analyze data dependences to schedule overlapped communication and computation at an SMP node. We are investigating an alternate approach, providing hierarchical parallel constructs via a run-time 8 DRAFT Submitted For Publication library. We are currently extending the KeLP programming system <ref> [8] </ref>, a C++ class library that provides irregular block decomposition and communication abstractions as first-class language objects.
Reference: [9] <author> S. J. Fink, </author> <title> "Hierarchical programming for block-structured scientific calculations." </title> <note> In preparation. 9 DRAFT Submitted For Publication </note>
Reference-contexts: We report results from a 2D red-black Gauss-Seidel Poisson solver running 2 Details omitted due to space constraints. A complete exposition will be reported elsewhere <ref> [9] </ref>. 5 DRAFT Submitted For Publication Platform CPU Clock Memory Data Cache Network (CPU/node) (MHz) per Node Alpha Farm Alpha 275 256 MB 16 KB L1, direct-mapped OC-3 ATM 21064A (4) 4 MB L2, direct-mapped Power Challenge MIPS 90 2-4 GB 4 MB HIPPI Array R8000 (8) 4-way set associative Platform
Reference: [10] <author> Message-Passing Interface Standard, </author> <title> "MPI: A message-passing interface standard," </title> <institution> University of Ten-nessee, Knoxville, TN, </institution> <month> Jun. </month> <year> 1995. </year>
Reference-contexts: To minimize noise from unpredictable external events, each number reported is the minimum from five runs. Each run consists of 100 iterations, with one iteration of warm-up not timed. Our applications use an extended version of the KeLP programming system [8]. KeLP, a C++ class library built on MPI <ref> [10] </ref>, provides high-level abstractions to manage block-structured applications with irregular block partitionings. For finite difference calculations, previous work has shown that KeLP performance is comparable to hand-coded MPI implementations [8].
Reference: [11] <author> G. M. </author> <title> Amdahl, "Validity of the single processor approach to achieving large scale computing capabilities," </title> <booktitle> in Proc. AFIPS Spring Joint Computer Conf., </booktitle> <address> (Atlantic City, NJ), </address> <pages> pp. 483-485, </pages> <month> Apr. </month> <year> 1967. </year>
Reference-contexts: When cache misses are frequent, memory bus contention may also come into play. A more general performance model may be needed for these cases. Non-uniform decompositions can reduce the communication penalty; naturally, the net improvement in an application's performance will be dictated by Amdahl's law <ref> [11] </ref>. On an n node by p processor SMP cluster, the model predicts that optimal intra-node partitioning reduces the communication penalty by a factor of p1 p .
Reference: [12] <author> W. W. Gropp and E. L. Lusk, </author> <title> "A taxonomy of programming models for symmetric multiprocessors and SMP clusters," </title> <booktitle> in Proceedings 1995: Programming models for massively parallel computers, </booktitle> <pages> pp. 2-7, </pages> <month> October </month> <year> 1995. </year>
Reference-contexts: An SMP cluster application may manage message-passing, lightweight threads, and shared memory in a of variety of styles and combinations to achieve high performance <ref> [12, 13, 14] </ref>. Unfortunately, managing these constructs can drown the application programmer in a morass of low-level implementation details, discouraging parallel implementations. A solution to this problem lies in higher-level programming models that shield the programmer from low-level details of message-passing and thread management.
Reference: [13] <author> I. Foster, C. Kesselman, and S. Tuecke, </author> <title> "The Nexus approach to integrating multithreading and communication," </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> vol. 37, </volume> <pages> pp. 70-82, </pages> <month> Aug. </month> <year> 1996. </year>
Reference-contexts: An SMP cluster application may manage message-passing, lightweight threads, and shared memory in a of variety of styles and combinations to achieve high performance <ref> [12, 13, 14] </ref>. Unfortunately, managing these constructs can drown the application programmer in a morass of low-level implementation details, discouraging parallel implementations. A solution to this problem lies in higher-level programming models that shield the programmer from low-level details of message-passing and thread management.
Reference: [14] <author> M. Haines, D. Cronk, and P. Mehrotra, </author> <title> "On the design of Chant: a talking threads package," </title> <booktitle> in Proc. Supercomputing, </booktitle> <address> (Washington, D.C), </address> <pages> pp. 350-9, </pages> <year> 1994. </year>
Reference-contexts: An SMP cluster application may manage message-passing, lightweight threads, and shared memory in a of variety of styles and combinations to achieve high performance <ref> [12, 13, 14] </ref>. Unfortunately, managing these constructs can drown the application programmer in a morass of low-level implementation details, discouraging parallel implementations. A solution to this problem lies in higher-level programming models that shield the programmer from low-level details of message-passing and thread management.
Reference: [15] <author> F. M. Hayes, </author> <title> "Design of the AlphaServer multiprocessor sever systems," </title> <journal> Digital Technical Journal, </journal> <volume> vol. 6, no. 3, </volume> <pages> pp. 8-19, </pages> <year> 1994. </year>
References-found: 15

