URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/jass/www/papers/ppopp90.ps
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/jass/www/papers.html
Root-URL: 
Title: Analysis of Event Synchronization in A Parallel Programming Tool  
Author: David Callahan Ken Kennedy Jaspal Subhlok 
Abstract: Understanding synchronization is important for a parallel programming tool that uses dependence analysis as the basis for advising programmers on the correctness of parallel constructs. This paper discusses static analysis methods that can be applied to parallel programs with event variable synchronization. The objective is to be able to predict potential data races in a parallel program. The focus is on how dependences and synchronization statements inside loops can be used to analyze complete programs with parallel loop and parallel case style parallelism. 
Abstract-found: 1
Intro-found: 1
Reference: [AK87] <author> R. Allen and K. Kennedy. </author> <title> Automatic translation of FORTRAN programs to vector form. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(4) </volume> <pages> 491-542, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: One of the most difficult challenges in the development of parallel programs for asynchronous shared-memory systems is avoiding errors caused by inadvertent data sharing, often referred to as data races, which can lead to unpredictable results. These races correspond to data dependences <ref> [AK87, Wol82] </ref>, where a data dependence links a pair of accesses to the same variable or memory location, with one such access being a write. <p> Our primary interest is in debugging scientific programs (where parallelism is primarily a performance issue) using event variable synchronization for software pipelining, so the restriction to serializable programs is reasonable. Correctness and Dependence Analysis A data dependence <ref> [AK87, Kuc78] </ref> exists between two statements if they both access some memory location M and at least one of them modifies that location: Definition 1 A data dependence s 1 s 2 (read "s 2 depends on s 1 ") exists if there is a memory location M such that both
Reference: [AM85] <author> W. F. Applebe and C. E. McDowell. </author> <title> Anomaly reporting | a tool for debugging and developing numerical algorithms. </title> <booktitle> In First International Conference on Supercomputers, </booktitle> <address> Florida, </address> <month> Decem-ber </month> <year> 1985. </year>
Reference-contexts: Unlike dynamic methods that use some form of run-time tracing [AP87, MC88, Sch89], our method is based entirely on static analysis and does not use any run-time information. Most previous work on static analysis of parallel programs has focused on task parallelism <ref> [Tay83, AM85, CS88, BK89] </ref> and is not suitable for the analysis of parallel loops. The method we propose overcomes this limitation and generalizes the loop analysis methods in [EP88]. This paper has 8 sections.
Reference: [AP87] <author> Todd Allen and David Padua. </author> <title> Debugging FORTRAN on a shared memory machine. </title> <booktitle> In Proceedings of the 1987 International Conference on Parallel Processing, </booktitle> <pages> pages 721-727, </pages> <month> September </month> <year> 1987. </year>
Reference-contexts: This paper develops a method for determining which dependences in a program cannot possibly result in a data race because of schedule restrictions enforced by event posts and waits. Unlike dynamic methods that use some form of run-time tracing <ref> [AP87, MC88, Sch89] </ref>, our method is based entirely on static analysis and does not use any run-time information. Most previous work on static analysis of parallel programs has focused on task parallelism [Tay83, AM85, CS88, BK89] and is not suitable for the analysis of parallel loops.
Reference: [ASU86] <author> A. Aho, R. Sethi, and J. Ullman. </author> <booktitle> Compilers: Principles, Techniques and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1986. </year>
Reference-contexts: The control flow subgraph is a directed graph, G c = (N; E c ), where the nodes N are program blocks <ref> [ASU86] </ref> built under the condition that a block in N must contain no control flow fl and may contain at most one WAIT, which must be the first statement in the block, and at most one POST, which must be the last statement in the block.
Reference: [BBC + 88] <author> V. Balasundaram, D. Baumgartner, D. Calla-han, K. Kennedy, and J. Subhlok. </author> <title> PTOOL: A system for static analysis of parallel programs. </title> <institution> Rice COMP TR88-71, Dept. of Computer Science, Rice University, </institution> <month> June </month> <year> 1988. </year>
Reference-contexts: These races correspond to data dependences [AK87, Wol82], where a data dependence links a pair of accesses to the same variable or memory location, with one such access being a write. At Rice, we have developed a system called Ptool <ref> [BBC + 88, Hen87] </ref> that permits a programmer to review the potential data races, if a particular loop were to execute in parallel.
Reference: [BK89] <author> V. Balasundaram and K. Kennedy. </author> <title> Compile time detection of race conditions in a parallel program. </title> <booktitle> In Proceedings of the International Conference on Supercomputing, </booktitle> <address> Crete, Greece, </address> <year> 1989. </year>
Reference-contexts: Unlike dynamic methods that use some form of run-time tracing [AP87, MC88, Sch89], our method is based entirely on static analysis and does not use any run-time information. Most previous work on static analysis of parallel programs has focused on task parallelism <ref> [Tay83, AM85, CS88, BK89] </ref> and is not suitable for the analysis of parallel loops. The method we propose overcomes this limitation and generalizes the loop analysis methods in [EP88]. This paper has 8 sections.
Reference: [CK87] <author> D. Callahan and K. Kennedy. </author> <title> Analysis of inter-procedural side effects in a parallel programming environment. </title> <booktitle> In Proceedings of the First International Conference on Supercomputing. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Athens, Greece, </address> <year> 1987. </year> <note> Available as Rice University, </note> <institution> Department of Computer Science Technical Report TR87-56, </institution> <month> July </month> <year> 1987. </year>
Reference-contexts: Since user inserted synchronization can be used to eliminate potential non-determinism caused by dependent statements specified to execute in parallel, it is important to analyze synchronization in a program and infer its effect on program behavior. Callahan and Kennedy <ref> [CK87] </ref> have developed a method for dealing with critical region style synchronization. In this paper we discuss the handling of event variable style synchronization. Extensions to other synchronization mechanisms are briefly discussed 1 DOALL 10 I = 1,10 . . . END DOALL PARALLEL BEGIN S1 PARALLEL . . .
Reference: [CS88] <author> D. Callahan and J. Subhlok. </author> <title> Static analysis of low-level synchronization. </title> <booktitle> In Proceedings of the ACM SIGPLAN and SIGOPS Workshop on Parallel and Distributed Debugging, </booktitle> <pages> pages 100-111, </pages> <address> Madison,WA, </address> <month> May </month> <year> 1988. </year> <note> Available as Rice University, </note> <institution> Dept. of Computer Science Technical Report TR88-70. </institution>
Reference-contexts: Unlike dynamic methods that use some form of run-time tracing [AP87, MC88, Sch89], our method is based entirely on static analysis and does not use any run-time information. Most previous work on static analysis of parallel programs has focused on task parallelism <ref> [Tay83, AM85, CS88, BK89] </ref> and is not suitable for the analysis of parallel loops. The method we propose overcomes this limitation and generalizes the loop analysis methods in [EP88]. This paper has 8 sections. <p> Clearly once the preserved sets are computed, it is a simple matter to verify which dependences are preserved. We have proved in another paper <ref> [CS88] </ref> that computation of preserved sets is NP-hard, even for simple programs without loops. Data Flow Equations Since the computation of preserved sets is NP-hard, we describe a method to compute a conservative approximation of preserved sets, which we call SCPreserved sets.
Reference: [EP88] <author> P. Emrath and D. Padua. </author> <title> Automatic detection of nondeterminacy in parallel programs. </title> <booktitle> In Proceedings of the ACM SIGPLAN and SIGOPS Workshop on Parallel and Distributed Debugging, </booktitle> <pages> pages 89-99, </pages> <address> Madison,WA, </address> <month> May </month> <year> 1988. </year>
Reference-contexts: Most previous work on static analysis of parallel programs has focused on task parallelism [Tay83, AM85, CS88, BK89] and is not suitable for the analysis of parallel loops. The method we propose overcomes this limitation and generalizes the loop analysis methods in <ref> [EP88] </ref>. This paper has 8 sections. Section 2 defines the computation paradigm to which our methods apply and Section 3 defines the problem to be solved in the paper. In section 4 we present a program graph structure, which is the framework for our analysis.
Reference: [FJS85] <author> P. O. Frederickson, R. E. Jones, and B. T. Smith. </author> <title> Synchronization and control of parallel algorithms. </title> <journal> Parallel Computing, </journal> (2):255-264, 1985. 
Reference-contexts: We now state the extensions to Fortran used for expressing parallelism and synchronization. Parallel DO and Parallel Case Parallelism can be expressed via two language constructs: parallel DO and parallel case. These are reasonably standard <ref> [FJS85, Par88] </ref> and we make no unusual assumptions. The parallel DO is syntactically and semantically very similar to the Fortran 77 DO loop. Here we use DOALL to indicate a parallel DO (illustrated in instances (iterations) of the loop body are started and proceed concurrently, but asynchronously.
Reference: [Hen87] <author> L. Henderson. </author> <title> The usefulness of dependecy-analysis tools in parallel programming: Experiences using PTOOL. </title> <type> Technical Report Preprint LA-UR-87-3135, </type> <institution> Los Alamos National Laboratory, </institution> <month> September </month> <year> 1987. </year>
Reference-contexts: These races correspond to data dependences [AK87, Wol82], where a data dependence links a pair of accesses to the same variable or memory location, with one such access being a write. At Rice, we have developed a system called Ptool <ref> [BBC + 88, Hen87] </ref> that permits a programmer to review the potential data races, if a particular loop were to execute in parallel.
Reference: [KKP + 81] <author> D. J. Kuck, R. H. Kuhn, D. A. Padua, B. Lea-sure, and M. Wolfe. </author> <title> Dependence graphs and compiler optimizations. </title> <booktitle> In Conference Record of the Eigth ACM Symposium on the Principles of Programming Languages, </booktitle> <pages> pages 207-218, </pages> <address> Williamsburgh, VA, </address> <month> January </month> <year> 1981. </year>
Reference-contexts: A data dependence is carried by a loop L if s 1 and s 2 are statement instances in different iterations of L. Much research has focused on identifying data dependence relationships and using them for loop restructuring transformations <ref> [KKP + 81] </ref>. Data dependences capture the constraints on a program's data flow that are necessary to ensure correct results if a program is executed in parallel.
Reference: [Kuc78] <author> D. Kuck. </author> <title> The Structures of Computers and Computation. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1978. </year>
Reference-contexts: Our primary interest is in debugging scientific programs (where parallelism is primarily a performance issue) using event variable synchronization for software pipelining, so the restriction to serializable programs is reasonable. Correctness and Dependence Analysis A data dependence <ref> [AK87, Kuc78] </ref> exists between two statements if they both access some memory location M and at least one of them modifies that location: Definition 1 A data dependence s 1 s 2 (read "s 2 depends on s 1 ") exists if there is a memory location M such that both
Reference: [MC88] <author> B. P. Miller and J. Choi. </author> <title> A mechanism for efficient debugging of parallel programs. </title> <booktitle> In Proceedings of the ACM SIGPLAN 88 Conference on Program Language Design and Implementation, </booktitle> <pages> pages 135-144, </pages> <address> Atlanta,GA, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: This paper develops a method for determining which dependences in a program cannot possibly result in a data race because of schedule restrictions enforced by event posts and waits. Unlike dynamic methods that use some form of run-time tracing <ref> [AP87, MC88, Sch89] </ref>, our method is based entirely on static analysis and does not use any run-time information. Most previous work on static analysis of parallel programs has focused on task parallelism [Tay83, AM85, CS88, BK89] and is not suitable for the analysis of parallel loops.
Reference: [Par88] <author> The Parallel Computing Forum. </author> <title> PCF Fortran: Language Definition, </title> <address> 1 edition, </address> <month> August </month> <year> 1988. </year>
Reference-contexts: We now state the extensions to Fortran used for expressing parallelism and synchronization. Parallel DO and Parallel Case Parallelism can be expressed via two language constructs: parallel DO and parallel case. These are reasonably standard <ref> [FJS85, Par88] </ref> and we make no unusual assumptions. The parallel DO is syntactically and semantically very similar to the Fortran 77 DO loop. Here we use DOALL to indicate a parallel DO (illustrated in instances (iterations) of the loop body are started and proceed concurrently, but asynchronously.
Reference: [Sch89] <author> E. Schonberg. </author> <title> On-the-fly detection of access anomalies. </title> <booktitle> In Proceedings of the ACM SIGPLAN 89 Conference on Program Language Design and Implementation, </booktitle> <pages> pages 285-297, </pages> <address> Portland, OR, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: This paper develops a method for determining which dependences in a program cannot possibly result in a data race because of schedule restrictions enforced by event posts and waits. Unlike dynamic methods that use some form of run-time tracing <ref> [AP87, MC88, Sch89] </ref>, our method is based entirely on static analysis and does not use any run-time information. Most previous work on static analysis of parallel programs has focused on task parallelism [Tay83, AM85, CS88, BK89] and is not suitable for the analysis of parallel loops.
Reference: [Tar81] <author> R. Tarjan. </author> <title> Fast algorithms for solving path problems. </title> <journal> JACM, </journal> <volume> 28(3) </volume> <pages> 594-614, </pages> <month> July </month> <year> 1981. </year>
Reference-contexts: A technique developed by Tarjan <ref> [Tar81] </ref> can be used to find a canonical regular expression representing all paths between any two nodes in a graph in nearly linear time.
Reference: [Tay83] <author> R. N. Taylor. </author> <title> A general purpose algorithm for analyzing concurrent programs. </title> <journal> Communications of the ACM, </journal> <volume> 26(5) </volume> <pages> 362-376, </pages> <month> May </month> <year> 1983. </year>
Reference-contexts: Unlike dynamic methods that use some form of run-time tracing [AP87, MC88, Sch89], our method is based entirely on static analysis and does not use any run-time information. Most previous work on static analysis of parallel programs has focused on task parallelism <ref> [Tay83, AM85, CS88, BK89] </ref> and is not suitable for the analysis of parallel loops. The method we propose overcomes this limitation and generalizes the loop analysis methods in [EP88]. This paper has 8 sections.
Reference: [Wol82] <author> M. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, University of Illinois at Urbana-Champaign, </institution> <year> 1982. </year> <month> 10 </month>
Reference-contexts: One of the most difficult challenges in the development of parallel programs for asynchronous shared-memory systems is avoiding errors caused by inadvertent data sharing, often referred to as data races, which can lead to unpredictable results. These races correspond to data dependences <ref> [AK87, Wol82] </ref>, where a data dependence links a pair of accesses to the same variable or memory location, with one such access being a write.
References-found: 19

