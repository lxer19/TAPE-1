URL: http://www.research.att.com/~dalia/pubs/wb.ps.gz
Refering-URL: http://www.research.att.com/~dalia/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: E-mail: fdolev,dalia,yvalg@cs.huji.ac.il  
Title: Warm Backup using Snooping  
Author: Danny Dolev Dalia Malki Yuval Yarom 
Address: Jerusalem 91904, Israel  
Affiliation: The Hebrew University of Jerusalem  
Abstract: Local Area Networks use a broadcast media to transfer messages between hosts. This allows for network snooping by unlisted parties. This paper proposes a novel way for cheaply replicating services in a local area network via snooping. We present a tool for Warm Backup of files that employs network snooping for data dissemination. The tool allows for a selective replication of files in the system. The use of snooping significantly reduces the overhead of file replication. Operations on non-replicated files suffer only a slight overhead. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. Alsberg and J. Day. </author> <title> A Principle for Resilient Sharing of Distributed Resources. </title> <booktitle> In Proceedings of the Second International Conference on Software Engineering, </booktitle> <pages> pages 627-644, </pages> <month> Oct. </month> <year> 1976. </year>
Reference-contexts: In order to guarantee the consistency of the primary and the backup, we have to introduce an acknowledgment message from the backup to the primary. This results in the paradigm depicted in Figure 1 (b). This paradigm is similar to other primary-backup mechanisms, such as <ref> [4, 1] </ref>, with several important differences: 1. A novel feature of the WB scheme is the utilization of broadcast hardware (and snooping) for the efficient dissemination of data to the primary and its backup. <p> Its drawback is in the handling of failures and recoveries, which involves re-electing a primary and is more expensive than the State-Machine approach. The work of Budhiraja et al. [3, 5] studies this approach formally. The primary-backup paradigm of the WB tool is similar to <ref> [1, 4] </ref>. The difference is that the WB tool exploits snooping to optimize the propagation of updates &gt;from the client to the backups. In addition, implementing the paradigms in [1, 4] would require changes to the NFS protocol, since the response to the client is delivered by the backup rather than <p> The primary-backup paradigm of the WB tool is similar to <ref> [1, 4] </ref>. The difference is that the WB tool exploits snooping to optimize the propagation of updates &gt;from the client to the backups. In addition, implementing the paradigms in [1, 4] would require changes to the NFS protocol, since the response to the client is delivered by the backup rather than the primary server. One of our goals was to produce a paradigm that is feasible to implement at the user level, without any visible change to the client. <p> Note that these issues are separate from the provision of a single server equivalent service during normal operation. We chose to leave these issues out of this paper, and solutions for them can be found in the literature, e.g. in <ref> [1, 2, 5, 14] </ref>. Also, since concurrent updates to files are rare in the Unix environment, we are not concerned with the possibility of network partitions. Our focus is on optimizing operation during normal behavior, i.e. the third issue.
Reference: [2] <author> A. Bhide and S. P. Morgan. </author> <title> A Highly Available Network File Server. </title> <type> RC 16161, </type> <institution> IBM Research, </institution> <month> May </month> <year> 1990. </year>
Reference-contexts: One of our goals was to produce a paradigm that is feasible to implement at the user level, without any visible change to the client. Both HA-NFS <ref> [2] </ref> and Harp [8] use the primary backup approach for replication, but utilize special hardware. The HA-NFS scheme relies on dual-port disks, and uses mirrored-updates for achieving atom-icity of updates. <p> Consequently, for best results, the WB server should run on a dedicated machine|the backup machine. We argue that this price is acceptable; the alternatives are either supporting a fully replicated file system, which burdens all the file servers in the system, or using a special hardware, as in <ref> [2] </ref>. A Formal Analysis We proceed with a more formal analysis of complexity of the protocol. A precise characterization of primary-backup protocols is provided in [5]. <p> Note that these issues are separate from the provision of a single server equivalent service during normal operation. We chose to leave these issues out of this paper, and solutions for them can be found in the literature, e.g. in <ref> [1, 2, 5, 14] </ref>. Also, since concurrent updates to files are rare in the Unix environment, we are not concerned with the possibility of network partitions. Our focus is on optimizing operation during normal behavior, i.e. the third issue.
Reference: [3] <author> N. Budhiraja. </author> <title> The Primary-Backup Approach: Lower and Upper Bounds. </title> <type> PhD thesis, </type> <institution> dept. of Computer Science, Cornell University, </institution> <month> June </month> <year> 1993. </year> <type> (TR 93-1353). </type>
Reference-contexts: This scheme has been used extensively in practice, because it is cheaper during normal operation. Its drawback is in the handling of failures and recoveries, which involves re-electing a primary and is more expensive than the State-Machine approach. The work of Budhiraja et al. <ref> [3, 5] </ref> studies this approach formally. The primary-backup paradigm of the WB tool is similar to [1, 4]. The difference is that the WB tool exploits snooping to optimize the propagation of updates &gt;from the client to the backups. <p> Its implementation is entirely at the user level, and is added at the server machine only. It is provably impossible to provide primary-backup replication with one backup, in the case of network partitions <ref> [3] </ref>. In case of network partitions, the WB tool allows the backup to diverge from the primary copy. 2 The WB Tool over NFS This section describes the structure of the WB tool within the NFS environment.
Reference: [4] <author> N. Budhiraja, K. Marzullo, F. B. Schneider, and S. Toueg. </author> <title> Optimal Primary-Backup Protocols. </title> <booktitle> In 6th Intl. Workshop on Distributed Algorithms proceedings (WDAG-6), (LCNS, </booktitle> <volume> 647), </volume> <pages> pages 362-378, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: In order to guarantee the consistency of the primary and the backup, we have to introduce an acknowledgment message from the backup to the primary. This results in the paradigm depicted in Figure 1 (b). This paradigm is similar to other primary-backup mechanisms, such as <ref> [4, 1] </ref>, with several important differences: 1. A novel feature of the WB scheme is the utilization of broadcast hardware (and snooping) for the efficient dissemination of data to the primary and its backup. <p> Its drawback is in the handling of failures and recoveries, which involves re-electing a primary and is more expensive than the State-Machine approach. The work of Budhiraja et al. [3, 5] studies this approach formally. The primary-backup paradigm of the WB tool is similar to <ref> [1, 4] </ref>. The difference is that the WB tool exploits snooping to optimize the propagation of updates &gt;from the client to the backups. In addition, implementing the paradigms in [1, 4] would require changes to the NFS protocol, since the response to the client is delivered by the backup rather than <p> The primary-backup paradigm of the WB tool is similar to <ref> [1, 4] </ref>. The difference is that the WB tool exploits snooping to optimize the propagation of updates &gt;from the client to the backups. In addition, implementing the paradigms in [1, 4] would require changes to the NFS protocol, since the response to the client is delivered by the backup rather than the primary server. One of our goals was to produce a paradigm that is feasible to implement at the user level, without any visible change to the client.
Reference: [5] <author> N. Budhiraja, K. Marzullo, F. B. Schneider, and S. Toueg. </author> <title> The Primary-Backup Approach. </title> <editor> In S. Mullender, editor, </editor> <booktitle> Distributed Systems. </booktitle> <publisher> ACM Press, </publisher> <year> 1993. </year>
Reference-contexts: This scheme has been used extensively in practice, because it is cheaper during normal operation. Its drawback is in the handling of failures and recoveries, which involves re-electing a primary and is more expensive than the State-Machine approach. The work of Budhiraja et al. <ref> [3, 5] </ref> studies this approach formally. The primary-backup paradigm of the WB tool is similar to [1, 4]. The difference is that the WB tool exploits snooping to optimize the propagation of updates &gt;from the client to the backups. <p> A Formal Analysis We proceed with a more formal analysis of complexity of the protocol. A precise characterization of primary-backup protocols is provided in <ref> [5] </ref>. They present several formal performance parameters concerned with building a fault tolerant service using the primary backup approach: (1) The number of replica, 2 In the final version of this paper, we intend to provide performance measurements for the Andrew benchmark. <p> Note that these issues are separate from the provision of a single server equivalent service during normal operation. We chose to leave these issues out of this paper, and solutions for them can be found in the literature, e.g. in <ref> [1, 2, 5, 14] </ref>. Also, since concurrent updates to files are rare in the Unix environment, we are not concerned with the possibility of network partitions. Our focus is on optimizing operation during normal behavior, i.e. the third issue. <p> Also, since concurrent updates to files are rare in the Unix environment, we are not concerned with the possibility of network partitions. Our focus is on optimizing operation during normal behavior, i.e. the third issue. In the terminology of <ref> [5] </ref>, the service blocking time is the longest period between the time a client request is received at the primary, and the time a response is sent. Let ffi be an upper bound on the communication latency between two machines. <p> Let ffi be an upper bound on the communication latency between two machines. The blocking time of the WB tool with a single backup is ffi, which matches the lower bounds achieved in <ref> [5] </ref> (making similar assumptions about failure types, and ignoring processing time). 3 Their analysis can be refined by noting that the latency of short messages, e.g. acknowledgments, is much smaller than the latency of large data messages. Let * be the latency of the acknowledgment messages. Usually * t ffi. <p> The scheme of [10] for fault-tolerance in a distributed environment requires a reliable Publishing mechanism, which is not achievable without software protocols. Snooping can provide a tool for achieving the reliability of this scheme. To achieve this, we 3 For multiple backups, <ref> [5] </ref> makes the assumption that a designated backup can intercept messages from all other backups in a single ffi delay time. This assumption does not hold in the environments we have in mind.
Reference: [6] <author> D. Dolev and D. Malki. </author> <title> On Distributed Algorithms in a Broadcast Domain. </title> <booktitle> In Intl. Conference on Automata, Languages and Programming, </booktitle> <pages> pages 371-387, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: This offers an efficient way to disseminate messages to multiple destinations. Future networks such as the high-speed FDDI ring and wireless networks also possess the broadcast capability. Understanding the potential in broadcast communication is, therefore, important (see also <ref> [6] </ref>). The ability to snoop on a broadcast network has led us to devise a completely new scheme for a cheap replication of files on a LAN. The Warm Backup (WB) tool snoops on the network, and updates the state of the backup by applying the updates it intercepts.
Reference: [7] <author> Y. Harari. </author> <title> Warm Backup Tool for Unix Network File System. </title> <type> internal manuscript, </type> <year> 1992. </year>
Reference-contexts: Both these approaches are standard in such a system, and their details are beyond the scope of this compact presentation. The topic of re-integrating an NFS server upon recovery is detailed elsewhere <ref> [7] </ref>. 3 Implementation Details The WB is feasible to implement, although we have learned that its implementation involves a fair bit of hacking system code. The WB tool has been implemented in two parts: the backup server and the NFS mediator.
Reference: [8] <author> B. Liskov, S. Ghemawat, R. Gruber, P. John-son, L. Shrira, and M. Williams. </author> <title> Replication in the Harp File System. </title> <booktitle> In Proceedings of the 13th Symposium on Operating Systems Principles, </booktitle> <pages> pages 226-238, </pages> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: One of our goals was to produce a paradigm that is feasible to implement at the user level, without any visible change to the client. Both HA-NFS [2] and Harp <ref> [8] </ref> use the primary backup approach for replication, but utilize special hardware. The HA-NFS scheme relies on dual-port disks, and uses mirrored-updates for achieving atom-icity of updates.
Reference: [9] <author> K. Marzullo and F. Schmuck. </author> <title> Supplying High Availability with a Standard Network File System. </title> <booktitle> In 4th Intl. Conf. Distributed Computing Systems, </booktitle> <pages> pages 447-453. </pages> <publisher> IEEE, </publisher> <month> June </month> <year> 1988. </year>
Reference-contexts: The advantage of this approach is that faults are completely masked from the client. However, the requirement of reliable and ordered delivery of client requests to all the replica presents a noticeable burden on the system. The RNFS system <ref> [9] </ref> and the Deceit system [14], which is based on it, are examples of replicated file systems implemented using this approach. They provide NFS clients with a completely compatible interface, while underneath they replicate each file operation on a set of disks. <p> They provide NFS clients with a completely compatible interface, while underneath they replicate each file operation on a set of disks. Both projects report that the reliable communication between the replication stubs incurs a considerable slowdown for non-replicated files <ref> [9, 13] </ref> (e.g. in RNFS write operations are 2-3 times slower than regular NFS). Most of the slowdown is incurred by the additional communication layer and the ordering of the updates. As a result, additional replica incur only a small overhead.
Reference: [10] <author> M. Powell and D. Presotto. </author> <title> Publishing: a Reliable Broadcast Communication Mechanism. </title> <booktitle> In Symposium on Operating Systems Principles, </booktitle> <volume> number 9, </volume> <pages> pages 100-109, </pages> <month> October </month> <year> 1983. </year>
Reference-contexts: below provides experimental results, showing an overhead of less than 8% (on average) for non-replicated files, and up to 56% for replicated files. 1.1 Related Work To the best of our knowledge Powell and Presotto were the first to describe a high availability mechanism based on snooping in a LAN <ref> [10] </ref>. The paper introduces a general scheme to recover from partial failures in a distributed systems, based on checkpoint and replay. Their approach uses a special node, a recording process, which collects all the interactions between nodes in the system for later replay. <p> The advantage of asynchronous update is that the remote backup is not a part of the update loop, and the response to local updates is not delayed by backing up. In addition, in our approach, the primary server is not disrupted by the back up process. The scheme of <ref> [10] </ref> for fault-tolerance in a distributed environment requires a reliable Publishing mechanism, which is not achievable without software protocols. Snooping can provide a tool for achieving the reliability of this scheme.
Reference: [11] <author> M. Satyanarayanan, J. Kistler, P. Kumar, M. Okasaki, E. Siegel, and D. Steere. Coda: </author> <title> A Highly Available File System for a Distributed Workstation Environment. </title> <journal> IEEE trans. on Computers, </journal> <volume> 39(4) </volume> <pages> 447-459, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: In this way, updates are never lost, but the actual write to the disk may lag behind. There are other replicated file systems like LOCUS [16] and Coda <ref> [11] </ref>, that do not maintain the Unix file system structure or semantics. These systems have their own kernel implementations of both client and server. Their replication scheme employs version time-stamping, and allows different replica to diverge.
Reference: [12] <author> F. Schneider. </author> <title> Implementing Fault Tolerant Services Using the State Machine Approach: A Tutorial. </title> <journal> Computing Surveys, </journal> <volume> 22(4) </volume> <pages> 299-319, </pages> <month> De-cember </month> <year> 1990. </year>
Reference-contexts: This section does not attempt to cover the vast number of works in this area. It relates to a chosen few, that represent corresponding leading approaches to this problem. One approach for replicating services is the State Machine approach <ref> [12] </ref>. In this scheme, the client requests are delivered to all the servers in the same order, to act upon. The servers are symmetrical and in case any of them fails, all the other replica have consistent states of the service.
Reference: [13] <author> A. Siegel. </author> <title> Perfomance in Flexible Distributed File Systems. </title> <type> PhD thesis, </type> <institution> dept. of Computer Science, Cornell University, </institution> <month> Feb </month> <year> 1992. </year> <type> (TR 92-1266). </type>
Reference-contexts: They provide NFS clients with a completely compatible interface, while underneath they replicate each file operation on a set of disks. Both projects report that the reliable communication between the replication stubs incurs a considerable slowdown for non-replicated files <ref> [9, 13] </ref> (e.g. in RNFS write operations are 2-3 times slower than regular NFS). Most of the slowdown is incurred by the additional communication layer and the ordering of the updates. As a result, additional replica incur only a small overhead.
Reference: [14] <author> A. Siegel, K. Birman, and K. Marzullo. Deceit: </author> <title> A Flexible Distributed File System. </title> <type> TR 89-1042, </type> <institution> dept. of computer science, Cornell University, </institution> <address> Ithaca, NY, </address> <month> Nov 89. </month>
Reference-contexts: The advantage of this approach is that faults are completely masked from the client. However, the requirement of reliable and ordered delivery of client requests to all the replica presents a noticeable burden on the system. The RNFS system [9] and the Deceit system <ref> [14] </ref>, which is based on it, are examples of replicated file systems implemented using this approach. They provide NFS clients with a completely compatible interface, while underneath they replicate each file operation on a set of disks. <p> Note that these issues are separate from the provision of a single server equivalent service during normal operation. We chose to leave these issues out of this paper, and solutions for them can be found in the literature, e.g. in <ref> [1, 2, 5, 14] </ref>. Also, since concurrent updates to files are rare in the Unix environment, we are not concerned with the possibility of network partitions. Our focus is on optimizing operation during normal behavior, i.e. the third issue.
Reference: [15] <author> Sun Micronsystems Inc. NFS: </author> <title> Network File System Protocol Specification. </title> <type> RFC 1094, </type> <institution> SRI Network Information Center, </institution> <month> March </month> <year> 1989. </year>
Reference-contexts: In order to do this filtering, the server has to be able to identify the files on the primary server machine. The only identification sent over the network by the NFS protocol is an opaque file handle, obtained by the client when the file is opened (see <ref> [15] </ref>). The WB server obtains a copy of the relevant file handles at startup, and compares these handles against the file handles in the intercepted datagrams. 1 The mediator The mediator process resides on the primary NFS server machine, and simply "hides" the NFS server.
Reference: [16] <author> B. Walker, G. Popek, R. English, C. Kline, and G. Thiel. </author> <title> The LOCUS Distributed Operating System. </title> <booktitle> In 9th Symp. on Operating Systems Principles, </booktitle> <pages> pages 49-70, </pages> <year> 1983. </year>
Reference-contexts: In this way, updates are never lost, but the actual write to the disk may lag behind. There are other replicated file systems like LOCUS <ref> [16] </ref> and Coda [11], that do not maintain the Unix file system structure or semantics. These systems have their own kernel implementations of both client and server. Their replication scheme employs version time-stamping, and allows different replica to diverge.
References-found: 16

