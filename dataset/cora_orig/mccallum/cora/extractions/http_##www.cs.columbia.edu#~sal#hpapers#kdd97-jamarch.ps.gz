URL: http://www.cs.columbia.edu/~sal/hpapers/kdd97-jamarch.ps.gz
Refering-URL: http://www.cs.columbia.edu:80/~sal/recent-papers.html
Root-URL: 
Email: wfan@cs.columbia.edu)  (pkc@cs.fit.edu)  
Title: JAM: Java agents for Meta-Learning over Distributed Databases  
Author: Salvatore Stolfo, Andreas L. Prodromidis Shelley Tselepis, Wenke Lee, Wei Fan (sal, andreas, sat, wenke, Philip K. Chan 
Note: Supported by IBM  
Date: March 5, 1997  
Address: New York, NY 10027  Melbourne, FL 32901  
Affiliation: Department of Computer Science Columbia University  Computer Science Florida Institute of Technology  
Abstract: In this paper, we describe the JAM system, a distributed, scalable and portable agent-based data mining system that employs a general approach to scaling data mining applications that we have come to call meta-learning. JAM provides a set of learning programs, implemented either as JAVA applets or applications, that compute models over data stored locally at a site. JAM also provides a set of meta-learning agents for combining multiple models that were learned (perhaps) at different sites. It employs a special distribution mechanism which allows the migration of the derived models or classifier agents to other remote sites. We describe the overall architecture of the JAM system and the specific implementation currently under development at Columbia University. One of JAM's target applications is fraud and intrusion detection in financial information systems. A brief description of this learning task and JAM's applicability are also described. Interested users may download JAM from http://www.cs.columbia.edu/~sal/JAM/PROJECT. fl This research is supported by the Intrusion Detection Program (BAA9603) of the Defense Advanced Research Projects Agency under grant F30602-96-1-0311, the Database and Expert Systems and Knowledge Models and Cognitive Systems Programs of the National Science Foundation under grant IRI-96-32225, the CISE Research Infrastructure Grant Program of the National Science Foundation under grant CDA-96-25374 and, the Center for Advanced Technology at Polytechnic University (not Columbia University) of the New York State Science and Technology Foundation under grant Polytechnic 423115-445. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth, </publisher> <address> Belmont, CA, </address> <year> 1984. </year>
Reference-contexts: In prior publications we introduced a number of meta-learning techniques including arbitration, combining [3] and hierarchical tree-structured meta-learning systems. Other publications have reported performance results on standard test problems and data sets with discussions of related techniques, Wolpert's stacking [9], Breiman's bagging <ref> [1] </ref> and Zhang's combining [10] to name a few. We shall not repeat this exposition in this paper.
Reference: [2] <author> P. Chan. </author> <title> An Extensible Meta-Learning Approach for Scalable and Accurate Inductive Learning. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Columbia University, </institution> <address> New York, NY, </address> <year> 1996. </year> <month> (forthcoming). </month>
Reference-contexts: Other parameters include the host of the CFM, the Cross-Validation Fold, the Meta-Learning Fold, the Meta-Learning Level, the names of the local learning agent and the local meta-learning agent, etc. Refer to <ref> [2] </ref> for more information on the meaning and use of these parameters. (Notice that Marmalade has established that Strawberry and Mango are its peer Datasites, having acquired this information from the CFM.) Then, Marmalade partitions the thyroid database (noted as thyroid.1.bld and thyroid.2.bld in the Data Set panel) for the 2-Cross-Validation
Reference: [3] <author> P. Chan and S. Stolfo. </author> <title> Toward parallel and distributed learning by meta-learning. </title> <booktitle> In Working Notes AAAI Work. Know. Disc. Databases, </booktitle> <pages> pages 227-240, </pages> <year> 1993. </year>
Reference-contexts: Our meta-learning approach is intended to be scalable as well as portable and extensible. In prior publications we introduced a number of meta-learning techniques including arbitration, combining <ref> [3] </ref> and hierarchical tree-structured meta-learning systems. Other publications have reported performance results on standard test problems and data sets with discussions of related techniques, Wolpert's stacking [9], Breiman's bagging [1] and Zhang's combining [10] to name a few. We shall not repeat this exposition in this paper.
Reference: [4] <author> P. Chan and S. Stolfo. </author> <title> A comparative evaluation of voting and meta-learning on partitioned data. </title> <booktitle> In Proc. Twelfth Intl. Conf. Machine Learning, </booktitle> <pages> pages 90-98, </pages> <year> 1995. </year>
Reference-contexts: Here meta-learning seeks to compute a "meta-classifier" that integrates in some principled fashion the separately learned classifiers to boost overall predictive accuracy. In the following pages we present a summary overview of a variety of ways of accomplishing this task by way of meta-learning as previously reported in <ref> [4] </ref>. We seek to continue developing meta-learning systems and apply these techniques to a range of large-scale distributed applications by utilizing existing agent-based infrastructures for deployment over the internet. <p> Ripper and CART were each able to catch 80% of the fraudulent transactions (True Positive or TP) but also misclassify 16% of the legitimate transactions (False Positive or FP) while 6 This meta-learning strategy is denoted class-attribute-combiner as defined in <ref> [4, 5] </ref>. 7 The section detailing the meta-learning strategies in [5] describes the various bounds placed on the meta-training data sets while still producing accurate meta-classifiers. 13 Bayes exhibited 80% TP and 13% FP in one setting and 80% TP and 19the three base classi-fiers with the least correlated error and
Reference: [5] <author> P. Chan and S. Stolfo. </author> <title> Learning arbiter and combiner trees from partitioned data for scaling machine learning. </title> <booktitle> In Proc. Intl. Conf. Knowledge Discovery and Data Mining, </booktitle> <pages> pages 39-44, </pages> <year> 1995. </year>
Reference-contexts: Ripper and CART were each able to catch 80% of the fraudulent transactions (True Positive or TP) but also misclassify 16% of the legitimate transactions (False Positive or FP) while 6 This meta-learning strategy is denoted class-attribute-combiner as defined in <ref> [4, 5] </ref>. 7 The section detailing the meta-learning strategies in [5] describes the various bounds placed on the meta-training data sets while still producing accurate meta-classifiers. 13 Bayes exhibited 80% TP and 13% FP in one setting and 80% TP and 19the three base classi-fiers with the least correlated error and <p> Ripper and CART were each able to catch 80% of the fraudulent transactions (True Positive or TP) but also misclassify 16% of the legitimate transactions (False Positive or FP) while 6 This meta-learning strategy is denoted class-attribute-combiner as defined in [4, 5]. 7 The section detailing the meta-learning strategies in <ref> [5] </ref> describes the various bounds placed on the meta-training data sets while still producing accurate meta-classifiers. 13 Bayes exhibited 80% TP and 13% FP in one setting and 80% TP and 19the three base classi-fiers with the least correlated error and in the second it combined the four most accurate base
Reference: [6] <author> William W. Cohen. </author> <title> Fast effective rule induction. </title> <booktitle> In Proc. Twelfth International Conference. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: Some machine learning algorithms generate concise and very readable textual outputs, e.g., the rule sets from Ripper <ref> [6] </ref>. It is thus counter-intuitive to translate the text to graph form for display purposes.
Reference: [7] <editor> Gregory Piatetsky-Shapiro Usama Fayyad and Padhraic Smyth. </editor> <title> The kdd process for extracting useful knowledge from data. </title> <journal> Communications of the ACM, </journal> <volume> 39(11) </volume> <pages> 27-34, </pages> <month> November </month> <year> 1996. </year>
Reference-contexts: Right: A ID3 tree-structured classifier is being displayed in the Classifier Visualization Panel 3.3 Classifier Visualization JAM provides graph drawing tools to help users understand the learned knowledge <ref> [7] </ref>. There are many kinds of classifiers, e.g., a decision tree by ID3, that can be represented as graphs. In JAM we have employed major components of JavaDot [8], an extensible visualization system, to display the classifier and allows the user to analyze the graph.
Reference: [8] <author> Naser S. Barghouti Wenke Lee. Javadot: </author> <title> An extensible visualization environment. </title> <type> Technical Report CUCS-02-97, </type> <institution> Department of Computer Science, Columbia University, </institution> <address> New York, NY, </address> <year> 1997. </year>
Reference-contexts: There are many kinds of classifiers, e.g., a decision tree by ID3, that can be represented as graphs. In JAM we have employed major components of JavaDot <ref> [8] </ref>, an extensible visualization system, to display the classifier and allows the user to analyze the graph.
Reference: [9] <author> D. Wolpert. </author> <title> Stacked generalization. </title> <booktitle> Neural Networks, </booktitle> <volume> 5 </volume> <pages> 241-259, </pages> <year> 1992. </year>
Reference-contexts: In prior publications we introduced a number of meta-learning techniques including arbitration, combining [3] and hierarchical tree-structured meta-learning systems. Other publications have reported performance results on standard test problems and data sets with discussions of related techniques, Wolpert's stacking <ref> [9] </ref>, Breiman's bagging [1] and Zhang's combining [10] to name a few. We shall not repeat this exposition in this paper.
Reference: [10] <author> X. Zhang, M. Mckenna, J. Mesirov, and D. Waltz. </author> <title> An efficient implementation of the backpropagation algorithm on the connection machine CM-2. </title> <type> Technical Report RL89-1, </type> <institution> Thinking Machines Corp., </institution> <year> 1989. </year> <month> 16 </month>
Reference-contexts: In prior publications we introduced a number of meta-learning techniques including arbitration, combining [3] and hierarchical tree-structured meta-learning systems. Other publications have reported performance results on standard test problems and data sets with discussions of related techniques, Wolpert's stacking [9], Breiman's bagging [1] and Zhang's combining <ref> [10] </ref> to name a few. We shall not repeat this exposition in this paper.
References-found: 10

