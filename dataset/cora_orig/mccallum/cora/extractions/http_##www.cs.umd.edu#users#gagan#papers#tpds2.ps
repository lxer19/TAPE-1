URL: http://www.cs.umd.edu/users/gagan/papers/tpds2.ps
Refering-URL: http://www.cs.umd.edu/users/gagan/pub.html
Root-URL: 
Email: fgagan, als, saltzg@cs.umd.edu  
Title: An Integrated Runtime and Compile-time Approach for Parallelizing Structured and Block Structured Applications 1 2  
Author: Gagan Agrawal, Alan Sussman Joel Saltz 
Address: College Park, MD 20742  College Park, MD 20742  
Affiliation: Dept. of Computer Sc., University of Maryland,  UMIACS and Dept. of Computer Sc., University of Maryland,  
Abstract: In compiling applications for distributed memory machines, runtime analysis is required when data to be communicated cannot be determined at compile-time. One such class of applications requiring runtime analysis is block structured codes. These codes employ multiple structured meshes, which may be nested (for multigrid codes) and/or irregularly coupled (called multiblock or irregularly coupled regular mesh problems). In this paper, we present runtime and compile-time analysis for compiling such applications on distributed memory parallel machines in an efficient and machine-independent fashion. We have designed and implemented a runtime library which supports the runtime analysis required. The library is currently implemented on several different systems. We have also developed compiler analysis for determining data access patterns at compile-time and inserting calls to the appropriate runtime routines. Our methods can be used by compilers for HPF-like parallel programming languages in compiling codes in which data distribution, loop bounds and/or strides are unknown at compile-time. To demonstrate the efficacy of our approach, we have implemented our compiler analysis in the Fortran 90D/HPF compiler developed at Syracuse University. We have experimented with a multiblock Navier-Stokes solver template and a multigrid code. Our experimental results show that our primitives have low runtime communication overheads and the compiler parallelized codes perform within 20% of the codes parallelized by manually inserting calls to the runtime library.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Christopher A. Atwood. </author> <title> Selected computations of transonic cavity flows. </title> <booktitle> In Proceedings of the 1993 ASME Fluids Engineering Conference, Forum on Computational Aero- and Hydro-Acoustics, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: They are commonly used for modeling complex objects which cannot be modeled using a single regular mesh. Examples include air quality modeling [33, 36], computational fluid dynamics [48], structure and galaxy formation [30, 44], simulation of high performance aircrafts <ref> [1, 7, 35] </ref>, large scale climate modeling [13], reservoir modeling for porous media [13], simulation of propulsion systems [13], computational combustion dynamics [13], geophysical databases [37], and land cover dynamics [24].
Reference: [2] <author> Shahid Bokhari. </author> <title> Communication overhead on the Intel iPSC-860 hypercube. </title> <type> Interim report, </type> <institution> ICASE, NASA Langley Research Center. </institution>
Reference-contexts: Note that this schedule building cost is independent of the amount of data transmitted. The time taken for building the schedule is approximately the same as the time required for sending an 18 Kilobytes message from one processor to another on an iPSC/860 <ref> [2] </ref>. Clearly, if there are no opportunities for reusing schedules and the problem size is relatively small, runtime preprocessing can become prohibitively expensive. In Figure 6, Set III shows the overhead of building communication schedule when the schedule is reused 100 times. <p> Note that the bandwidth achieved for bare communication (Set I) is nearly 2 Megabytes/second, which is the same as best reported bandwidth on an iPSC/860 <ref> [2] </ref>. 5.2 Multiblock Code The second factor that we are interested in measuring is the overhead of parallelizing a code through our prototype compiler, as compared to the best performance obtained by inserting calls to our library routines manually.
Reference: [3] <author> Zeki Bozkus, Alok Choudhary, Geoffrey Fox, Tomasz Haupt, Sanjay Ranka, and Min-You Wu. </author> <title> Compiling Fortran 90D/HPF for distributed memory MIMD computers. </title> <note> To appear in the Journal of Parallel and Distributed Computing, </note> <year> 1994. </year>
Reference-contexts: We present compiler analysis for determining the communication patterns at compile-time and inserting calls to appropriate primitives. We have implemented this compiler analysis as a part of the Fortran 90D/HPF compiler developed at Syracuse University <ref> [3] </ref>. We present experimental results to demonstrate the efficacy of our approach. We have experimented 3 SPMD is a model of parallel programming in which all processors execute a copy of the same program text [25]. 2 with one multiblock application [48] and one multigrid code [39]. <p> The Fortran 90D/HPF compiler developed at Syracuse uses a variety of runtime routines to handle cases when loop bounds and strides are not known, but does not consider cases when dynamic data distributions are used or when a statement involves arrays with different data distributions <ref> [3] </ref>. <p> Regular section move is a general case and is used as a default case whenever communication is required and cannot be handled by filling overlap cells. In general, a richer set of routines can be used (e.g. including temporary-shift, transfer etc. <ref> [3] </ref>) and the compiler can do an extended form of the analysis described here for deciding which runtime routine to insert. <p> If the offsets are not compile-time constants, then we use a regular section move to handle communication. This situation can also be handled by shifts into a temporary array <ref> [3] </ref>. The fifth condition says that along dimension j a loop variable does not appear in either the left hand side or the right hand side index and the loop invariant scalars are different.
Reference: [4] <author> W. Briggs. </author> <title> A Multigrid Tutorial. </title> <publisher> SIAM, </publisher> <year> 1987. </year> <month> 22 </month>
Reference-contexts: Codes for many scientific and engineering applications employ multiple structured meshes. These meshes may be nested (as in multigrid codes) or may be irregularly coupled (called Multiblock or Irregularly Coupled Regular Mesh Problems) [8]. Multigrid techniques are commonly used to accelerate iterative partial-differential equation solvers <ref> [4, 34] </ref>. Multigrid codes employ a number of meshes at different levels of resolution. In multiblock problems, the data is divided into several interacting regions (called blocks or subdomains). <p> Support for distributing loops iterations among the processors and converting global distributed arrays references to local references is also required. 2.2 Multigrid Applications Multigrid techniques are frequently used to accelerate iterative partial differential equation solvers <ref> [4, 34] </ref>. Multigrid codes employ a number of meshes at different levels of resolution.
Reference: [5] <author> B. Chapman, P. Mehrotra, and H. Zima. </author> <title> Programming in Vienna Fortran. </title> <journal> Scientific Programming, </journal> <volume> 1(1) </volume> <pages> 31-50, </pages> <month> Fall </month> <year> 1992. </year>
Reference-contexts: The authors assume all responsibility for the contents of the paper. 1 Recently there have been major efforts in developing programming language and compiler support for distributed memory machines. Based on the initial works of projects like Fortran D [12, 23] and Vienna Fortran <ref> [5, 49] </ref> , the High Performance Fortran Forum has recently defined the first version of High Performance Fortran (HPF) [25]. HPF allows programmer to specify the layout of distributed data and specify parallelism through operations on array sections and through parallel loops. <p> Reducing communication costs is crucial in achieving good performance on applications [20, 22]. While current systems like the Fortran D project [23] and the Vienna Fortran Compilation system <ref> [5] </ref> have implemented a number of optimizations for reducing communication costs (like message blocking, collective communication, message coalescing and aggregation), these optimizations have been developed largely for the programs where all information (data distribution, loop bounds and strides) is available at compile-time. <p> This compiler has also been 5 6 extended to use runtime support for codes having indirection arrays [19]. The Vienna Fortran Compi--lation System developed at the University of Vienna <ref> [5] </ref> has similar limitations. The Fortran 90D/HPF compiler developed at Syracuse uses a variety of runtime routines to handle cases when loop bounds and strides are not known, but does not consider cases when dynamic data distributions are used or when a statement involves arrays with different data distributions [3]. <p> Our library supports a communication primitive Overlap Cell Fill Sched, which computes a schedule that is used to direct the filling of overlap cells along a given dimension of a distributed array. Communication for filling in overlap cells has been implemented in other systems <ref> [5, 15, 32] </ref>, so we do not discuss the details here. The schedules produced by Overlap Cell Fill Sched and Regular Section Copy Sched are employed by a primitive called Data Move that carries out both interprocessor communication (sends and receives) and intra-processor data copying.
Reference: [6] <author> Siddhartha Chatterjee, John R. Gilbert, Fred J.E. Long, Robert Schreiber, and Shang-Hua Teng. </author> <title> Generating local addresses and communication sets for data-parallel programs. </title> <booktitle> In Proceedings of the Fourth ACM SIG-PLAN Symposium on Principles & Practice of Parallel Programming (PPOPP), </booktitle> <pages> pages 149-158, </pages> <month> May </month> <year> 1993. </year> <journal> ACM SIGPLAN Notices, </journal> <volume> Vol. 28, No. </volume> <pages> 7. </pages>
Reference-contexts: The details of these calculations, for block and cyclic distributions, have been developed by Koelbel [28]. Recently, in separate work, Chatterjee et al. <ref> [6] </ref>, Stichnoth [42, 43] and Gupta et al. [16, 17] have developed methods for analyzing communication in the general block-cyclic distribution case. Our current implementation supports regular section moves only for the block distribution case, since block distribution is the proper one to use for multiblock and multigrid applications. <p> Our current implementation supports regular section moves only for the block distribution case, since block distribution is the proper one to use for multiblock and multigrid applications. However, a primitive with such functionality can be implemented for the general block-cyclic case using methods described in <ref> [6, 16, 43] </ref>. A regular section move primitive can be invoked for analyzing the communication associated with any forall loop, but this may result in unnecessarily high runtime overheads. <p> Our current implementation is restricted to doing these transformations for block distribution, but similar functionality can be provided for the general block-cyclic case <ref> [6, 16, 43] </ref>. These communication and loop partitioning primitives are also applicable when the data distribution is known at compile-time, but the data to be communicated and the loop bounds on each processor cannot be determined because of symbolic loop bounds and/or strides. <p> At compile-time, if the data is not known to distributed in block or cyclic manner, then multiple nested do loops need to be generated for handling the loop <ref> [6, 43] </ref>. In Figure 4 we show an example of how the calls to primitives for filling in overlap cells are inserted by the compiler. In this example, the loop bounds and strides are known at compile-time, but exact data distribution is not known at compile time.
Reference: [7] <author> Kalpana Chawla and William R. Van Dalsem. </author> <title> Numerical simulation of a powered-lift landing. In Proceedings of the 72nd Fluid Dynamics Panel Meeting and Symposium on Computational and Experimental Assessment of Jets in Cross Flow, </title> <address> Winchester, UK. AGARD, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: They are commonly used for modeling complex objects which cannot be modeled using a single regular mesh. Examples include air quality modeling [33, 36], computational fluid dynamics [48], structure and galaxy formation [30, 44], simulation of high performance aircrafts <ref> [1, 7, 35] </ref>, large scale climate modeling [13], reservoir modeling for porous media [13], simulation of propulsion systems [13], computational combustion dynamics [13], geophysical databases [37], and land cover dynamics [24].
Reference: [8] <author> A. Choudhary, G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, S. Ranka, and J. Saltz. </author> <title> Software support for irregular and loosely synchronous problems. </title> <booktitle> Computing Systems in Engineering, 3(1-4):43-52, 1992. Papers presented at the Symposium on High-Performance Computing for Flight Vehicles, </booktitle> <month> December </month> <year> 1992. </year>
Reference-contexts: Codes for many scientific and engineering applications employ multiple structured meshes. These meshes may be nested (as in multigrid codes) or may be irregularly coupled (called Multiblock or Irregularly Coupled Regular Mesh Problems) <ref> [8] </ref>. Multigrid techniques are commonly used to accelerate iterative partial-differential equation solvers [4, 34]. Multigrid codes employ a number of meshes at different levels of resolution. In multiblock problems, the data is divided into several interacting regions (called blocks or subdomains).
Reference: [9] <author> R. Das, D. J. Mavriplis, J. Saltz, S. Gupta, and R. Ponnusamy. </author> <title> The design and implementation of a parallel unstructured Euler solver using software primitives. </title> <journal> AIAA Journal, </journal> <volume> 32(3) </volume> <pages> 489-496, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: One such class of applications is comprised of codes which use indirection arrays to access data in parallel loops. Runtime support and compiler analysis have been developed for executing this class of applications on distributed memory machines <ref> [9, 11, 26] </ref>. In certain applications such as single regular mesh based codes, compile-time techniques suffice for optimizing communication and obtaining good performance [41]. Multiblock and multigrid codes is an important class of applications which do not use indirection arrays but still need runtime analysis for efficient execution. <p> There is an important difference in the nature of the runtime analysis done for generating schedules in this library and in analysis required for determining communication in codes using indirection arrays <ref> [9] </ref>. In codes where indirection arrays are used and are partitioned across multiple processors, the runtime preprocessing requires communication between processors to determine the data that needs to be sent by each processors during the actual execution of the loop. <p> In a parallel loop, if data is accessed through indirection arrays, runtime primitives for irregular problems can be used for analyzing communication <ref> [9] </ref>. The details we discuss here only pertain to parallel loops not accessing data through indirection arrays. HPF also provides a number of intrinsic functions (such as reduction, spread, etc.).
Reference: [10] <author> R. Das, R. Ponnusamy, J. Saltz, and D. Mavriplis. </author> <title> Distributed memory compiler methods for irregular problems data copy reuse and runtime partitioning. </title> <editor> In J. Saltz and P. Mehrotra, editors, </editor> <booktitle> Languages, Compilers and Runtime Environments for Distributed Memory Machines, </booktitle> <pages> pages 185-220. </pages> <publisher> Elsevier, </publisher> <year> 1992. </year>
Reference-contexts: One such class of applications is comprised of codes in which indirection arrays are used to access distributed arrays. A Runtime library and the compiler analysis for invoking appropriate runtime routines have been developed for this class of applications <ref> [10, 11, 18] </ref>. Codes for many scientific and engineering applications employ multiple structured meshes. These meshes may be nested (as in multigrid codes) or may be irregularly coupled (called Multiblock or Irregularly Coupled Regular Mesh Problems) [8]. Multigrid techniques are commonly used to accelerate iterative partial-differential equation solvers [4, 34].
Reference: [11] <author> Raja Das, Joel Saltz, and Reinhard von Hanxleden. </author> <title> Slicing analysis and indirect access to distributed arrays. </title> <booktitle> In Proceedings of the 6th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 152-168. </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1993. </year> <note> Also available as University of Maryland Technical Report CS-TR-3076 and UMIACS-TR-93-42. </note>
Reference-contexts: One such class of applications is comprised of codes in which indirection arrays are used to access distributed arrays. A Runtime library and the compiler analysis for invoking appropriate runtime routines have been developed for this class of applications <ref> [10, 11, 18] </ref>. Codes for many scientific and engineering applications employ multiple structured meshes. These meshes may be nested (as in multigrid codes) or may be irregularly coupled (called Multiblock or Irregularly Coupled Regular Mesh Problems) [8]. Multigrid techniques are commonly used to accelerate iterative partial-differential equation solvers [4, 34]. <p> One such class of applications is comprised of codes which use indirection arrays to access data in parallel loops. Runtime support and compiler analysis have been developed for executing this class of applications on distributed memory machines <ref> [9, 11, 26] </ref>. In certain applications such as single regular mesh based codes, compile-time techniques suffice for optimizing communication and obtaining good performance [41]. Multiblock and multigrid codes is an important class of applications which do not use indirection arrays but still need runtime analysis for efficient execution.
Reference: [12] <author> Geoffrey Fox, Seema Hiranandani, Ken Kennedy, Charles Koelbel, Uli Kremer, Chau-Wen Tseng, and Min-You Wu. </author> <title> Fortran D language specification. </title> <type> Technical Report CRPC-TR90079, </type> <institution> Center for Research on Parallel Computation, Rice University, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: The authors assume all responsibility for the contents of the paper. 1 Recently there have been major efforts in developing programming language and compiler support for distributed memory machines. Based on the initial works of projects like Fortran D <ref> [12, 23] </ref> and Vienna Fortran [5, 49] , the High Performance Fortran Forum has recently defined the first version of High Performance Fortran (HPF) [25]. HPF allows programmer to specify the layout of distributed data and specify parallelism through operations on array sections and through parallel loops.
Reference: [13] <institution> Survey of principal investigators of grand challenge applications: Workshop on grand challenge applications and software technology, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: They are commonly used for modeling complex objects which cannot be modeled using a single regular mesh. Examples include air quality modeling [33, 36], computational fluid dynamics [48], structure and galaxy formation [30, 44], simulation of high performance aircrafts [1, 7, 35], large scale climate modeling <ref> [13] </ref>, reservoir modeling for porous media [13], simulation of propulsion systems [13], computational combustion dynamics [13], geophysical databases [37], and land cover dynamics [24]. <p> Examples include air quality modeling [33, 36], computational fluid dynamics [48], structure and galaxy formation [30, 44], simulation of high performance aircrafts [1, 7, 35], large scale climate modeling <ref> [13] </ref>, reservoir modeling for porous media [13], simulation of propulsion systems [13], computational combustion dynamics [13], geophysical databases [37], and land cover dynamics [24]. <p> Examples include air quality modeling [33, 36], computational fluid dynamics [48], structure and galaxy formation [30, 44], simulation of high performance aircrafts [1, 7, 35], large scale climate modeling <ref> [13] </ref>, reservoir modeling for porous media [13], simulation of propulsion systems [13], computational combustion dynamics [13], geophysical databases [37], and land cover dynamics [24]. <p> Examples include air quality modeling [33, 36], computational fluid dynamics [48], structure and galaxy formation [30, 44], simulation of high performance aircrafts [1, 7, 35], large scale climate modeling <ref> [13] </ref>, reservoir modeling for porous media [13], simulation of propulsion systems [13], computational combustion dynamics [13], geophysical databases [37], and land cover dynamics [24].
Reference: [14] <author> Al Geist, A. Beguelin, J. Dongarra, W. Jiang, R. Manchek, and V. Sunderam. </author> <title> PVM 3 user's guide and reference manual. </title> <type> Technical Report ORNL/TM-12187, </type> <institution> Oak Ridge National Laboratory, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: Our library supports dynamic data distributions, but we have not concentrated on efficient algorithms for data redistribution at runtime [46]. This library is currently implemented on the Intel iPSC/860 and Paragon, the Thinking Machines' CM-5 and the PVM message passing environment for a network of workstations <ref> [14] </ref>. The design of the 8 library is architecture independent and therefore it can be easily ported to any distributed memory parallel machine or any environment which supports message passing (e.g.
Reference: [15] <author> Michael Gerndt. </author> <title> Updating distributed variables in local computations. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 2(3) </volume> <pages> 171-193, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: Consequently, the data to be communicated needs to be determined at runtime. The second inner loop involves interactions among neighboring elements of the meshes. Since blocks may be partitioned across processors, this also requires communication. This form of communication can be handled by allocating ghost or overlap cells <ref> [15, 32] </ref> and filling them (by communicating with neighboring 4 processor (s)) before a parallel loop. However, if data distribution is not known at compile-time, then runtime analysis is required for determining the communication required. <p> Our library supports a communication primitive Overlap Cell Fill Sched, which computes a schedule that is used to direct the filling of overlap cells along a given dimension of a distributed array. Communication for filling in overlap cells has been implemented in other systems <ref> [5, 15, 32] </ref>, so we do not discuss the details here. The schedules produced by Overlap Cell Fill Sched and Regular Section Copy Sched are employed by a primitive called Data Move that carries out both interprocessor communication (sends and receives) and intra-processor data copying.
Reference: [16] <author> S.K.S. Gupta, S.D. Kaushik, C.H. Huang, and P. Sadayappan. </author> <title> On compiling array expressions for efficient execution on distributed memory machines. </title> <type> Technical Report CISRC-4/94-TR19, </type> <institution> Ohio State University, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: The details of these calculations, for block and cyclic distributions, have been developed by Koelbel [28]. Recently, in separate work, Chatterjee et al. [6], Stichnoth [42, 43] and Gupta et al. <ref> [16, 17] </ref> have developed methods for analyzing communication in the general block-cyclic distribution case. Our current implementation supports regular section moves only for the block distribution case, since block distribution is the proper one to use for multiblock and multigrid applications. <p> Our current implementation supports regular section moves only for the block distribution case, since block distribution is the proper one to use for multiblock and multigrid applications. However, a primitive with such functionality can be implemented for the general block-cyclic case using methods described in <ref> [6, 16, 43] </ref>. A regular section move primitive can be invoked for analyzing the communication associated with any forall loop, but this may result in unnecessarily high runtime overheads. <p> Our current implementation is restricted to doing these transformations for block distribution, but similar functionality can be provided for the general block-cyclic case <ref> [6, 16, 43] </ref>. These communication and loop partitioning primitives are also applicable when the data distribution is known at compile-time, but the data to be communicated and the loop bounds on each processor cannot be determined because of symbolic loop bounds and/or strides.
Reference: [17] <author> S.K.S. Gupta, S.D. Kaushik, S. Mufti, S. Sharma, C.-H. Huang, and P. Sadayappan. </author> <title> On compiling array expressions for efficient execution on distributed memory machines. </title> <booktitle> In Proceedings of the 1993 International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1993. </year>
Reference-contexts: The details of these calculations, for block and cyclic distributions, have been developed by Koelbel [28]. Recently, in separate work, Chatterjee et al. [6], Stichnoth [42, 43] and Gupta et al. <ref> [16, 17] </ref> have developed methods for analyzing communication in the general block-cyclic distribution case. Our current implementation supports regular section moves only for the block distribution case, since block distribution is the proper one to use for multiblock and multigrid applications.
Reference: [18] <author> R. v. Hanxleden, K. Kennedy, C. Koelbel, R. Das, and J. Saltz. </author> <title> Compiler analysis for irregular problems in Fortran D. </title> <booktitle> In Proceedings of the 5th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> New Haven, CT, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: One such class of applications is comprised of codes in which indirection arrays are used to access distributed arrays. A Runtime library and the compiler analysis for invoking appropriate runtime routines have been developed for this class of applications <ref> [10, 11, 18] </ref>. Codes for many scientific and engineering applications employ multiple structured meshes. These meshes may be nested (as in multigrid codes) or may be irregularly coupled (called Multiblock or Irregularly Coupled Regular Mesh Problems) [8]. Multigrid techniques are commonly used to accelerate iterative partial-differential equation solvers [4, 34].
Reference: [19] <author> Reinhard v. Hanxleden. </author> <title> Handling irregular problems with Fortran D a preliminary report. </title> <booktitle> In Proceedings of the Fourth Workshop on Compilers for Parallel Computers, </booktitle> <address> Delft, The Netherlands, </address> <month> December </month> <year> 1993. </year> <note> Also available as CRPC Technical Report CRPC-TR93339-S. </note>
Reference-contexts: This compiler has also been 5 6 extended to use runtime support for codes having indirection arrays <ref> [19] </ref>. The Vienna Fortran Compi--lation System developed at the University of Vienna [5] has similar limitations.
Reference: [20] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Evaluation of compiler optimizations for Fortran D on MIMD distributed-memory machines. </title> <booktitle> In Proceedings of the Sixth International Conference on Supercomputing. </booktitle> <publisher> ACM Press, </publisher> <month> July </month> <year> 1992. </year> <month> 23 </month>
Reference-contexts: HPF offers the promise of significantly easing the task of programming distributed memory machines and making programs independent of a single machine architecture. Reducing communication costs is crucial in achieving good performance on applications <ref> [20, 22] </ref>. <p> In Figure 1, we show how the area around an aircraft wing has been modeled with a multiblock mesh. 4 Message aggregation is a communication optimization in which several data items to be communicated are collected into one message <ref> [20] </ref>. 3 The main body of the program, in a typical multiblock application, consists of an outer sequential (time step) loop, and two inner parallel loops. The inner loops iterate over the set of blocks (meshes). In the first inner loop, boundary conditions are applied over individual blocks.
Reference: [21] <author> Seema Hiranandani, Ken Kennedy, John Mellor-Crummey, and Ajay Sethi. </author> <title> Advanced compilation techniques for fortran d. </title> <type> Technical Report CRPC-TR 93338-S, </type> <note> Center for Research in Parallel Computation, </note> <month> October </month> <year> 1993. </year>
Reference-contexts: In the Fortran D compiler developed at Rice university, detailed communication optimizations and compilation techniques based upon dependence analysis have been investigated in detail, but the focus is on cases when compile-time analysis alone suffices <ref> [21, 47] </ref>. This compiler has also been 5 6 extended to use runtime support for codes having indirection arrays [19]. The Vienna Fortran Compi--lation System developed at the University of Vienna [5] has similar limitations.
Reference: [22] <author> Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Compiler optimizations for Fortran D on MIMD distributed-memory machines. </title> <booktitle> In Proceedings Supercomputing '91, </booktitle> <pages> pages 86-100. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1991. </year>
Reference-contexts: HPF offers the promise of significantly easing the task of programming distributed memory machines and making programs independent of a single machine architecture. Reducing communication costs is crucial in achieving good performance on applications <ref> [20, 22] </ref>.
Reference: [23] <author> Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: The authors assume all responsibility for the contents of the paper. 1 Recently there have been major efforts in developing programming language and compiler support for distributed memory machines. Based on the initial works of projects like Fortran D <ref> [12, 23] </ref> and Vienna Fortran [5, 49] , the High Performance Fortran Forum has recently defined the first version of High Performance Fortran (HPF) [25]. HPF allows programmer to specify the layout of distributed data and specify parallelism through operations on array sections and through parallel loops. <p> HPF offers the promise of significantly easing the task of programming distributed memory machines and making programs independent of a single machine architecture. Reducing communication costs is crucial in achieving good performance on applications [20, 22]. While current systems like the Fortran D project <ref> [23] </ref> and the Vienna Fortran Compilation system [5] have implemented a number of optimizations for reducing communication costs (like message blocking, collective communication, message coalescing and aggregation), these optimizations have been developed largely for the programs where all information (data distribution, loop bounds and strides) is available at compile-time. <p> In distributed memory compilation, the owner computes rule is often used for distributing loop iterations <ref> [23] </ref>. Owner computes means that a particular loop iteration is executed by the processor owning the left-hand side array element written into during that iteration. <p> The forall construct is not included in the HPF subset specified for initial implementation by vendors [25]. In compiling the forall construct and the independent statement, data dependence analysis is required for determining when message aggregation can be done <ref> [23] </ref>. <p> Let i1 k = i j . The loop accesses elements with indices ranging from c1 k fl lo j + d1 k to c1 k fl hi j + d1 k . If the owner computes rule <ref> [23] </ref> is used, then the loop is partitioned based upon the portion of the left hand side array 14 C ORIGINAL HPF CODE C Arrays A, B are distributed identically forall (i = 1:100:2,j = 1:50) A (i,j) = B (2*j,i) C TRANSFORMED CODE NumSrcDim = 2 NumDestDim = 2 SrcDim
Reference: [24] <author> J.R.G.Townshend, C.O.Justice, W. Li, C.Gurney, and J.McManus. </author> <title> Global land cover classification by remote sensing:present capabilities and future possibilities. </title> <booktitle> Remote Sensing of Environment, </booktitle> <volume> 35 </volume> <pages> 243-256, </pages> <year> 1991. </year>
Reference-contexts: air quality modeling [33, 36], computational fluid dynamics [48], structure and galaxy formation [30, 44], simulation of high performance aircrafts [1, 7, 35], large scale climate modeling [13], reservoir modeling for porous media [13], simulation of propulsion systems [13], computational combustion dynamics [13], geophysical databases [37], and land cover dynamics <ref> [24] </ref>.
Reference: [25] <author> C. Koelbel, D. Loveman, R. Schreiber, G. Steele, Jr., and M. Zosel. </author> <title> The High Performance Fortran Handbook. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: Based on the initial works of projects like Fortran D [12, 23] and Vienna Fortran [5, 49] , the High Performance Fortran Forum has recently defined the first version of High Performance Fortran (HPF) <ref> [25] </ref>. HPF allows programmer to specify the layout of distributed data and specify parallelism through operations on array sections and through parallel loops. Proposed HPF compilers are being designed to produce Single Program Multiple Data (SPMD) 3 Fortran 77 (F77) code with message passing and/or runtime communication primitives. <p> We present experimental results to demonstrate the efficacy of our approach. We have experimented 3 SPMD is a model of parallel programming in which all processors execute a copy of the same program text <ref> [25] </ref>. 2 with one multiblock application [48] and one multigrid code [39]. We have measured the runtime overheads of our primitives. We have compared the performance of compiler parallelized multiblock and multigrid templates with those of the manually parallelized (i.e. parallelized by inserting calls to the runtime library manually) versions. <p> Such an arrangement allows a schedule to be used multiple times in an iterative code. Consider a single statement forall loop as specified in HPF. This is a parallel loop in which loop bounds and strides associated with any loop variable cannot be functions of any other loop variable <ref> [25] </ref>. If there is only a single array on the right hand side, then this forall loop can be thought as copying a rectilinear section of data from the right hand side array to the left hand array, potentially involving change of offsets, strides and index permutation. <p> ghost cells by explicitly stating the number of cells at the beginning and end of each dimension: $ DIMENSION A (105,105) 5 Distribute is a HPF directive which specifies the partition of the index space of a data object among a set of abstract processors according to a given pattern <ref> [25] </ref>. 9 $ ALIGN A (i,j) WITH T (i:2:3,j:2:3) 6 This example says that an array of size 105x105 is aligned along a template of size 100x100, with 2 external ghost cells at the beginning of each dimension and 3 external ghost cells at the end of each dimension. <p> The HPF specification also allows multiple nested forall loops (called a forall construct) and an independent statement for expressing parallelism. The forall construct is not included in the HPF subset specified for initial implementation by vendors <ref> [25] </ref>. In compiling the forall construct and the independent statement, data dependence analysis is required for determining when message aggregation can be done [23]. <p> lo 1 : hi 1 : st 1 ; : : : ; i m = lo m : hi m : st m ) 6 Align is a HPF directive which specifies that certain data objects are to be mapped in a similar way as certain other data objects <ref> [25] </ref>. 10 A (f 1 ; f 2 ; : : : ; f j ) = B (g 1 ; g 2 ; : : : ; g n ) The i k ; (k = 1::m) are the loop variables associated with the forall statement. lo k , hi
Reference: [26] <author> C. Koelbel and P. Mehrotra. </author> <title> Compiling global name-space parallel loops for distributed execution. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 440-451, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: We conclude in Section 6. 2 Block Structured Applications It is well established that for many codes, a combination of runtime and compile-time analysis is required for efficient execution on distributed memory machines <ref> [26, 27] </ref>. One such class of applications is comprised of codes which use indirection arrays to access data in parallel loops. Runtime support and compiler analysis have been developed for executing this class of applications on distributed memory machines [9, 11, 26]. <p> One such class of applications is comprised of codes which use indirection arrays to access data in parallel loops. Runtime support and compiler analysis have been developed for executing this class of applications on distributed memory machines <ref> [9, 11, 26] </ref>. In certain applications such as single regular mesh based codes, compile-time techniques suffice for optimizing communication and obtaining good performance [41]. Multiblock and multigrid codes is an important class of applications which do not use indirection arrays but still need runtime analysis for efficient execution. <p> In the Kali compiler <ref> [26] </ref>, the need for combined compile-time and runtime analysis was outlined, but the details of runtime primitives and compiler implementation were not investigated for codes that did not use indirection arrays. 3 Runtime Support In this section we discuss the functionality of the runtime support library that we have developed.
Reference: [27] <author> C. Koelbel, P. Mehrotra, and J. Van Rosendale. </author> <title> Supporting shared data structures on distributed memory architectures. </title> <booktitle> In Proceedings of the Second ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming (PPOPP), </booktitle> <pages> pages 177-186. </pages> <publisher> ACM Press, </publisher> <month> March </month> <year> 1990. </year>
Reference-contexts: We conclude in Section 6. 2 Block Structured Applications It is well established that for many codes, a combination of runtime and compile-time analysis is required for efficient execution on distributed memory machines <ref> [26, 27] </ref>. One such class of applications is comprised of codes which use indirection arrays to access data in parallel loops. Runtime support and compiler analysis have been developed for executing this class of applications on distributed memory machines [9, 11, 26].
Reference: [28] <author> Charles Koelbel. </author> <title> Compile-time generation of regular communication patterns. </title> <booktitle> In Proceedings Supercomputing '91. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1991. </year>
Reference-contexts: The set of data elements to be sent and received by each processor can be computed at runtime when information about data distribution, loop bounds and strides is available. The details of these calculations, for block and cyclic distributions, have been developed by Koelbel <ref> [28] </ref>. Recently, in separate work, Chatterjee et al. [6], Stichnoth [42, 43] and Gupta et al. [16, 17] have developed methods for analyzing communication in the general block-cyclic distribution case.
Reference: [29] <author> Scott R. Kohn and Scott B. Baden. </author> <title> An implementation of the LPAR parallel programming model for scientific computations. </title> <booktitle> In Proceedings of the Sixth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 759-766. </pages> <publisher> SIAM, </publisher> <month> March </month> <year> 1993. </year>
Reference-contexts: Our experimental results show that the primitives have low runtime communication overheads and that the compiler parallelized codes perform within 20% of the codes parallelized by inserting calls to the runtime library manually. Several other researchers have also developed runtime libraries or programming environments for multi-block applications. Baden <ref> [29] </ref> has developed a Lattice Programming Model (LPAR). This system is targeted towards applications having a large number of small blocks and therefore allows a block to be assigned only to one processor.
Reference: [30] <author> George Lake. </author> <type> Personal communication. </type>
Reference-contexts: These codes are also called Irregularly Coupled Regular Mesh (ICRM) codes. They are commonly used for modeling complex objects which cannot be modeled using a single regular mesh. Examples include air quality modeling [33, 36], computational fluid dynamics [48], structure and galaxy formation <ref> [30, 44] </ref>, simulation of high performance aircrafts [1, 7, 35], large scale climate modeling [13], reservoir modeling for porous media [13], simulation of propulsion systems [13], computational combustion dynamics [13], geophysical databases [37], and land cover dynamics [24].
Reference: [31] <author> Max Lemke and Daniel Quinlan. </author> <title> P++, a C++ virtual shared grids based programming environment for architecture-independent development of structured grid applications. </title> <type> Technical Report 611, </type> <institution> GMD, </institution> <month> February </month> <year> 1992. </year>
Reference-contexts: This system is targeted towards applications having a large number of small blocks and therefore allows a block to be assigned only to one processor. This system achieves only coarse grained parallelism whereas our system can achieve both coarse grained and fine grained parallelism. Quinlan <ref> [31] </ref> has developed P++, a set of C++ libraries for mesh based applications. While this library provides a convenient interface, the libraries do not optimize communication overheads. Our library, in contrast, reduces communication costs by using message aggregation 4 . The rest of this paper is organized as follows.
Reference: [32] <author> J. Li and M. Chen. </author> <title> Compiling communication-efficient programs for massively parallel machines. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 361-376, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Consequently, the data to be communicated needs to be determined at runtime. The second inner loop involves interactions among neighboring elements of the meshes. Since blocks may be partitioned across processors, this also requires communication. This form of communication can be handled by allocating ghost or overlap cells <ref> [15, 32] </ref> and filling them (by communicating with neighboring 4 processor (s)) before a parallel loop. However, if data distribution is not known at compile-time, then runtime analysis is required for determining the communication required. <p> Our library supports a communication primitive Overlap Cell Fill Sched, which computes a schedule that is used to direct the filling of overlap cells along a given dimension of a distributed array. Communication for filling in overlap cells has been implemented in other systems <ref> [5, 15, 32] </ref>, so we do not discuss the details here. The schedules produced by Overlap Cell Fill Sched and Regular Section Copy Sched are employed by a primitive called Data Move that carries out both interprocessor communication (sends and receives) and intra-processor data copying.
Reference: [33] <author> R. Mathur, L.K. Peters, and R.D. </author> <title> Saylor. Sub-grid representation of emission source clusters in regional air quality modeling. Atmospheric Environment, </title> <address> 26A:3219-3238, </address> <year> 1992. </year>
Reference-contexts: These codes are also called Irregularly Coupled Regular Mesh (ICRM) codes. They are commonly used for modeling complex objects which cannot be modeled using a single regular mesh. Examples include air quality modeling <ref> [33, 36] </ref>, computational fluid dynamics [48], structure and galaxy formation [30, 44], simulation of high performance aircrafts [1, 7, 35], large scale climate modeling [13], reservoir modeling for porous media [13], simulation of propulsion systems [13], computational combustion dynamics [13], geophysical databases [37], and land cover dynamics [24].
Reference: [34] <author> S. McCormick. </author> <title> Multilevel Projection Methods for Partial Differential Equations. </title> <publisher> SIAM, </publisher> <year> 1992. </year>
Reference-contexts: Codes for many scientific and engineering applications employ multiple structured meshes. These meshes may be nested (as in multigrid codes) or may be irregularly coupled (called Multiblock or Irregularly Coupled Regular Mesh Problems) [8]. Multigrid techniques are commonly used to accelerate iterative partial-differential equation solvers <ref> [4, 34] </ref>. Multigrid codes employ a number of meshes at different levels of resolution. In multiblock problems, the data is divided into several interacting regions (called blocks or subdomains). <p> Support for distributing loops iterations among the processors and converting global distributed arrays references to local references is also required. 2.2 Multigrid Applications Multigrid techniques are frequently used to accelerate iterative partial differential equation solvers <ref> [4, 34] </ref>. Multigrid codes employ a number of meshes at different levels of resolution.
Reference: [35] <author> R. Meakin. </author> <title> Moving body overset grid methods for complete aircraft tiltrotor simulations, </title> <booktitle> AIAA-93-3350. In Proceedings of the 11th AIAA Computational Fluid Dynamics Conference, </booktitle> <month> July </month> <year> 1993. </year>
Reference-contexts: They are commonly used for modeling complex objects which cannot be modeled using a single regular mesh. Examples include air quality modeling [33, 36], computational fluid dynamics [48], structure and galaxy formation [30, 44], simulation of high performance aircrafts <ref> [1, 7, 35] </ref>, large scale climate modeling [13], reservoir modeling for porous media [13], simulation of propulsion systems [13], computational combustion dynamics [13], geophysical databases [37], and land cover dynamics [24].
Reference: [36] <author> Odman M.T. and A.G. Russell. </author> <title> A multiscale finite element pollutant transport scheme for urban and regional modeling. Atmospheric Environment, </title> <address> 25A:2385-2398, </address> <year> 1991. </year>
Reference-contexts: These codes are also called Irregularly Coupled Regular Mesh (ICRM) codes. They are commonly used for modeling complex objects which cannot be modeled using a single regular mesh. Examples include air quality modeling <ref> [33, 36] </ref>, computational fluid dynamics [48], structure and galaxy formation [30, 44], simulation of high performance aircrafts [1, 7, 35], large scale climate modeling [13], reservoir modeling for porous media [13], simulation of propulsion systems [13], computational combustion dynamics [13], geophysical databases [37], and land cover dynamics [24].
Reference: [37] <author> Richard Muntz. </author> <type> Personal communication. </type>
Reference-contexts: Examples include air quality modeling [33, 36], computational fluid dynamics [48], structure and galaxy formation [30, 44], simulation of high performance aircrafts [1, 7, 35], large scale climate modeling [13], reservoir modeling for porous media [13], simulation of propulsion systems [13], computational combustion dynamics [13], geophysical databases <ref> [37] </ref>, and land cover dynamics [24].
Reference: [38] <author> Naomi H. Naik and John Van Rosendale. </author> <title> The improved robustness of multigrid elliptic solvers based on multiple semicoarsened grids. </title> <journal> SIAM Journal of Numerical Analysis, </journal> <volume> 30(1) </volume> <pages> 215-229, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: Accordingly, one may distribute each mesh over the entire set of processors, or some meshes may be distributed over parts of the processor space. A particular form of multigrid technique is the semi-coarsening multigrid technique <ref> [38] </ref>. Semi-coarsening multigrid works as follows. Starting from the finest mesh, coarser meshes are generated by coarsening by different factors along different dimensions. There may be many meshes, having the same number of mesh points, but obtained by different coarsening factor along each dimensions (i.e. they may have different shapes).
Reference: [39] <author> Andrea Overman and John Van Rosendale. </author> <title> Mapping robust parallel multigrid algorithms to scalable memory architectures. </title> <booktitle> In Proceedings of 1993 Copper Mountain Conference on Multigrid Methods, </booktitle> <month> April </month> <year> 1993. </year>
Reference-contexts: We present experimental results to demonstrate the efficacy of our approach. We have experimented 3 SPMD is a model of parallel programming in which all processors execute a copy of the same program text [25]. 2 with one multiblock application [48] and one multigrid code <ref> [39] </ref>. We have measured the runtime overheads of our primitives. We have compared the performance of compiler parallelized multiblock and multigrid templates with those of the manually parallelized (i.e. parallelized by inserting calls to the runtime library manually) versions. <p> In Figure 2 we show how the semi-coarsened mesh are generated and how they can be mapped to the processor space. Multigrid codes are usually written for varying resolution factors, and a varying total number of multi-grid levels <ref> [39] </ref>. If different meshes at the same level of resolution are mapped to disjoint subsets of the processor space, the data distribution for each individual meshes may not be known at compile-time. The restriction and prolongation steps require regular section moves between meshes at different levels of resolution. <p> In multi-block problems, the problem geometry is divided into a number of blocks of different sizes which should be distributed onto disjoint portions of the processor space. Similarly, in multigrid codes, communication overheads can typically be reduced by distributing each coarse mesh over a part of the processor space <ref> [39] </ref>. The current version of HPF does not provide any convenient mechanism for distributing arrays (or templates) onto a part of the processor space. <p> During each iteration on a block, these ghost cells need to be filled in, which means large amount of communication as compared to the computation involved. 5.3 Multigrid Code We have also experimented with a semi-coarsening multigrid code developed by Rosendale and Over-man <ref> [39] </ref>. This contains nearly 2,500 lines of F77 code. We discussed the semi-coarsening multigrid technique and the mapping policy used in parallelizing such an application earlier in Section 2. We rewrote this code using forall loops and including distribution directives and then parallelized it using our compiler. <p> We rewrote this code using forall loops and including distribution directives and then parallelized it using our compiler. This code had also been parallelized by inserting the calls to the library routines manually <ref> [39] </ref>. In Figure 9, we compare the performance of these two parallel versions. We did not create a separate hand parallelized F90 version since most of the subroutines in this code are fairly small and therefore rewriting it in F90 using forall loops did not involve introducing large temporary arrays.
Reference: [40] <author> A. Rogers and K. Pingali. </author> <title> Compiling for distributed memory architectures. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 5(3) </volume> <pages> 281-298, </pages> <month> March </month> <year> 1994. </year> <month> 24 </month>
Reference-contexts: Such cases, however, do not appear in codes we are familiar with and can be handled by runtime resolution <ref> [40] </ref>, in which communication for each element in the sequential loop is determined by using runtime guards. The HPF specification for forall loops allows the lower bound, upper bound and stride expressions for each loop variable to be evaluated in any order.
Reference: [41] <author> Z. Shen, Z. Li, and P.-C. Yew. </author> <title> An empirical study of Fortran programs for parallelizing compilers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(3) </volume> <pages> 330-343, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: Runtime support and compiler analysis have been developed for executing this class of applications on distributed memory machines [9, 11, 26]. In certain applications such as single regular mesh based codes, compile-time techniques suffice for optimizing communication and obtaining good performance <ref> [41] </ref>. Multiblock and multigrid codes is an important class of applications which do not use indirection arrays but still need runtime analysis for efficient execution.
Reference: [42] <author> J. Stichnoth, D. O'Hallaron, and T. Gross. </author> <title> Generating communication for array statements: Design, implementation, and evaluation. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> 1994. to appear. </note>
Reference-contexts: The details of these calculations, for block and cyclic distributions, have been developed by Koelbel [28]. Recently, in separate work, Chatterjee et al. [6], Stichnoth <ref> [42, 43] </ref> and Gupta et al. [16, 17] have developed methods for analyzing communication in the general block-cyclic distribution case. Our current implementation supports regular section moves only for the block distribution case, since block distribution is the proper one to use for multiblock and multigrid applications.
Reference: [43] <author> James M. Stichnoth. </author> <title> Efficient compilation of array statements for private memory multicomputers. </title> <type> Technical Report CMU-CS-93-109, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: The details of these calculations, for block and cyclic distributions, have been developed by Koelbel [28]. Recently, in separate work, Chatterjee et al. [6], Stichnoth <ref> [42, 43] </ref> and Gupta et al. [16, 17] have developed methods for analyzing communication in the general block-cyclic distribution case. Our current implementation supports regular section moves only for the block distribution case, since block distribution is the proper one to use for multiblock and multigrid applications. <p> Our current implementation supports regular section moves only for the block distribution case, since block distribution is the proper one to use for multiblock and multigrid applications. However, a primitive with such functionality can be implemented for the general block-cyclic case using methods described in <ref> [6, 16, 43] </ref>. A regular section move primitive can be invoked for analyzing the communication associated with any forall loop, but this may result in unnecessarily high runtime overheads. <p> Our current implementation is restricted to doing these transformations for block distribution, but similar functionality can be provided for the general block-cyclic case <ref> [6, 16, 43] </ref>. These communication and loop partitioning primitives are also applicable when the data distribution is known at compile-time, but the data to be communicated and the loop bounds on each processor cannot be determined because of symbolic loop bounds and/or strides. <p> At compile-time, if the data is not known to distributed in block or cyclic manner, then multiple nested do loops need to be generated for handling the loop <ref> [6, 43] </ref>. In Figure 4 we show an example of how the calls to primitives for filling in overlap cells are inserted by the compiler. In this example, the loop bounds and strides are known at compile-time, but exact data distribution is not known at compile time.
Reference: [44] <author> J.M. Stone and M.L. Norman. Zeus-2d: </author> <title> A radiation magnetohydrodynamics code for astrophysical flows in two space dimensions: I. the hydrodynamic algorithms and tests. </title> <journal> The Astrophysical Journal Supplements, </journal> <volume> 80(753), </volume> <year> 1992. </year>
Reference-contexts: These codes are also called Irregularly Coupled Regular Mesh (ICRM) codes. They are commonly used for modeling complex objects which cannot be modeled using a single regular mesh. Examples include air quality modeling [33, 36], computational fluid dynamics [48], structure and galaxy formation <ref> [30, 44] </ref>, simulation of high performance aircrafts [1, 7, 35], large scale climate modeling [13], reservoir modeling for porous media [13], simulation of propulsion systems [13], computational combustion dynamics [13], geophysical databases [37], and land cover dynamics [24].
Reference: [45] <author> Alan Sussman, Gagan Agrawal, and Joel Saltz. </author> <title> A manual for the multiblock PARTI runtime primitives, revision 4.1. </title> <institution> Technical Report CS-TR-3070.1 and UMIACS-TR-93-36.1, University of Maryland, Department of Computer Science and UMIACS, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: We call our runtime library the Multiblock Parti library <ref> [45] </ref>. <p> We do not discuss the details of the primitives which handle data distribution specification; for more details, see <ref> [45] </ref>. In our runtime system, communication is performed in two phases.
Reference: [46] <author> R. Thakur, A. Choudhary, and G. Fox. </author> <title> Runtime array redistribution in HPF programs. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference (SHPCC-94), </booktitle> <month> May </month> <year> 1994. </year> <note> to appear. </note>
Reference-contexts: Our library supports dynamic data distributions, but we have not concentrated on efficient algorithms for data redistribution at runtime <ref> [46] </ref>. This library is currently implemented on the Intel iPSC/860 and Paragon, the Thinking Machines' CM-5 and the PVM message passing environment for a network of workstations [14].
Reference: [47] <author> C.-W. Tseng, S. Hiranandani, and K. Kennedy. </author> <title> Preliminary experiences with the Fortran D compiler. </title> <booktitle> In Proceedings Supercomputing '93, </booktitle> <pages> pages 338-350. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1993. </year>
Reference-contexts: In the Fortran D compiler developed at Rice university, detailed communication optimizations and compilation techniques based upon dependence analysis have been investigated in detail, but the focus is on cases when compile-time analysis alone suffices <ref> [21, 47] </ref>. This compiler has also been 5 6 extended to use runtime support for codes having indirection arrays [19]. The Vienna Fortran Compi--lation System developed at the University of Vienna [5] has similar limitations.
Reference: [48] <author> V.N. Vatsa, M.D. Sanetrik, and E.B. Parlette. </author> <title> Development of a flexible and efficient multigrid-based multi-block flow solver; AIAA-93-0677. </title> <booktitle> In Proceedings of the 31st Aerospace Sciences Meeting and Exhibit, </booktitle> <month> January </month> <year> 1993. </year>
Reference-contexts: We present experimental results to demonstrate the efficacy of our approach. We have experimented 3 SPMD is a model of parallel programming in which all processors execute a copy of the same program text [25]. 2 with one multiblock application <ref> [48] </ref> and one multigrid code [39]. We have measured the runtime overheads of our primitives. We have compared the performance of compiler parallelized multiblock and multigrid templates with those of the manually parallelized (i.e. parallelized by inserting calls to the runtime library manually) versions. <p> These codes are also called Irregularly Coupled Regular Mesh (ICRM) codes. They are commonly used for modeling complex objects which cannot be modeled using a single regular mesh. Examples include air quality modeling [33, 36], computational fluid dynamics <ref> [48] </ref>, structure and galaxy formation [30, 44], simulation of high performance aircrafts [1, 7, 35], large scale climate modeling [13], reservoir modeling for porous media [13], simulation of propulsion systems [13], computational combustion dynamics [13], geophysical databases [37], and land cover dynamics [24]. <p> Since the number of blocks is typically quite small (i.e. at most a few dozens), at least some of the blocks will have to be distributed across multiple processors. Multiblock applications are typically written for a variable number of meshes and mesh sizes <ref> [48] </ref>. If this is the case, the data distribution can only be decided after reading in the number of meshes and their sizes. Two types of communication are required in multiblock applications. The interaction between different blocks requires bulk data movement, which we call a regular section move. <p> We have parallelized a template from a multiblock computation fluid dynamics application that solves the thin-layer Navier-Stokes equations over a 3D surface (multiblock TLNS3D). The sequential F77 code was developed by Vatsa et al. <ref> [48] </ref> at NASA Langley Research Center, and consists of nearly 18,000 lines of code. The template, which was designed to include portions of the entire code that are representative of the major computation and communication patterns of the original code, consists of nearly 2,000 lines of F77 code.
Reference: [49] <author> H. Zima, P. Brezany, B. Chapman, P. Mehrotra, and A. Schwald. </author> <title> Vienna Fortran | a language specification, version 1.1. </title> <type> Interim Report 21, </type> <institution> ICASE, NASA Langley Research Center, </institution> <month> March </month> <year> 1992. </year> <month> 25 </month>
Reference-contexts: The authors assume all responsibility for the contents of the paper. 1 Recently there have been major efforts in developing programming language and compiler support for distributed memory machines. Based on the initial works of projects like Fortran D [12, 23] and Vienna Fortran <ref> [5, 49] </ref> , the High Performance Fortran Forum has recently defined the first version of High Performance Fortran (HPF) [25]. HPF allows programmer to specify the layout of distributed data and specify parallelism through operations on array sections and through parallel loops. <p> Once a processor subspace has been declared, arrays or templates can be distributed onto it. For example, $ TEMPLATE T (100,100) $ DISTRIBUTE T (BLOCK,BLOCK) ONTO P1 5 Similar functionality is available in Vienna Fortran <ref> [49] </ref>, where a distribution can be mapped to a set of processors. The support for distributing and aligning arrays in current version of HPF may not always be ideal for load balancing in many applications. <p> Vienna Fortran <ref> [49] </ref> allows general distribution and alignment functions which can be used to achieve such support for external ghost cells. Note that our purpose here is not to introduce new syntax but to achieve the additional functionality that we need.
References-found: 49

