URL: ftp://ftp.cis.upenn.edu/pub/pelachaud/esca94/esca94.ps.Z
Refering-URL: http://www.cis.upenn.edu/~hms/publications.html
Root-URL: 
Email: Internet: pelachau@graphics.cis.upenn.edu, prevost@linc.cis.upenn.edu  
Title: SIGHT AND SOUND: GENERATING FACIAL EXPRESSIONS AND SPOKEN INTONATION FROM CONTEXT  
Author: Catherine Pelachaud and Scott Prevost 
Address: 200 South 33rd Street, Philadelphia, PA 19104-6389, USA  
Affiliation: Computer and Information Science, University of Pennsylvania  
Abstract: This paper presents an implemented system for automatically producing prosodically appropriate speech and corresponding facial expressions for animated, three-dimensional agents that respond to simple database queries. Unlike previous text-to-facial animation approaches, the system described here produces synthesized speech and facial animations entirely from scratch, starting with semantic representations of the message to be conveyed, which are based in turn on a discourse model and a small database of facts about the modeled world. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Benoit. </author> <booktitle> Why synthesize talking faces? In Proceedings of the ESCA Workshopon Speech Synthesis, </booktitle> <pages> pages 253-256, </pages> <address> Autrans, </address> <year> 1990. </year> <pages> ESCA. </pages>
Reference: [2] <author> N.M. Brooke. </author> <title> Computer graphics synthesis of talking faces. </title> <booktitle> In Proceedings of the ESCA Workshop on Speech Synthesis, Autrans, 1990. ESCA. </booktitle>
Reference: [3] <author> J. Cassell, C. Pelachaud, N. Badler, M. Steedman, B. Achorn, T. Becket, B. Douville, S. Prevost, and M. Stone. </author> <title> Animated conversation: Rule based generation of facial expression, gesture and spoken intonation for multiple conversational agents. </title> <booktitle> In SIGGRAPH'94, </booktitle> <year> 1994. </year> <note> To appear. </note>
Reference-contexts: N00014-90-J-1863, and ARO grant no. DAAL03-89-C0031. protocols and speaker attitudes ([7], [8], [14]). We propose that integrating models for generating proper intonation and facial expressions will improve the intelligibility and naturalness of utterances produced both by meaning-to-speech systems and by more elaborate systems involving virtual animated human agents (e.g. <ref> [3] </ref>). The intonation generation model is based on Combinatory Categorial Grammar (CCG - cf. [19]), a formalism which easily integrates the notions of syntactic constituency, prosodic phrasing and information structure. <p> The duration specifications are then automatically annotated with pitch accent peaks and intonational boundaries in preparation for processing by the facial expression rules (see also <ref> [3] </ref>). The facial animation system starts from a functional group including lip shapes, conversational signals, punctuator signals, regulators and manipulators, offering algorithms which incorporate synchrony ([5]), create coarticu-lation effects, emotional signals, and eye and head movements ([15], [16]). The rules automatically generate the facial actions corresponding to the input utterance. <p> Future areas of research include evaluating results and exploring the relevance of our current system to large scale animation systems involving autonomous virtual human agents (cf. <ref> [3] </ref>).
Reference: [4] <author> Michael M. Cohen and Dominic W. Massaro. </author> <title> Modeling coarticulation in synthetic visual speech. </title> <editor> In D. Thalmann N. Magnenat-Thalmann, editor, </editor> <title> Computer Animation '93. </title> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference: [5] <author> W.S. Condon and W.D. Osgton. </author> <title> Speech and body motion synchrony of the speaker-hearer. </title> <editor> In D.H. Horton and J.J. Jenkins, editors, </editor> <booktitle> The perception of Language, </booktitle> <pages> pages 150-184. </pages> <publisher> Academic Press, </publisher> <year> 1971. </year>
Reference: [6] <author> J. Davis and J. Hirschberg. </author> <title> Assigning intonational features in synthesized spoken discourse. </title> <booktitle> In Proceedings of the 26th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 187-193, </pages> <address> Buffalo, </address> <year> 1988. </year>
Reference-contexts: Previous work in the area of intonation generation includes studies by Terken ([21]), Houghton, Isard and Pearson (cf. [11]), Davis and Hirschberg (cf. <ref> [6] </ref>, [10]), and Zacharski et al. ([23]). Benoit et al. ([1]), Brooke ([2]), Cohen et al. ([4]), Hill et al. ([9]), Lewis et al. ([12]) and Terzopoulos et al. ([22]) have worked on synchronizing lip movements with speech, producing quite striking results.
Reference: [7] <author> S. Duncan. </author> <title> Some signals and rules for taking speaking turns in conversations. In Weitz, editor, Nonverbal Communication. </title> <publisher> Oxford University Press, </publisher> <year> 1974. </year>
Reference: [8] <author> P. Ekman. </author> <title> About brows: emotional and conversational signals. </title> <editor> In M. von Cranach, K. Foppa, W. Lepenies, and D. Ploog, editors, </editor> <title> Human ethology: claims and limits of a new discipline: </title> <booktitle> contributions to the Colloquium, </booktitle> <pages> pages 169-248. </pages> <publisher> Cambridge University Press, </publisher> <address> Cambridge, England; New-York, </address> <year> 1979. </year>
Reference-contexts: The usual disclaimers apply. The research was supported in part by NSF grant nos. IRI90-18513, IRI90-16592, IRI91-17110 and CISE IIP-CDA-88-22719, DARPA grant no. N00014-90-J-1863, and ARO grant no. DAAL03-89-C0031. protocols and speaker attitudes ([7], <ref> [8] </ref>, [14]). We propose that integrating models for generating proper intonation and facial expressions will improve the intelligibility and naturalness of utterances produced both by meaning-to-speech systems and by more elaborate systems involving virtual animated human agents (e.g. [3]).
Reference: [9] <author> D.R. Hill, A. Pearce, and B. Wyvill. </author> <title> Animating speech: an automated approach using speech synthesised by rules. </title> <journal> The Visual Computer, </journal> <volume> 3 </volume> <pages> 277-289, </pages> <year> 1988. </year>
Reference: [10] <author> J. Hirschberg. </author> <title> Accent and discourse context: Assigning pitch accent in synthetic speech. </title> <booktitle> In Proceedings of AAAI: </booktitle> <year> 1990, </year> <pages> pages 952-957, </pages> <year> 1990. </year>
Reference-contexts: Previous work in the area of intonation generation includes studies by Terken ([21]), Houghton, Isard and Pearson (cf. [11]), Davis and Hirschberg (cf. [6], <ref> [10] </ref>), and Zacharski et al. ([23]). Benoit et al. ([1]), Brooke ([2]), Cohen et al. ([4]), Hill et al. ([9]), Lewis et al. ([12]) and Terzopoulos et al. ([22]) have worked on synchronizing lip movements with speech, producing quite striking results.
Reference: [11] <author> G. Houghton and M. Pearson. </author> <title> The production of spoken dialogue. </title> <editor> In M. Zock and G. Sabah, editors, </editor> <booktitle> Advances in Natural Language Generation: An Interdisciplinary Perspective, </booktitle> <volume> Vol. 1. </volume> <publisher> Pinter Publishers, </publisher> <address> London, </address> <year> 1988. </year>
Reference-contexts: Results from our current implementation demonstrate the system's ability to generate a variety of intonational possibilities and facial animations for a given sentence depending on the discourse context. Previous work in the area of intonation generation includes studies by Terken ([21]), Houghton, Isard and Pearson (cf. <ref> [11] </ref>), Davis and Hirschberg (cf. [6], [10]), and Zacharski et al. ([23]). Benoit et al. ([1]), Brooke ([2]), Cohen et al. ([4]), Hill et al. ([9]), Lewis et al. ([12]) and Terzopoulos et al. ([22]) have worked on synchronizing lip movements with speech, producing quite striking results.
Reference: [12] <author> J.P. Lewis and F.I. Parke. </author> <title> Automated lip-synch and speech synthesis for character animation. </title> <booktitle> CHI + GI, </booktitle> <pages> pages 143-147, </pages> <year> 1987. </year>
Reference: [13] <author> M. Liberman and A. L. Buchsbaum. </author> <title> Structure and usage of current Bell Labs text to speech programs. </title> <type> Technical Memorandum TM 11225-850731-11, </type> <institution> AT&T Bell Laboratories, </institution> <year> 1985. </year>
Reference-contexts: H* L L+H* LH$ The final aspect of speech generation involves translating such a string into a form usable by a suitable speech synthesizer. The current implementation uses the Bell Labora--tories TTS system <ref> [13] </ref> as a post-processor to synthesize the speech wave and produce precise timing specifications for phonemes. The duration specifications are then automatically annotated with pitch accent peaks and intonational boundaries in preparation for processing by the facial expression rules (see also [3]).
Reference: [14] <author> D.W. Massaro. </author> <title> Speech Perception byEar and Eye: A Paradigm for Psychological Inquiry. </title> <publisher> Cambridge University Press, </publisher> <year> 1989. </year>
Reference-contexts: The usual disclaimers apply. The research was supported in part by NSF grant nos. IRI90-18513, IRI90-16592, IRI91-17110 and CISE IIP-CDA-88-22719, DARPA grant no. N00014-90-J-1863, and ARO grant no. DAAL03-89-C0031. protocols and speaker attitudes ([7], [8], <ref> [14] </ref>). We propose that integrating models for generating proper intonation and facial expressions will improve the intelligibility and naturalness of utterances produced both by meaning-to-speech systems and by more elaborate systems involving virtual animated human agents (e.g. [3]).
Reference: [15] <author> C. Pelachaud, N.I. Badler, and M. Steedman. </author> <title> Linguistic issues in facial animation. </title> <editor> In N. Magnenat-Thalmann and D. Thalmann, editors, </editor> <booktitle> Computer Animation '91, </booktitle> <pages> pages 15-30. </pages> <publisher> Springer-Verlag, </publisher> <year> 1991. </year>
Reference: [16] <author> C. Pelachaud, M.L. Viaud, and H. Yahia. </author> <title> Rule-structured facial animation system. </title> <booktitle> In IJCAI 93, </booktitle> <year> 1993. </year>
Reference-contexts: The facial animation system starts from a functional group including lip shapes, conversational signals, punctuator signals, regulators and manipulators, offering algorithms which incorporate synchrony ([5]), create coarticu-lation effects, emotional signals, and eye and head movements ([15], <ref> [16] </ref>). The rules automatically generate the facial actions corresponding to the input utterance. Conversational signals, such as movements occurring on accents (e.g. the raising of an eyebrow), start and end with the accented word.
Reference: [17] <author> S. Prevost and M. Steedman. </author> <title> Generating contextually appropriate intonation. </title> <booktitle> In Proceedings of the 6th Conference of the European Chapter of the Association for Computational Linguistics, </booktitle> <pages> pages 332-340, </pages> <address> Utrecht, </address> <year> 1993. </year>
Reference-contexts: 1 Introduction As research on the simulation of autonomous virtual human agents progresses, two major issues in human-machine interaction must be addressed. First, proper intonation is necessary for conveying the information structure of utterances with respect to the underlying discourse structure, expressing important distinctions of contrast and focus ([19], <ref> [17] </ref>, [18]). Second, realistic facial expressions and lip movements help in providing relevant information about discourse structure, turn-taking fl We are grateful to AT&T Bell Laboratories for allowing us access to the TTS speech synthesizer, and to Mark Beutnagel, Julia Hirschberg, and Richard Sproat for patient advice on its use. <p> The system described here expands the work of the aforementioned researchers by linking contextually appropriate intonation with the corresponding facial expressions, and generating the 3D facial animations automatically from semantic, information structural and discourse structural representations. 2 The Implementation Using the CCG theory of prosody outlined in [19], <ref> [17] </ref> and [18], the implemented system undertakes the task of specifying contextually appropriate intonation and facial animation for spoken responses to database queries. <p> appropriate rheme as amplifiers, and where the context includes alternative components and audio qualities: (3) Proposition: s : produce (flamplif iers; flclean (bass)) Theme: s : produce (x; flclean (bass))nnp : x Rheme: np : flamplif iers From the output of the content generator, the CCG generation module (described in <ref> [17] </ref>) produces a string of words and Pierrehumbert-style markings representing the response, as shown in example (4). (4) AMPLIFIERS produce CLEAN bass. H* L L+H* LH$ The final aspect of speech generation involves translating such a string into a form usable by a suitable speech synthesizer.
Reference: [18] <author> S. Prevost and M. Steedman. </author> <title> Using context to specify intonation in speech synthesis. </title> <booktitle> In Proceedings of the 3rd European Conference of Speech Communication and Technology (EU-ROSPEECH), </booktitle> <pages> pages 2103-2106, </pages> <address> Berlin, </address> <year> 1993. </year>
Reference-contexts: 1 Introduction As research on the simulation of autonomous virtual human agents progresses, two major issues in human-machine interaction must be addressed. First, proper intonation is necessary for conveying the information structure of utterances with respect to the underlying discourse structure, expressing important distinctions of contrast and focus ([19], [17], <ref> [18] </ref>). Second, realistic facial expressions and lip movements help in providing relevant information about discourse structure, turn-taking fl We are grateful to AT&T Bell Laboratories for allowing us access to the TTS speech synthesizer, and to Mark Beutnagel, Julia Hirschberg, and Richard Sproat for patient advice on its use. <p> The system described here expands the work of the aforementioned researchers by linking contextually appropriate intonation with the corresponding facial expressions, and generating the 3D facial animations automatically from semantic, information structural and discourse structural representations. 2 The Implementation Using the CCG theory of prosody outlined in [19], [17] and <ref> [18] </ref>, the implemented system undertakes the task of specifying contextually appropriate intonation and facial animation for spoken responses to database queries. <p> x:component (x)&produce (x; flclean (bass))= (s : produce (x; flclean (bass))nnp : x) Rheme: s : produce (x; flclean (bass))nnp : x The content generation module has the task of determining the semantics and information structure of the response, marking focused items based on the contrastive stress algorithm described in <ref> [18] </ref>.
Reference: [19] <author> M. Steedman. </author> <title> Structure and intonation. </title> <booktitle> Language, </booktitle> <pages> pages 260-296, </pages> <year> 1991. </year>
Reference-contexts: The intonation generation model is based on Combinatory Categorial Grammar (CCG - cf. <ref> [19] </ref>), a formalism which easily integrates the notions of syntactic constituency, prosodic phrasing and information structure. Based on the CCG grammar, a simple discourse model and a small knowledge base represented in Prolog, the system produces spoken responses to database queries with appropriate intonation. <p> The system described here expands the work of the aforementioned researchers by linking contextually appropriate intonation with the corresponding facial expressions, and generating the 3D facial animations automatically from semantic, information structural and discourse structural representations. 2 The Implementation Using the CCG theory of prosody outlined in <ref> [19] </ref>, [17] and [18], the implemented system undertakes the task of specifying contextually appropriate intonation and facial animation for spoken responses to database queries.
Reference: [20] <author> A. Takeuchi and K. Nagao. </author> <title> Communicative facial displays as a new conversational modality. </title> <booktitle> In ACM/IFIP INTERCHI '93, </booktitle> <address> Amsterdam, </address> <year> 1993. </year>
Reference: [21] <author> J. Terken. </author> <title> The distribution of accents in instructions as a function of discourse structure. </title> <booktitle> Language and Structure, </booktitle> <volume> 27 </volume> <pages> 269-289, </pages> <year> 1984. </year>
Reference: [22] <author> D. Terzopoulos and K. Waters. </author> <title> Techniques for realistic facial modelling and animation. </title> <editor> In N. Magnenat-Thalmann and D. Thalmann, editors, </editor> <booktitle> Computer Animation '91, </booktitle> <pages> pages 45-58. </pages> <publisher> Springer-Verlag, </publisher> <year> 1991. </year>
Reference: [23] <author> R. Zacharski, A.I.C. Monaghan, D.R. Ladd, and J. Delin. </author> <title> BRIDGE: Basic research on intonation in dialogue generation. </title> <type> Technical report, HCRC: </type> <institution> University of Edinburgh, </institution> <year> 1993. </year> <type> Unpublished manuscript. </type>
References-found: 23

