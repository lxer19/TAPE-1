URL: http://www.cs.columbia.edu/~simonb/papers/ls_smile.ps.gz
Refering-URL: http://www.cs.columbia.edu/~simonb/pub.html
Root-URL: http://www.cs.columbia.edu
Title: Interactive 3D Modeling from Multiple Images using Scene Regularities  
Author: Heung-Yeung Shum, Richard Szeliski, Simon Baker Mei Hanz, and P. Anandan 
Web: http://www.research.microsoft.com/research/vision/  
Affiliation: Microsoft Research, Columbia University, Carnegie Mellon University  
Abstract: We present some recent progress in designing and implementing two interactive image-based 3D modeling systems. The first system constructs 3D models from a collection of panoramic image mosaics. A panoramic mosaic consists of a set of images taken around the same viewpoint, and a camera matrix associated with each input image. The user first interactively specifies features such as points, lines, and planes. Our system recovers the camera pose for each mosaic from known line directions and reference points. It then constructs the 3D model using all available geometrical constraints. The second system extracts structure from stereo by representing the scene as a collection of approximately planar layers. The user first interactively segments the images into corresponding planar regions. Our system recovers a composite mosaic for each layer, estimates the plane equation for the layer, and optionally recovers the camera locations as well as out-of-plane displacements. By taking advantage of known scene regularities, our interactive systems avoid difficult feature correspondence problems that occur in traditional automatic modeling systems. They also shift the interactive high-level structural model specification stage to precede (or intermix with) the 3D geometry recovery. They are thus able to extract accurate wire frame and texture-mapped 3D models from multiple image sequences.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> O. D. Faugeras. </author> <title> What can be seen in three dimensions with an uncalibrated stereo rig? In Second European Conference on Computer Vision (ECCV'92), </title> <address> pages 563-578, Santa Margherita Liguere, Italy, May 1992. </address> <publisher> Springer-Verlag. </publisher>
Reference: 2. <author> R. Mohr, L. Veillon, and L. Quan. </author> <title> Relative 3D reconstruction using multiple uncalibrated images. </title> <booktitle> In IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'93), </booktitle> <pages> pages 543-548, </pages> <address> New York, New York, </address> <month> June </month> <year> 1993. </year>
Reference: 3. <author> O. Faugeras. </author> <title> Three-dimensional computer vision: A geometric viewpoint. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1993. </year>
Reference-contexts: The input consists of a collection of images I k taken with known camera matrices P k . The camera matrices can be estimated when they are not known a priori, either using traditional structure from motion <ref> [3, 7, 8] </ref>, or directly from the homographies relating sprites in different images [32, 33]. Our goal is to estimate the layer sprites L l , the plane vectors n l , and the residual depths Z l .
Reference: 4. <author> A. Shashua. </author> <title> Projective structure from uncalibrated images: Structure from motion and recognition. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 16(8) </volume> <pages> 778-790, </pages> <month> August </month> <year> 1994. </year>
Reference: 5. <author> A. Azarbayejani and A. P. Pentland. </author> <title> Recursive estimation of motion, structure, and focal length. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 17(6) </volume> <pages> 562-575, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Moreover, such correspondences may not be available in regions of the scene that are untextured. Automated techniques often require manual clean-up and post-processing to segment the scene into coherent objects and surfaces, or to triangulate sparse point matches <ref> [5] </ref>. They may also be required to enforce geometric constraints such as known orientations of surfaces. For instance, building interiors and exte-riors provide vertical and horizontal lines and parallel and perpendicular planes.
Reference: 6. <author> O. D. Faugeras, Laveau S., Robert L., Csurka G., and Zeller C. </author> <title> 3-D reconstruction of urban scenes from sequences of images. </title> <booktitle> Computer Vision and Image Understanding, </booktitle> <volume> 69(3) </volume> <pages> 292-309, </pages> <month> March </month> <year> 1998. </year>
Reference: 7. <author> P. Beardsley, P. Torr, and A. Zisserman. </author> <title> 3D model acquisition from extended image sequences. </title> <booktitle> In Fourth European Conference on Computer Vision (ECCV'96), </booktitle> <volume> volume 2, </volume> <pages> pages 683-695, </pages> <address> Cambridge, England, April 1996. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: The input consists of a collection of images I k taken with known camera matrices P k . The camera matrices can be estimated when they are not known a priori, either using traditional structure from motion <ref> [3, 7, 8] </ref>, or directly from the homographies relating sprites in different images [32, 33]. Our goal is to estimate the layer sprites L l , the plane vectors n l , and the residual depths Z l . <p> We also plan to incorporate automatic line detection, corner detection as well as inter-image correspondence and other feature detections to further automate the system. If we use more features with automatic feature extraction and correspondence techniques, robust modeling techniques should also be developed <ref> [7] </ref>. For the layered stereo modeling system, we plan to automate the interactive masking process by only specifying layers in few (e.g., the first and the last) images and by incorporating motion segmentation and color segmentation techniques.
Reference: 8. <author> M. Pollefeys, R. Koch, and L. Van Gool. </author> <title> Self-calibration and metric reconstruction in spite of varying and unknown internal camera parameters. </title> <booktitle> In Sixth International Conference on Computer Vision (ICCV'98), </booktitle> <pages> pages 90-95, </pages> <address> Bombay, </address> <month> January </month> <year> 1998. </year>
Reference-contexts: Using multiple panoramas, more complete and accurate 3D models can be constructed. Multiple-image stereo matching can be used to recover a more detailed description of surface shape than can be obtained by simply triangulating matched feature points <ref> [8] </ref>. Unfortunately, stereo fails in regions without texture. A simple depth map also cannot capture the full complexity of a large-scale environment. Methods for overcoming this limitation include volumetric stereo techniques [18, 19] and model-based stereo [12]. <p> The input consists of a collection of images I k taken with known camera matrices P k . The camera matrices can be estimated when they are not known a priori, either using traditional structure from motion <ref> [3, 7, 8] </ref>, or directly from the homographies relating sprites in different images [32, 33]. Our goal is to estimate the layer sprites L l , the plane vectors n l , and the residual depths Z l .
Reference: 9. <author> P. Torr, A. W. Fitzgibbon, and A. Zisserman. </author> <title> Maintaining multiple motion model hypotheses over many views to recover matching structure. </title> <booktitle> In Sixth International Conference on Computer Vision (ICCV'98), </booktitle> <pages> pages 485-491, </pages> <address> Bombay, </address> <month> January </month> <year> 1998. </year>
Reference: 10. <author> Photomodeler. </author> <note> //www.photomodeler.com. </note>
Reference-contexts: The idea of using geometric constraints has previously been exploited in several interactive modeling systems. For example, PhotoModeler <ref> [10] </ref> is a comme-cial product which constructs 3D models from several images, using photogram-metry techniques and manually specified points. The TotalCalib system, on the other hand, estimates the fundamental matrix from a few hand-matched points, and then predicts and verifies other potential matching points [11].
Reference: 11. <author> S. Bougnoux and L. Robert. Totalcalib: </author> <title> a fast and reliable system for off-line calibration of image sequences. </title> <booktitle> In IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'97), </booktitle> <month> June </month> <year> 1997. </year> <note> The Demo Session. </note>
Reference-contexts: For example, PhotoModeler [10] is a comme-cial product which constructs 3D models from several images, using photogram-metry techniques and manually specified points. The TotalCalib system, on the other hand, estimates the fundamental matrix from a few hand-matched points, and then predicts and verifies other potential matching points <ref> [11] </ref>. The Facade system exploits the known rectahedral structure of building exteriors to directly recover solid 3D models (blocks) from multiple images [12]. This paper presents two interactive (semi-automated) systems for recovering 3D models of large-scale environments from multiple images.
Reference: 12. <author> P. E. Debevec, C. J. Taylor, and J. Malik. </author> <title> Modeling and rendering architecture from photographs: A hybrid geometry- and image-based approach. </title> <journal> Computer Graphics (SIGGRAPH'96), </journal> <pages> pages 11-20, </pages> <month> August </month> <year> 1996. </year>
Reference-contexts: The TotalCalib system, on the other hand, estimates the fundamental matrix from a few hand-matched points, and then predicts and verifies other potential matching points [11]. The Facade system exploits the known rectahedral structure of building exteriors to directly recover solid 3D models (blocks) from multiple images <ref> [12] </ref>. This paper presents two interactive (semi-automated) systems for recovering 3D models of large-scale environments from multiple images. Our first system uses one or more panoramic image mosaics, i.e., collections of images taken from the same viewpoint that have been registered together [13]. Panoramas offer several advantages over regular images. <p> Unfortunately, stereo fails in regions without texture. A simple depth map also cannot capture the full complexity of a large-scale environment. Methods for overcoming this limitation include volumetric stereo techniques [18, 19] and model-based stereo <ref> [12] </ref>. In this paper, we propose a different approach|extending the concept of layered motion estimates [20, 21] and "shallow" objects [22] to true multi-image stereo matching. <p> In this case, the homographic warps used in the previous section are not applicable. However, using a similar argument to that in Sections 3.2 and 3.3, it is easy to show (see also <ref> [36, 12] </ref>) that: u k = H l where H l k = P k (n T l Q fl l is the planar homography of Section 3.3, t kl = P k q l is the epipole, and it is assumed that the plane equation vector n l = (n <p> To compute the residual depth map Z l , we could optimize the same (or a similar) consistency metric as that used in Section 2.2 to estimate the plane equation. Doing so is essentially solving a simpler (or what <ref> [12] </ref> would call "model-based") stereo problem. In fact, almost any stereo algorithm could be used to compute Z l . <p> We are also planning to combine our layered stereo modeling system with the panorama modeling system. The idea is to build a rough model using panorama modeling system and refine it using layered stereo wherever it is appropriate (similar in spirit to the model-based stereo of <ref> [12] </ref>). We are also investigating representations beyond texture-mapped 3D models, i.e, image-based rendering approaches [42] such as view-dependent texture maps [12] and layered depth images [43]. Integrating all of these into one interactive modeling system will enable users to easily construct complex photorealistic 3D models from images. <p> The idea is to build a rough model using panorama modeling system and refine it using layered stereo wherever it is appropriate (similar in spirit to the model-based stereo of <ref> [12] </ref>). We are also investigating representations beyond texture-mapped 3D models, i.e, image-based rendering approaches [42] such as view-dependent texture maps [12] and layered depth images [43]. Integrating all of these into one interactive modeling system will enable users to easily construct complex photorealistic 3D models from images.
Reference: 13. <author> H.-Y. Shum, M. Han, and R. Szeliski. </author> <title> Interactive construction of 3d models from panoramic mosaics. </title> <booktitle> In IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'98), </booktitle> <pages> pages 427-433, </pages> <address> Santa Barbara, </address> <month> June </month> <year> 1998. </year>
Reference-contexts: This paper presents two interactive (semi-automated) systems for recovering 3D models of large-scale environments from multiple images. Our first system uses one or more panoramic image mosaics, i.e., collections of images taken from the same viewpoint that have been registered together <ref> [13] </ref>. Panoramas offer several advantages over regular images. First, we can decouple the modeling problem into a zero baseline problem (building panoramas from images taken with rotating camera) and a wide baseline stereo or structure from motion problem (recovering 3D model from one or more panoramas).
Reference: 14. <author> R. I. </author> <title> Hartley. Self-calibration from multiple views of a rotating camera. </title> <booktitle> In Third European Conference on Computer Vision (ECCV'94), </booktitle> <volume> volume 1, </volume> <pages> pages 471-478, </pages> <address> Stockholm, Sweden, May 1994. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Second, the intrinsic camera calibrations are recovered as part of the panorama construction <ref> [14] </ref>. Due to recent advances, it is now possible to construct panoramas even with hand-held cameras [15]. Unlike previous work on 3D reconstruction from multiple panoramas [16, 17], our 3D modeling system exploits important regularities present in the environment, such as walls with known orientations.
Reference: 15. <author> R. Szeliski and H.-Y. Shum. </author> <title> Creating full view panoramic image mosaics and texture-mapped models. </title> <journal> Computer Graphics (SIGGRAPH'97), </journal> <pages> pages 251-258, </pages> <month> August </month> <year> 1997. </year>
Reference-contexts: Second, the intrinsic camera calibrations are recovered as part of the panorama construction [14]. Due to recent advances, it is now possible to construct panoramas even with hand-held cameras <ref> [15] </ref>. Unlike previous work on 3D reconstruction from multiple panoramas [16, 17], our 3D modeling system exploits important regularities present in the environment, such as walls with known orientations.
Reference: 16. <author> L. McMillan and G. Bishop. </author> <title> Plenoptic modeling: An image-based rendering system. </title> <journal> Computer Graphics (SIGGRAPH'95), </journal> <pages> pages 39-46, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: Second, the intrinsic camera calibrations are recovered as part of the panorama construction [14]. Due to recent advances, it is now possible to construct panoramas even with hand-held cameras [15]. Unlike previous work on 3D reconstruction from multiple panoramas <ref> [16, 17] </ref>, our 3D modeling system exploits important regularities present in the environment, such as walls with known orientations. Fortunately, the man-made world is full of constraints such as parallel lines, lines with known directions, planes with lines and points on them. <p> Texture maps for the 3D model. Fig. 4. Two views of the interactive system. Fig. 5. A more complex 3D model from a single panorama. Fig. 6. Two input panoramas of an indoor scene. Fig. 7. Two views of a 3D model from multiple panoramas. painted depth map <ref> [16] </ref>. Figure 8 (f) shows the depth map computed by painting every pixel in every sprite with its corresponding color coded Z value, and then re-compositing the image. Notice how the depth discontinuities are much crisper and cleaner than those available with traditional stereo correspondence algorithms.
Reference: 17. <author> S. B. Kang and R. Szeliski. </author> <title> 3-D scene data recovery using omnidirectional multi-baseline stereo. </title> <booktitle> In IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'96), </booktitle> <pages> pages 364-370, </pages> <address> San Francisco, California, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: Second, the intrinsic camera calibrations are recovered as part of the panorama construction [14]. Due to recent advances, it is now possible to construct panoramas even with hand-held cameras [15]. Unlike previous work on 3D reconstruction from multiple panoramas <ref> [16, 17] </ref>, our 3D modeling system exploits important regularities present in the environment, such as walls with known orientations. Fortunately, the man-made world is full of constraints such as parallel lines, lines with known directions, planes with lines and points on them.
Reference: 18. <author> S. M. Seitz and C. M. Dyer. </author> <title> Photorealistic scene reconstrcution by space coloring. </title> <booktitle> In IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'97), </booktitle> <pages> pages 1067-1073, </pages> <address> San Juan, Puerto Rico, </address> <month> June </month> <year> 1997. </year>
Reference-contexts: Unfortunately, stereo fails in regions without texture. A simple depth map also cannot capture the full complexity of a large-scale environment. Methods for overcoming this limitation include volumetric stereo techniques <ref> [18, 19] </ref> and model-based stereo [12]. In this paper, we propose a different approach|extending the concept of layered motion estimates [20, 21] and "shallow" objects [22] to true multi-image stereo matching.
Reference: 19. <author> R. Szeliski and P. Golland. </author> <title> Stereo matching with transparency and matting. </title> <booktitle> In Sixth International Conference on Computer Vision (ICCV'98), </booktitle> <pages> pages 517-524, </pages> <address> Bombay, </address> <month> January </month> <year> 1998. </year>
Reference-contexts: Unfortunately, stereo fails in regions without texture. A simple depth map also cannot capture the full complexity of a large-scale environment. Methods for overcoming this limitation include volumetric stereo techniques <ref> [18, 19] </ref> and model-based stereo [12]. In this paper, we propose a different approach|extending the concept of layered motion estimates [20, 21] and "shallow" objects [22] to true multi-image stereo matching. <p> However, EM models mixtures of probability distributions, rather than the kind of partial occlusion mixing that occurs at sprite boundaries [26]. Stereo techniques inspired by matte extraction [39] are needed to refine the color/opacity estimates for each layer <ref> [19] </ref>. Such an algorithm would work by re-synthesizing each input image from the current sprite estimates, and then adjusting pixel colors and opacities so at to minimize the difference between the original and re-synthesized images.
Reference: 20. <author> J. Y. A. Wang and E. H. Adelson. </author> <title> Layered representation for motion analysis. </title> <booktitle> In IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'93), </booktitle> <pages> pages 361-366, </pages> <address> New York, New York, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: A simple depth map also cannot capture the full complexity of a large-scale environment. Methods for overcoming this limitation include volumetric stereo techniques [18, 19] and model-based stereo [12]. In this paper, we propose a different approach|extending the concept of layered motion estimates <ref> [20, 21] </ref> and "shallow" objects [22] to true multi-image stereo matching. Our second interactive modeling system reconstructs the 3D scene as a collection of approximately planar layers, each of which has an explicit 3D plane equation, a color image with per-pixel opacity, and optionally a per-pixel out-of-plane displacement [23]. <p> A number of automatic techniques have been developed to initialize the layers, e.g, merging <ref> [20, 28, 29] </ref>, splitting [30, 28], color segmentation [31] and plane fitting to a recovered depth map. In our system, we interactively initialize the layers because we wish to focus initially on techniques for creating composite (mosaic) sprites from multiple images, estimating the sprite plane equations, and refining layer assignments.
Reference: 21. <author> Y. Weiss. </author> <title> Smoothness in layers: Motion segmentation using nonparametric mixture estimation. </title> <booktitle> In IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'97), </booktitle> <pages> pages 520-526, </pages> <address> San Juan, Puerto Rico, </address> <month> June </month> <year> 1997. </year>
Reference-contexts: A simple depth map also cannot capture the full complexity of a large-scale environment. Methods for overcoming this limitation include volumetric stereo techniques [18, 19] and model-based stereo [12]. In this paper, we propose a different approach|extending the concept of layered motion estimates <ref> [20, 21] </ref> and "shallow" objects [22] to true multi-image stereo matching. Our second interactive modeling system reconstructs the 3D scene as a collection of approximately planar layers, each of which has an explicit 3D plane equation, a color image with per-pixel opacity, and optionally a per-pixel out-of-plane displacement [23].
Reference: 22. <author> H. S. Sawhney and A. R. Hanson. </author> <title> Identification and 3D description of `shallow' environmental structure over a sequence of images. </title> <booktitle> In IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'91), </booktitle> <pages> pages 179-185, </pages> <address> Maui, Hawaii, June 1991. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: A simple depth map also cannot capture the full complexity of a large-scale environment. Methods for overcoming this limitation include volumetric stereo techniques [18, 19] and model-based stereo [12]. In this paper, we propose a different approach|extending the concept of layered motion estimates [20, 21] and "shallow" objects <ref> [22] </ref> to true multi-image stereo matching. Our second interactive modeling system reconstructs the 3D scene as a collection of approximately planar layers, each of which has an explicit 3D plane equation, a color image with per-pixel opacity, and optionally a per-pixel out-of-plane displacement [23].
Reference: 23. <author> S. Baker, R. Szeliski, and P. Anandan. </author> <title> A layered approach to stereo reconstruction. </title> <booktitle> In IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'98), </booktitle> <pages> pages 434-441, </pages> <address> Santa Barbara, </address> <month> June </month> <year> 1998. </year>
Reference-contexts: Our second interactive modeling system reconstructs the 3D scene as a collection of approximately planar layers, each of which has an explicit 3D plane equation, a color image with per-pixel opacity, and optionally a per-pixel out-of-plane displacement <ref> [23] </ref>. This representation allows us to account for inter-surface occlusions, which traditional stereo systems have trouble modeling correctly. 2 3D modeling from panoramas 2.1 Interactive modeling system Our modeling system uses one or more panoramas.
Reference: 24. <author> R. T. Collins and R. S. Weiss. </author> <title> Vanishing point calculation as a statistical inference on the unit sphere. </title> <booktitle> In Third International Conference on Computer Vision (ICCV'90), </booktitle> <pages> pages 400-403, </pages> <address> Osaka, Japan, December 1990. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: Because each "line projection plane" is perpendicular to the line (i.e., ~ n pi ~ m = 0), we want to minimize e = i X ~ n pi ~ n T This is equivalent to finding the vanishing point of the lines <ref> [24] </ref>. The advantage of the above formulation is that the sign ambiguity of ~ n pi can be ignored. When only two parallel lines are given, the solution is simply the cross product of two line projection plane normals.
Reference: 25. <author> G. Golub and C. F. Van Loan. </author> <title> Matrix Computation, third edition. </title> <publisher> The John Hopkins University Press, </publisher> <address> Baltimore and London, </address> <year> 1996. </year>
Reference-contexts: By separating hard constraints from soft ones, we obtain a least-squares system with equality constraints. Intuitively, the difference between soft and hard constraints is their weights in the least-squares formulation. Soft constraints have unit weights, while hard constraints have very large weights <ref> [25] </ref>. Some constraints (e.g., a point is known) are inherently hard, therefore equality constraints. Some constraints (e.g., a feature location on a 2D model or panorama) are most appropriate as soft constraints because they are based on noisy image measurements. Take a point on a plane for an example. <p> In other words, we would like to solve the linear system (soft constraints) Ax = b subject to (hard constraints) Cx = q where A is m fi n, C is p fi n. A solution to the above problem is to use the QR factorization <ref> [25] </ref>. Before we can apply the equality-constrained linear system solver, we must check whether the linear system formed by all constraints is solvable. In general, the system may consist of several subsystems (connected components) which can be solved independently.
Reference: 26. <author> J. F. Blinn. Jim Blinn's corner: Compositing, </author> <title> part 1: </title> <journal> Theory. IEEE Computer Graphics and Applications, </journal> <volume> 14(5) </volume> <pages> 83-87, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: In what follows, we will drop the image coordinates u k unless they are needed to explain a warping operation explicitly. Our hypothesis is that we can reconstruct the world as a collection of L approximately planar layers. Following <ref> [26] </ref>, we denote a layer "sprite" image by L l (u l ) = (ff l r l ; ff l g l ; ff l b l ; ff l ), where r l = r l (u l ) is the red band, g l = g l (u <p> If we wanted to, we could use an EM (expectation maximization) algorithm to obtain graded (continuous) assignments [37, 38]. However, EM models mixtures of probability distributions, rather than the kind of partial occlusion mixing that occurs at sprite boundaries <ref> [26] </ref>. Stereo techniques inspired by matte extraction [39] are needed to refine the color/opacity estimates for each layer [19].
Reference: 27. <author> J. Torborg and J. T. Kajiya. Talisman: </author> <title> Commodity realtime 3D graphics for the PC. </title> <booktitle> In Computer Graphics Proceedings, Annual Conference Series, </booktitle> <pages> pages 353-363, </pages> <booktitle> Proc. </booktitle> <address> SIGGRAPH'96 (New Orleans), </address> <month> August </month> <year> 1996. </year> <note> ACM SIGGRAPH. </note>
Reference-contexts: We use homogeneous coordinates in this section for both 3D world coordinates x = (x; y; z; 1) T and for 2D image coordinates u = (u; v; 1) T . 5 The terminology comes from computer graphics, where sprites are used to quickly (re-)render scenes composed of many objects <ref> [27] </ref>. Fig. 1. Suppose K images I k are captured by K cameras P k . We assume the scene can be represented by L sprite images L l on planes n T l x = 0 with depth offsets Z l .
Reference: 28. <author> H.S. Sawhney and S. Ayer. </author> <title> Compact representations fo videos through dominant and multiple motion estimation. </title> <journal> PAMI, </journal> <volume> 18(8) </volume> <pages> 814-830, </pages> <year> 1996. </year>
Reference-contexts: A number of automatic techniques have been developed to initialize the layers, e.g, merging <ref> [20, 28, 29] </ref>, splitting [30, 28], color segmentation [31] and plane fitting to a recovered depth map. In our system, we interactively initialize the layers because we wish to focus initially on techniques for creating composite (mosaic) sprites from multiple images, estimating the sprite plane equations, and refining layer assignments. <p> A number of automatic techniques have been developed to initialize the layers, e.g, merging [20, 28, 29], splitting <ref> [30, 28] </ref>, color segmentation [31] and plane fitting to a recovered depth map. In our system, we interactively initialize the layers because we wish to focus initially on techniques for creating composite (mosaic) sprites from multiple images, estimating the sprite plane equations, and refining layer assignments. <p> computed by choosing the best possible layer for each pixel: B lk (u k ) = 1 if ^ P kl (u k ) = min l 0 ^ P kl 0 (u k ) 0 otherwise : The simplest ways of defining P kl is the residual intensity difference <ref> [28] </ref>; another possibility is the residual normal flow magnitude [30].
Reference: 29. <author> M. J. Black and A. D. Jepson. </author> <title> Estimating optical flow in segmented images using variable-order parametric models with local deformations. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 18(10) </volume> <pages> 972-986, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: A number of automatic techniques have been developed to initialize the layers, e.g, merging <ref> [20, 28, 29] </ref>, splitting [30, 28], color segmentation [31] and plane fitting to a recovered depth map. In our system, we interactively initialize the layers because we wish to focus initially on techniques for creating composite (mosaic) sprites from multiple images, estimating the sprite plane equations, and refining layer assignments.
Reference: 30. <author> M. Irani, P. Anandan, and S. Hsu. </author> <title> Mosiac based representations of video sequences and their applications. </title> <booktitle> In 5th ICCV, </booktitle> <pages> pages 605-611, </pages> <year> 1995. </year>
Reference-contexts: A number of automatic techniques have been developed to initialize the layers, e.g, merging [20, 28, 29], splitting <ref> [30, 28] </ref>, color segmentation [31] and plane fitting to a recovered depth map. In our system, we interactively initialize the layers because we wish to focus initially on techniques for creating composite (mosaic) sprites from multiple images, estimating the sprite plane equations, and refining layer assignments. <p> pixel: B lk (u k ) = 1 if ^ P kl (u k ) = min l 0 ^ P kl 0 (u k ) 0 otherwise : The simplest ways of defining P kl is the residual intensity difference [28]; another possibility is the residual normal flow magnitude <ref> [30] </ref>.
Reference: 31. <author> S. Ayer, P. Schroeter, and J. Bigun. </author> <title> Segmentation of moving objects by robust parameter estimation over multiple frames. </title> <booktitle> In 3rd ECCV, </booktitle> <pages> pages 316-327, </pages> <year> 1994. </year>
Reference-contexts: A number of automatic techniques have been developed to initialize the layers, e.g, merging [20, 28, 29], splitting [30, 28], color segmentation <ref> [31] </ref> and plane fitting to a recovered depth map. In our system, we interactively initialize the layers because we wish to focus initially on techniques for creating composite (mosaic) sprites from multiple images, estimating the sprite plane equations, and refining layer assignments.
Reference: 32. <author> Q.-T. Luong and O. Faugeras. </author> <title> Determining the fundamental matrix with planes: Instability and new algorithms. </title> <booktitle> In IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'93), </booktitle> <pages> pages 489-494, </pages> <address> New York, New York, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: The camera matrices can be estimated when they are not known a priori, either using traditional structure from motion [3, 7, 8], or directly from the homographies relating sprites in different images <ref> [32, 33] </ref>. Our goal is to estimate the layer sprites L l , the plane vectors n l , and the residual depths Z l . <p> Alternatively, we can first compute a set of unconstrained homographies using a standard mosaic construction algorithm, and then invoke a structure from motion algorithm to recover the plane equation (and also the camera matrices, if desired) <ref> [32, 33] </ref>. 3.3 Estimation of Layer Sprites Before we can compute the layer sprite images L l , we need to choose 2D coordinate systems for the planes.
Reference: 33. <author> R. Szeliski and P. Torr. </author> <title> Geometrically constrained structure from motion: Points on planes. In European Workshop on 3D Structure from Multiple Images of Large-scale Environments (SMILE), </title> <address> Freiburg, Germany, </address> <month> June </month> <year> 1998. </year>
Reference-contexts: The camera matrices can be estimated when they are not known a priori, either using traditional structure from motion [3, 7, 8], or directly from the homographies relating sprites in different images <ref> [32, 33] </ref>. Our goal is to estimate the layer sprites L l , the plane vectors n l , and the residual depths Z l . <p> Alternatively, we can first compute a set of unconstrained homographies using a standard mosaic construction algorithm, and then invoke a structure from motion algorithm to recover the plane equation (and also the camera matrices, if desired) <ref> [32, 33] </ref>. 3.3 Estimation of Layer Sprites Before we can compute the layer sprite images L l , we need to choose 2D coordinate systems for the planes.
Reference: 34. <author> J.R. Bergen, P. Anandan, K.J. Hanna, and R. Hingorani. </author> <title> Hierarchical model-based motion estimation. </title> <booktitle> In 2nd ECCV, </booktitle> <pages> pages 237-252, </pages> <year> 1992. </year>
Reference-contexts: Typically, this value is found using some form of gradient decent, such as the Gauss-Newton method, and the optimization is performed in a hierarchical (i.e. pyramid based) fashion to avoid local extrema <ref> [34] </ref>. To apply this approach, we compute the Jacobian of the image warp H l 1k with respect to the parameters of n l . <p> A standard point tracking and structure from motion algorithm is used to recover a camera matrix for each image. To initialize our algorithm, we interactively specify how many layers and then perform a rough assignment of pixels to layers. Next, an automatic hierarchical parametric motion estimation algorithm similar to <ref> [34] </ref> is used to find the homographies between the layers, as defined in Equation (7). For the experiments presented in this paper, we set Q l = P 1 , i.e. we reconstruct the sprites in the coordinate system of the first camera.
Reference: 35. <author> H.-Y. Shum and R. Szeliski. </author> <title> Panoramic image mosaicing. </title> <type> Technical Report MSR-TR-97-23, </type> <institution> Microsoft Research, </institution> <month> September </month> <year> 1997. </year>
Reference-contexts: One simple method is to take the mean of the color or intensity values. A refinement is to use a "feathering" algorithm, where the averaging is weighted by the distance of each pixel from the nearest invisible pixel in M kl <ref> [35] </ref>. Alternatively, robust techniques can be used to estimate L l from the warped images. 3.4 Estimation of Residual Depth In general, the scene will not be exactly piecewise planar. <p> The coordinate system on the left corner (red) is the world coordinate, and the coordinate system in the middle (green) is the camera coordinate. The panorama is composed of 60 images using the method of creating full-view panoramas <ref> [35] </ref>. The extracted texture maps (without top and bottom faces) are shown in Figure 3. Notice how the texture maps in Figure 3 have different sampling rates from the original images.
Reference: 36. <author> R. Kumar, P. Anandan, and K. Hanna. </author> <title> Direct recovery of shape from multiple views: A parallax based approach. </title> <booktitle> In Twelfth International Conference on Pattern Recognition (ICPR'94), volume A, </booktitle> <pages> pages 685-688, </pages> <address> Jerusalem, Israel, October 1994. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: In this case, the homographic warps used in the previous section are not applicable. However, using a similar argument to that in Sections 3.2 and 3.3, it is easy to show (see also <ref> [36, 12] </ref>) that: u k = H l where H l k = P k (n T l Q fl l is the planar homography of Section 3.3, t kl = P k q l is the epipole, and it is assumed that the plane equation vector n l = (n
Reference: 37. <author> T. Darrell and A. Pentland. </author> <title> Robust estimation of a multi-layered motion representation. </title> <booktitle> In IEEE Workshop on Visual Motion, </booktitle> <pages> pages 173-178, </pages> <address> Princeton, New Jersey, October 1991. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: If we wanted to, we could use an EM (expectation maximization) algorithm to obtain graded (continuous) assignments <ref> [37, 38] </ref>. However, EM models mixtures of probability distributions, rather than the kind of partial occlusion mixing that occurs at sprite boundaries [26]. Stereo techniques inspired by matte extraction [39] are needed to refine the color/opacity estimates for each layer [19].
Reference: 38. <author> A. Jepson and M. J. Black. </author> <title> Mixture models for optical flow computation. </title> <booktitle> In IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'93), </booktitle> <pages> pages 760-761, </pages> <address> New York, New York, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: If we wanted to, we could use an EM (expectation maximization) algorithm to obtain graded (continuous) assignments <ref> [37, 38] </ref>. However, EM models mixtures of probability distributions, rather than the kind of partial occlusion mixing that occurs at sprite boundaries [26]. Stereo techniques inspired by matte extraction [39] are needed to refine the color/opacity estimates for each layer [19].
Reference: 39. <author> A. R. Smith and J. F. </author> <title> Blinn. Blue screen matting. </title> <booktitle> In Computer Graphics Proceedings, Annual Conference Series, </booktitle> <pages> pages 259-268, </pages> <booktitle> Proc. </booktitle> <address> SIGGRAPH'96 (New Orleans), </address> <month> August </month> <year> 1996. </year> <note> ACM SIGGRAPH. </note>
Reference-contexts: If we wanted to, we could use an EM (expectation maximization) algorithm to obtain graded (continuous) assignments [37, 38]. However, EM models mixtures of probability distributions, rather than the kind of partial occlusion mixing that occurs at sprite boundaries [26]. Stereo techniques inspired by matte extraction <ref> [39] </ref> are needed to refine the color/opacity estimates for each layer [19]. Such an algorithm would work by re-synthesizing each input image from the current sprite estimates, and then adjusting pixel colors and opacities so at to minimize the difference between the original and re-synthesized images.
Reference: 40. <author> T. Vieville, C. Zeller, and L. Robert. </author> <title> Using collineations to compute motion and structure in an uncalibrated image sequence. </title> <journal> International Journal of Computer Vision, </journal> <volume> 20(3) </volume> <pages> 213-242, </pages> <year> 1996. </year>
Reference-contexts: For the experiments presented in this paper, we set Q l = P 1 , i.e. we reconstruct the sprites in the coordinate system of the first camera. Using these homographies, we find the best plane estimate for each layer using a Euclidean structure from motion algorithm <ref> [40] </ref>. The results of applying these steps to the MPEG flower garden sequence are shown in Figure 8. Figures 8 (a) and (b) show the first and last image in the subsequence we used (the first seven even images). Figure 8 (c) shows the initial pixel labeling into seven layers.
Reference: 41. <author> E. L. Walker and M. Herman. </author> <title> Geometric reasoning for constructing 3D scene descriptions from images. </title> <journal> Artificial Intelligence, </journal> <volume> 37 </volume> <pages> 275-290, </pages> <year> 1988. </year>
Reference-contexts: Our results show that it is desirable and practical for the modeling systems to take advantage of as many regularities and priori knowledge about man-made environments (such as vertices, lines, and planes) as possible <ref> [41] </ref>. Our panorama 3D modeling system decomposes the modeling process into a zero baseline problem (panorama construction) and a wide baseline problem (stereo or structure from motion).
Reference: 42. <institution> Workshop on image-based modeling and rendering. //graphics.stanford.edu/im98/, </institution> <month> March </month> <year> 1998. </year>
Reference-contexts: The idea is to build a rough model using panorama modeling system and refine it using layered stereo wherever it is appropriate (similar in spirit to the model-based stereo of [12]). We are also investigating representations beyond texture-mapped 3D models, i.e, image-based rendering approaches <ref> [42] </ref> such as view-dependent texture maps [12] and layered depth images [43]. Integrating all of these into one interactive modeling system will enable users to easily construct complex photorealistic 3D models from images.
Reference: 43. <author> J. Shade, S. Gortler, L.-W. He, and R. Szeliski. </author> <title> Layered depth images. </title> <booktitle> In Computer Graphics (SIGGRAPH'98) Proceedings, </booktitle> <address> Orlando, </address> <month> July </month> <year> 1998. </year> <note> ACM SIGGRAPH. </note>
Reference-contexts: We are also investigating representations beyond texture-mapped 3D models, i.e, image-based rendering approaches [42] such as view-dependent texture maps [12] and layered depth images <ref> [43] </ref>. Integrating all of these into one interactive modeling system will enable users to easily construct complex photorealistic 3D models from images.
References-found: 43

