URL: http://www.cs.utah.edu/~ppsloan/si3d.ps.gz
Refering-URL: http://www.cs.utah.edu/~ppsloan/publications.html
Root-URL: 
Title: Time Critical Lumigraph Rendering  
Author: Peter-Pike Sloan Michael F. Cohen Steven J. Gortler 
Keyword: CR Descriptors: I.3.7 [Computer Graphics] Three-Dimensional Graphics and Realism; Additional Keywords: image-based rendering, critical time rendering  
Affiliation: University of Utah  Microsoft Research  Harvard University  
Note: To appear in the 1997 Symposium on Interactive 3D Graphics conference proceedings  
Abstract: It was illustrated in 1996 that the light leaving the convex hull of an object (or entering a convex region of empty space) can be fully characterized by a 4D function over the space of rays crossing a surface surrounding the object (or surrounding the empty space) [10, 8]. Methods to represent this function and quickly render individual images from this representation given an arbitrary cameras were also described. This paper extends the work outlined by Gortler et al [8] by demonstrating a taxonomy of methods to accelerate the rendering process by trading off quality for time. Given the specific limitation of a given hardware configuration, we discuss methods to tailor a critical time rendering strategy using these methods. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Beers, A. C., Agrawala, M., and Chaddha, N. </author> <title> Rendering from compressed textures. </title> <booktitle> In Computer Graphics Proceedings, Annual Conference Series, </booktitle> <year> 1996 (1996), </year> <pages> pp. 373-378. </pages>
Reference-contexts: Leveraging hardware texture mapping is sensitive to texture memory limitations (in our case 4Mb) and the bandwidth between the host and the accelerator. Architectures that support rendering from compressed textures <ref> [15, 1] </ref> and ones that read textures directly from host memory could overcome this bottleneck. This paper extends the work outlined by Gortler et al by demonstrating a taxonomy of methods to limit the amount of texture information required per frame. These methods make different trade-offs between quality and time.
Reference: [2] <author> Benton, S. A. </author> <title> Survey of holographic stereograms. </title> <booktitle> Proc. SPIE Int. </booktitle> <publisher> Soc. Opt. </publisher> <address> Eng. (USA) 367 (1983), </address> <pages> 15-19. </pages>
Reference-contexts: Moving back and forth along the line of motion provides parallax cues in the direction of motion. Moving perpendicular to the line appears to just rotate a 2D image. This same trick is used in the construction of horizontal parallax only holographic stereograms <ref> [2] </ref>. 3.2 Using Projective Textures A second type of method uses the current image as a newly defined texture map for subsequent nearby images.
Reference: [3] <author> Bryson, S., and Johan, S. </author> <title> Time management, simultaneity and time-critical computation in interactive unsteady visualization environments. </title> <booktitle> In Visualization '96 (1996), </booktitle> <pages> pp. 255-261. </pages>
Reference-contexts: As the texture memory is used up the lowest benefit node is deleted and replaced by the next node in line to be added. The time budget is dynamicaly rescaled as in <ref> [3] </ref> so that the desired frame rate is met.
Reference: [4] <author> Chen, S. E. </author> <title> Quicktime VR an image-based approach to virtual environment navigation. </title> <booktitle> In SIGGRAPH 95 Conference Proceedings (Aug. </booktitle> <year> 1995), </year> <editor> R. Cook, Ed., </editor> <booktitle> Annual Conference Series, ACM SIGGRAPH, </booktitle> <publisher> Addison Wesley, </publisher> <pages> pp. 29-38. </pages> <address> held in Los Angeles, California, </address> <month> 06-11 August </month> <year> 1995. </year>
Reference-contexts: The complexity at both the modeling and rendering stages has led to the development of methods to directly capture the appearance of real world objects and then use this information to render new images. These methods have been called image based rendering or view interpolation <ref> [5, 4, 11, 6, 13, 10, 8] </ref> since they typically start with images as input and then synthesize new views from the input images.
Reference: [5] <author> Chen, S. E., and Williams, L. </author> <title> View interpolation for image synthesis. </title> <booktitle> In Computer Graphics (SIGGRAPH '93 Proceedings) (Aug. </booktitle> <year> 1993), </year> <editor> J. T. Kajiya, Ed., </editor> <volume> vol. 27, </volume> <pages> pp. 279-288. </pages>
Reference-contexts: The complexity at both the modeling and rendering stages has led to the development of methods to directly capture the appearance of real world objects and then use this information to render new images. These methods have been called image based rendering or view interpolation <ref> [5, 4, 11, 6, 13, 10, 8] </ref> since they typically start with images as input and then synthesize new views from the input images. <p> The ap 3 To appear in the 1997 Symposium on Interactive 3D Graphics conference proceedings proximate geometry of the object is then used to warp the texture to the new viewpoint. This is similar in flavor to the original view interpolation work by Chen and Williams <ref> [5] </ref> and the more recent work by Seitz and Dyer [13] and by Debevec et al [6]. After an initial image is created, a matrix M is constructed and used for each subsequent frame.
Reference: [6] <author> Debevec, P. E., Taylor, C. J., and Malik, J. </author> <title> Modeling and rendering architecture from photographs: A hybrid geometry-and-image-based approach. </title> <booktitle> In Computer Graphics Proceedings, Annual Conference Series, </booktitle> <year> 1996 (1996), </year> <pages> pp. 11-20. </pages>
Reference-contexts: The complexity at both the modeling and rendering stages has led to the development of methods to directly capture the appearance of real world objects and then use this information to render new images. These methods have been called image based rendering or view interpolation <ref> [5, 4, 11, 6, 13, 10, 8] </ref> since they typically start with images as input and then synthesize new views from the input images. <p> This is similar in flavor to the original view interpolation work by Chen and Williams [5] and the more recent work by Seitz and Dyer [13] and by Debevec et al <ref> [6] </ref>. After an initial image is created, a matrix M is constructed and used for each subsequent frame. The composite modeling and viewing matrix used for the initial frame transforms points (x,y,z) in model space to screen coordinates between -1 and 1 in X and Y.
Reference: [7] <author> Funkhouser, T. A., and S equin, C. H. </author> <title> Adaptive display algorithm for interactive frame rates during visualization of complex virtual environments. </title> <booktitle> In Computer Graphics (SIGGRAPH '93 Proceedings) (Aug. </booktitle> <year> 1993), </year> <editor> J. T. Kajiya, Ed., </editor> <volume> vol. 27, </volume> <pages> pp. 247-254. </pages>
Reference-contexts: To do this, like in <ref> [7] </ref> we define two concepts for each node, the benefit that will be derived from adding that node, and the cost of using it. The nodes are sorted in order of the ratio of their benefit:cost.
Reference: [8] <author> Gortler, S. J., Grzeszczuk, R., Szeliski, R., and Cohen, M. F. </author> <booktitle> The lumigraph. In Computer Graphics Proceedings, Annual Conference Series, </booktitle> <year> 1996 (1996), </year> <pages> pp. 43-54. </pages>
Reference-contexts: The complexity at both the modeling and rendering stages has led to the development of methods to directly capture the appearance of real world objects and then use this information to render new images. These methods have been called image based rendering or view interpolation <ref> [5, 4, 11, 6, 13, 10, 8] </ref> since they typically start with images as input and then synthesize new views from the input images. <p> These methods have been called image based rendering or view interpolation [5, 4, 11, 6, 13, 10, 8] since they typically start with images as input and then synthesize new views from the input images. In two papers, Lightfield Rendering [10] and The Lumi-graph <ref> [8] </ref> it was shown that the light leaving the convex hull of an object (or entering a convex region of empty space) can be characterized by a 4D function over the space of rays crossing a surface surrounding the object (or surrounding the empty space). <p> The third step in the process should be fast to allow interactive exploration of the Lumigraph by real-time manipulation of a virtual camera. Both papers <ref> [10, 8] </ref> discuss different methods to make this possible, however, both are limited in different ways. <p> These methods make different trade-offs between quality and time. Given the specific limitation of a given hardware configuration, one can use these methods to tailor a critical time rendering method. In addition, the methods lead naturally to progressive refinement rendering strategies. 2 The Lumigraph In both [10] and <ref> [8] </ref>, the 4D Lumigraph function was parameterized by the intersection of a ray with a pair of planes (Figure 2). Fully enclosing a region of space involves using a series of such pairs of planes, however, we will restrict ourselves in this paper to discussing a single pair. <p> Given a discretization, one also needs to select a blending function between nodal values both for projection of the continuous function into the discrete one and for reconstruction. We will use a quadralinear basis for these purposes as was done in <ref> [8] </ref>. 2.2 Use of Geometry So far there has been no notion of the geometry of the object since one of the major features of pure image based rendering is that no geometry is needed. <p> On the other hand, given some approximate geometry as can be constructed from a series of silhouettes [14] one can improve the reconstruction of images as discussed in <ref> [8] </ref>. We will also use the approximate geometry in the reconstruction methods discussed below. 2.3 Reconstruction as Texture Mapping Gortler et al [8] describe a texture mapping process to perform the reconstruction of images with hardware acceleration. <p> On the other hand, given some approximate geometry as can be constructed from a series of silhouettes [14] one can improve the reconstruction of images as discussed in <ref> [8] </ref>. We will also use the approximate geometry in the reconstruction methods discussed below. 2.3 Reconstruction as Texture Mapping Gortler et al [8] describe a texture mapping process to perform the reconstruction of images with hardware acceleration. <p> The triangles in Figure 2 would simply be twice as large in each direction. This results in more depth of field blurring which is ameliorated somewhat by the depth correction afforded by the use of the approximate geometry <ref> [8] </ref>. Cutting the resolution in uv also results in the need of less texture memory at a cost of overall blurring. <p> Figure 6 shows the results of the same sequence shown earlier for the RE2, when run on the O2 machine. 6 Conclusion At SIGGRAPH 1996, Levoy and Hanrahan [10] and Gortler at al <ref> [8] </ref> showed how a four dimensional data structure, called a Lumigraph in the latter work, could be constructed to capture the full appearance of a bounded object or the light entering a bounded region of empty space. Methods were presented to quickly reconstruct images from the Lumigraph from arbitrary objects.
Reference: [9] <author> Hoppe, H. </author> <title> Progressive meshes. </title> <booktitle> In SIGGRAPH 96 Conference Proceedings (Aug. </booktitle> <year> 1996), </year> <editor> H. Rushmeier, Ed., </editor> <booktitle> Annual Conference Series, ACM SIGGRAPH, </booktitle> <publisher> Addison Wesley, </publisher> <pages> pp. 99-108. </pages> <address> held in New Orleans, Louisiana, </address> <month> 4-9 August </month> <year> 1996. </year>
Reference-contexts: As more nodes are transferred they are inserted into the triangulation (initially without data) and schedule the texture to be decompressed and added to the new node. The geometry also could, of course, also be represented in a progressive format <ref> [9] </ref>. With a simple modification of the arbitrary triangulation algorithm the data for vertices can be smoothly blended in over time, or blended out as the vertex is removed.
Reference: [10] <author> Levoy, M., and Hanrahan, P. </author> <title> Light field rendering. </title> <booktitle> In Computer Graphics Proceedings, Annual Conference Series, </booktitle> <year> 1996 (1996), </year> <pages> pp. 31-42. </pages>
Reference-contexts: The complexity at both the modeling and rendering stages has led to the development of methods to directly capture the appearance of real world objects and then use this information to render new images. These methods have been called image based rendering or view interpolation <ref> [5, 4, 11, 6, 13, 10, 8] </ref> since they typically start with images as input and then synthesize new views from the input images. <p> These methods have been called image based rendering or view interpolation [5, 4, 11, 6, 13, 10, 8] since they typically start with images as input and then synthesize new views from the input images. In two papers, Lightfield Rendering <ref> [10] </ref> and The Lumi-graph [8] it was shown that the light leaving the convex hull of an object (or entering a convex region of empty space) can be characterized by a 4D function over the space of rays crossing a surface surrounding the object (or surrounding the empty space). <p> The third step in the process should be fast to allow interactive exploration of the Lumigraph by real-time manipulation of a virtual camera. Both papers <ref> [10, 8] </ref> discuss different methods to make this possible, however, both are limited in different ways. <p> These methods make different trade-offs between quality and time. Given the specific limitation of a given hardware configuration, one can use these methods to tailor a critical time rendering method. In addition, the methods lead naturally to progressive refinement rendering strategies. 2 The Lumigraph In both <ref> [10] </ref> and [8], the 4D Lumigraph function was parameterized by the intersection of a ray with a pair of planes (Figure 2). Fully enclosing a region of space involves using a series of such pairs of planes, however, we will restrict ourselves in this paper to discussing a single pair. <p> We have not yet taken advantage of the JPEG decompression hardware, but the initial results are promising. Figure 6 shows the results of the same sequence shown earlier for the RE2, when run on the O2 machine. 6 Conclusion At SIGGRAPH 1996, Levoy and Hanrahan <ref> [10] </ref> and Gortler at al [8] showed how a four dimensional data structure, called a Lumigraph in the latter work, could be constructed to capture the full appearance of a bounded object or the light entering a bounded region of empty space.
Reference: [11] <author> McMillan, L., and Bishop, G. </author> <title> Plenoptic modeling: An image-based rendering system. </title> <booktitle> In SIGGRAPH 95 Conference Proceedings (Aug. </booktitle> <year> 1995), </year> <editor> R. Cook, Ed., </editor> <booktitle> Annual Conference Series, ACM SIGGRAPH, </booktitle> <publisher> Addison Wesley, </publisher> <pages> pp. 39-46. </pages> <address> held in Los Angeles, California, </address> <month> 06-11 August </month> <year> 1995. </year>
Reference-contexts: The complexity at both the modeling and rendering stages has led to the development of methods to directly capture the appearance of real world objects and then use this information to render new images. These methods have been called image based rendering or view interpolation <ref> [5, 4, 11, 6, 13, 10, 8] </ref> since they typically start with images as input and then synthesize new views from the input images.
Reference: [12] <author> Segal, M., Korobkin, C., van Widenfelt, R., Foran, J., and Haeberli, P. E. </author> <title> Fast shadows and lighting effects using texture mapping. </title> <booktitle> In Computer Graphics (SIGGRAPH '92 Proceedings) (July 1992), </booktitle> <editor> E. E. Catmull, Ed., </editor> <volume> vol. 26, </volume> <pages> pp. 249-252. </pages>
Reference-contexts: This texture matrix, M, is the used to find texture coordinates for vertices of the approximate model for each subsequent frame <ref> [12] </ref>. The texture coordinates for each vertex are set to their (x,y,z) location. These are transformed by M into the 2D UV texture coordinates.
Reference: [13] <author> Seitz, S. M., and Dyer, C. R. </author> <title> View morphing. </title> <booktitle> In Computer Graphics Proceedings, Annual Conference Series, </booktitle> <year> 1996 (1996), </year> <pages> pp. 21-30. </pages>
Reference-contexts: The complexity at both the modeling and rendering stages has led to the development of methods to directly capture the appearance of real world objects and then use this information to render new images. These methods have been called image based rendering or view interpolation <ref> [5, 4, 11, 6, 13, 10, 8] </ref> since they typically start with images as input and then synthesize new views from the input images. <p> This is similar in flavor to the original view interpolation work by Chen and Williams [5] and the more recent work by Seitz and Dyer <ref> [13] </ref> and by Debevec et al [6]. After an initial image is created, a matrix M is constructed and used for each subsequent frame.
Reference: [14] <author> Szeliski, R. </author> <title> Rapid octree construction from image sequences. CVGIP: </title> <booktitle> Image Understanding 58, </booktitle> <month> 1 (July </month> <year> 1993), </year> <pages> 23-32. </pages>
Reference-contexts: On the other hand, given some approximate geometry as can be constructed from a series of silhouettes <ref> [14] </ref> one can improve the reconstruction of images as discussed in [8]. We will also use the approximate geometry in the reconstruction methods discussed below. 2.3 Reconstruction as Texture Mapping Gortler et al [8] describe a texture mapping process to perform the reconstruction of images with hardware acceleration.
Reference: [15] <author> Torborg, J., and Kajiya, J. T. Talisman: </author> <title> Commodity realtime 3d graphics for the pc. </title> <booktitle> In Computer Graphics Proceedings, Annual Conference Series, </booktitle> <year> 1996 (1996), </year> <pages> pp. 353-363. </pages>
Reference-contexts: Leveraging hardware texture mapping is sensitive to texture memory limitations (in our case 4Mb) and the bandwidth between the host and the accelerator. Architectures that support rendering from compressed textures <ref> [15, 1] </ref> and ones that read textures directly from host memory could overcome this bottleneck. This paper extends the work outlined by Gortler et al by demonstrating a taxonomy of methods to limit the amount of texture information required per frame. These methods make different trade-offs between quality and time.
Reference: [16] <editor> Williams, L. Pyramidal parametrics. </editor> <booktitle> In Computer Graphics (SIGGRAPH '83 Proceedings) (July 1983), </booktitle> <volume> vol. 17, </volume> <pages> pp. 1-11. 7 </pages>
Reference-contexts: Cutting the resolution in uv also results in the need of less texture memory at a cost of overall blurring. A 2D or 4D Mipmap <ref> [16] </ref> of multiple levels can be constructed, however, just two levels of a 4D Mipmap already results in a size 1/16th that of the original Lumigraph [ see Color Plate 1] . * Fixed pattern of st nodes: if our budget allows a fixed number of textures, for example 9, we
References-found: 16

