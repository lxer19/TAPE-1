URL: ftp://coast.cs.purdue.edu/pub/COAST/papers/ivan-krsul/krsul-spaf-authorship-analysis.ps
Refering-URL: http://www.cs.purdue.edu/coast/coast-library.html
Root-URL: http://www.cs.purdue.edu
Email: fkrsul,spafg@cs.purdue.edu  
Title: Authorship Analysis: Identifying The Author of a Program  
Author: Ivan Krsul Eugene H. Spafford 
Date: September 4, 1996  
Address: West Lafayette, IN 47907-1398  
Affiliation: The COAST Project Department of Computer Sciences Purdue University  
Pubnum: Technical Report TR-96-052  
Abstract: Authorship analysis on computer software is a difficult problem. In this paper we explore the classification of programmers' style, and try to find a set of characteristics that remain constant for a significant portion of the programs that this programmer might produce. Our goal is to show that it is possible to identify the author of a program by examining programming style characteristics. Within a closed environment, the results of this paper support the conclusion that, for a specific set of programmers, it is possible to identify the author of any individual program. Also, based on previous work and our observations during the experiments described herein we believe that the probability of finding two programmers who share exactly those same characteristics should be very small.
Abstract-found: 1
Intro-found: 1
Reference: [BB89] <author> A. Benander and B. Benander. </author> <title> An empirical study of COBOL programs via a style analyzer: The benefits of good programming style. </title> <journal> The Journal of Systems and Software, </journal> <volume> 10(2) </volume> <pages> 271-279, </pages> <year> 1989. </year>
Reference-contexts: However, they have distinctive styles. Authorship analysis should not consider their authors to be the same. Many other sources have influenced our choice of metrics <ref> [LC90, BB89, OC90, Coo87] </ref> but do not contain a specific set of rules, metrics or proverbs. All these sources provide material from which we may derive useful metrics.
Reference: [BM85] <author> R. Berry and B. Meekings. </author> <title> A style analysis of C programs. </title> <journal> Communications of the ACM, </journal> <volume> 28(1) </volume> <pages> 80-88, </pages> <year> 1985. </year>
Reference-contexts: Percentage of lines of code with inline comments. * Metric STY6c: Ratio of lines of block style comments to lines of code. * Metric STY7: Ratio of white lines to lines of code [RN93, pages 70-71]. 4.2.2 Programming Style Metrics * Metric PRO1: Mean program line length (characters per line) <ref> [BM85] </ref>. * Metric PRO2: A vector of metrics that will consider name lengths. * Metric PRO2a: Mean local variable name length. * Metric PRO2b: Mean global variable name length. * Metric PRO2c: Mean function name length. * Metric PRO2d: Mean function parameter length. * Metric PRO3: A vector of metrics that <p> Figure 7 shows three common debugging styles in C. frequently used debugging techniques, and programmers frequently prefer one over the other. The choice of debugging technique can be a useful metric. * Metric PSM4: The assert macro is used. * Metric PSM5: Lines of code per function <ref> [KP78, BM85] </ref>. * Metric PSM6: Variable count to lines of code ratio. <p> Rather, each instance of the if, for, while, do, case statements and the ? operator increases our decision count by one. * Metric PSM9: Is the goto keyword used? Software designers and programmers still rely on these <ref> [BM85] </ref>. * Metric PSM10: Simple software complexity metrics offer little information that might be application independent [OC89].
Reference: [BS84] <author> H. Berghel and D. Sallach. </author> <title> Measurements of program similarity in identical task environments. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 19(8) </volume> <pages> 65-76, </pages> <year> 1984. </year>
Reference-contexts: A better indentation measurement must include a consistency value. Cook and Oman briefly explored the use of software complexity metrics to define a relationship between programs and programmers, concluding that these are inadequate measures of stylistic factors and domain attributes [OC89]. Two other studies by Berghel and Sallach <ref> [BS84] </ref> and Evangelist [Eva84] support these findings. 2.4 Software Forensics Spafford and Weeber suggested that a technique they called software forensics could be used to examine and analyze software in any form, be it source code for any language or executable images, to identify the author [WS93].
Reference: [Coo87] <author> Doug Cooper. Condensed Pascal. W. W. Norton and Company, </author> <year> 1987. </year>
Reference-contexts: However, they have distinctive styles. Authorship analysis should not consider their authors to be the same. Many other sources have influenced our choice of metrics <ref> [LC90, BB89, OC90, Coo87] </ref> but do not contain a specific set of rules, metrics or proverbs. All these sources provide material from which we may derive useful metrics.
Reference: [Dau90] <author> K. Dauber. </author> <title> The Idea of Authorship in America. </title> <institution> The University of Wisconsin Press, </institution> <year> 1990. </year>
Reference-contexts: However, it is in literature where we find the closest parallel. Authorship analysis in literature has been widely debated for hundreds of years, and a large body of knowledge has been developed <ref> [Dau90] </ref>. Authorship analysis on computer software, however, is different and more difficult than in literature. Several reasons make this problem difficult. Authorship analysis in computer software does not use the same stylistic characteristics as authorship analysis in literature.
Reference: [Dis37] <editor> B. Disraeli. Venetia. </editor> <address> New York and London, </address> <month> 1837. </month>
Reference-contexts: In actual practice, there are documented cases where people have used ad-hoc techniques, based on programming style, to make conclusions about authorship of programs [Spa89, Spa88, LS93]. 2.2 Authorship Analysis in Literature Hundreds of books and essays have been written on this topic, some as early as 1837 <ref> [Dis37] </ref>. Specially interesting is W. Elliott's attempt to resolve the authorship of Shakespeare's work with a computer by examining literary minutiae, from word frequency to punctuation and proclivity to use clauses and compounds [EV91].
Reference: [EV91] <author> W. Elliot and R. Valenza. </author> <title> Was the Earl of Oxford the true Shakespeare? Notes and Queries, </title> <booktitle> 38 </booktitle> <pages> 501-506, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: Specially interesting is W. Elliott's attempt to resolve the authorship of Shakespeare's work with a computer by examining literary minutiae, from word frequency to punctuation and proclivity to use clauses and compounds <ref> [EV91] </ref>. For three years, the Shakespeare Clinic of Claremont Colleges used computers to see which of fifty-eight claimed authors of Shakespeare's works matched Shakespeare's writing style. <p> Other more conventional tests examined were hyphenated compound words, relative clauses per thousand, grade-level of writing, and percentage of open and feminine ended lines <ref> [EV91] </ref>. Although much controversy surrounds the specific results obtained by Elliott's computer analysis, it is clear from the results that works attributed to Shakespeare fit a narrow and distinctive profile. W. Elliot and R. Valenza write in [EV91] that our conclusion from the clinic was that Shakespeare fit within a fairly <p> per thousand, grade-level of writing, and percentage of open and feminine ended lines <ref> [EV91] </ref>. Although much controversy surrounds the specific results obtained by Elliott's computer analysis, it is clear from the results that works attributed to Shakespeare fit a narrow and distinctive profile. W. Elliot and R. Valenza write in [EV91] that our conclusion from the clinic was that Shakespeare fit within a fairly narrow, distinctive profile under our best tests. If his poems were written by a committee, it was a remarkably consistent committee.
Reference: [Eva84] <author> M. </author> <title> Evangelist. Program complexity and programming style. </title> <booktitle> In Proceedings of the International Conference of Data Engineering, </booktitle> <pages> pages 534-541. </pages> <publisher> IEEE, </publisher> <year> 1984. </year>
Reference-contexts: Cook and Oman briefly explored the use of software complexity metrics to define a relationship between programs and programmers, concluding that these are inadequate measures of stylistic factors and domain attributes [OC89]. Two other studies by Berghel and Sallach [BS84] and Evangelist <ref> [Eva84] </ref> support these findings. 2.4 Software Forensics Spafford and Weeber suggested that a technique they called software forensics could be used to examine and analyze software in any form, be it source code for any language or executable images, to identify the author [WS93].
Reference: [GJM91] <author> C. Ghezzi, M. Jazayeri, and D. Mandrioli. </author> <title> Fundamentals of Software Engineering. </title> <publisher> Prentice Hall, </publisher> <address> first edition, </address> <year> 1991. </year>
Reference-contexts: Education is only one of many factors that have an effect on the evolution of programming styles. Not only do software engineering models impose particular naming conventions, parameter passing methods and commenting styles; they also impose a planning and development strategy. The waterfall model <ref> [GJM91] </ref>, for example, encourages the design of precise specifications, utilization of program modules and extensive module testing. These have a marked impact on programming style. The programming style of any given programmer varies also from language to language, or because of external constraints placed by managers or tools 3 .
Reference: [Gri81] <author> S. Grier. </author> <title> A tool that detects plagiarism in Pascal programs. </title> <journal> ACM SIGCSE Bulletin, </journal> <volume> 13(1) </volume> <pages> 15-20, </pages> <year> 1981. </year>
Reference-contexts: While plagiarism detection needs to detect the similarity between these two programs, authorship analysis does not. For the purpose of authorship identification, these two programs have distinct authors. Many people have devoted time and resources to the development of plagiarism detection <ref> [Ott77, Gri81, Jan88, Wha86] </ref>, and a comprehensive analysis of their work is beyond the scope of this paper. We can, however give a simple example that will illustrate how measurements traditionally used for plagiarism detection are ill suited to authorship analysis. Consider the two functions shown in Figure 4.
Reference: [GS92] <author> S. Garfinkel and E. Spafford. </author> <title> Practical Unix Security. </title> <publisher> O'Reilly & Associates, Inc., </publisher> <year> 1992. </year>
Reference-contexts: However, in this paper we show that the identification process in computer software can be made reliable for a subset of the programmers and programs. fl This paper has been submitted to Computers and Security and is currently under consideration. y Contact person for questions concerning the paper. 1 <ref> [GS92] </ref> defines Trojan horses as programs that appear to have one function but actually perform another function; viruses as programs that modify other programs in a computer, inserting copies of themselves; and logic bombs as hidden features in programs that go off after certain conditions are met. <p> Some programmers tend to ignore the error return values of system calls that are considered reliable <ref> [GS92, page 164] </ref>. Thus, a metric can be obtained out of the percentage of reliable system calls whose error codes are ignored by the programmer. Also, some programmers tend to overlook the error codes returned by system calls that should never have them ignored (e.g. malloc).
Reference: [Han91] <author> D. Hanson. </author> <title> Code generation interface for ANSI C. </title> <journal> Software Practice and Experience, </journal> <volume> 38 </volume> <pages> 963-988, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: Furthermore, it is possible for an experienced programmer to get low software quality scores and for a beginner to get high scores (if he followed a textbook algorithm for his program). A software analyzer built for the lcc C compiler front-end developed at Princeton <ref> [Han91] </ref> was used to generate most of the raw data, including all the programming structure metrics. Once the calculation of these metrics had been performed, a series of Perl programs were used to collect the metrics that depended on the information that was discarded by the C preprocessor.
Reference: [HH92] <author> W. Hope and K. Holston. </author> <title> The Shakespeare Controversy. </title> <publisher> McFarland & Company, </publisher> <year> 1992. </year>
Reference-contexts: Such great figures as Mark Twain, Irving Wall and Sigmund Freud have debated this particular issue at length <ref> [HH92] </ref>. The issue of identifying program authorship was explored by Cook and Oman [OC89] as a means for determining instances of software theft and plagiarism.
Reference: [HU79] <author> John Hopcroft and Jeffrey Ullman. </author> <title> Introduction to Automata Theory, Languages, and Computation. </title> <publisher> Addison-Wesley, </publisher> <address> first edition, </address> <year> 1979. </year>
Reference-contexts: In this context, functions that do nothing successfully are functions that correctly test for boundary conditions on their input parameters. We must note that it is an undecidable problem to determine the correctness of an arbitrary function <ref> [HU79] </ref>. * Metric PSM14: Do comments and code agree? Kernighan and Plauger write in [KP78] that a comment is of zero (or negative) value if it is wrong. Ranade and Nash [RN93, page 89] devote a rule to the truth of every comment.
Reference: [Jan88] <author> H. Jankowitz. </author> <title> Detecting plagiarism in student Pascal programs. </title> <journal> Computer Journal, </journal> <volume> 31(1) </volume> <pages> 1-8, </pages> <year> 1988. </year>
Reference-contexts: While plagiarism detection needs to detect the similarity between these two programs, authorship analysis does not. For the purpose of authorship identification, these two programs have distinct authors. Many people have devoted time and resources to the development of plagiarism detection <ref> [Ott77, Gri81, Jan88, Wha86] </ref>, and a comprehensive analysis of their work is beyond the scope of this paper. We can, however give a simple example that will illustrate how measurements traditionally used for plagiarism detection are ill suited to authorship analysis. Consider the two functions shown in Figure 4.
Reference: [JW88] <author> R. Johnson and D. Wichern. </author> <title> Applied Multivariate Statistical Analysis. </title> <publisher> Prentice Hall, </publisher> <address> second edition, </address> <year> 1988. </year>
Reference-contexts: The method used during the early phases of the experiments is a statistical analysis method called discriminant analysis. This method, described in <ref> [SAS, JW88] </ref> is a multivariate technique concerned with separating observations and with allocating new observations into previously defined groups. On subsequent phases of the experiments we used the neural networks and likelihood classifiers that the LNKnet software, developed at the MIT Lincoln Laboratory, provides.
Reference: [KL95] <author> Linda Kukolich and Richard Lippmann. </author> <title> LNKnet User's Guide. </title> <institution> MIT Lincoln Laboratory, MIT Technology Licensing Office, RM E32-300, 200 Carleton Street, </institution> <address> Cambridge, MA 02142-1324, </address> <month> April </month> <year> 1995. </year>
Reference-contexts: On subsequent phases of the experiments we used the neural networks and likelihood classifiers that the LNKnet software, developed at the MIT Lincoln Laboratory, provides. The software is described in <ref> [KL95] </ref>. 5.1 Discriminant Analysis with SAS This technique works best with those metrics that show little variation between programs (for a specific programmer) and large variations among programmers. Unfortunately, analysis of the metrics collected shows that these two criteria are not necessarily correlated. <p> Table 3, extracted from the LNKnet User's Guide <ref> [KL95] </ref>, shows the classification mechanisms available. The Neural Networks and Classification Algorithms in this software are sophisticated enough to eliminate from consideration those metrics that contribute little to the classification. <p> The cross-validation error rate is obtained from summing the errors from the tests. LNKnet provides three data normalization methods that either scale or rotate the input space <ref> [KL95] </ref>. Simple normalization rescales each input feature independently to have a mean of 0 and a variance of 1 10 . Principal Components Analysis (PCA) rotates the input space to make the direction of greatest variance the first dimension 11 . <p> For a given test pattern, the class which has the highest likelihood times the class prior probability is selected as the class of a pattern. <ref> [KL95] </ref>. Gaussian classifiers are the most common and simple classifiers. A Gaussian classifier models each class with a Gaussian distribution centered around on the mean of that class. This classifier also performs exceedingly well with our test data, with an overall accuracy of 100%. <p> Figure 11 shows how the classification errors drop sharply with the addition of few metrics. 13 A description of these parameters can be found in the LNKnet User's Guide <ref> [KL95] </ref>. sharply with the addition of very few metrics. does not yield the sharp drop that LDA normalization does. normalization. This graph shows how the classification error drops sharply with the addition of very few metrics. <p> The simplest algorithms is to store all the training patterns and to find distances to them all for each testing pattern. The LVQ training algorithm can improve the performance of a nearest cluster classifier by moving cluster centers <ref> [KL95] </ref>. The best error rates were obtained by using an LDA normalization with 4-fold cross-validation. The error rates are around the 22% mark. 5.2.4 Histogram A Histogram classifier is a likelihood classifier. It estimates the likelihood of each class by creating a set of histograms for each input feature. <p> In testing, the likelihoods for each input dimension are multiplied to give an overall likelihood for each class <ref> [KL95] </ref>. The LNKnet Histogram classifier provides several options for dividing the input space into bins. The one that seems to work best for our particular application is the Uniformly Segmented Hypercubes option in which a fixed set of bins evenly divides the space into smaller hypercubes. <p> These partition the input space into decision regions using threshold logic nodes or rules. The BINTREE classifier works well with problems with a small number of uncorrelated input features, but may have difficulty on databases with high dimensionality <ref> [KL95] </ref>. The performance of the BINTREE classifier is less than optimal, with an error rate of 30%.
Reference: [KP78] <author> B. Kernighan and P. Plauger. </author> <title> The Elements of Programming Style. </title> <publisher> McGraw-Hill Book Company, </publisher> <address> second edition, </address> <year> 1978. </year>
Reference-contexts: Only programmer two has chosen temporary variable names that reflect the use that will be given to the variable. 4. One of the programs has a significant bug: The return of the function is undefined when the two input strings are empty (i.e. it fails to do nothing gracefully <ref> [KP78] </ref>). 5. The program for programmer one has an incorrect comment. 6. Only one of the programmers has consistently indented his programs using three spaces. The rest used only two spaces. 7. The placement of curly braces (f) is distinct for all programmers. <p> from a wide variety of likely sources: * Oman and Cook [OC91] collected a list of 236 style rules that can be used as a base for deriving metrics of programming style. * Conte, Dunsmore and Shen [SCS86] give a comprehensive list of software complexity metrics. * Kernighan and Plauger <ref> [KP78] </ref> give over seventy programming rules that should be part of good programming practice. * Van Tassel [Tas78] devotes a chapter to programming style for improving the readability of programs. * Jay Ranade and Alan Nash [RN93] give more than three hundred pages of style rules specifically for the C programming <p> This vector will consist of: * Metric PRO3a: Some names use the underscore character. * Metric PRO3b: Use of temporary variables 4 that are named XXX1, XXX2, etc. <ref> [KP78] </ref>, or tmp, temp, tmpXXX or tempXXX [RN93]. * Metric PRO3c: Percentage of variable names that start with an uppercase letter. * Metric PRO3d: Percentage of function names that start with an uppercase letter. * Metric PRO4: Global variable count to mean local variable count ratio. <p> metric might give us a better metric for measuring the frequency of usage of global variables. * Metric PRO6: Use of conditional compilation. * Metric PRO7: Preference of either while, for or do loops. * Metric PRO8: Does the programmer use comments that are nearly an echo of the code <ref> [KP78, page 143] </ref> [RN93, page 82]? * Metric PRO9: Type of function parameter declaration. <p> Figure 7 shows three common debugging styles in C. frequently used debugging techniques, and programmers frequently prefer one over the other. The choice of debugging technique can be a useful metric. * Metric PSM4: The assert macro is used. * Metric PSM5: Lines of code per function <ref> [KP78, BM85] </ref>. * Metric PSM6: Variable count to lines of code ratio. <p> (), socket (), etc. * Metric PSM12: Does the programmer rely on the internal representation of data objects? This metric would check for programmers relying on the size and byte order of integers, the size of floats, etc. * Metric PSM13: Do functions do nothing successfully? Kernighan and Plauger in <ref> [KP78, pages 111-114] </ref> and Jay Ranade and Alan Nash in [RN93, page 32] emphasize the need to make sure that there are no unexpected side effects in functions when these must do nothing. <p> We must note that it is an undecidable problem to determine the correctness of an arbitrary function [HU79]. * Metric PSM14: Do comments and code agree? Kernighan and Plauger write in <ref> [KP78] </ref> that a comment is of zero (or negative) value if it is wrong. Ranade and Nash [RN93, page 89] devote a rule to the truth of every comment. Even if the comments were initially accurate, it is possible that during the maintenance cycle of a program they became inaccurate.
Reference: [Krs94] <author> Ivan Krsul. </author> <title> Authorhip analysis: Identifying the author of a program. </title> <type> Master's thesis, </type> <institution> Department of Computer Science, Purdue University, </institution> <year> 1994. </year>
Reference-contexts: Real-time misuse detection systems could be enhanced by inclusion of authorship information. A programmer signature constructed from the identifying characteristics of programs constitutes a pattern that can be used in the monitoring of abnormal system usage <ref> [Krs94] </ref>. 2.1 Related Work In literature, the question of Shakespeare's identity has engaged the wits and energy of a wide range of people for more than two hundred years. Such great figures as Mark Twain, Irving Wall and Sigmund Freud have debated this particular issue at length [HH92]. <p> collected by these scripts. 4.3 Experiment Structure The experimental data for this paper was gathered in three distinct stages: * A preliminary stage helped us determine the proper methods for calculating the metrics, eliminated those metrics that were clearly inappropriate for our purposes, and coexisted with the tool development phase <ref> [Krs94] </ref>. * A pilot experiment was performed with a small number of programmers, each of whom wrote three short and simple programs [Krs94, KS95]. <p> stage helped us determine the proper methods for calculating the metrics, eliminated those metrics that were clearly inappropriate for our purposes, and coexisted with the tool development phase [Krs94]. * A pilot experiment was performed with a small number of programmers, each of whom wrote three short and simple programs <ref> [Krs94, KS95] </ref>. <p> Because it involves the semantic analysis of English sentences, it is unlikely that this process will be automated soon. 7 A detailed description of the programs can be found in <ref> [Krs94, KS95] </ref> Table 1: Distribution of Programs Group Identification Programs Students 1 (Projects for the Fall 1993 term) 57 Students 2 (Programs developed for other terms) 6 Pilot 1 (Programs developed by students for the pilot experiment) 18 Pilot 2 (Programs developed by experienced programmers for the pilot experiment) 6 Faculty
Reference: [KS95] <author> Ivan Krsul and Eugene Spafford. </author> <title> Authorship analysis: Identifying the author of a program. </title> <booktitle> In Proceedings of the 18th National Information Systems Security Conference, </booktitle> <pages> pages 514-524. </pages> <institution> National Institute of Standards and Technology, </institution> <month> October </month> <year> 1995. </year>
Reference-contexts: stage helped us determine the proper methods for calculating the metrics, eliminated those metrics that were clearly inappropriate for our purposes, and coexisted with the tool development phase [Krs94]. * A pilot experiment was performed with a small number of programmers, each of whom wrote three short and simple programs <ref> [Krs94, KS95] </ref>. <p> Because it involves the semantic analysis of English sentences, it is unlikely that this process will be automated soon. 7 A detailed description of the programs can be found in <ref> [Krs94, KS95] </ref> Table 1: Distribution of Programs Group Identification Programs Students 1 (Projects for the Fall 1993 term) 57 Students 2 (Programs developed for other terms) 6 Pilot 1 (Programs developed by students for the pilot experiment) 18 Pilot 2 (Programs developed by experienced programmers for the pilot experiment) 6 Faculty
Reference: [LC90] <author> A. Lake and C. Cook. </author> <title> STYLE: An automated program style analyzer for Pascal. </title> <journal> ACM SIGCSE Bulletin, </journal> <volume> 22(3) </volume> <pages> 29-33, </pages> <year> 1990. </year>
Reference-contexts: However, they have distinctive styles. Authorship analysis should not consider their authors to be the same. Many other sources have influenced our choice of metrics <ref> [LC90, BB89, OC90, Coo87] </ref> but do not contain a specific set of rules, metrics or proverbs. All these sources provide material from which we may derive useful metrics.
Reference: [Led87] <author> Henry Ledgard. </author> <title> C With Excellence: Programming Proverbs. </title> <publisher> Hayden Books, </publisher> <year> 1987. </year>
Reference-contexts: devotes a chapter to programming style for improving the readability of programs. * Jay Ranade and Alan Nash [RN93] give more than three hundred pages of style rules specifically for the C programming language. * Henry Ledgard gives a comprehensive list of C programming proverbs that contribute to programming excellence <ref> [Led87] </ref>. functions are functionally equivalent, and one may be a plagiarized version of the other. However, they have distinctive styles. Authorship analysis should not consider their authors to be the same.
Reference: [LS93] <author> T. A. Longstaff and E. E. Schultz. </author> <title> Beyond preliminary analysis of the WANK and OILZ worms: A case study of malicious code. </title> <journal> Computers & Security, </journal> <volume> 12(1) </volume> <pages> 61-77, </pages> <year> 1993. </year>
Reference-contexts: In actual practice, there are documented cases where people have used ad-hoc techniques, based on programming style, to make conclusions about authorship of programs <ref> [Spa89, Spa88, LS93] </ref>. 2.2 Authorship Analysis in Literature Hundreds of books and essays have been written on this topic, some as early as 1837 [Dis37]. Specially interesting is W.
Reference: [OC89] <author> Paul W. Oman and Curt R. Cook. </author> <title> Programming style authorship analysis. </title> <booktitle> In Seventeenth Annual ACM Computer Science Conference Proceedings, </booktitle> <pages> pages 320-326. </pages> <publisher> ACM, </publisher> <year> 1989. </year>
Reference-contexts: Such great figures as Mark Twain, Irving Wall and Sigmund Freud have debated this particular issue at length [HH92]. The issue of identifying program authorship was explored by Cook and Oman <ref> [OC89] </ref> as a means for determining instances of software theft and plagiarism. Finally, Spafford and Weeber suggested that it might be feasible to analyze the remnants of software, typically the remains of a virus or Trojan horse, and identify its author [WS93]. <p> A better indentation measurement must include a consistency value. Cook and Oman briefly explored the use of software complexity metrics to define a relationship between programs and programmers, concluding that these are inadequate measures of stylistic factors and domain attributes <ref> [OC89] </ref>. <p> However, if all of the o's in the sample have their centers filled in, that feature may identify the author. This has a high degree of correlation with the identification of program authors using style analysis <ref> [OC89] </ref>. However, software forensics is really a superset of authorship analysis using style analysis because some of the measurements suggested by Spafford and Weeber include, but are not limited to, all the measurements made by Cook and Oman. <p> of the if, for, while, do, case statements and the ? operator increases our decision count by one. * Metric PSM9: Is the goto keyword used? Software designers and programmers still rely on these [BM85]. * Metric PSM10: Simple software complexity metrics offer little information that might be application independent <ref> [OC89] </ref>. The metrics that we could consider are: cyclomatic complexity number, program volume, complexity of data structures used, mean live variables per statement, and mean variable span [SCS86]. * Metric PSM11: Error detection after system calls that rarely fail.
Reference: [OC90] <author> P. Oman and C. Cook. </author> <title> Typographic style is more than cosmetic. </title> <journal> Communications of the ACM, </journal> <volume> 33(5) </volume> <pages> 506-520, </pages> <year> 1990. </year>
Reference-contexts: However, they have distinctive styles. Authorship analysis should not consider their authors to be the same. Many other sources have influenced our choice of metrics <ref> [LC90, BB89, OC90, Coo87] </ref> but do not contain a specific set of rules, metrics or proverbs. All these sources provide material from which we may derive useful metrics.
Reference: [OC91] <author> P. Oman and C. Cook. </author> <title> A programming style taxonomy. </title> <journal> Journal of Systems Software, </journal> <volume> 15(4) </volume> <pages> 287-301, </pages> <year> 1991. </year>
Reference-contexts: Programmers are comfortable using it and the language is commonly used in the academic community and in industry. 4.1 Sources for the Collection of Metrics We considered metrics for authorship detection from a wide variety of likely sources: * Oman and Cook <ref> [OC91] </ref> collected a list of 236 style rules that can be used as a base for deriving metrics of programming style. * Conte, Dunsmore and Shen [SCS86] give a comprehensive list of software complexity metrics. * Kernighan and Plauger [KP78] give over seventy programming rules that should be part of good
Reference: [Ott77] <author> K. Ottenstein. </author> <title> An algorithmic approach to the detection and prevention of plagiarism. </title> <journal> ACM SIGCSE Bulletin, </journal> <volume> 8(4) </volume> <pages> 30-41, </pages> <year> 1977. </year>
Reference-contexts: While plagiarism detection needs to detect the similarity between these two programs, authorship analysis does not. For the purpose of authorship identification, these two programs have distinct authors. Many people have devoted time and resources to the development of plagiarism detection <ref> [Ott77, Gri81, Jan88, Wha86] </ref>, and a comprehensive analysis of their work is beyond the scope of this paper. We can, however give a simple example that will illustrate how measurements traditionally used for plagiarism detection are ill suited to authorship analysis. Consider the two functions shown in Figure 4.
Reference: [RN93] <author> J. Ranade and A. Nash. </author> <title> The Elements of C Programming Style. </title> <publisher> McGraw-Hill Inc., </publisher> <year> 1993. </year>
Reference-contexts: Shen [SCS86] give a comprehensive list of software complexity metrics. * Kernighan and Plauger [KP78] give over seventy programming rules that should be part of good programming practice. * Van Tassel [Tas78] devotes a chapter to programming style for improving the readability of programs. * Jay Ranade and Alan Nash <ref> [RN93] </ref> give more than three hundred pages of style rules specifically for the C programming language. * Henry Ledgard gives a comprehensive list of C programming proverbs that contribute to programming excellence [Led87]. functions are functionally equivalent, and one may be a plagiarized version of the other. <p> We do not examine include files or type declarations because there is no consistent method for differentiating between those declarations that are imported from external modules, and those that are native to the programmer. 4.2.1 Programming Layout Metrics * Metric STY1: A vector of metrics indicating indentation style <ref> [RN93, pages 68-69] </ref>: * Metric STY1a: Indentation of C statements within surrounding blocks. * Metric STY1b: Percentage of open curly brackets (f) that are alone in a line. * Metric STY1c: Percentage of open curly brackets (f) that are the first character in a line. * Metric STY1d: Percentage of open <p> vector will be composed of: * Metric STY6a: Use of borders to highlight comments. * Metric STY6b: Percentage of lines of code with inline comments. * Metric STY6c: Ratio of lines of block style comments to lines of code. * Metric STY7: Ratio of white lines to lines of code <ref> [RN93, pages 70-71] </ref>. 4.2.2 Programming Style Metrics * Metric PRO1: Mean program line length (characters per line) [BM85]. * Metric PRO2: A vector of metrics that will consider name lengths. * Metric PRO2a: Mean local variable name length. * Metric PRO2b: Mean global variable name length. * Metric PRO2c: Mean function <p> This vector will consist of: * Metric PRO3a: Some names use the underscore character. * Metric PRO3b: Use of temporary variables 4 that are named XXX1, XXX2, etc. [KP78], or tmp, temp, tmpXXX or tempXXX <ref> [RN93] </ref>. * Metric PRO3c: Percentage of variable names that start with an uppercase letter. * Metric PRO3d: Percentage of function names that start with an uppercase letter. * Metric PRO4: Global variable count to mean local variable count ratio. <p> us a better metric for measuring the frequency of usage of global variables. * Metric PRO6: Use of conditional compilation. * Metric PRO7: Preference of either while, for or do loops. * Metric PRO8: Does the programmer use comments that are nearly an echo of the code [KP78, page 143] <ref> [RN93, page 82] </ref>? * Metric PRO9: Type of function parameter declaration. <p> However, in this paper we will follow the convention that a variable is temporary if it there is no direct association between its name and its (assumed) use. 5 Debugging is difficult. Many nonstandard techniques have been developed <ref> [RN93] </ref>, and we cannot hope to identify all forms of debugging symbols. However, there are some techniques that are widely used and we will concentrate on these. or constants containing the words debug or dbg [RN93, pages38-53]. <p> Many nonstandard techniques have been developed [RN93], and we cannot hope to identify all forms of debugging symbols. However, there are some techniques that are widely used and we will concentrate on these. or constants containing the words debug or dbg <ref> [RN93, pages38-53] </ref>. Figure 7 shows three common debugging styles in C. frequently used debugging techniques, and programmers frequently prefer one over the other. <p> rely on the internal representation of data objects? This metric would check for programmers relying on the size and byte order of integers, the size of floats, etc. * Metric PSM13: Do functions do nothing successfully? Kernighan and Plauger in [KP78, pages 111-114] and Jay Ranade and Alan Nash in <ref> [RN93, page 32] </ref> emphasize the need to make sure that there are no unexpected side effects in functions when these must do nothing. In this context, functions that do nothing successfully are functions that correctly test for boundary conditions on their input parameters. <p> We must note that it is an undecidable problem to determine the correctness of an arbitrary function [HU79]. * Metric PSM14: Do comments and code agree? Kernighan and Plauger write in [KP78] that a comment is of zero (or negative) value if it is wrong. Ranade and Nash <ref> [RN93, page 89] </ref> devote a rule to the truth of every comment. Even if the comments were initially accurate, it is possible that during the maintenance cycle of a program they became inaccurate.
Reference: [SAS] <institution> The SAS Institute. </institution> <note> SAS/STAT User's Guide. Volume 1, ANOVA-FREQ, fourth edition. </note>
Reference-contexts: The method used during the early phases of the experiments is a statistical analysis method called discriminant analysis. This method, described in <ref> [SAS, JW88] </ref> is a multivariate technique concerned with separating observations and with allocating new observations into previously defined groups. On subsequent phases of the experiments we used the neural networks and likelihood classifiers that the LNKnet software, developed at the MIT Lincoln Laboratory, provides. <p> Surprisingly, most of the metrics that showed large variations among programmers were eliminated as well. The performance of our statistical analysis with the remaining metrics was discouraging, with only twenty percent of the programs being classified correctly. The step discrimination tool provided by the SAS program <ref> [SAS] </ref> should theoretically be capable of eliminating metrics that are not useful from the statistical base. Unfortunately, this tool was not helpful because it failed to eliminate any of the metrics from our set. To resolve this issue, we built a tool that helped us visualize the metrics collected.
Reference: [SCS86] <author> H. Dunsmore S. Conte and V. Shen. </author> <title> Software Engineering Metrics and Models. </title> <publisher> The Benjamin/Cummings Publishing Company, </publisher> <year> 1986. </year>
Reference-contexts: But authorship analysis should not consider them identical. If both authors happen to write these functions independently, and this is a common occurrence, our system should identify them as unique. 4 Our Experiment The term Software Metric was defined by Conte, Dunsmore and Shen in <ref> [SCS86] </ref> as: Software metrics are used to characterize the essential features for software quantitatively, so that classification, comparison, and mathematical analysis can be applied. What we are trying to measure, for establishing the authorship of a program, is precisely some of these features. <p> 4.1 Sources for the Collection of Metrics We considered metrics for authorship detection from a wide variety of likely sources: * Oman and Cook [OC91] collected a list of 236 style rules that can be used as a base for deriving metrics of programming style. * Conte, Dunsmore and Shen <ref> [SCS86] </ref> give a comprehensive list of software complexity metrics. * Kernighan and Plauger [KP78] give over seventy programming rules that should be part of good programming practice. * Van Tassel [Tas78] devotes a chapter to programming style for improving the readability of programs. * Jay Ranade and Alan Nash [RN93] give <p> To simplify the computation of this metric, we have chosen to modify the definition of decision count as given in <ref> [SCS86] </ref>. We do not count each logical operator inside a test as a separate decision. <p> The metrics that we could consider are: cyclomatic complexity number, program volume, complexity of data structures used, mean live variables per statement, and mean variable span <ref> [SCS86] </ref>. * Metric PSM11: Error detection after system calls that rarely fail. Some programmers tend to ignore the error return values of system calls that are considered reliable [GS92, page 164].
Reference: [Spa88] <author> Eugene H. Spafford. </author> <title> The internet worm program: An analysis. </title> <type> Technical Report CSD-TR-823, </type> <institution> Department of Computer Science. Purdue University, </institution> <year> 1988. </year>
Reference-contexts: In actual practice, there are documented cases where people have used ad-hoc techniques, based on programming style, to make conclusions about authorship of programs <ref> [Spa89, Spa88, LS93] </ref>. 2.2 Authorship Analysis in Literature Hundreds of books and essays have been written on this topic, some as early as 1837 [Dis37]. Specially interesting is W.
Reference: [Spa89] <author> Eugene H. Spafford. </author> <title> The internet worm program: An analysis. </title> <journal> Computer Communication Review, </journal> <volume> 19(1), </volume> <month> January </month> <year> 1989. </year>
Reference-contexts: In actual practice, there are documented cases where people have used ad-hoc techniques, based on programming style, to make conclusions about authorship of programs <ref> [Spa89, Spa88, LS93] </ref>. 2.2 Authorship Analysis in Literature Hundreds of books and essays have been written on this topic, some as early as 1837 [Dis37]. Specially interesting is W.
Reference: [Tas78] <author> Dennie Van Tassel. </author> <title> Program Style, Design, Efficiency, Debugging, and Testing. </title> <publisher> Prentice Hall, </publisher> <year> 1978. </year>
Reference-contexts: style rules that can be used as a base for deriving metrics of programming style. * Conte, Dunsmore and Shen [SCS86] give a comprehensive list of software complexity metrics. * Kernighan and Plauger [KP78] give over seventy programming rules that should be part of good programming practice. * Van Tassel <ref> [Tas78] </ref> devotes a chapter to programming style for improving the readability of programs. * Jay Ranade and Alan Nash [RN93] give more than three hundred pages of style rules specifically for the C programming language. * Henry Ledgard gives a comprehensive list of C programming proverbs that contribute to programming excellence
Reference: [Wha86] <author> G. Whale. Plague: </author> <title> Detection of plagiarism using program structure. </title> <booktitle> In Proceedings of the Ninth Australian Computer Science Conference, </booktitle> <pages> pages 231-241, </pages> <year> 1986. </year>
Reference-contexts: While plagiarism detection needs to detect the similarity between these two programs, authorship analysis does not. For the purpose of authorship identification, these two programs have distinct authors. Many people have devoted time and resources to the development of plagiarism detection <ref> [Ott77, Gri81, Jan88, Wha86] </ref>, and a comprehensive analysis of their work is beyond the scope of this paper. We can, however give a simple example that will illustrate how measurements traditionally used for plagiarism detection are ill suited to authorship analysis. Consider the two functions shown in Figure 4.
Reference: [WS93] <author> Stephen A. Weeber and Eugene H. Spafford. </author> <title> Software forensics: Can we track code to its authors? Computers & Security, </title> <booktitle> 12(6) </booktitle> <pages> 585-595, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: They theorize that this technique, they call software forensics, could be used to examine and analyze software in any form; be it source code for any language or executable images <ref> [WS93] </ref>. <p> Finally, Spafford and Weeber suggested that it might be feasible to analyze the remnants of software, typically the remains of a virus or Trojan horse, and identify its author <ref> [WS93] </ref>. In actual practice, there are documented cases where people have used ad-hoc techniques, based on programming style, to make conclusions about authorship of programs [Spa89, Spa88, LS93]. 2.2 Authorship Analysis in Literature Hundreds of books and essays have been written on this topic, some as early as 1837 [Dis37]. <p> by Berghel and Sallach [BS84] and Evangelist [Eva84] support these findings. 2.4 Software Forensics Spafford and Weeber suggested that a technique they called software forensics could be used to examine and analyze software in any form, be it source code for any language or executable images, to identify the author <ref> [WS93] </ref>. Spafford and Weeber wrote the following of software forensics: It would be similar to the use of handwriting analysis by law enforcement officials to identify the authors of documents involved in crimes, or to provide confirmation of the role of a suspect. distribution is clustered around the 10% mark.
References-found: 35

