URL: http://www.cs.cmu.edu/afs/cs/academic/class/15750-s98/www/handouts/treap-seidel.ps.gz
Refering-URL: http://www.cs.cmu.edu/afs/cs/academic/class/15750-s98/www/handouts/
Root-URL: 
Title: Randomized Search Trees  
Author: Raimund Seidel Cecilia R. Aragon 
Address: Berkeley CA 94720  D-66041 Saarbrucken, GERMANY  Berkeley CA 94720  
Affiliation: Computer Science Division University of California Berkeley  Fachberich Informatik Universitat des Saarlandes  Computer Science Division University of California Berkeley  
Abstract: We present a randomized strategy for maintaining balance in dynamically changing search trees that has optimal expected behavior. In particular, in the expected case a search or an update takes logarithmic time, with the update requiring fewer than two rotations. Moreover, the update time remains logarithmic, even if the cost of a rotation is taken to be proportional to the size of the rotated subtree. Finger searches and splits and joins can be performed in optimal expected time also. We show that these results continue to hold even if very little true randomness is available, i.e. if only a logarithmic number of truely random bits are available. Our approach generalizes naturally to weighted trees, where the expected time bounds for accesses and updates again match the worst case time bounds of the best deterministic methods. We also discuss ways of implementing our randomized strategy so that no explicit balance information is maintained. Our balancing strategy and our algorithms are exceedingly simple This paper is dedicated to the memory of Gene Lawler . and should be fast in practice.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G.M. Adel'son-Velskii and Y.M. Landis, </author> <title> An algorithm for the organization of information. </title> <journal> Soviet Math. Dokl. </journal> <month> 3 </month> <year> (1962) </year> <month> 1259-1262. </month>
Reference-contexts: Nonetheless, a fair number of strategies have been developed for maintaining approximate balance in such changing search trees. Examples are AVL-trees <ref> [1] </ref>, (a; b)-trees [4], BB (ff)-trees [25], red-black trees [13], and many others. All these classes of trees guarantee that accesses and updates can be performed in O (log n) worst case time. Some sort of balance information stored with the nodes is used for the restructuring during updates. <p> How would one insert an item x into a weighted randomized search tree with fixed weight k? This can most easily be done if the distribution function F is the identity, i.e. we start with random variables uniformly distributed in the interval <ref> [0; 1] </ref>. The distribution function G k for the maximum of k such random variables has the form G k (z) = z k . <p> From this it follows that x should be inserted into the tree with priority r 1=k , where r is a random number chosen uniformly from the interval <ref> [0; 1] </ref>. Since the only operations involving priorities are comparisons and the logarithm function is monotonic, one can also store (log r)=k instead. Adapting the tree to changing weights is also possible, see section 5.11 . Finally there is the question of how much "randomness" is required for our method. <p> This can be achieved as follows: Let the priorities be real random numbers drawn uniformly from the interval <ref> [0; 1] </ref>. Such numbers can be generated piece-meal by adding more and more random bits as digits to their binary representations. The idea is, of course, to generate only as much of the binary representation as needed. The only priority operation in the update algorithms are comparisons between priorities. <p> Let us first address problem (a). In section 3 we briefly outlined how to realize weighted randomized search trees. As priority p for an item x of weight w use u 1=w (or equivalently (log u)=w), where u is a random number uniformly distributed in <ref> [0; 1] </ref>. This simulates generating the maximum of w random numbers drawn independently and uniformly from [0; 1]. If the new weight of x is w 0 and one chooses as new priority p 0 = v 1=w 0 , where v is a new random number drawn from [0; 1], <p> As priority p for an item x of weight w use u 1=w (or equivalently (log u)=w), where u is a random number uniformly distributed in <ref> [0; 1] </ref>. This simulates generating the maximum of w random numbers drawn independently and uniformly from [0; 1]. If the new weight of x is w 0 and one chooses as new priority p 0 = v 1=w 0 , where v is a new random number drawn from [0; 1], then p 0 is not necessarily larger than p. <p> in <ref> [0; 1] </ref>. This simulates generating the maximum of w random numbers drawn independently and uniformly from [0; 1]. If the new weight of x is w 0 and one chooses as new priority p 0 = v 1=w 0 , where v is a new random number drawn from [0; 1], then p 0 is not necessarily larger than p. This means that D 0 (x), the new depth of x, could be larger than the old depth D (x), inspite of the weight increase, which is expected to make the depth of x smaller. <p> This difficulty does not arise if one chooses as new priority p 0 = u 1=w 0 (or equivalently (log u)=w 0 ), where u is the random number originally drawn from <ref> [0; 1] </ref>. Note that although the random variable p and p 0 are highly dependent, each has the correct distribution, and this is all that is required. Since w 0 &gt; w we have u 1=w 0 &gt; u 1=w , i.e. p 0 &gt; p. <p> When implementing the method outlined here, one needs to store for every item the priority implicitly in two pieces (w; u), where integer w is the weight and u is a random number from <ref> [0; 1] </ref>. When two priorities (w; u) and (w; u) are to be compared one has to compare u 1=w with u 1=w . Alternatively one could store the pieces (w; log u) and use (log u)=w for the explicit comparison. This raises the issue of the cost of arithmetic. <p> Following the definition of a weighted randomized search tree the priorities of an n node tree of total weight W can be viewed as follows: W random numbers are drawn independently and uniformly from the interval <ref> [0; 1] </ref> and certain n of those chosen numbers are selected to be the priorities. Now basic arguments show that with probability at most 1=W k2 the difference of any two of the W chosen numbers is smaller than 1=W k . <p> Now the addresses of the nodes can serve as priorities, i.e. the node L [i] has priority i. We will assume here that the underlying treap has the min-heap property, and not the max-heap property. Thus L <ref> [1] </ref> will the root of the tree. Randomized Search Trees 30 How does one insert into or delete from an n-node tree stored in this fashion? Basically one needs to update a random permutation.
Reference: [2] <author> A. Andersson and T. Ottmann, </author> <title> Faster uniquely represented dictionaries. </title> <booktitle> Proc. 32nd FOCS (1991) 642-649. </booktitle>
Reference-contexts: Bob Tarjan [29] pointed out to us that this method also yields a good randomized method for the so-called unique representation problem where one would like subsets of a finite universe to have unique tree representations (see e.g. <ref> [2] </ref> and [27]). 7.2 Locations as priorities Here we store the nodes of the tree in an array L [] in random order. Now the addresses of the nodes can serve as priorities, i.e. the node L [i] has priority i.
Reference: [3] <author> H. Baumgarten, H. Jung, and K. Mehlhorn, </author> <title> Dynamic point location in general subdivision. </title> <booktitle> Proc. 3rd ACM-SIAM Symp. on Discrete Algorithms (SODA) (1992) 250-258. </booktitle>
Reference-contexts: We store with every node v in the tree besides the two children pointers also two pointers set to the left parent and right parent of v, respectively, or to nil if such an parent does not exist. Such parent pointers have been used before in <ref> [3] </ref>. Please note that during a rotation or when adding or clipping a leaf these parent pointers can be maintained at constant cost. Thus no significant increase in update times is incurred.
Reference: [4] <author> R. Bayer and E. McCreight, </author> <title> Organization and maintenance of large ordered indices. </title> <type> Act. </type> <institution> Inf. </institution> <month> 1 </month> <year> (1972) </year> <month> 173-189. </month> <title> Randomized Search Trees 32 </title>
Reference-contexts: Nonetheless, a fair number of strategies have been developed for maintaining approximate balance in such changing search trees. Examples are AVL-trees [1], (a; b)-trees <ref> [4] </ref>, BB (ff)-trees [25], red-black trees [13], and many others. All these classes of trees guarantee that accesses and updates can be performed in O (log n) worst case time. Some sort of balance information stored with the nodes is used for the restructuring during updates.
Reference: [5] <author> S.W. Bent, D.D. Sleator, and R.E. Tarjan, </author> <title> Biased search trees. </title> <journal> SIAM J. Comput. </journal> <month> 14 </month> <year> (1985) </year> <month> 545-568. </month>
Reference-contexts: For the static case an "optimal" tree of this kind can be constructed off-line by a dynamic programming technique. For the dynamic case strategies are known, such as biased 2-3 trees <ref> [5] </ref> and D-trees [17], that allow accessing an item of "weight" w in worst case time O (log (W=w)), which is basically optimal. (Here W is the sum of the weights of all the items in the tree.) Updates can be performed in time O (log (W= minfw; w; w+g), where
Reference: [6] <author> S.W. Bent and J.R. Driscoll, </author> <title> Randomly balanced search trees. </title> <type> Manuscript (1991). </type>
Reference-contexts: With a strategy of this sort the copying cost incurred through the size changes is easy to be seen constant in the amortized sense. 7.3 Computing priorities from subtree sizes This method was suggested by Bent and Driscoll <ref> [6] </ref>. It assumes that for every node x in the tree the size S (x) of its subtree is known. In a number of applications of search trees this information is stored with every node in any case.
Reference: [7] <author> R.P. Brent, </author> <title> Fast Multiple Precision Evaluation of Elementary Functions. </title> <journal> J. </journal> <note> of the ACM 23 (1976) 242-251. </note>
Reference-contexts: This seems clear in practice, since there one would definitely use a floating point implementation. (This is not to say that weighted trees are necessarily practical.) From the theoretical point of view, the assumption of constant time evaluation of those functions is not that unrealistic since Brent <ref> [7] </ref> has shown that, when measured in the bit model, evaluating such functions up to a relative error of 2 m is only slightly more costly than multyplying two m bit numbers.
Reference: [8] <author> M. Brown, </author> <title> Addendum to "A Storage Scheme for Height-Balanced Trees." </title> <journal> Inf. </journal> <note> Proc. Letters 8 (1979) 154-156. </note>
Reference-contexts: All the strategies discussed so far involve reasonably complex restructuring algorithms that require some balance information to be stored with the tree nodes. However, Brown <ref> [8] </ref> has pointed out that some of the unweighted schemes can be implemented without storing any balance infor Randomized Search Trees 3 mation explicitly.
Reference: [9] <author> K.L. Clarkson, K. Mehlhorn, and R. Seidel, </author> <title> Four results on randomized incremental construction. </title> <journal> Comp. </journal> <note> Geometry: Theory and Applications 3 (1993) 185-212. </note>
Reference-contexts: We will just consider the case of R. The other case is symmetric. The crucial observation is that for i &gt; ` the 0-1 random variables A i;` are independent and one can therefore apply so-called Chernoff bounds <ref> [14, 9] </ref> which, in one form, state that if random variable Z is the sum of independent 0-1 variables, then Pr [Z c Ex [Z]] &lt; e c ln (c=e)Ex [Z] .
Reference: [10] <author> L. Devroye, </author> <title> A note on the height of binary search trees. </title> <journal> J. </journal> <note> of the ACM 33 (1986) 489-498. </note>
Reference-contexts: In contrast to the random variables studied in this section the random variable h n , the height of an n-node unweighted randomized search tree, is quite difficult to analyze exactly. Devroye <ref> [10] </ref>, though, has shown that h n = ln n ! fl almost surely, as n ! 1, where fl = 4:31107::: is the unique solution of fl ln (2e=fl) = 1 with fl 2. 4.2 The weighted case Recall that in the weighted case every item x i has associated
Reference: [11] <author> M. </author> <title> Dietzfelbinger, </title> <type> (private communication). </type>
Reference-contexts: No analogue of Theorem 3.3 about limited randomness for skip lists has appeared in the literature. However, Kurt Mehlhorn [19] has adapted the approach taken in this paper to apply to skip lists also. Also Martin Dietzfelbinger <ref> [11] </ref> apparently has proved results in this direction. Comparing skip lists and randomized search trees seems a fruitless exercise. They are both conceptually reasonably simple and both are reasonably simply to implement. Both have been implemented and are for instance availabe as part of LEDA [21, 24].
Reference: [12] <author> I. Galperin and R.L. Rivest, </author> <title> Scapegoat Trees. </title> <booktitle> Proc. 4th ACM-SIAM Symp. on Discrete Algorithms (SODA) (1993) 165-174. </booktitle>
Reference-contexts: Moreover, this is undesirable in a caching or paging environment where the writes involved in the restructuring will dirty memory locations or pages that might otherwise stay clean. Recently Galperin and Rivest <ref> [12] </ref> proposed a new scheme called "scapegoat trees," which also needs basically no balance information at all and achieves logarithmic search time even in the worst case. However logarithmic update time is achieved only in the amortized sense.
Reference: [13] <author> L.J. Guibas and R. Sedgewick, </author> <title> A dichromatic framework for balanced trees. </title> <booktitle> Proc. 19th FOCS (1978) 8-21. </booktitle>
Reference-contexts: Nonetheless, a fair number of strategies have been developed for maintaining approximate balance in such changing search trees. Examples are AVL-trees [1], (a; b)-trees [4], BB (ff)-trees [25], red-black trees <ref> [13] </ref>, and many others. All these classes of trees guarantee that accesses and updates can be performed in O (log n) worst case time. Some sort of balance information stored with the nodes is used for the restructuring during updates.
Reference: [14] <author> T. Hagerup and C. Rub, </author> <title> A guided tour of Chernoff bounds. </title> <journal> Inf. </journal> <note> Proc. Letters 33 (1989/90) 305-308. </note>
Reference-contexts: We will just consider the case of R. The other case is symmetric. The crucial observation is that for i &gt; ` the 0-1 random variables A i;` are independent and one can therefore apply so-called Chernoff bounds <ref> [14, 9] </ref> which, in one form, state that if random variable Z is the sum of independent 0-1 variables, then Pr [Z c Ex [Z]] &lt; e c ln (c=e)Ex [Z] .
Reference: [15] <author> K. Hoffman, K. Mehlhorn, P. Rosenstiehl, and R.E. Tarjan, </author> <title> Sorting Jordan sequences in linear time using level linked search trees. </title> <journal> Inform. </journal> <note> and Control 68 (1986) 170-184. </note>
Reference-contexts: Also, splits and joins of treaps can be performed faster if handles to the minimum and maximum key items in the treaps are available. These operations are discussed in detail in sections 5.7 and 5.8. Some applications such as so-called Jordan sorting <ref> [15] </ref> require the efficient excision of a subsequence, i.e. splitting a set of X of items into Y = fx 2 X j a x:key bg and Z = fx 2 X j x:key &lt; a or x:key &gt; bg.
Reference: [16] <author> E. McCreight, </author> <title> Priority search trees. </title> <journal> SIAM J. Comput. </journal> <month> 14 </month> <year> (1985) </year> <month> 257-276. </month>
Reference: [17] <author> K. Mehlhorn, </author> <title> Sorting and Searching. </title> <publisher> Springer (1984). </publisher>
Reference-contexts: It can even be shown that "most" rotations occur "close" to the leaves; roughly speaking, for BB (ff)-trees this means that the number of times that some subtree of size s is rotated is O (m=s) (see <ref> [17] </ref>). <p> For the static case an "optimal" tree of this kind can be constructed off-line by a dynamic programming technique. For the dynamic case strategies are known, such as biased 2-3 trees [5] and D-trees <ref> [17] </ref>, that allow accessing an item of "weight" w in worst case time O (log (W=w)), which is basically optimal. (Here W is the sum of the weights of all the items in the tree.) Updates can be performed in time O (log (W= minfw; w; w+g), where w and w+
Reference: [18] <author> K. Mehlhorn, </author> <title> Multi-dimensional Searching and Computational Geometry. </title> <publisher> Springer (1984). </publisher>
Reference-contexts: is important for the parallel use of these search trees, and also for applications in computational geometry where the nodes of a primary tree have secondary search structures associated with them that have to be completely recomputed upon rotation in the primary tree (e.g. range trees and segment trees; see <ref> [18] </ref>). Sometimes it is desirable that some items can be accessed more easily than others.
Reference: [19] <author> K. Mehlhorn, </author> <title> (private communication). </title>
Reference-contexts: Apparently again the expected performance characteristics of weighted skip lists match the ones of weighted randomized search trees listed in Theorem 3.2. No analogue of Theorem 3.3 about limited randomness for skip lists has appeared in the literature. However, Kurt Mehlhorn <ref> [19] </ref> has adapted the approach taken in this paper to apply to skip lists also. Also Martin Dietzfelbinger [11] apparently has proved results in this direction. Comparing skip lists and randomized search trees seems a fruitless exercise. They are both conceptually reasonably simple and both are reasonably simply to implement.
Reference: [20] <author> K. Mehlhorn and S. Naher, </author> <title> Algorithm Design and Software Libraries: Recent Develop ments in the LEDA Project. Algorithms, </title> <booktitle> Software, Architectures, Information Processing 92, </booktitle> <volume> Vol. 1, </volume> <publisher> Elsevier Science Publishers B.V., </publisher> <year> 1992 </year>
Reference-contexts: The only possible exception are the results about costly rotations: here, it seems, only partial results are known for skip lists (see [21],[23, section 8.1.1]). Just as with randomized search trees it is possible to generalize skip lists to a weighted version <ref> [20] </ref>. This is done by appropriately biasing the random choice that determines how far up the hierachy a list item is to go. Apparently again the expected performance characteristics of weighted skip lists match the ones of weighted randomized search trees listed in Theorem 3.2.
Reference: [21] <author> K. Mehlhorn and S. Naher, LEDA, </author> <title> a Platform for Combinatorial and Geometric Com puting. </title> <note> To appear in Commun. ACM, </note> <month> January </month> <year> 1995. </year>
Reference-contexts: Also Martin Dietzfelbinger [11] apparently has proved results in this direction. Comparing skip lists and randomized search trees seems a fruitless exercise. They are both conceptually reasonably simple and both are reasonably simply to implement. Both have been implemented and are for instance availabe as part of LEDA <ref> [21, 24] </ref>. They seem to be almost identical in their important performance characteristics. Differences such as randomized search trees can be implemented using only exactly n pointers, whereas this appears to be impossible for skip lists, are not of particularly great practical significance.
Reference: [22] <author> K. Mehlhorn and R. </author> <type> Raman (private communication). </type>
Reference-contexts: Relying on short excess paths Finally let us consider the method, suggested to us by Mehlhorn and Raman <ref> [22] </ref>, that traverses the excess path and relies on the fact that in expectation this path is short, at least for unweighted trees. (In the weighted case one can concoct examples where this expectation can become arbitrarily large.) The advantage of this method is that only the one usual parent pointer
Reference: [23] <author> K. Mulmuley, </author> <title> Computational Geometry: An Introduction through Randomized Algorithms. </title> <publisher> Prentice Hall (1994). </publisher>
Reference-contexts: Moreover, the 2-max property is implied by 8-wise independence because of the following remarkable lemma that is essentially due to Mulmuley <ref> [23] </ref>. Randomized Search Trees 28 Lemma 6.1 Let X be a set of n random variables, each uniformly distributed over a common integer range of size at least n. X has the d-max property if its random variables are (3d + 2)-wise independent. <p> X has the d-max property if its random variables are (3d + 2)-wise independent. A proof of this lemma (or rather, of a related version) can be found in Mulmuley's book <ref> [23, section 10.1] </ref>. We now need to show that results of section 4 about unweighted trees continue to hold up to a constant factor if the random priorities satisfy the 2-max property. This is clear for the central Corollaries 4.5 and 4.6. <p> There seems to be ample room for both skip lists and randomized search trees. We would like to refer the reader to chapter 1 of Mulmuley's book <ref> [23] </ref>, where the two structures are used and discussed as two prototypical randomization strategies. 9 Acknowledgements We would like to thank Kurt Mehlhorn for his constructive comments and criticism.
Reference: [24] <author> S. Naher, </author> <title> LEDA User Manual Version 3.0. </title> <type> Tech. Report MPI-I-93-109, </type> <institution> Max-Planck Institut fur Informatik, </institution> <month> Saarbrucken </month> <year> (1993). </year> <title> Randomized Search Trees 33 </title>
Reference-contexts: Also Martin Dietzfelbinger [11] apparently has proved results in this direction. Comparing skip lists and randomized search trees seems a fruitless exercise. They are both conceptually reasonably simple and both are reasonably simply to implement. Both have been implemented and are for instance availabe as part of LEDA <ref> [21, 24] </ref>. They seem to be almost identical in their important performance characteristics. Differences such as randomized search trees can be implemented using only exactly n pointers, whereas this appears to be impossible for skip lists, are not of particularly great practical significance.
Reference: [25] <author> J. Nievergelt and E.M. Reingold, </author> <title> Binary search trees of bounded balance. </title> <journal> SIAM J. Comput. </journal> <month> 2 </month> <year> (1973) </year> <month> 33-43. </month>
Reference-contexts: Nonetheless, a fair number of strategies have been developed for maintaining approximate balance in such changing search trees. Examples are AVL-trees [1], (a; b)-trees [4], BB (ff)-trees <ref> [25] </ref>, red-black trees [13], and many others. All these classes of trees guarantee that accesses and updates can be performed in O (log n) worst case time. Some sort of balance information stored with the nodes is used for the restructuring during updates.
Reference: [26] <author> W. Pugh, </author> <title> Skip Lists: A Probabilistic Alternative to Balanced Trees. </title> <journal> Commun. </journal> <note> ACM 33 (1990) 668-676. </note>
Reference-contexts: Our strategy and algorithms are exceedingly simple and should be fast in practice. For unweighted trees our strategy can be implemented without storage space for balance information. Randomized search trees are not the only randomized data structure for storing dynamic ordered sets. Bill Pugh <ref> [26] </ref> has proposed and popularized another randomized scheme called skip lists. Although the two schemes are quite different they have almost identical expected performace characteristics. We offer a brief comparison in the last section. Section 2 of the paper describes treaps, the basic structure underlying randomized search trees. <p> Randomized Search Trees 31 We leave the implementation of joins and splits via this method as an exercise. 8 Randomized search trees and Skip lists Skip lists are a probabilistic search structure that were proposed and popularized by Pugh <ref> [26] </ref>. They can be viewed as a hierarchy of coarser and coarser lists constructed over an initial linked list, with a coarser list in the hierarchy guiding the search in the next finer list in the hierarchy.
Reference: [27] <author> W. Pugh and T. Teitelbaum, </author> <title> Incremental Computation via Function Caching. </title> <booktitle> Proc. 16th ACM POPL (1989) 315-328. </booktitle>
Reference-contexts: Bob Tarjan [29] pointed out to us that this method also yields a good randomized method for the so-called unique representation problem where one would like subsets of a finite universe to have unique tree representations (see e.g. [2] and <ref> [27] </ref>). 7.2 Locations as priorities Here we store the nodes of the tree in an array L [] in random order. Now the addresses of the nodes can serve as priorities, i.e. the node L [i] has priority i.
Reference: [28] <author> D.D. </author> <type> Sleator (private communication). </type>
Reference-contexts: The other stores the nodes of the tree in a random permutation and uses node addresses as priorities. The last method recomputes priorities from subtree sizes. 7.1 Priorities from hash functions This method is based on an initial idea of Danny Sleator <ref> [28] </ref>. He suggested to choose and associate with a randomized search tree a hash function h. For every item in the tree the priority is then declared to be h (k), where k is the key of the item.
Reference: [29] <author> R.E. </author> <title> Tarjan (private communication). </title>
Reference-contexts: Note, however, that priorities are compared only during updates, and that priority comparisons are coupled with rotations. This means that the expected number of priorities looked at during a deletion is less than 2 and during an insertion it is less than 4. Bob Tarjan <ref> [29] </ref> pointed out to us that this method also yields a good randomized method for the so-called unique representation problem where one would like subsets of a finite universe to have unique tree representations (see e.g. [2] and [27]). 7.2 Locations as priorities Here we store the nodes of the tree
Reference: [30] <author> D.D. Sleator and R.E. Tarjan, </author> <title> Self-adjusting binary search trees. </title> <journal> J. </journal> <note> of the ACM 32 (1985) 652-686. </note>
Reference-contexts: There are methods that require absolutely no balance information to be maintained. A partiu-carly attractive one was proposed by Sleator and Tarjan <ref> [30] </ref>. Their "splay trees" use an extremely simple restructuring strategy and still achieve all the access and update time bounds mentioned above both for the unweighted and for the weighted case (where the weights do not even need to be known to the algorithm).
Reference: [31] <author> J. Vuillemin, </author> <title> A Unifying Look at Data Structures. </title> <journal> Commun. </journal> <note> ACM 23 (1980) 229-239. </note>
References-found: 31

