URL: http://www.cs.wisc.edu/~cao/papers/icache.ps
Refering-URL: http://www.cs.wisc.edu/~cao/papers/edwards.html
Root-URL: 
Email: fcao,chengjieg@cs.wisc.edu  
Title: Maintaining Strong Cache Consistency in the World-Wide Web  
Author: Pei Cao and Chengjie Liu 
Keyword: World Wide Web, Cache Consistency, Invalidation Protocols, Distributed Systems, Performance Analysis and Measurements.  
Address: Madison, WI 53706  
Affiliation: Department of Computer Science University of Wisconsin-Madison  
Abstract: As the Web continues to explode in size, caching becomes increasingly important. With caching comes the problem of cache consistency. Conventional wisdom holds that strong cache consistency is too expensive for the Web, and weak consistency methods such as Time-To-Live (TTL) are most appropriate. This study compares three consistency approaches: adaptive TTL, polling-every-time and invalidation, through analysis, implementation and trace replay in a simulated environment. Our analysis shows that weak consistency methods save network bandwidth mostly at the expense of returning stale documents to users. Our experiments show that invalidation generates a comparable amount of network traffic and server workload to adaptive TTL and has similar average client response times, while polling-every-time results in more control messages, higher server workload and longer client response times. We show that, contrary to popular belief, strong cache consistency can be maintained for the Web with little or no extra cost than the current weak consistency approaches, and it should be maintained using an invalidation-based protocol. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Thomas E. Anderson, Michael D. Dahlin, Jeanna M. Neefe, David A. Patterson, Drew S. Roselli, and Randolph Y. Wang. </author> <title> Serverless network file systems. </title> <booktitle> In Proceedings of 15th ACM Symposium on Operating Systems Principles, </booktitle> <month> De-cember </month> <year> 1995. </year>
Reference-contexts: The unique challenge in the Web is to scale these protocols to the size of the Internet. Though many recent studies on distributed file systems followed the trend of letting client workstations assume more responsibilities, including caching, consistency maintanence and failure resilience <ref> [4, 1] </ref>, these techniques do not easily apply to the current Web because most web clients have limited resources. 3 Consistency Approaches This section discusses in more detail the three cache consistency approaches, the load they put on the network, and the consistency they provide under the cur rent Internet. 3.1
Reference: [2] <author> Mary Baker, John H. Hartman, Michael D. Kupfer, Ken W. Shirriff, and John Ousterhout. </author> <title> Measurements of a distributed file system. </title> <booktitle> In Proceedings of 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 198-211, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: The adaptive TTL approach handles the problem by adjusting a document's time-to-live based on observations of its lifetime. The approach, also called the Alex protocol, was first proposed in [6]. Adaptive TTL takes advantage of the fact that file lifetime distributions tend to be bimodal <ref> [4, 2] </ref>; if a file has not been modified for a long time, it tends to stay unchanged.
Reference: [3] <author> A. Bestavros. </author> <title> Demand-based resource allocation to reduce traffic and balance load in distributed information systems. </title> <booktitle> In Proceedings of the 1995 IEEE Symposium on Parallel and Distributed Processing, </booktitle> <month> October </month> <year> 1995. </year>
Reference-contexts: We choose the bimodal distribution because it has been observed in the file life time distribution in many real systems <ref> [3, 11] </ref>, and adaptive TTL works particularly well under such distribution. 5.3 Simulation by Trace Replay We measure the performance of adaptive TTL, polling-every-time, and invalidation by running the traces through the implemented prototypes in an environment emulating the Internet. We pick five workstations, connected with a fast Ethernet (100Mb/s). <p> The average hot file life times are chosen to sample the life time range from less than a day to over 5 days, which roughly corresponds to the life time range for hot files found in other studies <ref> [3, 11] </ref>.
Reference: [4] <author> Matthew A. </author> <title> Blaze. Caching in Large-Scale Distributed File Systems. </title> <type> PhD thesis, </type> <institution> Princeton University, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: The unique challenge in the Web is to scale these protocols to the size of the Internet. Though many recent studies on distributed file systems followed the trend of letting client workstations assume more responsibilities, including caching, consistency maintanence and failure resilience <ref> [4, 1] </ref>, these techniques do not easily apply to the current Web because most web clients have limited resources. 3 Consistency Approaches This section discusses in more detail the three cache consistency approaches, the load they put on the network, and the consistency they provide under the cur rent Internet. 3.1 <p> The adaptive TTL approach handles the problem by adjusting a document's time-to-live based on observations of its lifetime. The approach, also called the Alex protocol, was first proposed in [6]. Adaptive TTL takes advantage of the fact that file lifetime distributions tend to be bimodal <ref> [4, 2] </ref>; if a file has not been modified for a long time, it tends to stay unchanged.
Reference: [5] <author> John B. Carter, John K. Bennett, and Willy Zwaenepoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Proceedings of 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: We choose Harvest due to its source code availability and good performance [20]. Cache consistency problems exist in any system that uses some form of cache to speed up accesses. In particular, cache consistency protocols have been studied extensively in computer architecture [12], distributed shared memory (DSM) <ref> [16, 5] </ref>, network and distributed file systems [19, 13, 18], and distributed database systems [9, 10]. The consistency problems are slightly different in the four contexts.
Reference: [6] <author> V. Cate. </author> <title> Alex a global file system. </title> <booktitle> In Proceedings of the 1992 USENIX File System Workshop, </booktitle> <pages> pages 1-12, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: In this approach, a write is complete when the modification is registered in the server's file system. We compare invalidation and polling-every-time with a widely-used weak consistency approach, adaptive TTL <ref> [6] </ref>, through analysis, implementation and trace replay experiments. Adaptive TTL is shown to perform the best among existing weak consistency protocols [11]. <p> Similar difficulties exist with the client polling approach in deciding when to send "if-modified-since" requests. The adaptive TTL approach handles the problem by adjusting a document's time-to-live based on observations of its lifetime. The approach, also called the Alex protocol, was first proposed in <ref> [6] </ref>. Adaptive TTL takes advantage of the fact that file lifetime distributions tend to be bimodal [4, 2]; if a file has not been modified for a long time, it tends to stay unchanged. <p> Studies <ref> [6, 11] </ref> have shown that adaptive TTL can keep the probability of stale documents within reasonable bounds (&lt; 5%). The Harvest cache manager [7] 3 mainly uses this approach to maintain cache consistency, with the percentage set to 50% 1 .
Reference: [7] <author> A. Chankhunthod, P. Danzig, C. Neerdaels, M. Schwartz, and K. Worrell. </author> <title> A hierarchical internet object cache. </title> <booktitle> In Proceedings of the 1996 USENIX Technical Conference, </booktitle> <address> San Diego, CA, </address> <month> January </month> <year> 1996. </year>
Reference-contexts: We implemented the three consistency approaches in the popular Web caching system Harvest <ref> [7] </ref>, and compared their performance by replaying Web server traces through the prototypes running on workstations connected by an Ethernet. Our experiments show that the invalidation approach performs the best among the three consistency approaches, It provides strong consistency at a cost that is similar to that of adaptive TTL. <p> Another study [20] describes a light-weight caching server that employs both adaptive TTL and invalidation for cache consistency. However, the paper focuses on comparing the performance differences between the light-weight server and the CERN proxy server, and does not compare the consistency approaches. Though we chose the Harvest <ref> [8, 7] </ref> system for our implementations, there are many other Web caching software. Popular browsers such as Netscape and the Internet Explorer all offer some form of client caching. CERN proxy [17] offers caching for all its clients, using TTL as the consistency mechanism. <p> Studies [6, 11] have shown that adaptive TTL can keep the probability of stale documents within reasonable bounds (&lt; 5%). The Harvest cache manager <ref> [7] </ref> 3 mainly uses this approach to maintain cache consistency, with the percentage set to 50% 1 . Though adaptive TTL keeps the frequency of stale documents low, it does not eliminate its occurance. Two other approaches can provide stronger consistency guarantees.
Reference: [8] <author> P. B. Danzig, R. S. Hall, and M. F. Schwartz. </author> <title> A case for caching file objects inside internetworks. </title> <booktitle> In Proceedings of SIGCOMM '93, </booktitle> <pages> pages 239-248, </pages> <year> 1993. </year>
Reference-contexts: Another study [20] describes a light-weight caching server that employs both adaptive TTL and invalidation for cache consistency. However, the paper focuses on comparing the performance differences between the light-weight server and the CERN proxy server, and does not compare the consistency approaches. Though we chose the Harvest <ref> [8, 7] </ref> system for our implementations, there are many other Web caching software. Popular browsers such as Netscape and the Internet Explorer all offer some form of client caching. CERN proxy [17] offers caching for all its clients, using TTL as the consistency mechanism.
Reference: [9] <author> Michael Franklin. </author> <title> Client Data Caching: A Foundation for High Performance Object Database Systems. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1996. </year>
Reference-contexts: Cache consistency problems exist in any system that uses some form of cache to speed up accesses. In particular, cache consistency protocols have been studied extensively in computer architecture [12], distributed shared memory (DSM) [16, 5], network and distributed file systems [19, 13, 18], and distributed database systems <ref> [9, 10] </ref>. The consistency problems are slightly different in the four contexts. In computer architecture and DSM systems, the consistency algorithms must handle multiple writers to a data item, and are subject to the most stringent limits on CPU and storage overheads. <p> This remains part of our future work. In distributed databases, the database system must provide transactional guarantees over a set of data accesses in the presence of caches <ref> [9, 10] </ref>. In comparison, the Web provides a much more primitive interface. Thus, the Web cache consistency problem is much simpler than the database cache consistency problem.
Reference: [10] <author> Michael J. Franklin, Michael J. Carey, and Miron Livny. </author> <title> Transactional client-server cache consistency: Alternatives and performance. </title> <booktitle> ACM Transaction on Database Systems, </booktitle> <year> 1997. </year>
Reference-contexts: Cache consistency problems exist in any system that uses some form of cache to speed up accesses. In particular, cache consistency protocols have been studied extensively in computer architecture [12], distributed shared memory (DSM) [16, 5], network and distributed file systems [19, 13, 18], and distributed database systems <ref> [9, 10] </ref>. The consistency problems are slightly different in the four contexts. In computer architecture and DSM systems, the consistency algorithms must handle multiple writers to a data item, and are subject to the most stringent limits on CPU and storage overheads. <p> This remains part of our future work. In distributed databases, the database system must provide transactional guarantees over a set of data accesses in the presence of caches <ref> [9, 10] </ref>. In comparison, the Web provides a much more primitive interface. Thus, the Web cache consistency problem is much simpler than the database cache consistency problem. <p> In comparison, the Web provides a much more primitive interface. Thus, the Web cache consistency problem is much simpler than the database cache consistency problem. However, many of the performance tradeoffs between polling (called "validity check" in <ref> [10] </ref>) and invalidation (called "change notification" in [10]) are similar in both contexts. In this paper we mainly compare strong cache consistency protocols with the current weak cache consistency approach in the Web. <p> In comparison, the Web provides a much more primitive interface. Thus, the Web cache consistency problem is much simpler than the database cache consistency problem. However, many of the performance tradeoffs between polling (called "validity check" in <ref> [10] </ref>) and invalidation (called "change notification" in [10]) are similar in both contexts. In this paper we mainly compare strong cache consistency protocols with the current weak cache consistency approach in the Web. We plan to look more into leveraging the techniques in database consistency algorithms to improve the performance of Web consistency protocols.
Reference: [11] <author> James Gwertzman and Margo Seltzer. </author> <title> Worldwide web cache consistency. </title> <booktitle> In Proceedings of the 1996 USENIX Technical Conference, </booktitle> <address> San Diego, CA, </address> <month> January </month> <year> 1996. </year>
Reference-contexts: We compare invalidation and polling-every-time with a widely-used weak consistency approach, adaptive TTL [6], through analysis, implementation and trace replay experiments. Adaptive TTL is shown to perform the best among existing weak consistency protocols <ref> [11] </ref>. <p> In particular, Gwertzman and Seltzer's paper <ref> [11] </ref> gave an excellent comparison of cache consistency approaches via simulation, and concluded that a weak-consistency approach such as adaptive TTL would be best for Web caching. The main metric used in [11] is network traffic. <p> In particular, Gwertzman and Seltzer's paper <ref> [11] </ref> gave an excellent comparison of cache consistency approaches via simulation, and concluded that a weak-consistency approach such as adaptive TTL would be best for Web caching. The main metric used in [11] is network traffic. The study did not address many other important questions, such as server loads, client response times, and consistency message latency. Another study that is similar to ours is Worrell's thesis [21]. The study investigates using invalidation as the consistency approach in hierarchical network object caches. <p> Studies <ref> [6, 11] </ref> have shown that adaptive TTL can keep the probability of stale documents within reasonable bounds (&lt; 5%). The Harvest cache manager [7] 3 mainly uses this approach to maintain cache consistency, with the percentage set to 50% 1 . <p> We choose the bimodal distribution because it has been observed in the file life time distribution in many real systems <ref> [3, 11] </ref>, and adaptive TTL works particularly well under such distribution. 5.3 Simulation by Trace Replay We measure the performance of adaptive TTL, polling-every-time, and invalidation by running the traces through the implemented prototypes in an environment emulating the Internet. We pick five workstations, connected with a fast Ethernet (100Mb/s). <p> The average hot file life times are chosen to sample the life time range from less than a day to over 5 days, which roughly corresponds to the life time range for hot files found in other studies <ref> [3, 11] </ref>.
Reference: [12] <author> John L. Hennessy and David A. Patterson. </author> <title> Computer Architecture A Quantitative Approach. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1990. </year>
Reference-contexts: We choose Harvest due to its source code availability and good performance [20]. Cache consistency problems exist in any system that uses some form of cache to speed up accesses. In particular, cache consistency protocols have been studied extensively in computer architecture <ref> [12] </ref>, distributed shared memory (DSM) [16, 5], network and distributed file systems [19, 13, 18], and distributed database systems [9, 10]. The consistency problems are slightly different in the four contexts.
Reference: [13] <author> John H. Howard, Michael Kazar, Sherri G. Me-nees, David A. Nichols, M. Satyanarayanan, Robert N. Sidebotham, and Michael J. West. </author> <title> Scale and performance in a distributed file system. </title> <journal> ACM Transactions on Computer Systems, </journal> <pages> pages 6(1) 51-81, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: Cache consistency problems exist in any system that uses some form of cache to speed up accesses. In particular, cache consistency protocols have been studied extensively in computer architecture [12], distributed shared memory (DSM) [16, 5], network and distributed file systems <ref> [19, 13, 18] </ref>, and distributed database systems [9, 10]. The consistency problems are slightly different in the four contexts. In computer architecture and DSM systems, the consistency algorithms must handle multiple writers to a data item, and are subject to the most stringent limits on CPU and storage overheads. <p> We plan to look more into leveraging the techniques in database consistency algorithms to improve the performance of Web consistency protocols. Of the four contexts, the cache consistency problem of network and distributed file systems <ref> [19, 13, 18] </ref> is most similar to the Web consistency problem. The main differences are that the Web is orders of magnitudes bigger than any distributed file system, and the systems participating in the Web are heterogeneous, use different operating systems and belong to different organizations. <p> For example, the TTL approach is very similar to the NFS protocol for cache consistency [19], the polling-every-time approach is similar to what is adopted in the Sprite file systems (clients contact the server on every file open/close) [18], and invalidations are essentially call-backs in AFS <ref> [13] </ref>. The unique challenge in the Web is to scale these protocols to the size of the Internet.
Reference: [14] <author> Stefanos Kaxiras and James Goodman. </author> <title> Implementation and performance of the GLOW kilo-processor extensions to sci on the wisconsin wind tunnel. </title> <booktitle> In Proceedings of the 2nd International Workshop on SCI-Based High-Performance Low-Cost Computing, </booktitle> <month> March </month> <year> 1995. </year>
Reference-contexts: For example, our initial interest in this work came from discussions with Stefanos Kaxiras and James Goodman at University of Wisconsin at Madison on applying 2 the GLOW scalable hardware shared-memory coher-ence protocol <ref> [14] </ref> to cache consistency on the Web. GLOW uses hierarchical caching and hardware mul-ticast network for invalidation messages.
Reference: [15] <author> Paul Leach and Jeff Mogul. </author> <title> The HTTP hit-metering protocol. </title> <type> Internet draft, </type> <month> November </month> <year> 1996. </year> <note> URL ftp://ieft.org/internet-draft/draft-mogul-http-hit-metering-00.txt. </note>
Reference-contexts: What we have shown is that, for those that do care, invalidation is feasible, and it is a better approach than TTL or client polling. For those commercial Web sites that want to control the accesses to its contents, invalidation should be merged with other hit-metering protocols <ref> [15] </ref> to provide both the benefits of caching and the availability of access control.
Reference: [16] <author> Kai Li and Paul Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: We choose Harvest due to its source code availability and good performance [20]. Cache consistency problems exist in any system that uses some form of cache to speed up accesses. In particular, cache consistency protocols have been studied extensively in computer architecture [12], distributed shared memory (DSM) <ref> [16, 5] </ref>, network and distributed file systems [19, 13, 18], and distributed database systems [9, 10]. The consistency problems are slightly different in the four contexts.
Reference: [17] <author> A. Luotonen, H. Frystyk, and T. Berners-Lee. </author> <title> CERN HTTPD public domain full-featured hypertext/proxy server 16 with caching. </title> <type> Technical report, </type> <note> Available from http://www.w3.org/hypertext/WWW/ Daemon/Status.html, </note> <year> 1994. </year>
Reference-contexts: Though we chose the Harvest [8, 7] system for our implementations, there are many other Web caching software. Popular browsers such as Netscape and the Internet Explorer all offer some form of client caching. CERN proxy <ref> [17] </ref> offers caching for all its clients, using TTL as the consistency mechanism. We choose Harvest due to its source code availability and good performance [20]. Cache consistency problems exist in any system that uses some form of cache to speed up accesses.
Reference: [18] <author> Michael N. Nelson, Brent B. Welch, and John K. Ousterhout. </author> <title> Caching in the Sprite file system. </title> <journal> ACM Transactions on Computer Systems, </journal> <pages> pages 6(1) 134-154, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: Cache consistency problems exist in any system that uses some form of cache to speed up accesses. In particular, cache consistency protocols have been studied extensively in computer architecture [12], distributed shared memory (DSM) [16, 5], network and distributed file systems <ref> [19, 13, 18] </ref>, and distributed database systems [9, 10]. The consistency problems are slightly different in the four contexts. In computer architecture and DSM systems, the consistency algorithms must handle multiple writers to a data item, and are subject to the most stringent limits on CPU and storage overheads. <p> We plan to look more into leveraging the techniques in database consistency algorithms to improve the performance of Web consistency protocols. Of the four contexts, the cache consistency problem of network and distributed file systems <ref> [19, 13, 18] </ref> is most similar to the Web consistency problem. The main differences are that the Web is orders of magnitudes bigger than any distributed file system, and the systems participating in the Web are heterogeneous, use different operating systems and belong to different organizations. <p> Despite the differences, there are similarities in the solutions. For example, the TTL approach is very similar to the NFS protocol for cache consistency [19], the polling-every-time approach is similar to what is adopted in the Sprite file systems (clients contact the server on every file open/close) <ref> [18] </ref>, and invalidations are essentially call-backs in AFS [13]. The unique challenge in the Web is to scale these protocols to the size of the Internet.
Reference: [19] <author> R. Sandberg, D. Boldberg, S. Kleiman, D. Walsh, and B. Lyon. </author> <title> Design and implementation of the Sun network filesystem. </title> <booktitle> In Summer Usenix Conference Proceedings, </booktitle> <pages> pages 119-130, </pages> <month> June </month> <year> 1985. </year>
Reference-contexts: Cache consistency problems exist in any system that uses some form of cache to speed up accesses. In particular, cache consistency protocols have been studied extensively in computer architecture [12], distributed shared memory (DSM) [16, 5], network and distributed file systems <ref> [19, 13, 18] </ref>, and distributed database systems [9, 10]. The consistency problems are slightly different in the four contexts. In computer architecture and DSM systems, the consistency algorithms must handle multiple writers to a data item, and are subject to the most stringent limits on CPU and storage overheads. <p> We plan to look more into leveraging the techniques in database consistency algorithms to improve the performance of Web consistency protocols. Of the four contexts, the cache consistency problem of network and distributed file systems <ref> [19, 13, 18] </ref> is most similar to the Web consistency problem. The main differences are that the Web is orders of magnitudes bigger than any distributed file system, and the systems participating in the Web are heterogeneous, use different operating systems and belong to different organizations. <p> Despite the differences, there are similarities in the solutions. For example, the TTL approach is very similar to the NFS protocol for cache consistency <ref> [19] </ref>, the polling-every-time approach is similar to what is adopted in the Sprite file systems (clients contact the server on every file open/close) [18], and invalidations are essentially call-backs in AFS [13]. The unique challenge in the Web is to scale these protocols to the size of the Internet.
Reference: [20] <author> D. Wessels. </author> <title> Intelligent caching for the world-wide web objects. </title> <booktitle> In Proceedings of INET-95, </booktitle> <year> 1995. </year>
Reference-contexts: However, the results in [21] relies on the existence of a hierarchical caching structure, which significantly reduces the overhead for invalidation. Unfortunately, hierarchical caches are not yet widely present in the Internet. Thus, we focus on invalidation in the absence of caching hierarchies. Another study <ref> [20] </ref> describes a light-weight caching server that employs both adaptive TTL and invalidation for cache consistency. However, the paper focuses on comparing the performance differences between the light-weight server and the CERN proxy server, and does not compare the consistency approaches. <p> Popular browsers such as Netscape and the Internet Explorer all offer some form of client caching. CERN proxy [17] offers caching for all its clients, using TTL as the consistency mechanism. We choose Harvest due to its source code availability and good performance <ref> [20] </ref>. Cache consistency problems exist in any system that uses some form of cache to speed up accesses.

References-found: 20

