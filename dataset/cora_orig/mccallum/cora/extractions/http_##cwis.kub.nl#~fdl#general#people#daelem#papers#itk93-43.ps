URL: http://cwis.kub.nl/~fdl/general/people/daelem/papers/itk93-43.ps
Refering-URL: http://ilk.kub.nl/~antalb/pubs-time.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Learnability and Markedness in Data-Driven Acquisition of Stress  
Author: Walter Daelemans Steven Gillis Gert Durieux Antal van den Bosch 
Date: 43, May 1993  
Pubnum: ITK Research Report No.  
Abstract: This paper investigates the computational grounding of learning theories developed within a metrical phonology approach to stress assignment. In current research, the Principles and Parameters approach to learning stress is pervasive. We point out some inherent problems associated with this approach in learning the stress system of a particular language by setting parameters (the case of Dutch), which is shown to be an inherently noisy problem. The paper focuses on two aspects of this problem: we empirically examine the effect of input encodings on learnability, and we investigate the possibility of a data-oriented approach as an alternative to the principles and parameters approach. We show that data-oriented similarity-based machine learning techniques like Backpropagation Learning, Instance-Based Learning and Analogical Modeling working on phonemic input encodings (i) are able to learn metrical phonology abstractions based on concepts like syllable weight, (ii) that their performance can be related to various degrees of markedness of metrical phenomena, and (iii) that in addition, they are able to extract generalizations which cannot be expressed within the metrical framework without recourse to lexical marking. We also provide a quantitative comparison of the performance of the three algorithms investigated.
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D., Kibler, D. & Albert, M. </author> <year> 1991. </year> <title> Instance-Based Learning Algorithms. </title> <booktitle> Machine Learning 6, </booktitle> <pages> 37-66. </pages>
Reference-contexts: All simulations were run 50 epochs. Before training, all connection weights were initialized with random floating point values between -.5 and .5. We used a learning rate of 0.2 and a momentum of 0.9. 1.5.2 Instance-Based Learning Instance-based learning <ref> (IBL, Aha et al. 1991) </ref> is a framework and methodology for incremental supervised machine learning. The distinguishing feature of IBL is the fact that no explicit abstractions are constructed on the basis of the training examples during the training phase.
Reference: <author> Chomsky, N. </author> <year> 1981. </year> <title> Principles and parameters in syntactic theory. </title> <editor> In Hornstein, N. & Ligh-foot, D. eds. </editor> <title> Explanations in linguistics: The logical problem of language acquisition. </title> <address> 22 London: </address> <publisher> Longman. </publisher> <pages> pp. 32-75. </pages>
Reference-contexts: They all approach the learning problem from the angle of the `principles and parameters' framework <ref> (Chomsky 1981) </ref>. In this approach the learner comes to the task of language learning equipped with a priori knowledge incorporated in a universal grammar that constrains him to entertain only useful generalizations.
Reference: <author> Church, K. </author> <year> 1992. </year> <title> Comment on Computational Learning Models for Metrical Phonology. </title> <editor> In: R. Levine (ed.) </editor> <booktitle> Formal Grammar: Theory and Implementation, </booktitle> <address> O.U.P., </address> <year> 1992, </year> <pages> 318-326. </pages>
Reference: <author> Daelemans, W. & van den Bosch, A. </author> <year> 1992. </year> <title> Generalization Performance of Backpropagation Learning on a Syllabification Task. </title> <editor> In: M.F.J. Drossaers and A. Nijholt (eds.) </editor> <booktitle> Connectionism And Natural Language Processing. Proceedings Third Twente Workshop On Language Technology, </booktitle> <pages> pp. 27-38. </pages>
Reference-contexts: Our approach to the problem of weighing the relative importance of features is based on the concept of Information Gain (IG, also used in learning inductive decision trees, Quinlan, 1986), and first introduced (as far as we know) in IBL in <ref> (Daelemans and Van den Bosch, 1992) </ref> in the context of a syllable segmentation task. The idea is to interpret the training set as an information source capable of generating a number of messages (the different categories) with a certain probability.
Reference: <author> Derwing, B. L. & Skousen, R. </author> <year> 1989. </year> <title> Real Time Morphology: Symbolic Rules or Analogical Networks. </title> <booktitle> Berkeley Linguistic Society 15: </booktitle> <pages> 48-62. </pages>
Reference: <author> Devijver, P.A. & Kittler, J. </author> <year> 1982. </year> <title> Pattern Recognition. A Statistical Approach. </title> <publisher> London: Prentice-Hall. </publisher>
Reference-contexts: IBL is inspired to some extent on psychological research on exemplar-based categorization (as opposed to classical and probabilistic categorization, Smith and Medin, 1981). Finally, as far as algorithms are concerned, IBL finds its inspiration in statistical pattern recognition, especially the rich research tradition on the nearest-neighbour decision rule <ref> (see e.g. Devijver and Kittler, 1982, for an overview) </ref>. The operation of the basic algorithm is quite simple: for each pattern to be assigned a category (test item), it is checked whether this pattern has been encountered in the training set earlier.
Reference: <author> Dresher, E. </author> <year> 1992. </year> <title> A Learning Model for a Parametric Theory in Phonology. </title> <editor> In: R. Levine (ed.) </editor> <booktitle> Formal Grammar: Theory and Implementation, </booktitle> <address> O.U.P., </address> <year> 1992, </year> <pages> 290-317. </pages>
Reference: <author> Dresher, E. & Kaye, J. </author> <year> 1990. </year> <title> A computational learning model for metrical phonology. </title> <journal> Cognition 34: </journal> <pages> 137-195. </pages>
Reference-contexts: Starting from a finite set of parameters, each with a finite set of values, the number of possible grammars developed by the learner is restricted to a finite set. It is assumed that universal grammar specifies a number of parameters relevant to the metrical domain <ref> (see Dresher & Kaye 1990) </ref>. The computational models add a learning theory to the linguistic notion of universal grammar. <p> A second goal of the present research is a qualitative comparison of the results of our data-driven approach to the constructs and insights of the metrical framework. Dresher & Kaye <ref> (1990, Dresher, 1992) </ref> explicitly mention table-lookup (storing weight strings with their associated stress string) as an uninteresting data-driven approach, mainly because in their view it is empirically inadequate as it cannot generalize to new cases. <p> The most straightforward way to present stress assignment in Dutch is by reviewing the settings of the relevant metrical parameters <ref> (see Dresher & Kaye 1990, Trommelen & Zonneveld 1989, 1990) </ref>: P1 The word-tree is strong on the [Left/Right] Right P2 Feet are [Binary/Unbounded] Binary P3 Feet are built from the [Left/Right] Right P4 Feet are strong on the [Left/Right] Left P5 Feet are quantity sensitive [Yes/No] Yes P6 Feet are quantity
Reference: <author> Gillis, S., G. Durieux, W. Daelemans, & A. van den Bosch. </author> <year> 1992. </year> <title> Exploring Artificial Learning Algorithms: Learning to Stress Dutch Simplex Words. </title> <note> Antwerp Papers in Linguistics 71. </note>
Reference: <author> Gupta, P. & Touretzky, D. </author> <year> 1991. </year> <title> Connectionist models and linguistic theory: Investigations of stress systems in language. </title> <journal> Unpublished ms. </journal>
Reference-contexts: They dedicate a specialized module to determining if there exist obvious conflicts (such as the ones illustrated above for Dutch). Eventually, a brute force learner is invoked to deal with similar input. They also indicate that the set of parameters will undoubtedly have to be extended <ref> (see also Gupta & Touretzky 1991) </ref>. But keeping the present set of parameters as sufficient, for the sake of the argument, a number of serious problems turn up when we try to analyze how a learner of Dutch might fix the values of the parameters. <p> It was found that the more marked stress patterns (in terms of exception features) are less accurately learned. Hence, metrical markedness and ease of learning also correspond on the level of individual categories of words. Our experiment has thus provided computational grounding <ref> (Gupta & Touretzky, 1991) </ref> to the application of the metrical phonology framework to Dutch stress assignment.
Reference: <author> Nyberg, E. </author> <year> 1991. </year> <title> A non-deterministic, success-driven model of parameter setting in language acquisition. </title> <type> Unpublished PhD, </type> <institution> Carnegie Mellon University. </institution>
Reference: <author> Quinlan, J. R. </author> <year> 1986. </year> <title> Induction Of Decision Trees. </title> <booktitle> Machine Learning 1: </booktitle> <pages> 81-106. </pages>
Reference-contexts: We extended the basic IBL algorithm proposed by Aha et al. (1991) with a technique for assigning a different importance to different features. Our approach to the problem of weighing the relative importance of features is based on the concept of Information Gain <ref> (IG, also used in learning inductive decision trees, Quinlan, 1986) </ref>, and first introduced (as far as we know) in IBL in (Daelemans and Van den Bosch, 1992) in the context of a syllable segmentation task.
Reference: <editor> Riesbeck, C. K. & Schank, R.S. </editor> <year> 1987. </year> <title> Inside Case-Based Reasoning. </title> <address> Hillsdale: </address> <publisher> Erlbaum. </publisher>
Reference: <author> Rumelhart, D.E., Hinton, G.E. & Williams, R.J. </author> <year> 1986. </year> <title> Learning Internal Representations By Error Propagation. </title> <editor> In D.E. Rumelhart & McClelland, J.L. (Eds.), </editor> <booktitle> Parallel Distributed Processing: Explorations Into The Microstructure Of Cognition, </booktitle> <volume> Vol. </volume> <pages> 2. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <pages> pp. 216-271. </pages>
Reference-contexts: We will see that in our version of IBL, an information-theoretic metric is used to achieve a similar result. 1.5.1 Backpropagation Of Errors We chose Backpropagation of errors (BP) <ref> (Rumelhart, Hinton & Williams, 1986) </ref> as one of our inductive techniques because it has empirically been shown to be relatively successful in learning natural language processing sub-problems (e.g., Daelemans & Van den Bosch, 1992, 4 A more detailed classification of these algorithms in the space of possible machine learning algorithms is
Reference: <author> Skousen, R. </author> <year> 1989. </year> <title> Analogical Modeling of Language. </title> <publisher> Kluwer, Dordrecht. </publisher>
Reference-contexts: The metric also shows that the encoding used does not offer many clues for the prediction of the antepenultimate stress. Word length (in number of syllables) provides most information gain here. The (rounded) information gain value is used to weigh similarity matching. 1.5.3 Analogical Modeling Analogical Modeling <ref> (Skousen 1989) </ref> is another similarity-based framework, meant to provide an alternative to rule-based linguistic descriptions as a model of actual language usage.
Reference: <author> Smith, E.E. & Medin, D.L. </author> <year> 1981. </year> <title> Categories and Concepts. </title> <address> Cambridge, MA, </address> <publisher> Harvard University Press. </publisher>
Reference-contexts: In linguistics, a similar emphasis on analogy to stored examples instead of explicit but inaccessible rules, is present in the work of, amongst others, Derwing and Skousen (1989). IBL is inspired to some extent on psychological research on exemplar-based categorization <ref> (as opposed to classical and probabilistic categorization, Smith and Medin, 1981) </ref>. Finally, as far as algorithms are concerned, IBL finds its inspiration in statistical pattern recognition, especially the rich research tradition on the nearest-neighbour decision rule (see e.g. Devijver and Kittler, 1982, for an overview).
Reference: <author> Stanfill, C. & Waltz, D.L. </author> <year> 1986. </year> <title> Toward Memory-based Reasoning. </title> <journal> Communications of the ACM 29: </journal> <pages> 1213-1228. </pages>
Reference: <author> Trommelen, M. & Zonneveld, W. </author> <year> 1989. </year> <note> Klemtoon en Metrische Fonologie. Muiderberg: Coutinho. 23 Trommelen, </note> <author> M. & Zonneveld, W. </author> <year> 1990. </year> <title> Stress In English And Dutch: A Comparison. </title> <booktitle> Dutch Working Papers in English Language and Linguistics 17. </booktitle>
Reference-contexts: For the purpose of this study, a lexicon of Dutch polysyllabic monomorphematic words was compiled (the lexicon will be described in more detail in section 1.3). We found that approximately 80% of the 4868 monomorphemes are regular according to a state-of-the-art metrical analysis <ref> (Trommelen & Zonneveld 1989, 1990) </ref>. The remaining 20% have to be dealt with in terms of idiosyncratic markings (specification of a lexical foot, exceptions to extrametricality, a combination of these two exception mechanisms, or simply a marking of the irregular pattern in the lexicon).
Reference: <author> Weijters, A. & Hoppenbrouwers, G. </author> <year> 1990. </year> <editor> Netspraak: Een Neuraal Netwerk Voor Grafeem-Foneem-Omzetting. Tabu, </editor> <volume> 20: </volume> <pages> 1-25. </pages>
Reference: <author> Weiss, S. & Kulikowski, C. </author> <year> 1991. </year> <title> Computer Systems That Learn. </title> <address> San Mateo: </address> <publisher> Morgan Kaufmann. </publisher> <pages> 24 </pages>
Reference-contexts: of the three data-driven learning algorithms on learning the stress assignment task, and studied the learning curve produced by each algorithm by presenting different training set sizes. 2.1 METHOD To be relatively certain that the performance results reported approximate the true error rate, we set up a 10-fold cross-validation experiment <ref> (10-fold CV, Weiss & Kulikowski, 1991) </ref>. In this set-up, the dataset is partitioned ten times, each time with a different 10% of the dataset as the test set, and the remaining 90% as training set. <p> We therefore get as many simulations as there are items in the dataset. This computationally very costly method has as its major advantage that it provides the best possible estimate of the true error rate of a learning algorithm <ref> (Weiss & Kulikowski 1991) </ref>. 3.2 DATA CODING The data were encoded (i) as strings of syllable weights of the last three syllables of the word (encoding-1), and (ii) using the phonemic information contained in the rhyme projections of the last three syllables (encoding-2).
References-found: 20

