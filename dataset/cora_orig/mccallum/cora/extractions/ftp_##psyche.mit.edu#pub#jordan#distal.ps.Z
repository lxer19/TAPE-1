URL: ftp://psyche.mit.edu/pub/jordan/distal.ps.Z
Refering-URL: http://www.ai.mit.edu/projects/jordan.html
Root-URL: 
Title: Forward models: Supervised learning with a distal teacher  
Author: Michael I. Jordan David E. Rumelhart 
Affiliation: Department of Brain and Cognitive Sciences Massachusetts Institute of Technology  Department of Psychology Stanford University  
Date: 16, 307-354, 1992.  
Pubnum: Cognitive Science,  
Abstract: Internal models of the environment have an important role to play in adaptive systems in general and are of particular importance for the supervised learning paradigm. In this paper we demonstrate that certain classical problems associated with the notion of the "teacher" in supervised learning can be solved by judicious use of learned internal models as components of the adaptive system. In particular, we show how supervised learning algorithms can be utilized in cases in which an unknown dynamical system intervenes between actions and desired outcomes. Our approach applies to any supervised learning algorithm that is capable of learning in multi-layer networks. *This paper is a revised version of MIT Center for Cognitive Science Occasional Paper #40. We wish to thank Michael Mozer, Andrew Barto, Robert Jacobs, Eric Loeb, and James McClelland for helpful comments on the manuscript. This project was supported in part by BRSG 2 S07 RR07047-23 awarded by the Biomedical Research Support Grant Program, Division of Research Resources, National Institutes of Health, by a grant from ATR Auditory and Visual Perception Research Laboratories, by a grant from Siemens Corporation, by a grant from the Human Frontier Science Program, and by grant N00014-90-J-1942 awarded by the Office of Naval Research. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Atkeson, C. G., & Reinkensmeyer, D. J. </author> <year> (1988). </year> <title> Using associative content-addressable memories to control robots. </title> <booktitle> IEEE Conference on Decision and Control. </booktitle> <address> San Francisco, CA. </address>
Reference: <author> Barto, A. G. </author> <year> (1989). </year> <title> From chemotaxis to cooperativity: Abstract exercises in neuronal learning strategies. </title> <editor> In R. M. Durbin, R. C. Maill, & G. J. Mitchison (Eds.), </editor> <booktitle> The computing neurone. </booktitle> <address> Reading, MA: </address> <publisher> Addison-Wesley Publishers. </publisher>
Reference: <author> Barto, A. G., Sutton, R. S., & Anderson, C. W. </author> <year> (1983). </year> <title> Neuronlike adaptive elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> SMC-13, </volume> <pages> 834-846. </pages>
Reference-contexts: This problem is generally posed as an avoidance control problem in which the only corrective information provided by the environment is a signal to indicate that failure has occured <ref> (Barto, Sutton, & Anderson, 1983) </ref>. The delay between actions (forces applied to the cart) and the failure signal is unknown and indeed can be arbitrarily large.
Reference: <author> Becker, S. & Hinton, G. E. </author> <year> (1989). </year> <title> Spatial coherence as an internal teacher for a neural network. </title> <type> (Tech. Rep. </type> <institution> CRG-TR-89-7). Toronto: University of Toronto. </institution>
Reference: <author> Carlson, A. B. </author> <year> (1986). </year> <title> Communication Systems. </title> <address> New York: </address> <publisher> McGraw-Hill. </publisher>
Reference-contexts: For example, if the environment is viewed as a communications channel over which a message is to be transmitted, then it may be desirable to undo the distorting effects of the environment by placing it in series with an inverse model <ref> (Carlson, 1986) </ref>. A second example, shown in Figure 6, arises in control system design.
Reference: <author> Craig, J. J. </author> <year> (1986). </year> <title> Introduction to Robotics. </title> <address> Reading, MA: </address> <publisher> Addison-Wesley Publishers. 45 Gelfand, </publisher> <editor> I. M. & Fomin, S. V. </editor> <year> (1963). </year> <title> Calculus of variations. </title> <address> Englewood Cliffs, N. </address>
Reference-contexts: To obtain an analog of the next-state function in Equation 1, the following differential equation can be derived for the angular motion of the links, using standard Newtonian or Lagrangian dynamical formulations <ref> (Craig, 1986) </ref>: M (q) q + C (q; _ q) _ q + G (q) = t ; (22) where M (q) is an inertia matrix, C (q; _ q) is a matrix of Coriolis and centripetal terms, and G (q) is the vector of torque due to gravity. <p> In particular, to obtain a "next-state function," we rewrite Equation 22 by solving for the accelerations to yield: q = M 1 (q)[t C (q; _ q) _ q G (q)]; (23) where the existence of M 1 (q) is always assured <ref> (Craig, 1986) </ref>. Equation 23 expresses the state-dependent relationship between torques and accelerations at each moment in time: Given the state variables q (t) and _ q (t), and given the torque t (t), the acceleration q (t) can be computed by substitution in Equation 23.
Reference: <editor> J.: </editor> <publisher> Prentice-Hall. </publisher>
Reference: <author> Goodwin, G. C. & Sin, K. S. </author> <year> (1984). </year> <title> Adaptive filtering prediction and control. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall. </publisher>
Reference: <author> Grossberg, S. </author> <year> (1987). </year> <title> Competitive learning: From interactive activation to adaptive resonance. </title> <journal> Cognitive Science, </journal> <volume> 11, </volume> <pages> 23-63. </pages>
Reference: <author> Hinton, G. E. & Sejnowski, T. J. </author> <year> (1986). </year> <title> Learning and relearning in Boltzmann machines. </title> <editor> In D. E. Rumelhart & J. L. McClelland (Eds.), </editor> <booktitle> Parallel distributed processing: </booktitle> <volume> Volume 1, </volume> <pages> 282-317. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: This formulation is based on variational calculus and is closely allied with methods in optimal control theory (Kirk, 1970; LeCun, 1987). The algorithm that results is a form of "backpropagation-in-time" <ref> (Rumelhart, Hinton, & Williams, 1986) </ref> in a recurrent network that incorporates a learned forward model.
Reference: <author> Hogan, N. </author> <year> (1984). </year> <title> An organising principle for a class of voluntary movements. </title> <journal> Journal of Neuroscience, </journal> <volume> 4, </volume> <pages> 2745-2754. </pages>
Reference: <author> Jordan, M. I. </author> <year> (1983). </year> <title> Mental practice. Unpublished dissertation proposal, Center for Human Information Processing, </title> <institution> University of California, </institution> <address> San Diego. </address>
Reference-contexts: Any supervised learning algorithm can be used as long as it is capable of learning a mapping across a composite network that includes a previously trained subnetwork; in particular, Boltzmann learning is applicable <ref> (Jordan, 1983) </ref>. We begin by introducing a useful shorthand for describing backpropagation in layered networks. A layered network can be described as a parameterized mapping from an input vector x to an output vector y: y = (x; w); (4) where w is a vector of parameters (weights).
Reference: <author> Jordan, M. I. </author> <year> (1986). </year> <title> Serial order: A parallel, distributed processing approach. </title> <type> (Technical Report 8604). </type> <institution> La Jolla, CA: University of California, </institution> <address> San Diego. </address>
Reference-contexts: As in the local optimization case, the equations for computing the gradient 10 Alternatively, Figure 9 can be thought of as a special case of Figure 10 in which the backprop agated error signals stop at the state units <ref> (cf. Jordan, 1986) </ref>. 19 involve the multiplication of the performance error y fl y by a series of transpose Jacobian matrices, several of which are unknown a priori.
Reference: <author> Jordan, M. I. </author> <year> (1988). </year> <title> Supervised learning and systems with excess degress of freedom. </title> <type> (COINS Tech. Rep. 88-27). </type> <institution> Amherst, MA: University of Massachusetts, Computer and Information Sciences. </institution>
Reference: <author> Jordan, M. I., & Rosenbaum, D. A. </author> <year> (1989). </year> <title> Action. </title> <editor> In M. I. Posner (Ed.), </editor> <booktitle> Foundations of Cognitive Science. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: There are two general approaches to learning inverse models using supervised learning algorithms: the distal learning approach presented above and an alternative approach that we refer to as "direct inverse modeling" <ref> (cf. Jordan & Rosenbaum, 1989) </ref>. We begin by describing the latter approach. Direct inverse modeling Direct inverse modeling treats the problem of learning an inverse model as a classical supervised learning problem (Widrow & Stearns, 1985).
Reference: <author> Jordan, M.I. </author> <year> (1990). </year> <title> Motor learning and the degrees of freedom problem. </title> <editor> In M. Jeannerod (Ed.), </editor> <title> Attention and Performance, XIII. </title> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. </publisher>
Reference: <author> Jordan, M. I., & Jacobs, R. A. </author> <year> (1990). </year> <title> Learning to control an unstable system with forward modeling. </title> <editor> In D. Touretzky (Ed.), </editor> <booktitle> Advances in Neural Information Processing Systems 2. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kawato, M. </author> <year> (1990). </year> <title> Computational schemes and neural network models for formation and control of multijoint arm trajectory. </title> <editor> In W. T. Miller, III, R. S. Sutton, & P. J. Werbos (Eds.), </editor> <title> Neural Networks for Control. </title> <publisher> Cambridge: MIT Press. </publisher>
Reference: <author> Kawato, M., Furukawa, K., & Suzuki, R. </author> <year> (1987). </year> <title> A hierarchical neural-network model for control and learning of voluntary movement. </title> <journal> Biological Cybernetics, </journal> <volume> 57, </volume> <pages> 169-185. </pages>
Reference-contexts: This approach is equivalent to using a simple fixed-gain proportional-derivative (PD) feedback controller to stabilize the system along a set of reference trajectories and thereby generate training data. 14 Such use of an auxiliary feedback controller is similar to its use in the feedback-error learning <ref> (Kawato, et al., 1987) </ref> and direct inverse modeling (Atkeson & Reinkensmeyer, 1988; Miller, 1987) approaches.
Reference: <author> Kirk, D. E. </author> <year> (1970). </year> <title> Optimal control theory. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall. </publisher>
Reference: <author> Kohonen, T. </author> <year> (1982). </year> <title> Self-organized formation of topologically correct feature maps. </title> <journal> Biological Cybernetics, </journal> <volume> 43, </volume> <pages> 56-69. </pages> <note> 46 Kuperstein, </note> <author> M. </author> <year> (1988). </year> <title> Neural model of adaptive hand-eye coordination for single postures. </title> <journal> Science, </journal> <volume> 239, </volume> <pages> 1308-1311. </pages>
Reference: <author> LeCun, Y. </author> <year> (1985). </year> <title> A learning scheme for asymmetric threshold networks. </title> <booktitle> Proceedings of Cognitiva 85. </booktitle> <address> Paris, France. </address>
Reference: <author> LeCun, Y. </author> <year> (1987). </year> <institution> Modeles connexionnistes de l'apprentissage. Unpublished doctoral dissertation, Universite de Paris, VI. </institution>
Reference-contexts: base the learning rule on the stochastic gradient of J, that is, the gradient evaluated along a particular sample trajectory y ff : J ff = 2 n=1 ff [n] y ff [n]) T (y fl The gradient of this cost functional can be obtained using the calculus of variations <ref> (see also LeCun, 1987, Narendra & Parthasarathy, 1990) </ref>.
Reference: <author> Linsker, R. </author> <year> (1988). </year> <title> Self-organization in a perceptual network. </title> <journal> Computer, </journal> <volume> 21, </volume> <pages> 105-117. </pages>
Reference: <author> Ljung, L. & Soderstrom, T. </author> <year> (1986). </year> <title> Theory and practice of recursive identification. </title> <publisher> Cambridge: MIT Press. </publisher>
Reference-contexts: Suppose that the forward model has been trained previously and is a perfect model of the environment; that is, the predicted sensation equals the actual sensation for all actions and all states. 2 In the engineering literature, this learning process is referred to as "system identification" <ref> (Ljung & Soderstrom, 1986) </ref>. 6 p [n 1] to predicted sensations ^ y [n] in the context of a given state vector.
Reference: <author> Miller, W. T. </author> <year> (1987). </year> <title> Sensor-based control of robotic manipulators using a general learning algorithm. </title> <journal> IEEE Journal of Robotics and Automation, </journal> <volume> 3, </volume> <pages> 157-165. </pages>
Reference: <author> Mozer, M. C. & Bachrach, J. </author> <year> (1990). </year> <title> Discovering the structure of a reactive environment by exploration. </title> <editor> In D. Touretzky (Ed.), </editor> <booktitle> Advances in Neural Information Processing Systems 2. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Munro, P. </author> <year> (1987). </year> <title> A dual back-propagation scheme for scalar reward learning. </title> <booktitle> Proceedings of the Ninth Annual Conference of the Cognitive Science Society. </booktitle> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. </publisher>
Reference: <author> Narendra, K. S. & Parthasarathy, K. </author> <year> (1990). </year> <title> Identification and control of dynamical systems using neural networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 1, </volume> <pages> 4-27. </pages>
Reference: <author> Nguyen, D. & Widrow, B. </author> <year> (1989). </year> <title> The truck backer-upper: An example of self-learning in neural networks. </title> <booktitle> In: Proceedings of the International Joint Conference on Neural Networks, </booktitle> <volume> 2, </volume> <pages> 357-363. </pages> <address> Piscataway, NJ: </address> <publisher> IEEE Press. </publisher>
Reference: <author> Parker, D. </author> <year> (1985). </year> <title> Learning-logic. </title> <type> (Tech. Rep. </type> <institution> TR-47). Cambridge, MA: MIT Sloan School of Management. </institution>
Reference: <author> Robinson, A. J. & Fallside, F. </author> <year> (1989). </year> <title> Dynamic reinforcement driven error propagation networks with application to game playing. </title> <booktitle> Proceedings of Neural Information Systems. American Institute of Physics. </booktitle>
Reference: <author> Rosenblatt, F. </author> <year> (1962). </year> <title> Principles of neurodynamics. </title> <address> New York: </address> <publisher> Spartan. </publisher>
Reference: <author> Rumelhart, D. E. </author> <year> (1986). </year> <title> Learning sensorimotor programs in parallel distributed processing systems. US-Japan Joint Seminar on Competition and Cooperation in Neural Nets, II. </title> <note> Unpublished presentation. 47 Rumelhart, </note> <author> D. E., Hinton, G. E., Williams, R. J. </author> <year> (1986). </year> <title> Learning internal rep-resentations by error propagation. </title> <editor> In D. E. Rumelhart & J. L. McClelland (Eds.), </editor> <booktitle> Parallel distributed processing: </booktitle> <volume> Volume 1, </volume> <pages> 318-363. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: It is generally desired to minimize this cost. Backpropagation is an algorithm for computing gradients of the cost functional. The details of the algorithm can be found elsewhere <ref> (e.g., Rumelhart, et al., 1986) </ref>; our intention here is to develop a simple notation that hides the details. <p> This formulation is based on variational calculus and is closely allied with methods in optimal control theory (Kirk, 1970; LeCun, 1987). The algorithm that results is a form of "backpropagation-in-time" <ref> (Rumelhart, Hinton, & Williams, 1986) </ref> in a recurrent network that incorporates a learned forward model.
Reference: <author> Rumelhart, D. E., Smolensky, P., McClelland, J. L. & Hinton, G. E. </author> <year> (1986). </year> <title> Schemata and sequential thought processes in PDP models. </title> <editor> In D. E. Rumelhart & J. L. McClelland (Eds.), </editor> <booktitle> Parallel distributed processing: </booktitle> <volume> Volume 2, </volume> <pages> 7-57. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: It is generally desired to minimize this cost. Backpropagation is an algorithm for computing gradients of the cost functional. The details of the algorithm can be found elsewhere <ref> (e.g., Rumelhart, et al., 1986) </ref>; our intention here is to develop a simple notation that hides the details. <p> This formulation is based on variational calculus and is closely allied with methods in optimal control theory (Kirk, 1970; LeCun, 1987). The algorithm that results is a form of "backpropagation-in-time" <ref> (Rumelhart, Hinton, & Williams, 1986) </ref> in a recurrent network that incorporates a learned forward model.
Reference: <author> Rumelhart, D. E. & Zipser, D. </author> <year> (1986). </year> <title> Feature discovery by competitive learning. </title>
Reference-contexts: It is generally desired to minimize this cost. Backpropagation is an algorithm for computing gradients of the cost functional. The details of the algorithm can be found elsewhere <ref> (e.g., Rumelhart, et al., 1986) </ref>; our intention here is to develop a simple notation that hides the details. <p> This formulation is based on variational calculus and is closely allied with methods in optimal control theory (Kirk, 1970; LeCun, 1987). The algorithm that results is a form of "backpropagation-in-time" <ref> (Rumelhart, Hinton, & Williams, 1986) </ref> in a recurrent network that incorporates a learned forward model.
Reference: <editor> In D. E. Rumelhart & J. L. McClelland (Eds.), </editor> <booktitle> Parallel distributed processing: </booktitle> <volume> Volume 1, </volume> <pages> 151-193. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Schmidhuber, J. H. </author> <year> (1990). </year> <title> An on-line algorithm for dynamic reinforcement learning and planning in reactive environments. </title> <booktitle> In: Proceedings of the International Joint Conference on Neural Networks, </booktitle> <volume> 2, </volume> <pages> 253-258. </pages> <address> Piscataway, NJ: </address> <publisher> IEEE Press. </publisher>
Reference: <author> Sutton, R. S. </author> <year> (1984). </year> <title> Temporal credit assignment in reinforcement learning. </title> <type> (COINS Tech. Rep. 84-02). </type> <institution> Amherst, MA: University of Massachusetts, Computer and Information Sciences. </institution>
Reference: <author> Sutton, R. S. </author> <year> (1988). </year> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 9-44. </pages>
Reference-contexts: See also Nguyen & Widrow (1989) for an application to a kinematic problem. 40 The approach adopted by Jordan and Jacobs involves learning a forward model whose output is an integrated quantity|an estimate of the inverse of the time until failure. This estimate is learned using temporal difference techniques <ref> (Sutton, 1988) </ref>. At time steps on which failure occurs, the target value for the forward model is unity: e (t) = 1 ^z (t); where ^z (t) is the output of the forward model, and e (t) is the error term used to change the weights.
Reference: <author> Sutton, R. S. </author> <year> (1990). </year> <title> Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> Proceedings of the Seventh International Conference on Machine Learning. </booktitle>
Reference: <author> Werbos, P. </author> <year> (1974). </year> <title> Beyond regression: New tools for prediction and analysis in the behavioral sciences. Unpublished doctoral dissertation, </title> <publisher> Harvard University. </publisher>
Reference: <author> Werbos, P. </author> <year> (1987). </year> <title> Building and understanding adaptive systems: A statistical/numerical approach to factory automation and brain research. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 17, </volume> <pages> 7-20. </pages>
Reference-contexts: In the current section we present preliminary results that aim to broaden the scope of the distal learning approach to address problems in which the maximum delay is not known <ref> (see also Werbos, 1987) </ref>.
Reference: <author> Widrow, B. & Hoff, M. E. </author> <year> (1960). </year> <title> Adaptive switching circuits. </title> <booktitle> Institute of Radio Engineers, Western Electronic Show and Convention, Convention Record, </booktitle> <volume> Part 4, </volume> <pages> 96-104. </pages>
Reference: <author> Widrow, B., & Stearns, S. D. </author> <year> (1985). </year> <title> Adaptive signal processing. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall. </publisher> <pages> 48 </pages>
Reference-contexts: Jordan & Rosenbaum, 1989). We begin by describing the latter approach. Direct inverse modeling Direct inverse modeling treats the problem of learning an inverse model as a classical supervised learning problem <ref> (Widrow & Stearns, 1985) </ref>. As shown in Figure 7, the idea is to observe the input/output behavior of the environment and to train an inverse model directly by reversing the roles of the inputs and outputs.
References-found: 45

