URL: ftp://ftp.cse.ucsc.edu/pub/ml/trac_expert.ml95.ps
Refering-URL: http://www.cse.ucsc.edu/~mark/ftp-ml-root/ExpertBibDir/expbib.html
Root-URL: http://www.cse.ucsc.edu
Email: mark@cs.ucsc.edu, manfred@cs.ucsc.edu  
Title: Tracking the Best Expert  
Author: Mark Herbster and Manfred Warmuth 
Address: Santa Cruz  
Affiliation: Computer and Information Sciences University of California,  
Abstract: We generalize the recent worst-case loss bounds for on-line algorithms where the additional loss of the algorithm on the whole sequence of examples over the loss of the best expert is bounded. The generalization allows the sequence to be partitioned into segments and the goal is to bound the additional loss of the algorithm over the sum of the losses of the best experts of each segment. This is to model situations in which the examples change and different experts are best for certain segments of the sequence of examples. In the single expert case the additional loss is proportional to log n, where n is the number of experts and the constant of proportionality depends on the loss function. When the number of segments is at most k + 1 and the sequence of length ` then we can bound the additional loss of our algorithm over the best partitioning by O(k log n + k log(`=k)). Note that it takes the same order of bits to denote the sequence of experts and the boundaries of the segments. When the loss per trial is bounded by one then we obtain additional loss bounds that are independent of the length of the sequence. The bound becomes O(k log n + k log(L=k)), where L is the loss of the best partition into k + 1 segments. Our algorithms for tracking the best expert are simple adaptations of Vovk's original algorithm for the single best expert case. These algorithms keep one weight per expert and spend O(1) time per weight in each trial.
Abstract-found: 1
Intro-found: 1
Reference: [CBFH + 94] <author> N. Cesa-Bianchi, Y. Freund, D. P. Helmbold, D. Haussler, R. E. Schapire, and M. K. Warmuth. </author> <title> How to use expert advice. </title> <type> Technical Report UCSC-CRL-94-33, </type> <institution> Univ. of Calif. Computer Research Lab, </institution> <address> Santa Cruz, CA, </address> <year> 1994. </year> <note> An extended abstract appeared in STOC '93. </note>
Reference-contexts: The class of loss function contains essentially all common loss functions except for the absolute loss and the discrete loss (counting prediction mistakes) which are treated as special cases <ref> [LW94, Vov90, CBFH + 94] </ref>. For example if the loss function is the square (or relative entropy) loss, then c L = 1 2 (or c L = 1, respectively). <p> This gives us Equation (5.9) and we are done. 2 Note above tuning requires knowledge of upper bounds L and k. Strategies for generating unknown parameters are given in <ref> [CBFH + 94] </ref>. 6 EXPERIMENTAL RESULTS In this section we discuss some experimental results on artificial data. These experiments are mainly meant to provide a visualization of how our algorithms track the best expert and should not be seen as empirical evidence of the practical usefulness of the algorithms.
Reference: [DMW88] <author> A. DeSantis, G. Markowsky, and M. N. Wegman. </author> <title> Learning probabilistic prediction functions. </title> <booktitle> In Proc. 29th Annu. IEEE Sympos. Found. Comput. Sci., </booktitle> <pages> pages 110-119. </pages> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1988. </year>
Reference-contexts: the special case when the outcomes lie in f0; 1g and the relative entropy loss simplifies to the standard log loss, it is easily seen that y t = 1 P n i=1 w t;i x t;i can be used to witness that the log loss function is (1; 1)-realizable <ref> [DMW88] </ref>. Otherwise we Initialization: Set the weights to initial values w s 1;1 = : : : = w s Parameters: 0 &lt; c; and 0 ff 1. Prediction: Let v t;i = w s t;i =W t , where W t = P n t;i .
Reference: [HKW94] <author> D. Haussler, J. Kivinen, and M. K. Warmuth. </author> <title> Tight worst-case loss bounds for predicting with expert advice. </title> <type> Technical Report UCSC-CRL-94-36, </type> <institution> University of California, Santa Cruz, Computer Research Laboratory, </institution> <month> November </month> <year> 1994. </year> <note> An extended abstract appeared in Eurocolt 1993. </note>
Reference-contexts: It simply keeps one weight per expert representing the belief in the expert's prediction and then decreases the weight as a function of the loss of the expert. Previous work of Vovk [Vov90] and others <ref> [HKW94] </ref> has produced an algorithm with an upper bound on the additional loss of the algorithm over the best expert of the form c L ln n for a large class of loss functions, where c L is a constant which only depends on the loss function L and n is <p> Our methods work for the same general class of loss functions that Vovk's algorithm can handle, for which there are tight bounds on the additional loss <ref> [HKW94] </ref> for the non-shifting case. This Fixed-share Algorithm essentially obtains the same additional loss of O (c L k (log n + log ` k )) as the sketched algorithm that would use Vovk's original algorithm with exponentially many partition-experts. <p> L ent (p; q) = p ln q + (1 p) ln 1q (c = 1; = 1) We cite this without proof from Haussler, et al. [HKW94, Example 3.2, Lemma 3.10, Theorem 4.1, Example 4.2 and Example 4.3]. General conditions for (c; )-realizability are given in <ref> [Vov90, HKW94] </ref>. Most common loss functions are (c; 1=c)-realizable where c depends on the loss function. The most notable exception is the absolute loss which is treated separately in the full paper. In some cases a straightforward prediction satisfying Condition (3.2) may exist. <p> As a loss function we used the square loss, because of its widespread use and because the task of tuning the learning rate for this loss function is simple (the best choice is = 2 <ref> [Vov90, HKW94] </ref>). We considered a sequence of 800 trials with Algorithm four distinct segments, beginning at trial 1, 201, 401, and 601. On each trial the outcome (y t ) was 0. The prediction tuple (x t ) contained the predictions of 64 experts.
Reference: [KW95] <author> J. Kivinen and M. K. Warmuth. </author> <title> Exponentiated gradient versus gradient descent for linear predictors. </title> <note> In To appear in the proceedings of STOC 95., </note> <year> 1995. </year>
Reference-contexts: Examples are algorithms for learning k-literal disjunction over n variables [Lit88, Lit89] and algorithms whose additional loss bound over the loss of the best linear combination of experts is bounded <ref> [KW95] </ref>.
Reference: [Lit88] <author> N. Littlestone. </author> <title> Learning when irrelevant attributes abound: A new linear-threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 285-318, </pages> <year> 1988. </year>
Reference-contexts: Besides Vovk's algorithm and the Weighted Majority Algorithm [LW94], which are the basis of this work, a number of such algorithms have been developed. Examples are algorithms for learning k-literal disjunction over n variables <ref> [Lit88, Lit89] </ref> and algorithms whose additional loss bound over the loss of the best linear combination of experts is bounded [KW95].
Reference: [Lit89] <author> N. Littlestone. </author> <title> Mistake Bounds and Logarithmic Linear-threshold Learning Algorithms. </title> <type> PhD thesis, Technical Report UCSC-CRL-89-11, </type> <institution> University of California Santa Cruz, </institution> <year> 1989. </year>
Reference-contexts: Besides Vovk's algorithm and the Weighted Majority Algorithm [LW94], which are the basis of this work, a number of such algorithms have been developed. Examples are algorithms for learning k-literal disjunction over n variables <ref> [Lit88, Lit89] </ref> and algorithms whose additional loss bound over the loss of the best linear combination of experts is bounded [KW95].
Reference: [LW94] <author> N. Littlestone and M. K. Warmuth. </author> <title> The weighted majority algorithm. </title> <journal> Information and Computation, </journal> <volume> 108(2) </volume> <pages> 212-261, </pages> <year> 1994. </year>
Reference-contexts: The class of loss function contains essentially all common loss functions except for the absolute loss and the discrete loss (counting prediction mistakes) which are treated as special cases <ref> [LW94, Vov90, CBFH + 94] </ref>. For example if the loss function is the square (or relative entropy) loss, then c L = 1 2 (or c L = 1, respectively). <p> For example if the loss function is the square (or relative entropy) loss, then c L = 1 2 (or c L = 1, respectively). In this paper we consider a modification of the above goal introduced by Littlestone and Warmuth <ref> [LW94] </ref> in which the sequence is subdivided into at most k + 1 arbitrary segments. Each segment has a best expert. The sequences of segments and its associated sequence of best experts is called a partitioning. The modified goal is to perform well relative to the best partitioning. <p> Littlestone and Warmuth <ref> [LW94] </ref> solve this problem for their WML algorithm by placing a cap on how small the relative weight of each expert may become. Thus any weight may recover as it is always no smaller than a fixed fraction of the total weight of all other experts. <p> Thus although the algorithms are trivial, proving the additional loss bounds takes some care. In the full paper we also apply our methods to the absolute and the discrete loss and obtain improved additional loss bounds over the ones obtained with WML <ref> [LW94] </ref>. We believe that our technique constitutes a practical method for tracking the best expert with provable worst-case additional loss bounds. The essential ingredient for our success seems to be an algorithm with a multiplicative weight update whose loss bound grows logarithmically with the dimension of the problem. <p> The essential ingredient for our success seems to be an algorithm with a multiplicative weight update whose loss bound grows logarithmically with the dimension of the problem. Besides Vovk's algorithm and the Weighted Majority Algorithm <ref> [LW94] </ref>, which are the basis of this work, a number of such algorithms have been developed. Examples are algorithms for learning k-literal disjunction over n variables [Lit88, Lit89] and algorithms whose additional loss bound over the loss of the best linear combination of experts is bounded [KW95].
Reference: [Vov90] <author> V. Vovk. </author> <title> Aggregating strategies. </title> <booktitle> In Proc. 3rd Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 371-383. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: The master algorithm that combines the experts predictions does not need to know the particular problem domain. It simply keeps one weight per expert representing the belief in the expert's prediction and then decreases the weight as a function of the loss of the expert. Previous work of Vovk <ref> [Vov90] </ref> and others [HKW94] has produced an algorithm with an upper bound on the additional loss of the algorithm over the best expert of the form c L ln n for a large class of loss functions, where c L is a constant which only depends on the loss function L <p> The class of loss function contains essentially all common loss functions except for the absolute loss and the discrete loss (counting prediction mistakes) which are treated as special cases <ref> [LW94, Vov90, CBFH + 94] </ref>. For example if the loss function is the square (or relative entropy) loss, then c L = 1 2 (or c L = 1, respectively). <p> Each partition-expert represents a single partitioning of the trial sequence, and predicts on each trial as the expert associated with the segment which contains the current trial. Thus using Vovk's algorithm <ref> [Vov90] </ref> we obtain a bound of c L ln i=0 i n (n 1) i = O (c L k (log n + log ` the additional loss over the best partitioning. <p> L ent (p; q) = p ln q + (1 p) ln 1q (c = 1; = 1) We cite this without proof from Haussler, et al. [HKW94, Example 3.2, Lemma 3.10, Theorem 4.1, Example 4.2 and Example 4.3]. General conditions for (c; )-realizability are given in <ref> [Vov90, HKW94] </ref>. Most common loss functions are (c; 1=c)-realizable where c depends on the loss function. The most notable exception is the absolute loss which is treated separately in the full paper. In some cases a straightforward prediction satisfying Condition (3.2) may exist. <p> As a loss function we used the square loss, because of its widespread use and because the task of tuning the learning rate for this loss function is simple (the best choice is = 2 <ref> [Vov90, HKW94] </ref>). We considered a sequence of 800 trials with Algorithm four distinct segments, beginning at trial 1, 201, 401, and 601. On each trial the outcome (y t ) was 0. The prediction tuple (x t ) contained the predictions of 64 experts.
References-found: 8

