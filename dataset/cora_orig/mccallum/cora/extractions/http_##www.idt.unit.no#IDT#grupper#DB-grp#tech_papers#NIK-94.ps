URL: http://www.idt.unit.no/IDT/grupper/DB-grp/tech_papers/NIK-94.ps
Refering-URL: http://www.idt.unit.no/IDT/grupper/DB-grp/tech_papers/tech_papers.html
Root-URL: 
Email: frhj,rogerg@idt.unit.no  
Title: Digital Video Archives  
Author: Rune Hjelsvold and Roger Midtstraum 
Date: June 17, 1994  
Affiliation: Department of Computer Systems and Telematics Norwegian Institute of Technology  
Abstract: In this paper we discuss the role that a video data base may play in a digital video archive and some of the challenges that video data bases are facing. Problems related to large-scale storage and delivery of video data are briefly discussed while a modelling and querying aspecs are treated more thoroughly. We propose a generic data model that captures video data structures and that supports thematic indexing of a video stream. A video archive prototype based on a simplified version of the data model is presented and some of the experiences made are discussed. 
Abstract-found: 1
Intro-found: 1
Reference: [A + 93] <editor> Allen et al. VCTV: </editor> <title> A Video-On-Demand Market Test. </title> <journal> AT&T Technical Journal, </journal> <month> Jan-uary/February </month> <year> 1993. </year>
Reference-contexts: Most current digital video applications do not really take advantage of a digital video stream, they are more like digitised versions of traditional video cassette recorders - VCRs. One example of this is the Video-On-Demand service - VOD <ref> [A + 93] </ref>, that gives the user on-line access to a digital library of movies. The service allows the users to select movies by titles, genre, actors, or production year.
Reference: [DSP91] <author> G. Davenport, T.A. Smith, and N. Pincever. </author> <title> Cinematic Primitives for Multimedia. </title> <journal> IEEE Computer Graphics & Applications, </journal> <month> July </month> <year> 1991. </year>
Reference-contexts: The concept of StructuralComponent is specialised into the CompoundUnit, Sequence, Scene and Shot subclasses and a hierarchical relationship is defined between the different subclasses. The semantics of the subclasses are best explained bottom-up starting with the Shot which can be considered as the basic structural unit. As defined in <ref> [DSP91] </ref> a Shot consists of one or more frames generated and recorded contiguously, representing a continuous action in time and space. Shots which are related in time and space are assembled in a Scene and a number of scenes which together give a meaning are put together in a Sequence.
Reference: [EN94] <author> R. Elmasri and S.B. Navathe. </author> <title> Fundamentals of Database Systems. </title> <publisher> The Ben-jamin/Cummings Publishing Company, </publisher> <address> 2nd edition, </address> <year> 1994. </year>
Reference-contexts: In our generic model, a segmentation approach is used to define the video document structure which can provide well defined levels of abstraction. The generic data model is given in Figure 2 using the enhanced-ER notation from <ref> [EN94] </ref>. The intention has been to make the following possible within the same framework: 3 1. Structuring of video material, 2. free annotations of video material, and 3. sharing and reuse of video data. Video data is stored in a video database as contiguous groups of frames called StoredVideoSegments.
Reference: [Gal91] <author> D. Le Gall. </author> <title> MPEG: A Video Compression Standard for Multimedia Applications. </title> <journal> Communications of the ACM, </journal> <volume> 32(4), </volume> <year> 1991. </year>
Reference-contexts: frames per second (fps) while the frame rate for the American NTSC standard is 30 fps. 3 Several digital video formats exist ranging from low quality formats such as the Quarter-CIF (QCIF) the one of the two video formats used in teleconferencing [Lio91] to high-definition television (HDTV) standards offering high-qualityvideo <ref> [Gal91] </ref>. 4 JPEG, the ISO standard for still image compression [Wal91], takes advantage of spatial redundancy. 5 MPEG, the ISO standard for video compression [Gal91], and H.261, the CCITT standard for video compression [Lio91], take advantage of both spatial and temporal redundancies. 2 an example the T-mass T-TL-2600 tape robot which <p> low quality formats such as the Quarter-CIF (QCIF) the one of the two video formats used in teleconferencing [Lio91] to high-definition television (HDTV) standards offering high-qualityvideo <ref> [Gal91] </ref>. 4 JPEG, the ISO standard for still image compression [Wal91], takes advantage of spatial redundancy. 5 MPEG, the ISO standard for video compression [Gal91], and H.261, the CCITT standard for video compression [Lio91], take advantage of both spatial and temporal redundancies. 2 an example the T-mass T-TL-2600 tape robot which can store 13 terabyte of data on one robot. 2.3 Video Data Delivery The most important difference between traditional data and continuous time media
Reference: [Gra93] <author> Silicon Graphics. </author> <title> Challenge network resource servers data sheet, </title> <year> 1993. </year>
Reference-contexts: To handle this amount of data streaming through the system, one has to introduce parallelism. Large scale video servers are appearing in the market today. Three different approaches have been introduced: * Parallel Servers, such as the Silicon Graphics Challenge <ref> [Gra93] </ref>, are homogeneous and tightly coupled servers possibly with several processors connected to each other via high-capacity internal busses.
Reference: [Her91] <author> R.G. Herrtwich, </author> <title> editor. Network and Operting System Support for Digital Audio and Video. </title> <booktitle> Second International Workshop, </booktitle> <address> Heidelberg, Germany, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: All components within a video information system, including the video DBMS, the communications network and the viewing station should support this isochronity. Much research is being performed regarding network and operating system support for video data - e.g. <ref> [Her91] </ref> and [VR92]. Less research has, however, been devoted to database support of video. Oracle is one of the first commercial database suppliers announcing support for video data in their Oracle Media Server [LL94].
Reference: [Hje94] <author> R. Hjelsvold. </author> <title> Video Information Contents and Architecture. </title> <booktitle> In Proceedings of the 4th International Conference on Extending Database Technology, </booktitle> <address> Cambridge, UK, </address> <month> March 28-31 </month> <year> 1994. </year>
Reference-contexts: The use of a video database to support the different activities within a television company was also discussed in <ref> [Hje94] </ref>. To achieve the goal of being able to supporte diverse applications, a digital video archive has to provide a common framework for the different applications. Figure 1 shows the main activities connected to a video archive. <p> Our model provides a generic framework that can be tailored to specific application domains. 3.2 Thematic Indexing The structure of a video document captures some aspects of the video material but is not suited as a representation of every characteristic of the material. As discussed in [Smi92] and <ref> [Hje94] </ref> it should be possible to make detailed descriptions of the content of the video material which are not necessarily directly linked to structural components but more often to arbitrary frame sequences. <p> A set of general annotation types, however, (see the subclasses shown) should be provided by the generic model while allowing the applications to augment the descriptive power with domain specific annotation types. Different types of annotations are more thor oughly discussed in <ref> [Hje94] </ref> and [Mer93]. 4 are shown in the diagram.) 3.3 Sharing and Reuse of Video Mate rial As recognised in [MD89], the same basic video material may be used in several different video documents.
Reference: [KHT91] <author> W. Kameyama, T. Hanamura, and H. Tom-inaga. </author> <title> A Proposal of Multimedia Document Architecture and Video Document Architecture. </title> <booktitle> In Proceedings of ICC '91 The International Conference on Communication Conference Record, </booktitle> <address> Denver, USA, </address> <year> 1991. </year>
Reference-contexts: This is because a single frame spans a very short interval of time and because there are so many individual frames even in a quite short video document (the Eu-ropean video standard, PAL, results for instance in 25 frames per second). [RD89] and <ref> [KHT91] </ref> strongly emphasize the need for some sort of structuring method. From experiments with a television news archive [Mer93] we have learned that abstractions such as scenes and news items make it easier for the user to make references to video information and easier to comprehend its contents.
Reference: [KO93] <author> Y. Tonomura K. Otsuji. </author> <title> Projection detecting filter for video cut detection. </title> <booktitle> In Proceedings 10 of the first ACM International Conference on Multimedia, </booktitle> <address> Anaheim, </address> <month> August 1-6 </month> <year> 1993. </year>
Reference-contexts: However, it seems clear that if all descriptions are to be made after the video material has been produced it would still be a rather time consuming process. Work has been done to try to automatically detect shot transitions, see for instance [ZKS93] or <ref> [KO93] </ref>. The methods proposed so far are not reliable, partly because of special effects used to smooth out transitions. This kind of information is precisely known some production stage and it would be a superior solution to preserve this information during the lifetime of the video material.
Reference: [LG93] <author> T.D.C. Little and A. Ghafoor. </author> <title> Interval-Based Conceptual Models for Time-Dependent Multimedia Data. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 5(4), </volume> <year> 1993. </year>
Reference-contexts: Figure 4 shows a video stream where two frame sequences are non-overlapping (a) and where two frame sequences overlap (b). A frame sequence is essentially a temporal interval with a temporal relation between the frames. Temporal interval operators are therefore applicable to frame sequences. In <ref> [LG93] </ref> the following temporal interval operators are discussed: * A Equals B: Returns true if the two sequences A and B are identical. because frame sequence 2 is reused * A Before B: Returns true if A happens before B. (The complementary operator is Af ter.) * A M eets B:
Reference: [Lio91] <author> M. Liou. </author> <title> Overview of the px64 kbit/s Video Coding Standard. </title> <journal> Communications of the ACM, </journal> <volume> 32(4), </volume> <year> 1991. </year>
Reference-contexts: The size of the video stream depends on the quality of the video format 3 being used. The Common Intermediate Format (CIF) used in teleconferencing is a medium quality format that can be used as an illustrative example. The uncompressed bit rates for transmitting CIF is 36.45 Mbit/s <ref> [Lio91] </ref>. 1 GB disk space, which most users today find quite spacious, can only store three and a half minutes of such a video stream. The data volumes that uncompressed video generates cannot easily be handled in the computer systems. Fortunately, video data is redundant in two dimensions. <p> The European analog video standard, PAL, specifies 25 frames per second (fps) while the frame rate for the American NTSC standard is 30 fps. 3 Several digital video formats exist ranging from low quality formats such as the Quarter-CIF (QCIF) the one of the two video formats used in teleconferencing <ref> [Lio91] </ref> to high-definition television (HDTV) standards offering high-qualityvideo [Gal91]. 4 JPEG, the ISO standard for still image compression [Wal91], takes advantage of spatial redundancy. 5 MPEG, the ISO standard for video compression [Gal91], and H.261, the CCITT standard for video compression [Lio91], take advantage of both spatial and temporal redundancies. 2 <p> one of the two video formats used in teleconferencing <ref> [Lio91] </ref> to high-definition television (HDTV) standards offering high-qualityvideo [Gal91]. 4 JPEG, the ISO standard for still image compression [Wal91], takes advantage of spatial redundancy. 5 MPEG, the ISO standard for video compression [Gal91], and H.261, the CCITT standard for video compression [Lio91], take advantage of both spatial and temporal redundancies. 2 an example the T-mass T-TL-2600 tape robot which can store 13 terabyte of data on one robot. 2.3 Video Data Delivery The most important difference between traditional data and continuous time media such as audio and video, is the timing requirements.
Reference: [LL94] <author> A. Laursen and B. Linder. </author> <title> Delivering Real-time Audio/Video with the Oracle Media Server. </title> <booktitle> In Proceedings of the EOUG Oracle User Forum 94, </booktitle> <address> Maastricth, Netherlands, </address> <month> April 17-20 </month> <year> 1994. </year>
Reference-contexts: Much research is being performed regarding network and operating system support for video data - e.g. [Her91] and [VR92]. Less research has, however, been devoted to database support of video. Oracle is one of the first commercial database suppliers announcing support for video data in their Oracle Media Server <ref> [LL94] </ref>. Even though Oracle Media Server is supported by the Oracle7 DBMS, it is not an integrated part of the DBMS. Much research and developments have to be done before DBMS's really manage video data.
Reference: [MD89] <author> W.E. Mackay and G. Davenport. </author> <title> Virtual Video Editing In Interactive Multimedia Applications. </title> <journal> Communications of the ACM, </journal> <volume> 32(7), </volume> <year> 1989. </year>
Reference-contexts: Different types of annotations are more thor oughly discussed in [Hje94] and [Mer93]. 4 are shown in the diagram.) 3.3 Sharing and Reuse of Video Mate rial As recognised in <ref> [MD89] </ref>, the same basic video material may be used in several different video documents.
Reference: [Mer93] <author> P. Merok. </author> <title> Data Models for Digital Film and Video Archives. </title> <type> Master's thesis, </type> <institution> Norwegian Institute of Technology, </institution> <year> 1993. </year> <note> In Norwegian. </note>
Reference-contexts: From experiments with a television news archive <ref> [Mer93] </ref> we have learned that abstractions such as scenes and news items make it easier for the user to make references to video information and easier to comprehend its contents. Other researchers have shown that video information browsing is difficult (see [Ste91]). <p> A set of general annotation types, however, (see the subclasses shown) should be provided by the generic model while allowing the applications to augment the descriptive power with domain specific annotation types. Different types of annotations are more thor oughly discussed in [Hje94] and <ref> [Mer93] </ref>. 4 are shown in the diagram.) 3.3 Sharing and Reuse of Video Mate rial As recognised in [MD89], the same basic video material may be used in several different video documents.
Reference: [Mon81] <author> J. Monaco. </author> <title> How to Read a Film. The Art, Technology, Language, History and Theory of Film and Media. </title> <publisher> Oxford University Press, </publisher> <year> 1981. </year> <title> [nCU94] nCUBE. </title> <booktitle> Oracle dbms on ncube ??? In Proceedings of SIGMOD, </booktitle> <month> June </month> <year> 1994. </year>
Reference-contexts: Other researchers have shown that video information browsing is difficult (see [Ste91]). Our experiments show that structural abstractions give valuable support to video browsers. The structure part of our model is inspired from film theory <ref> [Mon81] </ref> and work based on segmentation of video material. It is built around the concept of a StructuralComponent which has an associated FrameSequence of video material. The concept of StructuralComponent is specialised into the CompoundUnit, Sequence, Scene and Shot subclasses and a hierarchical relationship is defined between the different subclasses.
Reference: [Par91] <institution> Parallax Graphics, Inc. </institution> <note> XVideo User's Guide, </note> <year> 1991. </year>
Reference-contexts: The player is implemented in C++ and C and the interface is based on the X11 window system. The video player runs on a SUN SPARCstation LX where a Parallax XVideo board <ref> [Par91] </ref> is installed. A JPEG chip (C-CUBE) resides on the board and is used to support real-time video compression. Two 2.1 GB Seagate Barracuda harddisks are connected to the workstation and provide the storage facility for compressed digital video.
Reference: [RD89] <author> B. Rubin and G. Davenport. </author> <title> Structured Content Modeling for Cinematic Information. </title> <journal> SIGCIHI Bulletin, </journal> <volume> 21(2), </volume> <month> October </month> <year> 1989. </year>
Reference-contexts: This is because a single frame spans a very short interval of time and because there are so many individual frames even in a quite short video document (the Eu-ropean video standard, PAL, results for instance in 25 frames per second). <ref> [RD89] </ref> and [KHT91] strongly emphasize the need for some sort of structuring method. From experiments with a television news archive [Mer93] we have learned that abstractions such as scenes and news items make it easier for the user to make references to video information and easier to comprehend its contents.
Reference: [Ske93] <author> S. Skeide. Filmserver. </author> <type> Master's thesis, </type> <institution> Norwe-gian Institute of Technology, </institution> <year> 1993. </year> <note> In norwe-gian. </note>
Reference-contexts: This work was primarily done by Sigurd Skeide <ref> [Ske93] </ref> and it was partly based on source code developed by Norwegian Telecom Research. The player is implemented in C++ and C and the interface is based on the X11 window system. The video player runs on a SUN SPARCstation LX where a Parallax XVideo board [Par91] is installed.
Reference: [Smi92] <author> T.G.A. Smith. </author> <title> If You Could See What I Mean... Descriptions of Video in an Anthropologist's Notebook. </title> <type> Master's thesis, </type> <institution> MIT, </institution> <year> 1992. </year>
Reference-contexts: Our model provides a generic framework that can be tailored to specific application domains. 3.2 Thematic Indexing The structure of a video document captures some aspects of the video material but is not suited as a representation of every characteristic of the material. As discussed in <ref> [Smi92] </ref> and [Hje94] it should be possible to make detailed descriptions of the content of the video material which are not necessarily directly linked to structural components but more often to arbitrary frame sequences.
Reference: [SP91] <author> T.G.A. Smith and N.C. Pincever. </author> <title> Parsing Movies in Context. </title> <booktitle> In Proceedings of the 1991 Summer USENIX Conference, </booktitle> <address> Nashville, USA, </address> <year> 1991. </year>
Reference-contexts: Some applications may expand the common data model with specific concepts while still using the core concepts while less demanding applications may use only a subset. To describe - e.g. index, contents which are not intimately related to structural components, we have adopted ideas from the stratification approach in <ref> [SP91] </ref>. The stratification approach, whilst strong on free annotations, has ignored the need for structure as a tool to navigate and comprehend large volumes of video data. In our generic model, a segmentation approach is used to define the video document structure which can provide well defined levels of abstraction.
Reference: [Ste91] <author> S.M. Stevens. </author> <title> Next Generation Network and Operating System Requirements for Continuous Time Media. </title> <booktitle> In Proceedings of the Second International Workshop for Network and Operating System Support for Digital Audio and Video, </booktitle> <address> Heidelberg, Germany, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: From experiments with a television news archive [Mer93] we have learned that abstractions such as scenes and news items make it easier for the user to make references to video information and easier to comprehend its contents. Other researchers have shown that video information browsing is difficult (see <ref> [Ste91] </ref>). Our experiments show that structural abstractions give valuable support to video browsers. The structure part of our model is inspired from film theory [Mon81] and work based on segmentation of video material. It is built around the concept of a StructuralComponent which has an associated FrameSequence of video material.
Reference: [Tir94] <author> W.R. Tirman. </author> <title> Strategies and capabilities in video services provisioning. </title> <type> Technical report, </type> <institution> Digital Equipment Corporation, </institution> <month> February 8 </month> <year> 1994. </year>
Reference-contexts: The busses will also connect the processors to network interface cards and storage de vices. * Distributed Servers, such as the Digital video server <ref> [Tir94] </ref>, consist of individual nodes connected via a more or less general communications network.
Reference: [VR92] <author> P. Venkat Rangan, </author> <title> editor. Network and Operating System Support for Digital Audio and Video. </title> <booktitle> Third International Workshop, </booktitle> <address> La Jolla, California, USA, </address> <month> Novemer </month> <year> 1992. </year>
Reference-contexts: All components within a video information system, including the video DBMS, the communications network and the viewing station should support this isochronity. Much research is being performed regarding network and operating system support for video data - e.g. [Her91] and <ref> [VR92] </ref>. Less research has, however, been devoted to database support of video. Oracle is one of the first commercial database suppliers announcing support for video data in their Oracle Media Server [LL94].
Reference: [Wal91] <author> G.K. Wallace. </author> <title> The JPEG Still Picture Compression Standard. </title> <journal> Communications of the ACM, </journal> <volume> 32(4), </volume> <year> 1991. </year>
Reference-contexts: American NTSC standard is 30 fps. 3 Several digital video formats exist ranging from low quality formats such as the Quarter-CIF (QCIF) the one of the two video formats used in teleconferencing [Lio91] to high-definition television (HDTV) standards offering high-qualityvideo [Gal91]. 4 JPEG, the ISO standard for still image compression <ref> [Wal91] </ref>, takes advantage of spatial redundancy. 5 MPEG, the ISO standard for video compression [Gal91], and H.261, the CCITT standard for video compression [Lio91], take advantage of both spatial and temporal redundancies. 2 an example the T-mass T-TL-2600 tape robot which can store 13 terabyte of data on one robot. 2.3
Reference: [ZKS93] <author> H.J. Zhang, A. Kankanhalli, and S.W. Smo-liar. </author> <title> Automatic partitioning of full-motion video. </title> <journal> Multimedia Systems, </journal> <volume> 1(1), </volume> <year> 1993. </year> <pages> 11 12 </pages>
Reference-contexts: However, it seems clear that if all descriptions are to be made after the video material has been produced it would still be a rather time consuming process. Work has been done to try to automatically detect shot transitions, see for instance <ref> [ZKS93] </ref> or [KO93]. The methods proposed so far are not reliable, partly because of special effects used to smooth out transitions. This kind of information is precisely known some production stage and it would be a superior solution to preserve this information during the lifetime of the video material.
References-found: 25

