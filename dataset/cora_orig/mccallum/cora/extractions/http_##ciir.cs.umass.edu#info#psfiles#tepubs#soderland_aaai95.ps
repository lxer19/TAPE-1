URL: http://ciir.cs.umass.edu/info/psfiles/tepubs/soderland_aaai95.ps
Refering-URL: http://cobar.cs.umass.edu/pubfiles/te.html
Root-URL: 
Email: soderlan@cs.umass.edu lehnert@cs.umass.edu  
Title: on Empirical Methods in Discourse Interpretation and Generation Learning Domain-Specific Discourse Rules for Information Extraction  
Author: Stephen Soderland and Wendy Lehnert 
Address: Amherst, MA 01003-4610  
Affiliation: Department of Computer Science University of Massachusetts  
Note: AAAI 1995 Spring Symposium  
Abstract: This paper describes a system that learns discourse rules for domain-specific analysis of unrestricted text. The goal of discourse analysis in this context is to transform locally identified references to relevant information in the text into a coherent representation of the entire text. This involves a complex series of decisions about merging coreferential objects, filtering out irrelevant information, inferring missing information, and identifying logical relations between domain objects. The Wrap-Up discourse analyzer induces a set of classifiers from a training corpus to handle these discourse decisions. Wrap-Up is fully trainable, and not only determines what classifiers are needed based on domain output specifications, but automatically selects the features needed by each classifier. Wrap-Up's classifiers blend linguistic knowledge with real world domain knowledge. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Grosz, B.; Sidner C. </author> <year> 1986. </year> <title> Attention, intention and the structure of discourse, </title> <type> 175-204. </type> <note> Computational Linguistics, 12(3). </note>
Reference: <author> Hobbs, J. </author> <year> 1978. </year> <title> Resolving Pronoun References, </title> <type> 311-338. Lingua, 44(4). </type>
Reference-contexts: Wrap-Up differs from other work on discourse, which has often involved tracking shifts in topic and in the speaker/writer's goals (Grosz and Sidner 1986; Liddy et al. 1993) or in resolving anaphoric references <ref> (Hobbs 1978) </ref>. Domain-specific discourse analysis that processes unrestricted text may concern itself with some of these issues, but only as a means to its main objective of transforming bits and pieces of extracted information into a coherent representation.
Reference: <author> Lehnert, W. </author> <year> 1990. </year> <title> Symbolic/Subsymbolic Sentence Analysis: Exploiting the Best of Two Worlds, 151-158. </title> <booktitle> Advances in Connectionist and Neural Computation Theory. vol. </booktitle> <address> 1.. Norwood, NJ: </address> <publisher> Ablex Publishing. </publisher>
Reference: <author> Lehnert, W.; McCarthy, J.; Soderland, S.; Riloff, E.; Cardie, C.; Peterson, J.; Feng, F; Dolan, C.; Goldman, S. </author> <year> 1993. </year> <title> UMass/Hughes: Description of the CIRCUS System as Used for MUC-5, 257-259. </title> <booktitle> In Proceedings of the Fifth Message Understanding Conference. </booktitle> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference: <author> Liddy, L.; McVearry, K.; Paik, W.; Yu, E.; McKenna, M. </author> <year> 1993. </year> <title> Development, Implementation, and Testing of a Discourse Model for Newspaper Texts, 159-164. </title> <booktitle> In Proceedings of the Human Language Technology Workshop. </booktitle> <publisher> Morgan Kaufmann Publishers. </publisher> <address> MUC-5. </address> <year> 1993. </year> <booktitle> Proceedings of the Fifth Message Understanding Conference. </booktitle> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference: <author> Quinlan, J.R. </author> <year> 1986. </year> <title> Induction of Decision Trees, 81-106. </title> <journal> Machine Learning, </journal> <volume> 1. </volume>
Reference-contexts: The Wrap-Up Discourse Analyzer Wrap-Up is a domain-independent framework that is instantiated with a set of classifiers to handle domain-specific discourse analysis. During its training phase, Wrap-Up derives a set of ID3 decision trees <ref> (Quinlan 1986) </ref> from a representative set of training texts. Each training text has a hand-coded output key indicating the objects and relationships that should be identified for that text.
Reference: <author> Riloff, E. </author> <year> 1993. </year> <title> Automatically Constructing a Dictionary for Information Extraction Tasks, 811-816. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence. </booktitle>
Reference-contexts: The input to Wrap-Up is a set of domain objects identified by the University of Massachusetts CIRCUS sentence analyzer (Lehnert 1990; Lehnert et al. 1993). CIRCUS uses domain-specific extraction patterns to create a "concept node" (CN) for each noun phrase that contains relevant information <ref> (Riloff 1993) </ref>. Each CN has a case frame for the extracted information along with the position of the reference and a list of extraction patterns used. Wrap-Up must merge information from coreferential CN's, discard information that was erroneously identified during sentence analysis, and determine logical relationships between objects. <p> Once available, this corpus of training texts can be used repeatedly for knowledge acquisition at all levels of processing. The same training corpus was used to induce a dictionary of CN definitions used by CIRCUS in sentence analysis <ref> (Riloff 1993) </ref>. A thousand texts provided the training for Wrap-Up in the microelectronics domain.
Reference: <author> Soderland, S. and Lehnert, W. </author> <year> 1994a. </year> <title> Corpus-Driven Knowledge Acquisition for Discourse Analysis, 827-832. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence. </booktitle>
Reference-contexts: About half of the objects identified by CIR CUS in this domain were spurious. Those that persist past Wrap-Up's filtering step are a source of noise in the training for later steps. System Performance As previously reported <ref> (Soderland and Lehnert 1994a, 1994b) </ref> the full system with Wrap-Up compares favorably with the discourse module used in the MUC-5 evaluation by UMass, which was only partially trainable. Here we will look at the performance of Wrap-Up from the point-of-view of individual classifiers, as shown in Figure 5.
Reference: <author> Soderland, S. and Lehnert, W. </author> <year> 1994b. </year> <title> Wrap-Up: a Trainable Discourse Module for Information Extraction, 131-158. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2. </volume>
References-found: 9

