URL: http://www.research.att.com/~lewis/papers/lewis92e.ps
Refering-URL: http://www.csi.uottawa.ca/~debruijn/irbib.html
Root-URL: 
Email: (lewis@tira.uchicago.edu)  
Title: Feature Selection and Feature Extraction for Text Categorization  
Author: David D. Lewis 
Date: 212-217, 1992.  
Note: Appeared (with same pagination) in Speech and Natural Language: Proceedings of a workshop held at Harriman, New York, February 23-26, 1992. Morgan Kaufmann, San Mateo, CA, pp.  
Address: Chicago, Chicago, IL 60637  
Affiliation: Center for Info. and Language Studies, University of  
Abstract: The effect of selecting varying numbers and kinds of features for use in predicting category membership was investigated on the Reuters and MUC-3 text categorization data sets. Good categorization performance was achieved using a statistical classifier and a proportional assignment strategy. The optimal feature set size for word-based indexing was found to be surprisingly low (10 to 15 features) despite the large training sets. The extraction of new text features by syntactic analysis and feature clustering was investigated on the Reuters data set. Syntactic indexing phrases, clusters of these phrases, and clusters of words were all found to provide less effective representations than individual words. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Hayes, P. and Weinstein, S. CONSTRUE/TIS: </author> <title> a system for content-based indexing of a database of news stories. </title> <booktitle> In IAAI-90, </booktitle> <year> 1990. </year>
Reference-contexts: 1. Introduction Text categorization|the automated assigning of natural language texts to predefined categories based on their content|is a task of increasing importance. Its applications include indexing texts to support document retrieval <ref> [1] </ref>, extracting data from texts [2], and aiding humans in these tasks. The indexing language used to represent texts influences how easily and effectively a text categorization system can be built, whether the system is built by human engineering, statistical training, or a combination of the two. <p> Data Sets and Tasks Our first data set was a set of 21,450 Reuters newswire stories from the year 1987 [4]. These stories have been manually indexed using 135 financial topic categories, to support document routing and retrieval. Particular care was taken in assigning categories <ref> [1] </ref>. All stories dated April 7, 1987 and earlier went into a set of 14,704 training documents, and all stories from April 8, 1987 or later went into a test set of 6,746 documents. The second data set consisted of 1,500 documents from the U.S. <p> Recall Reuters e e e e e e e e e e e MUC-3 u u u u u u u u u u u uu u (w/ 10 features) and MUC-3 (w/ 15 features) test sets. a different, and possibly easier, testset drawn from the Reuters data <ref> [1] </ref>. This level of performance, the result of a 9.5 person-year effort, is an admirable target for learning-based systems to shoot for. Comparison with published results on MUC-3 are difficult, since we simplified the complex MUC-3 task.
Reference: 2. <editor> Sundheim, B., ed. </editor> <booktitle> Proceedings of the Third Message Understanding Evaluation and Conference, </booktitle> <publisher> Morgan Kauf-mann, </publisher> <address> Los Altos, CA, </address> <month> May </month> <year> 1991. </year>
Reference-contexts: 1. Introduction Text categorization|the automated assigning of natural language texts to predefined categories based on their content|is a task of increasing importance. Its applications include indexing texts to support document retrieval [1], extracting data from texts <ref> [2] </ref>, and aiding humans in these tasks. The indexing language used to represent texts influences how easily and effectively a text categorization system can be built, whether the system is built by human engineering, statistical training, or a combination of the two. <p> The second data set consisted of 1,500 documents from the U.S. Foreign Broadcast Information Service (FBIS) that had previously been used in the MUC-3 evaluation of natural language processing systems <ref> [2] </ref>. The documents are mostly translations from Spanish to English, and include newspaper stories, transcripts of broadcasts, communiques, and other material. The MUC-3 task required extracting simulated database records ("templates") describing terrorist incidents from these texts.
Reference: 3. <author> Lewis, D. and Croft, W. </author> <title> Term clustering of syntactic phrases. </title> <booktitle> In ACM SIGIR-90, </booktitle> <pages> pp. 385-404, </pages> <year> 1990. </year>
Reference-contexts: This strategy is referred to as term clustering. Syntactic phrase indexing and term clustering have op posite effects on the properties of a text representation, which led us to investigate combining the two techniques <ref> [3] </ref>. However, the small size of standard text retrieval test collections, and the variety of approaches available for query interpretation, made it difficult to study purely representational issues in text retrieval experiments. In this paper we examine indexing language properties using two text categorization data sets. <p> Why did phrase clustering fail? In earlier work on the CACM collection <ref> [3] </ref>, we identified lack of training data as a primary impediment to high quality cluster formation. The Reuters corpus provided approximately 1.5 million phrase occurrences, a factor of 25 more than CACM.
Reference: 4. <author> Lewis, D. </author> <title> Representation and Learning in Information Retrieval. </title> <type> PhD thesis, </type> <institution> Computer Science Dept.; Univ. </institution> <address> of Mass.; Amherst, MA, </address> <year> 1992. </year> <type> Technical Report 91-93. </type>
Reference-contexts: We obtain much clearer results, as well as producing a new text categorization method capable of handing multiple, overlapping categories. 2. Data Sets and Tasks Our first data set was a set of 21,450 Reuters newswire stories from the year 1987 <ref> [4] </ref>. These stories have been manually indexed using 135 financial topic categories, to support document routing and retrieval. Particular care was taken in assigning categories [1]. <p> The result is that the estimates of P (C j = 1jD m ) can be quite inaccurate, as well as inconsistent across categories and documents. We investigated several strategies for dealing with this problem and settled on proportional assignment <ref> [4] </ref>. Each category is assigned to its top scoring documents on the test set in a designated multiple of the percentage of documents it was assigned to on the training corpus. <p> Clustering features requires defining a set of metafea-tures on which the similarity of the features will be judged. We experimented with forming clusters from words under three metafeature definitions, and from phrases under eight metafeature definitions <ref> [4] </ref>. Metafea-tures were based on presence or absence of features in documents, or on the strength of association of features with categories of documents. In all cases, similarity between metafeature vectors was measured using the cosine correlation.
Reference: 5. <author> Fuhr, N. </author> <title> Models for retrieval with probabilistic indexing. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 25(1) </volume> <pages> 55-72, </pages> <year> 1989. </year>
Reference-contexts: Category assignments should be quite consistent on our test set, but less so on our training set. 3. Categorization Method The statistical model used in our experiments was proposed by Fuhr <ref> [5] </ref> for probabilistic text retrieval, but the adaptation to text categorization is straightforward. possibility that the values of the binary features for a document is not known with certainty, though that aspect of the model was not used in our experiments. 3.1.
Reference: 6. <author> Duda, R. and Hart, P. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley-Interscience, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: Given accurate estimates of P (C j = 1jD m ), decision theory tells us that the optimal strategy, assuming all errors have equal cost, is to set a single threshold p and assign C j to a document exactly when P (C j = 1jD m ) &gt;= p <ref> [6] </ref>. However, as is common in probabilistic models for text classification tasks, the formula in Figure 1 makes assumptions about the independence of probabilities which do not hold for textual data. <p> e e e e e e e ee e e MUC-3 (Test) u u u u u u u u u u uu u u Reuters (Training) b b b sets of words on Reuters and MUC-3 test sets, and on Reuters training set. category membership, was one possible villain <ref> [6] </ref>. We checked for overfitting directly by testing the induced classifiers on the training set. The thicker line in Figure 4 shows the effectiveness of the Reuters classifiers when tested on the 14,704 stories used to train them.
Reference: 7. <author> Hamming, R. </author> <title> Coding and Information Theory. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1980. </year>
Reference-contexts: We are continuing to investigate other approaches. 3.2. Feature Selection A primary concern of ours was to examine the effect of feature set size on text categorization effectiveness. All potential features were ranked for each category by expected mutual information <ref> [7] </ref> between assignment of * WORDS-DF2: Starts with all words tokenized by parts. Capitalization and syntactic class ignored. Stopwords discarded based on syntactic tags. Tokens consisting solely of digits and punctuation removed. Words occurring in fewer than 2 training documents removed.
Reference: 8. <author> Church, K. </author> <title> A stochastic parts program and noun phrase parser for unrestricted text. </title> <booktitle> In Second Conference on Applied NLP, </booktitle> <pages> pp. 136-143, </pages> <year> 1988. </year>
Reference-contexts: For the Reuters data we adopted a conservative approach to syntactic phrase indexing. The phrasal indexing language consisted only of simple noun phrases, i.e. head nouns and their immediate premodifiers. Phrases were formed using parts, a stochastic syntactic class tagger and simple noun phrase bracketing program <ref> [8] </ref>.
Reference: 9. <author> Lewis, D. </author> <title> Evaluating text categorization. </title> <booktitle> In Speech and Natural Language Workshop, </booktitle> <pages> pp. 312-318, </pages> <month> Feb. </month> <year> 1991. </year>
Reference-contexts: For a set of k categories and d documents a total of n = kd categorization decisions are made. We used mi-croaveraging, which considers all kd decisions as a single group, to compute average effectiveness <ref> [9] </ref>. The proportionality parameter in our categorization method was varied to show the possible tradeoffs between recall and precision. As a single summary figure for recall precision curves we took the breakeven point, i.e. the highest value (interpolated) at which recall and precision are equal. 6.
Reference: 10. <author> Fuhr, N., et al. </author> <title> AIR/X|a rule-based multistage indexing system for large subject fields. </title> <booktitle> In RIAO 91, </booktitle> <pages> pp. 606-623, </pages> <year> 1991. </year>
Reference-contexts: A breakeven point of 0.65 on Reuters and 0.48 on MUC-3 is reached. For comparison, the operational AIR/X system uses both rule-based and statistical techniques to achieve a microaveraged breakeven point of approximately 0:65 in indexing a physics database <ref> [10] </ref>. The CONSTRUE rule-based text categorization system achieves a microaveraged breakeven of around 0:90 on 214 0 0.4 0.8 0 0.2 0.4 0.6 0.8 1 Prec.
Reference: 11. <author> Lewis, D. </author> <title> Data extraction as text categorization: An experiment with the MUC-3 corpus. </title> <booktitle> In Proceedings MUC-3, </booktitle> <month> May </month> <year> 1991. </year> <month> 217 </month>
Reference-contexts: This is despite being limited in most cases to 50% the score achievable by methods that attempted cross-referencing <ref> [11] </ref>. 6.1. Feature Selection show the breakeven point reached for categorization runs with various size sets of words, again on both the Reuters and MUC-3 data sets.
References-found: 11

