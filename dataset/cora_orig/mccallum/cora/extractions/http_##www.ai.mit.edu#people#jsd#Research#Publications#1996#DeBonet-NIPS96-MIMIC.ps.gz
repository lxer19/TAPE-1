URL: http://www.ai.mit.edu/people/jsd/Research/Publications/1996/DeBonet-NIPS96-MIMIC.ps.gz
Refering-URL: http://www.ai.mit.edu/people/jsd/cv/
Root-URL: 
Title: MIMIC: Finding Optima by Estimating Probability Densities  
Author: Jeremy S. De Bonet, Charles L. Isbell, Jr., Paul Viola 
Note: Advances in Neural Information Processing Systems 1997 MIT Press, Cambridge, MA  
Address: Cambridge, MA 02139  
Affiliation: Artificial Intelligence Laboratory Massachusetts Institute of Technology  
Abstract: In many optimization problems, the structure of solutions reflects complex relationships between the different input parameters. For example, experience may tell us that certain parameters are closely related and should not be explored independently. Similarly, experience may establish that a subset of parameters must take on particular values. Any search of the cost landscape should take advantage of these relationships. We present MIMIC, a framework in which we analyze the global structure of the optimization landscape. A novel and efficient algorithm for the estimation of this structure is derived. We use knowledge of this structure to guide a randomized search through the solution space and, in turn, to refine our estimate of the structure. Our technique obtains significant speed gains over other randomized optimization procedures. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Baluja, S. and Caruana, R. </author> <year> (1995). </year> <title> Removing the genetics from the standard genetic algorithm. </title> <type> Technical report, </type> <institution> Carnegie Mellon Univerisity. </institution>
Reference-contexts: One of our goals is to incorporate insights from GAs in a principled optimization framework. There have been other attempts to capture the advantages of GAs. Population Based Incremental Learning (PBIL) attempts to incorporate the notion of a candidate population by replacing it with a single probability vector <ref> (Baluja and Caru-ana, 1995) </ref>. Each element of the vector is the probability that a particular bit in a solution is on. During the learning process, the probability vector can be thought of as a simple model of the optimization landscape. <p> MIMIC the algorithm above with 200 samples per iteration 2. PBIL standard population based incremental learning 3. RHC randomized hill climbing 4. GA a standard genetic algorithm with single crossover and 10% mutation rate 5.1 Four Peaks The Four Peaks problem is taken from <ref> (Baluja and Caruana, 1995) </ref>.
Reference: <author> Baum, E. B., Boneh, D., and Garrett, C. </author> <year> (1995). </year> <title> Where genetic algorithms excel. </title> <booktitle> In Proceedings of the Conference on Computational Learning Theory, </booktitle> <address> New York. </address> <institution> Association for Computing Machinery. </institution>
Reference: <author> Holland, J. H. </author> <year> (1975). </year> <title> Adaptation in Natural and Artificial Systems. </title> <publisher> The Michigan University Press. </publisher>
Reference: <author> Kirkpatrick, S., Gelatt, C., and Vecchi, M. </author> <year> (1983). </year> <title> Optimization by Simulated Annealing. </title> <journal> Science, </journal> <volume> 220(4598) </volume> <pages> 671-680. </pages>
Reference: <author> Lang, K. </author> <year> (1995). </year> <title> Hill climbing beats genetic search on a boolean circuit synthesis problem of koza's. </title> <booktitle> In Twelfth International Conference on Machine Learning. </booktitle>
Reference: <author> Sabes, P. N. and Jordan, M. I. </author> <year> (1995). </year> <title> Reinforcement learning by probability matching. </title> <editor> In David S. Touretzky, M. M. and Perrone, M., editors, </editor> <booktitle> Advances in Neural Information Processing, volume 8, </booktitle> <address> Denver 1995. </address> <publisher> MIT Press, </publisher> <address> Cambridge. </address>
Reference-contexts: Nevertheless, even in these sorts of problems PBIL often out-performs genetic algorithms because those algorithms are hindered by the fact that random crossovers are infrequently beneficial. A very distinct, but related technique was proposed by Sabes and Jordan for a reinforcement learning task <ref> (Sabes and Jordan, 1995) </ref>. In their framework, the learner must generate actions so that a reinforcement function can be completely explored. Simultaneously, the learner must exploit what it has learned so as to optimize the long-term reward.
References-found: 6

