URL: ftp://theory.lcs.mit.edu/pub/people/rdb/msthesis.ps.Z
Refering-URL: http://theory.lcs.mit.edu/~cilk/papers.html
Root-URL: 
Title: Managing Storage for Multithreaded Computations  
Author: by Robert D. Blumofe Campbell L. Searle 
Degree: (1988) Submitted to the Department of Electrical Engineering and Computer Science in partial fulfillment of the requirements for the degree of Master of Science at the  Signature of Author  Certified by Charles E. Leiserson Professor of Electrical Engineering and Computer Science Thesis Supervisor Accepted by  Chairman, Departmental Committee on Graduate Students  
Note: c Massachusetts Institute of Technology  
Date: September 1992  1992  September 8, 1992  
Affiliation: Sc.B., Brown University  MASSACHUSETTS INSTITUTE OF TECHNOLOGY  Department of Electrical Engineering and Computer Science  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Anant Agarwal, Beng-Hong Lim, David Kranz, and John Kubiatowicz. </author> <month> APRIL: </month> <title> A processor architecture for multiprocessing. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 104-114, </pages> <address> Seattle, Washington, </address> <month> May </month> <year> 1990. </year> <note> Also: </note> <institution> MIT Laboratory for Computer Science Technical Report MIT/LCS/TM-450. </institution>
Reference-contexts: At run time, a scheduler employs dataflow concepts to dynamically order execution of the threads. Other systems have schedulers that dynamically order threads based on the availability of data in shared memory multiprocessors <ref> [1, 4, 13] </ref> or on the arrival of messages in message-passing multicomputers [2, 10, 20, 35]. Rapid execution of a multithreaded computation on a parallel computer requires exposing and exploiting parallelism in the computation by keeping enough threads concurrently active to keep the processors of the computer busy.
Reference: [2] <author> William C. Athas and Charles L. Seitz. </author> <title> Multicomputers: Message-passing concurrent computers. </title> <journal> Computer, </journal> <volume> 21(8) </volume> <pages> 9-24, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: At run time, a scheduler employs dataflow concepts to dynamically order execution of the threads. Other systems have schedulers that dynamically order threads based on the availability of data in shared memory multiprocessors [1, 4, 13] or on the arrival of messages in message-passing multicomputers <ref> [2, 10, 20, 35] </ref>. Rapid execution of a multithreaded computation on a parallel computer requires exposing and exploiting parallelism in the computation by keeping enough threads concurrently active to keep the processors of the computer busy.
Reference: [3] <author> Sandeep Bhatt, David Greenberg, Tom Leighton, and Pangfeng Liu. </author> <title> Tight bounds for on-line tree embeddings. </title> <booktitle> In Proceedings of the Second Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 344-350, </pages> <address> San Francisco, California, </address> <month> January </month> <year> 1991. </year>
Reference-contexts: The work on lazy task creation [23] and the work on dynamic tree embedding <ref> [3, 22] </ref> may provide some pointers in this direction. Of course, an algorithm that removes the lg P factor from the space bound of LDF would be a nice improvement. 77 78 Chapter 9.
Reference: [4] <author> Bob Boothe and Abhiram Ranade. </author> <title> Improved multithreading techniques for hiding communication latency in multiprocessors. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 214-223, </pages> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: At run time, a scheduler employs dataflow concepts to dynamically order execution of the threads. Other systems have schedulers that dynamically order threads based on the availability of data in shared memory multiprocessors <ref> [1, 4, 13] </ref> or on the arrival of messages in message-passing multicomputers [2, 10, 20, 35]. Rapid execution of a multithreaded computation on a parallel computer requires exposing and exploiting parallelism in the computation by keeping enough threads concurrently active to keep the processors of the computer busy.
Reference: [5] <author> Richard P. Brent. </author> <title> The parallel evaluation of general arithmetic expressions. </title> <journal> Journal of the ACM, </journal> <volume> 21(2) </volume> <pages> 201-206, </pages> <month> April </month> <year> 1974. </year>
Reference-contexts: A computer with P processors can execute at most P tasks per step, and since the computation has T 1 tasks, T P T 1 =P . And, of course, we also have T P T 1 . Brent's Theorem <ref> [5, Lemma 2] </ref> yields the bound T P T 1 =P + T 1 .
Reference: [6] <author> David E. Culler. </author> <title> Resource management for the tagged token dataflow architecture. </title> <type> Master's thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, </institution> <month> January </month> <year> 1980. </year> <note> Available as MIT Laboratory for Computer Science Technical Report MIT/LCS/TR-332. </note>
Reference-contexts: The effectiveness of these heuristics is documented with encouraging empirical evidence but no provable time bounds. We consider two of these heuristic techniques: bounded loops and the course-grain throttle. Culler's bounded loops technique <ref> [6, 7, 8] </ref> uses compile-time analysis to augment the program code with resource management code. For each loop of the program, the resource management code computes a value called the k-bound ; a k-bounded loop can have at most k iterations simultaneously active.
Reference: [7] <author> David E. Culler. </author> <title> Managing Parallelism and Resources in Scientific Dataflow Programs. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Massachusetts 79 80 Bibliography Institute of Technology, </institution> <month> March </month> <year> 1990. </year> <note> Available as MIT Laboratory for Computer Science Technical Report MIT/LCS/TR-446. </note>
Reference-contexts: To date, the space requirements of multithreaded computations have been managed with heuristics or not at all <ref> [7, 8, 12, 14, 17, 23, 30, 34] </ref>. In this thesis, we use algorithmic techniques to address the problem of managing storage for multithreaded This thesis describes joint work with Charles E. Leiserson. 9 10 Chapter 1. Introduction computations. <p> The effectiveness of these heuristics is documented with encouraging empirical evidence but no provable time bounds. We consider two of these heuristic techniques: bounded loops and the course-grain throttle. Culler's bounded loops technique <ref> [6, 7, 8] </ref> uses compile-time analysis to augment the program code with resource management code. For each loop of the program, the resource management code computes a value called the k-bound ; a k-bounded loop can have at most k iterations simultaneously active. <p> The compile-time analysis that generates the code that computes the k-bounds is based on heuristics developed from a systematic study of loops in scientific dataflow programs (programs employing only iteration and primitive recursion) <ref> [7] </ref>. These heuristics attempt to set the k-bounds so that the exposed parallelism is maximized under the constraint that space 73 usage stays within the machine's capacity. Ruggiero's course-grain throttle technique [30] makes storage allocation decisions based on overall machine activity at run-time.
Reference: [8] <author> David E. Culler and Arvind. </author> <title> Resource requirements of dataflow programs. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 141-150, </pages> <address> Honolulu, Hawaii, </address> <month> May </month> <year> 1988. </year> <note> Also: </note> <institution> MIT Laboratory for Computer Science, </institution> <note> Computation Structures Group Memo 280. </note>
Reference-contexts: In attempting to expose parallelism, however, schedulers often end up exposing more parallelism than the computer can actually exploit, and since each active thread requires the use of a certain amount of memory, such schedulers can easily overrun the memory capacity of the machine <ref> [8, 12, 14, 30, 34] </ref>. To date, the space requirements of multithreaded computations have been managed with heuristics or not at all [7, 8, 12, 14, 17, 23, 30, 34]. <p> To date, the space requirements of multithreaded computations have been managed with heuristics or not at all <ref> [7, 8, 12, 14, 17, 23, 30, 34] </ref>. In this thesis, we use algorithmic techniques to address the problem of managing storage for multithreaded This thesis describes joint work with Charles E. Leiserson. 9 10 Chapter 1. Introduction computations. <p> The effectiveness of these heuristics is documented with encouraging empirical evidence but no provable time bounds. We consider two of these heuristic techniques: bounded loops and the course-grain throttle. Culler's bounded loops technique <ref> [6, 7, 8] </ref> uses compile-time analysis to augment the program code with resource management code. For each loop of the program, the resource management code computes a value called the k-bound ; a k-bounded loop can have at most k iterations simultaneously active.
Reference: [9] <author> David E. Culler, Anurag Sah, Klaus Erik Schauser, Thorsten von Eicken, and John Wawrzynek. </author> <title> Fine-grain parallelism with minimal hardware support: A compiler-controlled threaded abstract machine. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 164-175, </pages> <address> Santa Clara, California, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Introduction In the course of investigating schemes for general purpose MIMD parallel computation, many diverse research groups have converged on multithreading as a dominant paradigm. As an example, modern dataflow systems <ref> [9, 11, 16, 24, 25, 26, 31, 32] </ref> partition the dataflow instructions into fixed groups called threads and arrange the instructions of each thread into a fixed sequential order at compile time. At run time, a scheduler employs dataflow concepts to dynamically order execution of the threads.
Reference: [10] <author> William J. Dally, Linda Chao, Andrew Chien, Soha Hassoun, Waldemar Horwat, Jon Ka-plan, Paul Song, Brian Totty, and Scott Wills. </author> <title> Architecture of a message-driven processor. </title> <booktitle> In Proceedings of the 14th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 189-196, </pages> <address> Pittsburgh, Pennsylvania, </address> <month> June </month> <year> 1987. </year> <note> Also: </note> <institution> MIT Artificial Intelligence Lab Memo MIT/AI/TR-1069. </institution>
Reference-contexts: At run time, a scheduler employs dataflow concepts to dynamically order execution of the threads. Other systems have schedulers that dynamically order threads based on the availability of data in shared memory multiprocessors [1, 4, 13] or on the arrival of messages in message-passing multicomputers <ref> [2, 10, 20, 35] </ref>. Rapid execution of a multithreaded computation on a parallel computer requires exposing and exploiting parallelism in the computation by keeping enough threads concurrently active to keep the processors of the computer busy.
Reference: [11] <author> V. G. Grafe and J. E. Hoch. </author> <title> The Epsilon-2 hybrid dataflows architecture. </title> <booktitle> In COMPCON 90, </booktitle> <pages> pages 88-93, </pages> <address> San Francisco, California, </address> <month> February </month> <year> 1990. </year>
Reference-contexts: Introduction In the course of investigating schemes for general purpose MIMD parallel computation, many diverse research groups have converged on multithreading as a dominant paradigm. As an example, modern dataflow systems <ref> [9, 11, 16, 24, 25, 26, 31, 32] </ref> partition the dataflow instructions into fixed groups called threads and arrange the instructions of each thread into a fixed sequential order at compile time. At run time, a scheduler employs dataflow concepts to dynamically order execution of the threads.
Reference: [12] <author> Robert H. Halstead, Jr. </author> <title> Multilisp: A language for concurrent symbolic computation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(4) </volume> <pages> 501-538, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: In attempting to expose parallelism, however, schedulers often end up exposing more parallelism than the computer can actually exploit, and since each active thread requires the use of a certain amount of memory, such schedulers can easily overrun the memory capacity of the machine <ref> [8, 12, 14, 30, 34] </ref>. To date, the space requirements of multithreaded computations have been managed with heuristics or not at all [7, 8, 12, 14, 17, 23, 30, 34]. <p> To date, the space requirements of multithreaded computations have been managed with heuristics or not at all <ref> [7, 8, 12, 14, 17, 23, 30, 34] </ref>. In this thesis, we use algorithmic techniques to address the problem of managing storage for multithreaded This thesis describes joint work with Charles E. Leiserson. 9 10 Chapter 1. Introduction computations. <p> For example, many languages express parallelism with the future construct <ref> [12, 15, 21] </ref>. 37 The expression (future X), where X is an arbitrary expression, creates a task to evaluate X and also creates an object known as a future to eventually hold the value of X. When created, the future is in an unresolved, or undetermined, state. <p> Scheduling nonstrict, depth-first multithreaded computations Chapter 8 Related work Storage management for multithreaded computations has been a concern for a number of years. In 1985, Halstead <ref> [12] </ref> described this problem. A classical difficulty for concurrent architectures occurs when there is too much parallelism in the program being executed. <p> Halstead, in completing the quoted paragraph 74 Chapter 8. Related work above, made the following observation: Ideally, parallel tasks should be created until the processing power of the parallel machine is fully utilized (we may call this saturation) and then execution within each task should become sequential. <ref> [12] </ref> To emulate this ideal behavior, Halstead considered an unfair scheduling policy. When a processor executes a thread that spawns a child, the processor places the parent thread into a local LIFO pending queue and begins work on the child thread.
Reference: [13] <author> Robert H. Halstead, Jr. and Tetsuya Fujita. MASA: </author> <title> A multithreaded processor architecture for parallel symbolic computing. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 443-451, </pages> <address> Honolulu, Hawaii, </address> <month> May </month> <year> 1988. </year>
Reference-contexts: At run time, a scheduler employs dataflow concepts to dynamically order execution of the threads. Other systems have schedulers that dynamically order threads based on the availability of data in shared memory multiprocessors <ref> [1, 4, 13] </ref> or on the arrival of messages in message-passing multicomputers [2, 10, 20, 35]. Rapid execution of a multithreaded computation on a parallel computer requires exposing and exploiting parallelism in the computation by keeping enough threads concurrently active to keep the processors of the computer busy.
Reference: [14] <author> Waldemar Horwat. </author> <title> Concurrent Smalltalk on the message-driven processor. </title> <type> Technical Report MIT/AI/TR-1321, </type> <institution> MIT Artificial Intelligence Laboratory, </institution> <month> September </month> <year> 1991. </year> <note> Bibliography 81 </note>
Reference-contexts: In attempting to expose parallelism, however, schedulers often end up exposing more parallelism than the computer can actually exploit, and since each active thread requires the use of a certain amount of memory, such schedulers can easily overrun the memory capacity of the machine <ref> [8, 12, 14, 30, 34] </ref>. To date, the space requirements of multithreaded computations have been managed with heuristics or not at all [7, 8, 12, 14, 17, 23, 30, 34]. <p> To date, the space requirements of multithreaded computations have been managed with heuristics or not at all <ref> [7, 8, 12, 14, 17, 23, 30, 34] </ref>. In this thesis, we use algorithmic techniques to address the problem of managing storage for multithreaded This thesis describes joint work with Charles E. Leiserson. 9 10 Chapter 1. Introduction computations.
Reference: [15] <author> Waldemar Horwat, Andrew A. Chien, and William J. Dally. </author> <title> Experience with CST: </title> <booktitle> Programming and implementation. In Proceedings of the ACM SIGPLAN '89 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 101-109, </pages> <address> Portland, Oregon, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: For example, many languages express parallelism with the future construct <ref> [12, 15, 21] </ref>. 37 The expression (future X), where X is an arbitrary expression, creates a task to evaluate X and also creates an object known as a future to eventually hold the value of X. When created, the future is in an unresolved, or undetermined, state.
Reference: [16] <author> Robert A. </author> <title> Iannucci. Toward a dataflow / von Neumann hybrid architecture. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 131-140, </pages> <address> Honolulu, Hawaii, </address> <month> May </month> <year> 1988. </year> <note> Also: </note> <institution> MIT Laboratory for Computer Science, </institution> <note> Computation Structures Group Memo 275. </note>
Reference-contexts: Introduction In the course of investigating schemes for general purpose MIMD parallel computation, many diverse research groups have converged on multithreading as a dominant paradigm. As an example, modern dataflow systems <ref> [9, 11, 16, 24, 25, 26, 31, 32] </ref> partition the dataflow instructions into fixed groups called threads and arrange the instructions of each thread into a fixed sequential order at compile time. At run time, a scheduler employs dataflow concepts to dynamically order execution of the threads.
Reference: [17] <author> Suresh Jagannathan and Jim Philbin. </author> <title> A customizable substrate for concurrent languages. </title> <booktitle> In Proceedings of the ACM SIGPLAN '92 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 55-67, </pages> <address> San Francisco, California, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: To date, the space requirements of multithreaded computations have been managed with heuristics or not at all <ref> [7, 8, 12, 14, 17, 23, 30, 34] </ref>. In this thesis, we use algorithmic techniques to address the problem of managing storage for multithreaded This thesis describes joint work with Charles E. Leiserson. 9 10 Chapter 1. Introduction computations. <p> Characterizing the performance of Halstead's unfair scheduling policy is even more difficult when we consider time bounds. Though this policy attempts to compute a greedy schedule by allowing idle processors to steal pending threads from other processors, success depends on the thread stealing algorithm. Other researchers <ref> [17, 23, 34] </ref> have considered variants of unfair scheduling, but none have fully developed or analyzed thread stealing algorithms. A multithreaded computation with no data dependency edges is equivalent to a backtrack search problem, and in this context, Zhang [36] actually did develop and analyze a thread stealing algorithm.
Reference: [18] <author> Christos Kaklamanis and Giuseppe Persiano. </author> <title> Branch-and-bound and backtrack search on mesh-connected arrays of processors. </title> <booktitle> In Proceedings of the Fourth Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 118-126, </pages> <address> San Diego, California, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: Though Zhang did not 75 make the observation, his algorithm also demonstrates linear expansion of space: S P (X ) S 1 P . Other researchers <ref> [18, 29] </ref> have considered backtrack search on fixed-connection networks, but their algorithms explore the tree in breadth-first order and consequently demonstrate poor space performance. 76 Chapter 8. Related work Chapter 9 Conclusions The results of this thesis just begin to develop our algorithmic understanding of nonstrictness in multithreaded computations.
Reference: [19] <author> Richard M. Karp and Yanjun Zhang. </author> <title> A randomized parallel branch-and-bound procedure. </title> <booktitle> In Proceedings of the Twentieth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 290-300, </pages> <address> Chicago, Illinois, </address> <month> May </month> <year> 1988. </year>
Reference-contexts: This algorithm, which we call GDF', uses fewer accesses to the global queue while still computing an equally good schedule. To obtain an algorithm that is efficient for large machines, we use the technique of Karp and Zhang <ref> [19] </ref> to replace the global priority queue with P local queues, one for each processor. By combining this technique with a new technique to throttle the execution and thereby maintain 39 to evaluate expressions A and B. <p> Of course, this approach could result in processors with empty queues sitting idle while other processors have large queues. Thus, we require each processor to have some access to non-local queues in order to facilitate some type of load balancing. The technique of Karp and Zhang <ref> [19] </ref> suggests a randomized algorithm in which threads are located in random queues in order to achieve some balance. At the end of this chapter, we show that the naive adoption of this technique does not work.
Reference: [20] <author> Stephen W. Keckler and William J. Dally. </author> <title> Processor coupling: Integrating compile time and runtime scheduling for parallelism. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 202-213, </pages> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: At run time, a scheduler employs dataflow concepts to dynamically order execution of the threads. Other systems have schedulers that dynamically order threads based on the availability of data in shared memory multiprocessors [1, 4, 13] or on the arrival of messages in message-passing multicomputers <ref> [2, 10, 20, 35] </ref>. Rapid execution of a multithreaded computation on a parallel computer requires exposing and exploiting parallelism in the computation by keeping enough threads concurrently active to keep the processors of the computer busy.
Reference: [21] <author> David A. Kranz, Robert H. Halstead, Jr., and Eric Mohr. Mul-T: </author> <title> A high-performance parallel Lisp. </title> <booktitle> In Proceedings of the SIGPLAN '89 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 81-90, </pages> <address> Portland, Oregon, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: For example, many languages express parallelism with the future construct <ref> [12, 15, 21] </ref>. 37 The expression (future X), where X is an arbitrary expression, creates a task to evaluate X and also creates an object known as a future to eventually hold the value of X. When created, the future is in an unresolved, or undetermined, state. <p> Concurrency arises because the expression (future X) returns the future as its value without waiting for the future to resolve. Thus, the computation containing (future X) can proceed concurrently with the evaluation of X. <ref> [21] </ref> Consider the following code fragment: (let ((a (future A)) (b (future B))) (+ C (F a b))) Such a code fragment could appear for example in a Mul-T [21] program. Figure 5.2 (a) illustrates the corresponding multithreaded computation. <p> Thus, the computation containing (future X) can proceed concurrently with the evaluation of X. <ref> [21] </ref> Consider the following code fragment: (let ((a (future A)) (b (future B))) (+ C (F a b))) Such a code fragment could appear for example in a Mul-T [21] program. Figure 5.2 (a) illustrates the corresponding multithreaded computation. In this example, the thread evaluating this code can spawn child threads to evaluate expressions A and B concurrently; to the parent thread, identifiers a and b are futures until they resolve.
Reference: [22] <author> Tom Leighton, Mark Newman, Abhiram G. Ranade, and Eric Schwabe. </author> <title> Dynamic tree embeddings in butterflies and hypercubes. </title> <booktitle> In Proceedings of the 1989 ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 224-234, </pages> <address> Santa Fe, New Mexico, </address> <month> June </month> <year> 1989. </year> <note> 82 Bibliography </note>
Reference-contexts: The work on lazy task creation [23] and the work on dynamic tree embedding <ref> [3, 22] </ref> may provide some pointers in this direction. Of course, an algorithm that removes the lg P factor from the space bound of LDF would be a nice improvement. 77 78 Chapter 9.
Reference: [23] <author> Eric Mohr, David A. Kranz, and Robert H. Halstead, Jr. </author> <title> Lazy task creation: A technique for increasing the granularity of parallel programs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 264-280, </pages> <month> July </month> <year> 1991. </year> <note> Also: </note> <institution> MIT Laboratory for Computer Science Technical Report MIT/LCS/TM-449. </institution>
Reference-contexts: To date, the space requirements of multithreaded computations have been managed with heuristics or not at all <ref> [7, 8, 12, 14, 17, 23, 30, 34] </ref>. In this thesis, we use algorithmic techniques to address the problem of managing storage for multithreaded This thesis describes joint work with Charles E. Leiserson. 9 10 Chapter 1. Introduction computations. <p> Characterizing the performance of Halstead's unfair scheduling policy is even more difficult when we consider time bounds. Though this policy attempts to compute a greedy schedule by allowing idle processors to steal pending threads from other processors, success depends on the thread stealing algorithm. Other researchers <ref> [17, 23, 34] </ref> have considered variants of unfair scheduling, but none have fully developed or analyzed thread stealing algorithms. A multithreaded computation with no data dependency edges is equivalent to a backtrack search problem, and in this context, Zhang [36] actually did develop and analyze a thread stealing algorithm. <p> An algorithm that can keep groups of closely related threads in the same processor or that can exploit specific fixed-connection networks to keep related threads close to one another would alleviate some of the communication costs. The work on lazy task creation <ref> [23] </ref> and the work on dynamic tree embedding [3, 22] may provide some pointers in this direction. Of course, an algorithm that removes the lg P factor from the space bound of LDF would be a nice improvement. 77 78 Chapter 9.
Reference: [24] <author> Rishiyur S. Nikhil and Arvind. </author> <booktitle> Can dataflow subsume von Neumann computing? In Proceedings of the 16th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 262-272, </pages> <address> Jerusalem, Israel, </address> <month> May </month> <year> 1989. </year> <note> Also: </note> <institution> MIT Laboratory for Computer Science, </institution> <note> Computation Structures Group Memo 292. </note>
Reference-contexts: Introduction In the course of investigating schemes for general purpose MIMD parallel computation, many diverse research groups have converged on multithreading as a dominant paradigm. As an example, modern dataflow systems <ref> [9, 11, 16, 24, 25, 26, 31, 32] </ref> partition the dataflow instructions into fixed groups called threads and arrange the instructions of each thread into a fixed sequential order at compile time. At run time, a scheduler employs dataflow concepts to dynamically order execution of the threads.
Reference: [25] <author> Rishiyur S. Nikhil, Gregory M. Papadopoulos, and Arvind. </author> <title> fl T: A multithreaded massively parallel architecture. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 156-167, </pages> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year> <note> Also: </note> <institution> MIT Laboratory for Computer Science, </institution> <note> Computation Structures Group Memo 325-1. </note>
Reference-contexts: Introduction In the course of investigating schemes for general purpose MIMD parallel computation, many diverse research groups have converged on multithreading as a dominant paradigm. As an example, modern dataflow systems <ref> [9, 11, 16, 24, 25, 26, 31, 32] </ref> partition the dataflow instructions into fixed groups called threads and arrange the instructions of each thread into a fixed sequential order at compile time. At run time, a scheduler employs dataflow concepts to dynamically order execution of the threads.
Reference: [26] <author> Gregory M. Papadopoulos and Denneth R. Traub. </author> <title> Multithreading: A revisionist view of dataflow architectures. </title> <booktitle> In Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 342-351, </pages> <address> Toronto, Canada, </address> <month> May </month> <year> 1991. </year> <note> Also: </note> <institution> MIT Laboratory for Computer Science, </institution> <note> Computation Structures Group Memo 330. </note>
Reference-contexts: Introduction In the course of investigating schemes for general purpose MIMD parallel computation, many diverse research groups have converged on multithreading as a dominant paradigm. As an example, modern dataflow systems <ref> [9, 11, 16, 24, 25, 26, 31, 32] </ref> partition the dataflow instructions into fixed groups called threads and arrange the instructions of each thread into a fixed sequential order at compile time. At run time, a scheduler employs dataflow concepts to dynamically order execution of the threads.
Reference: [27] <author> Prabhakar Raghavan. </author> <title> Probabilistic construction of deterministic algorithms: Approximating packing integer programs. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 37(2) </volume> <pages> 130-143, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: To show that for any given iteration t, with high probability, no processor uses more than O (rS 1 ) space for activation frame storage, we use the following result due to Raghavan <ref> [27, Theorem 1] </ref>. 59 Lemma 13 (Raghavan) Let a 1 ; a 2 ; : : : ; a k be reals in (0; 1]. Let 1 ; 2 ; : : : ; k be independent Bernoulli trials, and let = P k i=1 a i i .
Reference: [28] <author> Abhiram Ranade. </author> <title> How to emulate shared memory. </title> <booktitle> In Proceedings of the 28th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 185-194, </pages> <address> Los Angeles, California, </address> <month> October </month> <year> 1987. </year>
Reference-contexts: When the amount of information that needs to be sent with each thread is just some constant amount, then these requirements are met by a hypercube or indirect butterfly using Ranade's algorithm <ref> [28] </ref> to do the routing.
Reference: [29] <author> Abhiram Ranade. </author> <title> Optimal speedup for backtrack search on a butterfly network. </title> <booktitle> In Proceedings of the Third Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 40-48, </pages> <address> Hilton Head, South Carolina, </address> <month> July </month> <year> 1991. </year> <note> Bibliography 83 </note>
Reference-contexts: Though Zhang did not 75 make the observation, his algorithm also demonstrates linear expansion of space: S P (X ) S 1 P . Other researchers <ref> [18, 29] </ref> have considered backtrack search on fixed-connection networks, but their algorithms explore the tree in breadth-first order and consequently demonstrate poor space performance. 76 Chapter 8. Related work Chapter 9 Conclusions The results of this thesis just begin to develop our algorithmic understanding of nonstrictness in multithreaded computations.
Reference: [30] <author> Carlos A. Ruggiero and John Sargeant. </author> <title> Control of parallelism in the Manchester dataflow machine. </title> <booktitle> In Functional Programming Languages and Computer Architecture, number 274 in Lecture Notes in Computer Science, </booktitle> <pages> pages 1-15. </pages> <publisher> Springer-Verlag, </publisher> <year> 1987. </year>
Reference-contexts: In attempting to expose parallelism, however, schedulers often end up exposing more parallelism than the computer can actually exploit, and since each active thread requires the use of a certain amount of memory, such schedulers can easily overrun the memory capacity of the machine <ref> [8, 12, 14, 30, 34] </ref>. To date, the space requirements of multithreaded computations have been managed with heuristics or not at all [7, 8, 12, 14, 17, 23, 30, 34]. <p> To date, the space requirements of multithreaded computations have been managed with heuristics or not at all <ref> [7, 8, 12, 14, 17, 23, 30, 34] </ref>. In this thesis, we use algorithmic techniques to address the problem of managing storage for multithreaded This thesis describes joint work with Charles E. Leiserson. 9 10 Chapter 1. Introduction computations. <p> These heuristics attempt to set the k-bounds so that the exposed parallelism is maximized under the constraint that space 73 usage stays within the machine's capacity. Ruggiero's course-grain throttle technique <ref> [30] </ref> makes storage allocation decisions based on overall machine activity at run-time. When a process (thread) wants to spawn a child, it must request an activation name from the resource management system.
Reference: [31] <author> Mitsuhisa Sato, Yuetsu Kodama, Shuichi Sakai, Yoshinori Yamaguchi, and Yasuhito Koumura. </author> <title> Thread-based programming for the EM-4 hybrid dataflow machine. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 146-155, </pages> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: Introduction In the course of investigating schemes for general purpose MIMD parallel computation, many diverse research groups have converged on multithreading as a dominant paradigm. As an example, modern dataflow systems <ref> [9, 11, 16, 24, 25, 26, 31, 32] </ref> partition the dataflow instructions into fixed groups called threads and arrange the instructions of each thread into a fixed sequential order at compile time. At run time, a scheduler employs dataflow concepts to dynamically order execution of the threads.
Reference: [32] <author> Kenneth R. Traub. </author> <title> Sequential Implementation of Lenient Programming Languages. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, </institution> <year> 1988. </year> <note> Available as MIT Laboratory for Computer Science Technical Report MIT/LCS/TR-417. </note>
Reference-contexts: Introduction In the course of investigating schemes for general purpose MIMD parallel computation, many diverse research groups have converged on multithreading as a dominant paradigm. As an example, modern dataflow systems <ref> [9, 11, 16, 24, 25, 26, 31, 32] </ref> partition the dataflow instructions into fixed groups called threads and arrange the instructions of each thread into a fixed sequential order at compile time. At run time, a scheduler employs dataflow concepts to dynamically order execution of the threads.
Reference: [33] <author> Leslie G. Valiant. </author> <title> A bridging model for parallel computation. </title> <journal> Communications of the ACM, </journal> <volume> 33(8) </volume> <pages> 103-111, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: Distributed scheduling algorithms We can view the lg P factors in the space bound and the average available parallelism required to achieve linear speedup as the computational slack required by Valiant's bulk-synchronous model <ref> [33] </ref>. The space bound S P (X ) = O (S 1 P lg P ) indicates that Algorithm LDF (6 lg P ) requires memory to scale sufficiently to allow each physical processor enough space to simulate lg P virtual processors.
Reference: [34] <author> Mark T. Vandevoorde and Eric S. Roberts. WorkCrews: </author> <title> An abstraction for controlling parallelism. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 17(4) </volume> <pages> 347-366, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: In attempting to expose parallelism, however, schedulers often end up exposing more parallelism than the computer can actually exploit, and since each active thread requires the use of a certain amount of memory, such schedulers can easily overrun the memory capacity of the machine <ref> [8, 12, 14, 30, 34] </ref>. To date, the space requirements of multithreaded computations have been managed with heuristics or not at all [7, 8, 12, 14, 17, 23, 30, 34]. <p> To date, the space requirements of multithreaded computations have been managed with heuristics or not at all <ref> [7, 8, 12, 14, 17, 23, 30, 34] </ref>. In this thesis, we use algorithmic techniques to address the problem of managing storage for multithreaded This thesis describes joint work with Charles E. Leiserson. 9 10 Chapter 1. Introduction computations. <p> Characterizing the performance of Halstead's unfair scheduling policy is even more difficult when we consider time bounds. Though this policy attempts to compute a greedy schedule by allowing idle processors to steal pending threads from other processors, success depends on the thread stealing algorithm. Other researchers <ref> [17, 23, 34] </ref> have considered variants of unfair scheduling, but none have fully developed or analyzed thread stealing algorithms. A multithreaded computation with no data dependency edges is equivalent to a backtrack search problem, and in this context, Zhang [36] actually did develop and analyze a thread stealing algorithm.
Reference: [35] <author> Thorsten von Eicken, David E. Culler, Seth Copen Goldstein, and Klaus Erik Schauser. </author> <title> Active messages: a mechanism for integrated communication and computation. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 256-266, </pages> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: At run time, a scheduler employs dataflow concepts to dynamically order execution of the threads. Other systems have schedulers that dynamically order threads based on the availability of data in shared memory multiprocessors [1, 4, 13] or on the arrival of messages in message-passing multicomputers <ref> [2, 10, 20, 35] </ref>. Rapid execution of a multithreaded computation on a parallel computer requires exposing and exploiting parallelism in the computation by keeping enough threads concurrently active to keep the processors of the computer busy.
Reference: [36] <author> Yanjun Zhang. </author> <title> Parallel Algorithms for Combinatorial Search Problems. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering and Computer Science, University of California at Berkeley, </institution> <month> November </month> <year> 1989. </year> <institution> Available as University of California at Berkeley, Computer Science Division, </institution> <type> Technical Report UCB/CSD 89/543. </type>
Reference-contexts: Other researchers [17, 23, 34] have considered variants of unfair scheduling, but none have fully developed or analyzed thread stealing algorithms. A multithreaded computation with no data dependency edges is equivalent to a backtrack search problem, and in this context, Zhang <ref> [36] </ref> actually did develop and analyze a thread stealing algorithm.
References-found: 36

