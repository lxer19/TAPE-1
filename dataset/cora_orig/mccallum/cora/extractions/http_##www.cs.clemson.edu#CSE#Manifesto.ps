URL: http://www.cs.clemson.edu/CSE/Manifesto.ps
Refering-URL: http://www.cs.clemson.edu/CSE/index.html
Root-URL: http://www.cs.clemson.edu
Title: A Computational Science Manifesto  
Author: D. E. Stevenson 
Note: Computational Science.  
Date: June 1, 1994  
Address: Clemson, SC 29634-1906  
Affiliation: Department of Computer Science Clemson University  
Abstract: This is a "living document" which represents an ongoing project in the development of 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Claude Brezinski. </author> <title> Convergence acceleration methods: the past decade. </title> <journal> Journal of Computational and Applied Mathematics, </journal> <volume> 12 & 13 </volume> <pages> 19-36, </pages> <year> 1985. </year>
Reference-contexts: Hence, interpolation is all there is. 12 * We want algorithms which are parameterized to architectural considerations. * Asymptotic expansions seem to be key. * Aiken and Neville style algorithms seem to be key <ref> [1] </ref> as well. The method seems to be 1. Let M P (h) be a discretization method which "solves problem P ". 2.
Reference: [2] <author> T. J. I'a Bromwich. </author> <title> An introduction to the theory of infinite series. </title> <publisher> Macmillan, </publisher> <address> 2 edition, </address> <year> 1926. </year> <note> Cited in Olver for historical items; see section 104. </note>
Reference-contexts: Interpolation 3 is the foundation of extrapolation. 6.1 Historical Aspect In particular, what is the relationship of Gregory tables (forward and backward differencing) and passing to the limit. Extrapolation is connected to Neville's Algorithm [10] and asymptotic expansions [5]. From [5], a bit of history (more in <ref> [2] </ref>). Infinite series were taken over by Cauchy, et al. to put onto a firm footing. Asymptotic series had to go because they diverge.
Reference: [3] <author> G. C. Fox. </author> <title> computing comes of age: Supercomputing level parallel computations at caltech. </title> <journal> Concurrency: Pract. and Exp., </journal> <volume> 1(1):94, </volume> <month> Sept. </month> <year> 1989. </year>
Reference: [4] <author> B. McNamara. </author> <title> The mass market for supercomputing. </title> <institution> Int. J. Supercomput. Appli., 1(4):4, </institution> <year> 1989. </year>
Reference: [5] <author> F. W. J. Olver. </author> <title> Asymptotics and Special Functions. </title> <publisher> Academic Press, </publisher> <year> 1974. </year>
Reference-contexts: Fundamental Tenet: Extrapolation is to computation as limits are to analysis. Interpolation 3 is the foundation of extrapolation. 6.1 Historical Aspect In particular, what is the relationship of Gregory tables (forward and backward differencing) and passing to the limit. Extrapolation is connected to Neville's Algorithm [10] and asymptotic expansions <ref> [5] </ref>. From [5], a bit of history (more in [2]). Infinite series were taken over by Cauchy, et al. to put onto a firm footing. Asymptotic series had to go because they diverge. <p> Interpolation 3 is the foundation of extrapolation. 6.1 Historical Aspect In particular, what is the relationship of Gregory tables (forward and backward differencing) and passing to the limit. Extrapolation is connected to Neville's Algorithm [10] and asymptotic expansions <ref> [5] </ref>. From [5], a bit of history (more in [2]). Infinite series were taken over by Cauchy, et al. to put onto a firm footing. Asymptotic series had to go because they diverge.
Reference: [6] <author> C. M. Pancake. </author> <title> Software support for parallel computing: Where are we headed? CACM, </title> <booktitle> 34(11) </booktitle> <pages> 53-64, </pages> <year> 1991. </year>
Reference-contexts: Hence, we take software issues as being dominate in this discussion. The general headings seem to be [ Following <ref> [6] </ref>] following general issues: 1. General Concerns [6] * No capital available for developing generic and meta tools [8] * User Expectations [6] * Instability of the manufacturing base and rapid change imply necessity for portability.[6] * Users don't want to program [6] * We don't know how people want to <p> Hence, we take software issues as being dominate in this discussion. The general headings seem to be [ Following <ref> [6] </ref>] following general issues: 1. General Concerns [6] * No capital available for developing generic and meta tools [8] * User Expectations [6] * Instability of the manufacturing base and rapid change imply necessity for portability.[6] * Users don't want to program [6] * We don't know how people want to use the machine and low experience levels <p> Hence, we take software issues as being dominate in this discussion. The general headings seem to be [ Following <ref> [6] </ref>] following general issues: 1. General Concerns [6] * No capital available for developing generic and meta tools [8] * User Expectations [6] * Instability of the manufacturing base and rapid change imply necessity for portability.[6] * Users don't want to program [6] * We don't know how people want to use the machine and low experience levels of [6] users and systems people alike.[6] * Compilability [6] * Performance versus correctness and <p> The general headings seem to be [ Following <ref> [6] </ref>] following general issues: 1. General Concerns [6] * No capital available for developing generic and meta tools [8] * User Expectations [6] * Instability of the manufacturing base and rapid change imply necessity for portability.[6] * Users don't want to program [6] * We don't know how people want to use the machine and low experience levels of [6] users and systems people alike.[6] * Compilability [6] * Performance versus correctness and stability [6] 13 * Performance versus design and programming costs [6] * Performance versus scalability [6] * Low experience base <p> No capital available for developing generic and meta tools [8] * User Expectations <ref> [6] </ref> * Instability of the manufacturing base and rapid change imply necessity for portability.[6] * Users don't want to program [6] * We don't know how people want to use the machine and low experience levels of [6] users and systems people alike.[6] * Compilability [6] * Performance versus correctness and stability [6] 13 * Performance versus design and programming costs [6] * Performance versus scalability [6] * Low experience base [6] 2. Human Machine Interface [6] * Cutting cost and design turn around time are keys to <p> tools [8] * User Expectations <ref> [6] </ref> * Instability of the manufacturing base and rapid change imply necessity for portability.[6] * Users don't want to program [6] * We don't know how people want to use the machine and low experience levels of [6] users and systems people alike.[6] * Compilability [6] * Performance versus correctness and stability [6] 13 * Performance versus design and programming costs [6] * Performance versus scalability [6] * Low experience base [6] 2. Human Machine Interface [6] * Cutting cost and design turn around time are keys to success [6] * Software maintenance in face of <p> Instability of the manufacturing base and rapid change imply necessity for portability.<ref> [6] </ref> * Users don't want to program [6] * We don't know how people want to use the machine and low experience levels of [6] users and systems people alike.[6] * Compilability [6] * Performance versus correctness and stability [6] 13 * Performance versus design and programming costs [6] * Performance versus scalability [6] * Low experience base [6] 2. Human Machine Interface [6] * Cutting cost and design turn around time are keys to success [6] * Software maintenance in face of portability requirement.[6] * We don't understand how <p> necessity for portability.<ref> [6] </ref> * Users don't want to program [6] * We don't know how people want to use the machine and low experience levels of [6] users and systems people alike.[6] * Compilability [6] * Performance versus correctness and stability [6] 13 * Performance versus design and programming costs [6] * Performance versus scalability [6] * Low experience base [6] 2. Human Machine Interface [6] * Cutting cost and design turn around time are keys to success [6] * Software maintenance in face of portability requirement.[6] * We don't understand how to program parallel machines [6] * Nondeterministic bugs and <p> don't want to program <ref> [6] </ref> * We don't know how people want to use the machine and low experience levels of [6] users and systems people alike.[6] * Compilability [6] * Performance versus correctness and stability [6] 13 * Performance versus design and programming costs [6] * Performance versus scalability [6] * Low experience base [6] 2. Human Machine Interface [6] * Cutting cost and design turn around time are keys to success [6] * Software maintenance in face of portability requirement.[6] * We don't understand how to program parallel machines [6] * Nondeterministic bugs and debugging [6] * Visual understanding <p> * We don't know how people want to use the machine and low experience levels of <ref> [6] </ref> users and systems people alike.[6] * Compilability [6] * Performance versus correctness and stability [6] 13 * Performance versus design and programming costs [6] * Performance versus scalability [6] * Low experience base [6] 2. Human Machine Interface [6] * Cutting cost and design turn around time are keys to success [6] * Software maintenance in face of portability requirement.[6] * We don't understand how to program parallel machines [6] * Nondeterministic bugs and debugging [6] * Visual understanding of algorithms lost in programming <p> people want to use the machine and low experience levels of <ref> [6] </ref> users and systems people alike.[6] * Compilability [6] * Performance versus correctness and stability [6] 13 * Performance versus design and programming costs [6] * Performance versus scalability [6] * Low experience base [6] 2. Human Machine Interface [6] * Cutting cost and design turn around time are keys to success [6] * Software maintenance in face of portability requirement.[6] * We don't understand how to program parallel machines [6] * Nondeterministic bugs and debugging [6] * Visual understanding of algorithms lost in programming [6] * Time spent in <p> and systems people alike.<ref> [6] </ref> * Compilability [6] * Performance versus correctness and stability [6] 13 * Performance versus design and programming costs [6] * Performance versus scalability [6] * Low experience base [6] 2. Human Machine Interface [6] * Cutting cost and design turn around time are keys to success [6] * Software maintenance in face of portability requirement.[6] * We don't understand how to program parallel machines [6] * Nondeterministic bugs and debugging [6] * Visual understanding of algorithms lost in programming [6] * Time spent in optimization might be "pessimization."[6] * Tools [6] Ease of use.[6] Designing for ease <p> design and programming costs <ref> [6] </ref> * Performance versus scalability [6] * Low experience base [6] 2. Human Machine Interface [6] * Cutting cost and design turn around time are keys to success [6] * Software maintenance in face of portability requirement.[6] * We don't understand how to program parallel machines [6] * Nondeterministic bugs and debugging [6] * Visual understanding of algorithms lost in programming [6] * Time spent in optimization might be "pessimization."[6] * Tools [6] Ease of use.[6] Designing for ease of use. "Familiarity often outweighs technical advantages: : : "[6] [4].[6] Vectoring compilers won't do the trick [6] <p> Performance versus scalability <ref> [6] </ref> * Low experience base [6] 2. Human Machine Interface [6] * Cutting cost and design turn around time are keys to success [6] * Software maintenance in face of portability requirement.[6] * We don't understand how to program parallel machines [6] * Nondeterministic bugs and debugging [6] * Visual understanding of algorithms lost in programming [6] * Time spent in optimization might be "pessimization."[6] * Tools [6] Ease of use.[6] Designing for ease of use. "Familiarity often outweighs technical advantages: : : "[6] [4].[6] Vectoring compilers won't do the trick [6] Need domain specific tools [6] - <p> Human Machine Interface <ref> [6] </ref> * Cutting cost and design turn around time are keys to success [6] * Software maintenance in face of portability requirement.[6] * We don't understand how to program parallel machines [6] * Nondeterministic bugs and debugging [6] * Visual understanding of algorithms lost in programming [6] * Time spent in optimization might be "pessimization."[6] * Tools [6] Ease of use.[6] Designing for ease of use. "Familiarity often outweighs technical advantages: : : "[6] [4].[6] Vectoring compilers won't do the trick [6] Need domain specific tools [6] - Meta tools needed [6] - Heterogenuous environment tools [6] <p> around time are keys to success <ref> [6] </ref> * Software maintenance in face of portability requirement.[6] * We don't understand how to program parallel machines [6] * Nondeterministic bugs and debugging [6] * Visual understanding of algorithms lost in programming [6] * Time spent in optimization might be "pessimization."[6] * Tools [6] Ease of use.[6] Designing for ease of use. "Familiarity often outweighs technical advantages: : : "[6] [4].[6] Vectoring compilers won't do the trick [6] Need domain specific tools [6] - Meta tools needed [6] - Heterogenuous environment tools [6] Visualization. <p> <ref> [6] </ref> * Nondeterministic bugs and debugging [6] * Visual understanding of algorithms lost in programming [6] * Time spent in optimization might be "pessimization."[6] * Tools [6] Ease of use.[6] Designing for ease of use. "Familiarity often outweighs technical advantages: : : "[6] [4].[6] Vectoring compilers won't do the trick [6] Need domain specific tools [6] - Meta tools needed [6] - Heterogenuous environment tools [6] Visualization. Lack of graphics based tools, etc and reliance on text oriented things [6]. <p> debugging <ref> [6] </ref> * Visual understanding of algorithms lost in programming [6] * Time spent in optimization might be "pessimization."[6] * Tools [6] Ease of use.[6] Designing for ease of use. "Familiarity often outweighs technical advantages: : : "[6] [4].[6] Vectoring compilers won't do the trick [6] Need domain specific tools [6] - Meta tools needed [6] - Heterogenuous environment tools [6] Visualization. Lack of graphics based tools, etc and reliance on text oriented things [6]. <p> of algorithms lost in programming <ref> [6] </ref> * Time spent in optimization might be "pessimization."[6] * Tools [6] Ease of use.[6] Designing for ease of use. "Familiarity often outweighs technical advantages: : : "[6] [4].[6] Vectoring compilers won't do the trick [6] Need domain specific tools [6] - Meta tools needed [6] - Heterogenuous environment tools [6] Visualization. Lack of graphics based tools, etc and reliance on text oriented things [6]. Unsatisfactory state of representation of three dimensional objects [8] (may be dated, but probably not.) Lack of mature geometric language [8] Move to graphic languages and or audio techniques. virtual reality?[6] <p> <ref> [6] </ref> * Time spent in optimization might be "pessimization."[6] * Tools [6] Ease of use.[6] Designing for ease of use. "Familiarity often outweighs technical advantages: : : "[6] [4].[6] Vectoring compilers won't do the trick [6] Need domain specific tools [6] - Meta tools needed [6] - Heterogenuous environment tools [6] Visualization. Lack of graphics based tools, etc and reliance on text oriented things [6]. Unsatisfactory state of representation of three dimensional objects [8] (may be dated, but probably not.) Lack of mature geometric language [8] Move to graphic languages and or audio techniques. virtual reality?[6] 3. Algorithms [6] * Non-deterministic <p> use.<ref> [6] </ref> Designing for ease of use. "Familiarity often outweighs technical advantages: : : "[6] [4].[6] Vectoring compilers won't do the trick [6] Need domain specific tools [6] - Meta tools needed [6] - Heterogenuous environment tools [6] Visualization. Lack of graphics based tools, etc and reliance on text oriented things [6]. Unsatisfactory state of representation of three dimensional objects [8] (may be dated, but probably not.) Lack of mature geometric language [8] Move to graphic languages and or audio techniques. virtual reality?[6] 3. Algorithms [6] * Non-deterministic behavior and bugs [6] * Discretizations as a factor in algorithms [8] * Performance <p> environment tools <ref> [6] </ref> Visualization. Lack of graphics based tools, etc and reliance on text oriented things [6]. Unsatisfactory state of representation of three dimensional objects [8] (may be dated, but probably not.) Lack of mature geometric language [8] Move to graphic languages and or audio techniques. virtual reality?[6] 3. Algorithms [6] * Non-deterministic behavior and bugs [6] * Discretizations as a factor in algorithms [8] * Performance measuring technology is lacking [8] * Statistical tests for measurements lacking. Need test for unknown populations [8]. * Optimization [6] * Adaptive or polyalgorithms algorithms needed [8] * Reliability [6] * Efficiency [6] * <p> graphics based tools, etc and reliance on text oriented things <ref> [6] </ref>. Unsatisfactory state of representation of three dimensional objects [8] (may be dated, but probably not.) Lack of mature geometric language [8] Move to graphic languages and or audio techniques. virtual reality?[6] 3. Algorithms [6] * Non-deterministic behavior and bugs [6] * Discretizations as a factor in algorithms [8] * Performance measuring technology is lacking [8] * Statistical tests for measurements lacking. Need test for unknown populations [8]. * Optimization [6] * Adaptive or polyalgorithms algorithms needed [8] * Reliability [6] * Efficiency [6] * Algorithm-architecture pairs.[6] * Correctness [6] * <p> language [8] Move to graphic languages and or audio techniques. virtual reality?<ref> [6] </ref> 3. Algorithms [6] * Non-deterministic behavior and bugs [6] * Discretizations as a factor in algorithms [8] * Performance measuring technology is lacking [8] * Statistical tests for measurements lacking. Need test for unknown populations [8]. * Optimization [6] * Adaptive or polyalgorithms algorithms needed [8] * Reliability [6] * Efficiency [6] * Algorithm-architecture pairs.[6] * Correctness [6] * Robustness [6] * Speedup [6] * Scaling of problem parameterized to number of processors [6, 8]. * Making behavior obvious [6] * Making critical path evident [6] 14 * Performance <p> virtual reality?<ref> [6] </ref> 3. Algorithms [6] * Non-deterministic behavior and bugs [6] * Discretizations as a factor in algorithms [8] * Performance measuring technology is lacking [8] * Statistical tests for measurements lacking. Need test for unknown populations [8]. * Optimization [6] * Adaptive or polyalgorithms algorithms needed [8] * Reliability [6] * Efficiency [6] * Algorithm-architecture pairs.[6] * Correctness [6] * Robustness [6] * Speedup [6] * Scaling of problem parameterized to number of processors [6, 8]. * Making behavior obvious [6] * Making critical path evident [6] 14 * Performance prediction [6] * Scalability predictions [6] Problems scale differently; algorithms <p> Algorithms <ref> [6] </ref> * Non-deterministic behavior and bugs [6] * Discretizations as a factor in algorithms [8] * Performance measuring technology is lacking [8] * Statistical tests for measurements lacking. Need test for unknown populations [8]. * Optimization [6] * Adaptive or polyalgorithms algorithms needed [8] * Reliability [6] * Efficiency [6] * Algorithm-architecture pairs.[6] * Correctness [6] * Robustness [6] * Speedup [6] * Scaling of problem parameterized to number of processors [6, 8]. * Making behavior obvious [6] * Making critical path evident [6] 14 * Performance prediction [6] * Scalability predictions [6] Problems scale differently; algorithms scale differently. <p> bugs <ref> [6] </ref> * Discretizations as a factor in algorithms [8] * Performance measuring technology is lacking [8] * Statistical tests for measurements lacking. Need test for unknown populations [8]. * Optimization [6] * Adaptive or polyalgorithms algorithms needed [8] * Reliability [6] * Efficiency [6] * Algorithm-architecture pairs.[6] * Correctness [6] * Robustness [6] * Speedup [6] * Scaling of problem parameterized to number of processors [6, 8]. * Making behavior obvious [6] * Making critical path evident [6] 14 * Performance prediction [6] * Scalability predictions [6] Problems scale differently; algorithms scale differently. Snir "Phenomenon of algorithm [6] discontinuities" [6]. <p> Need test for unknown populations [8]. * Optimization <ref> [6] </ref> * Adaptive or polyalgorithms algorithms needed [8] * Reliability [6] * Efficiency [6] * Algorithm-architecture pairs.[6] * Correctness [6] * Robustness [6] * Speedup [6] * Scaling of problem parameterized to number of processors [6, 8]. * Making behavior obvious [6] * Making critical path evident [6] 14 * Performance prediction [6] * Scalability predictions [6] Problems scale differently; algorithms scale differently. Snir "Phenomenon of algorithm [6] discontinuities" [6]. <p> Need test for unknown populations [8]. * Optimization <ref> [6] </ref> * Adaptive or polyalgorithms algorithms needed [8] * Reliability [6] * Efficiency [6] * Algorithm-architecture pairs.[6] * Correctness [6] * Robustness [6] * Speedup [6] * Scaling of problem parameterized to number of processors [6, 8]. * Making behavior obvious [6] * Making critical path evident [6] 14 * Performance prediction [6] * Scalability predictions [6] Problems scale differently; algorithms scale differently. Snir "Phenomenon of algorithm [6] discontinuities" [6]. Performance predictability and scalability prediction [6] <p> Need test for unknown populations [8]. * Optimization [6] * Adaptive or polyalgorithms algorithms needed [8] * Reliability [6] * Efficiency [6] * Algorithm-architecture pairs.[6] * Correctness [6] * Robustness [6] * Speedup [6] * Scaling of problem parameterized to number of processors <ref> [6, 8] </ref>. * Making behavior obvious [6] * Making critical path evident [6] 14 * Performance prediction [6] * Scalability predictions [6] Problems scale differently; algorithms scale differently. Snir "Phenomenon of algorithm [6] discontinuities" [6]. Performance predictability and scalability prediction [6] Scalability of processors and problem size.[6] 4. <p> Need test for unknown populations [8]. * Optimization <ref> [6] </ref> * Adaptive or polyalgorithms algorithms needed [8] * Reliability [6] * Efficiency [6] * Algorithm-architecture pairs.[6] * Correctness [6] * Robustness [6] * Speedup [6] * Scaling of problem parameterized to number of processors [6, 8]. * Making behavior obvious [6] * Making critical path evident [6] 14 * Performance prediction [6] * Scalability predictions [6] Problems scale differently; algorithms scale differently. Snir "Phenomenon of algorithm [6] discontinuities" [6]. Performance predictability and scalability prediction [6] Scalability of processors and problem size.[6] 4. Programming Paradigm [6] * Geometry critical part of problem <p> [8]. * Optimization <ref> [6] </ref> * Adaptive or polyalgorithms algorithms needed [8] * Reliability [6] * Efficiency [6] * Algorithm-architecture pairs.[6] * Correctness [6] * Robustness [6] * Speedup [6] * Scaling of problem parameterized to number of processors [6, 8]. * Making behavior obvious [6] * Making critical path evident [6] 14 * Performance prediction [6] * Scalability predictions [6] Problems scale differently; algorithms scale differently. Snir "Phenomenon of algorithm [6] discontinuities" [6]. Performance predictability and scalability prediction [6] Scalability of processors and problem size.[6] 4. Programming Paradigm [6] * Geometry critical part of problem solving in this domain [8] * <p> Adaptive or polyalgorithms algorithms needed [8] * Reliability <ref> [6] </ref> * Efficiency [6] * Algorithm-architecture pairs.[6] * Correctness [6] * Robustness [6] * Speedup [6] * Scaling of problem parameterized to number of processors [6, 8]. * Making behavior obvious [6] * Making critical path evident [6] 14 * Performance prediction [6] * Scalability predictions [6] Problems scale differently; algorithms scale differently. Snir "Phenomenon of algorithm [6] discontinuities" [6]. Performance predictability and scalability prediction [6] Scalability of processors and problem size.[6] 4. Programming Paradigm [6] * Geometry critical part of problem solving in this domain [8] * Computation is data, control, and <p> needed [8] * Reliability <ref> [6] </ref> * Efficiency [6] * Algorithm-architecture pairs.[6] * Correctness [6] * Robustness [6] * Speedup [6] * Scaling of problem parameterized to number of processors [6, 8]. * Making behavior obvious [6] * Making critical path evident [6] 14 * Performance prediction [6] * Scalability predictions [6] Problems scale differently; algorithms scale differently. Snir "Phenomenon of algorithm [6] discontinuities" [6]. Performance predictability and scalability prediction [6] Scalability of processors and problem size.[6] 4. Programming Paradigm [6] * Geometry critical part of problem solving in this domain [8] * Computation is data, control, and coordination.[6] * Need heterogeneous <p> * Correctness <ref> [6] </ref> * Robustness [6] * Speedup [6] * Scaling of problem parameterized to number of processors [6, 8]. * Making behavior obvious [6] * Making critical path evident [6] 14 * Performance prediction [6] * Scalability predictions [6] Problems scale differently; algorithms scale differently. Snir "Phenomenon of algorithm [6] discontinuities" [6]. Performance predictability and scalability prediction [6] Scalability of processors and problem size.[6] 4. Programming Paradigm [6] * Geometry critical part of problem solving in this domain [8] * Computation is data, control, and coordination.[6] * Need heterogeneous environment virtual machine [6] * Effort involved in speed-up may lead <p> <ref> [6] </ref> * Robustness [6] * Speedup [6] * Scaling of problem parameterized to number of processors [6, 8]. * Making behavior obvious [6] * Making critical path evident [6] 14 * Performance prediction [6] * Scalability predictions [6] Problems scale differently; algorithms scale differently. Snir "Phenomenon of algorithm [6] discontinuities" [6]. Performance predictability and scalability prediction [6] Scalability of processors and problem size.[6] 4. Programming Paradigm [6] * Geometry critical part of problem solving in this domain [8] * Computation is data, control, and coordination.[6] * Need heterogeneous environment virtual machine [6] * Effort involved in speed-up may lead to no <p> <ref> [6] </ref> * Scaling of problem parameterized to number of processors [6, 8]. * Making behavior obvious [6] * Making critical path evident [6] 14 * Performance prediction [6] * Scalability predictions [6] Problems scale differently; algorithms scale differently. Snir "Phenomenon of algorithm [6] discontinuities" [6]. Performance predictability and scalability prediction [6] Scalability of processors and problem size.[6] 4. Programming Paradigm [6] * Geometry critical part of problem solving in this domain [8] * Computation is data, control, and coordination.[6] * Need heterogeneous environment virtual machine [6] * Effort involved in speed-up may lead to no speedup or worsening performance.[6] * Lack <p> [6, 8]. * Making behavior obvious <ref> [6] </ref> * Making critical path evident [6] 14 * Performance prediction [6] * Scalability predictions [6] Problems scale differently; algorithms scale differently. Snir "Phenomenon of algorithm [6] discontinuities" [6]. Performance predictability and scalability prediction [6] Scalability of processors and problem size.[6] 4. Programming Paradigm [6] * Geometry critical part of problem solving in this domain [8] * Computation is data, control, and coordination.[6] * Need heterogeneous environment virtual machine [6] * Effort involved in speed-up may lead to no speedup or worsening performance.[6] * Lack of experience with parallel algorithms impediment.[6] * "Even 10 years <p> Snir "Phenomenon of algorithm <ref> [6] </ref> discontinuities" [6]. Performance predictability and scalability prediction [6] Scalability of processors and problem size.[6] 4. Programming Paradigm [6] * Geometry critical part of problem solving in this domain [8] * Computation is data, control, and coordination.[6] * Need heterogeneous environment virtual machine [6] * Effort involved in speed-up may lead to no speedup or worsening performance.[6] * Lack of experience with parallel algorithms impediment.[6] * "Even 10 years from now|or maybe forever|a fully automatic parallelizing compiler [6] will not be a practical solution" (Smith,[6]) * Domain specific tools and meta tools for integrating <p> this domain [8] * Computation is data, control, and coordination.<ref> [6] </ref> * Need heterogeneous environment virtual machine [6] * Effort involved in speed-up may lead to no speedup or worsening performance.[6] * Lack of experience with parallel algorithms impediment.[6] * "Even 10 years from now|or maybe forever|a fully automatic parallelizing compiler [6] will not be a practical solution" (Smith,[6]) * Domain specific tools and meta tools for integrating domains.[6] * Need languages which include information about what we're trying to do as well [6] as how it might be done.[6] * Need tools and notations to harness heterogeneous environment.[6] * Behavior of <p> of experience with parallel algorithms impediment.<ref> [6] </ref> * "Even 10 years from now|or maybe forever|a fully automatic parallelizing compiler [6] will not be a practical solution" (Smith,[6]) * Domain specific tools and meta tools for integrating domains.[6] * Need languages which include information about what we're trying to do as well [6] as how it might be done.[6] * Need tools and notations to harness heterogeneous environment.[6] * Behavior of a run is not obvious from program, not is the critical path evident.[6] * Computation and coordination are the issues.[6] * Visual content of parallel algorithm lost in coding.[6] * Scientific programmers <p> and notations to harness heterogeneous environment.[6] * Behavior of a run is not obvious from program, not is the critical path evident.[6] * Computation and coordination are the issues.[6] * Visual content of parallel algorithm lost in coding.[6] * Scientific programmers must get together to determine the operations of interest <ref> [6, 8] </ref> * Automate the algorithmic aspects of undergraduate mathematics [8]. * Communicate in standard mathematical notation and terminology [8]. * There is no universally accepted programming paradigm. "Lack of strong, unified programming [6] model for parallelism" [6]. * Use of software templates to displace programming language design.[6] * "The advantage <p> parallel algorithm lost in coding.<ref> [6] </ref> * Scientific programmers must get together to determine the operations of interest [6, 8] * Automate the algorithmic aspects of undergraduate mathematics [8]. * Communicate in standard mathematical notation and terminology [8]. * There is no universally accepted programming paradigm. "Lack of strong, unified programming [6] model for parallelism" [6]. * Use of software templates to displace programming language design.[6] * "The advantage of a single, flat memory space is that it gives the programmer [6] flexibility: : : "[6]. Snir: "The issue is not shared memory versus distributed [6] memory. <p> coding.<ref> [6] </ref> * Scientific programmers must get together to determine the operations of interest [6, 8] * Automate the algorithmic aspects of undergraduate mathematics [8]. * Communicate in standard mathematical notation and terminology [8]. * There is no universally accepted programming paradigm. "Lack of strong, unified programming [6] model for parallelism" [6]. * Use of software templates to displace programming language design.[6] * "The advantage of a single, flat memory space is that it gives the programmer [6] flexibility: : : "[6]. Snir: "The issue is not shared memory versus distributed [6] memory. <p> in standard mathematical notation and terminology [8]. * There is no universally accepted programming paradigm. "Lack of strong, unified programming <ref> [6] </ref> model for parallelism" [6]. * Use of software templates to displace programming language design.[6] * "The advantage of a single, flat memory space is that it gives the programmer [6] flexibility: : : "[6]. Snir: "The issue is not shared memory versus distributed [6] memory. The issue is whether you can or cannot ignore communications as affecting performance."[6] [6]. * A possible way to go is multiple levels separating computational framework from the [6] data distribution and processor control.[6] * <p> paradigm. "Lack of strong, unified programming <ref> [6] </ref> model for parallelism" [6]. * Use of software templates to displace programming language design.[6] * "The advantage of a single, flat memory space is that it gives the programmer [6] flexibility: : : "[6]. Snir: "The issue is not shared memory versus distributed [6] memory. The issue is whether you can or cannot ignore communications as affecting performance."[6] [6]. * A possible way to go is multiple levels separating computational framework from the [6] data distribution and processor control.[6] * Both control flow and data flow are important.[6] 5. Architectural Dependancies and Intrusions [6] <p> templates to displace programming language design.<ref> [6] </ref> * "The advantage of a single, flat memory space is that it gives the programmer [6] flexibility: : : "[6]. Snir: "The issue is not shared memory versus distributed [6] memory. The issue is whether you can or cannot ignore communications as affecting performance."[6] [6]. * A possible way to go is multiple levels separating computational framework from the [6] data distribution and processor control.[6] * Both control flow and data flow are important.[6] 5. Architectural Dependancies and Intrusions [6] * Virtual Machine [6] * Supercomputer, workstation, visualization engine must cooperate.[6] * Machine dependent numerical <p> is that it gives the programmer <ref> [6] </ref> flexibility: : : "[6]. Snir: "The issue is not shared memory versus distributed [6] memory. The issue is whether you can or cannot ignore communications as affecting performance."[6] [6]. * A possible way to go is multiple levels separating computational framework from the [6] data distribution and processor control.[6] * Both control flow and data flow are important.[6] 5. Architectural Dependancies and Intrusions [6] * Virtual Machine [6] * Supercomputer, workstation, visualization engine must cooperate.[6] * Machine dependent numerical techniques.[6] * Language dependent numerical techniques.[6] 15 * Architectural interference with design.[6] * Loss of <p> <ref> [6] </ref> memory. The issue is whether you can or cannot ignore communications as affecting performance."[6] [6]. * A possible way to go is multiple levels separating computational framework from the [6] data distribution and processor control.[6] * Both control flow and data flow are important.[6] 5. Architectural Dependancies and Intrusions [6] * Virtual Machine [6] * Supercomputer, workstation, visualization engine must cooperate.[6] * Machine dependent numerical techniques.[6] * Language dependent numerical techniques.[6] 15 * Architectural interference with design.[6] * Loss of understanding of the code due to transformations during parallelization,[6] vec torization, optimization. And it really doesn't help [3][6] * There <p> is whether you can or cannot ignore communications as affecting performance."<ref> [6] </ref> [6]. * A possible way to go is multiple levels separating computational framework from the [6] data distribution and processor control.[6] * Both control flow and data flow are important.[6] 5. Architectural Dependancies and Intrusions [6] * Virtual Machine [6] * Supercomputer, workstation, visualization engine must cooperate.[6] * Machine dependent numerical techniques.[6] * Language dependent numerical techniques.[6] 15 * Architectural interference with design.[6] * Loss of understanding of the code due to transformations during parallelization,[6] vec torization, optimization. And it really doesn't help [3][6] * There are several languages: Fortran,
Reference: [7] <author> R. M. Panoff. </author> <booktitle> The carolinas summer institute in computational science, </booktitle> <year> 1991. </year> <title> text of proposal submitted to NSF Undergraduate Faculty Enhancement: </title> <type> NSF 90-112. </type>
Reference-contexts: besides being an essential component of almost every science, is an enabling component of an undergraduate curriculum: effective use of computation in the instruction program allows both the teacher and the student to go beyond the watered-down examples which lend themselves to easy analytic solutions and to tackle real-world problems." <ref> [7] </ref> Obstacles to initiating or incorporating effective CLS programs: * CLS is an emerging discipline. This makes it hard for outsiders to become familiar with the content, focus, promise, and importance. CLS impacts way many sciences are studied.
Reference: [8] <author> John R. Rice. </author> <title> Mathematical aspects of scientific software. </title> <editor> In J. R. Rice, editor, </editor> <booktitle> Mathematical Aspects of Software, volume 14 of Institute for Mathematics and Its Applications (IMA). </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1988. </year>
Reference-contexts: We also must redefine "what it means to prove a result, and what techniques are reliable."<ref> [8] </ref> We see the possibility of a new field: computational analysis [8]: focusing in round-off, conditioning (Frechet derivatives), and algorithm development. We have mathematics with computational uncertainty arising from the data and operations. This uncertainty leads to questions of epistemology: how do we know that a result is right. <p> What is the role of performance in the search for solutions? "Being hard to solve is measured by how much computation an algorithm must do, not by how hard it is to discover the algorithm." <ref> [8] </ref> Asymptotic performance analysis is an example of giving algorithms without understanding problem. Real question might be something like "is this exact instance that I currently must solve have linear behavior?" Information-based complexity theory useful? Asymptotic methods for expressing functions become more important as the number of function evaluations increases. <p> John R. Rice <ref> [8] </ref> "Most theorems that apply to prove correctness of computed results have unverifiable hypotheses."[8]. Now, we see the derivation of descriptions of systems evolving by three separate mathematical forms. Among the foundational issues is that of which metaphysics to adopt: standard, non-standard, intuitionistic, or constructive reals. <p> Fundamental Metaphysical Decision: what view (s) of the (formal) real numbers do we adopt and how do we view the derivation process as it relates to the Fundamental Cri terion. 4.1.3 What are the Objects? Performance theorems <ref> [8] </ref>. Want theorems of the form Consider problem Class P defined by properties p 1 ; p 2 ; : : : : Then algorithm A always solves problems from Class P . The class P must somehow correspond to real problems. <p> The representation of operators in F and the preservation of properties. 6. Rules for justification for the foundations. 4.2 Epistemology Major question: What is knowledge in the computational sciences? A posterior techniques may be in the offing <ref> [8] </ref> Computing multiple solutions using various techniques should be considered [8] Performance as an epistemologic area. See metaphysics of performance (above). Two bases for knowledge: asymptotic algorithms and approximation theory [8] Performance measurement is an epistemologic problem since these have to be run on real computers solving real problems. <p> The representation of operators in F and the preservation of properties. 6. Rules for justification for the foundations. 4.2 Epistemology Major question: What is knowledge in the computational sciences? A posterior techniques may be in the offing <ref> [8] </ref> Computing multiple solutions using various techniques should be considered [8] Performance as an epistemologic area. See metaphysics of performance (above). Two bases for knowledge: asymptotic algorithms and approximation theory [8] Performance measurement is an epistemologic problem since these have to be run on real computers solving real problems. <p> foundations. 4.2 Epistemology Major question: What is knowledge in the computational sciences? A posterior techniques may be in the offing <ref> [8] </ref> Computing multiple solutions using various techniques should be considered [8] Performance as an epistemologic area. See metaphysics of performance (above). Two bases for knowledge: asymptotic algorithms and approximation theory [8] Performance measurement is an epistemologic problem since these have to be run on real computers solving real problems. We need reliable information of the quality of the techniques and programs [8] Performance measurement and evaluation might need to be information-based [11] 4.2.1 Education as Epistemological Function From [8]: (a) Enormous <p> See metaphysics of performance (above). Two bases for knowledge: asymptotic algorithms and approximation theory <ref> [8] </ref> Performance measurement is an epistemologic problem since these have to be run on real computers solving real problems. We need reliable information of the quality of the techniques and programs [8] Performance measurement and evaluation might need to be information-based [11] 4.2.1 Education as Epistemological Function From [8]: (a) Enormous effort is invested in teaching people algorithms. (b) People forget most of the algorithms they learn. (c) Many algorithms of arithmetic, algebra, calculus, linear algebra etc. can be imple mented as <p> approximation theory <ref> [8] </ref> Performance measurement is an epistemologic problem since these have to be run on real computers solving real problems. We need reliable information of the quality of the techniques and programs [8] Performance measurement and evaluation might need to be information-based [11] 4.2.1 Education as Epistemological Function From [8]: (a) Enormous effort is invested in teaching people algorithms. (b) People forget most of the algorithms they learn. (c) Many algorithms of arithmetic, algebra, calculus, linear algebra etc. can be imple mented as scientific software and run on cheap machines. (d) Many educators who expound the virtues of learning algorithms <p> Hence, we take software issues as being dominate in this discussion. The general headings seem to be [ Following [6]] following general issues: 1. General Concerns [6] * No capital available for developing generic and meta tools <ref> [8] </ref> * User Expectations [6] * Instability of the manufacturing base and rapid change imply necessity for portability.[6] * Users don't want to program [6] * We don't know how people want to use the machine and low experience levels of [6] users and systems people alike.[6] * Compilability [6] * <p> Lack of graphics based tools, etc and reliance on text oriented things [6]. Unsatisfactory state of representation of three dimensional objects <ref> [8] </ref> (may be dated, but probably not.) Lack of mature geometric language [8] Move to graphic languages and or audio techniques. virtual reality?[6] 3. Algorithms [6] * Non-deterministic behavior and bugs [6] * Discretizations as a factor in algorithms [8] * Performance measuring technology is lacking [8] * Statistical tests for <p> Lack of graphics based tools, etc and reliance on text oriented things [6]. Unsatisfactory state of representation of three dimensional objects <ref> [8] </ref> (may be dated, but probably not.) Lack of mature geometric language [8] Move to graphic languages and or audio techniques. virtual reality?[6] 3. Algorithms [6] * Non-deterministic behavior and bugs [6] * Discretizations as a factor in algorithms [8] * Performance measuring technology is lacking [8] * Statistical tests for measurements lacking. Need test for unknown populations [8]. * Optimization [6] * <p> Unsatisfactory state of representation of three dimensional objects <ref> [8] </ref> (may be dated, but probably not.) Lack of mature geometric language [8] Move to graphic languages and or audio techniques. virtual reality?[6] 3. Algorithms [6] * Non-deterministic behavior and bugs [6] * Discretizations as a factor in algorithms [8] * Performance measuring technology is lacking [8] * Statistical tests for measurements lacking. Need test for unknown populations [8]. * Optimization [6] * Adaptive or polyalgorithms algorithms needed [8] * Reliability [6] * Efficiency [6] * Algorithm-architecture pairs.[6] * Correctness [6] * Robustness [6] * Speedup [6] * Scaling of <p> of three dimensional objects <ref> [8] </ref> (may be dated, but probably not.) Lack of mature geometric language [8] Move to graphic languages and or audio techniques. virtual reality?[6] 3. Algorithms [6] * Non-deterministic behavior and bugs [6] * Discretizations as a factor in algorithms [8] * Performance measuring technology is lacking [8] * Statistical tests for measurements lacking. Need test for unknown populations [8]. * Optimization [6] * Adaptive or polyalgorithms algorithms needed [8] * Reliability [6] * Efficiency [6] * Algorithm-architecture pairs.[6] * Correctness [6] * Robustness [6] * Speedup [6] * Scaling of problem parameterized to number of processors [6, <p> of mature geometric language <ref> [8] </ref> Move to graphic languages and or audio techniques. virtual reality?[6] 3. Algorithms [6] * Non-deterministic behavior and bugs [6] * Discretizations as a factor in algorithms [8] * Performance measuring technology is lacking [8] * Statistical tests for measurements lacking. Need test for unknown populations [8]. * Optimization [6] * Adaptive or polyalgorithms algorithms needed [8] * Reliability [6] * Efficiency [6] * Algorithm-architecture pairs.[6] * Correctness [6] * Robustness [6] * Speedup [6] * Scaling of problem parameterized to number of processors [6, 8]. * Making behavior obvious [6] * Making critical path evident [6] <p> Algorithms [6] * Non-deterministic behavior and bugs [6] * Discretizations as a factor in algorithms <ref> [8] </ref> * Performance measuring technology is lacking [8] * Statistical tests for measurements lacking. Need test for unknown populations [8]. * Optimization [6] * Adaptive or polyalgorithms algorithms needed [8] * Reliability [6] * Efficiency [6] * Algorithm-architecture pairs.[6] * Correctness [6] * Robustness [6] * Speedup [6] * Scaling of problem parameterized to number of processors [6, 8]. * Making behavior obvious [6] * Making critical path evident [6] 14 * Performance prediction [6] * Scalability predictions [6] Problems <p> Need test for unknown populations [8]. * Optimization [6] * Adaptive or polyalgorithms algorithms needed [8] * Reliability [6] * Efficiency [6] * Algorithm-architecture pairs.[6] * Correctness [6] * Robustness [6] * Speedup [6] * Scaling of problem parameterized to number of processors <ref> [6, 8] </ref>. * Making behavior obvious [6] * Making critical path evident [6] 14 * Performance prediction [6] * Scalability predictions [6] Problems scale differently; algorithms scale differently. Snir "Phenomenon of algorithm [6] discontinuities" [6]. Performance predictability and scalability prediction [6] Scalability of processors and problem size.[6] 4. <p> Snir "Phenomenon of algorithm [6] discontinuities" [6]. Performance predictability and scalability prediction [6] Scalability of processors and problem size.[6] 4. Programming Paradigm [6] * Geometry critical part of problem solving in this domain <ref> [8] </ref> * Computation is data, control, and coordination.[6] * Need heterogeneous environment virtual machine [6] * Effort involved in speed-up may lead to no speedup or worsening performance.[6] * Lack of experience with parallel algorithms impediment.[6] * "Even 10 years from now|or maybe forever|a fully automatic parallelizing compiler [6] will not <p> and notations to harness heterogeneous environment.[6] * Behavior of a run is not obvious from program, not is the critical path evident.[6] * Computation and coordination are the issues.[6] * Visual content of parallel algorithm lost in coding.[6] * Scientific programmers must get together to determine the operations of interest <ref> [6, 8] </ref> * Automate the algorithmic aspects of undergraduate mathematics [8]. * Communicate in standard mathematical notation and terminology [8]. * There is no universally accepted programming paradigm. "Lack of strong, unified programming [6] model for parallelism" [6]. * Use of software templates to displace programming language design.[6] * "The advantage <p> run is not obvious from program, not is the critical path evident.[6] * Computation and coordination are the issues.[6] * Visual content of parallel algorithm lost in coding.[6] * Scientific programmers must get together to determine the operations of interest [6, 8] * Automate the algorithmic aspects of undergraduate mathematics <ref> [8] </ref>. * Communicate in standard mathematical notation and terminology [8]. * There is no universally accepted programming paradigm. "Lack of strong, unified programming [6] model for parallelism" [6]. * Use of software templates to displace programming language design.[6] * "The advantage of a single, flat memory space is that it gives <p> critical path evident.[6] * Computation and coordination are the issues.[6] * Visual content of parallel algorithm lost in coding.[6] * Scientific programmers must get together to determine the operations of interest [6, 8] * Automate the algorithmic aspects of undergraduate mathematics <ref> [8] </ref>. * Communicate in standard mathematical notation and terminology [8]. * There is no universally accepted programming paradigm. "Lack of strong, unified programming [6] model for parallelism" [6]. * Use of software templates to displace programming language design.[6] * "The advantage of a single, flat memory space is that it gives the programmer [6] flexibility: : : "[6].
Reference: [9] <author> H. L. Royden. </author> <title> Real Analysis. </title> <publisher> MacMillan, </publisher> <address> 2 edition, </address> <year> 1968. </year>
Reference-contexts: Even within the foundations of R, there are several problems of foundations which need investigation. the following taken from <ref> [9] </ref> and represent the critical structural basis for real analysis. It seem necessary that computational science must understand how these formal results hold in the computational environment. 1. The number hierarchy from floating point F through the complex numbers C. 2. Membership in sets. 3. Pairs, Cartesian products. 4.
Reference: [10] <author> J. Stoer and R. </author> <title> Bulirsch. Introduction to Numerical Analysis. </title> <publisher> Springer-Verlag, </publisher> <year> 1979. </year> <note> trans. of Einfuhrung in die Numerische Mathematik. </note>
Reference-contexts: Fundamental Tenet: Extrapolation is to computation as limits are to analysis. Interpolation 3 is the foundation of extrapolation. 6.1 Historical Aspect In particular, what is the relationship of Gregory tables (forward and backward differencing) and passing to the limit. Extrapolation is connected to Neville's Algorithm <ref> [10] </ref> and asymptotic expansions [5]. From [5], a bit of history (more in [2]). Infinite series were taken over by Cauchy, et al. to put onto a firm footing. Asymptotic series had to go because they diverge.
Reference: [11] <editor> J. F. Traub, editor. </editor> <booktitle> Symposium on Analytic Computational Complexity. </booktitle> <publisher> Academic Press, </publisher> <year> 1976. </year> <month> 34 </month>
Reference-contexts: We need reliable information of the quality of the techniques and programs [8] Performance measurement and evaluation might need to be information-based <ref> [11] </ref> 4.2.1 Education as Epistemological Function From [8]: (a) Enormous effort is invested in teaching people algorithms. (b) People forget most of the algorithms they learn. (c) Many algorithms of arithmetic, algebra, calculus, linear algebra etc. can be imple mented as scientific software and run on cheap machines. (d) Many educators
References-found: 11

