URL: http://www.cs.duke.edu/~jsv/Papers/NoV93.distr_sorting.ps.gz
Refering-URL: http://www.cs.duke.edu/~jsv/Papers/catalog/node7.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: Email: nodine@mcrc.mot.com.  Email: jsv@cs.duke.edu.  
Title: Deterministic Distribution Sort in Shared and Distributed Memory Multiprocessors (extended abstract)  
Author: Mark H. Nodine Jeffrey Scott Vitter 
Note: Part of this research was done while the author was at Brown University, supported in part by an IBM Graduate Fellowship, by NSF research grants CCR-9007851 and IRI-9116451, and by Army Research Office grant DAAL03-91-G-0035.  Part of this research was done while the author was at Brown University. Support was provided in part by Presidential Young Investigator Award CCR-9047466 with matching funds from IBM, by NSF research grant CCR-9007851, and by Army Research Office grant DAAL03-91-G-0035.  
Address: Cambridge Res. Ctr. One Kendall Square, Bldg. 200 Cambridge, MA 02139  Box 90129 Durham, NC 27708-0129  
Affiliation: Motorola  Dept. of Computer Science Duke University,  
Abstract: We present an elegant deterministic load balancing strategy for distribution sort that is applicable to a wide variety of parallel disks and parallel memory hierarchies with both single and parallel processors. The simplest application of the strategy is an optimal deterministic algorithm for external sorting with multiple disks and parallel processors. In each input/output (I/O) operation, each of the D 1 disks can simultaneously transfer a block of B contiguous records. Our two measures of performance are the number of I/Os and the amount of work done by the CPU(s); our algorithm is simultaneously optimal for both measures. We also show how to sort deterministically in parallel memory hierarchies. When the processors are interconnected by any sort of a PRAM, our algorithms are optimal for all parallel memory hierarchies; when the interconnection network is a hypercube, our algorithms are either optimal or best-known. 
Abstract-found: 1
Intro-found: 1
Reference: [AAC] <author> Alok Aggarwal, Bowen Alpern, Ashok K. Chandra, and Marc Snir, </author> <title> "A Model for Hierarchical Memory," </title> <booktitle> Proceedings of 19th Annual ACM Symposium on Theory of Computing (May 1987), </booktitle> <pages> 305-314. </pages>
Reference-contexts: The difficulty in designing optimal algorithms is dealing with the partitioning of secondary storage into separate disks. 2.2 Parallel multilevel hierarchies The first multilevel hierarchy memory model that we consider is the Hierarchical Memory Model (HMM) proposed by Aggarwal et al. <ref> [AAC] </ref>, depicted in Figure 3a. In the HMM f (x) model, access to memory location x takes f (x) time. Figure 3a suggests the HMM dlog xe model, where each layer in the hierarchy is twice as large as the previous layer. <p> To get optimal performance, we determine the number S of buckets differently depending upon which hierarchical model we are using. Algorithm 2 gives the routine for computing the S 1 partition elements, based on <ref> [AAC, ViSb] </ref>. It works by recursively sorting sets of size N=G and choosing every blog N cth element. The approximate partition elements are selected from this subset. The specific value of G used in the algorithm is dependent upon which hierarchical memory model is being used.
Reference: [ACSa] <author> Alok Aggarwal, Ashok K. Chandra, and Marc Snir, </author> <title> "Hierarchical Memory with Block Transfer," </title> <booktitle> Proceedings of 28th Annual IEEE Symposium on Foundations of Computer Science (October 1987), </booktitle> <pages> 204-216. </pages>
Reference-contexts: Figure 3a can actually be taken as representative of the so-called "well-behaved" cost functions f (x), such as f (x) = x ff , ff &gt; 0. An elaboration of HMM is the Block Transfer (BT) model of Aggarwal et al. <ref> [ACSa] </ref>, depicted schematically in Figure 3b. Like HMM, it has a cost function f (x), but additionally it simulates the effect of block transfer by allowing the ` + 1 locations x, x 1, : : : x ` to be accessed at cost f (x) + `. <p> The only difference is that in Algorithm 1, we need to add another step right after Step (6) to reposition all the buckets into consecutive locations on each virtual memory hierarchy. This repositioning is done on a virtual-hierarchy-by-virtual-hierarchy basis, using the generalized matrix transposition algorithm given in <ref> [ACSa] </ref>. We concentrate in this section on the cost function f (x) = x ff , where 0 &lt; ff &lt; 1. <p> We need to make one more change to the algorithm for BT hierarchies, but one that is hard to write explicitly. Aggarwal et al. gave an algorithm called the "touch" algorithm <ref> [ACSa] </ref>. This algorithm takes an array of n consecutive records stored at the lowest possible level and passes them through the base memory level in order, using time O (n log log n) for 0 &lt; ff &lt; 1.
Reference: [AgV] <author> Alok Aggarwal and Jeffrey Scott Vitter, </author> <title> "The Input/Output Complexity of Sorting and Related Problems," </title> <booktitle> Communications of the ACM 31 (September 1988), </booktitle> <pages> 1116-1127. </pages>
Reference-contexts: An increasingly popular way to get further speedup is to use many disk drives working in parallel [GHK, GiS, Jil, Mag, PGK, Uni]. Aggarwal and Vitter did initial work in the use of parallel block transfer for sorting <ref> [AgV] </ref>, generalizing the sequential work of Floyd [Flo]. <p> Thus, D blocks can be transferred per I/O, as in the <ref> [AgV] </ref> model, but only if no two blocks access the same disk. This assumption is reasonable in view of the way real systems are constructed. Vitter and Shriver presented a randomized version of distribution sort in the D-disk model using two complementary partitioning techniques. <p> This assumption is reasonable in view of the way real systems are constructed. Vitter and Shriver presented a randomized version of distribution sort in the D-disk model using two complementary partitioning techniques. Their algorithm meets the I/O lower bound (1) for the more lenient model of <ref> [AgV] </ref>, and thus it is optimal. The difficulty in implementing distribution sort on a set of D parallel disks is making sure that each bucket can be read efficiently in parallel. <p> We also improve upon the deterministic Greed Sort algorithm in [NoV], which is known to be optimal only for the parallel disk models and not for hierarchical memories. The lower bounds are proved in <ref> [AgV] </ref> (Theorem 1) and [ViSb] (Theorems 2 and 3). <p> get the recurrence T (N ) = &lt; ST S + O (N=DB) if N &gt; M which has solution T (N ) = O N log S M = O N log (N=B) This is the same bound as was shown to be optimal for the parallel disk model <ref> [AgV] </ref>. The tricky part is showing that the internal processors can be used efficiently for any number of PRAM processors P M log minfM=B; log M g= log M (or up to P = M if log M = O (log (M=B))).
Reference: [ACF] <author> Bowen Alpern, Larry Carter, and Ephraim Feig, </author> <title> "Uniform Memory Hierarchies," </title> <booktitle> Proceedings of the 31st Annual IEEE Symposium 9 on Foundations of Computer Science (October 1990), </booktitle> <pages> 600-608. </pages>
Reference-contexts: The P processors are connected by some topology such as a hypercube or an EREW PRAM and their memories collectively have size M . model. (b) The BT model. (c) The UMH model. memory hierarchy is the Uniform Memory Hierarchy (UMH) of Alpern et al. <ref> [ACF] </ref>, depicted in Figure 3c. As with two-level hierarchies, multilevel hierarchies can be parallelized, as shown in Figure 4. The base memory levels of the H hierarchies are attached to H interconnected processors. We assume that the hierarchies are all of the same kind.
Reference: [Arg] <author> Lars Arge, </author> <month> January </month> <year> 1993, </year> <title> private communication. </title>
Reference-contexts: Recently, an alternative definition of auxiliary matrix was proposed that has a similar effect of making each bucket balanced within a factor of 2; the term a bh is defined to be 1 when the number of blocks per bucket is more than twice the desired evenly-balanced number <ref> [Arg] </ref>. After the rebalancing, Step (9) of Algorithm 3 routes any unprocessed virtual blocks into a contiguous region that does not overlap with any of the next v virtual hierarchies to be read. This operation takes time O (log H) by monotone routing [Lei, Section 3.4.3].
Reference: [BFP] <author> Manuel Blum, Robert W. Floyd, Vaughan Pratt, Ronald L. Rivest, and Robert E. Tarjan, </author> <title> "Time Bounds for Selection," </title> <editor> J. </editor> <booktitle> Computer and System Sciences 7 (1973), </booktitle> <pages> 448-461. </pages>
Reference: [Col] <author> Richard Cole, </author> <title> "Parallel Merge Sort," </title> <note> SIAM J. Computing 17 (August 1988), 770-785. </note>
Reference-contexts: To bound the amount of time spent processing each memoryload, we use a variety of techniques including using an algorithm of Rajasekaran and Reif [RaR] as part of a radix sort, Cole's EREW PRAM parallel merge sort <ref> [Col] </ref>, incremental updating, and even/odd partitioning. 6 Conclusions In this paper, we have described the first known deterministic algorithm for sorting optimally using parallel hierarchical memories. This algorithm improves upon the randomized algorithms of Vitter and Shriver [ViSa, ViSb] and the deterministic disk algorithm of Nodine and Vitter [NoV].
Reference: [CyP] <author> Robert Cypher and C. Greg Plaxton, </author> <title> "Deterministic Sorting in Nearly Logarithmic Time on the Hypercube and Related Computers," </title> <note> Journal of Computer and System Sciences (to appear), also appears in Proceedings of the 22nd Annual ACM Symposium on Theory of Computing, </note> <month> (May </month> <year> 1990), </year> <pages> 193-203. </pages>
Reference-contexts: We conjecture that such an approach results in globally balanced buckets with perhaps an even faster implementation. It is conceivable that the Sharesort algorithm of Cypher and Plaxton <ref> [CyP] </ref> may be applicable to parallel disks and parallel memory hierarchies to get an algorithm with performance similar to ours in the big-oh sense. Alternatively, our balancing results may be applicable to their parallel sorting model.
Reference: [Flo] <author> Robert W. Floyd, </author> <title> "Permuting Information in Idealized Two-Level Storage," in Complexity of Computer Computations, </title> <editor> R. Miller and J. Thatcher, ed., </editor> <publisher> Plenum, </publisher> <year> 1972, </year> <pages> 105-109. </pages>
Reference-contexts: An increasingly popular way to get further speedup is to use many disk drives working in parallel [GHK, GiS, Jil, Mag, PGK, Uni]. Aggarwal and Vitter did initial work in the use of parallel block transfer for sorting [AgV], generalizing the sequential work of Floyd <ref> [Flo] </ref>.
Reference: [GHK] <author> Garth Gibson, Lisa Hellerstein, Richard M. Karp, Randy H. Katz, and David A. Patterson, </author> <title> "Coding Techniques for Handling Failures in Large Disk Arrays," </title> <editor> U. C. </editor> <address> Berkeley, UCB/CSD 88/477, </address> <month> December </month> <year> 1988. </year>
Reference-contexts: An increasingly popular way to get further speedup is to use many disk drives working in parallel <ref> [GHK, GiS, Jil, Mag, PGK, Uni] </ref>. Aggarwal and Vitter did initial work in the use of parallel block transfer for sorting [AgV], generalizing the sequential work of Floyd [Flo].
Reference: [GiS] <author> David Gifford and Alfred Spector, </author> <title> "The TWA Reservation System," </title> <booktitle> Communications of the ACM 27 (July 1984), </booktitle> <pages> 650-665. </pages>
Reference-contexts: An increasingly popular way to get further speedup is to use many disk drives working in parallel <ref> [GHK, GiS, Jil, Mag, PGK, Uni] </ref>. Aggarwal and Vitter did initial work in the use of parallel block transfer for sorting [AgV], generalizing the sequential work of Floyd [Flo].
Reference: [GoS] <author> Mark Goldberg and Thomas Spencer, </author> <title> "Constructing a Maximal Independent Set in Parallel," </title> <journal> SIAM J. Discrete Math 2, </journal> <pages> 322-328. </pages>
Reference: [HoK] <author> Jia-Wei Hong and H. T. Kung, </author> <title> "I/O Complexity: The Red-Blue Pebble Game," </title> <booktitle> Proc. of the 13th Annual ACM Symposium on the Theory of Computing (May 1981), </booktitle> <pages> 326-333. </pages>
Reference: [IsS] <author> Amos Israeli and Y. Shiloach, </author> <title> "An Improved Parallel Algorithm for Maximal Matching," </title> <booktitle> Information Processing Letters 22 (1986), </booktitle> <pages> 57-60. </pages>
Reference: [Jil] <author> W. Jilke, </author> <title> "Disk Array Mass Storage Systems: The New Opportunity," </title> <publisher> Amperif Corporation, </publisher> <month> September </month> <year> 1986. </year>
Reference-contexts: An increasingly popular way to get further speedup is to use many disk drives working in parallel <ref> [GHK, GiS, Jil, Mag, PGK, Uni] </ref>. Aggarwal and Vitter did initial work in the use of parallel block transfer for sorting [AgV], generalizing the sequential work of Floyd [Flo].
Reference: [Lei] <author> F. Thomson Leighton, </author> <title> in Introduction to Parallel Algorithms and Architectures, </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1992. </year>
Reference-contexts: After the rebalancing, Step (9) of Algorithm 3 routes any unprocessed virtual blocks into a contiguous region that does not overlap with any of the next v virtual hierarchies to be read. This operation takes time O (log H) by monotone routing <ref> [Lei, Section 3.4.3] </ref>. Algorithm 5 gives the Rebalance subroutine. At most H 0 2s can be introduced into the auxiliary matrix A by the virtual blocks being processed. <p> Line (5) of Algorithm 6 is done with a parallel memory reference. Line (4) is accomplished by sorting according to destination address and doing monotone routing <ref> [Lei, x3.4.3] </ref>. In Section 4.2, we show that Fast Partial Match in line (2) of Algorithm 6 always matches at least dH 0 =4e of the 2s.
Reference: [Luba] <author> Michael Luby, </author> <title> "A Simple Parallel Algorithm for the Maximal Independent Set Problem," </title> <journal> SIAM J. Computing 15 (1986), </journal> <pages> 1036-1053. </pages>
Reference-contexts: from virtual hierarchy h to w [h] (5) Write out the virtual block onto virtual hierarchy w [h] the fastest known deterministic parallel algorithm for maximal matching (the simplest alternative) with n items is O (log 2 n) with a quartic number of processors for dense graphs (which we have) <ref> [Luba] </ref>. Since n = H 0 , this means that the fastest known algorithm is fi (log 2 H). <p> Finally, we can show the following lemma. Lemma 1 The expected number of vertices matched in Algorithm 7 is at least H 0 =4. This algorithm can be derandomized in an efficient way using the techniques of Luby <ref> [Luba, Lubb] </ref>. First, notice that we have H = (H 0 ) 3 processors available, so we can run up to (H 0 ) 2 copies of the Fast Partial Match algorithm simultaneously.
Reference: [Lubb] <author> Michael G. Luby, </author> <title> "Removing Randomness in a Parallel Computation Without a Processor Penalty," </title> <booktitle> International Computer Science Institute, </booktitle> <address> TR-89-044, </address> <month> July </month> <year> 1989, </year> <booktitle> also appears in Proceedings of the 29th Annual IEEE Symposium on the Foundations of Computer Science, </booktitle> <month> (October </month> <year> 1988), </year> <pages> 162-173. </pages>
Reference-contexts: Finally, we can show the following lemma. Lemma 1 The expected number of vertices matched in Algorithm 7 is at least H 0 =4. This algorithm can be derandomized in an efficient way using the techniques of Luby <ref> [Luba, Lubb] </ref>. First, notice that we have H = (H 0 ) 3 processors available, so we can run up to (H 0 ) 2 copies of the Fast Partial Match algorithm simultaneously.
Reference: [Mag] <author> Ninamary Buba Maginnis, </author> <title> "Store More, Spend Less: Mid-Range Options Around," </title> <month> Computer-world (November 16, </month> <year> 1987), </year> <pages> 71-82. </pages>
Reference-contexts: An increasingly popular way to get further speedup is to use many disk drives working in parallel <ref> [GHK, GiS, Jil, Mag, PGK, Uni] </ref>. Aggarwal and Vitter did initial work in the use of parallel block transfer for sorting [AgV], generalizing the sequential work of Floyd [Flo].
Reference: [NoV] <author> Mark H. Nodine and Jeffrey Scott Vitter, </author> <title> "Greed Sort: An Optimal External Sorting Algorithm for Multiple Disks," </title> <institution> Brown University, CS-91-20, </institution> <month> August </month> <year> 1991, </year> <title> also appears in shortened form in "Large-Scale Sorting in Parallel Memories," </title> <booktitle> Proc. 3rd Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <address> Hilton Head, SC (July 1991), </address> <pages> 29-39. </pages>
Reference-contexts: They posed as an open problem whether there is an optimal deterministic algorithm. An affirmative answer was provided by Nodine and Vitter using an algorithm based on merge sort called Greed Sort <ref> [NoV] </ref>. Unfortunately, the Greed Sort technique does not seem to yield optimal sorting bounds on memory hierarchies. 2 Disk striping is a commonly-used technique in which the D disks are synchronized, so that the D blocks accessed during an I/O are at the same relative position on each disk. <p> H hierarchies (of any of the types listed in Figure 3) have their base levels connected by H interconnected processors. We also improve upon the deterministic Greed Sort algorithm in <ref> [NoV] </ref>, which is known to be optimal only for the parallel disk models and not for hierarchical memories. The lower bounds are proved in [AgV] (Theorem 1) and [ViSb] (Theorems 2 and 3). <p> This algorithm improves upon the randomized algorithms of Vitter and Shriver [ViSa, ViSb] and the deterministic disk algorithm of Nodine and Vitter <ref> [NoV] </ref>. The algorithm applies to P-HMM, P-BT, and the parallel variants of the UMH models. In the parallel disk model with parallel CPUs, our algorithm is optimal simultaneously in terms of both the number of I/Os and the internal processing time.
Reference: [PGK] <author> David A. Patterson, Garth Gibson, and Randy H. Katz, </author> <title> "A Case for Redundant Arrays of Inexpensive Disks (RAID)," </title> <booktitle> Proceedings ACM SIGMOD Conference (June 1988), </booktitle> <pages> 109-116. </pages>
Reference-contexts: An increasingly popular way to get further speedup is to use many disk drives working in parallel <ref> [GHK, GiS, Jil, Mag, PGK, Uni] </ref>. Aggarwal and Vitter did initial work in the use of parallel block transfer for sorting [AgV], generalizing the sequential work of Floyd [Flo].
Reference: [RaR] <author> Sanguthevar Rajasekaran and John H. Reif, </author> <title> "Optimal and Sublogarithmic Time Randomized Parallel Sorting Algorithms," </title> <journal> SIAM J. Computing 18 (1989), </journal> <pages> 594-607. </pages>
Reference-contexts: To bound the amount of time spent processing each memoryload, we use a variety of techniques including using an algorithm of Rajasekaran and Reif <ref> [RaR] </ref> as part of a radix sort, Cole's EREW PRAM parallel merge sort [Col], incremental updating, and even/odd partitioning. 6 Conclusions In this paper, we have described the first known deterministic algorithm for sorting optimally using parallel hierarchical memories.
Reference: [Uni] <institution> University of California at Berkeley, "Massive Information Storage, Management, and Use (NSF Institutional Infrastructure Proposal)," </institution> <note> Technical Report No. UCB/CSD 89/493, Jan-uary 1989. </note>
Reference-contexts: An increasingly popular way to get further speedup is to use many disk drives working in parallel <ref> [GHK, GiS, Jil, Mag, PGK, Uni] </ref>. Aggarwal and Vitter did initial work in the use of parallel block transfer for sorting [AgV], generalizing the sequential work of Floyd [Flo].
Reference: [ViN] <author> Jeffrey Scott Vitter and Mark H. Nodine, </author> <title> "Large-Scale Sorting in Uniform Memory Hierarchies," Journal of Parallel and Distributed Computing (January 1993), also appears in shortened form in "Large-Scale Sorting in Parallel Memories," </title> <booktitle> Proc. 3rd Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <address> Hilton Head, SC (July 1991), </address> <pages> 29-39. </pages>
Reference-contexts: The Balance Sort approach we describe in the next section gives us optimal deterministic algorithms for all the models we consider. In particular we get deterministic (as well as more practical) versions of the optimal randomized algorithms of [ViSa], [ViSb] and <ref> [ViN] </ref>. H hierarchies (of any of the types listed in Figure 3) have their base levels connected by H interconnected processors. We also improve upon the deterministic Greed Sort algorithm in [NoV], which is known to be optimal only for the parallel disk models and not for hierarchical memories. <p> Our techniques can also be used to transform the randomized P-UMH algorithms of <ref> [ViN] </ref> into deterministic ones with our PRAM interconnection. In this paper, however, we concentrate on the P-HMM and P-BT models. 4 Parallel Memory Hierarchies This section describes the deterministic algorithm that serves as the basis for the upper bounds in Theorems 2 and 3.
Reference: [ViSa] <author> Jeffrey Scott Vitter and Elizabeth A. M. Shriver, </author> <title> "Algorithms for Parallel Memory I: Two-Level Memories," </title> <journal> Algorithmica, </journal> <note> to appear. </note>
Reference-contexts: They gave two algorithms, a modified merge sort and a distribution sort, that each achieved the optimal I/O bounds. Vitter and Shriver <ref> [ViSa] </ref> considered the more realistic D-disk model , in which the secondary storage is partitioned into D physically distinct disk drives, as in can count as a distinct disk in this definition, as long as each can operate independently of the other heads on the drive.) In each I/O operation, each <p> The Balance Sort approach we describe in the next section gives us optimal deterministic algorithms for all the models we consider. In particular we get deterministic (as well as more practical) versions of the optimal randomized algorithms of <ref> [ViSa] </ref>, [ViSb] and [ViN]. H hierarchies (of any of the types listed in Figure 3) have their base levels connected by H interconnected processors. <p> We likewise use partial striping. We also use a different method for computing the partitioning elements, described in <ref> [ViSa] </ref>. Finally, we let S = (M=B) 1=4 . The I/O bound is easy to show for the new algorithm. The A, X, L, and E arrays all reside in the internal memory, so there is no I/O cost to access them. <p> This algorithm improves upon the randomized algorithms of Vitter and Shriver <ref> [ViSa, ViSb] </ref> and the deterministic disk algorithm of Nodine and Vitter [NoV]. The algorithm applies to P-HMM, P-BT, and the parallel variants of the UMH models.
Reference: [ViSb] <author> Jeffrey Scott Vitter and Elizabeth A. M. Shriver, </author> <title> "Algorithms for Parallel Memory II: Hierarchical Multilevel Memories," </title> <journal> Algorith-mica, </journal> <note> to appear. 10 </note>
Reference-contexts: The Balance Sort approach we describe in the next section gives us optimal deterministic algorithms for all the models we consider. In particular we get deterministic (as well as more practical) versions of the optimal randomized algorithms of [ViSa], <ref> [ViSb] </ref> and [ViN]. H hierarchies (of any of the types listed in Figure 3) have their base levels connected by H interconnected processors. <p> We also improve upon the deterministic Greed Sort algorithm in [NoV], which is known to be optimal only for the parallel disk models and not for hierarchical memories. The lower bounds are proved in [AgV] (Theorem 1) and <ref> [ViSb] </ref> (Theorems 2 and 3). <p> To get optimal performance, we determine the number S of buckets differently depending upon which hierarchical model we are using. Algorithm 2 gives the routine for computing the S 1 partition elements, based on <ref> [AAC, ViSb] </ref>. It works by recursively sorting sets of size N=G and choosing every blog N cth element. The approximate partition elements are selected from this subset. The specific value of G used in the algorithm is dependent upon which hierarchical memory model is being used. <p> The proof of this fact is essentially that of the corresponding theorem in <ref> [ViSb] </ref> for their randomized algorithm. 4.4 Analysis of P-BT Almost the same algorithm will work for the P-BT model as we used in the P-HMM model. <p> This algorithm improves upon the randomized algorithms of Vitter and Shriver <ref> [ViSa, ViSb] </ref> and the deterministic disk algorithm of Nodine and Vitter [NoV]. The algorithm applies to P-HMM, P-BT, and the parallel variants of the UMH models.
References-found: 26

