URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/project/theo-4/text-learning/www/pww/papers/PWW/pwwMLJ.ps.gz
Refering-URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/project/theo-4/text-learning/www/pww/index.html
Root-URL: 
Email: dunja.mladenic@ijs.si, marko.grobelnik@ijs.si  Editor:  
Title: Feature selection for learning on large text data  
Author: DUNJA MLADENI C, MARKO GROBELNIK 
Keyword: feature subset selection, text categorization, machine learning on text data, problem decomposition  
Address: Jamova 39, 1111 Ljubljana, Slovenia  
Affiliation: Department of Intelligent Systems, J.Stefan Institute,  
Note: 1-34 c Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  
Abstract: The paper describes feature subset selection used in learning on text data (text-learning) and gives a brief overview of feature subset selection commonly used in machine learning. Several known and some new feature scoring measures used in feature subset selection on large text data are described. Experimental comparison of the described measures is given on real-world data collected from the Web. Machine learning techniques are used on data collected from Yahoo, a large text hierarchy of Web documents. The high number of features is reduced by feature subset selection and additionally by using `stop-list', pruning low frequency features and using a short description of each document given in the hierarchy instead of using document itself. Documents are represented as feature-vectors that include word sequences instead of including only single words as commonly used when learning on text data. Based on the hierarchical structure the problem is divided into subproblems, each representing one on the categories included in the Yahoo hierarchy. In our learning experiments, for each of the subproblems naive Bayesian classifier was used on text data. The result of learning is a set of independent classifiers, each used to predict probability that a new example is a member of the corresponding category. Experimental evaluation on real-world data shows that the proposed approach gives good results. The best performance was achieved by the feature selection based on a feature scoring measure known from information retrieval called Odds ratio when relatively small number of features was used. 
Abstract-found: 1
Intro-found: 1
Reference: 2. <author> Aha, D.W., & Bankert, R.L. </author> <year> (1994). </year> <title> Feature selection for case-based classification of cloud types: An empirical comparison, </title> <booktitle> Proceedings of the AAAI'94 Workshop on Case-Based Reasoning (pp. </booktitle> <pages> 106-112). </pages> <address> Seattle, </address> <publisher> WA:AAAI Press. </publisher>
Reference-contexts: Here we briefly describe several feature subset selection algorithms developed in machine learning that are based on the wrapper approach. Aha and Bankert <ref> [2] </ref> used a wrapper approach for feature subset selection in instance-based learning. They proposed a new search strategy that performs beam search using a kind of backward elimination.
Reference: 3. <author> Almuallin, H., & Dietterich, T.G. </author> <year> (1991). </year> <title> Efficient algorithms for identifying relevant features. </title> <booktitle> Proceedings of the Ninth Canadian Conference on Artificial Intelligence (pp. </booktitle> <pages> 38-45). </pages> <address> Vancouver, </address> <publisher> BC:Morgan Kaufmann. </publisher>
Reference-contexts: Here we briefly describe several feature subset selection algorithms used in machine learning that are based on the filtering approach. Almallim and Dietterich <ref> [3] </ref> developed several feature subset selection algorithms, starting with a simple exhaustive search and developing algorithms that use different heuristics. <p> Noise handling is enabled by setting the threshold to some positive value. If the threshold is set to 0, the evaluation is based on consistency check as used in <ref> [3] </ref>. Pfahringer [37] proposed using Minimum Description Length as an evaluation function of feature subset. In order to calculate Minimum Description Length of a feature subset, training examples are represented with a simple decision table that contains only features from the subset. <p> Skalak [42] used a wrapper approach for feature subset selection and for selecting a subset of examples to be stored in instance-based learning. Instead of using deterministic search strategy as used in <ref> [3] </ref>, [9] or [18] he used random mutation.

Reference: 5. <author> Bala, J., Huang, J., & Vafaie, H. </author> <year> (1995). </year> <title> Hybrid Learning Using Genetis Algorithms and Decision Trees for Pattern Classification. </title> <booktitle> Proceedings of the 14th International Joint Conference on Artificial Intelligence IJCAI-95 (pp. </booktitle> <pages> 719-724), </pages> <address> Montreal, Quebec. </address>
Reference-contexts: They proposed a new search strategy that performs beam search using a kind of backward elimination. Namely, instead of starting with an empty feature subset, their search randomly selects a fixed number of feature subsets and starts with the best among them. Bala et al. <ref> [5] </ref> and Cherkauer and Shavlik [10] used a wrapper approach for feature subset selection for decision tree induction. Their method uses genetic algorithm to perform the search. Caruana and Freitag [9] developed a wrapper feature subset selection method for decision tree induction.
Reference: 6. <author> Balabanovic, M., & Shoham, Y. </author> <year> (1995). </year> <title> Learning Information Retrieval Agents: Experiments with Automated Web Browsing. </title> <booktitle> Proceedings of the AAAI 1995 Spring Symposium on Information Gathering from Heterogeneous, Distributed Environments. </booktitle> <publisher> Stanford. </publisher>
Reference-contexts: Document representation Document categorization is here based on the usage of machine learning techniques on large text data. We based document representation on feature-vectors using the bag-of-words representation as commonly used in learning on text data (eg. [4], <ref> [6] </ref>, [7], [11], [17], [24], [33], [36], [44]). In learning on text data, one of the frequently used approaches to reduce the number of different words used as features that we also apply here is to use the publicly available "stop-list" containing common English words [14].
Reference: 7. <author> Berry, M.W., Dumais, S.T., & OBrein, G.W. </author> <year> (1995). </year> <title> Using linear algebra for intelligent information retrieval. </title> <journal> SIAM Review, </journal> <volume> 4, (Vol. 37), </volume> <pages> 573-595. </pages>
Reference-contexts: Document representation Document categorization is here based on the usage of machine learning techniques on large text data. We based document representation on feature-vectors using the bag-of-words representation as commonly used in learning on text data (eg. [4], [6], <ref> [7] </ref>, [11], [17], [24], [33], [36], [44]). In learning on text data, one of the frequently used approaches to reduce the number of different words used as features that we also apply here is to use the publicly available "stop-list" containing common English words [14].
Reference: 8. <author> Cardie, C. </author> <year> (1993). </year> <title> Using Decision Trees to Improve Case-Based Learning. </title> <booktitle> Proceedings of the 10th International Conference on Machine Learning ICML93 (pp. </booktitle> <pages> 25-32). </pages>
Reference-contexts: This is the reason why search heuristics are used in the next versions of the algorithm resulting in good but not necessarily optimal solutions. The idea is to use forward selection until a sufficient subset is encountered. Cardie <ref> [8] </ref> uses decision tree induction for feature subset selection. The idea is that only the features that appear in the induced decision tree are selected for learning using nearest-neighbor algorithm. Kira and Rendell [20] assigned weight to each feature and used nearest neighbor algorithm to update the weights.
Reference: 9. <author> Caruana, R., & Freitag, D. </author> <year> (1994). </year> <title> Greedy Attribute Selection. </title> <booktitle> Proceedings of the 11th International Conference on Machine Learning ICML94 (pp. </booktitle> <pages> 28-26). </pages>
Reference-contexts: Bala et al. [5] and Cherkauer and Shavlik [10] used a wrapper approach for feature subset selection for decision tree induction. Their method uses genetic algorithm to perform the search. Caruana and Freitag <ref> [9] </ref> developed a wrapper feature subset selection method for decision tree induction. <p> Skalak [42] used a wrapper approach for feature subset selection and for selecting a subset of examples to be stored in instance-based learning. Instead of using deterministic search strategy as used in [3], <ref> [9] </ref> or [18] he used random mutation.
Reference: 10. <author> Cherkauer, K.J., & Shavlik, J.W. </author> <year> (1996). </year> <title> Growing simpler decision trees to facilitate knowledge discovery, </title> <booktitle> Proceedings of the Second International Conference on Knowledge Discovery and Data Mining KDD-96 (pp. </booktitle> <pages> 315-318). </pages> <address> Portland, </address> <publisher> OR:AAAI Press. </publisher>
Reference-contexts: Namely, instead of starting with an empty feature subset, their search randomly selects a fixed number of feature subsets and starts with the best among them. Bala et al. [5] and Cherkauer and Shavlik <ref> [10] </ref> used a wrapper approach for feature subset selection for decision tree induction. Their method uses genetic algorithm to perform the search. Caruana and Freitag [9] developed a wrapper feature subset selection method for decision tree induction.
Reference: 11. <author> Cohen, W.W. </author> <year> (1995). </year> <title> Learning to Classify English Text with ILP Methods. </title> <booktitle> Proceedings of the Workshop on Inductive Logic Programming. </booktitle> <address> Leuven. </address>
Reference-contexts: Document representation Document categorization is here based on the usage of machine learning techniques on large text data. We based document representation on feature-vectors using the bag-of-words representation as commonly used in learning on text data (eg. [4], [6], [7], <ref> [11] </ref>, [17], [24], [33], [36], [44]). In learning on text data, one of the frequently used approaches to reduce the number of different words used as features that we also apply here is to use the publicly available "stop-list" containing common English words [14]. <p> We additionally reduce the high number of features by pruning low frequency features as suggested in <ref> [11] </ref>, [17] or [44]. The process of feature generation is performed in passes over documents, where i-grams are generated in the i-th pass only from the candidate features of length i-1 generated in the previous pass.
Reference: 12. <author> Domingos, P., & Pazzani, M. </author> <year> (1997). </year> <title> On the Optimality of the Simple Bayesian Classifier under Zero-One Loss. </title> <journal> Machine Learning, </journal> <volume> 29, </volume> <pages> 103-130. </pages>
Reference-contexts: In these ap proaches documents are represented as word-vectors where a feature is defined for each word position in the document having word at that position as a feature value. The assumption about feature independence used in naive Bayesian classifier is here clearly incorrect. According to <ref> [12] </ref> this does not necessarily mean that the classi fier will have poor performance because of that. Our document representation is extended and includes features representing not only single words but also word sequences that are handled in feature selection and learning the same way as the single words.
Reference: 13. <author> Filo, D., & Yang, J. </author> <year> (1997). </year> <institution> Yahoo! Inc. </institution> <note> http://www.yahoo.com/docs/pr/ </note>
Reference-contexts: The problem of automatic document categorization is well known in information retrieval and usually tested on publicly available data bases (eg. Reuters, MEDLINE). Here we use machine learning for document categorization using one of the existing Web hierarchies named Yahoo <ref> [13] </ref>. In this way, current situation on the Web captured in Web hierarchy can be used for document categorization. <p> Data description Domains used in our experiments are parts of the real-world data collected from the Yahoo <ref> [13] </ref> hierarchy. We use three different sized sub-domains each representing distinct part of the Yahoo hierarchy. The Yahoo hierarchy itself is currently built on approximately a million Web documents located on Internet all around the world. We refer here to these documents as actual Web documents.
Reference: 14. <author> Frakes W.B., & Baeza-Yates R., (Eds.) </author> <year> (1992). </year> <title> Information Retrieval: Data Structures & Algorithms. </title> <address> Englewood Cliffs: </address> <publisher> Prentice Hall. </publisher>
Reference-contexts: In learning on text data, one of the frequently used approaches to reduce the number of different words used as features that we also apply here is to use the publicly available "stop-list" containing common English words <ref> [14] </ref>. We extended document representation by generating new features that represent not only single words (unigrams) but also up to 5 words (1-grams, 2-grams, . . . 5-grams) occurring in a document as a sequence (eg. `machine learning', `world wide web').
Reference: 15. <author> Grobelnik, M., & Mladenic, D. </author> <year> (1998). </year> <title> Learning Machine: design and implementation. </title> <type> Technical Report IJS-DP-7824. </type> <institution> Department for Intelligent Systems, J.Stefan Institute, Slovenia. </institution>
Reference-contexts: We test our approach on three domains that are parts of the Yahoo hierarchy, since they are hierarchies themselves with the same problem characteristics as the whole Yahoo but smaller. The approach is tested using our recently developed machine learning system Learning Machine <ref> [15] </ref> that supports usage of different machine learning techniques on large data sets with especially designed modules for learning on text and 20 Table 4. Domain characteristics for three top Yahoo categories.

Reference: 17. <author> Joachims, T. </author> <year> (1997). </year> <title> A Probabilistic Analysis of the Rocchio Algorithm with TFIDF for Text Categorization. </title> <booktitle> Proceedings of the 14th International Conference on Machine Learning ICML97 (pp. </booktitle> <pages> 143-151). </pages>
Reference-contexts: Document representation Document categorization is here based on the usage of machine learning techniques on large text data. We based document representation on feature-vectors using the bag-of-words representation as commonly used in learning on text data (eg. [4], [6], [7], [11], <ref> [17] </ref>, [24], [33], [36], [44]). In learning on text data, one of the frequently used approaches to reduce the number of different words used as features that we also apply here is to use the publicly available "stop-list" containing common English words [14]. <p> We additionally reduce the high number of features by pruning low frequency features as suggested in [11], <ref> [17] </ref> or [44]. The process of feature generation is performed in passes over documents, where i-grams are generated in the i-th pass only from the candidate features of length i-1 generated in the previous pass. This process is similar to the large k-itemset generation used in association rules algorithm [1]. <p> Learning on text data Separate classifier for each of the defined subproblems is induced using naive Bayesian classifier on text data in similar way as described in <ref> [17] </ref>, [30] or [33]. In these ap proaches documents are represented as word-vectors where a feature is defined for each word position in the document having word at that position as a feature value. The assumption about feature independence used in naive Bayesian classifier is here clearly incorrect. <p> Domain characteristics and feature scoring In our first experiments on learning from text data [33] words were scored using Information gain (1) and learning was performed using naive Bayesian classifier the same way as used in <ref> [17] </ref>. It turned out that many features with high score were not very useful for our classification problem. Namely, Information gain assigns higher score to the features that make better distinction between class values.
Reference: 18. <author> John, G.H., Kohavi, R., & Pfleger, K. </author> <year> (1994). </year> <title> Irrelevant Features and the Subset Selection Problem. </title> <booktitle> Proceedings of the 11th International Conference on Machine Learning ICML94 (pp. </booktitle> <pages> 121-129). </pages>
Reference-contexts: According to John et al. <ref> [18] </ref> there are two main approaches to feature subset selection used in machine learning: filtering approach and wrapper approach. In information retrieval and thus also in text-learning the whole process of feature subset selection is simplified with the assumption of feature independence. <p> They proposed a new search strategy that uses bidirectional search (backward stepwise elimination) and additionally at each step removes all the features that were not used in the decision tree induced for the evaluation of the current feature subset. John et al. <ref> [18] </ref> defined notion of strong and weak relevance 5 using the machine learning algorithm that will use the selected feature subset. of a feature and used a wrapper feature subset selection for decision tree induction. <p> Skalak [42] used a wrapper approach for feature subset selection and for selecting a subset of examples to be stored in instance-based learning. Instead of using deterministic search strategy as used in [3], [9] or <ref> [18] </ref> he used random mutation. Vafaie and De Jong [43] used a kind of wrapper approach where instead of using AQ15 for evaluation and learning, the evaluation is performed using the approximation of model quality estimated by AQ15 that selects feature subset which maximally separates classes using Euclidean distance.
Reference: 19. <author> Kindo, T., Yoshida, H., Morimoto, T., & Watanabe, T. </author> <year> (1997). </year> <title> Adaptive Personal Information Filtering System that Organizes Personal Profiles Automatically. </title> <booktitle> Proceedings of the 15th International Joint Conference on Artificial Intelligence IJCAI-97 (pp. </booktitle> <pages> 716-721). </pages>
Reference-contexts: Slightly modified Cross entropy was used as feature selection measure in text-classification <ref> [19] </ref> named `Key word checker' KCH (F ). <p> They compared several learning algorithms and learn document category from the hierarchical structure, dividing classification task into a set of smaller 12 category with the approximate number of actual Web documents each category is based on <ref> (Yahoo UK & Ireland, Nov. 1997) </ref>. problems corresponding to the splits in the classification hierarchy nodes. They give results on three domains each having a 3-level hierarchy that is based on up to 1,000 documents and the biggest having 12 nodes.
Reference: 20. <author> Kira, K., & Rendell, L.A. </author> <year> (1992). </year> <title> The feature selection problem: Traditional methods and a new algorithm. </title> <booktitle> Proceedings of the Ninth National Conference on Artificial Intelligence AAAI-92 (pp.129-134). </booktitle> <publisher> AAAI Press/The MIT Press. </publisher>
Reference-contexts: The idea is to use forward selection until a sufficient subset is encountered. Cardie [8] uses decision tree induction for feature subset selection. The idea is that only the features that appear in the induced decision tree are selected for learning using nearest-neighbor algorithm. Kira and Rendell <ref> [20] </ref> assigned weight to each feature and used nearest neighbor algorithm to update the weights.
Reference: 21. <author> Koller, D., & Sahami, M. </author> <year> (1996). </year> <title> Toward optimal feature selection. </title> <booktitle> Proceedings of the 13th International Conference on Machine Learning ICML96 (pp. </booktitle> <pages> 284-292). </pages>
Reference-contexts: Training set is randomly sampled and the weights updated based on the difference between the each selected example and its two nearest neighbors: the nearest hit nearest neighbor having the same class value and the nearest miss nearest neighbor having the opposite class value. Koller and Sahami <ref> [21] </ref> proposed an algorithm for feature subset selection that uses backward elimination to eliminate predefined number of features. The idea is to select subset of features that keeps class probability distribution as close as possible to the original distribution that is obtained using all the features.
Reference: 22. <author> Koller, D., & Sahami, M. </author> <year> (1997). </year> <title> Hierarchically classifying documents using very few words. </title> <booktitle> Proceedings of the 14th International Conference on Machine Learning ICML97 (pp. </booktitle> <pages> 170-178). </pages>
Reference-contexts: Cross entropy for text Expected cross entropy used in text-classification experiments <ref> [22] </ref> is similar to Information gain (1). The difference is that instead of calculating average over all possible feature values, only the value denoting that word (or in our case word sequence) W occurred in a document is considered. <p> Learning from hierarchy The problem domain is represented as a classification hierarchy of examples. There is some work in machine learning addressing domains with hierarchy of features (eg. [40]) but not example or classification hierarchy. The recent work on hierarchically classifying documents by Koller and Sahami <ref> [22] </ref> is the most closely related work we are aware of. They used the Reuters dataset that does not have predetermined hierarchical classification structure, so they `identified labels that tend to subsume other labels, and used those as the higher level topics'. <p> Additionally to the high number of class values, a large number of features is needed in order to cover all the variety of the large number of documents included in different categories. As pointed out in <ref> [22] </ref>, a better idea is to split the whole problem into subproblems. We use the hierarchical structure to define subproblems each corresponding to the individual Yahoo category.
Reference: 23. <author> Kononenko, I. </author> <year> (1995). </year> <title> On biases estimating multi-valued attributes. </title> <booktitle> Proceedings of the 14th International Joint Conference on Artificial Intelligence IJCAI-95 (pp. </booktitle> <pages> 1034-1040). </pages>
Reference-contexts: Weight of evidence captures the difference between the probability of class C i occurring given the j-th feature value and the probability of class C i . Average weight of evidence for feature F is calculated over all class values <ref> [23] </ref> using the following formula: W eightOf Evid (F ) = P P odds (C i jF j ) odds (C i ) j Where P (C i ) is the probability of the i-th class value, P (F j ) is the probability of the j-th feature value, odds (C
Reference: 24. <author> Lam, W., Low, K.F., & Ho, C.Y. </author> <year> (1997). </year> <title> Using Bayesian Network Induction Approach for Text Categorization. </title> <booktitle> Proceedings of the 15th International Joint Conference on Artificial Intelligence IJCAI97 (pp. </booktitle> <pages> 745-750). </pages>
Reference-contexts: Document representation Document categorization is here based on the usage of machine learning techniques on large text data. We based document representation on feature-vectors using the bag-of-words representation as commonly used in learning on text data (eg. [4], [6], [7], [11], [17], <ref> [24] </ref>, [33], [36], [44]). In learning on text data, one of the frequently used approaches to reduce the number of different words used as features that we also apply here is to use the publicly available "stop-list" containing common English words [14].
Reference: 25. <author> Lewis, D.D. </author> <year> (1992). </year> <title> Feature Selection and Feature Extraction for Text Categorization. </title> <booktitle> Proceedings of Speech and Natural Language workshop (pp. </booktitle> <pages> 212-217). </pages> <address> Harriman, </address> <publisher> CA:Morgan Kaufmann. </publisher>
Reference-contexts: Where DF (W ) is the number of documents containing word W , ff is a user given parameter. All other variables are the same as in CrossEntropyT xt (3). 8 3.3. Mutual information for text M utual inf ormation used in text-classification experiments <ref> [25] </ref>, [44] is based on mutual information between two variable values known from information theory.
Reference: 26. <author> Lewis, D.D. </author> <year> (1995). </year> <title> Evaluating and optimizating autonomous text classification systems. </title> <booktitle> Proceedings of the 18th Annual International ACM-SIGIR Conference on Recsearch and Development in Information Retrieval (pp.246-254). </booktitle>
Reference-contexts: Thus usually the value of both is reported. Evaluation measures for text-classification systems are discussed in more details in <ref> [26] </ref>. F measure is a combination of Precision and Recall commonly used in information retrieval, where the relative importance of each is expressed with the value of parameter fi. F fi = (1+fi 2 )P recisionfiRecall fi 2 P recision+Recall .
Reference: 27. <author> Liu, H., & Setiono, R. </author> <year> (1996). </year> <title> A probabilistic approach to feature selection A filter solution. </title> <booktitle> Proceedings of the 13th International Conference on Machine Learning ICML'97 (pp. </booktitle> <pages> 319-327). </pages> <address> Bari. </address>
Reference-contexts: Reducing K to 0 means having much faster algorithm that is actually the same as the feature selection algorithms commonly used on text-data 4 (see Section 2.3). Liu and Setiono <ref> [27] </ref> used random sampling to search the space of all possible feature subsets. The predetermined number of subsets is evaluated and the smallest having inconsistency rate below the given threshold is selected.
Reference: 28. <author> Manly, B.F.J. </author> <year> (1994). </year> <title> Multivariate Statistical Methods a primer (Second ed.). </title> <publisher> Chapman & Hall. </publisher>
Reference-contexts: Filtering approach In the filtering approach illustrated in Figure 1, a feature subset is selected independently of the learning method that will use the selected features. There are methods in statistics that are used to reduce the number of features, like principal components analysis or factor analysis <ref> [28] </ref> that will not be addressed in this paper. Here we briefly describe several feature subset selection algorithms used in machine learning that are based on the filtering approach.
Reference: 29. <author> Mansuripur, M. </author> <year> (1987). </year> <title> Introduction to Information Theory. </title> <publisher> Prentice-Hall. </publisher>
Reference-contexts: (F; C) = P P i P (C i )I (C i ; F ) = I (C; F ) is the expected cross entropy, I (F j ; C) = P P (C i jF j ) P (C i ) is cross entropy (also called conditional mutual information) <ref> [29] </ref>. P (C i jF j ) is the conditional probability of the 7 i-th class value given the j-th feature value, P (C i ) is the probability of the i-th class value, P (F j ) is the probability of the j-th feature value.
Reference: 30. <author> Mitchell, </author> <title> T.M. </title> <booktitle> (1997). Machine Learning. </booktitle> <publisher> The McGraw-Hill Companies, Inc.. </publisher>
Reference-contexts: Learning on text data Separate classifier for each of the defined subproblems is induced using naive Bayesian classifier on text data in similar way as described in [17], <ref> [30] </ref> or [33]. In these ap proaches documents are represented as word-vectors where a feature is defined for each word position in the document having word at that position as a feature value. The assumption about feature independence used in naive Bayesian classifier is here clearly incorrect.
Reference: 31. <author> Mladenic, D. </author> <year> (1995). </year> <title> Automated model selection. </title> <booktitle> Proceedings of the MLNet familiarisation workshop: Knowledge level modelling and machine learning, ECML 95, </booktitle> <address> Heraklion. </address>
Reference-contexts: Namely, evaluation function for each candidate feature subset returns estimated quality of the model induced by the target learning algorithm using the feature subset. The idea is similar to automated model selection, where pruning parameters for decision tree induction are set by an optimization algorithm <ref> [31] </ref>, [32]. This can result in a rather time consuming process, since for each candidate feature subset that is evaluated during the search, the target learning algorithm is usually applied several times (eg. 10 in case of 10-fold cross validation used for estimating model quality).
Reference: 32. <author> Mladenic, D. </author> <year> (1995). </year> <title> Domain-Tailored Machine Learning. M.Sc. </title> <type> Thesis, </type> <institution> Faculty of computer and information science, University of Ljubljan, Slovenia. </institution>
Reference-contexts: Namely, evaluation function for each candidate feature subset returns estimated quality of the model induced by the target learning algorithm using the feature subset. The idea is similar to automated model selection, where pruning parameters for decision tree induction are set by an optimization algorithm [31], <ref> [32] </ref>. This can result in a rather time consuming process, since for each candidate feature subset that is evaluated during the search, the target learning algorithm is usually applied several times (eg. 10 in case of 10-fold cross validation used for estimating model quality).
Reference: 33. <author> Mladenic, D. </author> <year> (1996). </year> <title> Personal WebWatcher: Implementation and Design. </title> <type> Technical Report IJS-DP-7472. </type> <institution> Department for Intelligent Systems, J.Stefan Institute, Slovenia. </institution>
Reference-contexts: Document representation Document categorization is here based on the usage of machine learning techniques on large text data. We based document representation on feature-vectors using the bag-of-words representation as commonly used in learning on text data (eg. [4], [6], [7], [11], [17], [24], <ref> [33] </ref>, [36], [44]). In learning on text data, one of the frequently used approaches to reduce the number of different words used as features that we also apply here is to use the publicly available "stop-list" containing common English words [14]. <p> Learning on text data Separate classifier for each of the defined subproblems is induced using naive Bayesian classifier on text data in similar way as described in [17], [30] or <ref> [33] </ref>. In these ap proaches documents are represented as word-vectors where a feature is defined for each word position in the document having word at that position as a feature value. The assumption about feature independence used in naive Bayesian classifier is here clearly incorrect. <p> Domain characteristics and feature scoring In our first experiments on learning from text data <ref> [33] </ref> words were scored using Information gain (1) and learning was performed using naive Bayesian classifier the same way as used in [17]. It turned out that many features with high score were not very useful for our classification problem.
Reference: 34. <author> Mladenic, D. </author> <year> (1998). </year> <title> Feature subset selection in text-learning. </title> <booktitle> Proceedings of the 10th Euro-pean Conference on Machine Learning ECML98. </booktitle>
Reference-contexts: This is consistent with the results reported in text-learning on the problem of predicting clicked hyperlinks from the set of visited Web documents <ref> [34] </ref> where the feature selection based on Odds ratio achieved the best results. Similar observation regarding the number of features is reported on text categorization in [44], where the 23 Table 6.
Reference: 35. <author> Mladenic, D., & Grobelnik, M. </author> <year> (1998). </year> <title> Learning from large text data to assign keywords to documents. </title> <booktitle> Proceedings of the 15th International Conference on Machine Learning ICML98 (submitted). </booktitle>
Reference-contexts: In other words, precision and recall are claculated for the fixed probability threshold that is here set experimentally <ref> [35] </ref> to 0.95. Notice that one testing example can be originally assigned to several categories, since it can referenced from several parts of the hierarchy.
Reference: 36. <author> Pazzani, M., & Billsus, D. </author> <year> (1997). </year> <title> Learning and Revising User Profiles: The Identification of Interesting Web Sites. </title> <journal> Machine Learning, </journal> <volume> 27, </volume> <pages> 313-331. </pages>
Reference-contexts: Document representation Document categorization is here based on the usage of machine learning techniques on large text data. We based document representation on feature-vectors using the bag-of-words representation as commonly used in learning on text data (eg. [4], [6], [7], [11], [17], [24], [33], <ref> [36] </ref>, [44]). In learning on text data, one of the frequently used approaches to reduce the number of different words used as features that we also apply here is to use the publicly available "stop-list" containing common English words [14]. <p> Here we used only naive Bayesian classifier for learning as one of the learning algorithms reported to work well on text domains <ref> [36] </ref>. Naive Bayesian classifier uses the same conditional probability as used in Odds ratio for scoring the features. In this way, selected features are features expected to have the greatest influence to the posterior probability of class values returned by naive Bayesian classifier.
Reference: 37. <author> Pfahringer, B. </author> <year> (1995). </year> <title> Compression-Based Feature Subset Selection. </title> <editor> In P. Turney (Ed.), </editor> <booktitle> Proceedings of the IJCAI-95 Workshop on Data Engineering for Inductive Learning, Workshop Program Working Notes, </booktitle> <address> Montreal, Canada, </address> <year> 1995. </year>
Reference-contexts: Noise handling is enabled by setting the threshold to some positive value. If the threshold is set to 0, the evaluation is based on consistency check as used in [3]. Pfahringer <ref> [37] </ref> proposed using Minimum Description Length as an evaluation function of feature subset. In order to calculate Minimum Description Length of a feature subset, training examples are represented with a simple decision table that contains only features from the subset.
Reference: 38. <author> Quinlan, J.R. </author> <year> (1993). </year> <title> Constructing Decision Tree. In C4.5: Programs for Machine Learning. </title> <publisher> Morgan Kaufman Publishers. 39. </publisher> <editor> van Rijsbergen, C.J,. Harper, D.J., & Porter, M.F. </editor> <year> (1981). </year> <title> The selection of good search terms. </title> <booktitle> Information Processing & Management, </booktitle> <volume> 17, </volume> <pages> 77-91. </pages>
Reference-contexts: Feature scoring in text domains Scoring of individual features in feature subset selection approach used on text data can be performed using some of the measures used in machine learning for feature selection during the learning process, for example, Information gain used in decision tree induction <ref> [38] </ref>. In machine learning, several measures are known working for different kind of class and feature values. In our case we have binary-valued class (is the document relevant for the target concept?) and binary-valued features (does a feature occur in the document?).
Reference: 40. <author> Shapiro, A. </author> <year> (1987). </year> <title> Structured induction in expert systems. </title> <publisher> Addison-Wesley. </publisher> <pages> 34 </pages>
Reference-contexts: Learning from hierarchy The problem domain is represented as a classification hierarchy of examples. There is some work in machine learning addressing domains with hierarchy of features (eg. <ref> [40] </ref>) but not example or classification hierarchy. The recent work on hierarchically classifying documents by Koller and Sahami [22] is the most closely related work we are aware of.
Reference: 41. <author> Shaw Jr, W.M. </author> <year> (1995). </year> <title> Term-relevance computations and perfect retrieval performance. </title> <booktitle> Information Processing & Management, </booktitle> <pages> 31(4) 491-498. </pages>
Reference-contexts: i ) = 8 &gt; &gt; &gt; &gt; &gt; &gt; &gt; : n 2 n 2 1 1 1 ; P (X i ) = 1 1P (X i ) ; P (X i ) 6= 0 ^ P (X i ) 6= 1 We handle singularities as proposed in <ref> [41] </ref>: P (X i ) = 0 is replaced with P (X i ) = 1 n 2 and P (X i ) = 1 is replaced with P (X i ) = 1 1 n 2 , where n is the number of examples.
Reference: 42. <author> Skalak, D.B. </author> <year> (1994). </year> <title> Prototype and Feature Selection by Sampling and Random Mutation Hill Climbing Algorithms. </title> <booktitle> Proceedings of the 11th International Conference on Machine Learning ICML94 (pp. </booktitle> <pages> 293-301). </pages>
Reference-contexts: John et al. [18] defined notion of strong and weak relevance 5 using the machine learning algorithm that will use the selected feature subset. of a feature and used a wrapper feature subset selection for decision tree induction. Skalak <ref> [42] </ref> used a wrapper approach for feature subset selection and for selecting a subset of examples to be stored in instance-based learning. Instead of using deterministic search strategy as used in [3], [9] or [18] he used random mutation.
Reference: 43. <author> Vafaie, H., & De Jong, K. </author> <year> (1993). </year> <title> Robust feature selection algorithms. </title> <booktitle> Proceedings of the 5th IEEE International Conference on Tools for Artificial Intelligence (pp. </booktitle> <pages> 356-363). </pages> <address> Boston, MA: </address> <publisher> IEEE Press. </publisher>
Reference-contexts: Skalak [42] used a wrapper approach for feature subset selection and for selecting a subset of examples to be stored in instance-based learning. Instead of using deterministic search strategy as used in [3], [9] or [18] he used random mutation. Vafaie and De Jong <ref> [43] </ref> used a kind of wrapper approach where instead of using AQ15 for evaluation and learning, the evaluation is performed using the approximation of model quality estimated by AQ15 that selects feature subset which maximally separates classes using Euclidean distance. Their method uses genetic algorithm to perform the search. 2.3.

Reference: 1. <editor> LargeNGramSet := all single words in DocVec, </editor> <title> not in StopWordSet and 2. occurring MinNGramOcc times; 3. for NGramSize := 2 to MaxNGramSize do f </title>
Reference-contexts: The process of feature generation is performed in passes over documents, where i-grams are generated in the i-th pass only from the candidate features of length i-1 generated in the previous pass. This process is similar to the large k-itemset generation used in association rules algorithm <ref> [1] </ref>. Algorithm in Appendix A gives the whole process of new feature generation. In the first pass all single words not contained in the `stop-list' and having sufficient frequency (here we check for frequency 3) are taken LargeNGramSet.
Reference: 4. <author> CandNGramMap:=[]; </author> <title> 5. for SymVec := DocVec[1] to DocVec[jDocVecj] do f </title>
Reference-contexts: Document representation Document categorization is here based on the usage of machine learning techniques on large text data. We based document representation on feature-vectors using the bag-of-words representation as commonly used in learning on text data (eg. <ref> [4] </ref>, [6], [7], [11], [17], [24], [33], [36], [44]). In learning on text data, one of the frequently used approaches to reduce the number of different words used as features that we also apply here is to use the publicly available "stop-list" containing common English words [14].
Reference: 6. <author> NGramQueue:=[]; </author> <title> 7. for Sym := SymVec[1] to SymVec[jSymVecj] do f 8. if (TypeOf(Sym)==word)f 9. if (Sym not in StopWordSet)f 10. if Sym in LargeNGramSet then f 11. </title> <note> if (jNGramQueuej+1=NGramSize)f 12. if (Concatenated(NGramQueue) in LargeNGramSet)f </note>
Reference-contexts: Document representation Document categorization is here based on the usage of machine learning techniques on large text data. We based document representation on feature-vectors using the bag-of-words representation as commonly used in learning on text data (eg. [4], <ref> [6] </ref>, [7], [11], [17], [24], [33], [36], [44]). In learning on text data, one of the frequently used approaches to reduce the number of different words used as features that we also apply here is to use the publicly available "stop-list" containing common English words [14].
Reference: 13. <institution> NGramQueue.Push(Sym); </institution>
Reference-contexts: The problem of automatic document categorization is well known in information retrieval and usually tested on publicly available data bases (eg. Reuters, MEDLINE). Here we use machine learning for document categorization using one of the existing Web hierarchies named Yahoo <ref> [13] </ref>. In this way, current situation on the Web captured in Web hierarchy can be used for document categorization. <p> Data description Domains used in our experiments are parts of the real-world data collected from the Yahoo <ref> [13] </ref> hierarchy. We use three different sized sub-domains each representing distinct part of the Yahoo hierarchy. The Yahoo hierarchy itself is currently built on approximately a million Web documents located on Internet all around the world. We refer here to these documents as actual Web documents.
Reference: 14. <institution> CandNGramMap[Concatenated(NGramQueue)]++; </institution>
Reference-contexts: In learning on text data, one of the frequently used approaches to reduce the number of different words used as features that we also apply here is to use the publicly available "stop-list" containing common English words <ref> [14] </ref>. We extended document representation by generating new features that represent not only single words (unigrams) but also up to 5 words (1-grams, 2-grams, . . . 5-grams) occurring in a document as a sequence (eg. `machine learning', `world wide web').
Reference: 15. <author> NGramQueue.Pop(); 16. </author> <title> g else fNGramQueue.Push(Sym); NGramQueue.Pop();g 17. g else fNGramQueue.Push(Sym);g 18. g else fNGramQueue:=[];g 19. g /* if (Sym not in StopWordSet) */ 20. g else fNGramQueue:=[];g 21. g; /* for Sym */ 22. g; /* for SymVec </title> */ 
Reference-contexts: We test our approach on three domains that are parts of the Yahoo hierarchy, since they are hierarchies themselves with the same problem characteristics as the whole Yahoo but smaller. The approach is tested using our recently developed machine learning system Learning Machine <ref> [15] </ref> that supports usage of different machine learning techniques on large data sets with especially designed modules for learning on text and 20 Table 4. Domain characteristics for three top Yahoo categories.
Reference: 23. <author> LargeNGramSet += fNGram: </author> <title> CandNGramMap[NGram] MinNGramOccg; 24. g; 25. return LargeNGramSet; Figure A.1. Algorithm for the generation of features representing word sequences (n-grams). </title>
Reference-contexts: Weight of evidence captures the difference between the probability of class C i occurring given the j-th feature value and the probability of class C i . Average weight of evidence for feature F is calculated over all class values <ref> [23] </ref> using the following formula: W eightOf Evid (F ) = P P odds (C i jF j ) odds (C i ) j Where P (C i ) is the probability of the i-th class value, P (F j ) is the probability of the j-th feature value, odds (C
References-found: 46

