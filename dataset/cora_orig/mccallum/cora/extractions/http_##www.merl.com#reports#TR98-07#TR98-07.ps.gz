URL: http://www.merl.com/reports/TR98-07/TR98-07.ps.gz
Refering-URL: http://www.merl.com/reports/TR98-07/index.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Abstract-found: 0
Intro-found: 1
Reference: <author> Blum, A. </author> <year> (1992). </year> <title> Learning boolean functions in an infinite attribute space. </title> <journal> Machine Learning, </journal> <volume> 9(4) </volume> <pages> 373-386. </pages>
Reference-contexts: The use of a sparse architecture, as described above, coupled with the representation of each example as a list of active features is reminiscent of the infinite attribute models of Winnow <ref> (Blum, 1992) </ref>. 4.2. Weighted Majority Rather than evaluating the evidence for a given word W i using a single classifier, WinSpell combines evidence from multiple classifiers; the motivation for doing so is discussed below. Weighted Majority (Littlestone and Warmuth, 1994) is used to do the combination. <p> all the variables from the beginning, but rather add variables as necessary, the number of mistakes made on disjunctions and conjunctions is linear in the size of the largest example seen and in the number of relevant attributes; it is independent of the total number of attributes in the domain <ref> (Blum, 1992) </ref>. 11 Winnow was analyzed in the presence of various kinds of noise, and in cases where no linear threshold function can make perfect classifications (Littlestone, 1991).
Reference: <author> Breiman, L. </author> <year> (1994). </year> <title> Bagging predictors. </title> <type> Technical Report 421, </type> <institution> University of California, Berkeley. </institution>
Reference: <author> Cesa-Bianchi, N., Freund, Y., Helmbold, D. P., and Warmuth, M. </author> <year> (1994). </year> <title> On-line prediction and conversion strategies. </title> <booktitle> In Computational Learning Theory: </booktitle> <volume> Eurocolt '93, </volume> <pages> pages 205-216, </pages> <publisher> Oxford. Oxford University Press. </publisher>
Reference: <author> Chen, S. F. and Goodman, J. </author> <year> (1996). </year> <title> An empirical study of smoothing techniques for language modeling. </title> <booktitle> In Proc. 34th Annual Meeting of the Association for Computational Linguistics, </booktitle> <address> Santa Cruz, CA. </address>
Reference-contexts: Instead, BaySpell performs smoothing by interpolating between the MLE of P (f jW i ) and the MLE of the unigram probability, P (f ). Some means of incorporating a lower-order model in this way is generally regarded as essential for good performance <ref> (Chen and Goodman, 1996) </ref>.
Reference: <author> Cortes, C. and Vapnik, V. </author> <year> (1995). </year> <title> Support-vector networks. </title> <journal> Machine Learning, </journal> <volume> 20 </volume> <pages> 273-297. </pages>
Reference-contexts: A similar philosophy, albeit very different technically, is followed by the work on Support Vector Machines <ref> (Cortes and Vapnik, 1995) </ref>. Theoretical analysis has shown Winnow to be able to adapt quickly to a changing target concept (Herbster and Warmuth, 1995). We investigate this issue experimentally in Section 5.5. <p> The use of this strategy in Winnow shares much the same philosophy | if none of the technical underpinnings | as Support Vector Machines <ref> (Cortes and Vapnik, 1995) </ref>. Second, the two-layer architecture used here is related to various voting and boosting techniques studied in recent years in the learning community (Freund and Schapire, 1995; Breiman, 1994; Littlestone and Warmuth, 1994).
Reference: <author> Dietterich, T. G. </author> <year> (1998). </year> <title> Approximate statistical tests for comparing supervised classification learning algorithms. Neural Computation. </title> <note> To appear. 24 Fleiss, </note> <author> J. L. </author> <year> (1981). </year> <title> Statistical Methods for Rates and Proportions. </title> <publisher> John Wiley and Sons. </publisher>
Reference-contexts: When comparing scores, we tested for significance using a McNemar test <ref> (Dietterich, 1998) </ref> when possible; when data on individual trials was not available (the system comparison), or the comparison was across different test sets (the within/across study), we instead used a test for the difference of two proportions (Fleiss, 1981). All tests are reported for the 0.05 significance level. 5.2.
Reference: <author> Flexner, S. B., </author> <title> editor (1983). Random House Unabridged Dictionary. Random House, </title> <address> New York. </address> <note> Second edition. </note>
Reference-contexts: Acquiring confusion sets is an interesting problem in its own right; in the work reported here, however, we take our confusion sets largely from the list of "Words Commonly Confused" in the back of the Random House dictionary <ref> (Flexner, 1983) </ref>, which includes mainly homophone errors. A few confusion sets not in Random House were added, representing grammatical and typographic errors. <p> The algorithms were run on 21 confusion sets, which were taken largely from the list of "Words Commonly Confused" in the back of the Random House dictionary <ref> (Flexner, 1983) </ref>. The confusion sets were selected on the basis of being frequently-occurring in Brown and WSJ, and include mainly homophone confusions (e.g., fpeace; pieceg). Several confusion sets not in Random House were added, representing grammatical (e.g., famong; betweeng) and typographic (e.g., fmaybe; may beg) errors. 13 Table 1.
Reference: <author> Freund, Y. and Schapire, R. E. </author> <year> (1995). </year> <title> A decision-theoretic generalization of on-line learning and an application to boosting. </title> <booktitle> In Computational Learning Theory: </booktitle> <volume> Eurocolt '95, </volume> <pages> pages 23-37. </pages> <publisher> Springer-Verlag. </publisher>
Reference: <author> Gale, W. A., Church, K. W., and Yarowsky, D. </author> <year> (1993). </year> <title> A method for disambiguating word senses in a large corpus. </title> <journal> Computers and the Humanities, </journal> <volume> 26 </volume> <pages> 415-439. </pages>
Reference-contexts: The problem has started receiving attention in the literature only within about the last half-dozen years. A number of methods have been proposed, either for context-sensitive spelling correction directly, or for related lexical disambiguation tasks. The methods include word trigrams (Mays et al., 1991), Bayesian classifiers <ref> (Gale et al., 1993) </ref>, decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), a combination of part-of-speech trigrams and Bayesian hybrids (Golding and Schabes, 1996), and, more recently, transformation-based learning (Mangu and Brill, 1997), latent semantic analysis (Jones and Martin, 1997), and differential grammars (Powers, 1997).
Reference: <author> Golding, A. R. </author> <year> (1995). </year> <title> A Bayesian hybrid method for context-sensitive spelling correction. </title> <booktitle> In Proc. 3rd Workshop on Very Large Corpora, </booktitle> <address> Boston, MA. </address>
Reference-contexts: Context-sensitive spelling correction therefore fits the characterization presented above, and provides an excellent testbed for studying the performance of multiplicative weight-update algorithms on a real-world task. To evaluate the proposed Winnow-based algorithm, WinSpell, we compare it against BaySpell <ref> (Golding, 1995) </ref>, a statistics-based method that is among the most successful tried for the problem. We first compare WinSpell and BaySpell using the heavily-pruned feature set that BaySpell normally uses (typically 10-1000 features). WinSpell is found to perform comparably to BaySpell under this condition. <p> A number of methods have been proposed, either for context-sensitive spelling correction directly, or for related lexical disambiguation tasks. The methods include word trigrams (Mays et al., 1991), Bayesian classifiers (Gale et al., 1993), decision lists (Yarowsky, 1994), Bayesian hybrids <ref> (Golding, 1995) </ref>, a combination of part-of-speech trigrams and Bayesian hybrids (Golding and Schabes, 1996), and, more recently, transformation-based learning (Mangu and Brill, 1997), latent semantic analysis (Jones and Martin, 1997), and differential grammars (Powers, 1997). <p> Bayesian approach Of the various approaches that have been tried for context-sensitive spelling correction, the Bayesian hybrid method, which we call BaySpell, has been among the most successful, and is thus the method we adopt here as the benchmark for comparison with WinSpell. BaySpell has been described elsewhere <ref> (Golding, 1995) </ref>, and so will only be briefly reviewed here; however, the version used here uses an improved smoothing technique, which is described below.
Reference: <author> Golding, A. R. and Schabes, Y. </author> <year> (1996). </year> <title> Combining trigram-based and feature-based methods for context-sensitive spelling correction. </title> <booktitle> In Proc. 34th Annual Meeting of the Association for Computational Linguistics, </booktitle> <address> Santa Cruz, CA. </address>
Reference-contexts: The methods include word trigrams (Mays et al., 1991), Bayesian classifiers (Gale et al., 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), a combination of part-of-speech trigrams and Bayesian hybrids <ref> (Golding and Schabes, 1996) </ref>, and, more recently, transformation-based learning (Mangu and Brill, 1997), latent semantic analysis (Jones and Martin, 1997), and differential grammars (Powers, 1997).
Reference: <author> Herbster, M. and Warmuth, M. </author> <year> (1995). </year> <title> Tracking the best expert. </title> <booktitle> In Proc. 12th International Conference on Machine Learning, </booktitle> <pages> pages 286-294. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: A similar philosophy, albeit very different technically, is followed by the work on Support Vector Machines (Cortes and Vapnik, 1995). Theoretical analysis has shown Winnow to be able to adapt quickly to a changing target concept <ref> (Herbster and Warmuth, 1995) </ref>. We investigate this issue experimentally in Section 5.5. A further feature of WinSpell is that it can prune poorly-performing attributes, whose weight falls too low relative to the highest weight of an attribute used by the classifier.
Reference: <author> Holte, R. C., Acker, L. E., and Porter, B. W. </author> <year> (1989). </year> <title> Concept learning and the problem of small disjuncts. </title> <booktitle> In Proc. International Joint Conference on Artificial Intelligence, </booktitle> <address> Detroit. </address>
Reference: <author> Jones, M. P. and Martin, J. H. </author> <year> (1997). </year> <title> Contextual spelling correction using latent semantic analysis. </title> <booktitle> In Proc. 5th Conference on Applied Natural Language Processing, </booktitle> <address> Washington, DC. </address>
Reference-contexts: The methods include word trigrams (Mays et al., 1991), Bayesian classifiers (Gale et al., 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), a combination of part-of-speech trigrams and Bayesian hybrids (Golding and Schabes, 1996), and, more recently, transformation-based learning (Mangu and Brill, 1997), latent semantic analysis <ref> (Jones and Martin, 1997) </ref>, and differential grammars (Powers, 1997). <p> Two recent methods use some of the same test sets as we do, and thus can readily be compared: RuleS, a transformation-based learner (Mangu and Brill, 1997); and a method based on latent semantic analysis (LSA) <ref> (Jones and Martin, 1997) </ref>. We also compare to Baseline, the canonical straw man for this task, which simply identifies the most common member of the confusion set during training, and guesses it every time during testing. The results appear in Table 2.
Reference: <author> Katz, S. M. </author> <year> (1987). </year> <title> Estimation of probabilities from sparse data for the language model component of a speech recognizer. </title> <journal> IEEE Trans. on Acoustics, Speech, and Signal Processing, ASSP-35(3):400-401. </journal>
Reference-contexts: While the overall score for BaySpell with interpolative smoothing was 93.8%, it dropped to 85.8% with MLE likelihoods, 89.8% with interpolative smoothing using a fixed weighting constant (Ney et al., 1994), 91.6% with Katz smoothing <ref> (Katz, 1987) </ref>, and 93.4% with Kneser-Ney smoothing (Kneser and Ney, 1995). 5 This shows that while dependency resolution does not improve BaySpell much over Naive Bayes, interpolative smoothing does have a sizable benefit. 5.5.
Reference: <author> Kivinen, J. and Warmuth, M. K. </author> <year> (1995). </year> <title> Exponentiated gradient versus gradient descent for linear predictors. </title> <booktitle> In ACM Symp. on the Theory of Computing. </booktitle>
Reference: <author> Kneser, R. and Ney, H. </author> <year> (1995). </year> <title> Improved backing-off for m-gram language modeling. </title> <booktitle> In Proc. International Conf. on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages 181-184. </pages> <note> Vol. 1. </note>
Reference-contexts: While the overall score for BaySpell with interpolative smoothing was 93.8%, it dropped to 85.8% with MLE likelihoods, 89.8% with interpolative smoothing using a fixed weighting constant (Ney et al., 1994), 91.6% with Katz smoothing (Katz, 1987), and 93.4% with Kneser-Ney smoothing <ref> (Kneser and Ney, 1995) </ref>. 5 This shows that while dependency resolution does not improve BaySpell much over Naive Bayes, interpolative smoothing does have a sizable benefit. 5.5. Across-corpus performance The preceding experiments assumed that the training set will be representative of the test set.
Reference: <author> Kukich, K. </author> <year> (1992). </year> <title> Techniques for automatically correcting words in text. </title> <journal> ACM Computing Surveys, </journal> <volume> 24(4) </volume> <pages> 377-439. </pages>
Reference-contexts: Context-sensitive spelling correction is the task of fixing spelling errors that result in valid words, such as I'd like a peace of cake, where peace was typed when piece was intended. These errors account for anywhere from 25% to over 50% of observed spelling errors <ref> (Kukich, 1992) </ref>; yet they go undetected by conventional spell checkers, such as Unix spell, which only flag words that are not found in a word list. Context-sensitive spelling correction involves learning to characterize the linguistic contexts in which different words, such as piece and peace, tend to occur.
Reference: <author> Kucera, H. and Francis, W. N. </author> <year> (1967). </year> <title> Computational Analysis of Present-Day American English. </title> <publisher> Brown University Press, </publisher> <address> Providence, RI. </address>
Reference-contexts: The sections below describe the experimental methodology, and present the experiments, interleaved with discussion. 5.1. Methodology In the experiments that follow, the training and test sets were drawn from two corpora: the 1-million-word Brown corpus <ref> (Kucera and Francis, 1967) </ref> and a 3/4-million-word corpus of articles from The Wall Street Journal (WSJ) (Marcus et al., 1993). The algorithms were run on 21 confusion sets, which were taken largely from the list of "Words Commonly Confused" in the back of the Random House dictionary (Flexner, 1983).
Reference: <author> Littlestone, N. </author> <year> (1988). </year> <title> Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 285-318. </pages>
Reference-contexts: In this paper, we present a learning algorithm and an architecture with properties suitable for this class of problems. 2 The algorithm builds on recently introduced theories of multiplicative weight-update algorithms. It combines variants of Winnow <ref> (Littlestone, 1988) </ref> and Weighted Majority (Littlestone and Warmuth, 1994). <p> Winnow-based approach There are various ways to use a learning algorithm, such as Winnow <ref> (Littlestone, 1988) </ref>, to do the task of context-sensitive spelling correction. A straightforward approach would be to learn, for each confusion set, a discriminator that distinguishes specifically among the words in that set. <p> The outputs of the classifiers are combined into an output for the cloud using a variant of the Weighted Majority algorithm (Littlestone and Warmuth, 1994). Within each classifier, a variant of the Winnow algorithm <ref> (Littlestone, 1988) </ref> is used. Training occurs whenever the architecture interacts with the world, for example, by reading a sentence of text; the architecture thereby receives new values for its lower-level predicates, which in turn serve as an example for training the high-level ensembles of classifiers.
Reference: <author> Littlestone, N. </author> <year> (1991). </year> <title> Redundant noisy attributes, attribute errors, and linear threshold learning using Winnow. </title> <booktitle> In Proc. 4th Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 147-156, </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: of the largest example seen and in the number of relevant attributes; it is independent of the total number of attributes in the domain (Blum, 1992). 11 Winnow was analyzed in the presence of various kinds of noise, and in cases where no linear threshold function can make perfect classifications <ref> (Littlestone, 1991) </ref>.
Reference: <author> Littlestone, N. </author> <year> (1995). </year> <title> Comparing several linear-threshold learning algorithms on tasks involving superfluous attributes. </title> <booktitle> In Proc. 12th International Conference on Machine Learning, </booktitle> <pages> pages 353-361. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This condition was recently investigated experimentally (on simulated data) <ref> (Littlestone, 1995) </ref>. It was shown that redundant attributes dramatically affect a Bayesian predictor, while superfluous independent attributes have a less dramatic effect, and only when the number of attributes is very large (on the order of 10,000). <p> This is crucial in the analysis of the algorithm and has been shown to be crucial empirically as well <ref> (Littlestone, 1995) </ref>. One of the advantages of the multiplicative update algorithms is their logarithmic dependence on the number of domain features. This property allows one to learn higher-than-linear discrimination functions by increasing the dimensionality of the features.
Reference: <author> Littlestone, N. and Warmuth, M. K. </author> <year> (1994). </year> <title> The weighted majority algorithm. </title> <journal> Information and Computation, </journal> <volume> 108(2) </volume> <pages> 212-261. </pages>
Reference-contexts: In this paper, we present a learning algorithm and an architecture with properties suitable for this class of problems. 2 The algorithm builds on recently introduced theories of multiplicative weight-update algorithms. It combines variants of Winnow (Littlestone, 1988) and Weighted Majority <ref> (Littlestone and Warmuth, 1994) </ref>. Extensive analysis of these algorithms in the COLT literature has shown them to have exceptionally good behavior in the presence of irrelevant attributes, noise, and even a target function changing in time (Littlestone, 1988; Littlestone and Warmuth, 1994; Herbster and Warmuth, 1995). <p> All classifiers within the cloud learn the cloud's 7 high-level concept autonomously, as a function of the same lower-level predicates, but with different values of the learning parameters. The outputs of the classifiers are combined into an output for the cloud using a variant of the Weighted Majority algorithm <ref> (Littlestone and Warmuth, 1994) </ref>. Within each classifier, a variant of the Winnow algorithm (Littlestone, 1988) is used. <p> Weighted Majority Rather than evaluating the evidence for a given word W i using a single classifier, WinSpell combines evidence from multiple classifiers; the motivation for doing so is discussed below. Weighted Majority <ref> (Littlestone and Warmuth, 1994) </ref> is used to do the combination. The basic approach is to run several classifiers in parallel within each cloud to try to predict whether W i belongs in the sentence. Each classifier uses different values of the learning parameters, and therefore makes slightly different predictions.
Reference: <author> Mangu, L. and Brill, E. </author> <year> (1997). </year> <title> Automatic rule acquisition for spelling correction. </title> <booktitle> In Proc. 14th International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The methods include word trigrams (Mays et al., 1991), Bayesian classifiers (Gale et al., 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), a combination of part-of-speech trigrams and Bayesian hybrids (Golding and Schabes, 1996), and, more recently, transformation-based learning <ref> (Mangu and Brill, 1997) </ref>, latent semantic analysis (Jones and Martin, 1997), and differential grammars (Powers, 1997). <p> Two recent methods use some of the same test sets as we do, and thus can readily be compared: RuleS, a transformation-based learner <ref> (Mangu and Brill, 1997) </ref>; and a method based on latent semantic analysis (LSA) (Jones and Martin, 1997). We also compare to Baseline, the canonical straw man for this task, which simply identifies the most common member of the confusion set during training, and guesses it every time during testing.
Reference: <author> Marcus, M. P., Santorini, B., and Marcinkiewicz, M. </author> <year> (1993). </year> <title> Building a large annotated corpus of English: The Penn Treebank. </title> <journal> Computational Linguistics, </journal> <volume> 19(2) </volume> <pages> 313-330. </pages>
Reference-contexts: Methodology In the experiments that follow, the training and test sets were drawn from two corpora: the 1-million-word Brown corpus (Kucera and Francis, 1967) and a 3/4-million-word corpus of articles from The Wall Street Journal (WSJ) <ref> (Marcus et al., 1993) </ref>. The algorithms were run on 21 confusion sets, which were taken largely from the list of "Words Commonly Confused" in the back of the Random House dictionary (Flexner, 1983).
Reference: <author> Mays, E., Damerau, F. J., and Mercer, R. L. </author> <year> (1991). </year> <title> Context based spelling correction. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 27(5) </volume> <pages> 517-522. </pages>
Reference-contexts: The problem has started receiving attention in the literature only within about the last half-dozen years. A number of methods have been proposed, either for context-sensitive spelling correction directly, or for related lexical disambiguation tasks. The methods include word trigrams <ref> (Mays et al., 1991) </ref>, Bayesian classifiers (Gale et al., 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), a combination of part-of-speech trigrams and Bayesian hybrids (Golding and Schabes, 1996), and, more recently, transformation-based learning (Mangu and Brill, 1997), latent semantic analysis (Jones and Martin, 1997), and differential grammars (Powers, 1997).
Reference: <author> Ney, H., Essen, U., and Kneser, R. </author> <year> (1994). </year> <title> On structuring probabilistic dependences in stochastic language modelling. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 8 </volume> <pages> 1-38. </pages>
Reference-contexts: While the overall score for BaySpell with interpolative smoothing was 93.8%, it dropped to 85.8% with MLE likelihoods, 89.8% with interpolative smoothing using a fixed weighting constant <ref> (Ney et al., 1994) </ref>, 91.6% with Katz smoothing (Katz, 1987), and 93.4% with Kneser-Ney smoothing (Kneser and Ney, 1995). 5 This shows that while dependency resolution does not improve BaySpell much over Naive Bayes, interpolative smoothing does have a sizable benefit. 5.5. <p> This is done to provide a fair comparison with the Bayesian method which is a batch approach. 5. When using MLE likelihoods, we broke ties by choosing the word with the largest prior (ties arose when all words had probability 0.0). For Katz smoothing, we used absolute discounting <ref> (Ney et al., 1994) </ref>, as Good-Turing discounting resulted in invalid discounts for our task. For Kneser-Ney smoothing, we used absolute discounting and the backoff distribution based on the "marginal constraint". For all smoothing methods, we set the necessary parameters separately for each word W i using deleted estimation. 6.
Reference: <author> Powers, D. </author> <year> (1997). </year> <title> Learning and application of differential grammars. </title> <booktitle> In Proc. Meeting of the ACL Special Interest Group in Natural Language Learning, </booktitle> <address> Madrid. </address>
Reference-contexts: trigrams (Mays et al., 1991), Bayesian classifiers (Gale et al., 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), a combination of part-of-speech trigrams and Bayesian hybrids (Golding and Schabes, 1996), and, more recently, transformation-based learning (Mangu and Brill, 1997), latent semantic analysis (Jones and Martin, 1997), and differential grammars <ref> (Powers, 1997) </ref>.
Reference: <author> Roth, D. </author> <year> (1998). </year> <title> Learning to resolve natural language ambiguities: A unified approach. </title> <booktitle> In Proc. National Conference on Artificial Intelligence. </booktitle>
Reference-contexts: We pursue an alternative approach: that of learning the contextual characteristics of each word W i individually. This learning can then be used to distinguish word W i from any other word, as well as to perform a broad spectrum of other natural language tasks <ref> (Roth, 1998) </ref>. In the following, we briefly present the general approach, and then concentrate on the task at hand, context-sensitive spelling correction. The approach developed is influenced by the Neuroidal system suggested by Valiant (1994). <p> 63.4 90.5 92.6 93.4 95.7 their, there, they're 850 56.8 73.9 94.5 98.5 weather, whether 61 86.9 85.1 93.4 98.4 100.0 your, you're 187 89.3 91.4 90.9 97.3 Overall (14 sets) 1503 71.1 84.5 88.5 89.9 93.5 Overall (18 sets) 2940 70.6 82.8 91.8 95.6 Overall 4336 74.8 93.8 96.4 <ref> (Roth, 1998) </ref>; is it that Winnow, with its multiplicative update rule, is able to learn a better linear separator than the one given by Bayesian probability theory? Or is it that the non-Winnow enhancements of WinSpell, particularly weighted-majority voting, provide most of the leverage? To address these questions, we ran an
Reference: <author> Valiant, L. G. </author> <year> (1994). </year> <title> Circuits of the Mind. </title> <publisher> Oxford University Press. </publisher>
Reference: <author> Valiant, L. G. </author> <year> (1995). </year> <title> Rationality. </title> <booktitle> In Workshop on Computational Learning Theory, </booktitle> <pages> pages 3-14. </pages>
Reference-contexts: Learning is thus an on-line process that is done on a continuous basis 4 <ref> (Valiant, 1995) </ref>. correction, and in particular for correcting the words fdesert; dessert g. The bottom tier of the network consists of nodes for lower-level predicates, which in this application correspond to features of the domain. For clarity, only five nodes are shown; thousands typically occur in practice.
Reference: <author> Yarowsky, D. </author> <year> (1994). </year> <title> Decision lists for lexical ambiguity resolution: Application to accent restoration in Spanish and French. </title> <booktitle> In Proc. 32nd Annual Meeting of the Association for Computational Linguistics, </booktitle> <address> Las Cruces, NM. </address>
Reference-contexts: A number of methods have been proposed, either for context-sensitive spelling correction directly, or for related lexical disambiguation tasks. The methods include word trigrams (Mays et al., 1991), Bayesian classifiers (Gale et al., 1993), decision lists <ref> (Yarowsky, 1994) </ref>, Bayesian hybrids (Golding, 1995), a combination of part-of-speech trigrams and Bayesian hybrids (Golding and Schabes, 1996), and, more recently, transformation-based learning (Mangu and Brill, 1997), latent semantic analysis (Jones and Martin, 1997), and differential grammars (Powers, 1997).
References-found: 32

