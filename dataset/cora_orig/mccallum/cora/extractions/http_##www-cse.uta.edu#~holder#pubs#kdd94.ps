URL: http://www-cse.uta.edu/~holder/pubs/kdd94.ps
Refering-URL: http://www-cse.uta.edu/~holder/pubs.html
Root-URL: 
Email: Email: holder@cse.uta.edu, cook@cse.uta.edu, djoko@cse.uta.edu  
Title: Substructure Discovery in the SUBDUE System  
Author: Lawrence B. Holder, Diane J. Cook, and Surnjani Djoko 
Keyword: Subdue with non-structural discovery systems.  
Address: Box 19015, Arlington, TX 76019  
Affiliation: Learning and Planning Laboratory Department of Computer Science and Engineering University of Texas at Arlington  
Abstract: Because many databases contain or can be embellished with structural information, a method for identifying interesting and repetitive substructures is an essential component to discovering knowledge in such databases. This paper describes the Subdue system, which uses the minimum description length (MDL) principle to discover substructures that compress the database and represent structural concepts in the data. By replacing previously-discovered substructures in the data, multiple passes of Subdue produce a hierarchical description of the structural regularities in the data. Inclusion of background knowledge guides Subdue toward appropriate substructures for a particular domain or discovery goal, and the use of an inexact graph match allows a controlled amount of deviations in the instance of a substructure concept. We describe the application of Subdue to a variety of domains. We also discuss approaches to combining 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. Cheeseman, J. Kelly, M. Self, J. Stutz, W. Taylor, and D. Freeman. </author> <title> Autoclass: A bayesian classification system. </title> <booktitle> In Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <pages> pages 54-64, </pages> <year> 1988. </year>
Reference-contexts: Although Subdue can accept flat data as input, the discovery process has no model for the distribution of attribute values. Therefore, every value is distinct and unrelated to other values. Section 9 discusses approaches to integrating Subdue with non-structural discovery systems such as AutoClass <ref> [1] </ref> which provide for the inclusion of distribution models. <p> The new set of attributes can then be processed by a nonstructural discovery method, in which the discovered concepts will be biased by the inclusion of structural information. We are pursuing the second possibility by investigating the integration of Subdue with the AutoClass system <ref> [1] </ref>. AutoClass is an unsupervised, non-structural discovery system that identifies a set of classes describing the data. Each attribute is described by a given model (e.g., normal), and each class is described by particular instantiations of the models for each attribute.
Reference: [2] <author> D. Conklin, S. Fortier, J. Glasgow, and F. Allen. </author> <title> Discovery of spatial concepts in crystallographic databases. </title> <booktitle> In Proceedings of the ML92 Workshop on Machine Discovery, </booktitle> <pages> pages 111-116, </pages> <year> 1992. </year>
Reference-contexts: High-probability components of the model represent substructure common to the model's instances. The Labyrinth system [21] extends the Cobweb incremental conceptual clustering system [5] to form hierarchical concepts of the structured objects. The upper-level components of the structured-object hierarchy represent substructures common to the input objects. Conklin and Glasgow <ref> [2] </ref> have developed the i-mem system for constructing an image hierarchy, similar to that of Labyrinth, used for discovering common substructure in a set of images expressed in terms of a set of predefined relations.
Reference: [3] <author> D. J. Cook and L. B. Holder. </author> <title> Substructure discovery using minimum description length and background knowledge. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 1 </volume> <pages> 231-255, </pages> <year> 1994. </year>
Reference-contexts: The discovered substructure concepts allow abstraction over detailed structure in the original data and provide new, relevant attributes for interpreting the data. We describe a new version of our Subdue system <ref> [7, 8, 3] </ref> that discovers interesting substructures in structural data based on the minimum description length principle and optional background knowledge. The next section describes work related to substructure discovery.
Reference: [4] <author> M. Derthick. </author> <title> A minimal encoding approach to feature discovery. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 565-571, </pages> <year> 1991. </year>
Reference-contexts: The MDL principle has been used for decision tree induction [17], image processing [10, 14, 15], concept learning from relational data <ref> [4] </ref>, and learning models of non-homogeneous engineering domains [18]. We demonstrate how the minimum description length principle can be used to discover substructures in complex data. In particular, a substructure is evaluated based on how well it can compress the entire dataset using the minimum description length.
Reference: [5] <author> D. H. Fisher. </author> <title> Knowledge acquisition via incremental conceptual clustering. </title> <journal> Machine Learning, </journal> <volume> 2(2) </volume> <pages> 139-172, </pages> <year> 1987. </year>
Reference-contexts: Segen [20] describes a system for storing graphs using a probabilistic graph model to represent classes of graphs. High-probability components of the model represent substructure common to the model's instances. The Labyrinth system [21] extends the Cobweb incremental conceptual clustering system <ref> [5] </ref> to form hierarchical concepts of the structured objects. The upper-level components of the structured-object hierarchy represent substructures common to the input objects.
Reference: [6] <author> K. S. Fu. </author> <title> Syntactic Pattern Recognition and Applications. </title> <publisher> Prentice-Hall, </publisher> <year> 1982. </year>
Reference-contexts: CLiP addresses the computational complexity by estimating compression afforded by the substructures and using a linear-time approximation to graph isomorphism. Many results in grammatical inference are applicable to constrained classes of graphs (e.g., trees) <ref> [6, 13] </ref>, and discover graph-based production rules that can be interpreted as substructures common to the input graphs [9]. 3 Encoding Graphs The minimum description length (MDL) principle introduced by Rissanen [19] states that the best theory to describe a set of data is that theory which minimizes the description length
Reference: [7] <author> L. B. Holder and D. J. Cook. </author> <title> Discovery of inexact concepts from structural data. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 5(6) </volume> <pages> 992-994, </pages> <year> 1993. </year>
Reference-contexts: The discovered substructure concepts allow abstraction over detailed structure in the original data and provide new, relevant attributes for interpreting the data. We describe a new version of our Subdue system <ref> [7, 8, 3] </ref> that discovers interesting substructures in structural data based on the minimum description length principle and optional background knowledge. The next section describes work related to substructure discovery.
Reference: [8] <author> L. B. Holder, D. J. Cook, and H. Bunke. </author> <title> Fuzzy substructure discovery. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <pages> pages 218-223, </pages> <year> 1992. </year>
Reference-contexts: The discovered substructure concepts allow abstraction over detailed structure in the original data and provide new, relevant attributes for interpreting the data. We describe a new version of our Subdue system <ref> [7, 8, 3] </ref> that discovers interesting substructures in structural data based on the minimum description length principle and optional background knowledge. The next section describes work related to substructure discovery.
Reference: [9] <author> E. Jeltsch and H. J. Kreowski. </author> <title> Grammatical inference based on hyperedge replacement. </title> <booktitle> In Fourth International Workshop on Graph Grammars and Their Application to Computer Science, </booktitle> <pages> pages 461-474, </pages> <year> 1991. </year>
Reference-contexts: Many results in grammatical inference are applicable to constrained classes of graphs (e.g., trees) [6, 13], and discover graph-based production rules that can be interpreted as substructures common to the input graphs <ref> [9] </ref>. 3 Encoding Graphs The minimum description length (MDL) principle introduced by Rissanen [19] states that the best theory to describe a set of data is that theory which minimizes the description length of the entire data set.
Reference: [10] <author> Y. G. Leclerc. </author> <title> Constructing simple stable descriptions for image partitioning. </title> <journal> International Journal of Computational Vision, </journal> <volume> 3(1) </volume> <pages> 73-102, </pages> <year> 1989. </year>
Reference-contexts: The MDL principle has been used for decision tree induction [17], image processing <ref> [10, 14, 15] </ref>, concept learning from relational data [4], and learning models of non-homogeneous engineering domains [18]. We demonstrate how the minimum description length principle can be used to discover substructures in complex data.
Reference: [11] <author> R. Levinson. </author> <title> A self-organizing retrieval system for graphs. </title> <booktitle> In Proceedings of the Fourth National Conference on Artificial Intelligence, </booktitle> <pages> pages 203-206, </pages> <year> 1984. </year>
Reference-contexts: Section 10 summarizes our results with Subdue and discusses directions for future work. 2 Related Work Although specific to the blocks-world domain, Winston's Arch program [24] discovers substructure in order to deepen the hierarchical description of a scene and to group objects into more general concepts. Levinson <ref> [11] </ref> developed a system for storing labeled graphs in which individual graphs are represented by the set of vertices in a universal graph. Subgraphs of the universal graph used by several individual graphs suggest common substructure in the individual graphs.
Reference: [12] <author> R. S. Michalski and R. E. Stepp. </author> <title> Learning from observation: Conceptual clustering. </title> <editor> In R. S. Michalski, J. G. Carbonell, and T. M. Mitchell, editors, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, </booktitle> <pages> pages 331-363. </pages> <publisher> Tioga Publishing Company, </publisher> <year> 1983. </year>
Reference-contexts: The second rule, coverage, measures the fraction of structure in the input graph described by the substructure. The coverage rule is motivated from research in inductive learning and provides that concept descriptions describing more input examples are considered better <ref> [12] </ref>. Although the MDL principle measures the amount of structure, the coverage rule includes the relevance of this savings with respect to the size of the entire input graph.
Reference: [13] <author> L. Miclet. </author> <title> Structural Methods in Pattern Recognition. </title> <publisher> Chapman and Hall, </publisher> <year> 1986. </year>
Reference-contexts: CLiP addresses the computational complexity by estimating compression afforded by the substructures and using a linear-time approximation to graph isomorphism. Many results in grammatical inference are applicable to constrained classes of graphs (e.g., trees) <ref> [6, 13] </ref>, and discover graph-based production rules that can be interpreted as substructures common to the input graphs [9]. 3 Encoding Graphs The minimum description length (MDL) principle introduced by Rissanen [19] states that the best theory to describe a set of data is that theory which minimizes the description length
Reference: [14] <author> E. P. D. Pednault. </author> <title> Some experiments in applying inductive inference principles to surface reconstruction. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1603-1609, </pages> <year> 1989. </year>
Reference-contexts: The MDL principle has been used for decision tree induction [17], image processing <ref> [10, 14, 15] </ref>, concept learning from relational data [4], and learning models of non-homogeneous engineering domains [18]. We demonstrate how the minimum description length principle can be used to discover substructures in complex data.
Reference: [15] <author> A. Pentland. </author> <title> Part segmentation for object recognition. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 82-91, </pages> <year> 1989. </year>
Reference-contexts: The MDL principle has been used for decision tree induction [17], image processing <ref> [10, 14, 15] </ref>, concept learning from relational data [4], and learning models of non-homogeneous engineering domains [18]. We demonstrate how the minimum description length principle can be used to discover substructures in complex data.
Reference: [16] <author> R. Prather. </author> <title> Discrete Mathemetical Structures for Computer Science. </title> <publisher> Houghton Miffin Company, </publisher> <year> 1976. </year>
Reference-contexts: The first rule, compactness, is a generalization of Wertheimer's Factor of Closure, which states that human attention is drawn to closed structures [23]. A closed substructure has at least as many edges as vertices, whereas a non-closed substructure has fewer edges than vertices <ref> [16] </ref>. Compactness is thus defined as the ratio of the number of edges in the substructure to the number of vertices in the substructure. The second rule, coverage, measures the fraction of structure in the input graph described by the substructure.
Reference: [17] <author> J. R. Quinlan and R. L. Rivest. </author> <title> Inferring decision trees using the minimum description length principle. </title> <journal> Information and Computation, </journal> <volume> 80 </volume> <pages> 227-248, </pages> <year> 1989. </year>
Reference-contexts: The MDL principle has been used for decision tree induction <ref> [17] </ref>, image processing [10, 14, 15], concept learning from relational data [4], and learning models of non-homogeneous engineering domains [18]. We demonstrate how the minimum description length principle can be used to discover substructures in complex data. <p> Therefore, a typical row in the adjacency matrix will have much fewer than v 1s, where v is the total number of vertices in the graph. We apply a variant of the coding scheme used by Quinlan and Rivest <ref> [17] </ref> to encode bit strings with length n consisting of k 1s and (n k) 0s, where k t (n k). In our case, row i (1 i v) can be represented as a bit string of length v containing k i 1s.
Reference: [18] <author> R. B. Rao and S. C. Lu. </author> <title> Learning engineering models with the minimum description length principle. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 717-722, </pages> <year> 1992. </year>
Reference-contexts: The MDL principle has been used for decision tree induction [17], image processing [10, 14, 15], concept learning from relational data [4], and learning models of non-homogeneous engineering domains <ref> [18] </ref>. We demonstrate how the minimum description length principle can be used to discover substructures in complex data. In particular, a substructure is evaluated based on how well it can compress the entire dataset using the minimum description length.
Reference: [19] <author> J. Rissanen. </author> <title> Stochastic Complexity in Statistical Inquiry. </title> <publisher> World Scientific Publishing Company, </publisher> <year> 1989. </year>
Reference-contexts: Many results in grammatical inference are applicable to constrained classes of graphs (e.g., trees) [6, 13], and discover graph-based production rules that can be interpreted as substructures common to the input graphs [9]. 3 Encoding Graphs The minimum description length (MDL) principle introduced by Rissanen <ref> [19] </ref> states that the best theory to describe a set of data is that theory which minimizes the description length of the entire data set.
Reference: [20] <author> J. Segen. </author> <title> Graph clustering and model learning by data compression. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <pages> pages 93-101, </pages> <year> 1990. </year>
Reference-contexts: Levinson [11] developed a system for storing labeled graphs in which individual graphs are represented by the set of vertices in a universal graph. Subgraphs of the universal graph used by several individual graphs suggest common substructure in the individual graphs. Segen <ref> [20] </ref> describes a system for storing graphs using a probabilistic graph model to represent classes of graphs. High-probability components of the model represent substructure common to the model's instances. The Labyrinth system [21] extends the Cobweb incremental conceptual clustering system [5] to form hierarchical concepts of the structured objects.
Reference: [21] <author> K. Thompson and P. Langley. </author> <title> Concept formation in structured domains. </title> <editor> In D. H. Fisher and M. Pazzani, editors, </editor> <title> Concept Formation: Knowledge and Experience in Unsupervised Learning, chapter 5. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1991. </year>
Reference-contexts: Segen [20] describes a system for storing graphs using a probabilistic graph model to represent classes of graphs. High-probability components of the model represent substructure common to the model's instances. The Labyrinth system <ref> [21] </ref> extends the Cobweb incremental conceptual clustering system [5] to form hierarchical concepts of the structured objects. The upper-level components of the structured-object hierarchy represent substructures common to the input objects.
Reference: [22] <author> D. Waltz. </author> <title> Understanding line drawings of scenes with shadows. </title> <editor> In P. H. Winston, editor, </editor> <booktitle> The Psychology of Computer Vision. </booktitle> <publisher> McGraw-Hill, </publisher> <year> 1975. </year>
Reference-contexts: Figures 2 shows the graphs that were used for our experiments. To apply Subdue to image data, we extract edge information from the image and construct a graph representing the scene. The vertex labels follow the Waltz labelings <ref> [22] </ref> of junctions of edges. An edge arc represents the edge of an object in the image, and a space arc links non-connecting objects together. Figure 3 shows the scene used for our experiments.
Reference: [23] <author> M. Wertheimer. </author> <title> Laws of organization in perceptual forms. </title> <editor> In W. D. Ellis, editor, </editor> <booktitle> A Sourcebook of Gestalt Psychology, </booktitle> <pages> pages 331-363. </pages> <publisher> Harcourt, Brace and Company, </publisher> <year> 1939. </year>
Reference-contexts: The first rule, compactness, is a generalization of Wertheimer's Factor of Closure, which states that human attention is drawn to closed structures <ref> [23] </ref>. A closed substructure has at least as many edges as vertices, whereas a non-closed substructure has fewer edges than vertices [16]. Compactness is thus defined as the ratio of the number of edges in the substructure to the number of vertices in the substructure.
Reference: [24] <author> P. H. Winston. </author> <title> Learning structural descriptions from examples. </title> <editor> In P. H. Winston, editor, </editor> <booktitle> The Psychology of Computer Vision, </booktitle> <pages> pages 157-210. </pages> <publisher> McGraw-Hill, </publisher> <year> 1975. </year>
Reference-contexts: Section 9 discusses approaches to integrating Subdue with non-structural discovery systems such as AutoClass [1] which provide for the inclusion of distribution models. Section 10 summarizes our results with Subdue and discusses directions for future work. 2 Related Work Although specific to the blocks-world domain, Winston's Arch program <ref> [24] </ref> discovers substructure in order to deepen the hierarchical description of a scene and to group objects into more general concepts. Levinson [11] developed a system for storing labeled graphs in which individual graphs are represented by the set of vertices in a universal graph.
Reference: [25] <author> K. Yoshida, H. Motoda, and N. Indurkhya. </author> <title> Unifying learning methods by colored digraphs. </title> <booktitle> In Proceedings of the Learning and Knowledge Acquisition Workshop at IJCAI-93, </booktitle> <year> 1993. </year>
Reference-contexts: Conklin and Glasgow [2] have developed the i-mem system for constructing an image hierarchy, similar to that of Labyrinth, used for discovering common substructure in a set of images expressed in terms of a set of predefined relations. The CLiP system <ref> [25] </ref> for graph-based induction iteratively discovers patterns (substructures) in graphs by expanding and combining patterns discovered in previous iterations. CLiP addresses the computational complexity by estimating compression afforded by the substructures and using a linear-time approximation to graph isomorphism.
References-found: 25

