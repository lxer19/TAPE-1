URL: http://www.icsi.berkeley.edu/~fosler/papers/esca98_paper.ps.Z
Refering-URL: http://www.icsi.berkeley.edu/~fosler/papers/
Root-URL: http://www.icsi.berkeley.edu
Email: ffosler,morgang@icsi.berkeley.edu  
Title: on ``Modeling pronunciation variation for automatic speech recognition'', Kerkrade, EFFECTS OF SPEAKING RATE AND WORD
Author: Eric Fosler-Lussier Nelson Morgan 
Address: 1947 Center Street, Suite 600, Berkeley, CA 94704, USA  
Affiliation: International Computer Science Institute and University of California, Berkeley  
Date: May 1998  
Note: To appear in ESCA Tutorial and Research Workshop  Netherlands,  
Abstract: The possible set of pronunciations in continuous speech corpora change dynamically with many factors. Two variables, speaking rate and word predictability, seemed to be promising candidates for integration into dynamic ASR pronunciation models; however, our initial efforts to incorporate these factors into phone-level decision tree models met with limited success. In this paper, we confirm the intuition that these factors have an effect on ASR systems, and analyze the relationship between these factors and pronunciations in order to shed light on why the decision trees models failed. We present a statistical exploration of the effects of these factors at the word, syllable, and phone level in the Switchboard corpus. We show that both increased speaking rate and word likelihood can induce a significant shift in probabilities of the pronunciations of frequent words. Using these data, we hypothesize reasons for the difficulty in incorporating these dynamic measures into phone-level decision trees. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Saraclar. </author> <title> Automatic learning of a model for word pronunciations: Status report. In Conversational Speech Recognition Workshop: DARPA Hub-5E Evaluation, </title> <address> Baltimore, MD, </address> <month> May </month> <year> 1997. </year>
Reference-contexts: We argue that a model of word pronunciations should be dynamic; the changes in ASR models should be influenced by contextual factors that determine a probability distribution over pronunciations. The need to constrain the number of pronunciation alternatives was shown in a diagnostic study <ref> [1] </ref> where Switchboard lattices (baseline: 46% word error) were rescored with a dictionary which was augmented on a per-utterance basis with the "correct" pronunciations in the utterance (as determined by phone constraint decoding), reducing word error to 26%.
Reference: [2] <author> M. Weintraub, E. Fosler, C. Galles, Y.-H. Kao, S. Khudan-pur, M. Saraclar, and S. Wegmann. </author> <title> WS96 project report: Automatic learning of word pronunciation from data. </title> <editor> In F. Jelinek, editor, </editor> <booktitle> 1996 LVCSR Summer Research Workshop Technical Reports, chapter 3. Center for Language and Speech Processing, </booktitle> <institution> Johns Hopkins University, </institution> <month> April 25 </month> <year> 1997. </year> <note> Research Notes No. 24. </note>
Reference-contexts: Integration into pronunciation model We have been using decision-tree pronunciation models derived from work at the 1996 Johns Hopkins LVCSR Summer Research Workshop <ref> [2, 3] </ref>. These decision trees determine mappings from baseform phonemes to realized phones using information-theoretic clustering of surrounding phonemic contexts, similar to [4, 5]. <p> We generated a mapping from syllabified Pronlex dictionary baseforms to these hand transcriptions using a dynamic programming technique which uses phonetic features to calculate a distance metric between phones, as in <ref> [2] </ref>. In the cases where multiple pronunciations existed in the dictionary, 1 the closest baseform (in terms of the distance metric) to the realization was used. Pronunciation maps were generated for every baseform word, syllable and phone. <p> For one case (secondary stressed nucleus-only), a surprising reverse effect occurs| the probability of canonical pronunciation increases as rate increases. These data also confirm the commonly held intuition that syllabic stress is an important factor in pronunciation models, as shown by other researchers <ref> [13, 2] </ref>. We then chose to examine the 200 most frequent syllables in the Switchboard corpus, which provide 77% syllable coverage of the 4-hour transcription set, and 75% of the corpus at large.
Reference: [3] <author> E. Fosler, M. Weintraub, S. Wegmann, Y-H Kao, S. Khu-danpur, C. Galles, and M. Saraclar. </author> <title> Automatic learning of word pronunciation from data. </title> <booktitle> In ICSLP-96, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: Integration into pronunciation model We have been using decision-tree pronunciation models derived from work at the 1996 Johns Hopkins LVCSR Summer Research Workshop <ref> [2, 3] </ref>. These decision trees determine mappings from baseform phonemes to realized phones using information-theoretic clustering of surrounding phonemic contexts, similar to [4, 5]. <p> This suggests that speakers tend to delete phones rather than reduce durations during spontaneous speech| many pronunciation variations and a high phone deletion rate can be expected. As we have reported previously <ref> [3] </ref>, in an initial study at WS96 we attempted to characterize this variability for the Switchboard corpus.
Reference: [4] <author> F. Chen. </author> <title> Identification of contextual factors for pronounci-ation networks. </title> <booktitle> In IEEE ICASSP-90, </booktitle> <pages> pages 753-756, </pages> <year> 1990. </year>
Reference-contexts: Integration into pronunciation model We have been using decision-tree pronunciation models derived from work at the 1996 Johns Hopkins LVCSR Summer Research Workshop [2, 3]. These decision trees determine mappings from baseform phonemes to realized phones using information-theoretic clustering of surrounding phonemic contexts, similar to <ref> [4, 5] </ref>. Our goal was to place variables corresponding to speaking rate and word predictability directly into the pronunciation model as a feature for determining splits in the decision tree growing process.
Reference: [5] <author> M. Riley. </author> <title> A statistical model for generating pronunciation networks. </title> <booktitle> In IEEE ICASSP-91, </booktitle> <pages> pages 737-740, </pages> <year> 1991. </year>
Reference-contexts: Integration into pronunciation model We have been using decision-tree pronunciation models derived from work at the 1996 Johns Hopkins LVCSR Summer Research Workshop [2, 3]. These decision trees determine mappings from baseform phonemes to realized phones using information-theoretic clustering of surrounding phonemic contexts, similar to <ref> [4, 5] </ref>. Our goal was to place variables corresponding to speaking rate and word predictability directly into the pronunciation model as a feature for determining splits in the decision tree growing process. <p> Unfortunately, analysis becomes complex when track ing more than just a few pronunciations. Entropy This is a traditional measure for pronunciation learning systems <ref> [5] </ref>, and is a good measure of the spread of pronunciations in a training set.
Reference: [6] <author> N. Morgan and E. Fosler-Lussier. </author> <title> Combining multiple estimators of speaking rate. </title> <booktitle> In IEEE ICASSP-98, </booktitle> <address> Seattle, WA, </address> <month> May </month> <year> 1998. </year>
Reference-contexts: For interactive systems, this particular measure is not determinable at recognition time. As speaking rate estimators can sometimes be unreliable, however, we wanted to see what the best-case scenario might be. Mrate A signal-processing speaking rate measure that we have been developing at ICSI <ref> [6] </ref>. We defer description of this measure and its relationship to our studies until section 4. <p> As mentioned before, we are developing a signal processing measure of rate called mrate. A full description of the algorithm can be found in <ref> [6] </ref>. The measure correlates pretty well with transcribed syllable rate ( ~ :67), although for faster speaking rates, it tends to underestimate the rate somewhat.
Reference: [7] <author> S. Greenberg. </author> <title> WS96 project report: The switchboard transcription project. </title> <editor> In F. Jelinek, editor, </editor> <booktitle> 1996 LVCSR Summer Research Workshop Technical Reports, chapter 6. Center for Language and Speech Processing, </booktitle> <institution> Johns Hopkins University, </institution> <month> April 25 </month> <year> 1997. </year> <note> Research Notes No. 24. </note>
Reference-contexts: We defer description of this measure and its relationship to our studies until section 4. Since we were still developing our baseline Switchboard ASR system, we decided to train and test the decision trees on the portion of the Switchboard corpus that was hand transcribed by linguists at ICSI <ref> [7] </ref>. However, we found that the trees on the whole failed to use our new factors as splitting criteria. <p> Materials The Switchboard data used are approximately four hours of phonetically hand transcribed utterances with syllable boundary markers, provided by ICSI for the Johns Hop-kins Summer Research Workshop series <ref> [7] </ref>. We generated a mapping from syllabified Pronlex dictionary baseforms to these hand transcriptions using a dynamic programming technique which uses phonetic features to calculate a distance metric between phones, as in [2].
Reference: [8] <author> M. Ostendorf, B. Byrne, M. Bacchiani, M. Finke, A Gu-nawardana, K. Ross, S. Roweis, E. Shriberg, D. Talkin, A. Waibel, B. Wheatley, and T. Zeppenfeld. </author> <title> Modeling systematic variations in pronunciation via a language-dependent hidden speaking mode. </title> <editor> In F. Jelinek, editor, </editor> <booktitle> 1996 LVCSR Summer Research Workshop Technical Reports, chapter 4. Center for Language and Speech Processing, </booktitle> <institution> Johns Hopkins University, </institution> <month> April 25 </month> <year> 1997. </year> <note> Research Notes No. 24. </note>
Reference-contexts: This was surprising, particularly as other researchers have found that an earlier measure of ours called enrate (which did not correlate as well with transcribed rate as our current version of mrate) was a useful feature in determining a "hidden mode," which they then used to determine pronunciation probabilities (see <ref> [8] </ref> for details). Thus, we decided to check our assumptions, and conduct an investigation of the relationship between the above factors and pronunciation errors.
Reference: [9] <author> Nikki Mirghafori, Eric Fosler, and Nelson Morgan. </author> <title> Fast speakers in large vocabulary continuous speech recognition: Analysis & antidotes. </title> <booktitle> In Eurospeech-95, </booktitle> <year> 1995. </year>
Reference-contexts: We also used the 1996 JHU workshop HTK recognizer trained with the same Pronlex dictionary (hereafter referred to as the WS96 recognizer) to provide recognition hypotheses for error analysis. While analyzing only one recognizer can certainly highlight the idiosyncrasies of that system, the speech community has seen previously <ref> [9, 10] </ref> that speaking rate affected the output of all WSJ systems in a 1993 evaluation, so we have hope that these studies may be applicable to more than just the WS96 system. 1.3.
Reference: [10] <author> M. A. Siegler and R. M. Stern. </author> <title> On the effects of speech rate in large vocabulary speech recognition systems. </title> <booktitle> In IEEE ICASSP-95, </booktitle> <year> 1995. </year>
Reference-contexts: We also used the 1996 JHU workshop HTK recognizer trained with the same Pronlex dictionary (hereafter referred to as the WS96 recognizer) to provide recognition hypotheses for error analysis. While analyzing only one recognizer can certainly highlight the idiosyncrasies of that system, the speech community has seen previously <ref> [9, 10] </ref> that speaking rate affected the output of all WSJ systems in a 1993 evaluation, so we have hope that these studies may be applicable to more than just the WS96 system. 1.3.
Reference: [11] <author> J. Bernstein, G. Baldwin, M. Cohen, H. Murveit, and M. Weintraub. </author> <title> Phonological studies for speech recognition. </title> <booktitle> In DARPA Speech Recognition Workshop, </booktitle> <pages> pages 41|48, </pages> <month> February </month> <year> 1992. </year> <institution> Palo Alto, California. </institution>
Reference-contexts: Previous work As one moves to spontaneous speech corpora, such as Switchboard, the variability in word pronunciations increases. An immediate observation is that typical conversational speech is faster than typical read speech. The differences between these two speaking modes are more complex, however. Bernstein et al. <ref> [11] </ref> show that pronunciations in spontaneous speech are different from fast read speech. Although the number of words per second in spontaneous speech is similar to fast reading for most speakers, the number of phones per second for spontaneous speech is more like that for normal reading.
Reference: [12] <author> S. Greenberg. </author> <title> On the origins of speech intelligibility in the real world. In Proceedings of the ESCA Workshop on Robust Speech Recognition for Unknown Communication Channels, </title> <month> April </month> <year> 1997. </year>
Reference-contexts: This allowed us to cluster some of the data from the word-level experiments, but also gave us more context than on the phone level. In addition, it has been suggested that pronunciation phenomena are more often affected by syllabically internal rather than external context <ref> [12] </ref>.
Reference: [13] <author> M. Finke and A. Waibel. </author> <title> Flexible transcription alignment. </title> <booktitle> In 1997 IEEE Workshop on Automatic Speech Recognition and Understanding, </booktitle> <pages> pages 34-40, </pages> <address> Santa Barabara, CA, </address> <month> De-cember </month> <year> 1997. </year>
Reference-contexts: For one case (secondary stressed nucleus-only), a surprising reverse effect occurs| the probability of canonical pronunciation increases as rate increases. These data also confirm the commonly held intuition that syllabic stress is an important factor in pronunciation models, as shown by other researchers <ref> [13, 2] </ref>. We then chose to examine the 200 most frequent syllables in the Switchboard corpus, which provide 77% syllable coverage of the 4-hour transcription set, and 75% of the corpus at large.
Reference: [14] <author> Q. Summerfield. </author> <title> Articulatory rate and perceptual constancy in phonetic perception. </title> <journal> Journal of Experimental Psychology: Human Performance and Perception, </journal> <volume> 7 </volume> <pages> 1074-1095, </pages> <year> 1981. </year>
Reference-contexts: We are also looking into more localized measures of rate (i.e. estimating over a few syllables). Psychologists have suggested that many rate effects are localized <ref> [14] </ref>, and we have found that estimating rate using a small number of syllables locally, as opposed to interpausally, sharpens some of the pronunciation distinctions seen above. 5.
References-found: 14

