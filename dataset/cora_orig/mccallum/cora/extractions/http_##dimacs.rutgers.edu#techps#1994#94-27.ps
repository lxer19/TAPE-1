URL: http://dimacs.rutgers.edu/techps/1994/94-27.ps
Refering-URL: http://dimacs.rutgers.edu/TechnicalReports/1994.html
Root-URL: http://www.cs.rutgers.edu
Title: Parallel Implementation of N-body Algorithms  
Author: by Pangfeng Liu ;; 
Address: Piscataway, New Jersey 08855-1179  
Affiliation: DIMACS center Rutgers University  
Note: The  2 ONR Grant N00014-93-1-0944, NSF/DARPA grant CCR-89-08285, DARPA contract DABT 63 91-C-0031 3 DIMACS NSF grant STC-91-19999 DIMACS is a cooperative project of Rutgers University, Princeton University, AT&T Bell Laboratories and Bellcore. DIMACS is an NSF Science and Technology Center, funded under contract STC-91-19999; and also receives support from the New Jersey Commission on Science and Technology.  
Abstract: DIMACS Technical Report 94-27 May 1994 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S.J. Aarseth, M. Henon, and R. Wielen. </author> <title> Astronomy and Astrophysics, </title> <type> 37, </type> <year> 1974. </year>
Reference-contexts: The vector units are programmed in CDPEAC which provides an interface between C and the DPEAC assembly language for vector units [50]. The rest of the program is written in C. The experiments sketched here included three input distributions: the uniform distribution, the Plummer distribution <ref> [1] </ref> with mass M = 1 within a sphere, and two Plummer distributions at a colliding course. The Plummer model has very large density in the center. All three cases contained about 10 million bodies. 5.1 Breakdown of Running Time models at a colliding course, and uniform distribution. <p> Then we say that m became ready for q at time step t . Lemma 2 There exists a partition = 1 ; : : : ; m of the interval <ref> [1; T ] </ref> and a set R of T m messages (not including those sent by S) each of which satisfies the following property: if the message became ready during i its destination node is Q i . - 67 - Proof of Lemma. <p> The first step of the proof is similar to the proof of Lemma 2. For a fixed run of the algorithm we construct a partition, = f 1 ; : : : ; h g, of the time interval <ref> [1; T ] </ref>. Next, we construct a signature set R of non-special, critical vertices each of which became ready for some Q i during the corresponding time interval i . <p> Therefore i contains [T i ; T i+1 ) and part 3 follows. From part 3 of Lemma 5 the union of all i covers the time interval <ref> [1; T 1] </ref>, consequently we can define a partition = f 1 ; : : : ; h1 g of the interval [1; T 1] as follows. h1 = h1 [ j ; 1 i &lt; h 1: Definition. <p> From part 3 of Lemma 5 the union of all i covers the time interval <ref> [1; T 1] </ref>, consequently we can define a partition = f 1 ; : : : ; h1 g of the interval [1; T 1] as follows. h1 = h1 [ j ; 1 i &lt; h 1: Definition. Let R i be the set of critical vertices which are not special (v 62 S) but are ready for Q i during the interval i . <p> identified the signature set, it remains to estimate the probability that the signature set is small when T is large. 2 This estimation is completed in the following section. 7.2.3 A Refined Partition As mentioned in Section 7.1, our strategy will be to find a new partition of the interval <ref> [1; T ] </ref> and a corresponding signature set which is guaranteed to be large. In this section we identify O (jRj + h) arrival windows which cover the interval [1; T ]. <p> 7.2.3 A Refined Partition As mentioned in Section 7.1, our strategy will be to find a new partition of the interval <ref> [1; T ] </ref> and a corresponding signature set which is guaranteed to be large. In this section we identify O (jRj + h) arrival windows which cover the interval [1; T ]. We will find arrival windows to cover each ! i (1 i &lt; h) and argue that the sum of number of windows in each ! i is O (jRj + h). The next section will identify the new partition and signature set. Definitions. <p> From the discussion above every vertex in X ij must become ready for Q ij during fl ij . Finally, let X = [X ij and V = C [ R [ S. Since the arrival windows cover the interval <ref> [1; T ] </ref>, it follows that jXj T jV j. 7.2.4 Execution Templates Our goal in this section is to estimate the probability of the event that T and R are both large. We proceed in two stages; first we characterize the completion time in terms of an execution template. <p> the signature set, * C i ; 1 i &lt; h; are disjoint sets of tree vertices that are children of s i [ R i and become ready within N + i " ! * = f 1 ; : : : ; h1 g is a partition of <ref> [1; T 1] </ref>, * fl = f 11 ; : : : ; 1k 1 ; : : : ; h1 1 ; : : : ; h1 k h1 g is a partition of [1; T ], * X ij , 1 i &lt; h, 0 j k i are <p> * = f 1 ; : : : ; h1 g is a partition of [1; T 1], * fl = f 11 ; : : : ; 1k 1 ; : : : ; h1 1 ; : : : ; h1 k h1 g is a partition of <ref> [1; T ] </ref>, * X ij , 1 i &lt; h, 0 j k i are sets of tree vertices that are disjoint from V and ready for Q fl ij during fl ij , X = [X ij and jXj T jV j, * Q = fQ 1 ; : <p> Lemma 9 1. The parent of each vertex in c i becomes ready for Q i during i , 2. ! i is the union of receive delays of vertices f i , c i , and g i (if it exists), and 3. i=1 i = <ref> [1; T 1] </ref>. Proof. The parent of each vertex v in c i must become ready before v; furthermore, it cannot become ready before f i . <p> at or after T f i from the definition of f i so it cannot be received before T f thus the interval i contains [T i ; T i+1 ), and (3) follows. - 81 - From (3) of Lemma 9 the union of all i cover the interval <ref> [1; T 1] </ref>. We can therefore define a partition of [1; T 1] as follows, h1 = h1 [ j ; 1 i &lt; h 1: Definition. <p> f i so it cannot be received before T f thus the interval i contains [T i ; T i+1 ), and (3) follows. - 81 - From (3) of Lemma 9 the union of all i cover the interval <ref> [1; T 1] </ref>. We can therefore define a partition of [1; T 1] as follows, h1 = h1 [ j ; 1 i &lt; h 1: Definition. As before, a critical vertex v is in the signature set R if v is not a special vertex (v =2 S) and v becomes ready for Q i during i . <p> As a result the number of receive delays in F [ G [ C is at most 2h + 2 (jSj + jRj) = 4h + 2jRj. The receive delays identified thus far cover the interval <ref> [1; T ] </ref> and there are no more than 4h + 2jRj in number. They are not necessarily non-overlapping, however.
Reference: [2] <author> C. Anderson. </author> <title> An implementation of the fast multipole method without multipoles. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 13, </volume> <year> 1992. </year>
Reference-contexts: Greengard and Rokhlin [23] presented an O (N ) Fast Multipole Method which is provably correct to any fixed accuracy. The underlying numerical representations were subsequently refined and simplified by Zhao [55] and Anderson <ref> [2] </ref>. Recently, Sundaram [46] extended the fast multipole method to allow different bodies to be updated at different rates; this reduces 5 the arithmetic complexity over a large time period.
Reference: [3] <author> R. Anderson. </author> <title> Nearest neighbor trees and N-body simulation. </title> <type> manuscript, </type> <year> 1994. </year>
Reference-contexts: Figure 2.3 is an example of a recursive partition in two dimensions; the corresponding quad tree, which we call the BH-tree, is shown in Figure 2.4. Alternative tree decompositions have been suggested <ref> [3, 13] </ref>; the Barnes-Hut algorithm applies to these as well. The sequential Barnes-Hut algorithm constructs the BH-tree by inserting bodies into the cluster hierarchy one at a time. The ith body is added into the BH-tree consisting of the first i 1 bodies.
Reference: [4] <author> A.W. Appel. </author> <title> An efficient program for many-body simulation. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 6, </volume> <year> 1985. </year>
Reference-contexts: While this method is conceptually simple, vectorizes well, and is the algorithm of choice for many applications, its fi (N 2 ) arithmetic complexity rules it out for large-scale simulations involving millions of bodies. Beginning with Appel <ref> [4] </ref> and Barnes and Hut [7], there has been a flurry of interest in faster N-body algorithms. Experimental evidence shows that heuristic algorithms require far fewer operations for most initial distributions of interest, and within acceptable error bounds.
Reference: [5] <author> A. Bar-Noy and S. Kipnis. </author> <title> Designing broadcasting algorithms in the postal model for message-passing systems. </title> <booktitle> In 4th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <year> 1992. </year>
Reference-contexts: Our atomic message model differs from other communication models by emphasizing resource-efficient communications. However, it is still possible to link the atomic model to various communication models like the LogP model [14], postal model <ref> [5] </ref>, optical models [21], some PRAM models including QRQW PRAM [22] and Distributed Memory Machine [16, 27]. The relations among these models is worth further investigation.
Reference: [6] <author> J. Barnes. </author> <title> A modified tree code: Don't laugh; it runs. </title> <journal> Journal of Computational Physics, </journal> <volume> 87, </volume> <year> 1990. </year>
Reference-contexts: Grouping bodies does increase the number of calculations, but it also makes them more regular. More significant is the reduction in the time spent traversing the tree. This idea of grouping bodies was earlier used by Barnes <ref> [6] </ref>. A further reduction in tree traversal is possible by caching essential nodes. The key observation is that the set of essential nodes for two distinct groups that are close together in space are likely to have many elements in common. <p> Makino [34] suggested an algorithm that vectorizes tree traversals for different bodies. Finally Barnes <ref> [6] </ref> suggested an algorithm which reduces the number of tree traversals by computing accelerations on a group of bodies at a time. Each 31 member of the same group use the same essential data for force calculation. <p> The garbage collection avoids unnecessary data copying by allowing holes in the cache array, and still minimizes the number of blocks vector units will process. 4.5 Computing Acceleration in Groups By computing acceleration for multipole bodies as a group, Barnes's force computation algorithm <ref> [6] </ref> reduces the number of tree traversals without loss of accuracy. The bodies in one group share the same essential data collected from a single tree traversal. To maintain accuracy, the opening test divides a cluster if any body in a group cannot apply approximation on it. <p> The complication in BH-tree building can be solved by level adjustments and representative construction with very small overheads. The ORB bisectors can be adjusted to balance workload continuously with negligible remapping cost. Barnes's technique <ref> [6] </ref> of grouping bodies together for tree traversal is extremely effective. The extra computation due to unnecessary opening of internal nodes is much smaller than the time saved in preparing essential data (by either tree traversals or caching).
Reference: [7] <author> J. Barnes and P. Hut. </author> <title> A hierarchical O(N log N ) force-calculation algorithm. </title> <journal> Nature, </journal> <volume> 324, </volume> <year> 1986. </year>
Reference-contexts: While this method is conceptually simple, vectorizes well, and is the algorithm of choice for many applications, its fi (N 2 ) arithmetic complexity rules it out for large-scale simulations involving millions of bodies. Beginning with Appel [4] and Barnes and Hut <ref> [7] </ref>, there has been a flurry of interest in faster N-body algorithms. Experimental evidence shows that heuristic algorithms require far fewer operations for most initial distributions of interest, and within acceptable error bounds. <p> The common approach in adaptive N-body methods <ref> [7, 42, 44, 53, 54] </ref> is to rebuild the entire Barnes-Hut tree. In contrast our implementation dynamically adjusts the BH-tree to conform to the new distribution of bodies.
Reference: [8] <author> J.J. Bartholdi and L.K. Platzman. </author> <title> Heuristics based on spacefilling curves for combinatorial problems in Euclidean space. </title> <journal> Management Science, </journal> <volume> 34, </volume> <year> 1988. </year>
Reference-contexts: The worst-case length of a space-filling curve for N bodies in the unit square is fi ( p fact it has been established that the length of the Peano-Hilbert curve is always within a O (log N ) factor of the optimal TSP tour in d-dimension <ref> [8, 10, 38] </ref>. Theorem 3 [38] Suppose that N bodies are distributed within the unit square (alternatively, the unit cube).
Reference: [9] <author> C. Berge. </author> <title> Graphs and Hypergraphs. </title> <publisher> North-Holland, </publisher> <year> 1973. </year>
Reference-contexts: We present the proof of Theorem 4 and 5 in Chapter 7. 6.4 Message Scattering The off-line version of the message scattering problem in which the lists can be reordered is easily solved using standard bipartite graph edge-coloring techniques <ref> [9, 20] </ref>. If r is the maximum number of messages received by any node, then maxfr; mg steps are necessary and sufficient. However, the distributed version of the problem, without reordering, is not as simple.
Reference: [10] <author> D. Bertsimas and M. Grigni. </author> <title> Worst-case examples for the spacefilling curve heuristic for the Euclidean traveling salesman problem. Operation Research Letter, </title> <type> 8, </type> <year> 1989. </year>
Reference-contexts: The worst-case length of a space-filling curve for N bodies in the unit square is fi ( p fact it has been established that the length of the Peano-Hilbert curve is always within a O (log N ) factor of the optimal TSP tour in d-dimension <ref> [8, 10, 38] </ref>. Theorem 3 [38] Suppose that N bodies are distributed within the unit square (alternatively, the unit cube).
Reference: [11] <author> S. Bhatt, M. Chen, C. Lin, and P. Liu. </author> <title> Abstractions for parallel N-body simulation. </title> <booktitle> In Scalable High Performance Computing Conference SHPCC-92, </booktitle> <year> 1992. </year>
Reference-contexts: The low overhead is remarkable especially since we use the CM-5 vector units to accelerate the numerical calculations. Our methods have been used as the basis of a library that provides abstractions for tree computations on distributed-memory machines <ref> [11] </ref>. The goal of the library is to allow different N-body codes to be written at a high-level, independent of the details of data structure management and communication. <p> For this reason we separated the construction of the tree from the details of later stages of the algorithm. The interested reader is referred to <ref> [11] </ref> for further details concerning a library of abstractions for N-body algorithms. 3.2.1 Representation We represent the global BH-tree as follows. Since the oct-tree partitioning always divides a box at the center, each internal node represents a fixed region of space. <p> When messages wait a long time, there is the danger that communication delays can cause processor idling, thereby reducing overall performance greatly. In many applications it is also common practice to reduce communication costs (due primarily to system overheads) by aggregating data into fewer but longer, atomic, messages <ref> [11] </ref>. 1 First the sender notifies the receiver of the message length. Upon receipt of this notification, the receiver allocates sufficient buffer space and sends back an acknowledgment. This establishes a link between the sender and the receiver and the message is transmitted in the third step.
Reference: [12] <author> R. Blumofe and C. Leiserson. </author> <title> Space-efficient scheduling of multi-threaded computations. </title> <booktitle> In 25th Annual ACM Symposium on Theory of Computing, </booktitle> <year> 1993. </year>
Reference-contexts: The breadth-first nature of the protocol maintains a frontier of the tree within the message queues of all processors, and the size of the frontier may be much larger than the number of queues. Leiserson and Blumofe <ref> [12] </ref> proposed a new search procedure in which each processor performs a depth-first search so that space requirement is significantly reduced. Our reactive protocol for the atomic message model will be able to reduce queue length requirements if it can adopt Leiserson and Blumofe's space-efficient scheduling.
Reference: [13] <author> P. Callahan and S. Kosaraju. </author> <title> A decomposition of multi-dimension point-sets with applications to k-nearest-neighbors and N-body potential fields. </title> <booktitle> 24th Annual ACM Symposium on Theory of Computing, </booktitle> <year> 1992. </year> <month> 87 </month>
Reference-contexts: While the fast multipole method uses only O (N ) arithmetic operations, the number of operations to build and maintain the underlying data structures was not considered in the papers cited above. Callahan and Kosaraju <ref> [13] </ref> developed better data structures to cluster the bodies hierarchically; they bound the time for both data structure and numerical operations by O (N ). <p> Figure 2.3 is an example of a recursive partition in two dimensions; the corresponding quad tree, which we call the BH-tree, is shown in Figure 2.4. Alternative tree decompositions have been suggested <ref> [3, 13] </ref>; the Barnes-Hut algorithm applies to these as well. The sequential Barnes-Hut algorithm constructs the BH-tree by inserting bodies into the cluster hierarchy one at a time. The ith body is added into the BH-tree consisting of the first i 1 bodies.
Reference: [14] <author> D. Culler, R. Karp, D. Patterson, A. Sahay, K. Schauser, E. Santos, R. Subramonian, and T. Eicken. </author> <title> Logp: Towards a realistic model of parallel computation. </title> <booktitle> In 4th ACM PPOPP, </booktitle> <year> 1993. </year>
Reference-contexts: While message-passing systems have been in use for over a decade, relatively few results concerning the complexity of message-passing protocols are available. One reason for this discrepancy is the lack of theoretical models that appropriately capture issues related to communication; as stated in <ref> [14] </ref>, most theoretical models "encourage exploitation of formal loopholes, rather than rewarding development of techniques that yield performance across a range of current and future parallel machines." We propose an atomic model [33] to study the performance of message-passing programs. <p> Finally, the atomic model can be viewed as the limiting version of the LogP model <ref> [14] </ref>; with long messages of equal length the latency, overhead and gap parameters of the LogP model can be lumped into a single, unit time delay. <p> Our atomic message model differs from other communication models by emphasizing resource-efficient communications. However, it is still possible to link the atomic model to various communication models like the LogP model <ref> [14] </ref>, postal model [5], optical models [21], some PRAM models including QRQW PRAM [22] and Distributed Memory Machine [16, 27]. The relations among these models is worth further investigation.
Reference: [15] <author> R. Das, J. Saltz, D. Mavriplis, J. Wu, and H. Berryman. </author> <title> Unstructured mesh problems, Parti primitives and the ARF compiler. </title> <booktitle> In Parallel Processing for Scientific Computation, Proceedings of the Fifth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <year> 1991. </year>
Reference-contexts: Many high-level languages, including Fortran-D [19], CM-Fortran [48], Vienna-Fortran [17] support parallel operations on uniform parallel arrays. However, none of them supports parallel operations on irregular data structures. Some runtime systems do support run-time pointer (or index) interpretation for remote data access <ref> [15, 18, 30, 36] </ref>, but these approaches take advantage of static data access and communication patterns, and do not provide efficient solutions for dynamically changing data distribution and communication patterns.
Reference: [16] <editor> M. Dietzfelbinger and F. Meyer auf der Heide. </editor> <title> Simple, efficient shared memory simulations. </title> <booktitle> In 5th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <year> 1993. </year>
Reference-contexts: Our atomic message model differs from other communication models by emphasizing resource-efficient communications. However, it is still possible to link the atomic model to various communication models like the LogP model [14], postal model [5], optical models [21], some PRAM models including QRQW PRAM [22] and Distributed Memory Machine <ref> [16, 27] </ref>. The relations among these models is worth further investigation. Although the randomized reactive protocol completes the tree search within constant factor of optimal time steps with high probability, the number of tree nodes stored in the message queues may be very large.
Reference: [17] <author> B. Chapman et. al. </author> <title> Vienna FORTRAN A Fortran Language Extension for Distributed Memory Multiprocessors. In High Performance FORTRAN Forum, </title> <year> 1992. </year>
Reference-contexts: While tremendous research effort has concentrated on parallelizing scientific computations with uniform structures, efficient parallelizations of dynamic and irregular computations are not yet well understood. Many high-level languages, including Fortran-D [19], CM-Fortran [48], Vienna-Fortran <ref> [17] </ref> support parallel operations on uniform parallel arrays. However, none of them supports parallel operations on irregular data structures.
Reference: [18] <author> S. Hiranandani et. al. </author> <title> Performance of hashed cache data migration schemes on multi-computers. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 12, </volume> <year> 1991. </year>
Reference-contexts: Many high-level languages, including Fortran-D [19], CM-Fortran [48], Vienna-Fortran [17] support parallel operations on uniform parallel arrays. However, none of them supports parallel operations on irregular data structures. Some runtime systems do support run-time pointer (or index) interpretation for remote data access <ref> [15, 18, 30, 36] </ref>, but these approaches take advantage of static data access and communication patterns, and do not provide efficient solutions for dynamically changing data distribution and communication patterns.
Reference: [19] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, C. Tseng, and M. Wu. </author> <title> Fortran D Language Specification. In High Performance FORTRAN Forum, </title> <month> January </month> <year> 1992. </year>
Reference-contexts: Coping with these issues requires new ways to design data structures and communication protocols for distributed-memory architectures. While tremendous research effort has concentrated on parallelizing scientific computations with uniform structures, efficient parallelizations of dynamic and irregular computations are not yet well understood. Many high-level languages, including Fortran-D <ref> [19] </ref>, CM-Fortran [48], Vienna-Fortran [17] support parallel operations on uniform parallel arrays. However, none of them supports parallel operations on irregular data structures.
Reference: [20] <author> H. Gabow. </author> <title> Using Euler partitions to edge color bipartite multigraphs. </title> <journal> International Journal of Computer and Information Sciences, </journal> <volume> 5, </volume> <year> 1976. </year>
Reference-contexts: We present the proof of Theorem 4 and 5 in Chapter 7. 6.4 Message Scattering The off-line version of the message scattering problem in which the lists can be reordered is easily solved using standard bipartite graph edge-coloring techniques <ref> [9, 20] </ref>. If r is the maximum number of messages received by any node, then maxfr; mg steps are necessary and sufficient. However, the distributed version of the problem, without reordering, is not as simple.
Reference: [21] <author> M. Gereb-Graus and T. Tsantilas. </author> <title> Efficient optical communication in parallel computers. </title> <booktitle> In 4th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <year> 1992. </year>
Reference-contexts: Unlike these models however, we explicitly account for message contention and do not allow multiple messages to be received in one step by a processor. The issue of contention at the receiving module is also addressed in models for optical communication <ref> [21] </ref> and module parallel computers [27, 35]. <p> Our atomic message model differs from other communication models by emphasizing resource-efficient communications. However, it is still possible to link the atomic model to various communication models like the LogP model [14], postal model [5], optical models <ref> [21] </ref>, some PRAM models including QRQW PRAM [22] and Distributed Memory Machine [16, 27]. The relations among these models is worth further investigation.
Reference: [22] <author> P. Gibbons, Y. Matias, and V. Ramachandran. QRQW: </author> <title> Accounting for concurrency in PRAMs and asynchronous PRAMs. </title> <type> Technical Report BL011211-930301-05TM, </type> <institution> Bel-lLab, </institution> <year> 1993. </year>
Reference-contexts: Our atomic message model differs from other communication models by emphasizing resource-efficient communications. However, it is still possible to link the atomic model to various communication models like the LogP model [14], postal model [5], optical models [21], some PRAM models including QRQW PRAM <ref> [22] </ref> and Distributed Memory Machine [16, 27]. The relations among these models is worth further investigation. Although the randomized reactive protocol completes the tree search within constant factor of optimal time steps with high probability, the number of tree nodes stored in the message queues may be very large.
Reference: [23] <author> L. Greengard and V. Rokhlin. </author> <title> A fast algorithm for particle simulations. </title> <journal> Journal of Computational Physics, </journal> <volume> 73, </volume> <year> 1987. </year>
Reference-contexts: Greengard and Rokhlin <ref> [23] </ref> presented an O (N ) Fast Multipole Method which is provably correct to any fixed accuracy. The underlying numerical representations were subsequently refined and simplified by Zhao [55] and Anderson [2]. <p> Choosing L to be much larger than 1 speeds up the computation phase, but makes level-adjustment somewhat tricky. We made this modification mainly to adapt Salmon's multipole expansion [42] for better simulation accuracy. The same approach is also used in Greengard-Rokhlin's Fast - 21 - .. tree. Multipole Method <ref> [23] </ref>. The level adjustment procedure also makes it easy to update the BH tree incrementally. We can insert and delete bodies directly on the local trees because we do not explicitly maintain the global tree.
Reference: [24] <author> L. Hernquist. </author> <title> Vectorization of tree traversals. </title> <journal> Journal of Computational Physics, </journal> <volume> 87, </volume> <year> 1987. </year>
Reference-contexts: In his thesis [44] Singh surveyed many different approaches for vectorizing tree traversal and force calculation, and concluded that none of them provides sufficient speed up for conducting large-scale N-body calculations on vector supercomputers. Hernquist <ref> [24] </ref> suggested an algorithm that vectorizes the interaction calculations between the current body and the essential data in the same level, and computes the overall acceleration by computing the interactions one level at a time. Makino [34] suggested an algorithm that vectorizes tree traversals for different bodies.
Reference: [25] <author> W.J. Kaufmann III and L.L. </author> <title> Smarr. </title> <booktitle> Supercomputing and the Transformation of Science. </booktitle> <publisher> Scientific American Library, </publisher> <year> 1993. </year>
Reference-contexts: This massive computing power will enable researchers to solve problems that are now computationally intractable. Many scientific and engineering computations are notorious for their extensive computing requirements. As an example, consider the problem of weather prediction. In 1990 Wilhelm-son <ref> [25] </ref> simulated the evolution of thunderstorms over a region of 5400 square kilometers. On the Cray-2 supercomputer the simulation ran twice as fast as the thunderstorm would evolve in the real world.
Reference: [26] <author> J. F. Leathrum Jr. and J. Board Jr. </author> <title> The parallel fast multipole algorithm in three dimensions. </title> <type> manuscript, </type> <year> 1992. </year> <month> - 89 </month> - 
Reference-contexts: Singh etal. [44, 45] implemented the Barnes-Hut algorithm for the experimental DASH prototype. This thesis contrasts our approach and conclusions with both these efforts. Parallel implementations of the fast multipole method have been developed recently as well. Board and Leathrum <ref> [26] </ref> have implemented the 3D adaptive Fast Multipole Method on shared memory machines including the KSR [26], Zhao and Johnsson [56, 57] implemented their version of non-adaptive 3D multipole method on the Connection Machine CM-2, and Singh etal. [44] have implemented the 2D adaptive fast multipole method on the DASH prototype. <p> This thesis contrasts our approach and conclusions with both these efforts. Parallel implementations of the fast multipole method have been developed recently as well. Board and Leathrum <ref> [26] </ref> have implemented the 3D adaptive Fast Multipole Method on shared memory machines including the KSR [26], Zhao and Johnsson [56, 57] implemented their version of non-adaptive 3D multipole method on the Connection Machine CM-2, and Singh etal. [44] have implemented the 2D adaptive fast multipole method on the DASH prototype.
Reference: [27] <author> R. Karp, M. Luby, and F. Meyer auf der Heide. </author> <title> Efficient PRAM simulation on a distributed memory machine. </title> <booktitle> In 24th Annual ACM Symposium on Theory of Computing, </booktitle> <year> 1992. </year>
Reference-contexts: Unlike these models however, we explicitly account for message contention and do not allow multiple messages to be received in one step by a processor. The issue of contention at the receiving module is also addressed in models for optical communication [21] and module parallel computers <ref> [27, 35] </ref>. A key feature which distinguishes the atomic model is that once a message has been sent it cannot be retrieved; the sending processor must wait for the network to clear the send buffer after the message has been copied into the receive buffer at the destination. <p> Our atomic message model differs from other communication models by emphasizing resource-efficient communications. However, it is still possible to link the atomic model to various communication models like the LogP model [14], postal model [5], optical models [21], some PRAM models including QRQW PRAM [22] and Distributed Memory Machine <ref> [16, 27] </ref>. The relations among these models is worth further investigation. Although the randomized reactive protocol completes the tree search within constant factor of optimal time steps with high probability, the number of tree nodes stored in the message queues may be very large.
Reference: [28] <author> R.M. Karp and Y. Zhang. </author> <title> A randomized parallel branch-and-bound procedure. </title> <booktitle> In 20th Annual ACM Symposium on Theory of Computing, </booktitle> <year> 1988. </year>
Reference-contexts: We believe this is reasonable in applications that involve the atomic transfer of large data sets. Unit-delay assumptions are also made in the literature on PRAMs and complete networks <ref> [28, 29] </ref>. Unlike these models however, we explicitly account for message contention and do not allow multiple messages to be received in one step by a processor. The issue of contention at the receiving module is also addressed in models for optical communication [21] and module parallel computers [27, 35]. <p> Branch-and-bound search is similar to backtrack search, except that only a subtree of the search tree must necessarily be explored. Following Karp and Zhang <ref> [28, 29] </ref>, we model a branch-and-bound tree as a binary search tree, each of whose vertices has an associated cost. The cost of each vertex is strictly less than the cost of each of its children (for simplicity we assume that all vertex costs are distinct). <p> Non-critical vertices can, in principle, be pruned by the search process and need not be explored. Tight upper bounds for branch-and-bound, and hence for backtrack search, were given by Karp and Zhang <ref> [28] </ref> on the complete network which allows multiple messages to be simultaneously received at each node, and on the concurrent PRAM which essentially allows unsuccessful writes to be detected. The basic idea was to send each node to a random processor for further exploration. <p> The probability that the time until all messages have been received exceeds km is bounded by O (e m ), for sufficiently large k and m &gt; log p. Proof. We adapt Ranade's proof [39] of the result of Karp and Zhang <ref> [28] </ref>. Let T be the completion time of the protocol, the last time step at which a message is received. Let message M m be a message received at time step T , and let S be the node which was the source of message M m . <p> The branch-and-bound strategy is essentially that of Karp and Zhang <ref> [28] </ref>; their model allows any number of messages to be received at a node in one time step. Our technical contribution is to extend their result to the weaker atomic transmission model. The proofs of both results extend the techniques of the previous section.
Reference: [29] <author> R.M. Karp and Y. Zhang. </author> <title> Randomized parallel algorithms for backtrack search and branch-and-bound computations. </title> <journal> Journal of the ACM, </journal> <volume> 40, </volume> <year> 1993. </year>
Reference-contexts: We believe this is reasonable in applications that involve the atomic transfer of large data sets. Unit-delay assumptions are also made in the literature on PRAMs and complete networks <ref> [28, 29] </ref>. Unlike these models however, we explicitly account for message contention and do not allow multiple messages to be received in one step by a processor. The issue of contention at the receiving module is also addressed in models for optical communication [21] and module parallel computers [27, 35]. <p> Branch-and-bound search is similar to backtrack search, except that only a subtree of the search tree must necessarily be explored. Following Karp and Zhang <ref> [28, 29] </ref>, we model a branch-and-bound tree as a binary search tree, each of whose vertices has an associated cost. The cost of each vertex is strictly less than the cost of each of its children (for simplicity we assume that all vertex costs are distinct).
Reference: [30] <author> C. Koelbel, P. Mehrotra, and J. V. Rosendale. </author> <title> Supporting Shared Data Structures On Distributed Memory Architectures. </title> <type> Technical report, </type> <institution> ICASE, NASA Langley Research Center, </institution> <year> 1990. </year>
Reference-contexts: Many high-level languages, including Fortran-D [19], CM-Fortran [48], Vienna-Fortran [17] support parallel operations on uniform parallel arrays. However, none of them supports parallel operations on irregular data structures. Some runtime systems do support run-time pointer (or index) interpretation for remote data access <ref> [15, 18, 30, 36] </ref>, but these approaches take advantage of static data access and communication patterns, and do not provide efficient solutions for dynamically changing data distribution and communication patterns.
Reference: [31] <author> F. T. Leighton, M. J. Newman, A. Ranade, and E.J. Schwabe. </author> <title> Dynamic tree embeddings in butterfly and hypercubes. </title> <booktitle> In 1st Annual ACM Symposium on Parallel Algorithms and Architecture, </booktitle> <year> 1989. </year>
Reference-contexts: Proof. The analysis is similar to the generalized Chernoff bound given by Leighton etal. in <ref> [31] </ref>. We first estimate the expectation of e tX .
Reference: [32] <author> C. Leiserson, Z. Abuhamdeh, D. Douglas, C. Feynman, M. Ganmukhi, J. Hill, W. D. Hillis, B. Kuszmaul, M. St. Pierre, D. Wells, M. Wong, S. Yang, and R. Zak. </author> <title> The network architecture of the connection machine CM-5. </title> <booktitle> In 4th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <year> 1992. </year>
Reference-contexts: When more than one message, occupying send buffers of different nodes, are simultaneously destined for the same node, the network must deliver one message. Since every node executes a receive instruction during its reactive cycle, this requirement of the network satisfies the network contract of the CM-5 <ref> [32] </ref>: "The data network promises to eventually accept and deliver all messages injected into the network by the processors as long as the processors promise to eventually eject all messages from the network when they are delivered to the processors." With the reactive cycle and the network contract we are assured
Reference: [33] <author> P. Liu, W. Aiello, and S. Bhatt. </author> <title> An atomic model for message passing. </title> <booktitle> In 5th Annual ACM Symposium on Parallel Algorithms and Architecture, </booktitle> <year> 1993. </year>
Reference-contexts: The goal of the library is to allow different N-body codes to be written at a high-level, independent of the details of data structure management and communication. The second part of this thesis presents a communication model, the atomic message model <ref> [33] </ref>, to capture the important aspects of message-passing systems. The atomic message model is motivated by the problem of transferring large messages in a system with limited resources to store messages in transit, as well as limited bandwidth available at each node to send and receive messages. <p> this discrepancy is the lack of theoretical models that appropriately capture issues related to communication; as stated in [14], most theoretical models "encourage exploitation of formal loopholes, rather than rewarding development of techniques that yield performance across a range of current and future parallel machines." We propose an atomic model <ref> [33] </ref> to study the performance of message-passing programs. The model is simple and much more restricted in its capabilities in comparison with existing systems.
Reference: [34] <author> J. Makino. </author> <title> Comparison of two different tree algorithms. </title> <journal> Journal of Computational Physics, </journal> <volume> 88, </volume> <year> 1990. </year>
Reference-contexts: Hernquist [24] suggested an algorithm that vectorizes the interaction calculations between the current body and the essential data in the same level, and computes the overall acceleration by computing the interactions one level at a time. Makino <ref> [34] </ref> suggested an algorithm that vectorizes tree traversals for different bodies. Finally Barnes [6] suggested an algorithm which reduces the number of tree traversals by computing accelerations on a group of bodies at a time. Each 31 member of the same group use the same essential data for force calculation.
Reference: [35] <author> K. Mehlforn and U. Vishkin. </author> <title> Randomized and deterministic simulations of PRAMs by parallel machines with restricted granularity of parallel memories. </title> <journal> Acta Informatica, </journal> <volume> 21, </volume> <year> 1984. </year>
Reference-contexts: Unlike these models however, we explicitly account for message contention and do not allow multiple messages to be received in one step by a processor. The issue of contention at the receiving module is also addressed in models for optical communication [21] and module parallel computers <ref> [27, 35] </ref>. A key feature which distinguishes the atomic model is that once a message has been sent it cannot be retrieved; the sending processor must wait for the network to clear the send buffer after the message has been copied into the receive buffer at the destination.
Reference: [36] <author> S. Mirchandaney, J. Saltz, P. Mehrotra, and H. Berryman. </author> <title> A scheme for supporting automatic data migration on multicomputers. </title> <booktitle> In Proceedings of the Fifth Distributed Memory Computing Conference, </booktitle> <address> Charleston S.C., </address> <year> 1990. </year>
Reference-contexts: Many high-level languages, including Fortran-D [19], CM-Fortran [48], Vienna-Fortran [17] support parallel operations on uniform parallel arrays. However, none of them supports parallel operations on irregular data structures. Some runtime systems do support run-time pointer (or index) interpretation for remote data access <ref> [15, 18, 30, 36] </ref>, but these approaches take advantage of static data access and communication patterns, and do not provide efficient solutions for dynamically changing data distribution and communication patterns.
Reference: [37] <author> L. Nyland, J. Prins, and J. Reif. </author> <title> A data-parallel implementation of the adaptive fast multipole algorithm. </title> <booktitle> In DAGS/PC Symposium, </booktitle> <year> 1993. </year>
Reference-contexts: Finally, Nyland, Prins and Reif <ref> [37] </ref> describe a data-parallel implementation of the 3D adaptive Fast Multipole Method using the Proteus prototyping system. 2.1 The Barnes-Hut Algorithm All tree codes exploit the idea that the effect of a cluster of bodies at a distant point can be approximated by a small number of initial terms of an
Reference: [38] <author> L.K. Platzman and J.J. Bartholdi. </author> <title> Spacefilling curves and the planar traveling salesman problem. </title> <journal> Journal of the ACM, </journal> <year> 1989. </year>
Reference-contexts: The worst-case length of a space-filling curve for N bodies in the unit square is fi ( p fact it has been established that the length of the Peano-Hilbert curve is always within a O (log N ) factor of the optimal TSP tour in d-dimension <ref> [8, 10, 38] </ref>. Theorem 3 [38] Suppose that N bodies are distributed within the unit square (alternatively, the unit cube). <p> Theorem 3 <ref> [38] </ref> Suppose that N bodies are distributed within the unit square (alternatively, the unit cube). Then the total distance covered by the Peano-Hilbert traversal is O ( p (O (N 3 )). 4.2 Cache Modification A good traversal sequence alone does not guarantee efficient caching of essential data.
Reference: [39] <author> A. Ranade. </author> <title> A simpler analysis of the Karp-Zhang parallel branch-and-bound method. </title> <type> Technical Report UCB/CSD 90/586, </type> <institution> University of California, </institution> <year> 1990. </year>
Reference-contexts: The basic idea was to send each node to a random processor for further exploration. Ranade <ref> [39] </ref> gave an elegant alternative proof of the Karp-Zhang result. By extending Ranade's techniques we show that the random destination - 65 - strategy yields linear speedup for backtrack search in the atomic model. <p> The probability that the time until all messages have been received exceeds km is bounded by O (e m ), for sufficiently large k and m &gt; log p. Proof. We adapt Ranade's proof <ref> [39] </ref> of the result of Karp and Zhang [28]. Let T be the completion time of the protocol, the last time step at which a message is received. <p> We proceed in two stages; first we characterize the completion time in terms of an execution template. Then we show that execution templates corresponding to large completion times are unlikely. This follows the delay-sequence arguments used in the literature <ref> [39, 40] </ref>. Definition.
Reference: [40] <author> A. Ranade. </author> <title> Optimal speedup for backtrack search on a butterfly network. </title> <booktitle> In 3rd Annual ACM Symposium on Parallel Algorithms and Architecture, </booktitle> <year> 1991. </year>
Reference-contexts: We proceed in two stages; first we characterize the completion time in terms of an execution template. Then we show that execution templates corresponding to large completion times are unlikely. This follows the delay-sequence arguments used in the literature <ref> [39, 40] </ref>. Definition.
Reference: [41] <author> J. Reif and S. Tate. </author> <title> The complexity of N-body simulation. </title> <booktitle> In International Colloquium on Automata Languages and Programming, </booktitle> <year> 1993. </year>
Reference-contexts: Callahan and Kosaraju [13] developed better data structures to cluster the bodies hierarchically; they bound the time for both data structure and numerical operations by O (N ). On a related note, Reif and Tate <ref> [41] </ref> show that integrating N bodies with 2 N O (1) accuracy through N O (1) time steps is PSPACE-complete.
Reference: [42] <author> J. Salmon. </author> <title> Parallel Hierarchical N-body Methods. </title> <type> PhD thesis, </type> <institution> Caltech, </institution> <year> 1990. </year>
Reference-contexts: This thesis is inspired by the work reported in Salmon's thesis <ref> [42] </ref> as well as the papers of Warren and Salmon [53, 54]. Salmon implemented a version of the Barnes-Hut algorithm for gravitational N-body simulation and reported results on the NCUBE machine [42] as well as the Intel Touchstone Delta machine [53, 54]. <p> This thesis is inspired by the work reported in Salmon's thesis <ref> [42] </ref> as well as the papers of Warren and Salmon [53, 54]. Salmon implemented a version of the Barnes-Hut algorithm for gravitational N-body simulation and reported results on the NCUBE machine [42] as well as the Intel Touchstone Delta machine [53, 54]. We implement the same version of the Barnes-Hut algorithm, but introduce several new techniques to maintain dynamic data structures and for efficient communication. We report results from experiments on the Connection Machine CM-5. <p> Despite the differences in asymptotic running times, the overheads in the fully adaptive version of the fast multipole method are substantial and the algorithm by Barnes and Hut continues to be widely used in astrophysical simulations. Several parallel implementations of the Barnes-Hut's algorithm have been reported recently. Salmon <ref> [42] </ref> implemented the Barnes-Hut algorithm (with quadrupole approximations) on message-passing architectures including the NCUBE and Intel iPSC. Warren and Salmon [53, 54] report impressive performance from extensive runs on the 512 node Intel Touchstone Delta. Singh etal. [44, 45] implemented the Barnes-Hut algorithm for the experimental DASH prototype. <p> The entire process, starting with the construction of the BH-tree, is repeated for the desired number of time steps. One remark concerning distance measurements is in order. There are several ways to measure the distance between a body and a box. Salmon <ref> [42] </ref> discusses several alternatives in some detail. For consistency, we measure distances from bodies to the perimeter of a box in the L 1 metric. This is a conservative choice, and for sufficiently small avoids the problem of "detonating galaxies" [42]. <p> Salmon <ref> [42] </ref> discusses several alternatives in some detail. For consistency, we measure distances from bodies to the perimeter of a box in the L 1 metric. This is a conservative choice, and for sufficiently small avoids the problem of "detonating galaxies" [42]. In our experiments we use = 1; this corresponds to - 9 - = 0:5 for the original Barnes-Hut algorithm [42]. The overhead in building the tree, and traversing it while computing centers-of-mass and accelerations is negligible in sequential implementations. <p> This is a conservative choice, and for sufficiently small avoids the problem of "detonating galaxies" <ref> [42] </ref>. In our experiments we use = 1; this corresponds to - 9 - = 0:5 for the original Barnes-Hut algorithm [42]. The overhead in building the tree, and traversing it while computing centers-of-mass and accelerations is negligible in sequential implementations. With ten thousand bodies, more than 90% of the time is devoted to arithmetic operations involved in computing accelerations. <p> In fact, the methods presented in this thesis are being used to develop efficient parallel implementations of the fast multipole method as well. 2.2.2 Related Work In this section we describe the important aspects of Salmon's thesis <ref> [42] </ref> which motivated us initially, as well as the more recent reports of Warren and Salmon [53, 54], and of Singh etal. [44, 45]. We also point out the differences of our techniques from these approaches. Salmon [42] was the first to implement a parallel N-body simulation with adaptive hierarchical tree <p> Related Work In this section we describe the important aspects of Salmon's thesis <ref> [42] </ref> which motivated us initially, as well as the more recent reports of Warren and Salmon [53, 54], and of Singh etal. [44, 45]. We also point out the differences of our techniques from these approaches. Salmon [42] was the first to implement a parallel N-body simulation with adaptive hierarchical tree structure. The implementation is based on a modified version of the Barnes-Hut algorithm. <p> Salmon and Warren [53] implemented the modified Barnes-Hut algorithm on a 512 node Intel Touchstone Delta machine. Two very large scale simulations of the Cold Dark Matter model with 17.15 million bodies were reported. This is the largest astrophysics N-body simulation ever done [53]. Salmon <ref> [42] </ref> and Warren and Salmon [53] weight each body by the number of interactions in the previous time step. The volume enclosing the bodies is then recursively decomposed by orthogonal planes into regions of equal total weight. <p> He claims that a shared memory implementation can provide both programming simplicity and better performance than an explicit message passing implementation. The arguments in [44] about the advantages of shared-memory over message-passing implementations are based largely on comparisons to the initial implementations of Salmon <ref> [42] </ref> and Warren and Salmon [53]. Since our message-passing implementation is considerably simpler and more efficient, the import of the arguments of [44, 45] is less clear. For example, contrary to their claims, ORB can be implemented efficiently. <p> Choosing L to be much larger than 1 speeds up the computation phase, but makes level-adjustment somewhat tricky. We made this modification mainly to adapt Salmon's multipole expansion <ref> [42] </ref> for better simulation accuracy. The same approach is also used in Greengard-Rokhlin's Fast - 21 - .. tree. Multipole Method [23]. The level adjustment procedure also makes it easy to update the BH tree incrementally. <p> The common approach in adaptive N-body methods <ref> [7, 42, 44, 53, 54] </ref> is to rebuild the entire Barnes-Hut tree. In contrast our implementation dynamically adjusts the BH-tree to conform to the new distribution of bodies. <p> Warren and Salmon [53, 54] report that more than 85% of time in their implementation is devoted to force calculation. A simple body-to-body interaction requires thirty floating point operations. The more complicated quadrupole approximation takes more than seventy operations <ref> [42, 53] </ref>. There are two aspects of force calculation: tree traversal and two-body calculations. Tree traversal identifies essential nodes for calculating the acceleration of a body. Two-body calculations are used between the body and each of its essential nodes. <p> Although some bodies close to ORB bisectors will be duplicated in locally essential tress, they constitute only a very small fraction of total number of bodies. And the fraction decreases as the granularity and the percentage of bodies far away from bisectors increases. Salmon <ref> [42] </ref> reported that the complexity overhead, namely the locally essential trees construction, is the major source of inefficiency. We do not see this happen in our experiments; the implementation spends less than 9% of the time to build the essential trees.
Reference: [43] <author> C.L. Seitz. </author> <title> Multicomputers. In Developments in Concurrency and Communication, </title> <publisher> C.A.R Hoare (ed) Addison-Wesley, </publisher> <pages> pp 131-201, </pages> <year> 1990. </year>
Reference-contexts: And all-to-some message passing can finish within constant factor of optimal time with high probability if the destinations are uniformly distributed among processors. 6.2 The Atomic Message-passing Model We model a message-passing multicomputer as a collection of p nodes connected via an interconnection network <ref> [43] </ref>. For convenience of analysis we require that the system be synchronous, and operate in discrete time steps. 2 Each time step is divided between one communication step and one node computation step.
Reference: [44] <author> J. Singh. </author> <title> Parallel Hierarchical N-body Methods and their Implications for Multiprocessors. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1993. </year>
Reference-contexts: Several parallel implementations of the Barnes-Hut's algorithm have been reported recently. Salmon [42] implemented the Barnes-Hut algorithm (with quadrupole approximations) on message-passing architectures including the NCUBE and Intel iPSC. Warren and Salmon [53, 54] report impressive performance from extensive runs on the 512 node Intel Touchstone Delta. Singh etal. <ref> [44, 45] </ref> implemented the Barnes-Hut algorithm for the experimental DASH prototype. This thesis contrasts our approach and conclusions with both these efforts. Parallel implementations of the fast multipole method have been developed recently as well. <p> Board and Leathrum [26] have implemented the 3D adaptive Fast Multipole Method on shared memory machines including the KSR [26], Zhao and Johnsson [56, 57] implemented their version of non-adaptive 3D multipole method on the Connection Machine CM-2, and Singh etal. <ref> [44] </ref> have implemented the 2D adaptive fast multipole method on the DASH prototype. <p> being used to develop efficient parallel implementations of the fast multipole method as well. 2.2.2 Related Work In this section we describe the important aspects of Salmon's thesis [42] which motivated us initially, as well as the more recent reports of Warren and Salmon [53, 54], and of Singh etal. <ref> [44, 45] </ref>. We also point out the differences of our techniques from these approaches. Salmon [42] was the first to implement a parallel N-body simulation with adaptive hierarchical tree structure. The implementation is based on a modified version of the Barnes-Hut algorithm. <p> Data forwarding takes 40% of the total communication time in Salmon's implementation <ref> [44] </ref>. We too use the ORB decomposition and build locally essential trees so that the final compute-intensive stage is not slowed down by communication. <p> Finally, the data structures are not maintained incrementally. The program must sort all the keys to distribute bodies and build BH-trees for every iteration. Chapter 5 gives more details on timing results and comparisons. The DASH shared-memory architecture group at Stanford <ref> [44, 45] </ref> has investigated the implications of shared-memory programming for the Barnes-Hut algorithm. Each processor first builds a local tree; these are merged into a global tree stored in shared memory. Work is evenly distributed among processors by partitioning the bodies using a technique similar to [54]. <p> Work is evenly distributed among processors by partitioning the bodies using a technique similar to [54]. The shared memory provides a single address space for all processors. Memory coherence is enforced by hardware and memory cache is used to improve data access efficiency. In his thesis Singh <ref> [44] </ref> concluded that a shared memory implementation can exploit temporal locality by caching the essential data for different bodies. He claims that a shared memory implementation can provide both programming simplicity and better performance than an explicit message passing implementation. The arguments in [44] about the advantages of shared-memory over message-passing <p> In his thesis Singh <ref> [44] </ref> concluded that a shared memory implementation can exploit temporal locality by caching the essential data for different bodies. He claims that a shared memory implementation can provide both programming simplicity and better performance than an explicit message passing implementation. The arguments in [44] about the advantages of shared-memory over message-passing implementations are based largely on comparisons to the initial implementations of Salmon [42] and Warren and Salmon [53]. Since our message-passing implementation is considerably simpler and more efficient, the import of the arguments of [44, 45] is less clear. <p> The arguments in [44] about the advantages of shared-memory over message-passing implementations are based largely on comparisons to the initial implementations of Salmon [42] and Warren and Salmon [53]. Since our message-passing implementation is considerably simpler and more efficient, the import of the arguments of <ref> [44, 45] </ref> is less clear. For example, contrary to their claims, ORB can be implemented efficiently. Indeed it is expensive to compute ORB from scratch at every time step, but it is simple to incrementally adjust the partition quickly. The same is true for the BH-tree. <p> The same is true for the BH-tree. While shared-memory systems might ease certain programming tasks, the advantages for developing production-quality N-body codes are unclear. An additional example is the Barnes-Hut tree building. In his thesis <ref> [44] </ref> Singh reported two algorithms for building global BH-tree. The first algorithm has processors insert their bodies into a shared tree structure concurrently. Unfortunately, processors must interlock one another to modify the shared tree nodes during the insertion, thus the performance is not satisfying. <p> The second distributed memory style algorithm runs twice as fast as the first pure shared memory method. Contrary to the claim that shared memory implementation provides better performance <ref> [44] </ref>, the distributed memory style of programming used in the second algorithm provides better efficiency than the first pure shared-memory implementation, even on a shared-memory architecture. Finally, the fine grain and demand driven communication in shared memory machines may not be efficient in a large system. <p> Furthermore, ORB preserves data locality reasonably well 1 and permits simple load-balancing. Thus, while it is expensive to recompute the ORB at each time step <ref> [44] </ref>, the cost of incremental load-balancing is negligible as we will see in Chapter 5. We found that updating the ORB incrementally is cost-effective in comparison with either rebuilding it each time or with waiting for a large imbalance to occur before rebuilding. <p> The next step is to make the local trees be structurally consistent with the global BH-tree. This requires adjusting the levels of all leaf nodes which are split by ORB bisector planes. A similar process was developed independently in <ref> [44] </ref>; an additional complication in our case is that we build the BH-tree until each leaf contains up to L bodies. Choosing L to be much larger than 1 speeds up the computation phase, but makes level-adjustment somewhat tricky. <p> The common approach in adaptive N-body methods <ref> [7, 42, 44, 53, 54] </ref> is to rebuild the entire Barnes-Hut tree. In contrast our implementation dynamically adjusts the BH-tree to conform to the new distribution of bodies. <p> We cannot distinguish two cases unless we send a request to the owner of the node. This uncertainty complicates program control structures and increases communication overheads. The implementations of Singh <ref> [44] </ref> and Salmon-Warren [54] use this "transfer-data-on-demand" approach to collect essential data. Singh's implementation is based on shared memory architecture. All references of remote data are accomplished by implicit communication. The memory cache on each processor stores remote data and complicated hardware maintains cache coherence among processors. <p> The one-way communication for essential data is implemented as a "sender-oriented" protocol. The sender denotes the owner that sends out essential data, and the receivers are processors to which data is essential. Instead of letting receivers initiate the fine-grain demand-driven communication <ref> [44, 54] </ref>, the sender figures out its receivers and sends the information directly. The sender-oriented protocol avoids the inherent sequential access - 27 - pattern in receiver-oriented communication. <p> A tree traversal has to apply an opening test on every BH-node it encounters, and each test consists of distance calculation and possibly square root computation. The large number of floating point operations in the opening tests make tree traversal even more expensive. In his thesis <ref> [44] </ref> Singh surveyed many different approaches for vectorizing tree traversal and force calculation, and concluded that none of them provides sufficient speed up for conducting large-scale N-body calculations on vector supercomputers. <p> Each 31 member of the same group use the same essential data for force calculation. These techniques speed up the the computation by only a factor of five on vector supercomputers <ref> [44] </ref>. We use Barnes' technique of grouping bodies. Each of the largest tree nodes that have at most G bodies is considered a group 1 . The implementation details of grouping are in Section 4.5. We avoid expensive tree traversals by caching essential data. <p> Incremental BH-tree Update Our incremental tree structure is more efficient than the conceptually simpler method of [54]. The tree building phase in their implementation takes more than 12% of the total time. Singh etal. present a method similar to ours which takes about 5% to build the tree <ref> [44] </ref>. If the final phase in both these approaches is speeded up by grouping bodies as we do then the fraction of time in building the tree will be significantly higher. In contrast our code spends less than 5% of the total time to update the tree. scratch. <p> The maximum number of bodies in a leaf is 8. - 51 - 5.3 Communication and Locally Essential Trees In <ref> [44] </ref> Singh suggests that shared memory architecture has substantial advantages in programming complexity over an explicit message-passing programming paradigm, and the extra programming complexity translates into significant runtime overheads in message-passing implementation. However, in our implementation we do not see this happen. <p> Singh suggested that the total size of locally essential trees will be much larger than the size of actual global tree tree <ref> [44] </ref>. From the experimental results the amount of duplicated information in the locally essential trees is not significant compared with the global tree size. <p> When the group size G increases, the average size of the bounding box of a group also increases. As a result the cache hit rate decreases since the average distance between consecutive groups becomes larger. 5.5 Workload Balancing Singh <ref> [44] </ref> reported that rebuilding ORB for each iteration degrades overall performance, and alternative partitioning method should be used. However, our dynamic load balancing adjusts ORB bisectors with negligible cost. From the experimental data, we conclude that adjusting, not rebuilding, ORB bisectors can balance workload efficiently.
Reference: [45] <author> J. Singh, C. Holt, T. Totsuka, A. Gupta, and J. Hennessy. </author> <title> Load balancing and data locality in hierarchical N-body methods. </title> <type> Technical Report CSL-TR-92-505, </type> <institution> Stanford University, </institution> <year> 1992. </year>
Reference-contexts: Several parallel implementations of the Barnes-Hut's algorithm have been reported recently. Salmon [42] implemented the Barnes-Hut algorithm (with quadrupole approximations) on message-passing architectures including the NCUBE and Intel iPSC. Warren and Salmon [53, 54] report impressive performance from extensive runs on the 512 node Intel Touchstone Delta. Singh etal. <ref> [44, 45] </ref> implemented the Barnes-Hut algorithm for the experimental DASH prototype. This thesis contrasts our approach and conclusions with both these efforts. Parallel implementations of the fast multipole method have been developed recently as well. <p> being used to develop efficient parallel implementations of the fast multipole method as well. 2.2.2 Related Work In this section we describe the important aspects of Salmon's thesis [42] which motivated us initially, as well as the more recent reports of Warren and Salmon [53, 54], and of Singh etal. <ref> [44, 45] </ref>. We also point out the differences of our techniques from these approaches. Salmon [42] was the first to implement a parallel N-body simulation with adaptive hierarchical tree structure. The implementation is based on a modified version of the Barnes-Hut algorithm. <p> Finally, the data structures are not maintained incrementally. The program must sort all the keys to distribute bodies and build BH-trees for every iteration. Chapter 5 gives more details on timing results and comparisons. The DASH shared-memory architecture group at Stanford <ref> [44, 45] </ref> has investigated the implications of shared-memory programming for the Barnes-Hut algorithm. Each processor first builds a local tree; these are merged into a global tree stored in shared memory. Work is evenly distributed among processors by partitioning the bodies using a technique similar to [54]. <p> The arguments in [44] about the advantages of shared-memory over message-passing implementations are based largely on comparisons to the initial implementations of Salmon [42] and Warren and Salmon [53]. Since our message-passing implementation is considerably simpler and more efficient, the import of the arguments of <ref> [44, 45] </ref> is less clear. For example, contrary to their claims, ORB can be implemented efficiently. Indeed it is expensive to compute ORB from scratch at every time step, but it is simple to incrementally adjust the partition quickly. The same is true for the BH-tree.
Reference: [46] <author> S. Sundaram. </author> <title> Fast Algorithms for N-body Simulations. </title> <type> PhD thesis, </type> <institution> Cornell University, </institution> <year> 1993. </year>
Reference-contexts: Greengard and Rokhlin [23] presented an O (N ) Fast Multipole Method which is provably correct to any fixed accuracy. The underlying numerical representations were subsequently refined and simplified by Zhao [55] and Anderson [2]. Recently, Sundaram <ref> [46] </ref> extended the fast multipole method to allow different bodies to be updated at different rates; this reduces 5 the arithmetic complexity over a large time period.
Reference: [47] <author> R.E. Tarjan. </author> <title> Data Structures and Network Algorithms. </title> <institution> Society for Industrial and Applied Mathematics, </institution> <year> 1983. </year>
Reference-contexts: On the other hand, a shrink operation inserts the indices released from the essential data list into the heap. All the heap operations can be efficiently implemented as a d-heap within an array <ref> [47] </ref>. There are several drawbacks in the simple approach of maintaining free indices as a heap. First of all, not all of the free indices have to be stored in the heap. <p> The reduced number of heap elements improves the performance of heap insertions and deletions. Figure 4.6 outlines the heap deletion routine. A standard heap insertion routine can be found in <ref> [47] </ref>. Secondly, we use block number instead of array index to determine which free cell should be used first. The heap contains only block numbers that have free cells.
Reference: [48] <institution> Thinking Machine Corporation. CM-Fortran Programmer's Manual, </institution> <year> 1990. </year>
Reference-contexts: Coping with these issues requires new ways to design data structures and communication protocols for distributed-memory architectures. While tremendous research effort has concentrated on parallelizing scientific computations with uniform structures, efficient parallelizations of dynamic and irregular computations are not yet well understood. Many high-level languages, including Fortran-D [19], CM-Fortran <ref> [48] </ref>, Vienna-Fortran [17] support parallel operations on uniform parallel arrays. However, none of them supports parallel operations on irregular data structures.
Reference: [49] <author> Thinking Machine Corporation. </author> <title> The Connection Machine CM-5 Technical Summary, </title> <year> 1991. </year>
Reference-contexts: the ancestor into cache; case JUST RIGHT: skip n; /* compute acceleration */ for each essential node in cache compute the interaction between the body and the node; 4.3.1 Vector Units on CM-5 The accelerator hardware of the CM-5 consists of four vector units (VU) on each processing node (PN) <ref> [49, 52] </ref>. Each vector unit retrieves the same instructions from a SPARC instruction unit through a 64-bit bus. Therefore the vector units execute the same computation synchronously on different data. <p> Chapter 5 Experimental Results This chapter presents experimental results and compares them with related work. Our platform is the Connection Machine CM-5 with SPARC vector units <ref> [49] </ref>. Each processing node has 32M bytes of memory and can perform floating point operations at peak rate of 128 Mflop/s [52]. We use the CMMD communication library (version 3.0) [51].
Reference: [50] <institution> Thinking Machine Corporation. </institution> <month> CDPEAC: </month> <title> Using GCC to program in DPEAC, </title> <year> 1993. </year>
Reference-contexts: We use the CMMD communication library (version 3.0) [51]. The vector units are programmed in CDPEAC which provides an interface between C and the DPEAC assembly language for vector units <ref> [50] </ref>. The rest of the program is written in C. The experiments sketched here included three input distributions: the uniform distribution, the Plummer distribution [1] with mass M = 1 within a sphere, and two Plummer distributions at a colliding course.
Reference: [51] <institution> Thinking Machine Corporation. </institution> <note> CMMD Reference Manual, </note> <year> 1993. </year>
Reference-contexts: As an extreme example, if all messages are sent before any is received, a large machine will simply crash when the number of virtual channels has been exhausted. In the CMMD message-passing library (version 3.0) each outstanding send requires a virtual channel <ref> [51] </ref> and the number of channels is limited. Instead, we used a protocol which alternates sends with receives (Figure 3.6). The problem is thus reduced to ordering the messages to be sent. <p> Our platform is the Connection Machine CM-5 with SPARC vector units [49]. Each processing node has 32M bytes of memory and can perform floating point operations at peak rate of 128 Mflop/s [52]. We use the CMMD communication library (version 3.0) <ref> [51] </ref>. The vector units are programmed in CDPEAC which provides an interface between C and the DPEAC assembly language for vector units [50]. The rest of the program is written in C.
Reference: [52] <institution> Thinking Machine Corporation. </institution> <note> DPEAC Reference Manual, </note> <year> 1993. </year>
Reference-contexts: the ancestor into cache; case JUST RIGHT: skip n; /* compute acceleration */ for each essential node in cache compute the interaction between the body and the node; 4.3.1 Vector Units on CM-5 The accelerator hardware of the CM-5 consists of four vector units (VU) on each processing node (PN) <ref> [49, 52] </ref>. Each vector unit retrieves the same instructions from a SPARC instruction unit through a 64-bit bus. Therefore the vector units execute the same computation synchronously on different data. <p> When the pipelines on the four vector units are completely full and all the operands are in vector registers, the vector units operate at the theoretical peak rate of 128 Mflop/s <ref> [52] </ref>. 4.3.2 Using Vector Units for Force Computation The CM-5 vector units provide a feasible way to speed up force calculation. The interactions between a body and each of its essential data are calculated by the same formula. <p> Chapter 5 Experimental Results This chapter presents experimental results and compares them with related work. Our platform is the Connection Machine CM-5 with SPARC vector units [49]. Each processing node has 32M bytes of memory and can perform floating point operations at peak rate of 128 Mflop/s <ref> [52] </ref>. We use the CMMD communication library (version 3.0) [51]. The vector units are programmed in CDPEAC which provides an interface between C and the DPEAC assembly language for vector units [50]. The rest of the program is written in C.
Reference: [53] <author> M. Warren and J. Salmon. </author> <title> Astrophysical N-body simulations using hierarchical tree data structures. </title> <booktitle> In Proceedings of Supercomputing, </booktitle> <year> 1992. </year>
Reference-contexts: This thesis is inspired by the work reported in Salmon's thesis [42] as well as the papers of Warren and Salmon <ref> [53, 54] </ref>. Salmon implemented a version of the Barnes-Hut algorithm for gravitational N-body simulation and reported results on the NCUBE machine [42] as well as the Intel Touchstone Delta machine [53, 54]. <p> thesis is inspired by the work reported in Salmon's thesis [42] as well as the papers of Warren and Salmon <ref> [53, 54] </ref>. Salmon implemented a version of the Barnes-Hut algorithm for gravitational N-body simulation and reported results on the NCUBE machine [42] as well as the Intel Touchstone Delta machine [53, 54]. We implement the same version of the Barnes-Hut algorithm, but introduce several new techniques to maintain dynamic data structures and for efficient communication. We report results from experiments on the Connection Machine CM-5. <p> Several parallel implementations of the Barnes-Hut's algorithm have been reported recently. Salmon [42] implemented the Barnes-Hut algorithm (with quadrupole approximations) on message-passing architectures including the NCUBE and Intel iPSC. Warren and Salmon <ref> [53, 54] </ref> report impressive performance from extensive runs on the 512 node Intel Touchstone Delta. Singh etal. [44, 45] implemented the Barnes-Hut algorithm for the experimental DASH prototype. This thesis contrasts our approach and conclusions with both these efforts. <p> methods presented in this thesis are being used to develop efficient parallel implementations of the fast multipole method as well. 2.2.2 Related Work In this section we describe the important aspects of Salmon's thesis [42] which motivated us initially, as well as the more recent reports of Warren and Salmon <ref> [53, 54] </ref>, and of Singh etal. [44, 45]. We also point out the differences of our techniques from these approaches. Salmon [42] was the first to implement a parallel N-body simulation with adaptive hierarchical tree structure. The implementation is based on a modified version of the Barnes-Hut algorithm. <p> The implementation is based on a modified version of the Barnes-Hut algorithm. Instead of using only the center of mass to represent a cluster, the quadratic terms in the Taylor's expansion are used for force computation. Salmon and Warren <ref> [53] </ref> implemented the modified Barnes-Hut algorithm on a 512 node Intel Touchstone Delta machine. Two very large scale simulations of the Cold Dark Matter model with 17.15 million bodies were reported. This is the largest astrophysics N-body simulation ever done [53]. Salmon [42] and Warren and Salmon [53] weight each body <p> Salmon and Warren <ref> [53] </ref> implemented the modified Barnes-Hut algorithm on a 512 node Intel Touchstone Delta machine. Two very large scale simulations of the Cold Dark Matter model with 17.15 million bodies were reported. This is the largest astrophysics N-body simulation ever done [53]. Salmon [42] and Warren and Salmon [53] weight each body by the number of interactions in the previous time step. The volume enclosing the bodies is then recursively decomposed by orthogonal planes into regions of equal total weight. <p> Salmon and Warren <ref> [53] </ref> implemented the modified Barnes-Hut algorithm on a 512 node Intel Touchstone Delta machine. Two very large scale simulations of the Cold Dark Matter model with 17.15 million bodies were reported. This is the largest astrophysics N-body simulation ever done [53]. Salmon [42] and Warren and Salmon [53] weight each body by the number of interactions in the previous time step. The volume enclosing the bodies is then recursively decomposed by orthogonal planes into regions of equal total weight. Figure 2.5 shows the resulting decomposition, often called the orthogonal recursive bisection, ORB for short. <p> He claims that a shared memory implementation can provide both programming simplicity and better performance than an explicit message passing implementation. The arguments in [44] about the advantages of shared-memory over message-passing implementations are based largely on comparisons to the initial implementations of Salmon [42] and Warren and Salmon <ref> [53] </ref>. Since our message-passing implementation is considerably simpler and more efficient, the import of the arguments of [44, 45] is less clear. For example, contrary to their claims, ORB can be implemented efficiently. <p> within a parallelpiped is, in turn, computed by traversing the local BH-tree. 1 Clustering techniques which exploit the geometrical properties of the distribution will preserve locality better, but might lose some of the other attractive properties of ORB. 3.2 Building the BH-tree Unlike the first implementation of Warren and Salmon <ref> [53] </ref>, we chose to construct a representation of a distributed global BH-tree. An important consideration for us was to investigate abstractions that allow the applications programmer to declare a global data structure, a tree for example, without having to worry about the details of distributed-memory implementation. <p> The common approach in adaptive N-body methods <ref> [7, 42, 44, 53, 54] </ref> is to rebuild the entire Barnes-Hut tree. In contrast our implementation dynamically adjusts the BH-tree to conform to the new distribution of bodies. <p> Similarly, the ORB decomposition is incrementally updated to conform to changes in workload distribution. Chapter 4 Efficient Force Computation Calculating interactions among bodies is the most time consuming phase of the Barnes-Hut algorithm. Warren and Salmon <ref> [53, 54] </ref> report that more than 85% of time in their implementation is devoted to force calculation. A simple body-to-body interaction requires thirty floating point operations. The more complicated quadrupole approximation takes more than seventy operations [42, 53]. There are two aspects of force calculation: tree traversal and two-body calculations. <p> Warren and Salmon [53, 54] report that more than 85% of time in their implementation is devoted to force calculation. A simple body-to-body interaction requires thirty floating point operations. The more complicated quadrupole approximation takes more than seventy operations <ref> [42, 53] </ref>. There are two aspects of force calculation: tree traversal and two-body calculations. Tree traversal identifies essential nodes for calculating the acceleration of a body. Two-body calculations are used between the body and each of its essential nodes. <p> The rest of the overhead is less than 13% for a Plummer model, 15% for two colliding Plummer models. For the uniform distribution the corresponding figure is less than 9%. The performance figures from our implementation compare favorably with those reported by Warren and Salmon <ref> [53, 54] </ref> (see Figure 5.4). With uniform distribution of bodies, our implementation spends 91% of the total time performing interaction computations, and uses less than 9% of the time to manage Barnes-Hut tree, construct locally essential trees, prepare essential data via caching, and balance workload. <p> 512 node Delta 512 node Delta 256 node CM-5 number of bodies 8.8 million 8.8 million 10 million input distribution uniform uniform uniform time per simulation step (sec) 77 114 59 time % of interaction computation 85% 47% 91% other overhead 15% 53% 9% time percentage of force computation in <ref> [53] </ref> includes tree traversal and the actual percentage of overhead is higher. simulations, the percentage of tree nodes that span more than one processor domain is small.
Reference: [54] <author> M. Warren and J. Salmon. </author> <title> A parallel hashed oct-tree N-body algorithm. </title> <booktitle> In Proceedings of Supercomputing, </booktitle> <year> 1993. </year>
Reference-contexts: This thesis is inspired by the work reported in Salmon's thesis [42] as well as the papers of Warren and Salmon <ref> [53, 54] </ref>. Salmon implemented a version of the Barnes-Hut algorithm for gravitational N-body simulation and reported results on the NCUBE machine [42] as well as the Intel Touchstone Delta machine [53, 54]. <p> thesis is inspired by the work reported in Salmon's thesis [42] as well as the papers of Warren and Salmon <ref> [53, 54] </ref>. Salmon implemented a version of the Barnes-Hut algorithm for gravitational N-body simulation and reported results on the NCUBE machine [42] as well as the Intel Touchstone Delta machine [53, 54]. We implement the same version of the Barnes-Hut algorithm, but introduce several new techniques to maintain dynamic data structures and for efficient communication. We report results from experiments on the Connection Machine CM-5. <p> Several parallel implementations of the Barnes-Hut's algorithm have been reported recently. Salmon [42] implemented the Barnes-Hut algorithm (with quadrupole approximations) on message-passing architectures including the NCUBE and Intel iPSC. Warren and Salmon <ref> [53, 54] </ref> report impressive performance from extensive runs on the 512 node Intel Touchstone Delta. Singh etal. [44, 45] implemented the Barnes-Hut algorithm for the experimental DASH prototype. This thesis contrasts our approach and conclusions with both these efforts. <p> methods presented in this thesis are being used to develop efficient parallel implementations of the fast multipole method as well. 2.2.2 Related Work In this section we describe the important aspects of Salmon's thesis [42] which motivated us initially, as well as the more recent reports of Warren and Salmon <ref> [53, 54] </ref>, and of Singh etal. [44, 45]. We also point out the differences of our techniques from these approaches. Salmon [42] was the first to implement a parallel N-body simulation with adaptive hierarchical tree structure. The implementation is based on a modified version of the Barnes-Hut algorithm. <p> Those move to another processor domain will be inserted into the local trees of corresponding processors. If the movements of bodies cause imbalance of workload, the necessary ORB bisectors are adjusted to maintain the even distribution of workload. More recently, Warren and Salmon <ref> [54] </ref> reported a modified algorithm which uses a different criterion for applying center-of-mass approximations. The error bound of the new opening criterion is carefully analyzed for center-of-mass approximation. A similar Cold Dark Matter model with 8.8 million bodies was simulated on a 512 node Intel Touchstone Delta machine. <p> Each processor first builds a local tree; these are merged into a global tree stored in shared memory. Work is evenly distributed among processors by partitioning the bodies using a technique similar to <ref> [54] </ref>. The shared memory provides a single address space for all processors. Memory coherence is enforced by hardware and memory cache is used to improve data access efficiency. <p> The common approach in adaptive N-body methods <ref> [7, 42, 44, 53, 54] </ref> is to rebuild the entire Barnes-Hut tree. In contrast our implementation dynamically adjusts the BH-tree to conform to the new distribution of bodies. <p> First each processor sends and receives twice as many messages as the number of remote data because of two-way communication. The startup cost for sending large number of messages translates into huge communication overhead. One way to hide communication latency is to execute multiple tree traversals concurrently <ref> [54] </ref>. However, the multi-threaded method requires a complicated control structure. The two-way fine-grain communication either makes the communication overhead prohibitive or increases the programming complexity. Secondly, a processor may request a BH-node that does not exist. <p> We cannot distinguish two cases unless we send a request to the owner of the node. This uncertainty complicates program control structures and increases communication overheads. The implementations of Singh [44] and Salmon-Warren <ref> [54] </ref> use this "transfer-data-on-demand" approach to collect essential data. Singh's implementation is based on shared memory architecture. All references of remote data are accomplished by implicit communication. The memory cache on each processor stores remote data and complicated hardware maintains cache coherence among processors. <p> All references of remote data are accomplished by implicit communication. The memory cache on each processor stores remote data and complicated hardware maintains cache coherence among processors. The fine-grain demand-driven communication requires complex hardware to channel data among processors efficiently. Warren and Salmon's implementation <ref> [54] </ref> performs up to thirty tree traversals concurrently so that when one is blocked by communication, the others can still continue. The tree traversal, communication for remote data, and force calculation are all combined together to hide latency. <p> The one-way communication for essential data is implemented as a "sender-oriented" protocol. The sender denotes the owner that sends out essential data, and the receivers are processors to which data is essential. Instead of letting receivers initiate the fine-grain demand-driven communication <ref> [44, 54] </ref>, the sender figures out its receivers and sends the information directly. The sender-oriented protocol avoids the inherent sequential access - 27 - pattern in receiver-oriented communication. <p> The fine grain, demand driven approach may not be suitable for transferring large amount of information. For example, in Warren and Salmon's implementation <ref> [54] </ref> each processor traverses the BH-tree and demands remote essential data. <p> Similarly, the ORB decomposition is incrementally updated to conform to changes in workload distribution. Chapter 4 Efficient Force Computation Calculating interactions among bodies is the most time consuming phase of the Barnes-Hut algorithm. Warren and Salmon <ref> [53, 54] </ref> report that more than 85% of time in their implementation is devoted to force calculation. A simple body-to-body interaction requires thirty floating point operations. The more complicated quadrupole approximation takes more than seventy operations [42, 53]. There are two aspects of force calculation: tree traversal and two-body calculations. <p> Tree traversal is also expensive. Warren and Salmon reported that tree traversal uses more than 34% of the time in their latest implementation <ref> [54] </ref>. A tree traversal has to apply an opening test on every BH-node it encounters, and each test consists of distance calculation and possibly square root computation. The large number of floating point operations in the opening tests make tree traversal even more expensive. <p> The rest of the overhead is less than 13% for a Plummer model, 15% for two colliding Plummer models. For the uniform distribution the corresponding figure is less than 9%. The performance figures from our implementation compare favorably with those reported by Warren and Salmon <ref> [53, 54] </ref> (see Figure 5.4). With uniform distribution of bodies, our implementation spends 91% of the total time performing interaction computations, and uses less than 9% of the time to manage Barnes-Hut tree, construct locally essential trees, prepare essential data via caching, and balance workload. <p> Those on the deeper levels of the BH-tree have correct global information right in the local tree, and do not have to construct representative nodes via communication. Incremental BH-tree Update Our incremental tree structure is more efficient than the conceptually simpler method of <ref> [54] </ref>. The tree building phase in their implementation takes more than 12% of the total time. Singh etal. present a method similar to ours which takes about 5% to build the tree [44].
Reference: [55] <author> F. Zhao. </author> <title> An O(N ) algorithm for three dimensional N-body simulation. </title> <type> Technical report, </type> <institution> MIT, </institution> <year> 1987. </year>
Reference-contexts: Greengard and Rokhlin [23] presented an O (N ) Fast Multipole Method which is provably correct to any fixed accuracy. The underlying numerical representations were subsequently refined and simplified by Zhao <ref> [55] </ref> and Anderson [2]. Recently, Sundaram [46] extended the fast multipole method to allow different bodies to be updated at different rates; this reduces 5 the arithmetic complexity over a large time period.
Reference: [56] <author> F. Zhao and S.L. Johnsson. </author> <title> The parallel multipole method on the connection machine. </title> <type> Technical Report DCS/TR-749, </type> <institution> Yale University, </institution> <year> 1989. </year>
Reference-contexts: This thesis contrasts our approach and conclusions with both these efforts. Parallel implementations of the fast multipole method have been developed recently as well. Board and Leathrum [26] have implemented the 3D adaptive Fast Multipole Method on shared memory machines including the KSR [26], Zhao and Johnsson <ref> [56, 57] </ref> implemented their version of non-adaptive 3D multipole method on the Connection Machine CM-2, and Singh etal. [44] have implemented the 2D adaptive fast multipole method on the DASH prototype.
Reference: [57] <author> F. Zhao and S.L. Johnsson. </author> <title> The parallel multipole method on the connection machine. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <year> 1991. </year>
Reference-contexts: This thesis contrasts our approach and conclusions with both these efforts. Parallel implementations of the fast multipole method have been developed recently as well. Board and Leathrum [26] have implemented the 3D adaptive Fast Multipole Method on shared memory machines including the KSR [26], Zhao and Johnsson <ref> [56, 57] </ref> implemented their version of non-adaptive 3D multipole method on the Connection Machine CM-2, and Singh etal. [44] have implemented the 2D adaptive fast multipole method on the DASH prototype.
References-found: 57

