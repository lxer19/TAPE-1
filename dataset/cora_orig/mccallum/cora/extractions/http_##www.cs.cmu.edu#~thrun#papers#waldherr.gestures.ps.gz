URL: http://www.cs.cmu.edu/~thrun/papers/waldherr.gestures.ps.gz
Refering-URL: http://www.cs.cmu.edu/~thrun/papers/full.html
Root-URL: 
Title: Template-Based Recognition of Pose and Motion Gestures On a Mobile Robot  
Author: Stefan Waldherr Sebastian Thrun Roseli Romero Dimitris Margaritis 
Address: Pittsburgh, PA  
Affiliation: Computer Science Department Carnegie Mellon University  
Abstract: For mobile robots to assist people in everyday life, they must be easy to instruct. This paper describes a gesture-based interface for human robot interaction, which enables people to instruct robots through easy-to-perform arm gestures. Such gestures might be static pose gestures, which involve only a specific configuration of the person's arm, or they might be dynamic motion gestures (such as waving). Gestures are recognized in real-time at approximate frame rate, using a hybrid approach that integrates neural networks and template matching. A fast, color-based tracking algorithm enables the robot to track and follow a person reliably through office environments with drastically changing lighting conditions. Results are reported in the context of an interactive clean-up task, where a person guides the robot to specific locations that need to be cleaned, and the robot picks up trash which it then delivers to the nearest trash-bin. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Asoh, H.; Hayamizu, S.; Hara, I.; Motomura, Y.; Akaho, S.; and Matsui, T. </author> <year> 1997. </year> <title> Socially embedded learning of office-conversant robot jijo-2. </title> <booktitle> In Proceedings of IJCAI-97. IJCAI, </booktitle> <publisher> Inc. </publisher>
Reference-contexts: Due to the lack of a speech recognition system, his interface still required the user to operate a keyboard; however, the natural language component made instructing the robot significantly easier. More recently, Asoh and colleagues <ref> (Asoh et al. 1997) </ref> developed an interface that integrates a speech recognition system into a phrase-based natural language interface. They successfully instructed their office-conversant robot to navigate to office doors and other significant places in their environment, using verbal commands.
Reference: <author> Borenstein, J.; Everett, B.; and Feng, L. </author> <year> 1996. </year> <title> Navigating Mobile Robots: Systems and Techniques. Wellesley, MA: </title> <editor> A. K. Peters, </editor> <publisher> Ltd. </publisher>
Reference: <author> Burgard, W.; Cremers, A.; Fox, D.; Hahnel, D.; Lakemeyer, G.; Schulz, D.; Steiner, W.; and Thrun, S. </author> <year> 1998. </year> <title> The interactive museum tour-guide robot. </title> <booktitle> In Proceedings of AAAI-98. </booktitle>
Reference: <author> Crowley, J. </author> <year> 1997. </year> <title> Vision for man-machine interaction. </title> <booktitle> Robotics and Autonomous Systems 19:347358. </booktitle>
Reference: <author> Darrel, T.; Moghaddam, B.; and Pentland, A. </author> <year> 1996. </year> <title> Active face tracking and pose estimation in an interactive room. </title> <booktitle> In Proceedings of ICCV-96, </booktitle> <pages> 6772. </pages>
Reference: <author> Firby, R.; Kahn, R.; Prokopowicz, P.; and Swain, M. </author> <year> 1995. </year> <title> An architecture for active vision and action. </title> <booktitle> In Proceedings of IJCAI-95, </booktitle> <pages> 7279. </pages>
Reference-contexts: In a similar effort, Kahn and colleagues (Kahn et al. 1996) developed a gesture-based interface which has been demonstrated to reliably recognize static arm poses (pose gestures) such as pointing. This interface was successfully integrated into Firby's reactive plan-execution system RAP <ref> (Firby et al. 1995) </ref>, where it enabled people to instruct a robot to pick up free-standing objects. Both of these approaches, however, recognize only static pose gestures. They cannot recognize gestures that are defined through specific temporal patterns of arm movements, such as waving.
Reference: <author> Fox, D.; Burgard, W.; and Thrun, S. </author> <year> 1997. </year> <title> The dynamic window approach to collision avoidance. </title> <journal> IEEE Robotics and Automation 4(1). </journal>
Reference-contexts: Step 3: Servoing. If the robot is in visual servoing mode (meaning that it is following a person), it issues a motion command that makes the robot turn and move towards this person. The command is passed on to a collision avoidance method <ref> (Fox, Burgard, & Thrun 1997) </ref> that sets the actual velocity and motion direction of the robot in response to proximity sensor data. Step 4: Adaptation. Finally, the means and covariances X face ; S face ; X body ; S body are adapted, to compensate changes in illumination.
Reference: <author> Huber, E., and Kortenkamp, D. </author> <year> 1995. </year> <title> Using stereo vision to pursue moving agents with a mobile robot. </title> <booktitle> In Proceedings of IEEE ICRA-95. </booktitle>
Reference: <author> Kahn, R.; Swain, M.; Prokopowicz, P.; and Firby, R. </author> <year> 1996. </year> <title> Gesture recognition using the perseus architecture. </title> <booktitle> In Proceedings of the IEEE CVPR-96, </booktitle> <pages> 734741. </pages>
Reference-contexts: For example, Kortenkamp and colleagues (Kortenkamp, Huber, & Bonassi 1996) recently developed a gesture-based interface, which is capable of recognizing arm poses such as pointing towards a location on the ground. In a similar effort, Kahn and colleagues <ref> (Kahn et al. 1996) </ref> developed a gesture-based interface which has been demonstrated to reliably recognize static arm poses (pose gestures) such as pointing.
Reference: <author> King, S., and Weiman, C. </author> <year> 1990. </year> <title> Helpmate autonomous mobile robot navigation system. </title> <booktitle> In Proceedings of the SPIE Conference on Mobile Robots, 190198. </booktitle> <volume> Volume 2352. </volume>
Reference-contexts: Service robots cooperate with people and assist them in their everyday tasks. A landmark service robot is Helpmate Robotics's Helpmate robot, which has already been deployed at numerous hospitals worldwide <ref> (King & Weiman 1990) </ref>. In the near future, similar robots are expected to appear in various branches of entertainment, recreation, health-care, nursing, etc., and we expect them to interact closely with people. This upcoming generation of service robots opens up new research opportunities.
Reference: <author> Kortenkamp, D.; Bonassi, R.; and Murphy, R., eds. </author> <year> 1998. </year> <title> AI-based Mobile Robots: Case studies of successful robot systems. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Kortenkamp, D.; Huber, E.; and Bonassi, P. </author> <year> 1996. </year> <title> Recognizing and interpreting gestures on a mobile robot. </title> <booktitle> In Proceedings of AAAI-96, </booktitle> <pages> 915921. </pages>
Reference-contexts: They successfully instructed their office-conversant robot to navigate to office doors and other significant places in their environment, using verbal commands. Other researchers have proposed vision-based interfaces that allow people to instruct mobile robots via arm gestures. For example, Kortenkamp and colleagues <ref> (Kortenkamp, Huber, & Bonassi 1996) </ref> recently developed a gesture-based interface, which is capable of recognizing arm poses such as pointing towards a location on the ground.
Reference: <author> Moravec, H. P. </author> <year> 1988. </year> <title> Sensor fusion in certainty grids for mobile robots. </title> <journal> AI Magazine 6174. </journal>
Reference: <author> Pomerleau, D. </author> <year> 1993. </year> <title> Neural Network Perception for Mobile Robot Guidance. </title> <address> Boston, MA: </address> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: The neural network-based approach predicts the angles of the two arm segments relative to the person's body from the image segment. The input to the network is a down-sampled image segment, and the output are the angles, encoded using multi-unit Gaussian representations <ref> (Pomerleau 1993) </ref>. In our implementation, we used 60 output units, 30 for each of the two arm angles. The network was trained with Backpropagation, using a database of 758 hand-labeled training images.
Reference: <author> Rabiner, L., and Juang, B. </author> <year> 1986. </year> <title> An introduction to hidden markov models. </title> <journal> In IEEE ASSP Magazine. </journal>
Reference-contexts: To compensate differences in the exact timing when performing gestures, our approach uses the Viterbi algorithm <ref> (Rabiner & Juang 1986) </ref> for time alignment. The Viterbi alignment employs dynamic programming to find the best temporal alignment between the feature vector sequence and the gesture template, thereby compensating for variations in the exact timing of a gesture. performs the follow gesture.
Reference: <author> Simmons, R. </author> <year> 1995. </year> <title> The 1994 AAAI robot competition and exhibition. </title> <journal> AI Magazine 16(1). </journal>
Reference-contexts: In a pilot study, we have successfully instructed our robot to pick up trash scattered in an office building, and to deposit it into a trash-bin. This task was motivated by the clean-up an office task, which was designed for the AAAI-94 mobile robot competition <ref> (Simmons 1995) </ref>. Our scenario differs from the competition task in that a human interacts with the robot and initiates the clean-up task, which is then performed autonomously by the robot. In our experiments, we found the interface to be reliable and relatively easy to use.
Reference: <author> Swain, M. </author> <year> 1991. </year> <title> Color indexing. </title> <journal> International Journal of Computer Vision 7. </journal>
Reference: <author> Thrun, S. et al. </author> <year> 1998. </year> <title> Map learning and high-speed navigation in RHINO. </title> <editor> In Kortenkamp, D.; Bonassi, R.; and Murphy, R., eds., </editor> <title> AI-based Mobile Robots: Case studies of successful robot systems. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Torrance, M. C. </author> <year> 1994. </year> <title> Natural communication with robots. </title> <type> Master's thesis, </type> <institution> MIT Department of EECS, </institution> <address> Cambridge, MA. </address>
Reference-contexts: Copyright 1998, American Association for Artificial Intelligence (www.aaai.org). All rights reserved. The need for more effective human robot interfaces has been recognized. For example, in his M.Sc. thesis, Torrance developed a natural language interface for teaching mobile robots names of places in an indoor environment <ref> (Torrance 1994) </ref>. Due to the lack of a speech recognition system, his interface still required the user to operate a keyboard; however, the natural language component made instructing the robot significantly easier.
Reference: <author> Wong, C.; Kortenkamp, D.; and Speich, M. </author> <year> 1995. </year> <title> A mobile robot that recognizes people. </title> <booktitle> In Proceedings of ICTAI-95. </booktitle>
Reference: <author> Wren, C.; Azarbayejani, A.; Darrell, T.; and Pentland, A. </author> <year> 1997. </year> <title> Pfinder: Real-time tracking of the human body. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Learning 19(7):780785. </journal>
Reference: <author> Yang, J., and Waibel, A. </author> <year> 1995. </year> <title> Tracking human faces in real-time. </title> <type> Technical Report CMU-CS-95-210, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA. </address>
Reference-contexts: Many existing approaches assume that the camera is mounted at a fixed location. Such approaches typically rely on a static background, so that human motion can be detected through image differencing. Some approaches (e.g., <ref> (Yang & Waibel 1995) </ref>) can track people if the camera is mounted on a pan-tilt unit, which can impose mild changes in illumination.
References-found: 22

