URL: ftp://publications.ai.mit.edu/ai-publications/1000-1499/AIM-1440.ps.Z
Refering-URL: http://www.ai.mit.edu/projects/cbcl/old-course9.520/jordan.html
Root-URL: 
Title: Hierarchical Mixtures of Experts and the EM Algorithm  
Author: Michael I. Jordan and Robert A. Jacobs 
Note: Copyright c Massachusetts Institute of Technology, 1993  
Date: 1440 August 6, 1993  83  
Affiliation: MASSACHUSETTS INSTITUTE OF TECHNOLOGY ARTIFICIAL INTELLIGENCE LABORATORY and CENTER FOR BIOLOGICAL AND COMPUTATIONAL LEARNING DEPARTMENT OF BRAIN AND COGNITIVE SCIENCES  
Pubnum: A.I. Memo No.  C.B.C.L. Memo No.  
Abstract: We present a tree-structured architecture for supervised learning. The statistical model underlying the architecture is a hierarchical mixture model in which both the mixture coefficients and the mixture components are generalized linear models (GLIM's). Learning is treated as a maximum likelihood problem; in particular, we present an Expectation-Maximization (EM) algorithm for adjusting the parameters of the architecture. We also develop an on-line learning algorithm in which the parameters are updated incrementally. Comparative simulation results are presented in the robot dynamics domain. This report describes research done at the Dept. of Brain and Cognitive Sciences, the Center for Biological and Computational Learning, and the Artificial Intelligence Laboratory of the Massachusetts Institute of Technology. Support for CBCL is provided in part by a grant from the NSF (ASC-9217041). Support for the laboratory's artificial intelligence research is provided in part by the Advanced Research Projects Agency of the Dept. of Defense. The authors were supported by a grant from the McDonnell-Pew Foundation, by a grant from ATR Human Information Processing Research Laboratories, by a grant from Siemens Corporation, by by grant IRI-9013991 from the National Science Foundation, by grant N00014-90-J-1942 from the Office of Naval Research, and by NSF grant ECS-9216531 to support an Initiative in Intelligent Control at MIT. Michael I. Jordan is a NSF Presidential Young Investigator. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bourlard, H., & Kamp, Y. </author> <year> (1988). </year> <title> Auto-association by multilayer perceptrons and singular value decomposition. </title> <journal> Biological Cybernetics, </journal> <volume> 59, </volume> <pages> 291-294. </pages>
Reference-contexts: That 21 option is still available, although we lose the EM proof of convergence (cf. Jordan & Xu, 1993) and we lose the ability to fit the sub-networks efficiently with IRLS. One interesting example of such an application is the case where the experts are auto-associators <ref> (Bourlard & Kamp, 1988) </ref>, in which case the architecture fits hierarchically-nested local principal component decompositions. Another area in unsupervised learning worth exploring is the non-associative version of the hierarchical architecture.
Reference: <author> Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. </author> <year> (1984). </year> <title> Classification and Regres sion Trees. </title> <address> Belmont, CA: </address> <publisher> Wadsworth International Group. </publisher>
Reference: <author> Bridle, J. </author> <year> (1989). </year> <title> Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition. </title> <editor> In F. Fogelman-Soulie & J. Herault (Eds.), Neuro-computing: </editor> <booktitle> Algorithms, Architectures, and Applications. </booktitle> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Buntine, W. </author> <year> (1991). </year> <title> Learning classification trees. </title> <type> NASA Ames Technical Report FIA-90-12 19-01, </type> <institution> Moffett Field, </institution> <address> CA. </address>
Reference: <author> Cheeseman, P., Kelly, J., Self, M., Stutz, J., Taylor, W., & Freeman, D. </author> <year> (1988). </year> <pages> Autoclass: </pages>
Reference-contexts: It also opens the door to the Bayesian approaches that have been found to be useful in the context of unsupervised mixture model estimation <ref> (Cheeseman, et al., 1988) </ref>. Although we have not emphasized theoretical issues in this paper, there are a number of points that are worth mentioning.
References-found: 5

