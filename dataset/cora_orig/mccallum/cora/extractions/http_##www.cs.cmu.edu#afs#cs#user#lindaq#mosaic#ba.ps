URL: http://www.cs.cmu.edu/afs/cs/user/lindaq/mosaic/ba.ps
Refering-URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/user/lindaq/mosaic/my-home-page.html
Root-URL: 
Email: chase@cs.cmu.edu  
Title: BLAME ASSIGNMENT FOR ERRORS MADE BY LARGE VOCABULARY SPEECH RECOGNIZERS  
Author: Lin Chase 
Address: 5000 Forbes Avenue Pittsburgh, Pennsylvania 15213 USA  
Affiliation: The Robotics Institute Carnegie Mellon University  
Abstract: This paper describes an approach to identifying the reasons that speech recognition errors occur. The algorithm presented requires an accurate word transcript of the utterances being analyzed. It places errors into one of the categories: 1) due to out-of-vocabulary (OOV) word spoken, 2) search error, 3) homophone substitution, 4) language model overwhelming correct acoustics, 5) transcript/pronunciation problems, 6) confused acoustic models, or 7) miscellaneous/not possible to categorize. Some categorizations of errors can supply training data to automatic corrective training methods that refine acoustic models. Other errors supply language model and lexicon designers with examples that identify potential improvements. The algorithm is described and results on the combined evaluation test sets from 1992-1995 of the North American Business (NAB) [1] [2] [3] corpus using the Sphinx-II recognizer [4] are presented. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Paul, D. and Baker, J. </author> <title> The Design for the Wall Street Journal-based CSR Corpus. </title> <booktitle> in: DARPA Speech and Language Workshop. </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1992. </year>
Reference: 2. <author> Pallett, D., Fiscus, J., Fisher, W., Garofolo, J., Lund, B., and Przybocki, M. </author> <title> 1993 Benchmark Tests for the ARPA Spoken Language Program. </title> <booktitle> in: ARPA Speech and Natural Language Workshop. </booktitle> <year> 1994, </year> <pages> pp. 1540. </pages>
Reference: 3. <author> Pallett, D., Fiscus, J., Fisher, W., Garofolo, J., Lund, B., Martin, A., and Przybocki, M. </author> <title> 1994 Benchmark Tests for the ARPA Spoken Language Program. </title> <booktitle> in: ARPA Spoken Language Systems Technology Workshop. </booktitle> <year> 1995, </year> <pages> pp. 538. </pages>
Reference: 4. <author> Hwang, M.-Y. </author> <title> Subphonetic Acoustic Modeling for Speaker-Independent Continuous Speech Recognition. </title> <institution> School of Computer Science, Carnegie-Mellon University, </institution> <month> December </month> <year> 1993. </year>
Reference: 5. <author> Eide, E., Gish, H., Jeanrenaud, P., and Mielke, A. </author> <title> Understanding and Improving Speech Recognition performance Through the Use of Diagnostic Tools. </title> <booktitle> in: IEEE International Conference on Acoustics, Speech, and Signal Processing. </booktitle> <year> 1995, </year> <pages> pp. 221224. </pages>
Reference-contexts: 1. Introduction The main goal of this work, inspired in part by <ref> [5] </ref>, is to provide new feedback mechanisms to speech recognizers by automatically identifying the sources of errors produced by the system. These errors are categorized in terms of components of the recognition system so that adjustments to the system, either automatic or human-supervised, can be made.
Reference: 6. <author> Chase, L. L. </author> <title> Error-Responsive Feedback Mechanisms for Speech Recognizers. </title> <type> Ph.D. Thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, Pittsburgh PA USA, </institution> <note> also available as Tech Report CMU-RI-TR-97-18, </note> <month> April </month> <year> 1997. </year>
Reference-contexts: The error blame assignment techniques discussed here are being developed in conjunction with a new type of automatic confidence annotation technique. The new confidence annotators don't simply identify errors as being present or not, but rather classify them (probabilistically) as being in one of several classes of error <ref> [6] </ref>. Confidence annotation for the purposes of error blame assignment theoretically will be much less expensive in the long run than the technique described here, as it does not require access to an accurate word transcript. <p> distance metrics between basephones, including a simple match count at the frame level, a phonologically-based similarity measure (H W C )<ref> [6] </ref>, and an empirically derived confusion-based distance measure, 3. Defining Error Regions A detailed description of a technique in which error regions are identified and analyzed can be found in [6]. What follows is a summary of this technique. An error region is a contiguous set of frames of acoustic data. It starts at the beginning of a word and ends at the end of a (possibly distinct) word. <p> This error region is categorized in the HYP language model dominates REF acoustics cell of Table 1. The total score of the hypothesis portion of the error region was better by 48 points. (See <ref> [6] </ref> for a graphic version of the placement of error regions w.r.t. the cells of Table 1.) We can gain more insight if we look at what caused the errors using the blame assignment algorithm described in the next section. 4. Classifying Error Regions in applying the blame assignment algorithm. <p> We can also look for evidence that the aligner was straining to fit the REFerence transcript to the acoustic data. (Details of how this is computed can be found in <ref> [6] </ref>.) If this set of tests do not pass, then we move to the next round of tests. algorithm. (b) Confused Acoustics: Did some phone model or models give too high a score to the wrong answer? Or perhaps, did the right answer's models not respond with a high enough score? <p> To do this we test the earliest (leftmost) position in the error region in which the hypothesis and reference do not match. (Details can be found in <ref> [6] </ref>.) (c) If neither of the above work out, then we categorize the error as belonging to class Miscellaneous. Table 2 shows the distribution of categorizations for the NAB development data, a total of 1523 utterances. 5.
References-found: 6

