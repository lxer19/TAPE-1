URL: http://theory.cs.ucdavis.edu/theory_html/papers/wae.ps.gz
Refering-URL: http://theory.cs.ucdavis.edu/
Root-URL: http://www.cs.ucdavis.edu
Email: blackj@cs.ucdavis.edu martel@cs.ucdavis.edu qi@cs.ucdavis.edu  
Title: Graph and Hashing Algorithms for Modern Architectures: Design and Performance  
Author: John R. Black, Jr. Charles U. Martel Hongbin Qi 
Address: Davis, CA 95616, USA  
Affiliation: Department of Computer Science, University of California, Davis 1  
Date: August 20-22, 1998  
Note: Proceedings WAE'98, Saarbrucken, Germany,  Ed. Kurt Mehlhorn, pp. 1-3  More details on these experiments can be found at: http://theory.cs.ucdavis.edu/  
Abstract: We study the effects of caches on basic graph and hashing algorithms and show how cache effects influence the best solutions to these problems. We study the performance of basic data structures for storing lists of values and use these results to design and evaluate algorithms for hashing, Breadth-First-Search (BFS) and Depth-First-Search (DFS). For the basic data structures we show that array-based lists are much faster than linked list implementations for sequential access (often by a factor of 10). We also suggest a linked list variant which improves performance. For lists of boolean values, we show that a bit-vector type approach is faster for scans than either an integer or character array. We give a fairly precise characterization of the performance as a function of list and cache size. Our experiments also provide a fairly simple set of tests to explore these basic characteristics on a new computer system. The basic data structure performance results translate fairly well to DFS and BFS implementation. For dense graphs an adjacency matrix using a bit-vector is the universal winner (often resulting in speedups of a factor of 20 or more over an integer adjacency matrix), while for sparse graphs an array-based adjacency list is best. We study three classical hashing algorithms: chaining, double hashing and linear probing. Our experimental results show that, despite the theoretical superiority of double hashing and chaining, linear probing often outperforms both for random lookups. We explore variations on these traditional algorithms to improve their spatial locality and hence cache performance. Our results also suggest the optimal table size for a given setting. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Ahuja, M. Kodialam, A. Mishra and J. Orlin. </author> <title> Computational testing of maximum flow algorithms. Sloan working Paper, </title> <publisher> MIT, </publisher> <year> 1992. </year>
Reference-contexts: There have been a number of experimental studies of graph algorithms which focus on important problems such as shortest paths [9, 5], minimum spanning trees [17], network flow and matching <ref> [1, 4, 10, 19] </ref>, and min-cut algorithms [6]. These experiments provide valuable insight into the performance of different algorithms and can suggest new algorithmic choices.
Reference: [2] <author> Eli Biham. </author> <title> A fast new DES implementation in software. Technion, Computer Science Dept. Technical Report CS0891-1997. Graph and Hashing Algorithms for Modern Architectures: Design and Performance 12 </title>
Reference: [3] <author> S. Carr, K. Mckinley, and C. Tseng. </author> <title> Compiler optimizations for improving data locality. </title> <booktitle> In Sixth ASPLOS, </booktitle> <pages> 252-262, </pages> <year> 1994. </year>
Reference-contexts: This shows the general lack of focus on these issues by experimenters. 1.1. Related Work Because of its importance, compiler writers have spent considerable effort on generating code with good locality <ref> [3] </ref>, however substantial additional improvements can be gained by proper algorithm design. Moret and Shapiro discuss cache effects on graph algorithms in their MST paper [17]. They comment that data caching and performance is affected by the method used to store a graph.
Reference: [4] <author> B. Cherkassky, A. Goldberg, P. Martin, J. Setubal, and J. Stolfi. </author> <title> Augment or push? A computational study of bipartite matching and unit capacity flow algorithms. </title> <type> Technical Report 97-127, </type> <institution> NEC Research Institute, Inc., </institution> <month> August </month> <year> 1997. </year>
Reference-contexts: There have been a number of experimental studies of graph algorithms which focus on important problems such as shortest paths [9, 5], minimum spanning trees [17], network flow and matching <ref> [1, 4, 10, 19] </ref>, and min-cut algorithms [6]. These experiments provide valuable insight into the performance of different algorithms and can suggest new algorithmic choices.
Reference: [5] <author> B. Cherkassky, A. Goldberg, and T. Radzik. </author> <title> Shortest paths algorithms: theory and experimental evaluation. </title> <journal> Mathematical Programming, </journal> <volume> Vol. 73, </volume> <pages> 129-174, </pages> <month> June </month> <year> 1996. </year>
Reference-contexts: The desire to understand how different algorithms perform in practice has led to a recent increase in the experimental study of algorithms. There have been a number of experimental studies of graph algorithms which focus on important problems such as shortest paths <ref> [9, 5] </ref>, minimum spanning trees [17], network flow and matching [1, 4, 10, 19], and min-cut algorithms [6]. These experiments provide valuable insight into the performance of different algorithms and can suggest new algorithmic choices.
Reference: [6] <author> C. Chekuri, A. Goldberg, D. Karger, M. Levine, and C. Stein, </author> <title> Experimental study of minimum cut algorithms. </title> <type> Technical Report 96-132, </type> <institution> NEC Research Institute, Inc., </institution> <month> October </month> <year> 1996. </year>
Reference-contexts: There have been a number of experimental studies of graph algorithms which focus on important problems such as shortest paths [9, 5], minimum spanning trees [17], network flow and matching [1, 4, 10, 19], and min-cut algorithms <ref> [6] </ref>. These experiments provide valuable insight into the performance of different algorithms and can suggest new algorithmic choices.
Reference: [7] <author> T. Cormen, C. Leiserson, R. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> McGraw-Hill, </publisher> <year> 1990. </year>
Reference-contexts: We first study the performance of basic data structures in isolation and then use the indicated structures for Breadth-First-Search (BFS) Depth-First-Search (DFS) <ref> [7] </ref> and hashing. We also consider ways to predict performance and give indications of the effectiveness of these predictions by measurements of both actual run time and other statistics such as cache misses and number of instructions executed. <p> Although we did not experiment with other algorithms, we suspect that algorithms which access objects more than once, particularly with good spatial locality, would benefit even more from the nice behavior of the bitpacked adjacency matrix. This requires further exploration. 5. Hashing Chaining, double hashing and linear probing <ref> [7, 11] </ref> are the three most classic hashing algorithms. Traditionally, chaining and double hashing are considered superior to linear probing because they disperse the keys better and thus require fewer probes.
Reference: [8] <institution> Digital Semiconductor 21164 Alpha Microprocessor Hardware Reference Manual. Digital Equipment Corporation, Maynard, </institution> <address> MA, </address> <year> 1997. </year>
Reference-contexts: DEC0 and DEC1 have a 64K byte cache and use 16-byte cache blocks. Both Alphas have an 8K byte on-chip direct mapped L1 data-cache and a 96K 3-way set-associative L2 cache (21064 is off-chip, 21164 is on-chip) and both use 32-byte cache blocks <ref> [8] </ref>. The Pentium runs at 266 Mhz, has an L1 cache of 16K for instruction and 16K for data, both 4-way associative with 32-byte line size. The L2 cache is 512K bytes and has a direct 133 MHz bus.
Reference: [9] <author> G. Gallo and S. Pallottino. </author> <title> Shortest paths algorithms. </title> <journal> Annals of Operations Research, </journal> <volume> vol. 13, </volume> <pages> pp. 3-79, </pages> <year> 1988. </year>
Reference-contexts: The desire to understand how different algorithms perform in practice has led to a recent increase in the experimental study of algorithms. There have been a number of experimental studies of graph algorithms which focus on important problems such as shortest paths <ref> [9, 5] </ref>, minimum spanning trees [17], network flow and matching [1, 4, 10, 19], and min-cut algorithms [6]. These experiments provide valuable insight into the performance of different algorithms and can suggest new algorithmic choices. <p> We focused on five data structures to represent our graphs: three adjacency lists, and two were adjacency matrices. The three adjacency list implementations were (1) a linked list, called "LL" above, (2) an array-based adjacency list <ref> [9] </ref>, and (3) a "blocked" linked list where each node of the list contains multiple data items. The two adjacency matrix implementations were (1) a two-dimensional array of integers, and (2) a two-dimensional array of bits.
Reference: [10] <author> D. Johnson and C. McGeoch Ed. </author> <title> Network Flows and Matching. </title> <booktitle> DIMACS Series in Discrete Mathematics and Theoretical Computer Science, </booktitle> <year> 1993. </year>
Reference-contexts: There have been a number of experimental studies of graph algorithms which focus on important problems such as shortest paths [9, 5], minimum spanning trees [17], network flow and matching <ref> [1, 4, 10, 19] </ref>, and min-cut algorithms [6]. These experiments provide valuable insight into the performance of different algorithms and can suggest new algorithmic choices.
Reference: [11] <author> Donald Knuth. </author> <title> Sorting and searching, </title> <booktitle> the art of computer programming, </booktitle> <volume> Volume 3. </volume> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1973. </year>
Reference-contexts: Although we did not experiment with other algorithms, we suspect that algorithms which access objects more than once, particularly with good spatial locality, would benefit even more from the nice behavior of the bitpacked adjacency matrix. This requires further exploration. 5. Hashing Chaining, double hashing and linear probing <ref> [7, 11] </ref> are the three most classic hashing algorithms. Traditionally, chaining and double hashing are considered superior to linear probing because they disperse the keys better and thus require fewer probes. <p> Theoretical Analysis The expected number of probes for uniform hashing is well analyzed <ref> [11] </ref>, but adding a cache complicates things. However an approximate analysis explains the the performance in Figure 2. We have n keys in table of size T , cache capacity C in units of table entries. Let ff be the load factor and P the cache miss penalty. <p> For T &gt; C, the probability Graph and Hashing Algorithms for Modern Architectures: Design and Performance 11 that each location probed is not in the cache is approximated by (T C) T (*). The expected number of probes for a random unsuccessful search is 1 1ff <ref> [11] </ref>.
Reference: [12] <author> D. Knuth. </author> <title> The Stanford GraphBase. </title> <publisher> ACM Press, </publisher> <year> 1994. </year>
Reference-contexts: Graph Algorithm Results Our graph experiments focused on the main graph traversal algorithms: DFS and BFS. We show that the performance predicted by the analysis above is exhibited by these algorithms. We ran experiments on graphs from Knuth's Stanford GraphBase <ref> [12] </ref> and internally generated random graphs. GraphBase generates a variety of graphs which are both standard and available. We considered a range of graphs, but there was little difference in performance between random and structured graphs of the same size and density.
Reference: [13] <author> A. LaMarca and R. Ladner. </author> <title> The influence of caches on the performance of heaps. Journal of Experimental Algorithms, </title> <type> Vol.1, </type> <year> 1996. </year>
Reference-contexts: They attempt to normalize for machine effects by using running times relative to the time needed to scan the adjacency structure. More recently, several researchers have focused on designing algorithms to improve cache performance by improving the locality of the algorithms <ref> [16, 13, 14] </ref>. Lebeck and Wood focused on recoding the SPEC benchmarks, and also developed a cache-profiler to help in the design of faster algorithms. LaMarca and Ladner came up with improved heap and later sorting algorithms by improving locality.
Reference: [14] <author> A. LaMarca and R. Ladner. </author> <title> The influence of caches on the performance of sorting. </title> <booktitle> In the Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> 370-9, </pages> <year> 1997. </year>
Reference-contexts: They attempt to normalize for machine effects by using running times relative to the time needed to scan the adjacency structure. More recently, several researchers have focused on designing algorithms to improve cache performance by improving the locality of the algorithms <ref> [16, 13, 14] </ref>. Lebeck and Wood focused on recoding the SPEC benchmarks, and also developed a cache-profiler to help in the design of faster algorithms. LaMarca and Ladner came up with improved heap and later sorting algorithms by improving locality.
Reference: [15] <author> A. LaMarca and R. Ladner. </author> <title> Cache Performance Analysis of Algorithms. </title> <type> Preprint, </type> <year> 1997. </year>
Reference-contexts: Lebeck and Wood focused on recoding the SPEC benchmarks, and also developed a cache-profiler to help in the design of faster algorithms. LaMarca and Ladner came up with improved heap and later sorting algorithms by improving locality. They also developed a new methodology for analyzing cache effects <ref> [15] </ref>. While we didn't directly use their analysis tools since they were for direct-mapped caches and somewhat different access patterns, our analysis does use some of their ideas. These papers show that substantial improvements in performance can be gained by improving data locality.
Reference: [16] <author> A. Lebeck and D. Wood. </author> <title> Cache profiling and the spec benchmarks: a case study. </title> <journal> Computer, </journal> <volume> 27(10) </volume> <pages> 15-26, </pages> <year> 1994. </year>
Reference-contexts: They attempt to normalize for machine effects by using running times relative to the time needed to scan the adjacency structure. More recently, several researchers have focused on designing algorithms to improve cache performance by improving the locality of the algorithms <ref> [16, 13, 14] </ref>. Lebeck and Wood focused on recoding the SPEC benchmarks, and also developed a cache-profiler to help in the design of faster algorithms. LaMarca and Ladner came up with improved heap and later sorting algorithms by improving locality.
Reference: [17] <author> B. Moret and H. Shapiro. </author> <title> An Empirical Assessment of Algorithms for Constructing a Minimum spanning tree. </title> <journal> DIMACS Series in Discrete Math and Theoretical CS, </journal> <volume> vol. 15, </volume> <pages> 99-117, </pages> <year> 1994. </year>
Reference-contexts: The desire to understand how different algorithms perform in practice has led to a recent increase in the experimental study of algorithms. There have been a number of experimental studies of graph algorithms which focus on important problems such as shortest paths [9, 5], minimum spanning trees <ref> [17] </ref>, network flow and matching [1, 4, 10, 19], and min-cut algorithms [6]. These experiments provide valuable insight into the performance of different algorithms and can suggest new algorithmic choices. <p> Related Work Because of its importance, compiler writers have spent considerable effort on generating code with good locality [3], however substantial additional improvements can be gained by proper algorithm design. Moret and Shapiro discuss cache effects on graph algorithms in their MST paper <ref> [17] </ref>. They comment that data caching and performance is affected by the method used to store a graph. They attempt to normalize for machine effects by using running times relative to the time needed to scan the adjacency structure.
Reference: [18] <author> R. Orni and U. Vishkin. </author> <title> Two computer systems paradoxes: </title> <journal> serialize-to-parallelize and queuing concurrent-writes. </journal> <note> Preprint 1995. </note>
Reference: [19] <author> J. Setubal. </author> <title> Sequential and parallel experimental results with bipartite matching algorithms. </title> <type> Technical Report EC-96-09, </type> <institution> Institute of Computing, University of Campinas, Brasil, </institution> <year> 1996. </year>
Reference-contexts: There have been a number of experimental studies of graph algorithms which focus on important problems such as shortest paths [9, 5], minimum spanning trees [17], network flow and matching <ref> [1, 4, 10, 19] </ref>, and min-cut algorithms [6]. These experiments provide valuable insight into the performance of different algorithms and can suggest new algorithmic choices.
Reference: [20] <author> B. Smith, G. Heileman, and C. Abdallah. </author> <title> The Exponential Hash Function. Journal of Experimental Algorithms, </title> <type> Vol.2, </type> <year> 1997. </year>
Reference-contexts: Our results are similar in spirit to these but tackle different data structures and target different algorithms. Also, our approach focuses more on trying to understand the basic effects architectural features can have by studying them in simple settings. A recent hashing paper <ref> [20] </ref> develops a collision resolution scheme which can reduce the probes compared to double hashing for some very specialized settings. However, since they only look at probes rather than execution time they don't address the effects we study here.
Reference: [21] <author> A. Srivastava and A. Eustace. </author> <title> ATOM: A system for building customized program analysis tools. </title> <booktitle> In ACM Symposium on Programming Language Design and Implementation, </booktitle> <pages> 196-205, </pages> <year> 1994. </year>
Reference-contexts: We also tried using 12 data items per node (12 is the maximum number for a 64 byte allocation by malloc ()). This resulted in a 7-fold speedup compared to the single node LL, but this still makes it almost 50% slower than ARRAY. Our experiments using Atom <ref> [21] </ref> to study the cache misses on Alpha1 shows very much what we predicted. When n exceeds the cache size there are almost exactly n=8 cache misses for ARRAY reflecting the 8-word cache block brought in by each miss. <p> LP's performance is well explained by its fewer cache misses which is detailed in the next section and by the analysis in section 5.3.2. 5.2. Analysis of the Number of Cache misses We used Atom <ref> [21] </ref> to simulate our algorithms' cache behavior. We simulated a direct-mapped single-level cache which is the same as the DECstation cache. We chose to simulate the DECstation cache because a single-level cache would make our experimental results easier to analyze.
References-found: 21

