URL: http://www.cs.rice.edu/~adve/Papers/TPDS.mesh.ps.gz
Refering-URL: http://www.cs.rice.edu/~adve/Papers/
Root-URL: 
Title: Performance Analysis of Mesh Interconnection Networks with Deterministic Routing  
Author: Vikram S. Adve and Mary K. Vernon 
Keyword: Index terms Approximate Mean Value Analysis, closed queueing networks, finite buffers, hot-spots, multiprocessor interconnection networks, k-ary n-cube networks, mesh networks, near-neighbor communication, performance analysis, wormhole routing.  
Abstract: This paper develops detailed analytical performance models for k-ary n-cube networks with single-flit or infinite buffers, wormhole routing, and the non-adaptive deadlock-free routing scheme proposed by Dally and Seitz. In contrast to previous performance studies of such networks, the system is modeled as a closed queueing network that (1) includes the effects of blocking and pipelining of messages in the network, (2) allows for arbitrary source-destination probability distributions, and (3) explicitly models the virtual channels used in the deadlock-free routing algorithm. The models are used to examine several performance issues for 2-dimensional networks with shared-memory traffic. Some results obtained are: (1) when processors are allowed to have multiple outstanding requests, system performance is bandwidth-limited and hence network performance does not scale well with increasing system size, (2) communication locality improves system efficiency, but a very high level of locality is needed for system performance to scale well, (3) in contrast to previous hot-spot studies for indirect networks that assume non-blocking processors, this study finds that significant tree-saturation does not occur even in the presence of severe hot-spots in systems with up to four outstanding requests per processor, and (4) at some plausible system operating points there is a perceptible difference in the efficiencies of processors at different locations in the mesh due to asymmetric loads on the virtual channels by the deadlock avoidance algorithm. These results should prove useful for engineering high-performance systems based on low-dimensional k-ary n-cube networks. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal, B. Lim, D. Kranz and J. Kubiatowicz, </author> <month> APRIL: </month> <title> A Processor Architecture for Multiprocessing, </title> <booktitle> 17th Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1990, </year> <pages> 104-114. </pages>
Reference-contexts: Many existing and emerging multiprocessor sys tems use such low-dimensional direct networks to interconnect the processors, including the Intel Paragon, Cray T3D, Stanford Dash [14], M.I.T. Alewife <ref> [1] </ref>, M.I.T. J-Machine [16] and CMU-Intel iWarp [5]. In this paper, we develop performance models to study k-ary n-cube networks with wormhole routing, with either single-flit or infinite network buffers. Our model for the single-flit buffer case includes the deadlock free routing algorithm of Dally and Seitz [9]. <p> that all the processors in the system execute subtasks of large MIMD parallel programs. 2 Most previous studies of mesh networks, the hypercube and other k-ary n-cubes, have assumed a message-passing workload [8, 10, 11], but a number of recent shared-memory systems have also been based on mesh networks (Alewife <ref> [1] </ref>, Dash [14], Cray T3D). Our model of the network is applicable to both types of workloads. <p> This assumption can accurately capture the behavior of systems in which each processor can switch between multiple contexts <ref> [1, 21] </ref>, and should also be a reasonable model for many message-passing workloads. <p> r j,sd , is the sum of the following three terms: (i) the mean waiting time for the link from the node to its switch, (w node,sd | j ), (ii) the mean residence time for the header flit on each virtual channel c between s and d (r j,c,sd <ref> [1] </ref>), and (iii) the mean delay until the remaining flits of the message reach d (T catchup ): r j,sd = w node,sd | j + c S r j,c,sd [1] + T catchup , j-msg1, msg2, resp1, resp2-, (3) where the summation is over virtual channels, c, on the path <p> (ii) the mean residence time for the header flit on each virtual channel c between s and d (r j,c,sd <ref> [1] </ref>), and (iii) the mean delay until the remaining flits of the message reach d (T catchup ): r j,sd = w node,sd | j + c S r j,c,sd [1] + T catchup , j-msg1, msg2, resp1, resp2-, (3) where the summation is over virtual channels, c, on the path from s to d, including the channels out of the - 10 - - -- processor at s and into the processor at d. 4 Note that the above equation <p> Appendix B contains the details. Further development of the model equations requires new techniques for estimating r j,c,sd <ref> [1] </ref>, r proc , and the waiting time for the first network virtual channel when there are multiple channels from processor to switch. These approximations are motivated and outlined in Sections 3.2 - 3.4 respectively. Section 3.5 concludes with a discussion of the model complexity. 3.2. <p> c h , and approximate this term by u link,c h , the mean utilization of the link by messages on c (i.e., the fraction of time the link is actually transmitting flits from c h (iii) the one cycle for transferring the flit to the next queue: r j,c,sd <ref> [1] </ref> = w (c +1) sd | I + u link,c h + 1, (4) where a message from s to d enters (c +1) sd via input port I. <p> Otherwise the header flit has already reached the destination and the residence time of the k th flit is one plus the mean waiting time for a flit on c h r j,c,sd [k ] = K 1+u link,c h r (c +k -1) sd <ref> [1] </ref> + u link,c h otherwise d is k or more steps away from channel c (k &gt; 1). (5) hhhhhhhhhhhhhhhhhh 4. <p> I = S S S u j,c,i [k ] J 1 hh r j,c,i [k ] + S r j,c,i [l ] J + S J J 1 - S S u j,c,I [l ] hhhhhhhhhhhhhhh 2 hhhhhhhh M J O + S S J u j, (c -1) i <ref> [1] </ref> S r j,c,i [l ] J The calculation of each of these terms is explained below. (i) The total residence time of a message at c is random with an unknown distribution. <p> For each input port iI, the probability that a type j message is waiting to use c is approximated by the utilization of channel (c -1) i by header flits of type j messages that will next use c : u j, (c -1) i <ref> [1] </ref>. Multiplying by the total residence time of such a message and summing over iI and all message types j gives the third term. <p> These results further support conclusions in previous papers that a few contexts per processor are sufficient in systems that are being prototyped today <ref> [1, 21] </ref>. The figure also shows that larger channel buffers become increasingly important as N out is increased, because of the increasing contention. The performance difference between single-flit and infinite channel buffers is significant even for N out = 2 and becomes quite large for N out 4.
Reference: [2] <author> A. Agarwal, </author> <title> Limits on Interconnection Network Performance, </title> <journal> IEEE Trans. on Parallel and Distributed Systems 2, </journal> <month> 4 (October </month> <year> 1991), </year> <pages> 398-412. - 34 </pages> - - -- 
Reference-contexts: Mesh interconnection networks are a special case of k-ary n-cube networks in which the number of dimensions, n, is two. Recent studies of k-ary n-cubes with wormhole routing (a low-latency pipelined routing scheme [9]) have shown that under reasonable assumptions, the optimal value for n is two or three <ref> [2, 8, 10] </ref>. Many existing and emerging multiprocessor sys tems use such low-dimensional direct networks to interconnect the processors, including the Intel Paragon, Cray T3D, Stanford Dash [14], M.I.T. Alewife [1], M.I.T. J-Machine [16] and CMU-Intel iWarp [5]. <p> In this paper, we develop performance models to study k-ary n-cube networks with wormhole routing, with either single-flit or infinite network buffers. Our model for the single-flit buffer case includes the deadlock free routing algorithm of Dally and Seitz [9]. In contrast to previous analyses of these networks <ref> [2, 10, 11] </ref>, the models we derive are closed queueing network models. Also in contrast to previous work, (i) we include the effects of hhhhhhhhhhhhhhhhhh This research was supported by the National Science Foundation under grant number DCR-8451405, and by an IBM Graduate Fellowship. Vikram S. <p> where the summation is over virtual channels, c, on the path from s to d, including the channels out of the - 10 - - -- processor at s and into the processor at d. 4 Note that the above equation is similar in form to the equation used in <ref> [2] </ref> and [22]. One difference between our equation and the corresponding one in [2] is that we include the waiting time for the link that connects the processor to the switch, not just for the first switch buffer. <p> to d, including the channels out of the - 10 - - -- processor at s and into the processor at d. 4 Note that the above equation is similar in form to the equation used in <ref> [2] </ref> and [22]. One difference between our equation and the corresponding one in [2] is that we include the waiting time for the link that connects the processor to the switch, not just for the first switch buffer. <p> One difference between our equation and the corresponding one in [2] is that we include the waiting time for the link that connects the processor to the switch, not just for the first switch buffer. A difference between our model and both <ref> [2, 22] </ref> is that T catchup is not deterministic, since at each link the flits may or may not have to alternate with flits on the link's other virtual channel.
Reference: [3] <author> K. Bolding and L. Snyder, </author> <title> Mesh and torus chaotic routing, </title> <booktitle> Advanced Research in VLSI and Parallel Systems: Proceedings of the Brown/MIT Conference, </booktitle> <month> March </month> <year> 1992, </year> <pages> 333-347. </pages>
Reference-contexts: Whether the imbalance is significant for any particular system depends on several factors, including buffer size, message lengths and request rate. Recent studies have shown that mesh networks without end-around connections also have significant, symmetric imbalances in processor performance even under uniform communication <ref> [3, 7] </ref>. However, these imbalances occur because of unequal traffic requirements on the physical links, which arises from edge effects due to the lack of end-around connections.
Reference: [4] <author> K. Bolding, </author> <title> Non-Uniformities Introduced by Virtual Channel Deadlock Prevention, </title> <type> Technical Report 92-07-07, </type> <institution> Department of Computer Science and Engineering, University of Washington , July 1992. </institution>
Reference-contexts: For links far away from the terminal row or column, the traffic is more evenly balanced on the two virtual channels. In parallel work, Bolding has recently observed the same phenomenon <ref> [4] </ref> and gives similar data as in Figure 2.4, showing the buffer utilizations on the high and low channels for bidirectional and unidirectional topologies. 2.5.
Reference: [5] <author> S. Borkar, </author> <title> iWarp: An Integrated Solution to High-Speed Parallel Computation, </title> <booktitle> Proceedings of Supercomputing '88, </booktitle> <month> November </month> <year> 1988. </year>
Reference-contexts: Many existing and emerging multiprocessor sys tems use such low-dimensional direct networks to interconnect the processors, including the Intel Paragon, Cray T3D, Stanford Dash [14], M.I.T. Alewife [1], M.I.T. J-Machine [16] and CMU-Intel iWarp <ref> [5] </ref>. In this paper, we develop performance models to study k-ary n-cube networks with wormhole routing, with either single-flit or infinite network buffers. Our model for the single-flit buffer case includes the deadlock free routing algorithm of Dally and Seitz [9].
Reference: [6] <author> K. M. Chandy and D. Neuse, Linearizer: </author> <title> A Heuristic Algorithm for Queueing Network Models of Computer Systems, </title> <journal> Communications of the ACM 25(1982), </journal> <pages> 126-134. </pages>
Reference-contexts: In early model validation experiments, we found that the widely-used Schweitzer approximation for product-form networks [18] is not sufficiently accurate for the processor queues since the customer population for each processor, N out , can be small. Furthermore, previously developed approximations such as Linearizer <ref> [6] </ref>, which achieve greater accuracy by solving the equations at a few neighboring populations, introduce too much additional complexity into the model. Below we develop a new approximation for r proc [i ] that is empirically accurate and yet requires very little additional computation when N out is not large.
Reference: [7] <author> S. Chittor and R. Enbody, </author> <title> Performance Degradation in Large Wormhole-Routed Interprocessor Communication Networks, </title> <booktitle> Proc. 1990 International Conference on Parallel Processing, 1990, </booktitle> <address> I-424 - I-428. </address>
Reference-contexts: Whether the imbalance is significant for any particular system depends on several factors, including buffer size, message lengths and request rate. Recent studies have shown that mesh networks without end-around connections also have significant, symmetric imbalances in processor performance even under uniform communication <ref> [3, 7] </ref>. However, these imbalances occur because of unequal traffic requirements on the physical links, which arises from edge effects due to the lack of end-around connections.
Reference: [8] <author> W. J. Dally, </author> <title> A VLSI Architecture for Concurrent Data Structures, </title> <type> Ph.D. Thesis, </type> <institution> California Institute of Technology, </institution> <month> March </month> <year> 1986. </year>
Reference-contexts: Mesh interconnection networks are a special case of k-ary n-cube networks in which the number of dimensions, n, is two. Recent studies of k-ary n-cubes with wormhole routing (a low-latency pipelined routing scheme [9]) have shown that under reasonable assumptions, the optimal value for n is two or three <ref> [2, 8, 10] </ref>. Many existing and emerging multiprocessor sys tems use such low-dimensional direct networks to interconnect the processors, including the Intel Paragon, Cray T3D, Stanford Dash [14], M.I.T. Alewife [1], M.I.T. J-Machine [16] and CMU-Intel iWarp [5]. <p> A torus can be organized so that all links are of equal length, with each link being about twice as long as in the case without end-around connections <ref> [8] </ref>. 2.2. Organization of a Node A node in the system typically consists of one or more processors, some associated local memory, and a hardware switch that controls the routing of messages through the node (Figure 2.2). <p> Workload Assumptions We assume that all the processors in the system execute subtasks of large MIMD parallel programs. 2 Most previous studies of mesh networks, the hypercube and other k-ary n-cubes, have assumed a message-passing workload <ref> [8, 10, 11] </ref>, but a number of recent shared-memory systems have also been based on mesh networks (Alewife [1], Dash [14], Cray T3D). Our model of the network is applicable to both types of workloads. <p> These interpretations of the message contents and the associated message lengths in Table 4.1 are intended to represent a shared memory workload. Message-passing programs could be expected to exchange larger messages between processes, though less frequently <ref> [8] </ref>. The models can be modified to study such workloads; however, that is beyond the scope of this paper. The message sizes also reflect the assumption that the channels in the unidirectional torus are twice as wide as in the bidirectional networks with an equal number of wires per switch.
Reference: [9] <author> W. J. Dally and C. L. Seitz, </author> <title> Deadlock-Free Message Routing in Multiprocessor Interconnection Networks, </title> <journal> IEEE Trans. on Computers C-36, </journal> <month> 5 (May </month> <year> 1987), </year> <pages> 547-553. </pages>
Reference-contexts: Mesh interconnection networks are a special case of k-ary n-cube networks in which the number of dimensions, n, is two. Recent studies of k-ary n-cubes with wormhole routing (a low-latency pipelined routing scheme <ref> [9] </ref>) have shown that under reasonable assumptions, the optimal value for n is two or three [2, 8, 10]. Many existing and emerging multiprocessor sys tems use such low-dimensional direct networks to interconnect the processors, including the Intel Paragon, Cray T3D, Stanford Dash [14], M.I.T. Alewife [1], M.I.T. <p> Alewife [1], M.I.T. J-Machine [16] and CMU-Intel iWarp [5]. In this paper, we develop performance models to study k-ary n-cube networks with wormhole routing, with either single-flit or infinite network buffers. Our model for the single-flit buffer case includes the deadlock free routing algorithm of Dally and Seitz <ref> [9] </ref>. In contrast to previous analyses of these networks [2, 10, 11], the models we derive are closed queueing network models. <p> Dally and Seitz have proposed a deterministic routing scheme that uses the concept of virtual channels to break cycles and prevent deadlock in the networks with end-around connections <ref> [9] </ref>. In this scheme, each physical link is shared by two virtual channels that are fed by separate buffers. As long as both virtual channels have messages to send, they alternate their flits on the physical link.
Reference: [10] <author> W. J. Dally, </author> <title> Performance Analysis of k-ary n-cube Interconnection Networks, </title> <journal> IEEE Trans. on Computers C-39, </journal> <month> 6 (June </month> <year> 1990), </year> <pages> 775-785. </pages>
Reference-contexts: Mesh interconnection networks are a special case of k-ary n-cube networks in which the number of dimensions, n, is two. Recent studies of k-ary n-cubes with wormhole routing (a low-latency pipelined routing scheme [9]) have shown that under reasonable assumptions, the optimal value for n is two or three <ref> [2, 8, 10] </ref>. Many existing and emerging multiprocessor sys tems use such low-dimensional direct networks to interconnect the processors, including the Intel Paragon, Cray T3D, Stanford Dash [14], M.I.T. Alewife [1], M.I.T. J-Machine [16] and CMU-Intel iWarp [5]. <p> In this paper, we develop performance models to study k-ary n-cube networks with wormhole routing, with either single-flit or infinite network buffers. Our model for the single-flit buffer case includes the deadlock free routing algorithm of Dally and Seitz [9]. In contrast to previous analyses of these networks <ref> [2, 10, 11] </ref>, the models we derive are closed queueing network models. Also in contrast to previous work, (i) we include the effects of hhhhhhhhhhhhhhhhhh This research was supported by the National Science Foundation under grant number DCR-8451405, and by an IBM Graduate Fellowship. Vikram S. <p> Workload Assumptions We assume that all the processors in the system execute subtasks of large MIMD parallel programs. 2 Most previous studies of mesh networks, the hypercube and other k-ary n-cubes, have assumed a message-passing workload <ref> [8, 10, 11] </ref>, but a number of recent shared-memory systems have also been based on mesh networks (Alewife [1], Dash [14], Cray T3D). Our model of the network is applicable to both types of workloads.
Reference: [11] <author> E. Gelenbe, </author> <title> Performance Analysis of the Connection Machine, </title> <booktitle> Proc. ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems 18, </booktitle> <month> 1 (May </month> <year> 1990), </year> <pages> 183-191. </pages>
Reference-contexts: In this paper, we develop performance models to study k-ary n-cube networks with wormhole routing, with either single-flit or infinite network buffers. Our model for the single-flit buffer case includes the deadlock free routing algorithm of Dally and Seitz [9]. In contrast to previous analyses of these networks <ref> [2, 10, 11] </ref>, the models we derive are closed queueing network models. Also in contrast to previous work, (i) we include the effects of hhhhhhhhhhhhhhhhhh This research was supported by the National Science Foundation under grant number DCR-8451405, and by an IBM Graduate Fellowship. Vikram S. <p> Workload Assumptions We assume that all the processors in the system execute subtasks of large MIMD parallel programs. 2 Most previous studies of mesh networks, the hypercube and other k-ary n-cubes, have assumed a message-passing workload <ref> [8, 10, 11] </ref>, but a number of recent shared-memory systems have also been based on mesh networks (Alewife [1], Dash [14], Cray T3D). Our model of the network is applicable to both types of workloads.
Reference: [12] <author> P. Kermani and L. Kleinrock, </author> <title> Virtual Cut-through: A New Computer Communication Switching Technique, </title> <booktitle> Computer Networks 3(October 1979), </booktitle> <pages> 267-286. </pages>
Reference-contexts: Deadlock Avoidance for Finite Buffers In the ideal case, when buffer capacity is unlimited, deadlock cannot occur in the network and the wormhole routing scheme is equivalent to an optimized form 1 of the virtual cut-through routing algorithm defined for data communication networks <ref> [12] </ref>. In practice, buffer capacity in a node is limited and deadlock can occur in the networks with end-around connections because all the buffers in a cycle could be filled, with no message able to make progress along that cycle.
Reference: [13] <author> G. Lee, C. P. Kruskal and D. J. Kuck, </author> <title> The Effectiveness of Combining in Shared-Memory Parallel Computers in the Presence of `Hotspots', </title> <booktitle> Proc. International Conference on Parallel Processing, </booktitle> <year> 1986, </year> <pages> 35-41. </pages>
Reference-contexts: The issue has been studied using open queueing models (i.e., assuming non-blocking processors) in the context of multistage interconnection networks <ref> [13, 17, 23] </ref>. We examine the effect of hot-spots in mesh networks by assigning some fraction, F hot , of requests from each processor to a particular node in the system, while the remaining fraction 1-F hot is distributed uniformly across all processors.
Reference: [14] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W. Weber, A. Gupta, J. Hennessy, M. Horowitz and M. Lam, </author> <title> The Stanford DASH Multiprocessor, </title> <booktitle> IEEE Computer 25, </booktitle> <month> 3 (March </month> <year> 1992), </year> <pages> 63-79. </pages>
Reference-contexts: Many existing and emerging multiprocessor sys tems use such low-dimensional direct networks to interconnect the processors, including the Intel Paragon, Cray T3D, Stanford Dash <ref> [14] </ref>, M.I.T. Alewife [1], M.I.T. J-Machine [16] and CMU-Intel iWarp [5]. In this paper, we develop performance models to study k-ary n-cube networks with wormhole routing, with either single-flit or infinite network buffers. <p> the processors in the system execute subtasks of large MIMD parallel programs. 2 Most previous studies of mesh networks, the hypercube and other k-ary n-cubes, have assumed a message-passing workload [8, 10, 11], but a number of recent shared-memory systems have also been based on mesh networks (Alewife [1], Dash <ref> [14] </ref>, Cray T3D). Our model of the network is applicable to both types of workloads. <p> Finally, as explained in Section 3, a simple modification would allow the model to capture the behavior of hierarchical multiprocessors <ref> [14] </ref> containing multiple processors per node (by allowing the rate at which a node generates requests be proportional to the number of additional requests it can make before blocking). Our model does not restrict the communication patterns in the system.
Reference: [15] <author> T. Lin and L. Kleinrock, </author> <title> Performance Analysis of Finite-Buffered Multistage Interconnection Networks with a General Traffic Pattern, </title> <booktitle> Proc. 1991 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems, </booktitle> <address> San Diego, California, USA, </address> <month> May 21-24, </month> <year> 1991. </year>
Reference-contexts: larger finite buffers is a difficult problem. (Previous analytical models of interconnection networks that allow finite buffer sizes are based on a decomposition approximation in which each queue is analyzed in isolation, thus ignoring the dependencies between network stages caused by the blocking and pipelin-ing of messages; for example, see <ref> [15] </ref> and the references therein.) Finally, it would be worthwhile to develop a deadlock-free routing algorithm for the mesh network that does not lead to the imbalance in processor efficiencies that we have observed. 6. Acknowledgements.
Reference: [16] <author> M. D. Noakes, D. A. Wallach and W. J. Dally, </author> <title> The J-Machine Multicomputer: An Architectural Evaluation, </title> <booktitle> 20th Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1993, </year> <pages> 224-235. </pages>
Reference-contexts: Many existing and emerging multiprocessor sys tems use such low-dimensional direct networks to interconnect the processors, including the Intel Paragon, Cray T3D, Stanford Dash [14], M.I.T. Alewife [1], M.I.T. J-Machine <ref> [16] </ref> and CMU-Intel iWarp [5]. In this paper, we develop performance models to study k-ary n-cube networks with wormhole routing, with either single-flit or infinite network buffers. Our model for the single-flit buffer case includes the deadlock free routing algorithm of Dally and Seitz [9].
Reference: [17] <author> G. F. Pfister and V. A. Norton, </author> <title> `Hot Spot' Contention and Combining in Multistage Interconnection Networks, </title> <journal> IEEE Trans. on Computers C-34, </journal> <month> 10 (October </month> <year> 1985), </year> . 
Reference-contexts: Further more, in such systems, hot-spots can cause buffers to fill up in large portions of the network, severely increasing the latency of unrelated (non-hot) network traffic as well, a phenomenon called tree-saturation <ref> [17] </ref>. We use our model to study the effect of communication hot-spots in mesh networks, with processors that block after a limited number of outstanding requests. <p> The issue has been studied using open queueing models (i.e., assuming non-blocking processors) in the context of multistage interconnection networks <ref> [13, 17, 23] </ref>. We examine the effect of hot-spots in mesh networks by assigning some fraction, F hot , of requests from each processor to a particular node in the system, while the remaining fraction 1-F hot is distributed uniformly across all processors. <p> Hot-spot studies in indirect networks have shown that traffic to memory modules other than the hot module is slowed down as much as traffic to the hot module itself <ref> [17] </ref>. This phenomenon has been called tree saturation. <p> The above results suggest that the presence of hot-spots in mesh networks does not significantly increase response times for non-hot traffic, in systems of this size. This is different from the conclusions of Pfister and Nor-ton <ref> [17] </ref> for systems of the same size based on multistage interconnection networks. The principal reason for the hhhhhhhhhhhhhhhhhh 8.
Reference: [18] <author> P. Schweitzer, </author> <title> Approximate Analysis of Multiclass Closed Networks of Queues, </title> <booktitle> International Conference on Stochastic Control and Optimization, </booktitle> <year> 1979. </year>
Reference-contexts: Processor Residence Times (r proc [i]) The processor is modeled as an FCFS queueing center, where the service time is geometrically distributed with a mean value of t cycles. In early model validation experiments, we found that the widely-used Schweitzer approximation for product-form networks <ref> [18] </ref> is not sufficiently accurate for the processor queues since the customer population for each processor, N out , can be small.
Reference: [19] <author> H. S. Stone, </author> <title> High Performance Computer Architecture, </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1987. </year>
Reference-contexts: This ensures that the buffer capacity per switch is equal for all three topologies. - 7 - - -- Locality of Communication: A large class of scientific algorithms, called continuum models <ref> [19] </ref>, involve a grid structure where a particular variable depends only on its nearest neighbors. Such problems can be mapped to the mesh network so that any processor requires mostly values calculated by its four neighboring nodes (or some set of nodes situated within at most a few hops).
Reference: [20] <author> M. K. Vernon, E. D. Lazowska and J. Zahorjan, </author> <title> An Accurate and Efficient Performance Analysis Technique for Multiprocessor Snooping Cache-Consistency Protocols, </title> <booktitle> Proc. 15th International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1988. </year>
Reference-contexts: at the remote node. (The equations can easily be modified to reflect message processing by the node processor or message-handling co-processors.) We choose to develop approximate Mean Value Analysis models because of the previous success of this technique for analyzing other interconnection networks with features that violate separable model assumptions <ref> [20, 22] </ref>. Approximate Mean Value Analysis is based on estimating the mean round-trip time, or cycle time, for each class of customers in the queueing network, relative to some reference point. The processor serves as the reference point for the residence time equations in our model. <p> Multiplying by the total residence time of such a message and summing over iI and all message types j gives the third term. The remaining unknowns in the above equations (u j,c,i [k ], u link,c h ) are calculated using previously developed MVA techniques <ref> [20, 22] </ref>, as described in Appendix B. 3.3. Processor Residence Times (r proc [i]) The processor is modeled as an FCFS queueing center, where the service time is geometrically distributed with a mean value of t cycles.
Reference: [21] <author> W. Weber and A. Gupta, </author> <title> Exploring the Benefits of Multiple Hardware Contexts in a Multiprocessor Architecture: Preliminary Results, </title> <booktitle> The 16th. Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1989, </year> <pages> 273-280. </pages>
Reference-contexts: This assumption can accurately capture the behavior of systems in which each processor can switch between multiple contexts <ref> [1, 21] </ref>, and should also be a reasonable model for many message-passing workloads. <p> These results further support conclusions in previous papers that a few contexts per processor are sufficient in systems that are being prototyped today <ref> [1, 21] </ref>. The figure also shows that larger channel buffers become increasingly important as N out is increased, because of the increasing contention. The performance difference between single-flit and infinite channel buffers is significant even for N out = 2 and becomes quite large for N out 4.
Reference: [22] <author> D. L. Willick and D. L. Eager, </author> <title> An Analytic Model of Multistage Interconnection Networks, </title> <booktitle> Proc. ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <month> May </month> <year> 1990, </year> <pages> 192-202. </pages>
Reference-contexts: In other words, each possible message a processor can have outstanding is modeled as a separate customer in the system. When there are n &lt; N out requests outstanding, the remaining N out -n customers are served in FCFS order at the processor, 3 as in <ref> [22] </ref>. <p> at the remote node. (The equations can easily be modified to reflect message processing by the node processor or message-handling co-processors.) We choose to develop approximate Mean Value Analysis models because of the previous success of this technique for analyzing other interconnection networks with features that violate separable model assumptions <ref> [20, 22] </ref>. Approximate Mean Value Analysis is based on estimating the mean round-trip time, or cycle time, for each class of customers in the queueing network, relative to some reference point. The processor serves as the reference point for the residence time equations in our model. <p> To calculate r j,sd , we need to model the routing, pipelining, and blocking of messages in the network. These features require an approximate model solution. Our model for systems with infinite channel buffers is similar to the model developed for Banyan networks in <ref> [22] </ref>. Their equations assume that processor cycles are required to transfer a message into the network; we do not make this assumption. The only other difference is that we use a somewhat more accurate technique to estimate residence times at the processor for N out &gt;1. <p> summation is over virtual channels, c, on the path from s to d, including the channels out of the - 10 - - -- processor at s and into the processor at d. 4 Note that the above equation is similar in form to the equation used in [2] and <ref> [22] </ref>. One difference between our equation and the corresponding one in [2] is that we include the waiting time for the link that connects the processor to the switch, not just for the first switch buffer. <p> One difference between our equation and the corresponding one in [2] is that we include the waiting time for the link that connects the processor to the switch, not just for the first switch buffer. A difference between our model and both <ref> [2, 22] </ref> is that T catchup is not deterministic, since at each link the flits may or may not have to alternate with flits on the link's other virtual channel. <p> Multiplying by the total residence time of such a message and summing over iI and all message types j gives the third term. The remaining unknowns in the above equations (u j,c,i [k ], u link,c h ) are calculated using previously developed MVA techniques <ref> [20, 22] </ref>, as described in Appendix B. 3.3. Processor Residence Times (r proc [i]) The processor is modeled as an FCFS queueing center, where the service time is geometrically distributed with a mean value of t cycles. <p> Thus our finite buffer model has accuracy similar to the very accurate, less complex models of the infinite buffer case; validations of the infinite buffer model gave results very similar to the model in <ref> [22] </ref> and are not shown here. In cases of high network contention (eg., with N out 4 and t &lt; 50), the analytical single-flit buffer model tends to be somewhat optimistic.
Reference: [23] <author> P. Yew, N. Tzeng and D. H. Lawrie, </author> <title> Distributing Hot-Spot Addressing in Large-Scale Multiprocessors, </title> <journal> IEEE Trans. on Computers C-36, </journal> <month> 4 (April </month> <year> 1987), </year> <month> . - 35 </month> - - -- 
Reference-contexts: The issue has been studied using open queueing models (i.e., assuming non-blocking processors) in the context of multistage interconnection networks <ref> [13, 17, 23] </ref>. We examine the effect of hot-spots in mesh networks by assigning some fraction, F hot , of requests from each processor to a particular node in the system, while the remaining fraction 1-F hot is distributed uniformly across all processors.
References-found: 23

