URL: ftp://ftp.cs.arizona.edu/reports/1993/TR93-04.ps
Refering-URL: http://www.cs.arizona.edu/research/reports.html
Root-URL: http://www.cs.arizona.edu
Title: Cache and TLB Effectiveness in the Processing of Network Data  
Author: Michael A. Pagels Peter Druschel Larry L. Peterson 
Address: Tucson, AZ 85721  
Affiliation: Department of Computer Science The University of Arizona  
Note: March  
Date: TR 93 04  9, 1994  
Abstract: This paper considers the question of how effective caches are in processing network I/O. Our analysis shows that operating system structure plays a key role in the caches behavior, with BSD Unix (a monolithic OS) making more effective use of the cache than Mach (a microkernel OS). Moreover, closer inspection shows that several factors contribute to this result, including how TLBs are managed, how scheduling points are interspersed with data accesses, how data is laid out in memory, and how network functionality is distributed between user space and the kernel. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. B. Abbott and L. L. Peterson. </author> <title> Increasing network throughput by integrating protocol layers. </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> 1(1), </volume> <year> 1993. </year>
Reference-contexts: As context switches provide opportunities for active data to be flushed from the cache [11], access 13 patterns should be optimized to improve temporal and spatial locality. Integrated layer processing <ref> [1] </ref> or application data units [3] could be used to accumulate data accesses in attempts to improve temporal and/or spatial locality. BSD's cache efficacy may be indicative of a light-weight kernel's small cache footprint when processing under light load conditions.
Reference: [2] <author> J. B. Chen and B. N. Bershad. </author> <title> The impact of operating system structure on memory system performance. </title> <booktitle> In Proceedings of the Fourteenth ACM Symposium on Operating System Principles, </booktitle> <month> December </month> <year> 1993. </year>
Reference-contexts: This paper focuses on a related issue: the effectiveness of the cache in processing network data. Block memory operations, common to network data protocol processing, have been shown to be responsible for a large fraction of a system's memory reference cost <ref> [2] </ref>. Workstations employ caches to bridge the gap between CPU and main memory speeds, and one would hope that they help to reduce the stress network data puts on the memory bus.
Reference: [3] <author> D. D. Clark and D. L. Tennenhouse. </author> <title> Architectural considerations for a new generation of protocols. </title> <booktitle> In Proceedings of SIGCOMM 1990 Conference on Communications Architectures and Protocols, </booktitle> <month> September </month> <year> 1990. </year>
Reference-contexts: As context switches provide opportunities for active data to be flushed from the cache [11], access 13 patterns should be optimized to improve temporal and spatial locality. Integrated layer processing [1] or application data units <ref> [3] </ref> could be used to accumulate data accesses in attempts to improve temporal and/or spatial locality. BSD's cache efficacy may be indicative of a light-weight kernel's small cache footprint when processing under light load conditions.
Reference: [4] <author> P. Druschel, M. B. Abbott, M. A. Pagels, and L. L. Peterson. </author> <title> Network subsystem design. </title> <journal> IEEE Network, </journal> <month> July </month> <year> 1993. </year>
Reference-contexts: bandwidth is increasing to the point where it is within an order of magnitude of the memory bandwidth available on a desk-top workstation, making it necessary to limit the number of times network data crosses the memory bus if one hopes to deliver network bandwidth through to the application program <ref> [12, 4] </ref>. With this in mind, much attention has recently been paid to the design of the network interface, and how it interacts with operating system mechanisms designed to reduce the number of times network data is copied [13, 15, 14, 17, 6].
Reference: [5] <author> P. Druschel and L. L. Peterson. Fbufs: </author> <title> A high-bandwidth cross-domain transfer facility. </title> <booktitle> In Proceedings of the Fourteenth ACM Symposium on Operating System Principles, </booktitle> <month> December </month> <year> 1993. </year>
Reference-contexts: Protection mechanisms on many common hardware architectures limit processes from making use of TLB and cache loads performed by other processes in other address spaces. Control of the placement and use of buffers through a mechanism such as fast buffers (fbufs) <ref> [5] </ref> could improve cache effectiveness by increasing sharing and decreasing TLB misses. Use of group TLBs could provide a reduction to a single miss. Distribution of tasks among components also likely increases the number of context switch points encountered along the processing path.
Reference: [6] <author> P. Druschel, L. L. Peterson, and B. S. Davie. </author> <title> Experiences with a high-speed network adaptor: A software perspective. </title> <type> Technical Report TR94-05, </type> <institution> University of Arizona, Department of Computer Science, </institution> <year> 1994. </year>
Reference-contexts: With this in mind, much attention has recently been paid to the design of the network interface, and how it interacts with operating system mechanisms designed to reduce the number of times network data is copied <ref> [13, 15, 14, 17, 6] </ref>. This paper focuses on a related issue: the effectiveness of the cache in processing network data. Block memory operations, common to network data protocol processing, have been shown to be responsible for a large fraction of a system's memory reference cost [2].
Reference: [7] <author> C. A. Gleason, L. Johnson, S. T. Mangelsdorf, T. O. Meyer, and M. A. Forsyth. </author> <title> VLSI circuits for low-end and midrange PA-RISC computers. </title> <journal> Hewlett-Packard Journal, </journal> <pages> pages 12-22, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: This effectiveness is indicative of the benefits an application could obtain in a copy-free implementation; the application would directly obtain this cache residency. The measurements were taken on a 50MHz HP Apollo 720 workstation <ref> [8, 7] </ref>. These workstations 1 For the purpose of this paper, we do not distinguish between 128-byte mbufs, and larger (page-sized) mbuf clusters. 2 contain a 256KByte data cache and a 128KByte instruction cache.
Reference: [8] <author> R. B. Lee. </author> <title> Precision architecture. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 78-91, </pages> <month> January </month> <year> 1989. </year>
Reference-contexts: This effectiveness is indicative of the benefits an application could obtain in a copy-free implementation; the application would directly obtain this cache residency. The measurements were taken on a 50MHz HP Apollo 720 workstation <ref> [8, 7] </ref>. These workstations 1 For the purpose of this paper, we do not distinguish between 128-byte mbufs, and larger (page-sized) mbuf clusters. 2 contain a 256KByte data cache and a 128KByte instruction cache.
Reference: [9] <author> C. Maeda and B. N. Bershad. </author> <title> Protocol service decomposition for high-performance networking. </title> <booktitle> In Proceedings of the Fourteenth ACM Symposium on Operating System Principles, </booktitle> <month> December </month> <year> 1993. </year>
Reference-contexts: In order to increase cache effectiveness while using a microkernel operating system structure, the majority of network data processing could be migrated into the user's application. Implementing network protocols in the end-user application domain's (as opposed to separate network servers ) <ref> [9, 16] </ref> would improve cache effectiveness by causing all network data accesses to occur with the user's address space. Cache interference is still a potential problem, but as many applications would be involved in network data processing the likelihood of self-interference would be decreased.
Reference: [10] <author> H. E. Melesis and D. N. Serpanos. </author> <title> Designing communication subsystems for high-speed networks. </title> <journal> IEEE Network, </journal> <pages> pages 40-46, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: However, limited memory bandwidth on these workstations stands between the improvements in network bandwidth and the advantage to be gained by application programs <ref> [10] </ref>.
Reference: [11] <author> J. C. Mogul and A. Borg. </author> <title> The effect of context switches on cache performance. </title> <booktitle> In Forth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: Specifically, cache residency can be adversely effected primarily in two ways: (1) a task may self-interfere different addresses are accessed which happen to map to the same cache line, or (2) another task maybe executed whose cache accesses flush the cache lines loaded by the original task <ref> [11] </ref>. The structure of the UDP/IP stack (see Figure 1) provides a number of scheduling points where another task may adversely effect cache residency at copy-out time. <p> Improvements in cache effectiveness could be achieved by considering the layout of data in memory to minimize self-interferenceuniform distribution over the cache is best with direct-mapped caches. As context switches provide opportunities for active data to be flushed from the cache <ref> [11] </ref>, access 13 patterns should be optimized to improve temporal and spatial locality. Integrated layer processing [1] or application data units [3] could be used to accumulate data accesses in attempts to improve temporal and/or spatial locality.
Reference: [12] <author> J. Ousterhout. </author> <booktitle> Why aren't operating systems getting faster as fast as hardware? In Usenix 1990 Summer Conference, </booktitle> <pages> pages 247-256, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: bandwidth is increasing to the point where it is within an order of magnitude of the memory bandwidth available on a desk-top workstation, making it necessary to limit the number of times network data crosses the memory bus if one hopes to deliver network bandwidth through to the application program <ref> [12, 4] </ref>. With this in mind, much attention has recently been paid to the design of the network interface, and how it interacts with operating system mechanisms designed to reduce the number of times network data is copied [13, 15, 14, 17, 6].
Reference: [13] <author> C. Partridge and S. Pink. </author> <title> A faster UDP. </title> <journal> IEEE/ACM Transactions on Networking, </journal> <pages> pages 429-440, </pages> <month> August </month> <year> 1993. </year> <month> 15 </month>
Reference-contexts: With this in mind, much attention has recently been paid to the design of the network interface, and how it interacts with operating system mechanisms designed to reduce the number of times network data is copied <ref> [13, 15, 14, 17, 6] </ref>. This paper focuses on a related issue: the effectiveness of the cache in processing network data. Block memory operations, common to network data protocol processing, have been shown to be responsible for a large fraction of a system's memory reference cost [2].
Reference: [14] <author> K. K. Ramakrishnan. </author> <title> Performance considerations in designing network interfaces. </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> 11(2) </volume> <pages> 203-219, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: With this in mind, much attention has recently been paid to the design of the network interface, and how it interacts with operating system mechanisms designed to reduce the number of times network data is copied <ref> [13, 15, 14, 17, 6] </ref>. This paper focuses on a related issue: the effectiveness of the cache in processing network data. Block memory operations, common to network data protocol processing, have been shown to be responsible for a large fraction of a system's memory reference cost [2].
Reference: [15] <author> J. M. Smith and C. B. S. Traw. </author> <title> Giving applications access to Gb/s networking. </title> <journal> IEEE Network, </journal> <volume> 7(4) </volume> <pages> 44-52, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: With this in mind, much attention has recently been paid to the design of the network interface, and how it interacts with operating system mechanisms designed to reduce the number of times network data is copied <ref> [13, 15, 14, 17, 6] </ref>. This paper focuses on a related issue: the effectiveness of the cache in processing network data. Block memory operations, common to network data protocol processing, have been shown to be responsible for a large fraction of a system's memory reference cost [2].
Reference: [16] <author> C. A. Thekkath, T. D. Nguyen, E. Moy, and E. D. Lazowska. </author> <title> Implementing network protocols at user level. </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> 1(5) </volume> <pages> 554-565, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: In order to increase cache effectiveness while using a microkernel operating system structure, the majority of network data processing could be migrated into the user's application. Implementing network protocols in the end-user application domain's (as opposed to separate network servers ) <ref> [9, 16] </ref> would improve cache effectiveness by causing all network data accesses to occur with the user's address space. Cache interference is still a potential problem, but as many applications would be involved in network data processing the likelihood of self-interference would be decreased.
Reference: [17] <author> C. B. S. Traw and J. M. Smith. </author> <title> Hardware/software organization of a high-performance atm host interface. </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> 11(2) </volume> <pages> 240-253, </pages> <month> February </month> <year> 1993. </year> <month> 16 </month>
Reference-contexts: With this in mind, much attention has recently been paid to the design of the network interface, and how it interacts with operating system mechanisms designed to reduce the number of times network data is copied <ref> [13, 15, 14, 17, 6] </ref>. This paper focuses on a related issue: the effectiveness of the cache in processing network data. Block memory operations, common to network data protocol processing, have been shown to be responsible for a large fraction of a system's memory reference cost [2].
References-found: 17

