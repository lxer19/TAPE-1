URL: http://www.cs.rochester.edu/u/sandhya/papers/phd_thesis.ps.gz
Refering-URL: http://www.cs.rochester.edu/u/sandhya/papers/
Root-URL: 
Title: Synchronization, Coherence, and Consistency for High Performance Shared-Memory Multiprocessing  
Author: by Sandhya Dwarkadas J Robert Jump, Co-Chairman James B. Sinclair, Co-Chairman John K. Bennett 
Degree: A Thesis Submitted in Partial Fulfillment of the Requirements for the Degree Doctor of Philosophy Approved, Thesis Committee:  Professor Electrical and Computer Engineering  Associate Professor Electrical and Computer Engineering  Assistant Professor Electrical and Computer Engineering Ken Kennedy Professor  
Date: September, 1992  
Address: Houston, Texas  
Affiliation: RICE UNIVERSITY  Computer Science  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> S. V. Adve and M. D. Hill. </author> <title> Weak Ordering ANew Definition. </title> <booktitle> In Proceedings of the 17th International Symposium on Computer Architecture, </booktitle> <pages> pages 2-14. </pages> <publisher> IEEE, </publisher> <month> May </month> <year> 1990. </year>
Reference-contexts: Relaxed Consistency Models The use of write buffers is essential to hiding memory latency, especially in a hierarchical architecture. However, the buffering and pipelining of program writes leads to a relaxation of the consistency model provided to the user. Several consistency models have been presented in the literature <ref> [53, 40, 37, 29, 1] </ref>. They do not, however, use a uniform method of classification. We present a uniform taxonomy for consistency models that encompasses all existing models, as well as suggesting several variations. <p> Several consistency models have been proposed in the literature. These include sequential consistency [53], processor consistency [40], weak consistency <ref> [29, 1] </ref>, and release consistency [37]. While each of these consistency models provide an accurate view of the access ordering on which the user can rely, the terminology used to present this view has not been consistent. <p> Release Consistency The release consistency model is a variant of weak consistency defined in [37] and in <ref> [1] </ref>, which exploits the information about the partitioning of synchronization accesses into acquires and releases. An acquire is performed to gain access to a set of shared data addresses. A release grants access to some shared data.
Reference: [2] <author> A. Agarwal, M. Horowitz, and J. Hennessy. </author> <title> An Analytical Cache Model. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(2) </volume> <pages> 184-215, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: Analytical models, however, lack the accuracy of either simulation or hardware measurement. Some of the analytical models that have been proposed are by Smith [72], Thiebaut and Stone ([77] and [78]), and Agarwal <ref> [2] </ref>. Smith fits a continuous curve to the simulation results that he obtained for different line sizes, and derives an empirical model. Thiebaut and Stone [78] present a simple fractal model to predict the miss rate of a workload as a function of the cache size.
Reference: [3] <author> A. Agarwal, B. Lim, D. Kranz, and J. Kubiatowicz. </author> <month> APRIL: </month> <title> A Processor Architecture for Multiprocessing. </title> <booktitle> In Proceedings of the 17th International Symposium on Computer Architecture, </booktitle> <pages> pages 104-114. </pages> <publisher> IEEE, </publisher> <month> May </month> <year> 1990. </year>
Reference-contexts: Each node is a bus-based cluster of four processors, thereby utilizing the bus's snooping capability. DASH hides network latency through the use of release consistency (see Chapter 6), and by providing memory access operations for prefetching and delivering data. Like DASH, Alewife <ref> [3] </ref> is a cache-coherent, directory-based, shared-memory multiprocessor whose network is a low-dimension mesh. Structurally, its main difference from DASH is that each node contains a single SPARC-based processor (called April), rather than a cluster. <p> A special processor is needed at each memory module in order to implement the generalized synchronization mechanism. The HEP (developed by Denelcor [49]) associates a Full/Empty bit with every memory location, as does the April processor <ref> [3] </ref>. The bit is tested before a read or 12 write operation if a special symbol is prepended to the variable name. The operation blocks until the test succeeds, at which time the Full/Empty bit is complemented atomically with the execution of the operation. <p> The traditional problem with shared memory machines is that contention for shared resources becomes a limiting bottleneck beyond a few tens of processors. Recent research efforts have investigated the feasibility of providing efficient large-scale shared memory multiprocessors <ref> [54, 3, 20, 42] </ref>. This work augments these efforts with a view to studying the advantages of exploiting the snooping ability of bus-based architectures. Preliminary results contributing to the design of the Willow multiprocessor are presented in [12].
Reference: [4] <author> A. Agarwal, R. L. Sites, and M. Horowitz. ATUM: </author> <title> A New Technique for Capturing Address Traces Using Microcode. </title> <booktitle> In Proceedings of The 13th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 119-127, </pages> <month> June </month> <year> 1986. </year> <journal> vol. </journal> <volume> 14, no. </volume> <pages> 3. </pages>
Reference-contexts: Although this approach is accurate, it has the draw-back of being slow, requiring an overhead of over a hundred just to extract the address trace. ATUM, a technique for capturing address traces using microcode, requires writable control store <ref> [4] </ref>, although its overhead is an order of magnitude smaller than tracing using facilities such as the T-bit. Instruction-Level Simulation One of the main drawbacks of trace-driven simulation, especially in the simulation of multiprocessors, is the inability to predict changes in the execution path caused by architectural modifications. <p> A common method of simulation that alleviates this problem is an instruction-level software simulation of the architecture. Examples are MILS (Mips Instruction Level Simulator) and TRACER, a program that generates address traces for programs executing on a VAX computer <ref> [4] </ref>. In this method, the 19 simulation model of the architecture interpretively executes instructions and supplies the trace at run time to the architecture simulator. This ensures the correct interleaving of all accesses. <p> Tango used UNIX processes to simulate parallelism, resulting in high context switching overhead. Borg et al. [15] use link time code modification to generate long traces on sequential RISC machines. ATUM <ref> [4] </ref> captures address traces using microcode. <p> This avoids the large space overhead incurred by trace-driven simulation and the time involved in accessing these large traces from disk, as well as the specialized hardware needed to generate these traces <ref> [4, 21] </ref>. Execution-driven simulation also avoids another drawback of trace-driven simulation, where changes in the address trace due to architectural variations are not reflected in the simulations carried out. <p> The 68020 profiler was validated in [31]. For serial programs, the ratio of emulation time to actual workload execution time is called the slowdown factor or overhead. The overhead associated with address trace generation using instruction-level simulation is reported to be on the order of 1000 <ref> [4] </ref>. Using a hardware tracing facility (as in the T-bit of the VAX processor) that interpretively determines instruction and data address, the overhead is about 100. If microcoding is used, the overhead is about 10. These figures do not include cache simulation.
Reference: [5] <author> T. E. Anderson. </author> <title> The Performance Implications of Spin-Waiting Alternatives for Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the 1989 International Conference on Parallel Processing, </booktitle> <pages> pages II-170-II-174, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: This scheme does not, however, guarantee fairness. If FIFO ordering is required, the ticket lock [59] used in conjunction with the conditional test&set will result in exactly 2N accesses on the bus as well, while guaranteeing fairness. Anderson <ref> [5] </ref> describes similar modifications, and Glew [38] demonstrates their theoretical benefits. Glew's scheme, however, requires that both the read and the write of the RMW instruction go on to the bus. Our scheme requires only a locked write on the bus. <p> These include the ticket lock, an array-based queuing lock <ref> [5] </ref>, and a list-based queuing lock (MCS [59]). The ticket lock, while guaranteeing fairness, requires a fetch&increment operation. Anderson's lock is similar to the ticket lock, except for avoiding contention for a single polled variable. Anderson's scheme, however, requires space proportional to the number of processors.
Reference: [6] <author> G. R. Andrews and F. B. Schneider. </author> <title> Concepts and Notations for Concurrent Programming. </title> <journal> Computing Surveys, </journal> <volume> 15(1) </volume> <pages> 3-43, </pages> <month> March </month> <year> 1983. </year>
Reference-contexts: These are usually low-level and simple primitives that provide the basic mechanism for enforcing mutual exclusion in more complex synchronization mechanisms implemented in microcode or software. Andrews and Schneider <ref> [6] </ref> provide an overview of the more important language notations for writing concurrent programs. Dubois, Scheurich, and Briggs [29] and Dinning [27] provide surveys of existing synchronization techniques, and the architectural support provided for them. The most primitive atomic operations are atomic loads and stores.
Reference: [7] <author> J. Archibald. </author> <title> A Cache Coherence Approach for Large Multiprocessor Systems. </title> <booktitle> In Proceedings of the 1988 International Conference on Supercomputing, </booktitle> <pages> pages 337-345. </pages> <publisher> ACM, </publisher> <month> July </month> <year> 1988. </year> <pages> 133 134 </pages>
Reference-contexts: Processor-level cache coherence is implemented primarily in software. Software-based coherence gives the programmer flexibility but increases the off-cluster access time. The locking protocol is based in memory. Archibald presents a distributed-write, adaptive snooping protocol that dynamically determines whether a block is being actively shared <ref> [7] </ref>. He extends the protocol to cluster-based hierarchical multiprocessor organizations, introducing cluster ownership for writing. Copies in other clusters are invalidated on a write in any cluster. Since cluster controllers do not contain data, all traffic must go to the processor level to obtain modified information. <p> The protocols do not always provide optimal performance for other shared and non-shared data, however, since write-through increases the amount of traffic on the bus irrespective of whether it is really needed. A class of protocols based on write broadcast ([69] and <ref> [7] </ref>) provide dynamic type classification of the datum cached (i.e., read-only, local, or shared). Rudolph and Segall [69] use a mixed broadcast/ write-first protocol, which dynamically determines whether a block is written by more than one processor by updating all other cached copies on the first write by a processor. <p> The Read-Broadcast scheme, however, requires two extra bus accesses for every successful test&set operation, while the Read-Write-Broadcast scheme requires one 15 additional bus access. Our approach reduces this overhead to exactly N bus accesses, and is expandable to hierarchical architecture schemes. Archibald <ref> [7] </ref> presents a distributed-write scheme that dynamically determines whether a block is actively being shared. The protocol uses a total of eight states. The two states, remote-write1 and remote-write2, indicate that one or two writes have occurred on the bus with no intervening read or write from the processor.
Reference: [8] <author> J. Archibald and J. Baer. </author> <title> Cache Coherence Protocols: Evaluation Using a Multiprocessor Simulation Model. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 4(4) </volume> <pages> 273-298, </pages> <month> November </month> <year> 1986. </year>
Reference-contexts: Additional hardware is also required to maintain the queue of requests for a syncbit. 2.3 Coherency Protocols Many cache coherence protocols have been developed, both for bus-based architectures and for more general interconnection networks. Archibald and Baer present an evaluation of six approaches to the cache coherence problem <ref> [8] </ref>, all of which require a bus-based architecture and that each cache controller observe all bus transactions (snooping). Some of these, along with others in the literature, present solutions that adapt dynamically to the program's reference pattern, as well as provide some form of support for synchronization.
Reference: [9] <author> J. Baer and W. Wang. </author> <title> On the Inclusion Properties for Multi-Level Cache Hierarchies. </title> <type> Technical Report 87-11-08, </type> <institution> University of Washington, Department of Computer Science, University of Washington, </institution> <address> Seattle, WA 98195, </address> <month> November </month> <year> 1987. </year>
Reference-contexts: Unfortunately, the benefits of such an organization have been limited by the increase in memory latency and the necessity to enforce inclusion on the intermediate caches <ref> [9] </ref>. The inclusion property requires that every cache contain a copy of any data held in the caches nearer the processors that it serves, if coherence is to be maintained. <p> The proposed protocol can easily be extended to hierarchical bus architectures with a few modifications. An important criterion for the protocol to work correctly is that the inclusion property hold <ref> [9] </ref>; that is, any memory location that has copies in the lower level caches will also have copies in the higher level cache. This is guaranteed by flushing any line from the lower level caches when it is replaced in the higher level caches.
Reference: [10] <author> B. Beck, B. Kasten, and S. Thakkar. </author> <title> VLSI Assist for a Multiprocessor. </title> <booktitle> In Proceedings of the Second International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 10-20. </pages> <publisher> ACM, </publisher> <month> October </month> <year> 1987. </year>
Reference-contexts: This requires hardware support for locking a cache line during the RMW instruction. If N processors are spinning waiting for access to a lock, the number of bus accesses is O (N 2 ) for all N processors to gain access to the lock. In addition, the SLIC chip <ref> [10] </ref> that supports 11 low-level mutual exclusion has a limited number of lock gates that cannot be directly used by user processes. Hence, additional overhead is required for user access to these lock gates.
Reference: [11] <author> J. K. Bennett, J. B. Carter, and W. Zwaenepoel. </author> <title> Adaptive Software Cache Management for Distributed Shared Memory Architectures. </title> <booktitle> In Proceedings of the 17th International Symposium on Computer Architecture. IEEE, </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: Adaptive Caching Address references made by processors may normally be classified into a small number of categories <ref> [11, 81] </ref>. We investigate the effect on program performance of using multiple coherence protocols on different types of data. Two coherency modes are provided, write-invalidate when shared, and write-through when shared. <p> In situations where a lock is acquired by one processor several times before any other processor accesses the lock, it may be beneficial to invalidate all other copies instead of updating them <ref> [11] </ref>. One method of performing this invalidation is to add an additional state to the cache line to indicate if it has been accessed by any other processor. <p> We also present a uniform taxonomy for several extant consistency models, and provide an evaluation of their performance on our hierarchical bus-based architecture. 6.1 Adaptive Caching Address references made by processors may normally be classified into a small number of categories <ref> [11, 81] </ref>. We enumerate one possible categorization (following [11]) below, and indicate the coherency mode that we believe best suits the datum's access patterns. <p> We also present a uniform taxonomy for several extant consistency models, and provide an evaluation of their performance on our hierarchical bus-based architecture. 6.1 Adaptive Caching Address references made by processors may normally be classified into a small number of categories [11, 81]. We enumerate one possible categorization (following <ref> [11] </ref>) below, and indicate the coherency mode that we believe best suits the datum's access patterns.
Reference: [12] <author> J. K. Bennett, S. Dwarkadas, J. Greenwood, and E. Speight. </author> <title> Willow: A Scalable Shared-Memory Multiprocessor. In Supercomputing '92, </title> <note> to appear, </note> <month> November </month> <year> 1992. </year>
Reference-contexts: This technique is used to develop and evaluate architectural design decisions for a hierarchical bus-based shared-memory architecture. Several of these ideas are incorporated into the Willow shared-memory multiprocessor <ref> [12] </ref>. The dissertation is organized as follows. Chapter 2 provides an overview of previous and on-going research efforts relevant to this work. Chapter 3 describes the execution-driven simulator and validates the results generated against an existing multiprocessor, and a cycle-level simulator. <p> This work augments these efforts with a view to studying the advantages of exploiting the snooping ability of bus-based architectures. Preliminary results contributing to the design of the Willow multiprocessor are presented in <ref> [12] </ref>. We explore the advantages of an architecture based on a hierarchy of buses, with caches and memory distributed throughout the hierarchy. The hierarchy removes the bottleneck of a single bus resource, while retaining part of the snooping ability of a 50 bus-based architecture. <p> Our proposed architecture consists of a hierarchy of caches and buses, with memory at each level of the hierarchy, as shown in Figure 4.1. While the intermediate caches must support inclusion, inclusion-induced cache size blowup is prevented by the provision of memory at each level <ref> [12] </ref>. Thus, data is placed at a memory level that is closest to all processors that are going to share it. All memory is assumed to be globally shared and accessible if necessary via a global interconnect, as in the Willow multiprocessor design [12]. <p> the provision of memory at each level <ref> [12] </ref>. Thus, data is placed at a memory level that is closest to all processors that are going to share it. All memory is assumed to be globally shared and accessible if necessary via a global interconnect, as in the Willow multiprocessor design [12]. The global interconnect prevents extra levels of access delay in the case of bad data placement. There is only one path between any two processors. <p> In this section, we report the overall performance of the applications as compared to a baseline single-bus configuration. These results have been used in the evaluation of the Willow shared-memory multiprocessor architecture <ref> [12] </ref>. We study the effect of each architectural variation in detail in the following chapters. The main goal of our design and simulation of hierarchical multiprocessors is to maximize the efficiency of each processor involved in a computation. Hence, we chose speedup as our overall performance metric. <p> We have designed our architecture to include memory modules on each of the hierarchies of buses, with a view to exploiting access locality <ref> [12] </ref>. We saw from 9-13% improvement for SOR using hierarchical memory against the case in which the same data is moved to the global memory, causing the second-level cache to thrash. <p> Another option is to provide a dedicated region of memory for synchronization variables that is replicated at each level of the memory hierarchy, as in the Willow architecture <ref> [12] </ref>. This memory appears as an extra region in each cache, but the synchronization differs from cache memory in two important ways. <p> Our multiprocessor design uses a hierarchy of caches and buses. To improve the performance of the hierarchy, we also provided hierarchical memory by placing a memory module at each level in the hierarchy, as in the Willow architecture <ref> [12] </ref>. This allows the exploitation of access locality and avoids cache size blowup as the number of levels in the hierarchy are increased. The design goal was to reduce communication overhead and increase the number of processors that can be efficiently utilized to solve any given problem. <p> Ongoing work involves the determination of the behavior of a larger number of programs on hierarchical bus-based architectures, and the determination of whether the percentage benefits obtained from each architectural innovation justifies the cost. Both of these issues are being explored in the design of the Willow multiprocessor <ref> [12] </ref>.
Reference: [13] <author> B. N. Bershad, E. D. Lazowska, and H. M. Levy. </author> <title> PRESTO: A System for Object-Oriented Parallel Programming. </title> <type> Technical Report TR 87-09-01, </type> <institution> Department of Computer Science, University of Washington, </institution> <address> Seattle, WA, </address> <month> September </month> <year> 1987. </year>
Reference-contexts: PRESTO, written in C++, is an object-oriented programming environment designed for shared-memory systems and developed at the University of Washington <ref> [13] </ref>.
Reference: [14] <author> P. Bitar and A. M. Despain. </author> <title> Multiprocessor Cache Synchronization Issues, Innovations, Evolution. </title> <booktitle> In Proceedings of the 13th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 424-433, </pages> <year> 1986. </year> <month> 135 </month>
Reference-contexts: The hardware required consists of the Full/Empty bit and the logic to initialize a bit, to queue a process if the test fails, and to implement the indivisible update operations. Bitar and Despain <ref> [14] </ref> describe an eight-state cache protocol that provides efficient busy-wait locking. The first block of any data that is accessed in a mutually exclusive manner is fetched for write privilege and locked until the entire operation is done. <p> Extra hardware in the form of a busy-wait register is also required for efficiency reasons. Goodman, Vernon, and Woest [41] propose a set of efficient primitives for process synchronization in multiprocessors. This is similar to the idea by Bitar and Despain <ref> [14] </ref>, extended to the Multicube architecture (multidimensional grid of buses [42]). The protocol assumes that the interconnect supports broadcast, and that hardware combining is not implemented in the interconnect.
Reference: [15] <author> A. Borg, R. E. Kessler, and D. W. Wall. </author> <title> Generation and Analysis of Very Long Address Traces. </title> <booktitle> In Proceedings of the 17th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: Several approaches to the application of execution-driven simulation to cache and shared-memory performance evaluation have evolved in the last few years <ref> [15, 75, 32, 25] </ref>. We have adapted this technique for cache and shared-memory simulation in our testbed. We describe our approach and compare it to other existing approaches in Chapter 3. 2.4.3 Analytical Modeling Analytical models provide a quick first-cut estimate of cache performance. <p> Tango [25] is a multiprocessor simulation and tracing package based on the execution-driven approach that concentrates on program data accesses. Tango used UNIX processes to simulate parallelism, resulting in high context switching overhead. Borg et al. <ref> [15] </ref> use link time code modification to generate long traces on sequential RISC machines. ATUM [4] captures address traces using microcode.
Reference: [16] <author> C. S. Burrus and T. W. Parks. </author> <title> DFT/FFT and Convolution Algorithms. </title> <publisher> Wiley-Interscience, </publisher> <year> 1985. </year>
Reference-contexts: Multi-Way Merge uses an efficient algorithm to perform a Multi-Way Merge on all the sorted sublists in one step. 2. Fast Fourier Transform (FFT) The FFT program is based on the Cooley Tukey Radix 2 Decimation in Time algorithm <ref> [16] </ref>. A general description of the algorithm can be found in [64]. The FFT program is similar to the sorting algorithms in that for an N-point FFT, the first log 2 N log 2 P (where P is the number of processors) are performed independently on each processor.
Reference: [17] <author> J. B. Carter, J. K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and Performance of Munin. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <month> October </month> <year> 1991. </year>
Reference-contexts: The scalability of this directory scheme is, however, limited. Several researchers have attempted to provide an abstraction of shared memory on distributed memory hardware. Munin <ref> [17] </ref> is one such high-performance distributed shared memory system developed here at Rice University. Its distinguishing characteristics include the use of software-based release consistency and a multiple-protocol 16 adaptive cache coherence mechanism that exploits the access patterns of shared data objects.
Reference: [18] <author> J. K. Cheng and T. S. Huang. </author> <title> A Subgraph Isomorphism Algorithm Using Resolution. </title> <journal> Pattern Recognition, </journal> <volume> 11(9) </volume> <pages> 371-379, </pages> <year> 1981. </year>
Reference-contexts: It is an NP-complete problem found in pattern matching in a number of applications. Subgraph Isomorphism takes two graphs and finds all instances of one graph in the other. The program we used implemented a parallel version [26] of the algorithm in <ref> [18] </ref>. 4.3 Performance Evaluation In order to validate our design, we simulated the execution of several parallel applications on a variety of architectural variations using our execution-driven simulator. In this section, we report the overall performance of the applications as compared to a baseline single-bus configuration.
Reference: [19] <author> D. R. Cheriton, H. A. Goosen, and P. D. Boyle. </author> <title> Paradigm: A Highly Scalable Shared-Memory Multicomputer Architecture. </title> <journal> Computer, </journal> <volume> 24(2) </volume> <pages> 33-46, </pages> <month> Feb </month> <year> 1991. </year>
Reference-contexts: Wilson's design does not provide for memory at each level, which introduces a bottleneck if the caches are not large enough to hold all the program's accessed addresses. Wilson also uses a single invalidation-based cache coherence protocol. The ParaDiGM shared-memory multiprocessor <ref> [19] </ref> is a multiprocessor consisting of hierarchical-bus clusters connected by a switching network. ParaDiGM's memory is distributed amongst the clusters and placed at the root of each hierarchy. The hierarchy of buses is complemented by a network connecting the clusters together at the processor level. <p> Most of the data is only read-shared, and the data size is small enough to fit in each processor's cache for the data set size we chose. Evaluation of Hierarchical Memory The placement of memory on the most global bus <ref> [19] </ref> is reminiscent of interconnection-network multiprocessors with processors and memory separated by a switching network. We have designed our architecture to include memory modules on each of the hierarchies of buses, with a view to exploiting access locality [12].
Reference: [20] <author> D. R. Cheriton, A. Gupta, P. D. Boyle, and H. A. Goosen. </author> <title> The VMP Multiprocessor: Initial Experience, Refinements, and Performance. </title> <booktitle> In Proceedings of the 15th International Symposium on Computer Architecture. IEEE, </booktitle> <month> May </month> <year> 1988. </year>
Reference-contexts: The traditional problem with shared memory machines is that contention for shared resources becomes a limiting bottleneck beyond a few tens of processors. Recent research efforts have investigated the feasibility of providing efficient large-scale shared memory multiprocessors <ref> [54, 3, 20, 42] </ref>. This work augments these efforts with a view to studying the advantages of exploiting the snooping ability of bus-based architectures. Preliminary results contributing to the design of the Willow multiprocessor are presented in [12].
Reference: [21] <author> D. W. Clark. </author> <title> Cache Performance in the VAX-11/780. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 1(1) </volume> <pages> 24-37, </pages> <month> February </month> <year> 1983. </year>
Reference-contexts: Analysis using hardware measurement is, however, limited to an existing implementation of a cache-based architecture. Exact reproducibility is also not possible with hardware measurement. A comprehensive set of hardware measurements is presented by Clark <ref> [21] </ref>. 17 2.4.2 Simulation Since accurately modeling the characteristics of program behavior that determine the performance of caches has been found to be analytically intractable, simulation has been the prevailing technique used to yield accurate predictions [72]. Various methods of simulation in current use are described below. <p> This avoids the large space overhead incurred by trace-driven simulation and the time involved in accessing these large traces from disk, as well as the specialized hardware needed to generate these traces <ref> [4, 21] </ref>. Execution-driven simulation also avoids another drawback of trace-driven simulation, where changes in the address trace due to architectural variations are not reflected in the simulations carried out.
Reference: [22] <author> R. G. Covington, S. Dwarkadas, J. R. Jump, S. Madala, and J. B. Sinclair. </author> <title> The Efficient Simulation of Parallel Computer Systems. </title> <journal> International Journal in Computer Simulation, </journal> <volume> 1 </volume> <pages> 31-58, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: The method is a variant of execution-driven simulation <ref> [23, 22] </ref>. Our goal is to speed up the simulation compared to instruction- or cycle-level simulation without a significant loss of accuracy, and without incurring the large storage overheads of trace-driven approaches or requiring special hardware support. <p> In particular, changes in the execution sequence due to timing variations will not be reflected in the simulations. We have developed and evaluated an efficient technique for the simulation of shared-memory multiprocessors driven by real programs. The method used is a variant of execution-driven simulation <ref> [23, 22] </ref>. Our goal is to improve simulation performance without losing the accuracy that can be obtained by other techniques such as instruction-driven simulation, and without incurring the large storage overheads of 22 trace-driven approaches. Special hardware is not required to support execution-driven simulation. <p> In the case of message-based systems, the program may be profiled to generate only timing information at the beginning of each basic block. The message transmissions themselves 32 may be simulated in detail. This results in program slowdowns of 1.2 to 15 <ref> [22] </ref>. Program slowdown or the overhead of the simulation is defined as the ratio of the time required to execute the RPPT simulation of the program's execution to the time required to execute the program as a stand-alone Concurrent C application.
Reference: [23] <author> R. G. Covington, S. Madala, V. Mehta, J. R. Jump, and J. B. Sinclair. </author> <title> The Rice Parallel Processing Testbed. </title> <booktitle> In Proceedings of the ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 4-11, </pages> <address> Santa Fe, NM, </address> <month> May </month> <year> 1988. </year> <month> 136 </month>
Reference-contexts: The method is a variant of execution-driven simulation <ref> [23, 22] </ref>. Our goal is to speed up the simulation compared to instruction- or cycle-level simulation without a significant loss of accuracy, and without incurring the large storage overheads of trace-driven approaches or requiring special hardware support. <p> Simulation of the architecture during the actual execution of the parallel program under consideration permits us to determine the effects of the architecture on the execution of the program with a high degree of accuracy. We have developed this technique as part of the Rice Parallel Processing Testbed (RPPT) <ref> [23] </ref>, a simulation tool for the performance evaluation of parallel computer systems. RPPT performs a compile-time analysis of the program under test and converts it into one that will provide information on instruction execution times and memory accesses [31] to the simulator as it executes. <p> In this method, the 19 simulation model of the architecture interpretively executes instructions and supplies the trace at run time to the architecture simulator. This ensures the correct interleaving of all accesses. Execution-Driven Simulation Execution-driven simulation <ref> [23] </ref> extracts the address trace and simulates the cache during the actual execution of the program. <p> In particular, changes in the execution sequence due to timing variations will not be reflected in the simulations. We have developed and evaluated an efficient technique for the simulation of shared-memory multiprocessors driven by real programs. The method used is a variant of execution-driven simulation <ref> [23, 22] </ref>. Our goal is to improve simulation performance without losing the accuracy that can be obtained by other techniques such as instruction-driven simulation, and without incurring the large storage overheads of 22 trace-driven approaches. Special hardware is not required to support execution-driven simulation. <p> The technique that we use involves simulation of the architecture during the actual execution of the parallel program under consideration. This guarantees that changes in the execution path due to architectural modifications are accurately modeled. We have implemented this technique as part of the Rice Parallel Processing Testbed (RPPT) <ref> [23] </ref>, a simulation tool for the performance evaluation of parallel computer systems. A similar approach to cache simulation is used in [60], but only instruction addresses are traced. The TRAPEDS [75] approach is limited to non-multiprogrammed multicomputer address trace analysis, and does not handle shared-memory systems. <p> reduced, resulting in a total overhead of 35 to 90 for the simulation of a direct-mapped cache (for the CISC architectures). 3.1.2 The Rice Parallel Processing Testbed We have implemented our technique for the generation of addresses and simulation of caches as part of the Rice Parallel Processing Testbed (RPPT) <ref> [23] </ref>. The RPPT is a performance evaluation tool for studying the execution of parallel programs on concurrent systems. The RPPT uses execution-driven simulation to accurately describe the data dependent flow of execution in a parallel program and to efficiently provide estimates of user code execution times.
Reference: [24] <author> R.G. Covington and J.R. </author> <title> Jump. CSIM 2.0 User's Manual. </title> <type> Technical Report TR 8712, </type> <institution> Department of Electrical and Computer Engineering, Rice University, Houston, TX, </institution> <month> October </month> <year> 1987. </year>
Reference-contexts: A 31 PRESTO interface has been provided to the user by porting PRESTO on top of Concurrent C [61]. * YACSIM (Yet Another C SIMulation Package, originally called CSIM): This is a process-level, discrete-event simulator providing a set of procedures for process creation, delaying, event queue manipulation, and statistics collection <ref> [24, 51] </ref>.
Reference: [25] <author> H. Davis, S. R. Goldschmidt, and J. Hennessy. </author> <title> Tango: A Multiprocessor Simulation and Tracing System. </title> <type> Technical report, </type> <institution> Computer Systems Laboratory, Stanford University, </institution> <year> 1990. </year>
Reference-contexts: Several approaches to the application of execution-driven simulation to cache and shared-memory performance evaluation have evolved in the last few years <ref> [15, 75, 32, 25] </ref>. We have adapted this technique for cache and shared-memory simulation in our testbed. We describe our approach and compare it to other existing approaches in Chapter 3. 2.4.3 Analytical Modeling Analytical models provide a quick first-cut estimate of cache performance. <p> The post-processing phase in MPtrace is quite slow, generating about 3000 addresses per second. Overhead reduction in the trace generation phase is also important for MPtrace since this reduction directly affects the accuracy of the trace. Tango <ref> [25] </ref> is a multiprocessor simulation and tracing package based on the execution-driven approach that concentrates on program data accesses. Tango used UNIX processes to simulate parallelism, resulting in high context switching overhead. Borg et al. [15] use link time code modification to generate long traces on sequential RISC machines.
Reference: [26] <author> V. Debbad, G. Lauderdale, and R. Mukherjee. </author> <title> Distributed Vs. Shared Memory for Subgraph Isomorphism. </title> <institution> Rice University, Class Project, </institution> <month> December </month> <year> 1988. </year>
Reference-contexts: It is an NP-complete problem found in pattern matching in a number of applications. Subgraph Isomorphism takes two graphs and finds all instances of one graph in the other. The program we used implemented a parallel version <ref> [26] </ref> of the algorithm in [18]. 4.3 Performance Evaluation In order to validate our design, we simulated the execution of several parallel applications on a variety of architectural variations using our execution-driven simulator.
Reference: [27] <author> A. Dinning. </author> <title> A Survey of Synchronization Methods for Parallel Computers. </title> <booktitle> Computer, </booktitle> <pages> pages 66-77, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: Andrews and Schneider [6] provide an overview of the more important language notations for writing concurrent programs. Dubois, Scheurich, and Briggs [29] and Dinning <ref> [27] </ref> provide surveys of existing synchronization techniques, and the architectural support provided for them. The most primitive atomic operations are atomic loads and stores. Complex synchronization protocols can be built using these. They are, however, hard to design, understand, and prove correct, and are often inefficient.
Reference: [28] <author> M. Dubois, F. Briggs, I. Patil, and M. Balakrishnan. </author> <title> Trace-Driven Simulations of Parallel and Distributed Algorithms in Multiprocessors. </title> <booktitle> In Proceedings of The 13th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 909-915, </pages> <year> 1986. </year>
Reference-contexts: Event list manipulation overhead is incurred only during a process interaction point, when one process needs to communicate with another process. Such interactions can include message passing, synchronization, cache misses, and accesses to shared variables. Timing information and/or address traces for the program are generated on the fly <ref> [28] </ref> during execution by the execution-driven technique. This avoids the large space overhead incurred by trace-driven simulation and the time involved in accessing these large traces from disk, as well as the specialized hardware needed to generate these traces [4, 21].
Reference: [29] <author> M. Dubois, C. Scheurich, and F. A. Briggs. </author> <title> Synchronization, Coherence, and Event Ordering in Multiprocessors. </title> <booktitle> Computer, </booktitle> <pages> pages 9-21, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: Relaxed Consistency Models The use of write buffers is essential to hiding memory latency, especially in a hierarchical architecture. However, the buffering and pipelining of program writes leads to a relaxation of the consistency model provided to the user. Several consistency models have been presented in the literature <ref> [53, 40, 37, 29, 1] </ref>. They do not, however, use a uniform method of classification. We present a uniform taxonomy for consistency models that encompasses all existing models, as well as suggesting several variations. <p> These are usually low-level and simple primitives that provide the basic mechanism for enforcing mutual exclusion in more complex synchronization mechanisms implemented in microcode or software. Andrews and Schneider [6] provide an overview of the more important language notations for writing concurrent programs. Dubois, Scheurich, and Briggs <ref> [29] </ref> and Dinning [27] provide surveys of existing synchronization techniques, and the architectural support provided for them. The most primitive atomic operations are atomic loads and stores. Complex synchronization protocols can be built using these. They are, however, hard to design, understand, and prove correct, and are often inefficient. <p> However, this places severe restrictions on the hardware by not taking advantage of the available memory and network parallelism. Several relaxed consistency models that allow the buffering and pipelining of writes have been proposed <ref> [37, 40, 29] </ref>. We elaborate on these models in Chapter 6. 2.4 Cache and Shared-Memory Multiprocessor Evaluation Techniques Since an efficient cache is critical to system performance, various cache evaluation tools, including hardware measurement, trace-driven simulation, and analytical modeling, have been used. <p> Several consistency models have been proposed in the literature. These include sequential consistency [53], processor consistency [40], weak consistency <ref> [29, 1] </ref>, and release consistency [37]. While each of these consistency models provide an accurate view of the access ordering on which the user can rely, the terminology used to present this view has not been consistent. <p> In the example used to demonstrate when sequential consistency is not maintained, the read sequences for processors 2 and 3 do not violate processor processor consistency regardless of the class of data accessed. 111 Weak Consistency The weak consistency model was proposed by Dubois et al. <ref> [29] </ref>. We reclassify this model into two subclasses. These models allow both buffering and pipelining of writes. Weak Sequential Consistency A multiprocessor is said to provide weak sequential consistency if and only if it appears sequentially consistent at synchronization points.
Reference: [30] <author> S. Dwarkadas. </author> <title> Efficient Methods for Cache Performance Prediction. </title> <type> Master's thesis, </type> <institution> Department of Electrical and Computer Engineering, Rice University, Houston TX, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: A detailed description, analysis, 1 A basic block is a section of code which, once entered, has every instruction in it executed exactly once, and is not contained in a larger basic block. 25 and validation of the profiling technique can be found in <ref> [30] </ref> and [31]. The description of the profiler is followed by an overview of the simulation system that we have developed. 3.1.1 The Address Trace and Timing Profilers Conventional simulation techniques used to predict cache/shared-memory performance involve two main steps.
Reference: [31] <author> S. Dwarkadas, J. R. Jump, and J. B. Sinclair. </author> <title> Efficient Simulation of Cache Memories. </title> <booktitle> In Proceedings of the Winter Simulation Conference, </booktitle> <month> December </month> <year> 1989. </year>
Reference-contexts: RPPT performs a compile-time analysis of the program under test and converts it into one that will provide information on instruction execution times and memory accesses <ref> [31] </ref> to the simulator as it executes. This information can then be used to drive a simulation of the architecture at various levels of detail, depending on the accuracy required and the architectural features under study. <p> We adapted this model to develop an estimative simulation technique for the performance evaluation of cache-based multiprocessors <ref> [31] </ref>. 2.5 Summary We have reviewed several approaches to the design of shared-memory multiprocessors. The underlying goal of much of the recent research in this area has been the reduction and hiding of memory access latency. <p> A detailed description, analysis, 1 A basic block is a section of code which, once entered, has every instruction in it executed exactly once, and is not contained in a larger basic block. 25 and validation of the profiling technique can be found in [30] and <ref> [31] </ref>. The description of the profiler is followed by an overview of the simulation system that we have developed. 3.1.1 The Address Trace and Timing Profilers Conventional simulation techniques used to predict cache/shared-memory performance involve two main steps. <p> The execution-driven methodology has been used to implement profilers for the generation of address and timing information for the Motorola 68020, the Intel 80386, and the SPARC processor. The 68020 profiler was validated in <ref> [31] </ref>. For serial programs, the ratio of emulation time to actual workload execution time is called the slowdown factor or overhead. The overhead associated with address trace generation using instruction-level simulation is reported to be on the order of 1000 [4]. <p> If the performance of the system caches must be determined, address trace information may also be generated. In this case, the cache data structures and timing information must be updated, but coherency need not be maintained. The overheads involved are 33 60-180 <ref> [31] </ref> for a multiprocessor simulation including that of a set-associative cache with a write back algorithm, as compared to the Concurrent C version of the program. The program needs to be rescheduled only at process interaction points, which may be only the message sends and receives. <p> If only shared data is of interest, some run time overhead is necessary in order to distinguish between shared and private data. Analytical models may be used at any level to replace the detailed simulation of a system component <ref> [31] </ref>. Our simulation system runs as a user-level process on the existing operating system.
Reference: [32] <author> S. J. Eggers and R. H. Katz. </author> <title> A Characterization of Sharing in Parallel Programs and its Application to Coherency Protocol Evaluation. </title> <booktitle> In Proceedings of the 15th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 373-383. </pages> <publisher> IEEE, </publisher> <month> May </month> <year> 1988. </year> <month> 137 </month>
Reference-contexts: Several approaches to the application of execution-driven simulation to cache and shared-memory performance evaluation have evolved in the last few years <ref> [15, 75, 32, 25] </ref>. We have adapted this technique for cache and shared-memory simulation in our testbed. We describe our approach and compare it to other existing approaches in Chapter 3. 2.4.3 Analytical Modeling Analytical models provide a quick first-cut estimate of cache performance. <p> TRAPEDS also does not model communication accurately, since it does not evaluate the effects of contention. For these reasons, TRAPEDS is not an appropriate tool to study trade-offs in computation and communication. MPtrace <ref> [32] </ref> uses the execution-driven approach to generate traces of information that allow an address trace to be reconstructed. MPtrace achieves an overhead of 2 to 3 times the execution time of the program being simulated (excluding trace storage overhead) by transferring the work of trace reconstruction to the post-processing phase. <p> The performance of programs containing large amounts of any one category of data has been shown to depend on the coherence protocol employed <ref> [32] </ref>. In the case of finely interleaved data, it is more efficient to update copies in other caches, and in the case of coarsely interleaved data, it is more efficient to invalidate the other copies. Thus, no single coherence protocol provides optimal performance.
Reference: [33] <institution> Encore Computer Corporation. Multimax Technical Summary, </institution> <month> January </month> <year> 1990. </year>
Reference-contexts: The April processor used in the Alewife architecture was designed to hide the latency of network communication by supporting fast context switching, coarse-grain multithreading, and full/empty bit synchronization. 9 Several commercial multiprocessors (Sequent Symmetry [55], DEC Firefly [76], Encore Multimax <ref> [33] </ref>) have been built with a shared-bus architecture. Since shared single-bus architectures do not scale well, several hierarchical-bus architectures have been proposed. Wilson proposed a cache-coherent multiprocessor architecture with hierarchical buses [50]. Caches are placed at every level, linking processor to bus and every level to every other level.
Reference: [34] <author> M. Federwisch and L. Ball. Mpsas: </author> <title> A Programmer and User Manual. Sun Microsystems, </title> <year> 1990. </year>
Reference-contexts: In the next section, we provide an overview of the execution-driven approach, and explain the details of our simulator. Section 3.2 provides an evaluation of our simulation approach using the Sequent Symmetry [55], as well as a comparison with a cycle-level simulator <ref> [34] </ref>. Section 3.3 concludes with an overview of the simulation technique and the results we have presented here. 24 3.1 The Execution-Driven Technique In execution-driven simulation, the execution of the program and the simulation of the architecture are interleaved. <p> It therefore incurs less overhead than the Mergesort program. 3.2.2 Single Bus Sparc Architecture We also compared the simulator to a cycle-level simulation of a single-bus-based architecture using SPARC processors [68]. The cycle-level simulator is one derived from the single-bus multiprocessor simulator called MPSAS developed by Sun Microsystems <ref> [34, 44] </ref>. A cycle-level simulator provides detailed information about the execution of a program, and hence should be fairly accurate. Our comparisons evaluate the loss in 44 accuracy when using execution-driven simulation instead of cycle-level simulation, as well as the gain in simulation speed.
Reference: [35] <author> S. H. Fuller, J. K. Ousterhout, L. Raskin, P. I. Rubinfeld, P. J. Sindhu, and R. J. Swan. Multi-Microprocessors: </author> <title> An Overview and Working Example. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 66(2) </volume> <pages> 216-228, </pages> <month> February </month> <year> 1978. </year>
Reference-contexts: synchronization primitives, coherency protocols, and consistency models that have been developed and proposed, and indicate how they relate to the work presented in this thesis. 2.1 Multiprocessor Architectures State of the Art One of the first multiprocessor designs to address the need for non-uniform memory access (NUMA) was the Cm* <ref> [35] </ref>. The Cm* used commercially available microprocessors in a general purpose, cluster, bus-based, shared-memory multiprocessor configuration. However, the Cm* did not use caches, resulting in excessive traffic since all memory references to remote data went on to the network.
Reference: [36] <author> D. D. Gajski, D. J. Kuck, D. H. Lawrie, and A. H. Sameh. </author> <title> CEDAR A Large Scale Multiprocessor. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages 524-529, </pages> <month> August </month> <year> 1984. </year>
Reference-contexts: However, the Cm* did not use caches, resulting in excessive traffic since all memory references to remote data went on to the network. Another example of a multiprocessor design that does not use caches is the BBN Butterfly [67], which uses a multistage interconnection network instead of buses. Cedar <ref> [36] </ref> is a cluster-based architecture that connects clusters of eight processors through a global shu*e-exchange-based interconnect to shared memory. Each processor has local memory.
Reference: [37] <author> K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> Performance Evaluation of Memory Consistency Models for Shared-Memory Multiprocessors. </title> <booktitle> In ASPLOS-IV, </booktitle> <pages> pages 245-257, </pages> <address> New York, </address> <year> 1991. </year>
Reference-contexts: Relaxed Consistency Models The use of write buffers is essential to hiding memory latency, especially in a hierarchical architecture. However, the buffering and pipelining of program writes leads to a relaxation of the consistency model provided to the user. Several consistency models have been presented in the literature <ref> [53, 40, 37, 29, 1] </ref>. They do not, however, use a uniform method of classification. We present a uniform taxonomy for consistency models that encompasses all existing models, as well as suggesting several variations. <p> However, this places severe restrictions on the hardware by not taking advantage of the available memory and network parallelism. Several relaxed consistency models that allow the buffering and pipelining of writes have been proposed <ref> [37, 40, 29] </ref>. We elaborate on these models in Chapter 6. 2.4 Cache and Shared-Memory Multiprocessor Evaluation Techniques Since an efficient cache is critical to system performance, various cache evaluation tools, including hardware measurement, trace-driven simulation, and analytical modeling, have been used. <p> Several consistency models have been proposed in the literature. These include sequential consistency [53], processor consistency [40], weak consistency [29, 1], and release consistency <ref> [37] </ref>. While each of these consistency models provide an accurate view of the access ordering on which the user can rely, the terminology used to present this view has not been consistent. <p> Memory accesses may be subdivided into classes. All memory accesses (A) may be subdivided into two groups: accesses to shared data, and accesses to private data (O). Shared accesses themselves may be further divided into subgroups as in <ref> [37] </ref>, each of 104 which may have different types of orderings preserved for them. Shared accesses may be classified into competing (C) and non-competing (O) accesses. Two accesses to the same memory location are considered conflicting if at least one of the accesses is a store. <p> I Release Processor A L ((L G (O G X G ) G ) G fl R G ) G fl A I Loose A L ((L G N G ) G fl R G ) G fl A I cede non-synchronization accesses, as is permitted in the definition in <ref> [37] </ref>. Weak sequential and weak processor consistency remove any ordering restrictions on non-synchronization accesses. Release sequential and release processor consistency exploit information about the type of synchronization accesses being made. We describe these models in the following subsections. We illustrate our notation using the example in Figure 6.8. <p> Release Consistency The release consistency model is a variant of weak consistency defined in <ref> [37] </ref> and in [1], which exploits the information about the partitioning of synchronization accesses into acquires and releases. An acquire is performed to gain access to a set of shared data addresses. A release grants access to some shared data.
Reference: [38] <author> A. Glew and W. Hwu. </author> <title> Snoopy Cache Test-and-Test-and-Set Without Excessive Bus Contention. Computer Architecture News, </title> <year> 1990. </year>
Reference-contexts: This scheme does not, however, guarantee fairness. If FIFO ordering is required, the ticket lock [59] used in conjunction with the conditional test&set will result in exactly 2N accesses on the bus as well, while guaranteeing fairness. Anderson [5] describes similar modifications, and Glew <ref> [38] </ref> demonstrates their theoretical benefits. Glew's scheme, however, requires that both the read and the write of the RMW instruction go on to the bus. Our scheme requires only a locked write on the bus.
Reference: [39] <author> J. R. Goodman. </author> <title> Using Cache Memory to Reduce Processor-Memory Traffic. </title> <booktitle> In Proceedings of the 11th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 124-131, </pages> <month> June </month> <year> 1983. </year>
Reference-contexts: In addition, every lock operation results in multiple bus accesses proportional to the number of synchronizing processes, that is, O (N 2 ) bus operations, where N is the number of synchronizing processes. The write-once protocol <ref> [39] </ref> improves on write-through by updating main memory only on the first write to a cache block. This causes all other copies to be invalidated so that further updates can be carried out locally. <p> Multiple writes to non-shared variables become more expensive, although the protocol does improve on write-once <ref> [39] </ref> for shared variables. This scheme, along with the Read-Broadcast scheme that they present, has the advantage that only one bus operation is required to update all invalid copies of a cached block. This allows an efficient implementation of the test&test&set operation.
Reference: [40] <author> J. R. Goodman. </author> <title> Cache Consistency and Sequential Consistency. </title> <type> Technical Report #1006, </type> <institution> University of Wisconsin-Madison, </institution> <month> February </month> <year> 1991. </year>
Reference-contexts: Relaxed Consistency Models The use of write buffers is essential to hiding memory latency, especially in a hierarchical architecture. However, the buffering and pipelining of program writes leads to a relaxation of the consistency model provided to the user. Several consistency models have been presented in the literature <ref> [53, 40, 37, 29, 1] </ref>. They do not, however, use a uniform method of classification. We present a uniform taxonomy for consistency models that encompasses all existing models, as well as suggesting several variations. <p> However, this places severe restrictions on the hardware by not taking advantage of the available memory and network parallelism. Several relaxed consistency models that allow the buffering and pipelining of writes have been proposed <ref> [37, 40, 29] </ref>. We elaborate on these models in Chapter 6. 2.4 Cache and Shared-Memory Multiprocessor Evaluation Techniques Since an efficient cache is critical to system performance, various cache evaluation tools, including hardware measurement, trace-driven simulation, and analytical modeling, have been used. <p> Several consistency models have been proposed in the literature. These include sequential consistency [53], processor consistency <ref> [40] </ref>, weak consistency [29, 1], and release consistency [37]. While each of these consistency models provide an accurate view of the access ordering on which the user can rely, the terminology used to present this view has not been consistent. <p> We begin by informally classifying and describing several consistency models (we will shortly define them more precisely). Figure 6.7 depicts several combinations of order preservations. Sequential consistency is defined as in [53]. Processor sequential consistency is a variant of processor consistency <ref> [40] </ref> that imposes sequential ordering on synchronization accesses. Processor processor consistency, the variant first proposed by Goodman [40], removes this restriction. <p> Figure 6.7 depicts several combinations of order preservations. Sequential consistency is defined as in [53]. Processor sequential consistency is a variant of processor consistency <ref> [40] </ref> that imposes sequential ordering on synchronization accesses. Processor processor consistency, the variant first proposed by Goodman [40], removes this restriction. <p> Processor Consistency Goodman defines processor consistency as follows: "A multiprocessor is said to be processor consistent if the result of any execution is the same as if the operations of each individual processor appear in the sequential order specified by its program." <ref> [40] </ref> We reclassify processor consistency into two subclasses, processor sequential con sistency and processor processor consistency (the model proposed by Goodman).
Reference: [41] <author> J. R. Goodman, M. K. Vernon, and P. J. Woest. </author> <title> Efficient Synchronization Primitives for Large-Scale Cache-Coherent Multiprocessors. </title> <booktitle> In Proceedings of the Third International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 64-75. </pages> <publisher> ACM, </publisher> <month> April </month> <year> 1989. </year> <month> 138 </month>
Reference-contexts: The scalability of the hardware fetch& operation also depends on combining in the global interconnect, which has so far proven to be expensive (hardware combining increases the complexity of the interconnect in proportion to the number of processors in the system). Goodman, Vernon, and Woest <ref> [41] </ref> also argue that hardware fetch& is of little use if combining is implemented in software. The University of Illinois Cedar machine [83] provides a general atomic instruction that operates on synchronization variables, which consist of two words: a key, and a value. <p> An extra bus access is still incurred in order to put the cache line in the processor holding the lock in the lock-waiter state. Extra hardware in the form of a busy-wait register is also required for efficiency reasons. Goodman, Vernon, and Woest <ref> [41] </ref> propose a set of efficient primitives for process synchronization in multiprocessors. This is similar to the idea by Bitar and Despain [14], extended to the Multicube architecture (multidimensional grid of buses [42]). <p> Anderson's lock is similar to the ticket lock, except for avoiding contention for a single polled variable. Anderson's scheme, however, requires space proportional to the number of processors. The MCS lock is similar to the QOSB (Queue On Synch Bit) hardware primitive implemented in <ref> [41] </ref>, but is implemented in software. MCS locks guarantee fairness and spin only in local memory/cache. MCS locks, however, also require space proportional to the number of processors. Both the Anderson and MCS locks increase the latency in the one processor case.
Reference: [42] <author> J. R. Goodman and P. J. Woest. </author> <title> The Wisconsin Multicube: A New Large-Scale Cache-Coherent Multiprocessor. </title> <booktitle> In Proceedings of the 15th International Symposium on Computer Architecture, </booktitle> <pages> pages 422-431. </pages> <publisher> IEEE, </publisher> <year> 1988. </year>
Reference-contexts: The coherence protocol is also made more complex since the last copy of a data item must be migrated, rather than replaced. The Wisconsin Multicube <ref> [42] </ref> is a proposed cache-coherent, shared-memory multiprocessor intended to scale to over a thousand processors. Its topology is that of a grid of buses, with processing elements and caches at every intersection in the grid. <p> Goodman, Vernon, and Woest [41] propose a set of efficient primitives for process synchronization in multiprocessors. This is similar to the idea by Bitar and Despain [14], extended to the Multicube architecture (multidimensional grid of buses <ref> [42] </ref>). The protocol assumes that the interconnect supports broadcast, and that hardware combining is not implemented in the interconnect. The primitives make use of synchronization bits (syncbits, associated with each line in shared memory) to provide a simple mechanism for mutual exclusion. <p> The traditional problem with shared memory machines is that contention for shared resources becomes a limiting bottleneck beyond a few tens of processors. Recent research efforts have investigated the feasibility of providing efficient large-scale shared memory multiprocessors <ref> [54, 3, 20, 42] </ref>. This work augments these efforts with a view to studying the advantages of exploiting the snooping ability of bus-based architectures. Preliminary results contributing to the design of the Willow multiprocessor are presented in [12].
Reference: [43] <author> A. Gottlieb, R. Grishman, C. P. Kruskal, K. P. McAuliffe, L. Rudolph, and M. Snir. </author> <title> The NYU Ultracomputer Designing a MIMD, Shared Memory Parallel Machine. </title> <journal> IEEE Transactions on Computers, </journal> <pages> pages 175-189, </pages> <month> Feb </month> <year> 1983. </year>
Reference-contexts: Read-modify-write (RMW) primitives are provided by most existing processors. The most common set of primitives consists of test&set and reset. The microcode or software will usually repeat the test&set until the returned value is zero. Other RMW primitives include compare&swap [83], fetch&add <ref> [43] </ref>, and fetch& [63] operations, where is a function such as add, or, store, increment, or and. Typical implementations of these basic synchronization primitives cause a performance bottleneck in the presence of contention. <p> The 80386 also has the ability to lock the bus during the read and write operations of any instruction, a feature that can be exploited in the implementation of barriers. To overcome the O (N 2 ) complexity bottleneck, the NYU Ultracomputer <ref> [43] </ref> provides hardware combining for the fetch&add operation. When a single processor executes a fetch&add, the old value of the variable is returned, and the contents of the memory location is incremented by the amount specified.
Reference: [44] <author> J. Greenwood. </author> <title> The Design of a Scalable Hierarchical-Bus, Shared-Memory Multiprocessor. </title> <type> Master's thesis, </type> <institution> Rice University, Houston TX, </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: It therefore incurs less overhead than the Mergesort program. 3.2.2 Single Bus Sparc Architecture We also compared the simulator to a cycle-level simulation of a single-bus-based architecture using SPARC processors [68]. The cycle-level simulator is one derived from the single-bus multiprocessor simulator called MPSAS developed by Sun Microsystems <ref> [34, 44] </ref>. A cycle-level simulator provides detailed information about the execution of a program, and hence should be fairly accurate. Our comparisons evaluate the loss in 44 accuracy when using execution-driven simulation instead of cycle-level simulation, as well as the gain in simulation speed.
Reference: [45] <author> E. Hagersten, S. Haridi, and D. H. D. Warren. </author> <title> Cache and Interconnect Architectures in Multiprocessors, chapter The Cache Coherence Protocol of the Data Diffusion Machine. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1990. </year>
Reference-contexts: Each processor has local memory. Cedar uses a combination of high speed shared memory, a macro data-flow model of computation, and compiler technology to achieve high performance. 8 The Data Diffusion Machine (DDM) <ref> [45] </ref> and Kendall Square Research's KSR1 [48] are both cache-only memory architectures. They use distributed main memory and directory-based cache coherence. One disadvantage of this approach is that allowing migration of data at the memory level requires a mechanism to locate the data on a miss [74].
Reference: [46] <author> D. Hensgen, R. Finkel, and U. Manber. </author> <title> Two Algorithms for Barrier Synchronization. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 17(1) </volume> <pages> 1-17, </pages> <year> 1988. </year>
Reference-contexts: Its performance is better than that of the combining tree barrier on a hierarchical bus architecture because of the reduction in the number of reads and writes performed. The MCS barrier does worse than the tournament barrier <ref> [46, 56] </ref> when the hierarchy contains more than two levels, because of sharing across clusters that cannot be avoided (see Figure 5.10). When the hierarchy contains less than two levels, the smaller execution path of the MCS algorithm results in better performance. <p> The tournament barrier is the most efficient on a hierarchical bus architecture for a large number of processors since it reduces the number of (local and remote) bus accesses, while taking advantage of the parallelism in the network. The dissemination barrier <ref> [46] </ref> consists of log 2 P pairwise synchronizations per processor, resulting in O (P log 2 P ) remote operations. This results in excessive operations across levels of the hierarchy, and therefore poor performance. barrier mechanisms. The configuration of the hierarchy is the same as before.
Reference: [47] <institution> IEEE Computer Society. Futurebus+ P896.1, </institution> <month> February </month> <year> 1990. </year>
Reference-contexts: A separate lock line on the bus is necessary to indicate that a test&set or a reset operation is taking place. This already exists in many bus specifications (an example is the Future Bus+ <ref> [47] </ref>). 2. A shared line is required on the bus in order to indicate to the cache initiating the transaction that there are other caches with copies of the same location. 3. Hardware is required to implement a conditional test&set operation. <p> These values and design decisions represent realistic choices in 1992 technology. 66 For example, Cypress Semiconductor's CY7C605 [68] makes provision for the same cache size and organization. The FutureBus+ <ref> [47] </ref> specifications make provisions for up to a 256-bit wide data bus. We used a cluster size of four; in other words, there were four processors / nodes per cluster.
Reference: [48] <author> H. Burkhardt III, S. Frank, B. Knobe, and J. Rothnie. </author> <title> Overview of the KSR1 Computer System. </title> <type> Technical Report KSR-TR-9202001, </type> <institution> Kendall Square Research, </institution> <month> February </month> <year> 1992. </year>
Reference-contexts: Each processor has local memory. Cedar uses a combination of high speed shared memory, a macro data-flow model of computation, and compiler technology to achieve high performance. 8 The Data Diffusion Machine (DDM) [45] and Kendall Square Research's KSR1 <ref> [48] </ref> are both cache-only memory architectures. They use distributed main memory and directory-based cache coherence. One disadvantage of this approach is that allowing migration of data at the memory level requires a mechanism to locate the data on a miss [74].
Reference: [49] <author> H. F. Jordan. </author> <title> Performance Measurements on HEP a Pipelined MIMD Computer. </title> <booktitle> In Proceedings of the 10th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 207-212, </pages> <month> June </month> <year> 1983. </year>
Reference-contexts: The synchronization instruction consists of an evaluation of a condition, followed by an operation on the key and the value. A special processor is needed at each memory module in order to implement the generalized synchronization mechanism. The HEP (developed by Denelcor <ref> [49] </ref>) associates a Full/Empty bit with every memory location, as does the April processor [3]. The bit is tested before a read or 12 write operation if a special symbol is prepended to the variable name.
Reference: [50] <author> A. W. Wilson Jr. </author> <title> Hierarchical Cache/Bus Architecture for Shared Memory Multiprocessors. </title> <booktitle> In Proceedings of the 14th International Symposium on Computer Architecture, </booktitle> <pages> pages 244-252. </pages> <publisher> IEEE, </publisher> <year> 1987. </year>
Reference-contexts: Since shared single-bus architectures do not scale well, several hierarchical-bus architectures have been proposed. Wilson proposed a cache-coherent multiprocessor architecture with hierarchical buses <ref> [50] </ref>. Caches are placed at every level, linking processor to bus and every level to every other level. Wilson's design does not provide for memory at each level, which introduces a bottleneck if the caches are not large enough to hold all the program's accessed addresses. <p> In order to make the architecture scalable, we propose to use a hierarchy of caches and shared buses to interconnect multiple computer clusters. The private cache/ shared bus approach is recursively applied to additional levels of caching, as proposed by Wilson <ref> [50] </ref>. This produces a tree structure, as shown in Figure 4.1, with the links to the lower level branches provided by the higher level caches. In addition, we also have memory at every level.
Reference: [51] <author> J. R. </author> <title> Jump. YACSIM Reference Manual. </title> <institution> Rice University, </institution> <year> 1992. </year> <month> 139 </month>
Reference-contexts: A 31 PRESTO interface has been provided to the user by porting PRESTO on top of Concurrent C [61]. * YACSIM (Yet Another C SIMulation Package, originally called CSIM): This is a process-level, discrete-event simulator providing a set of procedures for process creation, delaying, event queue manipulation, and statistics collection <ref> [24, 51] </ref>.
Reference: [52] <author> R. H. Katz, S. J. Eggers, D. A. Wood, C. L. Perkins, and R. G. Sheldon. </author> <title> Implementing a Cache Consistency Protocol. </title> <booktitle> In Proceedings of the 12th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 276-283. </pages> <publisher> IEEE, </publisher> <year> 1985. </year>
Reference-contexts: Its potential disadvantage is that it incurs an initial write to memory even when the memory block is not being shared, thereby degrading performance. Synchronization performance is not improved by this protocol. Katz et al. <ref> [52] </ref> present an ownership-based multiprocessor cache consistency protocol, designed for implementation as a single chip VLSI cache controller. A 14 processor must "own" a block before it is allowed to update it. Ownership is acquired through special read and write operations. <p> The coherence protocol is a write back with invalidation of other copies when the data is shared, similar to the Berkeley protocol <ref> [52] </ref>. An additional feature of the Sequent coherence protocol is that when a cache that has the dirty copy of a cache line supplies the data, it also invalidates its own copy irrespective of whether the request was for a read or a write access.
Reference: [53] <author> L. Lamport. </author> <title> How to Make a Multiprocessor Computer That Correctly Executes Multiprocess Programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> c-28(9):690-691, </volume> <month> September </month> <year> 1979. </year>
Reference-contexts: Relaxed Consistency Models The use of write buffers is essential to hiding memory latency, especially in a hierarchical architecture. However, the buffering and pipelining of program writes leads to a relaxation of the consistency model provided to the user. Several consistency models have been presented in the literature <ref> [53, 40, 37, 29, 1] </ref>. They do not, however, use a uniform method of classification. We present a uniform taxonomy for consistency models that encompasses all existing models, as well as suggesting several variations. <p> Its distinguishing characteristics include the use of software-based release consistency and a multiple-protocol 16 adaptive cache coherence mechanism that exploits the access patterns of shared data objects. We endeavor to take similar advantage of data access behavior to develop a hardware-based coherence protocol for our shared-memory architecture. Sequential consistency <ref> [53] </ref> has been traditionally viewed as the consistency model expected by and provided to the user. However, this places severe restrictions on the hardware by not taking advantage of the available memory and network parallelism. <p> Several consistency models have been proposed in the literature. These include sequential consistency <ref> [53] </ref>, processor consistency [40], weak consistency [29, 1], and release consistency [37]. While each of these consistency models provide an accurate view of the access ordering on which the user can rely, the terminology used to present this view has not been consistent. <p> We begin by informally classifying and describing several consistency models (we will shortly define them more precisely). Figure 6.7 depicts several combinations of order preservations. Sequential consistency is defined as in <ref> [53] </ref>. Processor sequential consistency is a variant of processor consistency [40] that imposes sequential ordering on synchronization accesses. Processor processor consistency, the variant first proposed by Goodman [40], removes this restriction. <p> sequential consistency as follows: "A system is sequentially consistent if the result of any execution is the same as if the operations of all the processors were executed in some sequential order, and the operations of each individual processor appear in this sequence in the order specified by its program." <ref> [53] </ref> Each processor issues requests in the order specified by the program. All processors must see the same ordering of all processors' writes, for the variables accessed by each processor. Sequential consistency is the strictest form of consistency to which an architecture can adhere.
Reference: [54] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. S. Lam. </author> <title> The Stanford Dash Multiprocessor. </title> <booktitle> Computer, </booktitle> <pages> pages 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Experience has shown that placing 32 high-performance processors on a single bus taxes the architecture heavily, resulting in potentially poor performance for many algorithms. In addition, the Multicube synchronization scheme requires extensive hardware support for an efficient implementation. The Stanford DASH multiprocessor <ref> [54] </ref> is another NUMA machine that distributes memory among the processing nodes. It uses distributed directory-based cache coherence and maintains cache coherence by sending point-to-point messages between the nodes on an interconnection network. Each node is a bus-based cluster of four processors, thereby utilizing the bus's snooping capability. <p> The traditional problem with shared memory machines is that contention for shared resources becomes a limiting bottleneck beyond a few tens of processors. Recent research efforts have investigated the feasibility of providing efficient large-scale shared memory multiprocessors <ref> [54, 3, 20, 42] </ref>. This work augments these efforts with a view to studying the advantages of exploiting the snooping ability of bus-based architectures. Preliminary results contributing to the design of the Willow multiprocessor are presented in [12]. <p> As indicated in [70], this is because of the large number of invalidation misses caused by the random nature of the sharing of the space array. The performance of our architecture is slightly better than that reported for the same program on the Dash multiprocessor <ref> [54] </ref> when no buffers are used at the processor level in the architecture. With the use of buffers at the processor level, our simulations show a performance almost double that of the Dash multiprocessor.
Reference: [55] <author> T. Lovett and S. Thakkar. </author> <title> The Symmetry Multiprocessor System. </title> <booktitle> In Proceedings of the 1988 International Conference on Parallel Processing, </booktitle> <pages> pages 303-310, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: The April processor used in the Alewife architecture was designed to hide the latency of network communication by supporting fast context switching, coarse-grain multithreading, and full/empty bit synchronization. 9 Several commercial multiprocessors (Sequent Symmetry <ref> [55] </ref>, DEC Firefly [76], Encore Multimax [33]) have been built with a shared-bus architecture. Since shared single-bus architectures do not scale well, several hierarchical-bus architectures have been proposed. Wilson proposed a cache-coherent multiprocessor architecture with hierarchical buses [50]. <p> If N processes attempt to access a critical section at the same time, the memory system executes N RMW operations consecutively, even if only one is successful. The Intel 80386 used by the Sequent Symmetry multiprocessor <ref> [55] </ref> provides an "exchange byte" (XCHB) instruction that can be utilized for both test&set and fetch&store operations. This requires hardware support for locking a cache line during the RMW instruction. <p> Our results show that we have developed an efficient simulator with an accuracy of about 5% for the Sequent Symmetry <ref> [55] </ref>. The errors are mostly due to the variable execution times of each instruction in an architecture with a complex instruction set that our simulator does not attempt to predict. <p> In the next section, we provide an overview of the execution-driven approach, and explain the details of our simulator. Section 3.2 provides an evaluation of our simulation approach using the Sequent Symmetry <ref> [55] </ref>, as well as a comparison with a cycle-level simulator [34]. Section 3.3 concludes with an overview of the simulation technique and the results we have presented here. 24 3.1 The Execution-Driven Technique In execution-driven simulation, the execution of the program and the simulation of the architecture are interleaved. <p> In both simulation studies, the performance of I/O was not evaluated. Also, the effects on performance of virtual to physical memory mapping and translation were not modeled. These effects were small for the programs that we simulated. 36 3.2.1 The Sequent Symmetry The Sequent Symmetry <ref> [55] </ref> is a single bus, shared-memory multiprocessor with a private cache at each processor. The coherence protocol is a write back with invalidation of other copies when the data is shared, similar to the Berkeley protocol [52]. <p> loop is commonly used, and is as follows: int lock = 0; acquire_lock (lock) int *lock; - while (test&set (lock) == 1); - release_lock (lock) int *lock; - *lock = 0; - This type of lock is inefficient when used with a protocol such as that of the Sequent Symmetry <ref> [55] </ref> or the Illinois design [62] in that each waiting processor generates bus accesses as fast as it can to a common memory location.
Reference: [56] <author> B. Lubachevsky. </author> <title> Synchronization Barrier and Related Tools for Shared-Memory Parallel Programming. </title> <booktitle> In Proceedings of the 1989 International Conference on Paralle l Processing, </booktitle> <pages> pages II-175-II-179, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: Its performance is better than that of the combining tree barrier on a hierarchical bus architecture because of the reduction in the number of reads and writes performed. The MCS barrier does worse than the tournament barrier <ref> [46, 56] </ref> when the hierarchy contains more than two levels, because of sharing across clusters that cannot be avoided (see Figure 5.10). When the hierarchy contains less than two levels, the smaller execution path of the MCS algorithm results in better performance.
Reference: [57] <author> S. Madala. </author> <title> Concurrent C User's Manual. </title> <type> Technical Report TR 8701, </type> <institution> Dept. of Electrical and Computer Engineering, Rice University, Houston, TX, </institution> <month> January </month> <year> 1987. </year>
Reference-contexts: The basic components of the RPPT software system are as follows: * Concurrent C: This is the parallel C programming model provided to the user <ref> [57] </ref>. We extended the primitives that were provided for distributed memory pro grams to include those necessary for shared-memory programs. * PRESTO: This is the C++ programming model provided to the user.
Reference: [58] <author> E. McCreight. </author> <title> The Dragon Computer System: An early overview. </title> <type> Technical report, </type> <institution> Xerox Corp., </institution> <month> Sept </month> <year> 1984. </year>
Reference-contexts: Main memory is also updated along with other caches on a write. Again, with these protocols, the number of bus accesses for N synchronizing processes is O (N 2 ). The Firefly protocol [76] and the Dragon protocol <ref> [58] </ref> use a write-broadcast mechanism to update all copies of shared blocks. They dynamically determine whether blocks are shared, but do not check for inactive blocks in a cache. The Dragon protocol differs from the Firefly protocol in that main memory is not updated along with other cached copies.
Reference: [59] <author> J. M. Mellor-Crummey and M. L. Scott. </author> <title> Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(1) </volume> <pages> 21-65, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: Processor 2 sees the successful bus operation performed by processor 1, withdraws its bus request and spins locally in its cache waiting for the lock to be released. This scheme does not, however, guarantee fairness. If FIFO ordering is required, the ticket lock <ref> [59] </ref> used in conjunction with the conditional test&set will result in exactly 2N accesses on the bus as well, while guaranteeing fairness. Anderson [5] describes similar modifications, and Glew [38] demonstrates their theoretical benefits. <p> These include the ticket lock, an array-based queuing lock [5], and a list-based queuing lock (MCS <ref> [59] </ref>). The ticket lock, while guaranteeing fairness, requires a fetch&increment operation. Anderson's lock is similar to the ticket lock, except for avoiding contention for a single polled variable. Anderson's scheme, however, requires space proportional to the number of processors. <p> The time taken using the conditional test&set is fairly constant, increasing slightly as the number of layers is increased 83 in the hierarchical case. We also compared the performance of the conditional test&set with the software MCS list-based scheme developed in <ref> [59] </ref> (mcs.ttset.wb and mcs.ctset). The peak in the cost of the MCS lock in the two processor case reflects the lack of a compare-and-swap instruction, which the MCS lock requires for best performance [59]. Using the conditional test&set improves the performance of the MCS lock as well. <p> We also compared the performance of the conditional test&set with the software MCS list-based scheme developed in <ref> [59] </ref> (mcs.ttset.wb and mcs.ctset). The peak in the cost of the MCS lock in the two processor case reflects the lack of a compare-and-swap instruction, which the MCS lock requires for best performance [59]. Using the conditional test&set improves the performance of the MCS lock as well. Figure 5.4 shows the bus utilizations of each of the schemes. The conditional test&set can be seen to exhibit the lowest bus utilization. of the FFT program run on 16,384 points. <p> The above implementation would then result in P+1 bus operations. The critical path is, 87 however, proportional to P, even on an architecture not based on a single bus. This proportionality can be reduced by using a software tree-based algorithm as proposed in <ref> [59] </ref>. Here, the barrier is parallelized by distribution, removing the necessity for serial access to the barrier. <p> The combining tree barrier performs poorly on a distributed memory non-cache-coherent architecture because of the necessity to spin on remote locations. The MCS tree barrier <ref> [59] </ref> is efficient on a NUMA non-cache-coherent machine and somewhat less efficient on a single bus architecture. Its performance is better than that of the combining tree barrier on a hierarchical bus architecture because of the reduction in the number of reads and writes performed.
Reference: [60] <author> C. L. Mitchell and M. J. Flynn. </author> <title> A Workbench for Computer Architects. </title> <booktitle> IEEE Design and Test of Computers, </booktitle> <pages> pages 19-29, </pages> <month> February </month> <year> 1988. </year> <month> 140 </month>
Reference-contexts: We have implemented this technique as part of the Rice Parallel Processing Testbed (RPPT) [23], a simulation tool for the performance evaluation of parallel computer systems. A similar approach to cache simulation is used in <ref> [60] </ref>, but only instruction addresses are traced. The TRAPEDS [75] approach is limited to non-multiprogrammed multicomputer address trace analysis, and does not handle shared-memory systems. TRAPEDS also does not model communication accurately, since it does not evaluate the effects of contention.
Reference: [61] <author> R. Mukherjee. </author> <title> Efficient Simulation of Shared Memory Parallel Systems. </title> <type> Master's thesis, </type> <institution> Rice University, Houston TX, </institution> <month> May </month> <year> 1990. </year>
Reference-contexts: PRESTO, written in C++, is an object-oriented programming environment designed for shared-memory systems and developed at the University of Washington [13]. A 31 PRESTO interface has been provided to the user by porting PRESTO on top of Concurrent C <ref> [61] </ref>. * YACSIM (Yet Another C SIMulation Package, originally called CSIM): This is a process-level, discrete-event simulator providing a set of procedures for process creation, delaying, event queue manipulation, and statistics collection [24, 51].
Reference: [62] <author> M. Papamarcos and J. Patel. </author> <title> A Low Overhead Coherence Solution for Multiprocessors with Private Cache Memories. </title> <booktitle> In Proceedings of the 11th International Symposium on Computer Architecture, </booktitle> <pages> pages 340-347. </pages> <publisher> IEEE, </publisher> <year> 1984. </year>
Reference-contexts: A 14 processor must "own" a block before it is allowed to update it. Ownership is acquired through special read and write operations. The block is invalidated in all other caches before being written to. The Illinois protocol <ref> [62] </ref> enhances this protocol with a shared bus line that is used to dynamically determine if a block is shared. It thereby reduces the number of invalidate bus operations generated. An additional valid-exclusive state is used to avoid invalidation signals if the cache has the only copy of the data. <p> Thus, data can be 53 dynamically treated in a write-invalidate or write-update manner depending on its usage. The manner in which data is treated is software controllable. The protocol has four states invalid, exclusive, shared, and modified. The basic protocol is similar to that developed at Illinois <ref> [62] </ref>, with extensions for supporting efficient locking, as well as an update protocol. When lock data is modified and is present in more than one cache (shared state), it is updated in all the caches as well as main memory (similar to the Firefly protocol [76]). <p> is as follows: int lock = 0; acquire_lock (lock) int *lock; - while (test&set (lock) == 1); - release_lock (lock) int *lock; - *lock = 0; - This type of lock is inefficient when used with a protocol such as that of the Sequent Symmetry [55] or the Illinois design <ref> [62] </ref> in that each waiting processor generates bus accesses as fast as it can to a common memory location.
Reference: [63] <author> G. F. Pfister and V. A. Norton. </author> <title> Hot Spot Contention and Combining in Multistage Interconnection Networks. </title> <booktitle> In Proceedings of the 1985 International Conference on Parallel Processing, </booktitle> <pages> pages 790-797. </pages> <publisher> IEEE, </publisher> <year> 1985. </year>
Reference-contexts: Read-modify-write (RMW) primitives are provided by most existing processors. The most common set of primitives consists of test&set and reset. The microcode or software will usually repeat the test&set until the returned value is zero. Other RMW primitives include compare&swap [83], fetch&add [43], and fetch& <ref> [63] </ref> operations, where is a function such as add, or, store, increment, or and. Typical implementations of these basic synchronization primitives cause a performance bottleneck in the presence of contention. <p> The implementation is such that N processors can all access the same memory word in unit time. This operation can be used for synchronizing multiple readers and writers, and for managing highly parallel queues. The IBM RP3 <ref> [63] </ref> provides generalized fetch& operations, where is an operation such as add, or, store, or and. The execution of these primitives is distributed in the interconnection network between the processors and the memory module and relies on hardware combining in the interconnection network, thereby greatly complicating the design.
Reference: [64] <author> W. H. Press, B. P. Flannery, S. A. Teukolsky, and W. T. Vetterling. </author> <title> Numerical Recipes in C. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1988. </year>
Reference-contexts: Fast Fourier Transform (FFT) The FFT program is based on the Cooley Tukey Radix 2 Decimation in Time algorithm [16]. A general description of the algorithm can be found in <ref> [64] </ref>. The FFT program is similar to the sorting algorithms in that for an N-point FFT, the first log 2 N log 2 P (where P is the number of processors) are performed independently on each processor. The last log 2 P stages require synchronization at every stage. <p> Hence, this program achieves high speedups. The program partitions data by blocks of rows. 4. Gaussian Elimination This is a method of solving a system of linear equations (see <ref> [64] </ref> for a description of the algorithm). The task is partitioned at compile time. The processors actively share one row of data at every iteration, and must synchronize at every iteration. <p> Each processor works on every pth row or column (depending on how work is partitioned), where p is the number of processors. 5. Successive Over-Relaxation (SOR) This is an iterative method of solving partial differential equations (see <ref> [64] </ref> for a description of the algorithm). Each computation depends only on its nearest neighbors. Hence, active sharing is limited to the boundary elements. The processors must synchronize at every iteration. The program partitions data by blocks of rows. 6.
Reference: [65] <author> T. R. Puzak. </author> <title> Analysis of Cache Replacement Algorithms. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Massachusetts, </institution> <year> 1985. </year>
Reference-contexts: This technique cannot be used to obtain quick estimates or bounds on cache performance for a wide range of programs and cache organizations, since the simulations are costly both in time and space, and must be repeated for each cache organization. Puzak <ref> [65] </ref> presents a method called trace stripping that reduces time and space requirements of the simulation. The generation and availability of the address traces used in these simulation techniques are a problem.
Reference: [66] <author> M. J. Quinn. </author> <title> Designing Efficient ALgorithms for Parallel Computers. </title> <publisher> McGraw-Hill, </publisher> <year> 1987. </year>
Reference-contexts: We have chosen the following algorithms from both the numerical and non-numerical domains for study. 1. Sorting These are prescheduled partitioned algorithms <ref> [66] </ref>. Work is partitioned at compile time. We used two programs, both of which use the basic Mergesort technique - Bimerge [79] and Multi-Way Merge [80]. Both algorithms initially sort sublists independently on each processor. Bimerge then proceeds to merge the sorted sublists two at a time.
Reference: [67] <author> R. Retberg and R. Thomas. </author> <title> Contention is No Obstacle to Shared Memory Multiprocessing. </title> <journal> Communications of the ACM, </journal> <volume> 29(12), </volume> <month> December </month> <year> 1986. </year>
Reference-contexts: However, the Cm* did not use caches, resulting in excessive traffic since all memory references to remote data went on to the network. Another example of a multiprocessor design that does not use caches is the BBN Butterfly <ref> [67] </ref>, which uses a multistage interconnection network instead of buses. Cedar [36] is a cluster-based architecture that connects clusters of eight processors through a global shu*e-exchange-based interconnect to shared memory. Each processor has local memory.
Reference: [68] <author> ROSS Technology, Inc. </author> <title> SPARC RISC User's Guide, </title> <note> second edition, Feburary 1990. </note>
Reference-contexts: It therefore incurs less overhead than the Mergesort program. 3.2.2 Single Bus Sparc Architecture We also compared the simulator to a cycle-level simulation of a single-bus-based architecture using SPARC processors <ref> [68] </ref>. The cycle-level simulator is one derived from the single-bus multiprocessor simulator called MPSAS developed by Sun Microsystems [34, 44]. A cycle-level simulator provides detailed information about the execution of a program, and hence should be fairly accurate. <p> The processor level caches were 64 Kbytes in size and the ratio of the cache sizes from one level to the next in the hierarchical bus architecture was 1:4. These values and design decisions represent realistic choices in 1992 technology. 66 For example, Cypress Semiconductor's CY7C605 <ref> [68] </ref> makes provision for the same cache size and organization. The FutureBus+ [47] specifications make provisions for up to a 256-bit wide data bus. We used a cluster size of four; in other words, there were four processors / nodes per cluster.
Reference: [69] <author> L. Rudolph and Z. Segall. </author> <title> Dynamic Decentralized Cache Schemes for MIMD Parallel Processors. </title> <booktitle> In Proceedings of the 11th International Symposium on Computer Architecture, </booktitle> <pages> pages 340-347. </pages> <publisher> IEEE, </publisher> <year> 1984. </year>
Reference-contexts: A class of protocols based on write broadcast (<ref> [69] </ref> and [7]) provide dynamic type classification of the datum cached (i.e., read-only, local, or shared). Rudolph and Segall [69] use a mixed broadcast/ write-first protocol, which dynamically determines whether a block is written by more than one processor by updating all other cached copies on the first write by a processor. <p> If an update protocol is used, the above code will also degrade the performance of all processors with a copy of the location in their caches. To avoid the excessive overhead involved in bombarding the network/memory with writes, a simple modification has been proposed <ref> [69] </ref>.
Reference: [70] <author> J. P. Singh, W. Weber, and A. Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory. </title> <type> Technical report, </type> <institution> Stanford University, </institution> <year> 1991. </year>
Reference-contexts: Each computation depends only on its nearest neighbors. Hence, active sharing is limited to the boundary elements. The processors must synchronize at every iteration. The program partitions data by blocks of rows. 6. MP3D This is a 3-dimensional particle simulator, which is part of the SPLASH benchmark suite <ref> [70] </ref>. The program divides the space into cells and keeps track of each individual particle as it moves within this space. When two or more particles enter the same cell in the same time step, the particles interact in some random manner. 65 7. <p> The work was divided by row in this program. The data size used was a 128*128 matrix. MP3D (Figure 4.10) is a program that performs relatively poorly even with a hierarchical architecture. As indicated in <ref> [70] </ref>, this is because of the large number of invalidation misses caused by the random nature of the sharing of the space array.
Reference: [71] <author> A. J. Smith. </author> <title> Cache Memories. </title> <journal> Computing Surveys, </journal> <volume> 14(3), </volume> <month> September </month> <year> 1982. </year> <month> 141 </month>
Reference-contexts: Since it is difficult to map a characteristic workload to any given distribution, the accuracy of this technique is usually low. Trace-Driven Simulation The most common type of cache simulation is trace-driven simulation, which uses collected address traces to drive the simulation. In his survey paper <ref> [71] </ref>, Smith presents results from an extensive study using this technique. Trace-driven simulation provides an accurate estimate of cache performance on measured benchmarks.
Reference: [72] <author> A. J. Smith. </author> <title> Line (Block) Size Choice for CPU Cache Memories. </title> <journal> IEEE Transactions On Computers, </journal> <volume> C-36(9), </volume> <month> September </month> <year> 1987. </year>
Reference-contexts: A comprehensive set of hardware measurements is presented by Clark [21]. 17 2.4.2 Simulation Since accurately modeling the characteristics of program behavior that determine the performance of caches has been found to be analytically intractable, simulation has been the prevailing technique used to yield accurate predictions <ref> [72] </ref>. Various methods of simulation in current use are described below. Distribution-Driven Simulation In this method, a probability distribution is used to characterize the address trace that drives the simulation. <p> They also provide insight into the dependence of the cache miss rate on program and workload parameters. Analytical models, however, lack the accuracy of either simulation or hardware measurement. Some of the analytical models that have been proposed are by Smith <ref> [72] </ref>, Thiebaut and Stone ([77] and [78]), and Agarwal [2]. Smith fits a continuous curve to the simulation results that he obtained for different line sizes, and derives an empirical model.
Reference: [73] <author> P. Stenstrom. </author> <title> A Cache Consistency Protocol for Multiprocessors with Multistage Networks. </title> <booktitle> In Proceedings of the 16th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 407-415, </pages> <year> 1989. </year>
Reference-contexts: Archibald extends the protocol to cluster-based hierarchical multiprocessor organizations, introducing cluster ownership for writing. Copies in other clusters are invalidated on a write in any cluster, resulting in poor performance at least with regard to the specific case of locks. Per Stenstrom <ref> [73] </ref> describes a protocol for multiprocessors with multistage interconnection networks. Consistency traffic is restricted to the set of caches that have a copy of a shared block. The state information is distributed to the caches, and the memory modules need not be consulted for consistency actions.
Reference: [74] <author> P. Stenstrom, T. Joe, and A. Gupta. </author> <title> Comparative Performance Evaluation of Cache-Coherent NUMA and COMA Architectures. </title> <booktitle> In Proceedings of the 1992 International Symposium of Computer Architecture, </booktitle> <pages> pages 80-91, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: They use distributed main memory and directory-based cache coherence. One disadvantage of this approach is that allowing migration of data at the memory level requires a mechanism to locate the data on a miss <ref> [74] </ref>. The coherence protocol is also made more complex since the last copy of a data item must be migrated, rather than replaced. The Wisconsin Multicube [42] is a proposed cache-coherent, shared-memory multiprocessor intended to scale to over a thousand processors.
Reference: [75] <author> C. B. Stunkel and W. K. Fuchs. TRAPEDS: </author> <title> Producing Traces for Multicom-puters Via Execution-Driven Simulation. </title> <booktitle> In Proceedings of the 1989 ACM International Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 70-78, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: Several approaches to the application of execution-driven simulation to cache and shared-memory performance evaluation have evolved in the last few years <ref> [15, 75, 32, 25] </ref>. We have adapted this technique for cache and shared-memory simulation in our testbed. We describe our approach and compare it to other existing approaches in Chapter 3. 2.4.3 Analytical Modeling Analytical models provide a quick first-cut estimate of cache performance. <p> We have implemented this technique as part of the Rice Parallel Processing Testbed (RPPT) [23], a simulation tool for the performance evaluation of parallel computer systems. A similar approach to cache simulation is used in [60], but only instruction addresses are traced. The TRAPEDS <ref> [75] </ref> approach is limited to non-multiprogrammed multicomputer address trace analysis, and does not handle shared-memory systems. TRAPEDS also does not model communication accurately, since it does not evaluate the effects of contention. For these reasons, TRAPEDS is not an appropriate tool to study trade-offs in computation and communication.
Reference: [76] <author> C. P. Thacker and L. C. Stewart. Firefly: </author> <title> A Multiprocessor Workstation. </title> <booktitle> In Proceedings of the Second International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 164-172. </pages> <publisher> ACM, </publisher> <month> October </month> <year> 1987. </year>
Reference-contexts: The April processor used in the Alewife architecture was designed to hide the latency of network communication by supporting fast context switching, coarse-grain multithreading, and full/empty bit synchronization. 9 Several commercial multiprocessors (Sequent Symmetry [55], DEC Firefly <ref> [76] </ref>, Encore Multimax [33]) have been built with a shared-bus architecture. Since shared single-bus architectures do not scale well, several hierarchical-bus architectures have been proposed. Wilson proposed a cache-coherent multiprocessor architecture with hierarchical buses [50]. <p> Main memory is also updated along with other caches on a write. Again, with these protocols, the number of bus accesses for N synchronizing processes is O (N 2 ). The Firefly protocol <ref> [76] </ref> and the Dragon protocol [58] use a write-broadcast mechanism to update all copies of shared blocks. They dynamically determine whether blocks are shared, but do not check for inactive blocks in a cache. <p> When lock data is modified and is present in more than one cache (shared state), it is updated in all the caches as well as main memory (similar to the Firefly protocol <ref> [76] </ref>). In addition to using a write update mode for locks, the compiler may decide at compile time whether it would be more efficient to update certain data in all caches rather than invalidate it (data that exhibits finely interleaved sharing).
Reference: [77] <author> D. F. Thiebaut and H. S. Stone. </author> <title> Footprints in the Cache. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 5(4), </volume> <month> November </month> <year> 1987. </year>
Reference-contexts: This model handles only fully-associative caches at present. They also present a model for cache-reload transients based on the size of the cache and the sizes of the footprints in the cache of the competing programs <ref> [77] </ref>. The cache-reload transient is the set of cache 20 misses that occur when a process is reinitiated after being suspended temporarily. A program footprint is defined to be the set of lines in the cache in active use by the program.
Reference: [78] <author> D. F. Thiebaut, H. S. Stone, and J. L. Wolf. </author> <title> A Theory of Cache Behaviour. </title> <type> Technical Report RC 13309, </type> <institution> IBM Research Division, T. J. Watson Research Center, </institution> <address> Yorktown Heights, NY, </address> <month> November 87. </month>
Reference-contexts: They also provide insight into the dependence of the cache miss rate on program and workload parameters. Analytical models, however, lack the accuracy of either simulation or hardware measurement. Some of the analytical models that have been proposed are by Smith [72], Thiebaut and Stone ([77] and <ref> [78] </ref>), and Agarwal [2]. Smith fits a continuous curve to the simulation results that he obtained for different line sizes, and derives an empirical model. Thiebaut and Stone [78] present a simple fractal model to predict the miss rate of a workload as a function of the cache size. <p> Some of the analytical models that have been proposed are by Smith [72], Thiebaut and Stone ([77] and <ref> [78] </ref>), and Agarwal [2]. Smith fits a continuous curve to the simulation results that he obtained for different line sizes, and derives an empirical model. Thiebaut and Stone [78] present a simple fractal model to predict the miss rate of a workload as a function of the cache size. This model handles only fully-associative caches at present.
Reference: [79] <author> P. J. Varman, B. R. Iyer, D. J. Haderle, and S. M. Dunn. </author> <title> Parallel Merging: Algorithm and Implementation Results. </title> <booktitle> Parallel Computing, </booktitle> <pages> pages 165-177, </pages> <year> 1990. </year> <month> 142 </month>
Reference-contexts: We have chosen the following algorithms from both the numerical and non-numerical domains for study. 1. Sorting These are prescheduled partitioned algorithms [66]. Work is partitioned at compile time. We used two programs, both of which use the basic Mergesort technique - Bimerge <ref> [79] </ref> and Multi-Way Merge [80]. Both algorithms initially sort sublists independently on each processor. Bimerge then proceeds to merge the sorted sublists two at a time. Multi-Way Merge uses an efficient algorithm to perform a Multi-Way Merge on all the sorted sublists in one step. 2.
Reference: [80] <author> P. J. Varman and S. D. Scheufler. </author> <title> Merging Multiple Lists on Hierarchical-Memory Multiprocessors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <pages> pages 171-177, </pages> <year> 1991. </year>
Reference-contexts: We have chosen the following algorithms from both the numerical and non-numerical domains for study. 1. Sorting These are prescheduled partitioned algorithms [66]. Work is partitioned at compile time. We used two programs, both of which use the basic Mergesort technique - Bimerge [79] and Multi-Way Merge <ref> [80] </ref>. Both algorithms initially sort sublists independently on each processor. Bimerge then proceeds to merge the sorted sublists two at a time. Multi-Way Merge uses an efficient algorithm to perform a Multi-Way Merge on all the sorted sublists in one step. 2.
Reference: [81] <author> W. Weber and A. Gupta. </author> <title> Analysis of Cache Invalidation Patterns in Multiprocessors. </title> <booktitle> In Proceedings of the Third International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 243-256. </pages> <publisher> ACM, </publisher> <month> April </month> <year> 1989. </year>
Reference-contexts: Adaptive Caching Address references made by processors may normally be classified into a small number of categories <ref> [11, 81] </ref>. We investigate the effect on program performance of using multiple coherence protocols on different types of data. Two coherency modes are provided, write-invalidate when shared, and write-through when shared. <p> We also present a uniform taxonomy for several extant consistency models, and provide an evaluation of their performance on our hierarchical bus-based architecture. 6.1 Adaptive Caching Address references made by processors may normally be classified into a small number of categories <ref> [11, 81] </ref>. We enumerate one possible categorization (following [11]) below, and indicate the coherency mode that we believe best suits the datum's access patterns.
Reference: [82] <author> P. C. Yew, N. F. Tzeng, and D. H. Lawrie. </author> <title> Distributed Hot-Spot Addressing In Large-Scale Multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(4):388-395, </volume> <month> April </month> <year> 1987. </year>
Reference-contexts: However, on a hierarchical bus architecture or in a network with more than one path, this type of barrier does not utilize the parallelism available in the network. The combining tree barrier <ref> [82] </ref> is efficient on a hierarchical bus architecture, since it allows each cluster to combine its own pro 88 cessors' accesses in parallel. The SPARC RISC-based architecture we are using for our analysis does not, however, provide an atomic decrement instruction.
Reference: [83] <author> C. Q. Zhu and P. C. Yew. </author> <title> A Scheme to Enforce Data Dependence on Large Multiprocessor Systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <pages> pages 726-739, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: Read-modify-write (RMW) primitives are provided by most existing processors. The most common set of primitives consists of test&set and reset. The microcode or software will usually repeat the test&set until the returned value is zero. Other RMW primitives include compare&swap <ref> [83] </ref>, fetch&add [43], and fetch& [63] operations, where is a function such as add, or, store, increment, or and. Typical implementations of these basic synchronization primitives cause a performance bottleneck in the presence of contention. <p> Goodman, Vernon, and Woest [41] also argue that hardware fetch& is of little use if combining is implemented in software. The University of Illinois Cedar machine <ref> [83] </ref> provides a general atomic instruction that operates on synchronization variables, which consist of two words: a key, and a value. The synchronization instruction consists of an evaluation of a condition, followed by an operation on the key and the value.
References-found: 83

