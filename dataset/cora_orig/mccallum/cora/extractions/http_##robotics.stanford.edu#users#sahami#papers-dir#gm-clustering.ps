URL: http://robotics.stanford.edu/users/sahami/papers-dir/gm-clustering.ps
Refering-URL: http://robotics.stanford.edu/users/sahami/papers.html
Root-URL: 
Email: moises@erg.sri.com  sahami@cs.stanford.edu  
Title: A Probabilistic Approach to Full-Text Document Clustering  
Author: Moises Goldszmidt 
Address: 333 Ravenswood Ave. Menlo Park, CA 94025  Gates Building 1A  Stanford, CA 94305-9010  
Affiliation: SRI International  Mehran Sahami  Computer Science Department Stanford University  
Abstract: To address the issue of text document clustering, a suitable function is needed for measuring the distance between documents. In this paper we explore a function for scoring document similarity based on probabilistic considerations: similarity is scored according to the expectation of the same words appearing in two documents. This score enables the investigation of different smoothing methods for estimating the probability of a word appearing in a document, for purposes of clustering. Our experimental results show that these different smoothing methods may be more or less effective, depending on the degree of separability between the clusters. Furthermore, we show that the cosine coefficient widely used in information retrieval can be associated with a particular form of probabilistic smoothing in our model. We also introduce a specific scoring function that outperforms the cosine coefficient and its extensions, such as TFIDF weighting, in our experiments with document clustering tasks. This new scoring is based on normalizing (in the probabilistic sense) the cosine similarity score, and adding a scaling factor based on the characteristics of the corpus being clustered. Finally, our experiments indicate that our model, which assumes an asymmetry between positive (word appearance) and negative (word absence) information in the document clustering task, 
Abstract-found: 1
Intro-found: 1
Reference: [Cha93] <author> E. Charniak. </author> <title> Statistical Language Learning. </title> <publisher> MIT Press, </publisher> <address> Cam-bridge, MA, </address> <year> 1993. </year>
Reference-contexts: In trying to control variance in estimating P (Y = wjd; M ), it becomes critical to perform some type of smoothing. A simple smoothing technique that has been used in the context of computational linguistics <ref> [Cha93] </ref> is to use the arithmetic mean (AM) of P ML (Y = wjd; M ) and the maximum likelihood estimate of the unconditional distribution, P ML (Y = wjM ), where P ML (Y = wjM ) = doc2D ~(w; d) doc2D w2doc ~(w; d) For the case of P
Reference: [CKPT92] <author> D. R. Cutting, D. R. Karger, J. O. Pederson, and J. W. Tukey. Scatter/Gather: </author> <title> a cluster-based approach to browsing large document collections. </title> <booktitle> In Proceedings of ACM/SIGIR, </booktitle> <pages> pages 318-329, </pages> <year> 1992. </year>
Reference-contexts: Such cluster-based matching could speed the retrieval process and possibly find relevant documents that do not explicitly contain the words in the user's query. More recently, applications of document clustering such as Scatter/Gather <ref> [CKPT92] </ref> [HP96] have been used to enable entire collections and query retrieval results to be browsed more easily. Work in this area has shown that document clustering is often an effective way to give the user a better sense of the topics present in a set of documents [PSHD96]. <p> Rather, these frequencies are attenuated by a monotone shrinkage factor such as the log or square root. It has been reported that for the document clustering task, using the square root generally appears to give better performance than using the log <ref> [CKPT92] </ref>.
Reference: [CKS + 88] <author> P. Cheeseman, J. Kelly, M. Self, J. Stutz, W. Taylor, and D. Freeman. </author> <title> AutoClass: a Bayesian classification system. </title> <booktitle> In Proceedings of Machine Learning, </booktitle> <pages> pages 54-64, </pages> <year> 1988. </year>
Reference-contexts: Finally, we also experiment with alternative probabilistic approaches based on mixture models such as AutoClass <ref> [CKS + 88] </ref>, showing that they generally 3 produce inferior results. <p> only can preserve the clean, well understood probabilistic semantics of our overlap measure, but also can have a significant beneficial impact on the empirical performance. 15 5 Alternative Probabilistic Models An alternative approach to text clustering is based on the use of probabilistic mixture modeling, such as the AutoClass system <ref> [CKS + 88] </ref>. In our investigations of this approach, documents were represented as binary vectors (rather than word frequency counts). AutoClass was used to cluster documents as mixtures of independent binomial distributions over word appearances.
Reference: [FBY92] <author> William B. Frakes and Ricardo Baeza-Yates. </author> <title> Information Retrieval: Data Structures and Algorithms. </title> <publisher> Prentice Hall, </publisher> <year> 1992. </year>
Reference-contexts: we will refer to this similarity score as Sim (doc; doc 0 ) and will subsequently instantiate it with our measure of probabilistic overlap, using different probability estimation methods. 3.1 Hierarchical Agglomerative Clustering The most common clustering method employed in the information retrieval community over the past decade is HAC <ref> [FBY92] </ref>. This family of methods begins by placing each document into a distinct cluster. Pairwise similarities between all such clusters are computed, and the two closest clusters are then merged into a new cluster.
Reference: [FGG97] <author> Nir Friedman, Dan Geiger, and Moises Goldszmidt. </author> <title> Bayesian network classifiers. </title> <journal> Machine Learning, </journal> <volume> 29 </volume> <pages> 131-163, </pages> <year> 1997. </year>
Reference-contexts: This has also been observed empirically in the successful application of such symmetric probabilistic models to classification problems in text [LR94] [KS97] and other other domains <ref> [FGG97] </ref>. A full discussion of this point is beyond the scope of this paper. 6 Conclusion We have presented a probability-based measure for document similarity that is quite effective for clustering.
Reference: [Fuh89] <author> Norbert Fuhr. </author> <title> Models for retrieval with probabilistic indexing. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 25(1) </volume> <pages> 55-72, </pages> <year> 1989. </year> <month> 18 </month>
Reference-contexts: In this way, our score can cleanly capture the full generality of probabilistic indexing <ref> [Fuh89] </ref> techniques used in other contexts. Moreover, the parameters defining the contributions of different words or functional characteristic of the documents to the overall similarity score in these cases can be learned directly from the data.
Reference: [HP96] <author> Marti A. Hearst and Jan O. Pederson. </author> <title> Reexamining the cluster hypothesis: </title> <booktitle> Scatter/Gather on retrieval results. In Proceedings of ACM/SIGIR, </booktitle> <year> 1996. </year>
Reference-contexts: Such cluster-based matching could speed the retrieval process and possibly find relevant documents that do not explicitly contain the words in the user's query. More recently, applications of document clustering such as Scatter/Gather [CKPT92] <ref> [HP96] </ref> have been used to enable entire collections and query retrieval results to be browsed more easily. Work in this area has shown that document clustering is often an effective way to give the user a better sense of the topics present in a set of documents [PSHD96].
Reference: [KS97] <author> Daphne Koller and Mehran Sahami. </author> <title> Hierarchically classifying documents using very few words. </title> <booktitle> In Proceedings of Machine Learning, </booktitle> <pages> pages 170-178, </pages> <year> 1997. </year>
Reference-contexts: This has also been observed empirically in the successful application of such symmetric probabilistic models to classification problems in text [LR94] <ref> [KS97] </ref> and other other domains [FGG97]. A full discussion of this point is beyond the scope of this paper. 6 Conclusion We have presented a probability-based measure for document similarity that is quite effective for clustering.
Reference: [LR94] <author> David D. Lewis and M. Ringuette. </author> <title> Comparison of two learning algorithms for text categorization. </title> <booktitle> In Proceedings of SDAIR, </booktitle> <year> 1994. </year>
Reference-contexts: This has also been observed empirically in the successful application of such symmetric probabilistic models to classification problems in text <ref> [LR94] </ref> [KS97] and other other domains [FGG97]. A full discussion of this point is beyond the scope of this paper. 6 Conclusion We have presented a probability-based measure for document similarity that is quite effective for clustering.
Reference: [PSHD96] <author> Peter Pirolli, Patricia Schank, Marti Hearst, and Christine Diehl. </author> <title> Scatter/gather browsing communicates the topic structure of a very large text collection. </title> <booktitle> In Proceedings of CHI, </booktitle> <year> 1996. </year>
Reference-contexts: Work in this area has shown that document clustering is often an effective way to give the user a better sense of the topics present in a set of documents <ref> [PSHD96] </ref>. The success of such systems often hinges on the effectiveness of the clustering methods employed. There is a long history of empirical work in document clustering, an excellent survey of which is given by Willett [Wil88].
Reference: [Ras92] <author> E. Rasmussen. </author> <title> Clustering algorithms. </title> <editor> In William B. Frakes and Ricardo Baeza-Yates, editors, </editor> <booktitle> Information Retrieval: Data Structures and Algorithms. </booktitle> <publisher> Prentice Hall, </publisher> <year> 1992. </year>
Reference-contexts: While a number of methods for clustering exist, the two most widely applied to text domains are hierarchical agglomerative clustering (HAC) and iterative clustering techniques such as K-means <ref> [Ras92] </ref>. Both of these methods rely on the definition of a similarity score between pairs of documents.
Reference: [Sal71] <author> G. Salton. </author> <title> The SMART Information Retrieval System. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1971. </year>
Reference-contexts: Early work in information retrieval (IR) stressed the use of clustering as a means of improving the ability to find documents relevant to a query [vRJ71] <ref> [Sal71] </ref>. <p> This measure of similarity is generally referred to as the cosine coefficient <ref> [Sal71] </ref>. Each dimension of the vector corresponds to a distinct word in the union of all words in the corpus being clustered. A document is then represented as a vector containing the normalized frequency counts of the words in it. <p> We focus on the different ways of estimating this probability from the statistics in each vector d i and M , as well as the relationship of this equation to the cosine coefficient <ref> [Sal71] </ref> below. We first provide a derivation of Equation 1. 2.1 Deriving the Probabilistic Overlap Here we investigate one possible derivation of Equation 1 and reveal its underlying assumptions.
Reference: [SB87] <author> Gerard Salton and Chris Buckley. </author> <title> Term weighting approaches in automatic text retrieval. </title> <type> Technical Report 87-881, </type> <institution> Cornell University Computer Science Department, </institution> <month> November </month> <year> 1987. </year>
Reference-contexts: Moreover, this analysis reveals a scaling factor, given by the inverse of the probability of a word appearing in the corpus, that, when combined with our probabilistic similarity score, yields a clustering method that outperforms those based on the cosine coefficient and TFIDF weighting <ref> [SB87] </ref> in our experiments. Finally, we also experiment with alternative probabilistic approaches based on mixture models such as AutoClass [CKS + 88], showing that they generally 3 produce inferior results. <p> As will be seen below, the scaled NGM score of overlap performs better (in some case dramatically better) than any other score we 12 tested, including the cosine coefficient and TFIDF weighting method com-monly used in IR <ref> [SB87] </ref>. Realizing that methods for evaluating clustering algorithms are not without controversy, we use the following strategy (keeping aware of its limitations). We use previously labeled data and measure how well the clustering recovers the known label structure in the data. <p> For comparison, we also performed clustering using the cosine coefficient (with square root dampening) as a similarity score as in Equation 21. Also, recognizing the use of TFIDF weighting 14 in the IR literature <ref> [SB87] </ref> as an alternate means of term scaling, we also used this weighting scheme, in conjunction with the Cosine rule (without square root dampening), as yet another similarity score for comparison.
Reference: [SS97] <author> Hinrich Schuetze and Craig Silverstein. </author> <title> A comparison of projections for efficient document clustering. </title> <booktitle> In Proceedings of ACM/SIGIR, </booktitle> <year> 1997. </year>
Reference-contexts: There is a long history of empirical work in document clustering, an excellent survey of which is given by Willett [Wil88]. Indeed, the description of Scatter/Gather is very specific about the clustering methods used, reflecting the years of comparative work in the IR community that continues today <ref> [SS97] </ref>. While empirical work in document clustering has advanced the state of the art in performance, no equivalent advancement in theoretical analysis explains why the methods arrived at through experimentation work as well as they do.
Reference: [vR79] <author> C. J. van Rijsbergen. </author> <title> Information Retrieval. </title> <publisher> Butterworths, </publisher> <year> 1979. </year>
Reference-contexts: Early work in information retrieval (IR) stressed the use of clustering as a means of improving the ability to find documents relevant to a query [vRJ71] [Sal71]. This work was based on the Cluster Hypothesis <ref> [vR79] </ref>, which states that "closely associated documents tend to be relevant to the same requests." With this as a working assumption, document collections could be clustered a priori, and then new queries could simply be matched against clusters rather than against each document individually.
Reference: [vRJ71] <author> C. J. van Rijsbergen and N. Jardine. </author> <title> The use of hierarchic clustering in information retrieval. </title> <booktitle> Information Storage and Retrieval, </booktitle> <volume> 7 </volume> <pages> 217-240, </pages> <year> 1971. </year>
Reference-contexts: Early work in information retrieval (IR) stressed the use of clustering as a means of improving the ability to find documents relevant to a query <ref> [vRJ71] </ref> [Sal71].
Reference: [Wil88] <author> Peter Willett. </author> <title> Recent trends in hierarchical document clustering: A critical review. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 24(5) </volume> <pages> 577-597, </pages> <year> 1988. </year>
Reference-contexts: The success of such systems often hinges on the effectiveness of the clustering methods employed. There is a long history of empirical work in document clustering, an excellent survey of which is given by Willett <ref> [Wil88] </ref>. Indeed, the description of Scatter/Gather is very specific about the clustering methods used, reflecting the years of comparative work in the IR community that continues today [SS97]. <p> Depending on how the similarity of a document to a cluster is defined, 10 we can obtain different "flavors" of HAC; the most common are the single link, complete link, and group average methods. Previous work in IR <ref> [Wil88] </ref> has pointed out that the group average method generally produces superior results. We will concentrate on this method in this paper.
References-found: 17

