URL: http://www.neci.nj.nec.com/homepages/giles/papers/IEEE.TNN.tdnn.as.fsm.ps.Z
Refering-URL: http://www.neci.nj.nec.com/homepages/giles/html/CLG_pub.html
Root-URL: 
Email: E-mail: fdclouse,gcottrellg@ucsd.edu  E-mail: giles@research.nj.nec.com  E-mail: horne@research.nj.nec.com  
Title: Time-Delay Neural Networks: Representation and Induction of Finite State Machines  
Author: Daniel S. Clouse, C. Lee Giles, Bill G. Horne, and Garrison W. Cottrell D. S. Clouse and G. W. Cottrell C. L. Giles B. G. Horne 
Date: April 4, 1997  
Address: San Diego.  Princeton, NJ 08540,  College Park, Md. 20742.  Princeton, NJ 08540.  
Affiliation: University of California,  NEC Research Institute,  and Institute for Advanced Computer Studies, U. of Maryland,  NEC Research Institute,  
Note: IEEE TRANSACTIONS ON NEURAL NETWORKS 1  are with the  is with  is with  Accepted in IEEE Transactions on Neural Networks  
Abstract-found: 0
Intro-found: 0
Reference: [1] <author> A. Waibel, T. Hanazawa, G. Hinton, K. Shikano, and K. Lang, </author> <title> "Phoneme recognition using time-delay neural networks," </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> vol. 37, no. 3, </volume> <pages> pp. 328-339, </pages> <year> 1989. </year>
Reference: [2] <author> K. Lang, A. Waibel, and G. Hinton, </author> <title> "A time-delay neural network architecture for isolated word recognition," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 3, no. 1, </volume> <pages> pp. 23-44, </pages> <year> 1990. </year>
Reference: [3] <author> Eric A. Wan, </author> <title> "Time series prediction by using a connectionist network with internal delay lines," in Time Series Prediction: Forecasting the Future and Understanding the Past, </title> <editor> A. S. Weigend and N. A. Gershenfeld, Eds. </editor> <publisher> Addison Wesley, </publisher> <year> 1993. </year>
Reference-contexts: This early stopping is necessary to avoid infinite training when a suboptimal local minimum has been reached. Training for the TDNN architecture is slightly more complicated. Error from the final output must be propagated backwards across the internal tap-delay lines. See <ref> [3] </ref> for a description of the general algorithm. With the exception of the difference of this slightly more complicated training method, all details of the HDNN training were identical to that of the IDNN training. The results of the simulations for the four problems are presented in figure 2.
Reference: [4] <author> D. Angluin and C. H. Smith, </author> <title> "Inductive inference: Theory and methods," </title> <journal> Computing Surveys, </journal> <volume> vol. 15, no. 3, </volume> <pages> pp. 237-269, </pages> <year> 1983. </year>
Reference: [5] <author> Rafael C. Carrasco and Jose Oncina, </author> <title> Grammatical Inference and Applications. </title> <booktitle> Proceedings of the 2nd International Colloquium, vol. 862 of Lecture Notes on Computer Science, </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference: [6] <author> Laurent Miclet and Colin de la Higuera, </author> <title> Grammatical Inference: Learning Syntax from Sentences. </title> <booktitle> Proceedings of the 3nd International Colloquium, vol. 1147 of Lecture Notes on Computer Science, </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1996. </year>
Reference: [7] <author> Z. Kohavi, </author> <title> Switching and Finite Automata Theory, </title> <publisher> McGraw-Hill, Inc., </publisher> <address> New York, NY, </address> <note> second edition, </note> <year> 1978. </year>
Reference: [8] <author> B. G. Horne and D. R. Hush, </author> <title> "On the node complexity of neural networks," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 7, no. 9, </volume> <pages> pp. 1413-1426, </pages> <year> 1994. </year>
Reference-contexts: April 4, 1997 Accepted in IEEE Transactions on Neural Networks IEEE TRANSACTIONS ON NEURAL NETWORKS 4 sented by a feed-forward network with enough hidden units, some IDNN exists which can perform any mapping from recent input history to any boolean discrimination function <ref> [8] </ref>. Therefore, the IDNNs can, in fact, represent any DMM. We have already seen that the TDNN and IDNN classes are essentially functionally equivalent. This implies that TDNNs implement DMMs as well. The entire state of an IDNN is contained in the most recent d inputs.
Reference: [9] <author> S. W. Golomb, </author> <title> Shift Register Sequences, </title> <publisher> Aegean Park Press, </publisher> <address> Laguna Hills, CA, </address> <year> 1982. </year>
Reference-contexts: The entire state of an IDNN is contained in the most recent d inputs. From any state the next state is completely determined by the shift-register behavior of the tap-delay line. The resulting transition diagram is called a directed de Bruijn graph of diameter d <ref> [9] </ref>. The task of the IDNN is to learn which states in this fixed transition diagram are accepting states and which rejecting. We have determined the conditions under which such a transition diagram results in a minimal FSM.
Reference: [10] <author> C. Lee Giles, Bill G. Horne, and T. Lin, </author> <title> "Learning a class of large finite state machines with a recurrent neural network," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 8, no. 9, </volume> <pages> pp. 1359-1365, </pages> <year> 1995. </year>
Reference-contexts: II. Learning a large DMM An interesting demonstration of this understanding of TDNNs and DMMs is to learn a DMM with many states using a small subset of the possible training examples. This section is similar in spirit to Giles, Horne, and Lin <ref> [10] </ref> in which the authors show that a discrete time recurrent neural network is capable of learning a large finite memory machine (FMM), a larger subset of the FSMs. The machine learned here is a DMM of order 11.
Reference: [11] <author> David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams, </author> <title> "Learning internal representations by error propagation," in Parallel Distributed Processing: Explorations in the Microstructure of Cognition, </title> <editor> David E. Rumelhart and James L. McClelland, Eds., </editor> <volume> vol. 1, chapter 8. </volume> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1986. </year>
Reference-contexts: A trial consisted of a randomly chosen percentage of these strings on which the network was trained to the error criterion described below. During training (and testing), before introduction of a new string, the values of all the tap-delays were set to 0. Online back propagation <ref> [11] </ref> with a learning rate of 0.25 and momentum of 0.25 was used for training. A selective updating scheme was applied whereby weights were updated in an online fashion, but only if the absolute error on the current training sample was greater than 0.2 2 .
Reference: [12] <author> John A. Rice, </author> <title> Mathematical Statistics and Data Analysis, </title> <publisher> Brooks/Cole Publishing Company, </publisher> <address> Monterey, California, </address> <year> 1988. </year>
Reference-contexts: This same effect is also noticeable, though not as pronounced, when comparing repeated curves to their respective unrepeated curves. Statistical tests also support our expectations. We ran a multi-factor analysis of variance (ANOVA) <ref> [12] </ref> on the data summarized in the graphs. The factors of interest were the architecture (HDNN or IDNN), the width of terms (narrow or wide), the uniformity of terms (repeated or unrepeated), and the percent of possible patterns which were used in training.
References-found: 12

