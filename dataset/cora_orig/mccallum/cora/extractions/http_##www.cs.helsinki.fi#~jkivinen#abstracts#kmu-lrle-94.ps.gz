URL: http://www.cs.helsinki.fi/~jkivinen/abstracts/kmu-lrle-94.ps.gz
Refering-URL: http://www.cs.helsinki.fi/~jkivinen/abstracts/kmu-lrle-94.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Learning rules with local exceptions  
Author: Jyrki Kivinen Heikki Mannila Esko Ukkonen 
Address: P.O. Box 26 (Teollisuuskatu 23) FIN-00014 University of Helsinki, Finland  
Affiliation: Department of Computer Science  
Abstract: We present a learning algorithm for rule-based concept representations called ripple-down rule sets. Ripple-down rule sets allow us to deal with the exceptions for each rule separately by introducing exception rules, exception rules for each exception rule etc. up to a constant depth. These local exception rules are in contrast to decision lists, in which the exception rules must be placed into a global ordering of the rules. The localization of exceptions makes it possible to represent concepts that have no decision list representation. On the other hand, decision lists with a constant number of alternations between rules for different classes can be represented by constant depth ripple-down rule sets with only a polynomial increase in size. Our algorithm is an Occam algorithm for constant depth ripple-down rule sets and, hence, a PAC learning algorithm. It is based on repeatedly applying the greedy approximation method for the weighted set cover problem to find good exception rule sets.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Setsuo Arikawa, Satoru Miyano, Ayumi Shinohara, Satoru Kuhara, Yasuhito Mukouchi and Takeshi Shinohara. </author> <title> A machine discovery from amino acid sequences by decision trees over regular patterns. New Generation Computing, </title> <address> 11(3-4):361-375, </address> <year> 1993. </year>
Reference-contexts: As the number of substrings that occur is not polynomial in the length of the longest instance encountered, Rivest's algorithm is not applicable in this situation. Substring-based machine learning methods have been applied to classification tasks in biochemistry <ref> [1] </ref> and text databases [12]. For related theoretical work, see [9] and [13].
Reference: [2] <author> Avrim Blum. </author> <title> Learning Boolean functions in an infinite attribute space. </title> <journal> Machine Learning, </journal> <volume> 9(4) </volume> <pages> 373-386, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: For related theoretical work, see [9] and [13]. We can also apply our method for learning in the more traditional logic-based domains, in which case the extension of the allowed conditions makes it possible to use an infinite attribute space <ref> [2] </ref>. 2 Basic notions We represent rules, as well as the instances, by strings. Fix an alphabet for representing instances. For the rules we use an alphabet and an extra symbol ! that is not in . Often we choose = .
Reference: [3] <author> Anselm Blumer, Andrzej Ehrenfeucht, David Haussler and Manfred K. Warmuth. </author> <title> Oc-cam's razor. </title> <journal> Information Processing Letters, </journal> <volume> 24(6) </volume> <pages> 377-380, </pages> <month> April </month> <year> 1987. </year>
Reference-contexts: The algorithm given in this paper learns ripple-down rule sets with a small depth. The analysis of the algorithm is made within the PAC framework proposed by Valiant [15] and is based on showing that for a fixed k the algorithm is a polynomial time Occam algorithm <ref> [3] </ref>. <p> It is also possible to construct, in polynomial time, 1-adequate sets for the representation classes that are based on testing for a constant number of substrings or on Boolean constant size monomials over an infinite attribute base [9]. For k a constant, the algorithm RDRS is an Occam algorithm <ref> [3] </ref>. Theorem 5 Assume that the set R 0 chosen in the algorithm is f -adequate for the sample. Assume that a depth k classifier T is consistent with S and robust with respect to S. <p> Given a probability measure P in fl , let error P;T (H) be the probability of the error set of H with respect to T . Using the techniques of Blumer et al. <ref> [3] </ref>, the following learnability result can easily be derived from Theorem 5. 9 Theorem 6 Let f be a function such that the set R 0 chosen in the algorithm RDRS is always f -adequate for the sample, and let ff &gt; 0 be an arbitrary constant.
Reference: [4] <author> Jason Catlett. </author> <title> Ripple-down-rules as a mediating representation in interactive induction. </title> <editor> In A. Rappaport, J. Boose, B. Gaines and R. Neches, editors, </editor> <booktitle> AAAI-92 Workshop on Knowledge Representation Aspects of Knowledge Acquisition, </booktitle> <pages> pages 15-33. </pages> <booktitle> American Association for Artificial Intelligence, </booktitle> <month> July </month> <year> 1992. </year>
Reference-contexts: This is a problem, since classifiers produced by machine learning often need to be verified and tuned by human experts. Hence there is a need for representing exceptions in a more localized manner. Such a formalism is provided by ripple-down rule sets <ref> [7, 4] </ref>. For example, assigning an instance to class + if and only if it satisfies exactly one of the conditions P 1 and P 2 is easily achieved by the two ripple-down rules given in Figure 1.
Reference: [5] <author> V. Chvatal. </author> <title> A greedy heuristic for the set-covering problem. </title> <journal> Mathematics of Operations Research, </journal> <volume> 4(3) </volume> <pages> 233-235, </pages> <month> August </month> <year> 1979. </year>
Reference-contexts: A set J I is a solution to the problem if [ j2J s (j) = D. The cost of the solution is j2J w (j). Chvatal <ref> [5] </ref> has shown that if a solution of cost at most M exists, then the greedy method finds in polynomial time a solution with cost at most M H (jDj), where H (m) = i=1 1=i = fi (log m).
Reference: [6] <author> Peter Clark and Tim Niblett. </author> <title> The CN2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3(4) </volume> <pages> 261-283, </pages> <month> March </month> <year> 1989. </year> <month> 12 </month>
Reference-contexts: Otherwise it is assigned to the class u i where i is the least index for which x satisfies P i . Decision lists and their variants are commonly used in machine learning applications <ref> [6] </ref>. From a knowledge representation point of view, one of the main features of rules is that they tend to have exceptions [14].
Reference: [7] <author> P. Compton and R. Jansen. </author> <title> Knowledge in context: a strategy for expert system mainte-nance. </title> <booktitle> In Proceedings of AI' 88: 2nd Australian Joint Artificial Intelligence Conference, </booktitle> <pages> pages 292-306. </pages> <booktitle> Springer Lecture Notes in Artificial Intelligence 406, </booktitle> <year> 1988. </year>
Reference-contexts: This is a problem, since classifiers produced by machine learning often need to be verified and tuned by human experts. Hence there is a need for representing exceptions in a more localized manner. Such a formalism is provided by ripple-down rule sets <ref> [7, 4] </ref>. For example, assigning an instance to class + if and only if it satisfies exactly one of the conditions P 1 and P 2 is easily achieved by the two ripple-down rules given in Figure 1.
Reference: [8] <author> Andrzej Ehrenfeucht and David Haussler. </author> <title> Learning decision trees from random examples. </title> <journal> Information and Computation, </journal> <volume> 82(3) </volume> <pages> 231-246, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: The root of Tree (hLi u) tests for membership in (r 1 ). The subtrees are Tree (hL 1 i u 1 ) for the positive answer and Tree (hL 0 i u) for the negative answer. The rank <ref> [8] </ref> of Tree (hLi u) is at most the depth of L. However, the learning algorithm for constant rank decision trees 4 is not applicable to our setting where the number of possible tests (r) at the nodes is not polynomial in the size of the longest example.
Reference: [9] <author> Jyrki Kivinen, Heikki Mannila and Esko Ukkonen. </author> <title> Learning hierarchical rule sets. </title> <booktitle> In Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, </booktitle> <pages> pages 37-44, </pages> <address> New York, </address> <month> July </month> <year> 1992. </year> <institution> The Association for Computing Machinery. </institution>
Reference-contexts: As the number of substrings that occur is not polynomial in the length of the longest instance encountered, Rivest's algorithm is not applicable in this situation. Substring-based machine learning methods have been applied to classification tasks in biochemistry [1] and text databases [12]. For related theoretical work, see <ref> [9] </ref> and [13]. We can also apply our method for learning in the more traditional logic-based domains, in which case the extension of the allowed conditions makes it possible to use an infinite attribute space [2]. 2 Basic notions We represent rules, as well as the instances, by strings. <p> We say that L has at most k alternations if the partitioning into k blocks as described above is possible. (Depth 1 classifiers with at most k alternations are equivalent with k-level rule sets <ref> [9] </ref>.) Proposition 3 If L is a depth 1 ripple-down rule set with at most k alternations, there is a robust depth k ripple-down rule set L 0 of size O (jjLjj k ) such that (L 0 ) = (L). <p> It is also possible to construct, in polynomial time, 1-adequate sets for the representation classes that are based on testing for a constant number of substrings or on Boolean constant size monomials over an infinite attribute base <ref> [9] </ref>. For k a constant, the algorithm RDRS is an Occam algorithm [3]. Theorem 5 Assume that the set R 0 chosen in the algorithm is f -adequate for the sample. Assume that a depth k classifier T is consistent with S and robust with respect to S.
Reference: [10] <author> Leonard Pitt and Manfred K. Warmuth. </author> <title> Prediction-preserving reducibility. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 41(3) </volume> <pages> 430-467, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: The four-tuple (R; ; ; ) is called a concept representation class <ref> [10] </ref>. For notational convenience, we introduce a symbol true 62 fl and extend by defining (true) = fl . For example, we could let = be an arbitrary finite alphabet, R = fl and (r) be the collection of superstrings of r.
Reference: [11] <author> Ronald L. Rivest. </author> <title> Learning decision lists. </title> <journal> Machine Learning, </journal> <volume> 2(3) </volume> <pages> 229-246, </pages> <month> November </month> <year> 1987. </year>
Reference-contexts: These structures are known as decision lists <ref> [11] </ref>. Here each P i is a predicate over the set of instances, as we call the objects to be classified. Each u i , as well as u def , is from a usually small set of classes. <p> The running time and the sample complexity are exponential in the depth k of the ripple-down rule sets to be learned. 2 Aside from introducing localized exception rules, our approach differs from that of, e.g., Rivest <ref> [11] </ref> in allowing more freedom in the choice of conditions to be used in the rules. For instance, if we take our instances to be strings, the presence of a given substring in the instance could be used as a condition. <p> Such negations can always be introduced without affecting the learnability results of this paper. However, since these results hold only for robust ripple-down rule sets, they have no implications to learning general decision trees. Decision lists <ref> [11] </ref> are equivalent with depth 1 ripple-down rule sets. However, since decision list are typically very non-robust, and we are here mainly concerned with robust ripple-down rule sets, it is more to the point to compare decision lists with robust ripple-down rule sets.
Reference: [12] <author> Yasubumi Sakakibara, Kazuo Misue and Takeshi Koshiba. </author> <title> Text classification and automatic extraction of keywords by learning decision trees. </title> <booktitle> In Proceedings of the 9th IEEE Conference on Artificial Intelligence for Applications, </booktitle> <pages> pages 466-466, </pages> <address> Los Alamitos, California, March 1993. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: As the number of substrings that occur is not polynomial in the length of the longest instance encountered, Rivest's algorithm is not applicable in this situation. Substring-based machine learning methods have been applied to classification tasks in biochemistry [1] and text databases <ref> [12] </ref>. For related theoretical work, see [9] and [13].
Reference: [13] <author> Yasubumi Sakakibara and Rani Siromoney. </author> <title> A noise model on learning sets of strings. </title> <booktitle> In Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, </booktitle> <pages> pages 295-302, </pages> <address> New York, </address> <month> July </month> <year> 1992. </year> <institution> The Association for Computing Machinery. </institution>
Reference-contexts: Substring-based machine learning methods have been applied to classification tasks in biochemistry [1] and text databases [12]. For related theoretical work, see [9] and <ref> [13] </ref>. We can also apply our method for learning in the more traditional logic-based domains, in which case the extension of the allowed conditions makes it possible to use an infinite attribute space [2]. 2 Basic notions We represent rules, as well as the instances, by strings.
Reference: [14] <author> David S. Touretzky. </author> <title> The Mathematics of Inheritance Systems. </title> <publisher> Pitman, </publisher> <address> London, </address> <year> 1986. </year>
Reference-contexts: Decision lists and their variants are commonly used in machine learning applications [6]. From a knowledge representation point of view, one of the main features of rules is that they tend to have exceptions <ref> [14] </ref>.
Reference: [15] <author> Leslie G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <month> November </month> <year> 1984. </year> <month> 13 </month>
Reference-contexts: The algorithm given in this paper learns ripple-down rule sets with a small depth. The analysis of the algorithm is made within the PAC framework proposed by Valiant <ref> [15] </ref> and is based on showing that for a fixed k the algorithm is a polynomial time Occam algorithm [3].
References-found: 15

