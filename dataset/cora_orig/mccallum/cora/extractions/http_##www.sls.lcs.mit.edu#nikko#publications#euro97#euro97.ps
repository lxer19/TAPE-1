URL: http://www.sls.lcs.mit.edu/nikko/publications/euro97/euro97.ps
Refering-URL: http://www.sls.lcs.mit.edu/nikko/publications/index.html
Root-URL: 
Email: Email: nikko@speech.kth.se  
Phone: Tel. +46 8 790 75 63, FAX: +46 8 790 78 54,  
Title: SPARSE CONNECTION AND PRUNING IN LARGE DYNAMIC ARTIFICIAL NEURAL NETWORKS  
Author: Nikko Strm 
Address: Stockholm, Sweden  
Affiliation: Department of Speech, Music and Hearing KTH (Royal Institute of Technology),  
Abstract: This paper presents new methods for training large neural networks for phoneme probability estimation. A combination of the timedelay architecture and the recurrent network architecture is used to capture the important dynamic information of the speech signal. Motivated by the fact that the number of connections in fully connected recurrent networks grows super-linear with the number of hidden units, schemes for sparse connection and connection pruning are explored. It is found that sparsely connected networks outperform their fully connected counterparts with an equal or smaller number of connections. The networks are evaluated in a hybrid HMM/ANN system for phoneme recognition on the TIMIT database. The achieved phoneme error-rate, 28.3%, for the standard 39 phoneme set on the core test-set of the TIMIT database is not far from the lowest reported. All training and simulation software used is made freely available by the author 1 , making reproduction of the results feasible. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Elenius K. & Takacs, G., </author> <title> Acoustic-phonetic recognition of continuos speech by artificial neural networks, </title> <booktitle> STL QPSR 2-3 /1990, </booktitle> <pages> pp. 1-44, </pages> <institution> KTH, Dept. of Speech, Music and Hearing, Stockholm, Sweden, </institution> <year> 1990. </year>
Reference: [2] <author> Bourlard H. & Morgan N., </author> <title> Continuous Speech Recognition by Connectionist Statistical Methods, </title> <journal> IEEE trans. on Neural Networks, </journal> <volume> 4 (6), </volume> <pages> pp. 893-909, </pages> <year> 1993. </year>
Reference-contexts: Recurrent connections are not the only path to good results, but other existing solutions have different problems. Another architecture is used with good results in <ref> [2] </ref>. There, timedelay windows [4] are used instead to 1 The software and documentation of the NICO toolkit, used in the ANN simulations in this paper, can be downloaded from the home page: http://www.speech.kth.se/NICO/index.html capture the temporal cues of the speech signal. <p> The first experiments with TDNN successfully showed improved classification of stop consonants where the dynamics is of great importance [4]. Later it has been successfully utilized in full-fledged hybrid HMM/ANN speech recognition systems <ref> [2] </ref>. Recurrent neural networks (RNN) is a different course to utilizing the context in the classification and are currently the most successful architecture for phoneme recognition [3]. In RNN, units in the same layer are connected to each other with a timedelay of one. <p> This is intuitively reasonable as it corresponds to the length of a syllable. To reach a minimum of E, the gain parameter must be gradually decreased during the training. We have combined this with cross-validation in a manner similar to <ref> [2] </ref>. The idea is to decrease the gain parameter when the objective function fails to decrease on a validation set. To be more specific, the training data is partitioned into a training set and a smaller validation set.
Reference: [3] <author> Robinson A.J., </author> <title> An application of Recurrent Nets to Phone Probability Estimation, </title> <journal> IEEE trans. on Neural Networks, </journal> <volume> 5 (2), </volume> <pages> pp. 298-305, </pages> <year> 1994. </year>
Reference-contexts: 1. INTRODUCTION It is well-known that artificial neural networks can be successfully used for phoneme recognition problems, e.g., [1,2,3]. The phoneme recognition rate on the TIMIT database reported in <ref> [3] </ref> is several percent higher than that of all other systems a large difference for this type of test. Still, for the last years, the research activities on ANNs for speech recognition have not by far been as intensive as for the prevailing HMM paradigm. <p> For example, the recurrent network used in <ref> [3] </ref> is trained with special hardware and a rather complex training heuristic is used with several ad hoc parameters to be determined empirically. Recurrent connections are not the only path to good results, but other existing solutions have different problems. Another architecture is used with good results in [2]. <p> Later it has been successfully utilized in full-fledged hybrid HMM/ANN speech recognition systems [2]. Recurrent neural networks (RNN) is a different course to utilizing the context in the classification and are currently the most successful architecture for phoneme recognition <ref> [3] </ref>. In RNN, units in the same layer are connected to each other with a timedelay of one. This approach differs from TDNN in that the activity of a unit depends recursively on activities at all previous times. <p> The error gradient is computed using backpropagation through time in a similar manner as in <ref> [3] </ref>. A way to visualize backpropagation through time is to draw the spatial dimension of the network in one dimension, i.e., line up all units in one column. Then unfold the network in the time dimension by drawing one column of units for each time point. <p> We have adopted the approximate scheme to update the weights every N frames, i.e., approximate the derivative based on subsequences of the training data. This method is also used in <ref> [3] </ref>. The approximation is worse if the weights are updated 3 frequently, but on the other hand it is desirable to update frequently, to speed up training. In the simulations, the weights are updated every 20-30 frames ( N is random distributed R [20;30]). <p> The lowest rate, 28.3%, is already better than all reported HMM-based results, e.g., [11] (30.9%), but not quite as low as the currently lowest reported RNN result <ref> [3] </ref> (26.1%). The two rightmost columns of Figure 2 are designed to test the idea of [10] that connection to higher layers should be more sparse than lower layers, in order to decouple the output units.
Reference: [4] <author> Waibel A., Hanazawa T., Hinton G., Shikano K. & Lang K., </author> <title> Phoneme Recognition Using TimeDelay Neural Networks, </title> <type> ATR Technical Report TR-006, </type> <institution> ATR, </institution> <address> Japan, </address> <year> 1987. </year>
Reference-contexts: Recurrent connections are not the only path to good results, but other existing solutions have different problems. Another architecture is used with good results in [2]. There, timedelay windows <ref> [4] </ref> are used instead to 1 The software and documentation of the NICO toolkit, used in the ANN simulations in this paper, can be downloaded from the home page: http://www.speech.kth.se/NICO/index.html capture the temporal cues of the speech signal. <p> Further, the software toolkit used for training and evaluating the neural networks is made freely available 1 to promote further development in the field. 2. HMM/ANN HYBRID RECOGNITION The phoneme recognizer used in the evaluations on the TIMIT database, is a hybrid HMM/ANN (see for example <ref> [4] </ref>), where the phoneme output activities of the ANN are interpreted as the a posteriori phoneme probabilities p (c i | o), and the observation probabilities, p (o | c ), used in the HMM framework, are derived from the a posteriori probabilities using Bayess rule. ( ) ( ) p <p> Therefore, phonetic classification can be greatly enhanced by considering also the neighboring spectra. A step in this direction was taken in <ref> [4] </ref> when timedelay neural networks (TDNN) were introduced. Here we will use the term TDNN for all architectures where units are connected to lower layers with time-delayed connections so that the activities depend on the activities of lower layer units within a finite timedelay window. <p> The first experiments with TDNN successfully showed improved classification of stop consonants where the dynamics is of great importance <ref> [4] </ref>. Later it has been successfully utilized in full-fledged hybrid HMM/ANN speech recognition systems [2]. Recurrent neural networks (RNN) is a different course to utilizing the context in the classification and are currently the most successful architecture for phoneme recognition [3].
Reference: [5] <author> Young S., Jansen J., Odell J., Ollason D. & Woodland P., </author> <title> HTK Hidden Markov Toolkit, </title> <institution> Entropic Cambridge Research Laboratory, </institution> <year> 1995. </year>
Reference-contexts: In contrast to the existing high performing ANN solutions, the well-established Maximum Likelihood (ML) training paradigm for the standard HMM (e.g. <ref> [5] </ref>) has a robust, theoretically well established training scheme and must be considered a safer route to a functioning speech recognition system today. The HMM paradigm also has the advantage of a large mature body of easily available software.
Reference: [6] <institution> Strm Continuous speech recognition in the WAXHOLM dialogue system, </institution> <month> STL-QPSR 4 </month> <year> /1996, </year> <pages> pp. 67-95, </pages> <institution> KTH, Dept. of Speech, Music and Hearing, Stockholm, Sweden, </institution> <year> 1996. </year>
Reference-contexts: The input to the ANN is the feature vector and its first and second time derivatives. More details about the signal processing, the estimation of the HMM parameters and the dynamic decoding is given in <ref> [6] </ref>. 3. ANN STRUCTURE AND TRAINING 3.1 Recurrent connections and time delay windows Dynamic features of speech such as formant trajectories, are not captured by the short time spectrum used as input to the ANN. Therefore, phonetic classification can be greatly enhanced by considering also the neighboring spectra.
Reference: [7] <author> Strm N., </author> <title> Development of a Recurrent TimeDelay Neural Net Speech Recognition System, </title> <booktitle> STL-QPSR 2 3 /1992, </booktitle> <pages> pp. 1-44, </pages> <institution> KTH, Dept. of Speech, Music and Hearing, Stockholm, Sweden, </institution> <year> 1992. </year>
Reference-contexts: If the connections of an RNN can have multiple timedelays instead of just one time step, the resulting network has all the modeling power of both architectures. This unified architecture, RTDNN, introduced in <ref> [7] </ref>, is used in this study. In the RTDNN framework we attempt to preserve the best features of the TDNN and RNN structures. The concept of look-ahead connections is borrowed from TDNN. It simplifies ANN design because it makes delayed targets unnecessary and the dynamic structure more intuitive.
Reference: [8] <author> Thimm G. & Fiesler E., </author> <booktitle> Evaluating pruning methods , In 1995 International Symposium on Artificial Neural Networks, Proc. </booktitle> <pages> ISANN 95 , pp. </pages> <address> A2 20-25, </address> <institution> National Chiao-Tung University, Hsinchu, Taiwan, </institution> <year> 1995. </year>
Reference-contexts: Connection pruning is a method that reduces the runtime of trained networks. The most well-known method is optimal brain damage (OBD) [9]. An overview of this and other methods is found in <ref> [8] </ref>. In our experiments a coarse pruning criterion is used we simply remove all weights smaller (in magnitude) than a threshold. It is important that the network is retrained after pruning.
Reference: [9] <author> Le Cun Y., Denker J. S. & Solla S. A., </author> <title> Optimal brain damage, </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <editor> II , ed: Touretsky D. </editor> <booktitle> S., </booktitle> <pages> pp. 589-605, </pages> <address> San Mateo, </address> <publisher> California IEEE, Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: This inspired us to study connection pruning methods and sparse connection schemes to be able to work with large networks with a manageable number of connections. Connection pruning is a method that reduces the runtime of trained networks. The most well-known method is optimal brain damage (OBD) <ref> [9] </ref>. An overview of this and other methods is found in [8]. In our experiments a coarse pruning criterion is used we simply remove all weights smaller (in magnitude) than a threshold. It is important that the network is retrained after pruning.
Reference: [10] <author> Ghosh G. & Tumer K., </author> <title> Structural adaptation and generalization in supervised feed-forward networks, </title> <booktitle> Journal of Artificial Neural Networks , 1 (4), </booktitle> <pages> pp. 430-458, </pages> <year> 1994. </year>
Reference-contexts: The expected number of connections in the sparse network is then N f , where N is the number of connections in the fully connected network. In <ref> [10] </ref> it is pointed out that the sparse connectivity has the effect of decoupling the output units, i.e., all output units are not connected to the same hidden units. <p> The lowest rate, 28.3%, is already better than all reported HMM-based results, e.g., [11] (30.9%), but not quite as low as the currently lowest reported RNN result [3] (26.1%). The two rightmost columns of Figure 2 are designed to test the idea of <ref> [10] </ref> that connection to higher layers should be more sparse than lower layers, in order to decouple the output units. Their hypothesis is supported by our experiment and will be put to use in future studies.
Reference: [11] <author> Lamel L. & Gauvain J. L., </author> <title> High performance speaker independent phone recognition using CDHMM, </title> <booktitle> Proc. EUROSPEECH, </booktitle> <pages> pp. 121 124, </pages> <year> 1993. </year>
Reference-contexts: Because of limited computing resources, we have not yet trained ANNs with more than 600 units, but because the error-rate is constantly decreasing, we expect larger networks to perform even better. The lowest rate, 28.3%, is already better than all reported HMM-based results, e.g., <ref> [11] </ref> (30.9%), but not quite as low as the currently lowest reported RNN result [3] (26.1%). The two rightmost columns of Figure 2 are designed to test the idea of [10] that connection to higher layers should be more sparse than lower layers, in order to decouple the output units. <p> 120k 160k number of connections hidden units error rate 36.1% 33.1% 31.0% 29.1% 28.9% 32.3% 30.5% 29.1% 37.0% 31.1% 30.5% 29.0% 28.3% 28.7% 29.5% connections to output units recurrent connections connections from input units f u l l y - d e c o u p l i n g <ref> [11] </ref> is used. Error-rate is defined as the sum of insertions, deletions and substitutions per phone.
References-found: 11

