URL: ftp://ftp.cse.ucsc.edu/pub/tr/ucsc-crl-95-48.ps.Z
Refering-URL: http://www.cse.ucsc.edu/research/slvg/verity.html
Root-URL: http://www.cse.ucsc.edu
Title: Verity Visualization: Visual Mappings  
Author: Craig M. Wittenbrink Alex T. Pang Suresh Lodha 
Keyword: data quality, uncertainty glyphs, fat surfaces, perturbations, oscillations.  
Note: Partially supported by a grant from NSF IRI-9423881 and ONR grant N00014-92-J-1807  
Address: Santa Cruz, CA 95064 USA  
Affiliation: Baskin Center for Computer Engineering Information Sciences University of California, Santa Cruz  
Date: October 11, 1995  
Pubnum: UCSC-CRL-95-48  
Abstract: Visualized data often has dubious origins. One way to define data lineage is by describing the uncertainty. In addition, different forms of uncertainty and errors are also introduced as the data is derived, transformed, interpolated, and finally rendered. In the absence of integrated presentation of data and its associated uncertainty, the analysis of the visualization is incomplete at best and often leads to inaccurate or incorrect conclusions. This paper presents several techniques of presenting data together with uncertainty. The idea behind these techniques can be applied to both spatial (e.g. surface) and temporal (i.e. animation) domains. We describe these techniques of representing the truths about the data as verity visualization. The same techniques can also be used to make the users aware of the data quality or to emphasize and draw their attention to the uncertainty. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> John W. Tukey. </author> <title> The Collected Works of John W. Tukey: Volume V: </title> <journal> Graphics,: </journal> <pages> 1965-1985. </pages> <publisher> Wadsworth and Brooks, </publisher> <year> 1984. </year>
Reference-contexts: From one perspective, one might consider adding uncertainty parameters as additional dimensions or fields to visualize using existing surface, volume, flow, and multi-dimensional visualization methods. In fact, we do start with existing methods. However, even with the simple task of designing glyphs or icons that incorporate uncertainty information <ref> [1, 2, 3] </ref>, the process is sometimes counter-intuitive. For example, while a glyph may appear appropriate by itself, the user's perception of the glyph may be different when a group of them is presented in various scales and locations.
Reference: [2] <author> Edward R. Tufte. </author> <title> Envisioning Information. </title> <publisher> Graphics Press, </publisher> <year> 1990. </year>
Reference-contexts: From one perspective, one might consider adding uncertainty parameters as additional dimensions or fields to visualize using existing surface, volume, flow, and multi-dimensional visualization methods. In fact, we do start with existing methods. However, even with the simple task of designing glyphs or icons that incorporate uncertainty information <ref> [1, 2, 3] </ref>, the process is sometimes counter-intuitive. For example, while a glyph may appear appropriate by itself, the user's perception of the glyph may be different when a group of them is presented in various scales and locations. <p> Two approaches are being taken in this effort. One, the quantitative approach provides a domain independent measure. Examples include those suggested by Tufte <ref> [20, 2] </ref>: data-ink maximization, clutter and moire pattern minimization, and multi-functionality of graphic elements. Two, the qualitative approach provides a more subjective measure of the methods. The measure may vary among different application domains.
Reference: [3] <author> Craig M. Wittenbrink, Elijah Saxon, Jeff J. Furman, Alex T. Pang, and Suresh Lodha. </author> <title> Glyphs for visualizing uncertainty in environmental vector fields. </title> <booktitle> In SPIE & IS&T Conference Proceedings on Electronic Imaging: Visual Data Exploration and Analysis. SPIE, </booktitle> <month> Feb. 7-10 </month> <year> 1995. </year>
Reference-contexts: From one perspective, one might consider adding uncertainty parameters as additional dimensions or fields to visualize using existing surface, volume, flow, and multi-dimensional visualization methods. In fact, we do start with existing methods. However, even with the simple task of designing glyphs or icons that incorporate uncertainty information <ref> [1, 2, 3] </ref>, the process is sometimes counter-intuitive. For example, while a glyph may appear appropriate by itself, the user's perception of the glyph may be different when a group of them is presented in various scales and locations. <p> We believe that these techniques will help the scientists, graphics users, and lay people doing visualization. One example of our work is the development of a new type of vector glyph which shows statistical variation, error, or range in both the magnitude and bearing <ref> [3] </ref>. Another one uses iterated function systems to indicate the level of uncertainty in surface interpolation [18]. <p> Uncertainty glyphs: Glyphs or icons are graphics objects that encode information through their shape, color, size, and other attributes. Uncertainty glyphs are probes which can be placed in a graphic to indicate the confidence interval, error, or range. Examples of uncertainty glyphs for vector fields were presented in <ref> [3] </ref> and included both the uncertainty in direction and magnitude of the vector. The challenging aspects of uncertainty glyph design are in the design of their shapes, density and placement, and scaling. Fat surfaces: These are surfaces or envelopes which show the range of possible values in the data.
Reference: [4] <author> M. Kate Beard, Barbara P. Buttenfield, and Sarah B. Clapham. </author> <title> NCGIA research initiative 7: Visualization of spatial data quality. </title> <type> Technical Paper 91-26, </type> <institution> NCGIA, </institution> <month> October </month> <year> 1991. </year>
Reference-contexts: While this has been recognized and is often stated as a worthy goal in scientific visualization ( e.g. in the IEEE Visualization discussion on How to Lie with Visualization and the NCGIA initiative on Visualization of Spatial Data Quality <ref> [4] </ref> ), it has rarely been pursued or realized. This paper presents some methods that represent significant steps toward achieving this goal. 2 Uncertainty 2.1 What is Uncertainty? We define uncertainty as statistical variation or spread, error, and minimum-maximum ranges.
Reference: [5] <author> Barry N. Taylor and Chris E. Kuyatt. </author> <title> Guidelines for evaluating and expressing the uncertainty of NIST measurement results. </title> <type> Techni References 7 cal report, NIST Technical Note 1297, </type> <month> Jan-uary </month> <year> 1993. </year>
Reference-contexts: This paper presents some methods that represent significant steps toward achieving this goal. 2 Uncertainty 2.1 What is Uncertainty? We define uncertainty as statistical variation or spread, error, and minimum-maximum ranges. NIST has written a standards report on uncertainty, which includes operator error <ref> [5] </ref>, but for the discussion in this paper we consider three types of uncertainty: statistical either given by the estimated mean and standard deviation, which can be used to calculate a confidence interval, or an actual distribution of the data; error - a difference, or an absolute valued error among estimates
Reference: [6] <author> Alex Pang, Jeff Furman, and Wendell Nuss. </author> <title> Data quality issues in visualization. </title> <booktitle> In SPIE Vol. 2178 Visual Data Exploration and Analysis, </booktitle> <pages> pages 12-23. SPIE, </pages> <month> February </month> <year> 1994. </year>
Reference-contexts: Note that the term data quality has an inverse relationship with data uncertainty <ref> [6] </ref> and hence can also take advantage of the techniques presented in this paper. 2.2 Sources of Uncertainty In order to understand what is overlooked in visualization, we quickly review the sources of uncertainty, errors, and ranges within data.
Reference: [7] <author> Christopher Chatfield. </author> <title> Statistics for Technology, A Course In Applied Statistics. </title> <publisher> Chap-man and Hall, </publisher> <address> third edition, </address> <year> 1983. </year>
Reference-contexts: It is clear that different forms of uncertainty are introduced into the pipeline as data are acquired, transformed, and visualized. Starting with the data acquisition stage, one will note that nearly all data sets, whether from instrument measurements, numerical models, or data entry have a statistical variation <ref> [7] </ref>. With instruments, there is an experimental variability whether the measurements are taken by a machine or by a scientist. The more times the measurement is taken, the more confident the measurement. But there will be a statistical variation in these measurements.
Reference: [8] <author> Ned Greene and Michael Kass. </author> <title> Error-bounded antialiased rendering of complex environments. </title> <booktitle> In Proceedings of SIGGRAPH 94, </booktitle> <pages> pages 59-66, </pages> <address> Orlando, FL, </address> <month> Jul. 24-29 </month> <year> 1994. </year> <note> ACM SIGGRAPH. </note>
Reference-contexts: What is more interesting and perhaps not self evident is that uncertainty is also introduced in the visualization stage itself. Within the area rendering with radiosity, there has been some recent work in controlling the errors introduced in the rendering process <ref> [8, 9, 10] </ref>. As these researchers also pointed out, the rendering process introduces uncertainty arising from the data collection process, algorithmic errors, and computational accuracy and precision. Aside from radiosity, other rendering and visualization methods also suffer from unintentional and perhaps unavoidable errors introduced during the visualization process. <p> These are usually displayed using some straightforward method such as side by side comparison or differencing. For example, [9] used line plots to render uncertainty, <ref> [8] </ref> used difference images, and [10] used norms for the entire image. In surface interpolation, pseudo-coloring of the surface curvature or other properties of the surface is used [15]. In geographic and information systems, researchers are aware of the statistical variation, and have been more creative.
Reference: [9] <author> Dani Lischinski, Brian Smits, and Donald P. Greenberg. </author> <title> Bounds and error estimates for radiosity. </title> <booktitle> In Proceedings of SIGGRAPH 94, </booktitle> <pages> pages 67-74, </pages> <address> Orlando, FL, </address> <month> Jul. 24-29 </month> <year> 1994. </year> <note> ACM SIGGRAPH. </note>
Reference-contexts: What is more interesting and perhaps not self evident is that uncertainty is also introduced in the visualization stage itself. Within the area rendering with radiosity, there has been some recent work in controlling the errors introduced in the rendering process <ref> [8, 9, 10] </ref>. As these researchers also pointed out, the rendering process introduces uncertainty arising from the data collection process, algorithmic errors, and computational accuracy and precision. Aside from radiosity, other rendering and visualization methods also suffer from unintentional and perhaps unavoidable errors introduced during the visualization process. <p> These are usually displayed using some straightforward method such as side by side comparison or differencing. For example, <ref> [9] </ref> used line plots to render uncertainty, [8] used difference images, and [10] used norms for the entire image. In surface interpolation, pseudo-coloring of the surface curvature or other properties of the surface is used [15].
Reference: [10] <author> James Arvo, Kenneth Torrance, and Brian Smits. </author> <title> A framework for the analysis of error in global illumination algorithms. </title> <booktitle> In Proceedings of SIGGRAPH 94, </booktitle> <pages> pages 75-84, </pages> <address> Orlando, FL, </address> <month> Jul. 24-29 </month> <year> 1994. </year> <note> ACM SIG-GRAPH. </note>
Reference-contexts: What is more interesting and perhaps not self evident is that uncertainty is also introduced in the visualization stage itself. Within the area rendering with radiosity, there has been some recent work in controlling the errors introduced in the rendering process <ref> [8, 9, 10] </ref>. As these researchers also pointed out, the rendering process introduces uncertainty arising from the data collection process, algorithmic errors, and computational accuracy and precision. Aside from radiosity, other rendering and visualization methods also suffer from unintentional and perhaps unavoidable errors introduced during the visualization process. <p> These are usually displayed using some straightforward method such as side by side comparison or differencing. For example, [9] used line plots to render uncertainty, [8] used difference images, and <ref> [10] </ref> used norms for the entire image. In surface interpolation, pseudo-coloring of the surface curvature or other properties of the surface is used [15]. In geographic and information systems, researchers are aware of the statistical variation, and have been more creative.
Reference: [11] <author> W. E. Lorensen and H. E. Cline. </author> <title> Marching cubes: A high resolution 3D surface construction algorithm. </title> <journal> Computer Graphics, </journal> <volume> 21(4):163 - 169, </volume> <year> 1987. </year>
Reference-contexts: Aside from radiosity, other rendering and visualization methods also suffer from unintentional and perhaps unavoidable errors introduced during the visualization process. For example, while the holes arising from ambiguities in the marching cubes algorithm <ref> [11] </ref> have been fixed [12], the iso-surfaces are obtained using interpolation and may not reconstruct the original surface. The same is true for flow visualization methods, where implementors are faced with decisions on which integration algorithm to use. Surface modeling and animation are not immune.
Reference: [12] <author> Allen Van Gelder and Jane Wilhelms. </author> <title> Topological considerations in isosurface generation. </title> <journal> ACM Transactions on Graphics, </journal> <volume> 13(4), </volume> <month> October </month> <year> 1994. </year> <note> Also UCSC technical report UCSC-CRL-90-14. </note>
Reference-contexts: Aside from radiosity, other rendering and visualization methods also suffer from unintentional and perhaps unavoidable errors introduced during the visualization process. For example, while the holes arising from ambiguities in the marching cubes algorithm [11] have been fixed <ref> [12] </ref>, the iso-surfaces are obtained using interpolation and may not reconstruct the original surface. The same is true for flow visualization methods, where implementors are faced with decisions on which integration algorithm to use. Surface modeling and animation are not immune.
Reference: [13] <author> Gerald Farin. </author> <title> Curves and Surfaces for Computer Aided Geometric Design: A Practical Guide. </title> <publisher> Academic Press, </publisher> <pages> 88. </pages>
Reference-contexts: Surface modeling and animation are not immune. In surface interpolation a variety of tradeoffs exist in performance and results, and there is no ideal surface in many cases because of the many free parameters available <ref> [13] </ref>. In many cases the data that are to be interpolated have numerous errors, and may even lack topology information [14]. In animation, the process of creating the key frames is error prone.
Reference: [14] <author> Hugues Hoppe, Tony DeRose, Tom Duchamp, Mark Halstead, Hugert Jin, John McDonald, Jean Schweitzer, and Werner Stuetzle. </author> <title> Piecewise smooth surface reconstruction. </title> <booktitle> In Proceedings of SIGGRAPH 94, </booktitle> <pages> pages 295-303, </pages> <address> Orlando, FL, </address> <month> Jul. 24-29 </month> <year> 1994. </year> <note> ACM SIG-GRAPH. </note>
Reference-contexts: In surface interpolation a variety of tradeoffs exist in performance and results, and there is no ideal surface in many cases because of the many free parameters available [13]. In many cases the data that are to be interpolated have numerous errors, and may even lack topology information <ref> [14] </ref>. In animation, the process of creating the key frames is error prone.
Reference: [15] <author> Hans Hagen, Stefanie Hahmann, Thomas Schreiber, Ya-suo Nakajima, Burkard Wordenweber, and Petra Hollemann-Grundstedt. </author> <title> Surface interrogation algorithms. </title> <journal> IEEE Computer Graphics and Applications, </journal> <volume> 12(5) </volume> <pages> 53-60, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: For example, [9] used line plots to render uncertainty, [8] used difference images, and [10] used norms for the entire image. In surface interpolation, pseudo-coloring of the surface curvature or other properties of the surface is used <ref> [15] </ref>. In geographic and information systems, researchers are aware of the statistical variation, and have been more creative. However, they use essentially multivalued visualization methods, and simply add uncertainty as another parameter into the picture.
Reference: [16] <author> Manfred Brill, Hans Hagen, Hans-Christian Rodrian, Wladimir Djatschin, and Stanislav V. Klimenko. </author> <title> Streamball techniques for flow visualization. </title> <booktitle> In Proceedings: Visualization '94, </booktitle> <pages> pages 225-231. </pages> <publisher> IEEE Computer Society, </publisher> <year> 1994. </year>
Reference-contexts: New techniques are being developed for higher order data such as tensors, for new hardware features such as texture mapping for flow visualization, and for adding more and more variables into existing methods such as streamlines which result in stream balls <ref> [16] </ref>. Some newer approaches include animation for the display of uncertainty in fuzzily classified regions [17]. With few exceptions, most of the existing methods for visualizing uncertainty rely on the overloading approach where uncertainty parameters are treated as additional data fields to be mapped to visual 4.
Reference: [17] <author> Nahum D. </author> <title> Gershon. Visualization of fuzzy data using generalized animation. </title> <booktitle> In Proceedings of Visualization 92, </booktitle> <pages> pages 268-273. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> October </month> <year> 1992. </year>
Reference-contexts: Some newer approaches include animation for the display of uncertainty in fuzzily classified regions <ref> [17] </ref>. With few exceptions, most of the existing methods for visualizing uncertainty rely on the overloading approach where uncertainty parameters are treated as additional data fields to be mapped to visual 4. Visual Mappings of Data with Uncertainty 3 cues.
Reference: [18] <author> Craig M. Wittenbrink. </author> <title> Ifs fractal interpolation for 2d and 3d visualization. </title> <type> Technical report, </type> <institution> University of California, </institution> <address> Santa Cruz, Santa Cruz, CA, </address> <year> 1995. </year> <note> submitted to Visualization'95. </note>
Reference-contexts: One example of our work is the development of a new type of vector glyph which shows statistical variation, error, or range in both the magnitude and bearing [3]. Another one uses iterated function systems to indicate the level of uncertainty in surface interpolation <ref> [18] </ref>. We have done a classification of uncertainty visualization techniques, and concluded that only the scalar low density plot has been adequately explored, where the uncertainty may be shown with economy using Tukey's box plots [19], Tufte's quartile plots [20] and/or Cleveland's framed rectangles [21].
Reference: [19] <author> John W. Tukey. </author> <title> Exploratory Data Analysis. </title> <publisher> Addison Wesley, </publisher> <year> 1977. </year>
Reference-contexts: We have done a classification of uncertainty visualization techniques, and concluded that only the scalar low density plot has been adequately explored, where the uncertainty may be shown with economy using Tukey's box plots <ref> [19] </ref>, Tufte's quartile plots [20] and/or Cleveland's framed rectangles [21].
Reference: [20] <author> Edward R. Tufte. </author> <title> The Visual Display of Quantitative Information. </title> <publisher> Graphics Press, </publisher> <year> 1983. </year>
Reference-contexts: We have done a classification of uncertainty visualization techniques, and concluded that only the scalar low density plot has been adequately explored, where the uncertainty may be shown with economy using Tukey's box plots [19], Tufte's quartile plots <ref> [20] </ref> and/or Cleveland's framed rectangles [21]. <p> Two approaches are being taken in this effort. One, the quantitative approach provides a domain independent measure. Examples include those suggested by Tufte <ref> [20, 2] </ref>: data-ink maximization, clutter and moire pattern minimization, and multi-functionality of graphic elements. Two, the qualitative approach provides a more subjective measure of the methods. The measure may vary among different application domains.
Reference: [21] <author> William S. Cleveland. </author> <title> The Elements of Graphing Data. </title> <publisher> Wadsworth, </publisher> <year> 1985. </year>
Reference-contexts: We have done a classification of uncertainty visualization techniques, and concluded that only the scalar low density plot has been adequately explored, where the uncertainty may be shown with economy using Tukey's box plots [19], Tufte's quartile plots [20] and/or Cleveland's framed rectangles <ref> [21] </ref>.
References-found: 21

