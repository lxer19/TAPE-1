URL: http://www-anw.cs.umass.edu/cgi-bin/getfile/pub/anw/pub/singh/approx-rl-loss.ps.Z
Refering-URL: http://www-anw.cs.umass.edu/Publications/recent.html
Root-URL: 
Email: singh@cs.umass.edu, yee@cs.umass.edu  
Phone: (413)-545-1596  
Title: An Upper Bound on the Loss from Approximate Optimal-Value Functions  
Author: Satinder P. Singh Richard C. Yee 
Keyword: Reinforcement Learning, Approximation, Theoretical Analysis  
Note: Running Head: Loss from Approximate Optimal-Value Functions  
Address: Amherst, MA 01003  
Affiliation: Department of Computer Science University of Massachusetts  
Abstract: Many reinforcement learning (RL) approaches can be formulated from the theory of Markov decision processes and the associated method of dynamic programming (DP). The value of this theoretical understanding, however, is tempered by many practical concerns. One important question is whether DP-based approaches that use function approximation rather than lookup tables, can avoid catastrophic effects on performance. This note presents a result in Bertsekas (1987) which guarantees that small errors in the approximation of a task's optimal value function cannot produce arbitrarily bad performance when actions are selected greedily. We derive an upper bound on performance loss which is slightly tighter than that in Bertsekas (1987), and we show the extension of the bound to Q-learning (Watkins, 1989). These results provide a theoretical justification for a practice that is common in reinforcement learning. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Anderson, C. W. </author> <year> (1986). </year> <title> Learning and Problem Solving with Multilayer Connectionist Systems. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, Department of Computer and Information Science, University of Massachusetts, </institution> <address> Amherst, MA 01003. </address>
Reference: <author> Barto, A. G., Bradtke, S. J., & Singh, S. P. </author> <year> (1991). </year> <title> Real-time learning and control using asynchronous dynamic programming. </title> <type> Technical Report TR-91-57, </type> <institution> Department of Computer Science, University of Mas-sachusetts. </institution>
Reference-contexts: The analogous result holds for indefinite horizon, undiscounted MDTs <ref> (as defined in Barto et al. 1991) </ref>. Although this result does not address convergence, it nevertheless helps to validate many practical approaches to DP-based learning that use approximations. The theorem also does not directly address policy-derivation methods other than the indicated greedy one.
Reference: <author> Barto, A. G., Sutton, R. S., & Anderson, C. W. </author> <year> (1983). </year> <title> Neuronlike elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 13 (5), </volume> <pages> 834-846. </pages>
Reference: <author> Barto, A. G., Sutton, R. S., & Watkins, C. J. C. H. </author> <year> (1990). </year> <title> Learning and sequential decision making. </title> <editor> In Gabriel, M. & Moore, J. (Eds.), </editor> <booktitle> Learning and Computational Neuroscience: Foundations of Adaptive Networks, chapter 13, </booktitle> <pages> pages 539-602. </pages> <address> Cambridge, MA: </address> <publisher> Bradford Books/MIT Press. </publisher>
Reference: <author> Bertsekas, D. P. </author> <year> (1987). </year> <title> Dynamic programming: Deterministic and stochastic models. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice Hall. </publisher>
Reference-contexts: Barto for identifying that our result for Q-value functions was a simple extension to Bertsekas's (1987) result on the usual value functions in dynamic programming theory. 1 above by (2fl*)=(1 fl). 2 Problem Statement and Theorem We consider stationary Markovian decision tasks (MDTs) that have finite state and action sets <ref> (e.g., see Bertsekas, 1987, Barto et al., 1990) </ref>. Let S be the state set, A (x) be the action set for state x 2 S, and P xy (a) be the probability of transiting from state x to state y on the execution of action a 2 A (x). <p> Greedy policy derivation allows one to specify an error-criterion on approximations, i.e., that they be within * of optimal, under the max norm. Acknowledgement We thank Andrew Barto for identifying the connection to <ref> (Bertsekas, 1987) </ref>. This work was supported by a grant (ECS-9214866) to Prof. A. G. Barto from the National Science Foundation. 6
Reference: <author> Sutton, R. S. </author> <year> (1988). </year> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 9-44. </pages>
Reference: <author> Sutton, R. S. </author> <year> (1990). </year> <title> Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <editor> In Porter, B. W. & Mooney, R. H. (Eds.), </editor> <booktitle> Machine Learning: Proceedings of the Seventh International Conference (ML90), </booktitle> <pages> pages 216-224, </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Tesauro, G. </author> <year> (1992). </year> <title> Practical issues in temporal difference learning. </title> <journal> Machine Learning, </journal> <volume> 8 (3/4), </volume> <pages> 257-277. </pages>
Reference: <author> Watkins, C. J. C. H. </author> <year> (1989). </year> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> King's College, University of Cambridge, </institution> <address> Cambridge, England. </address> <note> 7 Watkins, </note> <author> C. J. C. H. & Dayan, P. </author> <year> (1992). </year> <title> Q-learning. </title> <journal> Machine Learning, </journal> <volume> 8 (3/4), </volume> <pages> 279-292. </pages>
Reference-contexts: Using a natural definition of the loss in performance due to approximation, we derive an upper bound on the loss which is slightly tighter than the one indicated in Bertsekas (1987). We also show the corresponding extension to Q-learning <ref> (Watkins, 1989) </ref>. Note that these results do not address the issue of converging to good approximations.
References-found: 9

