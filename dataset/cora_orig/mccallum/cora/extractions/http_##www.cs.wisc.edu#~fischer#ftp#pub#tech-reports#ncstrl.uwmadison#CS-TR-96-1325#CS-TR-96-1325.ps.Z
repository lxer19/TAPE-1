URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-96-1325/CS-TR-96-1325.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-96-1325/
Root-URL: http://www.cs.wisc.edu
Title: GLOBAL MEMORY MANAGEMENT FOR MULTI-SERVER DATABASE SYSTEMS  
Author: By Shivakumar Venkatarman 
Degree: A dissertation submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy (Computer Sciences) at the  
Date: 1996  
Address: WISCONSIN MADISON  
Affiliation: UNIVERSITY OF  
Abstract-found: 0
Intro-found: 1
Reference: [AMM + 95] <author> T. Agerwala, J. Martin, H. Mirza, D. Sadler, D. Dias, and M. Snir. </author> <title> SP2 System Architecture. </title> <journal> IBM Systems Journal, </journal> <volume> 34(2), </volume> <month> Jan </month> <year> 1995. </year>
Reference-contexts: Table 2 illustrates the bandwidth and latency for these networks. The performance figures for the Asynchronous Transfer Mode (ATM) network is taken from the paper by Thekkath and Levy [TL93] and the Message Passing Interface (MPI) values for the IBM SP/2 (Scalable POWERparallel2 Systems) <ref> [AMM + 95] </ref> are from the paper by Gropp et al. [GLS94]. We measured the bandwidth and the latency for other networks using the hardware and protocol specified in Table 2.
Reference: [BAC + 90] <author> H. Boral, W. Alexander, W. Clay, G. Copeland, S. Danforth, M. Franklin, B. Hart, M. Smith, and P. Valduriez. </author> <title> Prototyping Bubba, A Highly Parallel Database System. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 2(1), </volume> <month> March </month> <year> 1990. </year>
Reference-contexts: The memory management algorithms that we describe only requires simple modification to the client-server buffer management code. Data placement is very important for parallel shared nothing relational database systems (RDBMS) such as Gamma [DGS + 90] and Bubba <ref> [BAC + 90] </ref>. Data placement has been shown to be crucial in exploiting the CPU and memory resources of parallel relational database system by [Gha90, Meh94, CABK88]. They demonstrate the need for declustering relations to achieve high performance.
Reference: [BCF + 95] <author> N. Boden, D. Cohen, R. Felderman, A. Kulawik, C. Seitz, J. Seizovic, and W. Su. </author> <title> Myrinet A Gigabit-per-Second Local Area Network. </title> <journal> IEEE Micro, </journal> <volume> 15(1) </volume> <pages> 29-36, </pages> <month> Feb </month> <year> 1995. </year>
Reference-contexts: Today, the availability of networks with very high bandwidths, low latencies, and at low cost, has made multi-server database system architectures feasible. For example, in a Myrinet <ref> [BCF + 95] </ref> switched network, the link bandwidth is of the order of 140 Megabits (Mbits)/sec, and the round trip latency for transferring 8 kilobyte (KB) pages between two nodes is about 931 microseconds (s).
Reference: [Be89] <author> B. Bret and etal. </author> <title> The GemStone Data Management System. In Object Oriented Concpts, Databases and Applications, </title> <address> Brisbane, Australia, </address> <year> 1989. </year>
Reference-contexts: For the purpose of this thesis, we assume that the data is processed at the server. While this is not standard in OODBMS, some commercial systems like Versant [Tec91] and Gemstone <ref> [Be89] </ref> support this. 2.3 Access path This section describes the path taken by a page request that originates at the primary server until the page becomes resident in memory at the primary server.
Reference: [BG88] <author> D. Bitton and J. Gray. </author> <title> Disk Shadowing. </title> <booktitle> In Proc. of the 14th VLDB Conf. </booktitle> <address> Los Angeles, </address> <year> 1988. </year>
Reference-contexts: The disk that is modeled is a Fujitsu M2266. The disk parameters are shown in the Table 6. 46 Parameter Value Disks/Node 4 Disk Capacity 1 GB, 5.25" Pages/Cylinder 83 Cache Size 4 Pages/Context Cache Context 8 Transfer Rate 3.09 MB/sec Rotation 16.667 ms Seek Time <ref> [BG88] </ref> 0.618* p SeekDistance Settle Time 2.0 ms Table 6: Disk Parameters We emulate the interconnect of a modern multicomputer. The interconnect is mod eled as a network with bidirectional FIFO links with a link bandwidth of 15 MB/sec.
Reference: [BLA + 94] <author> A. Blumrich, K. Li, R. Alpert, C. Dubnicki, and E. Felten. </author> <title> Virtual Memory Mapped Network Interface for the SHRIMP Multicomputer. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <month> April </month> <year> 1994. </year> <month> 138 </month>
Reference-contexts: The DASH project at Stanford [LLG + 92] implements similar cache-coherent shared memory policies entirely in hardware. The SHRIMP project at Princeton <ref> [BLA + 94] </ref> has studied ways to efficiently map the network interface to virtual memory to achieve low latency, and high bandwidth communication, in a multi-computer, to support shared memory abstraction. <p> The SHRIMP project at Princeton [BLA + 94] has studied ways to efficiently map the network interface to virtual memory to achieve low latency, and high bandwidth communication, in a multi-computer, to support shared memory abstraction. The distributed shared memory literature for NUMA <ref> [LE90, LLG + 92, BLA + 94, RLD94] </ref> (non uniform memory accesses) and COMA [FBR93] (cache only memory accesses) machines, described above, study mechanisms to efficiently implement a shared memory abstraction so that applications on a parallel hardware make use of aggregate 17 memory.
Reference: [CABK88] <author> G. Copeland, W. Alexander, E. Boughter, and T. Keller. </author> <title> Data placement in bubba. </title> <booktitle> In SIGMOD, </booktitle> <pages> pages 99-108, </pages> <address> Chicago, IL, </address> <month> June </month> <year> 1988. </year> <title> Database Machine, Parallelism, Data Placement. </title>
Reference-contexts: Data placement is very important for parallel shared nothing relational database systems (RDBMS) such as Gamma [DGS + 90] and Bubba [BAC + 90]. Data placement has been shown to be crucial in exploiting the CPU and memory resources of parallel relational database system by <ref> [Gha90, Meh94, CABK88] </ref>. They demonstrate the need for declustering relations to achieve high performance. These studies focus on the effect of data placement on parallel relational query processing.
Reference: [CBZ91] <author> J. Carter, J. Bennett, and W. Zwaenepoel. </author> <title> Implementation and Performance of Munin. </title> <booktitle> In Proceedings of the 13th International Symposium on Operating System Principles, </booktitle> <address> Pacific Grove, CA, </address> <month> Oct </month> <year> 1991. </year>
Reference-contexts: Li and Hudak [LH89] present the implementation of shared virtual memory on a distributed memory system. They focus on providing efficient access to distributed shared memory and study several algorithms to maintain coherence in a distributed shared memory system. Carter et al. <ref> [CBZ91] </ref> present algorithms for maintaining coherence based on the applications access patterns. These papers have focused on efficiently maintaining memory consistency, for the most part ignoring the issues of paging between nodes or to disk.
Reference: [CDF + 94] <author> M. Carey, D. DeWitt, M. Franklin, N. Hall, McAuliffe, M., J. Naughton, D. Schuh, M. Solomon, C. Tan, O. Tsatalos, J. White, and M. Zwilling. </author> <title> Shoring Up Persistent Applications. </title> <booktitle> In Proceedings of the ACM SIGMOD International Conference on Management of Data, </booktitle> <month> May </month> <year> 1994. </year>
Reference-contexts: The details of the server architecture can be hidden from the client by providing the client with a single server interface to the 3 data at all the servers. An example of such a multi-server database system architecture is the Scalable Heterogeneous Object REpository <ref> [CDF + 94] </ref> (SHORE) persistent object database system developed at the University of Wisconsin-Madison. SHORE employs a symmetric peer-peer communication architecture that facilitates replacing the single-server bottleneck with multiple servers. <p> We conclude with the summary of the thesis, and future work in Chapter 6. 24 Chapter 2 Multi-Server Database System 2.1 Architecture The multi-server architecture we consider is loosely based on the SHORE <ref> [CDF + 94] </ref> database system architecture. A primary goal of SHORE is to provide a robust, high-performance, persistent object database system that is flexible enough to be employed in a wide range of applications and computing environments.
Reference: [CDG + 90] <author> M. Carey, D. DeWitt, G. Graefe, D. Haight, J. Richardson, D. Schuh, E. Shekita, and S. Vendenber. </author> <title> The EXODUS Extensible DBMS Project: An Overview. Readings in Object Oriented Databases, </title> <editor> S. Zdonik and D. Maier, 1990. eds., </editor> <publisher> Morgan-Kaufman. </publisher>
Reference-contexts: One solution that is employed to solve this problem is to split the database among many independent servers. The client is modified so that it processes data from multiple servers. An example of such an architecture is the client-server EXODUS <ref> [CDG + 90] </ref>. In this architecture, the servers do not share their resources, nor do they communicate with each other. All the co-ordination necessary to run distributed transactions (2 phase commit, distributed logging, and distributed queries), takes place at the client. <p> White and DeWitt [WD95], present a study of various ways of implementing logging in QuickStore [WD94]. Quick-store is an implementation of E [RC89], similar to C++ [Str91] that is persistent, on top of EXODUS <ref> [CDG + 90] </ref> storage manager. They study the tradeoffs between various page, and object logging schemes. 1.4 Thesis Contribution/Overview The contribution of this thesis is, the design and evaluation of algorithms to efficiently manage global memory for a cluster of database servers connected by a high speed network. <p> Most commercial relational database systems have adopted the query shipping architecture [KCWW92]. The advantages and disadvantages of the query shipping architecture are summarized in [HF86, fADF90]. Commercial client-server object database systems, Observer [HZ87], O2 [Deu91], Ob-jectStore [LLOW91], and client-server EXODUS <ref> [CDG + 90] </ref>, employ the data shipping architecture. They use a page as the unit of transfer between the client and the server. This is called the page-server architecture. Performance tradeoffs between page and object shipping architectures are examined by DeWitt et al. [DFMV90].
Reference: [CDN93] <author> M. Carey, D. DeWitt, and J. Naughton. </author> <title> The OO7 Benchmark. </title> <booktitle> In Proceedings of the ACM SIGMOD Internationational Conference on Management of Data, </booktitle> <pages> pages 12-21, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Some examples of such object database applications include the CAD design rule checker, the VLSI router, aircraft simulation for computing air pressure on the aircraft, and the OO7 benchmark. We model our workloads loosely on the OO7 benchmark. The OO7 <ref> [CDN93] </ref> benchmark is an object database benchmark developed at the University of Wisconsin that models workloads experienced by CAD database. The schema of the OO7 benchmark consists of a module object that points to an assembly hierarchy. The base of the assembly hierarchy points to composite part objects.
Reference: [CFLS91] <author> M. Carey, M. Franklin, M. Livny, and E. Shekita. </author> <title> Data Caching Tradeoffs in Client-Server DBMS Architectures. </title> <booktitle> In Proceedings of the ACM SIGMOD International Conference on the Management of Data, </booktitle> <address> Denver, </address> <month> June </month> <year> 1991. </year> <month> 139 </month>
Reference-contexts: The papers by Carey et al. <ref> [CFLS91] </ref>, Franklin et al. [FC92], and Wang and Rowe [WR91], and Wilkinson and Neimat [WN90] study the advantages and the potential pitfalls of o*oading work from the servers to the clients by caching pages and locks across transaction boundaries at the client.
Reference: [CFZ94] <author> M. Carey, M. Franklin, and M. Zahariodakis. </author> <title> Fine Grained Sharing in a Page Server OODBMS. </title> <booktitle> In Proceedings of the ACM SIGMOD Internation-ational Conference on Management of Data, </booktitle> <month> June </month> <year> 1994. </year>
Reference-contexts: Rahm [Rah93] compares and evaluates various buffer coherency and locking policies for shared nothing database systems. Markos et al. <ref> [CFZ94] </ref> examine the tradeoffs between object and page locking schemes for client-server database systems. They describe a new locking algorithm that enables servers to lock pages, or objects; escalate locks to pages, and de-escalate back to objects, based on the read/write contention to the objects on a page.
Reference: [CG90] <author> D. Comer and J. Griffoen. </author> <title> A New Design for Distributed Systems.: The Remote Memory Model. </title> <booktitle> In Proceedings of Summer USENIX Conference, </booktitle> <pages> pages 127-135, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Carter et al. [CBZ91] present algorithms for maintaining coherence based on the applications access patterns. These papers have focused on efficiently maintaining memory consistency, for the most part ignoring the issues of paging between nodes or to disk. Comer and Griffoen <ref> [CG90] </ref> describe a remote memory model in a cluster that has 15 several workstations, disk servers, and remote memory servers. The remote memory servers are dedicated machines whose memory is used by nodes with heavy paging activity.
Reference: [CLVW94] <author> P. Cao, S. Lim, S. Venkataraman, and J. Wilkes. </author> <title> TickerTAIP: A Parallel RAID Array. </title> <journal> ACM Transactions on Computer Systems, </journal> <month> August </month> <year> 1994. </year>
Reference-contexts: If the concern is about a single point of failure, then chained declustering described by Hsiao and DeWitt [HD90], or Redundant Array of Inexpensive Disks (RAID) <ref> [PCGK89, CLVW94] </ref>, or shared SCSI chains can be used; Chained declustering allows the servers to continue running in the event of a node 115 Parameter Value Number of Nodes 8 Aggregate memory 64 MB Message Cost 1 ms Number of Files 300 File Size 32 pages/file Aggregate File Size 76.8 MB
Reference: [Deu91] <author> O. </author> <title> Deux. The O2 System. </title> <journal> Communications of the ACM, </journal> <volume> 34(10), </volume> <month> Oct </month> <year> 1991. </year>
Reference-contexts: Most commercial relational database systems have adopted the query shipping architecture [KCWW92]. The advantages and disadvantages of the query shipping architecture are summarized in [HF86, fADF90]. Commercial client-server object database systems, Observer [HZ87], O2 <ref> [Deu91] </ref>, Ob-jectStore [LLOW91], and client-server EXODUS [CDG + 90], employ the data shipping architecture. They use a page as the unit of transfer between the client and the server. This is called the page-server architecture. Performance tradeoffs between page and object shipping architectures are examined by DeWitt et al. [DFMV90].
Reference: [DFMV90] <author> D. DeWitt, P. Futtersack, D. Maier, and F. Velez. </author> <title> A Study of Three Alternative Workstation Server Architectures for Object Oriented Database SYstems. </title> <booktitle> In Proceedings of the 16th International Conference on Very Large Data Bases, </booktitle> <month> Aug </month> <year> 1990. </year>
Reference-contexts: They use a page as the unit of transfer between the client and the server. This is called the page-server architecture. Performance tradeoffs between page and object shipping architectures are examined by DeWitt et al. <ref> [DFMV90] </ref>. The page server architecture is employed by most object database servers. The pages requested by a client are transferred to the client workstations where they are processed. Processing data at the client makes use of the computing resources at the client workstations.
Reference: [DGS + 90] <author> D. DeWitt, S. Ghandeharizadeh, D. Schneider, A. Bricker, H. Hsiao, and R. Rasmussen. </author> <title> The Gamma Database Machine Project. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 2(1), </volume> <month> March </month> <year> 1990. </year>
Reference-contexts: In contrast, we make use of data placement to simplify the implementation of global LRU. The memory management algorithms that we describe only requires simple modification to the client-server buffer management code. Data placement is very important for parallel shared nothing relational database systems (RDBMS) such as Gamma <ref> [DGS + 90] </ref> and Bubba [BAC + 90]. Data placement has been shown to be crucial in exploiting the CPU and memory resources of parallel relational database system by [Gha90, Meh94, CABK88]. They demonstrate the need for declustering relations to achieve high performance.
Reference: [DNSV94] <author> D. DeWitt, J. Naughton, J. Shafer, and S. Venkataraman. </author> <title> ParSets for Paral-lelizing OODBMS Traversals: A Performance Evaluation. </title> <booktitle> In Proceedings of the International Conference on Parallel and Distributed Information Systems, </booktitle> <address> Austin, Tx, </address> <month> Sept </month> <year> 1994. </year> <month> 140 </month>
Reference-contexts: Balancing the CPU load while utilizing global memory is an important area of future work. 6.2.3 Parallelism Introducing parallelism into object database workloads, for example ParSets <ref> [DNSV94] </ref>, distributes the use of memory among the servers. However, skew in the execution times at different servers could cause memory and CPU load imbalances.
Reference: [DWAP94] <author> M. Dahlin, R. Wang, T. Anderson, and D. Patterson. </author> <title> Cooperative Caching : Using Remote Client Memory to Improve File System Performance. </title> <booktitle> In Proceedings of the First Conference on Operating Systems Design and Implementation, </booktitle> <month> Nov </month> <year> 1994. </year>
Reference-contexts: Many recent papers have studied the issue of utilizing idle memory in a cluster of workstations. In the Network of Workstations (NOW) project at U.C Berkeley, Dahlin et al. <ref> [DWAP94] </ref>, evaluate algorithms that make use of remote memory in a cluster of workstations connected by a fast interconnect. Their approach to identify and utilize memory is to store a page that is the last copy in memory at a randomly chosen server. <p> The technique in option 2 is known as forwarding. Forwarding has been shown to make good use of data already present in global memory in studies by Franklin et al. [FCL92], and Dahlin et al. <ref> [DWAP94] </ref>. We therefore use option 2, forwarding, instead of always reading the page from disk. With forwarding, the owner reads the page from disk only if the page is absent in global memory.
Reference: [DY91] <author> A. Dan and P. Yu. </author> <title> Analytical Modelling of a Hierarchical Buffer for a Data Sharing Environment. </title> <booktitle> In Proceedings of the ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <month> May </month> <year> 1991. </year>
Reference-contexts: They describe a new locking algorithm that enables servers to lock pages, or objects; escalate locks to pages, and de-escalate back to objects, based on the read/write contention to the objects on a page. In a data sharing environment, Dan and Yu <ref> [DY91] </ref> use an analytical model is used to study buffer management in a two level buffer hierarchy. Recent papers by Dan et al. [DY92, DY93] studies callback-style shared disk caching algorithms and investigates the performance gains that are available by avoiding disk writes when transferring dirty 22 pages between sites.
Reference: [DY92] <author> A. Dan and P. Yu. </author> <title> Performance Analysis of Coherency Control Policies through Lock Retention. </title> <booktitle> In Proceedings of the International Conference on Management of Data, </booktitle> <month> June </month> <year> 1992. </year>
Reference-contexts: In a data sharing environment, Dan and Yu [DY91] use an analytical model is used to study buffer management in a two level buffer hierarchy. Recent papers by Dan et al. <ref> [DY92, DY93] </ref> studies callback-style shared disk caching algorithms and investigates the performance gains that are available by avoiding disk writes when transferring dirty 22 pages between sites. Adding this optimization to a shared-disk system results in a complex recovery scheme, and this is described in Mohan and Narang [MN91].
Reference: [DY93] <author> A. Dan and P. Yu. </author> <title> Performance Analysis of Buffer Coherency Policies in a Multisystem Data Sharing Environment. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(3), </volume> <month> March </month> <year> 1993. </year>
Reference-contexts: In a data sharing environment, Dan and Yu [DY91] use an analytical model is used to study buffer management in a two level buffer hierarchy. Recent papers by Dan et al. <ref> [DY92, DY93] </ref> studies callback-style shared disk caching algorithms and investigates the performance gains that are available by avoiding disk writes when transferring dirty 22 pages between sites. Adding this optimization to a shared-disk system results in a complex recovery scheme, and this is described in Mohan and Narang [MN91].
Reference: [FA90] <author> Inc Fujistsu America. </author> <title> M2265 Techniccal Manual. </title> <type> Technical Report 41FH5048E-01, </type> <institution> Fujistsu America Technical Assistance Center, </institution> <address> SanJose, CA, </address> <year> 1990. </year>
Reference-contexts: We use scaled down memory and database size to make simulation times feasible. Further, it is the relative size of the database to memory that is important in the performance measurements. The physical parameter are illustrated 14. The disk modeled is a Fujitsu Model M2266 <ref> [FA90] </ref> (1 GB, 5.25") disk drive. The node and disk parameters are shown in Table 6. The interconnect modeled is a network with point to point bidirectional FIFO links with a link bandwidth of 15 MB/sec. Each node has a finite number of message buffers to send and receive messages.
Reference: [fADF90] <author> The Committee for Advanced DBMS Functions. </author> <title> Third Generation Database System Manifesto. </title> <booktitle> SIGMOD Record, </booktitle> <volume> 19(3), </volume> <month> Sept </month> <year> 1990. </year> <month> 141 </month>
Reference-contexts: Further, the unit of data transfer between the client and the server in a data shipping architecture could be either an object or a page. Most commercial relational database systems have adopted the query shipping architecture [KCWW92]. The advantages and disadvantages of the query shipping architecture are summarized in <ref> [HF86, fADF90] </ref>. Commercial client-server object database systems, Observer [HZ87], O2 [Deu91], Ob-jectStore [LLOW91], and client-server EXODUS [CDG + 90], employ the data shipping architecture. They use a page as the unit of transfer between the client and the server. This is called the page-server architecture.
Reference: [FBR93] <author> S. Frank, H. Burkhardt, and J. Rothnie. </author> <title> The KSR1:Bridging the Gap Between Shared Memory and MPPs. </title> <booktitle> In Proceedings of the IEEE International Conference COMPCON, </booktitle> <pages> pages 285-294, </pages> <address> SanFrancisco, CA, </address> <month> February </month> <year> 1993. </year>
Reference-contexts: None 16 of these studies address the issue of duplicate pages in memory, nor do they consider data placement to be an issue. Transaction related issues, such as locking and recovery are not addressed. 1.3.2 Computer Architecture Kendall Square Research (KSR) <ref> [FBR93] </ref> used an "all-cache" shared memory architecture to manage the buffers of a parallel shared nothing system. KSR has specialized hardware to support data caching at a block level, that has a size of 128 bytes. It provides a shared memory abstraction on top of a highly scalable parallel architecture. <p> The distributed shared memory literature for NUMA [LE90, LLG + 92, BLA + 94, RLD94] (non uniform memory accesses) and COMA <ref> [FBR93] </ref> (cache only memory accesses) machines, described above, study mechanisms to efficiently implement a shared memory abstraction so that applications on a parallel hardware make use of aggregate 17 memory. Efficiently managing and exploiting the aggregate memory is left to the application.
Reference: [FC92] <author> M. Franklin and M. Carey. </author> <title> Client-Server Caching Revisited. </title> <booktitle> In Proceedings of the International Workshop on Distributed Object Management, </booktitle> <address> Edmon-ton, Canada, Aug 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The papers by Carey et al. [CFLS91], Franklin et al. <ref> [FC92] </ref>, and Wang and Rowe [WR91], and Wilkinson and Neimat [WN90] study the advantages and the potential pitfalls of o*oading work from the servers to the clients by caching pages and locks across transaction boundaries at the client.
Reference: [FCL92] <author> M. Franklin, M. Carey, and M. Livny. </author> <title> Global Memory Management in Client-Server DBMS Architectures. </title> <booktitle> In Proceedings of the 18th International Conference on Very Large Data Bases, </booktitle> <address> Vancouver, British Columbia, Canada, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: Efficiently managing and exploiting the aggregate memory is left to the application. It is not clear how database applications use such a distributed shared memory systems for buffer management. 1.3.3 Utilizing Idle Memory Franklin et al. <ref> [FCL92, FCL93] </ref> study ways to augment the memory of the server in a client-server database system by utilizing the memory and disk resources on client workstations. <p> The technique in option 2 is known as forwarding. Forwarding has been shown to make good use of data already present in global memory in studies by Franklin et al. <ref> [FCL92] </ref>, and Dahlin et al. [DWAP94]. We therefore use option 2, forwarding, instead of always reading the page from disk. With forwarding, the owner reads the page from disk only if the page is absent in global memory. <p> With forwarding, the owner reads the page from disk only if the page is absent in global memory. When the owner reads the page from disk and sends it to the primary server, it retains a copy of the page in local memory. Franklin et al. <ref> [FCL92] </ref> show that getting rid of this 29 page quickly from the owner's memory utilizes global memory better, since this page is not in use locally and there is a copy elsewhere in memory. <p> Call back locking is a pessimistic lock-based protocol, that has been shown to perform well for a wide range of workloads for client-server database systems in <ref> [FCL92] </ref>. Call back locking uses invalidations to ensure "read multiple, write one" memory coherence. The salient feature of the call back locking strategy adapted for a multi-server architecture is that, each server is responsible for granting lock requests to the page that it owns. <p> This is a modification proposed to the Base algorithm by Franklin et al. <ref> [FCL92] </ref> and we refer to this algorithm as the ClSv algorithm. This is, in effect, a hint to the buffer replacement policy that this page is a good candidate for replacement, because, the page is not in use at the owner and there is a duplicate copy elsewhere in memory. <p> With these parameters, the message cost to send a 8 KB page from one node to another takes about 550 s. Table 7 summarizes the CPU costs and execution parameters that are used in the simulations. The instruction count for the execution parameters are from Franklin et al. <ref> [FCL92] </ref>. Parameter Value Lock 300 Inst Table Lookup 100 Inst Fault 4,000 Inst Per Page 30,000 Inst Msg Protocol 11,000 Inst Lock Timeout 500 ms Table 7: Execution Parameters 47 3.4.2 Workload The workloads simulated in the experiments reflects those experienced by object database systems. <p> To model sharing among several clients, we maintain a shared locality set that is accessible to all the clients. The probability of directing accesses to the shared locality set is governed by P shared parameter. This workload characterization of object databases is similar to that used by <ref> [FCL92] </ref>. The probability of the client updating a page is specified by the write probability P write . We explore the effects of writes in only one experiment, since the 50 study of how idle memory is utilized is mostly orthogonal to the presence or absence of writes.
Reference: [FCL93] <author> M. Franklin, M. Carey, and M. Livny. </author> <title> Local Disk Caching in Client-Server Database Systems. </title> <booktitle> In Proceedings of the 19th International Conference on Very Large Data Bases, </booktitle> <address> Dublin, Ireland, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: Efficiently managing and exploiting the aggregate memory is left to the application. It is not clear how database applications use such a distributed shared memory systems for buffer management. 1.3.3 Utilizing Idle Memory Franklin et al. <ref> [FCL92, FCL93] </ref> study ways to augment the memory of the server in a client-server database system by utilizing the memory and disk resources on client workstations.
Reference: [FMP + 95] <author> M. Feeley, W. Morgan, F. Pighi, A. Karlin, H. Levy, and C. Thekkath. </author> <title> Implementing Global Memory Management in a Workstation Cluster. </title> <booktitle> In Proceedings of the 15th ACM Symposium on Operating Systems Principles, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: This study is done using file access traces for a collection of workstations connected by an ATM network. Feeley et al. <ref> [FMP + 95] </ref> describe an implementation of global LRU replacement policy in a cluster of workstations by maintaining approximate global state information at each workstation. This work was done contemporarily with our work.
Reference: [FZ91] <author> E. Felten and J. Zahorjan. </author> <title> Issues in the Implementation of a Remote Memory Paging System. </title> <type> Technical Report 91-03-09, </type> <institution> University of Washington, </institution> <month> March </month> <year> 1991. </year> <month> 142 </month>
Reference-contexts: Comer and Griffoen [CG90] describe a remote memory model in a cluster that has 15 several workstations, disk servers, and remote memory servers. The remote memory servers are dedicated machines whose memory is used by nodes with heavy paging activity. Felten and Zahorjan <ref> [FZ91] </ref> use remote memory as an extension of the processor's memory, and also as a faster swap space. When a node becomes idle, it registers itself as a memory server.
Reference: [FZT + 92] <author> M. Franklin, M. Zwilling, C. Tan, M. Carey, and D. DeWitt. </author> <title> Crash Recovery in Client-Server EXODUS. </title> <booktitle> In Proceedings of the ACM SIGMOD International Conference on the Management of Data, </booktitle> <address> San Diego, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: Mohan et al. [MHL + 92] describe the ARIES algorithm that uses Write Ahead Logging (WAL) to implement redo-only recovery scheme in a database server. Franklin et al. <ref> [FZT + 92] </ref> show that the straight forward implementation of the ARIES algorithm to a client-server database system is not sufficient, and propose modifications to it. <p> It then grants a write lock 31 Location Access Time s Primary server 150 Owner's memory 2,800 Remote memory 5,500 Local Disk I/O 9,900 Remote Disk I/O 11,500 Table 3: Memory Hierarchy on the page to the requesting server. Client-server ARIES [MN94], or client-server EXODUS recovery <ref> [FZT + 92] </ref>, or the recovery schemes described in White and DeWitt [WD95], for client-server database systems, can be applied, without changes, to the multi-server database system.
Reference: [Gha90] <author> S. Ghandeharzadeh. </author> <title> Physical Database Design in Multiprocessor Systems. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Wisconsin-Madison, </institution> <year> 1990. </year>
Reference-contexts: Data placement is very important for parallel shared nothing relational database systems (RDBMS) such as Gamma [DGS + 90] and Bubba [BAC + 90]. Data placement has been shown to be crucial in exploiting the CPU and memory resources of parallel relational database system by <ref> [Gha90, Meh94, CABK88] </ref>. They demonstrate the need for declustering relations to achieve high performance. These studies focus on the effect of data placement on parallel relational query processing.
Reference: [GLS94] <author> W. Gropp, E. Lusk, and A. Skjellum. </author> <title> Using MPI: Portable Parallel Programming with the Message-Passing Interface . MIT Press, </title> <year> 1994. </year>
Reference-contexts: Machine Software Bandwidth Remote Mem Type Type Protocol Mbits/sec 8 KB s Ethernet Sparc 20 TCP/IP 10 6,700 ATM AN2 Alpha DEC-ATM [TL93] 155 870 ATM AN2 Sparc 20 TCP/IP 100.2 - Myrinet Sparc 20 UIUC FM [PLC95] 140 551 Myrinet Sparc 20 TCP/IP 85.2 - SP/2 RS/6000 SP MPI <ref> [GLS94] </ref> 360 480 Table 2: Network Performance network, including the network card, and the software, is only 1,500 $ per host connection. Table 2 illustrates the bandwidth and latency for these networks. <p> The performance figures for the Asynchronous Transfer Mode (ATM) network is taken from the paper by Thekkath and Levy [TL93] and the Message Passing Interface (MPI) values for the IBM SP/2 (Scalable POWERparallel2 Systems) [AMM + 95] are from the paper by Gropp et al. <ref> [GLS94] </ref>. We measured the bandwidth and the latency for other networks using the hardware and protocol specified in Table 2. The bandwidths and latencies for the ATM and Myrinet switched networks are an order of magnitude better than the corresponding values for the Ethernet.
Reference: [HD90] <author> H. Hsiao and D. DeWitt. </author> <title> Chained Declustering: A New Availability Strategy for Multiprocessor Database Machine. </title> <booktitle> In Proceedings of the 6th International Conference on Data Engineering, </booktitle> <month> Feb </month> <year> 1990. </year>
Reference-contexts: The study in Venkataraman et al. [VLN95] shows that declustering is better than clustering for good global memory utilization in a cluster of servers. If the concern is about a single point of failure, then chained declustering described by Hsiao and DeWitt <ref> [HD90] </ref>, or Redundant Array of Inexpensive Disks (RAID) [PCGK89, CLVW94], or shared SCSI chains can be used; Chained declustering allows the servers to continue running in the event of a node 115 Parameter Value Number of Nodes 8 Aggregate memory 64 MB Message Cost 1 ms Number of Files 300 File
Reference: [HF86] <author> R. Hagmann and D. Ferrari. </author> <title> Performance Analysis of Several Back-End Database Architectures. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 11(1), </volume> <month> March </month> <year> 1986. </year>
Reference-contexts: Further, the unit of data transfer between the client and the server in a data shipping architecture could be either an object or a page. Most commercial relational database systems have adopted the query shipping architecture [KCWW92]. The advantages and disadvantages of the query shipping architecture are summarized in <ref> [HF86, fADF90] </ref>. Commercial client-server object database systems, Observer [HZ87], O2 [Deu91], Ob-jectStore [LLOW91], and client-server EXODUS [CDG + 90], employ the data shipping architecture. They use a page as the unit of transfer between the client and the server. This is called the page-server architecture.
Reference: [HKS + 88] <author> J. Howard, M. Kazar, M. Sherri, D. Nichols, M. Satynarayana, Sideboth-amand R., and J West. </author> <title> Sclae and Performance in a Distributed File System. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1), </volume> <month> Feb </month> <year> 1988. </year>
Reference-contexts: The National Center for Super Computing Application (NCSA) Mosaic web site [KBM94] has taken exactly this approach. It has a cluster of identical servers connected by an Fiber Distribute Data Interface (FDDI) network to a centralized Andrew File System <ref> [HKS + 88] </ref> (AFS) server. Figure 44 illustrates this configuration. When you request a URL from http://www.ncsa.uiuc.edu, the request is actually 102 routed to one of nine HP9000 series workstations. They use a Domain Name Server (DNS) to act as a virtual router or switch through which the requests travel.
Reference: [HZ87] <author> M. Hornick and S. Zdonik. </author> <title> A Shared Segmented Memory System for an Object-Oriented Database. </title> <journal> ACM Transactions on Office Information Systems, </journal> <volume> 5(1), </volume> <month> Jan </month> <year> 1987. </year> <month> 143 </month>
Reference-contexts: Most commercial relational database systems have adopted the query shipping architecture [KCWW92]. The advantages and disadvantages of the query shipping architecture are summarized in [HF86, fADF90]. Commercial client-server object database systems, Observer <ref> [HZ87] </ref>, O2 [Deu91], Ob-jectStore [LLOW91], and client-server EXODUS [CDG + 90], employ the data shipping architecture. They use a page as the unit of transfer between the client and the server. This is called the page-server architecture.
Reference: [ILP93] <author> L. Iftode, K. Li, and K. Petersen. </author> <title> Memory Servers for Multicomputers. </title> <booktitle> In Proceedings of the IEEE Spring COMPCON '93, </booktitle> <pages> pages 534-547, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: Their goal is to migrate data from portables that are in need of memory to fixed stationary servers. The data in the servers migrate from one server to another as the portables migrate. Iftode et al. <ref> [ILP93] </ref> have investigated the memory server model by introducing a remote memory server layer as a fast backing storage between local physical memory and disks. They discuss several design issues to support sequential and message passing programs in such an architecture.
Reference: [JLHB88] <author> E. Jul, H. Levy, N. Hutchinson, and A. Black. </author> <title> Fine Grained Mobility in the Emerald System. </title> <journal> ACM Transaction on Computer Systems, </journal> <volume> 6(1), </volume> <month> February </month> <year> 1988. </year>
Reference-contexts: They discuss several design issues to support sequential and message passing programs in such an architecture. Issues related to global memory management have been addressed in distributed systems such as Emerald <ref> [JLHB88] </ref>, where methods for allowing objects to migrate among sites are addressed. Migration in this case is to improve performance to bring the objects closer to sites where they are being accessed, and to simplify distributed programming applications, rather than to avoid disk I/O.
Reference: [KBM94] <author> E. Katz, M. Butler, and R. McGrath. </author> <title> A Scalable HTTP Server: The NCSA Prototype. </title> <journal> Computer Networks and ISDN Systems, </journal> <volume> 27 </volume> <pages> 155-164, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: One promising solution to the CPU bottleneck is to replace a single processor server with a cluster of servers. The National Center for Super Computing Application (NCSA) Mosaic web site <ref> [KBM94] </ref> has taken exactly this approach. It has a cluster of identical servers connected by an Fiber Distribute Data Interface (FDDI) network to a centralized Andrew File System [HKS + 88] (AFS) server. Figure 44 illustrates this configuration.
Reference: [KCWW92] <author> S. Khoshafian, A. Chan, A. Wong, and H. Wong. </author> <title> A Guide to Developing Client-Server SQL Applications. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: All the data processing takes place at the client. Further, the unit of data transfer between the client and the server in a data shipping architecture could be either an object or a page. Most commercial relational database systems have adopted the query shipping architecture <ref> [KCWW92] </ref>. The advantages and disadvantages of the query shipping architecture are summarized in [HF86, fADF90]. Commercial client-server object database systems, Observer [HZ87], O2 [Deu91], Ob-jectStore [LLOW91], and client-server EXODUS [CDG + 90], employ the data shipping architecture.
Reference: [LE90] <author> P. LaRowe and C. Ellis. </author> <title> Experimental Comparison of Memory Management Policies for NUMA Multiprocessors. </title> <type> Technical Report CS-1990-10, </type> <institution> Duke University, </institution> <month> April </month> <year> 1990. </year>
Reference-contexts: The SHRIMP project at Princeton [BLA + 94] has studied ways to efficiently map the network interface to virtual memory to achieve low latency, and high bandwidth communication, in a multi-computer, to support shared memory abstraction. The distributed shared memory literature for NUMA <ref> [LE90, LLG + 92, BLA + 94, RLD94] </ref> (non uniform memory accesses) and COMA [FBR93] (cache only memory accesses) machines, described above, study mechanisms to efficiently implement a shared memory abstraction so that applications on a parallel hardware make use of aggregate 17 memory.
Reference: [LH89] <author> K. Li and P. Hudak. </author> <title> Memory Coherence in Shared Virtual Memory Systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4), </volume> <month> November </month> <year> 1989. </year>
Reference-contexts: Li and Hudak <ref> [LH89] </ref> present the implementation of shared virtual memory on a distributed memory system. They focus on providing efficient access to distributed shared memory and study several algorithms to maintain coherence in a distributed shared memory system.
Reference: [Li86] <author> K. Li. </author> <title> Shared Virtual Memory on Loosely Coupled Multiprocessors. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Yale University, </institution> <month> September </month> <year> 1986. </year> <month> 144 </month>
Reference-contexts: related; we show that data placement combined with simple memory management policies are sufficient to effectively utilize the aggregate 20 system memory. 1.3.4 Managing Duplicates One of the early papers that alludes to the problem of duplication in a distributed shared memory environment is the work by Li and Hudak <ref> [Li86] </ref>. Their work primarily deals with maintaining coherence in a distributed shared virtual memory environment. Inefficient use of global memory due to duplication is identified as a problem, but is not studied in any detail, and no solutions are proposed.
Reference: [LLG + 92] <author> D. Lenoski, J. Laudon, K. Gharachorloo, D. Weber, A. Gupta, J. Hennessy, and M. Horowitz. </author> <title> The Stanford DASH Multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3), </volume> <month> March </month> <year> 1992. </year>
Reference-contexts: It provides a shared memory abstraction on top of a highly scalable parallel architecture. Reinhardt et al. [RLD94] propose mechanisms that exposes low-level communication and memory-system mechanisms so that programmers and compilers customize policies for a given application on highly parallel architecture. The DASH project at Stanford <ref> [LLG + 92] </ref> implements similar cache-coherent shared memory policies entirely in hardware. The SHRIMP project at Princeton [BLA + 94] has studied ways to efficiently map the network interface to virtual memory to achieve low latency, and high bandwidth communication, in a multi-computer, to support shared memory abstraction. <p> The SHRIMP project at Princeton [BLA + 94] has studied ways to efficiently map the network interface to virtual memory to achieve low latency, and high bandwidth communication, in a multi-computer, to support shared memory abstraction. The distributed shared memory literature for NUMA <ref> [LE90, LLG + 92, BLA + 94, RLD94] </ref> (non uniform memory accesses) and COMA [FBR93] (cache only memory accesses) machines, described above, study mechanisms to efficiently implement a shared memory abstraction so that applications on a parallel hardware make use of aggregate 17 memory.
Reference: [LLM88] <author> M. Litzkow, M. Livny, and M. Mukta. </author> <title> Condor A Hunter of Idle Workstations. </title> <booktitle> In Proceedings of the 8th International Conference on Distributed Computing Systems, </booktitle> <month> June </month> <year> 1988. </year>
Reference-contexts: The simulation experiments are run using Condor <ref> [LLM88] </ref>, which provides abundant CPU cycles by exploiting CPU cycles of idle workstations. Physical Model The hardware platform simulated is a shared nothing, distributed memory architecture, with a database server on each node. The number of physical nodes is controlled by the parameter N .
Reference: [LLOW91] <author> C. Lamb, G. Landis, J. Orenstein, and D. </author> <title> Weinreb. </title> <journal> The ObjectStore Database System . Communications of the ACM, </journal> <volume> 34(10), </volume> <month> October </month> <year> 1991. </year>
Reference-contexts: Most commercial relational database systems have adopted the query shipping architecture [KCWW92]. The advantages and disadvantages of the query shipping architecture are summarized in [HF86, fADF90]. Commercial client-server object database systems, Observer [HZ87], O2 [Deu91], Ob-jectStore <ref> [LLOW91] </ref>, and client-server EXODUS [CDG + 90], employ the data shipping architecture. They use a page as the unit of transfer between the client and the server. This is called the page-server architecture. Performance tradeoffs between page and object shipping architectures are examined by DeWitt et al. [DFMV90].
Reference: [LWY92] <author> A. Leff, J. Wolf, and P. Yu. </author> <title> LRU-based Replication Strategies in a LAN Remote Caching Architecture. </title> <booktitle> In Proceedings of 17th Annual Conference on Local Computer Networks, </booktitle> <address> Minneapolis, MN, </address> <month> Sept </month> <year> 1992. </year>
Reference-contexts: The authors propose three algorithms: optimal, greedy, and distributed. Optimal is used as a baseline for comparison; with the greedy algorithm each server manages its own memory independent of other servers. The distributed algorithm needs the knowledge of future access patterns to be implementable. Leff et al. <ref> [LWY92] </ref> present another study of duplication in a local area network, and Pu et al. [PLKC90] study the effect of duplicating objects on performance along similar lines. 21 1.3.5 Coherence, Locking, and Recovery There are several papers that study coherence policies for client-server database systems.
Reference: [LWY93] <author> A. Leff, J. Wolf, and P. Yu. </author> <title> Replication Algorithms in a Remote Caching Architecture. </title> <journal> IEEE Transactions on Parallel and Distributed Information Systems, </journal> <volume> 4, </volume> <month> August </month> <year> 1993. </year>
Reference-contexts: Inefficient use of global memory due to duplication is identified as a problem, but is not studied in any detail, and no solutions are proposed. A recent study on managing duplicates is the work by Leff et al. <ref> [LWY93] </ref>. They use an analytical model to study algorithms to manage duplicates for distributed memory systems. The algorithms work on the assumption that the servers have prior knowledge of the client's access patterns. The servers decide on the objects to replicate based on a static cost analysis.
Reference: [Meh94] <author> M. Mehta. </author> <title> Resource Allocation in Parallel Shared-Nothing Database Systems. </title> <type> PhD thesis, </type> <institution> University of Wisconsin-Madison, </institution> <year> 1994. </year>
Reference-contexts: Data placement is very important for parallel shared nothing relational database systems (RDBMS) such as Gamma [DGS + 90] and Bubba [BAC + 90]. Data placement has been shown to be crucial in exploiting the CPU and memory resources of parallel relational database system by <ref> [Gha90, Meh94, CABK88] </ref>. They demonstrate the need for declustering relations to achieve high performance. These studies focus on the effect of data placement on parallel relational query processing.
Reference: [MHL + 92] <author> C. Mohan, D. Haderle, B. Lindsay, H. Pirahesh, and P. Schwarz. </author> <title> ARIES: </title>
Reference-contexts: Adding this optimization to a shared-disk system results in a complex recovery scheme, and this is described in Mohan and Narang [MN91]. Mohan et al. <ref> [MHL + 92] </ref> describe the ARIES algorithm that uses Write Ahead Logging (WAL) to implement redo-only recovery scheme in a database server.
References-found: 52

