URL: http://dis.cs.umass.edu/~nagendra/meta-learn.ps
Refering-URL: http://dis.cs.umass.edu/~nagendra/home.html
Root-URL: 
Email: fnagendra,lesserg@cs.umass.edu  
Title: The Use of Meta-level Information in Learning Situation-Specific Coordination  
Author: M V Nagendra Prasad and Victor R Lesser 
Address: Amherst, MA 01003.  
Affiliation: Department of Computer Science University of Massachusetts,  
Abstract: Achieving effective cooperation in a multi-agent system is a difficult problem for a number of reasons such as limited and possibly out-dated views of activities of other agents and uncertainty about the outcomes of interacting non-local tasks. In this paper, we present a learning algorithm that endows agents with the capability to choose the appropriate coordination algorithm from a set of available coordination algorithms based on meta-level information about their problem solving situations. We present empirical results that strongly indicate the effectiveness of the learning algorithm.
Abstract-found: 1
Intro-found: 1
Reference: [ Aha, Kibler, & Albert, 1991 ] <author> Aha, D. W.; Kibler, D.; and Albert, M. K. </author> <year> 1991. </year> <title> Instance-based Learning Algorithms. </title> <booktitle> Machine Learning 6 </booktitle> <pages> 37-66. </pages>
Reference-contexts: Learning in COLLAGE (COordination Learner for muLtiple AGEnt systems) falls into the category of Instance-Based Learning algorithms <ref> [ Aha, Kibler, & Albert, 1991 ] </ref> originally proposed for supervised classification learning. We, however, use the IBL-paradigm for unsupervised learning of decision-theoretic choice. Learning involves running the multi-agent system on a large number of training coordination problem instances and observing the performance of different coordination strategies on these instances.
Reference: [ Crites & Barto, 1996 ] <author> Crites, R. H., and Barto, A. G. </author> <year> 1996. </year> <title> Improving elevator performance using reinforcement learning. </title> <booktitle> In Advances in Neural Information Processing Systems 8. </booktitle>
Reference-contexts: We then present some of our experimental results and conclude. 2 Related Work Much of the literature in multi-agent learning relies on reinforcement learning and classifier systems as learning algorithms. In Sen, Sekaran and Hale [ Sen, Sekaran, & Hale, 1994 ] and Crites and Barto <ref> [ Crites & Barto, 1996 ] </ref> , the agents do not communicate with one another and an agent treats the other agents as a part of the environment. Weiss [ Weiss, 1994 ] uses classifier systems for learning appropriate multi-agent hierarchical organization structuring relationships.
Reference: [ Decker & Lesser, 1993 ] <author> Decker, K. S., and Lesser, V. R. </author> <year> 1993. </year> <title> Quantitative modeling of complex computational task environments. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> 217-224. </pages>
Reference-contexts: Despite these limitations, combining their work on learning situation representations with the learning presented here on situation-based choice of coordination could have interesting implications for situation-specific learning. 3 Task Analysis, Environment Modeling, and Simulation 3.1 TAEMS The TMS framework (Task Analysis, Environment Modeling, and Simulation) <ref> [ Decker & Lesser, 1993 ] </ref> represents coordination problems in a formal, domain-independent way. A TMS model of a task environment specifies what actions are available to agents and how those actions relate to one another and to the performance of the system as a whole. <p> A task may have multiple ways to accomplish it, represented by multiple methods, that trade off the time to produce a result for the quality of the result. Besides tasks/subtask relationships, there can be other interrelationships between tasks in a task group <ref> [ Decker & Lesser, 1993 ] </ref> .
Reference: [ Decker & Lesser, 1995 ] <author> Decker, K. S., and Lesser, V. R. </author> <year> 1995. </year> <title> Designing a family of coordination algorithms. </title> <booktitle> In Proceedings of the First International Conference on Multi-Agent Systems, </booktitle> <pages> 73-80. </pages> <address> San Francisco, CA: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: 1 Introduction Coordination is the act of managing interdependencies in a multi-agent system <ref> [ Decker & Lesser, 1995 ] </ref> . Achieving effective coordination in a multi-agent system (MAS) is a difficult problem for a number of reasons. <p> goal of the agents is to work together to produce the highest possible quality for as many task groups as possible. 3.2 Environment-Specific Coordination Mechanisms In order to bring to bear different collections of coordination mechanisms for different multi-agent problem-solving environments, we use the Generalized Partial Global Planning (GPGP) approach <ref> [ Decker & Lesser, 1995 ] </ref> . <p> Rough commitments are a form of tacit social contract between agents about the completion times of their tasks. The latter two coordination strategies are the alternatives normally used in the distributed data processing domain [ Na-gendra Prasad et al., 1996 ] . <ref> [ Decker & Lesser, 1995 ] </ref> proposed balanced as a sophisticated strategy that exploits a number of mechanisms to achieve coordination. 4 COLLAGE: Learning Coordination 4.1 Learning Coordination Our learning algorithm, called COLLAGE, uses abstract meta-level information about coordination problem instances to learn to choose, for the given problem instance, the <p> For example, one global situation vector looks as follows: (0.82 0.77 0.66 0.89 1.0 0.87). Here the low value of the third component represents large quality gains by detecting and coordinating on hard interrelationships. Thus two of the more sophisticated coordination strategies called balanced and tough <ref> [ Decker & Lesser, 1995 ] </ref> are found to be better performers in this situation. <p> Even if the agents use sophisticated strategies to coordinate, they may not have the time to benefit from it. Hence, relatively simple coordination strategies like simple or mute <ref> [ Decker & Lesser, 1995 ] </ref> do better in this scenario. Note, however, that in most situation vectors, these trade offs are subtle and not as obvious as the above examples.
Reference: [ Durfee & Lesser, 1988 ] <author> Durfee, E., and Lesser, V. </author> <year> 1988. </year> <title> Predictability vs. responsiveness: Coordinating problem solvers in dynamic domains. </title> <booktitle> In Proceedings of the Seventh National Conference on Artificial Intelligence, </booktitle> <pages> 66-71. </pages>
Reference: [ Garvey & Lesser, 1993 ] <author> Garvey, A., and Lesser, V. </author> <year> 1993. </year> <title> Design-to-time real-time scheduling. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics 23(6) </journal> <pages> 1491-1502. </pages>
Reference-contexts: A design-to-time scheduling <ref> [ Garvey & Lesser, 1993 ] </ref> algorithm heuristically enumerates a promising subset of quality and time trade-offs to produce schedules that maximize quality given the deadlines.
Reference: [ Gilboa & Schmeidler, 1995 ] <author> Gilboa, I., and Schmeidler, D. </author> <year> 1995. </year> <title> Case-based Decision Theory. </title> <journal> The Quaterly Journal of Economics 605-639. </journal>
Reference-contexts: Hence, hand-coding the strategies by a designer is not a practical alternative. 4.2 Choosing a Coordination Strategy COLLAGE chooses a coordination strategy based on how the set of available strategies performed in similar past cases. We adopt the notation from Gilboa and Schmeidler <ref> [ Gilboa & Schmeidler, 1995 ] </ref> .
Reference: [ Nagendra Prasad et al., 1996 ] <author> Nagendra Prasad, M. V.; Decker, K. S.; Garvey, A.; and Lesser, V. R. </author> <year> 1996. </year> <title> Exploring Organizational Designs with TAEMS: A Case Study of Distributed Data Processing. </title> <booktitle> In Proceedings of the Second International Conference on Multi-Agent Systems. </booktitle> <address> Kyoto, Japan: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: The utility of a is defined as U ( p new ; a) = jM a j hq;a;ri2M a 5 Experiments 5.1 Experiments in the DDP domain Our experiments on learning coordination were conducted in the domain of distributed data processing <ref> [ Nagendra Prasad et al., 1996 ] </ref> . This domain consists of a number of geographically dispersed data processing centers (agents). <p> The processing centers have limited resources to conduct their analysis on the incoming data and they have to do this within certain deadlines. Results of processing data at a center may need to be communicated to other centers due the interrelationships between the tasks at these centers. <ref> [ Nagendra Prasad et al., 1996 ] </ref> developed a graph-grammar-based stochastic task structure description language and generation tool for modeling task structures arising in a domain such as this. They present the results of empirical explorations of the effects of varying deadlines and crisis task group arrival probability. <p> X-axis shows the communication cost and the Y-axis shows the coordination strategy. When not to learn! In order to test COLLAGE on interesting scenarios with a range of characteristics, we created a number of synthetic domain theories using graph grammar formalisms <ref> [ Nagendra Prasad et al., 1996 ] </ref> . Space limitations do not permit us to discuss all the results but we would like to briefly talk about a very interesting result seen in the synthetic grammar G3.
Reference: [ Sandholm & Crites, 1995 ] <author> Sandholm, T., and Crites, R. </author> <year> 1995. </year> <title> Multi-agent reinforcement learning in the repeated prisoner's dilemma. </title> <note> to appear in Biosystems. </note>
Reference-contexts: Weiss [ Weiss, 1994 ] uses classifier systems for learning appropriate multi-agent hierarchical organization structuring relationships. In Tan [ Tan, 1993 ] , the agents share perception information to overcome perceptual limitations or communicate policy functions. In Sandholm and Crites <ref> [ Sandholm & Crites, 1995 ] </ref> the agents are self-interested and an agent is not free to ask for any kind of information from the other agents.
Reference: [ Sen, Sekaran, & Hale, 1994 ] <author> Sen, S.; Sekaran, M.; and Hale, J. </author> <year> 1994. </year> <title> Learning to coordinate without sharing information. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> 426-431. </pages> <address> Seattle, WA: </address> <publisher> AAAI. </publisher>
Reference-contexts: We then present some of our experimental results and conclude. 2 Related Work Much of the literature in multi-agent learning relies on reinforcement learning and classifier systems as learning algorithms. In Sen, Sekaran and Hale <ref> [ Sen, Sekaran, & Hale, 1994 ] </ref> and Crites and Barto [ Crites & Barto, 1996 ] , the agents do not communicate with one another and an agent treats the other agents as a part of the environment.
Reference: [ Sugawara & Lesser, 1993 ] <author> Sugawara, T., and Lesser, V. R. </author> <year> 1993. </year> <title> On-line learning of coordination plans. </title> <booktitle> In Proceedings of the Twelfth International Workshop on Distributed AI. </booktitle>
Reference-contexts: The information shared is weak and they are studied in domains such as predator-prey [ Tan, 1993 ] or blocks world [ Weiss, 1994 ] where the need for sharing meta-level information and situating learning in it is not apparent. Sugawara and Lesser <ref> [ Sugawara & Lesser, 1993 ] </ref> also recognize the need for situation specificity in learning coordination, though they do have the notion of two phase coordination. <p> As the number of coordination alternatives become large in number, the learning phase could become computa-tionally very intensive and the instance-base size could increase enormously with respect to Mode 2. We are looking at how to integrate methods for progressively refining situation vectors such as those in <ref> [ Sugawara & Lesser, 1993 ] </ref> , ways to organize the instance-base to access and detect regions where there is insufficient learning and also ways to do more directed experimentation during learning rather than randomly sampling the problem space. In COLLAGE, all the agents form identical instance-bases.
Reference: [ Tan, 1993 ] <author> Tan, M. </author> <year> 1993. </year> <title> Multi-agent reinforcement learning: Independent vs. </title> <booktitle> cooperative agents. In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> 330-337. </pages>
Reference-contexts: Weiss [ Weiss, 1994 ] uses classifier systems for learning appropriate multi-agent hierarchical organization structuring relationships. In Tan <ref> [ Tan, 1993 ] </ref> , the agents share perception information to overcome perceptual limitations or communicate policy functions. In Sandholm and Crites [ Sandholm & Crites, 1995 ] the agents are self-interested and an agent is not free to ask for any kind of information from the other agents. <p> In cooperative systems, an agent can ask other agents for any information that it deems relevant to appropriately situate its learned local coordination knowledge. Agents sharing perceptual information as in Tan <ref> [ Tan, 1993 ] </ref> or bidding information as in Weiss [ Weiss, 1994 ] do not make explicit the notion of situating the local control knowledge in a more global abstract situation. The information shared is weak and they are studied in domains such as predator-prey [ Tan, 1993 ] or <p> information as in Tan <ref> [ Tan, 1993 ] </ref> or bidding information as in Weiss [ Weiss, 1994 ] do not make explicit the notion of situating the local control knowledge in a more global abstract situation. The information shared is weak and they are studied in domains such as predator-prey [ Tan, 1993 ] or blocks world [ Weiss, 1994 ] where the need for sharing meta-level information and situating learning in it is not apparent.
Reference: [ Weiss, 1994 ] <author> Weiss, G. </author> <year> 1994. </year> <title> Some studies in distributed machine learning and organizational design. </title> <type> Technical Report FKI-189-94, </type> <institution> Institut fur Informatik, TU Munchen. </institution>
Reference-contexts: In Sen, Sekaran and Hale [ Sen, Sekaran, & Hale, 1994 ] and Crites and Barto [ Crites & Barto, 1996 ] , the agents do not communicate with one another and an agent treats the other agents as a part of the environment. Weiss <ref> [ Weiss, 1994 ] </ref> uses classifier systems for learning appropriate multi-agent hierarchical organization structuring relationships. In Tan [ Tan, 1993 ] , the agents share perception information to overcome perceptual limitations or communicate policy functions. <p> In cooperative systems, an agent can ask other agents for any information that it deems relevant to appropriately situate its learned local coordination knowledge. Agents sharing perceptual information as in Tan [ Tan, 1993 ] or bidding information as in Weiss <ref> [ Weiss, 1994 ] </ref> do not make explicit the notion of situating the local control knowledge in a more global abstract situation. The information shared is weak and they are studied in domains such as predator-prey [ Tan, 1993 ] or blocks world [ Weiss, 1994 ] where the need for <p> ] or bidding information as in Weiss <ref> [ Weiss, 1994 ] </ref> do not make explicit the notion of situating the local control knowledge in a more global abstract situation. The information shared is weak and they are studied in domains such as predator-prey [ Tan, 1993 ] or blocks world [ Weiss, 1994 ] where the need for sharing meta-level information and situating learning in it is not apparent. Sugawara and Lesser [ Sugawara & Lesser, 1993 ] also recognize the need for situation specificity in learning coordination, though they do have the notion of two phase coordination.
References-found: 13

