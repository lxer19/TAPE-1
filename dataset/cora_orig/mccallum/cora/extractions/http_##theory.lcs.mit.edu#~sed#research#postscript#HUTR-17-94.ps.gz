URL: http://theory.lcs.mit.edu/~sed/research/postscript/HUTR-17-94.ps.gz
Refering-URL: http://theory.lcs.mit.edu/~sed/research/HUTR-17-94-abs.html
Root-URL: 
Email: net address: jaa@das.harvard.edu  net address: sed@das.harvard.edu  
Title: Improved Noise-Tolerant Learning and Generalized Statistical Queries  
Author: Javed A. Aslam Scott E. Decatur 
Note: Author was supported by Air Force Contract F49620-92-J-0466. Part of this research was conducted while the author was at MIT and supported by DARPA Contract N00014-87-K-825 and by NSF Grant CCR-89-14428. Author's  Author was supported by an NDSEG Doctoral Fellowship and by NSF Grant CCR-92-00884. Author's  
Date: July 1994  
Address: Cambridge, MA 02138  
Affiliation: Aiken Computation Laboratory Harvard University  
Abstract: The statistical query learning model can be viewed as a tool for creating (or demonstrating the existence of) noise-tolerant learning algorithms in the PAC model. The complexity of a statistical query algorithm, in conjunction with the complexity of simulating SQ algorithms in the PAC model with noise, determine the complexity of the noise-tolerant PAC algorithms produced. Although roughly optimal upper bounds have been shown for the complexity of statistical query learning, the corresponding noise-tolerant PAC algorithms are not optimal due to inefficient simulations. In this paper we provide both improved simulations and a new variant of the statistical query model in order to overcome these inefficiencies. We improve the time complexity of the classification noise simulation of statistical query algorithms. Our new simulation has a roughly optimal dependence on the noise rate. We also derive a simpler proof that statistical queries can be simulated in the presence of classification noise. This proof makes fewer assumptions on the queries themselves and therefore allows one to simulate more general types of queries. We also define a new variant of the statistical query model based on relative error, and we show that this variant is more natural and strictly more powerful than the standard additive error model. We demonstrate efficient PAC simulations for algorithms in this new model and give general upper bounds on both learning with relative error statistical queries and PAC simulation. We show that any statistical query algorithm can be simulated in the PAC model with malicious errors in such a way that the resultant PAC algorithm has a roughly optimal tolerable malicious error rate and sample complexity. Finally, we generalize the types of queries allowed in the statistical query model. We discuss the advantages of allowing these generalized queries and show that our results on improved simulations also hold for these queries. This paper is available from the Center for Research in Computing Technology, Division of Applied Sciences, Harvard University as technical report TR-17-94. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Dana Angluin. </author> <title> Computational learning theory: Survey and selected bibliography. </title> <booktitle> In Proceedings of the 24 th Annual ACM Symposium on the Theory of Computing, </booktitle> <year> 1992. </year>
Reference-contexts: Two standard complexity measures studied in the PAC model are sample complexity and time complexity. Efficient PAC learning algorithms have been developed for many function classes <ref> [1] </ref>, and PAC learning continues to be a popular model of machine learning. One criticism of the PAC model is that the data presented to the learner is assumed to be noise-free. <p> with respect to D as follows: STAT (f; D i+1 )[(x; f (x))] = P w j STAT (f; D)[(x; f (x)) ^ w j (x; f (x))] j=0 w j STAT (f; D)[ w j (x; f (x))] In the above equation w i, the values w j 2 <ref> [0; 1] </ref> are known, and P j 1. It is also the case that if the denominator of Equation 8 is less than = (*= log 1 * ), then the query need not be estimated. <p> Let Y be the random variable 1 m i=1 X i . Then for any t &gt; 0, : Lemma 8 (Corollary 5.2 in [15]) Let X 1 ; : : : ; X m be independent, identically distributed ran dom variables taking real values in the range <ref> [0; 1] </ref>. Let Y be the random variable 1 m i=1 X i and p = E (Y ).
Reference: [2] <author> Dana Angluin and Philip Laird. </author> <title> Learning from noisy examples. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 343-370, </pages> <year> 1988. </year>
Reference-contexts: In fact, most of the standard PAC learning algorithms would fail if even a small number of the labelled examples given to the learning algorithm were "noisy." Two popular noise models for both theoretical and experimental research are the classification noise model introduced by Angluin and Laird <ref> [2, 14] </ref> and the malicious error model introduced by Valiant [20] and further studied by Kearns and Li [13]. In the classification noise model, each example received by the learner is mislabelled randomly and independently with some fixed probability. <p> In the malicious error model, an adversary is allowed, with some fixed probability, to substitute a labelled example of his choosing for the labelled example the learner would ordinarily see. While a limited number of efficient PAC algorithms had been developed which tolerate classification noise <ref> [2, 11, 16] </ref>, no general framework for efficient learning 1 in the presence of classification noise was known until Kearns introduced the Statistical Query model [12]. In the SQ model, the example oracle of the standard PAC model is replaced by a statistics oracle. <p> Second, he has shown that calls to the statistics oracle can be simulated (with high probability) by a procedure which draws a sufficiently large sample from a classification noise oracle. An immediate consequence of these two properties is that nearly every PAC learning 1 Angluin and Laird <ref> [2] </ref> introduced a general framework for learning in the presence of classification noise. However, their methods do not yield computationally efficient algorithms in most cases. 1 algorithm can be transformed into one which tolerates arbitrary amounts of classification noise.
Reference: [3] <author> Dana Angluin and Leslie G. Valiant. </author> <title> Fast probabilistic algorithms for Hamiltonian circuits and matchings. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 18(2) </volume> <pages> 155-193, </pages> <month> April </month> <year> 1979. </year>
Reference-contexts: We make use of the following lemmas. Lemma 5 (Chernoff Bounds <ref> [3] </ref>) Let X 1 ; : : : ; X m be independent Bernoulli random variables, each of whose expectation is p. Let Y be the random variable 1 m i X i .
Reference: [4] <author> Javed Aslam and Scott Decatur. </author> <title> General bounds on statistical query learning and PAC learning with noise via hypothesis boosting. </title> <booktitle> In Proceedings of the 34 th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 282-291, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: The complexity of a statistical query algorithm in conjunction with the complexity of simulating SQ algorithms in the various noise models determine the complexity of the noise-tolerant PAC learning algorithms obtained. Kearns [12] has derived bounds on the minimum complexity of SQ algorithms, and Aslam and Decatur <ref> [4] </ref> have demonstrated a general technique for constructing SQ algorithms which are nearly optimal with respect to these bounds. <p> In particular, we show that our simulations can accommodate real-valued and probabilistic statistical queries. Probabilistic queries arise naturally when applying boosting techniques to algorithms which output probabilistic hypotheses <ref> [4] </ref>, while real-valued queries allow an algorithm to query the expected value of a real-valued function of la belled examples. Our results on improved simulations hold for these generalized queries in both the absence and presence of noise. The remainder of the paper is organized as follows. <p> We do so by applying boosting techniques [9, 10, 17] and specifically, these techniques as applied in the statistical query model <ref> [4] </ref>. We first prove some useful lemmas which allow us to decompose relative estimates of ratios and sums. Lemma 3 Let a = b=c where 0 a; b; c 1. <p> Here N 0 , 0 and 0 are the number of queries, worst case relative error, and worst case threshold, respectively, of algorithm A run with a constant accuracy parameter. Note that N 0 , 0 and 0 are independent of *. Proof: Aslam and Decatur <ref> [4] </ref> show that given an SQ learning algorithm A, one can construct a very efficient SQ algorithm by combining the output of O (log 1 * ) runs of A. Each run of A is made with respect to a different distribution and uses accuracy parameter * = 1=4.
Reference: [5] <author> Avrim Blum, Merrick Furst, Jeffery Jackson, Michael Kearns, Yishay Mansour, and Steven Rudich. </author> <title> Weakly learning DNF and characterizing statistical query learning using fourier analysis. </title> <booktitle> In Proceedings of the 26 th Annual ACM Symposium on the Theory of Computing, </booktitle> <month> May </month> <year> 1994. </year>
Reference-contexts: We show that a class is learnable with relative error statistical queries if and only if it is learnable with (standard) additive error statistical queries. Thus, known learnability and hardness results for statistical queries <ref> [5, 12] </ref> also hold in this variant. We demonstrate general bounds on the complexity of SQ learning with relative error statistical queries, and we show that many learning algorithms can naturally be written as highly efficient, relative error SQ algorithms. <p> By the above theorem, these classes are also learnable with relative error statistical queries. In addition, the hardness results of Kearns [12] for learning parity functions and the general hardness results of Blum et al. <ref> [5] </ref> based on Fourier analysis also hold for relative error statistical query learning. One can convert an additive error SQ algorithm to a relative error SQ algorithm in a more efficient way than described in the proof of Theorem 2.
Reference: [6] <author> Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth. </author> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of the ACM, </journal> <volume> 36(4) </volume> <pages> 929-865, </pages> <year> 1989. </year> <month> 18 </month>
Reference-contexts: In fact, the PAC learning algorithms obtained by simulating SQ algorithms in the absence of noise are inefficient when compared to the tight bounds known for noise-free PAC learning <ref> [6, 8] </ref>. These shortcomings could be consequences of either inefficient simulations or a deficiency in the model itself. <p> This simulation uses (1=t 2 fl ) examples. Since 1=t fl = (1=*) for all SQ algorithms [12], this simulation effectively uses (1=* 2 ) examples. However, the *-dependence of the general bound on sample complexity for PAC learning is ~ fi (1=*) <ref> [6, 8] </ref>. This (1=t 2 fl ) = (1=* 2 ) sample complexity results from the worst case assumption that large probabilities may need to be estimated with small additive error.
Reference: [7] <author> Scott Decatur. </author> <title> Statistical queries and faulty PAC oracles. </title> <booktitle> In Proceedings of the Sixth Annual ACM Workshop on Computational Learning Theory, </booktitle> <pages> pages 262-268. </pages> <publisher> ACM Press, </publisher> <month> July </month> <year> 1993. </year>
Reference-contexts: However, their methods do not yield computationally efficient algorithms in most cases. 1 algorithm can be transformed into one which tolerates arbitrary amounts of classification noise. Decatur <ref> [7] </ref> has demonstrated that calls to the statistics oracle can also be simulated (with high probability) by a procedure which draws a sufficiently large sample from a malicious error oracle. The amount of malicious error tolerable in such a simulation is proportional to the tolerance of the SQ algorithm. <p> The dependence on * and b of the required sample complexity is ~ O ( 1 * 2 (12 b ) 2 ). We next consider the simulation of relative error SQ algorithms in the presence of malicious errors. Decatur <ref> [7] </ref> showed that an SQ algorithm can be simulated in the presence of malicious errors with a maximum allowable error rate which depends on t fl , the smallest additive error required by the SQ algorithm. <p> We can simulate these SQ algorithms in the PAC model with malicious errors with roughly optimal malicious error tolerance and sample complexity. Decatur <ref> [7] </ref> gives an algorithm for learning conjunctions which tolerates a malicious error rate independent of the number of irrelevant attributes, thus depending only on the number of relevant attributes and the desired accuracy.
Reference: [8] <author> Andrzej Ehrenfeucht, David Haussler, Michael Kearns, and Leslie Valiant. </author> <title> A general lower bound on the number of examples needed for learning. </title> <journal> Information and Computation, </journal> <volume> 82(3) </volume> <pages> 247-251, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: In spite of this, the robust PAC learning algorithms obtained by simulating SQ algorithms in the presence of noise are inefficient when compared to known lower bounds for PAC learning in the presence of noise <ref> [8, 13, 18] </ref>. In fact, the PAC learning algorithms obtained by simulating SQ algorithms in the absence of noise are inefficient when compared to the tight bounds known for noise-free PAC learning [6, 8]. These shortcomings could be consequences of either inefficient simulations or a deficiency in the model itself. <p> In fact, the PAC learning algorithms obtained by simulating SQ algorithms in the absence of noise are inefficient when compared to the tight bounds known for noise-free PAC learning <ref> [6, 8] </ref>. These shortcomings could be consequences of either inefficient simulations or a deficiency in the model itself. <p> This simulation uses (1=t 2 fl ) examples. Since 1=t fl = (1=*) for all SQ algorithms [12], this simulation effectively uses (1=* 2 ) examples. However, the *-dependence of the general bound on sample complexity for PAC learning is ~ fi (1=*) <ref> [6, 8] </ref>. This (1=t 2 fl ) = (1=* 2 ) sample complexity results from the worst case assumption that large probabilities may need to be estimated with small additive error. <p> Note that we are within logarithmic factors of both the O (*) maximum allowable malicious error rate [13] and the (1=*) lower bound on the sample complexity for noise-free PAC learning <ref> [8] </ref>.
Reference: [9] <author> Yoav Freund. </author> <title> Boosting a weak learning algorithm by majority. </title> <booktitle> In Proceedings of the Third Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 202-216. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: We do so by applying boosting techniques <ref> [9, 10, 17] </ref> and specifically, these techniques as applied in the statistical query model [4]. We first prove some useful lemmas which allow us to decompose relative estimates of ratios and sums. Lemma 3 Let a = b=c where 0 a; b; c 1.
Reference: [10] <author> Yoav Freund. </author> <title> An improved boosting algorithm and its implications on learning complexity. </title> <booktitle> In Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, </booktitle> <pages> pages 391-398. </pages> <publisher> ACM Press, </publisher> <year> 1992. </year>
Reference-contexts: We do so by applying boosting techniques <ref> [9, 10, 17] </ref> and specifically, these techniques as applied in the statistical query model [4]. We first prove some useful lemmas which allow us to decompose relative estimates of ratios and sums. Lemma 3 Let a = b=c where 0 a; b; c 1.
Reference: [11] <author> Sally A. Goldman, Michael J. Kearns, and Robert E. Schapire. </author> <title> On the sample complexity of weak learning. </title> <booktitle> In Proceedings of the Third Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 217-231. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: In the malicious error model, an adversary is allowed, with some fixed probability, to substitute a labelled example of his choosing for the labelled example the learner would ordinarily see. While a limited number of efficient PAC algorithms had been developed which tolerate classification noise <ref> [2, 11, 16] </ref>, no general framework for efficient learning 1 in the presence of classification noise was known until Kearns introduced the Statistical Query model [12]. In the SQ model, the example oracle of the standard PAC model is replaced by a statistics oracle. <p> For additive error, one may consider any interval [a; a + M] and simply translate . 6 A weak learning algorithm is one which outputs an hypothesis whose accuracy is just slightly better than random guessing. 16 ing queries are also be probabilistic. Goldman, Kearns and Schapire <ref> [11] </ref> show that by allowing a weak learning algorithm to output a probabilistic hypothesis, the complexity of learning is reduced. Therefore this this generalization gives the algorithm designer more freedom and power.
Reference: [12] <author> Michael Kearns. </author> <title> Efficient noise-tolerant learning from statistical queries. </title> <booktitle> In Proceedings of the 25 th Annual ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 392-401, </pages> <address> San Diego, </address> <year> 1993. </year>
Reference-contexts: While a limited number of efficient PAC algorithms had been developed which tolerate classification noise [2, 11, 16], no general framework for efficient learning 1 in the presence of classification noise was known until Kearns introduced the Statistical Query model <ref> [12] </ref>. In the SQ model, the example oracle of the standard PAC model is replaced by a statistics oracle. <p> Two standard complexity measures of SQ algorithms are query complexity, the maximum number of statistics required, and tolerance, the minimum additive error required. Kearns <ref> [12] </ref> has demonstrated two important properties of the SQ model which make it worthy of study. <p> The complexity of a statistical query algorithm in conjunction with the complexity of simulating SQ algorithms in the various noise models determine the complexity of the noise-tolerant PAC learning algorithms obtained. Kearns <ref> [12] </ref> has derived bounds on the minimum complexity of SQ algorithms, and Aslam and Decatur [4] have demonstrated a general technique for constructing SQ algorithms which are nearly optimal with respect to these bounds. <p> We show that a class is learnable with relative error statistical queries if and only if it is learnable with (standard) additive error statistical queries. Thus, known learnability and hardness results for statistical queries <ref> [5, 12] </ref> also hold in this variant. We demonstrate general bounds on the complexity of SQ learning with relative error statistical queries, and we show that many learning algorithms can naturally be written as highly efficient, relative error SQ algorithms. <p> Step 3 can be accomplished by a generalization of a technique due to Laird [14]. The sample size required is m 2 = O 1 ffit fl log 1 12 b ) : Since 1=t fl = (1=*) for all SQ algorithms <ref> [12] </ref>, the sample complexity of the overall simulation is O 1 jQj *(12 b ) 2 log log 1 in the case of a finite query space or O VC-dim (Q) t fl (12 b ) + 1 ffi 8 in the case of an infinite query space of finite VC-dimension. <p> The required accuracy of this estimate is specified by the learner in the form of an additive error parameter. The limitation of this model is clearly evident in even the standard, noise-free statistical query simulation <ref> [12] </ref>. This simulation uses (1=t 2 fl ) examples. Since 1=t fl = (1=*) for all SQ algorithms [12], this simulation effectively uses (1=* 2 ) examples. However, the *-dependence of the general bound on sample complexity for PAC learning is ~ fi (1=*) [6, 8]. <p> The limitation of this model is clearly evident in even the standard, noise-free statistical query simulation <ref> [12] </ref>. This simulation uses (1=t 2 fl ) examples. Since 1=t fl = (1=*) for all SQ algorithms [12], this simulation effectively uses (1=* 2 ) examples. However, the *-dependence of the general bound on sample complexity for PAC learning is ~ fi (1=*) [6, 8]. <p> In each direction, the simulation uses polynomially bounded parameters if and only if the original algorithm uses polynomially bounded parameters. 2 Kearns <ref> [12] </ref> shows that almost all classes known to be PAC learnable are learnable with additive error statistical queries. By the above theorem, these classes are also learnable with relative error statistical queries. In addition, the hardness results of Kearns [12] for learning parity functions and the general hardness results of Blum <p> only if the original algorithm uses polynomially bounded parameters. 2 Kearns <ref> [12] </ref> shows that almost all classes known to be PAC learnable are learnable with additive error statistical queries. By the above theorem, these classes are also learnable with relative error statistical queries. In addition, the hardness results of Kearns [12] for learning parity functions and the general hardness results of Blum et al. [5] based on Fourier analysis also hold for relative error statistical query learning. <p> The sample complexity of the standard, noise-free PAC simulation of additive error SQ algorithms depends linearly on 1=t 2 fl <ref> [12] </ref>, while in Section 4.4, we show that the sample complexity of a noise-free PAC simulation of relative error SQ algorithms depends linearly on 1= 2 fl fl . <p> Decatur [7] gives an algorithm for learning conjunctions which tolerates a malicious error rate independent of the number of irrelevant attributes, thus depending only on the number of relevant attributes and the desired accuracy. This algorithm, while reasonably efficient, is based on an additive error SQ algorithm of Kearns <ref> [12] </ref> and therefore does not have an optimal sample complexity. We present an algorithm based on relative error statistical queries which tolerates the same malicious error rate and has a sample complexity whose dependence on * roughly matches the general lower bound for noise-free PAC learning.
Reference: [13] <author> Michael Kearns and Ming Li. </author> <title> Learning in the presence of malicious errors. </title> <booktitle> In Proceedings of the 20 th Annual ACM Symposium on Theory of Computing, </booktitle> <address> Chicago, Illinois, </address> <month> May </month> <year> 1988. </year>
Reference-contexts: number of the labelled examples given to the learning algorithm were "noisy." Two popular noise models for both theoretical and experimental research are the classification noise model introduced by Angluin and Laird [2, 14] and the malicious error model introduced by Valiant [20] and further studied by Kearns and Li <ref> [13] </ref>. In the classification noise model, each example received by the learner is mislabelled randomly and independently with some fixed probability. <p> In spite of this, the robust PAC learning algorithms obtained by simulating SQ algorithms in the presence of noise are inefficient when compared to known lower bounds for PAC learning in the presence of noise <ref> [8, 13, 18] </ref>. In fact, the PAC learning algorithms obtained by simulating SQ algorithms in the absence of noise are inefficient when compared to the tight bounds known for noise-free PAC learning [6, 8]. These shortcomings could be consequences of either inefficient simulations or a deficiency in the model itself. <p> The dependence on * of the maximum allowable error rate is ~ (*), while the dependence on * of the required sample complexity is ~ O (1=*). Note that we are within logarithmic factors of both the O (*) maximum allowable malicious error rate <ref> [13] </ref> and the (1=*) lower bound on the sample complexity for noise-free PAC learning [8].
Reference: [14] <author> Philip D. Laird. </author> <title> Learning from Good and Bad Data. </title> <booktitle> Kluwer international series in engineering and computer science. </booktitle> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, </address> <year> 1988. </year>
Reference-contexts: In fact, most of the standard PAC learning algorithms would fail if even a small number of the labelled examples given to the learning algorithm were "noisy." Two popular noise models for both theoretical and experimental research are the classification noise model introduced by Angluin and Laird <ref> [2, 14] </ref> and the malicious error model introduced by Valiant [20] and further studied by Kearns and Li [13]. In the classification noise model, each example received by the learner is mislabelled randomly and independently with some fixed probability. <p> Draw m 2 samples and test the O ( 1 t fl log 1 12 b ) hypotheses obtained in Step 2. Output one of these hypotheses whose error rate is at most *. Step 3 can be accomplished by a generalization of a technique due to Laird <ref> [14] </ref>. <p> However, in the absence of computational restrictions, all finite concept classes can be learned in the presence of classification noise using a sample complexity which depends linearly on 1=* <ref> [14] </ref>. As discussed in Section 4.5, many classes which are SQ learnable have algorithms with a constant worst case relative error fl . Can one show that all classes which are SQ learnable have algorithms with this property, or instead characterize exactly which classes do?
Reference: [15] <author> Colin McDiarmid. </author> <title> On the method of bounded differences. </title> <editor> In J. Siemons, editor, </editor> <booktitle> Surveys in Combinatorics, </booktitle> <pages> pages 149-188. </pages> <publisher> Cambridge University Press, </publisher> <address> Cambridge, 1989. </address> <publisher> London Mathematical Society LNS 141. </publisher>
Reference-contexts: Note that when the range of the queries is a unit sized interval, i.e. M = 1, these sample complexities and noise tolerances are identical to those for deterministic, f0; 1g-valued queries. Lemma 7 (Lemma 1.2 in <ref> [15] </ref>) Let X 1 ; : : :; X m be independent, identically distributed random variables taking real values in the range [a; a + M ]. Let Y be the random variable 1 m i=1 X i . <p> Let Y be the random variable 1 m i=1 X i . Then for any t &gt; 0, : Lemma 8 (Corollary 5.2 in <ref> [15] </ref>) Let X 1 ; : : : ; X m be independent, identically distributed ran dom variables taking real values in the range [0; 1]. Let Y be the random variable 1 m i=1 X i and p = E (Y ).
Reference: [16] <author> Yasubumi Sakakibara. </author> <title> Algorithmic Learning of Formal Languages and Decision Trees. </title> <type> PhD thesis, </type> <institution> Tokyo Institute of Technology, </institution> <month> October </month> <year> 1991. </year> <institution> (International Institute for Advanced Study of Social Information Science, Fujitsu Laboratories Ltd, </institution> <note> Research Report IIAS-RR-91-22E). </note>
Reference-contexts: In the malicious error model, an adversary is allowed, with some fixed probability, to substitute a labelled example of his choosing for the labelled example the learner would ordinarily see. While a limited number of efficient PAC algorithms had been developed which tolerate classification noise <ref> [2, 11, 16] </ref>, no general framework for efficient learning 1 in the presence of classification noise was known until Kearns introduced the Statistical Query model [12]. In the SQ model, the example oracle of the standard PAC model is replaced by a statistics oracle.
Reference: [17] <author> Robert Schapire. </author> <title> The strength of weak learnability. </title> <journal> Machine Learning, </journal> <volume> 5(2) </volume> <pages> 197-226, </pages> <year> 1990. </year>
Reference-contexts: We do so by applying boosting techniques <ref> [9, 10, 17] </ref> and specifically, these techniques as applied in the statistical query model [4]. We first prove some useful lemmas which allow us to decompose relative estimates of ratios and sums. Lemma 3 Let a = b=c where 0 a; b; c 1.
Reference: [18] <author> Hans Ulrich Simon. </author> <title> General bounds on the number of examples needed for learning probabilistic concepts. </title> <booktitle> In Proceedings of the Sixth Annual ACM Workshop on Computational Learning Theory, </booktitle> <pages> pages 402-411. </pages> <publisher> ACM Press, </publisher> <year> 1993. </year>
Reference-contexts: In spite of this, the robust PAC learning algorithms obtained by simulating SQ algorithms in the presence of noise are inefficient when compared to known lower bounds for PAC learning in the presence of noise <ref> [8, 13, 18] </ref>. In fact, the PAC learning algorithms obtained by simulating SQ algorithms in the absence of noise are inefficient when compared to the tight bounds known for noise-free PAC learning [6, 8]. These shortcomings could be consequences of either inefficient simulations or a deficiency in the model itself. <p> We refer to these algorithms as "dynamic" SQ algorithms. 2 Note that multiple runs of a dynamic SQ algorithm may produce many more queries which need to be estimated. With respect to , Simon <ref> [18] </ref> has shown a sample complexity, and therefore time complexity, lower bound of ( 1 (12) 2 ) for PAC learning in the presence of classification noise.
Reference: [19] <author> Leslie Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <month> November </month> <year> 1984. </year>
Reference-contexts: We begin by introducing the various models of learning required for the exposition that follows. Since Valiant's introduction of the Probably Approximately Correct model of learning <ref> [19] </ref>, PAC learning has proven to be an interesting and well studied model of machine learning. In an instance of PAC learning, a learner is given the task of determining a close approximation of an unknown f0; 1g-valued target function f from labelled examples of that function.
Reference: [20] <author> Leslie Valiant. </author> <title> Learning disjunctions of conjunctions. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <year> 1985, 1985. </year> <month> 19 </month>
Reference-contexts: learning algorithms would fail if even a small number of the labelled examples given to the learning algorithm were "noisy." Two popular noise models for both theoretical and experimental research are the classification noise model introduced by Angluin and Laird [2, 14] and the malicious error model introduced by Valiant <ref> [20] </ref> and further studied by Kearns and Li [13]. In the classification noise model, each example received by the learner is mislabelled randomly and independently with some fixed probability.
References-found: 20

