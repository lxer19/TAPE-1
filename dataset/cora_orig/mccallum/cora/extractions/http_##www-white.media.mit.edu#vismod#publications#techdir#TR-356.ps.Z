URL: http://www-white.media.mit.edu/vismod/publications/techdir/TR-356.ps.Z
Refering-URL: http://www.media.mit.edu/~baback/academic.html
Root-URL: http://www.media.mit.edu
Email: trevor,baback,sandy@media.mit.edu  
Title: Active Face Tracking and Pose Estimation in an Interactive Room  
Author: Trevor Darrell, Baback Moghaddam, and Alex P. Pentland 
Note: (Submitted, CVPR'96.)  
Abstract: M.I.T. Media Laboratory Perceptual Computing Group Technical Report No. 356. Abstract We demonstrate real-time face tracking and pose estimation in an unconstrained office environment with an active foveated camera. Using vision routines previously implemented for an interactive environment, we determine the spatial location of a user's head and guide an active camera to obtain foveated images of the face. Faces are analyzed using a set of eigenspaces indexed over both pose and world location. Closed loop feedback from the estimated facial location is used to guide the camera when a face is present in the foveated view. Our system can detect the head pose of an unconstrained user in real-time as he or she moves about an open room.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Bichsel, M., and Pentland, A., </author> <title> "Human Face Recognition and the Face Image Set's Topology," CVGIP: </title> <booktitle> Image Understanding, </booktitle> <volume> vol. 59, no. 2, </volume> <pages> pp. 254-261, </pages> <year> 1994. </year>
Reference-contexts: In the high-dimensional vector space of an input image, multiple-orientation training images are represented by a set of M distinct regions, each defined by the scatter of N individuals. Multiple views of a face form non-convex (yet connected) regions in image space <ref> [1] </ref>. Therefore the resulting ensemble is a highly complex and non-separable manifold. The difference between the two approaches is illustrated in Figure 5.
Reference: [2] <editor> Burl, M.C., et al., </editor> <booktitle> "Automating the Hunt for Volcanos on Venus", Proc. IEEE Conf. on Computer Vision & Pattern Recognition, </booktitle> <address> Seattle, WA, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: Fortunately, most of the information computed by a brute force evaluation of DFFS is of little importance-what is of interest is the location of the minima of the distance function. Following <ref> [2] </ref>, we use the zero-th order eigenvectors, E0, to perform spatial localization within the foveated camera view. We compute a coarse to fine search using the E0 template for each pose, and find the pose and offset which has maximal normalized correlation response.
Reference: [3] <author> Cover, M. and Thomas, J.A., </author> <title> Elements of Information Theory, </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: The optimal value of can now be determined by minimizing a suitable cost function J (). From an information-theoretic point of view, this cost function should be the Kullback-Leibler divergence <ref> [3] </ref> between the true density P (xj) and its estimate ^ P (xj) " P (xj) # Using the diagonalized forms of the Mahalanobis distance d (x) and its estimate ^ d (x) and the fact that E [y 2 i ] = i , it can be easily shown that
Reference: [4] <author> Darrell, T., and Pentland, A., </author> <title> "Space-Time Gestures," </title> <booktitle> Proc. IEEE Conf. on Computer Vision and Pattern Recognition, </booktitle> <address> New York, NY, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: This scheme can be viewed as a multiple-observer system where separate eigenspaces are simultaneously "competing" in describing the input image (see [12] and <ref> [4] </ref> for related work). Examples of eigenfaces for multiple poses (at the same spatial location) are shown in Figure 4. The key difference between the view-based and parametric representations can be understood by considering the geometry of facespace.
Reference: [5] <author> Darrell, T., Maes, P., Blumberg, B., and Pentland, A. P., </author> <title> "A Novel Environment for Situated Vision and Behavior", </title> <booktitle> Proc. IEEE Wkshp. for Visual Behaviors (CVPR-94), </booktitle> <publisher> IEEE C.S. Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1994 </year>
Reference-contexts: Finally we will show results demonstrating the accuracy of pose estimation in our real-time system. 2 Person Tracking Previously, we have implemented vision routines to track a user in an office setting as part of our ALIVE system, an Artificial Life Interactive Video Environment <ref> [5] </ref>. This system can track people and identify head/hand locations as they walk about a room, and provide foveation cues to guide an active camera to foveate head or hands.
Reference: [6] <author> Loeve, </author> <title> M.M., Probability Theory, </title> <publisher> Van Nostrand, Princeton, </publisher> <year> 1955. </year>
Reference-contexts: The basis functions in a Karhunen-Loeve Transform (KLT) <ref> [6] </ref> are obtained by solving the eigenvalue problem fl = T (4) where is the covariance matrix of the data, is the eigenvector matrix of and fl is the corresponding diagonal matrix of eigenvalues.
Reference: [7] <author> Moghaddam, B. and Pentland, A., </author> <title> "Probabilistic Visual Learning for Object Detection," </title> <booktitle> Proc. of Int'l Conf. on Comp. Vision, Camb., </booktitle> <address> MA, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: The latter (view-based) representation can yield a more accurate representation of the underlying geometry depending on the 6 degree of manifold complexity of the data. 3.1 MAP estimation with Eigenspaces Recently Moghaddam & Pentland <ref> [7] </ref> have shown that the DFFS measure can be combined with a corresponding "distance-in-feature-space" (DIFS) to yield an estimate of the probability density function for a class of images. <p> P ( j jx) &gt; P ( i jx) 8i 6= j (2) using Bayes rule P ( i jx) = n X P (xj j )P ( j ) We now review how an arbitrary density estimate P (xj i ) can be computed using the eigenspace technique of <ref> [7] </ref> specialized to the case of a Gaussian distribution. 3.2 Principal Component Imagery Given a set of m-by-n images fI t g N T t=1 , we can form a training set of vectors fx t g, where x 2 R N=mn , by lexicographic ordering of the pixel elements of
Reference: [8] <author> Murase, H., and Nayar, </author> <title> S.K., "Visual Learning and Recognition of 3D Objects from Appearance," </title> <journal> Int'l Journal of Computer Vision, </journal> <volume> vol. 14, no. 1, </volume> <year> 1995. </year>
Reference-contexts: In this way a single "parametric eigenspace" will encode both identity as well as pose. Such an approach, for example, has recently been used by Murase and Nayar <ref> [8] </ref> for general 3D object recognition and pose estimation. Pentland et al. [9] have suggested a view-based approach to face recognition under varying pose. In this formulation a separate set of "eigenfaces" is computed for each possible object pose.
Reference: [9] <author> Pentland, A., Moghaddam, B. and Starner, T., </author> <title> "View-based and modular eigenspaces for face recognition," </title> <booktitle> Proc. of IEEE Conf. on Computer Vision & Pattern Recognition, </booktitle> <month> June </month> <year> 1994. </year>
Reference-contexts: In this way a single "parametric eigenspace" will encode both identity as well as pose. Such an approach, for example, has recently been used by Murase and Nayar [8] for general 3D object recognition and pose estimation. Pentland et al. <ref> [9] </ref> have suggested a view-based approach to face recognition under varying pose. In this formulation a separate set of "eigenfaces" is computed for each possible object pose. <p> camera, then one could simply scale the 2-D image location of the head in the fixed view to compute a pan and tilt angle for the active camera, 4 as user moved across room and narrow camera tracks head. 5 along with the first 4 eigenvectors (E1 to E4). metric <ref> [9] </ref>). This scheme can be viewed as a multiple-observer system where separate eigenspaces are simultaneously "competing" in describing the input image (see [12] and [4] for related work). Examples of eigenfaces for multiple poses (at the same spatial location) are shown in Figure 4.
Reference: [10] <author> Turk, M., and Pentland, A., </author> <title> "Eigenfaces for Recognition," </title> <journal> Journal of Cognitive Neuroscience, </journal> <volume> Vol. 3, No. 1, </volume> <year> 1991. </year>
Reference: [11] <author> Weng, J.J., </author> <title> "On Comprehensive Visual Learning", </title> <booktitle> Proc. NSF/ARPA Workshop on Performance vs. Methodology in Computer Vision, </booktitle> <address> Seattle, WA, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: The run-time computational burden of having L different world locations each with a separate set of pose templates is k times the cost of a single location, since we need not evaluate the eigenspace likelihoods that for locations that are not in the nearest neighbor set <ref> [11] </ref>. 10 Pose 1 Pose 2 Pose 3 Location 1 Pose 1 Pose 2 Pose 3 Location 2 Pose 1 Pose 2 Pose 3 Location 3 11 (a) (b) moving user who walked across room while oscillating head.
Reference: [12] <author> Wilson, A., and Bobick, A., </author> <title> "Learning visual behavior for gesture analysis", to appear, </title> <booktitle> Proc. International Symposium on Computer Vision, Coral Gables, </booktitle> <month> November </month> <year> 1995. </year>
Reference-contexts: This scheme can be viewed as a multiple-observer system where separate eigenspaces are simultaneously "competing" in describing the input image (see <ref> [12] </ref> and [4] for related work). Examples of eigenfaces for multiple poses (at the same spatial location) are shown in Figure 4. The key difference between the view-based and parametric representations can be understood by considering the geometry of facespace.
Reference: [13] <author> C. Wren, A. Azarbayejani, T. Darrell, and A. Pentland, "Pfinder: </author> <title> Real-Time Tracking of the Human Body", </title> <note> in Proc SPIE Photonics East 1995, also available as MIT Media Lab Perceptual Computing Technical Report TR-353. </note>
Reference-contexts: We use only a single, calibrated, wide field-of-view camera to determine the 3-D position of these features. We do assume a fixed color background, and that the person is facing the camera/screen. For details of our method, see <ref> [13] </ref>; here we summarize the three main steps of the algorithm which are relevant to face tracking: 1. A multi-class color classification test is used to compute figure/ground segmentation, using a single Gaussian model of background pixel color and an n-class adaptive model of foreground (person) colors. 2.
References-found: 13

