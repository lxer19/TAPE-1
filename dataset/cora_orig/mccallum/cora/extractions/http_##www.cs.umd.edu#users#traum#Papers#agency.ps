URL: http://www.cs.umd.edu/users/traum/Papers/agency.ps
Refering-URL: http://www.cs.umd.edu/users/traum/Papers/papers.html
Root-URL: 
Email: traum@cs.umd.edu  
Title: A Reactive-Deliberative Model of Dialogue Agency  
Author: David R. Traum A. V. Williams 
Address: Building College Park, MD 20742 USA  
Affiliation: UMIACS, University of Maryland  
Abstract: For an agent to engage in substantive dialogues with other agents, there are several complexities which go beyond the scope of standard models of rational agency. In particular, an agent must reason about social attitudes that span more than one agent, as well as the dynamic and fallible process of plan execution. In this paper we sketch a theory of plan execution which allows the representation of failure and repair, extend the underlying agency model with social attitudes of mutual belief, obligation, and multi-agent plan execution, and describe an implemented dialogue agent which uses these notions, reacting to its environment and mental state, and deliberating and planning action only when more pressing concerns are absent.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> James F. Allen and C. Raymond Perrault. </author> <title> Analyzing intention in utterances. </title> <journal> Artificial Intelligence, </journal> <volume> 15(3) </volume> <pages> 143-178, </pages> <year> 1980. </year>
Reference-contexts: In [23] we argued that obligations play an important role in accounting for many of the interactions in dialog. For example, Table 1 shows the obligations resulting from the performance of speech acts. Obligations do not replace the plan-based model of speech acts (e.g., <ref> [10, 1] </ref>) but augment it. The resulting model more readily accounts for discourse behavior in adversarial situations and other situations where it is implausible that the agents adopt each others' goals.
Reference: 2. <author> James. F. Allen, L. K. Schubert, G. Ferguson, P. Heeman, C. H. Hwang, T. Kato, M. Light, N. Martin, B. Miller, M. Poesio, and D. R. Traum. </author> <title> The TRAINS project: a case study in building a conversational planning agent. </title> <journal> Journal of Experimental and Theoretical Artificial Intelligence, </journal> <volume> 7 </volume> <pages> 7-48, </pages> <year> 1995. </year>
Reference-contexts: In [21], the grounding acts from [22] were given formal specifications as executions related to conversation plans. 4 An Example Implemented Social Agent The dialogue manager of the TRAINS-93 system <ref> [2, 26] </ref> is implemented as a reactive deliberative agent. A rich model of the mental and conversational state (including private, nested, and mutual beliefs; private, proposed and shared plans; conversational goals, intentions, and obligations) is maintained and updated as the conversation progresses.
Reference: 3. <author> Cecile T. Balkanski. </author> <title> Modelling act-type relations in collaborative activity. </title> <type> Technical Report 23-90, </type> <institution> Harvard University Center for Research in Computing Technology, </institution> <year> 1990. </year>
Reference-contexts: Agents will observe executions and decide whether or not particular actions of interest have been performed. What we call actions are sometimes called action types [12] or activities <ref> [3] </ref>. We use the symbol &gt; to represent the realizes or is characterized by relation between an execution and an action.
Reference: 4. <author> Jon Barwise. </author> <title> The Situation in Logic, chapter 9: On the Model Theory of Common Knowledge. </title> <booktitle> CSLI Lecture Notes: Number 17. Center for The Study of Language and Information, </booktitle> <year> 1989. </year>
Reference-contexts: While it is still somewhat controversial how best to formally model mutual belief (e.g., as a conjunction of nested beliefs or a self-referential attitude, see <ref> [4] </ref>), an interesting question is how such mutual belief gets established among interacting agents.
Reference: 5. <author> Michael E. Bratman, David J. Israel, and Martha E. Pollack. </author> <title> Plans and resource-bounded practical reasoning. </title> <journal> Computational Intelligence, </journal> <volume> 4 </volume> <pages> 349-355, </pages> <year> 1988. </year>
Reference-contexts: She can also observe the situation she finds herself in, perhaps revising her beliefs and desires. She can also replan, revising the plans she is executing to correspond to different 1 The approach presented here was based on the BDI model introduced in <ref> [5] </ref>. 2 Many of these constraints will be implicit in any Representation of a recipe, e.g. as shared variables in two separate actions, or domain constraints on possible recipes. plan recipes, and thus changing her commitments and intentions.
Reference: 6. <author> P. Bretier and M. D. Sadek. </author> <title> A rational agent as the kernel of a cooperative spoken dialogue system: Implementing a logical theory of interaction. </title> <editor> In J. P. Muller, M. J. Wooldridge, and N. R. Jennings, editors, </editor> <booktitle> Intelligent Agents III Proceedings of the Third International Workshop on Agent Theories, Architectures, and Languages (ATAL-96), Lecture Notes in Artificial Intelligence. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Heidelberg, </address> <year> 1996. </year> <note> In this volume. </note>
Reference-contexts: Given this basic framework, the activity itself is up to the agent: observing the success or failure of its actions, deciding to do a new step in the plan, repair an execution, or replan. Rationality principles such as those by by Pollack (presented above), or those used in <ref> [6] </ref> will be important guidelines for a successful agent. However, it is important to distinguish these principles of rationality which guide an agent's deliberation from the definition of the mental attitudes which constitute agency, in order to reason about violations of the rules and irrational agents.
Reference: 7. <author> Herbert H. Clark. </author> <title> Arenas of Language Use. </title> <publisher> University of Chicago Press, </publisher> <year> 1992. </year>
Reference: 8. <author> Herbert H. Clark and Edward F. Schaefer. </author> <title> Contributing to discourse. </title> <journal> Cognitive Science, </journal> <volume> 13 </volume> <pages> 259-294, </pages> <year> 1989. </year> <note> Also appears as Chapter 5 in [7]. </note>
Reference-contexts: Clark and Shaefer call this process of reaching mutual belief (or common ground) grounding <ref> [8] </ref>. They present a descriptive model, in terms of presentation and acceptance phases that allow them to track the augmentation of common ground as the conversation proceeds.
Reference: 9. <author> Phillip R. Cohen and Hector J. Levesque. </author> <booktitle> Confirmations and joint action. In Proceedings IJCAI-91, </booktitle> <pages> pages 951-957, </pages> <year> 1991. </year>
Reference-contexts: Following the initiative of the other can be seen as an obligation-driven process, while leading the conversation will be goal-driven. 3.3 Multi-agent plan execution Another common social attitude is that of Joint intention <ref> [9] </ref> or shared plan [14]. These concepts are used to model the propensity of a collaborative team to act. The intuition here is that it is more than just a collection of individual intentions and beliefs that is responsible for the coordinated teamwork activity. <p> Even with notions of mutual belief of intentions (as in the SharedPlan formalism of [14, 13]), or commitment to inform an agent if an action is performed or impossible (as in the joint intentions of <ref> [9] </ref>), it is hard to see, in practice, what keeps an agent adhering to these commitments when its personal goals diverge. This formulation was also used to formally model the grounding process described in section 3.1, above.
Reference: 10. <author> Phillip R. Cohen and C. R. Perrault. </author> <title> Elements of a plan-based theory of speech acts. </title> <journal> Cognitive Science, </journal> <volume> 3(3) </volume> <pages> 177-212, </pages> <year> 1979. </year>
Reference-contexts: In [23] we argued that obligations play an important role in accounting for many of the interactions in dialog. For example, Table 1 shows the obligations resulting from the performance of speech acts. Obligations do not replace the plan-based model of speech acts (e.g., <ref> [10, 1] </ref>) but augment it. The resulting model more readily accounts for discourse behavior in adversarial situations and other situations where it is implausible that the agents adopt each others' goals.
Reference: 11. <author> F. Dignum and B. van Linder. </author> <title> Modeling social agents: Communication as action. </title> <editor> In J. P. Muller, M. J. Wooldridge, and N. R. Jennings, editors, </editor> <booktitle> Intelligent Agents III Proceedings of the Third International Workshop on Agent Theories, Architectures, and Languages (ATAL-96), Lecture Notes in Artificial Intelligence. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Heidelberg, </address> <year> 1996. </year> <note> In this volume. </note>
Reference-contexts: Obligations are quite different from and cannot be reduced to intentions and goals. In particular, an agent may be obliged to do an action that is contrary to his goals (for example, consider a child who has to apologize for hitting her younger brother). <ref> [11] </ref> use obligations in a similar way, noting also that authority (such as a pre-existing hierarchical relationship) can be important in the ability to force obligations on others. In [23] we argued that obligations play an important role in accounting for many of the interactions in dialog.
Reference: 12. <author> Alvin I. Goldman. </author> <title> A Theory of Human Action. </title> <publisher> Prentice Hall Inc., </publisher> <year> 1970. </year>
Reference-contexts: Agents will observe executions and decide whether or not particular actions of interest have been performed. What we call actions are sometimes called action types <ref> [12] </ref> or activities [3]. We use the symbol &gt; to represent the realizes or is characterized by relation between an execution and an action.
Reference: 13. <author> Barbara [J.] Grosz and Sarit Kraus. </author> <title> Collaborative plans for group activities. </title> <booktitle> In Proceedings IJCAI-93, </booktitle> <pages> pages 367-373, </pages> <year> 1993. </year>
Reference-contexts: We represent commitment as a relation between an agent and a proposition, Committed (Agt; ). The committed relation is similar to the Int.Th operator of <ref> [13] </ref>, while intends is like their Int.To. Finally, we represent present-directed intention as an abstract action in its own right, called try. ff &gt;Try (Agt; a; P) means that the occurrence ff is characterized by Agt attempting to do action a (intentionally) as part of executing Plan P. <p> Even with notions of mutual belief of intentions (as in the SharedPlan formalism of <ref> [14, 13] </ref>), or commitment to inform an agent if an action is performed or impossible (as in the joint intentions of [9]), it is hard to see, in practice, what keeps an agent adhering to these commitments when its personal goals diverge.
Reference: 14. <author> Barbara J. Grosz and Candace L. Sidner. </author> <title> Plans for discourse. </title> <editor> In P. R. Cohen, J. Morgan, and M. E. Pollack, editors, </editor> <title> Intentions in Communication. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: We represent (future-directed) intention as having two parameters other than the agent: an action which the agent intends to perform, and a plan which this action is intended to (in part) achieve Intends (Agt; a; P). This is roughly equivalent to the expression Int (Agt,By (a,P)) used by <ref> [19, 14] </ref>. Commitment is a weaker notion than intention. <p> Following the initiative of the other can be seen as an obligation-driven process, while leading the conversation will be goal-driven. 3.3 Multi-agent plan execution Another common social attitude is that of Joint intention [9] or shared plan <ref> [14] </ref>. These concepts are used to model the propensity of a collaborative team to act. The intuition here is that it is more than just a collection of individual intentions and beliefs that is responsible for the coordinated teamwork activity. <p> Even with notions of mutual belief of intentions (as in the SharedPlan formalism of <ref> [14, 13] </ref>), or commitment to inform an agent if an action is performed or impossible (as in the joint intentions of [9]), it is hard to see, in practice, what keeps an agent adhering to these commitments when its personal goals diverge.
Reference: 15. <author> J. Y. Halpern and Y. Moses. </author> <title> Knowledge and common knowledge in a distributed environment. </title> <journal> Journal of the ACM, </journal> <volume> 37(3) </volume> <pages> 549-587, </pages> <year> 1990. </year>
Reference-contexts: While it has long been known that is impossible to guarantee the establishment of mutual knowledge in an environment where message transmission could fail <ref> [15] </ref>, most formulations of speech acts have gone to the other extreme, and assumed mutual belief after the performance of any utterance within a shared situation (e.g., as the effect of a single agent speech act).
Reference: 16. <author> Jerry Hobbs. </author> <title> Ontological promiscuity. </title> <booktitle> In Proceedings ACL-85, </booktitle> <pages> pages 61-69, </pages> <year> 1985. </year>
Reference-contexts: Plan Recipe for Communication (Recipe CR) The acts of presenting and acknowledging the content are broken into some indeterminate number of conceptual sub-acts, about at the granularity of the propositions in <ref> [16] </ref>. The only constraints on performance of these is that for a particular piece of the content, the presentation must come before the acknowledgment, and both must occur to achieve mutual belief.
Reference: 17. <author> L. Thorne McCarty. Permissions and obligations: </author> <title> An informal introduction. </title> <type> Technical Report LRP-TR-19, </type> <institution> Dept. of Computer Science, Rutgers University, </institution> <year> 1986. </year>
Reference-contexts: Obligations represent what an agent should do, according to some set of norms; its formal aspects are examined using Deontic Logic (e.g., <ref> [27, 17] </ref>). Generally, obligation is defined in terms of a modal operator often called permissible. An action is obligatory if it is not permissible not to do it. An action is forbidden if it is not permissible.
Reference: 18. <author> Martha E. Pollack. </author> <title> Plans as complex mental attitudes. </title> <editor> In P. R. Cohen, J. Morgan, and M. E. Pollack, editors, </editor> <title> Intentions in Communication. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: From the point at which a plan is adopted to the point at which the intentions and commitments are dropped, we say that the agent is executing the plan. Plan execution is a generalization of Pollack's notion of an agent having a plan <ref> [19, 18] </ref>, to the case in which the agent is in the midst of execution. This has important consequences for the component mental attitudes, since, e.g., an agent will no longer intend to perform an action it has already completed.
Reference: 19. <author> Martha E. Pollack. </author> <title> Inferring Domain Plans in Question-Answering. </title> <type> PhD thesis, </type> <institution> Department of Computer and Information Science, University of Pennsylvania, </institution> <month> May </month> <year> 1986. </year>
Reference-contexts: We represent (future-directed) intention as having two parameters other than the agent: an action which the agent intends to perform, and a plan which this action is intended to (in part) achieve Intends (Agt; a; P). This is roughly equivalent to the expression Int (Agt,By (a,P)) used by <ref> [19, 14] </ref>. Commitment is a weaker notion than intention. <p> From the point at which a plan is adopted to the point at which the intentions and commitments are dropped, we say that the agent is executing the plan. Plan execution is a generalization of Pollack's notion of an agent having a plan <ref> [19, 18] </ref>, to the case in which the agent is in the midst of execution. This has important consequences for the component mental attitudes, since, e.g., an agent will no longer intend to perform an action it has already completed.
Reference: 20. <author> Yoav Shoham and Moshe Tennenholtz. </author> <title> On the synthesis of useful social laws for artificial agent societies. </title> <booktitle> In Proceedings AAAI-92, </booktitle> <pages> pages 276-281, </pages> <year> 1992. </year>
Reference-contexts: An action is forbidden if it is not permissible. Just because an action is obligatory with respect to a set of rules R does not mean that the agent will perform the action. So we do not adopt the model suggested by <ref> [20] </ref> in which agents' behavior cannot violate the defined social laws. If an obligation is not satisfied, then this means that one of the rules must have been broken.
Reference: 21. <author> David R. Traum. </author> <title> A Computational Theory of Grounding in Natural Language Conversation. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Rochester, </institution> <year> 1994. </year> <note> Also available as TR 545, </note> <institution> Department of Computer Science, University of Rochester. </institution>
Reference-contexts: the agent's behavior through the deliberative process in a local, step-by-step manner which is sensitive to the changing conditions as the dialogue progresses. 2 Aspects of Individual Agency The approach to agency is informally sketched below. 1 A more full exposition in terms of a situation logic was presented in <ref> [24, 21] </ref>. The attitudes of belief and desire form the starting points for reasoning and acting. From the desires (and with reference to beliefs and other goals and intentions), the agent will deliberate and choose a set of goals: conditions that the agent will try to achieve. <p> Their model is not well-suited for an on-line agent involved in dialogue, however, since it requires examination of subsequent spans of text in order to determine the boundaries of these phases. In <ref> [22, 21] </ref>, we developed a computational account of the grounding process. This account was based on speech act theory, using actions to introduce, acknowledge, and repair material. <p> These concepts are used to model the propensity of a collaborative team to act. The intuition here is that it is more than just a collection of individual intentions and beliefs that is responsible for the coordinated teamwork activity. In <ref> [21] </ref>, we developed a similar notion, that of agents executing a multi-agent plan. This is an extension of the notion of executing a plan described in section 2, above. A group of agents fA i g is executing a multi-agent plan MP iff: 1. <p> For any given execution, some parts of the content will be expressed explicitly as part of the compositional conventional meaning of the utterance, and others will be presented implicitly by conventions of situated meaning and Gricean implicatures. In <ref> [21] </ref>, the grounding acts from [22] were given formal specifications as executions related to conversation plans. 4 An Example Implemented Social Agent The dialogue manager of the TRAINS-93 system [2, 26] is implemented as a reactive deliberative agent. <p> More extended examples are presented in <ref> [23, 21] </ref>. The example starts with a declarative utterance by the User: U: There are oranges at Corning. At the core speech act level, this is interpreted as initiating both an inform (about the location of oranges), and a suggestion that the oranges be used in the current plan.
Reference: 22. <author> David R. Traum and James F. Allen. </author> <title> A speech acts approach to grounding in conversation. </title> <booktitle> In Proceedings 2nd International Conference on Spoken Language Processing (ICSLP-92), </booktitle> <pages> pages 137-40, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: Their model is not well-suited for an on-line agent involved in dialogue, however, since it requires examination of subsequent spans of text in order to determine the boundaries of these phases. In <ref> [22, 21] </ref>, we developed a computational account of the grounding process. This account was based on speech act theory, using actions to introduce, acknowledge, and repair material. <p> For any given execution, some parts of the content will be expressed explicitly as part of the compositional conventional meaning of the utterance, and others will be presented implicitly by conventions of situated meaning and Gricean implicatures. In [21], the grounding acts from <ref> [22] </ref> were given formal specifications as executions related to conversation plans. 4 An Example Implemented Social Agent The dialogue manager of the TRAINS-93 system [2, 26] is implemented as a reactive deliberative agent.
Reference: 23. <author> David R. Traum and James F. Allen. </author> <title> Discourse obligations in dialogue processing. </title> <booktitle> In Proceedings of the 32 th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 1-8, </pages> <year> 1994. </year>
Reference-contexts: be performed in order to have a grounded DU (which would thus realize the effects of the constituent core speech acts). 3.2 Obligations We claim that Obligations are necessary for modeling many social situations including natural language conversation, e.g., for capturing the effects of some speech acts, such as requests <ref> [23] </ref>. Obligations represent what an agent should do, according to some set of norms; its formal aspects are examined using Deontic Logic (e.g., [27, 17]). Generally, obligation is defined in terms of a modal operator often called permissible. <p> In <ref> [23] </ref> we argued that obligations play an important role in accounting for many of the interactions in dialog. For example, Table 1 shows the obligations resulting from the performance of speech acts. Obligations do not replace the plan-based model of speech acts (e.g., [10, 1]) but augment it. <p> More extended examples are presented in <ref> [23, 21] </ref>. The example starts with a declarative utterance by the User: U: There are oranges at Corning. At the core speech act level, this is interpreted as initiating both an inform (about the location of oranges), and a suggestion that the oranges be used in the current plan.
Reference: 24. <author> David R. Traum and James F. Allen. </author> <title> Towards a formal theory of repair in plan execution and plan recognition. </title> <booktitle> In Proceedings of the 13th Workshop of the UK Planning and Scheduling Special Interest Group, </booktitle> <month> September </month> <year> 1994. </year>
Reference-contexts: the agent's behavior through the deliberative process in a local, step-by-step manner which is sensitive to the changing conditions as the dialogue progresses. 2 Aspects of Individual Agency The approach to agency is informally sketched below. 1 A more full exposition in terms of a situation logic was presented in <ref> [24, 21] </ref>. The attitudes of belief and desire form the starting points for reasoning and acting. From the desires (and with reference to beliefs and other goals and intentions), the agent will deliberate and choose a set of goals: conditions that the agent will try to achieve.
Reference: 25. <author> David R. Traum and Elizabeth A. Hinkelman. </author> <title> Conversation acts in task-oriented spoken dialogue. </title> <journal> Computational Intelligence, </journal> <volume> 8(3) </volume> <pages> 575-599, </pages> <year> 1992. </year> <note> Special Issue on Non-literal language. </note>
Reference-contexts: These DUs are built up by single-utterance grounding acts. Recognizing the fact that multiple types of action occur in conversation, we extended speech act theory to the multi-level conversation act theory, described in <ref> [25] </ref>. As well as the grounding and core speech acts, there are also levels to model turn-taking behavior and higher order coherence of dialogue. A finite automaton was used to track the state of a DU, given a sequence of grounding acts in conversation.
Reference: 26. <author> David R. Traum, L. K. Schubert, M. Poesio, N. G. Martin, M. Light, C. H. Hwang, P. Heeman, G. Ferguson, and J. F. Allen. </author> <title> Knowledge representation in the TRAINS-93 conversation system. </title> <journal> International Journal of Expert Systems, </journal> <note> to appear 1996. </note>
Reference-contexts: In [21], the grounding acts from [22] were given formal specifications as executions related to conversation plans. 4 An Example Implemented Social Agent The dialogue manager of the TRAINS-93 system <ref> [2, 26] </ref> is implemented as a reactive deliberative agent. A rich model of the mental and conversational state (including private, nested, and mutual beliefs; private, proposed and shared plans; conversational goals, intentions, and obligations) is maintained and updated as the conversation progresses.
Reference: 27. <author> G. H. von Wright. </author> <title> Deontic logic. </title> <journal> Mind, </journal> <volume> 60 </volume> <pages> 1-15, </pages> <year> 1951. </year> <title> This article was processed using the L A T E X macro package with LLNCS style </title>
Reference-contexts: Obligations represent what an agent should do, according to some set of norms; its formal aspects are examined using Deontic Logic (e.g., <ref> [27, 17] </ref>). Generally, obligation is defined in terms of a modal operator often called permissible. An action is obligatory if it is not permissible not to do it. An action is forbidden if it is not permissible.
References-found: 27

