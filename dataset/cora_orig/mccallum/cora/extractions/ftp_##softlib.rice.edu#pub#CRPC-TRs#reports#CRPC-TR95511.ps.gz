URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR95511.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Email: bischof@mcs.anl.gov  
Title: Automatic Differentiation, Tangent Linear Models,  
Author: and (Pseudo)Adjoints Christian H. Bischof 
Keyword: Automatic Differentiation, Adjoint, Tangent Linear Model, MM5, ADIFOR, SparsLinC, Data Assimilation.  
Address: 9700 S. Cass Avenue, Argonne, IL 60439-4843  
Affiliation: Mathematics and Computer Science Division Argonne National Laboratory  
Abstract: This paper provides a brief introduction to automatic differentiation and relates it to the tangent linear model and adjoint approaches commonly used in meteorology. After a brief review of the forward and reverse mode of automatic differentiation, the ADIFOR automatic differentiation tool is introduced, and initial results of a sensitivity-enhanced version of the MM5 PSU/NCAR mesoscale weather model are presented. We also present a novel approach to the computation of gradients that uses a reverse mode approach at the time loop level and a forward mode approach at every time step. The resulting "pseudoadjoint" shares the characteristic of an adjoint code that the ratio of gradient to function evaluation does not depend on the number of independent variables. In contrast to a true adjoint approach, however, the nonlinearity of the model plays no role in the complexity of the derivative code. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Brett Averick, Jorge More, Christian Bischof, Alan Carle, and Andreas Griewank. </author> <title> Computing large sparse Jacobian matrices using automatic differentiation. </title> <journal> SIAM Journal on Scientific Computing, </journal> <volume> 15(2) </volume> <pages> 285-294, </pages> <year> 1994. </year>
Reference-contexts: For most grid problems, the width p of the compressed Jacobian is independent of the problem size and depends only on the local stencil chosen. Experimental results with this approach in computing large sparse Jacobians as they arise in large-scale nonlinear equations have been reported in <ref> [1] </ref>. This compressed Jacobian approach is also applicable to the computation of gradients of so-called partially separable functions [19], which are functions f that can be represented in the form f (x) = i=1 where each of the component functions f i has limited support.
Reference: [2] <author> Christian Bischof, Alan Carle, George Corliss, Andreas Griewank, and Paul Hovland. ADIFOR: </author> <title> Generating derivative codes from Fortran programs. </title> <journal> Scientific Programming, </journal> <volume> 1(1) </volume> <pages> 11-29, </pages> <year> 1992. </year>
Reference-contexts: The interpretation overhead 8 associated with using this trace for the purposes of automatic differentiation and its potentially very large size can be a serious computational bottleneck [31]. 4.1 The ADIFOR (Automatic Differentiation of Fortran) Tool Recently, a "source transformation" approach to automatic differentiation has been explored in the ADIFOR <ref> [2] </ref>, ADIC [7], and Odyssee [29, 30] tools. ADIFOR and Odyssee transform Fortran 77 code and ADIC transforms ANSI-C code. By applying the rules of automatic differentiation, these tools generate new code, which, when executed, computes derivatives without the overhead associated with trace interpretation schemes.
Reference: [3] <author> Christian Bischof, Alan Carle, and Peyvand Khademi. </author> <title> Fortran 77 interface specification to the SparsLinC library. </title> <type> Technical Report ANL/MCS-TM-196, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1994. </year>
Reference-contexts: Also note that the sparsity structure of J or rf i is computed as a byproduct of the derivative computation. The SparsLinC (Sparse Linear Combination) Library <ref> [3, 8] </ref> addresses the scenario where p is large and most of the vectors involved in vector linear combination are sparse. It provides support for sparse vector linear combination, in a fashion that is well suited to the use of this operation in the context of automatic differentiation.
Reference: [4] <author> Christian Bischof, George Corliss, and Andreas Griewank. </author> <title> Computing second- and higher-order derivatives through univariate Taylor series. </title> <journal> Optimization Methods and Software, </journal> <volume> 2 </volume> <pages> 211-232, </pages> <year> 1993. </year>
Reference-contexts: rx (i) = d (i), we compute the directional derivative d y fl d = lim y (x + h fl d) y (x) : (13) Forward mode code is easy to generate, preserves any parallelizable or vectorizable structures within the original code, and is readily generalized to higher-order derivatives <ref> [4] </ref>. If we wish to compute m directional derivatives, then running forward-mode code requires at most on the order of m times as much time and memory as the original code.
Reference: [5] <author> Christian Bischof, Larry Green, Kitty Haigler, and Tim Knauff. </author> <title> Parallel calculation of sensitivity derivatives for aircraft design using automatic differentiation. </title> <booktitle> In Proceedings of the 5th AIAA/NASA/USAF/ISSMO Symposium on Multidisciplinary Analysis and Optimization, </booktitle> <pages> pages 73-84. </pages> <institution> American Institute of Aeronautics and Astronautics, </institution> <year> 1994. </year> <pages> AIAA 94-4261. </pages>
Reference-contexts: Thus, if S is the identity, ADIFOR computes the full Jacobian; whereas if S is just a vector, ADIFOR computes the product of the Jacobian by a vector. In <ref> [5] </ref> the flexibility of the ADIFOR interface is exploited in a "stripmining" approach to decrease turnaround time for derivative computations by spawning several independent subprocesses computing parts of the desired gradient or Jacobian.
Reference: [6] <author> Christian Bischof and Paul Hovland. </author> <title> Using ADIFOR to compute dense and sparse Jacobians. </title> <type> Technical Report ANL/MCS-TM-158, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1991. </year>
Reference-contexts: Thus, this approach is more efficient than the normal forward mode or a divided-difference approximation when more than a few derivatives are computed at the same time. We also see that ADIFOR-generated code provides the directional derivative computation possibilities associated with the forward mode of automatic differentiation <ref> [6] </ref>.
Reference: [7] <author> Christian Bischof and Andrew Mauer. </author> <title> ADIC a tool for the automatic differentiation of C programs. Unpublished Information, </title> <institution> Argonne National Laboratory, </institution> <year> 1994. </year>
Reference-contexts: overhead 8 associated with using this trace for the purposes of automatic differentiation and its potentially very large size can be a serious computational bottleneck [31]. 4.1 The ADIFOR (Automatic Differentiation of Fortran) Tool Recently, a "source transformation" approach to automatic differentiation has been explored in the ADIFOR [2], ADIC <ref> [7] </ref>, and Odyssee [29, 30] tools. ADIFOR and Odyssee transform Fortran 77 code and ADIC transforms ANSI-C code. By applying the rules of automatic differentiation, these tools generate new code, which, when executed, computes derivatives without the overhead associated with trace interpretation schemes.
Reference: [8] <author> Christian H. Bischof. </author> <title> Automatic differentiation, tangent linear models and pseudo-adjoints. </title> <type> Preprint MCS-P472-1094, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1994. </year>
Reference-contexts: (0) = e j and ffiX (T ) T implies 8 1 t T : ffiX (t) T fl ffiX (t) = e T i @ X (0) @ X i (T ) : Hence, combining TLM and adjoint codes, we have many possibilities for computing the same derivative values <ref> [8] </ref>. <p> The seed matrix also provides a powerful mechanism for decreasing the computational complexity of derivative codes through judicious use of the chain rule <ref> [8, 21] </ref>. The running time and storage requirements of the ADIFOR-generated code are roughly proportional to the numbers of columns of S, which equals the g$p$ variable in the sample code above. ADIFOR has been successfully applied to codes from various domains of science. <p> Also note that the sparsity structure of J or rf i is computed as a byproduct of the derivative computation. The SparsLinC (Sparse Linear Combination) Library <ref> [3, 8] </ref> addresses the scenario where p is large and most of the vectors involved in vector linear combination are sparse. It provides support for sparse vector linear combination, in a fashion that is well suited to the use of this operation in the context of automatic differentiation.
Reference: [9] <author> Christian H. Bischof and Moe El-Khadiri. </author> <title> Extending compile-time reverse mode and exploiting partial separability in ADIFOR. </title> <type> Technical Report ANL/MCS-TM-163, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1992. </year>
Reference-contexts: It can be shown [19] that any function with a sparse Hessian is a partially separable one. The computation of the gradient of a partially separable function can be reduced to the problem of computing a sparse Jacobian <ref> [9] </ref> by realizing that the gradient of f can easily be obtained by summing the rows of the sparse Jacobian d G d x , where G (x) = B f 1 (x) f np (x) C Another approach is based on the realization that the workhorse of any mainly forward-mode
Reference: [10] <author> Daewon W. Byun, Robert Dennis, Dongming Hwang, Jr. Carlie Coats, and M. Talat Odman. </author> <title> Computational modelling issues in next generation air quality models. </title> <booktitle> In Proceedings of IMACS'94, </booktitle> <address> Atlanta, Georgia, </address> <year> 1994. </year>
Reference-contexts: ADIFOR has been successfully applied to codes from various domains of science. Experiences with meteorological codes, for example, have been reported in <ref> [10, 25, 26, 27] </ref>. Typically, ADIFOR-generated code runs two to four times faster than one-sided divided difference approximations when one computes more than 5-10 derivatives at one time.
Reference: [11] <author> D. G. Cacuci. </author> <title> Sensitivity theory for nonlinear systems, I: Nonlinear functional analysis approach. </title> <journal> Journal of Mathematical Physics, </journal> <volume> 22(12) </volume> <pages> 2794-2802, </pages> <year> 1981. </year>
Reference-contexts: derivatives of intermediate 4 y (1) = 1.0 do i = 1,n if (x (i) &gt; 0.0) then y (1) = y (1) 3 x (i) else y (2) = y (2) 3 x (i) endif enddo variables with respect to the independent variables, corresponding to the forward sensitivity formalism <ref> [11, 12] </ref>, whereas the reverse mode propagates the derivatives of the final values with respect to intermediate variables corresponding to the adjoint sensitivity formalism [11, 12]. In either case, automatic differentiation produces code that computes the values of the analytical derivatives accurate to machine precision. <p> x (i) else y (2) = y (2) 3 x (i) endif enddo variables with respect to the independent variables, corresponding to the forward sensitivity formalism <ref> [11, 12] </ref>, whereas the reverse mode propagates the derivatives of the final values with respect to intermediate variables corresponding to the adjoint sensitivity formalism [11, 12]. In either case, automatic differentiation produces code that computes the values of the analytical derivatives accurate to machine precision.
Reference: [12] <author> D. G. Cacuci. </author> <title> Sensitivity theory for nonlinear systems, II: Extension to additional classes of responses. </title> <journal> Journal of Mathematical Physics, </journal> <volume> 22(12) </volume> <pages> 2803-2812, </pages> <year> 1981. </year>
Reference-contexts: derivatives of intermediate 4 y (1) = 1.0 do i = 1,n if (x (i) &gt; 0.0) then y (1) = y (1) 3 x (i) else y (2) = y (2) 3 x (i) endif enddo variables with respect to the independent variables, corresponding to the forward sensitivity formalism <ref> [11, 12] </ref>, whereas the reverse mode propagates the derivatives of the final values with respect to intermediate variables corresponding to the adjoint sensitivity formalism [11, 12]. In either case, automatic differentiation produces code that computes the values of the analytical derivatives accurate to machine precision. <p> x (i) else y (2) = y (2) 3 x (i) endif enddo variables with respect to the independent variables, corresponding to the forward sensitivity formalism <ref> [11, 12] </ref>, whereas the reverse mode propagates the derivatives of the final values with respect to intermediate variables corresponding to the adjoint sensitivity formalism [11, 12]. In either case, automatic differentiation produces code that computes the values of the analytical derivatives accurate to machine precision.
Reference: [13] <author> Phillip E. Gill, Walter Murray, and Margaret H. Wright. </author> <title> Practical Optimization. </title> <publisher> Academic Press, </publisher> <address> London, </address> <year> 1981. </year> <month> 20 </month>
Reference-contexts: Divided difference approximations have the advantage that we need only the function as a "black box." A disadvantage, however, is that their accuracy is hard to assess (see, e.g., <ref> [13] </ref>). Symbolic Differentiation: Symbolic manipulators like Maple, Macsyma, or Reduce provide powerful capabilities for manipulating algebraic expressions but are, in general, unable to deal with constructs such as branches, loops, or subroutines that are inherent in computer codes.
Reference: [14] <author> G. A. Grell, J. Dudhia, and D. R. Stauffer. </author> <title> A description of the fifth-generation Penn State/NCAR mesoscale weather model (MM5). </title> <type> Technical Report NCAR/TN-398+STR, </type> <institution> National Center for Atmospheric Research, Boulder, Colorado, </institution> <month> June </month> <year> 1994. </year>
Reference-contexts: directional derivatives at the same time | so the ADIFOR-generated code may require significantly more memory than the original simulation code. 4.2 First Results with a Sensitivity-Enhanced Version of the MM5 Meso scale Weather Model The development of a sensitivity-enhanced version of the Fifth-Generation Penn State/NCAR mesoscale weather model (MM5) <ref> [14] </ref> using the ADIFOR automatic differentiation tool is in progress. ADIFOR expects code that complies with the Fortran 77 standard.
Reference: [15] <author> Andreas Griewank. </author> <title> The chain rule revisited in scientific computing. </title> <type> Preprint MCS-P227-0491, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1991. </year>
Reference-contexts: Automatic Differentiation: Automatic differentiation techniques rely on the fact that every function, no matter how complicated, is executed on a computer as a (potentially very long) sequence of elementary operations such as additions, multiplications, and elementary functions such as sin and cos (see, for example, <ref> [15, 28] </ref>). By applying the chain rule @ f (g (t)) fi t=t 0 @s fi fi @t fi fi over and over again to the composition of those elementary operations, one can compute, in a completely mechanical fashion, derivatives of F that are correct up to machine precision [18].
Reference: [16] <author> Andreas Griewank. </author> <title> Achieving logarithmic growth of temporal and spatial complexity in reverse automatic differentiation. </title> <journal> Optimization Methods and Software, </journal> <volume> 1(1) </volume> <pages> 35-54, </pages> <year> 1992. </year>
Reference-contexts: is of the order T fl runtime ( @ H ) + T fl runtime (H) + T fl runtime (sparse matrix-vector multiply) and storage complexity that is of the order storage ( @ H ) + T fl storage (X): When we employ the snapshotting scheme proposed by Griewank <ref> [16] </ref> to regenerate the X (i) from a series of checkpoints, the time complexity of the latter approach becomes of the order T fl runtime ( @ H ) + T fl (1 + log (T )) fl runtime (H) + T fl runtime (sparse matrix-vector multiply) and storage complexity is
Reference: [17] <author> Andreas Griewank, David Juedes, and Jay Srinivasan. ADOL-C, </author> <title> a package for the automatic differentiation of algorithms written in C/C++. </title> <type> Preprint MCS-P180-1190, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1990. </year>
Reference-contexts: In particular, we mention GRESS [20], and PADRE-2 [23] for Fortran Programs and ADOL-C <ref> [17] </ref> for C programs. GRESS, PADRE-2, and ADOL-C implement both the forward and reverse mode. To save control flow information and intermediate values, these tools generate a "trace" of the computation by writing down the particulars of every operation performed in the code.
Reference: [18] <author> Andreas Griewank and Shawn Reese. </author> <title> On the calculation of Jacobian matrices by the Markowitz rule. </title> <editor> In Andreas Griewank and George F. Corliss, editors, </editor> <title> Automatic Differentiation of Algorithms: Theory, </title> <booktitle> Implementation, and Application, </booktitle> <pages> pages 126-135. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1991. </year>
Reference-contexts: By applying the chain rule @ f (g (t)) fi t=t 0 @s fi fi @t fi fi over and over again to the composition of those elementary operations, one can compute, in a completely mechanical fashion, derivatives of F that are correct up to machine precision <ref> [18] </ref>. The techniques of automatic differentiation are directly applicable to computer programs of arbitrary length containing branches, loops, and subroutines. <p> Let s denote the adjoint of a particular variable s. As a consequence of the chain rule it can be shown (see, for example, <ref> [18] </ref>) that the statement s = f (v; w) in the original code implies v + = @ s w + = @ s 6 y (1) = 1.0; y (2) = 1.0; y1value (0) = y (1); c1 = 0; y2value (0) = y (2); c2 = 0; do i
Reference: [19] <author> Andreas Griewank and Philippe L. Toint. </author> <title> On the unconstrained optimization of partially separable objective functions. </title> <editor> In M.J.D. Powell, editor, </editor> <booktitle> Nonlinear Optimization 1981, </booktitle> <pages> pages 301-312, </pages> <address> London, 1981. </address> <publisher> Academic Press. </publisher>
Reference-contexts: Experimental results with this approach in computing large sparse Jacobians as they arise in large-scale nonlinear equations have been reported in [1]. This compressed Jacobian approach is also applicable to the computation of gradients of so-called partially separable functions <ref> [19] </ref>, which are functions f that can be represented in the form f (x) = i=1 where each of the component functions f i has limited support. Hence, the gradients rf i are sparse, even though the final gradient rf is dense. It can be shown [19] that any function with <p> so-called partially separable functions <ref> [19] </ref>, which are functions f that can be represented in the form f (x) = i=1 where each of the component functions f i has limited support. Hence, the gradients rf i are sparse, even though the final gradient rf is dense. It can be shown [19] that any function with a sparse Hessian is a partially separable one.
Reference: [20] <author> Jim E. Horwedel. GRESS: </author> <title> A preprocessor for sensitivity studies on Fortran programs. </title> <editor> In An-dreas Griewank and George F. Corliss, editors, </editor> <title> Automatic Differentiation of Algorithms: Theory, </title> <booktitle> Implementation, and Application, </booktitle> <pages> pages 243-250. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1991. </year>
Reference-contexts: In particular, we mention GRESS <ref> [20] </ref>, and PADRE-2 [23] for Fortran Programs and ADOL-C [17] for C programs. GRESS, PADRE-2, and ADOL-C implement both the forward and reverse mode.
Reference: [21] <author> Paul Hovland, Christian Bischof, Donna Spiegelman, and Mario Casella. </author> <title> Efficient derivative codes through automatic differentiation and interface contraction and an application in biostatistics. </title> <note> in preparation. </note>
Reference-contexts: The seed matrix also provides a powerful mechanism for decreasing the computational complexity of derivative codes through judicious use of the chain rule <ref> [8, 21] </ref>. The running time and storage requirements of the ADIFOR-generated code are roughly proportional to the numbers of columns of S, which equals the g$p$ variable in the sample code above. ADIFOR has been successfully applied to codes from various domains of science.
Reference: [22] <author> David Juedes. </author> <title> A taxonomy of automatic differentiation tools. </title> <editor> In Andreas Griewank and George Corliss, editors, </editor> <booktitle> Proceedings of the Workshop on Automatic Differentiation of Algorithms: Theory, Implementation, and Application, </booktitle> <pages> pages 315-330, </pages> <address> Philadelphia, </address> <year> 1991. </year> <note> SIAM. </note>
Reference-contexts: (10) when X i (T ) = ffiX i (T ) ; i = 1; : : :; n: 4 The ADIFOR Automatic Differentiation Tool and an Ap plication to the MM5 Mesoscale Weather Model There have been various implementations of automatic differentiation; an extensive survey can be found in <ref> [22] </ref>. In particular, we mention GRESS [20], and PADRE-2 [23] for Fortran Programs and ADOL-C [17] for C programs. GRESS, PADRE-2, and ADOL-C implement both the forward and reverse mode.
Reference: [23] <author> Koichi Kubota. PADRE2, </author> <title> a FORTRAN precompiler yielding error estimates and second derivatives. </title> <editor> In Andreas Griewank and George F. Corliss, editors, </editor> <title> Automatic Differentiation of Algorithms: Theory, </title> <booktitle> Implementation, and Application, </booktitle> <pages> pages 251-262. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1991. </year>
Reference-contexts: In particular, we mention GRESS [20], and PADRE-2 <ref> [23] </ref> for Fortran Programs and ADOL-C [17] for C programs. GRESS, PADRE-2, and ADOL-C implement both the forward and reverse mode. To save control flow information and intermediate values, these tools generate a "trace" of the computation by writing down the particulars of every operation performed in the code.
Reference: [24] <institution> Special Issue on Adjoint Applications in Dynamic Meterology. Tellus, 45a(5), </institution> <year> 1993. </year>
Reference-contexts: Data assimilation techniques | here one tries to adjust the initial state of a model to best reproduce some observed behavior (see, for example, [34, 33, 35]). A collection of papers on this subject can be found in <ref> [24] </ref>. To illustrate the tangent linear model and the adjoint, we assume that the state X of the system at time t satisfies the simple equation X (t) = H (X (t 1)); t = 0; : : : T (6) and that prognostic and diagnostic variables are the same.
Reference: [25] <author> Seon Ki Park and Kelvin Droegemeier. </author> <title> Effect of a microphysical parameterization on the evolution of linear perturbations in a convective cloud model. </title> <booktitle> In Preprints, Conference on Cloud Physics, </booktitle> <month> January </month> <year> 1995, </year> <title> Dallas, Texas. </title> <publisher> American Meteorological Society. </publisher>
Reference-contexts: They form the basis of the following commonly employed techniques. Sensitivity analysis techniques | here one tries to asses the sensitivity of the responses of a computational model with respect to perturbations in its parameters or initial conditions (see, for example, <ref> [27, 25, 26, 36] </ref>). Data assimilation techniques | here one tries to adjust the initial state of a model to best reproduce some observed behavior (see, for example, [34, 33, 35]). A collection of papers on this subject can be found in [24]. <p> ADIFOR has been successfully applied to codes from various domains of science. Experiences with meteorological codes, for example, have been reported in <ref> [10, 25, 26, 27] </ref>. Typically, ADIFOR-generated code runs two to four times faster than one-sided divided difference approximations when one computes more than 5-10 derivatives at one time.
Reference: [26] <author> Seon Ki Park and Kelvin Droegemeier. </author> <title> On the use of automatic differentiation to assess parametric sensitivity in convective-scale variational data assimilation. </title> <booktitle> In Preprints, Proc. Int. Symp. on Assimilation of Observations in Meteorology and Oceanography, </booktitle> <address> Tokyo, Japan, </address> <month> March </month> <year> 1995. </year> <title> World Meteorological Organization. </title> <type> 21 </type>
Reference-contexts: They form the basis of the following commonly employed techniques. Sensitivity analysis techniques | here one tries to asses the sensitivity of the responses of a computational model with respect to perturbations in its parameters or initial conditions (see, for example, <ref> [27, 25, 26, 36] </ref>). Data assimilation techniques | here one tries to adjust the initial state of a model to best reproduce some observed behavior (see, for example, [34, 33, 35]). A collection of papers on this subject can be found in [24]. <p> ADIFOR has been successfully applied to codes from various domains of science. Experiences with meteorological codes, for example, have been reported in <ref> [10, 25, 26, 27] </ref>. Typically, ADIFOR-generated code runs two to four times faster than one-sided divided difference approximations when one computes more than 5-10 derivatives at one time.
Reference: [27] <author> Seon Ki Park, Kelvin Droegemeier, Christian Bischof, and Tim Knauff. </author> <title> Sensitivity analysis of numerically-simulated convective storms using direct and adjoint methods. </title> <booktitle> In Preprints, 10th Conference on Numerical Weather Prediction, Portland, Oregon, </booktitle> <pages> pages 457-459. </pages> <publisher> American Meterologi-cal Society, </publisher> <year> 1994. </year>
Reference-contexts: They form the basis of the following commonly employed techniques. Sensitivity analysis techniques | here one tries to asses the sensitivity of the responses of a computational model with respect to perturbations in its parameters or initial conditions (see, for example, <ref> [27, 25, 26, 36] </ref>). Data assimilation techniques | here one tries to adjust the initial state of a model to best reproduce some observed behavior (see, for example, [34, 33, 35]). A collection of papers on this subject can be found in [24]. <p> ADIFOR has been successfully applied to codes from various domains of science. Experiences with meteorological codes, for example, have been reported in <ref> [10, 25, 26, 27] </ref>. Typically, ADIFOR-generated code runs two to four times faster than one-sided divided difference approximations when one computes more than 5-10 derivatives at one time.
Reference: [28] <author> Louis B. Rall. </author> <title> Automatic Differentiation: Techniques and Applications, </title> <booktitle> volume 120 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1981. </year>
Reference-contexts: Automatic Differentiation: Automatic differentiation techniques rely on the fact that every function, no matter how complicated, is executed on a computer as a (potentially very long) sequence of elementary operations such as additions, multiplications, and elementary functions such as sin and cos (see, for example, <ref> [15, 28] </ref>). By applying the chain rule @ f (g (t)) fi t=t 0 @s fi fi @t fi fi over and over again to the composition of those elementary operations, one can compute, in a completely mechanical fashion, derivatives of F that are correct up to machine precision [18].
Reference: [29] <author> Nicole Rostaing, Stephane Dalmas, and Andre Galligo. </author> <title> Automatic differentiation in Odysee. </title> <address> Tellus, 45a(5):558-568, </address> <month> October </month> <year> 1993. </year>
Reference-contexts: with using this trace for the purposes of automatic differentiation and its potentially very large size can be a serious computational bottleneck [31]. 4.1 The ADIFOR (Automatic Differentiation of Fortran) Tool Recently, a "source transformation" approach to automatic differentiation has been explored in the ADIFOR [2], ADIC [7], and Odyssee <ref> [29, 30] </ref> tools. ADIFOR and Odyssee transform Fortran 77 code and ADIC transforms ANSI-C code. By applying the rules of automatic differentiation, these tools generate new code, which, when executed, computes derivatives without the overhead associated with trace interpretation schemes. ADIFOR and ADIC mainly use the forward mode 2 .
Reference: [30] <author> Nicole Rostaing-Schmidt and Eric Hassold. </author> <title> Basic functional representation of programs for automatic differentiation in the Odyssee system. </title> <editor> In Francois-Xavier Le Dimet, editor, </editor> <booktitle> High-Performance Computing in the Geosciences, </booktitle> <address> Dordrecht, 1994. </address> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: with using this trace for the purposes of automatic differentiation and its potentially very large size can be a serious computational bottleneck [31]. 4.1 The ADIFOR (Automatic Differentiation of Fortran) Tool Recently, a "source transformation" approach to automatic differentiation has been explored in the ADIFOR [2], ADIC [7], and Odyssee <ref> [29, 30] </ref> tools. ADIFOR and Odyssee transform Fortran 77 code and ADIC transforms ANSI-C code. By applying the rules of automatic differentiation, these tools generate new code, which, when executed, computes derivatives without the overhead associated with trace interpretation schemes. ADIFOR and ADIC mainly use the forward mode 2 .
Reference: [31] <author> Edgar Soulie. </author> <title> User's experience with Fortran compilers for least squares problems. </title> <editor> In Andreas Griewank and George F. Corliss, editors, </editor> <title> Automatic Differentiation of Algorithms: Theory, </title> <booktitle> Implementation, and Application, </booktitle> <pages> pages 297-306. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1991. </year>
Reference-contexts: The interpretation overhead 8 associated with using this trace for the purposes of automatic differentiation and its potentially very large size can be a serious computational bottleneck <ref> [31] </ref>. 4.1 The ADIFOR (Automatic Differentiation of Fortran) Tool Recently, a "source transformation" approach to automatic differentiation has been explored in the ADIFOR [2], ADIC [7], and Odyssee [29, 30] tools. ADIFOR and Odyssee transform Fortran 77 code and ADIC transforms ANSI-C code.
Reference: [32] <author> Karl Svozil. </author> <title> Randomness and Undecidability in Physics. </title> <publisher> World Scientific, </publisher> <address> Singapore, </address> <year> 1993. </year>
Reference-contexts: To summarize, if we discount numerical instabilities arising from the complementary stability behavior of forward- and backward-integration of dynamical systems (see, for example, <ref> [32] </ref>), when properly initialized, the tangent linear model and the adjoint will compute the same sensitivities and provide a mechanism for developing a linear approximation of the model.
Reference: [33] <author> Olivier Talagrand. </author> <title> The use of adjoint equations in numerical modelling of the atmospheric circulation. </title> <editor> In Andreas Griewank and George Corliss, editors, </editor> <booktitle> Automatic Differentiation of Algorithms, </booktitle> <pages> pages 169-180, </pages> <address> Philadelphia, </address> <year> 1991. </year> <note> SIAM. </note>
Reference-contexts: Data assimilation techniques | here one tries to adjust the initial state of a model to best reproduce some observed behavior (see, for example, <ref> [34, 33, 35] </ref>). A collection of papers on this subject can be found in [24].
Reference: [34] <author> Jean-Noel Thepaut, Drasko Vasiljevic, Philippe Courtier, and Jean Pailleux. </author> <title> Variational assimilation of conventional meteorological observations with a multilevel primitive-equation model. </title> <editor> Q. J. R. </editor> <title> Meteorol. </title> <journal> Soc., </journal> <volume> 119 </volume> <pages> 153-186, </pages> <year> 1993. </year>
Reference-contexts: Data assimilation techniques | here one tries to adjust the initial state of a model to best reproduce some observed behavior (see, for example, <ref> [34, 33, 35] </ref>). A collection of papers on this subject can be found in [24].
Reference: [35] <author> Z. Wang, I. M. Navon, X. Zou, and K. J. Ingles. </author> <title> 4-D variational data assimilation with a global multilevel primitive equation model. </title> <booktitle> In Preprints, Proc. Int. Symp. on Assimilation of Observations in Meteorology and Oceanography, </booktitle> <address> Tokyo, Japan, </address> <month> March </month> <year> 1995. </year> <title> World Meteorological Organization. </title>
Reference-contexts: Data assimilation techniques | here one tries to adjust the initial state of a model to best reproduce some observed behavior (see, for example, <ref> [34, 33, 35] </ref>). A collection of papers on this subject can be found in [24].
Reference: [36] <author> Zhi Wang, Kelvin Droegemeier, Ming Xue, and Seon Ki Park. </author> <title> Sensitivity analysis of a 3-D compressible storm-scale model to input parameters. </title> <booktitle> In Preprints, Proc. Int. Symp. on Assimilation of Observations in Meteorology and Oceanography, </booktitle> <address> Tokyo, Japan, </address> <month> March </month> <year> 1995. </year> <title> World Meteorological Organization. </title> <type> 22 </type>
Reference-contexts: They form the basis of the following commonly employed techniques. Sensitivity analysis techniques | here one tries to asses the sensitivity of the responses of a computational model with respect to perturbations in its parameters or initial conditions (see, for example, <ref> [27, 25, 26, 36] </ref>). Data assimilation techniques | here one tries to adjust the initial state of a model to best reproduce some observed behavior (see, for example, [34, 33, 35]). A collection of papers on this subject can be found in [24].
References-found: 36

