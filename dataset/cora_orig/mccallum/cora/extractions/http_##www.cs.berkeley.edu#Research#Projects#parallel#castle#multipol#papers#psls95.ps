URL: http://www.cs.berkeley.edu/Research/Projects/parallel/castle/multipol/papers/psls95.ps
Refering-URL: http://www.cs.berkeley.edu/Research/Projects/parallel/castle/multipol/papers.html
Root-URL: http://www.cs.berkeley.edu
Email: fyelick,soumen,deprit,jjones,arvindk,cpweng@cs.berkeley.edu  
Title: Parallel Data Structures for Symbolic Computation  
Author: Katherine Yelick, Soumen Chakrabarti, Etienne Deprit, Jeff Jones, Arvind Krishnamurthy, and Chih-Po Wen 
Date: October 1995  
Note: Draft: To appear in Parallel Symbolic Languages and Systems,  
Affiliation: U.C. Berkeley, Computer Science Division  
Abstract: Symbolic applications often require dynamic irregular data structures, such as linked lists, unbalanced trees, and graphs, and they exhibit unpredictable computational patterns that lead to asynchronous communication and load imbalance when parallelized. In this paper we describe several symbolic applications and their parallelizations. The main problem in parallelization of each application was to replace the primary data structures with parallel versions that allow for high throughput, low latency access. In each case there are two problems to be solved: load balancing the parallel computation and sharing information about the solution as it is being constructed. The first problem is typically solved using a scheduling data structure, a stack, queue, or priority queue in sequential programs. The difficulty in parallelizing these structure is the trade-off between locality and load balancing: aggressive load balancing can lead to poor locality. The second problem of storing the solution depends much more on the type of solution, but range from simple scalar values to sets or tables. These structures use partitioning, full replication, or dynamic caching in their parallelizations. In sequential programming environments, common data structures are often provided through reusable libraries. We have built a parallel analog to these libraries, called Multipol. Multipol support irregular and asynchronous applications, including symbolic applications, discrete event simulation, and adaptive algorithms. The performance issues in Multipol include masking remote latency, elimination of communication, load balance, performance portability across machines, and local node performance.
Abstract-found: 1
Intro-found: 1
Reference: [BL94] <author> Robert D. Blumofe and Charles E. Leiserson. </author> <title> Scheduling multithreaded computations by work stealing. </title> <booktitle> In Thirty-Fifth Annual Symposium on Foundations of Computer Science (FOCS '94), </booktitle> <pages> pages 356-368, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: However, work stealing, using the TaskStealer data structure in Multipol, provides load balance that is almost as good as RandQue, but with better locality. (Work stealing is provably optimal, in the same sense that randomized task sharing is <ref> [BL94] </ref>.) The basic difference is that TaskStealer leaves tasks on the processor that created them until another processor becomes idle. The TaskStealer was originally used in the Eigenvalue and Grobner basis problems, until the simpler RandQue proved to work as well [CRY94].
Reference: [CDG + 93] <author> David E. Culler, Andrea Dusseau, Seth Copen Goldstein, Arvind Krishnamurthy, Steven Lumetta, Thorsten von Eicken, and Katherine Yelick. </author> <title> Parallel programming in Split-C. </title> <booktitle> In Supercomputing '93, </booktitle> <pages> pages 262-273, </pages> <address> Portland, Oregon, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: A operation completes sometime between the initiation and synchronization point, but no other ordering is guaranteed. Several applications can take advantage of relaxed consistency models. For bulk-synchronous problems such as EM3D <ref> [CDG + 93] </ref>, cell simulation [Ste94], and n-body solvers, data structure updates are delayed until the end of a computation phase, at which point all processors wait for all updates to complete. <p> Languages that support a global view of distributed data structures, for example, may incur costs from translating global indices into local ones [Ste94] or from checking whether a possibly remote address is actually local <ref> [CDG + 93] </ref>. Message passing models in which objects cannot span processor 10 boundaries avoid these overheads, but lose the ability to form abstractions across processors.
Reference: [CRY94] <author> Soumen Chakrabarti, Abhiram Ranade, and Katherine Yelick. </author> <title> Randomized load balancing for tree-structured computation. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference, </booktitle> <address> Knoxville, TN, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: A simple randomized scheduler sends each task to a random processor upon insertion. This scheduling structure is called RandQue in Multipol; it has poor locality properties if there is an advantage to executing tasks on the processor that created them, but is adequate for the bisection algorithm <ref> [CRY94] </ref>. The intervals stored in the RandQue act as the approximate solution as well as the scheduling structure. As the intervals shrink, the approximation improves until a solution of the desired accuracy is obtained. <p> The TaskStealer was originally used in the Eigenvalue and Grobner basis problems, until the simpler RandQue proved to work as well <ref> [CRY94] </ref>. The choice of a data structure for holding a partial solution is more difficult in Phylogeny than in the other search problems, because we need a representation of the result (success or failure) of every node searched so far.
Reference: [CSS + 91] <author> D. Culler, A. Sah, K. Schauser, T. von Eicken, and J. Wawrzynek. </author> <title> Fine-grain Parallelism with Minimal Hardware Support: A Compiler-Controlled Threaded Abstract Machine. </title> <booktitle> In Proc. of 4th Int. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> Santa-Clara, CA, </address> <month> April </month> <year> 1991. </year> <note> (Also available as Technical Report UCB/CSD 91/594, </note> <institution> CS Div., University of California at Berkeley). </institution> <month> 15 </month>
Reference-contexts: The scheduling policy used by one data structure can be changed without introducing anomalies, such as unexpected livelock or deadlock, into other parts of the program. The Multipol threads are designed for direct programming, in contrast to compiler-controlled threads 12 such as TAM <ref> [CSS + 91] </ref>, in that Multipol provides more flexibility such as arbitrary size threads and cus-tom schedulers. A set of macros can be used to facilitate programming.
Reference: [CY93] <author> Soumen Chakrabarti and Katherine Yelick. </author> <title> On the correctness of a distributed memory Grobner basis computation. In Rewriting Techniques and Applications, </title> <address> Montreal, Canada, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: In the phylogeny application and Grobner basis problem, not only are updates to the global set of results lazy, but each processor keeps partially completed cached copies of this set. This yields a correct, albeit different, execution than the sequential program <ref> [CY93, JY95] </ref>. 3.3 Communication Cost Reduction Some communication cannot be avoided, but its cost can be reduced by minimizing the number of messages (as opposed to the volume) and by using less expensive unacknowledged messages.
Reference: [DDR94] <author> J. Demmel, I. Dhillon, and H. Ren. </author> <title> On the correctness of parallel bisection in floating point. </title> <type> Tech Report UCB//CSD-94-805, </type> <institution> UC Berkeley Computer Science Division, </institution> <month> March </month> <year> 1994. </year> <note> available via anonymous ftp from tr-ftp.cs.berkeley.edu, in directory pub/tech-reports/csd/csd-94-805, file all.ps. </note>
Reference-contexts: A parallel implementation of bisection can use a static subdivision of the initial range, but this has poor parallel efficiency if the eigenvalues are clustered, because the work load is not balanced <ref> [DDR94] </ref>. A solution is to use a task queue with load balancing for the scheduling structure.
Reference: [DDvdGW93] <author> J. Demmel, J. Dongarra, R. van de Geijn, and D. Walker. </author> <title> LAPACK for distributed memory machines: the next generation. </title> <booktitle> In Proceedings of the Sixth SIAM Conference on Parallel Processing for Scientific Computing. </booktitle> <publisher> SIAM, </publisher> <year> 1993. </year>
Reference-contexts: The solution structure may be something a simple as a single value: in branch and bound algorithms, the bound represents a current approximation to the solution. The bisection algorithm for computing the eigenvalues, an algorithm used in the ScaLAPACK library, is also a search problem <ref> [DDvdGW93] </ref>. A symmetric tridiagonal N fi N real matrix is known to have N real eigenvalues and it is easy to find an initial range on the real line containing all eigenvalues.
Reference: [HW90] <author> Maurice P. Herlihy and Jeannette M. Wing. </author> <title> Linearizability: A correctness condition for concurrent objects. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <pages> pages 463-492, </pages> <month> July </month> <year> 1990. </year> <note> A preliminary version appeared in the proceedings of the 14th ACM Symposium on Principles of Programming Languages, </note> <year> 1987, </year> <title> under the title: Axioms for concurrent objects. </title>
Reference-contexts: Instead, long-running operations take a synchronization counter as an argument, which the caller can use to determine if the operation has completed. This leads to a relaxed consistency model for the data types, which is weaker than either sequential consistency [Lam79] or linearizability <ref> [HW90] </ref>. A operation completes sometime between the initiation and synchronization point, but no other ordering is guaranteed. Several applications can take advantage of relaxed consistency models.
Reference: [JY95] <author> J. Jones and K. Yelick. </author> <title> Parallelizing the phylogeny problem. </title> <booktitle> In Supercomputing '95, </booktitle> <month> December </month> <year> 1995. </year> <note> To appear. </note>
Reference-contexts: In the phylogeny application and Grobner basis problem, not only are updates to the global set of results lazy, but each processor keeps partially completed cached copies of this set. This yields a correct, albeit different, execution than the sequential program <ref> [CY93, JY95] </ref>. 3.3 Communication Cost Reduction Some communication cannot be avoided, but its cost can be reduced by minimizing the number of messages (as opposed to the volume) and by using less expensive unacknowledged messages.
Reference: [KB70] <author> Donald E. Knuth and Peter B. Bendix. </author> <title> Simple Word Problems in Universal Algebras, </title> <address> pages 263-297. </address> <publisher> Pergamon, Oxford, </publisher> <year> 1970. </year>
Reference-contexts: Given a set of rewrite rules, the procedure computes a new set that is in some sense complete, which allows for easy and efficient proofs in the resulting system <ref> [KB70] </ref>. The procedure may also fail to terminate or may halt unsuccessfully, although the reasons for these behaviors and techniques to help avoid them are beyond the scope of this discussion. The computation involves addition of new rewrite rules, called critical pairs, and the simplification of existing rules.
Reference: [KY94] <author> Arvind Krishnamurthy and Katherine Yelick. </author> <title> Optimizing parallel spmd programs. </title> <booktitle> In Proceedings of the Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1994. </year>
Reference-contexts: Techniques such as pipelining remote operations and multithreading can be used to hide latency. Even on a machines like the CM-5 with relatively low communication latency, the benefits from message overlap are noticeable: message pipelining of simple remote read and write operations can save as much as 30% <ref> [KY94] </ref> and overlap of higher level operations in the Grobner basis application saves about 10%. On workstation networks with longer hardware latencies and expensive remote message handlers, the savings should be even higher. The latency hiding techniques require the operations be nonblocking, or split-phase.
Reference: [Lam79] <author> Leslie Lamport. </author> <title> How to make a multiprocessor computer that correctly executes multi-process programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9):690-691, </volume> <month> September </month> <year> 1979. </year> <month> 16 </month>
Reference-contexts: Instead, long-running operations take a synchronization counter as an argument, which the caller can use to determine if the operation has completed. This leads to a relaxed consistency model for the data types, which is weaker than either sequential consistency <ref> [Lam79] </ref> or linearizability [HW90]. A operation completes sometime between the initiation and synchronization point, but no other ordering is guaranteed. Several applications can take advantage of relaxed consistency models.
Reference: [RV89] <author> Eric Roberts and Mark Vandevoorde. </author> <title> Work crews: An abstraction for controlling paral-lelism. </title> <type> Technical Report 42, </type> <institution> Digital Equipment Corporation Systems Research Center, Palo Alto, California, </institution> <year> 1989. </year>
Reference-contexts: The parallel solution is a task queue: each processor has its own queue of pairs, ordered by heuristics, and when a processor runs out of tasks it steals them from another processor's queue <ref> [RV89] </ref>. Even on a four processor shared memory machine, a centralized task queue proved to be a bottleneck, so a distributed version is used. As the computation proceeds, the set of rewrite rules progresses toward completeness, so the set itself is the approximate solution.
Reference: [Ste94] <author> Stephen Steinberg. </author> <title> Parallelizing a cell simulation: Analysis, abstraction, and portability. </title> <type> Master's thesis, </type> <institution> University of California, Berkeley, Computer Science Division, </institution> <month> December </month> <year> 1994. </year>
Reference-contexts: A operation completes sometime between the initiation and synchronization point, but no other ordering is guaranteed. Several applications can take advantage of relaxed consistency models. For bulk-synchronous problems such as EM3D [CDG + 93], cell simulation <ref> [Ste94] </ref>, and n-body solvers, data structure updates are delayed until the end of a computation phase, at which point all processors wait for all updates to complete. <p> Languages that support a global view of distributed data structures, for example, may incur costs from translating global indices into local ones <ref> [Ste94] </ref> or from checking whether a possibly remote address is actually local [CDG + 93]. Message passing models in which objects cannot span processor 10 boundaries avoid these overheads, but lose the ability to form abstractions across processors.
Reference: [Yel90] <author> Katherine A. Yelick. </author> <title> Using Abstraction in Explicitly Parallel Programs. </title> <type> PhD thesis, </type> <institution> MIT Laboratory for Computer Science, </institution> <address> Cambridge, MA 02139, </address> <month> December </month> <year> 1990. </year> <note> Also appeared as MIT/LCS/TR-507, </note> <month> July </month> <year> 1991. </year>
Reference-contexts: The sequential matching algorithm is recursive, so the program stack is the scheduling structure to be parallelized <ref> [Yel90] </ref>. As in Knuth-Bendix it is replaced by a task queue, in this case using order of insertion as the priority, i.e., each local queue is FIFO.
Reference: [YG92] <author> Katherine A. Yelick and Steven J. </author> <title> Garland. A parallel completion procedure for term rewriting systems. </title> <booktitle> In Conference on Automated Deduction, </booktitle> <address> Saratoga Springs, NY, </address> <year> 1992. </year> <month> 17 </month>
Reference-contexts: The creation and simplification of new rules constitutes the bulk of the work, so the data structure that holds pairs of rules is the primary scheduling structure in the parallel implementation <ref> [YG92] </ref>. The ordering of pairs within this scheduling queue is quite flexible, although a fairness must be guaranteed (pairs cannot be left in the queue forever) and if some rule pairs are favored over others, unnecessary work can be avoided.
References-found: 16

