URL: ftp://archive.cis.ohio-state.edu/pub/neuroprose/friedrich.ipmu96.ps.Z
Refering-URL: http://www.cs.bham.ac.uk/~wbl/biblio/gp-bibliography.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: chris@uni-wh.de  moraga@ls1.informatik.uni-dortmund.de  
Title: An Evolutionary Method to Find Good Building-Blocks for Architectures of Artificial Neural Networks  
Author: Christoph M. Friedrich Claudio Moraga 
Address: Alfred-Herrhausen Str. 44 58455 Witten, Germany  44221 Dortmund, Germany  
Affiliation: University of Witten/Herdecke Inst. for Technology Development and Systems Analysis  University of Dortmund Department of Computer Science  
Abstract: This paper deals with the combination of Evolutionary Algorithms and Artificial Neural Networks (ANN). A new method is presented, to find good building-blocks for architectures of Artificial Neural Networks. The method is based on Cellular Encoding, a representation scheme by F. Gruau, and on Genetic Programming by J. Koza. First it will be shown that a modified Cellular Encoding technique is able to find good architectures even for non-boolean networks. With the help of a graph-database and a new graph-rewriting method, it is secondly possible to build architectures from modular structures. The information about building-blocks for architectures is obtained by statistically analyzing the data in the graph-database. Simulation results for two real world problems are given.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Altenberg. </author> <title> The evolution of evolvability in genetic programming. </title> <editor> In K. E. Kinnear, editor, </editor> <booktitle> Advances in Genetic Programming. </booktitle> <publisher> MIT Press, </publisher> <year> 1994. </year> <month> ftp://ftp.mhpcc.edu/pub/incoming/alten-berg/LeeEEGP.ps.Z. </month>
Reference-contexts: But it gives no insight into the problem, how good architectures are built. It would be much more interesting to find building principles for good architectures. Especially for Genetic Programming, where the representation of a problem is a grammar-tree, Altenberg <ref> [1] </ref> suggested a method to find good building-blocks that are subtrees of the representation. He called this technique Genetic Engineering. It depends on the assumption, that some parts of the genetic program have a higher impact on the complete fitness of the genetic program than others.
Reference: [2] <author> T. </author> <title> Back. Evolutionary Algorithms in Theorie and Practice. </title> <type> PhD thesis, </type> <institution> University of Dortmund, </institution> <year> 1994. </year>
Reference-contexts: As a conclusion these algorithms find only solutions for special sets of problems. Many users find the architecture in a time-consuming trial and error cycle. Miller, Todd and Hedge [10] call it a black art and suggest that Evolutionary Algorithms (EA's <ref> [2] </ref>) are good candidates for an automated search in the space of possible network architectures because the fitness function associated with such a space has several properties which make it unappealing for classic methods: it is complex, noisy, non-differentiable, multimodal and deceptive. Much work has been done in this area.
Reference: [3] <author> J. Branke. </author> <title> Evolutionary algorithms for neural network design and training. </title> <booktitle> In Proceedings of the 1st Nordic Workshop on Genetic Algorithms and its Applications, </booktitle> <year> 1995. </year> <month> ftp://ftp.aifb.uni-karlsruhe.de/pub/jbr/Vaasa.ps. </month>
Reference-contexts: Much work has been done in this area. For a survey of recent work the paper of Branke <ref> [3] </ref> is suggested.
Reference: [4] <author> C. M. Friedrich. </author> <title> Entwicklung und Analyse Evolutionarer Algorithmen zur Optimierung der Struktur und Parameter von kunstlichen Neur-onalen Netzen. </title> <type> Master's thesis, </type> <institution> University of Dortmund; Department of Computer Science; 44221 Dortmund; Germany, </institution> <year> 1995. </year>
Reference: [5] <author> F. Gruau. </author> <title> Cellular encoding of genetic neural networks. </title> <type> Technical Report 92-21, </type> <institution> Ecole Nor-male Superieure de Lyon, Institut IMAG, </institution> <year> 1992. </year> <note> ftp://lip.ens-lyon.fr/pub/Rapports/RR/RR92/- RR92-21.ps.Z. </note>
Reference-contexts: Gruau <ref> [5] </ref> who named it Cellular Encoding. In this encoding technique, the information to develop an architecture is obtained by interpreting the information from a grammar-tree. The nodes of this tree encode information about graph rewriting operations.
Reference: [6] <author> B. Hassibi and D. Stork. </author> <title> Second order derivatives for network pruning: Optimal brain surgeon. </title> <editor> In S. Hansen, J. Cowan, and C. Giles, editors, </editor> <booktitle> Advances in Neural Information Processing Systems (NIPS) 5, </booktitle> <pages> pages 164-171. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: The constructive algorithms make restrictive assumptions for the building-blocks of the architecture, so not all architectures can be build. Pruning algorithms involve the problem of choosing the initial architecture and the decision which part of the network should be removed. Some pruning algorithms like OBS <ref> [6] </ref> are very computing-intensive. As a conclusion these algorithms find only solutions for special sets of problems. Many users find the architecture in a time-consuming trial and error cycle.
Reference: [7] <author> C. M. Hoffmann. </author> <title> Group-Theoretic Algorithms and Graph Isomorphism. </title> <publisher> Springer, </publisher> <year> 1979. </year> <note> LNCS 136. </note>
Reference-contexts: Two not-identical graphs are isomorph, if they are structurally identical, that means if it is possible to find a permutation of the node labels that make the graphs identical. Unfortunately the problem of testing graph isomorphism for the set of correct feedforward network graphs with shortcut connections is NP-complete <ref> [7] </ref>. The problem of graph isomorphism for the optimization of neural network architectures has been known before by many scientists who call it: problem of the competing conventions. It leads to bigger search spaces, because of the genotype-phenotype mapping of Genetic Algorithms.
Reference: [8] <author> J. R. Koza. </author> <title> Genetic programming, on the programming of Computers by means of natural selection. </title> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: The development of a network ends, if all cells evaluate the #end: operation, located as terminal at the leaves of the grammar-tree. As mentioned above, the cellular code is given as a grammar-tree. In this case it is possible to use Genetic Programming <ref> [8] </ref> to optimize the cellular code. Genetic Programming is a Genetic Algorithm, that uses trees instead of Bitstrings as representation. The recombination operator crossover is realized by exchanging the subtrees of two parent individuals.
Reference: [9] <author> D. Michie, D. J. Spiegelhalter, and C. C. Taylor. </author> <title> Machine Learning, neural and statistical classification. </title> <publisher> Ellis Horwood Ltd., </publisher> <year> 1994. </year>
Reference-contexts: It is a classification problem with 576 learning patterns and 192 test patterns. There are 8 input parameters and 2 output classes. The classification method is WTA (winner takes all). Prechelt and Michie <ref> [9] </ref> found networks having a classification error of 24.8 % on the test set. The activation function of the network was the tanh (x). Riedmiller [12] suggested, that this symmetric function (a scaled and shifted logistic function) can lead to faster learning than the asymmetric standard sigmoid.
Reference: [10] <author> G. F. Miller, P. M. Todd, and S. U. Hegde. </author> <title> Designing neural networks using genetic algorithms. </title> <editor> In J. David Schaffer, editor, </editor> <booktitle> Proceedings of the Third International Conference on Genetic Algorithms, </booktitle> <pages> pages 379-384. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1989. </year>
Reference-contexts: Some pruning algorithms like OBS [6] are very computing-intensive. As a conclusion these algorithms find only solutions for special sets of problems. Many users find the architecture in a time-consuming trial and error cycle. Miller, Todd and Hedge <ref> [10] </ref> call it a black art and suggest that Evolutionary Algorithms (EA's [2]) are good candidates for an automated search in the space of possible network architectures because the fitness function associated with such a space has several properties which make it unappealing for classic methods: it is complex, noisy, non-differentiable, <p> Different to Gruau's work, it is possible with the operator #seqcopy:, to create networks with shortcut connections. This is important because it is claimed by many scientists that shortcut connections can lead to better performing networks <ref> [10] </ref>. The new introduced operations #usplit: and #lsplit: make parallel splittings, but seperate the ingoing or outgoing links of the created nodes. This makes it possible to create sparsely connected networks with hierachical architectures. One property of the created networks should be their correctness.
Reference: [11] <author> Lutz Prechelt. </author> <title> PROBEN1 | A set of benchmarks and benchmarking rules for neural network training algorithms. </title> <type> Technical Report 21/94, </type> <institution> Fakultat fur Informatik, Universitat Karls-ruhe, D-76128 Karlsruhe, </institution> <address> Germany, Septem-ber 1994. ftp://ftp.ira.uka.de/pub/papers/tech-reports/1994/1994-21.ps.Z. </address>
Reference-contexts: The hidden node will be first rewritten with a #usplit: operation and the resulting nodes will be rewritten with the operation #par: or #lsplit:. 4 Experimental Results The data for the experiments are taken from the benchmark suite from L. Prechelt <ref> [11] </ref>. The task for the diabetes problem is to classify from some dia 3-1-2 network with the module usplit:par:lsplit: gnostic data (e.g. blood pressure, result of glucose tolerance test etc.), whether a female Pima indian was diabetes positive or not. <p> The best found architecture, a net with 31 nodes (20 hidden nodes) and 202 weights is shown in figure 7. The fitness for this network was 5.26 in the mean of five runs. Standard architectures, tested up to 32 hidden nodes show a fitness around 7. Prechelt <ref> [11] </ref> obtained similar results for standard architectures, but it must be noted, that he uses cross-validation, so the number of learning epochs are not directly comparable.
Reference: [12] <author> M. Riedmiller. </author> <title> Schnelle adaptive Lernverfahren fur mehrschichtige Feedforward-Netzwerke, Ver-gleich und Weiterentwicklung. </title> <type> Master's thesis, </type> <institution> Universitat Karlsruhe, Fakultat fur Informatik, </institution> <year> 1992. </year> <note> ftp://ftp.ira.uka.de/pub/neuron/papers/- Adaptives_Lernen.ps.gz. </note>
Reference-contexts: There are 8 input parameters and 2 output classes. The classification method is WTA (winner takes all). Prechelt and Michie [9] found networks having a classification error of 24.8 % on the test set. The activation function of the network was the tanh (x). Riedmiller <ref> [12] </ref> suggested, that this symmetric function (a scaled and shifted logistic function) can lead to faster learning than the asymmetric standard sigmoid. The learning set was presented 500 times, as learning algorithm, RPROP [13] was chosen.
Reference: [13] <author> M. Riedmiller and H. Braun. </author> <title> A direct adaptive method for faster backpropagation learning: The RPROP algorithm. </title> <editor> In H. Ruspini, editor, </editor> <booktitle> Proceedings of the IEEE International Conference on Neural Networks (ICNN), </booktitle> <pages> pages 586-591, </pages> <year> 1993. </year> <note> ftp://i11s16.ira.uka.de/pub/neuro/papers/riedml.- icnn93.ps.Z. </note>
Reference-contexts: The activation function of the network was the tanh (x). Riedmiller [12] suggested, that this symmetric function (a scaled and shifted logistic function) can lead to faster learning than the asymmetric standard sigmoid. The learning set was presented 500 times, as learning algorithm, RPROP <ref> [13] </ref> was chosen. This adaptive learning algorithm learns about 5 times faster than standard backpropagation and needs less computing time than backpropagation (Prechelt also uses RPROP in his experiments). Unfortunately the network simulator supports no cross-validation, thus learning was done without it.
Reference: [14] <author> A. Zell et al. </author> <title> SNNS User Manual, </title> <type> Version 4.0. </type> <institution> University of Stuttgart, Institute for Parallel and Distributed High Performance Systems, </institution> <address> Breitwie-senstr. 20-22; 70565 Stuttgart, 4.0 edition, </address> <year> 1995. </year> <month> ftp://ftp.informatik.uni-stuttgart.de/pub/SNNS. </month>
Reference-contexts: This database contains the cellular code, network graphs and fitness parameters like the number of necessary epochs for learning, classification error on the learning and test set and other parameters of the networks. network (by using the network simulator SNNS 4.0 <ref> [14] </ref>), it is checked, whether the fitness of this network was calculated five times before. If not, the fitness is calculated again and saved in the database. The resulting fitness of a network is the mean of all fitness evaluations of this network.
References-found: 14

