URL: http://www-wavelet.eecs.berkeley.edu/~vkgoyal/technical/dspw98.ps.Z
Refering-URL: http://www-wavelet.eecs.berkeley.edu/~vkgoyal/technical/dspw98.html
Root-URL: 
Title: BLOCK TRANSFORM ADAPTATION BY STOCHASTIC GRADIENT DESCENT  
Author: Vivek K Goyal and Martin Vetterli 
Note: To appear in Proc. IEEE Digital Signal Processing Workshop 1998, Bryce Canyon, UT, Aug. 1998. c fl1998 IEEE  
Affiliation: University of California, Berkeley Department of Electrical Engineering Computer Sciences  
Abstract: The problem of computing the eigendecomposition of an N fi N symmetric matrix is cast as an unconstrained minimization of either of two performance measures. The K = N(N 1)=2 independent parameters represent angles of distinct Givens rotations. Gradient descent is applied to the minimization problem, step size bounds for local convergence are given, and similarities to LMS adaptive filtering are noted. In adaptive transform coding it is often desirable for the transform to approximate a local Karhunen-Loeve Transform for the source. Determining such a transform is equivalent to finding the eigenvectors of the correlation matrix of the source; thus, the eigendecomposition methods developed here are applicable to adaptive transform coding. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> V. K Goyal and M. Vetterli, </author> <title> Adaptive transform coding using LMS-like principal component tracking, </title> <journal> IEEE Trans. Signal Proc., </journal> <month> Jan. </month> <year> 1998, </year> <note> submitted. </note>
Reference-contexts: The most common method of performance surface search is gradient descent, which leads to the LMS algorithm. This paper defines two useful performance surfaces for linear transform coding and analyzes gradient descent methods for these surfaces. Linear- and fixed-step random searches are also considered in <ref> [1] </ref>. The result is a set of new algorithms for adaptive linear transform coding. Finding a Karhunen-Loeve Transform (KLT) requires finding an orthonormal set of eigenvectors of a symmetric, positive semi-definite matrix; i.e., finding a KLT is an instance of the symmetric eigenproblem, a fundamental problem of numerical analysis. <p> (2) and (7) converges to a diagonalizing transform if 0 &lt; ff &lt; J min max ( i j ) 2 1 where J min = Q N Proof: The proof is similar to that of Theorem 1 but somewhat trick ier and is omitted for lack of space; see <ref> [1] </ref>. fl 5. IMPLEMENTATION POSSIBILITIES We would now like to apply these algorithms in an adaptive setting. <p> The key is the formulation of unconstrained minimization problems over a minimal number of parameters. Borrowing from the adaptive filtering literature, we have applied linear and fixed step random search and gradient descent to the resulting minimization problems. On the latter case is presented here; see <ref> [1] </ref> for full details. In the gradient descent case we derived step size bounds to ensure convergence in the absence of estimation noise. Through simulations we demonstrated that in the presence of estimation noise, the gradient descent converges when the step size is chosen small relative to the bound.
Reference: [2] <author> G. H. Golub and C. F. Van Loan, </author> <title> Matrix Computations, </title> <publisher> Johns Hopkins Univ. Press, </publisher> <address> Baltimore, MD, </address> <note> second edition, </note> <year> 1989. </year>
Reference-contexts: The idea of using performance surface search in the design of algorithms for this problem seems to be new, although the cost function which we will later call J 1 has been used in convergence analyses <ref> [2] </ref>. These algorithms are not competitive with cyclic Jacobi methods for computing a single eigendecomposition of a large matrix; however, they are potentially useful for computing eigendecompositions of a slowly varying sequence of matrices. M. Vetterli is also with Ecole Polytechnique Federale de Lausanne. <p> In addition, further insights may come from drawing together techniques from adaptive filtering, transform coding, and numerical linear algebra. The reader is referred to <ref> [2] </ref> for a thorough treatment of the techniques for computing eigendecompositions including the techniques specific to the common special case where the matrix is symmetric, to [3] for a review of transform coding, and to [4] for a review of adaptive FIR Wiener filtering. 2. <p> In our search for the best orthogonal transform it will be useful to represent the matrix in terms of the smallest possible number of parameters. Let e G i;j; denote a Givens (or Jacobi) rotation <ref> [2] </ref> of radians (counterclockwise) in the (i; j) coordinate plane.
Reference: [3] <author> A. Gersho and R. M. Gray, </author> <title> Vector Quantization and Signal Compression, </title> <publisher> Kluwer Acad. Pub., </publisher> <address> Boston, MA, </address> <year> 1992. </year>
Reference-contexts: The reader is referred to [2] for a thorough treatment of the techniques for computing eigendecompositions including the techniques specific to the common special case where the matrix is symmetric, to <ref> [3] </ref> for a review of transform coding, and to [4] for a review of adaptive FIR Wiener filtering. 2. PROBLEM DEFINITION AND BASIC STRATEGY Let fx n g n2N be a sequence of R N -valued random vectors and let X n = E [x n x T n ]. <p> The cost function J 2 (T ) = i=1 is also useful because, under the standard assumptions of transform coding, for a fixed rate, N p J 2 (T ) is proportional to the distortion <ref> [3] </ref>. Thus minimizing J 2 minimizes the distortion; J 2 (T ) is minimized by the transform which diagonalizes X. 4. METHODS FOR PERFORMANCE SURFACE SEARCH The effects of the time variation of X and estimation noise are left for subsequent sections.
Reference: [4] <author> B. Widrow and S. D. Stearns, </author> <title> Adaptive Signal Processing, </title> <publisher> Prentice-Hall, </publisher> <address> Upper Saddle River, NJ, </address> <year> 1985. </year>
Reference-contexts: The reader is referred to [2] for a thorough treatment of the techniques for computing eigendecompositions including the techniques specific to the common special case where the matrix is symmetric, to [3] for a review of transform coding, and to <ref> [4] </ref> for a review of adaptive FIR Wiener filtering. 2. PROBLEM DEFINITION AND BASIC STRATEGY Let fx n g n2N be a sequence of R N -valued random vectors and let X n = E [x n x T n ]. <p> However, because the parameter vector fi is adapted based on each source vector, the steady-state performance has a noisy stochastic component. This excess in J increases as the step size is increased. Qualitatively it is similar to the excess mean-square error in LMS filtering <ref> [4] </ref>. The steady-state value of J 1 decreases monotonically as ff is decreased, but the convergence is slower.
Reference: [5] <author> R. A. Horn and C. R. Johnson, </author> <title> Matrix Analysis, </title> <publisher> Cambridge Univ. Press, </publisher> <year> 1985. </year>
Reference-contexts: ] T 2 [=2; =2) K such that T fi XT T fi is diagonal, where T fi = G 1; 1 G 2; 2 : : : G K; K : (1) Proof: Since X is symmetric, there exists an orthogonal matrix S such that SXS T is diagonal <ref> [5] </ref>.
Reference: [6] <author> T. W. Anderson, I. Olkin, and L. G. Underhill, </author> <title> Generation of random orthogonal matrices, </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> vol. 8, no. 4, </volume> <pages> pp. 625-629, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: orthogonal matrix can be fac tored as S = ( e G 1;2; 1;2 ( e G N1;N; N1;N )D * ; where D * = diag (* 1 ; : : : ; * N ), * i = 1, i = 1; 2; : : : ; N <ref> [6] </ref>.
Reference: [7] <author> M. Vidyasagar, </author> <title> Nonlinear Systems Analysis, </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1978. </year>
Reference-contexts: A sufficient condition for local convergence is that the eigenvalues of I ffF lie in the unit circle. That the local exponential stability of the original nonlinear system can be inferred from an eigenvalue condition on the linearized sys tem follows from the continuous differentiability of F <ref> [7] </ref>. We now evaluate F .
Reference: [8] <author> V. K Goyal, J. Zhuang, and M. Vetterli, </author> <title> On-line algorithms for universal transform coding, </title> <journal> IEEE Trans. Inform. Th., </journal> <month> Apr. </month> <year> 1998, </year> <note> submitted. </note>
Reference-contexts: As the quantization becomes coarser, the convergence slows. Notice that with direct computation, quantization does not seem to lead to a nonzero steady-state error. This is suggestive of universal performance of the backward-adaptive scheme <ref> [8] </ref>. For a slowly varying source (! 1 = ! 2 = ! 3 = 0:001; see Fig. 1 (d)), we again have that the performance with = 0:125 or 0.25 is indistinguishable from the performance without quantization.
References-found: 8

