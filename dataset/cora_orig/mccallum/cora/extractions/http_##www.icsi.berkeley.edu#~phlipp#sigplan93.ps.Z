URL: http://www.icsi.berkeley.edu/~phlipp/sigplan93.ps.Z
Refering-URL: http://www.icsi.berkeley.edu/~phlipp/phlipp.publ.html
Root-URL: http://www.icsi.berkeley.edu
Title: Compiling Machine-Independent Parallel Programs control parallelism in a portable fashion. An optimizing compiler targeting MIMD,
Author: Michael Philippsen, Ernst A. Heinz, Paul Lukowicz 
Keyword: The effects of two major optimization techniques, synchronization point elimination and data/process alignment  
Note: The programming language used is Modula-2*, an extension of Modula-2, which incorporates both data and  are also quantified.  
Address: D-7500 Karlsruhe, F.R.Germany  
Affiliation: Universitat Karlsruhe Fakultat fur Informatik  
Abstract: email: phlipp@ira.uka.de This paper appeared in: ACM SIGPLAN Notices, 28(8):99-108, August 1993 Abstract Initial evidence is presented that explicitly parallel, machine-independent programs can automatically be translated into parallel machine code that is competitive in performance with hand-written code. The performance of the resulting code is compared to that of equivalent, carefully hand-coded and tuned programs. On a MasPar MP-1 (SIMD machine with up to 16k processors) the Modula-2* programs typically achieve 80% of the performance of the hand-coded parallel versions. When targeting sequential processors, the Modula-2* programs reach 90% of the performance of hand-coded sequential C. (There are no MIMD results yet.) 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Selim G. Akl. </author> <title> The Design and Analysis of Parallel Algorithms. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1989. </year>
Reference-contexts: Furthermore, the debugger is able to collect rudimentary profiling data by counting statement or subroutine invocations. 4 Benchmarks and Results At the moment, our benchmark suite consists of thirteen problems collected from the literature <ref> [1, 6, 11, 8, 5] </ref>. For each problem, we implemented the same algorithms in Modula-2*, in sequential C, and in MPL 1 . Then we measured the runtimes of our implementations on a 16K Mas-Par MP-1 (SIMD) and a SparcStation-1 (SISD) for widely ranging problem sizes. <p> The numbers are the accumulated sums over all problems and problem sizes (all data points). 4.3 Benchmarks The benchmark suite consists of thirteen problems collected from the literature <ref> [1, 6, 11, 8] </ref>. The problems 4.3.11, 4.3.2 and 4.3.8 have been defined in [5] to test the expressive power of parallel programming languages. Some problems (4.3.1, 4.3.5, 4.3.12) are chosen from text books on parallel programming [1, 6, 11]. <p> The problems 4.3.11, 4.3.2 and 4.3.8 have been defined in [5] to test the expressive power of parallel programming languages. Some problems (4.3.1, 4.3.5, 4.3.12) are chosen from text books on parallel programming <ref> [1, 6, 11] </ref>. The problem of finding the longest common subsequence (4.3.3) is well known in text processesing and computational biology [18]. The remaining problems have been introduced by other authors and compiler groups [12, 8, 10]. <p> Approach II: Again, the interval [a; b] is divided evenly over all processes. Then each process performs Newton's iteration. The algorithm terminates when a process finds the root. Note: This problem occurs frequently in science and engineering applications <ref> [1] </ref>. Since the implementations have total locality the performance is better than that of approach I. The Modula-2* compiler uses a general translation scheme for the FORALL statement that allows for nested parallelism. This generality, however, is more costly than the straightforward implementation of virtualization loops in the MPL program. <p> Thus, the expected performance growth for bigger problem sizes is not visible. 4.3.4 Red/Black Iteration Problem: Implement a red/black iteration, i.e., the kernel of a solver for partial differential equations. Approach: The implementation is straightforward. See for example <ref> [1] </ref>. It almost exclusively references neighbor-ing data elements ina n n-matrix. Note: This problem often serves as a case study for implementors of automatically parallelizing compilers, e.g. [10]. See the explanation for problem 4.3.1 (approach I). The Red/Black problem is quadratic.
Reference: [2] <author> Alberto Apostoli, Mikhail J. Atallah, Lawrence L. Larmore, and Scott McFaddin. </author> <title> Efficient parallel algorithms for string editing and related problems. </title> <type> Technical Report CSD-TR-724, </type> <institution> Purdue University, Department of Computer Sciences, </institution> <month> May </month> <year> 1990. </year>
Reference-contexts: It causes intensive access to neighboring data elements. The problem size is n =max (l; m). Note: The problem is presented in detail in [18]. The parallel solution is based on <ref> [2] </ref>. The curve of the MasPar performance is shaped similar to that of problem 4.3.1 (approach I). The effect of global versus XNET communication is smaller when few packets are sent (problem size &lt; 2 9 ). Due to limited memory, only problem sizes smaller than 16k are considered.
Reference: [3] <author> Utpal Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, Dor-drecht, London, </address> <year> 1988. </year>
Reference-contexts: Elimination of Synchronization Barriers The semantics of synchronous FORALLs in [19] require a vast number of synchronization barriers. Most real synchronous FORALLs, however, only need a fraction thereof to ensure correctness [9]. Redundant synchronization barriers can be detected with data dependence analysis <ref> [22, 3] </ref>. To understand the techniques of automatic synchronization barrier elimination consider the synchronous FORALL statement below, followed by two possible translations.
Reference: [4] <author> Ralph Butler and Ewing Lusk. </author> <title> User's Guide to the p4 Parallel Programming System. </title> <institution> Argonne National Laboratory, </institution> <year> 1992. </year>
Reference-contexts: Runtime system functions are provided by efficiently implementable, machine-independent macro interfaces. The MasPar MP-1 series runtime system makes use of the MasPar system library. The LAN runtime system is built on top of p4 <ref> [4] </ref>, a message passing parallel programming system available for a variety of machines. Therefore, we are able to target heterogenous LANs.
Reference: [5] <author> John T. Feo, </author> <title> editor. A Comparative Study of Parallel Programming Languages: The Salishan Problems. </title> <publisher> Elsevier Science Publishers, Holland, </publisher> <year> 1992. </year>
Reference-contexts: Furthermore, the debugger is able to collect rudimentary profiling data by counting statement or subroutine invocations. 4 Benchmarks and Results At the moment, our benchmark suite consists of thirteen problems collected from the literature <ref> [1, 6, 11, 8, 5] </ref>. For each problem, we implemented the same algorithms in Modula-2*, in sequential C, and in MPL 1 . Then we measured the runtimes of our implementations on a 16K Mas-Par MP-1 (SIMD) and a SparcStation-1 (SISD) for widely ranging problem sizes. <p> The numbers are the accumulated sums over all problems and problem sizes (all data points). 4.3 Benchmarks The benchmark suite consists of thirteen problems collected from the literature [1, 6, 11, 8]. The problems 4.3.11, 4.3.2 and 4.3.8 have been defined in <ref> [5] </ref> to test the expressive power of parallel programming languages. Some problems (4.3.1, 4.3.5, 4.3.12) are chosen from text books on parallel programming [1, 6, 11]. The problem of finding the longest common subsequence (4.3.3) is well known in text processesing and computational biology [18]. <p> As soon as a doctor and patient are paired, the doctor diagnoses the illness and treats the patient in a random amount of time. After finishing with a patient, the doctor rejoins the doctor's queue to await another patient. The output of the problem is intentionally unspecified (from <ref> [5] </ref>). Approach: The random amounts of time that patients are well and that doctors need to treat illnesses are counted down in parallel. The FIFO assignments of doctors to patients is done in parallel, too. <p> Approach: For each given prime p compute the power set fp i jp i ng. Combine any two power sets to a new one, while enforcing that the products remain n. Repeat the combination for all power sets. Note: The problem has been suggested in <ref> [5] </ref>. 4.3.9 Mandelbrot Set Problem: Compute the well-known Mandelbrot set. Approach: Perform all iterations in parallel. Note: Performance is excellent due to the absence of communication. <p> Include all isomers, but no duplicates (from <ref> [5] </ref>). Approach: The algorithm is partially based on [20] and has similarities to the approach used by Andrews in [5]. The scan, enumeration, and reduction library functions used in Modula-2* are more general than necessary for this problem. <p> Include all isomers, but no duplicates (from <ref> [5] </ref>). Approach: The algorithm is partially based on [20] and has similarities to the approach used by Andrews in [5]. The scan, enumeration, and reduction library functions used in Modula-2* are more general than necessary for this problem. This generality causes performance to degrade for problem sizes &gt; machine size. 4.3.12 Point in Polygon Problem: A simple polygon P with n edges and a point q are given.
Reference: [6] <author> Alan Gibbons and Wojciech Rytter. </author> <title> Efficient Parallel Algorithms. </title> <publisher> Cambridge University Press, </publisher> <year> 1988. </year>
Reference-contexts: Furthermore, the debugger is able to collect rudimentary profiling data by counting statement or subroutine invocations. 4 Benchmarks and Results At the moment, our benchmark suite consists of thirteen problems collected from the literature <ref> [1, 6, 11, 8, 5] </ref>. For each problem, we implemented the same algorithms in Modula-2*, in sequential C, and in MPL 1 . Then we measured the runtimes of our implementations on a 16K Mas-Par MP-1 (SIMD) and a SparcStation-1 (SISD) for widely ranging problem sizes. <p> The numbers are the accumulated sums over all problems and problem sizes (all data points). 4.3 Benchmarks The benchmark suite consists of thirteen problems collected from the literature <ref> [1, 6, 11, 8] </ref>. The problems 4.3.11, 4.3.2 and 4.3.8 have been defined in [5] to test the expressive power of parallel programming languages. Some problems (4.3.1, 4.3.5, 4.3.12) are chosen from text books on parallel programming [1, 6, 11]. <p> The problems 4.3.11, 4.3.2 and 4.3.8 have been defined in [5] to test the expressive power of parallel programming languages. Some problems (4.3.1, 4.3.5, 4.3.12) are chosen from text books on parallel programming <ref> [1, 6, 11] </ref>. The problem of finding the longest common subsequence (4.3.3) is well known in text processesing and computational biology [18]. The remaining problems have been introduced by other authors and compiler groups [12, 8, 10].
Reference: [7] <author> Stefan U. Hangen. </author> <title> Msdb a debugger, profiler and visualizer for Modula-2*. </title> <booktitle> In 3rd ACM/ONR Workshop on Parallel and Distributed Debugging, </booktitle> <address> San Diego, CA, </address> <month> May 17-18, </month> <year> 1993. </year>
Reference-contexts: Another interesting feature of these libraries is their functional diversity. Wherever possible, normal, masked, segmented, and universal (masked plus segmented) versions of the parallel operations are provided. 3.4 Parallel Debugger The Modula-2* source-level debugger <ref> [7] </ref> allows for visual interactive debugging under X-Windows. The central concepts of debugging parallel Modula-2* programs are process and data visualization. The debugger enables users to trace activities executed in parallel by providing abstraction mechanisms like grouping, parallel call trees, and simultaneous source code views in different windows.
Reference: [8] <author> Philipp J. Hatcher and Michael J. Quinn. </author> <title> Data-Parallel Programming on MIMD Computers. </title> <publisher> MIT Press Cambridge, </publisher> <address> Massachusetts, London, England, </address> <year> 1991. </year>
Reference-contexts: Furthermore, the debugger is able to collect rudimentary profiling data by counting statement or subroutine invocations. 4 Benchmarks and Results At the moment, our benchmark suite consists of thirteen problems collected from the literature <ref> [1, 6, 11, 8, 5] </ref>. For each problem, we implemented the same algorithms in Modula-2*, in sequential C, and in MPL 1 . Then we measured the runtimes of our implementations on a 16K Mas-Par MP-1 (SIMD) and a SparcStation-1 (SISD) for widely ranging problem sizes. <p> The numbers are the accumulated sums over all problems and problem sizes (all data points). 4.3 Benchmarks The benchmark suite consists of thirteen problems collected from the literature <ref> [1, 6, 11, 8] </ref>. The problems 4.3.11, 4.3.2 and 4.3.8 have been defined in [5] to test the expressive power of parallel programming languages. Some problems (4.3.1, 4.3.5, 4.3.12) are chosen from text books on parallel programming [1, 6, 11]. <p> Some problems (4.3.1, 4.3.5, 4.3.12) are chosen from text books on parallel programming [1, 6, 11]. The problem of finding the longest common subsequence (4.3.3) is well known in text processesing and computational biology [18]. The remaining problems have been introduced by other authors and compiler groups <ref> [12, 8, 10] </ref>. The benchmark suite does not contain standard numeric operations since we are convinced that these routine will require low level library implementation which is unlikely to be done by an end user in Modula-2*. <p> Approach: The solution is based on a data-parallel implementation of the GCD algorithm followed by an add-scan. Note: The problem was suggested by Hatcher <ref> [8] </ref>. The parallel invocation of a GCD procedure with its parallel while construct is the dominant cost producer in this example. <p> Approach: Process the adjacency matrix according to the property that if nodes x and m as well as nodes m and y are (transitively) adjacent, then x and y are (transitively) adjacent. The algorithm is due to Warshall [21]. Note: The problem was suggested by Hatcher <ref> [8] </ref>. The good result on the MasPar is caused by the fact that both MPL and Modula-2* must use general communication. 4.3.8 Hamming's Problem Problem: A set of primes fa; b; c; : : :g of arbitrary size and an integer n are given. <p> Approach: We implemented the classical prime sieve. However, rather than using a virtual process per candidate, the algorithm assigns a segment of candidates to each processor. This adaptive version works much faster since division can be replaced by indexing within each segment. Note: The problem was suggested by Hatcher <ref> [8] </ref>. The MPL implementation of the parallel adaptive work loops can take advantage of parallel register variables. Access to them is much faster than memory access. The Modula-2* compiler does not place the same variables into registers.
Reference: [9] <author> Ernst A. Heinz and Michael Philippsen. </author> <title> Synchronization barrier elimination in synchronous FORALLs. </title> <type> Technical Report No. 13/93, </type> <institution> University of Karlsruhe, Department of Informatics, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: Local accesses could not be achieved with a single FORALL. Elimination of Synchronization Barriers The semantics of synchronous FORALLs in [19] require a vast number of synchronization barriers. Most real synchronous FORALLs, however, only need a fraction thereof to ensure correctness <ref> [9] </ref>. Redundant synchronization barriers can be detected with data dependence analysis [22, 3]. To understand the techniques of automatic synchronization barrier elimination consider the synchronous FORALL statement below, followed by two possible translations.
Reference: [10] <author> Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Some problems (4.3.1, 4.3.5, 4.3.12) are chosen from text books on parallel programming [1, 6, 11]. The problem of finding the longest common subsequence (4.3.3) is well known in text processesing and computational biology [18]. The remaining problems have been introduced by other authors and compiler groups <ref> [12, 8, 10] </ref>. The benchmark suite does not contain standard numeric operations since we are convinced that these routine will require low level library implementation which is unlikely to be done by an end user in Modula-2*. <p> Approach: The implementation is straightforward. See for example [1]. It almost exclusively references neighbor-ing data elements ina n n-matrix. Note: This problem often serves as a case study for implementors of automatically parallelizing compilers, e.g. <ref> [10] </ref>. See the explanation for problem 4.3.1 (approach I). The Red/Black problem is quadratic. Problem size 2 7 requires 2 14 matrix elements and therefore corresponds to the machine size of the MasPar (16k). 4.3.5 List Rank Problem: A linked list of n elements is given.
Reference: [11] <author> Joseph JaJa. </author> <title> An Introduction to Parallel Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., </address> <year> 1992. </year>
Reference-contexts: Furthermore, the debugger is able to collect rudimentary profiling data by counting statement or subroutine invocations. 4 Benchmarks and Results At the moment, our benchmark suite consists of thirteen problems collected from the literature <ref> [1, 6, 11, 8, 5] </ref>. For each problem, we implemented the same algorithms in Modula-2*, in sequential C, and in MPL 1 . Then we measured the runtimes of our implementations on a 16K Mas-Par MP-1 (SIMD) and a SparcStation-1 (SISD) for widely ranging problem sizes. <p> The numbers are the accumulated sums over all problems and problem sizes (all data points). 4.3 Benchmarks The benchmark suite consists of thirteen problems collected from the literature <ref> [1, 6, 11, 8] </ref>. The problems 4.3.11, 4.3.2 and 4.3.8 have been defined in [5] to test the expressive power of parallel programming languages. Some problems (4.3.1, 4.3.5, 4.3.12) are chosen from text books on parallel programming [1, 6, 11]. <p> The problems 4.3.11, 4.3.2 and 4.3.8 have been defined in [5] to test the expressive power of parallel programming languages. Some problems (4.3.1, 4.3.5, 4.3.12) are chosen from text books on parallel programming <ref> [1, 6, 11] </ref>. The problem of finding the longest common subsequence (4.3.3) is well known in text processesing and computational biology [18]. The remaining problems have been introduced by other authors and compiler groups [12, 8, 10]. <p> All elements are stored in an array A [1::n]. Compute for each element its rank in the list. Approach: This problem is solved by pointer jumping. Note: Ranking the elements of a list is one of the elementary list processing tasks <ref> [11] </ref>.
Reference: [12] <author> Alan H. Karp and R. G. Babb. </author> <title> A comparison of 12 parallel Fortran dialects. </title> <journal> IEEE Software, </journal> <volume> 5(9) </volume> <pages> 52-67, </pages> <year> 1988. </year>
Reference-contexts: Some problems (4.3.1, 4.3.5, 4.3.12) are chosen from text books on parallel programming [1, 6, 11]. The problem of finding the longest common subsequence (4.3.3) is well known in text processesing and computational biology [18]. The remaining problems have been introduced by other authors and compiler groups <ref> [12, 8, 10] </ref>. The benchmark suite does not contain standard numeric operations since we are convinced that these routine will require low level library implementation which is unlikely to be done by an end user in Modula-2*. <p> Note: In <ref> [12] </ref>, Karp employs this problem to study parallel programming environments.
Reference: [13] <institution> Pawel Lukowicz. Code-Erzeugung fur Modula-2* fur verschiedene Maschinenarchitekturen. </institution> <type> Master's thesis, </type> <institution> University of Karlsruhe, Department of Informat-ics, </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: To keep major parts of the compiler machine-independent, Modula-2* programs are translated to a general intermediate representation. Based on a study of different parallel machines, we decided to use C augmented with a set of macros as an intermediate language <ref> [13] </ref>. Macros are expanded using target-specific include files yielding the appropriate parallel C derivate. Thus, retargeting the compiler only requires the exchange of the macro package and some libraries. Optimizations. On parallel machines, optimizations tend to improve program runtime dramatically.
Reference: [14] <author> MasPar Computer Corporation. </author> <title> MasPar Parallel Application Language (MPL) Reference Manual, </title> <month> September </month> <year> 1990. </year>
Reference-contexts: Program Space. Our compiler translates Modula-2* programs via C plus macros to MPL or C. The result 1 MPL <ref> [14] </ref> is a data-parallel extension of C designed for the MasPar MP-1 series. In MPL, the number of available processors, the SIMD architecture of the machine, its 2D mesh-connected processor network, and the distributed memory are visible.
Reference: [15] <author> Michael Philippsen. </author> <title> Automatic data distribution for nearest neighbor networks. </title> <booktitle> In Frontiers '92:The Fourth Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 178-185, </pages> <address> Mc Lean, Virginia, </address> <month> October 19-21, </month> <year> 1992. </year>
Reference-contexts: Layout is the assignment of aligned data structures and processes to the available processors. Desirable goals are (3) the exploitation of special hardware supported communication patterns and (4) simple address calculations. We use an automatic mapping <ref> [15] </ref> of arbitrary multidimensional arrays to processors and thus exploit grid communication if available and achieve efficient address calculations. To align arrays A and B of the following example, array A is enlarged and shifted to the left. All index expressions involved are transformed accordingly.
Reference: [16] <author> Michael Philippsen and Markus U. Mock. </author> <title> Data and process alignment in Modula-2*. </title> <editor> In Christoph W. Kessler, editor, </editor> <title> Automatic Parallelization New Approaches to Code Generation, Data Distribution, </title> <booktitle> and Performance Prediction, </booktitle> <pages> pages 171-191, </pages> <address> AP'93 Saar-brucken, Germany, March 1-3, </address> <year> 1993. </year>
Reference-contexts: Alignment is the task of finding an appropriate tradeoff between the two conflicting goals of (1) data locality and (2) maximum degree of parallelism. Our automatic alignment algorithm is descibed in <ref> [16] </ref> and briefly sketched below by means of an example. Layout is the assignment of aligned data structures and processes to the available processors. Desirable goals are (3) the exploitation of special hardware supported communication patterns and (4) simple address calculations.
Reference: [17] <author> Michael Philippsen and Walter F. Tichy. </author> <title> Modula-2* and its compilation. </title> <booktitle> In First International Conference of the Austrian Center for Parallel Computation, </booktitle> <address> Salzburg, Austria, </address> <year> 1991, </year> <pages> pages 169-183. </pages> <booktitle> Springer Ver-lag, Lecture Notes in Computer Science 591, </booktitle> <year> 1992. </year>
Reference-contexts: Thus, retargeting the compiler only requires the exchange of the macro package and some libraries. Optimizations. On parallel machines, optimizations tend to improve program runtime dramatically. Therefore, the Modula-2* compiler performs various optimizations and code restructurings summarized below (for more details see <ref> [17] </ref>). In the following subsections, we briefly sketch the main optimizations that are implemented in our Modula-2* compilers. In section 4.4 we show the quantitative effects of these techniques.
Reference: [18] <author> David Sankoff and Joseph B. Kruskal (eds). </author> <title> Time Warps, String Edits, and Macromolecules: The Theory and Practice of Sequence Comparison. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., </address> <year> 1983. </year>
Reference-contexts: Some problems (4.3.1, 4.3.5, 4.3.12) are chosen from text books on parallel programming [1, 6, 11]. The problem of finding the longest common subsequence (4.3.3) is well known in text processesing and computational biology <ref> [18] </ref>. The remaining problems have been introduced by other authors and compiler groups [12, 8, 10]. The benchmark suite does not contain standard numeric operations since we are convinced that these routine will require low level library implementation which is unlikely to be done by an end user in Modula-2*. <p> A common subsequence must be constructible from both A and B.) Approach: The solution uses a wave-front implementation of dynamic programming. It causes intensive access to neighboring data elements. The problem size is n =max (l; m). Note: The problem is presented in detail in <ref> [18] </ref>. The parallel solution is based on [2]. The curve of the MasPar performance is shaped similar to that of problem 4.3.1 (approach I). The effect of global versus XNET communication is smaller when few packets are sent (problem size &lt; 2 9 ).
Reference: [19] <author> Walter F. Tichy and Christian G. Herter. </author> <title> Modula-2*: An extension of Modula-2 for highly parallel, portable programs. </title> <type> Technical Report No. 4/90, </type> <institution> University of Karlsruhe, Department of Informatics, </institution> <month> January </month> <year> 1990. </year>
Reference-contexts: We present the benchmarks, experiments, and their results in section 4 and conclude with a discussion of the quantitative effects of two major optimization techniques. 2 Modula-2* The programming language Modula-2* was developed to allow for high-level, problem-oriented and machine-independent parallel programming. As described in <ref> [19] </ref>, it provides the following features: * An arbitrary number of processes operate on data in the same single address space. <p> The behavior of branches and loops inside synchronous FORALLs is defined with an MSIMD (multiple SIMD) machine in mind. This means that Modula-2* does require any synchronization between different branches of synchronous CASE or IF statements. The exact synchronous semantics of all Modula-2* statements are defined in <ref> [19] </ref>. The synchronous version of this FORALL operates much like the HPF FORALL, except that it is fully orthogonal to the rest of the language: Any statement, including conditionals, loops, other FORALLs, and subroutine calls may be placed in its body. Thus, the language explicitly supports nested and recursive parallelism. <p> In both FORALLs the process with index i will be executed where data element B [i] resides, resulting in local accesses. Local accesses could not be achieved with a single FORALL. Elimination of Synchronization Barriers The semantics of synchronous FORALLs in <ref> [19] </ref> require a vast number of synchronization barriers. Most real synchronous FORALLs, however, only need a fraction thereof to ensure correctness [9]. Redundant synchronization barriers can be detected with data dependence analysis [22, 3].
Reference: [20] <author> D. A. Turner. </author> <title> The semantic elegance of applicative languages. </title> <booktitle> In Conference on Functional Programming Languages and Computer Architecture, </booktitle> <pages> pages 85-92, </pages> <address> Portsmouth, NH, </address> <month> October </month> <year> 1981. </year>
Reference-contexts: Include all isomers, but no duplicates (from [5]). Approach: The algorithm is partially based on <ref> [20] </ref> and has similarities to the approach used by Andrews in [5]. The scan, enumeration, and reduction library functions used in Modula-2* are more general than necessary for this problem.
Reference: [21] <author> S. Warshall. </author> <title> A theorem on boolean matrices. </title> <journal> Journal of the ACM, </journal> <volume> 9(1) </volume> <pages> 11-12, </pages> <month> January </month> <year> 1962. </year>
Reference-contexts: Find its transitive closure. Approach: Process the adjacency matrix according to the property that if nodes x and m as well as nodes m and y are (transitively) adjacent, then x and y are (transitively) adjacent. The algorithm is due to Warshall <ref> [21] </ref>. Note: The problem was suggested by Hatcher [8].
Reference: [22] <author> Michael Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <booktitle> Research Monographs in Parallel and Distributed Computing. </booktitle> <publisher> Pitman, </publisher> <address> London, </address> <year> 1989. </year>
Reference-contexts: Elimination of Synchronization Barriers The semantics of synchronous FORALLs in [19] require a vast number of synchronization barriers. Most real synchronous FORALLs, however, only need a fraction thereof to ensure correctness [9]. Redundant synchronization barriers can be detected with data dependence analysis <ref> [22, 3] </ref>. To understand the techniques of automatic synchronization barrier elimination consider the synchronous FORALL statement below, followed by two possible translations.
References-found: 22

