URL: http://cag-www.lcs.mit.edu/~metcalf/papers/prefetch.ps.Z
Refering-URL: http://cag-www.lcs.mit.edu/~metcalf/papers/prefetch/
Root-URL: 
Title: Data Prefetching: A Cost/Performance Analysis  
Author: Chris Metcalf 
Address: Cambridge, Massachusetts 02139  
Affiliation: Laboratory for Computer Science Massachusetts Institute of Technology  
Abstract: This paper attempts to answer the question, To what extent is pre- fetching effective in hiding memory latency, and what is the minimal amount of hardware required to support prefetching? We begin by providing a classification of the different kinds of prefetching, and reconciling the various common performance metrics to allow fair comparisons. We then put forward an analytical model that gives the potential speedup with prefetching. We next detail the non-binding software prefetch technique and examine its performance, both with hand-inserted and compiler-inserted prefetches. We consider an elaborate hardware scheme meant to replace the software schemes entirely; then look at more reasonable schemes requiring only minimal extra hardware, and assess how much they add to the simple software prefetching model. We conclude with recommendations for CPU/cache architects. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anant Agarwal, Mark Horowitz, and John Hennessy. </author> <title> An analytical cache model. </title> <journal> ACM Theory of Computing Systems, </journal> <volume> 7(2) </volume> <pages> 184-215, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: Mowry et al. give a good model of prefetching in their paper [22]; for a more detailed analysis of non-prefetching caches, see <ref> [1] </ref>. Let us use N to represent the total number of instructions, and W , R, and S to represent the number of write, read, and synchronization operations.
Reference: [2] <author> Jean-Loup Baer and Tien-Fu Chen. </author> <title> An effective on-chip preloading scheme to reduce data access penalty. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <pages> pages 176-186, </pages> <month> November </month> <year> 1991. </year> <note> Also available as U. Washington CS TR 91-03-07. </note>
Reference-contexts: Additionally, hardware prefetching is expensive in terms of design time and chip area. 5.1 Architectural Model Chen and Baer <ref> [2, 6, 5] </ref> propose what is essentially a vector-stride- prefetching scheme, and spell out the details of how to identify vector strides in cache. <p> that we need to check the ORL for ordinary reads and writes as well for prefetches. 5.2 Results Chen and Baer use a trace-driven simulation, using pixie on a DEC- station 5000 to trace a set of benchmarks from SPEC (in [6]), and Perfect Club and a few others (in <ref> [2] </ref>). Cache warm start was simulated by ignoring the first 500,000 references. They use direct- mapped, 16-byte-line caches for all the various caches and tables.
Reference: [3] <author> David Callahan, Ken Kennedy, and Allan Porterfield. </author> <title> Software prefetching. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 40-52, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Many different solutions have been proposed to predict references enough in advance to prefetch them. A variety of hardware fl Area Exam. October, 1993. schemes have been suggested, ranging from just lengthening cache lines <ref> [3, 19, 30] </ref> to simple next-block prefetching [3, 4, 10, 29, 30, 31] to complex lookahead mechanisms such as stream-buffer preloading [15], instruction pipeline prefetching [18, 19, 20], and vector-style prefetching [8, 9]. Compilers are also implementing prefetching based on program analyses of increasing sophistication. <p> Many different solutions have been proposed to predict references enough in advance to prefetch them. A variety of hardware fl Area Exam. October, 1993. schemes have been suggested, ranging from just lengthening cache lines [3, 19, 30] to simple next-block prefetching <ref> [3, 4, 10, 29, 30, 31] </ref> to complex lookahead mechanisms such as stream-buffer preloading [15], instruction pipeline prefetching [18, 19, 20], and vector-style prefetching [8, 9]. Compilers are also implementing prefetching based on program analyses of increasing sophistication. <p> They show that two contexts can work well with prefetching, but by four contexts the negative effects overwhelm the positive ones. Prefetching a single loop ahead has also been examined by Callahan, Kennedy, and Porterfield in <ref> [3, 4, 25] </ref>. A somewhat more sophisticated analysis was carried out by Klaiber and Levy [16], who examined how to place prefetching instructions several loops ahead rather than a single loop.
Reference: [4] <author> David Callahan and Allan Porterfield. </author> <title> Data cache performance of supercomputer applications. </title> <booktitle> In Proceedings of Supercomputing '90, </booktitle> <pages> pages 564-572, </pages> <month> November </month> <year> 1990. </year>
Reference-contexts: Many different solutions have been proposed to predict references enough in advance to prefetch them. A variety of hardware fl Area Exam. October, 1993. schemes have been suggested, ranging from just lengthening cache lines [3, 19, 30] to simple next-block prefetching <ref> [3, 4, 10, 29, 30, 31] </ref> to complex lookahead mechanisms such as stream-buffer preloading [15], instruction pipeline prefetching [18, 19, 20], and vector-style prefetching [8, 9]. Compilers are also implementing prefetching based on program analyses of increasing sophistication. <p> They show that two contexts can work well with prefetching, but by four contexts the negative effects overwhelm the positive ones. Prefetching a single loop ahead has also been examined by Callahan, Kennedy, and Porterfield in <ref> [3, 4, 25] </ref>. A somewhat more sophisticated analysis was carried out by Klaiber and Levy [16], who examined how to place prefetching instructions several loops ahead rather than a single loop.
Reference: [5] <author> Tien-Fu Chen. </author> <title> Data Prefetching for High-Performance Processors. </title> <type> PhD thesis, </type> <institution> University of Washington at Seattle, </institution> <month> July </month> <year> 1993. </year> <note> Also available as TR 93-07-01. </note>
Reference-contexts: Additionally, hardware prefetching is expensive in terms of design time and chip area. 5.1 Architectural Model Chen and Baer <ref> [2, 6, 5] </ref> propose what is essentially a vector-stride- prefetching scheme, and spell out the details of how to identify vector strides in cache. <p> The exception is Espresso, which fails because the branch-prediction technique does poorly with its short blocks; prefetching at longer latencies saves only 15% of CPI da , unlike the 30% savings we see with 30-cycle latency. Chen's thesis <ref> [5] </ref> provides more timing details. In particular, it provides real speedups for four applications, Matmat, Mp3d, Water, and Cholesky, ranging from 1.1fi to 1.66fi in a multiprocessor with 80-cycle miss latency.
Reference: [6] <author> Tien-Fu Chen and Jean-Loup Baer. </author> <title> Reducing memory latency via nonblocking and prefetching caches. </title> <booktitle> In Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 51-61, </pages> <month> October </month> <year> 1992. </year> <note> Also available as U. Washington CS TR 92-06-03. </note>
Reference-contexts: Additionally, hardware prefetching is expensive in terms of design time and chip area. 5.1 Architectural Model Chen and Baer <ref> [2, 6, 5] </ref> propose what is essentially a vector-stride- prefetching scheme, and spell out the details of how to identify vector strides in cache. <p> Similarly, note that we need to check the ORL for ordinary reads and writes as well for prefetches. 5.2 Results Chen and Baer use a trace-driven simulation, using pixie on a DEC- station 5000 to trace a set of benchmarks from SPEC (in <ref> [6] </ref>), and Perfect Club and a few others (in [2]). Cache warm start was simulated by ignoring the first 500,000 references. They use direct- mapped, 16-byte-line caches for all the various caches and tables. <p> N varies from 4 to 64. Results are presented in terms of cycles per instruction (CPI) for data access, i.e. total data access time divided by number of instructions executed. The silicon cost for a 256-entry RPT is given as equivalent to 2K of cache memory. In <ref> [6] </ref>, they compare CPI da for the SPEC89 benchmarks. We converted the CPI da values to plain CPI for both their baseline model (using a write-buffer with read bypass, since most of the re-sults used the bypass model), and for the same model with hardware prefetching enabled. <p> Such non-blocking loads can be used to prefetch data well before they are needed for a given computation. In <ref> [6] </ref>, Chen and Baer use this technique along with compiler reorganization (instruction scheduling and register renaming) to maximize how far in advance a word can be requested from memory. <p> They explicitly use speculative load instructions instead of loads when they are prefetching. Speculative loads function just like the binding, non-blocking loads of <ref> [6] </ref>, but they do not cause a trap or page fault if they cannot be satisfied; instead they just set a special poison bit in the register.
Reference: [7] <author> William Y. Chen, Scott A. Mahlke, Pohua P. Chang, and Wen-mei W. Hwu. </author> <title> Data access microarchitectures for superscalar processors with compiler-assisted data prefetching. </title> <booktitle> In The 24th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 69-73, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: Prefetching a single loop ahead has also been examined by Callahan, Kennedy, and Porterfield in [3, 4, 25]. A somewhat more sophisticated analysis was carried out by Klaiber and Levy [16], who examined how to place prefetching instructions several loops ahead rather than a single loop. Chen and Mahle <ref> [7] </ref> examine prefetching in a multi-issue architecture; while the number of cycles between prefetch and use decrease, the otherwise idle issue slots provide ample room for adding prefetch instructions at no cost in cycle count.
Reference: [8] <author> G. C. Driscoll, J. J. Losq, T. R. Puzak, G. S. Rao, H. E. Sachar, and R. D. Villani. </author> <title> Cache miss directorya means of prefetching cache `missed' lines. </title> <journal> IBM Technical Disclosure Bulletin, </journal> <volume> 25(3A):1286, </volume> <month> August </month> <year> 1982. </year>
Reference-contexts: October, 1993. schemes have been suggested, ranging from just lengthening cache lines [3, 19, 30] to simple next-block prefetching [3, 4, 10, 29, 30, 31] to complex lookahead mechanisms such as stream-buffer preloading [15], instruction pipeline prefetching [18, 19, 20], and vector-style prefetching <ref> [8, 9] </ref>. Compilers are also implementing prefetching based on program analyses of increasing sophistication. Simple prefetching was available in hardware in the late '70s (e.g., in the IBM 370/168 [29]), but only recently have microprocessors begun providing the necessary hooks to allow prefetching to take place.
Reference: [9] <author> John W. C. Fu and Janak H. Patel. </author> <title> Data prefetching in multiprocessor vector cache memories. </title> <booktitle> In The 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 54-63, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: October, 1993. schemes have been suggested, ranging from just lengthening cache lines [3, 19, 30] to simple next-block prefetching [3, 4, 10, 29, 30, 31] to complex lookahead mechanisms such as stream-buffer preloading [15], instruction pipeline prefetching [18, 19, 20], and vector-style prefetching <ref> [8, 9] </ref>. Compilers are also implementing prefetching based on program analyses of increasing sophistication. Simple prefetching was available in hardware in the late '70s (e.g., in the IBM 370/168 [29]), but only recently have microprocessors begun providing the necessary hooks to allow prefetching to take place.
Reference: [10] <author> J. D. Gindele. </author> <title> Buffer block prefetching method. </title> <journal> IBM Technical Disclosure Bulletin, </journal> <volume> 20(2) </volume> <pages> 696-697, </pages> <month> July </month> <year> 1977. </year>
Reference-contexts: Many different solutions have been proposed to predict references enough in advance to prefetch them. A variety of hardware fl Area Exam. October, 1993. schemes have been suggested, ranging from just lengthening cache lines [3, 19, 30] to simple next-block prefetching <ref> [3, 4, 10, 29, 30, 31] </ref> to complex lookahead mechanisms such as stream-buffer preloading [15], instruction pipeline prefetching [18, 19, 20], and vector-style prefetching [8, 9]. Compilers are also implementing prefetching based on program analyses of increasing sophistication.
Reference: [11] <author> Anoop Gupta, John Hennessy, Kourosh Gharachorloo, Todd Mowry, and Wolf-Dietrich Weber. </author> <title> Comparative evaluation of latency reducing and tolerating techniques. </title> <booktitle> In The 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 254-263, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Overall, the prefetching was extremely successful for their model. In a separate paper from the same lab <ref> [11] </ref>, the authors find that while prefetching interacts well with relaxed consistency models in their DASH-like shared-memory multiprocessor, it does not work consistently well with multiple contexts per processor.
Reference: [12] <author> Mark D. Hill and Alan Jay Smith. </author> <title> Evaluating associativity in CPU caches. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(12) </volume> <pages> 1612-1630, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: [22] give data for N , W , R, S, m p , m s and l sync for their three benchmarks; appropriate data is provided in some other papers as well. (Data on the breakdown of cache hits into capacity, conflict, and compulsory misses can also be found in <ref> [12] </ref>.) Assuming optimistically that write-buffer prefetching can be eliminated by read-exclusive prefetching, and that read latency can be reduced to a single cycle by prefetching, they suggest maximum speedups of 4 to 6 for some typical applications.
Reference: [13] <institution> IBM. IBM Journal of Research and Development, </institution> <note> Special Issue on RISC System/6000. </note> <institution> IBM, </institution> <month> January </month> <year> 1990. </year>
Reference-contexts: Simple prefetching was available in hardware in the late '70s (e.g., in the IBM 370/168 [29]), but only recently have microprocessors begun providing the necessary hooks to allow prefetching to take place. Such processors include the IBM RS6000 <ref> [13] </ref>, Motorola 88100 [21], the DEC Alpha family [28], and the MIT Alewife group's Sparcle, among others. In this paper, we will begin by laying out a groundwork to assess various schemes, beginning in Section 2, and including an attempt to disentangle the various metrics used in prefetching papers.
Reference: [14] <author> Eric E. Johnson. </author> <title> Working set prefetching for cache memories. </title> <journal> ACM Computer Architecture News, </journal> <volume> 6(17) </volume> <pages> 137-141, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: More esoteric strategies have also been discussed in the literature, but have reported no hard facts; one such is Johnson's article <ref> [14] </ref> on precomputing and prefetching (or prebroadcasting) entire working sets in multiprocessors. 4.3 General Compiler-Assisted Software Prefetching A recent thesis by Selvidge [27] proposes a compiler algorithm that has similar loop prefetching as Mowry et al., including support for sparse arrays and loop indexes.
Reference: [15] <author> Norman P. Jouppi. </author> <title> Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers. </title> <booktitle> In The 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 364-373, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: 1 Introduction As processor speeds have increased over time, managing the memory hierarchy has become increasingly more important. Ten years ago, a VAX 11/780 had memory faster than the average instruction time <ref> [15] </ref>. Today, a typical CPU may be 20 cycles away from local memory, and a hundred or more cycles away from memory in a distributed multiprocessor. <p> A variety of hardware fl Area Exam. October, 1993. schemes have been suggested, ranging from just lengthening cache lines [3, 19, 30] to simple next-block prefetching [3, 4, 10, 29, 30, 31] to complex lookahead mechanisms such as stream-buffer preloading <ref> [15] </ref>, instruction pipeline prefetching [18, 19, 20], and vector-style prefetching [8, 9]. Compilers are also implementing prefetching based on program analyses of increasing sophistication.
Reference: [16] <author> Alexander C. Klaiber and Henry M. Levy. </author> <title> An architecture for software-controlled data prefetching. </title> <booktitle> In The 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 43-53, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Prefetching a single loop ahead has also been examined by Callahan, Kennedy, and Porterfield in [3, 4, 25]. A somewhat more sophisticated analysis was carried out by Klaiber and Levy <ref> [16] </ref>, who examined how to place prefetching instructions several loops ahead rather than a single loop.
Reference: [17] <author> David Kroft. </author> <title> Lockup-free instruction fetch/prefetch cache organi-zation. </title> <booktitle> In The 8th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 81-87, </pages> <month> May </month> <year> 1981. </year>
Reference-contexts: Using non-blocking (or lockup-free) caches is also considered. A non-blocking cache is one which can have multiple outstanding cache requests simultaneously; Kroft's implementation <ref> [17] </ref> uses miss information/status holding registers (MSHRs), which are used to record information relevant to outstanding cache requests. The MSHRs store information on the address referenced, the cache line that is being fetched, and where the returned datum should be sent to when it arrives in the cache.
Reference: [18] <author> Roland L. Lee, Pen-Chung Yew, and Duncan H. Lawrie. </author> <title> Data prefetching in shared memory multiprocessors. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <pages> pages 28-31, </pages> <month> August </month> <year> 1987. </year> <note> Also as Illinois CSRD TR 639, </note> <month> January </month> <year> 1987. </year>
Reference-contexts: A variety of hardware fl Area Exam. October, 1993. schemes have been suggested, ranging from just lengthening cache lines [3, 19, 30] to simple next-block prefetching [3, 4, 10, 29, 30, 31] to complex lookahead mechanisms such as stream-buffer preloading [15], instruction pipeline prefetching <ref> [18, 19, 20] </ref>, and vector-style prefetching [8, 9]. Compilers are also implementing prefetching based on program analyses of increasing sophistication.
Reference: [19] <author> Roland L. Lee, Pen-Chung Yew, and Duncan H. Lawrie. </author> <title> Multiprocessor cache design considerations. </title> <booktitle> In The 14th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 253-262, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: Many different solutions have been proposed to predict references enough in advance to prefetch them. A variety of hardware fl Area Exam. October, 1993. schemes have been suggested, ranging from just lengthening cache lines <ref> [3, 19, 30] </ref> to simple next-block prefetching [3, 4, 10, 29, 30, 31] to complex lookahead mechanisms such as stream-buffer preloading [15], instruction pipeline prefetching [18, 19, 20], and vector-style prefetching [8, 9]. Compilers are also implementing prefetching based on program analyses of increasing sophistication. <p> A variety of hardware fl Area Exam. October, 1993. schemes have been suggested, ranging from just lengthening cache lines [3, 19, 30] to simple next-block prefetching [3, 4, 10, 29, 30, 31] to complex lookahead mechanisms such as stream-buffer preloading [15], instruction pipeline prefetching <ref> [18, 19, 20] </ref>, and vector-style prefetching [8, 9]. Compilers are also implementing prefetching based on program analyses of increasing sophistication. <p> It can't be converted directly into a runtime or CPI value without knowing a great deal about the structure of the application and simulation model in question. In general, miss rate may not be well correlated with real speedup <ref> [19] </ref>, especially in multiprocessors, where decreased miss rate may come at an unacceptably high cost in terms of memory bandwidth.
Reference: [20] <author> Roland Lun Lee. </author> <title> The Effectiveness of Caches and Data Prefetch Buffers in Large-Scale Shared Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign,May 1987. </institution> <note> Also available as Illinois CSRD 670. </note>
Reference-contexts: A variety of hardware fl Area Exam. October, 1993. schemes have been suggested, ranging from just lengthening cache lines [3, 19, 30] to simple next-block prefetching [3, 4, 10, 29, 30, 31] to complex lookahead mechanisms such as stream-buffer preloading [15], instruction pipeline prefetching <ref> [18, 19, 20] </ref>, and vector-style prefetching [8, 9]. Compilers are also implementing prefetching based on program analyses of increasing sophistication.
Reference: [21] <author> Motorola. MCS88100: </author> <title> RISC Microprocessor User's Manual. </title> <institution> Motorola Inc., </institution> <year> 1988. </year>
Reference-contexts: Simple prefetching was available in hardware in the late '70s (e.g., in the IBM 370/168 [29]), but only recently have microprocessors begun providing the necessary hooks to allow prefetching to take place. Such processors include the IBM RS6000 [13], Motorola 88100 <ref> [21] </ref>, the DEC Alpha family [28], and the MIT Alewife group's Sparcle, among others. In this paper, we will begin by laying out a groundwork to assess various schemes, beginning in Section 2, and including an attempt to disentangle the various metrics used in prefetching papers.
Reference: [22] <author> Todd Mowry and Anoop Gupta. </author> <title> Tolerating latency through softwarecontrolled prefetching in shared-memory multiprocessors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 12(2) </volume> <pages> 87-106, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Thus, to approximate CPI, we use CP I = 1 + p m t mr . If we don't know p m for a given application, we have to estimate it; real codes range from 30 to 45 percent <ref> [22] </ref>, and numerical kernels are typically 50 to 60 percent. Thus a t mr improvement from 3 to 1.1 on a typical numerical kernel with p m = 0:5 might translate to an improvement of CPI from 2.5 to 1.55. <p> To understand how well a given architecture performs, we must have some sense of how well any architecture could perform given the application and memory subsystem in question. Mowry et al. give a good model of prefetching in their paper <ref> [22] </ref>; for a more detailed analysis of non-prefetching caches, see [1]. Let us use N to represent the total number of instructions, and W , R, and S to represent the number of write, read, and synchronization operations. <p> and s to represent the percentage of each kind of instruction overall): CPI = 1 + wl wb + rm p f pf (l pf + o pf ) + (1) rm p (1 f pf )(m s l miss + l fill ) + sl sync Mowry and Gupta <ref> [22] </ref> give data for N , W , R, S, m p , m s and l sync for their three benchmarks; appropriate data is provided in some other papers as well. (Data on the breakdown of cache hits into capacity, conflict, and compulsory misses can also be found in [12].) <p> This allows for intelligent prefetching based on knowledge of the code as a wholefor example, the ability to prefetch doubly-indexed arrays or linked listsand the potential for dramatically less hardware to achieve similar or better performance. 4.1 Hand-Inserted Prefetches Mowry and Gupta <ref> [22] </ref> study non-binding prefetching in a shared- memory multiprocessor. Their architectural model is based on the Stanford DASH multiprocessor, a shared-memory machine with memory distributed per-processor and with coherent caches maintained with a hardware directory protocol. <p> pf ) + rm p (1 f pf )(m s l miss + l fill ) + sl sync = 1 + rm p f pf + rm p (1 f pf )(m s l miss + l fill ) + sl sync Using the values on p. 93 of <ref> [22] </ref>, we compute the upper bounds on speedup for each application in Table 1. (We do this by taking the ratio of the computed CPI value with f pf = 0 and with f pf = 1.) We also use the given prefetch coverage values to compute the predicted speedup for <p> In [23], Mowry et al. describe the compiler support implemented after <ref> [22] </ref> was sent to press. They use a uniprocessor MIPS R4000-like model with pixie, with 8K primary and 256K secondary caches, both direct-mapped with 32-byte lines. First- level miss penalty is 12 cycles, and a miss to memory takes 75 cycles, with requests at most one every 20 cycles. <p> The prefetches are placed the right number of iterations in advance to compensate for memory latency. They get good results with their compiler algorithm, comparable to the results achieved by hand in <ref> [22] </ref>. The speedup in their benchmark set ranges from 1.05fi to 2fi, with 6 of the 13 benchmarks improving by over 1.45fi. 50% to 90% of the original memory cycles are eliminated.
Reference: [23] <author> Todd C. Mowry, Monica S. Lam, and Anoop Gupta. </author> <title> Design and eval-uation of a compiler algorithm for prefetching. </title> <booktitle> In Fifth International Conference on Architectural Support for ProgrammingLanguages and Operating Systems, </booktitle> <pages> pages 62-73, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: In <ref> [23] </ref>, Mowry et al. describe the compiler support implemented after [22] was sent to press. They use a uniprocessor MIPS R4000-like model with pixie, with 8K primary and 256K secondary caches, both direct-mapped with 32-byte lines. <p> Overall, it appears that the significantly more complex hardware scheme does not do significantly better than the simple software prefetch described in the previous section. If compiler technology such as that in <ref> [23] </ref> is available, and dusty-deck codes are not an issue, the RPT with LA-PC architecture appears not to be worth the extra cost for general-purpose machines. <p> The former market is generally a small one, and the latter can be expected to shrink with the steady migration of users to source-level portable systems, such as POSIX and Windows/NT, which can easily support software-oriented prefetching schemes. Software prefetching provides good latency coverage. Compiler- based schemes such as <ref> [23] </ref> or [27] provide the best results seen so far, with, e.g., speedups of about 5fi in systems with 120-cycle latencies. The hardware requirements are minimal: support for a prefetch instruction, possibly a prefetch-issue buffer, and a lockup- free cache.
Reference: [24] <author> Shien-Tai Pan, Kimming So, and Joseph T. Rahmeh. </author> <title> Improving the accuracy of dynamic branch prediction using branch correlation. </title> <booktitle> In The Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 76-84, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: To do this we use a branch prediction table, or BPT, such as described in <ref> [24] </ref>. The BPT keeps track of what direction a branch in the code is likely to go based on where it has jumped to in the past. <p> The LA-PC is restricted to a fixed number of instructions ahead of the real PC to avoid generating excessive prefetches, and its accuracy is constrained by its ability to guess branches correctly. A branch-prediction table such as that in <ref> [24] </ref> is used to improve the accuracy of the LA-PC. In theory, we would like to have the look ahead distance, d, be about the same as the latency to the next level of the memory hierarchy, ffi. Experimental verification shows that d 1:5ffi is a good value.
Reference: [25] <author> A. K. Porterfield. </author> <title> Software Methods for Improvement of Cache Performance on Supercomputer Applications. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> May </month> <year> 1989. </year> <note> Also available as Rice COMP TR 89-93. </note>
Reference-contexts: They show that two contexts can work well with prefetching, but by four contexts the negative effects overwhelm the positive ones. Prefetching a single loop ahead has also been examined by Callahan, Kennedy, and Porterfield in <ref> [3, 4, 25] </ref>. A somewhat more sophisticated analysis was carried out by Klaiber and Levy [16], who examined how to place prefetching instructions several loops ahead rather than a single loop.
Reference: [26] <author> Anne Rogers and Kai Li. </author> <title> Software support for speculative loads. </title> <booktitle> In Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 38-50, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: Based on the data from Chen and Baer and from Selvidge, simple non- blocking load support appears worth the cost of a simple interlock or scoreboarding scheme. 7 Adding Yet More Hardware: Speculative Loads A way of increasing the possible non-blocking load distance is outlined by Rogers and Li in <ref> [26] </ref>. They explicitly use speculative load instructions instead of loads when they are prefetching.
Reference: [27] <author> Charles Selvidge. </author> <title> Compilation-Based Prefetching for Memory Latency Tolerance. </title> <type> PhD thesis, </type> <institution> MIT, </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: Furthermore, note that different papers typically use different simulation environments. Memory latency (and memory models) vary dramatically in different papers, which can change speedup results by factors of three or more <ref> [27] </ref>. Cache sizes are also often widely different, as are problem sizes. These factors can be partly compensated for by examining papers which examine their architecture under a wide range of cache sizes or memory latencies, and then applying rough scaling factors to other results. <p> More esoteric strategies have also been discussed in the literature, but have reported no hard facts; one such is Johnson's article [14] on precomputing and prefetching (or prebroadcasting) entire working sets in multiprocessors. 4.3 General Compiler-Assisted Software Prefetching A recent thesis by Selvidge <ref> [27] </ref> proposes a compiler algorithm that has similar loop prefetching as Mowry et al., including support for sparse arrays and loop indexes. However, his research goes beyond traditional scientific code, and includes a separate algorithm to handle prefetching in non-loop contexts. <p> Combining the two, however, reduces it down to about 1.1, which is an overall performance improvement of almost three-fold. Further data on combining binding and non-binding prefetch is available in <ref> [27] </ref>. Selvidge models the effect of his prefetching algorithm on both stall and interlock memory models; in the stall model, reads and writes block, and in the interlock model, full/empty or scoreboard information is used to allow non-blocking loads. <p> Software prefetching provides good latency coverage. Compiler- based schemes such as [23] or <ref> [27] </ref> provide the best results seen so far, with, e.g., speedups of about 5fi in systems with 120-cycle latencies. The hardware requirements are minimal: support for a prefetch instruction, possibly a prefetch-issue buffer, and a lockup- free cache.
Reference: [28] <author> Dick Sites and Rich Witek. </author> <title> Alpha architecture technical summary. </title> <note> Available for anonymous FTP from gatekeeper.dec.com as pub/DEC/Alpha/technical-summary.txt, </note> <year> 1992. </year>
Reference-contexts: Simple prefetching was available in hardware in the late '70s (e.g., in the IBM 370/168 [29]), but only recently have microprocessors begun providing the necessary hooks to allow prefetching to take place. Such processors include the IBM RS6000 [13], Motorola 88100 [21], the DEC Alpha family <ref> [28] </ref>, and the MIT Alewife group's Sparcle, among others. In this paper, we will begin by laying out a groundwork to assess various schemes, beginning in Section 2, and including an attempt to disentangle the various metrics used in prefetching papers.
Reference: [29] <author> Alan Jay Smith. </author> <title> Sequential program prefetching in memory hierar-chies. </title> <journal> IEEE Computer, </journal> <volume> 11(12) </volume> <pages> 7-21, </pages> <month> December </month> <year> 1978. </year>
Reference-contexts: Many different solutions have been proposed to predict references enough in advance to prefetch them. A variety of hardware fl Area Exam. October, 1993. schemes have been suggested, ranging from just lengthening cache lines [3, 19, 30] to simple next-block prefetching <ref> [3, 4, 10, 29, 30, 31] </ref> to complex lookahead mechanisms such as stream-buffer preloading [15], instruction pipeline prefetching [18, 19, 20], and vector-style prefetching [8, 9]. Compilers are also implementing prefetching based on program analyses of increasing sophistication. <p> Compilers are also implementing prefetching based on program analyses of increasing sophistication. Simple prefetching was available in hardware in the late '70s (e.g., in the IBM 370/168 <ref> [29] </ref>), but only recently have microprocessors begun providing the necessary hooks to allow prefetching to take place. Such processors include the IBM RS6000 [13], Motorola 88100 [21], the DEC Alpha family [28], and the MIT Alewife group's Sparcle, among others.
Reference: [30] <author> Alan Jay Smith. </author> <title> Cache memories. </title> <journal> ACM Computing Surveys, </journal> <volume> 14(3) </volume> <pages> 473-530, </pages> <month> September </month> <year> 1982. </year>
Reference-contexts: Many different solutions have been proposed to predict references enough in advance to prefetch them. A variety of hardware fl Area Exam. October, 1993. schemes have been suggested, ranging from just lengthening cache lines <ref> [3, 19, 30] </ref> to simple next-block prefetching [3, 4, 10, 29, 30, 31] to complex lookahead mechanisms such as stream-buffer preloading [15], instruction pipeline prefetching [18, 19, 20], and vector-style prefetching [8, 9]. Compilers are also implementing prefetching based on program analyses of increasing sophistication. <p> Many different solutions have been proposed to predict references enough in advance to prefetch them. A variety of hardware fl Area Exam. October, 1993. schemes have been suggested, ranging from just lengthening cache lines [3, 19, 30] to simple next-block prefetching <ref> [3, 4, 10, 29, 30, 31] </ref> to complex lookahead mechanisms such as stream-buffer preloading [15], instruction pipeline prefetching [18, 19, 20], and vector-style prefetching [8, 9]. Compilers are also implementing prefetching based on program analyses of increasing sophistication.
Reference: [31] <author> Alan Jay Smith. </author> <title> Cache evaluation and the impact of workload choice. </title> <booktitle> In 12th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 64-73, </pages> <month> June </month> <year> 1985. </year>
Reference-contexts: Many different solutions have been proposed to predict references enough in advance to prefetch them. A variety of hardware fl Area Exam. October, 1993. schemes have been suggested, ranging from just lengthening cache lines [3, 19, 30] to simple next-block prefetching <ref> [3, 4, 10, 29, 30, 31] </ref> to complex lookahead mechanisms such as stream-buffer preloading [15], instruction pipeline prefetching [18, 19, 20], and vector-style prefetching [8, 9]. Compilers are also implementing prefetching based on program analyses of increasing sophistication.
References-found: 31

