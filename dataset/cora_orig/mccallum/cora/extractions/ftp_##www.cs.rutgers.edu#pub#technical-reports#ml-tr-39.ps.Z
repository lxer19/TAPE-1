URL: ftp://www.cs.rutgers.edu/pub/technical-reports/ml-tr-39.ps.Z
Refering-URL: http://www.cs.rutgers.edu/pub/technical-reports/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: gweiss@paul.rutgers.edu  
Title: Learning with Small Disjuncts  
Author: Gary M. Weiss 
Affiliation: Rutgers University/AT&T Bell Labs  
Abstract: Systems that learn from examples often create a disjunctive concept definition. The disjuncts in the concept definition which cover only a few training examples are referred to as small disjuncts. The problem with small disjuncts is that they are more error prone than large disjuncts, but may be necessary to achieve a high level of predictive accuracy [Holte, Acker, and Porter, 1989]. This paper extends previous work done on the problem of small disjuncts by investigating the reasons why small disjuncts are more error prone than large disjuncts, and evaluating the impact small disjuncts have on inductive learning. This paper shows that attribute noise, missing attributes, class noise, and training set size can each cause small disjuncts to be more error prone than large disjuncts. This paper also evaluates the impact that these factors have on learning with small disjuncts (i.e., on the error rate). It shows, for two artificial domains, that when low levels of attribute noise are applied only to the training set (the ability to learn the correct noise-free concept is being evaluated), small disjuncts are primarily responsible for making learning difficult. 
Abstract-found: 1
Intro-found: 1
Reference: [Ali and Pazzani, 1992] <author> Ali, K.M., and Pazzani, M.J., </author> <title> Reducing the Small Disjuncts Problem by Learning Probabilistic Concept Descriptions (Technical Report 92-111). </title> <address> Irvine, CA: </address> <institution> University of California, Dept. </institution> <note> of Information and Computer Sciences. </note> <editor> Forthcoming in Petsche (ed.), </editor> <booktitle> Computational Learning Theory and Natural Learning Systems, </booktitle> <volume> Vol. </volume> <pages> 3, </pages> <address> Cambridge, Massachusetts. </address> <publisher> MIT Press. </publisher>
Reference-contexts: This paper will demonstrate that various factors (most notably noise) cause small disjuncts to be more error prone than large disjuncts, and will then evaluate the impact that this has on learning. 2. Background Several papers have investigated the problem of small disjuncts [Holte, et. al, 1989], <ref> [Ali and Pazzani, 1992] </ref>, [Danyluk and Provost, 1993], and [Weiss, 1994], but none have provided a comprehensive explanation of why small disjuncts are more error prone than large disjuncts and under what circumstances they affect learning, and to what extent.
Reference: [Danyluk and Provost, 1993] <author> Danyluk, A.P. and Provost, F.J., </author> <title> Small Disjuncts in Action: Learning to Diagnose Errors in the Local Loop of the Telephone Network, </title> <booktitle> in Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <address> p81-88, </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Background Several papers have investigated the problem of small disjuncts [Holte, et. al, 1989], [Ali and Pazzani, 1992], <ref> [Danyluk and Provost, 1993] </ref>, and [Weiss, 1994], but none have provided a comprehensive explanation of why small disjuncts are more error prone than large disjuncts and under what circumstances they affect learning, and to what extent. <p> Holte et al., have shown that bias can cause small disjuncts to be more error prone than large disjuncts. Also of interest, Danyluk and Provost asserted that in the domain they were studying <ref> [Danyluk and Provost, 1993] </ref>, learning from noisy data was difficult because it is difficult to distinguish between noise and true exceptions, especially since errors in measurement and classification often occur systematically rather than randomly. <p> that: it contains common cases, rare cases and cases in between these two extremes the rare cases collectively cover a significant percentage of the overall cases This type of distribution has been seen in existing domains, including the KPa7KR chess endgame domain [Holte et al., 1989], the NYNEX MAX domain <ref> [Danyluk and Provost, 1993] </ref>, and the Wisconsin breast cancer domain [Weiss, 1994]. __________________ 2. Note that while "rare cases" may lead to small disjuncts, they are not the same thing. <p> The impact of small disjuncts is most significant at the 2% noise level. However, this impact is not significant enough to assert that small disjuncts are responsible for making learning difficult, when attribute noise (applied to both training and test sets) is present. 6.1.2 Systematic Noise In <ref> [Danyluk and Provost, 1993] </ref>, it states that it is the combination of small disjuncts and systematic noise which make learning difficult in the NYNEX MAX domain. Based on this, the experiments involving random attribute noise were repeated using systematic attribute noise.
Reference: [Holte et al., 1989] <author> Holte, R.C., Acker, L.E., and Porter, B.W., </author> <title> Concept Learning and the Problem of Small Disjuncts, </title> <booktitle> in Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <address> p813-818. San Mateo, California, </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The skewed distribution was designed so that: it contains common cases, rare cases and cases in between these two extremes the rare cases collectively cover a significant percentage of the overall cases This type of distribution has been seen in existing domains, including the KPa7KR chess endgame domain <ref> [Holte et al., 1989] </ref>, the NYNEX MAX domain [Danyluk and Provost, 1993], and the Wisconsin breast cancer domain [Weiss, 1994]. __________________ 2. Note that while "rare cases" may lead to small disjuncts, they are not the same thing.
Reference: [Langley and Kibler, 1991] <author> Langley, P. and Kibler, D., </author> <title> The Experimental Study of Machine Learning. </title>
Reference-contexts: The -m1 option is also used to disable the default stopping criterion. The default value for the -m option is 2, which stops a node from being split if the resulting nodes will have 2 or fewer outcomes. 4. This definition is consistent with the one used in <ref> [Langley and Kibler, 1991] </ref>, but is very different from the one in [Quinlan, 1986], in which the same value can be selected again. For classes with two values, this makes a factor of 2 difference. 5.
Reference: [Quinlan, 1986] <author> Quinlan, J.R., </author> <title> The Effect of Noise on Concept Learning, </title> <booktitle> in Machine Learning, an Artificial Intelligence Approach (Volume II), </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The default value for the -m option is 2, which stops a node from being split if the resulting nodes will have 2 or fewer outcomes. 4. This definition is consistent with the one used in [Langley and Kibler, 1991], but is very different from the one in <ref> [Quinlan, 1986] </ref>, in which the same value can be selected again. For classes with two values, this makes a factor of 2 difference. 5. This does not imply that the noise we are attempting to model has a random component. <p> Frequently, however, when the effects of noise are studied, noise is applied to the training set but not to the test set <ref> [Quinlan, 1986] </ref>. In this case, what is being studied is the ability to learn the correct concept (i.e., the noise free definition) when noise is present. Noise can be thought of as having two distinct, albeit interacting, effects [Weiss, 1994]: 1.
Reference: [Quinlan, 1993] <author> Quinlan, J.R., C4.5: </author> <title> Programs for Machine Learning, </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The Experiments For each experiment, five independent runs are performed, and the results averaged together. For each run, the cases are randomly split into disjoint training and test sets, which are fed into C4.5, a program for inducing decision trees from a set of preclassified training examples <ref> [Quinlan, 1993] </ref>. C4.5 was modified by the author to collect statistics relating to disjunct size and to disable the default pruning strategy 3 (pruning might obscure the small disjuncts in the underlying concept definition). The cases from the problem domains are initially noise-free.
Reference: [Weiss, 1994] <author> Weiss, G.M., </author> <title> The Problem with Noise and Small Disjuncts, </title> <type> (Technical Report ML-TR-38). </type> <institution> New Brunswick, NJ: Rutgers University, Dept. of Computer Science. </institution>
Reference-contexts: Background Several papers have investigated the problem of small disjuncts [Holte, et. al, 1989], [Ali and Pazzani, 1992], [Danyluk and Provost, 1993], and <ref> [Weiss, 1994] </ref>, but none have provided a comprehensive explanation of why small disjuncts are more error prone than large disjuncts and under what circumstances they affect learning, and to what extent. <p> A variant of this assertion that learning from noisy data is difficult because it is difficult to distinguish between noise and true exceptions, was investigated in <ref> [Weiss, 1994] </ref>. However, that paper only experimented with random class noise. This paper extends the work presented in [Weiss, 1994]. It utilizes artificial domains over which greater experimental control can be exerted, and also investigates the effect that systematic and random attribute noise have on learning with small disjuncts. 3. <p> A variant of this assertion that learning from noisy data is difficult because it is difficult to distinguish between noise and true exceptions, was investigated in <ref> [Weiss, 1994] </ref>. However, that paper only experimented with random class noise. This paper extends the work presented in [Weiss, 1994]. It utilizes artificial domains over which greater experimental control can be exerted, and also investigates the effect that systematic and random attribute noise have on learning with small disjuncts. 3. <p> between these two extremes the rare cases collectively cover a significant percentage of the overall cases This type of distribution has been seen in existing domains, including the KPa7KR chess endgame domain [Holte et al., 1989], the NYNEX MAX domain [Danyluk and Provost, 1993], and the Wisconsin breast cancer domain <ref> [Weiss, 1994] </ref>. __________________ 2. Note that while "rare cases" may lead to small disjuncts, they are not the same thing. Rare cases exist in the underlying population from which training and test cases are chosen, while small disjuncts exist in the structure built during learning. <p> In this case, what is being studied is the ability to learn the correct concept (i.e., the noise free definition) when noise is present. Noise can be thought of as having two distinct, albeit interacting, effects <ref> [Weiss, 1994] </ref>: 1. Noise applied to the training set perturbs the concept definition that is learned (this results in additional errors even if no noise is applied to the test set). 2. Noise applied to the test set causes additional errors (even if the correct concept definition is learned). <p> It would be useful to experiment with real world domains, but interpreting the results from these domains is more difficult than for artificial domains (see section 4). Nonetheless, such experimentation can provide some insight. Real world domains were used in <ref> [Weiss, 1994] </ref>, but only the effect of random class noise was examined. <p> For large training set sizes, learning with small disjuncts (i.e., with a skewed distribution) is more difficult than learning from a uniform distribution, but below a certain training set size, the opposite is true. 9. Acknowledgements Thanks to Andrea Danyluk and Rob Holte for their comments on <ref> [Weiss, 1994] </ref> (the predecessor of this paper), and to Foster Provost and especially Haym Hirsh for comments on both that paper and this current paper. -13-
References-found: 7

