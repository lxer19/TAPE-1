URL: ftp://hpsl.cs.umd.edu/pub/papers/isas96.ps.Z
Refering-URL: http://www.cs.umd.edu/projects/hpsl/papers.brandnew/LocalResources/tech-10-23.htm
Root-URL: 
Title: Interleaved Parallel Hybrid Arnoldi Method for a Parallel Machine and a Network of Workstations  
Author: Guy Edjlali Serge G. Petiton Nahid Emad 
Address: College Park, MD 20742, USA  59655 Villeneuve d'Ascq Cedex, FRANCE  Versailles, 45 Avenue des Etats-Unis, 78035 Versailles Cedex, FRANCE  
Affiliation: UMIACS and Computer Science Department, University of Maryland,  Laboratoire d'Informatique Fondamentale de Lille, Universite des Sciences et Technologies de Lille  Laboratoire PRISM, Universite de  
Abstract: The data-parallel paradigm which is effective for many numerical algebra computations is no more sufficient when programming heterogeneous machines. To efficiently utilize these new resources we need to combine the two paradigms: data-parallelism and task parallelism. This leads us to define new algorithms. In this paper, throughout an example we will illustrate this statement. The chosen example is the computation of some eigenvalues of large non-structured sparse non-Hermitian matrices. We present a new algorithm, Interleaved Arnoldi, and compare its performances to the parallel Arnoldi algorithm. The different environments we consider are: a CM5, a CM5 and a Sparc 20, a CM5 and 2 Sparc 20. We show that significant results can be obtained even when the heterogeneous machine consists of a CM5 and 2 Sparc 20 workstations. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Brezinski and M. R. Zaglia, </author> <title> A hybrid procedure for solving linear systems, </title> <type> Tech. Rep. </type> <institution> ANO-283, USTL, </institution> <month> September </month> <year> 1992. </year>
Reference: [2] <author> K. Chandy, I. Foster, K. Kennedy, C. Koelbel, and C. Tseng, </author> <title> Integrated Support for Task and Data Parallelism. </title> <journal> Journal of Supercomputing Applications, </journal> <volume> 8(2), </volume> <year> 1994. </year>
Reference-contexts: Integration of task and data-parallelism have received a lot attention during these last years. Some research are based on enhancements of data parallel languages or task parallel languages in order to integrate data parallelism and task parallelism. Fx [12] uses additional HPF directives to specify task parallelism. Fortran M <ref> [2] </ref> makes extensions to Fortran77 for task parallel computations, and also allows some data distribution statements. Braid [14] introduces data parallel extensions to the Men-tat Programming Language [10].
Reference: [3] <author> G. Edjlali, </author> <title> Contribution to the parallelization of hybrid iterative methods for sparse matrices on heterogeneous architecture (in French), </title> <type> PhD Thesis, </type> <institution> University of Paris 6, </institution> <month> December </month> <year> 1994. </year>
Reference-contexts: The cost of each iteration increases with the size of the sub-space. The number of iterations decreases with the size of the subspace. There is then a tradeoff to find between the execution time of each iteration and the number of iterations to reach convergence <ref> [6, 3] </ref>. The authors introduced in [4] a new adaptation of the iterative Arnoldi method, called the Hybrid Arnoldi method. The main differences with the classical iterative Arnoldi are : We start l Arnoldi processes with different initial guesses v i . <p> When m increases, the interest of the Heterogeneous Arnoldi over Parallel Arnoldi becomes more significant. These results are supported by the theory which says that the number of operations required for the projection phase is approximately n fl m 2 + n fl C fl m <ref> [3] </ref>, while it is 10 fl m 3 for the QR [9]. 4 Interleaved Arnoldi In the previous section, we have reduced the time of each iteration by parallelizing the Arnoldi method and by distributing each task on adapted architectures. <p> This algorithm is based on the use of the Arnoldi Hybrid method in an heterogeneous programming environment. The Hybrid Arnoldi method, presented on section 2, combines the intermediate results of different Arnoldi processes to accelerate the convergence. We implemented it <ref> [3, 8] </ref> on a network of super-computers and verified some of its numerical behaviors.
Reference: [4] <author> G. Edjlali, N. Emad, and S. Petiton, </author> <title> Hybrid Method on Network of Heterogeneous Parallel Computers, </title> <booktitle> Proceedings of the 14th IMACS World Congress, </booktitle> <address> Atlanta, </address> <month> July 11-15, </month> <year> 1994. </year>
Reference-contexts: The number of iterations decreases with the size of the subspace. There is then a tradeoff to find between the execution time of each iteration and the number of iterations to reach convergence [6, 3]. The authors introduced in <ref> [4] </ref> a new adaptation of the iterative Arnoldi method, called the Hybrid Arnoldi method. The main differences with the classical iterative Arnoldi are : We start l Arnoldi processes with different initial guesses v i .
Reference: [5] <author> N. Emad, S. Petiton, and G. Edjlali, </author> <title> The Iterative Arnoldi Hybrid Method, </title> <note> To appear. </note>
Reference-contexts: We chose the m i ( m k t n; 8k = 1; ` ) with respect to memory space and computational throughput. The Arnoldi (l,r,m,V ) algorithm is detailed in figure 1. Its numerical properties are described in <ref> [5] </ref>. The functions f i are often taken as linear combination of Ritz vectors [11]. One option is to choose the new initial guess v i as a function of the best Ritz vectors already computed. Throughout this paper, the experiments are carried with this assumption.
Reference: [6] <author> S. Petiton, </author> <title> Parallel Subspace Method for non-Hermitian Eigenproblems on the Connection Machine, Applied Numerical Mathematics Journal 10, </title> <publisher> North Holland, </publisher> <year> 1992. </year>
Reference-contexts: Therefore, this method can be divided in two main phases: a projection phase and a Ritz computation phase. After each iteration, we restart the method with a linear combination of some of the Ritz vectors [11]. The projection phase is highly parallel <ref> [6] </ref> and a data-parallel implementation on a CM-5 with CM-Fortran has been effectuated. The second phase is vectorial: as we do not have an easy access to a vectorial machine, a sequential version of it has been implemented. We call Parallel Arnoldi this implementation. <p> The cost of each iteration increases with the size of the sub-space. The number of iterations decreases with the size of the subspace. There is then a tradeoff to find between the execution time of each iteration and the number of iterations to reach convergence <ref> [6, 3] </ref>. The authors introduced in [4] a new adaptation of the iterative Arnoldi method, called the Hybrid Arnoldi method. The main differences with the classical iterative Arnoldi are : We start l Arnoldi processes with different initial guesses v i .
Reference: [7] <author> S. Petiton and G. Edjlali, </author> <title> Data parallel structures and algorithms for sparse matrix computation, </title> <booktitle> in Advances in Parallel Computing, </booktitle> <publisher> North-Holland, </publisher> <year> 1993. </year> <title> on the CM5, the Heterogeneous Arnoldi using the CM5 and one and the Interleaved Arnoldi using the CM5 two Sparc 20 computers, for C-distributed matrices with C=64. </title>
Reference-contexts: The sparse matrix-vector multiplication operation is the most costly operation used by the projection task. In a previous work <ref> [7] </ref>, we had shown that ma Initialization set a number l of different Arnoldi processes, a set of subspace size m i for i = 1; l, with m = (m 1 ; ; m ` ) T , a set of initial guess v 1 ; ; v ` with <p> C is the maximum number of non-zero elements per row or per column. The sparse matrices are stored using the SGP [13] or S 3 format <ref> [7] </ref>.
Reference: [8] <author> S.Petiton, G.Edjlali, </author> <title> Krylov Subspace Methods on Multi-Connection Machines Environment, </title> <booktitle> Proceedings of the Second European CM Users Meeting, </booktitle> <pages> pp 17-28, </pages> <editor> Jean-Michel Alimi et al. Editors, Meudon, </editor> <address> France, </address> <year> 1993. </year>
Reference-contexts: This algorithm is based on the use of the Arnoldi Hybrid method in an heterogeneous programming environment. The Hybrid Arnoldi method, presented on section 2, combines the intermediate results of different Arnoldi processes to accelerate the convergence. We implemented it <ref> [3, 8] </ref> on a network of super-computers and verified some of its numerical behaviors.
Reference: [9] <author> G. Golub and C.V. Loan, </author> <title> Matrix Computations, </title> <publisher> North Oxford Academic, </publisher> <year> 1983. </year>
Reference-contexts: These results are supported by the theory which says that the number of operations required for the projection phase is approximately n fl m 2 + n fl C fl m [3], while it is 10 fl m 3 for the QR <ref> [9] </ref>. 4 Interleaved Arnoldi In the previous section, we have reduced the time of each iteration by parallelizing the Arnoldi method and by distributing each task on adapted architectures. Having done so, the parallel (resp. sequential) machine is idle when the sequential (resp. parallel) machine is computing.
Reference: [10] <author> A. Grimshaw, </author> <title> The Mentat computation model data-driven support for object-oriented parallel processing. </title> <type> Technical Report CS-93-30, </type> <institution> University of Virginia, </institution> <month> May 93. </month>
Reference-contexts: Fx [12] uses additional HPF directives to specify task parallelism. Fortran M [2] makes extensions to Fortran77 for task parallel computations, and also allows some data distribution statements. Braid [14] introduces data parallel extensions to the Men-tat Programming Language <ref> [10] </ref>.
Reference: [11] <author> Y. Saad, </author> <title> Numerical Methods for Large Eigenvalue Problems, </title> <publisher> Manchester University Press, </publisher> <year> 1993. </year>
Reference-contexts: Therefore, this method can be divided in two main phases: a projection phase and a Ritz computation phase. After each iteration, we restart the method with a linear combination of some of the Ritz vectors <ref> [11] </ref>. The projection phase is highly parallel [6] and a data-parallel implementation on a CM-5 with CM-Fortran has been effectuated. The second phase is vectorial: as we do not have an easy access to a vectorial machine, a sequential version of it has been implemented. <p> The Hybrid Arnoldi (l,r,m,V ) approximates one eigenpair ( j , w j ) of A by the Ritz eigenvalues and eigenvectors ( j ; w j ) using Gramm-Schmidt orthogonaliza-tion process <ref> [11] </ref> ; i = 1; l and j = 1; r. We chose the m i ( m k t n; 8k = 1; ` ) with respect to memory space and computational throughput. The Arnoldi (l,r,m,V ) algorithm is detailed in figure 1. <p> The Arnoldi (l,r,m,V ) algorithm is detailed in figure 1. Its numerical properties are described in [5]. The functions f i are often taken as linear combination of Ritz vectors <ref> [11] </ref>. One option is to choose the new initial guess v i as a function of the best Ritz vectors already computed. Throughout this paper, the experiments are carried with this assumption. An important advantage of this algorithm is its large coarse granularity.
Reference: [12] <author> J. Subhlok, J. Stichnoth, D. O'Hallaron, and T. Gross, </author> <title> Exploiting task and data parallelism on a multicomputer. </title> <booktitle> In Proceedings of the Fourth ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming (PPOPP), </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: Integration of task and data-parallelism have received a lot attention during these last years. Some research are based on enhancements of data parallel languages or task parallel languages in order to integrate data parallelism and task parallelism. Fx <ref> [12] </ref> uses additional HPF directives to specify task parallelism. Fortran M [2] makes extensions to Fortran77 for task parallel computations, and also allows some data distribution statements. Braid [14] introduces data parallel extensions to the Men-tat Programming Language [10].
Reference: [13] <author> C. Weill-Duflos, </author> <title> Optimisation de methodes de resolution iteratives de grand systemes lineaire creux sur machine massivement parallele, </title> <type> PhD Thesis, </type> <institution> University Of Paris 6,1994. </institution>
Reference-contexts: Figures 3 and 4 illustrate the results for C-diagonal matrices when C=3 and C=63, while Figures 5 and 6 present results for C-distributed matrices when C=4 and C=64. C is the maximum number of non-zero elements per row or per column. The sparse matrices are stored using the SGP <ref> [13] </ref> or S 3 format [7].
Reference: [14] <author> E .West and A. Grishaw, </author> <title> Braid: Integrating Task and Data Parallelism. </title> <booktitle> In Proceedings of the Fifth Symposium on the Frontiers of Massively Parallel Computation. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <month> Febru-ary </month> <year> 1995. </year>
Reference-contexts: Fx [12] uses additional HPF directives to specify task parallelism. Fortran M [2] makes extensions to Fortran77 for task parallel computations, and also allows some data distribution statements. Braid <ref> [14] </ref> introduces data parallel extensions to the Men-tat Programming Language [10].
Reference: [15] <author> Message Passing Interface Forum. </author> <title> Document for a Standard Message-Passing Interface. </title> <type> Technical Report CS-93-214, </type> <institution> University of Tennessee, </institution> <month> November </month> <year> 1993. </year> <note> machines </note>
Reference-contexts: Fx [12] uses additional HPF directives to specify task parallelism. Fortran M [2] makes extensions to Fortran77 for task parallel computations, and also allows some data distribution statements. Braid [14] introduces data parallel extensions to the Men-tat Programming Language [10]. Communication libraries like PVM and MPI <ref> [15] </ref> may be used by the programmer to directly transfer messages from one data parallel task to another. 2 Parallel implementation of the Arnoldi method The iterative Arnoldi method computes the r algebraically largest eigenvalues and associated eigenvec-tors of a very large non-Hermitian matrix A of size n.
References-found: 15

