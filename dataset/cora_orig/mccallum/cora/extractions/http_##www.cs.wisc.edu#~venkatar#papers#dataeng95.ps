URL: http://www.cs.wisc.edu/~venkatar/papers/dataeng95.ps
Refering-URL: http://www.cs.wisc.edu/~venkatar/venkatar.html
Root-URL: 
Title: The Impact of Data Placement on Memory Management for Multi-Server OODBMS  
Author: Shivakumar Venkataraman Miron Livny Jeffrey F. Naughton 
Affiliation: Computer Sciences Department University of Wisconsin-Madison  
Abstract: In this study we demonstrate the close relationship between data placement and memory management for symmetric multi-server OODBMS. We propose and investigate memory management algorithms for two data placement strategies, namely declustering and clustering. Through a detailed simulation, we show that by declustering the data most of the benefits of complex global memory management algorithms are realized by simple algorithms. In contrast we show that when data is clustered, the simple algorithms perform poorly. 
Abstract-found: 1
Intro-found: 1
Reference: [BG88] <author> D. Bitton and J. Gray. </author> <title> "Disk Shadowing". </title> <booktitle> In Proc of VLDB . LA, </booktitle> <year> 1988. </year>
Reference-contexts: Object Size One page Cache Size 4 Pages/Context Per Object 30000 Inst Link Bandwidth 50 MB/sec Cache Context 8 Msg Protocol 11000 Inst Message Cost 550 s/8KB Transfer Rate 3.09 MB/sec Object Size 1 Page Message Buffers 160KB/Node Rotation 16.667 ms Lock Timeout 500 ms Memory Size/Node 8MB Seek Time <ref> [BG88] </ref> 0.618* p SDist Think Time 0 ms Settle Time 2.0 ms Cold Time 150000 m Sim Time 400000 ms Table 2: Simulation Parameters termines the fraction of the accesses directed towards the hot region.
Reference: [CBZ91] <author> J. Carter, J. Bennett, and W. Zwaenepoel. </author> <title> Implementation and Performance of Munin. </title> <booktitle> In Proc of SOSP, </booktitle> <address> Pacific Grove, CA, </address> <month> Oct </month> <year> 1991. </year>
Reference-contexts: There is a vast amount of literature on memory management in distributed memory systems, <ref> [LH89, CBZ91] </ref> have focussed on efficiently maintaining memory consistency, for the most part ignoring the issues of paging between nodes or to disk. [ILP93, FZ91] have studied how remote memory servers can be used as a backing store to avoid paging to disk, but have not concerned themselves with issues relating
Reference: [CFLS91] <author> M. Carey, M. Franklin, M. Livny, and E. Shekita. </author> <title> Data Caching Tradeoffs in Client-Server DBMS Architectures. </title> <booktitle> In Proc of SIG-MOD, </booktitle> <address> Denver, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: In the object oriented database world, [WR91, DY93, Rah93] have done performance studies on issues relating to: buffer coherency policies, performance of caching dirty pages at clients across transaction boundaries, the impact of caching pages at remote nodes, and their influence on recovery. <ref> [CFLS91, FCL92] </ref> have studied memory management issues for client-server architectures.
Reference: [Deu91] <author> O Deux. </author> <title> The O2 System. </title> <journal> Comm of the ACM, </journal> <volume> 34(10), </volume> <month> Oct </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Traditionally Object Oriented DataBase Management Systems (OODBMS) <ref> [Gro93, Deu91] </ref> have been built with a client-server architecture. Under heavy workloads the single server becomes a bottleneck. An obvious approach to solve this problem is to add more servers to the system; however, client-server systems do not scale well with the addition of more servers.
Reference: [DY93] <author> A. Dan and P S. Yu. </author> <title> Performance Analysis of Buffer Coherency Policies in a Multisyst em Data Sharing Environment. </title> <journal> IEEE Trans on PDS, </journal> <volume> 4(3), </volume> <month> Mar </month> <year> 1993. </year>
Reference-contexts: In the object oriented database world, <ref> [WR91, DY93, Rah93] </ref> have done performance studies on issues relating to: buffer coherency policies, performance of caching dirty pages at clients across transaction boundaries, the impact of caching pages at remote nodes, and their influence on recovery. [CFLS91, FCL92] have studied memory management issues for client-server architectures.
Reference: [FCL92] <author> M. Franklin, M. Carey, and M. Livny. </author> <title> Global Memory Management in Client-Server DBMS Architectures. </title> <booktitle> In Procs of VLDB, </booktitle> <month> Aug </month> <year> 1992. </year>
Reference-contexts: In the object oriented database world, [WR91, DY93, Rah93] have done performance studies on issues relating to: buffer coherency policies, performance of caching dirty pages at clients across transaction boundaries, the impact of caching pages at remote nodes, and their influence on recovery. <ref> [CFLS91, FCL92] </ref> have studied memory management issues for client-server architectures. <p> The locking and coherency strategy that we adopt is a slight variation of call back locking [WR91]. Call back locking is a pessimistic, lock-based protocol which has been shown to perform well for a wide range of workloads for client-server database systems <ref> [FCL92] </ref>. Call back locking uses invalidations to ensure "read multiple, write one" memory coherence. 4 Data Placement Strategies OODBMS provide mechanisms to group related objects. For example: Objectivity provides containers, Ontos has segments and files, and SHORE provides pools. <p> a copy, since we use the "read multiple, write one" technique to ensure con sistency of the pages. * LRU with Hate Hints: Hate This policy is similar to the base algorithm, except that the owner marks the page that it reads from disk as "hated", and retains locally as <ref> [FCL92] </ref>. This is in effect a hint, to the buffer replacement policy at the owner, that this page is a good candidate for replacement; as the page has a duplicate in memory and it is not used by a client connected to the owner .
Reference: [FZ91] <author> E. Felten and J. Zahorjan. </author> <title> Issues in the Implementation of a Remote Memory Paging System. </title> <type> Tech Report, </type> <institution> Univ of Washington, </institution> <month> Mar </month> <year> 1991. </year>
Reference-contexts: There is a vast amount of literature on memory management in distributed memory systems, [LH89, CBZ91] have focussed on efficiently maintaining memory consistency, for the most part ignoring the issues of paging between nodes or to disk. <ref> [ILP93, FZ91] </ref> have studied how remote memory servers can be used as a backing store to avoid paging to disk, but have not concerned themselves with issues relating to data placement, recovery, sharing and concurrency. [LWY93] used analytical modeling to study replication management algorithms for distributed memory systems.
Reference: [GCK88] <author> E. Boughter G. Copeland, W. Alexander and T. Keller. </author> <title> Data placement in bubba. </title> <booktitle> In SIG-MOD, </booktitle> <pages> pages 99-108, </pages> <address> Chicago, IL, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: The details of the simulator is presented as background material for the performance analysis in Section 7. We analyze the performance of memory management algorithms for the two data placement strategies on a variety of workloads in Section 8. 2 Related Work Data placement studies <ref> [Gha90, GCK88] </ref> for parallel shared nothing Relational DataBase Management Systems (RDBMS) have demonstrated the need for declustering relations to achieve high performance.
Reference: [Gha90] <author> Shahram Ghandeharzadeh. </author> <title> Physical Database Design in Multiprocessor Systems. </title> <type> PhD thesis, </type> <institution> Univ of Wisconsin-Madison, </institution> <year> 1990. </year>
Reference-contexts: The details of the simulator is presented as background material for the performance analysis in Section 7. We analyze the performance of memory management algorithms for the two data placement strategies on a variety of workloads in Section 8. 2 Related Work Data placement studies <ref> [Gha90, GCK88] </ref> for parallel shared nothing Relational DataBase Management Systems (RDBMS) have demonstrated the need for declustering relations to achieve high performance.
Reference: [Gro93] <author> EXODUS Project Group. </author> <title> EXODUS Storage Manager Architectural Overview . EXODUS Project Document, </title> <note> 1993. available by ftp from ftp.cs.wisc.edu. </note>
Reference-contexts: 1 Introduction Traditionally Object Oriented DataBase Management Systems (OODBMS) <ref> [Gro93, Deu91] </ref> have been built with a client-server architecture. Under heavy workloads the single server becomes a bottleneck. An obvious approach to solve this problem is to add more servers to the system; however, client-server systems do not scale well with the addition of more servers.
Reference: [GRO94] <author> SHORE GROUP. </author> <title> Shoring Up Persistent Applications. </title> <booktitle> Proc of SIGMOD, </booktitle> <month> May </month> <year> 1994. </year>
Reference-contexts: Under heavy workloads the single server becomes a bottleneck. An obvious approach to solve this problem is to add more servers to the system; however, client-server systems do not scale well with the addition of more servers. In order to avoid scaling problems inherent in client-server architectures, the SHORE <ref> [GRO94] </ref> persistent object base system employs a symmetric peer to peer communication architecture that facilitates replacing the single server with a collection of cooperating servers. The focus of our research is to develop and study the performance of memory management algorithms for such a collection of servers. <p> In contrast, when the data is clustered these simple algorithms perform very poorly. 1.1 Outline We begin this paper with a brief description of the related work in Section 2 and describe the SHORE <ref> [GRO94] </ref> architecture in Section 3. Section 4 and Section 5 contain descriptions of data placement and memory management algorithms. An impractical to implement but optimal algorithm that serves as a baseline memory management algorithm for comparing other algorithm is also presented. <p> study, we show that these two issues are related and demonstrate that, by placing data appropriately simple memory management algorithms are sufficient to effectively utilize the aggregate system memory. 3 SHORE Architecture SHORE (Scalable Heterogeneous Object REpository) is a new persistent object system under development at the University of Wisconsin <ref> [GRO94] </ref>. A primary goal for SHORE is to provide a robust, high-performance, persistent object system that is flexible enough to be employed in a wide range of applications and computing environments.
Reference: [ILP93] <author> Liviu Iftode, Kai Li, and Karin Petersen. </author> <title> Memory servers for multicomputers. </title> <booktitle> In Proc of IEEE Spring COMPCON '93, </booktitle> <pages> pp 534-547, </pages> <month> Feb </month> <year> 1993. </year>
Reference-contexts: There is a vast amount of literature on memory management in distributed memory systems, [LH89, CBZ91] have focussed on efficiently maintaining memory consistency, for the most part ignoring the issues of paging between nodes or to disk. <ref> [ILP93, FZ91] </ref> have studied how remote memory servers can be used as a backing store to avoid paging to disk, but have not concerned themselves with issues relating to data placement, recovery, sharing and concurrency. [LWY93] used analytical modeling to study replication management algorithms for distributed memory systems.
Reference: [LH89] <author> K. Li and P. Hudak. </author> <title> Memory Coherence in Shared Virtual Memory Systems. </title> <journal> ACM TOCS, </journal> <volume> 7(4), </volume> <month> Nov </month> <year> 1989. </year>
Reference-contexts: There is a vast amount of literature on memory management in distributed memory systems, <ref> [LH89, CBZ91] </ref> have focussed on efficiently maintaining memory consistency, for the most part ignoring the issues of paging between nodes or to disk. [ILP93, FZ91] have studied how remote memory servers can be used as a backing store to avoid paging to disk, but have not concerned themselves with issues relating
Reference: [LWY93] <author> A. Leff, J. Wolf, and P. Yu. </author> <title> Replication Algorithms in a Remote Caching Architecture. </title> <journal> IEEE Trans on PDIS, </journal> <volume> 4, </volume> <month> Aug </month> <year> 1993. </year>
Reference-contexts: consistency, for the most part ignoring the issues of paging between nodes or to disk. [ILP93, FZ91] have studied how remote memory servers can be used as a backing store to avoid paging to disk, but have not concerned themselves with issues relating to data placement, recovery, sharing and concurrency. <ref> [LWY93] </ref> used analytical modeling to study replication management algorithms for distributed memory systems.
Reference: [Rah93] <author> E Rahm. </author> <title> Empirical Performance Evaluation of Concurrency and Coherency Control Protocols for Databbase Sharing Systems. </title> <journal> ACM TODS, </journal> <volume> 18(2), </volume> <month> June </month> <year> 1993. </year>
Reference-contexts: In the object oriented database world, <ref> [WR91, DY93, Rah93] </ref> have done performance studies on issues relating to: buffer coherency policies, performance of caching dirty pages at clients across transaction boundaries, the impact of caching pages at remote nodes, and their influence on recovery. [CFLS91, FCL92] have studied memory management issues for client-server architectures.
Reference: [Sch90] <author> Herb Schwetman. </author> <title> Csim users' guide. </title> <type> MCC Tech Report ACT-126-90, </type> <institution> Microelectronics and Computer Technology Corp., </institution> <month> March </month> <year> 1990. </year>
Reference-contexts: The simulator was written using the CSIM/C++ <ref> [Sch90] </ref> process oriented simulation package. The parameters of the simulation model is shown in Table 2 The hardware platform simulated is a shared nothing distributed memory architecture, with a server on each node. The number of physical nodes is controlled by the N parameter.
Reference: [Ven95] <author> Shivakumar Venkataraman and Miron Livny and Jefferey Naughton. </author> <title> Impact of Declustering on Memory Mangement for Symmetric Server OODBMS. </title> <type> Tech Report, </type> <institution> Univ of Wisconsin-Madison, </institution> <note> In Preperation. </note>
Reference-contexts: Although we do not provide for any specific mechanisms to control duplication, we explore the effect of sharing on performance in the fifth experiment. We also performed experiments with workloads involving cold accesses, small clients and for a large system configurations. Details of these can be found in <ref> [Ven95] </ref>. 8.2 Expt 1:Client's Placement When the locality set is clustered the client has two options. a) it can access its private locality set by connecting to the server that owns it, or b) the client can access its private locality set by connecting to some other server.
Reference: [WR91] <author> Y. Wang and L. Rowe. </author> <title> Cache Consistency and Concurrency Control in a Client/Server DBMS Architecture. </title> <booktitle> Proc of SIGMOD, </booktitle> <month> June </month> <year> 1991. </year>
Reference-contexts: In the object oriented database world, <ref> [WR91, DY93, Rah93] </ref> have done performance studies on issues relating to: buffer coherency policies, performance of caching dirty pages at clients across transaction boundaries, the impact of caching pages at remote nodes, and their influence on recovery. [CFLS91, FCL92] have studied memory management issues for client-server architectures. <p> The client then places the object in its object cache. The locking and coherency strategy that we adopt is a slight variation of call back locking <ref> [WR91] </ref>. Call back locking is a pessimistic, lock-based protocol which has been shown to perform well for a wide range of workloads for client-server database systems [FCL92].
References-found: 18

