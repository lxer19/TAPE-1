URL: ftp://ftp.cs.uu.nl/pub/RUU/CS/techreps/CS-1998/1998-14.ps.gz
Refering-URL: http://www.cs.ruu.nl/docs/research/publication/TechList1.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: e-mail: hansz@cs.ruu.nl  
Title: Decision Trees: Equivalence and Propositional Operations  
Author: Hans Zantema 
Address: Padualaan 14, P.O. box 80.089, 3508 TB Utrecht, The Netherlands,  
Affiliation: Department of Computer Science, Utrecht University,  
Abstract: For the well-known concept of decision trees as it is used for inductive inference we study the natural concept of equivalence: two decision trees are equivalent if and only if they represent the same hypothesis. We present a simple efficient algorithm to establish whether two decision trees are equivalent or not. The complexity of this algorithm is bounded by the product of the sizes of both decision trees. The hypothesis represented by a decision tree is essentially a boolean function, just like a proposition. Although every boolean function can be represented in this way, we show that disjunctions and conjunctions of decision trees can not efficiently be represented as decision trees, and simply shaped propositions may require exponential size for representation as de cision trees.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Bryant, R. E. </author> <title> Graph-based algorithms for boolean function manipulation. </title> <journal> IEEE Transactions on Computers C-35, </journal> <volume> 8 (1986), </volume> <pages> 677-691. </pages>
Reference-contexts: In this paper we studied this equivalence relation for the class of decision trees. First we proposed a simple and efficient algorithm for establishing equivalence. This result is quite subtle, for instance for BDDs as presented in <ref> [1] </ref>, that only differ from decision trees in allowing sharing of subtrees, the question of establishing equivalence is NP-complete since every proposition can be represented in linear time as a BDD. Next we investigated how to express disjunctions and conjunctions of decision trees as decision trees again.
Reference: 2. <author> Cockett, J. R. B. </author> <title> Discrete decision theory: Manipulations. </title> <booktitle> Theoretical Computer Science 54 (1987), </booktitle> <pages> 215-236. </pages>
Reference-contexts: A some sloppier proof of the same observation has been given in <ref> [2] </ref>. 4 Deciding equivalence Given two decision trees T and U , how can we decide whether they are equivalent or not? A trivial algorithm for this is computing and comparing (T; s) and (U; s) for all 2 #A values for s. <p> It is not efficient: in worst case it is exponential in the size of the decision trees. Here the size size (T ) of a decision tree T is defined to be the number of (internal) nodes of T . A more efficient algorithm follows from observations in <ref> [2] </ref>, as we will discuss later on. Here we present a very simple quadratic algorithm for deciding equivalence of decision trees. <p> In <ref> [2] </ref> the following result is proved.
Reference: 3. <author> Cockett, J. R. B., and Herrera, J. A. </author> <title> Decision tree reduction. </title> <journal> Journal of the Association for Computing Machinery 37, </journal> <volume> 4 (1990), </volume> <pages> 815-842. </pages>
Reference-contexts: Finding the smallest tree consistent with some training set is NP-hard ([6]); from this example we see that replacing the resulting tree by a smaller equivalent tree may already be helpful. In <ref> [3] </ref> an efficient algorithm is presented for this goal. Instead of elaborating this kind of practical applications, in this paper we concentrate on basic theoretical issues.
Reference: 4. <author> Dietterich, T. G. </author> <title> Machine-learning research: Four current directions. </title> <booktitle> AI Magazine (Winter 1997), </booktitle> <pages> 97-136. </pages>
Reference: 5. <author> Hanson, L., and Salomon, P. </author> <title> Neural network ensembles. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence 12 (1990), </journal> <pages> 993-1001. </pages>
Reference: 6. <author> Hyafil, L., and Rivest, R. L. </author> <title> Constructing optimal binary decision trees is NP-complete. </title> <journal> Information Processing Letters 5, </journal> <volume> 1 (1976), </volume> <pages> 15-17. </pages>
Reference: 7. <author> Mitchell, T. M. </author> <title> Machine Learning. </title> <publisher> McGraw-Hill, </publisher> <year> 1997. </year>
Reference: 8. <author> Quinlan, J. R. </author> <title> Induction of decision trees. </title> <booktitle> Machine Learning 1 (1986), </booktitle> <pages> 81-106. 12 </pages>
Reference-contexts: The decision tree T represents the conjunction of q and r and is consistent with all 16 observations. However, the standard greedy algorithm for growing decision trees based on the information gain criterion as in <ref> [8] </ref> yields the decision tree p (T; T ), which is equivalent to the much smaller decision tree T . A usual objective, motivated by Occam's Razor, is trying to find a decision tree that is as small as possible.
References-found: 8

