URL: http://www.cs.utah.edu/~riloff/psfiles/emnlp.ps
Refering-URL: http://www.cs.utah.edu/~jshepher/jresume.html
Root-URL: 
Email: riloff@cs.utah.edu  
Title: In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing, 1997. A
Author: Ellen Riloff and Jessica Shepherd 
Address: Salt Lake City, UT 84112  
Affiliation: Department of Computer Science University of Utah  
Abstract: Semantic knowledge can be a great asset to natural language processing systems, but it is usually hand-coded for each application. Although some semantic information is available in general-purpose knowledge bases such as WordNet and Cyc, many applications require domain-specific lexicons that represent words and categories for a particular topic. In this paper, we present a corpus-based method that can be used to build semantic lexicons for specific categories. The input to the system is a small set of seed words for a category and a representative text corpus. The output is a ranked list of words that are associated with the category. A user then reviews the top-ranked words and decides which ones should be entered in the semantic lexicon. In experiments with five categories, users typically found about 60 words per category in 10-15 minutes to build a core se mantic lexicon.
Abstract-found: 1
Intro-found: 1
Reference: <author> Berwick, Robert C. </author> <year> 1989. </year> <title> Learning Word Meanings from Examples. In Semantic Structures: </title> <booktitle> Advances in Natural Language Processing. </booktitle> <publisher> Lawrence Erlbaum Associates, </publisher> <pages> chapter 3, pages 89-124. </pages>
Reference: <author> Brill, E. </author> <year> 1994. </year> <title> Some Advances in Rule-based Part of Speech Tagging. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pages 722-727. </pages> <publisher> AAAI Press/The MIT Press. </publisher>
Reference: <author> Carbonell, J. G. </author> <year> 1979. </year> <title> Towards a Self-Extending Parser. </title> <booktitle> In Proceedings of the 17th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 3-7. </pages>
Reference: <author> Cardie, C. </author> <year> 1993. </year> <title> A Case-Based Approach to Knowledge Acquisition for Domain-Specific Sentence Analysis. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 798-803. </pages> <publisher> AAAI Press/The MIT Press. </publisher>
Reference: <author> Church, K. </author> <year> 1989. </year> <title> A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text. </title> <booktitle> In Proceedings of the Second Conference on Applied Natural Language Processing. </booktitle>
Reference: <author> Granger, R. H. </author> <year> 1977. </year> <title> FOUL-UP: A Program that Figures Out Meanings of Words from Context. </title> <booktitle> In Proceedings of the Fifth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 172-178. </pages>
Reference: <author> Hastings, P. and S. Lytinen. </author> <year> 1994. </year> <title> The Ups and Downs of Lexical Acquisition. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pages 754-759. </pages> <publisher> AAAI Press/The MIT Press. </publisher>
Reference: <author> Jacobs, P. and U. Zernik. </author> <year> 1988. </year> <title> Acquiring Lexical Knowledge from Text: A Case Study. </title> <booktitle> In Proceedings of the Seventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 739-744. </pages>
Reference: <author> Lehnert, W., C. Cardie, D. Fisher, J. McCarthy, E. Riloff, and S. Soderland. </author> <year> 1992. </year> <title> University of Massachusetts: Description of the CIRCUS System as Used for MUC-4. </title> <booktitle> In Proceedings of the Fourth Message Understanding Conference (MUC-4), </booktitle> <pages> pages 282-288, </pages> <address> San Mateo, CA. </address> <publisher> Mor-gan Kaufmann. </publisher>
Reference-contexts: And for some applications, any word that is strongly associated with a category might be useful to include in the semantic lexicon. For example, words like ammunition or bullets are highly suggestive of a weapon. In the UMass/MUC-4 information extraction system <ref> (Lehnert et al., 1992) </ref>, the words ammunition and bullets were defined as weapons, mainly for the purpose of selectional restrictions. The human judges estimated that it took them approximately 10-15 minutes, on average, to judge the 200 words for each category.
Reference: <author> Lenat, D. B., M. Prakash, and M. Shepherd. </author> <year> 1986. </year> <title> CYC: Using Common Sense Knowledge to Overcome Brittleness and Knowledge-Acquisition Bottlenecks. </title> <journal> AI Magazine, </journal> <volume> 6 </volume> <pages> 65-85. </pages>
Reference-contexts: But semantic information is difficult to obtain. In most cases, semantic knowledge is encoded manually for each application. There have been a few large-scale efforts to create broad semantic knowledge bases, such as Word-Net (Miller, 1990) and Cyc <ref> (Lenat, Prakash, and Shepherd, 1986) </ref>. While these efforts may be useful for some applications, we believe that they will never fully satisfy the need for semantic knowledge. Many domains are characterized by their own sublanguage containing terms and jargon specific to the field.
Reference: <author> Miller, G. </author> <year> 1990. </year> <title> Wordnet: An On-line Lexical Database. </title> <journal> International Journal of Lexicography, </journal> <volume> 3(4). </volume> <booktitle> MUC-4 Proceedings. 1992. Proceedings of the Fourth Message Understanding Conference (MUC-4). </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: But semantic information is difficult to obtain. In most cases, semantic knowledge is encoded manually for each application. There have been a few large-scale efforts to create broad semantic knowledge bases, such as Word-Net <ref> (Miller, 1990) </ref> and Cyc (Lenat, Prakash, and Shepherd, 1986). While these efforts may be useful for some applications, we believe that they will never fully satisfy the need for semantic knowledge. Many domains are characterized by their own sublanguage containing terms and jargon specific to the field.
Reference: <author> Weischedel, R., M. Meteer, R. Schwartz, L. Ramshaw, and J. Palmucci. </author> <year> 1993. </year> <title> Coping with Ambiguity and Unknown Words through Probabilistic Models. </title> <journal> Computational Linguistics, </journal> <volume> 19(2) </volume> <pages> 359-382. </pages>
Reference: <author> Yarowsky, D. </author> <year> 1992. </year> <title> Word sense disambiguation using statistical models of Roget's categories trained on large corpora. </title> <booktitle> In Proceedings of the Fourteenth International Conference on Computational Linguistics (COLING-92), </booktitle> <pages> pages 454-460. </pages>
Reference-contexts: Given a few category members, we wondered whether it would be possible to collect surrounding contexts and use statistics to identify other words that also belong to the category. Our approach was motivated by Yarowsky's word sense disambiguation algorithm <ref> (Yarowsky, 1992) </ref> and the notion of statistical salience, although our system uses somewhat different statistical measures and techniques. We begin with a small set of seed words for a category. <p> The context windows do not cut across sentence boundaries. Note that our context window is much narrower than those used by other researchers <ref> (Yarowsky, 1992) </ref>. We experimented with larger window sizes and found that the narrow windows more consistently included words related to the target category. 3.
References-found: 13

