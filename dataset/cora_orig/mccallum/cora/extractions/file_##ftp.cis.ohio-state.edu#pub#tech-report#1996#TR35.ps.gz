URL: file://ftp.cis.ohio-state.edu/pub/tech-report/1996/TR35.ps.gz
Refering-URL: ftp://ftp.cis.ohio-state.edu/pub/tech-report/TRList.html
Root-URL: 
Email: Email: fbasak,pandag@cis.ohio-state.edu  
Title: Benefits of Processor Clustering in Designing Parallel Systems: When and How?  
Author: Debashis Basak and Dhabaleswar K. Panda Dhabaleswar K. Panda 
Keyword: parallel architectures, interconnection networks, clustered architectures, hierarchical architectures, scalability, messaging overheads.  
Note: Contact Author:  
Address: Columbus, OH 43210-1277  
Affiliation: Dept. of Computer and Information Science The Ohio State University,  
Abstract: Advances in multiprocessor interconnect technology are leading to high performance networks. However, software overheads associated with message passing are limiting the processors to get maximum performance from these networks, leading to under-utilization of network resources. Several research studies are ongoing for designing messaging protocols and hardware to reduce such overheads. However, with such protocols messaging overheads cannot be eliminated. Even though the overheads can be lowered, these will continue to be reasonably high compared to the network speed. In this paper we take an alternative approach and analyze on how to exploit emerging processor-cluster technology for designing high-performance and cost-effective parallel systems given that messaging overheads can limit maximum performance. Though processor-clusters are being used in some systems in an ad hoc manner, there is no formal analysis in the literature to show when and how processor clusters benefit in designing high performance and scalable systems. In this paper we present a design-space-graph framework for analyzing and solving this problem by considering processor-clustering, messaging overheads, and network performance in an integrated manner. Our analysis establishes the following three design guidelines. With messaging overheads constraining performance in a system, processor clustering can be used to build a) an equal-sized system with a smaller network or b) a larger system with an equal-sized network. With messaging overheads not being a constraint, a combination of processor clustering and wider channels can be used to build a range of larger-sized systems. These guidelines are validated through simulation and experimentation. All these guidelines lead to designing cost-effective and scalable parallel systems while delivering high performance. fl This research is supported in part by NSF Grant MIP-9309627 and an Ohio State University Presidential Fellowship. A preliminary version of this paper [7] has been presented in Int'l Parallel Processing Symposium, Apr. 96 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agrawal. </author> <title> Limits on Interconnection Network Performance. </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 398-412, </pages> <month> Oct </month> <year> 1991. </year>
Reference-contexts: Let the maximum rate of injection of messages per processor imposed by the bisection size, assuming no messaging overhead (t s = t p = 0:0), be denoted by nw messages/sec. Based on a contention model presented in <ref> [1, 4] </ref>, the limit nw for a (k n ,c; W ) system with currently popular wormhole routing in inter-cluster network, can be analytically derived as: nw = cL (k 1)t c (1 + (n + 1)=3) messages/sec, (3) where t c (in sec) denotes the channel cycle time in the
Reference: [2] <author> R. H. Arpaci and D. E. Culler et al. </author> <title> Empirical Evaluation of the CRAY-T3D: A Compiler Perspective. </title> <booktitle> In Proc. of the Int. Symp. on Computer Architecture, </booktitle> <pages> pages 320-331, </pages> <year> 1995. </year>
Reference-contexts: Each processor has a private memory. For communication across processors we used the remote memory access routines: shmem put () and shmem get (), 25 offering the maximum communication bandwidth <ref> [2, 18] </ref>. 9.2 Traffic patterns/applications We selected three traffic patterns/applications [19]: Bit Permute Complement exchanges (BPC), Fast Fourier Transform (FFT), and LU matrix decomposition. These are briefly discussed below: 1.
Reference: [3] <author> D. Basak and D. K. Panda. ClusterSim: </author> <title> A Simulator Testbed for Wormhole-Routed k-ary n-cube cluster-c Multiprocessor Systems. </title> <institution> Dept. of Computer Science, The Ohio State University, </institution> <note> working paper. </note>
Reference-contexts: Thus, to study the variation of such parameters we conducted experiments on the Ohio State clustered architecture simulation testbed - ClusterSim. ClusterSim <ref> [3] </ref> is an event-driven simulation 11 testbed developed using CSIM [23], offering flexibility to analyze the impact of varying system parameters like cluster size, topology, channel width, network channel cycle time, and messaging overheads on system performance.
Reference: [4] <author> D. Basak and D. K. Panda. </author> <title> Designing Clustered Multiprocessor Systems under Packaging and Technological Advancements. </title> <note> To appear in IEEE Trans. on Parallel and Distributed Systems, currently available at http://www.cis.ohio-state.edu/~panda/pac.html. </note>
Reference-contexts: Larger systems are being built by interconnecting multiple processor-clusters [24]. Such an approach offers scalability while exploiting the cost-effectiveness and convenience of processor-cluster modules. Similar motivations are also increasingly guiding the development of traditional parallel systems using processor-clusters <ref> [4, 8] </ref>. Previous research on using processor-clusters for building large parallel systems have mainly focused on proposing and proving different interconnection topologies [10, 20, 21] and studying the design problem under very realistic packaging con 1 straints [5, 22]. <p> Thus, in this paper we study increase in channel bandwidth due to increase in only channel width (W ). We represent W in bytes to reflect a trend of designing byte-multiple channel width to maintain an integral relationship with processor and memory modules <ref> [4] </ref>. The plane formed by the R and c axes is referred to as the R-c plane. Each point on the R-c plane, denoted by tuple (R; c), represents a parallel system configuration with (N = Rc) processors. <p> There are many different ways to factorize R into n such factors. Factorization with nearly equal values for all k 0 is preferred as it leads to smaller network organization and benefits from packaging compactness <ref> [4] </ref>. With (k 1 = = k n = k) the notation (k 1 fi fik n ,c,W ) can be further shortened to (k n ,c; W ). For example, the system configuration in Fig. 1 (a) can be represented as (4 fi 4,1,2) or (4 2 ,1,2). <p> Let the maximum rate of injection of messages per processor imposed by the bisection size, assuming no messaging overhead (t s = t p = 0:0), be denoted by nw messages/sec. Based on a contention model presented in <ref> [1, 4] </ref>, the limit nw for a (k n ,c; W ) system with currently popular wormhole routing in inter-cluster network, can be analytically derived as: nw = cL (k 1)t c (1 + (n + 1)=3) messages/sec, (3) where t c (in sec) denotes the channel cycle time in the <p> Higher integration is leading to higher value of c and as a result smaller R. However, there are limits on what technology <ref> [4] </ref> can offer at a given time. For example, the maximum allowable cluster size can be limited. The impact of such constraints on the design is studied in Sec. 8. <p> Such routers and connectors are expected to have higher costs. Let us first compute the total number of links, denoted earlier as N W , in the inter-cluster interconnection. This is an indication of connector/connectivity cost across routers <ref> [4] </ref>. Similarly, we estimate the change in router costs. Overall, the following analysis supports the intuition that interconnection costs reduce with larger cluster sizes (and possibly wider channels) while maintaining per-processor communication performance.
Reference: [5] <author> D. Basak and D. K. Panda. </author> <title> Designing Large Hierarchical Multiprocessor Systems under Processor, Interconnection, </title> <booktitle> and Packaging Advancements. In Proc. of the Int'l Conference on Parallel Processing, </booktitle> <pages> pages I:63-66, </pages> <year> 1994. </year>
Reference-contexts: Previous research on using processor-clusters for building large parallel systems have mainly focused on proposing and proving different interconnection topologies [10, 20, 21] and studying the design problem under very realistic packaging con 1 straints <ref> [5, 22] </ref>. Such processor clustering also demonstrates potential to alleviate the network under-utilization problem by allowing more number of processors to use a given set of network resources. However, no formal study is available in the literature which shows the benefits of processor clustering towards better utilization of network resources. <p> To ensure configurations not being MC (i.e. being B or NC), a very small t s = 0:25sec was assumed. 8 Design under technological constraints In earlier sections, we developed design guidelines in the absence of technological constraints. In <ref> [5] </ref> it was demonstrated that the design of a real system is constrained by packaging constraints limiting the maximum available channel width (W max ) and cluster size (c max ).
Reference: [6] <author> D. Basak, D. K. Panda, and M. Banikazemi. </author> <title> Benefits of Processor Clustering in Designing Large Parallel Systems: When and How? Technical Report OSU-CISRC-10/95-TR41, </title> <institution> The Ohio State University, </institution> <year> 1995. </year>
Reference-contexts: Uniform traffic is considered here as it is more representative of general traffic in a parallel system [22]. In the following subsections we use as a metric to analyze the communication performance of a parallel system. We first consider the overheads of messaging. In <ref> [6] </ref> it was demonstrated that under uniform traffic, the network bandwidth is a stronger restriction on performance than network interface. In this paper we assume the cluster module technology offering reasonable bandwidth at the interface for it not to be a bottleneck.
Reference: [7] <author> D. Basak, D. K. Panda, and M. Banikazemi. </author> <title> Benefits of Processor Clustering in Designing Large Parallel Systems: When and How? In Proc. </title> <booktitle> of the Int. Parallel Processing Symposium, </booktitle> <pages> pages 286-290, </pages> <month> Apr </month> <year> 1996. </year>
Reference: [8] <author> Cray Reasearch Inc. </author> <title> Cray T3D System Architecture Overview, </title> <booktitle> 1993. </booktitle> <pages> 31 </pages>
Reference-contexts: Larger systems are being built by interconnecting multiple processor-clusters [24]. Such an approach offers scalability while exploiting the cost-effectiveness and convenience of processor-cluster modules. Similar motivations are also increasingly guiding the development of traditional parallel systems using processor-clusters <ref> [4, 8] </ref>. Previous research on using processor-clusters for building large parallel systems have mainly focused on proposing and proving different interconnection topologies [10, 20, 21] and studying the design problem under very realistic packaging con 1 straints [5, 22]. <p> For the system shown in Fig. 1 (a), N W = 48W . With advancements in VLSI and packaging technologies, computing nodes having more than one processor on a single multi-chip module or processor-board are becoming increasingly viable. Many current parallel systems like the CRAY T3D <ref> [8] </ref>, Intel Paragon, Stanford DASH [13], and Sequent NUMA-Q [24] are using processor-cluster based design to build cost-effective systems. 16 processors. In systems built using processor-clusters, let c denote the number of processors available in a cluster, also referred to as the size of a cluster. <p> We also conducted experiments on a 64 node Cray T3D for configurations supportable on it. The T3D machine is a MIMD multiprocessor machine using a direct communication network with 3D torus topology <ref> [8] </ref>. Two alpha processors are attached to each node (cluster) of the torus and share a common hardware interface to the network. Each processor has a private memory.
Reference: [9] <author> W. J. Dally. </author> <title> Performance Analysis of k-ary n-cube Interconnection Networks. </title> <journal> IEEE Trans. on Computers, </journal> <volume> 39(6) </volume> <pages> 775-785, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: The bisection size of a network is the minimum number of network channels to be cut so as to divide the network into two equal halves <ref> [9] </ref>. Under uniform traffic, on the average half of the generated messages in the system need to cross the inter-cluster network bisection.
Reference: [10] <author> S. Dandamudi and D. Eager. </author> <title> On Hierarchical Hypercube Multicomputer Interconnection Network Design. </title> <journal> Jour. of Parallel and Distributed Computing, </journal> <volume> 12(3) </volume> <pages> 283-289, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Similar motivations are also increasingly guiding the development of traditional parallel systems using processor-clusters [4, 8]. Previous research on using processor-clusters for building large parallel systems have mainly focused on proposing and proving different interconnection topologies <ref> [10, 20, 21] </ref> and studying the design problem under very realistic packaging con 1 straints [5, 22]. Such processor clustering also demonstrates potential to alleviate the network under-utilization problem by allowing more number of processors to use a given set of network resources.
Reference: [11] <author> J. J. Dongarra and T. Dunigan. </author> <title> Message-Passing Performance of Various Computers. </title> <note> Available at http://www.netlib.org/utk/papers/latbw/commperf.html, Aug 1995. </note>
Reference-contexts: However, even with efficient protocols these overheads cannot be eliminated. Even though the overheads get lowered, these will continue to be reasonably high compared to network speeds. For example, on NCUBE-2 messaging start-up overhead is around 154 microseconds and network channel transmission time per-byte is only 0.4 microseconds <ref> [11] </ref>. This leads to a ratio of message overhead to per-byte transmission delay as 385. In contemporary systems like the Cray T3D, message overhead is 2.0 microseconds and per-byte network transmission time is 0.0033 microseconds, leading to a ratio of 600. <p> The first part involves a software latency which includes the overhead of procedure calls and possibly, memory copying from user space to system space. We represent such overhead as t s sec/message. Based on the study presented by Dongarra and Dunigan in <ref> [11] </ref>, representative values of t s on current machines are shown in Table 1. On most systems t s can be observed to vary from a few hundreds of cycles up to thousands of cycles. <p> Let t p sec/byte denote overhead incurred per byte of a message. The parameter t p determines the asymptotic message injection bandwidth offered by a communication library on a given system. Table 2 provides representative values of t p obtained on different ma chines and messaging platforms <ref> [11] </ref>. It can be observed that on most systems the asymptotic messaging bandwidth achieved is significantly lower than the theoretical channel maximum, in dicating relatively high messaging overheads on such systems. As with t s , the parameter t p also depends on the underlying communication mechanism/protocol.
Reference: [12] <author> T. Eicken, D. E. Culler, S. C. Goldstein, and K. E. Schauser. </author> <title> Active Messages: a Mechanism for Integrated Communication and Computation. </title> <booktitle> In The 19 th Annual International Symposium on Computer Architecure, </booktitle> <pages> pages 256-266, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: This can lead to much of the communication bandwidth offered by the expensive fast interconnect to remain unutilized. Several research studies are ongoing <ref> [18, 12] </ref> for designing messaging protocols and hardware to reduce such overheads. However, even with efficient protocols these overheads cannot be eliminated. Even though the overheads get lowered, these will continue to be reasonably high compared to network speeds.
Reference: [13] <author> D. Lenoski et al. </author> <title> The Stanford DASH Multiprocessor. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 63-79, </pages> <year> 1990. </year>
Reference-contexts: With advancements in VLSI and packaging technologies, computing nodes having more than one processor on a single multi-chip module or processor-board are becoming increasingly viable. Many current parallel systems like the CRAY T3D [8], Intel Paragon, Stanford DASH <ref> [13] </ref>, and Sequent NUMA-Q [24] are using processor-cluster based design to build cost-effective systems. 16 processors. In systems built using processor-clusters, let c denote the number of processors available in a cluster, also referred to as the size of a cluster.
Reference: [14] <author> N. Boden et al. Myrinet: </author> <title> A Gigabit-per-Second Local Area Network. </title> <booktitle> In IEEE Micro, </booktitle> <pages> pages 29-35, </pages> <year> 1995. </year>
Reference-contexts: Do we model the impact of increased channel width on router cost as sub-linear, linear, or super-linear? Market forces often have a critical role to play in determining such costs. Initial costs may be very high (super-linear) but with increasing demand leading to cheaper production <ref> [14] </ref>, typical costs tend to become sub-linear. A linear model is therefore reasonable and considered in further analysis. Let H n;W denote the cost of a router with n ports (supporting a n-dimensional mesh) each W bits wide.
Reference: [15] <author> W. Hsu and P. C. Yew. </author> <title> The Impact of Wiring Constraints on Hierarchical Network Performance. </title> <booktitle> In Proc. of the Int. Parallel Processing Symposium, </booktitle> <pages> pages 580-588, </pages> <month> Mar </month> <year> 1992. </year>
Reference-contexts: Thus, messaging overheads continue to be a dominant factor for poor utilization of network resources in current generation parallel systems. Advancements in VLSI and packaging technologies is now allowing the integration of a small number of processing elements, interconnected through a network, onto a single multi-chip or a board <ref> [15, 22] </ref> module. Such processor-cluster modules are being increasingly used instead of single processors to build variety of computing vehicles such as servers, workstations, etc. This is leading to greater demand and production for such modules, consequently bringing down per-unit cost. <p> For a fair cost comparison, one should also keep in mind the increased routing hardware inside clusters for intra-cluster communication. However, with higher level of integration inside clusters, we expect the intra-cluster routing capabilities being offered at comparatively much lower cost <ref> [15, 22] </ref>, leading to an overall lowering in interconnection cost with clustering. To match the increasing demand on computing bandwidth, parallel systems need to be scaled to larger configurations supporting more processing nodes.
Reference: [16] <author> S. L. Johnson. </author> <title> Issues in High-Performance Computer Networks. </title> <journal> In IEEE Computer Society TCCA Newsletter, </journal> <pages> pages 14-19, </pages> <month> Summer-Fall </month> <year> 1994. </year>
Reference-contexts: 1 Introduction The performance of a multiprocessor system is often limited by the bisection bandwidth of its interconnection network <ref> [16, 25] </ref>. In the recent past, systems have been built using advanced interconnection networks with 16 and 32 bit channels and channel cycle times of 6.6-10.0 ns. The offered bandwidth for inter-processor communication in such systems is very high.
Reference: [17] <author> V. Karamcheti and A. A. Chien. </author> <title> Software Overhead in Messaging Layers: Where Does the Time Go? In Proc. </title> <booktitle> of the Int. Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 51-60, </pages> <year> 1994. </year>
Reference-contexts: Such overheads, usually in the range of few to tens of microseconds, can severely limit the rate at which messages can be sent or received from a processor <ref> [17] </ref>. This can lead to much of the communication bandwidth offered by the expensive fast interconnect to remain unutilized. Several research studies are ongoing [18, 12] for designing messaging protocols and hardware to reduce such overheads. However, even with efficient protocols these overheads cannot be eliminated.
Reference: [18] <author> V. Karamcheti and A. A. Chien. </author> <title> A Comparison of Architectural Support for Messaging on the TMC CM-5 and the Cray T3D. </title> <booktitle> In Proc. of the Int. Symp. on Computer Architecture, </booktitle> <year> 1995. </year>
Reference-contexts: This can lead to much of the communication bandwidth offered by the expensive fast interconnect to remain unutilized. Several research studies are ongoing <ref> [18, 12] </ref> for designing messaging protocols and hardware to reduce such overheads. However, even with efficient protocols these overheads cannot be eliminated. Even though the overheads get lowered, these will continue to be reasonably high compared to network speeds. <p> Each processor has a private memory. For communication across processors we used the remote memory access routines: shmem put () and shmem get (), 25 offering the maximum communication bandwidth <ref> [2, 18] </ref>. 9.2 Traffic patterns/applications We selected three traffic patterns/applications [19]: Bit Permute Complement exchanges (BPC), Fast Fourier Transform (FFT), and LU matrix decomposition. These are briefly discussed below: 1.
Reference: [19] <author> V. Kumar, A Grama, A. Gupta, and G. Karypis. </author> <title> Introduction to Parallel Computing: Design and Analysis of Algorithms. </title> <publisher> Benjamin-Cummings Addison-Wesley Publishing Company, </publisher> <year> 1994. </year>
Reference-contexts: Each processor has a private memory. For communication across processors we used the remote memory access routines: shmem put () and shmem get (), 25 offering the maximum communication bandwidth [2, 18]. 9.2 Traffic patterns/applications We selected three traffic patterns/applications <ref> [19] </ref>: Bit Permute Complement exchanges (BPC), Fast Fourier Transform (FFT), and LU matrix decomposition. These are briefly discussed below: 1.
Reference: [20] <author> D. K. Panda and D. Basak. </author> <title> Issues in Designing Scalable Systems with k-ary n-cube cluster-c Organization. </title> <booktitle> In Proc. of the First International Workshop on Parallel Processing, India, </booktitle> <pages> pages 5-10, </pages> <year> 1994. </year>
Reference-contexts: Similar motivations are also increasingly guiding the development of traditional parallel systems using processor-clusters [4, 8]. Previous research on using processor-clusters for building large parallel systems have mainly focused on proposing and proving different interconnection topologies <ref> [10, 20, 21] </ref> and studying the design problem under very realistic packaging con 1 straints [5, 22]. Such processor clustering also demonstrates potential to alleviate the network under-utilization problem by allowing more number of processors to use a given set of network resources. <p> Processors inside a cluster communicate with each other using intra-cluster network (intra-cluster communication) provided as part of the processor-cluster module. This network connecting the processors inside a cluster can be a bus, crossbar, star, etc as discussed in <ref> [20] </ref>. Although the choice of the cluster topology is an important design issue, in this study we emphasize more on how and when processor-clusters can be used in designing balanced and larger cost-effective systems.
Reference: [21] <author> Y. R. Potlapalli and D. P. Agrawal. </author> <title> Enhancing the Performance of HMINs using Express Links. </title> <booktitle> In Int. Conf. on Parallel Processing, </booktitle> <pages> pages I56-I59, </pages> <year> 1995. </year>
Reference-contexts: Similar motivations are also increasingly guiding the development of traditional parallel systems using processor-clusters [4, 8]. Previous research on using processor-clusters for building large parallel systems have mainly focused on proposing and proving different interconnection topologies <ref> [10, 20, 21] </ref> and studying the design problem under very realistic packaging con 1 straints [5, 22]. Such processor clustering also demonstrates potential to alleviate the network under-utilization problem by allowing more number of processors to use a given set of network resources.
Reference: [22] <author> M. T. Raghunath and A. Ranade. </author> <title> Designing interconnection networks for multi-level packaging. </title> <booktitle> In Proc. of the Supercomputing, </booktitle> <pages> pages 772-781, </pages> <year> 1993. </year> <month> 32 </month>
Reference-contexts: Thus, messaging overheads continue to be a dominant factor for poor utilization of network resources in current generation parallel systems. Advancements in VLSI and packaging technologies is now allowing the integration of a small number of processing elements, interconnected through a network, onto a single multi-chip or a board <ref> [15, 22] </ref> module. Such processor-cluster modules are being increasingly used instead of single processors to build variety of computing vehicles such as servers, workstations, etc. This is leading to greater demand and production for such modules, consequently bringing down per-unit cost. <p> Previous research on using processor-clusters for building large parallel systems have mainly focused on proposing and proving different interconnection topologies [10, 20, 21] and studying the design problem under very realistic packaging con 1 straints <ref> [5, 22] </ref>. Such processor clustering also demonstrates potential to alleviate the network under-utilization problem by allowing more number of processors to use a given set of network resources. However, no formal study is available in the literature which shows the benefits of processor clustering towards better utilization of network resources. <p> For a fair cost comparison, one should also keep in mind the increased routing hardware inside clusters for intra-cluster communication. However, with higher level of integration inside clusters, we expect the intra-cluster routing capabilities being offered at comparatively much lower cost <ref> [15, 22] </ref>, leading to an overall lowering in interconnection cost with clustering. To match the increasing demand on computing bandwidth, parallel systems need to be scaled to larger configurations supporting more processing nodes. <p> Such latency-throughput characteristic is used to derive the maximum sustained throughput (point close to saturation) of a system. We represent the maximum sustained throughput as messages/sec. Uniform traffic is considered here as it is more representative of general traffic in a parallel system <ref> [22] </ref>. In the following subsections we use as a metric to analyze the communication performance of a parallel system. We first consider the overheads of messaging. In [6] it was demonstrated that under uniform traffic, the network bandwidth is a stronger restriction on performance than network interface.
Reference: [23] <author> H.D. Schwetman. </author> <title> Using CSIM to Model Complex Systems. </title> <booktitle> In Proceedings of the 1988 Winter Simulation Conference, </booktitle> <pages> pages 246-253, </pages> <year> 1988. </year>
Reference-contexts: Thus, to study the variation of such parameters we conducted experiments on the Ohio State clustered architecture simulation testbed - ClusterSim. ClusterSim [3] is an event-driven simulation 11 testbed developed using CSIM <ref> [23] </ref>, offering flexibility to analyze the impact of varying system parameters like cluster size, topology, channel width, network channel cycle time, and messaging overheads on system performance. ClusterSim accurately models flit-level transfer corresponding to wormhole/cut-through routing as in real systems like the Paragon and Cray T3D.
Reference: [24] <institution> Sequent Computer Systems. NUMA-Q. </institution> <note> Available at http://www.sequent.com/public/solution/numaq/index.html, 1996. </note>
Reference-contexts: Such processor-cluster modules are being increasingly used instead of single processors to build variety of computing vehicles such as servers, workstations, etc. This is leading to greater demand and production for such modules, consequently bringing down per-unit cost. Larger systems are being built by interconnecting multiple processor-clusters <ref> [24] </ref>. Such an approach offers scalability while exploiting the cost-effectiveness and convenience of processor-cluster modules. Similar motivations are also increasingly guiding the development of traditional parallel systems using processor-clusters [4, 8]. <p> With advancements in VLSI and packaging technologies, computing nodes having more than one processor on a single multi-chip module or processor-board are becoming increasingly viable. Many current parallel systems like the CRAY T3D [8], Intel Paragon, Stanford DASH [13], and Sequent NUMA-Q <ref> [24] </ref> are using processor-cluster based design to build cost-effective systems. 16 processors. In systems built using processor-clusters, let c denote the number of processors available in a cluster, also referred to as the size of a cluster.
Reference: [25] <author> A. Varma and C. S. Raghavendra. </author> <title> Interconnection Networks for Multiprocessors and Multi-computers. </title> <publisher> IEEE Computer Society, </publisher> <year> 1993. </year>
Reference-contexts: 1 Introduction The performance of a multiprocessor system is often limited by the bisection bandwidth of its interconnection network <ref> [16, 25] </ref>. In the recent past, systems have been built using advanced interconnection networks with 16 and 32 bit channels and channel cycle times of 6.6-10.0 ns. The offered bandwidth for inter-processor communication in such systems is very high.
References-found: 25

