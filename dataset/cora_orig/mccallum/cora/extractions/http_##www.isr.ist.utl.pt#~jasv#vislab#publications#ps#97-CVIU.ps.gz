URL: http://www.isr.ist.utl.pt/~jasv/vislab/publications/ps/97-CVIU.ps.gz
Refering-URL: http://www.isr.ist.utl.pt/~jasv/vislab/publications/publications.html
Root-URL: 
Email: e-mail: jasv@isr.isr.ist.utl.pt e-mail:giulio@vision.dist.unige.it  
Title: Visual Behaviors for Docking docking system is operating in real time and the performance is
Author: Jose Santos-Victor Giulio Sandini 
Note: The  
Address: Av. Rovisco Pais, 1 Via Opera Pia, 13 1096 Lisboa Codex Portugal 16145 Genova Italia  
Affiliation: Instituto Superior Tecnico DIST, Lira-Lab Instituto de Sistemas e Robotica University of Genova  
Abstract: This paper describes visual-based behaviors for docking operations in mobile robotics. Two different situations are presented : in the ego-docking, each robot is equipped with a camera, and the motion is controlled when docking to a surface, whereas in the eco-docking, the camera and all the necessary computational resources are placed in a single external docking station, which may serve several robots. In both situations, the goal consists in controlling both the orientation, aligning the camera optical axis with the surface normal, and the approaching speed (slowing down during the maneuver). These goals are accomplished without any effort to perform 3D reconstruction of the environment or any need to calibrate the setup, in contrast with traditional approaches. Instead, we use image measurements directly to close the control loop of the mobile robot. In the approach we propose, the robot motion is directly driven by the first order time-space image derivatives, which can be estimated robustly and fast. fl The authors wish to thank the anonymous reviewers for their helpful comments and suggestions, that helped to improve the quality and the presentation of the paper. The research described in this paper has been supported by the Special Projects on Robotics of the Italian National Council of Research and by the ESPRIT project VAP-II. A fellowship from a bilateral collaboration between Consiglio Nazionale delle Ricerche (CNR) and Junta Nacional de Investiga~c~ao Cientfica e Tecnologica (JNICT) is gratefully acknowledged by Jose Santos-Victor. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Y. Aloimonos. </author> <title> Purposive and qualitative active vision. </title> <booktitle> In Proc. of the 10th. IEEE International Conference on Pattern Recognition, </booktitle> <address> Atlantic City, NJ - USA, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: From the seminal paper of Ruzena Bajczy describing the peculiarity of Active Perception [3] through the papers on Active and Animate vision [4, 2] and, more recently, to the concept of Purposive Vision <ref> [1] </ref>. Along its evolution research about the perception/action relationship has suggested, at least, three major advances. <p> However, the normal flow alone conveys sufficient information to many perception problems <ref> [1, 8] </ref> and can be estimated robustly and fast. In our case, the robot motion-controllers are solely driven by information of first order temporal-spatial image derivatives, from which the normal flow can be computed directly 3 .
Reference: [2] <author> Y. Aloimonos, I. Weiss, and A. Banddophaday. </author> <title> Active vision. </title> <journal> International Journal of Computer Vision, </journal> <volume> 1(4) </volume> <pages> 333-356, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: 1 Introduction The intimate relationship between perception and action has been discussed and presented in different forms in the last years. From the seminal paper of Ruzena Bajczy describing the peculiarity of Active Perception [3] through the papers on Active and Animate vision <ref> [4, 2] </ref> and, more recently, to the concept of Purposive Vision [1]. Along its evolution research about the perception/action relationship has suggested, at least, three major advances.
Reference: [3] <author> R. </author> <title> Bajcsy. Active perception. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 76(8) </volume> <pages> 996-1005, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: 1 Introduction The intimate relationship between perception and action has been discussed and presented in different forms in the last years. From the seminal paper of Ruzena Bajczy describing the peculiarity of Active Perception <ref> [3] </ref> through the papers on Active and Animate vision [4, 2] and, more recently, to the concept of Purposive Vision [1]. Along its evolution research about the perception/action relationship has suggested, at least, three major advances.
Reference: [4] <author> D.H. Ballard. </author> <title> Animate vision. </title> <journal> Artificial Intelligence, </journal> <volume> 48 </volume> <pages> 57-86, </pages> <year> 1991. </year>
Reference-contexts: 1 Introduction The intimate relationship between perception and action has been discussed and presented in different forms in the last years. From the seminal paper of Ruzena Bajczy describing the peculiarity of Active Perception [3] through the papers on Active and Animate vision <ref> [4, 2] </ref> and, more recently, to the concept of Purposive Vision [1]. Along its evolution research about the perception/action relationship has suggested, at least, three major advances.
Reference: [5] <author> R. Cipolla and A. Blake. </author> <title> Surface orientation and time to contact from image divergence and deformation. </title> <booktitle> In Proc. of the 2nd. </booktitle> <address> ECCV, Santa Margherita, Italy, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: Both in the ego-docking or eco-docking, the motion perceived 12 in the image plane, by the camera, is given by the well known equations <ref> [5, 6, 24, 25] </ref> : u (x; y) = f x f x T z T x + ! x f x f y x 2 x y # v (x; y) = f y f y T z T y + ! x (1 + f 2 ) ! y
Reference: [6] <author> R. Cipolla, Y. Okamoto, and Y. Kuno. </author> <title> Robust structure from motion using motion parallax. </title> <booktitle> In Proc. of the 4th. </booktitle> <address> ICCV, Berlin, Germany, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Both in the ego-docking or eco-docking, the motion perceived 12 in the image plane, by the camera, is given by the well known equations <ref> [5, 6, 24, 25] </ref> : u (x; y) = f x f x T z T x + ! x f x f y x 2 x y # v (x; y) = f y f y T z T y + ! x (1 + f 2 ) ! y
Reference: [7] <author> B. Espiau, F. Chaumette, and Patrick Rives. </author> <title> A new approach to visual servoing in robotics. </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> 8(3) </volume> <pages> 313-326, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: It should again be stressed that the use of visual measurements directly in the motion controller leads to improved robustness as the system is continuously monitoring its performance <ref> [7, 22, 23, 26] </ref>. There is no need to calibrate the camera and no reconstruction of the environment is performed. 2 Sensory-motor coordination In this section we will address the problem of sensory-motor coordination.
Reference: [8] <author> C. Fermuller. </author> <title> Global 3d motion estimation. </title> <booktitle> In Proc. of the IEEE CVPR, </booktitle> <address> New York, USA, </address> <year> 1993. </year>
Reference-contexts: However, the normal flow alone conveys sufficient information to many perception problems <ref> [1, 8] </ref> and can be estimated robustly and fast. In our case, the robot motion-controllers are solely driven by information of first order temporal-spatial image derivatives, from which the normal flow can be computed directly 3 . <p> With this single constraint, we can only recover the normal flow, u ? , which is the component of the optical flow along the direction of the image gradient, which is the well known aperture problem <ref> [8, 10] </ref>. The normal flow can be computed directly from the first order space and time image derivatives, by : u ? = q x + I 2 : (22) Our estimation procedure is directly based on the first-order space and time image derivatives.
Reference: [9] <editor> C. Fermuller. Navigational preliminaries. In Y. Aloimonos, editor, </editor> <title> Active Perception. </title> <publisher> Lawrence Erlbaum Associates, </publisher> <year> 1993. </year>
Reference-contexts: through an "utilitarian" phase where action is driven by the need to improve the perceptual process, to arrive, more recently, to the concept of visual servoing and visual behaviors where action is eliciting and simplifying the perceptual processes which, in turn, drive the action itself (the "vision during action" approach <ref> [9, 20] </ref>). In this case the control loop becomes tighter and, if direct visual measures are used, motor control is directly driven by iconic information (I.e. data which are directly computed from the images).
Reference: [10] <author> B. Horn. </author> <title> Robot Vision. </title> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: However, it is well known that under the hypothesis of image brightness constancy, it is only possible to determine the component of the optical flow in the direction of the image gradient, the normal flow. This structural limitation, known as the aperture problem <ref> [10, 11] </ref>, has motivated the search of alternative methods and constraints on the optical flow allowing to recover both components of the optical flow [10, 14, 15, 16, 17, 19], which are often extremely complex and/or unstable 2 . <p> This structural limitation, known as the aperture problem [10, 11], has motivated the search of alternative methods and constraints on the optical flow allowing to recover both components of the optical flow <ref> [10, 14, 15, 16, 17, 19] </ref>, which are often extremely complex and/or unstable 2 . However, the normal flow alone conveys sufficient information to many perception problems [1, 8] and can be estimated robustly and fast. <p> With this single constraint, we can only recover the normal flow, u ? , which is the component of the optical flow along the direction of the image gradient, which is the well known aperture problem <ref> [8, 10] </ref>. The normal flow can be computed directly from the first order space and time image derivatives, by : u ? = q x + I 2 : (22) Our estimation procedure is directly based on the first-order space and time image derivatives.
Reference: [11] <author> B.K.P. Horn and B. Shunck. </author> <title> Determining optical flow. </title> <journal> Artificial Intelligence, </journal> <volume> 17 </volume> <pages> 185-203, </pages> <year> 1981. </year>
Reference-contexts: However, it is well known that under the hypothesis of image brightness constancy, it is only possible to determine the component of the optical flow in the direction of the image gradient, the normal flow. This structural limitation, known as the aperture problem <ref> [10, 11] </ref>, has motivated the search of alternative methods and constraints on the optical flow allowing to recover both components of the optical flow [10, 14, 15, 16, 17, 19], which are often extremely complex and/or unstable 2 . <p> The first-order constraint for the optical flow computation, assuming the image brightness constancy over time <ref> [11] </ref>, is given by : uI x + vI y = I t ; (21) where I x , I y and I t stand for the partial derivatives of the image with respect to x, y, and time t.
Reference: [12] <author> M. Irani, B. Rousso, and S. Peleg. </author> <title> Recovery of ego-motion using image stabilization. </title> <booktitle> In Proc. of the IEEE CVPR, </booktitle> <address> Seattle, USA, </address> <year> 1994. </year>
Reference-contexts: these derivatives or to the normal flow, as being equivalent. 4 The core of Section 3 is dedicated to the analysis of planar surfaces in motion, which leads to a global second order parametric description of the flow field [24], and to the use of a simplified affine motion model <ref> [13, 12, 27] </ref> with improved performance [18] in what regards to the stability and robustness of the estimates. A procedure uniquely based on the first order time and space image derivatives, is proposed to estimate the affine motion parameters. <p> The modeling error (higher at the image periphery) is still smaller than the estimation error of the full second order model. Then, to improve robustness, it is preferable to neglect the second order terms and approximate, instead, the motion field by an affine model <ref> [12, 13, 18, 27] </ref> : u (x; y) = u 0 + u x x + u y y 3.1 Estimating the affine parameters To estimate the affine parameters of the flow, the method we use relies solely on the first order spatio-temporal image derivatives.
Reference: [13] <author> J. Koenderink and J. van Doorn. </author> <title> Affine structure from motion. </title> <journal> Jounal of the Optical Society of America, </journal> <volume> 8(2) </volume> <pages> 377-385, </pages> <year> 1991. </year>
Reference-contexts: these derivatives or to the normal flow, as being equivalent. 4 The core of Section 3 is dedicated to the analysis of planar surfaces in motion, which leads to a global second order parametric description of the flow field [24], and to the use of a simplified affine motion model <ref> [13, 12, 27] </ref> with improved performance [18] in what regards to the stability and robustness of the estimates. A procedure uniquely based on the first order time and space image derivatives, is proposed to estimate the affine motion parameters. <p> The modeling error (higher at the image periphery) is still smaller than the estimation error of the full second order model. Then, to improve robustness, it is preferable to neglect the second order terms and approximate, instead, the motion field by an affine model <ref> [12, 13, 18, 27] </ref> : u (x; y) = u 0 + u x x + u y y 3.1 Estimating the affine parameters To estimate the affine parameters of the flow, the method we use relies solely on the first order spatio-temporal image derivatives.
Reference: [14] <author> J.J. Little and A. Verri. </author> <title> Analysis of differential and matching methods for optical flow. </title> <booktitle> In Proc. of the IEEE International Conference on Robotics and Automation, </booktitle> <pages> pages 173-180, </pages> <year> 1989. </year>
Reference-contexts: This structural limitation, known as the aperture problem [10, 11], has motivated the search of alternative methods and constraints on the optical flow allowing to recover both components of the optical flow <ref> [10, 14, 15, 16, 17, 19] </ref>, which are often extremely complex and/or unstable 2 . However, the normal flow alone conveys sufficient information to many perception problems [1, 8] and can be estimated robustly and fast.
Reference: [15] <author> H. Nagel. </author> <title> Displacement vectors derived from second-order intensity variations in image sequence. </title> <journal> CVGIP, </journal> <volume> 21 </volume> <pages> 85-117, </pages> <year> 1983. </year>
Reference-contexts: This structural limitation, known as the aperture problem [10, 11], has motivated the search of alternative methods and constraints on the optical flow allowing to recover both components of the optical flow <ref> [10, 14, 15, 16, 17, 19] </ref>, which are often extremely complex and/or unstable 2 . However, the normal flow alone conveys sufficient information to many perception problems [1, 8] and can be estimated robustly and fast.
Reference: [16] <author> H. Nagel and W. Enkelmann. </author> <title> An investigation of smoothness constraints for the estimation of displacement vector fields from image sequences. </title> <journal> IEEE Transactions on PAMI, </journal> <volume> 8 </volume> <pages> 565-593, </pages> <year> 1986. </year>
Reference-contexts: This structural limitation, known as the aperture problem [10, 11], has motivated the search of alternative methods and constraints on the optical flow allowing to recover both components of the optical flow <ref> [10, 14, 15, 16, 17, 19] </ref>, which are often extremely complex and/or unstable 2 . However, the normal flow alone conveys sufficient information to many perception problems [1, 8] and can be estimated robustly and fast.
Reference: [17] <author> H. H. Nagel. </author> <title> On the estimation of optical flow: Relations between different approaches and some new results. </title> <journal> Artificial Intelligence, </journal> <volume> 33 </volume> <pages> 299-323, </pages> <year> 1987. </year>
Reference-contexts: This structural limitation, known as the aperture problem [10, 11], has motivated the search of alternative methods and constraints on the optical flow allowing to recover both components of the optical flow <ref> [10, 14, 15, 16, 17, 19] </ref>, which are often extremely complex and/or unstable 2 . However, the normal flow alone conveys sufficient information to many perception problems [1, 8] and can be estimated robustly and fast.
Reference: [18] <author> S. Negahdaripour and S. Lee. </author> <title> Motion recovery from images sequences using only first order optical flow information. </title> <journal> International Journal of Computer Vision, </journal> <volume> 9(3) </volume> <pages> 163-184, </pages> <year> 1992. </year>
Reference-contexts: flow, as being equivalent. 4 The core of Section 3 is dedicated to the analysis of planar surfaces in motion, which leads to a global second order parametric description of the flow field [24], and to the use of a simplified affine motion model [13, 12, 27] with improved performance <ref> [18] </ref> in what regards to the stability and robustness of the estimates. A procedure uniquely based on the first order time and space image derivatives, is proposed to estimate the affine motion parameters. <p> By introducing the perspective projection in equation (17), and using the optical flow equations (16), one is led to the quadratic equations describing this planar motion <ref> [18, 24] </ref> : u (x; y) = u 0 + u x x + u y y + u xy xy + u xx x 2 which is a globally valid model for the optical flow, where the parameters are given by : u 0 = f x T x i <p> However, it has been shown analytically and experimentally in <ref> [18] </ref>, that the second order parameters estimates can have a percentage error up to 13 several orders of magnitude larger than the lower-order coefficients, even for perfect planar motion. <p> The modeling error (higher at the image periphery) is still smaller than the estimation error of the full second order model. Then, to improve robustness, it is preferable to neglect the second order terms and approximate, instead, the motion field by an affine model <ref> [12, 13, 18, 27] </ref> : u (x; y) = u 0 + u x x + u y y 3.1 Estimating the affine parameters To estimate the affine parameters of the flow, the method we use relies solely on the first order spatio-temporal image derivatives.
Reference: [19] <author> M. Otte and H. H. Nagel. </author> <title> Optical flow estimation: Advances and comparisons. </title> <booktitle> In Proc. of the 3rd. </booktitle> <address> ECCV, Stockholm, Sweeden, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: This structural limitation, known as the aperture problem [10, 11], has motivated the search of alternative methods and constraints on the optical flow allowing to recover both components of the optical flow <ref> [10, 14, 15, 16, 17, 19] </ref>, which are often extremely complex and/or unstable 2 . However, the normal flow alone conveys sufficient information to many perception problems [1, 8] and can be estimated robustly and fast.
Reference: [20] <author> G. Sandini, F. Gandolfo, E. Grosso, and M. Tistarelli. </author> <title> Vision during action. </title> <editor> In Y. Aloimonos, editor, </editor> <title> Active Perception. </title> <publisher> Lawrence Erlbaum Associates, </publisher> <year> 1993. </year>
Reference-contexts: through an "utilitarian" phase where action is driven by the need to improve the perceptual process, to arrive, more recently, to the concept of visual servoing and visual behaviors where action is eliciting and simplifying the perceptual processes which, in turn, drive the action itself (the "vision during action" approach <ref> [9, 20] </ref>). In this case the control loop becomes tighter and, if direct visual measures are used, motor control is directly driven by iconic information (I.e. data which are directly computed from the images).
Reference: [21] <author> J. Santos-Victor and G. </author> <title> Sandini. Visual based obstacle detection : a purposive approach using the normal flow. </title> <booktitle> In Proc. of the International Conference on Intelligent Autonomous Systems, </booktitle> <address> Karlsruhe, Germany, </address> <year> 1995. </year> <month> 34 </month>
Reference-contexts: In that case, only the flow information from the peripheral part of the visual fields was used to maintain the robot in the center of a corridor. In yet another experiment, the frontal part of the visual field has been used (extracting normal flow) to detect obstacles and stop <ref> [21] </ref>. For all these visual behaviors there is no need to know the calibration and/or the vehicle motion parameters and, moreover, they are all based on the same visual information (optic flow). Two factors characterize the different behaviors: 1.
Reference: [22] <author> J. Santos-Victor, G. Sandini, F. Curotto, and S. Garibaldi. </author> <title> Divergent stereo for robot navigation: Learning from bees. </title> <booktitle> In IEEE CVPR., </booktitle> <year> 1993. </year>
Reference-contexts: It should again be stressed that the use of visual measurements directly in the motion controller leads to improved robustness as the system is continuously monitoring its performance <ref> [7, 22, 23, 26] </ref>. There is no need to calibrate the camera and no reconstruction of the environment is performed. 2 Sensory-motor coordination In this section we will address the problem of sensory-motor coordination.
Reference: [23] <author> J. Santos-Victor, G. Sandini, F. Curotto, and S. Garibaldi. </author> <title> Divergent stereo in autonomous navigation : From bees to robots. </title> <journal> International Journal of Computer Vision, </journal> <note> Special Issue on Qualitative Vision, </note> <editor> Y. Aloimonos (Ed.), </editor> <volume> 14(2) </volume> <pages> 159-178, </pages> <year> 1995. </year>
Reference-contexts: It should again be stressed that the use of visual measurements directly in the motion controller leads to improved robustness as the system is continuously monitoring its performance <ref> [7, 22, 23, 26] </ref>. There is no need to calibrate the camera and no reconstruction of the environment is performed. 2 Sensory-motor coordination In this section we will address the problem of sensory-motor coordination. <p> In another experiment <ref> [23] </ref>, the same approach was used (with a divergent stereo set-up) for "centering" and "wall following" behaviors. In that case, only the flow information from the peripheral part of the visual fields was used to maintain the robot in the center of a corridor.
Reference: [24] <author> M. Subbarao and A. Waxman. </author> <title> Closed form solutions to image flow equations for planar surfaces in motion. </title> <journal> CVGIP, </journal> <volume> 36 </volume> <pages> 208-228, </pages> <year> 1986. </year>
Reference-contexts: time and space image derivatives, we will often refer either to these derivatives or to the normal flow, as being equivalent. 4 The core of Section 3 is dedicated to the analysis of planar surfaces in motion, which leads to a global second order parametric description of the flow field <ref> [24] </ref>, and to the use of a simplified affine motion model [13, 12, 27] with improved performance [18] in what regards to the stability and robustness of the estimates. A procedure uniquely based on the first order time and space image derivatives, is proposed to estimate the affine motion parameters. <p> Both in the ego-docking or eco-docking, the motion perceived 12 in the image plane, by the camera, is given by the well known equations <ref> [5, 6, 24, 25] </ref> : u (x; y) = f x f x T z T x + ! x f x f y x 2 x y # v (x; y) = f y f y T z T y + ! x (1 + f 2 ) ! y <p> By introducing the perspective projection in equation (17), and using the optical flow equations (16), one is led to the quadratic equations describing this planar motion <ref> [18, 24] </ref> : u (x; y) = u 0 + u x x + u y y + u xy xy + u xx x 2 which is a globally valid model for the optical flow, where the parameters are given by : u 0 = f x T x i
Reference: [25] <author> V. Sundareswaran. </author> <title> Egomotion from global flow field data. </title> <booktitle> In Proc. of the IEEE Workshop on Visual Motion, </booktitle> <address> Princeton, New Jersey, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: Both in the ego-docking or eco-docking, the motion perceived 12 in the image plane, by the camera, is given by the well known equations <ref> [5, 6, 24, 25] </ref> : u (x; y) = f x f x T z T x + ! x f x f y x 2 x y # v (x; y) = f y f y T z T y + ! x (1 + f 2 ) ! y
Reference: [26] <author> V. Sundareswaran, P. Bouthemy, and F. Chaumette. </author> <title> Active camera self-orientation using dynamic image parameters. </title> <booktitle> In Proc. of the 3rd. </booktitle> <address> ECCV, Stockholm, Sweeden, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: It should again be stressed that the use of visual measurements directly in the motion controller leads to improved robustness as the system is continuously monitoring its performance <ref> [7, 22, 23, 26] </ref>. There is no need to calibrate the camera and no reconstruction of the environment is performed. 2 Sensory-motor coordination In this section we will address the problem of sensory-motor coordination.
Reference: [27] <author> J. Wang and E. Adelson. </author> <title> Layered representation for motion analysis. </title> <booktitle> In Proc. of the IEEE CVPR, </booktitle> <address> New York, USA, </address> <year> 1993. </year>
Reference-contexts: these derivatives or to the normal flow, as being equivalent. 4 The core of Section 3 is dedicated to the analysis of planar surfaces in motion, which leads to a global second order parametric description of the flow field [24], and to the use of a simplified affine motion model <ref> [13, 12, 27] </ref> with improved performance [18] in what regards to the stability and robustness of the estimates. A procedure uniquely based on the first order time and space image derivatives, is proposed to estimate the affine motion parameters. <p> The modeling error (higher at the image periphery) is still smaller than the estimation error of the full second order model. Then, to improve robustness, it is preferable to neglect the second order terms and approximate, instead, the motion field by an affine model <ref> [12, 13, 18, 27] </ref> : u (x; y) = u 0 + u x x + u y y 3.1 Estimating the affine parameters To estimate the affine parameters of the flow, the method we use relies solely on the first order spatio-temporal image derivatives.
Reference: [28] <author> T. Yioshikawa. </author> <title> Foundations of Robotics : Analysis and control. </title> <publisher> MIT Press, </publisher> <year> 1990. </year> <month> 35 </month>
Reference-contexts: The constraint regarding the translation between the sensor and motor frames can be easily achieved in practice without the need for any specific calibration procedure, and is by no means critical. The motion of the camera depends on the motion of the mobile platform, fRg, and can be expressed <ref> [28] </ref> in fCg, according to : T C = C R R (T R + ! R fi R P OC ) where T C and ! C stand respectively for the camera linear and angular velocities relative to a fixed world coordinate frame, expressed in the camera coordinate frame, respectively.
References-found: 28

