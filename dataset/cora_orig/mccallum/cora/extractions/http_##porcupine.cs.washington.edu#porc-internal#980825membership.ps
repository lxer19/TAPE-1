URL: http://porcupine.cs.washington.edu/porc-internal/980825membership.ps
Refering-URL: http://porcupine.cs.washington.edu/porc-internal/index.html
Root-URL: http://www.cs.washington.edu
Title: A Highly Scalable Membership Protocol A Set of People  
Date: September 1, 1998  
Address: Seattle, WA 98195  
Affiliation: Department of Computer Science and Engineering University of Washington  
Abstract: Membership management (knowledge of the running processors and fault detection mechanism) is the background protocol of any work on cluster-based network servers. To date, existing procotols scale badly with the number of nodes, thus limiting the scale of cluster computing. This paper describes and proves a new membership protocol based on a two-level multicast that does scale to 1000 nodes or higher. Simulation results show that our protocol is particularly efficient XXX. 
Abstract-found: 1
Intro-found: 1
Reference: [Amir 95] <author> Y. Amir, L. Moser, P. Melliar-Smith, D. Agarwal and P. Ciarfella. </author> <title> The Totem Single-Ring Ordering and Membership Protocol. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 13(4), </volume> <month> Nov. </month> <year> 1995. </year>
Reference-contexts: Moreover, network may be partitioned in multiple unreachable sub-networks, and here too, there is no means to distinguish between faults and partition. A bunch of works has been reported on the membership management, either for cluster-based servers, as cited before, or group membership for communication <ref> [Birman 94, Amir 95] </ref>. Unfortunately, as quoted by van Re-nesse et al. [Van Renesse 98], existing membership management does not scale, thus limiting the scalability of cluster-based servers. For more than several tens of hosts, the failure detector either is unreasonably slow or make too many false fault detections.
Reference: [Anderson 96] <author> T. Anderson, M. Dahlin, J. Neefe, D. Patterson, D. Roselli and R. Wang. </author> <title> Serverless Network File Systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 14(1) </volume> <pages> 41-79, </pages> <month> Feb. </month> <year> 1996. </year> <month> 19 </month>
Reference-contexts: 1 Introduction Cluster-based network servers is becoming increasingly popular. In particular network of cheap PC connected by a high speed network combine incremental scalability, high availability and low cost [Fox 97]. This is particularly true for instance for parallel and distributed computing [Sens 97], File systems <ref> [Anderson 96] </ref>, and recently Internet based service, as WWW server [Pai 98] or Email server [Saito 98]. In order to make progress, each node in these cluster-based network servers must be able to determine the set of reachable nodes, even in the presence of message lost, faults and partitions.
Reference: [Birman 94] <author> K. Birman and R. van Renesse. </author> <title> Reliable Distributed Computing with the Isis Toolkit. </title> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> Cali-fornia, </address> <year> 1994. </year>
Reference-contexts: Moreover, network may be partitioned in multiple unreachable sub-networks, and here too, there is no means to distinguish between faults and partition. A bunch of works has been reported on the membership management, either for cluster-based servers, as cited before, or group membership for communication <ref> [Birman 94, Amir 95] </ref>. Unfortunately, as quoted by van Re-nesse et al. [Van Renesse 98], existing membership management does not scale, thus limiting the scalability of cluster-based servers. For more than several tens of hosts, the failure detector either is unreasonably slow or make too many false fault detections.
Reference: [Chandra 96] <author> T. Chandra, V. Hadzilacos, S. Toueg and B. Charron-Bost. </author> <title> On the Impossibility of Group Membership in Asynchronous Distributed Systems. </title> <booktitle> In Proc. 15th ACM Syposium on Principles of Distributed Computing, </booktitle> <address> Philadelphia, PA, </address> <pages> pp. 322-330, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: This is the role of the membership management to keep track of the alived nodes and to detects the faulty or unreachable ones. Membership management is well known to be a difficult problem, impossible to solve in asynchronous distributed systemes <ref> [Fischer 85, Chandra 96] </ref>. A node may appear failed because the running process and/or the network 1 is slow, and there is no means to establish the difference between being slow or being faulty (in a finite amount of time).
Reference: [Cristian 95] <author> F. Cristian and F. Schmuck. </author> <title> Agreeing on Processor Group Membership in Timed Asynchronous Distributed Systems. </title> <type> UCSD Technical Report, </type> <institution> CSE95-428, University of California, </institution> <address> San Diego, </address> <year> 1995. </year>
Reference-contexts: This hierarchical structure, similar to a tree of rings, fulfill our high scalability requirement by limiting network congestion, while the low depth of the tree gives the efficiency in membership convergence time. Network congestion happens, for instance, in any "single coordinator" based protocol <ref> [Cristian 95] </ref>, or when the nodes are cold booting (as after a power failure). <p> XXX. 5.1 Cold booting Let's evaluate how our protocol behave when all the nodes are cold booting at appropriately the same time. This case is catastrophic in most existing protocols <ref> [Cristian 95, Van Renesse 98] </ref>, and allow us to revisit the low details of our ptotocol. <p> Our protocol handles pretty efficiently partition recovery. Figure 6 shows that recovering from partitions takes between XXX and XXX seconds. 6 Comparisons with related works According to Cristian and Schmuck <ref> [Cristian 95] </ref>, our protocol relates to partitionable membership services in a timed asynchronous distributed systems. The three-round protocol described in their paper is the one with the most similarities to our protocol. In this protocol, a global group coordinator broadcasts a proposal to all the other nodes.
Reference: [Cristian 96] <author> F. Cristian. </author> <title> Group, Majority, and Strict agreement in Timed Asynchronous Distributed Systems. </title> <booktitle> In Proc. 16th Fault Tolerant Computing, Sendai, </booktitle> <address> Japan, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: The protocol presented in this paper will be used in Porcupine, and we 16 believe it could also be useful for other highly scalable cluster-based network server. An other research area of interest would be to construct majority or strict agreement <ref> [Cristian 96] </ref> with our protocol to support other various needs, as atomic multicast, in highly scalable cluster computing.
Reference: [Fischer 85] <author> M. Fischer, N. Lynch and M. Paterson. </author> <title> Impossibility of Distributed Consensus with One Faulty Process. </title> <journal> Journal of the ACM, </journal> <volume> 32(2) </volume> <pages> 374-382, </pages> <month> Apr. </month> <year> 1985. </year>
Reference-contexts: This is the role of the membership management to keep track of the alived nodes and to detects the faulty or unreachable ones. Membership management is well known to be a difficult problem, impossible to solve in asynchronous distributed systemes <ref> [Fischer 85, Chandra 96] </ref>. A node may appear failed because the running process and/or the network 1 is slow, and there is no means to establish the difference between being slow or being faulty (in a finite amount of time).
Reference: [Fox 97] <author> A. Fox, S. Gribble, Y. Chawathe, E. Brewer and P. Gauthier. </author> <title> Cluster-Based Scalable Network Services. </title> <booktitle> In Proc. 16th ACM Symposium on Operating Systems Principles, </booktitle> <address> Saint-Malo, France, </address> <month> Oct. </month> <year> 1997. </year>
Reference-contexts: 1 Introduction Cluster-based network servers is becoming increasingly popular. In particular network of cheap PC connected by a high speed network combine incremental scalability, high availability and low cost <ref> [Fox 97] </ref>. This is particularly true for instance for parallel and distributed computing [Sens 97], File systems [Anderson 96], and recently Internet based service, as WWW server [Pai 98] or Email server [Saito 98].
Reference: [Lee 96] <author> E. Lee and C. Thekkath. </author> <title> Petal: Distributed Virtual Disks. </title> <booktitle> In Proc. 7th Intl. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 84-92, </pages> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: Other examples of membership protocol for cluster-server is the periodic exchange of liveness message between all the nodes in the system, without a particular structure <ref> [Lee 96] </ref>.
Reference: [Pai 98] <author> V. Pai, M. Aron, G. Banga, M. Svendsen, P. Druschel, W. Zwaenepoel and E. </author> <title> Nahum. Locality-Aware Request Distribution in Cluster-based Network Servers. </title> <booktitle> In Proc. 8th Intl. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> San Jose, Califor-nia, </address> <month> Oct. </month> <year> 1998. </year>
Reference-contexts: In particular network of cheap PC connected by a high speed network combine incremental scalability, high availability and low cost [Fox 97]. This is particularly true for instance for parallel and distributed computing [Sens 97], File systems [Anderson 96], and recently Internet based service, as WWW server <ref> [Pai 98] </ref> or Email server [Saito 98]. In order to make progress, each node in these cluster-based network servers must be able to determine the set of reachable nodes, even in the presence of message lost, faults and partitions. <p> Our works primary goal is to be used by cluster-servers system. Some works assumes the membership is static, meaning no fault/recovery or addition of nodes can occurs without turning down the whole system, as in <ref> [Pai 98] </ref>. Microsoft scalable cluster [Vogels 98] constructs a membership service that scales to a maximum of ten nodes. The membership in the Star system uses a single global ring for fault detection and a protocol similar to the three round protocols described above to report failures [Sens 97].
Reference: [Saito 98] <author> Y. Saito, B. Bershad, H. Levy, E. Hoffman and B. Folliot. </author> <title> The Porcupine Scalable Mail Server. </title> <booktitle> In Proc. of 8th ACM SIGOPS European Workshop, </booktitle> <address> Sintra, Portugal, </address> <month> Sept. </month> <year> 1998. </year>
Reference-contexts: This is particularly true for instance for parallel and distributed computing [Sens 97], File systems [Anderson 96], and recently Internet based service, as WWW server [Pai 98] or Email server <ref> [Saito 98] </ref>. In order to make progress, each node in these cluster-based network servers must be able to determine the set of reachable nodes, even in the presence of message lost, faults and partitions. <p> This work is part of the Porcupine project, a highly scalable mail server developped at the University of Washington <ref> [Saito 98] </ref>. We have not be able to find any published membership protocol that scales to a thousand nodes. The protocol presented in this paper will be used in Porcupine, and we 16 believe it could also be useful for other highly scalable cluster-based network server.
Reference: [Sens 97] <author> P. Sens and B. Folliot. </author> <title> Performance Evaluation of Fault Tolerance for Parallel Applications in Networked Environments. </title> <booktitle> In Proc. of 26th Intl. Conf. on Parallel Processing, </booktitle> <address> Bloomingdale, Illinois, </address> <month> Aug. </month> <year> 1997. </year>
Reference-contexts: 1 Introduction Cluster-based network servers is becoming increasingly popular. In particular network of cheap PC connected by a high speed network combine incremental scalability, high availability and low cost [Fox 97]. This is particularly true for instance for parallel and distributed computing <ref> [Sens 97] </ref>, File systems [Anderson 96], and recently Internet based service, as WWW server [Pai 98] or Email server [Saito 98]. <p> Microsoft scalable cluster [Vogels 98] constructs a membership service that scales to a maximum of ten nodes. The membership in the Star system uses a single global ring for fault detection and a protocol similar to the three round protocols described above to report failures <ref> [Sens 97] </ref>. Other examples of membership protocol for cluster-server is the periodic exchange of liveness message between all the nodes in the system, without a particular structure [Lee 96].
Reference: [Van Renesse 98] <author> R. van Renesse, Y. Minsky and M. Hayden. </author> <title> A Gossip-Style Failure Detection Service. </title> <booktitle> In Proc. of Middleware'98, </booktitle> <address> The Lake District, England, </address> <month> Sept. </month> <year> 1998. </year> <month> 20 </month>
Reference-contexts: A bunch of works has been reported on the membership management, either for cluster-based servers, as cited before, or group membership for communication [Birman 94, Amir 95]. Unfortunately, as quoted by van Re-nesse et al. <ref> [Van Renesse 98] </ref>, existing membership management does not scale, thus limiting the scalability of cluster-based servers. For more than several tens of hosts, the failure detector either is unreasonably slow or make too many false fault detections. <p> Processors are divided into separate groups. Group structures have to be arranged in such a way that they more or less follow the physical network structure. They are currently managed by a hash of node identifier. A dynamic configuration is possible, as assigning groups by Ethernet subnet mask number <ref> [Van Renesse 98] </ref>. 1 Don't know where to put that... Here or in a separate section? 2 Our protocol makes extensively use of multicast provided by nowaday operating systems and routers (altough it would work, but with less scalability, with point to point communications). <p> XXX. 5.1 Cold booting Let's evaluate how our protocol behave when all the nodes are cold booting at appropriately the same time. This case is catastrophic in most existing protocols <ref> [Cristian 95, Van Renesse 98] </ref>, and allow us to revisit the low details of our ptotocol. <p> By distributing the single coorindator to a set of local coordinators (the first level ring) we avoid such network contention. 15 Recently a scalable gossip based failure detector has been proposed by van Rennesse et al. <ref> [Van Renesse 98] </ref>. The scalability relies on the construction of IP adresses, structured in host, subnetwork, domain. Gossip are mostly sent into subnets, with few gossips across subnets and fewer between domains. This structure also avoids network contention.
Reference: [Vogels 98] <author> W. Vogels, D. Dumitriu, A. Agrawal, T. Chia and K. Guo. </author> <title> Scalability of the Microsoft Cluster Service. </title> <booktitle> In Proc. 2nd USENIX Windows NT Symposium, </booktitle> <address> Seattle, Washington, </address> <pages> pp. 11-19, </pages> <month> Aug. </month> <year> 1998. </year> <month> 21 </month>
Reference-contexts: Our works primary goal is to be used by cluster-servers system. Some works assumes the membership is static, meaning no fault/recovery or addition of nodes can occurs without turning down the whole system, as in [Pai 98]. Microsoft scalable cluster <ref> [Vogels 98] </ref> constructs a membership service that scales to a maximum of ten nodes. The membership in the Star system uses a single global ring for fault detection and a protocol similar to the three round protocols described above to report failures [Sens 97].
References-found: 14

