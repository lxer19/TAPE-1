URL: ftp://ftp.cs.umass.edu/pub/lesser/sandholm-aaai96ss.ps
Refering-URL: http://www.cs.wustl.edu/~sandholm/
Root-URL: 
Email: sandholm@cs.umass.edu  brodley@ecn.purdue.edu  markus.sandholm@helsinki.fi  
Title: Comparison of Regression Methods, Symbolic Induction Methods and Neural Networks in Morbidity Diagnosis and Mortality
Author: Equine Gastrointestinal Colic Tuomas Sandholm Carla Brodley Von Braun Strae Markus Sandholm 
Date: Alexandar Vidovic  
Address: Amherst, MA 01003  West Lafayette, IN 47907  48712 Gescher-Hochmoor, Germany  Box 57, FIN-00014 Helsinki, Finland  
Affiliation: University of Massachusetts at Amherst Department of Computer Science  Purdue University School of Electrical and Computer Engineering  10  Helsinki University, Dept of Clinical Sciences Faculty of Veterinary Medicine  
Abstract: Classifier induction algorithms differ on what inductive hypotheses they can represent, and on how they search their space of hypotheses. No classifier is better than another for all problems: they have selective superiority. This paper empirically compares six classifier induction algorithms on the diagnosis of equine colic and the prediction of its mortality. The classification is based on simultaneously analyzing sixteen features measured from a patient. The relative merits of the algorithms (linear regression, decision trees, nearest neighbor classifiers, the Model Class Selection system, logistic regression (with and without feature selection), and neural nets) are qualitatively discussed, and the generalization accuracies quantitatively analyzed. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D. W. </author> <year> 1992. </year> <title> Tolerating Noisy, Irrelevant, and Novel Attributes in Instance-Based Learning Algorithms. </title> <journal> International Journal of Man-Machine Studies 36 </journal> <pages> 267-287. </pages>
Reference: <author> Brodley, C. E. </author> <year> 1993. </year> <title> Addressing the selective superiority problem: Automatic algorithm/model class selection. </title> <booktitle> In Machine Learning: Proceedings of the Tenth International Conference, </booktitle> <pages> 17-24, </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The results of empirical comparisons of existing algorithms illustrate that each algorithm has a selective superiority: it is best for some but not all classification tasks <ref> (Brodley 1993) </ref>. Selective superiority arises because each learning algorithm searches within a restricted hypothesis space defined by its class of models. For example, the class of first-order linear regression models is not appropriate when the data is best fit by a second-order linear regression model.
Reference: <author> Brodley, C. E. </author> <year> 1995. </year> <title> Recursive automatic bias selection for classifier construction. </title> <booktitle> Machine Learning 20: </booktitle> <pages> 63-94, </pages> <publisher> Kluwer, </publisher> <address> Hingham, MA. </address>
Reference: <author> Cost, S. and Salzberg, S. </author> <year> 1993. </year> <title> A weighted nearest neighbor algorithm for learning with symbolic features. </title> <booktitle> Machine Learning 10 </booktitle> <pages> 57-78. </pages>
Reference: <author> Duda, R. O. and Hart, P. E. </author> <year> 1973. </year> <title> Pattern classification and scene analysis. </title> <publisher> Wiley & Sons, </publisher> <address> New York. </address>
Reference-contexts: on the same data as the training, some methods would achieve 100% accuracy, because they would remember the classes of the training examples correctly. in the last few decades in the machine learning community, including various versions of perceptron (Nils-son 1965), version space (Mitchell 1977), decision tree (Quinlan 1986), instance-based <ref> (Duda & Hart 1973) </ref>, and neural net algorithms (Rumelhart & McClelland 1986). The results of empirical comparisons of existing algorithms illustrate that each algorithm has a selective superiority: it is best for some but not all classification tasks (Brodley 1993). <p> Decision trees are perhaps the most human-understandable learning method, which is important for trying to explain clas sification decisions. 3.3 Nearest neighbor classifier A k-nearest neighbor classifier <ref> (Duda & Hart 1973) </ref> is a set of n instances, each from one of m classes, that are used to classify feature vectors according to the majority classification of the feature vector's k nearest neighbors.
Reference: <author> Fayyad, U. M. and Irani, K. B. </author> <year> 1992. </year> <title> The attribute selection problem in decision tree generation. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> 104-110, </pages> <publisher> MIT Press. </publisher>
Reference: <author> Hecht-Nielsen, R. </author> <year> 1991. </year> <title> Neurocomputing. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA. </address>
Reference: <author> Mitchell, T. M. </author> <year> 1977. </year> <title> Version spaces: A candidate elimination approach to rule learning. </title> <booktitle> In Proceedings of the Fifth International Joint Conference on Artificial Intelligence, </booktitle> <pages> 305-310, </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: developed 1 If the evaluation were done on the same data as the training, some methods would achieve 100% accuracy, because they would remember the classes of the training examples correctly. in the last few decades in the machine learning community, including various versions of perceptron (Nils-son 1965), version space <ref> (Mitchell 1977) </ref>, decision tree (Quinlan 1986), instance-based (Duda & Hart 1973), and neural net algorithms (Rumelhart & McClelland 1986). The results of empirical comparisons of existing algorithms illustrate that each algorithm has a selective superiority: it is best for some but not all classification tasks (Brodley 1993).
Reference: <author> Nilsson, N. J. </author> <year> 1965. </year> <title> Learning machines. </title> <publisher> McGraw-Hill, </publisher> <address> New York. </address>
Reference-contexts: These methods, their relative merits, and the results regarding classification accuracy are discussed in the following subsections. 3.1 Linear regression A linear threshold unit (LTU) <ref> (Nilsson 1965) </ref> is a binary test of the form W T Y 0, where Y is a vector consisting of a constant 1 and the n features that describe the instance. W is a vector of n + 1 coefficients, also known as weights.
Reference: <author> Quinlan, J. R. </author> <year> 1986. </year> <title> Induction of decision trees. </title> <booktitle> Machine Learning 1: </booktitle> <pages> 81-106, </pages> <publisher> Kluwer, </publisher> <address> Hingham, MA. </address>
Reference-contexts: evaluation were done on the same data as the training, some methods would achieve 100% accuracy, because they would remember the classes of the training examples correctly. in the last few decades in the machine learning community, including various versions of perceptron (Nils-son 1965), version space (Mitchell 1977), decision tree <ref> (Quinlan 1986) </ref>, instance-based (Duda & Hart 1973), and neural net algorithms (Rumelhart & McClelland 1986). The results of empirical comparisons of existing algorithms illustrate that each algorithm has a selective superiority: it is best for some but not all classification tasks (Brodley 1993). <p> To select a test for a node in the tree, we choose the test that maximizes the information-gain ratio metric <ref> (Quinlan 1986) </ref>. Univariate decision tree algorithms require that each test have a discrete number of outcomes.
Reference: <author> Quinlan, J. R. </author> <year> 1987. </year> <title> Simplifying decision trees. </title> <journal> International Journal of Man-Machine Studies 27: </journal> <pages> 221-234. </pages>
Reference-contexts: One well-known approach to constructing a decision tree is to grow a tree until each of the terminal nodes (leaves) contains training instances from a single class only. The tree can then be pruned back with the objective of reducing the misclassification rate. Our algorithm uses reduced error pruning <ref> (Quinlan 1987) </ref>, which replaces a subtree with a leaf if it reduces the error on a set of data independent from the training data. (Note that this requires that we retain a portion of the training data to use for pruning the tree).
Reference: <author> Reevers, M. J., Curtis, C. R., Salman, M. D., Stashak, T. S. and Reif, J. F. </author> <year> 1992. </year> <title> Validation of logistic regression models used in the assessment of prognosis and the need for surgery in equine colic. </title> <journal> Prev. Vet. Med. </journal> <volume> 13: </volume> <pages> 155-172. </pages>
Reference-contexts: In other words, pathophysiological knowledge has guided decision making. Multiple logistic regression has been used to combine various predictors for most accurate prediction so far <ref> (Reevers et al. 1992) </ref>. Recently Sandholm et al. (1995) reported that increasing heart rate and plasma D-dimer together with decreasing chloride was a typical risk factor for non-survival, and that these three features could be used to enhance the accuracy of the logistic regression.
Reference: <author> Rumelhart, D. E. and McClelland, J. L. </author> <year> 1986. </year> <title> Parallel distributed processing. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: methods would achieve 100% accuracy, because they would remember the classes of the training examples correctly. in the last few decades in the machine learning community, including various versions of perceptron (Nils-son 1965), version space (Mitchell 1977), decision tree (Quinlan 1986), instance-based (Duda & Hart 1973), and neural net algorithms <ref> (Rumelhart & McClelland 1986) </ref>. The results of empirical comparisons of existing algorithms illustrate that each algorithm has a selective superiority: it is best for some but not all classification tasks (Brodley 1993). <p> The input units simply output their input. The hidden units and output unit output according to the logistic function (Rumel-hart & McClelland 1986). The weights of the connections were updated using the standard backpropagation rule <ref> (Rumelhart & McClelland 1986) </ref>. Backprop has two parameters: learning rate determines how fast the weights in the net are adjusted and momentum determines how slow it is to change the weight changes themselves on each update (Rumelhart & McClelland 1986). <p> The weights of the connections were updated using the standard backpropagation rule <ref> (Rumelhart & McClelland 1986) </ref>. Backprop has two parameters: learning rate determines how fast the weights in the net are adjusted and momentum determines how slow it is to change the weight changes themselves on each update (Rumelhart & McClelland 1986). In our experiments, learning rate was varied and momentum was set to one tenth of the learning rate. We experimented with different net topologies by varying the number of hidden units from a low of three to a high of 31.
Reference: <author> Sandholm, M., Vidovic, A., Puotunen-Reinert, A., Sankari, S., Nyholm, K. and Rita, H. </author> <year> 1995. </year> <title> D-dimer improves the prognostic value of combined clinical and laboratory data in equine gastrointestinal colic. </title> <journal> Acta vet. scand. </journal> <volume> 36, 2: </volume> <pages> 255-272. </pages> <note> Statistix User's Manual. 1992. Version 4.0. (c) 92 Analytical Software. </note>
Reference-contexts: This happened even though instances with missing feature values were ignored and the sixteenth feature (a ratio of two other features) was not included among the alternatives <ref> (Sandholm et al. 1995) </ref>. of hidden units, the learning rate, and the best time to stop training and generalization accuracy is sensitive to these choices. Decreasing the number of features reduced the generalization accuracy of logistic regression in morbidity classification, but enhanced it in mortality prediction from 65% to 73%.
Reference: <author> Young, P. </author> <year> 1984. </year> <title> Recursive estimation and time-series analysis. </title> <publisher> Springer-Verlag, </publisher> <address> New York. </address>
Reference-contexts: If W T Y 0, then the LTU infers that Y belongs to one class A, otherwise the LTU infers that Y belongs to the other class B. To find the set of weights that leads to an accurate classifier, we used the Recursive Least Squares (RLS) Procedure <ref> (Young 1984) </ref>. RLS, invented by Gauss, is a recursive version of the Least Squares (LS) Algorithm.
References-found: 15

