URL: ftp://ftp.cs.utexas.edu/pub/mooney/papers/rapture-connsci-94.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/ml/abstracts.html
Root-URL: 
Email: mahoney@cs.utexas.edu,  mooney@cs.utexas.edu,  
Phone: (512) 471-9589  (512) 471-9558  
Title: Combining Connectionist and Symbolic Learning to Refine Certainty-Factor Rule Bases  
Author: J. Jeffrey Mahoney and Raymond J. Mooney 
Date: June 1, 1993  
Address: Austin, TX 78712  
Affiliation: Dept. of Computer Sciences University of Texas  
Abstract: This paper describes Rapture | a system for revising probabilistic knowledge bases that combines connectionist and symbolic learning methods. Rapture uses a modified version of backpropagation to refine the certainty factors of a probabilistic rule base and it uses ID3's information-gain heuristic to add new rules. Results on refining three actual expert knowledge bases demonstrate that this combined approach generally performs better than previous methods. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Berenji, H. </author> <year> (1990). </year> <title> Refinement of approximate reasoning-based controllers by reinforcement learning. </title> <booktitle> In Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <pages> 475-479. </pages> <address> Evanston, IL. </address>
Reference-contexts: Recent methods for inducing Bayesian networks from data (Geiger et al., 1990; Cooper and Herskovits, 1992) could also prove useful in making structural changes to an existing network. Regarding fuzzy logic, there has been some recent work in using training data and connectionist learning methods to revise fuzzy controllers <ref> (Berenji, 1990) </ref>, and Rapture could potentially be extended to deal with these sorts of problems as well. 7 Conclusions Automatic refinement of probabilistic rule bases is an under-studied problem with important applications to the development of intelligent systems.
Reference: <author> Buchanan, G., and E.H. Shortliffe, e. </author> <year> (1984). </year> <title> Rule-Based Expert Systems:The MYCIN Experiments of the Stanford Heuristic Programming Project. </title> <address> Reading, MA: </address> <publisher> Addison-Wesley Publishing Co. </publisher>
Reference-contexts: Rapture has been tested on revising several real-world knowledge bases with encouraging results. In particular, we present results for revising rule bases for promoter recognition in DNA sequences (Towell et al., 1990), soybean disease diagnosis (Michalski and Chilausky, 1980), and diagnosis of bacterial infections <ref> (Buchanan and E.H. Shortliffe, 1984) </ref>. In the last domain, Rapture successfully revised a version of the Mycin knowledge-base.
Reference: <author> Cohen, W. </author> <year> (1992). </year> <title> Compiling prior knowledge into an explicit bias. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <pages> 102-110. </pages> <address> Aberdeen, Scotland. </address>
Reference: <author> Cooper, G. </author> <year> (1987). </year> <title> Probabilistic inference using belief networks is np-hard. </title> <type> Technical Report KSL-87-27, </type> <institution> Medical Computer Science Group, Stanford Univ., Stanford, </institution> <address> CA. </address>
Reference-contexts: Second, probabilistic sum is a simple, differentiable, non-linear function. This is crucial for implementing gradient descent using backpropagation. Further, other formalisms of uncertain reasoning (e.g. Bayesian networks) have been shown to be NP-Hard to evaluate in the general case <ref> (Cooper, 1987) </ref>. Even more significantly, however, is the widespread use of certainty factors. Despite recent criticism of certainty factors (Shafer and J. Pearl, 1990), there have been numerous knowledge-bases implemented using the certainty-factor model, which immediately gives our approach a large base of applicability.
Reference: <author> Cooper, G. G., and Herskovits, E. </author> <year> (1992). </year> <title> A bayesian method for the induction of probabilistic networks from data. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 309-347. </pages>
Reference: <author> Fahlman, S., and Lebiere, C. </author> <year> (1989). </year> <booktitle> The cascade-correlation learning architecture. In Advances in Neural Information Processing Systems 2, </booktitle> <pages> 524-532. </pages> <address> Denver, </address> <publisher> CO. </publisher>
Reference-contexts: The results on promoter recognition in Section 4.1 indicate that the Rapture approach produces a simpler and slightly more accurate revised knowledge base than Kbann. There has been a number of methods for growing a network architecture sufficient to classify a set of training examples, e.g. cascade correlation <ref> (Fahlman and Lebiere, 1989) </ref>, the upstart algorithm (Frean, 1990), and the tiling algorithm (Mezard and Nadal, 1989). By contrast, Rapture uses methods from decision-tree induction (Quinlan, 1986b) to add units to an existing network.
Reference: <author> Feldman, R. </author> <year> (1993). </year> <title> Probabilistic Revision of Logical Domain Theories. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Cornell University, Ithaca, NY. </institution> <note> 29 Frean, </note> <author> M. </author> <year> (1990). </year> <title> The upstart algorithm: A method for constructing and training feedfor--ward neural networks. </title> <journal> Neural Computation, </journal> <volume> 2 </volume> <pages> 198-209. </pages>
Reference-contexts: However, the current method is limited to adding new input units that directly feed into the output layer. Appropriately modified versions of connectionist methods for growing networks may prove useful in allowing Rapture to add new hidden units. Feldman's <ref> (Feldman, 1993) </ref> PTR system takes an initial rule base expressed as a collection of Horn clause rules, along with an expert's confidence values in the accuracy of each of these rules.
Reference: <author> Fu, L.-M. </author> <year> (1989). </year> <title> Integration of neural heuristics into knowledge-based inference. </title> <journal> Connection Science, </journal> <volume> 1(3) </volume> <pages> 325-339. </pages>
Reference-contexts: By using these values, along with a set of training examples, PTR is able to incrementally reformulate the rule base in such a way that is consistent with the training data, as well as maximizing one's confidence in the rules. Fu <ref> (Fu, 1989) </ref> and Lacher (Lacher, 1992) have also used backpropagation techniques to revise certainty factors on rules. Fu has apparently derived formulas for CFBP, although they are not given in the paper.
Reference: <author> Gallant, S. </author> <year> (1988). </year> <title> Connectionist expert systems. </title> <journal> Communications of the Association for Computing Machinery, </journal> <volume> 31 </volume> <pages> 152-169. </pages>
Reference-contexts: They report only modest improvements in the accuracy of the same Mycin rule base (one used in our experiments). Their experiments increase performance from 26:8% to 36:0%. Rapture has the advantage of being able to adjust certainty factors and add rules in addition to deleting rules. Gallant <ref> (Gallant, 1988) </ref> was apparently the first to design and implement a system that combines expert domain knowledge with connectionist learning. Given a set of training examples and expert supplied dependency information, his system builds a connectionist network that correctly classifies the training data.
Reference: <author> Geiger, D., Paz, A., and Pearl, J. </author> <year> (1990). </year> <title> Learning causal trees from dependence information. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> 770-776. </pages> <month> Boston,MA. </month>
Reference: <author> Ginsberg, A., Weiss, S. M., and Politakis, P. </author> <year> (1988). </year> <title> Automatic knowledge based refinement for classification systems. </title> <journal> Artificial Intelligence, </journal> <volume> 35 </volume> <pages> 197-226. </pages>
Reference-contexts: Compared to most previous work, Rapture deals with a wider range of theory refinement problems and has been more thoroughly tested on actual expert knowledge bases. The Seek2 system <ref> (Ginsberg et al., 1988) </ref> revises rule bases containing M-of-N rules, also known as choice-component rules. Seek2 uses specific heuristics to revise the threshold, M, of individual rules in order to improve performance on the training data.
Reference: <author> Heckerman, D. </author> <year> (1986). </year> <title> Probabilistic interpretations for Mycin's certainty factors. </title> <editor> In Kanal, L. N., and Lemmer, J. F., editors, </editor> <booktitle> Uncertainty in Artificial Intelligence, </booktitle> <pages> 167-196. </pages> <address> Ams-terdam: </address> <publisher> North Holland. </publisher>
Reference-contexts: In recent years, certainty-factors have been the subject of considerable criticism from researchers in uncertain reasoning (Shafer and J. Pearl, 1990). Certainty factors only have a clear probabilistic semantics if very restrictive assumptions about independence are made <ref> (Heckerman, 1986) </ref>. However, the basic revision framework in Rapture should be applicable to other uncertain reasoning formalisms such as Bayesian networks (Pearl, 1988), Dempster-Shafer theory (Shafer, 1976), or fuzzy logic (Zadeh, 1965).
Reference: <author> Hinton, G. E. </author> <year> (1986). </year> <title> Learning distributed representations of concepts. </title> <booktitle> In Proceedings of the Eighth Annual Conference of the Cognitive Science Society, </booktitle> <pages> 1-12. </pages> <address> Amherst, MA. </address>
Reference-contexts: Also, it is unclear how certainty-factor rules might be mapped into a Kbann network. Kbann allows the learning of new rules by including an underlying fully-connected network of low-weighted links. These links can be "recruited" by backpropagation and eventually mapped back into new rules. Weight decay <ref> (Hinton, 1986) </ref> is used to keep weights small and therefore help minimize the number of new rules that are eventually introduced. By contrast, Rapture uses symbolic methods to add a minimal number of new connections (rules) as needed.
Reference: <author> Lacher, R. </author> <year> (1992). </year> <title> Node error assignment in expert networks. </title> <editor> In Kandel, A., and Langholz, G., editors, </editor> <booktitle> Hybrid Architectures for Intelligent Systems, </booktitle> <pages> 29-48. </pages> <address> Boca Raton, FL: </address> <publisher> CRC Press, Inc. </publisher>
Reference-contexts: By using these values, along with a set of training examples, PTR is able to incrementally reformulate the rule base in such a way that is consistent with the training data, as well as maximizing one's confidence in the rules. Fu (Fu, 1989) and Lacher <ref> (Lacher, 1992) </ref> have also used backpropagation techniques to revise certainty factors on rules. Fu has apparently derived formulas for CFBP, although they are not given in the paper.
Reference: <author> Ling, X., and Valtorta, M. </author> <year> (1991). </year> <title> Revision of reduced theories. </title> <booktitle> In Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <pages> 519-523. </pages> <address> Evanston, IL. </address>
Reference-contexts: All evidence can then be combined giving an overall degree of confidence in the consequent. The use of probabilistic sum enables many small pieces of evidence to add up to significant evidence. This is lacking in formalisms that use only MIN or MAX for combining evidence <ref> (Ling and Valtorta, 1991) </ref>. Second, probabilistic sum is a simple, differentiable, non-linear function. This is crucial for implementing gradient descent using backpropagation. Further, other formalisms of uncertain reasoning (e.g. Bayesian networks) have been shown to be NP-Hard to evaluate in the general case (Cooper, 1987).
Reference: <author> Ma, Y., and Wilkins, D. C. </author> <year> (1991). </year> <title> Improving the performance of inconsistent knowledge bases via combined optimization method. </title> <booktitle> In Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <pages> 23-27. </pages> <address> Evanston, IL. </address>
Reference-contexts: Rapture uses heuristic methods such as backpropagation and information gain to refine multi-layer networks that use MIN, MAX, and probabilistic sum. Although the method does not guarantee convergence, it performs quite well in practice on fairly large, realistic problems. Ma and Wilkins <ref> (Ma and Wilkins, 1991) </ref> have developed methods for improving the accuracy of a certainty-factor knowledge base by deleting rules. They report only modest improvements in the accuracy of the same Mycin rule base (one used in our experiments). Their experiments increase performance from 26:8% to 36:0%.
Reference: <author> Mezard, M., and Nadal, J. </author> <year> (1989). </year> <title> Learning in feedforward layered networks: The tiling algorithm. </title> <journal> Journal of Physics, A22(12):2191-2203. </journal>
Reference-contexts: There has been a number of methods for growing a network architecture sufficient to classify a set of training examples, e.g. cascade correlation (Fahlman and Lebiere, 1989), the upstart algorithm (Frean, 1990), and the tiling algorithm <ref> (Mezard and Nadal, 1989) </ref>. By contrast, Rapture uses methods from decision-tree induction (Quinlan, 1986b) to add units to an existing network. However, the current method is limited to adding new input units that directly feed into the output layer.
Reference: <author> Michalski, R. S., and Chilausky, S. </author> <year> (1980). </year> <title> Learning by being told and learning from examples: An experimental comparison of the two methods of knowledge acquisition in the context of developing an expert system for soybean disease diagnosis. </title> <journal> Journal of Policy Analysis and Information Systems, </journal> <volume> 4(2) </volume> <pages> 126-161. </pages>
Reference-contexts: Rapture has been tested on revising several real-world knowledge bases with encouraging results. In particular, we present results for revising rule bases for promoter recognition in DNA sequences (Towell et al., 1990), soybean disease diagnosis <ref> (Michalski and Chilausky, 1980) </ref>, and diagnosis of bacterial infections (Buchanan and E.H. Shortliffe, 1984). In the last domain, Rapture successfully revised a version of the Mycin knowledge-base.
Reference: <author> Mooney, R. J., and Ourston, D. </author> <year> (1991). </year> <title> A multistrategy approach to theory refinement. </title> <booktitle> In Proceedings of the International Workshop on Multistrategy Learning, </booktitle> <pages> 115-130. </pages> <address> Harper's Ferry, W.Va. </address> <note> 30 O'Neill, </note> <author> M., and Chiafari, F. </author> <year> (1989). </year> <title> Escherichia coli promoters. </title> <journal> Journal of Biological Chemistry, </journal> <volume> 264 </volume> <pages> 5531-5534. </pages>
Reference-contexts: Either to date has only been run up to 100 examples. Either has also been run using a partial-matching technique, where examples are classified into the category that was closest to firing, in which case its' performance nearly matches that of Rapture <ref> (Mooney and Ourston, 1991) </ref>. 18 Training for this data set clearly took much more effort than the others, as seen in Rapture does indeed perform more efficiently than either backpropagation or Either. 4.3 MYCIN Results Experiments were also run on a version of the Mycin knowledge-base (Buchanan and E.H.
Reference: <author> Ourston, D., and Mooney, R. </author> <year> (1990). </year> <title> Changing the rules: a comprehensive approach to theory refinement. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> 815-820. </pages> <address> Detroit, MI. </address>
Reference-contexts: We compare our results to those obtained for purely inductive methods (ID3, standard backpropagation, and Rapture given no initial knowledge), a purely connectionist method for knowledge-base refinement (Kbann), and a purely symbolic method for knowledge-base refinement (Either <ref> (Ourston and Mooney, 1990) </ref>). Rapture generally produces more accurate results from fewer training examples than these competing approaches. In the promoter domain, it also produces a simpler revised rule base than Kbann or Either. The rest of this paper is organized as follows. <p> This includes overviews of two recent theory revision systems (Kbann and Either), as well as the certainty-factor formalism upon which Rapture is based. 2.1 EITHER Either <ref> (Ourston and Mooney, 1990) </ref> is a recent theory revision system that uses propositional Horn-clause logic to represent its theories. It begins with an expert-given rule base, and a set of correctly labelled (training) examples. 3 Either revises its rule base whenever a training example is classified incorrectly.
Reference: <author> Ourston, D., and Mooney, R. J. </author> <title> (in press). Theory refinement combining analytical and empirical methods. </title> <journal> Artificial Intelligence. </journal>
Reference: <author> Pazzani, M., and Kibler, D. </author> <year> (1992). </year> <title> The utility of background knowledge in inductive learning. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 57-94. </pages>
Reference: <author> Pearl, J. </author> <year> (1988). </year> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <address> San Mateo,CA: </address> <publisher> Morgan Kaufmann, Inc. </publisher>
Reference-contexts: Pearl, 1990). Certainty factors only have a clear probabilistic semantics if very restrictive assumptions about independence are made (Heckerman, 1986). However, the basic revision framework in Rapture should be applicable to other uncertain reasoning formalisms such as Bayesian networks <ref> (Pearl, 1988) </ref>, Dempster-Shafer theory (Shafer, 1976), or fuzzy logic (Zadeh, 1965). As long as the activation functions in the corresponding network implementations of these methods are differentiable, backpropagation techniques should be employable. <p> As long as the activation functions in the corresponding network implementations of these methods are differentiable, backpropagation techniques should be employable. For example, if Bayesian networks are restricted 24 to being singly-connected, there are linear-time algorithms for calculating the desired output probabilities <ref> (Pearl, 1988) </ref>. The resulting equations appear to be amenable to backpropagation. Recent methods for inducing Bayesian networks from data (Geiger et al., 1990; Cooper and Herskovits, 1992) could also prove useful in making structural changes to an existing network.
Reference: <author> Quinlan, J. R. </author> <year> (1986a). </year> <title> The effect of noise on concept learning. </title> <editor> In Michalski, R. S., Car-bonell, J. G., and Mitchell, T. M., editors, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, </booktitle> <volume> Volume II, </volume> <pages> 149-166. </pages> <publisher> Morgan Kaufman. </publisher>
Reference: <author> Quinlan, J. R. </author> <year> (1986b). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 81-106. </pages>
Reference-contexts: If all of the examples can be classified correctly through backpropagation alone, then the network is considered trained. Otherwise, symbolic methods are used to alter the network architecture. Specifically, features are added that help discriminate examples 2 according to ID3's information gain criterion <ref> (Quinlan, 1986b) </ref> and low-weighted links are deleted. Backpropagation and node addition/deletion continue in a cycle until all of the training examples are correctly classified. Once it has been trained, the revised rules can be read directly off of the network. <p> F N i |the false negatives for C i , and CE i |the true positives for all categories that are being confused with C i . We need to find a feature-value which can discriminate between these groups of examples. Quinlan's ID3 metric <ref> (Quinlan, 1986b) </ref> has been adopted by Rapture as the solution to this problem. ID3 is designed to build decision trees that classify examples into pre-defined categories. <p> There has been a number of methods for growing a network architecture sufficient to classify a set of training examples, e.g. cascade correlation (Fahlman and Lebiere, 1989), the upstart algorithm (Frean, 1990), and the tiling algorithm (Mezard and Nadal, 1989). By contrast, Rapture uses methods from decision-tree induction <ref> (Quinlan, 1986b) </ref> to add units to an existing network. However, the current method is limited to adding new input units that directly feed into the output layer. Appropriately modified versions of connectionist methods for growing networks may prove useful in allowing Rapture to add new hidden units.
Reference: <author> Rumelhart, D. E., Hinton, G. E., and Williams, J. R. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In Rumelhart, D. E., and McClelland, J. L., editors, </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> Vol. I, </volume> <pages> 318-362. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Pearl, 1990). There has been recent work on revising such probabilistic knowledge bases (Ginsberg et al., 1988; Fu, 1989). In particular, by representing a probabilistic rule base as a connectionist network, backpropagation methods <ref> (Rumelhart et al., 1986) </ref> can be used to modify the weights representing rule strengths (Fu, 1989; Lacher, 1992). However, these methods are restricted to revising numerical parameters and are unable to make structural changes to the knowledge base, such as adding new rules.
Reference: <author> Shafer, G. </author> <year> (1976). </year> <title> A Mathematical Theory of Evidence. </title> <publisher> Princeton, </publisher> <address> NJ: </address> <publisher> Princeton University Press. </publisher>
Reference-contexts: Pearl, 1990). Certainty factors only have a clear probabilistic semantics if very restrictive assumptions about independence are made (Heckerman, 1986). However, the basic revision framework in Rapture should be applicable to other uncertain reasoning formalisms such as Bayesian networks (Pearl, 1988), Dempster-Shafer theory <ref> (Shafer, 1976) </ref>, or fuzzy logic (Zadeh, 1965). As long as the activation functions in the corresponding network implementations of these methods are differentiable, backpropagation techniques should be employable.
Reference: <author> Shafer, G., and J. Pearl, e. </author> <year> (1990). </year> <title> Readings in Uncertain Reasoning. </title> <address> San Mateo,CA: </address> <publisher> Morgan Kaufmann, Inc. </publisher>
Reference-contexts: However, many real-world domains require some form of probabilistic reasoning. Consequently, many knowledge bases obtained from experts are represented using a probabilistic formalism, such as certainty factors, Dempster-Shafer theory, or Bayesian networks <ref> (Shafer and J. Pearl, 1990) </ref>. There has been recent work on revising such probabilistic knowledge bases (Ginsberg et al., 1988; Fu, 1989). <p> This is crucial for implementing gradient descent using backpropagation. Further, other formalisms of uncertain reasoning (e.g. Bayesian networks) have been shown to be NP-Hard to evaluate in the general case (Cooper, 1987). Even more significantly, however, is the widespread use of certainty factors. Despite recent criticism of certainty factors <ref> (Shafer and J. Pearl, 1990) </ref>, there have been numerous knowledge-bases implemented using the certainty-factor model, which immediately gives our approach a large base of applicability. Finally, and perhaps most importantly, are the empirical results. <p> A comparison of the current version of Rapture to an alternative version that uses an initial background network of low-weighted links could help elucidate the advantages and disadvantages of these two approaches. In recent years, certainty-factors have been the subject of considerable criticism from researchers in uncertain reasoning <ref> (Shafer and J. Pearl, 1990) </ref>. Certainty factors only have a clear probabilistic semantics if very restrictive assumptions about independence are made (Heckerman, 1986).
Reference: <author> Shavlik, J. W., Mooney, R. J., and Towell, G. G. </author> <year> (1991). </year> <title> Symbolic and neural learning algorithms: An experimental comparison. </title> <journal> Machine Learning, </journal> <volume> 6 </volume> <pages> 111-143. </pages>
Reference-contexts: This gives further support to the idea that this domain is one in which having every feature available for use is desirable. It is also noteworthy that backpropagation is significantly outperforming ID3. In most domains studies, these two systems generally perform at equivalent levels <ref> (Shavlik et al., 1991) </ref>, and future work will examine why backpropagation is performing so well. 5 Related Work This section reviews related work in both the connectionist and symbolic areas.
Reference: <author> Shavlik, J. W., and Towell, G. G. </author> <year> (1989). </year> <title> Combining explanation-based and neural learning: An algorithm and empirical results. </title> <journal> Connection Science, </journal> <volume> 1(3) </volume> <pages> 325-339. </pages>
Reference: <author> Shortliffe, E., and Buchanan, B. </author> <year> (1975). </year> <title> A model of inexact reasoning in medicine. </title> <journal> Mathematical Biosciences, </journal> <volume> 23 </volume> <pages> 351-379. </pages>
Reference: <author> Swartout, W. </author> <year> (1981). </year> <title> Explaining and justifying in expert consulting programs. </title> <booktitle> In Proceedings of the Seventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> 203-208. </pages> <address> Vancouver, BC. </address> <note> 31 Thompson, </note> <author> K., Langley, P., and Iba, W. </author> <year> (1991). </year> <title> Using background knowledge in concept formation. </title> <booktitle> In Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <pages> 554-558. </pages> <address> Evanston, IL. </address>
Reference-contexts: The approach therefore combines the effectiveness of connectionist learning methods with the interpretability of rules. Comprehensibility is important since it has been found that users will generally not accept a system's conclusions unless it can present meaningful explanations for them <ref> (Swartout, 1981) </ref>. Rapture has been tested on revising several real-world knowledge bases with encouraging results. In particular, we present results for revising rule bases for promoter recognition in DNA sequences (Towell et al., 1990), soybean disease diagnosis (Michalski and Chilausky, 1980), and diagnosis of bacterial infections (Buchanan and E.H.
Reference: <author> Towell, G., and Shavlik, J. </author> <year> (1992). </year> <title> Interpretation of artificial neural networks: Mapping knowledge-based neural networks into rules. </title> <editor> In Lippmann, R., Moody, J., and Touret-zky, D., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> vol. 4. </volume> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Backpropagation and node addition/deletion continue in a cycle until all of the training examples are correctly classified. Once it has been trained, the revised rules can be read directly off of the network. In the Kbann system <ref> (Towell and Shavlik, 1992) </ref>(see next section), revised networks are mapped back into rules to improve the comprehensibility of the final result. In Rapture, the direct correspondence between weighted links and probabilistic rules removes any distinction between the symbolic and connectionist representations.
Reference: <author> Towell, G. G. </author> <year> (1991). </year> <title> Symbolic Knowledge and Neural Networks: Insertion, Refinement, and Extraction. </title> <type> PhD thesis, </type> <institution> University of Wisconsin, Madison, WI. </institution>
Reference-contexts: Ourston and Mooney (in press) illustrate the success of this heuristic with results in several domains. 2.2 KBANN Kbann <ref> (Towell, 1991) </ref> is a revision system that combines a rule base with neural network learning. An expert-supplied rule base is converted into a neural network, which is then trained using connectionist techniques. After training, the network is translated into symbolic rules.
Reference: <author> Towell, G. G., Shavlik, J. W., and Noordewier, M. O. </author> <year> (1990). </year> <title> Refinement of approximate domain theories by knowledge-based artificial neural networks. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> 861-866. </pages> <address> Boston, MA. </address>
Reference-contexts: Rapture has been tested on revising several real-world knowledge bases with encouraging results. In particular, we present results for revising rule bases for promoter recognition in DNA sequences <ref> (Towell et al., 1990) </ref>, soybean disease diagnosis (Michalski and Chilausky, 1980), and diagnosis of bacterial infections (Buchanan and E.H. Shortliffe, 1984). In the last domain, Rapture successfully revised a version of the Mycin knowledge-base. <p> These data sets are discussed in detail in the following sections. 13 4.1 DNA Promoter Results A prokaryotic promoter is a short DNA sequence that precedes the beginnings of genes, and are locations where the protein RNA polymerase binds to the DNA structure <ref> (Towell et al., 1990) </ref>. A theory designed to recognize such strings composed of DNA-nucleotides was given to Rapture for revision. The data set used for these experiments is one of 106 examples, for which there are 53 positive examples (i.e. promoters), and 53 negative examples.
Reference: <author> Valtorta, M. </author> <year> (1988). </year> <title> Some results on the complexity of knowledge-base refinement. </title> <booktitle> In Proceedings of the Sixth International Workshop on Machine Learning, </booktitle> <pages> 326-331. </pages> <address> Ithaca, NY. </address>
Reference: <author> Valtorta, M. </author> <year> (1990). </year> <title> More results on the complexity of knowledge-base refinement:belief networks. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <pages> 419-424. </pages> <address> Austin, TX. </address>
Reference: <author> Zadeh, L. </author> <year> (1965). </year> <title> Fuzzy sets. </title> <journal> Information and Control, </journal> <volume> 8 </volume> <pages> 338-353. 32 </pages>
Reference-contexts: Pearl, 1990). Certainty factors only have a clear probabilistic semantics if very restrictive assumptions about independence are made (Heckerman, 1986). However, the basic revision framework in Rapture should be applicable to other uncertain reasoning formalisms such as Bayesian networks (Pearl, 1988), Dempster-Shafer theory (Shafer, 1976), or fuzzy logic <ref> (Zadeh, 1965) </ref>. As long as the activation functions in the corresponding network implementations of these methods are differentiable, backpropagation techniques should be employable. For example, if Bayesian networks are restricted 24 to being singly-connected, there are linear-time algorithms for calculating the desired output probabilities (Pearl, 1988).
References-found: 38

