URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3263/3263.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Title: On Graded QR Decompositions of Products of Matrices  
Author: G. W. Stewart 
Date: April 1994  
Affiliation: University of Maryland College Park Institute for Advanced Computer Studies TR-94-53 Department of Computer Science  
Pubnum: TR-3263  
Abstract: This paper is concerned with the singular values and vectors of a product M m = A 1 A 2 A m of matrices of order n. The chief difficulty with computing them from directly from M m is that with increasing m the ratio of the small to the large singular values of M m may fall below the rounding unit, so that the former are computed inaccurately. The solution proposed here is to compute recursively the factorization M m = QRP T , where Q is orthogonal, R is a graded upper triangular, and P T is a permutation. fl This report is available by anonymous ftp from thales.cs.umd.edu in the directory pub/reports. y Department of Computer Science and Institute for Advanced Computer Studies, University of Maryland, College Park, MD 20742. This work was supported in part by The National Science Foundation under Grant CCR 9115568. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Bojanczyk, J. G. Nagy, and R. J. Plemmons. </author> <title> Block RLS using row Householder reflections. Linear Algebra and Its Applications, </title> <address> 188-189:31-62, </address> <year> 1993. </year>
Reference-contexts: The rule of thumb is that singular values of M m whose ratio to the largest singular value is less than the rounding unit will have no accuracy. The product singular value decomposition (PSVD) is one way of circumventing this problem <ref> [1] </ref>.
Reference: [2] <author> J. J. Dongarra, J. R. Bunch, C. B. Moler, and G. W. Stewart. </author> <title> LINPACK User's Guide. </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1979. </year>
Reference-contexts: In practice, the norms should be downdated as in <ref> [2] </ref>.) In the second step, the rotations generated in the first are applied to R A . When a rotation in the (i; i + 1) plane is postmultiplied into R A , it creates a nonzero element in the (i + 1; i) position.
Reference: [3] <author> G. H. Golub and C. F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> Johns Hopkins University Press, </publisher> <address> Baltimore, Maryland, 2nd edition, </address> <year> 1989. </year>
Reference-contexts: The algorithm is quite inexpensive. An operation count shows that it requires 5 1 6 n 3 additions and 10 1 6 n 3 multiplications. When n is large enough, scaled rotations <ref> [3, x5.1.13] </ref> can be used to reduce the number of multiplications to 5 1 6 n 3 . In this case, the algorithm can be compared with five matrix multiplications. 3. <p> In step one we have used pivoting on column norms to establish a grading. This is the simplest method in an arsenal of techniques designed to reveal the rank of a matrix. Although there is a famous counter-example for which the method fails <ref> [3, x5.5.7] </ref>, in practice it works well. However, as we mentioned in the introduction, other rank revealing decompositions may be substituted for the QR decomposition obtained by pivoting on column norms. Turning now to step two of the algorithm, let us see how grading in R A can be lost.
Reference: [4] <author> M. T. Heath, A. J. Laub, C. C. Paige, and R. C. Ward. </author> <title> Computing the SVD of a product of two matrices. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 7 </volume> <pages> 1147-1159, </pages> <year> 1986. </year> <title> 10 The Graded QR Decomposition of a Matrix Product GWS </title>
Reference-contexts: An alternative is an algorithm for computing the singular value decomposition of a product of two matrices <ref> [4] </ref>. Given the singular value decomposition of M m , it can be used to calculate the singular value decomposition of M m+1 = M m A m+1 . However, it is not clear how the increasing spread of the singular values affects this algorithm. <p> Numerical Results The main problem with testing the algorithm is constructing test cases whose answers can be easily recognized. The solution taken here is an extension of an idea in <ref> [4] </ref>. The matrices U and V of left and right singular vectors are calculated from a random normal matrix, and a diagonal matrix is chosen.
Reference: [5] <author> R. Mathias and G. W. Stewart. </author> <title> A block qr algorithm and the singular value decomposition. </title> <journal> Linear Algebra and Its Applications, </journal> <volume> 182 </volume> <pages> 91-100, </pages> <year> 1993. </year>
Reference: [6] <author> G. W. Stewart. </author> <title> An updating algorithm for subspace tracking. </title> <journal> IEEE Transactions on Signal Processing, </journal> <volume> 40 </volume> <pages> 1535-1541, </pages> <year> 1992. </year>
Reference-contexts: A final point. Although we have chosen to work with QRP decompositions in which P is a permutation, it will be clear that P could equally well be an orthogonal matrix. Thus the approach taken here also applies to two-sided orthogonal decompositions such as the URV decomposition <ref> [6] </ref>. 2. The Algorithm In this section we will describe the algorithm for updating graded QRP decompositions. The description will be in two stages: first an overview at the matrix GWS The Graded QR Decomposition of a Matrix Product 3 level, then a detailed description.
References-found: 6

