URL: http://www.cs.cmu.edu/afs/cs/user/aberger/www/ps/candide.ps
Refering-URL: http://www.cs.utexas.edu/users/nl-acq/paper-history.html
Root-URL: 
Title: The Candide System for Machine Translation  
Author: Adam L. Berger, Peter F. Brown Stephen A. Della Pietra, Vincent J. Della Pietra, John R. Gillett, John D. Lafferty, Robert L. Mercer, Harry Printz, Lubos Ures 
Address: P.O. Box 704 Yorktown Heights, NY 10598  
Affiliation: IBM Thomas J. Watson Research Center  
Abstract: We present an overview of Candide, a system for automatic translation of French text to English text. Candide uses methods of information theory and statistics to develop a probability model of the translation process. This model, which is made to accord as closely as possible with a large body of French and English sentence pairs, is then used to generate English translations of previously unseen French sentences. This paper provides a tutorial in these methods, discussions of the training and operation of the system, and a summary of test results. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Allen, Arnold O. </author> <title> Probability, Statistics and Queueing Theory, </title> <publisher> Academic Press, </publisher> <address> New York, NY, </address> <year> 1978. </year>
Reference-contexts: This paper presents el ementary expositions of each of these ideas, and explains how they have been assembled to produce Candide. In Section 2 we introduce the necessary ideas from informa tion theory and statistics. The reader is assumed to know el ementary probability theory at the level of <ref> [1] </ref>. In Sections 3 and 4 we discuss our language and translation models. In Section 5 we describe the operation of Candide as it trans lates a French document. In Section 6 we present results of our internal evaluations and the arpa Machine Translation Project evaluations. <p> Typically the formula for the likelihood includes some con straints on the elements of to ensure that Pr (c) really is a probability distribution|that is, it is always a real value in <ref> [0; 1] </ref>, and for fixed the sum P c Pr (c) over all possible c vectors is 1. Consider the problem of training this parametric model to the data c; that is, adjusting the to maximize Pr (c). Finding the maximizing is an exercise in constrained optimization.
Reference: 2. <author> Baum, L. E. </author> <title> An inequality and associated maximization technique in statistical estimation of probabilistic func tions of a Markov process. Inequalities, </title> <type> 3, </type> <year> 1972, </year> <pages> pp 1-8. </pages>
Reference-contexts: Nevertheless, we can frequently apply an iterative technique called the Expectation-Maximization or EM Algorithm; this is a recipe for computing a sequence 1 ; 2 ; : : : of parameter vectors. It can be shown <ref> [2] </ref> that under suitable conditions, each iteration of the algorithm is guaranteed to produce a better model of the training vector c; that is, Pr i+1 (c) Pr i (c); (2) with strict inequality everywhere except at stationary points of Pr (c).
Reference: 3. <author> Brown, Peter F., Stephen A. Della Pietra, Vincent J. Della Pietra, Robert L. Mercer. </author> <title> The mathematics of statistical machine translation: parameter estimation. </title> <booktitle> Computational Linguistics 19(2), </booktitle> <month> June </month> <year> 1993, </year> <pages> pp 263 311. </pages>
Reference-contexts: During decoding, we simply use Pr (f; ^a j e). 4.2. EM-Trained Models We now sketch the structure of five models of increasing com plexity, the last of which is our EM-trained translation model. For an in-depth treatment, the reader is referred to <ref> [3] </ref>. 1. Word Translation This is our simplest model, intended to discover probable individual-word translations. The free parameters of this model are word translation probabilities t (f j j e i ). Each of these parameters is initialized to 1=jF j, where F is our French vocabulary. <p> Each word f in our French vocabulary F is placed in one of approximately 50 classes; likewise for each e in the English vocabulary E. The assignment of words to classes is made automatically through another statistical training procedure <ref> [3] </ref>. 5. Non-Deficient Alignment The preceding two models suffer from a problem we call deficiency: they assign non-zero probability to "alignments" that do not correspond to strings of French words at all. For instance, two French words may be assigned to lie at the same position in the sentence.
Reference: 4. <author> Jaynes, E. T. </author> <title> Notes on present status and future prospects. Maximum Entropy and Bayesian Methods, </title> <editor> W. T. Grandy and L. H. Schick, eds. </editor> <publisher> Kluwer Academic Press, </publisher> <year> 1990, </year> <pages> pp 1-13. </pages>
Reference-contexts: Since this pro cedure is costly in computer time, we develop such models only for the 2,000 most common English words. For more information about maximum-entropy modeling, the reader is referred to <ref> [4] </ref>. 5. Analysis-Transfer-Synthesis Although we try to obtain accurate estimates of the parame ters of our translation models by training on a large amount of text, this data is not used as effectively as it might be.
Reference: 5. <author> Jelinek, Frederick. </author> <title> A fast sequential decoding algorithm using a stack. </title> <journal> IBM Journal of Research and Develop ment, </journal> <volume> 13, </volume> <month> November </month> <year> 1969, </year> <pages> pp 675-685. </pages>
Reference-contexts: Even if we restricted ourselves to word strings of length k or less, for any realistic length and English vocabulary E , this is far too large a set to search exhaustively. Instead we adapt the well-known stack decoding algorithm <ref> [5] </ref> of speech recognition. Though we will say more about decoding in Section 6 below, most of our research effort has been devoted to the two modeling problems. This is not without reason. The translation scheme we have just described can fail in only two ways.
Reference: 6. <author> Jelinek, F., R. L. Mercer. </author> <title> Interpolated estimation of Markov source parameters from sparse data. </title> <booktitle> In Proceed ings, Workshop on Pattern Recognition in Practice, </booktitle> <address> Am sterdam, The Netherlands, </address> <year> 1980. </year>
Reference-contexts: There are 75,349,888 dis tinct trigrams in our training corpus, of which 53,737,350 occur exactly once. For this reason, we employ the technique of deleted interpola tion <ref> [6] </ref>: we express Pr (e k je k2 e k1 ) as a linear combination of the trigram probability T (e k je k2 e k1 ), the bigram prob ability B (e k je k1 ), the unigram probability U (e k ), and the uniform probability 1=jE j.
Reference: 7. <author> Lafferty, John, Daniel Sleator, Davy Temperly. </author> <title> Gram matical trigrams: a probabilistic model of link gram mar. </title> <booktitle> Proceedings of the 1992 AAAI Fall Symposium on Probabilistic Approaches to Natural Language. </booktitle>
Reference-contexts: This is a train able, probabilistic grammar that attempts to capture all the information present in the trigram model, and also to make the long-range connections among words needed to advance beyond it. Link grammars are discussed in detail in <ref> [7] </ref>. 4. Translation Modeling This section describes the elements of our translation model, Pr (f j e). We have two distinct translation models, both de scribed here: an EM-trained model, and a maximum-entropy model.
Reference: 8. <author> Merialdo, Bernard. </author> <title> Tagging text with a probabilistic model. </title> <booktitle> Proceedings of the IBM Natural Language ITL, </booktitle> <address> Paris, France, </address> <year> 1990, </year> <pages> pp 161-172. </pages>
Reference-contexts: During case and spelling correction, we correct any obvi ous spelling errors, and suppress the case variations in word spellings that arise from the conventions of English and French typography. During morphological analysis, we first use a hidden Markov model <ref> [8] </ref> to assign part-of-speech labels to the French, then use these labels to replace inflected verb forms with their in finitives, preceded by an appropriate tense marker. We also put nouns into singular form and precede them by number markers, and perform a variety of other morphological trans formations.
Reference: 9. <author> White, John S., Theresa A. O'Connell, Lynn M. Carl son. </author> <title> Evaluation of machine translation. In Human Lan guage Technology, </title> <publisher> Morgan Kaufman Publishers, </publisher> <year> 1993, </year> <pages> pp 206-210. </pages> <booktitle> Reprinted from Proceedings of the 1994 ARPA Workshop on Human Language Technology 6 </booktitle>
Reference-contexts: The final stage, synthesis, converts the intermediate English ^e 0 into a plain English sentence ^e. 7. Performance We evaluate our system in two ways: through participation in the arpa evaluations, and through our own internal tests. The arpa evaluation methodology, devised and executed by prc, is detailed in <ref> [9] </ref>; we recount it here briefly. arpa pro vides us with a set of French passages, which we process in two ways. First, the passages are translated without any hu man intervention. This is the fully-automatic mode.
References-found: 9

