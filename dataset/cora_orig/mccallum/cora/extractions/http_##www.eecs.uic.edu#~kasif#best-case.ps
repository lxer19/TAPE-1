URL: http://www.eecs.uic.edu/~kasif/best-case.ps
Refering-URL: http://www.eecs.uic.edu/~kasif/learn-research.html
Root-URL: 
Author: Salzberg, Arthur Delcher, David Heath, and Simon Kasif 
Keyword: machine learning, nearest-neighbor, geometric concepts.  
Date: 6, JUNE 1995 599  
Note: IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 17, NO.  Steven  
Abstract: Best-Case Results for Nearest Neighbor Abstract| In this paper we propose a theoretical model for analysis of classification methods, in which the teacher knows the classification algorithm and chooses examples in the best way possible. We apply this model using the nearest-neighbor learning algorithm, and develop upper and lower bounds on sample complexity for several different concept classes. For some concept classes, the sample complexity turns out to be exponential even using this best-case model, which implies that the concept class is inherently difficult for the nearest-neighbor algorithm. We identify several geometric properties that make learning certain concepts relatively easy. Finally we discuss the relation of our work to helpful teacher models, its application to decision-tree learning algorithms, and some of its implications for current experimental work. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> B.V. Dasarathy, </author> <title> Nearest Neighbor (NN) Norms: NN Pattern Classification Techniques, </title> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> California, </address> <year> 1991. </year>
Reference: [2] <author> D. Aha, D. Kibler, and M. Albert, </author> <title> "Instance-based learning algorithms", </title> <journal> Machine Learning, </journal> <volume> vol. 6, no. 1, </volume> <year> 1991. </year>
Reference-contexts: Experimental studies have shown that the predictive accuracy of nearest-neighbor algorithms is comparable to that of decision trees, rule learning systems, and neural net learning algorithms on many practical tasks (see, e.g., <ref> [2] </ref>, [13], among many others). In addition, it has long been known that the probability of error of the nearest neighbor rule is bounded above by twice the (optimal) Bayes probability of error [5]. The basic nearest-neighbor algorithm (henceforth NN) can be summarized as follows.
Reference: [3] <author> S. Salzberg, </author> <title> Learning with Nested Generalized Exemplars, </title> <publisher> Kluwer Academic Publishers, Norwell, </publisher> <address> MA, </address> <year> 1990. </year>
Reference: [4] <author> S. Cost and S. Salzberg, </author> <title> "A weighted nearest neighbor algorithm for learning with symbolic features", </title> <journal> Machine Learning, </journal> <volume> vol. 10, no. 1, </volume> <pages> pp. 57-78, </pages> <year> 1993. </year>
Reference: [5] <author> T. Cover and P. Hart, </author> <title> "Nearest neighbor pattern classification", </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> vol. 13, </volume> <pages> pp. 21-27, </pages> <year> 1967. </year>
Reference-contexts: III. Nearest-neighbor algorithms The nearest-neighbor algorithm has been a subject of both experimental and theoretical studies for many years (see, e.g., [11], <ref> [5] </ref>, [6], [12]). Experimental studies have shown that the predictive accuracy of nearest-neighbor algorithms is comparable to that of decision trees, rule learning systems, and neural net learning algorithms on many practical tasks (see, e.g., [2], [13], among many others). <p> In addition, it has long been known that the probability of error of the nearest neighbor rule is bounded above by twice the (optimal) Bayes probability of error <ref> [5] </ref>. The basic nearest-neighbor algorithm (henceforth NN) can be summarized as follows. An example is a vector of real numbers plus a label that represents the name of a category.
Reference: [6] <author> P.A. Devijver, </author> <title> "An overview of asymptotic properties of nearest neighbor rules", </title> <booktitle> in Pattern Recognition in Practice. </booktitle> <year> 1980, </year> <pages> pp. 343-350, </pages> <publisher> Elsevier Science Publishers B.V. </publisher>
Reference-contexts: III. Nearest-neighbor algorithms The nearest-neighbor algorithm has been a subject of both experimental and theoretical studies for many years (see, e.g., [11], [5], <ref> [6] </ref>, [12]). Experimental studies have shown that the predictive accuracy of nearest-neighbor algorithms is comparable to that of decision trees, rule learning systems, and neural net learning algorithms on many practical tasks (see, e.g., [2], [13], among many others).
Reference: [7] <author> T. </author> <title> Cover, "Geometric and statistical properties of systems of linear inequalities", </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 14, </volume> <pages> pp. 326-334, </pages> <year> 1965. </year>
Reference: [8] <author> M. Minsky and S. Papert, </author> <title> Perceptrons, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1969. </year>
Reference: [9] <author> S. Salzberg, A. Delcher, D. Heath, and S. Kasif, </author> <title> "Learning with a helpful teacher", </title> <type> Tech. Rep. 90/14, </type> <institution> Dept. of Computer Science, Johns Hopkins University, </institution> <note> 1990 (revised 1992). </note>
Reference-contexts: We begin with some simple observations about best-case results for hyperplanes and other geometric objects. These observations are presented merely to illustrate the type of results we will be presenting in the next section. More details on these and additional observations can be found in Salzberg et al. <ref> [9] </ref>, [10]. Observation 1: Under the best-case learning model, exactly two examples are required by algorithm NN to learn a concept defined by a hyperplane in any number of dimensions. <p> More details can be found in <ref> [9] </ref>. Note also that, in any number of dimensions, any point that is classified incorrectly using this technique is within a very small distance| no greater than the diagonal length of a grid cell|from an edge of the original tesselation. <p> This is best illustrated with a brief example using a decision tree algorithm. (More details can be found in <ref> [9] </ref>.) Consider Figure 8. The concept here is a two-category problem, where the categories are labelled A and B in the figure. Category A occurs in two disjoint regions of the 2-dimensional plane, in the upper left and lower right quadrants.
Reference: [10] <author> S. Salzberg, A. Delcher, D. Heath, and S. Kasif, </author> <title> "Learning with a helpful teacher", </title> <booktitle> in Proc. of the Twelfth International Joint Conference on Artificial Intelligence, </booktitle> <address> Sydney, Australia, </address> <month> August </month> <year> 1991, </year> <pages> pp. 705-711, </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: We begin with some simple observations about best-case results for hyperplanes and other geometric objects. These observations are presented merely to illustrate the type of results we will be presenting in the next section. More details on these and additional observations can be found in Salzberg et al. [9], <ref> [10] </ref>. Observation 1: Under the best-case learning model, exactly two examples are required by algorithm NN to learn a concept defined by a hyperplane in any number of dimensions.
Reference: [11] <author> E. F. Fix and J. Hodges, </author> <title> "Discriminatory analysis: Small sample performance", </title> <type> Tech. Rep. </type> <institution> Project 21-49-004, </institution> <type> Report No. 11, </type> <institution> USAF School of Aviation Medicine, Randolph Field, Texas, </institution> <month> August </month> <year> 1952. </year>
Reference-contexts: III. Nearest-neighbor algorithms The nearest-neighbor algorithm has been a subject of both experimental and theoretical studies for many years (see, e.g., <ref> [11] </ref>, [5], [6], [12]). Experimental studies have shown that the predictive accuracy of nearest-neighbor algorithms is comparable to that of decision trees, rule learning systems, and neural net learning algorithms on many practical tasks (see, e.g., [2], [13], among many others).
Reference: [12] <author> L. Devroye, </author> <title> "Automatic pattern recognition : A study of the probability of error", </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> vol. 10, no. 4, </volume> <pages> pp. 530-543, </pages> <year> 1988. </year>
Reference-contexts: III. Nearest-neighbor algorithms The nearest-neighbor algorithm has been a subject of both experimental and theoretical studies for many years (see, e.g., [11], [5], [6], <ref> [12] </ref>). Experimental studies have shown that the predictive accuracy of nearest-neighbor algorithms is comparable to that of decision trees, rule learning systems, and neural net learning algorithms on many practical tasks (see, e.g., [2], [13], among many others).
Reference: [13] <author> S. Salzberg, </author> <title> "A nearest hyperrectangle learning method", </title> <journal> Machine Learning, </journal> <volume> vol. 6, </volume> <pages> pp. 251-276, </pages> <year> 1991. </year>
Reference-contexts: Experimental studies have shown that the predictive accuracy of nearest-neighbor algorithms is comparable to that of decision trees, rule learning systems, and neural net learning algorithms on many practical tasks (see, e.g., [2], <ref> [13] </ref>, among many others). In addition, it has long been known that the probability of error of the nearest neighbor rule is bounded above by twice the (optimal) Bayes probability of error [5]. The basic nearest-neighbor algorithm (henceforth NN) can be summarized as follows.
Reference: [14] <author> D. Angluin, </author> <title> "Learning regular sets from queries and counterexamples", </title> <journal> Information and Computation, </journal> <volume> vol. 75, </volume> <pages> pp. 87-106, </pages> <year> 1987. </year>
Reference-contexts: Because the nearest-neighbor algorithm does not depend on the order of the inputs, the teacher is saved from having to calculate the optimal order in which to present a set of training examples. A. Related learning models In the theoretical computer science community, An-gluin <ref> [14] </ref> defined a different model of a helpful teacher, in which the learner is allowed to ask questions such as "is x a positive example?" She has produced a variety of results for learning regular expressions and context-free grammars.
Reference: [15] <author> S. Goldman and M. Kearns, </author> <title> "On the complexity of teaching", </title> <booktitle> in Proc. of the Fourth Annual Workshop on Computational Learning Theory, </booktitle> <address> Santa Cruz, CA, </address> <month> August </month> <year> 1991, </year> <pages> pp. 303-314, </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Our studies differ substantially from Angluin's work, since our teacher has more knowledge of the learner and much more control over the set of training examples. A more closely-related model is the one recently developed by Goldman and Kearns <ref> [15] </ref>. In their model, the teacher presents examples to the learner, who in turn is required to output its predictions on-line. Their teacher does not know the learning algorithm (unlike our model), but it does know the concept, and it is allowed to observe the predictions made by the learner.
Reference: [16] <author> K. Romanik and S. Salzberg, </author> <title> "Testing orthogonal shapes", </title> <booktitle> in Proc. of the 1992 Canadian Conference on Computational Geometry, </booktitle> <address> St. </address> <institution> Johns, Newfoundland, Canada, </institution> <month> August </month> <year> 1992, </year> <pages> pp. 216-222. </pages>
Reference-contexts: The goal of the teacher is to select examples so that the learner will produce a correct concept while making a minimal number of on-line mistakes. Our model is also related to the notion of "testability" introduced by Romanik <ref> [16] </ref>.
Reference: [17] <author> P. Hart, </author> <title> "The condensed nearest neighbor rule", </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> vol. 14, no. 3, </volume> <month> May </month> <year> 1968. </year>
Reference: [18] <author> C. Swonger, </author> <title> "Sample set condensation for a condensed nearest neighbor decision rule for pattern recognition", in Frontiers of Pattern Recognition, </title> <editor> S. Watanabe, </editor> <publisher> Ed., </publisher> <pages> pp. 511-519. </pages> <publisher> Academic Press, </publisher> <year> 1972. </year>
Reference-contexts: Nearest-neighbor editing was introduced by Hart [17],and modified and extended by Swonger <ref> [18] </ref>, Wilson [19], and Chang [20]. Ritter et al. [21] were the first to give an algorithm that finds a consistent subset of minimum size. Toussaint et al. [22] introduced the notion of using Voronoi boundaries for editing.
Reference: [19] <author> D. Wilson, </author> <title> "Asymptotic properties of nearest neighbor rules using edited data", </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> vol. 2, no. 3, </volume> <pages> pp. 408-421, </pages> <month> July </month> <year> 1972. </year>
Reference-contexts: Nearest-neighbor editing was introduced by Hart [17],and modified and extended by Swonger [18], Wilson <ref> [19] </ref>, and Chang [20]. Ritter et al. [21] were the first to give an algorithm that finds a consistent subset of minimum size. Toussaint et al. [22] introduced the notion of using Voronoi boundaries for editing.
Reference: [20] <author> C.-L. Chang, </author> <title> "Finding prototypes for nearest neighbor classifiers", </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 23, no. 11, </volume> <pages> pp. 1179-1184, </pages> <month> November </month> <year> 1974. </year>
Reference-contexts: Nearest-neighbor editing was introduced by Hart [17],and modified and extended by Swonger [18], Wilson [19], and Chang <ref> [20] </ref>. Ritter et al. [21] were the first to give an algorithm that finds a consistent subset of minimum size. Toussaint et al. [22] introduced the notion of using Voronoi boundaries for editing.
Reference: [21] <author> G. Ritter, H. Woodruff, S. Lowry, and T. Isenhour, </author> <title> "An algorithm for a selective nearest neighbor decision rule", </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> vol. 21, no. 6, </volume> <pages> pp. 665-669, </pages> <year> 1975. </year>
Reference-contexts: Nearest-neighbor editing was introduced by Hart [17],and modified and extended by Swonger [18], Wilson [19], and Chang [20]. Ritter et al. <ref> [21] </ref> were the first to give an algorithm that finds a consistent subset of minimum size. Toussaint et al. [22] introduced the notion of using Voronoi boundaries for editing. Later they showed that when the points of S are in general position, their method finds a minimal-size consistent subset [23].
Reference: [22] <author> G. Toussaint, B. Bhattacharya, and R. Poulsen, </author> <title> "The application of voronoi diagrams to nonparametric decision rules", </title> <booktitle> in Computer Science and Statistics: Proc. of the 16th Symposium on the Interface, </booktitle> <editor> L. Billard, Ed., </editor> <address> New York, </address> <year> 1984, </year> <pages> pp. 97-108, </pages> <publisher> Elsevier Science Publishers. </publisher>
Reference-contexts: Nearest-neighbor editing was introduced by Hart [17],and modified and extended by Swonger [18], Wilson [19], and Chang [20]. Ritter et al. [21] were the first to give an algorithm that finds a consistent subset of minimum size. Toussaint et al. <ref> [22] </ref> introduced the notion of using Voronoi boundaries for editing. Later they showed that when the points of S are in general position, their method finds a minimal-size consistent subset [23]. IV. Learning geometric concepts A.
Reference: [23] <author> B. Bhattacharya, R. Poulsen, and G. Toussaint, </author> <title> "Application of proximity graphs to editing nearest neighbor decision rules", </title> <type> Tech. Rep. </type> <institution> SOCS 92.19, School of Computer Science, McGill University, Montreal, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: Toussaint et al. [22] introduced the notion of using Voronoi boundaries for editing. Later they showed that when the points of S are in general position, their method finds a minimal-size consistent subset <ref> [23] </ref>. IV. Learning geometric concepts A. Learning simple convex concepts One of the simplest geometric concepts is a simple line or hyperplane, which translates into the learning problem of classifying examples from exactly two categories that are separated by a line, plane, or hyperplane.
Reference: [24] <author> F. P. Preparata and M. I. Shamos, </author> <title> Computational Geometry: An Introduction, </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: point p has two or more nearest neighbors in S, then p is in the Voronoi diagram of S. (Distance can be measured in different ways, but Euclidean distance is the most common method.) Thus, the partitioning of space induced by the NN algorithm is, by definition, a Voronoi diagram <ref> [24] </ref>. Observation 3: Suppose we are learning a concept with a triangular boundary. Suppose also that the triangle is not obtuse; i.e., no angle is greater than 90 ffi . Then exactly nine points are sufficient for Algorithm NN to learn the triangle, using the following strategy.
Reference: [25] <author> B. Baker, E. Grosse, and C. Rafferty, </author> <title> "Nonobtuse triangulation of polygons", </title> <journal> Discrete Computational Geometry, </journal> <volume> vol. 3, </volume> <pages> pp. 147-168, </pages> <year> 1988. </year>
Reference-contexts: It turns out that this triangulation problem is quite hard, even in 2-D, and our best result for learning non-convex polygons uses another method, described in Section IV-E. The problem of triangulating polygons with nonobtuse triangles has until recently been an open problem in computational geometry. Baker et al. <ref> [25] </ref> produced the first solution, which used a relatively large number of triangles (the precise bound was not given).
Reference: [26] <author> M. Bern and D. Eppstein, </author> <title> "Polynomial-size nonobtuse triangulation of polygons", </title> <booktitle> in Proc. of the 7th Annual Symposium on Computational Geometry, </booktitle> <address> New York, </address> <year> 1991, </year> <pages> pp. 342-350. </pages>
Reference-contexts: The problem of triangulating polygons with nonobtuse triangles has until recently been an open problem in computational geometry. Baker et al. [25] produced the first solution, which used a relatively large number of triangles (the precise bound was not given). Bern and Eppstein <ref> [26] </ref> reported a solution in two dimensions that uses O (n 2 ) triangles for an n-sided polygon, and later Bern et al. [27] improved this result to O (n) nonobtuse triangles.
Reference: [27] <author> M. Bern, S. Mitchell, and J. Ruppert, </author> <title> "Linear-size nonobtuse triangulation of polygons", </title> <booktitle> in Proc. 10th Annual ACM Symp. on Computational Geometry, </booktitle> <address> Stony Brook, New York, </address> <year> 1994, </year> <pages> pp. 221-230. </pages>
Reference-contexts: Baker et al. [25] produced the first solution, which used a relatively large number of triangles (the precise bound was not given). Bern and Eppstein [26] reported a solution in two dimensions that uses O (n 2 ) triangles for an n-sided polygon, and later Bern et al. <ref> [27] </ref> improved this result to O (n) nonobtuse triangles. Their method makes some progress towards solving our best-case learning problem, although a complete solution requires tri SALZBERG ET AL.: BEST-CASE RESULTS FOR NEAREST NEIGHBOR LEARNING 603 Fig. 2. <p> It is worth noting that this lower bound also implies a similar lower bound on the number of nonobtuse triangles required for triangulating the shape formed by a polygon plus its convex hull. Although the interior of a non-convex polygon can be triangulated using just O (n) nonobtuse triangles <ref> [27] </ref>, the interior plus exterior will require (n 2 ) nonobtuse triangles for some polygons. The theorems for exact learning of rectilinear objects both extend quite straightforwardly into d dimensions.
Reference: [28] <author> D. Heath, </author> <title> A Geometric Framework for Machine Learning, </title> <type> PhD thesis, </type> <institution> Johns Hopkins University, </institution> <year> 1992. </year>
Reference-contexts: Because nearest-neighbor partitions are by definition Voronoi diagrams, the problem we would like to solve is equivalent to the following Minimum Voronoi Cover problem: Given a planar tesselation, what is the smallest Voronoi diagram that contains that tesselation as a sub graph? Heath <ref> [28] </ref> showed that for arbitrary planar polyg onal tesselations this problem is NP-complete. E.1 Triangulations of the plane We now turn to the class of triangulations. A triangulation is a tesselation of some region into triangles, where the boundary of the triangulation may be any simple polygon.
Reference: [29] <author> M. Bern, H. Edelsbrunner, D. Eppstein, S. Mitchell, and S. Tan, </author> <title> "Edge insertion for optimal triangulations", </title> <booktitle> in LATIN '92: 1st Latin American Symposium on Theoretical Informatics, </booktitle> <editor> I. Si-mon, Ed., </editor> <address> Berlin, </address> <year> 1992, </year> <pages> pp. 46-60, </pages> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Note that if all triangles are nonobtuse, the strategy of Observation 4 above is much more efficient than the one just described. To learn a polygon-shaped concept using this method, one would first triangulate the polygon in such a way as to maximize h. Bern et al. <ref> [29] </ref> have developed an algorithm that can construct a triangulation of a polygon in O (n 2 log n) time that maximizes the minimum triangle al titude. Although this result does not give a lower bound for h, it does provide the maximum possible value for h.

References-found: 29

