URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/jjlien/www/FG98_23.ps.gz
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/jjlien/www/index.html
Root-URL: 
Email: jjlien@cs.cmu.edu  jeffcohn@vms.cis.pitt.edu  tk@cs.cmu.edu  ccl@vms.cis.pitt.edu  
Title: Automated Facial Expression Recognition Based on FACS Action Units  
Author: , James J. Lien Jeffrey F. Cohn Takeo Kanade Ching-Chung Li 
Address: Pittsburgh Pittsburgh, PA 15260  Pittsburgh  Pittsburgh, PA 15213  Pittsburgh  
Affiliation: 1 Department of Electrical Engineering University of  Department of Psychology University of  Vision and Autonomous Systems Center The Robotics Institute Carnegie Mellon University  Department of Electrical Engineering University of  
Abstract: Automated recognition of facial expression is an important addition to computer vision research because of its relevance to the study of psychological phenomena and the development of human-computer interaction (HCI). We developed a computer vision system that automatically recognizes individual action units or action unit combinations in the upper face using Hidden Markov Models (HMMs). Our approach to facial expression recognition is based on the Facial Action Coding System (FACS), which separates expressions into upper and lower face action. In this paper, we use three approaches to extract facial expression information: (1) facial feature point tracking, (2) dense flow tracking with principal component analysis (PCA), and (3) high gradient component detection (i.e., furrow detection). The recognition results of the upper face expressions using feature point tracking, dense flow tracking, and high gradient component detection are 85%, 93%, and 85%, respectively. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M.S. Bartlett, P.A. Viola, T.J. Sejnowski, B.A. Golomb, J. Larsen, J.C. Hager and P. Ekman, </author> <title> "Classifying Facial Action," </title> <booktitle> Advances in Neural Information Processing Systems 8, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1996. </year>
Reference-contexts: Principal component analysis (PCA) has excellent properties for our purposes, including image data compression and maintenance of a strong correlation between two consecutive motion frames. Since our goal is to recognize expression rather than identifying individuals or objects <ref> [1, 7, 13, 18] </ref>, we analyze facial motion using optical flow not the gray value to ignore differences across individual subjects. Before using PCA, we need to ensure that the pixel-wise flows of each frame have relative geometric correspondence.
Reference: [2] <author> J.N. Bassili, </author> <title> Emotion Recognition: The Role of Facial Movement and the Relative Importance of Upper and Lower Areas of the Face, </title> <journal> Journal of Personality and Social Psychology, </journal> <volume> Vol. 37, </volume> <pages> pp. 2049-2059, </pages> <year> 1979. </year>
Reference-contexts: The expressions are recognized in the context of the entire image sequence since analysis of a dynamic image produces more accurate and robust recognition of facial expression than that of a single static image <ref> [2] </ref>. Hidden Markov Models (HMMs) [16] are used for facial expression recognition because they perform well in the spatio-temporal domain and are analogous to human performance (e.g., for speech [16] and gesture recognition [21]). We use the Facial Action Coding System (FACS) [5] to identify facial action.
Reference: [3] <author> M.J. Black and Y. Yacoob, </author> <title> "Recognizing Facial Expressions under Rigid and Non-Rigid Facial Motions," </title> <booktitle> International Workshop on Automatic Face and Gesture Recognition, </booktitle> <address> Zurich, </address> <pages> pp. 12-17, </pages> <year> 1995. </year>
Reference-contexts: Other systems use averaged optical flow within local regions (e.g., forehead, eyes, nose, mouth, cheek, and chin) for recognition. In an individual region, the flow direction is changed to conform to the flow plurality of the region <ref> [3, 15, 20] </ref> or averaged over an entire region [11, 12]. Black and colleagues [3, 4] also assign parameter thresholds to their classification paradigm. These methods are relatively insensitive to subtle motion because information about small deviations is lost when their flow pattern is removed or thresholds are imposed. <p> In an individual region, the flow direction is changed to conform to the flow plurality of the region [3, 15, 20] or averaged over an entire region [11, 12]. Black and colleagues <ref> [3, 4] </ref> also assign parameter thresholds to their classification paradigm. These methods are relatively insensitive to subtle motion because information about small deviations is lost when their flow pattern is removed or thresholds are imposed. As a result, Copyright 1998 IEEE. <p> As a result, Copyright 1998 IEEE. Published in the Proceedings of FG98, April 14-16, 1998 in Nara, Japan. the recognition ability and accuracy of the systems may be reduced. Current recognition systems <ref> [3, 15, 20] </ref> analyze six prototypic expressions (joy, fear, anger, disgust, sadness and surprise) and classify them into emotion categories, rather than facial action.
Reference: [4] <author> M.J. Black, Y. Yacoob, A.D. Jepson, </author> <title> and D.J. Fleet, "Learning Parameterized Models of Image Motion," </title> <journal> Computer Vision and Pattern Recognition, </journal> <year> 1997. </year>
Reference-contexts: In an individual region, the flow direction is changed to conform to the flow plurality of the region [3, 15, 20] or averaged over an entire region [11, 12]. Black and colleagues <ref> [3, 4] </ref> also assign parameter thresholds to their classification paradigm. These methods are relatively insensitive to subtle motion because information about small deviations is lost when their flow pattern is removed or thresholds are imposed. As a result, Copyright 1998 IEEE.
Reference: [5] <author> P. Ekman and W.V. Friesen, </author> <title> "The Facial Action Coding System," </title> <publisher> Consulting Psychologists Press, Inc., </publisher> <address> San Francisco, CA, </address> <year> 1978. </year>
Reference-contexts: Hidden Markov Models (HMMs) [16] are used for facial expression recognition because they perform well in the spatio-temporal domain and are analogous to human performance (e.g., for speech [16] and gesture recognition [21]). We use the Facial Action Coding System (FACS) <ref> [5] </ref> to identify facial action. FACS is an anatomically based coding system that enables discrimination between closely related expressions. FACS divides the face into upper and lower face action and further subdivides motion into action units (AUs). AUs are defined as visibly discriminable muscle movements that combine to produce expressions.
Reference: [6] <author> I.A. Essa, </author> <title> "Analysis, Interpretation and Synthesis of Facial Expressions," </title> <type> Perceptual Computing Technical Report 303, </type> <institution> MIT Media Laboratory, </institution> <month> February </month> <year> 1995. </year>
Reference-contexts: The ability to recognize and understand facial expression automatically may facilitate communication. Automated recognition of individual motion sequences is a challenging task. Currently, most facial expression recognition systems use either complicated three-dimensional wireframe face models to recognize and synthesize facial expressions <ref> [6, 17] </ref> or consider only averaged local motion. Using vision techniques, however, it is difficult to design a motion-based three-dimensional face model that accurately represents facial geometric properties. Also, the initial adjustment between the three-dimensional wireframe and the surface images is manual, which affects the accuracy of the recognition results.
Reference: [7] <author> M. Kirby and L. Sirovich, </author> <title> "Application of the Karhuneh-Loeve Procedure for the Characterization of Human Faces," </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> Vol. 12, No. 1, </volume> <month> January </month> <year> 1990. </year>
Reference-contexts: Principal component analysis (PCA) has excellent properties for our purposes, including image data compression and maintenance of a strong correlation between two consecutive motion frames. Since our goal is to recognize expression rather than identifying individuals or objects <ref> [1, 7, 13, 18] </ref>, we analyze facial motion using optical flow not the gray value to ignore differences across individual subjects. Before using PCA, we need to ensure that the pixel-wise flows of each frame have relative geometric correspondence.
Reference: [8] <author> J.J. Lien, T. Kanade, A.J. Zlochower, J.F. Cohn, and C.C. Li, </author> <title> "Automatically Recognizing Facial Expressions in the Spatia-Temporal Domain," </title> <booktitle> Perceptual User Interface Workshop, </booktitle> <pages> pp. 94-97, </pages> <address> Banff, Alberta, Canada, </address> <year> 1997. </year>
Reference-contexts: In reality, humans are capable of producing thousands of expressions varying in complexity and meaning that are not fully captured with a limited number of expressions and emotion categories. Our goal is to develop a system that robustly recognizes both subtle feature motion and complex facial expressions <ref> [8] </ref>. 2. System Structure Our system uses three approaches to recognize facial action (Figure 1). Two of the approaches use optical flow to track facial motion. The use of optical flow is optimized for our purposes because facial skin and features naturally have great deal of texture.
Reference: [9] <author> Y. Linde, A. Buzo, and R. Gray, </author> <title> "An Algorithm for Vector Quantizer Design," </title> <journal> IEEE Trans. on Communications, </journal> <volume> Vol. COM-28, NO. 1, </volume> <month> January </month> <year> 1980. </year>
Reference-contexts: Recognition Using Hidden Markov Models After separately vector quantizing <ref> [9] </ref> the 12-dimensional training displacement vectors from feature point tracking; the 20-dimensional training weighted vectors from the PCA, and the 26-dimensional training mean and variance vectors from the high gradient component detection, the corresponding facial expression HMM sets representing the upper face expressions are trained.
Reference: [10] <author> B.D. Lucas and T. Kanade, </author> <title> "An Iterative Image Registration Technique with an Application to Stereo Vision," </title> <booktitle> Proceedings of the 7th International Joint Conference on Artificial Intelligence, </booktitle> <year> 1981. </year>
Reference-contexts: The movement of facial feature points is tracked across an image sequence using Lucas-Kanades optical flow algorithm, which has previously been shown to have high tracking accuracy <ref> [10, 14] </ref>. A computer mouse is used to manually mark 8 facial feature points around the contours of both brows in the first frame of each image sequence (see Figure 3).
Reference: [11] <author> K. Mase and A. Pentland, </author> <title> "Automatic Lipreading by Optical-Flow Analysis," </title> <journal> Systems and Computers in Japan, </journal> <volume> Vol. 22, No. 6, </volume> <year> 1991. </year>
Reference-contexts: Other systems use averaged optical flow within local regions (e.g., forehead, eyes, nose, mouth, cheek, and chin) for recognition. In an individual region, the flow direction is changed to conform to the flow plurality of the region [3, 15, 20] or averaged over an entire region <ref> [11, 12] </ref>. Black and colleagues [3, 4] also assign parameter thresholds to their classification paradigm. These methods are relatively insensitive to subtle motion because information about small deviations is lost when their flow pattern is removed or thresholds are imposed. As a result, Copyright 1998 IEEE.
Reference: [12] <author> K. Mase, </author> <title> "Recognition of Facial Expression from Optical Flow," </title> <journal> IEICE Transactions, </journal> <volume> Vol. E74, </volume> <pages> pp. 3474-3483, </pages> <year> 1991. </year>
Reference-contexts: Other systems use averaged optical flow within local regions (e.g., forehead, eyes, nose, mouth, cheek, and chin) for recognition. In an individual region, the flow direction is changed to conform to the flow plurality of the region [3, 15, 20] or averaged over an entire region <ref> [11, 12] </ref>. Black and colleagues [3, 4] also assign parameter thresholds to their classification paradigm. These methods are relatively insensitive to subtle motion because information about small deviations is lost when their flow pattern is removed or thresholds are imposed. As a result, Copyright 1998 IEEE.
Reference: [13] <author> H. Murase and S.K. Nayar, </author> <title> "Visual Learning and Recognition of 3-D Objects from Appearance," </title> <journal> International Journal of Computer Vision, </journal> <volume> 14, </volume> <pages> pp. 5-24, </pages> <year> 1995. </year>
Reference-contexts: Principal component analysis (PCA) has excellent properties for our purposes, including image data compression and maintenance of a strong correlation between two consecutive motion frames. Since our goal is to recognize expression rather than identifying individuals or objects <ref> [1, 7, 13, 18] </ref>, we analyze facial motion using optical flow not the gray value to ignore differences across individual subjects. Before using PCA, we need to ensure that the pixel-wise flows of each frame have relative geometric correspondence.
Reference: [14] <author> C.J. Poelman, </author> <title> "The Paraperspective and Projective Factorization Methods for Recovering Shape and Motion," </title> <type> Technical Report CMU-CS-95-173, </type> <institution> Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: The movement of facial feature points is tracked across an image sequence using Lucas-Kanades optical flow algorithm, which has previously been shown to have high tracking accuracy <ref> [10, 14] </ref>. A computer mouse is used to manually mark 8 facial feature points around the contours of both brows in the first frame of each image sequence (see Figure 3).
Reference: [15] <author> M. Rosenblum, Y. Yacoob and L.S. Davis, </author> <title> "Human Emotion Recognition from Motion Using a Radial Basis Function Network Architecture," </title> <booktitle> Proceedings of the Workshop on Motion of Non-rigid and Articulated Objects, </booktitle> <address> Austin, TX, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: Other systems use averaged optical flow within local regions (e.g., forehead, eyes, nose, mouth, cheek, and chin) for recognition. In an individual region, the flow direction is changed to conform to the flow plurality of the region <ref> [3, 15, 20] </ref> or averaged over an entire region [11, 12]. Black and colleagues [3, 4] also assign parameter thresholds to their classification paradigm. These methods are relatively insensitive to subtle motion because information about small deviations is lost when their flow pattern is removed or thresholds are imposed. <p> As a result, Copyright 1998 IEEE. Published in the Proceedings of FG98, April 14-16, 1998 in Nara, Japan. the recognition ability and accuracy of the systems may be reduced. Current recognition systems <ref> [3, 15, 20] </ref> analyze six prototypic expressions (joy, fear, anger, disgust, sadness and surprise) and classify them into emotion categories, rather than facial action.
Reference: [16] <author> L.R. Rabiner, </author> <title> "An Introduction to Hidden Markov Models," </title> <journal> IEEE ASSP Magazine, </journal> <pages> pp. 4-16, </pages> <month> January </month> <year> 1986. </year>
Reference-contexts: The expressions are recognized in the context of the entire image sequence since analysis of a dynamic image produces more accurate and robust recognition of facial expression than that of a single static image [2]. Hidden Markov Models (HMMs) <ref> [16] </ref> are used for facial expression recognition because they perform well in the spatio-temporal domain and are analogous to human performance (e.g., for speech [16] and gesture recognition [21]). We use the Facial Action Coding System (FACS) [5] to identify facial action. <p> Hidden Markov Models (HMMs) <ref> [16] </ref> are used for facial expression recognition because they perform well in the spatio-temporal domain and are analogous to human performance (e.g., for speech [16] and gesture recognition [21]). We use the Facial Action Coding System (FACS) [5] to identify facial action. FACS is an anatomically based coding system that enables discrimination between closely related expressions. FACS divides the face into upper and lower face action and further subdivides motion into action units (AUs).
Reference: [17] <author> D. Terzopoulos and K. Waters, </author> <title> "Analysis of Facial Images Using Physical and Anatomical Models," </title> <booktitle> IEEE International Conference on Computer Vision, </booktitle> <pages> pp. 727-732, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: The ability to recognize and understand facial expression automatically may facilitate communication. Automated recognition of individual motion sequences is a challenging task. Currently, most facial expression recognition systems use either complicated three-dimensional wireframe face models to recognize and synthesize facial expressions <ref> [6, 17] </ref> or consider only averaged local motion. Using vision techniques, however, it is difficult to design a motion-based three-dimensional face model that accurately represents facial geometric properties. Also, the initial adjustment between the three-dimensional wireframe and the surface images is manual, which affects the accuracy of the recognition results.
Reference: [18] <author> M. Turk and A. Pentland, </author> <title> "Eigenfaces for Recognition," </title> <journal> Journal of Cognitive Neuroscience, </journal> <volume> Vol. 3, No. 1, </volume> <pages> pp. 71-86, </pages> <year> 1991. </year>
Reference-contexts: Principal component analysis (PCA) has excellent properties for our purposes, including image data compression and maintenance of a strong correlation between two consecutive motion frames. Since our goal is to recognize expression rather than identifying individuals or objects <ref> [1, 7, 13, 18] </ref>, we analyze facial motion using optical flow not the gray value to ignore differences across individual subjects. Before using PCA, we need to ensure that the pixel-wise flows of each frame have relative geometric correspondence.
Reference: [19] <author> Y.T. Wu, T. Kanade, J. F. Cohn, and C.C. Li, </author> <title> Optical Flow Estimation Using Wavelet Motion Model, </title> <address> ICCV, </address> <year> 1998. </year>
Reference-contexts: To include more detailed and robust motion information from larger regions of the face, we use Wu's dense flow algorithm <ref> [19] </ref> to track each pixel of the entire face image (Figure 5).
Reference: [20] <author> J. Yacoob and L. Davis, </author> <title> "Computing Spatio-Temporal Representations of Human Faces," </title> <booktitle> In Proc. Computer Vision and Pattern Recognition, CVPR-94, </booktitle> <pages> pp. 70-75, </pages> <address> Seattle, WA, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: Other systems use averaged optical flow within local regions (e.g., forehead, eyes, nose, mouth, cheek, and chin) for recognition. In an individual region, the flow direction is changed to conform to the flow plurality of the region <ref> [3, 15, 20] </ref> or averaged over an entire region [11, 12]. Black and colleagues [3, 4] also assign parameter thresholds to their classification paradigm. These methods are relatively insensitive to subtle motion because information about small deviations is lost when their flow pattern is removed or thresholds are imposed. <p> As a result, Copyright 1998 IEEE. Published in the Proceedings of FG98, April 14-16, 1998 in Nara, Japan. the recognition ability and accuracy of the systems may be reduced. Current recognition systems <ref> [3, 15, 20] </ref> analyze six prototypic expressions (joy, fear, anger, disgust, sadness and surprise) and classify them into emotion categories, rather than facial action.
Reference: [21] <author> J. Yang, </author> <title> "Hidden Markov Model for Human Performance Modeling," </title> <type> Ph.D. Dissertation, </type> <institution> University of Akron, </institution> <month> August </month> <year> 1994. </year>
Reference-contexts: Hidden Markov Models (HMMs) [16] are used for facial expression recognition because they perform well in the spatio-temporal domain and are analogous to human performance (e.g., for speech [16] and gesture recognition <ref> [21] </ref>). We use the Facial Action Coding System (FACS) [5] to identify facial action. FACS is an anatomically based coding system that enables discrimination between closely related expressions. FACS divides the face into upper and lower face action and further subdivides motion into action units (AUs).
References-found: 21

