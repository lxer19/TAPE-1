URL: http://www.eecs.umich.edu/PPP/ThesisAlex2.ps.gz
Refering-URL: http://www.eecs.umich.edu/PPP/publist.html
Root-URL: http://www.cs.umich.edu
Title: Modulo Scheduling, Machine Representations, and Register-Sensitive Algorithms  Modulo Scheduling, Machine Representations, and Register-Sensitive Algorithms  
Author: by Alexandre Edouard Eichenberger Dr. Santosh G. Abraham, Co-Chair Professor Edward S. Davidson, Co-Chair by Alexandre Edouard Eichenberger Co-Chairs: Santosh G. Abraham and Edward S. Davidson 
Degree: A dissertation submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy (Computer Science and Engineering) in The  Doctoral Committee:  Professor John R. Birge Professor John P. Hayes Professor Trevor N. Mudge Professor Yale N. Patt  
Note: ABSTRACT  
Date: 1997  
Affiliation: University of Michigan  
Abstract: High performance compilers increasingly rely on accurate modeling of the machine resources to efficiently exploit the instruction level parallelism of an application. In this dissertation, we first propose a reduced machine description that results in significantly faster detection of resource contentions while preserving the scheduling constraints present in the original machine description. This approach reduces a machine description in an automated, error-free, and efficient fashion. Moreover, it fully supports the elaborate scheduling techniques that are used by high-performance compilers, such as scheduling an operation earlier than others that are already scheduled, unscheduling operations due to resource conflicts, and efficient handling of periodic resource requirements found in software pipelined schedules. Reduced machine descriptions resulted in processing the queries to the resource model in 58.1% of the original time for a benchmark of 1327 loops scheduled by a state-of-the-art modulo scheduler for the Cydra 5 machine. Scheduling techniques such as Modulo Scheduling are also increasingly successful at efficiently exploiting the instruction level parallelism up to the resource limit of the machine, resulting in high performance but increased register requirements. In this dissertation, we propose an optimal register-sensitive algorithm for modulo 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. V. Aho, J. E. Hopcroft, and J. D. Ullman. </author> <title> The Design and Analysis of Computer Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1974. </year>
Reference-contexts: This heuristic has a linear computational complexity in the number of edges and results in valid stage schedules with no larger register requirements. First, the heuristic detects all cut edges, using a linear-time algorithm that enumerates the biconnected components of an undirected graph <ref> [1, pp. 179-187] </ref>, i.e. the underlying cycles of the dependence graph. Cut edges are then simply the edges linking vertices from distinct biconnected components. <p> disjoint Relations (5.12) and (5.16) by formulating the following constraint: k f k u + z 0 (5.18) Inequality (5.18) is equivalent to the union of Relations (5.12) and (5.16) because when k u &gt; k f , Inequality (5.18) holds regardless of the value of z since z 2 <ref> [0; 1] </ref> and when k u = k f , Inequality (5.18) holds precisely when z = 0. <p> However, by the assignment constraints, each of the summations in Inequality (5.22) is either 0 or 1. Thus the left hand side of Inequality (5.22) is in the range <ref> [2; 1] </ref>. Inequality (5.22) is thus trivially satisfied and introduces no new constraints if r &lt; row i .
Reference: [2] <author> A. V. Aho, R. Sethi, and J. D. Ullman. </author> <booktitle> Compilers: Principles, Techniques and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1988. </year>
Reference-contexts: However, by the assignment constraints, each of the summations in Inequality (5.22) is either 0 or 1. Thus the left hand side of Inequality (5.22) is in the range <ref> [2; 1] </ref>. Inequality (5.22) is thus trivially satisfied and introduces no new constraints if r &lt; row i . <p> Then, local analysis for each basic block determines the cycle-by-cycle live range of each virtual register that is live-in, live-out, and/or referenced in the basic block <ref> [2, pp 534-525] </ref>. This second step is highly machine dependent; we assume in our examples and experiments that a virtual register is reserved in the cycle where its earliest-define operation is scheduled and becomes free in the cycle following its last-use operation. <p> As a result, the register requirements of this predicated block increase from 2 to 3. This increase in register requirements is mainly due to two reasons. First, without knowledge about predicates, the dataflow analysis must make conservative assumptions about the side effects of the predicated operations <ref> [2] </ref>.
Reference: [3] <author> A. Aiken, A. Nicolau, and S. Novak. </author> <title> Resource-constrained software pipelining. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 6(12) </volume> <pages> 1248-1269, </pages> <month> De-cember </month> <year> 1995. </year>
Reference-contexts: Several alternative approaches to modulo scheduling have been proposed to address the above limitation, i.e. suboptimal performance due to a unique initiation interval per loop schedule. One approach, proposed by Aiken et al <ref> [3] </ref>, constructs a software pipeline schedule by scheduling operations of several consecutive iterations and testing for repeating states until a steady-state pattern is found along all paths. This approach handles loops with arbitrary control flow graphs, machines with finite resources, and operations with simple resource patterns.
Reference: [4] <author> J. R. Allen, K. Kennedy, C. Porterfield, and J. Warren. </author> <title> Conversion of control dependence to data dependence. </title> <booktitle> In Conference Record of the 10th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 177-189, </pages> <month> January </month> <year> 1983. </year>
Reference: [5] <author> V. H. Allen, U. R. Shah, and K. M. Reddy. </author> <title> Petri net versus modulo scheduling for software pipelining. </title> <booktitle> In Proceedings of the 28th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 105-110, </pages> <month> November </month> <year> 1995. </year>
Reference-contexts: Recently, optimizations based on unrolling a loop body 82 before applying a modulo scheduling algorithm have also been investigated [56]. Tech--niques based on Petri nets have also been proposed to determine a suitable degree of unrolling for modulo schedules <ref> [5] </ref>. Because high-performance schedules result in increased register requirements, research was also conducted on developing efficient register-sensitive modulo scheduling algorithms. Huff has investigated a heuristic based on a bidirectional slack-scheduling method that schedules operations early or late depending on their number of stretchable input and output flow dependencies [49].
Reference: [6] <author> E. R. Altman. </author> <title> Optimal Software Pipelining with Function Unit and Register Constraints. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering, McGill University, Montreal, Canada, </institution> <month> October </month> <year> 1995. </year>
Reference-contexts: Altman <ref> [6] </ref> has independently derived a model that minimizes the register requirements without requiring loop unrolling by jointly scheduling operations and allocating lifetimes to physical registers. This result is obtained by performing as many copy operations as necessary for lifetimes that span several stages. <p> After the strings are mapped, mapping the remaining resource usages is unrestricted and therefore trivial. Thus this machine satisfies the mapping-free property as well. Note that, since the definition of this third machine includes all machines with some mix of fully pipelined and non-pipelined execution units as defined in <ref> [6] </ref>, they constitute an important and common class of machines. Thus, for these three classes of machines and for any other machines with the mapping-free property, Inequality (5.2) insures that there are no resource conflicts and that a legal mapping to resource instances can be found.
Reference: [7] <author> E. R. Altman, R. Govindarajan, and G. R. Gao. </author> <title> Scheduling and mapping: Software pipelining in the presence of structural hazards. </title> <booktitle> In Proceedings of the ACM SIGPLAN'95 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 139-150, </pages> <year> 1995. </year>
Reference-contexts: Ning and Gao's results were extended for machines with finite resources and fully-pipelined functional units by Govindarajan et al [44] using an algorithm based on an integer linear programming formulation that directly minimizes the buffer requirements of a schedule. Altman et al <ref> [7] </ref> have independently developed a formulation for machines with complex resource requirements. Their formulation simultaneously enforces the finite resource constraints and performs the mapping of operations to specific functional units. For example [7], in a loop with 3 divide operations on a machine with 2 divide functional units, the mapping problem <p> Altman et al <ref> [7] </ref> have independently developed a formulation for machines with complex resource requirements. Their formulation simultaneously enforces the finite resource constraints and performs the mapping of operations to specific functional units. For example [7], in a loop with 3 divide operations on a machine with 2 divide functional units, the mapping problem determines at compile time which divide operation is executed on which divide functional unit. <p> A third way is to use the formulation recently proposed by Altman et al <ref> [7] </ref> which handles both the scheduling and the mapping problems simultaneously. 5.3.3 Simple Formulation of the Scheduling Constraints The third condition that a valid schedule must satisfy is that each scheduling dependence present in its dependence graph is enforced.
Reference: [8] <author> V. Bala. </author> <type> Personal communication. </type> <month> February </month> <year> 1996. </year>
Reference-contexts: Their approach also addresses the handling of basic block boundary conditions at the cost of introducing new states; recent experimental evidence gathered by Bala and Rubin shows, however, that no additional states are introduced for the Alpha, PA RISC, and MIPS families if minimal finite-state automata are constructed <ref> [8] </ref>. The principal advantage of automaton-based approaches is that a single table lookup can determine the next contention-free cycle. A potential problem of this approach is the size of these automata, an issue that is addressed in the literature in the following two ways.
Reference: [9] <author> V. Bala and N. Rubin. </author> <title> Efficient instruction scheduling using finite state automata. </title> <booktitle> Proceedings of the 28th Annual International Symposium on Microar-chitecture, </booktitle> <pages> pages 46-56, </pages> <month> November </month> <year> 1995. </year>
Reference-contexts: The technique proposed by Proebsting and Fraser directly results in minimal finite-state automata [76]. This approach was recently extended for unrestricted scheduling models by Bala and Rubin using a forward and reverse pair of automata <ref> [9] </ref>. In their approach, operations considered in order of monotonically increasing (or decreasing) schedule time are quickly scheduled using a forward automaton. Additional operations are then inserted in the schedule in cycles recognized as contention-free by the forward and reverse automata. <p> Supporting unrestricted scheduling models also requires the consistency of the stored state to be maintained when scheduling additional operations <ref> [9] </ref>, as inserted operations introduce additional resource requirements. Thus, handling unrestricted scheduling models introduces both memory and computation overhead that is similar to, or may exceed, the overhead incurred by the reservation table approach. <p> Table 2.3 shows the results of the reductions for the DEC Alpha 21064 [28] using the machine description presented by Bala and Rubin <ref> [9] </ref>. Comparing the original description to the specific reduction for a 64 bit word bitvector representation, the average word usage is decreased by a factor of 6.9.
Reference: [10] <author> R. A. Ballance, A. B. Maccabe, and K. J. Ottenstein. </author> <title> The program dependence web: A representation supporting control-, data-, and demand-driven interpretation of imperative languages. </title> <booktitle> Proceedings of the ACM SIGPLAN'90 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 257-271, </pages> <year> 1990. </year> <month> 259 </month>
Reference-contexts: This information is used to refine dataflow analysis, optimization, scheduling, and allocation in presence of hyperblocks. Additionally, this information is used to conditionally reserve functional units when modulo scheduling under the Reverse-IF 212 Conversion scheme [95]. A complementary approach is taken with the Gated Single Assignment (GSA) form <ref> [10] </ref>, where precise predicate information is embedded in the dataflow graph. This approach is used in the Polaris parallelizing compiler to refine data and memory dependence analysis and to aid loop parallelization [90].
Reference: [11] <author> G. R. Beck, D. W. L. Yen, and T. L. Anderson. </author> <title> The Cydra 5 mini-supercomputer: Architecture and implementation. </title> <journal> In The Journal of Supercomputing, </journal> <volume> volume 7, </volume> <pages> pages 143-180, </pages> <year> 1993. </year>
Reference-contexts: The major advantage of this approach over previous work [9][68][76] is that no restrictions are imposed on scheduling algorithms other than the need to satisfy the constraints of the machine itself. Reduced representations for the DEC Alpha 21064 [28], MIPS R3000/- R3010 [53], and Cydra 5 <ref> [11] </ref> indicate potentially 4.0 to 6.9 times faster contention queries, while requiring 22 to 67% of the memory storage used by the original machine descriptions. <p> Experiments with the DEC Alpha 21064 [28], MIPS R3000/R3010 [53], and Cy-dra 5 <ref> [11] </ref> machines indicate potentially 4.0 to 6.9 times faster contention queries, while requiring 22 to 67% of the memory storage used by the original machine descriptions. <p> Dynamic measurements obtained when scheduling 1327 loops from the Perfect Club [13], SPEC-89 [91], and the Livermore Fortran Kernels [65] for the Cy-dra 5 machine <ref> [11] </ref> indicate that the essential work performed by the contention queries decreases by a factor of 2.76 to 3.30, depending on the functionality required by the scheduler. This decrease in essential work results, in turn, in a 1.72 to 1.83 faster execution time of the contention queries. <p> A more detailed performance analysis of the contention query module for the Cydra 5 machine is presented in Section 2.7. As a proof of concept, we investigated our technique on the Cydra 5 machine <ref> [11] </ref> which has the most complex resource requirements of the three machines. The ma 29 chine configuration investigated here has 7 functional units: 2 memory port, 2 address generation, 1 FP adder, 1 FP multiplier, and 1 branch unit. <p> Other more efficient techniques could be implemented by, for example, factoring resource reservations that are used by all or a subset of the alternative operations and checking each factor at most once. The check-with-alt function, when compiling for the Cydra 5 <ref> [11] </ref>, is used to determine which of the two address generation units should be used, e.g. adding to an address register can be performed by either the a1add or the a2add operation. <p> We investigate the performance of the optimal stage scheduler, the stage-scheduling heuristics, and other schedulers for a benchmark suite of 1327 loops from the Perfect Club, SPEC-89, and the Livermore Fortran Kernels for a machine with complex resource usage, the Cydra 5 <ref> [11] </ref>. Our empirical findings show that the register requirements decrease by 19.8% on average when applying the optimal stage scheduler 87 to the MRT-schedules of the register-insensitive modulo scheduler employed in our experiments. <p> The contributions of this chapter are threefold. First, we con 161 tribute a formulation of the resource constraints for machines with finite resources and arbitrary reservation tables. This formulation enables us, for example, to precisely handle the resource requirements of the Cydra 5 <ref> [11] </ref>, a machine with complex resource requirements. Second, we contribute a precise formulation of the register requirements of a modulo schedule.
Reference: [12] <author> D. Bernstein and M. Rodeh. </author> <title> Global instruction scheduling for superscalar machines. </title> <booktitle> In Proceedings of the ACM SIGPLAN'91 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 241-255, </pages> <month> June </month> <year> 1991. </year>
Reference: [13] <author> M. Berry et al. </author> <title> The Perfect Club Benchmarks: Effective performance evaluation of supercomputers. </title> <journal> The International Journal of Supercomputer Applications, </journal> <volume> 3(3) </volume> <pages> 5-40, </pages> <month> Fall </month> <year> 1989. </year>
Reference-contexts: Efficient representations are important since high performance compilers spend a significant amount of compilation time scheduling operations, and thus testing for potential resource contentions. When a benchmark suite of 1327 loops from the Perfect Club <ref> [13] </ref>, SPEC-89 [91], and the Livermore Fortran Kernels [65] is scheduled for the Cydra 5 machine [11][27], approximately 50% of the total time is spent modeling the resources (i.e. answering queries such as "can this operation be scheduled in this cycle"); the other 50% of the total time is spent scheduling <p> Dynamic measurements obtained when scheduling 1327 loops from the Perfect Club <ref> [13] </ref>, SPEC-89 [91], and the Livermore Fortran Kernels [65] for the Cy-dra 5 machine [11] indicate that the essential work performed by the contention queries decreases by a factor of 2.76 to 3.30, depending on the functionality required by the scheduler. <p> For the Cydra 5 machine descriptions, we also verified that precisely the same schedules were produced regardless of the machine description used by the compiler when scheduling a benchmark suite of 1327 loops obtained from the Perfect Club <ref> [13] </ref>, SPEC-89 [91], and the Livermore Fortran Kernels [65]. <p> The algorithm satisfies the definition of the unrestricted scheduling model since it schedules operations in arbitrary order and may reverse scheduling decisions. We used a benchmark of loops obtained from the Perfect Club <ref> [13] </ref>, SPEC-89 [91], and the Livermore Fortran Kernels [65] which consists exclusively of innermost loops with no early exits, no procedure calls, and fewer than 30 basic blocks, as compiled by the Cydra 5 Fortran77 compiler [27]. <p> along edge (4; 2) by one stage, i.e. by II cycles, as shown in in register requirements by increasing SL. 136 4.6 Measurements In this section we investigate the register requirements of the integer and floating point register files for a benchmark of 1327 loops obtained from the Perfect Club <ref> [13] </ref>, SPEC-89 [91], and the Livermore Fortran Kernels [65] compiled for the Cydra 5 as described in Section 2.7. We use here the Iterative Modulo Scheduler [78] to produce high quality MRT-schedules, which are used as input to both the optimal stage scheduling algorithm and the stage scheduling heuristics.
Reference: [14] <author> D. A. Berson, R. Gupta, and M. L. Soffa. </author> <title> URSA: A unified resource allocator for registers and functional units in VLIW architectures. </title> <booktitle> In IFIP Working Conference on Architecture and Compilation Techniques for Fine and Medium Grain Parallelism, </booktitle> <pages> pages 241-255, </pages> <month> January </month> <year> 1993. </year>
Reference: [15] <author> Preston Briggs, Keith D. Cooper, Ken Kennedy, and L. Torczon. </author> <title> Coloring heuristics for register allocation. </title> <booktitle> Proceedings of the ACM SIGPLAN'89 Conference on Programming Language Design and Implementation, </booktitle> <volume> 24(7) </volume> <pages> 275-284, </pages> <month> June </month> <year> 1989. </year>
Reference: [16] <author> R. A. Brualdi. </author> <title> Introductory combinatorics. </title> <address> New York: </address> <publisher> North-Holland, </publisher> <year> 1992. </year>
Reference-contexts: Given a valid partition i 0 ; i 1 ; i 2 : : : i m , the number of row permutations is defined by the multiset permutation formula <ref> [16] </ref>: II! (3.2) and corresponds to the number of distinct permutations of the MRT rows that can be obtained with the given partition, where two rows with the same number of operations are not considered distinct.
Reference: [17] <author> P. Camion. </author> <title> Characterization of totally unimodular matrices. </title> <journal> In Proc. Amer. Math. Soc., </journal> <volume> volume 16, </volume> <pages> pages 1068-1073, </pages> <year> 1965. </year>
Reference-contexts: X a i;j 0 mod 2 8i 2 I i2I 253 This definition is used in the following theorem by Camion <ref> [17] </ref> to show the TU property of a matrix. Theorem A.2 Let A be a matrix with entries equal to 0, 1, and -1.
Reference: [18] <author> G. J. Chaitin. </author> <title> Register allocation and spilling via graph coloring. </title> <booktitle> Proceedings of the ACM SIGPLAN'82 Symposium on Compiler Construction, </booktitle> <pages> pages 98-105, </pages> <month> June </month> <year> 1982. </year>
Reference-contexts: The second contribution of this chapter is a set of heuristics that reduces the register requirements by allowing non-interfering virtual registers that overlap in time to share a common virtual register. We refer to this process as the bundling of compatible virtual registers. Bundling is similar to Chaitin-style coalescing <ref> [18] </ref> in that two or more virtual registers are combined; however, bundling differs in that it combines virtual registers with distinct values. <p> Figure 6.4c illustrates the def components for the producers of x. 221 6.1.3 Interference In this section, we compute the interference between two predicated live ranges based on Chaitin's definition: two live ranges interfere if one of them is live at a definition point of the other <ref> [18] </ref>. This definition implies that overlapping def components do not interfere, provided no operations use these values. In the context of predicated blocks, this definition implies that simultaneous writes to a unique physical register should be tolerated by the hardware of the machine.
Reference: [19] <author> P. P. Chang, S. A. Mahlke, W. Y. Chen, N. J. Warter, and W. W. Hwu. </author> <title> IMPACT: An architectural framework for multiple-instruction-issue processors. </title> <booktitle> In Proceedings of the Eighteenth Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 266-275, </pages> <month> May </month> <year> 1991. </year>
Reference: [20] <author> P. P. Chang, N. J. Warter, S. A. Mahlke, W. Y. Chen, and W. W. Hwu. </author> <title> Three architectural models for compiler-controlled speculative execution. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 44(4) </volume> <pages> 481-494, </pages> <month> April </month> <year> 1995. </year>
Reference: [21] <author> L. Chao, A. LaPaugh, and E. H. Sha. </author> <title> Rotation scheduling: A loop pipelining algorithm. </title> <booktitle> 30th Design Automaton Conference, </booktitle> <pages> pages 556-572, </pages> <year> 1993. </year>
Reference-contexts: Thus conventional optimization techniques can be used at each step to improve the performance of the resulting code. 84 A technique based on repeatedly transforming a schedule to a more compact schedule using a mechanism analogous to retiming has also been investigated, by Chao et al <ref> [21] </ref> for example, and was found to result in schedules with good performance, i.e. small initiation interval and short schedule length.
Reference: [22] <author> A. E. Charlesworth. </author> <title> An approach to scientific array processing: The architectural design of the AP-120B/FPS-164 family. </title> <journal> Computer, </journal> <volume> 14(9) </volume> <pages> 18-27, </pages> <month> September </month> <year> 1981. </year>
Reference-contexts: The advantage of this approach is that the increased flexibility in the scheduling domain may result in loop schedules with the highest achievable performance. This approach was named by Charlesworth <ref> [22] </ref> for its analogy to hardware pipelines and was formulated by Rau [80] for simple loop iterations without conditional statements.
Reference: [23] <author> S. Chaudhuri, R. A. Walker, and J. E. Mitchell. </author> <title> Analyzing and exploiting the structure of the constraints in the ILP approach to the scheduling problem. </title> <journal> IEEE Transactions on Very Large Scale Integration Systems, </journal> <volume> 2(4) </volume> <pages> 456-471, </pages> <month> December </month> <year> 1994. </year>
Reference-contexts: In this section, we reformulate the scheduling constraints so that the scheduling constraints are structured, thus resulting in a more efficient representation of the modulo scheduling problem. The basic idea for this reformulation is due to Chaudhuri et al <ref> [23] </ref> which has such a reformulation for straight line (nonloop, nonbranching) code. The adaptation of this idea to modulo schedules for loop code is, however, not straightforward and substantially different in detail, as is the proof of the validity of this adaptation. <p> row i and is trivially satisfied otherwise, we can simply state that the scheduling constraints of all the scheduling edges are satisfied when: k i + r + l i;j 1 % (r+l i;j 1)modII X x=0 8r 2 [0; II); 8 (i; j) 2 E sched Chaudhuri et al <ref> [23] </ref> make following observation for two dependent operations in straight line (nonloop, nonbranching) code: consider operation i, with latency l, that produces a value used by operation j.
Reference: [24] <author> R. G. Cytron. </author> <title> Compile-Time Scheduling and Optimization for Asynchronous Machines. </title> <type> PhD thesis, </type> <institution> Department of Electrical and Computer Engineering, University of Illinois, Urbana, IL, </institution> <year> 1984. </year>
Reference-contexts: For example, if the sum of the latencies along a cycle is l and the sum of the dependence distances along the same cycle is d, then II must be larger than l=d. Calculating minimum RecMII becomes more complicated when several recurrence cycles have conflicting requirements <ref> [24] </ref>. Techniques to compute RecMII are based on enumeration [27], minimization by a linear programming problem [29], or minimal cost-to-time ratio cycle problem [49][78].
Reference: [25] <author> E. S. Davidson, L. E. Shar, A. T. Thomas, and J. H. Patel. </author> <title> Effective control for pipelined computers. </title> <booktitle> Spring COMPCON-75 digest of papers, </booktitle> <pages> pages 181-184, </pages> <month> February </month> <year> 1975. </year>
Reference-contexts: We present our conclusions in Section 2.8. 12 2.1 Related Work Resource contention in multipipeline scheduling may be based directly on reservation tables, or on the forbidden latency sets or contention-recognizing state machines derived from them, as introduced by Davidson et al <ref> [25] </ref>. Traditionally, reservation tables contain much redundant information that consumes memory and increases query response time. As a result, recent advances favor finite-state automaton approaches.
Reference: [26] <author> J. C. Dehnert, P. Y.-T. Hsu, and J. P. Bratt. </author> <title> Overlapped loop support in the Cydra 5. </title> <booktitle> 3rd International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 181-227, </pages> <month> May </month> <year> 1989. </year>
Reference: [27] <author> J. C. Dehnert and R. A. Towle. </author> <title> Compiling for the Cydra 5. </title> <journal> In The Journal of Supercomputing, </journal> <volume> volume 7, </volume> <pages> pages 181-227, </pages> <year> 1993. </year>
Reference-contexts: The proposed approach fully supports unrestricted scheduling models where operations can be scheduled in arbitrary order and prior scheduling decisions can be reversed. Unrestricted scheduling is essential to accommodate the elaborate scheduling techniques used by today's high performance compilers. The Cydra 5 compiler <ref> [27] </ref>, for example, uses an operation-driven scheduler that reduces the schedule length of a basic block by scheduling operations along the critical path first. Operation-driven schedulers consider operations in topological order, not in order of monotonically increasing (or decreasing) schedule time. <p> The ma 29 chine configuration investigated here has 7 functional units: 2 memory port, 2 address generation, 1 FP adder, 1 FP multiplier, and 1 branch unit. The original machine description used by the Cydra 5 Fortran77 compiler <ref> [27] </ref> was manually optimized, i.e. some physical resources were eliminated from the machine description as they did not introduce any new forbidden latencies [84]. This description models 56 resources and 152 distinct patterns of resource usages, resulting in 52 distinct operation classes with 10223 forbidden latencies. <p> We used a benchmark of loops obtained from the Perfect Club [13], SPEC-89 [91], and the Livermore Fortran Kernels [65] which consists exclusively of innermost loops with no early exits, no procedure calls, and fewer than 30 basic blocks, as compiled by the Cydra 5 Fortran77 compiler <ref> [27] </ref>. The input to the scheduling algorithms consists of the Fortran77 compiler intermediate representation after load-store elimination, recurrence back-substitution, and IF-conversion. <p> Calculating minimum RecMII becomes more complicated when several recurrence cycles have conflicting requirements [24]. Techniques to compute RecMII are based on enumeration <ref> [27] </ref>, minimization by a linear programming problem [29], or minimal cost-to-time ratio cycle problem [49][78]. <p> The modulo scheduling approach was also extended for loops with conditional exits, such as while loops [87][89]. Furthermore, several optimizations were proposed to successfully increase the available instruction level parallelism for modulo schedules, such as elimination of redundant loads and stores across loop iterations <ref> [27] </ref> as well as transformations to reduce the critical paths caused by data dependence [85] or control dependence [87]. Recently, optimizations based on unrolling a loop body 82 before applying a modulo scheduling algorithm have also been investigated [56].
Reference: [28] <institution> Digital Equipment Corp., Maynard, </institution> <address> MA. </address> <note> DecChip 21064 Microprocessor Hardware Reference Manual EC-N0079-72. </note>
Reference-contexts: The major advantage of this approach over previous work [9][68][76] is that no restrictions are imposed on scheduling algorithms other than the need to satisfy the constraints of the machine itself. Reduced representations for the DEC Alpha 21064 <ref> [28] </ref>, MIPS R3000/- R3010 [53], and Cydra 5 [11] indicate potentially 4.0 to 6.9 times faster contention queries, while requiring 22 to 67% of the memory storage used by the original machine descriptions. <p> Using our approach, the resource requirements can be expressed in terms close to the actual hardware structure of the target machine, and the reduced machine description used by the compiler is generated in an error-free and automated fashion. Experiments with the DEC Alpha 21064 <ref> [28] </ref>, MIPS R3000/R3010 [53], and Cy-dra 5 [11] machines indicate potentially 4.0 to 6.9 times faster contention queries, while requiring 22 to 67% of the memory storage used by the original machine descriptions. <p> The reservation tables associated with the machine descriptions of the original model, the discrete reduction, and the 64 bit word bitvector reduction are shown, respectively, in Figures 2.5a, 2.5b, and 2.5c. Table 2.3 shows the results of the reductions for the DEC Alpha 21064 <ref> [28] </ref> using the machine description presented by Bala and Rubin [9]. Comparing the original description to the specific reduction for a 64 bit word bitvector representation, the average word usage is decreased by a factor of 6.9.
Reference: [29] <author> B. Dupont de Dinechin. </author> <title> Simplex scheduling: More than lifetime-sensitive instruction scheduling. </title> <booktitle> Proceedings of the International Conference on Parallel Architecture and Compiler Techniques, </booktitle> <pages> pages 327-330, </pages> <year> 1994. </year>
Reference-contexts: Calculating minimum RecMII becomes more complicated when several recurrence cycles have conflicting requirements [24]. Techniques to compute RecMII are based on enumeration [27], minimization by a linear programming problem <ref> [29] </ref>, or minimal cost-to-time ratio cycle problem [49][78]. Relating ResMII and RecMII to the available instruction level parallelism of a loop for a given machine, we can state that a loop has insufficient instruction level parallelism for a given machine when RecMII, rather than ResMII, constrains MII. <p> As a result, minimizing Buff is equivalent to minimizing integral MaxLive. Another approximation of the register requirements found in the literature, and used by Ning and Gao [70] and Dupont de Dinechin <ref> [29] </ref>, approximates MaxLive by computing the average lifetime requirements, referred to as AvgLive, which corresponds to the sum of all the register lifetimes divided by II. <p> Ning and Gao's polynomial-time algorithm [70] has also been used by Dupont de Dinechin <ref> [29] </ref> to find modulo schedules with low register requirements. <p> After the 1 II factor is removed from Equation (5.27), the formulation is identical to the one proposed by Dupont de Dinechin <ref> [29] </ref>, for example, except for the presence of the +1 term (which is particular to our virtual register model) in Inequality (5.26). 5.3.6 Buffer Requirements As indicated in Section 3.2, buffers must be reserved for only intervals that are integer multiples of II cycles.
Reference: [30] <author> K. Ebcioglu, R. D. Groves, K.-C. Kim, G. M. Silberman, and I. Ziv. </author> <title> VLIW compilation techniques in a superscalar environment. </title> <booktitle> In Proceedings of the ACM SIGPLAN'94 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 36-48. </pages> <year> 1994. </year>
Reference: [31] <author> A. E. Eichenberger and E. S. Davidson. </author> <title> A reduced multipipeline machine description that preserves scheduling constraints. </title> <type> Technical Report CSE-TR-266-95, </type> <institution> University of Michigan, </institution> <address> Ann Arbor, MI, </address> <year> 1995. </year>
Reference-contexts: We propose a reduced machine representation that is precise and efficient for a compiler to use in its scheduling algorithms. Our contribution is a technique <ref> [31] </ref> that reduces a machine representation to result in significantly faster detection of resource contentions while exactly preserving the scheduling constraints present in the original machine. The resulting representation is expressed using reservation tables that determine the usage of synthesized resources for each operation.
Reference: [32] <author> A. E. Eichenberger and E. S. Davidson. </author> <title> Register allocation for predicated code. </title> <booktitle> Proceedings of the 28th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 338-349, </pages> <month> November </month> <year> 1995. </year>
Reference-contexts: The framework is based on the logical relations 7 among the predicate values defined and used in an enlarged predicated block. Using this framework, we propose a technique <ref> [32] </ref> that reduces the register requirements by allowing non-interfering virtual registers whose lifetimes overlap in time to share a common virtual register.
Reference: [33] <author> A. E. Eichenberger and E. S. Davidson. </author> <title> Stage scheduling: A technique to reduce the register requirements of a modulo schedule. </title> <booktitle> Proceedings of the 28th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 180-191, </pages> <month> November </month> <year> 1995. </year>
Reference-contexts: We contribute an optimal stage scheduling algorithm [35][37], referred to as the MinReg Stage-Scheduler, that finds a schedule with minimum register requirements among all stage schedules. We also contribute a set of stage scheduling heuristics <ref> [33] </ref> that closely approximate the minimum solution with linear, or at most quadratic, computational complexity. 6 Our results indicate that, for all the 920 loops successfully scheduled by the Min--Reg Modulo-Scheduler in no more than 15 minutes on a HP-9000/715 workstation, the register requirements are decreased by 22.2% on average by
Reference: [34] <author> A. E. Eichenberger and E. S. Davidson. </author> <title> Efficient formulation for optimal modulo schedulers. </title> <booktitle> Proceedings of the ACM SIGPLAN'97 Conference on Programming Language Design and Implementation, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: In the 1327 loop benchmark suite for the Cydra 5 machine, for example,the MinReg Modulo-Scheduler finds schedules with lower register requirements for 13.0% of the 1327 loops, compared to minimizing the best approximation of the actual register requirements found in the literature [29][70]. Also, using the structured formulation <ref> [34] </ref> results in a decrease in the total execution time of the solver by a factor of 8.6, over all the loops successfully scheduled by both the structured and the traditional formulation of the modulo scheduling search space.
Reference: [35] <author> A. E. Eichenberger, E. S. Davidson, and S. G. Abraham. </author> <title> Minimum register requirements for a modulo schedule. </title> <booktitle> Proceedings of the 27th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 75-84, </pages> <month> November </month> <year> 1994. </year> <month> 261 </month>
Reference-contexts: The work of Wang et al [92] has also investigated a two step heuristic to modulo scheduling. Following the publication of our stage scheduler for minimum register requirements <ref> [35] </ref>, Wang et al presented a stage scheduler that minimizes the buffer requirements [93].
Reference: [36] <author> A. E. Eichenberger, E. S. Davidson, and S. G. Abraham. </author> <title> Optimum modulo schedules for minimum register requirements. </title> <booktitle> Proceedings of the International Conference on Supercomputing, </booktitle> <pages> pages 31-40, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: Modulo scheduling results in high performance code, but increased register requirements due to higher levels of concurrency [64]. Thus, developing register-sensitive modulo schedulers is particularly critical for high performance machines. Our first contribution to register-sensitive modulo schedulers is an optimal algorithm <ref> [36] </ref> that finds a schedule with minimum register requirements among all 5 maximum throughput modulo schedules. <p> This large gap is partly explained by the strong simplification (no resource conflict) of the lower bound, a gap that is particularly apparent for large loops and negligible for small loops <ref> [36] </ref>.
Reference: [37] <author> A. E. Eichenberger, E. S. Davidson, and S. G. Abraham. </author> <title> Minimizing register requirements of a modulo schedule via optimum stage scheduling. </title> <journal> In International Journal of Parallel Programming, </journal> <volume> volume 24, </volume> <pages> pages 103-132, </pages> <year> 1996. </year>
Reference: [38] <author> C. Eisenbeis, W. Jalby, and A. </author> <title> Lichnewsky. Squeezing more performance out of a Cray-2 by vector block scheduling. </title> <booktitle> Proceedings of Supercomputing '88, </booktitle> <pages> pages 237-246, </pages> <month> November </month> <year> 1988. </year>
Reference: [39] <author> C. Eisenbeis, S. Lelait, and B. Marmol. </author> <title> The meeting graph: A new model for loop cyclic register allocation. </title> <booktitle> Proceedings of the International Conference on Parallel Architecture and Compiler Techniques, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: Hendren et al [46] have also investigated register allocation algorithms tailored for modulo scheduling and based on cyclic interval graphs. They have also proposed spilling heuristics. Eisenbeis et al <ref> [39] </ref> have also contributed a register allocation technique based on cyclic graphs. All the modulo scheduling techniques presented above have relied on the original modulo constraints that enforce a unique initiation interval among consecutive loop iterations.
Reference: [40] <author> C. Eisenbeis and D. Windheiser. </author> <title> Optimal software pipelining in presence of resource constraints. </title> <booktitle> Proceedings of the International Conference on Parallel Architecture and Compiler Techniques, </booktitle> <month> August </month> <year> 1993. </year>
Reference-contexts: Some algorithms treat each step separately; e.g. Eisenbeis's software pipelining approach <ref> [40] </ref> uses distinct heuristics for Steps 1, 2, and 3 below. Other algorithms combine steps to optimize some combination of their distinct objectives; e.g. Huff's 72 lifetime-sensitive modulo scheduler [49] or Rau's Iterative Modulo Scheduler [78] com- bines Steps 1 and 2 below. 1. <p> For example, the work of Eisenbeis and Windheiser has investigated an algorithm for determining the optimal unrolling degree necessary to find a modulo schedule that fully saturates a critical resource of the machine <ref> [40] </ref>. During the first step, the optimal unrolling degree is determined and an MRT-schedule is found; during the second step, a stage schedule is found that minimizes the schedule length for the given MRT-schedule.
Reference: [41] <author> J. R. Ellis. Bulldog: </author> <title> a Compiler for VLIW Architectures. </title> <publisher> The MIT Press, </publisher> <year> 1985. </year>
Reference-contexts: Also interesting would be to extend modulo scheduling heuristics to efficiently handle machines with clusters of functional units, perhaps along the lines of the scheduling technique proposed by Ellis <ref> [41] </ref> for straight line code. 242 APPENDIX A Completion of Proof of Theorem 4.1 We demonstrate here that the stage scheduling problem for minimum integral MaxLive can be solved with a linear programming (LP) solver.
Reference: [42] <author> J. A. Fisher. </author> <title> Trace scheduling: a technique for global microcode compaction. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 30(7) </volume> <pages> 478-490, </pages> <month> July </month> <year> 1981. </year>
Reference: [43] <author> J. R. Goodman and W.-C. Hsu. </author> <title> Code scheduling and register allocation in large basic blocks. </title> <booktitle> Proceedings of the International Conference on Supercomputing, </booktitle> <pages> pages 442-452, </pages> <year> 1988. </year>
Reference-contexts: With pre-scheduling bundling heuristics, interferences between virtual registers of a predicated block cannot be computed as precisely as in Section 6.1.3 since the scheduling time of the operations is unknown. Instead, we must rely on the earliest and latest feasible scheduling time of an operation <ref> [43] </ref> [49]. In our current implementation, the pre-scheduling bundling heuristics rely on a weaker form of interference where two virtual registers are considered to interfere if any two of their producer operations do not execute under mutually disjoint predicates.
Reference: [44] <author> R. Govindarajan, E. R. Altman, and G. R. Gao. </author> <title> Minimizing register requirements under resource-constrained rate-optimal software pipelining. </title> <booktitle> Proceedings of the 27th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 85-94, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Prior to our work, modulo scheduling algorithms based on linear programming or integer linear programming formulations have not directly minimized MaxLive, but have instead minimized approximations of the register requirements. For example, 67 the work of Govindarajan et al <ref> [44] </ref> and Wang et al [93] approximates registers by conceptual FIFO buffers, initially proposed by Ning and Gao [70]. Unlike registers, conceptual FIFO buffers are reserved for an interval of time that is always an integer multiple of II cycles. <p> This corollary can be shown by applying the proof of Theorem 4.2 for the acyclic parts. We conclude this section by contrasting the complexity of our stage scheduling model summarized in Figure 4.7 with the modulo scheduling model presented by Govindarajan et al <ref> [44] </ref>, where both models minimize integral MaxLive for a machine with finite resources, but we assume a given MRT and Govindarajan et al do not. <p> Their scheduling algorithm handles loop iterations with arbitrary dependence graphs. Ning and Gao's results were extended for machines with finite resources and fully-pipelined functional units by Govindarajan et al <ref> [44] </ref> using an algorithm based on an integer linear programming formulation that directly minimizes the buffer requirements of a schedule. Altman et al [7] have independently developed a formulation for machines with complex resource requirements. <p> The first type corresponds the assignment constraints, which insure that each operation in the loop is assigned to exactly one cycle in the schedule. The formulation of the assignment constraints is presented in Section 5.3.1 using the formulation proposed by Govindarajan et al <ref> [44] </ref>. The second type represents the resource constraints, which insure that no cycle of the schedule requires more resources than are available in the 167 Machine: (input) Q set of resource types. M q number of resources of type q. <p> The third type corresponds the scheduling constraints, which enforce arbitrary scheduling dependences between pairs of operations. A traditional formulation of the scheduling constraints is shown in Section 5.3.3 using the formulation devised by Govindarajan et al <ref> [44] </ref>. A novel and more structured formulation of the loop scheduling constraints is developed in Section 5.3.4. In the latter part of this section, we describe three objective functions that account for the register requirements or some approximation of the register requirements. <p> The total buffer requirements is then simply equal to the sum of the buffer requirements associated with each operation, i.e. Buf f = i=0 The formulation of Buff is identical to the one proposed by Govindarajan et al <ref> [44] </ref>, except for the presence of the +1 term (which is particular to our virtual register model) in Inequality (5.29). Note that, as formulated here, the formulation of Buff is not structured because of Inequality (5.29). 5.3.7 Register Requirements We now define the register requirements associated with a modulo schedule.
Reference: [45] <author> J. C. Gyllenhaal. </author> <title> A machine description language for compilation. </title> <type> Master's thesis, </type> <institution> Department of Electrical and Computer Engineering, University of Illinois, Urbana, IL, </institution> <year> 1994. </year>
Reference-contexts: in this cycle without resource contention?" Typically, this functionality has been provided by a contention query module that processes the machine description of a target machine, generates an internal representation of the resource requirements, and provides for a querying mechanism [19][27][42]<ref> [45] </ref>[59]. The IMPACT compiler, for example, implemented such a module [45] to produce high performance schedules for a wide range of machines, from existing architectures such as X86, PA-RISC, and SPARC to research architectures such as PlayDoh [54].
Reference: [46] <author> L. J. Hendren, G. R. Gao, E. R. Altman, and C. Mukerji. </author> <title> A register allocation framework based on hierarchical cyclic interval graphs. </title> <booktitle> Proceedings of the 4th International Conference on Compiler Construction. Lecture Notes in Computer Science 641, </booktitle> <publisher> Springer, </publisher> <pages> pages 176-191, </pages> <year> 1992. </year>
Reference-contexts: By using loop unrolling, with its consequent code expansion, their algorithm typically achieves register allocations that are within four registers of the MaxLive lower bound on machines without such hardware support. Hendren et al <ref> [46] </ref> have also investigated register allocation algorithms tailored for modulo scheduling and based on cyclic interval graphs. They have also proposed spilling heuristics. Eisenbeis et al [39] have also contributed a register allocation technique based on cyclic graphs. <p> For register allocators based on the classical graph coloring method, originally proposed by Chaitin [15][18], register allocation for predicted codes can be simply achieved by using a refined interference graph instead of the conventional one. However, several register allocators depart from the graph coloring method: e.g. Hendren et al <ref> [46] </ref> investigate a framework based on cyclic interval graphs, introducing the notion of time in the register allocator paradigm. This additional notion of time is particularly useful for the live ranges of a loop, where live ranges may cross the boundary of an iteration.
Reference: [47] <author> F. S. Hillier and G. J. Lieberman. </author> <title> Introduction to Mathematical Programming. </title> <publisher> McGraw-Hill, </publisher> <year> 1990. </year>
Reference-contexts: 1;4 + p 2;3 + p 3;4 + p 4;6 + p 5;6 + p 6;7 + 5 (4.11) We can now reduce the problem of finding a stage schedule that results in the minimum integral MaxLive to a well-known class of problems, solved by a linear programming (LP) solver <ref> [47] </ref>. Note, however, that (4.11) is not acceptable as the input to an LP-solver, because the objective function cannot contain any max functions.
Reference: [48] <author> P. Y. Hsu. </author> <title> Highly Concurrent Scalar Processing. </title> <type> PhD thesis, </type> <institution> Department of Electrical and Computer Engineering, University of Illinois, Urbana, IL, </institution> <year> 1986. </year> <month> 262 </month>
Reference-contexts: In straight line code, for example, trace scheduling is used to merge several basic blocks along a frequent execution path into a single enlarged block [42][50]. These blocks can be further enlarged by using predicated execution <ref> [48] </ref> to merge several execution traces [60][86]. Similarly, in loop code, software pipelining extends the scope of compilation beyond one basic block by overlapping the execution of consecutive loop iterations [27][48][49][78][94]. <p> While performance is generally increased by using predicated execution <ref> [48] </ref> to exploit instruction level parallelism in the presence of conditional branches [27][60][78][94], code with predicated operations may result in artificially higher register requirements, if the predicate under which registers are defined and used is not taken into account when allocating virtual registers to physical registers. <p> Register Allocation for Predicated Code High performance compilers use predicated execution <ref> [48] </ref> to increase the instruction level parallelism of an application by merging several basic blocks into an enlarged predicated block [27][60][78][94] with IF-conversion [4][72]. <p> They developed an iterative scheduling algorithm that could schedule and unschedule operations to result in high-performance schedules even in the presence of complex resource requirements. Conditionals in the loop iterations were handled by using IF-conversion [4][72] to replace each operation in conditional statements with a predicated operation <ref> [48] </ref>, i.e. an operation that completes normally when a logical expression, referred to as a predicate, evaluates to true; otherwise, the predicated operation is transformed into a no-op and has no side effect. <p> We also contrast our stage scheduling heuristics to published work on lifetime-sensitive modulo scheduling heuristics. The concept of displacing an operation or a group of operations by multiples of II cycles in a modulo schedule was first proposed by Hsu <ref> [48] </ref>. In his work, loops with general dependence graphs are processed in two steps. First, the strongly connected components of the dependence graph are identified and their operations are scheduled with the smallest feasible II using a combinatorial search procedure. <p> Our experimental data is presented in Section 5.6 and our conclusions in Section 5.7. 5.1 Related Work Searching for modulo schedules with the highest steady-state throughput over the entire space of modulo schedules was first formulated using an integer linear programming formulation by Hsu <ref> [48] </ref>. This formulation was obtained for arbitrary dependence graphs on machines with finite resources and simple resource reservation tables. <p> A well established approach uses predication to merge several basic blocks into a single enlarged predicated block. This approach relies on predicated operations <ref> [48] </ref>, a class of operations that complete normally when a logical expression, referred to as the predicate of an operation, evaluates to true. Otherwise, the predicated operation is transformed into a no-op and has no side effect.
Reference: [49] <author> R. A. Huff. </author> <title> Lifetime-sensitive modulo scheduling. </title> <booktitle> Proceedings of the ACM SIG--PLAN'93 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 258-267, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Some algorithms treat each step separately; e.g. Eisenbeis's software pipelining approach [40] uses distinct heuristics for Steps 1, 2, and 3 below. Other algorithms combine steps to optimize some combination of their distinct objectives; e.g. Huff's 72 lifetime-sensitive modulo scheduler <ref> [49] </ref> or Rau's Iterative Modulo Scheduler [78] com- bines Steps 1 and 2 below. 1. <p> As a result, the register edge (load, add) cannot be removed from G 0 . To determine the conditions under which edges can be safely removed, we introduce the MinDist relation as presented by Huff <ref> [49] </ref>. MinDist (x; y) represents the minimum number of cycles, possibly negative, that must occur between the time that operation X is scheduled until the time that operation Y is scheduled, if there is a path in the dependence graph from X to Y . <p> MinDist is guaranteed to converge to a solution if the sum of the lengths along each cycle in the dependence graph is nonpositive <ref> [49] </ref>. <p> Fortunately, the minimum initiation interval due to recurrences (RecMII) precludes this situation, namely II RecMII guarantees that the sum of the l i;j ! i;j fl II length along each cycle is nonpositive, as otherwise the scheduling constraints of such a cycle could not be simultaneously satisfied <ref> [49] </ref>. Theorem 3.2 (Edge Removal Test) Consider the operations A, B, and C in V with edges (a; b) and (a; c) in E sched and E reg of G. <p> Because high-performance schedules result in increased register requirements, research was also conducted on developing efficient register-sensitive modulo scheduling algorithms. Huff has investigated a heuristic based on a bidirectional slack-scheduling method that schedules operations early or late depending on their number of stretchable input and output flow dependencies <ref> [49] </ref>. Llosa et al have recently proposed two heuristics that are based on a bidirectional slack-scheduling method with a scheduling priority function tailored to minimize the register requirements [57][58]. A detailed comparison of these approaches with our approach for register-sensitive modulo scheduling is provided in Chapter 4. <p> We first present an algorithm that performs Step 2 in an optimal fashion, minimizing MaxLive, the minimum number of registers required to generate spill-free code <ref> [49] </ref>. This optimal algorithm, referred to as MinReg Stage-Scheduler, proceeds in linear time (in the number of edges) for loops whose dependence graphs have acyclic underlying graphs 1 . Otherwise, the optimal algorithm uses a general linear-programming approach to handle general dependence graphs with unrestricted loop-carried dependences and common subexpressions. <p> Heuristics for register-sensitive modulo scheduling have been proposed by Huff <ref> [49] </ref> and Llosa et al [57][58]. These approaches are flexible in that they directly generate a modulo schedule, thus potentially generating schedules with lower register requirements. <p> First, the heuristic removes all redundant edges, which has a quadratic computational complexity in the number of edges if the MinDist relation is provided. Computing the MinDist relation <ref> [49] </ref> corresponds to an all-pairs longest-path problem which has a cubic computational complexity in the number of operations (in each strongly connected component). However, this relation may be readily available, as some modulo schedulers use this relation to determine the minimum initiation interval. <p> Alternatively, to schedule 80% of the loops successfully with the chosen II and no spilling, the MinReg Stage-Scheduler needs at least 45 registers. The second curve in Figure 4.16, labeled "Schedule Independent Lower Bound," corresponds to M inAvg in <ref> [49] </ref> and is a lower bound on the register requirements formed by assuming a machine with unlimited issue width and unlimited functional resources. The lower bound is obtained by computing the MinDist relation and by assuming that no register lifetime spans more than the minimum scheduling distance (i.e. <p> The latency and dependence distance of the outgoing edges from start and of the incoming edges to stop are 0. Using the methodology presented by Huff, the MinDist relation can be used to determine a lower bound and an upper bound on the schedule time of each operation <ref> [49] </ref>. We know that operation i cannot be scheduled earlier than MinDist (start; i) cycles after the start pseudo operation. Similarly, we know that operation i cannot be scheduled later than MinDist (i; stop) cycles before the stop pseudo operation. <p> We also use a lower bound on the register requirements, i.e. M inAvg as computed by Huff <ref> [49] </ref>, to bound the values of the objective functions. The resource constraints are formulated for the reduced machine representation of the Cydra 5 shown in Figure 2.5b, as produced by the algorithm described in Chapter 2 for the discrete representation. <p> In this graph, the X-axis represents MaxLive and the Y-axis represents the fraction of loops scheduled. This graph presents three curves. The first curve, labeled "Schedule Independent Lower Bound," corresponds to <ref> [49] </ref> and is representative of the register requirements of a machine without any resource conflicts. The second curve presents the register requirements of the schedules generated by the MinReg Modulo 201 Modulo-Scheduler. Scheduler. <p> With pre-scheduling bundling heuristics, interferences between virtual registers of a predicated block cannot be computed as precisely as in Section 6.1.3 since the scheduling time of the operations is unknown. Instead, we must rely on the earliest and latest feasible scheduling time of an operation [43] <ref> [49] </ref>. In our current implementation, the pre-scheduling bundling heuristics rely on a weaker form of interference where two virtual registers are considered to interfere if any two of their producer operations do not execute under mutually disjoint predicates. <p> Figure 6.7 presents the fraction of the loops in the benchmark suite that can be scheduled for a machine with any given number of registers without spilling and without increasing II. The "Schedule Independent Lower Bound" curve corresponds to <ref> [49] </ref> and is representative of the register requirements of a machine without any resource conflicts, but does not support bundling, i.e. it ignores the possible decrease in register requirements due to the bundling of compatible virtual registers. <p> If a larger decrease in register requirements is desirable, our best stage scheduling heuristics can be employed. If further reductions in register requirements are needed, we can compute a lower bound on the register requirements, as proposed by Huff <ref> [49] </ref>, to estimate the likelihood of finding schedule with lower register requirements, by searching for either a new MRT-schedule, a minimum MaxLive stage schedule, or a combination of both.
Reference: [50] <author> W. W. Hwu, S. A. Mahlke, W. Y. Chen, P. P. Chang, J. Warter N, R. A. Bring-mann, R. G. Ouellette, R. E. Hank, T. Kiyohara, G. E. Haab, J. G. Holm, and D. M. Lavery. </author> <title> The superblock: An effective technique for VLIW and superscalar compilation. </title> <journal> In The Journal of Supercomputing, </journal> <volume> volume 7, </volume> <pages> pages 229-248, </pages> <year> 1993. </year>
Reference: [51] <author> R. C. Johnson. </author> <type> Personal communication. </type> <month> June </month> <year> 1995. </year>
Reference-contexts: However, ongoing research on efficient algorithms for precise or approximate analysis of predicated code <ref> [51] </ref> will significantly improve 216 the execution time of this approach. We now present the mechanisms required by our framework in more detail. 6.1.1 Predicate Extraction The predicate extraction mechanism allows the compiler to find how predicates are defined and used.
Reference: [52] <author> K. Ebcioglu and T. Nakatani. </author> <title> A New Compilation Technique for Parallelizing Loops with Unpredictable Branches on a VLIW Architecture. </title> <editor> In D. Gelern-ter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 213-229. </pages> <publisher> Pitman/The MIT Press, </publisher> <year> 1990. </year>
Reference: [53] <author> G. Kane and J. Heinrich. </author> <title> MIPS RISC Architecture. </title> <publisher> Prentice Hall, </publisher> <year> 1992. </year>
Reference-contexts: The major advantage of this approach over previous work [9][68][76] is that no restrictions are imposed on scheduling algorithms other than the need to satisfy the constraints of the machine itself. Reduced representations for the DEC Alpha 21064 [28], MIPS R3000/- R3010 <ref> [53] </ref>, and Cydra 5 [11] indicate potentially 4.0 to 6.9 times faster contention queries, while requiring 22 to 67% of the memory storage used by the original machine descriptions. <p> Using our approach, the resource requirements can be expressed in terms close to the actual hardware structure of the target machine, and the reduced machine description used by the compiler is generated in an error-free and automated fashion. Experiments with the DEC Alpha 21064 [28], MIPS R3000/R3010 <ref> [53] </ref>, and Cy-dra 5 [11] machines indicate potentially 4.0 to 6.9 times faster contention queries, while requiring 22 to 67% of the memory storage used by the original machine descriptions. <p> resource usages / operation 17.3 7.3 8.1 8.3 8.5 word usages / operation 11.0 5.6 5.6 2.4 1.6 Table 2.4: Results for the MIPS R3000/R3010: 15 operation classes, 428 forbidden latencies (all 34). 32 33 34 35 36 Table 2.4 shows the results of the reductions for the MIPS R3000/R3010 <ref> [53] </ref> using the machine description presented by Proebsting and Fraser [76]. Comparing the original description to the 64 bit word bitvector reduction, the reduced machine description decreases the average word usage by a factor of 6.9.
Reference: [54] <author> V. Kathail, M. S. Schlansker, and B. R. Rau. </author> <title> HPL PlayDoh architecture specification: Version 1.0. </title> <type> Technical Report HPL-93-80, </type> <institution> HP Laboratories, </institution> <month> February </month> <year> 1994. </year>
Reference-contexts: The IMPACT compiler, for example, implemented such a module [45] to produce high performance schedules for a wide range of machines, from existing architectures such as X86, PA-RISC, and SPARC to research architectures such as PlayDoh <ref> [54] </ref>. With the recent emphasis on exploiting instruction level parallelism, compile time is increasingly spent in the contention query module as several cycles of a schedule, possibly from several basic blocks [12][67], are queried per operation in order to achieve good schedules. <p> This extraction mechanism is very sensitive to the instruction set architecture, since it relies on the precise semantics of the operations that define the predicate val 217 ues. In this work, we adopt the predication scheme of the HPL PlayDoh architecture <ref> [54] </ref>. Support for predicated execution is provided by a file of 1-bit predicate registers which holds the predicate values, an additional source operand for most operations to specify a predicate register, and a rich set of operations to define values for these predicate registers.
Reference: [55] <author> M. Lam. </author> <title> Software Pipelining: An effective scheduling technique for VLIW machines. </title> <booktitle> Proceedings of the ACM SIGPLAN'88 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 318-328, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: The scope of modulo scheduling has been widened to a large variety of loops. Loops with conditional statements are handled using hierarchical reduction <ref> [55] </ref> or IF- and Reverse-IF-conversion [95]. Modulo scheduling has also been extended to 57 a large variety of loops with early exits, such as while loops [87][89]. <p> Theorem 3.1 We can find a register allocation for a modulo schedule that requires no more than MaxLive registers. The resulting register allocation may require unrolling the modulo schedule some finite number of times, using modulo variable expansion <ref> [55] </ref>. 65 Proof. We demonstrate the theorem by considering the trace of operations, t, that is generated when executing the given modulo schedule an infinite number of times. <p> To address this performance issue, Lam proposed a hierarchical reduction technique <ref> [55] </ref> that schedules the operations of then and else statements separately, and then treats the operations of both partial schedules as a unique pseudo operation that consumes the union of the resource requirements of both partial sched 81 ules. <p> Otherwise, there is a class of mapping-free machines for which a mapping can be trivially found by unrolling the resulting schedule some finite number of times, and using a technique similar to modulo variable expansion <ref> [55] </ref>. For these machines, their approach does not require unrolling but introduces additional constraints on the scheduling space.
Reference: [56] <author> D. M. Lavery and W. W. Hwu. </author> <title> Unrolling-based optimizations for modulo scheduling. </title> <booktitle> Proceedings of the 28th Annual International Symposium on Mi-croarchitecture, </booktitle> <pages> pages 327-337, </pages> <month> November </month> <year> 1995. </year>
Reference-contexts: Recently, optimizations based on unrolling a loop body 82 before applying a modulo scheduling algorithm have also been investigated <ref> [56] </ref>. Tech--niques based on Petri nets have also been proposed to determine a suitable degree of unrolling for modulo schedules [5]. Because high-performance schedules result in increased register requirements, research was also conducted on developing efficient register-sensitive modulo scheduling algorithms.
Reference: [57] <author> J. Llosa, A. Gonzalez, E. Ayguade, and M. Valero. </author> <title> Swing modulo scheduling: A lifetime-sensitive approach. </title> <booktitle> Proceedings of the International Conference on Parallel Architecture and Compiler Techniques, </booktitle> <month> October </month> <year> 1996. </year>
Reference: [58] <author> J. Llosa, M. Valero, E. Ayguade, and A. Gonzalez. </author> <title> Hypernode reduction modulo scheduling. </title> <booktitle> Proceedings of the 28th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 350-360, </pages> <month> November </month> <year> 1995. </year>
Reference: [59] <author> G. P. Lowney, S. M. Freudenberger, T. J. Karzes, W. D. Lichtenstein, R. P. Nix, J. S. O'Donnell, and J. C. Ruttenberg. </author> <title> The Multiflow trace scheduling compiler. </title> <journal> In The Journal of Supercomputing, </journal> <volume> volume 7, </volume> <pages> pages 51-142, </pages> <year> 1993. </year>
Reference-contexts: In particular, our approach effectively supports schedulers that achieve high performance by using a backtracking mechanism that reverses a limited number of previous scheduling decisions <ref> [59] </ref>, by using software pipelining to overlap the execution of consecutive loop iterations [48][55][57][81], or a combination of both [27][49][78][83]. <p> Limited backtracking is used in numerous compilers, e.g. when scheduling software pipelined loops [27][49][78][83][94] and scalar code <ref> [59] </ref>. The proposed approach also precisely handles basic block boundary conditions, i.e. the dangling resource requirements from predecessor basic blocks. In general, the resource requirements at the beginning of a basic block consist of the union of all the resource requirements dangling from predecessor basic blocks.
Reference: [60] <author> S. A. Mahlke. </author> <title> Exploiting Instruction Level Parallelism in the Presence of Conditional Branches. </title> <type> PhD thesis, </type> <institution> Department of Electrical and Computer Engineering, University of Illinois, Urbana, IL, </institution> <year> 1996. </year>
Reference-contexts: Therefore, all optimizations modifying predicates would have to maintain the consistency of the dataflow graph at each step. Another approach is to reconstruct a dataflow graph from the predicated blocks each time dataflow analysis is needed <ref> [60] </ref>. This approach eliminates the complexity of maintaining a consistent dataflow graph and requires no changes to the classical dataflow analysis, traditional optimizations and register allocation. Additionally, the complexity of this approach is low, resulting in low compile time and good maintainability. However, this approach may produce in conservative results.
Reference: [61] <author> S. A. Mahlke, W. Y. Chen, W.-M. W. Hwu, B. R. Rau, and M. S. Schlansker. </author> <title> Sentinel scheduling for VLIW and superscalar processors. </title> <booktitle> In 5th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 238-247, </pages> <month> October </month> <year> 1992. </year> <month> 263 </month>
Reference: [62] <author> S. A. Mahlke, D. C. Lin, W. Y. Chen, R. E. Hank, and R. A. Bringmann. </author> <title> Effective compiler support for predicated execution using hyperblocks. </title> <booktitle> Proceedings of the 25th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 45-54, </pages> <month> December </month> <year> 1992. </year>
Reference: [63] <author> W. Mangione-Smith. </author> <title> Performance Bounds and Buffer Space Requirements. </title> <type> PhD thesis, </type> <institution> University of Michigan, Department of Electrical Engineering and Computer Science, </institution> <address> Ann Arbor, MI, </address> <year> 1992. </year>
Reference: [64] <author> W. Mangione-Smith, S. G. Abraham, and E. S. Davidson. </author> <title> Register requirements of pipelined processors. </title> <booktitle> Proceedings of the International Conference on Supercomputing, </booktitle> <pages> pages 260-271, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: This problem is crucial to the performance of future machines as higher levels of parallelism inherently exacerbate the register requirements <ref> [64] </ref>. In this dissertation, we address the issue of register-sensitive schedulers in the context of software pipelined loops. First, our research aims to understand the fundamental relation between instruction level parallelism and register requirements for a set of loop benchmarks under realistic assumptions. <p> Modulo scheduling uses the same schedule for each iteration of a loop and initiates successive iterations at a constant rate, i.e. one Initiation Interval (II clock cycles) apart. Modulo scheduling results in high performance code, but increased register requirements due to higher levels of concurrency <ref> [64] </ref>. Thus, developing register-sensitive modulo schedulers is particularly critical for high performance machines. Our first contribution to register-sensitive modulo schedulers is an optimal algorithm [36] that finds a schedule with minimum register requirements among all 5 maximum throughput modulo schedules. <p> Since modulo scheduling exploits a higher level of parallelism, it results in higher register requirements because more values are needed to support more concurrent operations. This effect is inherent to parallelism in execution and will be exacerbated by wider machines and higher latency pipelines <ref> [64] </ref>. As a result, developing scheduling techniques that exploit instruction level parallelism while containing the register requirements is crucial to the performance of future machines. Our research aims at understanding the fundamental relationship between instruction level parallelism and register requirements for a set of benchmarks under realistic assumptions. <p> This improved schedule is shown in Figure 4.2. The stage schedule presented in Figure 4.2 results in the minimum register requirements for this kernel, MRT, and set of functional unit latencies. Mangione-Smith et al <ref> [64] </ref> have shown that for dependence graphs with at most a single use per virtual register, as in Example 4.1, minimizing each skip factor individually always results in the minimum register requirements. However, this result does not apply to general dependence graphs with unrestricted common subexpressions and loop carried dependences. <p> First, the register requirements increase as more values are needed to support more concurrent operations. This effect is inherent to parallelism and is exacerbated by wider machines and higher latency pipelines <ref> [64] </ref>. Second, we show in this chapter that the register requirements increase for predicated code as current compilers do not allocate registers as well in predicated code as in unpredicated code.
Reference: [65] <author> F. H. McMahon. </author> <title> The Livermore Fortran Kernels: A computer test of the numerical performance range. </title> <type> Technical Report UCRL-53745, </type> <institution> Lawrence Livermore National Laboratory, Livermore, California, </institution> <year> 1986. </year>
Reference-contexts: Efficient representations are important since high performance compilers spend a significant amount of compilation time scheduling operations, and thus testing for potential resource contentions. When a benchmark suite of 1327 loops from the Perfect Club [13], SPEC-89 [91], and the Livermore Fortran Kernels <ref> [65] </ref> is scheduled for the Cydra 5 machine [11][27], approximately 50% of the total time is spent modeling the resources (i.e. answering queries such as "can this operation be scheduled in this cycle"); the other 50% of the total time is spent scheduling operations (i.e. deciding the order in which operations <p> Dynamic measurements obtained when scheduling 1327 loops from the Perfect Club [13], SPEC-89 [91], and the Livermore Fortran Kernels <ref> [65] </ref> for the Cy-dra 5 machine [11] indicate that the essential work performed by the contention queries decreases by a factor of 2.76 to 3.30, depending on the functionality required by the scheduler. <p> For the Cydra 5 machine descriptions, we also verified that precisely the same schedules were produced regardless of the machine description used by the compiler when scheduling a benchmark suite of 1327 loops obtained from the Perfect Club [13], SPEC-89 [91], and the Livermore Fortran Kernels <ref> [65] </ref>. <p> The algorithm satisfies the definition of the unrestricted scheduling model since it schedules operations in arbitrary order and may reverse scheduling decisions. We used a benchmark of loops obtained from the Perfect Club [13], SPEC-89 [91], and the Livermore Fortran Kernels <ref> [65] </ref> which consists exclusively of innermost loops with no early exits, no procedure calls, and fewer than 30 basic blocks, as compiled by the Cydra 5 Fortran77 compiler [27]. The input to the scheduling algorithms consists of the Fortran77 compiler intermediate representation after load-store elimination, recurrence back-substitution, and IF-conversion. <p> by II cycles, as shown in in register requirements by increasing SL. 136 4.6 Measurements In this section we investigate the register requirements of the integer and floating point register files for a benchmark of 1327 loops obtained from the Perfect Club [13], SPEC-89 [91], and the Livermore Fortran Kernels <ref> [65] </ref> compiled for the Cydra 5 as described in Section 2.7. We use here the Iterative Modulo Scheduler [78] to produce high quality MRT-schedules, which are used as input to both the optimal stage scheduling algorithm and the stage scheduling heuristics.
Reference: [66] <author> W. M. Meleis and E. S. Davidson. </author> <title> Optimal local allocation for a multiple-issue machine. </title> <booktitle> Proceedings of the International Conference on Supercomputing, </booktitle> <pages> pages 107-116, </pages> <year> 1994. </year>
Reference-contexts: The work on optimal modulo schedulers can also be extended to cover a wider range of machines, such as machines with clusters of functional units. It can also be extended to consider optimal generation of spill code, as proposed for straight line code by Meleis and Davidson <ref> [66] </ref>.
Reference: [67] <author> S.-M. Moon and K. Ebcioglu. </author> <title> An efficient resource-constrained global scheduling technique for superscalar and VLIW processors. </title> <booktitle> Proceedings of the 25th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 55-71, </pages> <month> September </month> <year> 1992. </year>
Reference: [68] <author> T. Muller. </author> <title> Employing finite automata for resource scheduling. </title> <booktitle> Proceedings of the 26th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 12-20, </pages> <year> 1993. </year>
Reference-contexts: As a result, recent advances favor finite-state automaton approaches. In this chapter, however, we propose a reduced reservation table approach that eliminates much of the redundancy and does not suffer from the limitations of the automaton approaches, as detailed below. Proebsting and Fraser [76] as well as Muller <ref> [68] </ref> proposed a contention query module using a finite-state automaton that recognizes all contention-free schedules. The technique proposed by Proebsting and Fraser directly results in minimal finite-state automata [76].
Reference: [69] <author> G. L. Nemhauser and L. A. Wolsey. </author> <title> Integer and Combinatorial Optimization. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: G. 5.3.4 Efficient Formulation of the Scheduling Constraints Solving an integer linear programming model is generally implemented by iteratively solving a linear programming model where additional constraints are introduced (and removed) to force each integer variable to an integer value without omitting from the solution space any optimal (integer) solution <ref> [69] </ref>. A branch-and-bound algorithm can be used to determine which parts of the search space to consider, and each branch-and-bound node is generally evaluated by solving a linear programming model with the original constraints augmented by some additional constraints that force variables to integer values. <p> In this demonstration, we use the following matrix property. Definition A.3 An m fi n integral matrix A is totally unimodular (TU) if the determinant of each square submatrix of A is equal to 0, 1, or -1. The TU property is used in the following theorem <ref> [69, pp. 536 and 541] </ref> to guarantee that the LP-relaxation of an integer linear programming problem results in an integer solutions, if any solution is found: Theorem A.1 The value z LP = minfcx : Ax b; x 2 R n + g produced by an LP-solver integral if m fi <p> Since the vectors [ff 0 fi 0 ] and [1 0] of Problem (A.17) have integer values, it is sufficient to show that the constraint matrix of Problem (A.17) is TU. Showing that a matrix is TU is NP in general <ref> [69, p. 540] </ref>; because of the structure of Problem (A.17). However, we show here that its constraint matrix is always TU. We introduce the following definition and theorems that are used in the demonstration. Definition A.4 Let A be a matrix with entries equal to 0, 1, and -1. <p> The TU-preserving transformations that we used are found in <ref> [69, pp 540] </ref> and described here. <p> We also use a theorem that indicates how network matrices relate to incidence matrices <ref> [69, pp. 546-547] </ref>. Theorem A.5 A network matrix is precisely a matrix whose columns represent the incidence matrix of a graph after one row has been deleted and a number of pivots have been executed until the resulting matrix is of the form [I A].
Reference: [70] <author> Q. Ning and G. R. Gao. </author> <title> A novel framework of register allocation for software pipelining. </title> <booktitle> Twentieth Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, </booktitle> <pages> pages 29-42, </pages> <year> 1993. </year>
Reference-contexts: For example, 67 the work of Govindarajan et al [44] and Wang et al [93] approximates registers by conceptual FIFO buffers, initially proposed by Ning and Gao <ref> [70] </ref>. Unlike registers, conceptual FIFO buffers are reserved for an interval of time that is always an integer multiple of II cycles. Figure 3.3 illustrates the difference between the steady-state register and buffer requirements for the modulo schedule of Example 3.1. Notice in the vr1 buffer. <p> As a result, minimizing Buff is equivalent to minimizing integral MaxLive. Another approximation of the register requirements found in the literature, and used by Ning and Gao <ref> [70] </ref> and Dupont de Dinechin [29], approximates MaxLive by computing the average lifetime requirements, referred to as AvgLive, which corresponds to the sum of all the register lifetimes divided by II. <p> Ning and Gao proposed an algorithm based on a linear programming formulation that results in a schedule with the highest steady-state throughput over all modulo schedules, and minimum average lifetime among such schedules for a machine with an unlimited number of resources <ref> [70] </ref>. In their work, and elsewhere [44][93], the register 162 requirements of a schedule are approximated by conceptual FIFO buffers that are reserved for an interval of time that is a multiple of II. Their scheduling algorithm handles loop iterations with arbitrary dependence graphs. <p> Ning and Gao's polynomial-time algorithm <ref> [70] </ref> has also been used by Dupont de Dinechin [29] to find modulo schedules with low register requirements. <p> The incidence matrices are matrices of size m fi n, where m is the number of vertices and n is the number of edges, i.e. n = jE sched [ E reg j). Ning and Gao <ref> [70] </ref> have shown that the above theorem applies in the case where E sched = E reg . Our proof for the general case where E sched 6= E reg is directly based on their proof with the necessary adaptation for giving it the broader applicability that we require here.
Reference: [71] <author> S. Olariu. </author> <title> Optimal greedy heuristic to color interval graphs. </title> <journal> Information Processing letters, </journal> <volume> 37(1) </volume> <pages> 21-25, </pages> <year> 1991. </year>
Reference-contexts: From the definition of MaxLive, we know that there are no more than MaxLive live values in any cycle of the operation trace t. Finding a register allocation for t can be effectuated by coloring the interval graph associated with t. Olariu <ref> [71] </ref> has shown for interval graphs that a linear-time greedy heuristic is guaranteed to find a minimum coloring, namely a coloring with MaxLive colors.
Reference: [72] <author> J. C. H. Park and M. S. Schlansker. </author> <title> On predicated execution. </title> <type> Technical Report HPL-91-58, </type> <institution> HP Laboratories, </institution> <month> May </month> <year> 1991. </year>
Reference: [73] <author> J. H. Patel and E. S. Davidson. </author> <title> Improving the throughput of a pipeline by insertion of delays. </title> <booktitle> Proceedings of the Third Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 159-164, </pages> <year> 1976. </year>
Reference-contexts: Thus we are free to represent the machine with any set of synthetic resources and usages that preserves the forbidden latencies. Step 2. We build the generating set of maximal resources which is defined as a set of resources that contains all maximal resources associated with the target machine <ref> [73] </ref>. <p> Or conversely, we may say that the machine has excessive resource parallelism for the loop. Patel has shown that for acyclic dependence graphs, that at least one functional unit can always be fully saturated by adding delays to a task schedule <ref> [73] </ref>. If we view a single loop iteration as a task, such delays might have to be applied between operations as well as between the successive pipeline stages used by a single operation.
Reference: [74] <author> K. Paton. </author> <title> An algorithm for finding a fundamental set of cycles of a graph. </title> <journal> Communications of the ACM, </journal> <volume> 12(9) </volume> <pages> 514-518, </pages> <month> September </month> <year> 1969. </year> <month> 264 </month>
Reference-contexts: Compute all s i;j using Equation (4.4) and search for all elementary cycles in the underlying graph of the dependence graph GfV; E sched [ E reg g <ref> [74] </ref>. 2. If the underlying dependence graph is acyclic, the solution that produces the minimum integral MaxLive is obtained by setting the values of all p i;j to zero. 3. Otherwise, build an underlying-cycle constraint for each elementary cycle of the underlying dependence graph using Equations (4.6) and (4.7).
Reference: [75] <author> S. S. Pinter. </author> <title> Register allocation with instruction scheduling: a new approach. </title> <booktitle> Proceedings of the ACM SIGPLAN'93 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 248-257, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: In this scheme, additional scheduling edges would supply some further temporal constraints on the subsequent scheduling. Some edges are guaranteed not to actually constrain the scheduler, as shown by Pinter <ref> [75] </ref>. Others, however, may constrain the scheduler.
Reference: [76] <author> T. A. Proebsting and C. W. Fraser. </author> <title> Detecting pipeline structural hazards quickly. </title> <booktitle> Twenty-First Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, </booktitle> <pages> pages 280-286, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: As a result, recent advances favor finite-state automaton approaches. In this chapter, however, we propose a reduced reservation table approach that eliminates much of the redundancy and does not suffer from the limitations of the automaton approaches, as detailed below. Proebsting and Fraser <ref> [76] </ref> as well as Muller [68] proposed a contention query module using a finite-state automaton that recognizes all contention-free schedules. The technique proposed by Proebsting and Fraser directly results in minimal finite-state automata [76]. <p> Proebsting and Fraser <ref> [76] </ref> as well as Muller [68] proposed a contention query module using a finite-state automaton that recognizes all contention-free schedules. The technique proposed by Proebsting and Fraser directly results in minimal finite-state automata [76]. This approach was recently extended for unrestricted scheduling models by Bala and Rubin using a forward and reverse pair of automata [9]. In their approach, operations considered in order of monotonically increasing (or decreasing) schedule time are quickly scheduled using a forward automaton. <p> A potential problem of this approach is the size of these automata, an issue that is addressed in the literature in the following two ways. First, operations of a target machine can be combined 13 into classes of operations that have compatible resource contentions <ref> [76] </ref>. Second, large automata can be factored into sets of smaller ones [9][68]. However, although factoring reduces the size of the automata, it increases the number of table lookups necessary to process a contention query. <p> Figure 2.1b illustrates this matrix computed for our example machine. While these sets are computed for each operation of the target machine, we need list these sets only for each operation class, as presented by Proebsting and Fraser <ref> [76] </ref>. In general, two operations belong to the same operation class if they have the same sets of forbidden latencies, i.e. operations X and Y belong to the same class if F X;Z = F Y;Z and F Z;X = F Z;Y for each operation Z of the target machine. <p> It is significantly larger than the machine descriptions used in previous studies, e.g. it has 3.5 times more operation classes and 2.4 times more forbidden latencies than the MIPS R3000/R3010 machine description used in <ref> [76] </ref>. Our algorithm reduced this original Cydra 5 machine description in less than 11 minutes on a Sun Sparc-20 workstation. <p> usages / operation 11.0 5.6 5.6 2.4 1.6 Table 2.4: Results for the MIPS R3000/R3010: 15 operation classes, 428 forbidden latencies (all 34). 32 33 34 35 36 Table 2.4 shows the results of the reductions for the MIPS R3000/R3010 [53] using the machine description presented by Proebsting and Fraser <ref> [76] </ref>. Comparing the original description to the 64 bit word bitvector reduction, the reduced machine description decreases the average word usage by a factor of 6.9. Proebsting and Fraser reported a (forward-only) finite-state automaton for this processor with 6175 states [76]. <p> [53] using the machine description presented by Proebsting and Fraser <ref> [76] </ref>. Comparing the original description to the 64 bit word bitvector reduction, the reduced machine description decreases the average word usage by a factor of 6.9. Proebsting and Fraser reported a (forward-only) finite-state automaton for this processor with 6175 states [76].
Reference: [77] <author> B. R. Rau. </author> <title> Iterative Modulo Scheduling: An algorithm for software pipelining loops. </title> <booktitle> Proceedings of the 27th Annual International Symposium on Microarchi-tecture, </booktitle> <pages> pages 63-74, </pages> <month> November </month> <year> 1994. </year>
Reference: [78] <author> B. R. Rau. </author> <title> Iterative Modulo Scheduling. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 24(1) </volume> <pages> 2-64, </pages> <year> 1996. </year>
Reference-contexts: other 50% of the total time is spent scheduling operations (i.e. deciding the order in which operations are scheduled, initiating the queries to the resource model, and enforcing the data dependences), when using a 4 given machine description for the Cydra 5 and our implementation of the Iterative Modulo Scheduler <ref> [78] </ref>. We propose a reduced machine representation that is precise and efficient for a compiler to use in its scheduling algorithms. <p> When using the optimal stage scheduler in conjunction with the register-insensitive Iterative Modulo Scheduler <ref> [78] </ref>, the register requirements decrease by 19.9% on average. Moreover, the average register requirements decrease by 19.8 and 19.2% when our best stage scheduling heuristics with unlimited schedule length and bounded schedule length, respectively, are employed. <p> Moreover, experimental results indicate that software pipelined loops can achieve higher throughput in less compilation time when some limited number of scheduling decisions can be reversed due to violated dependence or resource constraints, as shown by Rau <ref> [78] </ref>. Limited backtracking is used in numerous compilers, e.g. when scheduling software pipelined loops [27][49][78][83][94] and scalar code [59]. The proposed approach also precisely handles basic block boundary conditions, i.e. the dangling resource requirements from predecessor basic blocks. <p> This mechanism is a key component of the scheduling algorithm proposed by Rau to generate high performance software-pipelined loop schedules at a low level of computational complexity <ref> [78] </ref>. In the experiments presented in Section 2.7, this mechanism enables the scheduling algorithm to find schedules with maximal throughput in 95.5% of the loops, versus 93.8% without it. <p> Each entry contains a flag indicating whether the corresponding resource has been reserved by an operation in the current partial schedule. Entries may contain additional fields, such as a field identifying which operation is reserving the corresponding resource, as used in the Iterative Modulo Scheduler algorithm <ref> [78] </ref>, or a field identifying the predicate under which the resource is reserved, as proposed in the Enhanced Modulo Scheduling scheme [94]. <p> Also, some scheduler algorithms already know whether the operation that is currently being assigned to a cycle will conflict with some operations in the partial schedule. For example, the Iterative Modulo Scheduler <ref> [78] </ref> first attempts to schedule an operation in a contention-free cycle; however, if no such cycle is found, it chooses some cycle in which to assign the operation, regardless of the resource conflicts. Thus the assign&free function must always be used. <p> we have selected a state-of-the-art scheduler with low computational complexity that results in high performance code, and is thus likely to be representative of the scheduling algorithms used in future high-performance compilers. 41 We implemented a scheduler for software pipelined loops using the algorithm developed and described by Rau in <ref> [78] </ref>. This algorithm, referred to as the Iterative Modulo Scheduler, exploits the instruction level parallelism present in loop iterations by overlapping the execution of consecutive iterations. This modulo scheduler has been designed to produce schedules with near optimal steady-state throughput while efficiently dealing with realistic machine models. <p> This modulo scheduler has been designed to produce schedules with near optimal steady-state throughput while efficiently dealing with realistic machine models. Experimental findings presented by Rau <ref> [78] </ref> show that the algorithm of this scheduler requires the scheduling of only 59% more operations than does acyclic list scheduling while resulting in schedules that are optimal in II for 96.0% of loops in the benchmark suite described below. <p> The benchmark suite consists of the 1327 loops successfully modulo scheduled by the Cydra 5 Fortran77 compiler, with 1002 from the Perfect Club, 298 from SPEC, and 27 from the Livermore Fortran Kernels. This benchmark of loops was used in <ref> [78] </ref> and was provided to us by Rau. The characteristics of the generated schedules for the 1327 loop benchmark are summarized in Table 2.5. The first two rows indicate the number of operations per loop iteration and the initiation interval of the software pipelined loops. <p> This ratio is within 0.5% of that obtained in <ref> [78] </ref>. A key feature of the Iterative Modulo Scheduler algorithm is that it can reverse a limited number of scheduling decisions. In this chapter, the scheduler may perform up to 6N scheduling decisions, where N is the number of operations in the loop being scheduled. <p> However, since the initiation interval of a modulo schedule must be some integer number of cycles, an integer valued MII can be obtained simply by rounding up to the next larger integer value, by unrolling the loop some number of times, or a combination of both <ref> [78] </ref>. The resource constrained lower bound, ResMII, is determined by accounting for all the resources consumed by each of the operations of an iteration. <p> used x times on a machine with q identical copies of resource x, clearly II cannot be smaller than x=q because of the modulo scheduling constraint. 62 Determining the ResMII lower bound can be difficult, in general, when an operation can be mapped to different opcodes with distinct resource usages <ref> [78] </ref>. The recurrence constrained lower bound, RecMII, is determined by accounting for all the elementary cycles in the dependence graph that are created by recurrences, i.e. loop-carried dependences. <p> Some algorithms treat each step separately; e.g. Eisenbeis's software pipelining approach [40] uses distinct heuristics for Steps 1, 2, and 3 below. Other algorithms combine steps to optimize some combination of their distinct objectives; e.g. Huff's 72 lifetime-sensitive modulo scheduler [49] or Rau's Iterative Modulo Scheduler <ref> [78] </ref> com- bines Steps 1 and 2 below. 1. MRT-scheduling primarily addresses resource constraints and is best implemented by using a modulo reservation table (MRT) [48][73] which contains II rows, one per clock cycle, and one column for each resource at which conflicts may occur. <p> We use here the Iterative Modulo Scheduler <ref> [78] </ref> to produce high quality MRT-schedules, which are used as input to both the optimal stage scheduling algorithm and the stage scheduling heuristics. Thus, in this section, every schedule of a loop shares the same MRT and differs only by the stage in which each operation is placed. <p> We then investigate the effect of increasing II on the register requirements in Section 4.6.3. To facilitate comparison with previous work, the MRT-schedules used in Sections 4.6.1 and 4.6.2 correspond to the schedules computed by Rau and used in <ref> [78] </ref>. In Section 4.6.3 however, we use our implementation of the Iterative Modulo Scheduler to compute the required MRT-schedules, since we also need MRT-schedules with larger initiation intervals. 4.6.1 Schedules with Unlimited Schedule Length We first investigate the register requirements of stage schedules with unlimited schedule length. <p> The high quality of the MRT-schedules is indicated by the fact that 96.0% of the loops scheduled by the Iterative Modulo Scheduler <ref> [78] </ref> achieve M II. Notice the fraction of operations in underlying cycles. If the workload is "equal number of iteration" from each loop in the benchmark, 57.9% of the operations belong to some underlying cycle. <p> The following algorithm finds a schedule with the highest steady state throughput over all modulo schedules, and the minimum register requirements among such schedules: 1. Compute the Minimum Initiation Interval (M II) <ref> [78] </ref> and set the tentative II to M II. 2. Build the integer linear programming system as indicated in Figure 5.4 and search for a solution. 3. If the system built in Step 2 fails to find a feasible solution, increase II by one and return to Step 2. <p> Since the Iterative Modulo Scheduler proposed by Rau <ref> [78] </ref> results in schedules with optimal throughput for 1274 (or 96.0%) of the 1327 loops in the benchmark suite, we only need to investigate here the remaining 53 loops. <p> In particular, the 3*UP+RSS stage scheduling heuristic with bounded schedule length combined with Rau's Iterative Modulo Scheduler <ref> [78] </ref> schedules 92.8% of the 920 loops with no more than 3 additional registers, and 98.3% with no more than 6 additional registers.
Reference: [79] <author> B. R. Rau and J. A. Fisher. </author> <title> Instruction-level parallel processing: History, overview, and perspective. </title> <journal> In The Journal of Supercomputing, </journal> <volume> volume 7, </volume> <pages> pages 9-50, </pages> <year> 1993. </year>
Reference-contexts: In this section, we present some of the approaches that have recently been published in this area. A more complete and historical description of this area can be found in <ref> [79] </ref>. Specific work on optimal sched-ulers for loop code based on linear programming and integer programming models is omitted from this section and is presented in detail in Chapters 4 and 5. This general area of research may be divided into two broad categories.
Reference: [80] <author> B. R. Rau and C. D. Glaeser. </author> <title> Some scheduling techniques and an easily schedula-ble horizontal architecture for high performance scientific computing. </title> <booktitle> Fourteenth Annual Workshop on Microprogramming, </booktitle> <pages> pages 183-198, </pages> <month> October </month> <year> 1981. </year>
Reference-contexts: Register-Sensitive Modulo Scheduling Modulo scheduling is a software pipelining technique that achieves higher levels of instruction level parallelism by overlapping the execution of consecutive loop iterations <ref> [80] </ref>. Modulo scheduling uses the same schedule for each iteration of a loop and initiates successive iterations at a constant rate, i.e. one Initiation Interval (II clock cycles) apart. Modulo scheduling results in high performance code, but increased register requirements due to higher levels of concurrency [64]. <p> With sufficient overlap, some machine resources can be fully utilized, resulting in a schedule with maximum steady state throughput. Modulo scheduling is a software pipelining technique that results in high performance code while requiring only modest compilation time by restricting the scheduling space <ref> [80] </ref>. Modulo scheduling uses the same schedule for each iteration of a loop and initiates successive iterations at a constant rate, i.e. one Initiation Interval (II clock cycles) apart. <p> Figure 3.1b presents a valid schedule for one iteration of this simple kernel. Modulo scheduling exploits the instruction level parallelism present among the iterations of a loop by overlapping the execution of consecutive iterations <ref> [80] </ref>. Modulo scheduling restricts the space of software pipelined schedules by requiring every loop iteration to use the same schedule and by initiating consecutive loop iterations at a constant rate, i.e. one Initiation Interval (II clock cycles) apart. <p> Schedule lengths reported in our measurements, however, are SL, not SL 0 . As steady-state throughput is simply the reciprocal of II, choosing the minimum feasible II achieves the highest possible steady-state performance. The initiation interval is bounded by the minimum initiation interval (MII) <ref> [80] </ref>, which is a lower bound on the smallest feasible value of II for which a modulo schedule can be found. This lower bound is constrained either by critical resources being fully utilized or by critical loop-carried dependence cycles. In general, MII may be noninteger. <p> The advantage of this approach is that the increased flexibility in the scheduling domain may result in loop schedules with the highest achievable performance. This approach was named by Charlesworth [22] for its analogy to hardware pipelines and was formulated by Rau <ref> [80] </ref> for simple loop iterations without conditional statements. Since then, a large body of research has sought efficient software pipelining algorithms that achieve high performance code for a wide range of loops, e.g. loops with conditionals, while loops, and nested loops. <p> This approach achieves two major goals. First, it handles queries 236 significantly faster: for example, dynamic measurements obtained when scheduling the 1327-loop benchmark for the Cydra 5 using the Iterative Modulo Scheduler <ref> [80] </ref> indicates an average decrease in work units (i.e. the essential work performed by the contention queries) by a factor of 2.76, resulting in queries that complete on average in 58.2% of the time when a reduced machine description is used instead of the original machine representation. <p> When using the optimal stage scheduler in conjunction with the register-insensitive Iterative Modulo Scheduler <ref> [80] </ref>, the average register requirements decrease by 19.9%. Moreover, the average register requirements decrease by 19.8 and 19.2% when our best stage scheduling heuristics with unlimited schedule length and bounded schedule length, respectively, are employed.
Reference: [81] <author> B. R. Rau, C. D. Glaeser, and R. L. </author> <title> Picard. Efficient code generation for horizontal architectures: Compiler techniques and architecture support. </title> <booktitle> Proceedings of the Ninth Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 131-139, </pages> <year> 1982. </year>
Reference: [82] <author> B. R. Rau, M. Lee, P. P. Tirumalai, and M. S. Schlansker. </author> <title> Register allocation for software pipelined loops. </title> <booktitle> Proceedings of the ACM SIGPLAN'92 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 283-299, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: Modulo scheduling has also been extended to 57 a large variety of loops with early exits, such as while loops [87][89]. Furthermore, the code expansion due to modulo scheduling can be eliminated by using special hardware, e.g. support for rotating register files and predicated execution <ref> [82] </ref>. Since modulo scheduling exploits a higher level of parallelism, it results in higher register requirements because more values are needed to support more concurrent operations. This effect is inherent to parallelism in execution and will be exacerbated by wider machines and higher latency pipelines [64]. <p> The MRT can also model specialized resources by allowing only certain types of operations in each column. The schedule of an iteration can be divided into stages of II cycles each. The number of stages in an iteration is referred to as the stage count (SC) <ref> [82] </ref> and corresponds to the maximum number of concurrent iterations in a modulo schedule. notice that indeed the number of concurrent iterations never exceeds 4 in the execution trace depicted in Figure 3.1c. <p> In this dissertation, we quantify the register requirements of a modulo schedule by computing MaxLive, the maximum number of live values in any single cycle of the loop schedule <ref> [82] </ref>. MaxLive corresponds to the minimum number of registers required to generate spill-free code. MaxLive is a tight bound, i.e. we can guarantee that there is a feasible register allocation with exactly MaxLive registers, as indicated by following theorem. <p> This register allocation uses precisely MaxLive registers. 2 Note that the degree of unrolling required in Theorem 3.1 may be impractical. However, in practice, Rau et al <ref> [82] </ref> have shown that register allocation algorithms for modulo schedules can achieve register allocations that are within one register of the MaxLive lower bound for the vast majority of their modulo scheduled loops without loop unrolling on machines with rotating register files and predicated execution to support modulo scheduling. <p> Similarly, the 1 Because a new iteration is initiated every II cycles, the minimum degree of unrolling, u, is determined by the longest lifetime, i.e. u = d l II e where l is the length in cycles of the longest lifetime among the loop variant lifetimes <ref> [82] </ref>. 66 steady-state register requirements can be quickly computed by wrapping the lifetimes for one iteration around a vector of length II. <p> A detailed comparison of these approaches with our approach for register-sensitive modulo scheduling is provided in Chapter 4. Register allocation algorithms for modulo schedules have been investigated by Rau et al <ref> [82] </ref>. Their algorithm achieves register allocations that are within one register of the MaxLive lower bound for the vast majority of their modulo-scheduled loops on machines with rotating register files and predicated execution to support modulo-scheduling. <p> This additional notion of time is particularly useful for the live ranges of a loop, where live ranges may cross the boundary of an iteration. Another approach, investigated by Rau et al <ref> [82] </ref>, proposes 211 a general framework for the allocation of registers in software pipelined loops for various code generation and hardware support schemes. <p> However, several register allocators depart from the graph coloring method [46]<ref> [82] </ref> as graph coloring methods do not provide a notion of time that is particularly useful for the live ranges of a loop, which may cross the boundary of an iteration. Also, non-traditional constraints such as the one presented in [82] to support various code generation and hardware support schemes are difficult to express within the graph coloring framework.
Reference: [83] <author> J. R. Ruttenberg, G. R. Gao, and A. Stoutchinin. </author> <title> Software pipelining showdown: Optimal vs. heuristic methods in a production compiler. </title> <booktitle> Proceedings of the ACM SIGPLAN'96 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 1-11, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Our second contribution is a more structured formulation of the modulo scheduling solution space. This more efficient formulation addresses a major concern with modulo schedulers that are based on integer linear programming solvers <ref> [83] </ref>, which is their traditionally high execution time.
Reference: [84] <author> M. S. Schlansker. </author> <type> Personal communication. </type> <month> June </month> <year> 1995. </year>
Reference-contexts: The original machine description used by the Cydra 5 Fortran77 compiler [27] was manually optimized, i.e. some physical resources were eliminated from the machine description as they did not introduce any new forbidden latencies <ref> [84] </ref>. This description models 56 resources and 152 distinct patterns of resource usages, resulting in 52 distinct operation classes with 10223 forbidden latencies.
Reference: [85] <author> M. S. Schlansker and V. Kathail. </author> <title> Acceleration of first and higher order recurrences on processors with instruction level parallelism. </title> <booktitle> Proceedings of the Languages and Compilers for Parallel Computing, 6th International Workshop, </booktitle> <month> August </month> <year> 1993. </year>
Reference-contexts: Furthermore, several optimizations were proposed to successfully increase the available instruction level parallelism for modulo schedules, such as elimination of redundant loads and stores across loop iterations [27] as well as transformations to reduce the critical paths caused by data dependence <ref> [85] </ref> or control dependence [87]. Recently, optimizations based on unrolling a loop body 82 before applying a modulo scheduling algorithm have also been investigated [56]. Tech--niques based on Petri nets have also been proposed to determine a suitable degree of unrolling for modulo schedules [5].
Reference: [86] <author> M. S. Schlansker and V. Kathail. </author> <title> Critical path reduction for scalar code. </title> <booktitle> Proceedings of the 28th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 57-69, </pages> <month> November </month> <year> 1995. </year> <month> 265 </month>
Reference: [87] <author> M. S. Schlansker, V. Kathail, and S. Anik. </author> <title> Height reduction of control recurrences for ILP processors. </title> <booktitle> Proceedings of the 27th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 40-51, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: such as while loops <ref> [87] </ref>[89]. Furthermore, several optimizations were proposed to successfully increase the available instruction level parallelism for modulo schedules, such as elimination of redundant loads and stores across loop iterations [27] as well as transformations to reduce the critical paths caused by data dependence [85] or control dependence [87]. Recently, optimizations based on unrolling a loop body 82 before applying a modulo scheduling algorithm have also been investigated [56]. Tech--niques based on Petri nets have also been proposed to determine a suitable degree of unrolling for modulo schedules [5]. <p> 2 , (x 1 ^ p 0 ) if (i&gt;10) S4; p4 (u) = (i&gt;10) ?p3; p 4 , (x 2 ^ p 3 ) S4 ?p4; S5 ?p5; x 2 ) x 3 Table 6.1: Extracting P-facts. types are required to evaluate several comparisons in parallel, as advocated in <ref> [87] </ref>. For example, the or type may be used to implement a "wired-or" function of several comparisons executed in parallel.
Reference: [88] <author> E. Su, A. Lain, S. Ramaswamy, D. J. Palermo, E. W. Hodges IV, and P. Banerjee. </author> <title> Advanced compilation techniques in the PARADIGM compiler for distributed-memory multicomputers. </title> <booktitle> Proceedings of the International Conference on Supercomputing, </booktitle> <pages> pages 424-433, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: This framework is applicable either before or after scheduling. The current implementation relies on a symbolic package to generate accurate results, using an approach similar to the one taken in the PARADIGM compiler <ref> [88] </ref>. For register allocators based on the classical graph coloring method, originally proposed by Chaitin [15][18], register allocation for predicted codes can be simply achieved by using a refined interference graph instead of the conventional one. However, several register allocators depart from the graph coloring method: e.g.
Reference: [89] <author> P. P. Tirumalai, M. Lee, and M. S. Schlansker. </author> <title> Parallelization of loops with exits on pipelined architectures. </title> <booktitle> Proceedings of Supercomputing '90, </booktitle> <pages> pages 200-212, </pages> <month> November </month> <year> 1990. </year>
Reference: [90] <author> P. Tu and D Padua. </author> <title> Gated SSA-based demand-driven symbolic analysis for parallelizing compilers. </title> <booktitle> Proceedings of the International Conference on Supercomputing, </booktitle> <pages> pages 414-423, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: A complementary approach is taken with the Gated Single Assignment (GSA) form [10], where precise predicate information is embedded in the dataflow graph. This approach is used in the Polaris parallelizing compiler to refine data and memory dependence analysis and to aid loop parallelization <ref> [90] </ref>. In this chapter, we illustrate the impact of predicated code on the register requirements and outline our general framework in Section 6.1. The bundling heuristics after scheduling and before scheduling are respectively introduced in Sections 6.2 and 6.3. <p> This approach presents none of the cited drawbacks of the previous approaches; moreover, precise information about predicate expressions of live ranges may also be extremely useful to other optimizations, such as memory disambiguation <ref> [90] </ref>. The drawback of this approach is its reliance on a more expensive analysis, partly due to the use of a symbolic package to detect predicate expression disjointness and partly due to the computation of a more complicated interference relation.
Reference: [91] <author> J. Uniejewski. </author> <title> SPEC Benchmark Suite: Designed for today's advanced system. </title> <journal> SPEC Newsletter, </journal> <month> Fall </month> <year> 1989. </year>
Reference-contexts: Efficient representations are important since high performance compilers spend a significant amount of compilation time scheduling operations, and thus testing for potential resource contentions. When a benchmark suite of 1327 loops from the Perfect Club [13], SPEC-89 <ref> [91] </ref>, and the Livermore Fortran Kernels [65] is scheduled for the Cydra 5 machine [11][27], approximately 50% of the total time is spent modeling the resources (i.e. answering queries such as "can this operation be scheduled in this cycle"); the other 50% of the total time is spent scheduling operations (i.e. <p> Dynamic measurements obtained when scheduling 1327 loops from the Perfect Club [13], SPEC-89 <ref> [91] </ref>, and the Livermore Fortran Kernels [65] for the Cy-dra 5 machine [11] indicate that the essential work performed by the contention queries decreases by a factor of 2.76 to 3.30, depending on the functionality required by the scheduler. <p> For the Cydra 5 machine descriptions, we also verified that precisely the same schedules were produced regardless of the machine description used by the compiler when scheduling a benchmark suite of 1327 loops obtained from the Perfect Club [13], SPEC-89 <ref> [91] </ref>, and the Livermore Fortran Kernels [65]. <p> The algorithm satisfies the definition of the unrestricted scheduling model since it schedules operations in arbitrary order and may reverse scheduling decisions. We used a benchmark of loops obtained from the Perfect Club [13], SPEC-89 <ref> [91] </ref>, and the Livermore Fortran Kernels [65] which consists exclusively of innermost loops with no early exits, no procedure calls, and fewer than 30 basic blocks, as compiled by the Cydra 5 Fortran77 compiler [27]. <p> (4; 2) by one stage, i.e. by II cycles, as shown in in register requirements by increasing SL. 136 4.6 Measurements In this section we investigate the register requirements of the integer and floating point register files for a benchmark of 1327 loops obtained from the Perfect Club [13], SPEC-89 <ref> [91] </ref>, and the Livermore Fortran Kernels [65] compiled for the Cydra 5 as described in Section 2.7. We use here the Iterative Modulo Scheduler [78] to produce high quality MRT-schedules, which are used as input to both the optimal stage scheduling algorithm and the stage scheduling heuristics.
Reference: [92] <author> J. Wang, C. Eisenbeis, M. Jourdan, and B. Su. </author> <title> Decomposed software pipelining: A new perspective and a new approach. </title> <journal> In International Journal of Parallel Programming, </journal> <volume> volume 22, </volume> <pages> pages 357-379, </pages> <year> 1994. </year>
Reference-contexts: This result was obtained for directed acyclic dependence graphs on machines with simple reservation tables, i.e. each type of operation reserves one resource for a specifiable duration. The work of Wang et al <ref> [92] </ref> has also investigated a two step heuristic to modulo scheduling. Following the publication of our stage scheduler for minimum register requirements [35], Wang et al presented a stage scheduler that minimizes the buffer requirements [93].
Reference: [93] <author> J. Wang, A. Krall, and M.A. Ertl. </author> <title> Decomposed software pipelining with reduced register requirement. </title> <booktitle> In Proceedings of the International Conference on Parallel Architecture and Compiler Techniques, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: Prior to our work, modulo scheduling algorithms based on linear programming or integer linear programming formulations have not directly minimized MaxLive, but have instead minimized approximations of the register requirements. For example, 67 the work of Govindarajan et al [44] and Wang et al <ref> [93] </ref> approximates registers by conceptual FIFO buffers, initially proposed by Ning and Gao [70]. Unlike registers, conceptual FIFO buffers are reserved for an interval of time that is always an integer multiple of II cycles. <p> The work of Wang et al [92] has also investigated a two step heuristic to modulo scheduling. Following the publication of our stage scheduler for minimum register requirements [35], Wang et al presented a stage scheduler that minimizes the buffer requirements <ref> [93] </ref>. As presented in Section 3.2, buffers only approximate registers in that a buffer must be reserved for a time interval that is an integer multiple of II cycles, whereas registers may be reserved for an arbitrary number of cycles.
Reference: [94] <author> N. J. Warter. </author> <title> Modulo Scheduling with Isomorphic Control Transformations. </title> <type> PhD thesis, </type> <institution> Department of Electrical and Computer Engineering, University of Illi-nois, Urbana, IL, </institution> <year> 1994. </year>
Reference-contexts: Entries may contain additional fields, such as a field identifying which operation is reserving the corresponding resource, as used in the Iterative Modulo Scheduler algorithm [78], or a field identifying the predicate under which the resource is reserved, as proposed in the Enhanced Modulo Scheduling scheme <ref> [94] </ref>. Because the number of entries tested to detect resource contentions is proportional to the number of resource usages over all reduced reservation tables, the primary objective of the selection heuristic is to minimize the number of resource usages in the reduced machine description. <p> Experimental evidence gathered by Warter <ref> [94] </ref> indicates that the enhanced modulo scheduling technique achieves lower initiation intervals. The modulo scheduling approach was also extended for loops with conditional exits, such as while loops [87][89].
Reference: [95] <author> N. J. Warter, G. E. Haab, K. Subramanian, and J. W. Bockhaus. </author> <title> Enhanced Modulo Scheduling for loops with conditional branches. </title> <booktitle> Proceedings of the 25th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 170-179, </pages> <month> Decem-ber </month> <year> 1992. </year>
Reference-contexts: The scope of modulo scheduling has been widened to a large variety of loops. Loops with conditional statements are handled using hierarchical reduction [55] or IF- and Reverse-IF-conversion <ref> [95] </ref>. Modulo scheduling has also been extended to 57 a large variety of loops with early exits, such as while loops [87][89]. Furthermore, the code expansion due to modulo scheduling can be eliminated by using special hardware, e.g. support for rotating register files and predicated execution [82]. <p> This information is used to refine dataflow analysis, optimization, scheduling, and allocation in presence of hyperblocks. Additionally, this information is used to conditionally reserve functional units when modulo scheduling under the Reverse-IF 212 Conversion scheme <ref> [95] </ref>. A complementary approach is taken with the Gated Single Assignment (GSA) form [10], where precise predicate information is embedded in the dataflow graph. This approach is used in the Polaris parallelizing compiler to refine data and memory dependence analysis and to aid loop parallelization [90].
Reference: [96] <author> N. J. Warter-Perez and N Partamian. </author> <title> Modulo scheduling with multiple initiation intervals. </title> <booktitle> Proceedings of the 28th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 111-118, </pages> <month> November </month> <year> 1995. </year> <month> 266 </month>
Reference-contexts: However, this assumption may result in suboptimal schedules in the presence of unbalanced conditional statements in a loop body since loop iterations are initiated every II cycles regardless of whether a short or long conditional statement is executed. This performance issue was recently addressed by Warter-Perez and Partamian <ref> [96] </ref> where the modulo scheduling approach was extended to multi 83 ple initiation intervals for machines supporting predicated operations and compound predicates [96]. They report performance improvements of up to 25%, 5% on average, for a small benchmark of loops from the Perfect benchmark suite. <p> This performance issue was recently addressed by Warter-Perez and Partamian <ref> [96] </ref> where the modulo scheduling approach was extended to multi 83 ple initiation intervals for machines supporting predicated operations and compound predicates [96]. They report performance improvements of up to 25%, 5% on average, for a small benchmark of loops from the Perfect benchmark suite. Several alternative approaches to modulo scheduling have been proposed to address the above limitation, i.e. suboptimal performance due to a unique initiation interval per loop schedule.
References-found: 96

