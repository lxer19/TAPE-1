URL: ftp://ftp.wins.uva.nl/pub/computer-systems/aut-sys/reports/Kav96.ps.gz
Refering-URL: http://www.fwi.uva.nl/research/neuro/publications/publications.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> J.S. Albus. </author> <title> A new approach to manipulator control: The cerebellar model articulation controller (cmac). </title> <journal> Transactions of the ASME, Journal of Dynamics Systems Measurement and Control, </journal> <volume> 97 </volume> <pages> 220-227, </pages> <year> 1975. </year>
Reference-contexts: This discussion comes next. 3.7 CMAC 3.7.1 Introduction CMAC (Cerebellar Model Articulation controller) is a type of neural network introduced by Al-bus <ref> [1] </ref> to explain information-processing characteristics of the cerebellum in the brain, which is the part of the brain controlling motor activities of the body. CMAC learns the association between its multidimensional input and a single output value by using a kind of look-up table with local interpolation. <p> Based on what we just discussed we choose a 2-D CMAC. We take each dimension to have 10 weights as shown in Figure (3.3). Also assume that our inputs are scaled such that each of the components has a value in the range <ref> [1; 10] </ref>, this could be easily done provided we know the actual ranges for each of the coordinates. This setup could be shown more precisely as follows: Input Space : (s 1 ; s 2 ) where s 1 ; s 2 2 [1; 10] Weight Space : w xy for <p> components has a value in the range <ref> [1; 10] </ref>, this could be easily done provided we know the actual ranges for each of the coordinates. This setup could be shown more precisely as follows: Input Space : (s 1 ; s 2 ) where s 1 ; s 2 2 [1; 10] Weight Space : w xy for x; y = 1; 2; : : : ; 10 The mapping procedure of input points to the weights of the CMAC could be then explained as follows: (s 1 ; s 2 ) ! w xy where x = ceiling (s 1
Reference: [2] <author> A.G. Barto. </author> <title> Learning by statistical cooperation of self-interested neuron-like computing elements. </title> <journal> Human Neurobiology, </journal> <volume> 4 </volume> <pages> 229-256, </pages> <year> 1985. </year>
Reference-contexts: In the rest of this chapter this approach to learning will be explained in relation with artificial neural networks. For further discussions on reinforcement learning, the following material could be used: Barto [3], <ref> [2] </ref> and Williams [33]. 9 3.3 Reinforcement Learning Supervised learning, learning with a teacher, is probably the most common learning method used in the field of artificial neural networks. In this approach a certain input-output mapping is learned by some examples which are presented to the network by the teacher.
Reference: [3] <author> A.G. Barto. </author> <title> Reinforcement learning and adaptive critic method. </title> <booktitle> In Handbook of Intelligence control, Neural, Fuzzy and Adaptive approaches, </booktitle> <pages> pages 469-491. </pages> <publisher> Van Nostrand Reinhold, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: In the rest of this chapter this approach to learning will be explained in relation with artificial neural networks. For further discussions on reinforcement learning, the following material could be used: Barto <ref> [3] </ref>, [2] and Williams [33]. 9 3.3 Reinforcement Learning Supervised learning, learning with a teacher, is probably the most common learning method used in the field of artificial neural networks. <p> Now if the system is to improve its performance, the point corresponding to its state on the performance surface should move towards higher points <ref> [3] </ref>. The difference between learning tasks is in the type of information available to the system about the characteristics of this performance surface.
Reference: [4] <author> A.G. Barto, R.S. Sutton, and C.J.C.H. Watkins. </author> <title> Neuronlike adaptive elements that can solve difficult learning problems. </title> <journal> IEEE Transactions, </journal> <volume> 13 </volume> <pages> 834-846, </pages> <year> 1983. </year>
Reference-contexts: Now we are going to discuss some popular designs of RL-Controllers. 3.5 Reinforcement Learning Methods 3.5.1 Adaptive Critic Method (discrete) Adaptive critic was an early method of reinforcement learning presented by Barto, Sutton and Anderson <ref> [4] </ref> which they used in their classic task of pole-balancing. The adaptive critic learning systems contain two parts. The 'critic' which has the task of predicting future costs. And the 'actor' which produces actions at each state x.
Reference: [5] <author> D. Burghes and A. Graham. </author> <title> Introduction to control theory including optimal control. </title> <publisher> Ellis Horwood Ltd., Publishers, </publisher> <address> West Sussex, </address> <year> 1980. </year>
Reference-contexts: Then in Section 2.7 some control laws are presented and discussed. The contents of this chapter are mainly based on the material from a number of books. These are: Amerongen et al. [10], Narendra & Annaswamy [19], Leigh [15], Burghes & Graham <ref> [5] </ref> and Dorst [8]. Of course, explicit references will be provided in the text when necessary. 2.2 System A system is defined as an aggregation of objects united by some kind of interaction or interdependence.
Reference: [6] <author> R.H. Crites and A.G. Barto. </author> <title> Improving elavator performance using reinforcement learning. </title> <institution> University of Massachusetts, Computer Science Department, </institution> <year> 1995. </year>
Reference-contexts: The controllers will be then trying to become experts in their tasks and at the same time cooperate with each other through the reinforcement signals they receive. The former is more of a coordinated control and the latter cooperated control. In a recent work, Crites & Barto <ref> [6] </ref> showed that the parallel control results in better performance when applied to an elevator dispatching problem. In many real-world tasks involving real people, coordination is usually regarded more important than cooperation, but is it also the case with the autonomous agents? 53
Reference: [7] <author> T.J.J. Van den Boom. </author> <title> MIMO system identification for H 1 robust control, </title> <type> pages 161-179. </type> <institution> Delft University of Technology, Department of Electrical Engineering, </institution> <year> 1992. </year>
Reference-contexts: Variable delay caused by the transfer time of water to each of the top or lower vessels. In this thesis we will be dealing with a simplified version of the water vessel process consisting only of a single vessel as shown in Figure 1.2. h p Boom <ref> [7] </ref> discusses a simple nonlinear approximation of the water vessel process which is used in our experiments and will be explained in Section 4.2. <p> The nonlinear differential equation of the single water vessel system when linearized is reduced to a first order equation as shown by Boom <ref> [7] </ref>. 6 x (t) 2.6 Adaptive Systems Many of the real-world processes are of a nonlinear nature. For example in most of the mechanical devices this is caused by highly nonlinear friction forces. The exact parameters of these processes are usually not known to us. <p> and compared. 4.2 A Model of The Process d i and d o denoting the diameters of the vessel at its top and exit. h = : 0 d i = 8:0 cm The following differential equation is used for the purpose of modeling the system as described in Boom <ref> [7] </ref>: dh = A i 25 where h represents the height of the water in the vessel at time t and A i the area of the vessel at the top and f i and f o resp. the inflow and the outflow of the vessel.
Reference: [8] <author> L. Dorst. </author> <title> An introduction to robotics for the computer science. </title> <institution> University of Amsterdam, Department of Computer Science and Mathematics, </institution> <year> 1992. </year>
Reference-contexts: Then in Section 2.7 some control laws are presented and discussed. The contents of this chapter are mainly based on the material from a number of books. These are: Amerongen et al. [10], Narendra & Annaswamy [19], Leigh [15], Burghes & Graham [5] and Dorst <ref> [8] </ref>. Of course, explicit references will be provided in the text when necessary. 2.2 System A system is defined as an aggregation of objects united by some kind of interaction or interdependence.
Reference: [9] <editor> R.F. Drenick and R.A. Shahbender. </editor> <title> Adaptive servomechanisms. </title> <journal> AIEE Transactions, </journal> <volume> 76 </volume> <pages> 286-292, </pages> <year> 1957. </year>
Reference-contexts: Adaptive control refers to the control of this partially known systems. The term adaptive system was introduced by Drenick & Shahbender <ref> [9] </ref> in control theory to represent control systems that monitor their own performance and adjust their parameters in the direction of better performance.
Reference: [10] <editor> J. Van Amerongen et al. </editor> <publisher> Regeltechniek (dutch). Open Universiteit, </publisher> <year> 1992. </year> <note> ISBN 90-358-0705-7. </note>
Reference-contexts: It starts with the definitions of some elementary terms from control theory. Then in Section 2.7 some control laws are presented and discussed. The contents of this chapter are mainly based on the material from a number of books. These are: Amerongen et al. <ref> [10] </ref>, Narendra & Annaswamy [19], Leigh [15], Burghes & Graham [5] and Dorst [8]. Of course, explicit references will be provided in the text when necessary. 2.2 System A system is defined as an aggregation of objects united by some kind of interaction or interdependence. <p> Based on what we just discussed we choose a 2-D CMAC. We take each dimension to have 10 weights as shown in Figure (3.3). Also assume that our inputs are scaled such that each of the components has a value in the range <ref> [1; 10] </ref>, this could be easily done provided we know the actual ranges for each of the coordinates. This setup could be shown more precisely as follows: Input Space : (s 1 ; s 2 ) where s 1 ; s 2 2 [1; 10] Weight Space : w xy for <p> components has a value in the range <ref> [1; 10] </ref>, this could be easily done provided we know the actual ranges for each of the coordinates. This setup could be shown more precisely as follows: Input Space : (s 1 ; s 2 ) where s 1 ; s 2 2 [1; 10] Weight Space : w xy for x; y = 1; 2; : : : ; 10 The mapping procedure of input points to the weights of the CMAC could be then explained as follows: (s 1 ; s 2 ) ! w xy where x = ceiling (s 1
Reference: [11] <author> V.W. Eveleigh. </author> <title> Adaptive control and optimization techniques. </title> <publisher> McGraw-Hill Book Company, </publisher> <address> New York, </address> <year> 1967. </year>
Reference-contexts: The term adaptive system was introduced by Drenick & Shahbender [9] in control theory to represent control systems that monitor their own performance and adjust their parameters in the direction of better performance. The following definition of an adaptive system is taken from Eveleigh <ref> [11] </ref>: Adaptive System is a system which is provided with a means of continuously monitoring its own performance in relation to a given figure of merit or optimal condition and means of modifying its own parameters by a closed-loop action so as to approach this optimum. 2.7 Control Laws The controller
Reference: [12] <author> S. Haykin. </author> <title> Neural Networks- A Comprehensive Foundation. </title> <publisher> Macmillan Publishing Company, </publisher> <year> 1994. </year>
Reference-contexts: The learning involves adjusting the weights of the neural network as a response to the data presented to it. For a detailed discussion of different designs and learning methods of neural networks the following books are good sources: Haykin <ref> [12] </ref>; Hertz, Krogh & Palmer [13]; Rumelhart & McClelland [21]. Some of the well-known characteristics of artificial neural networks are their ability to learn mappings based on input-output examples and to generalize upon them. Robustness and fault-tolerance are also some other useful properties of neural networks.
Reference: [13] <author> J. Hertz, A. Krogh, and R.G. Palmer. </author> <title> Introduction to the theory of neural computation. </title> <publisher> Addison Wesley, </publisher> <year> 1991. </year>
Reference-contexts: The learning involves adjusting the weights of the neural network as a response to the data presented to it. For a detailed discussion of different designs and learning methods of neural networks the following books are good sources: Haykin [12]; Hertz, Krogh & Palmer <ref> [13] </ref>; Rumelhart & McClelland [21]. Some of the well-known characteristics of artificial neural networks are their ability to learn mappings based on input-output examples and to generalize upon them. Robustness and fault-tolerance are also some other useful properties of neural networks.
Reference: [14] <author> S.S. Keerthi and B. Ravindran. </author> <title> A tutorial survey of reinforcement learning. </title> <institution> Indian Institute of Science, Department of Computer Science and Automation, Bangalore, </institution> <year> 1994. </year>
Reference-contexts: And of course we assume that no model of the process is available and the learning occurs real-time without it. The material used in the following two sections are based on the works of Sathiya & Ravin-dran <ref> [14] </ref>, Sutton [23][24] and Watkins [27]. References will be provided explicitly where necessary. 3.4 Evaluation Function Recall that the goal of reinforcement learning is to find a policy for selecting actions in a way that the selected sequence of actions will be optimal according to a certain performance measure.
Reference: [15] <author> J.R. Leigh. </author> <title> Essentials of nonlinear control theory. </title> <publisher> Peter Peregrinus Ltd., </publisher> <address> London, </address> <year> 1983. </year>
Reference-contexts: Then in Section 2.7 some control laws are presented and discussed. The contents of this chapter are mainly based on the material from a number of books. These are: Amerongen et al. [10], Narendra & Annaswamy [19], Leigh <ref> [15] </ref>, Burghes & Graham [5] and Dorst [8]. Of course, explicit references will be provided in the text when necessary. 2.2 System A system is defined as an aggregation of objects united by some kind of interaction or interdependence. <p> We will talk more about these aspects in Section 2.7 when we discuss different control laws. 2.5 Linearity and Nonlinearity Before defining the linearity of a system let's see when a function is called linear. A function f is said to be linear if the following conditions hold <ref> [15] </ref>: 1. Homogeneity Condition: f (ffx) = fff (x) for any x in the domain of f and for any ff; (2.1) 2.
Reference: [16] <author> W.T. Miller, R.P. Hewes, F.H. Glanz, and L.G. Kraft. </author> <title> Real time control of an industrial manipulator using a neural network based learning controller. </title> <journal> IEEE Journal of Robotics Research, </journal> <volume> 6.2:84-98, </volume> <year> 1987. </year> <month> 54 </month>
Reference-contexts: CMAC learns the association between its multidimensional input and a single output value by using a kind of look-up table with local interpolation. The fast convergence and high accuracy of the CMAC have proven it to be quite successful when used in on-line learning controlling systems (Miller <ref> [16] </ref>). In Section 3.7.4 more of these characteristics will be discussed. But we will start with explaining the CMAC's structure in some more detail. 3.7.2 Discrete CMAC Let's define a function f : R k ! R.
Reference: [17] <author> M. Minsky and S. Papert. </author> <title> Perceptrons: An introduction to Computational Geometry. </title> <publisher> The M.I.T. Press, </publisher> <year> 1969. </year>
Reference-contexts: The research in this field resulted in building simple artificial neural networks called perceptrons in late 1950s (Rosenblatt [20]). However, near the end of 1960s the limitations on the performance of perceptrons on one hand (Minsky & Papert <ref> [17] </ref>) and the lack of proper technology on the other, caused the research to slow down in this field. Little progress was made until 1980s, when the availability of powerful microprocessors and important theoretical breakthroughs opened new doors for artificial neural networks.
Reference: [18] <editor> M.L. Minsky. </editor> <booktitle> Steps toward artificial intelligence. In Proceedings of Institute of Radio Engineers, </booktitle> <pages> pages 8-30, </pages> <year> 1961. </year>
Reference-contexts: Despite all the controversies, this theory has been one of the most influential learning theories in psychology because of its common-sense approach. The term Reinforcement Learning however, is never used in psychology. We could trace its usage back to Minsky <ref> [18] </ref>, Waltz and Fu [26] in their studies of learning in the fields of AI and control theory, inspired by animal learning. In the rest of this chapter this approach to learning will be explained in relation with artificial neural networks.
Reference: [19] <author> K.S. Narendra and A.M. Annaswamy. </author> <title> Stable Adaptive Systems. </title> <publisher> Prentice-Hall Inc., Englewood Cliffs, </publisher> <editor> N. J. </editor> <volume> 07632, </volume> <year> 1989. </year>
Reference-contexts: It starts with the definitions of some elementary terms from control theory. Then in Section 2.7 some control laws are presented and discussed. The contents of this chapter are mainly based on the material from a number of books. These are: Amerongen et al. [10], Narendra & Annaswamy <ref> [19] </ref>, Leigh [15], Burghes & Graham [5] and Dorst [8]. Of course, explicit references will be provided in the text when necessary. 2.2 System A system is defined as an aggregation of objects united by some kind of interaction or interdependence. <p> Of course, explicit references will be provided in the text when necessary. 2.2 System A system is defined as an aggregation of objects united by some kind of interaction or interdependence. When one or more aspects of the system change with time, it is generally called a dynamical system <ref> [19] </ref>. To analyze a system, we should first make sure what its relevant inputs and outputs are and how they are related to each other. The behaviour of a dynamic system is usually characterized by differential equations that are derived from the properties of the components involved and their interactions.
Reference: [20] <author> F. Rosenblatt. </author> <title> Principles of Neurodynamics. </title> <publisher> Spartan Books, </publisher> <address> New York, </address> <year> 1959. </year>
Reference-contexts: The idea emerged that the network of interconnected neurons in the brain could be simulated by electronic circuits able to perform simple computational tasks. The research in this field resulted in building simple artificial neural networks called perceptrons in late 1950s (Rosenblatt <ref> [20] </ref>). However, near the end of 1960s the limitations on the performance of perceptrons on one hand (Minsky & Papert [17]) and the lack of proper technology on the other, caused the research to slow down in this field.
Reference: [21] <author> D.E. Rumelhart and J.L. McClelland. </author> <title> Parallel Distributed Processing: Exploration in the Microstructure of Cognition. </title> <publisher> The M.I.T. Press, </publisher> <year> 1986. </year>
Reference-contexts: The learning involves adjusting the weights of the neural network as a response to the data presented to it. For a detailed discussion of different designs and learning methods of neural networks the following books are good sources: Haykin [12]; Hertz, Krogh & Palmer [13]; Rumelhart & McClelland <ref> [21] </ref>. Some of the well-known characteristics of artificial neural networks are their ability to learn mappings based on input-output examples and to generalize upon them. Robustness and fault-tolerance are also some other useful properties of neural networks. As outlined in Rumelhart & McClelland [21] artificial neural networks are suitable for learning <p> Krogh & Palmer [13]; Rumelhart & McClelland <ref> [21] </ref>. Some of the well-known characteristics of artificial neural networks are their ability to learn mappings based on input-output examples and to generalize upon them. Robustness and fault-tolerance are also some other useful properties of neural networks. As outlined in Rumelhart & McClelland [21] artificial neural networks are suitable for learning problems involving pattern association, auto association and classification.
Reference: [22] <author> G. Schram, L. Karsten, B.J.A. Krose, and F.C.A. Groen. </author> <title> Optimal attitude control of satellites by artificial neural networks: a pilot study. </title> <type> Technical Report CS-94-05, </type> <institution> Department of Computer Systems, Faculty of Mathematics and Computer Science, University of Amsterdam, </institution> <year> 1994. </year>
Reference: [23] <author> R.S. Sutton. </author> <title> Temporal credit assignment in reinforcement learning. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, Amherst, </institution> <year> 1984. </year>
Reference-contexts: Therefore, the main task of the reinforcement learning system is to somehow produce an estimate of the gradient using the changes in reinforcement during learning. There are two main types of reinforcement learning tasks as Sutton <ref> [23] </ref> distinguishes: * Nonassociative RL, in which the task of the learning system is to find a single optimal action with reinforcement signal being the only piece of information the learning system receives from the environment and no other sensory information are available to it.
Reference: [24] <author> R.S. Sutton. </author> <title> Learning to predict by the method of temporal difference. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44, </pages> <year> 1988. </year>
Reference-contexts: If that's not the case then a large n must be used. Therefore, for better performance using this method the value of n has to be changed during learning based on how well ^ J is learned. Temporal Difference Learning Sutton <ref> [24] </ref> suggested the popular learning method called temporal difference especially applicable to learning of delayed rewards. This method uses an estimate of the sum in Eq. 3.1 based on geometrically averaging J u (n) (x) from Section 3.4.1. <p> This is an extreme case of TD () learning in which we continuously adapt ^ J at each time-step using the error in consecutive predictions. It is useful to mention another extreme case in which = 1. As Sutton <ref> [24] </ref> shows, in this case the adaptation is simply an equivalent of supervised learning method which uses the difference 14 between the predictions of ^ J and the actual outcome of the evaluation function J u . <p> However, choosing an appropriate instead of simply = 0 or = 1 could improve the computational efficiency a great deal (Sutton <ref> [24] </ref>). One method is to start with = 1 and decrease it to 0 as ^ J improves its approximation.
Reference: [25] <institution> E.L. Thorndike. Animal Intelligence. Hafner and Darien, CT, </institution> <year> 1911. </year>
Reference-contexts: To see what this means it might be helpful to recall the Thorndikes's "Law of Effect" from 1911 <ref> [25] </ref>: Of several responses made to the same situation, those which are accompanied or closely followed by satisfaction to the animal will, other things being equal, be more firmly connected with the situation, so that, when it recurs, they will be more likely to recur; those which are accompanied or closely
Reference: [26] <author> M.D. Waltz and K.S. Fu. </author> <title> A heuristic approach to reinforcement learning control systems. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 10 </volume> <pages> 390-398, </pages> <year> 1965. </year>
Reference-contexts: Despite all the controversies, this theory has been one of the most influential learning theories in psychology because of its common-sense approach. The term Reinforcement Learning however, is never used in psychology. We could trace its usage back to Minsky [18], Waltz and Fu <ref> [26] </ref> in their studies of learning in the fields of AI and control theory, inspired by animal learning. In the rest of this chapter this approach to learning will be explained in relation with artificial neural networks.
Reference: [27] <author> C.J.C.H. Watkins. </author> <title> Learning from delayed rewards. </title> <type> PhD thesis, </type> <institution> Cambridge University, Cam-bridge, </institution> <year> 1984. </year>
Reference-contexts: And of course we assume that no model of the process is available and the learning occurs real-time without it. The material used in the following two sections are based on the works of Sathiya & Ravin-dran [14], Sutton [23][24] and Watkins <ref> [27] </ref>. References will be provided explicitly where necessary. 3.4 Evaluation Function Recall that the goal of reinforcement learning is to find a policy for selecting actions in a way that the selected sequence of actions will be optimal according to a certain performance measure. <p> Sofge & White [30] have successfully applied a gradient method to optimizing a manufacturing process. 3.5.3 Q-Learning This method was first introduced by Watkins <ref> [27] </ref>. For a more detailed discussion of Q-learning see also Watkins & Dayan [28]. In Q-Learning unlike the adaptive critic approach we learn only one function. This function is called Q, or action value function.
Reference: [28] <author> C.J.C.H. Watkins and P. Dayan. </author> <title> Technical note: </title> <journal> Q-learning. Machine Learning, </journal> <volume> 8 </volume> <pages> 279-292, </pages> <year> 1992. </year>
Reference-contexts: Sofge & White [30] have successfully applied a gradient method to optimizing a manufacturing process. 3.5.3 Q-Learning This method was first introduced by Watkins [27]. For a more detailed discussion of Q-learning see also Watkins & Dayan <ref> [28] </ref>. In Q-Learning unlike the adaptive critic approach we learn only one function. This function is called Q, or action value function. For a given state and action the optimal Q-function gives the estimated future cost when that action is performed and the same policy is used in the future.
Reference: [29] <author> P. Werbos. </author> <title> Approximate dynamic programming for real-time control and neural modelling. In Handbook of Intelligence control, Neural, Fuzzy and Adaptive approaches. </title> <publisher> Van Nostrand Reinhold, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: This would result in the following learning rule for updating the parameters of the actor: V := V fi @ ^u (x; V ) @V A more detailed discussion of gradient based methods is given in Werbos <ref> [29] </ref>. Sofge & White [30] have successfully applied a gradient method to optimizing a manufacturing process. 3.5.3 Q-Learning This method was first introduced by Watkins [27]. For a more detailed discussion of Q-learning see also Watkins & Dayan [28].
Reference: [30] <author> D.A. White and D.A. Sofge. </author> <title> Applied learning: Optimal control for manufacturing. In D.A. White and D.A. </title> <editor> Sofge, editors, </editor> <booktitle> Handbook of Intelligence control, Neural, Fuzzy and Adaptive approaches. </booktitle> <publisher> Van Nostrand Reinhold, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: This would result in the following learning rule for updating the parameters of the actor: V := V fi @ ^u (x; V ) @V A more detailed discussion of gradient based methods is given in Werbos [29]. Sofge & White <ref> [30] </ref> have successfully applied a gradient method to optimizing a manufacturing process. 3.5.3 Q-Learning This method was first introduced by Watkins [27]. For a more detailed discussion of Q-learning see also Watkins & Dayan [28]. In Q-Learning unlike the adaptive critic approach we learn only one function.
Reference: [31] <author> D.A. White and D.A. Sofge. </author> <title> Cmac learning method. In D.A. White and D.A. </title> <editor> Sofge, editors, </editor> <booktitle> Handbook of Intelligence control, Neural, Fuzzy and Adaptive approaches, </booktitle> <pages> pages 269-273. </pages> <publisher> Van Nostrand Reinhold, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: Then the derivative of CMAC could be calculated with respect to its input, the output of the controller, such that we can propagate the error through the controller network for its adaptation. A CMAC architecture with continuous differentiable mapping is introduced by Sofge & White <ref> [31] </ref> and explained below. 3.7.3 Continuous CMAC We could extend the localized mapping explained above in order to achieve real-valued mappings in the weight space. To do this, we use the fractional values of the input points to select the corresponding weight without having to transform them to integers first.
Reference: [32] <author> D.A. White and D.A. Sofge, </author> <title> editors. Handbook of Intelligent control, Neural, Fuzzy and Adaptive approaches. </title> <publisher> Van Nostrand Reinhold, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: The research has shown that neurocontrollers are able to perform well in the control of an unknown process under noise and are adaptive to dynamical changes in the system. Sofge and White discuss several types of these neurocontrollers in their excellent book <ref> [32] </ref>. 1.2 Water Vessel Process An example of a nonlinear dynamic process is the so called water vessel process. The water vessel process consists of four vessels connected with each other as shown in Figure 1.1.
Reference: [33] <author> R.J. Williams. </author> <title> Toward a theory of reinforcement learning connectionist systems. </title> <type> Technical Report NU-CCS-88-3, </type> <institution> College of Computer Science, Northeastern University, </institution> <address> Boston, </address> <year> 1988. </year> <month> 55 </month>
Reference-contexts: In the rest of this chapter this approach to learning will be explained in relation with artificial neural networks. For further discussions on reinforcement learning, the following material could be used: Barto [3], [2] and Williams <ref> [33] </ref>. 9 3.3 Reinforcement Learning Supervised learning, learning with a teacher, is probably the most common learning method used in the field of artificial neural networks. In this approach a certain input-output mapping is learned by some examples which are presented to the network by the teacher.
References-found: 33

