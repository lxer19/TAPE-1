URL: http://www.pdos.lcs.mit.edu/papers/cffs-usenix97.ps
Refering-URL: http://www.pdos.lcs.mit.edu/papers/cffs.html
Root-URL: 
Email: fganger,kaashoekg@lcs.mit.edu  
Title: Embedded Inodes and Explicit Grouping: Exploiting Disk Bandwidth for Small Files  
Author: Gregory R. Ganger and M. Frans Kaashoek 
Web: http://www.pdos.lcs.mit.edu/  
Address: Cambridge MA 02139, USA  
Affiliation: M.I.T. Laboratory for Computer Science  
Note: First appeared in the Proceedings of the USENIX Technical Conference,January, 1997, pp. 1-17.  
Abstract: Small file performance in most file systems is limited by slowly improving disk access times, even though current file systems improve on-disk locality by allocating related data objects in the same general region. The key insight for why current file systems perform poorly is that locality is insufficient exploiting disk bandwidth for small data objects requires that they be placed adjacently. We describe C-FFS (Co-locating Fast File System), which introduces two techniques, embedded inodes and explicit grouping, for exploiting what disks do well (bulk data movement) to avoid what they do poorly (reposition to new locations). With embedded inodes, the inodes for most files are stored in the directory with the corresponding name, removing a physical level of indirection without sacrificing the logical level of indirection. With explicit grouping, the data blocks of multiple small files named by a given directory are allocated adjacently and moved to and from the disk as a unit in most cases. Measurements of our C-FFS implementation show that embedded inodes and explicit grouping have the potential to increase small file throughput (for both reads and writes) by a factor of 5-7 compared to the same file system without these techniques. The improvement comes directly from reducing the number of disk accesses required by an order of magnitude. Preliminary experience with software-development applications shows performance improvements ranging from 10-300 percent. 
Abstract-found: 1
Intro-found: 1
Reference: [Baker91] <author> M. Baker, J. Hartman, M. Kupfer, K. Shirriff, J. Ousterhout, </author> <title> Measurements of a Distributed File System, </title> <booktitle> ACM Symposium on Operating Systems Principles, </booktitle> <year> 1991, </year> <pages> pp. 198-212. </pages>
Reference-contexts: Because most files are small (e.g., we observe that 79% of all files on our file servers are less than 8 KB in size) and most files accessed are small (e.g., <ref> [Baker91] </ref> reports that 80% of file accesses are to files of less than 10KB), file system performance is often limited by disk access times rather than disk bandwidth. <p> Assuming that free extents of disk blocks are always available, LFS works extremely well for write activity. However, the design is based on the assumption that file caches will absorb all read activity and does not help in improving read performance. Unfortunately, anecdotal evidence, measurements of real systems (e.g., <ref> [Baker91] </ref>), and simulation studies (e.g., [Dahlin94]) all indicate that main memory caches have not eliminated read traffic. This paper describes the co-locating fast file system (C-FFS), which introduces two techniques for exploiting disk bandwidth for small files and metadata: embedded inodes and explicit grouping. <p> In addition to this static view of file size distributions, studies of dynamic file system activity have reported similar behavior. For example, <ref> [Baker91] </ref> states that, although most of the bytes accessed are in large files, the vast majority of file accesses are to small files (e.g., about 80% of the files accessed during their study were less than 10KB). <p> So long as neither cleaning nor read traffic represent significant portions of the workload, this will offer the highest performance. Unfortunately, while it may be feasible to limit cleaning activity to idle pe riods [Blackwell95], anecdotal evidence, measurements of real systems (e.g., <ref> [Baker91] </ref>), and simulation studies (e.g., [Dahlin94]) all suggest that main memory caches have not eliminated read traffic as hoped. Our work attempts to achieve performance improvements for both reads and writes of small files and metadata.
Reference: [Blackwell95] <author> T. Blackwell, J. Harris, M. Seltzer, </author> <title> Heuristic Cleaning Algorithms in Log-Structured File Systems, </title> <booktitle> USENIX Technical Conference, </booktitle> <month> January </month> <year> 1995, </year> <pages> pp. 277-288. </pages>
Reference-contexts: So long as neither cleaning nor read traffic represent significant portions of the workload, this will offer the highest performance. Unfortunately, while it may be feasible to limit cleaning activity to idle pe riods <ref> [Blackwell95] </ref>, anecdotal evidence, measurements of real systems (e.g., [Baker91]), and simulation studies (e.g., [Dahlin94]) all suggest that main memory caches have not eliminated read traffic as hoped. Our work attempts to achieve performance improvements for both reads and writes of small files and metadata.
Reference: [Chamberlin81] <author> D. Chamberlin, M. Astrahan, et. al., </author> <title> A History and Evaluation of System R, </title> <journal> Communications of the ACM, </journal> <volume> Vol. 24, No. 10, </volume> <year> 1981, </year> <pages> pp. 632-646. </pages>
Reference-contexts: One of the extra advantages of embedded inodes is the elimination of one sequencing constraint when creating and deleting files. There are several more direct and more comprehensive approaches to reducing the performance cost of maintaining metadata integrity, including write-ahead logging [Hagmann87, Chutani92, Journal92, Sweeney96], shadow-paging <ref> [Chamberlin81, Stonebraker87, Chao92, Seltzer93] </ref> and soft updates [Ganger95]. As shown in Section 4, our work complements such approaches. Of course, there is a variety of other work that has improved file system performance via better caching, prefetching, write-back, indexing, scheduling and disk array mechanisms.
Reference: [Chao92] <author> C. Chao, R. English, D. Jacobson, A. Stepanov, J. Wilkes, Mime: </author> <title> A High-Performance Parallel Storage Device with Strong Recovery Guarantees, </title> <institution> Hewlett-Packard Laboratories Report, HPL-CSP-92-9, </institution> <month> Novem-ber </month> <year> 1992. </year>
Reference-contexts: One of the extra advantages of embedded inodes is the elimination of one sequencing constraint when creating and deleting files. There are several more direct and more comprehensive approaches to reducing the performance cost of maintaining metadata integrity, including write-ahead logging [Hagmann87, Chutani92, Journal92, Sweeney96], shadow-paging <ref> [Chamberlin81, Stonebraker87, Chao92, Seltzer93] </ref> and soft updates [Ganger95]. As shown in Section 4, our work complements such approaches. Of course, there is a variety of other work that has improved file system performance via better caching, prefetching, write-back, indexing, scheduling and disk array mechanisms.
Reference: [Chutani92] <author> S. Chutani, O. Anderson, M. Kazar, B. Leverett, W. Mason, R. Sidebotham, </author> <title> The Episode File System, </title> <booktitle> Winter USENIX Conference, </booktitle> <month> January </month> <year> 1992, </year> <pages> pp. 43-60. </pages>
Reference-contexts: So, by keeping the two items in the same sector, we can guarantee that they will be consistent with respect to each other. For file systems that use synchronous writes to ensure proper sequencing, this can result in a two-fold performance improvement [Ganger94]. For more aggressive implementations (e.g., <ref> [Hagmann87, Chutani92, Ganger95] </ref>), this reduces complexity and the amount of book-keeping required. Directory sizes A potential down-side of embedded inodes is that the directory size can increase substantially. <p> One of the extra advantages of embedded inodes is the elimination of one sequencing constraint when creating and deleting files. There are several more direct and more comprehensive approaches to reducing the performance cost of maintaining metadata integrity, including write-ahead logging <ref> [Hagmann87, Chutani92, Journal92, Sweeney96] </ref>, shadow-paging [Chamberlin81, Stonebraker87, Chao92, Seltzer93] and soft updates [Ganger95]. As shown in Section 4, our work complements such approaches. Of course, there is a variety of other work that has improved file system performance via better caching, prefetching, write-back, indexing, scheduling and disk array mechanisms.
Reference: [Dahlin94] <author> M. Dahlin, R. Wang, T. Anderson, D. Patterson, </author> <title> Cooperative Caching: Using Remote Client Memory to Improve File System Performance, </title> <booktitle> USENIX Symposium on Operating Systems Design and Implementation (OSDI), </booktitle> <month> November </month> <year> 1994, </year> <pages> pp. 267-280. </pages>
Reference-contexts: However, the design is based on the assumption that file caches will absorb all read activity and does not help in improving read performance. Unfortunately, anecdotal evidence, measurements of real systems (e.g., [Baker91]), and simulation studies (e.g., <ref> [Dahlin94] </ref>) all indicate that main memory caches have not eliminated read traffic. This paper describes the co-locating fast file system (C-FFS), which introduces two techniques for exploiting disk bandwidth for small files and metadata: embedded inodes and explicit grouping. <p> So long as neither cleaning nor read traffic represent significant portions of the workload, this will offer the highest performance. Unfortunately, while it may be feasible to limit cleaning activity to idle pe riods [Blackwell95], anecdotal evidence, measurements of real systems (e.g., [Baker91]), and simulation studies (e.g., <ref> [Dahlin94] </ref>) all suggest that main memory caches have not eliminated read traffic as hoped. Our work attempts to achieve performance improvements for both reads and writes of small files and metadata.
Reference: [Engler95] <author> D. Engler, M.F. Kaashoek, J. O'Toole Jr., Exok-ernel: </author> <title> an operating system architecture for application-level resource management, </title> <booktitle> ACM Symposium on Operating Systems Principles, </booktitle> <month> Dec. </month> <year> 1995, </year> <pages> pp. 251-266. </pages>
Reference-contexts: These techniques depend on high bandwidth to deliver high performance for small files, while the conventional approach depends on disk access times (which have improved much less). The C-FFS implementation evaluated here is the default file system for the Intel x86 version of the exok-ernel operating system <ref> [Engler95] </ref>. It includes most of the functionality expected from an FFS-like file system. Its major limitations are that it currently does not support prefetching or fragments (the units of allocation are 4 KB blocks). <p> We view our work as complementary to these. 6 Discussion The C-FFS implementation described and evaluated in this paper is part of the experimental exokernel OS <ref> [Engler95] </ref>. We have found the exokernel to be an excellent platform for systems research of this kind.
Reference: [Forin94] <author> A. Forin, G. Malan, </author> <title> An MS-DOS File System for UNIX, </title> <booktitle> Winter USENIX Conference, </booktitle> <month> January </month> <year> 1994, </year> <pages> pp. 337-354. </pages>
Reference-contexts: to open a file; it allows the inodes for all names in a directory to be accessed without requesting additional blocks; it eliminates one of the ordering constraints required for integrity during file creation and deletion; it eliminates the need for static (over-)allocation of inodes, increasing the usable disk capacity <ref> [Forin94] </ref>; and it simplifies the implementation and increases the efficiency of explicit grouping (there is a synergy between these two techniques). Explicit grouping places the data blocks of multiple files at adjacent disk locations and accesses them as a single unit most of the time.
Reference: [Ganger94] <author> G. Ganger, Y. Patt, </author> <title> Metadata Update Perfor--mance in File Systems, </title> <booktitle> USENIX Symposium on Operating Systems Design and Implementation (OSDI), Novem-ber 1994, </booktitle> <pages> pp. 49-60. </pages>
Reference-contexts: Simplifying integrity maintenance Although the original goal of embedded inodes was to reduce the number of separate disk requests, a pleasant side effect is that we can also eliminate one of the sequencing constraints associated with metadata updates <ref> [Ganger94] </ref>. In particular, by eliminating the physical separation between a name and the corresponding inode, C-FFS exploits a disk drive characteristic to atomically update the pair. <p> So, by keeping the two items in the same sector, we can guarantee that they will be consistent with respect to each other. For file systems that use synchronous writes to ensure proper sequencing, this can result in a two-fold performance improvement <ref> [Ganger94] </ref>. For more aggressive implementations (e.g., [Hagmann87, Chutani92, Ganger95]), this reduces complexity and the amount of book-keeping required. Directory sizes A potential down-side of embedded inodes is that the directory size can increase substantially. <p> The result is a 250% increase in file deletion throughput, provided both by the reduction in the number of disk requests and improved locality (i.e., the same block gets overwritten repeatedly as the multiple inodes that it contains are re-initialized). Because there exist techniques (e.g., soft updates <ref> [Ganger94] </ref>) that have been shown to effectively eliminate the performance cost of maintaining metadata integrity, we repeat the same experiments with this cost removed. Figure 6 shows these results. <p> Figure 6 shows these results. We have not yet actually implemented soft updates in C-FFS, but rather emulate it by using delayed writes for all metadata updates <ref> [Ganger94] </ref> shows that this will accurately predict the performance impact of soft updates. The change in performance that we observe for the conventional file system is consistent with previous studies.
Reference: [Ganger95] <author> G. Ganger, Y. Patt, </author> <title> Soft Updates: A Solution to the Metadata Update Problem in File Systems, </title> <type> Technical Report CSE-TR-254-95, </type> <institution> University of Michigan, </institution> <address> Ann Arbor, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: So, by keeping the two items in the same sector, we can guarantee that they will be consistent with respect to each other. For file systems that use synchronous writes to ensure proper sequencing, this can result in a two-fold performance improvement [Ganger94]. For more aggressive implementations (e.g., <ref> [Hagmann87, Chutani92, Ganger95] </ref>), this reduces complexity and the amount of book-keeping required. Directory sizes A potential down-side of embedded inodes is that the directory size can increase substantially. <p> There are several more direct and more comprehensive approaches to reducing the performance cost of maintaining metadata integrity, including write-ahead logging [Hagmann87, Chutani92, Journal92, Sweeney96], shadow-paging [Chamberlin81, Stonebraker87, Chao92, Seltzer93] and soft updates <ref> [Ganger95] </ref>. As shown in Section 4, our work complements such approaches. Of course, there is a variety of other work that has improved file system performance via better caching, prefetching, write-back, indexing, scheduling and disk array mechanisms.
Reference: [Gingell87] <author> R. Gingell, J. Moran, W. Shannon, </author> <title> Virtual Memory Architecture in SunOS, </title> <booktitle> Summer USENIX Conference, </booktitle> <month> June </month> <year> 1987, </year> <pages> pp. 81-94. </pages>
Reference-contexts: Therefore, our file cache is indexed by both disk address 5 , like the original UNIX buffer cache, and higher-level identities, like the SunOS integrated caching and virtual memory system <ref> [Gingell87, Moran87] </ref>. C-FFS uses physical identities to insert newly-read blocks of a group into the cache without back-translating to discover their file/offset identities. Instead, C-FFS inserts these blocks into the cache based on physical disk address and an invalid file/offset identity.
Reference: [Hagmann87] <author> R. Hagmann, </author> <title> Reimplementing the Cedar File System Using Logging and Group Commit, </title> <booktitle> ACM Symposium on Operating Systems Principles, </booktitle> <month> November </month> <year> 1987, </year> <pages> pp. 155-162. </pages>
Reference-contexts: So, by keeping the two items in the same sector, we can guarantee that they will be consistent with respect to each other. For file systems that use synchronous writes to ensure proper sequencing, this can result in a two-fold performance improvement [Ganger94]. For more aggressive implementations (e.g., <ref> [Hagmann87, Chutani92, Ganger95] </ref>), this reduces complexity and the amount of book-keeping required. Directory sizes A potential down-side of embedded inodes is that the directory size can increase substantially. <p> One of the extra advantages of embedded inodes is the elimination of one sequencing constraint when creating and deleting files. There are several more direct and more comprehensive approaches to reducing the performance cost of maintaining metadata integrity, including write-ahead logging <ref> [Hagmann87, Chutani92, Journal92, Sweeney96] </ref>, shadow-paging [Chamberlin81, Stonebraker87, Chao92, Seltzer93] and soft updates [Ganger95]. As shown in Section 4, our work complements such approaches. Of course, there is a variety of other work that has improved file system performance via better caching, prefetching, write-back, indexing, scheduling and disk array mechanisms.
Reference: [Herrin93] <author> E. Herrin, R. Finkel, </author> <title> The Viva File System, </title> <type> Technical Report #225-93, </type> <institution> University of Kentucky, Lex-ington, </institution> <year> 1993. </year>
Reference-contexts: embedding inodes halves the number of blocks actually dirtied when removing the files because there are no separate inode blocks. 4.3 File System Aging To get a handle on the impact of file system fragmentation on the performance of C-FFS, we use an aging program similar to that described in <ref> [Herrin93] </ref>. The program simply creates and deletes a large number of files. The probability that the next operation performed is a file creation (rather than a deletion) is taken from a distribution centered around a desired file system utilization. <p> Another is to cluster blocks (i.e., allocate them contiguously and read/write them as a unit when appropriate) [Peacock88, McVoy91]. Enumeration of a large file's disk blocks can also be significantly improved by using extents (instead of per-block pointers), B+ trees [Sweeney96] and/or sparse bitmaps <ref> [Herrin93] </ref>. We build on previous work by trying to exploit disk bandwidth for small files and metadata. To increase the efficiency of the many small disk requests that characterize accesses to small files and metadata, file systems often try to localize logically related objects.
Reference: [HP91] <author> Hewlett-Packard Company, </author> <title> HP C2247 3.5-inch SCSI-2 Disk Drive Technical Reference Manual, </title> <address> Edition 1, </address> <month> December </month> <year> 1991. </year>
Reference-contexts: So, while reducing seek times can improve performance somewhat, aggressive co-location has the potential to provide much higher returns. Not only are per-byte costs small relative to per-request costs, but they have been decreasing and are likely to continue to do so. For example, the HP C2247 disk drive <ref> [HP91, HP92] </ref> of a few years ago had only half as many sectors on each track as the HP C3653 listed in Table 1, but an average access time that was only 33% higher.
Reference: [HP92] <author> Hewlett-Packard Company, </author> <title> HP C2244/45/46/47 3.5-inch SCSI-2 Disk Drive Technical Reference Manual, Part Number 5960-8346, </title> <address> Edition 3, </address> <month> September </month> <year> 1992. </year>
Reference-contexts: So, while reducing seek times can improve performance somewhat, aggressive co-location has the potential to provide much higher returns. Not only are per-byte costs small relative to per-request costs, but they have been decreasing and are likely to continue to do so. For example, the HP C2247 disk drive <ref> [HP91, HP92] </ref> of a few years ago had only half as many sectors on each track as the HP C3653 listed in Table 1, but an average access time that was only 33% higher.
Reference: [HP96] <institution> Hewlett-Packard Company, </institution> <address> http://www.dmo.hp.com/ disks/oemdisk/c3653a.html, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: As a result, this approach is generally limited to providing less than a factor of two in performance (and in practice much less). To help illustrate the above claims, Table 1 lists characteristics for three state-of-the-art (for 1996) disk drives <ref> [HP96, Quantum96, Seagate96] </ref>. Figure 2 shows, for the same three drives, average access times as a function of the request size. Several points can be inferred from these graphs. <p> Seek &lt; 1 ms 0.6 ms (0.5 ms) 1.0 ms Average Seek 8.7 ms (0.8 ms) 8.0 ms (1.5 ms) 7.9 ms Maximum Seek 16.5 ms (1.0 ms) 19.0 ms (1.0 ms) 18.0 ms Table 1: Characteristics of three modern disk drives, taken from <ref> [HP96, Seagate96, Quantum96] </ref>. N/A indicates that the information was not available.
Reference: [Journal92] <author> NCR Corporation, </author> <title> Journaling File System Administrator Guide, Release 2.00, NCR Document D1-2724-A, </title> <month> April </month> <year> 1992. </year>
Reference-contexts: One of the extra advantages of embedded inodes is the elimination of one sequencing constraint when creating and deleting files. There are several more direct and more comprehensive approaches to reducing the performance cost of maintaining metadata integrity, including write-ahead logging <ref> [Hagmann87, Chutani92, Journal92, Sweeney96] </ref>, shadow-paging [Chamberlin81, Stonebraker87, Chao92, Seltzer93] and soft updates [Ganger95]. As shown in Section 4, our work complements such approaches. Of course, there is a variety of other work that has improved file system performance via better caching, prefetching, write-back, indexing, scheduling and disk array mechanisms.
Reference: [Kaashoek96] <author> M.F. Kaaskoek, D. Engler, G. Ganger, D. Wal-lach, </author> <title> Server Operating Systems, </title> <booktitle> ACM SIGOPS Euro-pean Workshop, </booktitle> <month> September </month> <year> 1996, </year> <pages> pp. 141-148. </pages>
Reference-contexts: In this paper, we investigate co-locating files based on the name space; other approaches based on application-specific knowledge are worth investigating. For example, one application-specific approach is to group files that make up a single hypertext document <ref> [Kaashoek96] </ref>. We are investigating extensions to the file system interface to allow this information to be passed to the file system. The result will be a file system that groups files based on application hints when they are available and name space relationships when they are not.
Reference: [McKusick84] <author> M. McKusick, W. Joy, S. Leffler, R. Fabry, </author> <title> A Fast File System for UNIX, </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> Vol. 2, No. 3, </volume> <month> August </month> <year> 1984, </year> <pages> pp. 181-197. </pages>
Reference-contexts: One approach often used in file systems like the fast file system (FFS) <ref> [McKusick84] </ref> is to place related data objects (e.g., an inode and the data blocks it points to) near each other on disk (e.g., in the same cylinder group) in order to reduce disk access times. <p> One sim-ple approach, which does have some drawbacks, is to increase the size of the basic file block <ref> [McKusick84] </ref>. Another is to cluster blocks (i.e., allocate them contiguously and read/write them as a unit when appropriate) [Peacock88, McVoy91]. Enumeration of a large file's disk blocks can also be significantly improved by using extents (instead of per-block pointers), B+ trees [Sweeney96] and/or sparse bitmaps [Herrin93]. <p> We build on previous work by trying to exploit disk bandwidth for small files and metadata. To increase the efficiency of the many small disk requests that characterize accesses to small files and metadata, file systems often try to localize logically related objects. For example, the Fast File System <ref> [McKusick84] </ref> breaks the file system's disk storage into cylinder groups and attempts to allocate most new objects in the same cylinder group as related objects (e.g., inodes in same cylinder group as containing directory and data blocks in same cylinder group as owning inode).
Reference: [McKusick94] <author> M. McKusick, T.J. Kowalski, </author> <title> Fsck The UNIX File System Check Program, 4.4 BSD System Manager's Manual, </title> <publisher> O'Reilley & Associates, Inc., </publisher> <address> Se-bastopol, CA, </address> <year> 1994, </year> <pages> pp. 3 1-21. </pages>
Reference-contexts: File system recovery One concern that re-arranging file system metadata raises is that of integrity in the face of system failures and media corruption. Regarding the first, we have had no difficulties constructing an off-line file system recovery program much like the UNIX FSCK utility <ref> [McKusick94] </ref>. Although inodes are no longer at statically determined locations, they can all be found (assuming no media corruption) by following the directory hierarchy.
Reference: [McVoy91] <author> L. McVoy, S. Kleiman, </author> <title> Extent-like Performance from a UNIX File System, </title> <booktitle> Winter USENIX Conference, </booktitle> <month> January </month> <year> 1991, </year> <pages> pp. 1-11. </pages>
Reference-contexts: However, while the time required to fetch the first byte of data is high (i.e., measured in mil liseconds), the subsequent data bandwidth is reasonable (&gt; 10 MB/second). Unfortunately, although file systems have been very successful at exploiting this bandwidth for large files <ref> [Peacock88, McVoy91, Sweeney96] </ref>, they have failed to do so for small file activity (and the corresponding metadata activity). <p> Allocation for groups Before describing what is different about the allocation routines used to group small files, we want to stress what is not different. Placement of data for large files remains unchanged and should exploit clustering technology <ref> [Peacock88, McVoy91] </ref>. <p> One sim-ple approach, which does have some drawbacks, is to increase the size of the basic file block [McKusick84]. Another is to cluster blocks (i.e., allocate them contiguously and read/write them as a unit when appropriate) <ref> [Peacock88, McVoy91] </ref>. Enumeration of a large file's disk blocks can also be significantly improved by using extents (instead of per-block pointers), B+ trees [Sweeney96] and/or sparse bitmaps [Herrin93]. We build on previous work by trying to exploit disk bandwidth for small files and metadata.
Reference: [Moran87] <author> J. Moran, </author> <title> SunOS Virtual Memory Implementation, </title> <booktitle> European UNIX Users Group (EUUG) Conference, Spring 1988, </booktitle> <pages> pp. 285-300. </pages>
Reference-contexts: Therefore, our file cache is indexed by both disk address 5 , like the original UNIX buffer cache, and higher-level identities, like the SunOS integrated caching and virtual memory system <ref> [Gingell87, Moran87] </ref>. C-FFS uses physical identities to insert newly-read blocks of a group into the cache without back-translating to discover their file/offset identities. Instead, C-FFS inserts these blocks into the cache based on physical disk address and an invalid file/offset identity.
Reference: [Mullender84] <author> S. Mullender, A. Tanenbaum, </author> <title> Immediate Files, </title> <journal> Software-Practice and Experience, </journal> <volume> 14 (4), </volume> <month> April </month> <year> 1984, </year> <pages> pp. 365-368. </pages>
Reference-contexts: Co-locating related objects and reading/writing them as a unit offers qualitatively larger improvements in performance. Immediate files are a form of co-location for small files. The idea, as proposed by <ref> [Mullender84] </ref>, is to expand the inode to the size of a block and include the first part of the file in it.
Reference: [Ousterhout85] <author> J. Ousterhout, H. Da Costa, D. Harrison, J. Kunze, M. Kupfer, J. Thompson, </author> <title> A Trace-Driven Analysis of the UNIX 4.2 BSD File System, </title> <booktitle> ACM Symposium on Operating System Principles, </booktitle> <year> 1985, </year> <pages> pp. 15-24. </pages>
Reference: [Peacock88] <author> J.K. Peacock, </author> <title> The Counterpoint Fast File System, </title> <booktitle> USENIX Winter Conference, </booktitle> <month> February </month> <year> 1988, </year> <pages> pp. 243-249. </pages>
Reference-contexts: However, while the time required to fetch the first byte of data is high (i.e., measured in mil liseconds), the subsequent data bandwidth is reasonable (&gt; 10 MB/second). Unfortunately, although file systems have been very successful at exploiting this bandwidth for large files <ref> [Peacock88, McVoy91, Sweeney96] </ref>, they have failed to do so for small file activity (and the corresponding metadata activity). <p> Allocation for groups Before describing what is different about the allocation routines used to group small files, we want to stress what is not different. Placement of data for large files remains unchanged and should exploit clustering technology <ref> [Peacock88, McVoy91] </ref>. <p> One sim-ple approach, which does have some drawbacks, is to increase the size of the basic file block [McKusick84]. Another is to cluster blocks (i.e., allocate them contiguously and read/write them as a unit when appropriate) <ref> [Peacock88, McVoy91] </ref>. Enumeration of a large file's disk blocks can also be significantly improved by using extents (instead of per-block pointers), B+ trees [Sweeney96] and/or sparse bitmaps [Herrin93]. We build on previous work by trying to exploit disk bandwidth for small files and metadata.
Reference: [Quantum96] <institution> Quantum Corporation, </institution> <address> http://www.quantum. com/products/atlas2/, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: As a result, this approach is generally limited to providing less than a factor of two in performance (and in practice much less). To help illustrate the above claims, Table 1 lists characteristics for three state-of-the-art (for 1996) disk drives <ref> [HP96, Quantum96, Seagate96] </ref>. Figure 2 shows, for the same three drives, average access times as a function of the request size. Several points can be inferred from these graphs. <p> Seek &lt; 1 ms 0.6 ms (0.5 ms) 1.0 ms Average Seek 8.7 ms (0.8 ms) 8.0 ms (1.5 ms) 7.9 ms Maximum Seek 16.5 ms (1.0 ms) 19.0 ms (1.0 ms) 18.0 ms Table 1: Characteristics of three modern disk drives, taken from <ref> [HP96, Seagate96, Quantum96] </ref>. N/A indicates that the information was not available.
Reference: [Rosenblum92] <author> M. Rosenblum, J. Ousterhout, </author> <title> The Design and Implementation of a Log-Structured File System, </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 10 (1), </volume> <month> Febru-ary </month> <year> 1992, </year> <pages> pp. 25-52. </pages>
Reference-contexts: The idea is to delay, remap and cluster all modified blocks, only writing large chunks to the disk <ref> [Rosenblum92] </ref>. Assuming that free extents of disk blocks are always available, LFS works extremely well for write activity. However, the design is based on the assumption that file caches will absorb all read activity and does not help in improving read performance. <p> Again, this favors the conventional organization. For the non-microbenchmark experiments, we do not do this. 4.2 Small File Performance with explicit grouping, with neither and with both. The micro-benchmark, based on the small-file benchmark from <ref> [Rosenblum92] </ref>, has four phases: create and write 10000 1KB files, read the same files in the same order, overwrite the same files in the same order, and then remove the files in the same order. <p> The log-structured file system's answer to the disk performance problem is to delay, remap and cluster all new data, only writing large chunks to the disk <ref> [Rosenblum92] </ref>. So long as neither cleaning nor read traffic represent significant portions of the workload, this will offer the highest performance.
Reference: [Rosenblum95] <author> M. Rosenblum, E. Bugnion, S. Herrod, E. Witchel, A. Gupta, </author> <title> The Impact of Architectural Trends on Operating System Performance, </title> <booktitle> ACM Symposium on Operating Systems Principles, </booktitle> <month> Dec. </month> <year> 1995, </year> <pages> pp. 285-298. </pages>
Reference: [Ruemmler91] <author> C. Ruemmler, J. Wilkes, </author> <title> Disk Shuffling, </title> <type> Technical Report HPL-CSP-91-30, </type> <institution> Hewlett-Packard Laboratories, </institution> <month> October 3, </month> <year> 1991. </year>
Reference-contexts: Similarly, several researchers have investigated the value of moving the most popular (i.e., most heavily used) data to the centermost disk cylinders in order to reduce disk seek distances <ref> [Vongsathorn90, Ruemmler91, Staelin91] </ref>. As described in Section 2, simply locating related objects near each other offers some performance gains, but such locality affects only the seek time and is thus limited in scope. Co-locating related objects and reading/writing them as a unit offers qualitatively larger improvements in performance.
Reference: [Ruemmler93] <author> C. Ruemmler, J. Wilkes, </author> <title> UNIX Disk Access Patterns, </title> <booktitle> Winter USENIX Conference, </booktitle> <month> January </month> <year> 1993, </year> <pages> pp. 405-420. </pages>
Reference: [Seagate96] <institution> Seagate Technology, Inc., </institution> <address> http://www.seagate. com/stor/stortop.shtml, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: As a result, this approach is generally limited to providing less than a factor of two in performance (and in practice much less). To help illustrate the above claims, Table 1 lists characteristics for three state-of-the-art (for 1996) disk drives <ref> [HP96, Quantum96, Seagate96] </ref>. Figure 2 shows, for the same three drives, average access times as a function of the request size. Several points can be inferred from these graphs. <p> Seek &lt; 1 ms 0.6 ms (0.5 ms) 1.0 ms Average Seek 8.7 ms (0.8 ms) 8.0 ms (1.5 ms) 7.9 ms Maximum Seek 16.5 ms (1.0 ms) 19.0 ms (1.0 ms) 18.0 ms Table 1: Characteristics of three modern disk drives, taken from <ref> [HP96, Seagate96, Quantum96] </ref>. N/A indicates that the information was not available.
Reference: [Seltzer93] <author> M. Seltzer, K. Bostic, M. McKusick, C. Staelin, </author> <title> An Implementation of a Log-Structured File System for UNIX, </title> <booktitle> Winter USENIX Conference, </booktitle> <month> January </month> <year> 1993, </year> <pages> pp. 201-220. </pages>
Reference-contexts: Externalized inodes are kept in a dynamically-growable, file-like structure that is similar to the IFILE in BSD-LFS <ref> [Seltzer93] </ref>. Some differences are that the externalized inode structure grows as needed but does not shrink and its blocks do not move once they have been allocated. <p> One of the extra advantages of embedded inodes is the elimination of one sequencing constraint when creating and deleting files. There are several more direct and more comprehensive approaches to reducing the performance cost of maintaining metadata integrity, including write-ahead logging [Hagmann87, Chutani92, Journal92, Sweeney96], shadow-paging <ref> [Chamberlin81, Stonebraker87, Chao92, Seltzer93] </ref> and soft updates [Ganger95]. As shown in Section 4, our work complements such approaches. Of course, there is a variety of other work that has improved file system performance via better caching, prefetching, write-back, indexing, scheduling and disk array mechanisms.
Reference: [Seltzer95] <author> M. Seltzer, K. Smith, M. Balakrishnan, J. Chang, S. McMains, V. Padmanabhan, </author> <title> File System Logging Verses Clustering: A Performance Comparison, </title> <booktitle> USENIX Technical Conference, </booktitle> <month> January </month> <year> 1995, </year> <pages> pp. 249-264. </pages>
Reference: [Smith96] <author> K. Smith, M. Seltzer, </author> <title> A Comparison of FFS Disk Allocation Policies, </title> <booktitle> USENIX Technical Conference, </booktitle> <month> January </month> <year> 1996, </year> <pages> pp. 15-25. </pages>
Reference: [Staelin91] <editor> Smart Filesystems, </editor> <booktitle> Winter USENIX Conference, </booktitle> <year> 1991, </year> <pages> pp. 45-51. </pages>
Reference-contexts: Similarly, several researchers have investigated the value of moving the most popular (i.e., most heavily used) data to the centermost disk cylinders in order to reduce disk seek distances <ref> [Vongsathorn90, Ruemmler91, Staelin91] </ref>. As described in Section 2, simply locating related objects near each other offers some performance gains, but such locality affects only the seek time and is thus limited in scope. Co-locating related objects and reading/writing them as a unit offers qualitatively larger improvements in performance.
Reference: [Stonebraker81] <author> M. Stonebraker, </author> <title> Operating System Support for Database Management, </title> <journal> Communications of the ACM, </journal> <volume> 24 (7), </volume> <year> 1981, </year> <pages> pp. 412-418. </pages>
Reference: [Stonebraker87] <author> M. Stonebraker, </author> <title> The Design of the POST-GRES Storage System, </title> <booktitle> Very Large Data Base Conference, </booktitle> <month> September </month> <year> 1987, </year> <pages> pp. 289-300. </pages>
Reference-contexts: One of the extra advantages of embedded inodes is the elimination of one sequencing constraint when creating and deleting files. There are several more direct and more comprehensive approaches to reducing the performance cost of maintaining metadata integrity, including write-ahead logging [Hagmann87, Chutani92, Journal92, Sweeney96], shadow-paging <ref> [Chamberlin81, Stonebraker87, Chao92, Seltzer93] </ref> and soft updates [Ganger95]. As shown in Section 4, our work complements such approaches. Of course, there is a variety of other work that has improved file system performance via better caching, prefetching, write-back, indexing, scheduling and disk array mechanisms.
Reference: [Sweeney96] <author> A. Sweeney, D. Doucette, W. Hu, C. Anderson, M. Nishimoto, G. Peck, </author> <title> Scalability in the XFS File System, </title> <booktitle> USENIX Technical Conference, </booktitle> <year> 1996, </year> <pages> pp. 1-14. </pages>
Reference-contexts: However, while the time required to fetch the first byte of data is high (i.e., measured in mil liseconds), the subsequent data bandwidth is reasonable (&gt; 10 MB/second). Unfortunately, although file systems have been very successful at exploiting this bandwidth for large files <ref> [Peacock88, McVoy91, Sweeney96] </ref>, they have failed to do so for small file activity (and the corresponding metadata activity). <p> Another is to cluster blocks (i.e., allocate them contiguously and read/write them as a unit when appropriate) [Peacock88, McVoy91]. Enumeration of a large file's disk blocks can also be significantly improved by using extents (instead of per-block pointers), B+ trees <ref> [Sweeney96] </ref> and/or sparse bitmaps [Herrin93]. We build on previous work by trying to exploit disk bandwidth for small files and metadata. To increase the efficiency of the many small disk requests that characterize accesses to small files and metadata, file systems often try to localize logically related objects. <p> One of the extra advantages of embedded inodes is the elimination of one sequencing constraint when creating and deleting files. There are several more direct and more comprehensive approaches to reducing the performance cost of maintaining metadata integrity, including write-ahead logging <ref> [Hagmann87, Chutani92, Journal92, Sweeney96] </ref>, shadow-paging [Chamberlin81, Stonebraker87, Chao92, Seltzer93] and soft updates [Ganger95]. As shown in Section 4, our work complements such approaches. Of course, there is a variety of other work that has improved file system performance via better caching, prefetching, write-back, indexing, scheduling and disk array mechanisms.
Reference: [Vongsathorn90] <author> P. Vongsathorn, S. Carson, </author> <title> A System for Adaptive Disk Rearrangement, </title> <journal> Software Practice and Experience, </journal> <volume> Vol. 20, No. 3, </volume> <month> March </month> <year> 1990, </year> <pages> pp. 225-242. </pages>
Reference-contexts: Similarly, several researchers have investigated the value of moving the most popular (i.e., most heavily used) data to the centermost disk cylinders in order to reduce disk seek distances <ref> [Vongsathorn90, Ruemmler91, Staelin91] </ref>. As described in Section 2, simply locating related objects near each other offers some performance gains, but such locality affects only the seek time and is thus limited in scope. Co-locating related objects and reading/writing them as a unit offers qualitatively larger improvements in performance.
Reference: [Worthington94] <author> B. Worthington, G. Ganger, Y. Patt, </author> <title> Scheduling Algorithms for Modern Disk Drives, </title> <booktitle> ACM SIGMETRICS Conference, </booktitle> <month> May </month> <year> 1994, </year> <pages> pp. 241-251. </pages>
Reference-contexts: The disk on the system is a Seagate ST31200 (see Table 2). The disk driver, originally taken from NetBSD, supports scatter/gather I/O and uses a C-LOOK scheduling algorithm <ref> [Worthington94] </ref>. The disk prefetches sequential disk data into its on-board cache. During the experiments, there was no other activity on the system, and no virtual memory paging occurred. In all of our experiments, we forcefully write back all dirty blocks before considering the measurement complete.
Reference: [Worthington95] <author> B. Worthington, G. Ganger, Y. Patt, J. Wilkes, </author> <title> On-Line Extraction of SCSI Disk Drive Parameters, </title> <booktitle> ACM SIGMETRICS Conference, </booktitle> <month> May </month> <year> 1995, </year> <pages> pp. 146-156. </pages>
Reference-contexts: Second, seek times do not drop linearly with seek distance for small distances. Seeking a single cylinder (or just switching between tracks) generally costs a full millisecond, and this cost rises quickly for slightly longer seek distances <ref> [Worthington95] </ref>. Third, it is successful only when no other activity moves the disk arm between related requests. As a result, this approach is generally limited to providing less than a factor of two improvement in performance (and often much less).
References-found: 41

