URL: http://www.sls.lcs.mit.edu/~drew/A0555.ps
Refering-URL: http://www.sls.lcs.mit.edu/~drew/publications.html
Root-URL: 
Email: fdrew, jrgg@sls.lcs.mit.edu  
Title: HETEROGENEOUS ACOUSTIC MEASUREMENTS FOR PHONETIC CLASSIFICATION  
Author: Andrew K. Halberstadt and James R. Glass 
Address: Cambridge, Massachusetts 02139 USA  
Affiliation: Spoken Language Systems Group Laboratory for Computer Science Massachusetts Institute of Technology  
Abstract: In this paper we describe our recent efforts to improve acoustic-phonetic modeling by developing sets of heterogeneous, phone-class-specific measurements, and combining these diverse measurements into a probabilistic classification framework. We first describe a baseline classifier using homogeneous measurements. After comparing selected sub-tasks to known human performance, we define sets of phone-class-specific measurements which improve within-class classification performance. Subsequently, we combine these heterogeneous measurements into an overall context-independent classification framework. We report on a series of phonetic classification experiments using the TIMIT acoustic-phonetic corpus. Our overall framework achieves 79.0% accuracy on the NIST core test set. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. T. Chun. </author> <title> A Hierarchical Feature Representation for Phonetic Classification. </title> <type> M.Eng. thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, </institution> <address> Cambridge, </address> <month> March </month> <year> 1996. </year>
Reference-contexts: To date we have used homogeneous feature vectors to represent the acoustic-phonetic information needed to discriminate among all sounds. Although this framework has worked well, obtaining good phonetic classification and recognition results <ref> [1, 4, 9] </ref>, our recent work indicates that further gains can be obtained by incorporating heterogeneous, phone-class-specific measurements into our framework. There are at least two potential advantages to this approach. <p> Multiple trials of model training were performed and combined to produce more robust mixture models. 3. BASELINE CLASSIFIER A baseline classifier was first established using homogeneous measurements. This measurement set reflects previous classification work <ref> [1] </ref> in our group. After preemphasis and DC-offset removal, a short-time Fourier transform (STFT) analysis was performed every 5 ms using a 20.5 ms Hamming window. <p> In these expressions, f represents all of the measurements that might be used by the system. Thus, each set of heterogeneous measurements is a subset of f . In fact, we can cast the above decoding as a hierarchical process <ref> [1] </ref>. <p> This strict framework can be thought of as a strategy for pruning the full MAP framework, and other pruning strategies could also be devised which save computation with minimal effect on performance <ref> [1] </ref>. 7. CONCLUSIONS These experiments demonstrate the viability of using heterogeneous, phone-class-specific measurements to improve the performance of acoustic-phonetic modeling techniques.
Reference: [2] <author> R. A. Cole and Y. K. Methusamy. </author> <title> Perceptual studies on vowels excised from continuous speech. </title> <booktitle> In ICSLP, </booktitle> <pages> pages 10911094, </pages> <address> Banff, Canada, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: For this purpose, we made use of previously reported perceptual studies concerning human classification performance using the TIMIT corpus. We examine the sub-tasks of vowel and stop classification below. Cole and Methusamy <ref> [2] </ref> have performed perceptual studies on vowels excised from TIMIT. For this study, 16 vowel labels from TIMIT were selected, and 168 tokens of each were extracted, for a total of 2688 tokens.
Reference: [3] <author> L. Gillick and S.J. Cox. </author> <title> Some statistical issues in the comparison of speech recognition algorithms. </title> <booktitle> In ICASSP, </booktitle> <pages> pages 532535, </pages> <address> Glasgow, Scotland, </address> <month> May </month> <year> 1989. </year>
Reference-contexts: We used the standard NIST 462 speaker training set, and 24 speaker core test set for final testing. An independent set of 50 speakers was used for system development. For purposes of significance testing by McNemar's test <ref> [3] </ref>, we also evaluated classification performance on a test set of 118 speakers, which was the full NIST 168 speaker test set minus our development set.
Reference: [4] <author> J. Glass, J. Chang, and M. McCandless. </author> <title> A probabilistic framework for feature-based speech recognition. </title> <booktitle> In ICSLP, </booktitle> <pages> pages 22772280, </pages> <address> Philadelphia, </address> <month> October </month> <year> 1996. </year>
Reference-contexts: To date we have used homogeneous feature vectors to represent the acoustic-phonetic information needed to discriminate among all sounds. Although this framework has worked well, obtaining good phonetic classification and recognition results <ref> [1, 4, 9] </ref>, our recent work indicates that further gains can be obtained by incorporating heterogeneous, phone-class-specific measurements into our framework. There are at least two potential advantages to this approach.
Reference: [5] <author> W. D. Goldenthal. </author> <title> Statistical Trajectory models for Phonetic Recognition. </title> <type> Ph.D. thesis, </type> <institution> Department of Aeronautics and Astronautics, Massachusetts Institute of Technology, </institution> <address> Cambridge, </address> <month> September </month> <year> 1994. </year>
Reference-contexts: This baseline configuration achieved classification accuracies of 78.9% and 78.4% on the development and core test sets, respectively. These results compare favorably with others previously reported in the literature <ref> [5, 9, 11, 13] </ref>. Figure 1 shows a bubble plot of the baseline classifier confusion matrix on the development set. Nearly 80% of the confusions occur by choosing an alternative in the correct manner class. Another 7% occur due to confusions involving the closure/silence class.
Reference: [6] <author> L. Lamel, R. Kassel, and S. Seneff. </author> <title> Speech database development: Design and analysis of the acoustic-phonetic corpus. </title> <booktitle> In Proc. of the DARPA Speech Recognition Workshop, </booktitle> <address> Palo Alto, </address> <month> February </month> <year> 1986. </year> <note> Report No. SAIC-86/1546. </note>
Reference-contexts: The classification experiments reported in this paper are context-independent (one model per phone), although the feature measurements were allowed to draw information from outside the boundaries of the current speech segment. All experiments were conducted using the TIMIT acoustic-phonetic corpus <ref> [6] </ref>. In accordance with common practice [8], we collapsed the 61 TIMIT labels into 39 labels before scoring and we ignored glottal stops.
Reference: [7] <author> L. F. Lamel. </author> <title> Formalizing Knowledge used in Spectrogram Reading: Acoustic and Perceptual Evidence from Stops. </title> <type> Ph.D. thesis, </type> <institution> Department of Electrical and Computer Engineering, Mas-sachusetts Institute of Technology, </institution> <address> Cambridge, </address> <month> May </month> <year> 1988. </year>
Reference-contexts: Our baseline classifier obtains 69.8% accuracy on the development set in this 16-way vowel identification task. Although the test sets are not exactly the same, this result indicates that humans and machines are performing about equally well in this task. Lamel <ref> [7] </ref> has reported on perception of stop consonants extracted from TIMIT. The results are broken down according to context.
Reference: [8] <author> K. F. Lee and H. W. Hon. </author> <title> Speaker-independent phone recognition using hidden Markov models. </title> <journal> IEEE Trans. Acoust., Speech, Signal Processing, </journal> <volume> 37(11):16411648, </volume> <month> November </month> <year> 1989. </year>
Reference-contexts: The classification experiments reported in this paper are context-independent (one model per phone), although the feature measurements were allowed to draw information from outside the boundaries of the current speech segment. All experiments were conducted using the TIMIT acoustic-phonetic corpus [6]. In accordance with common practice <ref> [8] </ref>, we collapsed the 61 TIMIT labels into 39 labels before scoring and we ignored glottal stops. For the experiments in this paper, we make reference to the manner classes of vowels/semi-vowels (VS), nasals/flaps (NF), stops (ST), weak fricatives (WF), strong fricatives (SF), and closures/silence (CL). <p> axr VS 15 l el VS 19 m em NF 20 n en nx NF 21 ng eng NF 25 z SF 26 s SF 27 sh zh SF 31 dh WF 32 th WF 33 b ST 37 g ST 38 k ST Table 1: 39 phone classes from <ref> [8] </ref>, and manner class membership. # of tokens in set Task Train Dev Test Core Overall 140,225 15,057 35,697 7,215 Vowel/Semivowel 58,840 6,522 15,387 3,096 Nasal/Flap 14,176 1,502 3,566 731 Stop 16,134 1,685 4,022 799 Fric/Clos/Sil 51,075 5,348 12,722 2,589 Table 2: Number of tokens in each data set, ignoring glottal
Reference: [9] <author> H. Leung, B. Chigier, and J. Glass. </author> <title> A comparative study of signal representations and classification techniques for speech recognition. </title> <booktitle> In ICASSP, </booktitle> <pages> pages 680683, </pages> <address> Minneapolis, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: To date we have used homogeneous feature vectors to represent the acoustic-phonetic information needed to discriminate among all sounds. Although this framework has worked well, obtaining good phonetic classification and recognition results <ref> [1, 4, 9] </ref>, our recent work indicates that further gains can be obtained by incorporating heterogeneous, phone-class-specific measurements into our framework. There are at least two potential advantages to this approach. <p> This baseline configuration achieved classification accuracies of 78.9% and 78.4% on the development and core test sets, respectively. These results compare favorably with others previously reported in the literature <ref> [5, 9, 11, 13] </ref>. Figure 1 shows a bubble plot of the baseline classifier confusion matrix on the development set. Nearly 80% of the confusions occur by choosing an alternative in the correct manner class. Another 7% occur due to confusions involving the closure/silence class. <p> Our final results of 79.0% on the core test set compare favorably to results in the literature. Zahorian [13] reports 77.0% on the core test set, while Leung et al. <ref> [9] </ref> report 78.0% on a different test set. The design of a feature extraction mechanism for pattern classification frequently leads to a tradeoff between retaining as much relevant information as possible while at the same time avoiding unmanageably high classifier dimensionality.
Reference: [10] <author> R. P. Lippmann. </author> <title> Speech perception by humans and machines. </title> <booktitle> In Proc. of the ESCA Workshop on the Auditory Basis of Speech Perception, </booktitle> <pages> pages 309316, </pages> <publisher> Keele University, </publisher> <editor> U. K., </editor> <month> July </month> <year> 1996. </year>
Reference-contexts: These results indicated that for stop consonants, there is a significant amount of low-level acoustic phonetic information which the automatic classifier is not effectively extracting. This experimental outcome is consistent with results in the literature comparing human and machine performance in a variety of speech recognition tasks <ref> [10] </ref>. These results also motivated our attempts to extract more low-level acoustic-phonetic information from the speech signal through the use of heterogeneous measurements. 5. HETEROGENEOUS FEATURES Analysis of our classification systems showed that the within-phone-class performance was dependent on the time-frequency resolution of the Fourier analysis.
Reference: [11] <author> P. Schmid. </author> <title> Explicit N-best Formant Features for Segment-Based Speech Recognition. </title> <type> Ph.D. thesis, </type> <institution> Department of Computer Science and Engineering, Oregon Graduate Institute of Science and Technology, </institution> <address> Portland, </address> <month> October </month> <year> 1996. </year>
Reference-contexts: This baseline configuration achieved classification accuracies of 78.9% and 78.4% on the development and core test sets, respectively. These results compare favorably with others previously reported in the literature <ref> [5, 9, 11, 13] </ref>. Figure 1 shows a bubble plot of the baseline classifier confusion matrix on the development set. Nearly 80% of the confusions occur by choosing an alternative in the correct manner class. Another 7% occur due to confusions involving the closure/silence class. <p> The pitch measurement was calculated using a cepstral-based method. These measurements resulted in a vowel/semi-vowel accuracy of 74.3% on the development set, which improves upon the 73.1% (0.02 significance level) obtained by the baseline system, and is competitive with previously reported results <ref> [11] </ref>. For nasals, baseline measurements were altered by changing the Hamming window duration to 28.5 ms and adding a measure of average pitch, giving a total of 62 measurements per segment.
Reference: [12] <author> X. Wang, S. A. Zahorian, and S. Auberg. </author> <title> Analysis of speech segments using variable spectral/temporal resolution. </title> <booktitle> In ICSLP, </booktitle> <pages> pages 12211224, </pages> <address> Philadelphia, </address> <month> October </month> <year> 1996. </year>
Reference-contexts: In a six-way stop classification task, these measurements obtained 83.4% on the development set, compared to 79.6% for the baseline (10 4 significance level), and compare favorably to previously reported results <ref> [12] </ref>. For fricatives and closures, a 26.5 ms Hamming window was used for frame-based calculations. Time derivatives of only 11 MFCC's (instead of 12) were extracted at the segment boundaries.
Reference: [13] <author> S. A. Zahorian, P. Silsbee, and X. Wang. </author> <title> Phone classification with segmental features and a binary-pair partitioned neural network classifier. </title> <booktitle> In ICASSP, </booktitle> <pages> pages 10111014, </pages> <address> Munich, Ger-many, </address> <month> April </month> <year> 1997. </year>
Reference-contexts: This baseline configuration achieved classification accuracies of 78.9% and 78.4% on the development and core test sets, respectively. These results compare favorably with others previously reported in the literature <ref> [5, 9, 11, 13] </ref>. Figure 1 shows a bubble plot of the baseline classifier confusion matrix on the development set. Nearly 80% of the confusions occur by choosing an alternative in the correct manner class. Another 7% occur due to confusions involving the closure/silence class. <p> We compare the performance of these measurements to the baseline and also report the McNemar significance level of the difference. For vowel/semivowel measurements, we used 62 dimensions. The first 60 dimensions were calculated as in <ref> [13] </ref>. These involve calculation of MFCC-like frame-based measurements, followed by a cosine transform in the time dimension to encode the trajectories of the frame-based features. <p> We have obtained preliminary solutions for handling the two challenges of developing diverse features to improve classification accuracy and having a framework to combine the features into an overall system. Our final results of 79.0% on the core test set compare favorably to results in the literature. Zahorian <ref> [13] </ref> reports 77.0% on the core test set, while Leung et al. [9] report 78.0% on a different test set.
References-found: 13

