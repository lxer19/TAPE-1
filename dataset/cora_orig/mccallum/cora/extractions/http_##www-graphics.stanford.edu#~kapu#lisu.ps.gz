URL: http://www-graphics.stanford.edu/~kapu/lisu.ps.gz
Refering-URL: http://www-graphics.stanford.edu/~kapu/resume.html
Root-URL: http://www.cs.stanford.edu
Title: Vision Methods for an Autonomous Machine Based on Range Imaging  
Author: Pulli, Kari, 
Keyword: machine vision, range image, object recognition, intelligent robots, segmentation  
Date: 72, 1993  (Received 5 May 1993)  
Note: Acta Univ. Oul. C  
Address: 90570 Oulu, Finland  Finland  
Affiliation: Department of Electrical Engineering, University of Oulu,  Oulu,  
Abstract: Range imagery produces the kind of geometric information about the environment that an autonomous machine needs for operation. Not only does the range data provide explicit information about the obstacles that might hinder movements, but it also provides a means of recognizing objects that should be manipulated. TULKO, the "Machine of the Future" project, has chosen laser range imagery as it main source of vision data. In TULKO the location and orientation of the work space is determined with respect to a paper roll manipulator. The locations of individual paper rolls must also be determined in such a way that the manipulator can transfer them. In the actual application the manipulator can autonomously load the rolls from a platform to a ship. This thesis presents a Hough transform based method for locating standing paper rolls of a known radius directly from the depth data. The locations are transferred into the manipulator coordinate system by calibrating the sensor and manipulator coordinator systems. For object recognition from depth maps, a new method for range image segmentation was created. The method first calculates local surface normals from the depth data using robust methods, and decomposes the normal vector into three orthogonal components. Based on two of the components it is possible to determine discontinuities in the surface orientation. A third component, a scaled depth value, is added to the two normal components, and the resulting triplets are considered as a single color vector with three "color" bands. This multi-band image is then segmented using a hierarchical connected component method. The resulting segmentation is refined by robustly fitting surface equations to the segments and thereby determining at the region borders to which segment the individual pixels belong. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aho, A.V., Hopcroft, J.E. & Ullman, J.D. </author> <title> (1974) The Design and Analysis of Computer Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> New York, 470 p. </address>
Reference-contexts: As the number of nodes and arcs, and the maximum clique size in the match graph grow, the execution time of the maximal clique problem grows exponentially, i.e. the problem is NP-complete <ref> (Aho et al. 1974) </ref>. There are some methods to simplify the problem. One such method is to take advantage of possible symmetries within the object, which may reduce the problem drastically.
Reference: <author> Alapuranen, P. & Westman, T. </author> <title> (1992) Integrated co-processor for connected component analysis. (in Finnish), </title> <institution> internal report of Computer Laboratory, Univ. of Oulu, Finland. </institution>
Reference-contexts: This is desirable, as this causes also curved but locally smooth regions to be segmented correctly into a single region. The method results in a reliable and robust segmentation, which can also be efficiently implemented in parallel hardware <ref> (Alapuranen & Westman 1992) </ref>. In the following subsections, we first describe the basic connected component analysis and then the merging of the resulting region adjacency graph (RAG). 5.5.1. Connected component analysis The first stage computes the initial image segmentation based on the connectivity of the adjacent pixels. <p> In the LTS implementation, the most demanding part is associated with median calculation, for which hardware implementations exist. Hardware implementation for the hierarchical connected component analysis has been studied recently in our laboratory <ref> (Alapuranen & Westman 1992) </ref>. 61 Fig. 5.14 The segmentation of a pile of planar and curved blocks using a 3 fi 3 operator size and the LSQ method. Depth information is not used in segmentation. 5.8. Summary A simple but powerful method for range image segmentation has been presented.
Reference: <author> Bajcsy R., Solina F. & Gupta A. </author> <title> (1990) Segmentation versus object representation| are they separable?. In: Jain, R.C & Jain A.K. (eds) Analysis and Interpretation of Range Images, </title> <publisher> Springer Verlag, </publisher> <address> New York, </address> <pages> pp. 207-224. </pages>
Reference-contexts: In robotic vision, to simplify means to partition images into entities that correspond to individual regions, objects, and parts in the real world and to describe these entities only in sufficient detail for performing a required task <ref> (Bajcsy et al. 1990) </ref>. Thus, the segmentation can be seen as a compression of abundant measurement data into a symbolic form to facilitate higher-level scene Fig. 3.3 Tools lying on a table have been first separated from the background and then the different regions have been labeled. 28 analysis processes.
Reference: <author> Ballard, D.H. </author> <title> (1981) Generalizing the Hough transform to detect arbitrary shapes. </title> <booktitle> Pattern Recognition 13: </booktitle> <pages> 111-122. </pages>
Reference: <author> Ben-Israel, A. & Greville, T.N.E. </author> <title> (1974) Generalized inverses: Theory and Applications. </title> <publisher> Wiley, </publisher> <address> New York. </address>
Reference: <author> Besl, P.J. </author> <title> (1988a) Surfaces in Range Image Understanding. </title> <publisher> Springer Verlag, </publisher> <address> New York, p. </address> <month> 339. </month>
Reference-contexts: The last requirement includes a learning capability, which is a difficult research issue in its own right. 3.2.3. Mathematical description Object recognition from depth maps can be defined as generalized inverse set mapping <ref> (Besl 1988a) </ref>. The world can be approximated to consist of N obj objects, the i th object being denoted A i . <p> The basic approach would be to fit a continuous differentiable function to data, and compute its derivatives analytically. If the data is clean enough, computationally more efficient methods suffice, such as the local quadratic surface least squares (LSQ) approach presented in <ref> (Besl 1988a) </ref>. Another possibility is to use the slightly more complex local LSQ planar fitting method of Taylor et al. (1989). The problem in the LSQ methods is that the orientation discontinuities get blurred. <p> The problem in the LSQ methods is that the orientation discontinuities get blurred. Further, if there is noise present in the measurement data, the data must first be filtered, which often blurs images even more. Gaussian filtering (or approximately Gaussian such as binomial filtering <ref> (Besl 1988a) </ref>) attenuates Gaussian noise well but blurs sharp edges. Median filtering removes shot noise without blurring edges, but it also makes roof edges flat. 48 Fig. 5.3 The solid line depicts a least squares fit, while the broken line results from a robust fit. <p> The images are of good quality, but we have tested the effect of adding salt-and-pepper noise to the images. Normal vectors are approximated both by using the LTS method (Rousseeuw & Leroy 1987) and by using a local quadratic surface LSQ method <ref> (Besl 1988a) </ref>. The methods were implemented for 3 fi 3, 5 fi 5, and 7 fi 7 neighborhoods. The LSQ method is a very fast mask-based method. The full implementation of the LTS method, on the other hand, would mean processing through all the possible triangles within a neighborhood.
Reference: <author> Besl, P.J. </author> <title> (1988b) Active, optical range imaging sensors. </title> <booktitle> Machine Vision and Applications 1: </booktitle> <pages> 127-152. </pages>
Reference: <author> Besl, P.J. & Jain, </author> <title> R.C. (1985) Three-dimensional object recognition. </title> <booktitle> Computing Survey 17(1): </booktitle> <pages> 75-145. </pages>
Reference-contexts: The generalized Hough transform is shown to implement object-feature graph matching efficiently. 3.2. Recognition system 3.2.1. Recognition system components The logical components of a complete object recognition system and their interactions can be depicted as in Fig. 3.1 <ref> (Besl & Jain 1985) </ref>. The four fundamental system domains are: the real world domain, the digitized sensor data domain, the symbolic description domain, and the modeling domain. Several processes map information from one domain to another.
Reference: <author> Besl, P.J. & Jain, </author> <title> R.C. (1988) Segmentation through variable-order surface fitting. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence 10(2): </journal> <pages> 167-192. </pages>
Reference-contexts: There are also methods that characterize surfaces using tools from differential geometry <ref> (Besl & Jain 1988) </ref>. Edge detection tries to extract discontinuities and form a closed boundary around components (Bhanu et al. 1986; Tomita & Kanade 1984; Fan et al. 1987).
Reference: <author> Besl, P.J., Birch, J. & Watson, L. </author> <title> (1989) Robust window operators. </title> <booktitle> Machine Vision and Applications 2: </booktitle> <pages> 179-191. </pages>
Reference-contexts: In image (a) there is a step edge, in image (b) a roof edge. More reliable alternatives to direct LSQ methods and pre-filtering are the so-called robust methods, such as M-estimators (Huber 1981), iterative reweighting least squares <ref> (Besl et al. 1989) </ref>, least median squares (LMedS) (Rousseeuw & Leroy 1987) and least trimmed squares (LTS) (Rousseeuw & Leroy 1987; Koivunen & Pietikainen 1992).
Reference: <author> Bhanu, B., Lee, S., Ho, C.C. & Henderson, T. </author> <title> (1986) Range data processing: Representation of surfaces by edges. </title> <booktitle> Proceedings of the 8th International Conference on Pattern Recognition: </booktitle> <pages> 236-238. </pages>
Reference: <author> Bolles, </author> <title> R.C. & Cain, R.A. (1982) Recognizing and locating partially visible objects: The local-feature-focus method. </title> <journal> International Journal of Robotics Research 1(3): </journal> <pages> 57-82. </pages>
Reference-contexts: There are some methods to simplify the problem. One such method is to take advantage of possible symmetries within the object, which may reduce the problem drastically. Another method is called the local-feature-focus (LFF) <ref> (Bolles & Cain 1982) </ref>, which has certain similarities to the RANSAC (Fischler & Bolles 1981). In the LFF one searches for special subsets of features of an object, hypothesizes an object, and verifies the hypothesis against the original image.
Reference: <author> Canny, J.F. </author> <title> (1986) A computational approach to edge detection. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence PAMI-8(6): </journal> <pages> 679-698. </pages>
Reference-contexts: The Hough transform can be generalized to detect arbitrary shapes for any orientation and scale (Merlin & Farber 1975; Ballard 1981). In the following description we assume that edge points and edge directions have been detected using some edge detectors (c.f. <ref> (Canny 1986) </ref>). The generalized Hough transform (GHT) begins with selecting a localization point L within a template of the idealized shape. From each point on the edge we move a variable distance R, in a variable direction ', so as to arrive at L.
Reference: <editor> Dane, C. & Bajcsy, R. </editor> <title> (1981) Three-dimensional segmentation using the Gaussian image and spatial information. </title> <booktitle> Proceedings of the IEEE Computer Society Conference on Pattern Recognition and Image Processing (PRIP): </booktitle> <pages> 54-56. </pages> <booktitle> 66 Davies, E.R. (1990) Machine Vision: Theory, Algorithms, </booktitle> <address> Practicalities. </address> <publisher> Academic Press, London. </publisher>
Reference: <author> Davignon, A. </author> <title> (1992) Contribution of edges and regions to range image segmentation. </title> <booktitle> Proceedings of the Applications of Artificial Intelligence X: Machine Vision and Robotics SPIE vol. </booktitle> <volume> 1708: </volume> <pages> 228-239. </pages>
Reference: <author> Elfes, A. </author> <title> (1990) Occupancy grids: A stochastic spatial representation for active robot perception. </title> <booktitle> Proceedings of 6th Conference on Uncertainty and AI, </booktitle> <publisher> AAAI. </publisher>
Reference-contexts: Range imagery has yet another advantage over intensity images in robot vision. Since range measuring devices measure the distance to the closest object surface in each particular direction, it is possible to deduce whether certain locations are free or occupied. Elfes <ref> (Elfes 1990) </ref> has produced occupancy grids, which explicitly maintain probabilistic estimates of the occupancy state of each cell in a spatial lattice.
Reference: <author> Fan, T-J. </author> <title> (1990) Describing and Recognizing 3-D Objects Using Surface Properties. </title> <publisher> Springer Verlag, </publisher> <address> New York, p. </address> <month> 142. </month>
Reference: <author> Fan, T-J., Medioni, G. & Nevatia, R. </author> <title> (1987) Segmented descriptions of 3-d surfaces. </title> <journal> IEEE Journal of Robotics and Automation RA-3(6): </journal> <pages> 527-538. </pages>
Reference: <author> Faugeras, O.D., Hebert, M. & Pauchon, E. </author> <title> (1983) Segmentation of range data into planar and quadratic patches. </title> <booktitle> Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pp. 8-13. </pages>
Reference: <author> Fischler, M.A. & Bolles, </author> <title> R.C. (1981) Random sample consensus: A paradigm for model fitting with applications to image analysis and automatic cartography. </title> <journal> Communications of the ACM 24(6): </journal> <pages> 381-395. </pages>
Reference-contexts: Thus, the dimensionality of the backprojected hyperplane is lowered from one (line) to zero (point). A method called random sample consensus (RANSAC) <ref> (Fischler & Bolles 1981) </ref> is similar in spirit to this kind of constraint utilization. The original RANSAC, however, randomly picks n pixels for determining the n parameters of a curve and tests the result against the other data, but the application of RANSAC into the HT is straightforward. <p> There are some methods to simplify the problem. One such method is to take advantage of possible symmetries within the object, which may reduce the problem drastically. Another method is called the local-feature-focus (LFF) (Bolles & Cain 1982), which has certain similarities to the RANSAC <ref> (Fischler & Bolles 1981) </ref>. In the LFF one searches for special subsets of features of an object, hypothesizes an object, and verifies the hypothesis against the original image. Because of possible occlusions one may need to search for several different subsets.
Reference: <author> Fischler, M.A. & Firschein, O. </author> <title> (1987) Parallel guessing: A strategy for high-speed computation. </title> <booktitle> Pattern Recognition 20: </booktitle> <pages> 257-263. </pages>
Reference: <author> Haralick, R.M. & Shapiro, L.G. </author> <title> (1992) Computer and Robot Vision, </title> <journal> vol. </journal> <volume> 1. </volume> <publisher> Addison-Wesley, </publisher> <address> New York. </address>
Reference-contexts: If the sensor data can be similarly segmented, the matching of the data against the inner models will be greatly eased. 3.3. Segmentation An image segmentation is the partitioning of an image into a set of non-overlapping regions whose union is the entire image <ref> (Haralick & Shapiro 1992) </ref>. The purpose of image segmentation is to decompose the image into parts that are meaningful with respect to a particular application. For example, in two-dimensional (2D) part recognition, a segmentation might be performed to separate 2D objects from the background as in Fig. 3.3. <p> Although it is in general difficult to say what constitutes a meaningful segmen tation, some basic rules exist <ref> (Haralick & Shapiro 1992) </ref>: * Regions of a segmented image should be uniform and homogeneous with respect to some characteristics, such as gray level in intensity images or surface continuity in range images. * Region interiors should be simple and without many small holes. * Adjacent regions of a segmentation should <p> These three components are treated as three color bands, and the resulting image can be segmented using a region growing type color image segmentation method. We use a method consisting of connected component analysis <ref> (Haralick & Shapiro 1992) </ref> followed by merging of a region adjacency graph (Zucker 1976) which was developed for color images (Westman et al. 1990). <p> component C if there is a sequence of pixels (p 0 ; p 1 ; : : : ; p n ) of C where p 0 = p, p n = q, and p i is a neighbor of p i1 for i = 1; : : : n <ref> (Haralick & Shapiro 1992) </ref>. Thus, the definition of a connected component depends on the definition of the neighbor.
Reference: <author> Heikkila, T. & Roning, J. </author> <year> (1992) </year> <month> PEM-modelling: </month> <title> A framework for designing intelligent robot control. </title> <journal> Journal of Robotics and Mechatronics 4(5): </journal> <pages> 437-444. </pages>
Reference-contexts: PEM-model The control scheme is based on a hierarchically organized set of Planning-Executing-Monitoring (PEM) cycles <ref> (Heikkila & Roning 1992) </ref>. Every PEM cycle is a goal-oriented module, which consists of three generic activities|planning, executing, and monitoring|and a separate meta control mechanism, which takes care of the control of generic activities inside a PEM cycle. This is illustrated in Fig. 2.2.
Reference: <author> Horowitz, S.L. & Pavlidis, T. </author> <title> (1974) Picture segmentation by a directed split-and-merge procedure. </title> <booktitle> Proceedings of the 2nd International Joint Conference on Pattern Recognition, </booktitle> <pages> pp. 424-433. </pages>
Reference: <author> Hough, </author> <title> P.V.C (1962) A method and means for recognizing complex patterns. </title> <type> U.S. Patent 3069654. </type>
Reference-contexts: A more detailed description of RAG's will be given in 5.5.2.. 29 3.4. Hough transform Hough transform <ref> (Hough 1962) </ref> is a robust method for detecting patterns in images by transforming image features into points in parameter space and locating clusters thus formed. <p> In this section, we first study the basic Hough transform for line detection in intensity images. We then proceed to the generalized Hough transform, and conclude with methods for improving the effectivity of the Hough transform. 3.4.1. Introduction into Hough transform The Hough transform (HT) <ref> (Hough 1962) </ref> was first introduced for detecting curves in bubble chamber photographs. The key ideas of the method can be illustrated by studying an example where the line passing through colinear image points is detected.
Reference: <author> Huber, P. </author> <title> (1981) Robust Statistics, </title> <publisher> John Wiley & Sons, </publisher> <address> New York. </address>
Reference-contexts: In image (a) there is a step edge, in image (b) a roof edge. More reliable alternatives to direct LSQ methods and pre-filtering are the so-called robust methods, such as M-estimators <ref> (Huber 1981) </ref>, iterative reweighting least squares (Besl et al. 1989), least median squares (LMedS) (Rousseeuw & Leroy 1987) and least trimmed squares (LTS) (Rousseeuw & Leroy 1987; Koivunen & Pietikainen 1992).
Reference: <author> Illingworth, J. & Kittler, J. </author> <title> (1987) The adaptive Hough transform. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence PAMI-9(5): </journal> <pages> 690-697. </pages>
Reference: <author> Illingworth, J. & Kittler, J. </author> <title> (1988) A Survey of the Hough Transform. Computer Vision, Graphics, </title> <booktitle> and Image Processing 44: </booktitle> <pages> 87-116. </pages>
Reference: <author> Jiang, X.Y. & Bunke, H. </author> <title> (1989) Segmentation of the needle map of objects with curved surfaces. </title> <journal> Pattern Recognition Letters 10: </journal> <pages> 181-187. </pages>
Reference: <author> Kasif, S., Kitchen, L. & Rosenfeld, A. </author> <title> (1983) A Hough transform technique for subgraph isomorphism. </title> <journal> Pattern Recognition Letters 2: </journal> <pages> 83-88. </pages>
Reference: <author> Koivunen, V. & Pietikainen, M. </author> <title> (1992) Experiments with combined edge and region-based range image segmentation. </title> <booktitle> Theory & Applications of Image Analysis| Selected Papers from the 7th Scandinavian Conference on Image Analysis. </booktitle> <publisher> World Scientific, Singapore. </publisher>
Reference: <author> Krishnamurthy, E.V. & Ziavras, S.G. </author> <title> (1988) Matrix g-inversion on the Connection Machine. </title> <type> Internal report, </type> <institution> University of Maryland. </institution>
Reference-contexts: The iteration X k+1 = X k (2I AX k ) (4..9) is performed until kX k+1 X k k 0. We have taken the same norm for the ending condition as in <ref> (Krishnamurthy & Ziavras 1988) </ref>, i.e. the absolute value of the maximum element of the difference matrix should be less than 10 n . We chose the n = 10, and we break the iteration after 100 iteration if the convergence has not been achieved.
Reference: <author> Lawson, C.L. & Hanson, </author> <title> R.J. (1984) Solving least squares problems. </title> <publisher> Prentice Hall. </publisher>
Reference: <author> Li, H. & Lavin, M.A. </author> <title> (1986) Fast Hough transform: A hierarchical approach. Computer Vision, Graphics, and Image Processing 36: 139-161. 67 Merlin, P.M. & Farber, D.J. (1975) A parallel mechanism for detecting curves in pictures. </title> <journal> IEEE Transactions on Computers 28: </journal> <pages> 96-98. </pages>
Reference: <author> Oshima, M. & Shirai, Y. </author> <title> (1983) Object recognition using three dimensional information. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence PAMI-5(4): </journal> <pages> 353-362. </pages>
Reference: <author> Pieska, S., Riekki, J. & Taipale, T. </author> <title> (1991) Controlling autonomous loading manipulator with Planning-Executing-Monitoring cycles. </title> <booktitle> Poster, SPIE's Intelligent Robotic Systems Symposium, </booktitle> <address> Boston. </address>
Reference-contexts: TULKO 2.2.1. An autonomous paper roll manipulator In 1989 the TULKO project began. This is a joint project between the University of Oulu and the Technical Centre of Finland, and it will continue until May 1993 <ref> (Pieska et al. 1991) </ref>. The name TULKO is an abbreviation of the project's Finnish name "Tulevaisuuden Kone" or "The Machine of the Future", and it is supported by the Technology Development Centre of Finland (TEKES) and by several private companies.
Reference: <author> Pong, T.C., Shapiro, L.G. & Haralick, </author> <title> R.M. (1981) A facet model region growing algorithm. </title> <booktitle> Proceedings of the IEEE Computer Society Conference on Pattern Recognition and Image Processing (PRIP): </booktitle> <pages> 122-139. </pages>
Reference: <author> Pratt, V. </author> <title> (1987) Direct least-squares fitting of algebraic surfaces. </title> <booktitle> Computer Graphics 21(4): </booktitle> <pages> 145-152. </pages>
Reference: <author> Pulli, K. </author> <title> (1991) 3d graphics on the multiprocessor system DAMP. </title> <type> Master's thesis. </type> <institution> University of Oulu, Department of Electrical Engineering, </institution> <address> p. </address> <month> 72. </month>
Reference-contexts: The coordinates expressed in the coordinate system of the laser pointer can be transformed to the manipulator coordinate system if the orientation and location of one coordinate system is known with reference to the other. This transformation can be expressed as follows <ref> (Pulli 1991) </ref>: we define M i j as the transformation matrix which converts a point P j in coordinate system j to 42 a point P i in coordinate system i; i.e.
Reference: <author> Pulli, K., Pietikainen, M. </author> <title> (1993) Range image segmentation through fusing orientation and depth information. Submitted for Machine Vision and Applications. </title>
Reference-contexts: In the rest of the chapter we present a simple but powerful range image segmentation method based on fusing local approximations of normal vectors with depth information <ref> (Pulli and Pietikainen 1993) </ref>. The result of the segmentation is a set of homogeneous surface regions that need not be planar|they can also be curved.
Reference: <author> Riekki, J. </author> <title> (1993) A control system for an autonomous machine. (in Finnish), </title> <booktitle> Licentiate in Technology thesis. </booktitle> <institution> University of Oulu, Department of Electrical Engineering. </institution>
Reference-contexts: The White Scanner sends a laser beam turned to a plane of light into the scene and calculates distances using triangulation. Its field of depth is 150-300 cm, and the measuring volume at a distance of 225 cm is 80 cm wide and 100 cm high <ref> (Riekki 1993) </ref>. The parts and the operation principle are presented in Fig. 2.5. The system consists of a light source (10 mW Helium-Neon laser, wavelength 633 nm), three mirrors, a video camera, a computing unit, a user interface, and a connection to a workstation.
Reference: <author> Riekki, J., Roning, J., Silven, O., Pietikainen, M. & Koivunen, V. </author> <title> (1991) Pick-and-place guidance utilizing an integrated control method and structured light ranging. </title> <booktitle> Proceedings of the SPIE Intelligent Robots and Computer Vision X, SPIE vol. </booktitle> <volume> 1607: </volume> <pages> 689-699. </pages>
Reference-contexts: The main goal of the research in TULKO is to increase the autonomy of machines by developing intelligent control systems and sensors. The control system is part of a larger system, which includes a high-level goal-oriented planner <ref> (Riekki et al. 1991) </ref>. We have applied the integrated control system to pick-and-place guidance, 15 utilizing structured light ranging. Other sensors used include force sensors and sonars. As an instance of an autonomous machine we have implemented a prototype of a paper roll manipulator (see Fig. 2.1). <p> The basic ideas of the control method were first tested with a simulator of an indoor mobile robot (Roning et al. 1990). In the second phase, experiments were performed with industrial robots equipped with range and force sensors <ref> (Riekki et al. 1991) </ref>. Having concluded the laboratory experiments we expanded our research to an outdoor application where the test equipment includes a large paper roll manipulator equipped with a sophisticated gripping device and the appropriate controls. 2.2.2.
Reference: <author> Rioux, M. & Cournoyer, L. </author> <title> (1989) The NRCC Three Dimensional Image Data Files. </title> <type> CNRC 29077, </type> <institution> National Research Council Canada. </institution>
Reference-contexts: However, the laser ranging system we had is suitable only for laboratory use. Here we describe our laboratory ranging system. We also used range data from a data library <ref> (Rioux & Cournoyer 1989) </ref>, and we briefly describe the ranging system used for obtaining the images in that library. The scanner that was used in the laboratory phase is the Technical Arts 100 A or the White Scanner (Technical Arts 1989). <p> Results We have tested our segmentation method on several range images containing both planar and curved objects. The images were obtained from the NRCC (National Research Council of Canada) range image library <ref> (Rioux & Cournoyer 1989) </ref>. The images are of good quality, but we have tested the effect of adding salt-and-pepper noise to the images. Normal vectors are approximated both by using the LTS method (Rousseeuw & Leroy 1987) and by using a local quadratic surface LSQ method (Besl 1988a).
Reference: <author> Roning, J., Riekki, J. & Kemppainen, S. </author> <title> (1990) Simulator for developing mobile robot systems. </title> <booktitle> Proceedings of the Mobile Robots V, SPIE vol. </booktitle> <volume> 1388: </volume> <pages> 350-360. </pages>
Reference-contexts: Its intended task is the loading and unloading of paper rolls onto and from ships, railroad wagons, etc. The basic ideas of the control method were first tested with a simulator of an indoor mobile robot <ref> (Roning et al. 1990) </ref>. In the second phase, experiments were performed with industrial robots equipped with range and force sensors (Riekki et al. 1991).
Reference: <author> Rousseeuw, P. & Leroy, A. </author> <title> (1987) Robust Regression & Outlier Detection. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, p. </address> <month> 329. </month>
Reference-contexts: In image (a) there is a step edge, in image (b) a roof edge. More reliable alternatives to direct LSQ methods and pre-filtering are the so-called robust methods, such as M-estimators (Huber 1981), iterative reweighting least squares (Besl et al. 1989), least median squares (LMedS) <ref> (Rousseeuw & Leroy 1987) </ref> and least trimmed squares (LTS) (Rousseeuw & Leroy 1987; Koivunen & Pietikainen 1992). Because LSQ methods try to fit a function to all of the data, the outliers that deviate a lot from the bulk of the data pull the fit towards them. <p> are achieved 49 Fig. 5.4 A constant change in the orientation of a normal vector does not map to a constant change in the y-component's value. when h = bn=2c + dp=2e; (5..2) where p is the dimension of the function that is fitted (for a plane p = 3) <ref> (Rousseeuw & Leroy 1987) </ref>, and bc and de denote rounding to the closest lower or higher integer, respectively. <p> The images were obtained from the NRCC (National Research Council of Canada) range image library (Rioux & Cournoyer 1989). The images are of good quality, but we have tested the effect of adding salt-and-pepper noise to the images. Normal vectors are approximated both by using the LTS method <ref> (Rousseeuw & Leroy 1987) </ref> and by using a local quadratic surface LSQ method (Besl 1988a). The methods were implemented for 3 fi 3, 5 fi 5, and 7 fi 7 neighborhoods. The LSQ method is a very fast mask-based method.
Reference: <author> Rousseeuw, P. & van Zomeren, B. </author> <title> (1990) Unmasking multivariate outliers and leverage points. With Comments and Rejoinder, </title> <journal> Journal of the American Statistical Association 85(411): </journal> <pages> 633-651. </pages>
Reference-contexts: We chose to use LTS as our principal method for obtaining normal vectors. It has a high breakdown point, and compared to the LMedS, it has a better convergence rate and a smoother objective function <ref> (Rousseeuw & van Zomeren 1990) </ref>.
Reference: <author> Sabata, B., Arman, F & Aggarwal, J.K. </author> <title> (1990) Segmentation of 3-d range images using pyramidal data structures. </title> <booktitle> Proceedings of the 3rd International Conference in Computer Vision, </booktitle> <pages> pp. 662-666. </pages>
Reference: <author> Shirai, Y. </author> <title> (1987) Three-Dimensional Computer Vision. </title> <publisher> Springer Verlag, </publisher> <address> Berlin, p. </address> <month> 297. </month>
Reference: <author> Shirai, Y. & Suwa, M. </author> <title> (1971) Recognition of polyhedra with a range finder. </title> <booktitle> Proceedings of the 3rd International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 80-87. </pages>
Reference: <author> Shvaytser, H & Bergen, J.R. </author> <title> (1991) Monte Carlo Hough transforms. </title> <booktitle> Proceedings of the 7th Scandinavian Conference on Image Analysis, </booktitle> <pages> pp. 80-87. </pages>
Reference: <author> Sklansky, J. </author> <title> (1978) On the Hough technique for curve detection. </title> <journal> IEEE Transactions on Computers 21 </journal> <pages> 46-57. </pages>
Reference-contexts: It has been shown to be equivalent to template matching (Stockman & Agrawala 1977) and also to spatial matched filter <ref> (Sklansky 1978) </ref>. The GHT can locate features from the images by mapping the image data to a parameter space. It is a robust method, that is able to perform well in the presence of noise and occlusions.
Reference: <author> Stockman, </author> <title> G.C. & Agrawala, A.K. (1977) Equivalence of Hough curve detection to curve matching. </title> <journal> Communications of the ACM 20: </journal> <pages> 820-822. </pages> <note> 68 Taylor, </note> <author> R.W., Savini, M. & Reeves, </author> <title> A.P. (1989) Fast segmentation of range imagery into planar regions. Computer Vision, Graphics, and Image Processing 45: 42-60. Technical Arts (1989) User's Manual and Application Programming Guide. Preliminary Data, </title> <type> Technical Arts Corporation. </type>
Reference-contexts: A method that seems to be very general and useful in many different phases of the object recognition process is the Hough transform (HT), especially the generalized 63 Hough transform (GHT). It has been shown to be equivalent to template matching <ref> (Stockman & Agrawala 1977) </ref> and also to spatial matched filter (Sklansky 1978). The GHT can locate features from the images by mapping the image data to a parameter space. It is a robust method, that is able to perform well in the presence of noise and occlusions.
Reference: <author> Tomita, F, & Kanade, T. </author> <title> (1984) A 3d vision system: Generating and matching shape descriptions in range images. </title> <booktitle> Proceedings of the 1st Conference on Artificial Intelligence Applications, </booktitle> <pages> pp. 186-191. </pages>
Reference: <author> Westman, T., Harwood, D., Laitinen, T. & Pietikainen, M. </author> <title> (1990) Color segmentation by hierarchical connected component analysis with image enhancement by symmetric neighborhood filters. </title> <booktitle> Proceedings of the 10th International Conference on Computer Vision and Pattern Recognition Systems and Applications, </booktitle> <pages> pp. 769-802. </pages>
Reference-contexts: We use a method consisting of connected component analysis (Haralick & Shapiro 1992) followed by merging of a region adjacency graph (Zucker 1976) which was developed for color images <ref> (Westman et al. 1990) </ref>. Using the results of the connected component analysis, we show that polynomial surface equations describing the surface geometry can be easily found, and they can in turn be used to further refine the earlier segmentation result. Figure 5.2 presents a diagram of the segmentation method. 5.2. <p> The hierarchical connected component analysis approach described in the following section seems to be especially suitable for this purpose. 5.5. Segmentation by hierarchical connected component analysis The segmentation procedure in <ref> (Westman et al. 1990) </ref> is a robust and iterative connected component analysis method for color images, which works by first merging pixels and then regions, depending on their average boundary contrast in the color space. <p> Too low thresholds result in several tiny isolated regions, while setting thresholds too high leads to undersegmentation. A safe way is to use smaller thresholds and more merging iterations. There are also ways of choosing thresholds based on image complexity using a cumulative difference histogram <ref> (Westman et al. 1990) </ref>. In all the images one basic segmentation and one merging phase were used. We used 4-connectivity, and the thresholds used were 7 (ffi) and 15 (*). They correspond to 4:9 ffi and 10:5 ffi , respectively, in the surface orientation differences.
Reference: <author> Xu, L., Oja, E. & Kultanen, P. </author> <title> (1990) A new curve detection method: Random Hough transform (RHT). </title> <journal> Pattern Recognition Letters 11(5): </journal> <pages> 331-338. </pages>
Reference: <author> Yokoya, N. & Levine, </author> <title> M.D. (1989) Range image segmentation based on differential geometry: A hybrid approach. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence PAMI-11(6): </journal> <pages> 643-649. </pages>
Reference-contexts: Fan et al. (1987) calculate the surface curvature and locate step and roof edges, as well as ridges (smooth extrema), using the zero-crossings and extrema of the curvature along several directions. Separate edge points are then combined into a closed boundary. Yokoya and Levine <ref> (Yokoya & Levine 1989) </ref> use a hybrid of the region and edge-based methods. They combine three methods: the differential geometry approach of Besl, step edges derived from depth values, and roof edges from the partial derivatives of the depth values.
Reference: <author> Zucker, </author> <title> S.W. (1976) Region growing: Childhood and adolescence. </title> <booktitle> Computer Graphics and Image Processing 5: </booktitle> <pages> 382-399. </pages>
Reference-contexts: These three components are treated as three color bands, and the resulting image can be segmented using a region growing type color image segmentation method. We use a method consisting of connected component analysis (Haralick & Shapiro 1992) followed by merging of a region adjacency graph <ref> (Zucker 1976) </ref> which was developed for color images (Westman et al. 1990). Using the results of the connected component analysis, we show that polynomial surface equations describing the surface geometry can be easily found, and they can in turn be used to further refine the earlier segmentation result. <p> = (V; E; L; ff), where V is a set of vertices; E 2 V fi V is a set of edges; L is a set of labels; and ff : V [ E ! L is a function that assigns a label to each arc and vertex of G <ref> (Zucker 1976) </ref>. The vertices correspond to regions, and the edges represent the spatial adjacency of these regions, i.e. for two adjacent regions there is a connecting edge. Associated with vertices is regional information, such as area of the region, etc.
References-found: 57

