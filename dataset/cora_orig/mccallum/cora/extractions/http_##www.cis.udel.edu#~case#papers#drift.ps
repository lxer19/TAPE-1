URL: http://www.cis.udel.edu/~case/papers/drift.ps
Refering-URL: http://www.cis.udel.edu/~case/colt.html
Root-URL: http://www.cis.udel.edu
Email: case@cis.udel.edu.  sanjay@comp.nus.edu.sg.  arun@cse.unsw.edu.au.  fstephan@math.uni-heidelberg.de.  
Title: Predictive Learning Models for Concept Drift  
Author: John Sanjay Jain, Arun Sharma, Frank Stephan, EU, susanne.kaufmann@t-online.de. 
Address: Susanne Kaufmann, Universitat Karlsruhe  Heidelberg  101A Smith Hall, DE 19716-2586, USA,  Singapore, Singapore 119260, Republic of Singapore,  76128 Karlsruhe, Germany,  Sydney, NSW 2052, Australia,  Heidelberg, Im Neuenheimer Feld 294, 69120 Heidelberg, Germany, EU,  
Affiliation: Case, University of Delaware  National University of Singapore  University of New South Wales  Universitat  Department of Computer and Information Sciences, University of Delaware,  Department of Computer Science, School of Computing, National University of  Insitut fur Logik, Komplexitat und Deduktionssysteme, Universitat Karlsruhe,  School of Computer Science and Engineering, University of New South Wales,  -Mathematisches Institut, Universitat  
Abstract:  
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Angluin. </author> <title> Finding patterns common to a set of strings. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 21 </volume> <pages> 46-62, </pages> <year> 1980. </year>
Reference-contexts: Note that here and in the subsequent sections, logarithms are base 2. While polynomials of bounded degree are shown to be learnable with reasonable constant permanence under all our criteria, we show that the natural concept class of pattern languages 5 <ref> [1] </ref> with erasing separates martingale learners from density learners (also from frequency learners and uniform density learners). A martingale learner succeeds on the erasing pattern languages already at the surprising small constant permanence 7. <p> So one of the predictions on the interval from x (k + 2) k 1 to x (k + 2) + k + 1 is correct. The pattern languages <ref> [1] </ref> are a prominent and natural language class. We consider a known natural extension with the aim of showing that some natural class S separates the ability to learn by a martingale from that to learn by a frequency learner. <p> A pattern is a schemata consisting of variables and (Boolean) constants. It generates the language of all words which can be obtained by replacing each variable by a binary string. A pattern language <ref> [1] </ref> is called erasing if the variables in the defining pattern may be replaced by the empty string.
Reference: [2] <author> P. Bartlett, S. Ben-David, and S. Kulkarni. </author> <title> Learning changing concepts by exploiting the structure of change. </title> <booktitle> In Proceedings of the Ninth Annual Conference on Computational Learning Theory, </booktitle> <address> Desenzano del Garda, Italy. </address> <publisher> ACM Press, </publisher> <month> July </month> <year> 1996. </year>
Reference-contexts: 1 Introduction In many machine learning situations, the concepts to be learned or the concepts auxiliarily useful to learn may drift with time <ref> [2, 3, 5, 6, 8, 10, 17] </ref>. As in the just previous references, to sufficiently track drifting concepts to permit learning something of them at all, it is necessary to consider some restrictions on the nature of the drift. <p> For example, Helmbold and Long [8] bound the probability of disagreement between subsequent concepts. Blum and Chalasani [3] place some constraints on how many different concepts may be used, or the frequency of concept switches. Bartlett, Ben-David and Kulkarni <ref> [2] </ref> consider `class of legal function sequences' based on some constraints (such as being formed from a walk on a directed graph). Most of the above work which was theoretical was in a setting similar to PAC learning. Our work addresses drift in a more general computability setting. <p> One might argue that such an improvement is due only to the ease of finding an algorithm and not to any real difference between the two concepts. The next example shows that there is a class S such that S 0 <ref> [2] </ref> is frequency learnable for computable intervals while the general class S [2] is not learnable under any reasonable criterion for arbitrary drift. Example 4.4 Let S be the class of all increasing binary functions, that is, S = f0 n 1 1 : n 2 N g. <p> The next example shows that there is a class S such that S 0 <ref> [2] </ref> is frequency learnable for computable intervals while the general class S [2] is not learnable under any reasonable criterion for arbitrary drift. Example 4.4 Let S be the class of all increasing binary functions, that is, S = f0 n 1 1 : n 2 N g. <p> Let I 0 ; I 1 ; : : : be a computable partition of N such that every interval I n contains at least two elements. Let S <ref> [2] </ref> 0 be the class of all functions f 2 S [2] which in addition coincide with some g n 2 S on every interval I n . We have that S [2] 0 is frequency learnable while S [2] itself is not learnable under any reasonable learning criterion since S <p> Let I 0 ; I 1 ; : : : be a computable partition of N such that every interval I n contains at least two elements. Let S <ref> [2] </ref> 0 be the class of all functions f 2 S [2] which in addition coincide with some g n 2 S on every interval I n . We have that S [2] 0 is frequency learnable while S [2] itself is not learnable under any reasonable learning criterion since S [2] = 0 f0; 1g 1 . Proof. <p> Let S <ref> [2] </ref> 0 be the class of all functions f 2 S [2] which in addition coincide with some g n 2 S on every interval I n . We have that S [2] 0 is frequency learnable while S [2] itself is not learnable under any reasonable learning criterion since S [2] = 0 f0; 1g 1 . Proof. The learner for S [2] 0 predicts f (x) = 0, if x = min (I n ) for some n, and predicts f <p> Let S <ref> [2] </ref> 0 be the class of all functions f 2 S [2] which in addition coincide with some g n 2 S on every interval I n . We have that S [2] 0 is frequency learnable while S [2] itself is not learnable under any reasonable learning criterion since S [2] = 0 f0; 1g 1 . Proof. The learner for S [2] 0 predicts f (x) = 0, if x = min (I n ) for some n, and predicts f (x 1) otherwise. <p> 0 be the class of all functions f 2 S <ref> [2] </ref> which in addition coincide with some g n 2 S on every interval I n . We have that S [2] 0 is frequency learnable while S [2] itself is not learnable under any reasonable learning criterion since S [2] = 0 f0; 1g 1 . Proof. The learner for S [2] 0 predicts f (x) = 0, if x = min (I n ) for some n, and predicts f (x 1) otherwise. <p> We have that S <ref> [2] </ref> 0 is frequency learnable while S [2] itself is not learnable under any reasonable learning criterion since S [2] = 0 f0; 1g 1 . Proof. The learner for S [2] 0 predicts f (x) = 0, if x = min (I n ) for some n, and predicts f (x 1) otherwise. <p> Since the length of the intervals is at least 2, there are never more than two consecutive errors, which occur in the adversary case, at the end of the last and the beginning of the new interval. So S <ref> [2] </ref> 0 is learnable with frequency 1 out of 3. For the second result on the nonlearnability of the whole class S [2], consider any f0; 1g-valued function f with f (0) = 0. <p> So S <ref> [2] </ref> 0 is learnable with frequency 1 out of 3. For the second result on the nonlearnability of the whole class S [2], consider any f0; 1g-valued function f with f (0) = 0. <p> This rest I 0 can then be divided into intervals of length 2 on which f is also non-decreasing. Thus S <ref> [2] </ref> contains all f0; 1g-valued functions with f (0) = 0 and is thus not learnable. <p> The next example shows that disjointness itself without knowing the intervals can already yield advantages in terms of learnability with higher frequencies. In contrast to the previous examples, one has one single machine which, for every partition, learns the functions in the corresponding class S <ref> [2] </ref> 0 with frequency 2 out of 5. This learner succeeds even for nonrecursive partitions and does not need any apriori knowledge on the actual positions of the intervals. <p> Example 4.5 Let S contain all functions which are 0 at all but one argument and let I 0 ; I 1 ; I 2 ; : : : be a (not necessarily recursive) partition of N into intervals of length at least 2. Then the subclass S <ref> [2] </ref> 0 of all functions f 2 S [2] which coincide with functions in S on the intervals I 0 ; I 1 ; I 2 ; : : : is learnable with frequency 2 out of 5. The whole class S [2] is not learnable with frequency 2 out of <p> Then the subclass S <ref> [2] </ref> 0 of all functions f 2 S [2] which coincide with functions in S on the intervals I 0 ; I 1 ; I 2 ; : : : is learnable with frequency 2 out of 5. The whole class S [2] is not learnable with frequency 2 out of 5, though it is learnable with frequency 2 <p> Then the subclass S <ref> [2] </ref> 0 of all functions f 2 S [2] which coincide with functions in S on the intervals I 0 ; I 1 ; I 2 ; : : : is learnable with frequency 2 out of 5. The whole class S [2] is not learnable with frequency 2 out of 5, though it is learnable with frequency 2 out of 6. The corresponding densities of the best possible learning algorithms are 1 2 and 1 Proof. Consider any f 2 S [2] 0 . <p> The whole class S <ref> [2] </ref> is not learnable with frequency 2 out of 5, though it is learnable with frequency 2 out of 6. The corresponding densities of the best possible learning algorithms are 1 2 and 1 Proof. Consider any f 2 S [2] 0 . For any n, there is a g n 2 S with f = g n on I n . Consider an algorithm that always predicts 0. Let J be an interval of length 5. <p> Therefore the above algorithm, which always predicts 0, is correct on 2 of the arguments in J . Thus, S <ref> [2] </ref> 0 is learnable with frequency 2 out of 5 by this algorithm. <p> Furthermore, the learning algorithm predicts correctly with density 1 2 , since, for any n, f is 0 on at least half of the inputs from I 1 [ I 2 [ : : : [ I n . For the general case S <ref> [2] </ref> and any interval J = fx; x + 1; x + 2g of length 3 one knows that f must coincide with some g 2 S either on fx; x + 1g or fx + 1; x + 2g. <p> So if f (x + 1) 6= 0 then 19 either f (x) = 0 or f (x + 2) = 0. Therefore the algorithm to predict always 0 is again a frequency learner for S <ref> [2] </ref> | but with the reduced rate 1 out of 3. The class S [p] is also learnable with frequency 2 out of 6 and | by Fact 2.3 (b) | S [p] is also learnable with (uniform) density 1 3 . <p> On the other hand one should note that every f , with f (3x + 1) = 0, for all x, is in S <ref> [2] </ref>: one can take intervals as f3x; 3x + 1g and f3x + 1; 3x + 2g. So, for each predictor M , there is an f which differs from the predictions for all inputs of the form 3x + 2 and 3x + 3.
Reference: [3] <author> A. Blum and P. Chalasani. </author> <title> Learning switching concepts. </title> <booktitle> In Proceedings of the fifth Annual Workshop on Computational Learning Theory, </booktitle> <address> Pittsburgh, Pennsylvania, </address> <pages> pages 231-242. </pages> <publisher> ACM Press Computer Society Press, </publisher> <year> 1992. </year>
Reference-contexts: 1 Introduction In many machine learning situations, the concepts to be learned or the concepts auxiliarily useful to learn may drift with time <ref> [2, 3, 5, 6, 8, 10, 17] </ref>. As in the just previous references, to sufficiently track drifting concepts to permit learning something of them at all, it is necessary to consider some restrictions on the nature of the drift. <p> As in the just previous references, to sufficiently track drifting concepts to permit learning something of them at all, it is necessary to consider some restrictions on the nature of the drift. For example, Helmbold and Long [8] bound the probability of disagreement between subsequent concepts. Blum and Chalasani <ref> [3] </ref> place some constraints on how many different concepts may be used, or the frequency of concept switches. Bartlett, Ben-David and Kulkarni [2] consider `class of legal function sequences' based on some constraints (such as being formed from a walk on a directed graph). <p> Example 4.1 There is a class S and permanence p such that S [p] is frequency 2 out of 3 learnable but not frequency 3 out of 4 learnable even under computable concept drift. Proof. Let S be the class of all constant functions and consider the class S <ref> [3] </ref> 0 . It is easy to see that the algorithm to predict the last value succeeds to show that S [3] is learnable with frequency 2 out of 3. <p> Proof. Let S be the class of all constant functions and consider the class S <ref> [3] </ref> 0 . It is easy to see that the algorithm to predict the last value succeeds to show that S [3] is learnable with frequency 2 out of 3.
Reference: [4] <author> J. Case and C. Smith. </author> <title> Comparison of identification criteria for machine inductive inference. </title> <journal> Theoretical Computer Science, </journal> <volume> 25 </volume> <pages> 193-220, </pages> <year> 1983. </year>
Reference-contexts: So the class S is learnable by approximations with density 1. The above example is also predictable with respect to all criteria defined in Section 1. This does not generalize since self-reference | as in <ref> [4] </ref> | might be employed. There are functions which, on input 0, output a program for the function itself. So one has that they are learnable with respect to all the criteria of Fulk and Jain: they are even "finitely" learnable. <p> natural to ask whether some relation of the kind NV = PEx holds | where NV is the criterion to predict a function almost everywhere correctly by a total machine and PEx is to infer the function by a learner who on every input outputs only programs of total functions <ref> [4] </ref>. The next theorem shows that only one direction holds.
Reference: [5] <author> M. Devaney and A. Ram. </author> <title> Dynamically adjusting concepts to accommodate changing contexts. </title> <booktitle> In Proceedings of the ICML-96 Pre-Conference Workshop on Learning in Context-Sensitive Domains, </booktitle> <address> Bari, Italy, </address> <year> 1994. </year> <note> Journal submission. </note>
Reference-contexts: 1 Introduction In many machine learning situations, the concepts to be learned or the concepts auxiliarily useful to learn may drift with time <ref> [2, 3, 5, 6, 8, 10, 17] </ref>. As in the just previous references, to sufficiently track drifting concepts to permit learning something of them at all, it is necessary to consider some restrictions on the nature of the drift.
Reference: [6] <author> Y. Freund and Y. Mansour. </author> <title> Learning under persistent drift. </title> <editor> In S. Ben-David, editor, </editor> <booktitle> Proceedings of the Third European Conference on Computational Learning Theory (EuroCOLT'97), volume 1208 of Lecture Notes in Computer Science, </booktitle> <pages> pages 94-108. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1997. </year>
Reference-contexts: 1 Introduction In many machine learning situations, the concepts to be learned or the concepts auxiliarily useful to learn may drift with time <ref> [2, 3, 5, 6, 8, 10, 17] </ref>. As in the just previous references, to sufficiently track drifting concepts to permit learning something of them at all, it is necessary to consider some restrictions on the nature of the drift.
Reference: [7] <author> M. Fulk and S. Jain. </author> <title> Approximate inference and scientific method. </title> <journal> Information and Computation, </journal> <volume> 114(2) </volume> <pages> 179-191, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: So the pattern 0x1xy generates words like 01, 010, 011, 0010, 00100, 00101 and so on, but it does not generate the words 0000 and 11111 since the constants 0 and 1 cannot be removed. Example 3.6 If S is the class of all erasing pattern languages then S <ref> [7] </ref> can be learned by a martingale but S [p] is not frequency learnable even for very fast growing permanences p. For constant permanence p, it is also impossible to learn S [p] with some density q &gt; 0. Proof. denotes the empty string. <p> In particular m n (since otherwise the pattern could not generate 0 n 11) and thus 0 n 10 is in the language by taking x = 0 n2m 10. An analysis similar to above also holds with 0 and 1 interchanged. Considering S <ref> [7] </ref>, one observes that a martingale can use the fact that no function f 2 S [7] takes the characteristic function 1; 0; 1; 1; 1; 1; 0; 1 on the strings 1 n 00, 1 n 01, 1 n 10, 1 n 11, 0 n+1 00, 0 n+1 01, 0 <p> An analysis similar to above also holds with 0 and 1 interchanged. Considering S <ref> [7] </ref>, one observes that a martingale can use the fact that no function f 2 S [7] takes the characteristic function 1; 0; 1; 1; 1; 1; 0; 1 on the strings 1 n 00, 1 n 01, 1 n 10, 1 n 11, 0 n+1 00, 0 n+1 01, 0 n+1 10, 0 n+1 11 (since either the first four or the last four strings are <p> It follows that M learns S [p] 0 with uniform density 1. 5 Density Learning Of Predictions Versus Of Programs The notions of density learners and uniform density learners presented above in the present paper are different from those introduced by Fulk and Jain <ref> [7, Definitions 6, 7, 9 and 10] </ref>. The differences can be motivated partly by the fact that the functions f 2 S [p] in the general case do not coincide with computable functions on any infinite computable domain. We sketch these differences in this final section. <p> The next theorem shows that only one direction holds. Theorem 5.1 If some general class S (possibly but not necessarily generated by some concept drift ) is learnable with (uniform) density q &gt; 0 in the manner defined by Fulk and Jain <ref> [7] </ref> and if this learner outputs for any input only programs for total functions, then S is also predictable with (uniform) density q as defined in the Definition 1.4 above. <p> Proof. Let ' i denote the partial computable function computed by program i [13]. Suppose M learns with (uniform) density q in the manner defined by Fulk and Jain <ref> [7] </ref> and that M outputs, for any input, only programs for total functions. <p> We now show that, for every computable learner M , there is a function f of the given 23 form such that f does not coincide with any index output by M on an infinite set | this directly implies the non-learnability under all the criteria of Fulk and Jain <ref> [7] </ref>. Let e 0 ; e 1 ; : : : be an enumeration of all programs output by M on some input. All these programs compute total functions. <p> Thus it coincides with a given function ' e n on at most n 2 inputs. Thus M does not output any function which agrees with f on an infinite set. Therefore M does not learn f under the learning criteria given by Fulk and Jain <ref> [7] </ref>. A similar result to the previous one is the following which of course also holds with uniform density in place of density.
Reference: [8] <author> D. Helmbold and P. </author> <title> Long. Tracking drifting concepts by minimizing disagreements. </title> <journal> Machine Learning, </journal> <volume> 14 </volume> <pages> 27-46, </pages> <year> 1994. </year> <month> 25 </month>
Reference-contexts: 1 Introduction In many machine learning situations, the concepts to be learned or the concepts auxiliarily useful to learn may drift with time <ref> [2, 3, 5, 6, 8, 10, 17] </ref>. As in the just previous references, to sufficiently track drifting concepts to permit learning something of them at all, it is necessary to consider some restrictions on the nature of the drift. <p> As in the just previous references, to sufficiently track drifting concepts to permit learning something of them at all, it is necessary to consider some restrictions on the nature of the drift. For example, Helmbold and Long <ref> [8] </ref> bound the probability of disagreement between subsequent concepts. Blum and Chalasani [3] place some constraints on how many different concepts may be used, or the frequency of concept switches.
Reference: [9] <author> S. Kaufmann and F. Stephan. </author> <title> Robust learning with infinite additional information. </title> <editor> In S. Ben--David, editor, </editor> <booktitle> Proceedings of the Third European Conference on Computational Learning Theory (EuroCOLT'97), volume 1208 of Lecture Notes in Computer Science, </booktitle> <pages> pages 316-330. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1997. </year>
Reference-contexts: Our work addresses drift in a more general computability setting. In the present paper we consider some pleasantly modest restrictions on the rate with which one concept changes into another, model concepts as functions and employ as our principal learning vehicle (computable) martingale betting strategies <ref> [9, 15] </ref>. N denotes the set of natural numbers f0; 1; 2; : : :g. Functions (as concepts) considered in this paper have domain N or, in some special cases, the set of binary strings f0; 1g fl which is identified with N in a standard way. <p> The learner may be bogged down by difficult predictions even if it has some restricted knowledge which is enough to correctly predict the majority of values. A well-known setting that models such a case is the world of gambling <ref> [9] </ref>. Here a gambler may decide whether and how much to bet on a certain prediction coming true or whether to pass if it is too difficult to make a prediction with a reasonable chance of success.
Reference: [10] <author> M. </author> <title> Kubat. A machine learning based approach to load balancing in computer networks. </title> <journal> Cybernetics and Systems, </journal> <volume> 23 </volume> <pages> 389-400, </pages> <year> 1992. </year>
Reference-contexts: 1 Introduction In many machine learning situations, the concepts to be learned or the concepts auxiliarily useful to learn may drift with time <ref> [2, 3, 5, 6, 8, 10, 17] </ref>. As in the just previous references, to sufficiently track drifting concepts to permit learning something of them at all, it is necessary to consider some restrictions on the nature of the drift.
Reference: [11] <author> N. Littlestone. </author> <title> Mistake Bounds and Logarithmic Linear-Threshold Learning Algorithms. </title> <type> PhD thesis, </type> <institution> University of California, Santa Cruz, </institution> <year> 1989. </year>
Reference-contexts: Thus S can be learned with density q. 6 Some Concluding Remarks Finally, we would like to point out a connection between our model and the mistake-bound learning model of Littlestone <ref> [11] </ref>. 8 Consider the setting from [11] in which a machine M predicts the values of a function f on a sequence of arguments x 0 ; x 1 ; x 2 ; : : : as follows. <p> Thus S can be learned with density q. 6 Some Concluding Remarks Finally, we would like to point out a connection between our model and the mistake-bound learning model of Littlestone <ref> [11] </ref>. 8 Consider the setting from [11] in which a machine M predicts the values of a function f on a sequence of arguments x 0 ; x 1 ; x 2 ; : : : as follows.
Reference: [12] <author> M. Li and P. Vitanyi. </author> <title> An Introduction to Kolmogorov Complexity and Its Applications. </title> <publisher> Springer Verlag, </publisher> <address> Heidelberg, </address> <note> second edition, </note> <year> 1997. </year>
Reference-contexts: There is a random function f which also satisfies this relation for all x 1 <ref> [12] </ref>. This f is in S [p], for any permanence p, since every prefix of f is extended by some g 2 S. On the other hand this sequence is not learnable by a martingale because of its randomness. <p> So let n be so large that 2 n &gt; p and the Kolmogorov complexity <ref> [12] </ref> of any string of the form 0 fl 1 fl 0 fl 1 fl 0 fl 1 fl of length up to 4n + 4 is below 3n 1.
Reference: [13] <author> H. Rogers. </author> <title> Theory of Recursive Functions and Effective Computability. </title> <publisher> McGraw Hill, </publisher> <address> New York, 1967. </address> <publisher> Reprinted, MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: In other words, the set X of all correct predictions need only be of some minimum "density." We employ a notion of density introduced by Tennenbaum <ref> [13, x9.5/9-38] </ref> in formalizing this approach to frequency learners. <p> Hence, there are always situations where that restriction on the drift pays off, that is, where knowledge of some regularity within the drift allows construction of better learning algorithms. Any computability terminology used below and not explained herein may be found in <ref> [13] </ref>. 2 Martingale, Frequency and Density Learners The first result states that everything that can be learned by a frequency learner can also be learned by a martingale learner. <p> However, the subclass S [p] 0 S [p] consists only of functions with finite support and is therefore learnable with uniform density 1. 17 Proof. Let A be an infinite immune set <ref> [13, x8.2] </ref> and define S as g 2 S , g is computable and (8x) [g (x) = 0 _ g (x) 2 A f1; 2; : : : ; xg ] For every predictor M , there is a function f 2 S [p], such that M fails to predict <p> For the positive part, in fact, if the permanence is at least a constant k &gt; 1, then the class can be predicted with density k1 k . Proof. Let ' i denote the partial computable function computed by program i <ref> [13] </ref>. Suppose M learns with (uniform) density q in the manner defined by Fulk and Jain [7] and that M outputs, for any input, only programs for total functions.
Reference: [14] <author> J. Royer. </author> <title> Inductive inference of approximations. </title> <journal> Information and Control, </journal> <volume> 70 </volume> <pages> 156-178, </pages> <year> 1986. </year>
Reference-contexts: Tennenbaum called the limit inferior 4 of the sequence 1 x+1 (A (0) + A (1) + : : : + A (x)) the density of the set A. 5 Royer <ref> [14] </ref> introduced the related notion of uniform density of a set A to be the limit inferior of the sequence minf 1 x+1 (A (y) + A (y + 1) + : : :+ A (y + x)) : y 2 N g. <p> Proof. (a): This implication of learning with uniform density towards learning with normal density follows directly from the definition. The separation follows ideas of Royer <ref> [14] </ref>. Consider the class S of all primitive recursive functions which are 0 on the set X = fx : (9y) [2 y &lt; x &lt; 2 y+1 y] g. Then S [p] contains all total functions which are 0 on X.
Reference: [15] <author> C. Schnorr. </author> <title> Zufalligkeit und Wahrscheinlichkeit. </title> <booktitle> Lecture Notes in Mathematics. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1971. </year>
Reference-contexts: Our work addresses drift in a more general computability setting. In the present paper we consider some pleasantly modest restrictions on the rate with which one concept changes into another, model concepts as functions and employ as our principal learning vehicle (computable) martingale betting strategies <ref> [9, 15] </ref>. N denotes the set of natural numbers f0; 1; 2; : : :g. Functions (as concepts) considered in this paper have domain N or, in some special cases, the set of binary strings f0; 1g fl which is identified with N in a standard way. <p> It follows that the sum f (0) + f (1) + : : : + f (x) is greater than x 2 12 (log (x)) 2 and the sequence x ! f (0)+f (1)+:::+f (x) the limit inferior (and also the limit superior) 1 2 . (d): Schnorr <ref> [15, Section 10] </ref> shows that every binary function not learnable by a martingale satisfies the law of large numbers, that is, the density of 1's converges to 1 2 . <p> The other way, to fix the parameter but not the machine, does not help since every computable function is predictable with frequency 1 out of 1 | by its own program | but the class of all computable functions is not learnable by a martingale <ref> [15] </ref>. 3 Concrete Classes In this section optimal and nearly optimal bounds are derived for the permanence necessary and sufficient to learn certain concrete classes under drift. Suppose S is a class of up to k binary functions.
Reference: [16] <author> D. Widder. </author> <title> Advanced Calculus. </title> <publisher> Prentice-Hall, </publisher> <address> NJ, </address> <note> second edition, </note> <year> 1961. </year>
Reference-contexts: (or succeeds on f or wins on f ) if and only if the function x ! m (f (0)f (1) : : : f (x)) is not bounded by any constant. 4 The definition of the limit inferior can be found in most advanced calculus text books, for example, <ref> [16] </ref>.
Reference: [17] <author> S. Wrobel. </author> <title> Concept Formation and Knowledge Revision. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1994. </year> <month> 26 </month>
Reference-contexts: 1 Introduction In many machine learning situations, the concepts to be learned or the concepts auxiliarily useful to learn may drift with time <ref> [2, 3, 5, 6, 8, 10, 17] </ref>. As in the just previous references, to sufficiently track drifting concepts to permit learning something of them at all, it is necessary to consider some restrictions on the nature of the drift.
References-found: 17

