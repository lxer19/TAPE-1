URL: http://s2k-ftp.cs.berkeley.edu:8000/postgres/papers/S2K-95-62.ps.Z
Refering-URL: http://s2k-ftp.cs.berkeley.edu:8000/mariposa/papers.html
Root-URL: 
Email: mariposa@postgres.Berkeley.EDU 1  
Title: Data Replication in Mariposa  
Author: Jeff Sidell, Paul M. Aoki, Sanford Barr, Adam Sah, Carl Staelin Michael Stonebraker and Andrew Yu 
Date: June 8, 1995  
Note: Draft of:  
Address: Berkeley, CA 94720-1776  
Affiliation: Department of Electrical Engineering and Computer Sciences University of California  
Abstract: The Mariposa distributed data manager uses an economic model for managing the allocation of both storage objects and queries to servers. In this paper, we present extensions to the economic model which support replica management, as well as our mechanisms for propagating updates among replicas. We show how our replica control mechanism can be used to provide consistent, although potentially stale, views of data across many machines without expensive per-transaction synchronization. We present a rule-based conflict resolution mechanism, which can be used to enhance traditional time-stamp serialization. We discuss the effects of our replica system on query processing for both read-only and read-write queries. We further demonstrate how the replication model and mechanisms naturally support name service in Mariposa. 
Abstract-found: 1
Intro-found: 1
Reference: [ACHA93] <author> S. Acharya and S. B. Zdonik. </author> <title> An efficient scheme for dynamic data replication. </title> <type> Technical Report CS-93-43, </type> <institution> Brown University, Providence, RI, </institution> <month> Sept. </month> <year> 1993. </year>
Reference-contexts: Classic work on replica management concentrated on the file allocation problem [CHU69, DOWD82] | that is, the problem of finding optimal static data layouts. More recent work has been done to develop techniques that adjust the data layout dynamically, such as learning algorithms <ref> [WOLF92, ACHA93] </ref>. Data replication entails maintaining physical and/or semantic consistency of the various copies. There has also been a tremendous amount of work on this problem, known as replica control. See [CHEN92] for a useful overview. Classic distributed data managers require that copies be kept fully consistent.
Reference: [AGRA93] <author> D. Agrawal and S. Sengupta. </author> <title> Modular synchronization in distributed, multiversion databases: Version control and concurrency control. </title> <journal> IEEE Trans. on Knowledge and Data Eng., </journal> <volume> 5(1) </volume> <pages> 126-137, </pages> <month> Feb. </month> <year> 1993. </year>
Reference-contexts: However, because the expense of synchronizing updates remains relatively high, work has also been done in constructing weak consistency models. These models typically place bounds on one or more divergence parameters. For example, some systems place bounds on time (temporal divergence control) <ref> [AGRA93] </ref> or the number of update transactions (value-based divergence control) [KRIS91]. Finally, other systems focus on flexible specification of divergence parameters. Both quasi-copies [ALON90] and epsilon serializability [PU91] permit value-based and temporal divergence control on the underlying data.
Reference: [ALON90] <author> R. Alonso, D. Barbara, and H. Garcia-Molina. </author> <title> Data caching issues in an informational retrieval system. </title> <journal> ACM Trans. on Database Sys., </journal> <volume> 15(3) </volume> <pages> 359-384, </pages> <month> Sept. </month> <year> 1990. </year>
Reference-contexts: These models typically place bounds on one or more divergence parameters. For example, some systems place bounds on time (temporal divergence control) [AGRA93] or the number of update transactions (value-based divergence control) [KRIS91]. Finally, other systems focus on flexible specification of divergence parameters. Both quasi-copies <ref> [ALON90] </ref> and epsilon serializability [PU91] permit value-based and temporal divergence control on the underlying data. Bayou [TERR94] takes a slightly different approach, providing various kinds of intuitive consistency guarantees for each user "session" by controlling data divergence as well as using information specific to a session's read/write dependencies.
Reference: [CHEN92] <author> S.-W. Chen and C. Pu. </author> <title> A structural classification of integrated replica control mechanisms. </title> <type> Technical Report CUCS-006-92, </type> <institution> Columbia Univ., </institution> <address> New York, NY, </address> <year> 1992. </year>
Reference-contexts: Data replication entails maintaining physical and/or semantic consistency of the various copies. There has also been a tremendous amount of work on this problem, known as replica control. See <ref> [CHEN92] </ref> for a useful overview. Classic distributed data managers require that copies be kept fully consistent. Because of this, considerable effort has gone into improving the basic techniques for ensuring this kind of consistency, such as two-phase commit [ELAB85, SAMA93].
Reference: [CHU69] <author> W. W. Chu. </author> <title> Optimal file allocation in a multiple computer system. </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-18(10):885-889, </volume> <month> Oct. </month> <year> 1969. </year>
Reference-contexts: The number and placement of replicated objects affects both the performance and the availability of a distributed database. The problem of optimizing this aspect of the physical database design is known as replica management. Classic work on replica management concentrated on the file allocation problem <ref> [CHU69, DOWD82] </ref> | that is, the problem of finding optimal static data layouts. More recent work has been done to develop techniques that adjust the data layout dynamically, such as learning algorithms [WOLF92, ACHA93]. Data replication entails maintaining physical and/or semantic consistency of the various copies.
Reference: [DOWD82] <author> L. W. Dowdy and D. V. Foster. </author> <title> Comparative models of the file assignment problem. </title> <journal> Computing Surveys, </journal> <volume> 14(2) </volume> <pages> 287-313, </pages> <month> June </month> <year> 1982. </year>
Reference-contexts: The number and placement of replicated objects affects both the performance and the availability of a distributed database. The problem of optimizing this aspect of the physical database design is known as replica management. Classic work on replica management concentrated on the file allocation problem <ref> [CHU69, DOWD82] </ref> | that is, the problem of finding optimal static data layouts. More recent work has been done to develop techniques that adjust the data layout dynamically, such as learning algorithms [WOLF92, ACHA93]. Data replication entails maintaining physical and/or semantic consistency of the various copies.
Reference: [ELAB85] <author> A. El Abbadi, D. Skeen, and F. Cristian. </author> <title> An efficient, fault-tolerant protocol for replicated data management. </title> <booktitle> Proc. 4th ACM SIGACT-SIGMOD Conf. on Principles of Database Sys., </booktitle> <pages> pages 215-228, </pages> <month> Mar. </month> <year> 1985. </year>
Reference-contexts: See [CHEN92] for a useful overview. Classic distributed data managers require that copies be kept fully consistent. Because of this, considerable effort has gone into improving the basic techniques for ensuring this kind of consistency, such as two-phase commit <ref> [ELAB85, SAMA93] </ref>. However, because the expense of synchronizing updates remains relatively high, work has also been done in constructing weak consistency models. These models typically place bounds on one or more divergence parameters.
Reference: [GARC82] <author> H. Garcia-Molina and G. Wiederhold. </author> <title> Read-only transactions in a distributed database. </title> <journal> ACM Trans. on Database Sys., </journal> <volume> 7(2) </volume> <pages> 209-234, </pages> <month> June </month> <year> 1982. </year>
Reference-contexts: Mariposa provides consistency at the cost of greater staleness. We provide a transactionally consistent view of the data as of a time in the past. The services provided by the Mariposa storage manager permit a particularly natural kind of time-vintage/time-bounded <ref> [GARC82] </ref> query model. 3 Economic Replica Management This section describes replica management in Mariposa: the mechanism by which a Mariposa site acquires, maintains and discards copies of an object. The copy mechanism we describe provides the basis for the discussion of read and read/write semantics in Section 5.
Reference: [KRIS91] <author> N. Krishnakumar and A. J. Bernstein. </author> <title> Bounded ignorance in replicated systems. </title> <booktitle> Proc. 10th ACM SIGACT-SIGMOD Conf. on Principles of Database Sys., </booktitle> <pages> pages 63-74, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: These models typically place bounds on one or more divergence parameters. For example, some systems place bounds on time (temporal divergence control) [AGRA93] or the number of update transactions (value-based divergence control) <ref> [KRIS91] </ref>. Finally, other systems focus on flexible specification of divergence parameters. Both quasi-copies [ALON90] and epsilon serializability [PU91] permit value-based and temporal divergence control on the underlying data.
Reference: [PU91] <author> C. Pu and A. Leff. </author> <title> Replica control in distributed systems: An asynchronous approach. </title> <booktitle> Proc. 1991 ACM SIGMOD Conf. on Management of Data, </booktitle> <pages> pages 377-386, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: These models typically place bounds on one or more divergence parameters. For example, some systems place bounds on time (temporal divergence control) [AGRA93] or the number of update transactions (value-based divergence control) [KRIS91]. Finally, other systems focus on flexible specification of divergence parameters. Both quasi-copies [ALON90] and epsilon serializability <ref> [PU91] </ref> permit value-based and temporal divergence control on the underlying data. Bayou [TERR94] takes a slightly different approach, providing various kinds of intuitive consistency guarantees for each user "session" by controlling data divergence as well as using information specific to a session's read/write dependencies.
Reference: [SAH94] <author> A. Sah, J. Blow, and B. Dennis. </author> <title> An introduction to the Rush language. </title> <booktitle> Proc. 1994 Tcl Conf. (Tcl'94), </booktitle> <pages> pages 105-116, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Copies are at the granularity of fragments. * Rule-based system management: The behavior of Mariposa sites is controlled by a production rule system, provided as part of the Rush scripting language <ref> [SAH94] </ref>. Rules are of the form on event where condition do action. The rule system is intended as a flexible tool to implement policy governing buying and selling data, pricing for query processing, etc. In [STON94b], we expressed query processing in terms of the following economic framework.
Reference: [SAMA93] <author> G. Samaras, K. Britton, A. Citron, and C. Mohan. </author> <title> Two-phase commit optimizations and tradeoffs in the commercial environment. </title> <booktitle> Proc. 9th Int. Conf. on Data Engineering, </booktitle> <pages> pages 520-529, </pages> <month> Apr. </month> <year> 1993. </year>
Reference-contexts: See [CHEN92] for a useful overview. Classic distributed data managers require that copies be kept fully consistent. Because of this, considerable effort has gone into improving the basic techniques for ensuring this kind of consistency, such as two-phase commit <ref> [ELAB85, SAMA93] </ref>. However, because the expense of synchronizing updates remains relatively high, work has also been done in constructing weak consistency models. These models typically place bounds on one or more divergence parameters.
Reference: [STON87] <author> M. Stonebraker. </author> <title> The design of the POSTGRES storage system. </title> <booktitle> Proc. 13th Int. Conf on Very Large Data Bases, </booktitle> <pages> pages 289-300, </pages> <month> Sept. </month> <year> 1987. </year> <month> 22 </month>
Reference-contexts: All three techniques take advantage of certain aspects of the Mariposa storage system, which we briefly discuss at this time. Mariposa uses the POSTGRES "no overwrite" storage system <ref> [STON87] </ref>. Each record has an object identifier (OID), a time stamp (TMIN) at which it becomes valid and another timestamp (TMAX) at which it ceases to be valid. An insert at a site S causes a new record to be constructed with an OID and TMIN field but not TMAX.
Reference: [STON94a] <author> M. Stonebraker, P. M. Aoki, R. Devine, W. Litwin, and M. Olson. Mariposa: </author> <title> A new architecture for distributed data. </title> <booktitle> Proc. 10th Int. Conf. on Data Engineering, </booktitle> <pages> pages 54-65, </pages> <month> Feb. </month> <year> 1994. </year>
Reference-contexts: 1 Introduction In this paper we describe replica management in the Mariposa distributed data base management system <ref> [STON94a] </ref>. There has been considerable research devoted to studying replica management, resulting in a wide variety of proposed solutions. In Mariposa we have adopted an economic framework for managing data objects and query processing [STON94b]. <p> Doing so would require that every site send a message to every name service every time it created, dropped or moved a fragment. Hence, the original Mariposa design <ref> [STON94a] </ref> assumed that name servers could have a specified quality-of-service, which was the degree of staleness they maintained. Name service therefore fits very naturally into the consistency model described in the previous sections.
Reference: [STON94b] <author> M. Stonebraker, R. Devine, M. Kornacker, W. Litwin, A. Pfeffer, and C. Staelin. </author> <title> An economic paradigm for query processing and data migration in Mariposa. </title> <booktitle> Proc. 3rd Int. Symp. on Parallel and Distributed Info. Sys., </booktitle> <pages> pages 58-67, </pages> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: There has been considerable research devoted to studying replica management, resulting in a wide variety of proposed solutions. In Mariposa we have adopted an economic framework for managing data objects and query processing <ref> [STON94b] </ref>. This paper describes how we propose to integrate replica management and replica control into this framework. In addition to describing the design of our replication system, we discuss the impact of replication on query y Author's present address: Hewlett-Packard Laboratories, M/S 1U-13, P.O. <p> Rules are of the form on event where condition do action. The rule system is intended as a flexible tool to implement policy governing buying and selling data, pricing for query processing, etc. In <ref> [STON94b] </ref>, we expressed query processing in terms of the following economic framework. A user presents a query to a broker module. Along with the query, the user specifies a bid curve. <p> The remainder of the paper is structured as follows: In Section 2, we cover related work. Section 3 3 and Section 4 present the basic replication management and control mechanism. Section 5 presents a strategy for query optimization and execution that generalizes and modifies the proposal in <ref> [STON94b] </ref>. Section 6 describes how the same replica control mechanisms and strategies can be used for name service. <p> The seller can bid on queries whose range can be satisfied. All are reasonable strategies, and a site can execute any one of them. 7 Conclusions In this paper, we have shown how the economic mechanisms described in <ref> [STON94b] </ref> can be extended to support replicated fragments. Replica management falls neatly into the economic model. Sites buy and sell copies of fragments in response to changing activity. We have also defined replica control in terms of the economic model.
Reference: [STON90] <author> M. Stonebraker, A. Jhingran, J. Goh, and S. Potamianos. </author> <title> On rules, procedures, caching and views in data base systems. </title> <booktitle> Proc. 1990 ACM SIGMOD Conf. on Management of Data, </booktitle> <pages> pages 281-290, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: The first mechanism by which update streams are generated, triggers, takes advantage of the Postgres rule system <ref> [STON90] </ref>.
Reference: [TERR94] <author> D. B. Terry, A. J. Demers, K. Petersen, M. J. Spreitzer, M. M. Theimer, and B. B. Welch. </author> <title> Session guarantees for weakly consistent replicated data. </title> <booktitle> Proc. 3rd Int. Symp. on Parallel and Distributed Info. Sys., </booktitle> <pages> pages 140-149, </pages> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: Finally, other systems focus on flexible specification of divergence parameters. Both quasi-copies [ALON90] and epsilon serializability [PU91] permit value-based and temporal divergence control on the underlying data. Bayou <ref> [TERR94] </ref> takes a slightly different approach, providing various kinds of intuitive consistency guarantees for each user "session" by controlling data divergence as well as using information specific to a session's read/write dependencies.
Reference: [WOLF92] <author> O. Wolfson and S. Jajodia. </author> <title> An algorithm for dynamic data distribution. </title> <booktitle> Proc. 2nd Wksp. on the Management of Replicated Data, </booktitle> <pages> pages 62-65, </pages> <month> Nov. </month> <year> 1992. </year> <month> 23 </month>
Reference-contexts: Classic work on replica management concentrated on the file allocation problem [CHU69, DOWD82] | that is, the problem of finding optimal static data layouts. More recent work has been done to develop techniques that adjust the data layout dynamically, such as learning algorithms <ref> [WOLF92, ACHA93] </ref>. Data replication entails maintaining physical and/or semantic consistency of the various copies. There has also been a tremendous amount of work on this problem, known as replica control. See [CHEN92] for a useful overview. Classic distributed data managers require that copies be kept fully consistent.
References-found: 18

