URL: http://drl.cs.uiuc.edu/pubs/pdpta98.ps
Refering-URL: http://drl.cs.uiuc.edu/pubs/pdpta98.html
Root-URL: http://www.cs.uiuc.edu
Title: Collective I/O on a SGI Cray Origin 2000: Strategy and Performance  
Author: Y. Cho, M. Winslett, J. Lee, Y. Chen S. Kuo, K. Motukuri 
Keyword: Collective I/O, Distributed shared memory system, Shared file system  
Address: Urbana, IL, U.S.A.  
Affiliation: Department of Computer Science, University of Illinois  
Abstract: Panda is a library for collective I/O of multidimensional arrays, designed for SPMD-style applications running on a distributed memory system. In this paper, we describe our experience of porting Panda to the SGI Cray Origin 2000, which utilizes shared memory and a shared file system. On the Origin we used, RAIDs and file system inappropriately configured for scientific applications is the limiting factor for I/O performance, to such a degree that a single I/O node can nearly saturate the file system, limiting scalability. We determined that Panda would scale up nicely with faster RAIDs and a file system configuration highly tuned for large scientific applications, as each of Panda's I/O nodes can deliver data to the Origin's file system at a minimum sustained rate of 80 MB/sec, with 1-4 I/O nodes. We also determined that nodes do not need to be dedicated to I/O to sustain high utilization of the Origin's file system. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. E. Seamons, Y. Chen, P. Jones, J. Jozwiak, and M. Winslett. </author> <title> Server (a) (b) Directed Collective I/O in Panda. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <month> November </month> <year> 1995. </year>
Reference-contexts: Panda supports HPF-style BLOCK, CYCLIC, fl and AMR-style data distributions across the multiple compute nodes on which Panda clients are running. Panda's approach to high-performance I/O in this environment is called server-directed I/O <ref> [1] </ref>. memory and on disk provided by Panda. (BLOCK, BLOCK) across 4 compute nodes arranged in a 2fi2 mesh. Each piece of the distributed array is called a compute chunk, and each compute chunk resides in the memory of one compute node. <p> As hypothesized earlier, the Origin's very fast interconnect makes a careful choice of I/O nodes unnecessary. We conclude that there is no advantage to dedicating nodes to I/O on the Origin 2000. 5 Related work Numerous runtime libraries and file systems provide collective I/O <ref> [6, 1, 7, 8, 5, 9] </ref>, but performance studies of these libraries on the Origin 2000 are just beginning.
Reference: [2] <institution> Origin Servers Technical Overview. </institution> <type> Technical report, </type> <institution> Silicon Graphics Inc., </institution> <month> April </month> <year> 1997. </year>
Reference-contexts: Memory is organized by 1 We use the term I/O node to refer to a single processor performing I/O. (a) (b) nodes. 128-byte cache lines and each cache line is associated with data bits for a directory-based cache coherence scheme <ref> [2] </ref>. Whenever a cache line is updated, corresponding data bits are looked up to invalidate other copies of the same cache line. Figure 2 (b) shows the interconnection fabric for 32 processors, our configuration used for experiments. <p> Figure 2 (b) shows the interconnection fabric for 32 processors, our configuration used for experiments. Each pair of nodes is connected to a router and routers form a hypercube <ref> [2] </ref>. Table 1 gives details of the Origin 2000 we used at NCSA. 32 processors (16 nodes) share a total of 16 disks connected to two SCSI-2 RAID level 5 adapters. The two RAIDs are striped via XLV (XFS volume manager) to create a single logical volume. <p> We focus on write operations because (i) the CSAR applications are simulations, which are write intensive, and (ii) in general, write operations are slower than read operations in RAID level 5 because of parity update <ref> [2] </ref>. Also, we focus on arrays that have different distributions in memory and on disk.
Reference: [3] <editor> Message Passing Toolkit: </editor> <title> MPI Programmer's Manual SR-2197 1.2. </title> <type> Technical report, </type> <institution> Silicon Graphics Inc., </institution> <month> January </month> <year> 1998. </year>
Reference-contexts: To attain this bandwidth when there are multiple processes, users need to tune SGI's environment variables for MPI. For instance, variables MPI BUFS PER HOST and MPI BUFS PER PROC need to be set to minimize contention for the message buffer <ref> [3] </ref>. Small messages (8-16 KB) tend to perform better as the number of sender-receiver pairs increases. This suggests that Panda's buffer size on the I/O nodes should be set so that most data messages are 8-16 KB, if the interconnect ever becomes a limiting factor in performance.
Reference: [4] <author> Y. Cho, M. Winslett, M. Subramaniam, Y. Chen, S. Kuo, and K. E. Seamons. </author> <title> Exploiting Local Data in Parallel Array I/O on a Practical Network of Workstations. </title> <booktitle> In Proceedings of the Fifth Workshop on I/O in Parallel and Distributed Systems, </booktitle> <pages> pages 1-13, </pages> <month> November </month> <year> 1997. </year>
Reference-contexts: The experiments in the next section address the question of how many I/O nodes are needed for peak performance, and whether nodes need to be dedicated to I/O or can be shared between computation and I/O. In previous work <ref> [4] </ref>, we found that part-time I/O gave good performance on a small network of workstations, while maximizing the resources available for computations but only if I/O nodes are very carefully chosen to minimize the data transfer over the interconnect.
Reference: [5] <author> D. Kotz. </author> <title> Disk-directed I/O for MIMD multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 15(1) </volume> <pages> 41-74, </pages> <month> February </month> <year> 1997. </year>
Reference-contexts: On Origins with faster file system configurations, the extra cost of strided accesses will be even more apparent. The use of separate threads for communication and disk access, as is done in implementations of disk-directed I/O <ref> [5] </ref>, would alleviate this problem. We also tested part-time I/O using 16 compute nodes, where the first k compute nodes become I/O nodes at I/O time. As shown in the second graph in Figure 6, we obtain almost the same aggregate throughput as with dedicated I/O nodes. <p> As hypothesized earlier, the Origin's very fast interconnect makes a careful choice of I/O nodes unnecessary. We conclude that there is no advantage to dedicating nodes to I/O on the Origin 2000. 5 Related work Numerous runtime libraries and file systems provide collective I/O <ref> [6, 1, 7, 8, 5, 9] </ref>, but performance studies of these libraries on the Origin 2000 are just beginning.
Reference: [6] <author> R. Bordawekar, J. Rosario, and A. Choud-hary. </author> <title> Design and Evaluation of Primitives for Parallel I/O. </title> <booktitle> In Proceedings of the Supercomputing '93, </booktitle> <pages> pages 452-461, </pages> <year> 1993. </year>
Reference-contexts: As hypothesized earlier, the Origin's very fast interconnect makes a careful choice of I/O nodes unnecessary. We conclude that there is no advantage to dedicating nodes to I/O on the Origin 2000. 5 Related work Numerous runtime libraries and file systems provide collective I/O <ref> [6, 1, 7, 8, 5, 9] </ref>, but performance studies of these libraries on the Origin 2000 are just beginning.
Reference: [7] <author> R. Bennett, K. Bryant, A. Sussman, R. Das, and J. Saltz. Jovian: </author> <title> Framework for Optimizing Parallel I/O. </title> <booktitle> In Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <pages> pages 10-20, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: As hypothesized earlier, the Origin's very fast interconnect makes a careful choice of I/O nodes unnecessary. We conclude that there is no advantage to dedicating nodes to I/O on the Origin 2000. 5 Related work Numerous runtime libraries and file systems provide collective I/O <ref> [6, 1, 7, 8, 5, 9] </ref>, but performance studies of these libraries on the Origin 2000 are just beginning.
Reference: [8] <author> P. F. Corbett, D. G. Feitelson, J. Prost, and S. J. Bayler. </author> <title> Parallel Access to Files in the Vesta File System. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 472-481, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: As hypothesized earlier, the Origin's very fast interconnect makes a careful choice of I/O nodes unnecessary. We conclude that there is no advantage to dedicating nodes to I/O on the Origin 2000. 5 Related work Numerous runtime libraries and file systems provide collective I/O <ref> [6, 1, 7, 8, 5, 9] </ref>, but performance studies of these libraries on the Origin 2000 are just beginning.
Reference: [9] <author> Rajesh Bordawekar. </author> <title> Implementation of Collective I/O in the Intel Paragon Parallel File Systems: Initial Experiences. </title> <booktitle> In Proceedings of International Conference on Supercomputing, </booktitle> <pages> pages 20-28, </pages> <month> July </month> <year> 1997. </year>
Reference-contexts: As hypothesized earlier, the Origin's very fast interconnect makes a careful choice of I/O nodes unnecessary. We conclude that there is no advantage to dedicating nodes to I/O on the Origin 2000. 5 Related work Numerous runtime libraries and file systems provide collective I/O <ref> [6, 1, 7, 8, 5, 9] </ref>, but performance studies of these libraries on the Origin 2000 are just beginning.
Reference: [10] <author> R. Thakur, W. Gropp, and E. Lusk. </author> <title> A Case for Using MPI's Derived Datatypes to Improve I/O Performance. </title> <type> Technical Report Preprint ANL/MCS-P717-0598, </type> <institution> Argonne National Laboratory, </institution> <month> May </month> <year> 1998. </year>
Reference-contexts: ROMIO is an implementation of the MPI-IO standard interface which uses two-phase I/O for collective I/O operations <ref> [10] </ref>. [10] presents collective read performance on a variety of parallel platforms including the Origin 2000, for a 3D array distributed (BLOCK, BLOCK, BLOCK) in memory and (BLOCK, *, *) on disk, using all the compute processors as I/O processors at I/O time as with our part-time I/O nodes. <p> ROMIO is an implementation of the MPI-IO standard interface which uses two-phase I/O for collective I/O operations <ref> [10] </ref>. [10] presents collective read performance on a variety of parallel platforms including the Origin 2000, for a 3D array distributed (BLOCK, BLOCK, BLOCK) in memory and (BLOCK, *, *) on disk, using all the compute processors as I/O processors at I/O time as with our part-time I/O nodes. <p> Our results in Figure 4 suggest that fewer I/O nodes might perform better. Alternatively, our results using a simulated faster file system (Figure 5) coupled with the performance results in <ref> [10] </ref>, suggest that server-directed I/O may have significant performance advantage over two-phase I/O for the types of array I/O operations discussed in this paper. 6 Summary and Conclusion We have ported the Panda parallel I/O library, designed for collective I/O of SPMD-style applications running on a distributed memory system, to a
References-found: 10

