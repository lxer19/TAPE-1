URL: http://diva.eecs.berkeley.edu/~ananth/IDwithFeedback.ps.gz
Refering-URL: http://diva.eecs.berkeley.edu/~ananth/
Root-URL: http://www.cs.berkeley.edu
Title: Identification plus Transmission over Channels with Perfect Feedback  
Author: S. Venkatesan V. Anantharam zx 
Keyword: Identification theory, identification plus transmission, point to-multipoint communication.  
Address: Berkeley.  Cory Hall,  ley, Berkeley, CA 94720.  
Affiliation: Cornell University and U.C. Berkeley. Univ. of California,  Dept. of EECS, U.C. Berke  
Note: Research supported by NSF IRI 9005849, IRI 9310670, NCR 9422513, and the AT&T Foundation.  Address all correspondence to the first author: 341,  
Date: 24 March 1996  
Abstract: We study the following problem: a transmitter is connected to N different receivers by a single discrete memoryless channel W , with positive Shannon capacity C, whose output is available to all the receivers and the transmitter, i.e., there is perfect and instantaneous feedback. In n channel uses, the transmitter wishes to send one of M messages to one of the N receivers, whose identity is a priori unknown to the receivers. The requirements are that the intended recipient should decode the transmitted message correctly with high probability, and any other receiver should recognize with high probability that the message is not intended for it (these probabilities are averaged over the M messages). In this situation, we prove that M and N can simultaneously grow as exp[nR 1 o(n)] and expfexp[nR 2 o(n)]g, respectively, for all rate-pairs (R 1 ; R 2 ) satisfying R 1 C and R 2 max P :I(P ;W )R 1 H(P W ), provided the transmitter can use randomized encoding. If the encoding must be deterministic, we prove that all rate-pairs (R 1 ; R 2 ) satisfying R 1 C and R 2 R 1 + max P :I(P ;W )R 1 H(W jP ) are achievable. In both cases, we establish these rate regions as optimal by proving "strong" converses. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Ahlswede and G. Dueck. </author> <title> Identification in the presence of feedback a discovery of new capacity formulas. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. 35(No. 1), </volume> <month> January </month> <year> 1989. </year>
Reference-contexts: It is well known that feedback does not increase the transmission capacity of a DMC. In marked contrast, feedback can have a dramatic effect on the identification capacity of a DMC, as demonstrated by Ahlswede and Dueck in <ref> [1] </ref> (the sequel to [2]). There, it was shown that feedback allows positive second-order identification rates even when the encoding has to be deterministic. <p> As in <ref> [1] </ref>, we consider both the case where randomized encoding is allowed, and the case where the encoding is restricted to be deterministic. <p> The identification theorems of <ref> [1] </ref> can be viewed as special cases of the above results, obtained by setting the transmission rate requirement R 1 to zero. As in [1], the converses proved here are "strong." As a by-product of these converses, we also have a new proof of the strong converse to Shannon's coding theorem <p> The identification theorems of <ref> [1] </ref> can be viewed as special cases of the above results, obtained by setting the transmission rate requirement R 1 to zero. As in [1], the converses proved here are "strong." As a by-product of these converses, we also have a new proof of the strong converse to Shannon's coding theorem for DMCs with feedback, a result first proved by Kemperman [6]. <p> In the presence of feedback, the transmitter can choose its channel input at each step of communication based on the outputs from all previous steps, according to some feedback strategy. As in <ref> [1] </ref>, we will consider strategies that can be randomized, as well as strategies that must be deterministic. <p> The idea of using channel noise to generate randomness for identification coding in the presence of feedback is one of the key principles in the results of <ref> [1] </ref>. The details of the idea there are somewhat different, and the method outlined here (based on the second part of Lemma 3.1) seems to allow a somewhat simpler analysis. Lemma 3.2 is based on the arguments in Section III of [1], though it is not stated there in this form. <p> one of the key principles in the results of <ref> [1] </ref>. The details of the idea there are somewhat different, and the method outlined here (based on the second part of Lemma 3.1) seems to allow a somewhat simpler analysis. Lemma 3.2 is based on the arguments in Section III of [1], though it is not stated there in this form. It is the essence of the " p n trick" of [1] | so called because the equivalent of the array element encoding was done there with about p n channel uses | and can in fact be used to prove <p> Lemma 3.2 is based on the arguments in Section III of <ref> [1] </ref>, though it is not stated there in this form. It is the essence of the " p n trick" of [1] | so called because the equivalent of the array element encoding was done there with about p n channel uses | and can in fact be used to prove all known achievability 8 results in identification theory. <p> However, it turns out that we can prove Kemperman's result here with very little additional effort, and will therefore not appeal to it directly. Our proof of this result is different from the original one. The idea for bounding the identification rate is similar to that in <ref> [1] </ref>. Pick any fl 2 (; 1 ), say fl = (1 + )=2. <p> This will prove the lemma because D (pkq) 2 (p q) 2 for all p, q in <ref> [0; 1] </ref> (see Lemma 12.6.1 on p. 300 of [3]), so that if J 2=* then D (*k1=J ) * 2 =2, and (N 1) expfS D (*k1=J )g N exp (S* 2 =2). Let the first row of the array be arbitrary.
Reference: [2] <author> R. Ahlswede and G. Dueck. </author> <title> Identification via channels. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. 35(No. 1), </volume> <month> January </month> <year> 1989. </year>
Reference-contexts: It is also well known that allowing randomization in the encoding process does not help in any way. On the other hand, if M = 1, we have the identification problem introduced by Ahlswede and Dueck <ref> [2] </ref>. Here, we could think of the transmitter as simply sending an alarm or "identifying" signal to one of the N receivers (there is no message to be transmitted). In [2], it was shown that one of N = expfexp [nR o (n)]g receivers (doubly exponential in n) could be reliably <p> On the other hand, if M = 1, we have the identification problem introduced by Ahlswede and Dueck <ref> [2] </ref>. Here, we could think of the transmitter as simply sending an alarm or "identifying" signal to one of the N receivers (there is no message to be transmitted). In [2], it was shown that one of N = expfexp [nR o (n)]g receivers (doubly exponential in n) could be reliably identified in n channel uses, provided randomization was allowed in the encoding process. <p> It is well known that feedback does not increase the transmission capacity of a DMC. In marked contrast, feedback can have a dramatic effect on the identification capacity of a DMC, as demonstrated by Ahlswede and Dueck in [1] (the sequel to <ref> [2] </ref>). There, it was shown that feedback allows positive second-order identification rates even when the encoding has to be deterministic.
Reference: [3] <author> T.M. Cover and J.A. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> John Wiley, </publisher> <year> 1991. </year>
Reference-contexts: This will prove the lemma because D (pkq) 2 (p q) 2 for all p, q in [0; 1] (see Lemma 12.6.1 on p. 300 of <ref> [3] </ref>), so that if J 2=* then D (*k1=J ) * 2 =2, and (N 1) expfS D (*k1=J )g N exp (S* 2 =2). Let the first row of the array be arbitrary. Then, choose a random second row, by picking each element independently and equiprobably from [J ].
Reference: [4] <author> I. Csiszar and J. Korner. </author> <title> Information Theory: Coding Theorems for Discrete Memoryless Systems. </title> <publisher> Academic Press, </publisher> <year> 1981. </year>
Reference-contexts: All logarithms and exponentials will be to the base e. If J is an integer, then [J ] will denote the set f1; 2; : : : ; J g. Finally, the notation for all standard information-theoretic quantities will be that of <ref> [4] </ref>. The discrete memoryless channel (DMC) connecting the transmitter and the receivers is assumed to have finite input and output alphabets X and Y, respectively, and transition probability function W = fW (yjx) : x 2 X ; y 2 Yg. <p> This is of course a standard result in channel coding <ref> [4] </ref> that can be used to prove Shannon's coding theorem for DMCs. <p> Thus, the sets D fl a have all the properties postulated earlier in (6), (8), and (9), and the converses are proved. Appendix Proof of Lemma 3.1: As mentioned earlier, the first part of the lemma is a standard result in channel coding. A proof can be found in <ref> [4] </ref> (Theorem 5.2 on p. 165). We will therefore only sketch the proof of the second part. In the course of the proof, we will make use of some simple results on types, without explicit reference (all of them can be found in [4]). <p> A proof can be found in <ref> [4] </ref> (Theorem 5.2 on p. 165). We will therefore only sketch the proof of the second part. In the course of the proof, we will make use of some simple results on types, without explicit reference (all of them can be found in [4]). Let W n (P ) be the set of those DMCs V (with alphabets X and Y) such that nP (x)V (yjx) is an integer for all x; y.
Reference: [5] <author> T.S. Han and S. Verdu. </author> <title> New results in the theory of identification via channels. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. 38(No. 1), </volume> <month> January </month> <year> 1992. </year>
Reference-contexts: The general problem of simultaneous identification and transmission was studied by Han and Verdu <ref> [5] </ref>, as a variant of the identification problem with potential applications in multiuser communication. <p> However, as pointed out in <ref> [5] </ref>, this scheme is far from optimal because it needlessly conveys the address and message information to all the receivers. <p> This scheme would permit N = expfexp [nffC o (n)]g and M = exp [n (1ff)C o (n)], provided randomization was allowed in the address encoding. While this is certainly better than the straightforward solution, it is still suboptimal. In <ref> [5] </ref>, it was shown that in fact N = expfexp [nC o (n)]g and M = exp [nC o (n)] are simultaneously achievable, if the address and message are jointly encoded using an "identification plus transmission" code (or IT code), instead of separately as above; moreover, the joint encoding does not
Reference: [6] <author> J.H.B. Kemperman. </author> <title> Strong converses for a general memoryless channel with feedback. </title> <journal> Trans. 6th Prague Conf. Information Theory, Stat. </journal> <note> Dec. Fct's and Rand. Proc., 1973. 20 </note>
Reference-contexts: As in [1], the converses proved here are "strong." As a by-product of these converses, we also have a new proof of the strong converse to Shannon's coding theorem for DMCs with feedback, a result first proved by Kemperman <ref> [6] </ref>. We will formulate the problem more precisely in Section 2 and then state our main result in Theorem 2.1. 2 Statement of problem and results The following conventions will be in effect in the rest of the paper. All logarithms and exponentials will be to the base e. <p> The encoding may involve randomization, but this of course does not help at all in the context of transmission codes. Since &lt; 1, the strong converse to Shannon's theorem for DMCs with feedback (first proved by Kemperman <ref> [6] </ref>) yields lim sup n 1 log M n C. This implies that R 1 = lim inf n 1 log M n C, which is the required bound on the transmission rate both for general and deterministic IT codes.
References-found: 6

