URL: http://www.cs.helsinki.fi/~mannila/postscripts/icdt-tutorial.ps
Refering-URL: http://www.cs.helsinki.fi/research/pmdm/datamining/publications.html
Root-URL: 
Email: E-mail: Heikki.Mannila@cs.helsinki.fi  
Title: Methods and problems in data mining  
Author: Heikki Mannila 
Web: URL: http://www.cs.helsinki.fi/~mannila/  
Address: FIN-00014 Helsinki, Finland  
Affiliation: Department of Computer Science University of Helsinki,  
Abstract: Knowledge discovery in databases and data mining aim at semiautomatic tools for the analysis of large data sets. We consider some methods used in data mining, concentrating on levelwise search for all frequently occurring patterns. We show how this technique can be used in various applications. We also discuss possibilities for compiling data mining queries into algorithms, and look at the use of sampling in data mining. We conclude by listing several open research problems in data mining and knowledge discovery.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> R. Agrawal, T. Imielinski, and A. Swami. </author> <title> Mining association rules between sets of items in large databases. </title> <booktitle> In Proceedings of ACM SIGMOD Conference on Management of Data (SIGMOD'93), </booktitle> <pages> pages 207 - 216, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: this section we discuss three data mining problems where instantiations of the above algorithm can be used. 4.1 Association rules Given a schema R = fA 1 ; : : : ; A p g of attributes with domain f0; 1g, and a relation r over R, an association rule <ref> [1] </ref> about r is an expression of the form X ) B, where X R and B 2 R n X. <p> Namely, for each frequent set X and each B 2 X verify whether the rule X n fBg ) B has sufficiently high confidence. How can one find all frequent sets X? This can be done in a multitude of ways <ref> [1, 2, 16, 18, 43, 48] </ref>. A typical approach [2] is to use that fact that all subsets of a frequent set are also frequent. A way of applying the framework of Algorithm Find-frequent-patterns is as follows.
Reference: 2. <author> R. Agrawal, H. Mannila, R. Srikant, H. Toivonen, and A. I. Verkamo. </author> <title> Fast dis covery of association rules. </title> <editor> In U. M. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 307 - 328. </pages> <publisher> AAAI Press, </publisher> <address> Menlo Park, CA, </address> <year> 1996. </year>
Reference-contexts: Namely, for each frequent set X and each B 2 X verify whether the rule X n fBg ) B has sufficiently high confidence. How can one find all frequent sets X? This can be done in a multitude of ways <ref> [1, 2, 16, 18, 43, 48] </ref>. A typical approach [2] is to use that fact that all subsets of a frequent set are also frequent. A way of applying the framework of Algorithm Find-frequent-patterns is as follows. <p> How can one find all frequent sets X? This can be done in a multitude of ways [1, 2, 16, 18, 43, 48]. A typical approach <ref> [2] </ref> is to use that fact that all subsets of a frequent set are also frequent. A way of applying the framework of Algorithm Find-frequent-patterns is as follows. <p> This method has the advantage that rows that do not contribute to any frequent set will not be inspected more than once. For comparisons of the two approaches, see <ref> [2, 18, 43] </ref>. The algorithms described above work quite nicely on large input relations. <p> A simple way of alleviating the problems caused by the volume of data (i.e., the number of rows) is to use sampling. Even small samples can give quite good approximation to the association rules <ref> [2, 48] </ref> or functional dependencies [28] that hold in a relation. See [27] for a general analysis on the relationship between the logical form of the discovered knowledge and the sample sizes needed for discovering it.
Reference: 3. <author> R. Agrawal and K. Shim. </author> <title> Developing tightly-coupled data mining applications on a relational database system. </title> <booktitle> In Proc. of the 2nd Int'l Conference on Knowledge Discovery in Databases and Data Mining, </booktitle> <pages> pages 287-290, </pages> <year> 1996. </year>
Reference-contexts: How useful is the concept of condensed representation? [36] System and language issues 6. What is a good architecture of a data mining system? How should a database management system and the search modules be connected? <ref> [19, 3] </ref> 7. Develop a language for expressing KDD queries and techniques for optimizing such queries. Some suggestions are given in [22, 45, 40]. 8. A subproblem of the previous one: how can caching strategies help in pro cessing sequences of related queries? [19] 9.
Reference: 4. <author> S. Berchtold, D. A. Keim, and H. P. Kriegel. </author> <title> The X-tree: An index structure for high-dimensional data. </title> <booktitle> In Proceedings of the 22nd International Conference on Very Large Data Bases (VLDB'96), </booktitle> <pages> pages 28-29, </pages> <address> Mumbay, India, 1996. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The literature on the area is huge, and too wide to even scratch here. Similarity searches are often needed in data mining applications: how does one find objects that are roughly similar to the a query point. Again, the literature is vast, and we provide only two recent pointers: <ref> [4, 49] </ref>. 2 The KDD process The goal of knowledge discovery is to obtain useful knowledge from large collections of data. Such a task is inherently interactive and iterative: one cannot expect to obtain useful knowledge simply by pushing a lot of data to a black box.
Reference: 5. <author> P. A. Boncz, W. Quak, and M. L. Kersten. </author> <title> Monet and its geographical exten sions: a novel approach to high-performance GIS processing. </title> <editor> In P. M. G. Apers, M. Bouzeghoub, and G. Gardarin, editors, </editor> <booktitle> Advances in Database Technology -EDBT'96, </booktitle> <pages> pages 147-166, </pages> <year> 1996. </year>
Reference-contexts: A very interesting experiment in this direction is the work on the Monet database server developed at CWI in the Netherlands by Martin Kersten and others <ref> [5, 19] </ref>. The Monet system is based on the vertical partitioning of the relations: a relation with k attributes is decomposed into k relations, each with two attributes: the OID and one of the original attributes.
Reference: 6. <author> L. De Raedt and M. Bruynooghe. </author> <title> A theory of clausal discovery. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence (IJCAI-93), </booktitle> <pages> pages 1058 - 1053, </pages> <address> Chambery, France, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This point of view has either implicitly or explicitly been used in discovering integrity constraints from databases, in inductive logic programming, and in machine learning <ref> [6, 7, 26, 30, 32] </ref>; some theoretical results can be found in [37], and a suggested logical formalism in [23]. While the frequency of occurrence of a pattern or the truth of a sentence can defined rigorously, the interestingness of patterns or sentences seems much harder to specify and measure. <p> The problems are very varying, from architectural issues to specific algorithmic questions. For brevity, the descriptions are quite succinct, and I also provide only a couple of references. Framework and general theory 1. Develop a general theory of data mining. Possible starting points are <ref> [6, 7, 23, 26, 30, 37] </ref>. (One might call this the theory of inductive databases.) 2.
Reference: 7. <author> L. De Raedt and S. Dzeroski. </author> <title> First-order jk-clausal theories are PAC-learnable. </title> <journal> Artificial Intelligence, </journal> <volume> 70:375 - 392, </volume> <year> 1994. </year>
Reference-contexts: This point of view has either implicitly or explicitly been used in discovering integrity constraints from databases, in inductive logic programming, and in machine learning <ref> [6, 7, 26, 30, 32] </ref>; some theoretical results can be found in [37], and a suggested logical formalism in [23]. While the frequency of occurrence of a pattern or the truth of a sentence can defined rigorously, the interestingness of patterns or sentences seems much harder to specify and measure. <p> The problems are very varying, from architectural issues to specific algorithmic questions. For brevity, the descriptions are quite succinct, and I also provide only a couple of references. Framework and general theory 1. Develop a general theory of data mining. Possible starting points are <ref> [6, 7, 23, 26, 30, 37] </ref>. (One might call this the theory of inductive databases.) 2.
Reference: 8. <author> T. Eiter and G. Gottlob. </author> <title> Identifying the minimal transversals of a hypergraph and related problems. </title> <journal> SIAM Journal on Computing, </journal> <volume> 24(6):1278 - 1304, </volume> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: Solutions to the two following problems would imply considerable progress for this problem. 13. Finding the keys of a relation contains as a subproblem the problem of finding transversals of hypergraphs <ref> [33, 8, 36] </ref>. Given a hypergraph H, can the set T r (H) of its transversals be computed in time polynomial in jHj and jT r (H)j? 14. When one reduces the problem of finding keys to transversals of hypergraphs, one has to solve the following preliminary problem.
Reference: 9. <author> U. M. Fayyad, S. G. Djorgovski, and N. Weir. </author> <title> Automating the analysis and cata loging of sky surveys. </title> <editor> In U. M. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 471 - 494. </pages> <publisher> AAAI Press, </publisher> <address> Menlo Park, CA, </address> <year> 1996. </year>
Reference-contexts: Efficient support for such iteration is one important development topic in KDD. Prominent applications of KDD include health care data, financial applications, and scientific data [39, 30]. One of the more spectacular applications is the SKICAT system <ref> [9] </ref>, which operates on 3 terabytes of image data, producing a classification of approximately 2 billion sky objects into a few classes. The task is obviously impossible to do manually.
Reference: 10. <author> U. M. Fayyad, G. Piatetsky-Shapiro, and P. Smyth. </author> <title> From data mining to know ledge discovery: An overview. </title> <editor> In U. M. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, pages 1 -34. </booktitle> <publisher> AAAI Press, </publisher> <address> Menlo Park, CA, </address> <year> 1996. </year>
Reference-contexts: Discovering knowledge from data should therefore be seen as a process containing several steps: 1. understanding the domain, 2. preparing the data set, 3. discovering patterns (data mining), 4. postprocessing of discovered patterns, and 5. putting the results into use. See <ref> [10] </ref> for a slightly different process model and excellent discussion.
Reference: 11. <editor> U. M. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, editors. </editor> <title> Ad vances in Knowledge Discovery and Data Mining. </title> <publisher> AAAI Press, </publisher> <address> Menlo Park, CA, </address> <year> 1996. </year>
Reference-contexts: There is a suspicion that there might be nuggets of useful information hiding in the masses of unanalyzed or underanalyzed data, and therefore semiautomatic methods for locating interesting information from data would be useful. Data mining has in the 1990's emerged as visible research and development area; see <ref> [11] </ref> for a recent overview of the area. This tutorial describes some methods of data mining and also lists a variety of open problems, both in the theory of data mining and in the systems side of it. <p> Before starting on the KDD process, I digress briefly to other some topics not treated in this paper. An important issue in data mining is its relationship with machine learning and statistics. I refer to <ref> [11, 31] </ref> for some discussions on this. Visualization of data is an important technique for obtaining useful information from large masses of data. The area is large; see [25] for an overview. Visualization can also be useful for making the discovered patterns easier to understand.
Reference: 12. <author> T. Fukuda et al. </author> <title> Data mining using two-dimensional optimized association rules: Scheme, algorithms, visualization. </title> <booktitle> In Proceedings of ACM SIGMOD Conference on Management of Data (SIGMOD'96), </booktitle> <pages> pages 13-23, </pages> <year> 1996. </year>
Reference-contexts: Some suggestions are given in [22, 45, 40]. 8. A subproblem of the previous one: how can caching strategies help in pro cessing sequences of related queries? [19] 9. Extend the association rule framework to handle attributes with continuous values. (Some partial solutions to this problem are given in <ref> [12, 13, 47] </ref>.) 10. Investigate the usefulness of temporal databases in the mining of event se quence data. 11. Develop tools for selecting, grouping, and visualizing discovered knowledge [29, 30]. How can background knowledge be used? Algorithmic open problems 12.
Reference: 13. <author> T. Fukuda et al. </author> <title> Mining optimized association rules for numeric attributes. </title> <booktitle> In Pro ceedings of the Fifteenth ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems (PODS'96), </booktitle> <year> 1996. </year>
Reference-contexts: Some suggestions are given in [22, 45, 40]. 8. A subproblem of the previous one: how can caching strategies help in pro cessing sequences of related queries? [19] 9. Extend the association rule framework to handle attributes with continuous values. (Some partial solutions to this problem are given in <ref> [12, 13, 47] </ref>.) 10. Investigate the usefulness of temporal databases in the mining of event se quence data. 11. Develop tools for selecting, grouping, and visualizing discovered knowledge [29, 30]. How can background knowledge be used? Algorithmic open problems 12.
Reference: 14. <editor> Z. Galil and E. Ukkonen, editors. </editor> <booktitle> 6th Annual Symposium on Combinatorial Pat tern Matching (CPM 95), volume 937 of Lecture Notes in Computer Science, </booktitle> <address> Berlin, 1995. </address> <publisher> Springer. </publisher>
Reference-contexts: The view of data mining as locating frequently occurring and interesting patterns from data suggests that data mining can benefit from the extensive research done in the area of combinatorial pattern matching (CPM); see, e.g., <ref> [14] </ref>.
Reference: 15. <author> J. Gray, A. Bosworth, A. Layman, and H. Pirahesh. </author> <title> Data Cube: A relational ag gregation operator generalizing group-by, </title> <booktitle> cross-tab, and sub-totals. In 12th International Conference on Data Engineering (ICDE'96), </booktitle> <pages> pages 152 - 159, </pages> <address> New Orleans, Louisiana, </address> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: Another, less obvious example is given by the collection of frequent sets of a 0-1 valued relation [36]: the collection of frequent sets can be used to give approximate answers to arbitrary boolean queries about the data, even though the frequent sets represent only conjunctive concepts. The data cube <ref> [15] </ref> can also be viewed as a condensed representation for a class of queries. Similarly, in computational geometry the notion of an "-approximation [41] is closely related. Developing condensed representations for various classes of patterns seems a promising way of improving the effectiveness of data mining algorithms.
Reference: 16. <author> J. Han and Y. Fu. </author> <title> Discovery of multiple-level association rules from large data bases. </title> <booktitle> In Proceedings of the 21st International Conference on Very Large Data Bases (VLDB'95), </booktitle> <pages> pages 420 - 431, </pages> <address> Zurich, Swizerland, </address> <year> 1995. </year>
Reference-contexts: Namely, for each frequent set X and each B 2 X verify whether the rule X n fBg ) B has sufficiently high confidence. How can one find all frequent sets X? This can be done in a multitude of ways <ref> [1, 2, 16, 18, 43, 48] </ref>. A typical approach [2] is to use that fact that all subsets of a frequent set are also frequent. A way of applying the framework of Algorithm Find-frequent-patterns is as follows.
Reference: 17. <author> K. Hatonen, M. Klemettinen, H. Mannila, P. Ronkainen, and H. Toivonen. </author> <title> Know ledge discovery from telecommunication network alarm databases. </title> <booktitle> In 12th International Conference on Data Engineering (ICDE'96), </booktitle> <pages> pages 115 - 122, </pages> <address> New Orleans, Louisiana, </address> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: For example, a telecommunications network alarm database is used to collect all the notifications about abnormal situations in the network. The number of event types is around 200, and there are 1000-10000 alarms per day <ref> [17] </ref>. As a first step in analyzing such data, one can try to find which event types occur frequently close together. Denoting by E the set of all event types, an episode ' is a partially ordered set of elements from E.
Reference: 18. <author> M. Holsheimer, M. Kersten, H. Mannila, and H. Toivonen. </author> <title> A perspective on data bases and data mining. </title> <booktitle> In Proceedings of the First International Conference on Knowledge Discovery and Data Mining (KDD'95), </booktitle> <pages> pages 150 - 155, </pages> <address> Montreal, Canada, </address> <month> Aug. </month> <year> 1995. </year>
Reference-contexts: Namely, for each frequent set X and each B 2 X verify whether the rule X n fBg ) B has sufficiently high confidence. How can one find all frequent sets X? This can be done in a multitude of ways <ref> [1, 2, 16, 18, 43, 48] </ref>. A typical approach [2] is to use that fact that all subsets of a frequent set are also frequent. A way of applying the framework of Algorithm Find-frequent-patterns is as follows. <p> This method has the advantage that rows that do not contribute to any frequent set will not be inspected more than once. For comparisons of the two approaches, see <ref> [2, 18, 43] </ref>. The algorithms described above work quite nicely on large input relations. <p> The system is built on the extensive use of main memory, has an extensible set of basic operations, and supports shared-memory parallelism. Experiments with Monet on data mining applications have produced quite good results <ref> [18, 19] </ref>. 6 Sampling Data mining is often difficult for at least two reasons: first, there are lots of data, and second, the data is multidimensional. The hypothesis or pattern space is in most cases exponential in the number of attributes, so the multidimensionality can actually be the harder problem.
Reference: 19. <author> M. Holsheimer, M. Kersten, and A. Siebes. </author> <title> Data surveyor: Searching the nuggets in parallel. </title> <editor> In U. M. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 447 - 467. </pages> <publisher> AAAI Press, </publisher> <address> Menlo Park, CA, </address> <year> 1996. </year>
Reference-contexts: In the next section we show how this algorithm can be used to solve several data mining problems. If line 6 is instantiated differently, hill-climbing searches for best descriptions <ref> [19, 30] </ref> can also be fitted into this framework. In hill-climbing, the set C will contain only the neighbors of the current "most interesting" pattern. The generic algorithm suggests a data mining system architecture consisting of a discovery module and a database management system. <p> A very interesting experiment in this direction is the work on the Monet database server developed at CWI in the Netherlands by Martin Kersten and others <ref> [5, 19] </ref>. The Monet system is based on the vertical partitioning of the relations: a relation with k attributes is decomposed into k relations, each with two attributes: the OID and one of the original attributes. <p> The system is built on the extensive use of main memory, has an extensible set of basic operations, and supports shared-memory parallelism. Experiments with Monet on data mining applications have produced quite good results <ref> [18, 19] </ref>. 6 Sampling Data mining is often difficult for at least two reasons: first, there are lots of data, and second, the data is multidimensional. The hypothesis or pattern space is in most cases exponential in the number of attributes, so the multidimensionality can actually be the harder problem. <p> How useful is the concept of condensed representation? [36] System and language issues 6. What is a good architecture of a data mining system? How should a database management system and the search modules be connected? <ref> [19, 3] </ref> 7. Develop a language for expressing KDD queries and techniques for optimizing such queries. Some suggestions are given in [22, 45, 40]. 8. A subproblem of the previous one: how can caching strategies help in pro cessing sequences of related queries? [19] 9. <p> Develop a language for expressing KDD queries and techniques for optimizing such queries. Some suggestions are given in [22, 45, 40]. 8. A subproblem of the previous one: how can caching strategies help in pro cessing sequences of related queries? <ref> [19] </ref> 9. Extend the association rule framework to handle attributes with continuous values. (Some partial solutions to this problem are given in [12, 13, 47].) 10. Investigate the usefulness of temporal databases in the mining of event se quence data. 11.
Reference: 20. <author> T. Imielinski. </author> <title> A database view on data mining. Invited talk at the KDD'95 con ference. </title>
Reference-contexts: Currently, data mining research and development consists mainly of isolated applications. One can even argue that data mining is today at the same state as data management was in the 1960's <ref> [20, 21] </ref>: then all data management applications were ad hoc; only the advent of the relational model and powerful query languages made it possible to develop applications fast. Consequently, data mining would need a similar theoretical framework.
Reference: 21. <author> T. Imielinski and H. Mannila. </author> <title> Database mining: a new frontier. </title> <journal> Communications of the ACM, </journal> <note> 1996. To appear. </note>
Reference-contexts: Currently, data mining research and development consists mainly of isolated applications. One can even argue that data mining is today at the same state as data management was in the 1960's <ref> [20, 21] </ref>: then all data management applications were ad hoc; only the advent of the relational model and powerful query languages made it possible to develop applications fast. Consequently, data mining would need a similar theoretical framework.
Reference: 22. <author> T. Imielinski and A. Virmani. M-sql: </author> <title> Query language for database mining. </title> <type> Tech nical report, </type> <institution> Rutgers University, </institution> <month> January </month> <year> 1996. </year>
Reference-contexts: What is a good architecture of a data mining system? How should a database management system and the search modules be connected? [19, 3] 7. Develop a language for expressing KDD queries and techniques for optimizing such queries. Some suggestions are given in <ref> [22, 45, 40] </ref>. 8. A subproblem of the previous one: how can caching strategies help in pro cessing sequences of related queries? [19] 9. Extend the association rule framework to handle attributes with continuous values. (Some partial solutions to this problem are given in [12, 13, 47].) 10.
Reference: 23. <author> M. Jaeger, H. Mannila, and E. Weydert. </author> <title> Data mining as selective theory extraction in probabilistic logic. </title> <editor> In R. Ng, editor, </editor> <booktitle> SIGMOD'96 Data Mining Workshop, </booktitle> <institution> The University of British Columbia, Department of Computer Science, </institution> <type> TR 96-08, </type> <pages> pages 41-46, </pages> <year> 1996. </year>
Reference-contexts: This point of view has either implicitly or explicitly been used in discovering integrity constraints from databases, in inductive logic programming, and in machine learning [6, 7, 26, 30, 32]; some theoretical results can be found in [37], and a suggested logical formalism in <ref> [23] </ref>. While the frequency of occurrence of a pattern or the truth of a sentence can defined rigorously, the interestingness of patterns or sentences seems much harder to specify and measure. <p> The problems are very varying, from architectural issues to specific algorithmic questions. For brevity, the descriptions are quite succinct, and I also provide only a couple of references. Framework and general theory 1. Develop a general theory of data mining. Possible starting points are <ref> [6, 7, 23, 26, 30, 37] </ref>. (One might call this the theory of inductive databases.) 2.
Reference: 24. <author> M. Kantola, H. Mannila, K.-J. Raiha, and H. Siirtola. </author> <title> Discovering functional and inclusion dependencies in relational databases. </title> <journal> International Journal of Intelligent Systems, </journal> <volume> 7(7):591 - 607, </volume> <month> Sept. </month> <year> 1992. </year>
Reference-contexts: It is a special case of the problem of finding the functional dependencies that hold in a given relation. Applications of the key finding problem include database design, semantic query optimization <ref> [24, 44, 46] </ref>; one can also argue that finding functional dependencies is a necessary step in some types of structure learning. The size of an instance of the key finding problem is given by two parameters: the number of rows, and the number of columns.
Reference: 25. <author> D. Keim and H. Kriegel. </author> <title> Visualization techniques for mining large databases: A comparison. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <note> 1996. to appear. </note>
Reference-contexts: An important issue in data mining is its relationship with machine learning and statistics. I refer to [11, 31] for some discussions on this. Visualization of data is an important technique for obtaining useful information from large masses of data. The area is large; see <ref> [25] </ref> for an overview. Visualization can also be useful for making the discovered patterns easier to understand. Clustering is obviously a central technique in analyzing large data collections. The literature on the area is huge, and too wide to even scratch here.
Reference: 26. <author> J.-U. Kietz and S. Wrobel. </author> <title> Controlling the complexity of learning in logic through syntactic and task-oriented models. </title> <editor> In S. Muggleton, editor, </editor> <booktitle> Inductive Logic Programming, </booktitle> <pages> pages 335 - 359. </pages> <publisher> Academic Press, </publisher> <address> London, </address> <year> 1992. </year>
Reference-contexts: This point of view has either implicitly or explicitly been used in discovering integrity constraints from databases, in inductive logic programming, and in machine learning <ref> [6, 7, 26, 30, 32] </ref>; some theoretical results can be found in [37], and a suggested logical formalism in [23]. While the frequency of occurrence of a pattern or the truth of a sentence can defined rigorously, the interestingness of patterns or sentences seems much harder to specify and measure. <p> The problems are very varying, from architectural issues to specific algorithmic questions. For brevity, the descriptions are quite succinct, and I also provide only a couple of references. Framework and general theory 1. Develop a general theory of data mining. Possible starting points are <ref> [6, 7, 23, 26, 30, 37] </ref>. (One might call this the theory of inductive databases.) 2.
Reference: 27. <author> J. Kivinen and H. Mannila. </author> <title> The power of sampling in knowledge discovery. </title> <booktitle> In Proceedings of the Thirteenth ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems (PODS'94), </booktitle> <pages> pages 77 - 85, </pages> <address> Minneapolis, MN, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: A simple way of alleviating the problems caused by the volume of data (i.e., the number of rows) is to use sampling. Even small samples can give quite good approximation to the association rules [2, 48] or functional dependencies [28] that hold in a relation. See <ref> [27] </ref> for a general analysis on the relationship between the logical form of the discovered knowledge and the sample sizes needed for discovering it. The problem with using sampling is that the results can be wrong, with a small probability. <p> What is the relationship between the logical form of sentences to be dis covered and the computational complexity of the discovery task? (The issue of logical form vs. sample size is considered in <ref> [27] </ref>.) 3. Prove or disprove the CPM principle (Section 3). 4. What can be said about the performance of Algorithm 3 and its analogues for other problems? 5. How useful is the concept of condensed representation? [36] System and language issues 6.
Reference: 28. <author> J. Kivinen and H. Mannila. </author> <title> Approximate dependency inference from relations. </title> <journal> Theoretical Computer Science, </journal> <volume> 149(1):129 - 149, </volume> <year> 1995. </year>
Reference-contexts: A simple way of alleviating the problems caused by the volume of data (i.e., the number of rows) is to use sampling. Even small samples can give quite good approximation to the association rules [2, 48] or functional dependencies <ref> [28] </ref> that hold in a relation. See [27] for a general analysis on the relationship between the logical form of the discovered knowledge and the sample sizes needed for discovering it. The problem with using sampling is that the results can be wrong, with a small probability. <p> The problem with using sampling is that the results can be wrong, with a small probability. A possibility is to first use a sample and then verify (and, if necessary, correct) the results against the whole data set. For instances of this scheme, see <ref> [28, 48] </ref>; also the generic algorithm can be modified to correspond to this approach. We give the sample-and-correct algorithm for finding functional dependencies. Algorithm 3 Finding the keys of a relation by sampling and correcting. Input. A relation r over schema R. Output. The set of keys of r.
Reference: 29. <author> M. Klemettinen, H. Mannila, P. Ronkainen, H. Toivonen, and A. I. Verkamo. </author> <title> Finding interesting rules from large sets of discovered association rules. </title> <booktitle> In Proceedings of the Third International Conference on Information and Knowledge Management (CIKM'94), </booktitle> <pages> pages 401 - 407, </pages> <address> Gaithersburg, MD, </address> <month> Nov. </month> <year> 1994. </year> <note> ACM. </note>
Reference-contexts: To make the idea of generating rules and selecting interesting ones from them work, one has to provide the user methods and tools for selecting, ordering, and grouping of rules. See <ref> [29, 30] </ref> for some work along these lines. Many data mining systems try to do the pruning of uninteresting rules while the rules are located; it seems to me that the user's needs are so hard to predict that an automatic selection of interesting patterns is not easy. <p> Extend the association rule framework to handle attributes with continuous values. (Some partial solutions to this problem are given in [12, 13, 47].) 10. Investigate the usefulness of temporal databases in the mining of event se quence data. 11. Develop tools for selecting, grouping, and visualizing discovered knowledge <ref> [29, 30] </ref>. How can background knowledge be used? Algorithmic open problems 12. Design an algorithm for the key finding problem that works in polynomial time with respect to the size of the output and the number of attributes, and in subquadratic time in the number of rows.
Reference: 30. <author> W. Kloesgen. </author> <title> Efficient discovery of interesting statements in databases. </title> <journal> Journal of Intelligent Information Systems, </journal> <volume> 4(1):53 - 69, </volume> <year> 1995. </year>
Reference-contexts: Efficient support for such iteration is one important development topic in KDD. Prominent applications of KDD include health care data, financial applications, and scientific data <ref> [39, 30] </ref>. One of the more spectacular applications is the SKICAT system [9], which operates on 3 terabytes of image data, producing a classification of approximately 2 billion sky objects into a few classes. The task is obviously impossible to do manually. <p> This point of view has either implicitly or explicitly been used in discovering integrity constraints from databases, in inductive logic programming, and in machine learning <ref> [6, 7, 26, 30, 32] </ref>; some theoretical results can be found in [37], and a suggested logical formalism in [23]. While the frequency of occurrence of a pattern or the truth of a sentence can defined rigorously, the interestingness of patterns or sentences seems much harder to specify and measure. <p> In the next section we show how this algorithm can be used to solve several data mining problems. If line 6 is instantiated differently, hill-climbing searches for best descriptions <ref> [19, 30] </ref> can also be fitted into this framework. In hill-climbing, the set C will contain only the neighbors of the current "most interesting" pattern. The generic algorithm suggests a data mining system architecture consisting of a discovery module and a database management system. <p> To make the idea of generating rules and selecting interesting ones from them work, one has to provide the user methods and tools for selecting, ordering, and grouping of rules. See <ref> [29, 30] </ref> for some work along these lines. Many data mining systems try to do the pruning of uninteresting rules while the rules are located; it seems to me that the user's needs are so hard to predict that an automatic selection of interesting patterns is not easy. <p> The problems are very varying, from architectural issues to specific algorithmic questions. For brevity, the descriptions are quite succinct, and I also provide only a couple of references. Framework and general theory 1. Develop a general theory of data mining. Possible starting points are <ref> [6, 7, 23, 26, 30, 37] </ref>. (One might call this the theory of inductive databases.) 2. <p> Extend the association rule framework to handle attributes with continuous values. (Some partial solutions to this problem are given in [12, 13, 47].) 10. Investigate the usefulness of temporal databases in the mining of event se quence data. 11. Develop tools for selecting, grouping, and visualizing discovered knowledge <ref> [29, 30] </ref>. How can background knowledge be used? Algorithmic open problems 12. Design an algorithm for the key finding problem that works in polynomial time with respect to the size of the output and the number of attributes, and in subquadratic time in the number of rows.
Reference: 31. <author> H. Mannila. </author> <title> Data mining: machine learning, statistics, and databases. </title> <booktitle> In Pro ceedings of the 8th International Conference on Scientific and Statistical Database Management, Stockholm, </booktitle> <pages> pages 1-6, </pages> <year> 1996. </year>
Reference-contexts: Before starting on the KDD process, I digress briefly to other some topics not treated in this paper. An important issue in data mining is its relationship with machine learning and statistics. I refer to <ref> [11, 31] </ref> for some discussions on this. Visualization of data is an important technique for obtaining useful information from large masses of data. The area is large; see [25] for an overview. Visualization can also be useful for making the discovered patterns easier to understand.
Reference: 32. <author> H. Mannila and K.-J. Raiha. </author> <title> Design by example: An application of Armstrong relations. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 33(2):126 - 141, </volume> <year> 1986. </year>
Reference-contexts: This point of view has either implicitly or explicitly been used in discovering integrity constraints from databases, in inductive logic programming, and in machine learning <ref> [6, 7, 26, 30, 32] </ref>; some theoretical results can be found in [37], and a suggested logical formalism in [23]. While the frequency of occurrence of a pattern or the truth of a sentence can defined rigorously, the interestingness of patterns or sentences seems much harder to specify and measure.
Reference: 33. <author> H. Mannila and K.-J. Raiha. </author> <title> Design of Relational Databases. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Wokingham, UK, </address> <year> 1992. </year>
Reference-contexts: The number of attributes p is typically somewhere from 5 to 50. However, for some data mining applications, p could easily be 1000. While the problem of finding the keys of a relation is simple to state, its algorithmic properties turn out to be surprising complex. See <ref> [33, 34] </ref> for a variety of results, and Section 8 for theoretically intriguing open problems. The algorithm for finding P I (d; p) in Section 3 can straightforwardly be applied to finding the keys of a relation. The patterns are sets of attributes. <p> Solutions to the two following problems would imply considerable progress for this problem. 13. Finding the keys of a relation contains as a subproblem the problem of finding transversals of hypergraphs <ref> [33, 8, 36] </ref>. Given a hypergraph H, can the set T r (H) of its transversals be computed in time polynomial in jHj and jT r (H)j? 14. When one reduces the problem of finding keys to transversals of hypergraphs, one has to solve the following preliminary problem.
Reference: 34. <author> H. Mannila and K.-J. Raiha. </author> <title> On the complexity of dependency inference. </title> <journal> Discrete Applied Mathematics, </journal> <volume> 40:237 - 243, </volume> <year> 1992. </year>
Reference-contexts: The number of attributes p is typically somewhere from 5 to 50. However, for some data mining applications, p could easily be 1000. While the problem of finding the keys of a relation is simple to state, its algorithmic properties turn out to be surprising complex. See <ref> [33, 34] </ref> for a variety of results, and Section 8 for theoretically intriguing open problems. The algorithm for finding P I (d; p) in Section 3 can straightforwardly be applied to finding the keys of a relation. The patterns are sets of attributes.
Reference: 35. <author> H. Mannila and H. Toivonen. </author> <title> Discovering generalized episodes using minimal oc currences. </title> <booktitle> In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining (KDD'96), </booktitle> <pages> pages 146 - 151, </pages> <address> Portland, Oregon, Aug. 1996. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: The algorithm can be further improved by using incremental recognition of episodes; see [38] for details, and <ref> [35] </ref> for extensions with logical variables etc. The results are good: the algorithms are efficient, and using them one can find easily comprehensible results about the combinations of event types that occur together. <p> s within a window of width W , i.e., whether there are indices 1 i 1 &lt; i 2 &lt; &lt; i k n such that i k i 1 W and for all j = 1; : : :; k we have a i j = b j . <ref> [38, 35] </ref> (The solution should work for very large alphabets.) Acknowledgements Comments from Dimitrios Gunopulos and Hannu Toivonen are gratefully acknowledged.
Reference: 36. <author> H. Mannila and H. Toivonen. </author> <title> Multiple uses of frequent sets and condensed rep resentations. </title> <booktitle> In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining (KDD'96), </booktitle> <pages> pages 189 - 194, </pages> <address> Portland, Oregon, Aug. 1996. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: The information about the frequent sets can actually be used to approximate fairly accurately the confidences and supports of a far wider set of rules, including negation and disjunction <ref> [36] </ref>. 4.2 Finding episodes from sequences The basic ideas of the algorithm for finding association rules are fairly widely applicable. In this section we describe an application of the same ideas to the problem of finding repeated episodes in sequences of events. <p> Another, less obvious example is given by the collection of frequent sets of a 0-1 valued relation <ref> [36] </ref>: the collection of frequent sets can be used to give approximate answers to arbitrary boolean queries about the data, even though the frequent sets represent only conjunctive concepts. The data cube [15] can also be viewed as a condensed representation for a class of queries. <p> Prove or disprove the CPM principle (Section 3). 4. What can be said about the performance of Algorithm 3 and its analogues for other problems? 5. How useful is the concept of condensed representation? <ref> [36] </ref> System and language issues 6. What is a good architecture of a data mining system? How should a database management system and the search modules be connected? [19, 3] 7. Develop a language for expressing KDD queries and techniques for optimizing such queries. <p> Solutions to the two following problems would imply considerable progress for this problem. 13. Finding the keys of a relation contains as a subproblem the problem of finding transversals of hypergraphs <ref> [33, 8, 36] </ref>. Given a hypergraph H, can the set T r (H) of its transversals be computed in time polynomial in jHj and jT r (H)j? 14. When one reduces the problem of finding keys to transversals of hypergraphs, one has to solve the following preliminary problem.
Reference: 37. <author> H. Mannila and H. Toivonen. </author> <title> On an algorithm for finding all interesting sentences. </title> <booktitle> In Cybernetics and Systems, Volume II, The Thirteenth European Meeting on Cybernetics and Systems Research, </booktitle> <pages> pages 973 - 978, </pages> <address> Vienna, Austria, </address> <month> Apr. </month> <year> 1996. </year>
Reference-contexts: This point of view has either implicitly or explicitly been used in discovering integrity constraints from databases, in inductive logic programming, and in machine learning [6, 7, 26, 30, 32]; some theoretical results can be found in <ref> [37] </ref>, and a suggested logical formalism in [23]. While the frequency of occurrence of a pattern or the truth of a sentence can defined rigorously, the interestingness of patterns or sentences seems much harder to specify and measure. <p> Their running time is approximately O (N F ), where N = np is the size of the input and F is the sum of the sizes of the sets in the candidate collection C during the operation of the algorithm <ref> [37] </ref>. This is nearly linear, and the algorithms seem to scale nicely to tens of millions of examples. Typically the only case when they fail is when the output is too large, i.e., there are too many frequent sets. <p> The problems are very varying, from architectural issues to specific algorithmic questions. For brevity, the descriptions are quite succinct, and I also provide only a couple of references. Framework and general theory 1. Develop a general theory of data mining. Possible starting points are <ref> [6, 7, 23, 26, 30, 37] </ref>. (One might call this the theory of inductive databases.) 2.
Reference: 38. <author> H. Mannila, H. Toivonen, and A. I. Verkamo. </author> <title> Discovering frequent episodes in sequences. </title> <booktitle> In Proceedings of the First International Conference on Knowledge Discovery and Data Mining (KDD'95), </booktitle> <pages> pages 210 - 215, </pages> <address> Montreal, Canada, </address> <month> Aug. </month> <year> 1995. </year>
Reference-contexts: The algorithm can be further improved by using incremental recognition of episodes; see <ref> [38] </ref> for details, and [35] for extensions with logical variables etc. The results are good: the algorithms are efficient, and using them one can find easily comprehensible results about the combinations of event types that occur together. <p> s within a window of width W , i.e., whether there are indices 1 i 1 &lt; i 2 &lt; &lt; i k n such that i k i 1 W and for all j = 1; : : :; k we have a i j = b j . <ref> [38, 35] </ref> (The solution should work for very large alphabets.) Acknowledgements Comments from Dimitrios Gunopulos and Hannu Toivonen are gratefully acknowledged.
Reference: 39. <author> C. J. Matheus, G. Piatetsky-Shapiro, and D. McNeill. </author> <title> Selecting and report ing what is interesting. </title> <editor> In U. M. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 495 - 515. </pages> <publisher> AAAI Press, </publisher> <address> Menlo Park, CA, </address> <year> 1996. </year>
Reference-contexts: Efficient support for such iteration is one important development topic in KDD. Prominent applications of KDD include health care data, financial applications, and scientific data <ref> [39, 30] </ref>. One of the more spectacular applications is the SKICAT system [9], which operates on 3 terabytes of image data, producing a classification of approximately 2 billion sky objects into a few classes. The task is obviously impossible to do manually.
Reference: 40. <author> R. Meo, G. Psaila, and S. Ceri. </author> <title> A new SQL-like operator for mining association rules. </title> <booktitle> In Proceedings of the 22nd International Conference on Very Large Data Bases (VLDB'96), </booktitle> <year> 1996. </year> <note> To appear. </note>
Reference-contexts: What is a good architecture of a data mining system? How should a database management system and the search modules be connected? [19, 3] 7. Develop a language for expressing KDD queries and techniques for optimizing such queries. Some suggestions are given in <ref> [22, 45, 40] </ref>. 8. A subproblem of the previous one: how can caching strategies help in pro cessing sequences of related queries? [19] 9. Extend the association rule framework to handle attributes with continuous values. (Some partial solutions to this problem are given in [12, 13, 47].) 10.
Reference: 41. <author> K. Mulmuley. </author> <title> Computational Geometry: An Introduction Through Randomized Algorithms. </title> <publisher> Prentice Hall, </publisher> <address> New York, </address> <year> 1993. </year>
Reference-contexts: The data cube [15] can also be viewed as a condensed representation for a class of queries. Similarly, in computational geometry the notion of an "-approximation <ref> [41] </ref> is closely related. Developing condensed representations for various classes of patterns seems a promising way of improving the effectiveness of data mining algorithms.
Reference: 42. <author> B. Padmanabhan and A. Tuzhilin. </author> <title> Pattern discovery in temporal databases: A temporal logic approach. </title> <booktitle> In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining (KDD'96), </booktitle> <pages> pages 351-354, </pages> <year> 1996. </year>
Reference-contexts: The results are good: the algorithms are efficient, and using them one can find easily comprehensible results about the combinations of event types that occur together. See also <ref> [42] </ref> for a temporal logic approach to this area. 4.3 Finding keys or functional dependencies The key finding problem is: given a relation r, find all minimal keys of r. It is a special case of the problem of finding the functional dependencies that hold in a given relation.
Reference: 43. <author> A. Savasere, E. Omiecinski, and S. Navathe. </author> <title> An efficient algorithm for mining asso ciation rules in large databases. </title> <booktitle> In Proceedings of the 21st International Conference on Very Large Data Bases (VLDB'95), </booktitle> <pages> pages 432 - 444, </pages> <address> Zurich, Swizerland, </address> <year> 1995. </year>
Reference-contexts: Namely, for each frequent set X and each B 2 X verify whether the rule X n fBg ) B has sufficiently high confidence. How can one find all frequent sets X? This can be done in a multitude of ways <ref> [1, 2, 16, 18, 43, 48] </ref>. A typical approach [2] is to use that fact that all subsets of a frequent set are also frequent. A way of applying the framework of Algorithm Find-frequent-patterns is as follows. <p> This method has the advantage that rows that do not contribute to any frequent set will not be inspected more than once. For comparisons of the two approaches, see <ref> [2, 18, 43] </ref>. The algorithms described above work quite nicely on large input relations.
Reference: 44. <author> J. Schlimmer. </author> <title> Using learned dependencies to automatically construct sufficient and sensible editing views. In Knowledge Discovery in Databases, </title> <booktitle> Papers from the 1993 AAAI Workshop (KDD'93), </booktitle> <pages> pages 186 - 196, </pages> <address> Washington, D.C., </address> <year> 1993. </year>
Reference-contexts: It is a special case of the problem of finding the functional dependencies that hold in a given relation. Applications of the key finding problem include database design, semantic query optimization <ref> [24, 44, 46] </ref>; one can also argue that finding functional dependencies is a necessary step in some types of structure learning. The size of an instance of the key finding problem is given by two parameters: the number of rows, and the number of columns.
Reference: 45. <author> W. Shen, K. Ong, B. Mitbander, and C. Zaniolo. </author> <title> Metaqueries for data mining. </title> <editor> In U. M. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 375-398. </pages> <publisher> AAAI Press, </publisher> <address> Menlo Park, CA, </address> <year> 1996. </year>
Reference-contexts: What is a good architecture of a data mining system? How should a database management system and the search modules be connected? [19, 3] 7. Develop a language for expressing KDD queries and techniques for optimizing such queries. Some suggestions are given in <ref> [22, 45, 40] </ref>. 8. A subproblem of the previous one: how can caching strategies help in pro cessing sequences of related queries? [19] 9. Extend the association rule framework to handle attributes with continuous values. (Some partial solutions to this problem are given in [12, 13, 47].) 10.
Reference: 46. <author> M. Siegel. </author> <title> Automatic rule derivation for semantic query optimization. </title> <type> Technical Report BUCS Tech Report # 86-013, </type> <institution> Boston University, Computer Science Department, </institution> <month> Dec. </month> <year> 1986. </year>
Reference-contexts: It is a special case of the problem of finding the functional dependencies that hold in a given relation. Applications of the key finding problem include database design, semantic query optimization <ref> [24, 44, 46] </ref>; one can also argue that finding functional dependencies is a necessary step in some types of structure learning. The size of an instance of the key finding problem is given by two parameters: the number of rows, and the number of columns.
Reference: 47. <author> R. Srikant and R. Agrawal. </author> <title> Mining quantitative association rules in large relational tables. </title> <booktitle> In Proceedings of ACM SIGMOD Conference on Management of Data (SIGMOD'96), </booktitle> <pages> pages 1-12, </pages> <address> Montreal, Canada, </address> <year> 1996. </year>
Reference-contexts: Some suggestions are given in [22, 45, 40]. 8. A subproblem of the previous one: how can caching strategies help in pro cessing sequences of related queries? [19] 9. Extend the association rule framework to handle attributes with continuous values. (Some partial solutions to this problem are given in <ref> [12, 13, 47] </ref>.) 10. Investigate the usefulness of temporal databases in the mining of event se quence data. 11. Develop tools for selecting, grouping, and visualizing discovered knowledge [29, 30]. How can background knowledge be used? Algorithmic open problems 12.
Reference: 48. <author> H. Toivonen. </author> <title> Sampling large databases for association rules. </title> <booktitle> In Proceedings of the 22nd International Conference on Very Large Data Bases (VLDB'96), </booktitle> <pages> pages 134 - 145, </pages> <address> Mumbay, India, Sept. 1996. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Namely, for each frequent set X and each B 2 X verify whether the rule X n fBg ) B has sufficiently high confidence. How can one find all frequent sets X? This can be done in a multitude of ways <ref> [1, 2, 16, 18, 43, 48] </ref>. A typical approach [2] is to use that fact that all subsets of a frequent set are also frequent. A way of applying the framework of Algorithm Find-frequent-patterns is as follows. <p> A simple way of alleviating the problems caused by the volume of data (i.e., the number of rows) is to use sampling. Even small samples can give quite good approximation to the association rules <ref> [2, 48] </ref> or functional dependencies [28] that hold in a relation. See [27] for a general analysis on the relationship between the logical form of the discovered knowledge and the sample sizes needed for discovering it. <p> The problem with using sampling is that the results can be wrong, with a small probability. A possibility is to first use a sample and then verify (and, if necessary, correct) the results against the whole data set. For instances of this scheme, see <ref> [28, 48] </ref>; also the generic algorithm can be modified to correspond to this approach. We give the sample-and-correct algorithm for finding functional dependencies. Algorithm 3 Finding the keys of a relation by sampling and correcting. Input. A relation r over schema R. Output. The set of keys of r.
Reference: 49. <author> D. A. White and R. Jain. </author> <title> Algorithms and strategies for similarity retrieval. </title> <type> Tech nical Report VCL-96-101, </type> <institution> Visual Computing Laboratory, University of California, </institution> <address> San Diego, 9500 Gilman Drive, Mail Code 0407, La Jolla, CA 92093-0407, </address> <month> July </month> <year> 1996. </year> <title> This article was processed using the L A T E X macro package with LLNCS style </title>
Reference-contexts: The literature on the area is huge, and too wide to even scratch here. Similarity searches are often needed in data mining applications: how does one find objects that are roughly similar to the a query point. Again, the literature is vast, and we provide only two recent pointers: <ref> [4, 49] </ref>. 2 The KDD process The goal of knowledge discovery is to obtain useful knowledge from large collections of data. Such a task is inherently interactive and iterative: one cannot expect to obtain useful knowledge simply by pushing a lot of data to a black box.
References-found: 49

