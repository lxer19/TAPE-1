URL: http://elysium.cs.ucdavis.edu/~benson/publications/nav-csab92.ps
Refering-URL: http://elysium.cs.ucdavis.edu/~benson/publications/publications.html
Root-URL: http://www.cs.ucdavis.edu
Email: benson@cs.ucdavis.edu  prieditis@cs.ucdavis.edu  
Phone: Phone: (916) 752-3168 Fax: (916) 752-4767  Phone: (916) 752-6958 Fax: (916) 752-4767  
Title: Learning Continous Space Navigation Heuristics in Real Time Autonomous robots Problem solving and planning Goal
Author: Gregory D. Benson 
Note: Topic Areas:  
Address: Davis, CA 95616  Davis, CA 95616  
Affiliation: Department of Computer Science University of California  Department of Computer Science University of California  
Pubnum: Armand Prieditis  
Abstract: Navigating a robot from an initial position to a goal position in an environment with unknown obstacles is a fundamental problem. One approach to solving this problem involves learning a table of suggested actions for each state of the robot; this table then helps the robot navigate. This approach is limited to discrete rather than continuous spaces and table size can grow large for complex environments, thus reducing the ability to make decisions in real time. An alternative approach, which works in continuous and discrete spaces, involves applying a shortest path algorithm to find a path from the initial to the goal position. This approach assumes a complete or nearly complete environment map. Also, the probability that the robot will be able to follow the path in its entirety decreases quickly as environment uncertainty rises and the time taken to find a complete path may prevent the robot from making decisions in real time. This paper describes a method to learn continuous-space heuristics (distance estimators) in real time for real-time decision making. Our model assumes a point robot that travels through a continuous two-dimensional world which is occupied by both convex and concave polygonal obstacles. Simulations of a simple learning algorithm show that effective navigation heuristics can be learned relatively quickly in such environments. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Blum, A., Raghavan, P., and Schieber, B. </author> <title> Navigating in unfamiliar geometric terrain. </title> <booktitle> In ACM Symposium on Theory of Computing (1991), </booktitle> <pages> pp. 494-504. </pages>
Reference-contexts: In [8] Papadimitriou and Yannakakis prove lower bounds for the shortest path an algorithm can obtain when the map is not known and the robot cannot backtrack. Blum et al in <ref> [1] </ref> develop algorithms that meet the lower bounds established in [8].
Reference: [2] <author> Cormen, T. H., Leiserson, C. E., and Rivest, R. L. </author> <title> Introduction to Alogorithms. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: We also do not live in a world where plans can always be executed in their entirety. Yet work in robot path finding algorithms based on computational geometry assume exactly that. These methods apply off-line searching algorithms, such as Dijkstra's algorithm <ref> [2] </ref>, to find a shortest path from any start position to any goal position given a map of the environment built from visibility or spatial graphs [9, 7, 3]. As the environment becomes large the time to search the map increases, thus reducing the ability to navigate in real time.
Reference: [3] <author> Iyengar, S. S., et al. </author> <title> Robot navigation algorithms using learned spatial graphs. </title> <booktitle> Robotica 4 (1986), </booktitle> <pages> 93-100. </pages>
Reference-contexts: These methods apply off-line searching algorithms, such as Dijkstra's algorithm [2], to find a shortest path from any start position to any goal position given a map of the environment built from visibility or spatial graphs <ref> [9, 7, 3] </ref>. As the environment becomes large the time to search the map increases, thus reducing the ability to navigate in real time.
Reference: [4] <author> J. del R. Mill an, and Torras, C. </author> <title> A reinforcement connectionist approach to robot path finding in non-maze-like environments. </title> <booktitle> Machine Learning 8, </booktitle> <month> 3/4 (May </month> <year> 1992), </year> <pages> 363-395. </pages>
Reference-contexts: 1 Introduction We do not live in a grid world. Yet computer-based models of robot learners, such as reinforcement learning <ref> [4, 6, 5, 10] </ref> do. Since reinforcement learning methods build a table of actions to apply for particular state, they require a finite set of actions and states to work-hence the gridworld assumption.
Reference: [5] <author> Lin, L. J. </author> <title> Self-improving reactive agents based on reinforcement learning, planning and teaching. </title> <booktitle> Machine Learning 8, </booktitle> <month> 3/4 (May </month> <year> 1992), </year> <pages> 363-395. </pages>
Reference-contexts: 1 Introduction We do not live in a grid world. Yet computer-based models of robot learners, such as reinforcement learning <ref> [4, 6, 5, 10] </ref> do. Since reinforcement learning methods build a table of actions to apply for particular state, they require a finite set of actions and states to work-hence the gridworld assumption.
Reference: [6] <author> Mahadevan, S., and Connell, J. </author> <title> Automatic programming of behavior-based robots using reinforcement learning. </title> <type> Technical report rc 16359, </type> <institution> IBM, T.J. Watson Research Center, </institution> <address> Yorktown Heights, NY, </address> <year> 1990. </year>
Reference-contexts: 1 Introduction We do not live in a grid world. Yet computer-based models of robot learners, such as reinforcement learning <ref> [4, 6, 5, 10] </ref> do. Since reinforcement learning methods build a table of actions to apply for particular state, they require a finite set of actions and states to work-hence the gridworld assumption.
Reference: [7] <author> Ooomen, B. J., et al. </author> <title> Robot navigation in unknown terrains using learned visibility graphs. part i: The disjoint convex obstacle case. </title> <journal> IEEE Journal of Roboticss and Automation 3, </journal> <month> 6 (Dec. </month> <year> 1987), </year> <pages> 672-681. </pages>
Reference-contexts: These methods apply off-line searching algorithms, such as Dijkstra's algorithm [2], to find a shortest path from any start position to any goal position given a map of the environment built from visibility or spatial graphs <ref> [9, 7, 3] </ref>. As the environment becomes large the time to search the map increases, thus reducing the ability to navigate in real time.
Reference: [8] <author> Papadimitriou, C. H., and Yannakakis, M. </author> <title> Shortest paths without a map. </title> <booktitle> Theoretical Computer Science 84 (Oct. </booktitle> <year> 1991), </year> <pages> 127-150. </pages>
Reference-contexts: In <ref> [8] </ref> Papadimitriou and Yannakakis prove lower bounds for the shortest path an algorithm can obtain when the map is not known and the robot cannot backtrack. Blum et al in [1] develop algorithms that meet the lower bounds established in [8]. <p> In <ref> [8] </ref> Papadimitriou and Yannakakis prove lower bounds for the shortest path an algorithm can obtain when the map is not known and the robot cannot backtrack. Blum et al in [1] develop algorithms that meet the lower bounds established in [8]. Although this work is theoretically interesting, it relies on an oversimplified model of the world that uses only rectangular obstacles, and the practical application of this work is limited. 2 Rather than trying to build a complete map, our method learns a generalization of the environment.
Reference: [9] <author> Rao, N. S. V. </author> <title> Algorithmic framework for learned robot navigation in unknown terrains. </title> <booktitle> IEEE Computer (June 1989), </booktitle> <pages> 37-43. </pages>
Reference-contexts: These methods apply off-line searching algorithms, such as Dijkstra's algorithm [2], to find a shortest path from any start position to any goal position given a map of the environment built from visibility or spatial graphs <ref> [9, 7, 3] </ref>. As the environment becomes large the time to search the map increases, thus reducing the ability to navigate in real time.
Reference: [10] <author> Watkins, C. J. C. H. </author> <title> Learning with Delayed Rewards. </title> <type> PhD thesis, </type> <institution> Psychology Department, Cambridge University, </institution> <address> Cambridge, England, </address> <year> 1989. </year> <month> 10 </month>
Reference-contexts: 1 Introduction We do not live in a grid world. Yet computer-based models of robot learners, such as reinforcement learning <ref> [4, 6, 5, 10] </ref> do. Since reinforcement learning methods build a table of actions to apply for particular state, they require a finite set of actions and states to work-hence the gridworld assumption.
References-found: 10

