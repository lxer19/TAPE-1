URL: ftp://ftp.cse.ucsc.edu/pub/tr/ucsc-crl-93-36.ps.Z
Refering-URL: ftp://ftp.cse.ucsc.edu/pub/tr/README.html
Root-URL: http://www.cse.ucsc.edu
Title: Worst-case Quadratic Loss Bounds for On-line Prediction of Linear Functions by Gradient Descent  
Author: Nicolo Cesa-Bianchi Philip M. Long Manfred K. Warmuth 
Keyword: prediction, Widrow-Hoff algorithm, gradient descent, smoothing, inner product spaces, computational learning theory, on-line learning, linear systems.  
Address: Santa Cruz, CA 95064  Milano, Via Comelico 39, 20135 Milano (ITALY).  P.O. Box 90129, Durham, NC 27708 USA.  Santa Cruz, Santa Cruz, CA 95064 USA.  
Affiliation: Board of Studies in Computer and Information Sciences University of California, Santa Cruz  DSI, Universita di  Computer Science Department, Duke University,  Computer Science Department, UC  
Pubnum: UCSC-CRL-93-36  
Email: Email address: cesabian@dsi.unimi.it.  Email address: plong@cs.duke.edu.  Email: manfred@cse.ucsc.edu.  
Date: October 12, 1993  
Abstract: In this paper we study the performance of gradient descent when applied to the problem of on-line linear prediction in arbitrary inner product spaces. We show worst-case bounds on the sum of the squared prediction errors under various assumptions concerning the amount of a priori information about the sequence to predict. The algorithms we use are variants and extensions of on-line gradient descent. Whereas our algorithms always predict using linear functions as hypotheses, none of our results requires the data to be linearly related. In fact, the bounds proved on the total prediction loss are typically expressed as a function of the total loss of the best fixed linear predictor with bounded norm. All the upper bounds are tight to within constants. Matching lower bounds are provided in some cases. Finally, we apply our results to the problem of on-line prediction for classes of smooth functions. 
Abstract-found: 1
Intro-found: 1
Reference: [CFH + 93] <author> N. Cesa-Bianchi, Y. Freund, D.P. Helmbold, D. Haussler, R.E. Schapire, and M.K. Warmuth. </author> <title> How to use expert advice. </title> <booktitle> Proceedings of the 25th ACM Symposium on the Theory of Computation, </booktitle> <year> 1993. </year>
Reference-contexts: We adopt a worst-case outlook, following <ref> [Daw84, Vov90, LW91, LLW91, FMG92, MF92, CFH + 93] </ref> and many others, assuming nothing about the environment of the predictor, in particular the pairs (x 1 ; y 1 ); : : : ; (x m ; y m ). <p> In many cases we can even bound the additional loss of the algorithm over the above infimum similarly to the additional loss bounds of <ref> [CFH + 93] </ref> obtained in a simpler setting. <p> Then, m X (^y t y t ) 2 inf " m X (f (x t ) y t ) 2 + 2W X E + W 2 X 2 : In the next section, we show that techniques from <ref> [CFH + 93] </ref> may also be applied to obtain a L Y=X (s) + O (Y E + Y 2 ) bound on the total loss (unnormalized) when bounds X on jjx t jj and Y on jy t j are known for all t. <p> It is an open problem whether, instead of using adversarial arguments as we do here, our lower bounds can already be obtained when the examples are randomly and independently drawn from a natural distribution. For more simple functions this was done in <ref> [CFH + 93] </ref>: the lower bounds there are with respect to uniform distributions and the upper bounds which essentially meet the lower bounds are proven for the worst-case as done in this paper.
Reference: [Daw84] <author> A. Dawid. </author> <title> Statistical theory: The prequential approach. </title> <journal> Journal of the Royal Statistical Society (Series A), </journal> <pages> pages 278-292, </pages> <year> 1984. </year>
Reference-contexts: We adopt a worst-case outlook, following <ref> [Daw84, Vov90, LW91, LLW91, FMG92, MF92, CFH + 93] </ref> and many others, assuming nothing about the environment of the predictor, in particular the pairs (x 1 ; y 1 ); : : : ; (x m ; y m ).
Reference: [DH73] <author> R. O. Duda and P. E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <year> 1973. </year>
Reference-contexts: The derivation is based on previous derivations used in the proof of convergence of the on-line gradient descent algorithm (see, e.g. <ref> [DH73] </ref>). Lemma 4.1: Choose x; ^ w 1 ; w 2 X ; y 2 R; &gt; 0. Let ^y = ( ^ w 1 ; x) and ^ w 2 = ^ w 1 + (y ^y)x.
Reference: [FM91] <author> V. Faber and J. Mycielski. </author> <title> Applications of learning theorems. </title> <journal> Fundamenta Informaticae, </journal> <volume> 15(2) </volume> <pages> 145-167, </pages> <year> 1991. </year>
Reference-contexts: Our bounds are worst-case in the sense that they hold for all sequences of pairs (x t ; y t ). (In some cases we assume the norm of the x t 's is bounded by a second parameter.) Faber and Mycielski <ref> [FM91] </ref> noted that a natural class of smooth functions of a single real variable can be defined using inner products as above. The same class of smooth functions, as well as linear functions in R n , has been heavily studied in Statistics [Har91] (however, with probabilistic assumptions). <p> We call this algorithm GD (defined below). In this paper we analyze the behavior of GD in the case in which there isn't necessarily a w for which for all t, y t = (w; x t ). Faber and Mycielski <ref> [FM91] </ref> also studied this case, but their algorithms made use of side information which, in this paper, we assume is not available. Gradient descent is an algorithm design technique which has achieved considerable practical success in more complicated hypothesis spaces, in particular neural networks [Tou89, Tou90, LMT91, MHL92]. <p> Overview of results that holds on any sequence s = h (x t ; y t )i t 2 (X fi R) fl . We may apply our general bounds to a class of smooth functions of a single real variable, in the manner used by Faber and Mycielski <ref> [FM91] </ref> in the case that there is a perfect smooth function. The smoothness of a function is measured by the 2-norm of its derivative. <p> P t (f (x t ) y t ) 2 E, then the predictions ^y t of the special case of the general GD algorithm applied to this problem satisfy X (^y t y t ) 2 inf " t # p A bound of X t was proved by <ref> [FM91] </ref> in the case when E = 0. It is surprising that the time required for the algorithm we describe for this problem to make its tth prediction ^y t is O (t) in the uniform cost model provided that all past examples and predictions are saved. <p> In Theorem 5.2 we extend our result to apply to classes of smooth functions of n &gt; 1 real variables studied by Faber and Mycielski <ref> [FM91] </ref> in the absence of noise. We further show that upper bound (2.3), even viewed as bound on the excess of the algorithm's total loss over the loss of the best function of "size" at most W , is optimal, constants included. <p> Normalized loss If we run algorithm GD with learning rate set in each trial t to fi jjx t jj 2 , we can then prove a variant of Theorem 4.1 for a different notion of loss (previously studied by Faber and Mycielski <ref> [FM91] </ref>) which we call normalized loss. The normalized loss incurred by an algorithm predicting ^y t on a trial (x t ; y t ) is defined by (^y t y t ) 2 jjx t jj 2 . <p> on applications of Theorem 4.3, we note that analogs of the other results of Section 4 can be obtained in a similar manner. 5.1 Smooth functions of a single variable We begin with a class of smooth functions of a single real variable that was studied by Faber and Mycielski <ref> [FM91] </ref> in a similar context, except using the assumption that there was a function f in the class such that y t = f (x t ) for all t. <p> This algorithm is illustrated in Figure 5.2. 5.2 Smooth functions of several variables Theorem 5.1 can be generalized to higher dimensions as follows. The analogous generalization in the absence of noise was carried out in <ref> [FM91] </ref>. The domain X is R n + . We define the set SMO W;n to be all functions f : R n + ! R for which there is a function ~ f such that 20 5.
Reference: [FMG92] <author> M. Feder, N. Merhav, and M. Gutman. </author> <title> Universal prediction of individual sequences. </title> <journal> IEEE transactions of information theory, </journal> <volume> 38 </volume> <pages> 1258-1270, </pages> <year> 1992. </year>
Reference-contexts: We adopt a worst-case outlook, following <ref> [Daw84, Vov90, LW91, LLW91, FMG92, MF92, CFH + 93] </ref> and many others, assuming nothing about the environment of the predictor, in particular the pairs (x 1 ; y 1 ); : : : ; (x m ; y m ).
Reference: [GL89] <author> G.H. Golub and C.F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> The John Hopkins UP, </publisher> <year> 1989. </year>
Reference-contexts: We define the norm jjAjj of a matrix A by jjAjj 2 = sup jjvjj 2 =1 jjAvjj 2 : This is the norm induced by the Euclidean norm for vectors in R n (see <ref> [GL89] </ref>.) Notice that jjAvjj 2 jjAjj 2 jjvjj 2 (Cauchy-Schwartz inequality). We will make use of the following well-known facts. Fact 6.1 ([HJ85]): For any real matrix A, jjAjj 2 = p max , where max is the largest eigenvalue of A T A.
Reference: [Har91] <author> W. Hardle. </author> <title> Smoothing Techniques. </title> <publisher> Springer Verlag, </publisher> <year> 1991. </year>
Reference-contexts: The same class of smooth functions, as well as linear functions in R n , has been heavily studied in Statistics <ref> [Har91] </ref> (however, with probabilistic assumptions). Thus, general results for learning classes of functions defined by arbitrary inner product spaces can be applied in a variety of circumstances. 1 The general results will hold for finite and infinite dimensional vector spaces. 2 1. <p> Similar function classes have also often been studied in nonparametric statistics (see, e.g. <ref> [Har91] </ref>) using probabilistic assumptions on the generation of the x t 's. Let R + be the set of nonnegative reals.
Reference: [HJ85] <author> R.A. Horn and C.R. Johnson. </author> <title> Matrix Analysis. </title> <publisher> Cambridge University Press, </publisher> <year> 1985. </year>
Reference: [Kac37] <author> S. Kaczmarz. </author> <title> Angenaherte Auflosung von systemen linearer gleichungen. </title> <journal> Bull. Acad. Polon. Sci. Lett. A, </journal> <volume> 35 </volume> <pages> 355-357, </pages> <year> 1937. </year>
Reference-contexts: (using the natural notion of the distance between elements of an inner product space). 2 Even though in the neural network community this algorithm is usually credited to Widrow and Hoff [WH60], a similar algorithm for the iterative solution of a system of linear equations was previously developed by Kaczmarz <ref> [Kac37] </ref>. 3 To be precise, if X has countably infinite dimension, then GD can still be viewed as a mapping performing on-line gradient descent. Such a mapping is clearly noncomputable in general since each step might involve the update of an infinite number of coefficients.
Reference: [KL92] <author> D. Kimber and P.M. </author> <title> Long. The learning complexity of smooth functions of a single variable. </title> <booktitle> The 1992 Workshop on Computational Learning Theory, </booktitle> <pages> pages 153-159, </pages> <year> 1992. </year> <note> 28 References </note>
Reference-contexts: In the case E = 0, however, there is an algorithm with an optimal bound on P t (^y t y t ) 2 which computes its tth prediction in O (log t) time <ref> [KL92] </ref>, raising the hope that there might be a similarly efficient robust algorithm. In Theorem 5.2 we extend our result to apply to classes of smooth functions of n &gt; 1 real variables studied by Faber and Mycielski [FM91] in the absence of noise.
Reference: [LLW91] <author> N. Littlestone, </author> <title> P.M. Long, and M.K. Warmuth. On-line learning of linear functions. </title> <booktitle> Proceedings of the 23rd ACM Symposium on the Theory of Computation, </booktitle> <pages> pages 465-475, </pages> <year> 1991. </year>
Reference-contexts: We adopt a worst-case outlook, following <ref> [Daw84, Vov90, LW91, LLW91, FMG92, MF92, CFH + 93] </ref> and many others, assuming nothing about the environment of the predictor, in particular the pairs (x 1 ; y 1 ); : : : ; (x m ; y m ). <p> We further show that upper bound (2.3), even viewed as bound on the excess of the algorithm's total loss over the loss of the best function of "size" at most W , is optimal, constants included. Littlestone, Long and, Warmuth <ref> [LLW91] </ref> proved bounds for another algorithm for learning linear functions in R n , in which the x t 's were measured using the infinity norm, and the w's were measured using 1-norm. <p> The bounds for the two algorithms are incomparable because different norms are used to measure the sizes of the x's and the w's. However, the algorithm of <ref> [LLW91] </ref> does not appear to generalize to arbitrary inner product spaces as did the GD algorithm, and therefore those techniques do not appear to be as widely applicable.
Reference: [LMT91] <editor> R.P. Lippman, J.E. Moody, and D.S. Touretsky. </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 3. </volume> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: Faber and Mycielski [FM91] also studied this case, but their algorithms made use of side information which, in this paper, we assume is not available. Gradient descent is an algorithm design technique which has achieved considerable practical success in more complicated hypothesis spaces, in particular neural networks <ref> [Tou89, Tou90, LMT91, MHL92] </ref>. Despite this success, there appears not to be a principled method for tuning the learning rate.
Reference: [Lue84] <author> D.G. Luenberger. </author> <title> Linear and Nonlinear Programming. </title> <publisher> Addison-Wesley, </publisher> <year> 1984. </year>
Reference-contexts: If min = 0, then the bound holds vacuously. Assume then min &gt; 0. Via an application of the Kantorovich inequality to the square matrix A T A (see e.g. <ref> [Lue84] </ref>) it can be shown that R ( ^ x t+1 ) 1 ( min + max ) 2 R ( ^ x t ): (6:3) Therefore, we get 4 min max By summing up over all iterations t we obtain 4 min max 1 X R ( ^ x t
Reference: [LW91] <author> N. Littlestone and M.K. Warmuth. </author> <title> The weighted majority algorithm. </title> <type> Technical Report UCSC-CRL-91-28, </type> <institution> UC Santa Cruz, </institution> <month> October </month> <year> 1991. </year> <note> A preliminary version appeared in the Proceedings of the 30th Annual IEEE Symposium on the Foundations of Computer Science, </note> <month> October 89, </month> <pages> pages 256-261. </pages>
Reference-contexts: We adopt a worst-case outlook, following <ref> [Daw84, Vov90, LW91, LLW91, FMG92, MF92, CFH + 93] </ref> and many others, assuming nothing about the environment of the predictor, in particular the pairs (x 1 ; y 1 ); : : : ; (x m ; y m ).
Reference: [MF92] <author> N. Merhav and M. Feder. </author> <title> Universal sequential learning and decision from individual data sequences. </title> <booktitle> The 1992 Workshop on Computational Learning Theory, </booktitle> <pages> pages 413-427, </pages> <year> 1992. </year>
Reference-contexts: We adopt a worst-case outlook, following <ref> [Daw84, Vov90, LW91, LLW91, FMG92, MF92, CFH + 93] </ref> and many others, assuming nothing about the environment of the predictor, in particular the pairs (x 1 ; y 1 ); : : : ; (x m ; y m ).
Reference: [MHL92] <editor> J.E. Moody, S.J. Hanson, and R.P. Lippman. </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 4. </volume> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: Faber and Mycielski [FM91] also studied this case, but their algorithms made use of side information which, in this paper, we assume is not available. Gradient descent is an algorithm design technique which has achieved considerable practical success in more complicated hypothesis spaces, in particular neural networks <ref> [Tou89, Tou90, LMT91, MHL92] </ref>. Despite this success, there appears not to be a principled method for tuning the learning rate.
Reference: [MS91] <author> J. Mycielski and S. Swierczkowski. </author> <title> General learning theorems. </title> <type> Unpublished, </type> <year> 1991. </year>
Reference-contexts: When X is an arbitrary real vector space, and therefore its elements may not be uniquely represented by finite tuples of reals, the GD algorithm is a natural generalization of on-line gradient descent 3 and may viewed as follows <ref> [MS91] </ref>. 4 After each trial t, there is a set S t of elements w of X for which (w; x t ) = y t .
Reference: [Myc88] <author> J. Mycielski. </author> <title> A learning algorithm for linear operators. </title> <journal> Proceedings of the American Mathematical Society, </journal> <volume> 103(2) </volume> <pages> 547-550, </pages> <year> 1988. </year>
Reference-contexts: Mycielski <ref> [Myc88] </ref> had already treated the special case of linear functions in R n . The algorithm they analyzed for this "noise-free" case was a generalization of the on-line gradient descent algorithm 2 to arbitrary inner product spaces. We call this algorithm GD (defined below).
Reference: [Tou89] <editor> David S. Touretsky. </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 1. </volume> <publisher> Morgan Kaufmann, </publisher> <year> 1989. </year>
Reference-contexts: Faber and Mycielski [FM91] also studied this case, but their algorithms made use of side information which, in this paper, we assume is not available. Gradient descent is an algorithm design technique which has achieved considerable practical success in more complicated hypothesis spaces, in particular neural networks <ref> [Tou89, Tou90, LMT91, MHL92] </ref>. Despite this success, there appears not to be a principled method for tuning the learning rate.
Reference: [Tou90] <editor> David S. Touretsky. </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 2. </volume> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: Faber and Mycielski [FM91] also studied this case, but their algorithms made use of side information which, in this paper, we assume is not available. Gradient descent is an algorithm design technique which has achieved considerable practical success in more complicated hypothesis spaces, in particular neural networks <ref> [Tou89, Tou90, LMT91, MHL92] </ref>. Despite this success, there appears not to be a principled method for tuning the learning rate.
Reference: [Vov90] <author> V. Vovk. </author> <title> Aggregating strategies. </title> <booktitle> In Proceedings of the 3nd Workshop on Computational Learning Theory, </booktitle> <pages> pages 371-383. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: We adopt a worst-case outlook, following <ref> [Daw84, Vov90, LW91, LLW91, FMG92, MF92, CFH + 93] </ref> and many others, assuming nothing about the environment of the predictor, in particular the pairs (x 1 ; y 1 ); : : : ; (x m ; y m ).
Reference: [WH60] <author> B. Widrow and M.E. Hoff. </author> <title> Adaptive switching circuits. </title> <booktitle> 1960 IRE WESCON Conv. Record, </booktitle> <pages> pages 96-104, </pages> <year> 1960. </year>
Reference-contexts: "takes a step" in the direction of the element of S t which is closest to ^ w t (using the natural notion of the distance between elements of an inner product space). 2 Even though in the neural network community this algorithm is usually credited to Widrow and Hoff <ref> [WH60] </ref>, a similar algorithm for the iterative solution of a system of linear equations was previously developed by Kaczmarz [Kac37]. 3 To be precise, if X has countably infinite dimension, then GD can still be viewed as a mapping performing on-line gradient descent.
Reference: [You88] <author> N. Young. </author> <title> An introduction to Hilbert space. </title> <publisher> Cambridge University Press, </publisher> <year> 1988. </year>
Reference-contexts: The last requirement can be dropped essentially without affecting the definition (see e.g. <ref> [You88, page 25] </ref>). For x 2 X , the norm of x, denoted by jjxjj, is defined by jjxjj = q (These definitions are taken from [You88].) An example of an inner product is the dot product in R n . <p> The last requirement can be dropped essentially without affecting the definition (see e.g. [You88, page 25]). For x 2 X , the norm of x, denoted by jjxjj, is defined by jjxjj = q (These definitions are taken from <ref> [You88] </ref>.) An example of an inner product is the dot product in R n . For x; y 2 R n for some positive integer n, the dot product of x and y is defined to be x y = i=1 6 4. <p> Let L 2 (R + ) be the space of (measurable) functions g from R + to R for which R 1 0 g (u) 2 du is finite. L 2 (R + ) is well known to be an inner product space (see, e.g. <ref> [You88] </ref>), with the inner product defined by (g 1 ; g 2 ) = 0 Further, we define g 3 = g 2 + g 1 by (8x) g 3 (x) = g 2 (x) + g 1 (x); (8x) g 3 (x) = g 1 (x): 18 5. <p> Let L 2 (R n + ) be the space of (measurable) functions g from R n + to R for which Z 1 : : : 0 is finite. Again, it is well known (see e.g. <ref> [You88] </ref>), that L 2 (R n + ) has an inner product defined by (g 1 ; g 2 ) = 0 Z 1 g 1 (x)g 2 (x) dx n : : : dx 1 : Now apply algorithm GD to this particular inner product space, L 2 (R n <p> See e.g. <ref> [You88] </ref> for details. 7. Lower bounds 25 3. There exists w 2 R n such that kwk = Y =X and n X (y t (w; x t )) 2 = E: Proof. Choose X; Y; E &gt; 0 and choose n 2 N so that (7.1) is satisfied.
References-found: 23

