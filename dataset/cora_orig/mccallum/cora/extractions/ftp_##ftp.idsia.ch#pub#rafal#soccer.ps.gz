URL: ftp://ftp.idsia.ch/pub/rafal/soccer.ps.gz
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00408.html
Root-URL: 
Email: rafal@idsia.ch  marco@idsia.ch  juergen@idsia.ch  
Title: Learning Team Strategies: Soccer Case Studies  
Author: RAFA L P. SA LUSTOWICZ MARCO A. WIERING J URGEN SCHMIDHUBER Editors: Michael Huhns and Gerhard Weiss 
Keyword: Multiagent Reinforcement Learning, Soccer, TD-Q Learning, Evaluation Functions, Probabilistic Incremental Program Evolution, Coevolution.  
Address: Corso Elvezia 36, 6900 Lugano, Switzerland  
Affiliation: IDSIA,  
Note: Machine Learning, (1998, in press) c 1998 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  
Abstract: We use simulated soccer to study multiagent learning. Each team's players (agents) share action set and policy, but may behave differently due to position-dependent inputs. All agents making up a team are rewarded or punished collectively in case of goals. We conduct simulations with varying team sizes, and compare several learning algorithms: TD-Q learning with linear neural networks (TD-Q), Probabilistic Incremental Program Evolution (PIPE), and a PIPE version that learns by coevolution (CO-PIPE). TD-Q is based on learning evaluation functions (EFs) mapping input/action pairs to expected reward. PIPE and CO-PIPE search policy space directly. They use adaptive probability distributions to synthesize programs that calculate action probabilities from current inputs. Our results show that linear TD-Q encounters several difficulties in learning appropriate shared EFs. PIPE and CO-PIPE, however, do not depend on EFs and find good policies faster and more reliably. This suggests that in some multiagent learning scenarios direct search in policy space can offer advantages over EF-based approaches. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Albus, J. S. </author> <year> (1975). </year> <title> A new approach to manipulator control: The cerebellar model articulation controller (CMAC). Dynamic Systems, </title> <booktitle> Measurement and Control, </booktitle> <volume> 97 </volume> <pages> 220-227. </pages>
Reference: <author> Asada, M., Uchibe, E., Noda, S., Tawaratsumida, S., & Hosoda, K. </author> <year> (1994). </year> <title> A vision-based reinforcement learning for coordination of soccer playing behaviors. </title> <booktitle> In Proceedings of AAAI-94 Workshop on AI and A-life and Entertainment, </booktitle> <pages> pages 16-21. </pages>
Reference: <author> Baluja, S. </author> <year> (1994). </year> <title> Population-based incremental learning: A method for integrating genetic search based function optimization and competitive learning. </title> <type> Technical Report CMU-CS-94-163, </type> <institution> Carnegie Mellon University, Pittsburgh. </institution>
Reference: <author> Baluja, S. & Caruana, R. </author> <year> (1995). </year> <title> Removing the genetics from the standard genetic algorithm. </title> <editor> In Prieditis, A. and Russell, S., editors, </editor> <booktitle> Machine Learning: Proceedings of the Twelfth International Conference, </booktitle> <pages> pages 38-46, </pages> <address> San Francisco, CA. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference: <author> Bertsekas, D. P. & Tsitsiklis, J. N. </author> <year> (1996). </year> <title> Neuro-Dynamic Programming. </title> <publisher> Athena Scientific, </publisher> <address> Belmont, MA. </address>
Reference: <author> Cramer, N. L. </author> <year> (1985). </year> <title> A representation for the adaptive generation of simple sequential programs. </title> <editor> In Grefenstette, J., editor, </editor> <booktitle> Proceedings of an International Conference on Genetic Algorithms and Their Applications, </booktitle> <pages> pages 183-187, </pages> <address> Hillsdale NJ. </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference: <author> Crites, R. & Barto, A. </author> <year> (1996). </year> <title> Improving elevator performance using reinforcement learning. </title> <editor> In Touretzky, D., Mozer, M., and Hasselmo, M., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 8, </booktitle> <pages> pages 1017-1023, </pages> <address> Cambridge MA. </address> <publisher> MIT Press. </publisher>
Reference-contexts: In fact, initial experiments with CMACS and complex inputs already led to promising results. (2) Partial observ-ability. Q-learning assumes that the environment is fully observable; otherwise it is not guaranteed to work. Still, Q-learning variants already have been successfully applied to partially observable environments, e.g., <ref> (Crites & Barto, 1996) </ref>.
Reference: <author> Dickmanns, D., Schmidhuber, J., & Winklhofer, A. </author> <year> (1987). </year> <title> Der genetische Algorithmus: Eine Implementierung in Prolog. </title> <institution> Fortgeschrittenenpraktikum, Institut fur Informatik, Lehrstuhl Prof. Radig, Technische Universitat Munchen. </institution>
Reference: <author> Gallant, S. I. </author> <year> (1993). </year> <title> Neural Network Learning and Expert Systems. </title> <publisher> MIT Press, </publisher> <address> Cambridge MA. </address>
Reference-contexts: A promising remedy may be to use action frequency dependent learning rates. This will make update steps for all actions almost equal such that successes/failures will not easily lead to dominating actions. Another remedy may be the pocket algorithm <ref> (Gallant, 1993) </ref> (which stores the best EF found so far) or the more complex success-story algorithm (Wiering & Schmidhuber, 1996; Schmidhuber et al., 1997a; Schmidhuber et al., 1997b) that backtracks once the reward per time interval decreases.
Reference: <author> Koza, J. R. </author> <year> (1992). </year> <title> Genetic Programming On the Programming of Computers by Means of Natural Selection. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Generic Random Constants. A generic random constant (compare also "ephemeral random constant" <ref> (Koza, 1992) </ref>) is a zero argument function (a terminal). When accessed during program creation, it is either instantiated to a random value from a predefined, problem-dependent set of constants (here: [0;1)) or a value previously stored in the PPT (see below). Program Representation.
Reference: <author> Levin, L. A. </author> <year> (1973). </year> <title> Universal sequential search problems. </title> <journal> Problems of Information Transmission, </journal> <volume> 9(3) </volume> <pages> 265-266. </pages>
Reference: <author> Levin, L. A. </author> <year> (1984). </year> <title> Randomness conservation inequalities: Information and independence in mathematical theories. </title> <journal> Information and Control, </journal> <volume> 61 </volume> <pages> 15-37. </pages>
Reference: <author> Li, M. & Vitanyi, P. M. B. </author> <year> (1993). </year> <title> An Introduction to Kolmogorov Complexity and its Applications. </title> <publisher> Springer-Verlag, </publisher> <address> New York. </address>
Reference-contexts: LEARNING TEAM STRATEGIES 11 4. TD-Q Learning One of the most widely known and promising EF-based approaches to reinforcement learning is TD-Q learning (Sutton, 1988; Watkins, 1989; Peng & Williams, 1996; Wiering & Schmidhuber, 1997). We use an o*ine TD () Q-variant <ref> (Lin, 1993) </ref>. For efficiency reasons our TD-Q version uses linear neural networks (networks with hidden units require too much simulation time). To implement policy-sharing we use the same networks for all players of a team.
Reference: <author> Lin, L. J. </author> <year> (1993). </year> <title> Reinforcement Learning for Robots Using Neural Networks. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, Pittsburgh. </institution>
Reference-contexts: LEARNING TEAM STRATEGIES 11 4. TD-Q Learning One of the most widely known and promising EF-based approaches to reinforcement learning is TD-Q learning (Sutton, 1988; Watkins, 1989; Peng & Williams, 1996; Wiering & Schmidhuber, 1997). We use an o*ine TD () Q-variant <ref> (Lin, 1993) </ref>. For efficiency reasons our TD-Q version uses linear neural networks (networks with hidden units require too much simulation time). To implement policy-sharing we use the same networks for all players of a team.
Reference: <author> Littman, M. L. </author> <year> (1994). </year> <title> Markov games as a framework for multi-agent reinforcement learning. </title> <editor> In Prieditis, A. and Russell, S., editors, </editor> <booktitle> Machine Learning: Proceedings of the Eleventh International Conference, </booktitle> <pages> pages 157-163, </pages> <address> San Francisco, CA. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference: <author> Luke, S., Hohn, C., Farris, J., Jackson, G., & Hendler, J. </author> <year> (1997). </year> <title> Co-evolving soccer softbot team coordination with genetic programming. </title> <booktitle> In Proceedings of the First International Workshop on RoboCup, at the International Joint Conference on Artificial Intelligence (IJCAI-97). </booktitle>
Reference-contexts: LEARNING TEAM STRATEGIES 19 Acknowledgments Thanks to Richard Sutton, Cristina Versino, Jieyu Zhao, Nicol Schraudolph, and Luca Gambardella for valuable comments and suggestions. Notes 1. After submission of this paper another recent attempt at learning soccer team strategies in more complex environments was published <ref> (Luke et al., 1997) </ref>.
Reference: <author> Matsubara, H., Noda, I., & Hiraki, K. </author> <year> (1996). </year> <title> Learning of cooperative actions in multi-agent systems: a case study of pass play in soccer. </title> <editor> In Sen, S., editor, </editor> <booktitle> Working Notes for the AAAI 20 SA LUSTOWICZ, WIERING AND SCHMIDHUBER 96 Spring Symposium on Adaptation, Coevolution and Learning in Multi-agent Systems, </booktitle> <pages> pages 63-67, </pages> <address> Menlo Park, CA. </address> <publisher> AAAI Press. </publisher>
Reference: <author> Nadella, R. & Sen, S. </author> <year> (1996). </year> <title> Correlating internal parameters and external performance: learning soccer agents. </title> <editor> In Weiss, G., editor, </editor> <booktitle> Distributed Artificial Intelligence Meets Machine Learning. Learning in Multi-Agent Environments, </booktitle> <pages> pages 137-150. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin. </address>
Reference: <author> Nowlan, S. J. & Hinton, G. E. </author> <year> (1992). </year> <title> Simplifying neural networks by soft weight sharing. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 173-193. </pages>
Reference-contexts: If some multiagent cooperation task indeed can be solved by homogeneous agents then policy-sharing is quite natural as it allows for greatly reducing the number of adaptive free parameters. This tends to reduce the number of required training examples (learning time) and increase generalization performance, e.g., <ref> (Nowlan & Hinton, 1992) </ref>. Challenges of Multiagent Learning. One challenge is the "partial observabil-ity problem" (POP): in general no learner's input will tell the learner everything about its environment (which includes other changing learners). This means that each learner's environment may change in an inherently unpredictable way.
Reference: <author> Peng, J. & Williams, R. </author> <year> (1996). </year> <title> Incremental multi-step Q-learning. </title> <journal> Machine Learning, </journal> <volume> 22 </volume> <pages> 283-290. </pages>
Reference: <author> Sahota, M. </author> <year> (1993). </year> <title> Real-time intelligent behaviour in dynamic environments: Soccer-playing robots. </title> <type> Master's thesis, </type> <institution> University of British Columbia. </institution>
Reference: <author> Sa lustowicz, R. P. & Schmidhuber, J. </author> <year> (1997). </year> <title> Probabilistic incremental program evolution. </title> <journal> Evolutionary Computation, </journal> <volume> 5(2) </volume> <pages> 123-141. </pages>
Reference-contexts: Members of this class are Levin search (Levin, 1973; Levin, 1984; Solomonoff, 1986; Li & Vitanyi, 1993; Wiering & Schmidhuber, 1996; Schmidhuber, 1997a), Genetic Programming (Cramer, 1985; Dickmanns et al., 1987; Koza, 1992) and Probabilistic Incremental Program Evolution <ref> (PIPE, Sa lustowicz & Schmidhuber, 1997) </ref>. Comparison. In our case study we compare two learning algorithms, each representative of its class: TD-Q learning (Lin, 1993; Peng & Williams, 1996; Wiering & Schmidhuber, 1997) with linear neural networks (TD-Q) and Probabilistic Incremental Program Evolution (PIPE, Sa lustowicz & Schmidhuber, 1997). <p> Koza, 1992) and Probabilistic Incremental Program Evolution <ref> (PIPE, Sa lustowicz & Schmidhuber, 1997) </ref>. Comparison. In our case study we compare two learning algorithms, each representative of its class: TD-Q learning (Lin, 1993; Peng & Williams, 1996; Wiering & Schmidhuber, 1997) with linear neural networks (TD-Q) and Probabilistic Incremental Program Evolution (PIPE, Sa lustowicz & Schmidhuber, 1997). We also report results for a PIPE variant based on coevolution (CO-PIPE, Sa lustowicz et al., 1997). <p> We also report results for a PIPE variant based on coevolution <ref> (CO-PIPE, Sa lustowicz et al., 1997) </ref>. <p> In simple simulations we set ASET := ASET S and ~ i (p; t) := ~ i s (p; t). In complex simulations we set ASET := ASET C and ~ i (p; t) := ~ i c (p; t). We use PIPE as described in <ref> (Sa lustowicz & Schmidhuber, 1997) </ref>, except for "elitist learning" which we omit due to high environmental stochasticity. A PIPE alternative for searching program space would be Genetic Programming (GP) (Cramer, 1985; Dickmanns et al., 1987; Koza, 1992). <p> A PIPE alternative for searching program space would be Genetic Programming (GP) (Cramer, 1985; Dickmanns et al., 1987; Koza, 1992). We chose PIPE over GP because it compared favorably with Koza's GP variant in previous experiments <ref> (Sa lustowicz & Schmidhuber, 1997) </ref>. Action Selection. Action selection depends on 5 (8) variables when simple (complex) actions are used: the "greediness" parameter g 2 IR, and 4 (7) "action values" A a 2 IR, 8a 2 ASET . <p> A more detailed description can be found in <ref> (Sa lustowicz & Schmidhuber, 1997) </ref>. Program Instructions.
Reference: <author> Sa lustowicz, R. P., Wiering, M. A., & Schmidhuber, J. </author> <year> (1997a). </year> <title> Evolving soccer strategies. </title> <booktitle> In Proceedings of the Fourth International Conference on Neural Information Processing (ICONIP'97), </booktitle> <pages> pages 502-506, </pages> <address> Singapore. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Our comparatively complex case study will involve simulations with varying sets of continuous-valued inputs and actions, simple physical laws to model ball bounces and friction, and up to 11 players (agents) on each team. We will include certain results reported in <ref> (Sa lustowicz et al., 1997a,b) </ref>. Results Overview. Our results indicate: linear TD-Q has severe problems in learning and keeping appropriate shared EFs. It learns relatively slowly, and once it achieves fairly good performance it tends to break down. This effect becomes more pronounced as team size increases.
Reference: <author> Sa lustowicz, R. P., Wiering, M. A., & Schmidhuber, J. </author> <year> (1997b). </year> <title> On learning soccer strategies. </title> <editor> In Gerstner, W., Germond, A., Hasler, M., and Nicoud, J.-D., editors, </editor> <booktitle> Proceedings of the Seventh International Conference on Artificial Neural Networks (ICANN'97), volume 1327 of Lecture Notes in Computer Science, </booktitle> <pages> pages 769-774, </pages> <address> Berlin Heidelberg. </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Schmidhuber, J. </author> <year> (1997a). </year> <title> Discovering neural nets with low Kolmogorov complexity and high generalization capability. </title> <booktitle> Neural Networks, </booktitle> <volume> 10(5) </volume> <pages> 857-873. </pages>
Reference: <author> Schmidhuber, J. </author> <year> (1997b). </year> <title> A general method for incremental self-improvement and multi-agent learning in unrestricted environments. </title> <editor> In Yao, X., editor, </editor> <booktitle> Evolutionary Computation: Theory and Applications. </booktitle> <publisher> Scientific Publ. Co., Singapore. In press. </publisher>
Reference-contexts: We use linear networks to keep simulation time comparable to that of PIPE and CO-PIPE | more complex approximators would require significantly more computational resources. PIPE and CO-PIPE are based on probability vector coding of program instructions <ref> (Schmidhuber, 1997b) </ref>, Population-Based Incremental Learning (Baluja, 1994; Baluja & Caruana, 1995) and tree coding of programs used in variants of Genetic Programming (Cramer, 1985; Koza, 1992). They synthesize programs that calculate action probabilities from inputs. Experiences with programs are stored in adaptive probability distributions over all possible programs.
Reference: <author> Schmidhuber, J., Zhao, J., & Schraudolph, N. </author> <year> (1997a). </year> <title> Reinforcement learning with self-modifying policies. </title> <editor> In Thrun, S. and Pratt, L., editors, </editor> <booktitle> Learning to learn, </booktitle> <pages> pages 293-309. </pages> <publisher> Kluwer. </publisher>
Reference: <author> Schmidhuber, J., Zhao, J., & Wiering, M. </author> <year> (1997b). </year> <title> Shifting inductive bias with success-story algorithm, adaptive Levin search, and incremental self-improvement. </title> <journal> Machine Learning, </journal> <volume> 28 </volume> <pages> 105-130. </pages>
Reference-contexts: We use linear networks to keep simulation time comparable to that of PIPE and CO-PIPE | more complex approximators would require significantly more computational resources. PIPE and CO-PIPE are based on probability vector coding of program instructions <ref> (Schmidhuber, 1997b) </ref>, Population-Based Incremental Learning (Baluja, 1994; Baluja & Caruana, 1995) and tree coding of programs used in variants of Genetic Programming (Cramer, 1985; Koza, 1992). They synthesize programs that calculate action probabilities from inputs. Experiences with programs are stored in adaptive probability distributions over all possible programs.
Reference: <author> Solomonoff, R. </author> <year> (1986). </year> <title> An application of algorithmic probability to problems in artificial intelligence. </title> <editor> In Kanal, L. N. and Lemmer, J. F., editors, </editor> <booktitle> Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 473-491. </pages> <publisher> Elsevier Science Publishers. </publisher>
Reference: <author> Stone, P. & Veloso, M. </author> <year> (1996a). </year> <title> Beating a defender in robotic soccer: Memory-based learning of a continuous function. </title> <editor> In Tesauro, G., Touretzky, D. S., and Leen, T. K., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 8, </booktitle> <pages> pages 896-902, </pages> <address> Cambridge MA. </address> <publisher> MIT Press. </publisher>
Reference: <author> Stone, P. & Veloso, M. </author> <year> (1996b). </year> <title> A layered approach to learning client behaviors in the robocup soccer server. </title> <note> To appear in Applied Artificial Intelligence (AAI) in 1998. </note>
Reference: <author> Sutton, R. S. </author> <year> (1988). </year> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44. </pages>
Reference-contexts: We also report results for a PIPE variant based on coevolution (CO-PIPE, Sa lustowicz et al., 1997). We chose TD-Q learning and PIPE because both methods have already been successfully applied to interesting singleagent tasks (Lin, 1993; Sa lustowicz & Schmidhuber, 1997) (another reason for choosing TD learning <ref> (Sutton, 1988) </ref> is its popularity due to a successful application to backgammon (Tesauro, 1994)). Linear TD-Q selects actions according to linear neural networks trained with the delta rule (Widrow & Hoff, 1960) to map player inputs to evaluations of alternative actions.
Reference: <author> Sutton, R. S. </author> <year> (1996). </year> <title> Generalization in reinforcement learning: Successful examples using sparse coarse coding. </title> <editor> In Touretzky, D. S., Mozer, M. C., and Hasselmo, M. E., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 8, </booktitle> <pages> pages 1038-1045, </pages> <address> Cambridge MA. </address> <publisher> MIT Press. </publisher>
Reference: <author> Sutton, R. S. </author> <year> (1997). </year> <booktitle> Personal communication at the Seventh International Conference on Artificial Neural Networks (ICANN'97). </booktitle>
Reference-contexts: TD-Q's problems are due to a combination of reasons. (1) Linear networks. Linear networks have limited expressive power. They seem unable to learn and keep appropriate evaluation functions (EFs). Increasing expressive power by adding hidden units (time-consuming!) or using local function approximators such as CMACS (Albus, 1975; Sutton, 1996) <ref> (as proposed by Sutton, personal communication, 1997) </ref> may significantly improve TD-Q's performance. In fact, initial experiments with CMACS and complex inputs already led to promising results. (2) Partial observ-ability. Q-learning assumes that the environment is fully observable; otherwise it is not guaranteed to work.
Reference: <author> Tesauro, G. </author> <year> (1994). </year> <title> TD-gammon, a self-teaching backgammon program, achieves master-level play. </title> <journal> Neural Computation, </journal> <volume> 6(2) </volume> <pages> 215-219. </pages>
Reference-contexts: We chose TD-Q learning and PIPE because both methods have already been successfully applied to interesting singleagent tasks (Lin, 1993; Sa lustowicz & Schmidhuber, 1997) (another reason for choosing TD learning (Sutton, 1988) is its popularity due to a successful application to backgammon <ref> (Tesauro, 1994) </ref>). Linear TD-Q selects actions according to linear neural networks trained with the delta rule (Widrow & Hoff, 1960) to map player inputs to evaluations of alternative actions.
Reference: <author> Versino, C. & Gambardella, L. M. </author> <year> (1997). </year> <title> Learning real team solutions. </title> <editor> In Weiss, G., editor, </editor> <booktitle> DAI Meets Machine Learning, volume 1221 of Lecture Notes in Artificial Intelligenc, </booktitle> <pages> pages 40-61. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin. </address>
Reference: <author> Watkins, C. </author> <year> (1989). </year> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> King's College, </institution> <address> Cambridge. </address>
Reference: <author> Weiss, G. </author> <year> (1996). </year> <title> Adaptation and learning in multi-agent systems: Some remarks and a bibliography. </title> <editor> In Weiss, G. and Sen, S., editors, </editor> <booktitle> Adaptation and Learning in Multi-Agent Systems, volume 1042 of Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 1-21. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin Heidelberg. </address>
Reference: <author> Widrow, B. & Hoff, M. E. </author> <year> (1960). </year> <title> Adaptive switching circuits. </title> <booktitle> 1960 IRE WESCON Convention Record, </booktitle> <volume> 4 </volume> <pages> 96-104. </pages> <address> New York: </address> <note> IRE. Reprinted in Anderson and Rosenfeld [1988]. LEARNING TEAM STRATEGIES 21 Wiering, </note> <author> M. A. & Schmidhuber, J. </author> <year> (1996). </year> <title> Solving POMDPs with Levin search and EIRA. </title> <editor> In Saitta, L., editor, </editor> <booktitle> Machine Learning: Proceedings of the Thirteenth International Conference, </booktitle> <pages> pages 534-542, </pages> <address> San Francisco, CA. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: Linear TD-Q selects actions according to linear neural networks trained with the delta rule <ref> (Widrow & Hoff, 1960) </ref> to map player inputs to evaluations of alternative actions. We use linear networks to keep simulation time comparable to that of PIPE and CO-PIPE | more complex approximators would require significantly more computational resources. <p> Once all fist entries have been processed we start processing the second entries, and so on. The networks are trained using the delta-rule <ref> (Widrow & Hoff, 1960) </ref> with learning rate lr n . 5. Experiments We conduct two different types of simulations simple and complex. During simple simulations we use simple input vectors ~ i s (p; t) and simple actions from ASET S .
Reference: <author> Wiering, M. A. & Schmidhuber, J. </author> <year> (1997). </year> <title> Fast online Q(). </title> <type> Technical Report IDSIA-21-97, </type> <institution> IDSIA, Lugano, Switzerland. </institution>
References-found: 40

