URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/dbj/www/ftp/srds96.ps.gz
Refering-URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/dbj/www/ft.html
Root-URL: 
Email: sean@watson.ibm.com dbj@cs.cmu.edu  
Title: Minimizing Timestamp Size for Completely Asynchronous Optimistic Recovery with Minimal Rollback  
Author: Sean W. Smith David B. Johnson T. J. 
Address: NY 10532 Pittsburgh, PA 15213  
Affiliation: IBM Research Division Computer Science Department  Watson Research Center Carnegie Mellon University Hawthorne,  
Abstract: Basing rollback recovery on optimistic message logging and replay avoids the need for synchronization between processes during failure-free execution. Some previous research has also attempted to reduce the need for synchronization during recovery, but these protocols have suffered from three problems: not eliminating all synchronization during recovery, not minimizing rollback, or providing these properties but requiring large timestamps. This paper makes two contributions: we present a new rollback recovery protocol, based on our previous work, that provides these properties (asynchronous recovery, minimal rollback) while reducing the timestamp size; and we prove that no protocol can provide these properties and have asymptotically smaller timestamps. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> B. Bhargava and S. Lian. </author> <title> Independent Checkpointing and Concurrent Rollback Recovery for Distributed Systems An Optimistic Approach. </title> <booktitle> Seventh Symposium on Reliable Distributed Systems. </booktitle> <pages> 3-12. </pages> <publisher> IEEE, </publisher> <year> 1988. </year>
Reference-contexts: For each incoming message, the rollback protocol can decide to discard the message only when the message is a knowable orphan. 1.3. Previous Work In this paper, we concentrate on rollback based on optimistic message logging and replay. Recovery protocols based instead on checkpointing without message logging (e.g., <ref> [1, 3, 4, 5, 8, 15, 16, 29] </ref>) may force processes to roll back further than otherwise required, since processes can only recover states that have been checkpointed.
Reference: [2] <author> A. Borg, J. Baumbach, and S. Glazer. </author> <title> A Message System Supporting Fault Tolerance. </title> <booktitle> Proceedings of the Ninth ACM Symposium on Operating Systems Principles. </booktitle> <pages> 90-99. </pages> <year> 1983. </year>
Reference-contexts: Recovery protocols based instead on checkpointing without message logging (e.g., [1, 3, 4, 5, 8, 15, 16, 29]) may force processes to roll back further than otherwise required, since processes can only recover states that have been checkpointed. Recovery protocols based on pessimistic message logging (e.g., <ref> [2, 9, 11, 21] </ref>) can cause processes to delay execution until incoming messages are logged to stable storage. In this section, we discuss previous work in optimistic message logging and replay, for protocols that reduce the need for synchronization during recovery.
Reference: [3] <author> D. Briatico, A. Ciuffoletti, and L. Simoncini. </author> <title> A Distributed Domino Effect Free Recovery Algorithm. </title> <booktitle> IEEE Symposium on Reliability in Distributed Software and Database Systems. </booktitle> <pages> 207-215. </pages> <month> October </month> <year> 1984. </year>
Reference-contexts: For each incoming message, the rollback protocol can decide to discard the message only when the message is a knowable orphan. 1.3. Previous Work In this paper, we concentrate on rollback based on optimistic message logging and replay. Recovery protocols based instead on checkpointing without message logging (e.g., <ref> [1, 3, 4, 5, 8, 15, 16, 29] </ref>) may force processes to roll back further than otherwise required, since processes can only recover states that have been checkpointed.
Reference: [4] <author> K. M. Chandy and L. Lamport. </author> <title> Distributed Snapshots: Determining Global States of Distributed Systems. </title> <journal> ACM Transactions on Computer Systems. </journal> <volume> 3: </volume> <pages> 63-75. </pages> <month> February </month> <year> 1985. </year>
Reference-contexts: For each incoming message, the rollback protocol can decide to discard the message only when the message is a knowable orphan. 1.3. Previous Work In this paper, we concentrate on rollback based on optimistic message logging and replay. Recovery protocols based instead on checkpointing without message logging (e.g., <ref> [1, 3, 4, 5, 8, 15, 16, 29] </ref>) may force processes to roll back further than otherwise required, since processes can only recover states that have been checkpointed.
Reference: [5] <editor> A. Ciuffoletti. La Coordinazione Delle Attivita Di Ripristino Nei Sistemi Distribuiti. A.I.C.A. </editor> <booktitle> Annual Conference Proceedings. </booktitle> <month> October </month> <year> 1989. </year>
Reference-contexts: For each incoming message, the rollback protocol can decide to discard the message only when the message is a knowable orphan. 1.3. Previous Work In this paper, we concentrate on rollback based on optimistic message logging and replay. Recovery protocols based instead on checkpointing without message logging (e.g., <ref> [1, 3, 4, 5, 8, 15, 16, 29] </ref>) may force processes to roll back further than otherwise required, since processes can only recover states that have been checkpointed.
Reference: [6] <author> T. H. Corman, C. E. Leiserson, and R. L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: This result establishes the asymptotic optimality of timestamp size in our new protocol. Since many definitions of asymptotic complexity only discuss functions of one variable, we review the more general definitions <ref> [6] </ref>. A function f (v; s) is in W (g (v; s)) when there exist constants c; v 0 ; s 0 such that for any pair v; s with v v 0 and s s 0 , 0 cg (v; s) f (v; s).
Reference: [7] <author> O. P. Damani and V. J. Garg. </author> <title> How to Recover Efficiently and Asynchronously When Optimism Fail. </title> <institution> Electrical and Computer Engineering Technical Report TR-PDS-1995-014, University of Texas at Austin. </institution> <month> August </month> <year> 1995. </year> <note> A revised version appears in the Sixteenth International Conference on Distributed Computing Systems, </note> <month> May </month> <year> 1996. </year>
Reference-contexts: In our 1995 paper [27], we eliminated all synchronization and minimized the number of rollbacks, but used large timestamps. Damani and Garg <ref> [7] </ref>, in a subsequent paper, further reduced timestamp size, but sacrificed some asynchrony and minimality properties. This research was performed while the first author was with Los Alamos National Laboratory, and this paper is registered as a Los Alamos Unclassified Release. <p> A process begins a new incarnation when it rolls back and restarts in response to anyone's failure [28]. A process begins a new version when it rolls back and restarts only in response to its own failure <ref> [7] </ref>. In message logging protocols, processes checkpoint their local state occasionally, and log all incoming messages. <p> We introduce three measures of state intervals: let s I be the maximal number of state intervals in any single incarnation of any process [28]; let s V be the maximal number in any single version <ref> [7] </ref>; and let s L be the maximal number in any single live history [28]. We have s I s V , since many incarnations may comprise a single version. <p> Let r M be the maximal number of rollbacks at any one process. Of the previous work shown in Table 1, Strom and Yemini [28] use the smallest timestamps, followed by Damani and Garg <ref> [7] </ref>, followed by our previous protocol [27]. The timestamp size required by the protocol presented in this paper is substantially less than in our previous protocol. <p> Strom and Yemini used timestamps of size O (n log s L ) bits. Some subsequent work in optimistic recovery minimized the number of rollbacks by sacrificing asynchrony during recovery <ref> [13, 20, 24, 7] </ref>, and some of these even reduced the timestamp size to O (log s L ) bits [13, 24]. Smith, Johnson, and Tygar. Our earlier protocol [27] achieves fully asynchronous recovery while also minimizing rollbacks and wasted computation. <p> Together, the timestamps require O (n log s Y + R log s I + R log r M ) bits. Damani and Garg. Damani and Garg <ref> [7] </ref> present an optimistic protocol that requires little synchronization, minimizes the number of rollbacks, and requires timestamps consisting of a version index and a state index for each process. <p> The Appendix presents the proofs of these arguments. 1 In that paper, we characterized timestamp size in terms of the number of entries. Damani and Garg <ref> [7] </ref> characterize timestamp size in terms of number of integers, since some entries may require more than one integer. In this paper, we characterize timestamp size in terms of the number of bits, in order to maximize accuracy. 2 Damani and Garg [7] express this bound as O (n 2 f) <p> Damani and Garg <ref> [7] </ref> characterize timestamp size in terms of number of integers, since some entries may require more than one integer. In this paper, we characterize timestamp size in terms of the number of bits, in order to maximize accuracy. 2 Damani and Garg [7] express this bound as O (n 2 f) integers, where f is the maximal number of times any one process has failed, by bounding R by nF and bounding F by nf . 2.
Reference: [8] <author> E. N. Elnozahy, D. B. Johnson and W. Zwaenepoel. </author> <title> The Performance of Consistent Checkpointing. </title> <booktitle> Eleventh IEEE Symposium on Reliable Distributed Systems. </booktitle> <pages> 39-47. </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: For each incoming message, the rollback protocol can decide to discard the message only when the message is a knowable orphan. 1.3. Previous Work In this paper, we concentrate on rollback based on optimistic message logging and replay. Recovery protocols based instead on checkpointing without message logging (e.g., <ref> [1, 3, 4, 5, 8, 15, 16, 29] </ref>) may force processes to roll back further than otherwise required, since processes can only recover states that have been checkpointed.
Reference: [9] <author> E. N. Elnozahy and W. Zwaenepoel. Manetho: </author> <title> Transparent Rollback-Recovery with Low Overhead, Limited Rollback and Fast Output Commit. </title> <journal> IEEE Transactions on Computers. </journal> <volume> 41 (5): </volume> <pages> 526-531. </pages> <month> May </month> <year> 1992 </year>
Reference-contexts: Recovery protocols based instead on checkpointing without message logging (e.g., [1, 3, 4, 5, 8, 15, 16, 29]) may force processes to roll back further than otherwise required, since processes can only recover states that have been checkpointed. Recovery protocols based on pessimistic message logging (e.g., <ref> [2, 9, 11, 21] </ref>) can cause processes to delay execution until incoming messages are logged to stable storage. In this section, we discuss previous work in optimistic message logging and replay, for protocols that reduce the need for synchronization during recovery.
Reference: [10] <author> C. J. Fidge. </author> <title> Timestamps in Message-Passing Systems That Preserve the Partial Ordering. </title> <booktitle> Eleventh Australian Computer Science Conference. </booktitle> <pages> 56-67. </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: The Protocol The technique of using partial order time <ref> [10, 18, 25] </ref> to describe distributed asynchronous computation is well-known. Experience puts a total order on the state intervals at each individual process; the sending of a message makes the state interval containing the send precede the state interval begun by the receive.
Reference: [11] <author> D. B. Johnson and W. Zwaenepoel. </author> <title> Sender-Based Message Logging. </title> <booktitle> Seventeenth Annual International Symposium on Fault-Tolerant Computing. </booktitle> <pages> 14-19. </pages> <year> 1987. </year>
Reference-contexts: Recovery protocols based instead on checkpointing without message logging (e.g., [1, 3, 4, 5, 8, 15, 16, 29]) may force processes to roll back further than otherwise required, since processes can only recover states that have been checkpointed. Recovery protocols based on pessimistic message logging (e.g., <ref> [2, 9, 11, 21] </ref>) can cause processes to delay execution until incoming messages are logged to stable storage. In this section, we discuss previous work in optimistic message logging and replay, for protocols that reduce the need for synchronization during recovery.
Reference: [12] <author> D. B. Johnson. </author> <title> Distributed System Fault Tolerance Using Message Logging and Checkpointing. </title> <type> Ph.D. thesis, </type> <institution> Rice University, </institution> <year> 1989. </year>
Reference: [13] <author> D. B. Johnson and W. Zwaenepoel. </author> <title> Recovery in Distributed Systems Using Optimistic Message Logging and Checkpointing. </title> <journal> Journal of Algorithms. </journal> <volume> 11: </volume> <pages> 462-491. </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: Strom and Yemini used timestamps of size O (n log s L ) bits. Some subsequent work in optimistic recovery minimized the number of rollbacks by sacrificing asynchrony during recovery <ref> [13, 20, 24, 7] </ref>, and some of these even reduced the timestamp size to O (log s L ) bits [13, 24]. Smith, Johnson, and Tygar. Our earlier protocol [27] achieves fully asynchronous recovery while also minimizing rollbacks and wasted computation. <p> Some subsequent work in optimistic recovery minimized the number of rollbacks by sacrificing asynchrony during recovery [13, 20, 24, 7], and some of these even reduced the timestamp size to O (log s L ) bits <ref> [13, 24] </ref>. Smith, Johnson, and Tygar. Our earlier protocol [27] achieves fully asynchronous recovery while also minimizing rollbacks and wasted computation.
Reference: [14] <author> D. B. Johnson. </author> <title> Efficient Transparent Optimistic Rollback Recovery for Distributed Application Programs. </title> <booktitle> Twelfth IEEE Symposium on Reliable Distributed Systems. </booktitle> <pages> 86-95. </pages> <month> October </month> <year> 1993. </year>
Reference: [15] <author> R. Koo and S. Toueg. </author> <title> Checkpointing and Rollback-Recovery for Distributed Systems. </title> <journal> IEEE Transactions on Software Engineering. </journal> <volume> 13 (1): </volume> <pages> 23-31. </pages> <month> January </month> <year> 1987. </year>
Reference-contexts: For each incoming message, the rollback protocol can decide to discard the message only when the message is a knowable orphan. 1.3. Previous Work In this paper, we concentrate on rollback based on optimistic message logging and replay. Recovery protocols based instead on checkpointing without message logging (e.g., <ref> [1, 3, 4, 5, 8, 15, 16, 29] </ref>) may force processes to roll back further than otherwise required, since processes can only recover states that have been checkpointed.
Reference: [16] <author> P. Leu and B. Bhargava. </author> <title> Concurrent Robust Checkpointing and Recovery in Distributed Systems. </title> <booktitle> Fourth International Conference on Data Engineering. </booktitle> <pages> 154-163. </pages> <year> 1988. </year>
Reference-contexts: For each incoming message, the rollback protocol can decide to discard the message only when the message is a knowable orphan. 1.3. Previous Work In this paper, we concentrate on rollback based on optimistic message logging and replay. Recovery protocols based instead on checkpointing without message logging (e.g., <ref> [1, 3, 4, 5, 8, 15, 16, 29] </ref>) may force processes to roll back further than otherwise required, since processes can only recover states that have been checkpointed.
Reference: [17] <author> K. Li, J. F. Naughton and J. S. Plank. </author> <title> Real-Time, Concurrent Checkpointing for Parallel Programs. </title> <booktitle> Second ACM SIGPLAN Symposium on Principles and Practices of Parallel Programming. </booktitle> <pages> 79-88. </pages> <year> 1990. </year>
Reference: [18] <author> F. Mattern. </author> <title> Virtual Time and Global States of Distributed Systems. </title> <editor> In Cosnard, et al, ed., </editor> <booktitle> Parallel and Distributed Algorithms. </booktitle> <address> Amsterdam: </address> <publisher> North-Holland, </publisher> <year> 1989. </year> <pages> 215-226. </pages>
Reference-contexts: The Protocol The technique of using partial order time <ref> [10, 18, 25] </ref> to describe distributed asynchronous computation is well-known. Experience puts a total order on the state intervals at each individual process; the sending of a message makes the state interval containing the send precede the state interval begun by the receive.
Reference: [19] <author> P. M. Merlin and B. Randell. </author> <title> State Restoration in Distributed Systems. </title> <booktitle> International Symposium on Fault-Tolerant Computing. </booktitle> <month> June </month> <year> 1978. </year>
Reference: [20] <author> S. L. Peterson and P. Kearns. </author> <title> Rollback Based on Vector Time. </title> <booktitle> Twelfth IEEE Symposium on Reliable Distributed Systems. </booktitle> <pages> 68-77. </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: Strom and Yemini used timestamps of size O (n log s L ) bits. Some subsequent work in optimistic recovery minimized the number of rollbacks by sacrificing asynchrony during recovery <ref> [13, 20, 24, 7] </ref>, and some of these even reduced the timestamp size to O (log s L ) bits [13, 24]. Smith, Johnson, and Tygar. Our earlier protocol [27] achieves fully asynchronous recovery while also minimizing rollbacks and wasted computation.
Reference: [21] <author> M. L. Powell and D. L. Presotto. </author> <title> Publishing: A Reliable Broadcast Communication Mechanism. </title> <booktitle> Proceedings of the Ninth ACM Symposium on Operating Systems Principles. </booktitle> <pages> 100-109. </pages> <year> 1983. </year>
Reference-contexts: Recovery protocols based instead on checkpointing without message logging (e.g., [1, 3, 4, 5, 8, 15, 16, 29]) may force processes to roll back further than otherwise required, since processes can only recover states that have been checkpointed. Recovery protocols based on pessimistic message logging (e.g., <ref> [2, 9, 11, 21] </ref>) can cause processes to delay execution until incoming messages are logged to stable storage. In this section, we discuss previous work in optimistic message logging and replay, for protocols that reduce the need for synchronization during recovery.
Reference: [22] <author> B. Randell. </author> <title> System Structure for Fault Tolerance. </title> <journal> IEEE Transactions on Software Engineering. </journal> <volume> SE-1: </volume> <pages> 220-232, </pages> <year> 1975. </year>
Reference-contexts: Consequently, a process can restore a previous state by restoring a preceding checkpoint and replaying the subsequent logged messages in the order originally received. (The ability to restore arbitrary previous states, between check-pointed states, eliminates the domino effect <ref> [22, 23] </ref>.) In optimistic protocols, processes log messages by buffering them in volatile memory and later writing them to stable storage asynchronously.
Reference: [23] <author> D. L. Russell. </author> <title> State Restoration in Systems of Communicating Processes. </title> <journal> IEEE Transactions on Software Engineering. </journal> <volume> 6 (2): </volume> <pages> 183-194. </pages> <month> March </month> <year> 1980. </year>
Reference-contexts: Consequently, a process can restore a previous state by restoring a preceding checkpoint and replaying the subsequent logged messages in the order originally received. (The ability to restore arbitrary previous states, between check-pointed states, eliminates the domino effect <ref> [22, 23] </ref>.) In optimistic protocols, processes log messages by buffering them in volatile memory and later writing them to stable storage asynchronously.
Reference: [24] <author> A. P. Sistla and J. L. Welch. </author> <title> Efficient Distributed Recovery Using Message Logging. </title> <booktitle> Eighth ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> 223-238. </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: Strom and Yemini used timestamps of size O (n log s L ) bits. Some subsequent work in optimistic recovery minimized the number of rollbacks by sacrificing asynchrony during recovery <ref> [13, 20, 24, 7] </ref>, and some of these even reduced the timestamp size to O (log s L ) bits [13, 24]. Smith, Johnson, and Tygar. Our earlier protocol [27] achieves fully asynchronous recovery while also minimizing rollbacks and wasted computation. <p> Some subsequent work in optimistic recovery minimized the number of rollbacks by sacrificing asynchrony during recovery [13, 20, 24, 7], and some of these even reduced the timestamp size to O (log s L ) bits <ref> [13, 24] </ref>. Smith, Johnson, and Tygar. Our earlier protocol [27] achieves fully asynchronous recovery while also minimizing rollbacks and wasted computation.
Reference: [25] <author> S. W. Smith. </author> <title> A Theory of Distributed Time. </title> <institution> Computer Science Technical Report CMU-CS-93-231, Carnegie Mellon University. </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: The Protocol The technique of using partial order time <ref> [10, 18, 25] </ref> to describe distributed asynchronous computation is well-known. Experience puts a total order on the state intervals at each individual process; the sending of a message makes the state interval containing the send precede the state interval begun by the receive. <p> In its usual form, partial order time decomposes into linear timelines (one for each process) and links (one for each received message) between each timeline. In previous work <ref> [25, 26] </ref>, we have generalized this structure to allow for more general models at processes, and for hierarchies of time. We use and to denote time orderings within a single process, and ! and ! to denote time orderings across two or more processes. <p> In earlier work <ref> [25, 26] </ref>, we show how this mechanism applies to more general forms of time, including partial orders in which the local time at individual processes forms timetrees instead of timelines.
Reference: [26] <author> S. W. Smith. </author> <title> Secure Distributed Time for Secure Distributed Protocols. </title> <type> Ph.D. thesis. </type> <institution> Computer Science Technical Report CMU-CS-94-177, Carnegie Mellon University. </institution> <month> September </month> <year> 1994. </year>
Reference-contexts: The transitive closure of the union of these two relations comprises a partial order on the state intervals of all processes. As described in our earlier work <ref> [26] </ref>, issues such as failure require generalizations such as timetrees (partial orders on the state intervals at individual processes) and multiple levels of time. Section 2.1 reviews partial order time. <p> In its usual form, partial order time decomposes into linear timelines (one for each process) and links (one for each received message) between each timeline. In previous work <ref> [25, 26] </ref>, we have generalized this structure to allow for more general models at processes, and for hierarchies of time. We use and to denote time orderings within a single process, and ! and ! to denote time orderings across two or more processes. <p> In earlier work <ref> [25, 26] </ref>, we show how this mechanism applies to more general forms of time, including partial orders in which the local time at individual processes forms timetrees instead of timelines. <p> The key requirement, again, is that processes have the ability to sort state intervals in the timetrees of other processes. 2.2. Four Levels of Time Our earlier protocol <ref> [26, 27] </ref> introduced the notion of system time and user time. System time organizes the system state intervals at each process into a linear sequence, reflecting the order in which they happened.
Reference: [27] <author> S. W. Smith, D. B. Johnson and J. D. Tygar. </author> <title> Completely Asynchronous Optimistic Recovery with Minimal Rollbacks. </title> <booktitle> 25th International Symposium on Fault-Tolerant Computing. </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: In their seminal paper, Strom and Yemini [28] removed most synchronization from recovery, but permitted a worst case in which a single failure could lead to an exponential number of rollbacks. In our 1995 paper <ref> [27] </ref>, we eliminated all synchronization and minimized the number of rollbacks, but used large timestamps. Damani and Garg [7], in a subsequent paper, further reduced timestamp size, but sacrificed some asynchrony and minimality properties. <p> The views and conclusions contained in this document are those of the authors alone. In this paper, we make two contributions. First, we present a new optimistic rollback recovery protocol, based on our earlier work <ref> [27] </ref>, that preserves all properties of asynchronous recovery and minimal rollback, but reduces the timestamp size over our previous protocol. Second, we prove that no optimistic recovery protocol can have a smaller bound on timestamp size and still preserve all of these properties. 1.1. <p> Let r M be the maximal number of rollbacks at any one process. Of the previous work shown in Table 1, Strom and Yemini [28] use the smallest timestamps, followed by Damani and Garg [7], followed by our previous protocol <ref> [27] </ref>. The timestamp size required by the protocol presented in this paper is substantially less than in our previous protocol. But this timestamp size is still larger than in Damani and Garg's protocol, although unlike their protocol, our protocol fully preserves all properties of asynchronous recovery and minimal rollback. <p> Some subsequent work in optimistic recovery minimized the number of rollbacks by sacrificing asynchrony during recovery [13, 20, 24, 7], and some of these even reduced the timestamp size to O (log s L ) bits [13, 24]. Smith, Johnson, and Tygar. Our earlier protocol <ref> [27] </ref> achieves fully asynchronous recovery while also minimizing rollbacks and wasted computation. <p> The key requirement, again, is that processes have the ability to sort state intervals in the timetrees of other processes. 2.2. Four Levels of Time Our earlier protocol <ref> [26, 27] </ref> introduced the notion of system time and user time. System time organizes the system state intervals at each process into a linear sequence, reflecting the order in which they happened. <p> As in our earlier work <ref> [27] </ref>, we define a predicate KNOWABLE ORPHAN (A U ; B S ) to capture this property. The predicate KNOWABLE ORPHAN (A U ; B S ) is defined when A S ! B S for some A S 2 U to S (A U ). <p> Before a process q accepts a user-level message, it checks whether the user state that sent the message is a knowable orphanif so, q rejects the message. 2.4. An Efficient Test for Knowable Orphans Mapping Ordering Across Levels. Our earlier protocol <ref> [27] </ref> worked because we tracked system time and user time, and were able to compare states across these levels. Our new protocol works because it suffices to track compressed system time instead of system time; and to track failure time instead of user time. <p> Cross-Level Comparison. In our previous protocol, we defined a way to compare state intervals between the user and system levels <ref> [27] </ref>. Here, we extend this definition to accommodate cross-level comparison through the intermediate levels. <p> Establish the result for A F and C F , and for C F and B F . The proof of (2) can be found in <ref> [27] </ref>. Proof of Lemma 2. Two consecutive system states either map to the same compressed system state, or to consecutive compressed system states. Proof of Lemma 3. First we consider (1). Each branch-point in a failure timetree also is a branch in the corresponding user timetree.
Reference: [28] <author> R. Strom and S. Yemini. </author> <title> Optimistic Recovery in Distributed Systems. </title> <journal> ACM Transactions on Computer Systems. </journal> <volume> 3: </volume> <pages> 204-226. </pages> <month> August </month> <year> 1985. </year>
Reference-contexts: 1. Introduction Rollback recovery can provide fault tolerance for long-running applications in asynchronous distributed systems. Basing rollback recovery on optimistic message logging and replay avoids the need for synchronization during failure-free operation, and can add fault tolerance transparently. In their seminal paper, Strom and Yemini <ref> [28] </ref> removed most synchronization from recovery, but permitted a worst case in which a single failure could lead to an exponential number of rollbacks. In our 1995 paper [27], we eliminated all synchronization and minimized the number of rollbacks, but used large timestamps. <p> The state at a surviving process is an orphan when it causally depends on such lost computation. A process begins a new incarnation when it rolls back and restarts in response to anyone's failure <ref> [28] </ref>. A process begins a new version when it rolls back and restarts only in response to its own failure [7]. In message logging protocols, processes checkpoint their local state occasionally, and log all incoming messages. <p> This paper establishes a theoretical limit where the underly ing computation is opaque. across all processes (V = n + F ). We introduce three measures of state intervals: let s I be the maximal number of state intervals in any single incarnation of any process <ref> [28] </ref>; let s V be the maximal number in any single version [7]; and let s L be the maximal number in any single live history [28]. We have s I s V , since many incarnations may comprise a single version. <p> We introduce three measures of state intervals: let s I be the maximal number of state intervals in any single incarnation of any process <ref> [28] </ref>; let s V be the maximal number in any single version [7]; and let s L be the maximal number in any single live history [28]. We have s I s V , since many incarnations may comprise a single version. Additionally, let s Y be the maximum number of system state intervals (defined below) in an incarnation; s I s Y . <p> Let v i be the number of versions at the ith process, and let r i be the number of rollbacks. Let r M be the maximal number of rollbacks at any one process. Of the previous work shown in Table 1, Strom and Yemini <ref> [28] </ref> use the smallest timestamps, followed by Damani and Garg [7], followed by our previous protocol [27]. The timestamp size required by the protocol presented in this paper is substantially less than in our previous protocol. <p> But this timestamp size is still larger than in Damani and Garg's protocol, although unlike their protocol, our protocol fully preserves all properties of asynchronous recovery and minimal rollback. Strom and Yemini. Strom and Yemini <ref> [28] </ref> opened the area of optimistic recovery. Their protocol provided mostly asynchronous recovery, but required some blocking and additional messages.
Reference: [29] <author> Y.-M. Wang and W. K. Fuchs. </author> <title> Lazy Checkpoint Coordination for Bounding Rollback Propagation. </title> <booktitle> Twelfth IEEE Symposium on Reliable Distributed Systems. </booktitle> <pages> 78-85. </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: For each incoming message, the rollback protocol can decide to discard the message only when the message is a knowable orphan. 1.3. Previous Work In this paper, we concentrate on rollback based on optimistic message logging and replay. Recovery protocols based instead on checkpointing without message logging (e.g., <ref> [1, 3, 4, 5, 8, 15, 16, 29] </ref>) may force processes to roll back further than otherwise required, since processes can only recover states that have been checkpointed.
References-found: 29

