URL: ftp://ftp.cs.brown.edu/pub/techreports/96/cs96-08.ps.Z
Refering-URL: http://www.cs.brown.edu/publications/techreports/reports/CS-96-08.html
Root-URL: http://www.cs.brown.edu/
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> K. J. Astrom. </author> <title> Optimal control of Markov decision processes with incomplete state estimation. </title> <journal> Journal of Mathematical Analysis and Applications, </journal> <volume> 10 </volume> <pages> 174-205, </pages> <year> 1965. </year>
Reference-contexts: This belief mdp is such that an optimal policy for it, coupled with the correct state estimator, will give rise to optimal behavior (in the discounted infinite-horizon sense) for the 9 original pomdp <ref> [27, 1] </ref>. The remaining problem, then, is to solve this mdp.
Reference: [2] <author> Dimitri P. Bertsekas. </author> <title> Dynamic Programming and Optimal Control. </title> <publisher> Athena Scientific, </publisher> <address> Belmont, Massachusetts, </address> <year> 1995. </year> <note> Volumes 1 and 2. </note>
Reference-contexts: framework, it is assumed that, although there may be a great deal of uncertainty about the effects of an agent's actions, there is never any uncertainty about the agent's current state|it has complete and perfect perceptual abilities. 2 Markov decision processes are described in depth in a variety of texts <ref> [2, 20] </ref>; we will just briefly cover the necessary background. 2.1 Basic Framework A Markov decision process can be described as a tuple hS; A; T; Ri, where * S is a finite set of states of the world; * A is a finite set of actions; * T : S
Reference: [3] <author> Craig Boutilier and David Poole. </author> <title> Computing optimal policies for partially observable decision processes using compact representations. </title> <type> (manuscript), </type> <year> 1995. </year>
Reference-contexts: However, this work has served as a substrate for development of more complex and efficient representations <ref> [3] </ref>. One important facet of the pomdp approach is that there is no distinction drawn between actions taken to change the state of the world and actions taken to gain information. This is important because, in general, every action has both types of effect.
Reference: [4] <author> Anthony R. Cassandra, Leslie Pack Kaelbling, and Michael L. Littman. </author> <title> Acting optimally in partially observable stochastic domains. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <address> Seattle, WA, </address> <year> 1994. </year>
Reference-contexts: A more detailed derivation of the running time is available [13]. 5 Understanding Policies In this section we introduce a very simple example and use it to illustrate some properties of pomdp policies. Other examples are explore in an earlier paper <ref> [4] </ref>. 5.1 The Tiger Problem Imagine an agent standing in front of two closed doors. Behind one of the doors is a tiger and behind the other is a large reward.
Reference: [5] <author> Hsien-Te Cheng. </author> <title> Algorithms for Partially Observable Markov Decision Processes. </title> <type> PhD thesis, </type> <institution> University of British Columbia, British Columbia, Canada, </institution> <year> 1988. </year>
Reference-contexts: If we could do this, we might be able to reach a computation time per iteration that is polynomial in jSj, jAj, jj, jV t1 j, and jV t j. Cheng <ref> [5] </ref> and Smallwood and Sondik [25] also try to avoid generating all of V + t by constructing V t directly. However, their algorithms still have worst-case running times exponential in at least one of the problem parameters [13].
Reference: [6] <author> Lonnie Chrisman. </author> <title> Reinforcement learning with perceptual aliasing: The perceptual distinctions approach. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 183-188, </pages> <address> San Jose, California, 1992. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: This approach has the potential significant advantage of being able to learn a model that is complex enough to support optimal (or good) behavior without making irrelevant distinctions; this idea has been pursued by Chrisman <ref> [6] </ref> and McCallum [16, 17].
Reference: [7] <author> Thomas Dean, Leslie Pack Kaelbling, Jak Kirman, and Ann Nicholson. </author> <title> Planning under time constraints in stochastic domains. </title> <journal> Artificial Intelligence, </journal> <note> To Appear. </note>
Reference-contexts: In many cases, we may not want a full policy; methods for developing partial policies and conditional plans for completely observable domains are the subject of much current interest <ref> [8, 7, 11] </ref>. A weakness of the methods described in this paper is that they require the states of the world to be represented enumeratively, rather than through compositional representations such as Bayes nets or probabilistic operator descriptions.
Reference: [8] <author> Mark Drummond and John Bresina. </author> <title> Anytime synthetic projection. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pages 138-144. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: In many cases, we may not want a full policy; methods for developing partial policies and conditional plans for completely observable domains are the subject of much current interest <ref> [8, 7, 11] </ref>. A weakness of the methods described in this paper is that they require the states of the world to be represented enumeratively, rather than through compositional representations such as Bayes nets or probabilistic operator descriptions.
Reference: [9] <author> Ronald A. Howard. </author> <title> Dynamic Programming and Markov Processes. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1960. </year>
Reference-contexts: Howard <ref> [9] </ref> showed that there exists a stationary policy, fl , that is optimal for every starting state.
Reference: [10] <author> R. E. </author> <title> Kalman. A new approach to linear filtering and prediction problems. </title> <journal> Transactions of the American Society of Mechanical Engineers Journal of Basic Engineering, </journal> <volume> 82 </volume> <pages> 35-45, </pages> <year> 1960. </year>
Reference: [11] <author> N. Kushmerick, S. Hanks, and D. Weld. </author> <title> An Algorithm for Probabilistic Planning. </title> <type> Technical Report 93-06-03, </type> <institution> University of Washington Department of Computer Science and Engineering, </institution> <month> June </month> <year> 1993. </year> <note> To appear in Artificial Intelligence. 25 </note>
Reference-contexts: In many cases, we may not want a full policy; methods for developing partial policies and conditional plans for completely observable domains are the subject of much current interest <ref> [8, 7, 11] </ref>. A weakness of the methods described in this paper is that they require the states of the world to be represented enumeratively, rather than through compositional representations such as Bayes nets or probabilistic operator descriptions.
Reference: [12] <author> Michael L. Littman. </author> <title> The witness algorithm: Solving partially observable Markov deci-sion processes. </title> <type> Technical Report CS-94-40, </type> <institution> Brown University, </institution> <address> Providence, Rhode Island, </address> <year> 1994. </year>
Reference-contexts: On each iteration, we can enumerate all of the t-step policy trees, then compute the maximum of their value functions to get V t . If the value functions are represented by sets of policy trees, the test for termination can be implemented exactly using linear programming <ref> [12] </ref>. This is, of course, hopelessly computationally intractable. Each t-step policy tree contains (jj t 1)=(jj 1) nodes (the branching factor is jj, the number of possible observations).
Reference: [13] <author> Michael L. Littman, Anthony R. Cassandra, and Leslie Pack Kaelbling. </author> <title> An efficient algorithm for dynamic programming in partially observable Markov decision processes. </title> <type> Technical Report CS-95-19, </type> <institution> Brown University, </institution> <address> Providence, Rhode Island, </address> <year> 1995. </year>
Reference-contexts: Cheng [5] and Smallwood and Sondik [25] also try to avoid generating all of V + t by constructing V t directly. However, their algorithms still have worst-case running times exponential in at least one of the problem parameters <ref> [13] </ref>. In fact, if we could solve this problem, then RP=NP [13], so we will pursue a slightly different approach. Instead of computing V t directly, we will compute, for each action a, a set Q a t of t-step policy trees that have action a at their root. <p> However, their algorithms still have worst-case running times exponential in at least one of the problem parameters <ref> [13] </ref>. In fact, if we could solve this problem, then RP=NP [13], so we will pursue a slightly different approach. Instead of computing V t directly, we will compute, for each action a, a set Q a t of t-step policy trees that have action a at their root. <p> In what sense is the witness algorithm superior to previous algorithms for solving pomdps, then? Experiments indicate that the witness algorithm is faster in practice over a wide range of problem sizes <ref> [13] </ref>. The primary complexity-theoretic difference is that the witness algorithm runs in polynomial time in the number of policy trees in Q a t . <p> Figure 9 illustrates the relationship between p and p new . Now we can state the witness theorem <ref> [13] </ref>: The true Q-function, Q a t , differs from the approximate Q-function, ^ Q a t , if and only if there is some p 2 U , o 2 O, and p 0 2 V t1 for which there is some b such that V p new (b) &gt; <p> A more detailed derivation of the running time is available <ref> [13] </ref>. 5 Understanding Policies In this section we introduce a very simple example and use it to illustrate some properties of pomdp policies. Other examples are explore in an earlier paper [4]. 5.1 The Tiger Problem Imagine an agent standing in front of two closed doors. <p> It gives a uniform treatment of action to gain information and action to change the world. Although they are derived through the domain of continuous belief spaces, elegant finite-state controllers may sometimes be constructed using algorithms such as the witness algorithm. However, experimental results <ref> [13] </ref> suggest, even the witness algorithm becomes impractical for problems of modest size (jSj &gt; 15 and jj &gt; 15).
Reference: [14] <author> Michael L. Littman, Anthony R. Cassandra, and Leslie Pack Kaelbling. </author> <title> Learning policies for partially observable environments: Scaling up. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: Our current work explores the use of function-approximation methods for representing value functions and the use of simulation in order to concentrate the approximations on the frequently visited parts of the belief space <ref> [14] </ref>. The results of this work are encouraging and have allowed us to get a very good solution to an 89 state, 16 observation instance of a hallway navigation problem similar to the one described in the introduction.
Reference: [15] <author> William S. Lovejoy. </author> <title> A survey of algorithmic methods for partially observed Markov decision processes. </title> <journal> Annals of Operations Research, </journal> <volume> 28(1) </volume> <pages> 47-65, </pages> <year> 1991. </year>
Reference-contexts: Much of the content of this paper is a recapitulation of work in the operations-research literature <ref> [15, 18, 25, 27, 31] </ref>. We have developed new ways of viewing the problem that are, perhaps, more consistent with the AI perspective. We begin by introducing the theory of Markov decision processes (mdps) and pomdps.
Reference: [16] <author> R. Andrew McCallum. </author> <title> Overcoming incomplete perception with utile distinction memory. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <address> Amherst, Massachusetts, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This approach has the potential significant advantage of being able to learn a model that is complex enough to support optimal (or good) behavior without making irrelevant distinctions; this idea has been pursued by Chrisman [6] and McCallum <ref> [16, 17] </ref>.
Reference: [17] <author> R. Andrew McCallum. </author> <title> Instance-based utile distinctions for reinforcement learning with hidden state. </title> <booktitle> In Proceedings of the Twelfth International Conference Machine Learning, </booktitle> <pages> pages 387-395, </pages> <address> San Francisco, CA, 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This approach has the potential significant advantage of being able to learn a model that is complex enough to support optimal (or good) behavior without making irrelevant distinctions; this idea has been pursued by Chrisman [6] and McCallum <ref> [16, 17] </ref>.
Reference: [18] <author> George E. Monahan. </author> <title> A survey of partially observable Markov decision processes: Theory, models, and algorithms. </title> <journal> Management Science, </journal> <volume> 28(1) </volume> <pages> 1-16, </pages> <year> 1982. </year>
Reference-contexts: Much of the content of this paper is a recapitulation of work in the operations-research literature <ref> [15, 18, 25, 27, 31] </ref>. We have developed new ways of viewing the problem that are, perhaps, more consistent with the AI perspective. We begin by introducing the theory of Markov decision processes (mdps) and pomdps. <p> We will call the elements of this set the useful policy trees. The ability to find the set of useful policy trees serves as a basis for a more efficient version of the value-iteration algorithm <ref> [18] </ref>.
Reference: [19] <author> Robert C. Moore. </author> <title> A formal theory of knowledge and action. </title> <editor> In Jerry R. Hobbs and Robert C. Moore, editors, </editor> <title> Formal Theories of the Commonsense World. </title> <publisher> Ablex Publishing Company, </publisher> <address> Norwood, New Jersey, </address> <year> 1985. </year>
Reference-contexts: This is essentially a planning problem: given a complete and correct model of the world dynamics and a reward structure, find an optimal way to behave. In the artificial intelligence (AI) literature, a deterministic version of this problem has been addressed by adding knowledge preconditions to traditional planning systems <ref> [19] </ref>. Because we are interested in stochastic domains, however, we must depart from the traditional AI planning model.
Reference: [20] <author> Martin L. Puterman. </author> <title> Markov Decision Processes. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: framework, it is assumed that, although there may be a great deal of uncertainty about the effects of an agent's actions, there is never any uncertainty about the agent's current state|it has complete and perfect perceptual abilities. 2 Markov decision processes are described in depth in a variety of texts <ref> [2, 20] </ref>; we will just briefly cover the necessary background. 2.1 Basic Framework A Markov decision process can be described as a tuple hS; A; T; Ri, where * S is a finite set of states of the world; * A is a finite set of actions; * T : S <p> That is, max jV V t (s) V fl (s)j &lt; 2* 1 fl It is often the case that V t = fl long before V t is near V fl ; tighter bounds may be obtained using the span semi-norm on the value function <ref> [20] </ref>. 3 Partial Observability What happens if the agent is no longer able to determine the state it is currently in with complete reliability? A naive approach would be for the agent to map the most recent observation directly into an action without remembering anything from the past.
Reference: [21] <author> L. R. Rabiner. </author> <title> A tutorial on hidden Markov models and selected applications in speech recognition. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 77(2) </volume> <pages> 257-286, </pages> <year> 1989. </year>
Reference-contexts: We are optimistic and hope to extend these techniques (and others) to get good solutions to large problems. 24 Another area that is not addressed in this paper is the acquisition of a world model. One approach is to extend techniques for learning hidden Markov models <ref> [21, 28] </ref> to learn pomdp models. Then, we could apply algorithms of the type described in this paper to the learned models. Another approach is to combine the learning of the model with the computation of the policy.
Reference: [22] <author> Katsushige Sawaki and Akira Ichikawa. </author> <title> Optimal control for partially observable Markov decision processes over an infinite horizon. </title> <journal> Journal of the Operations Research Society of Japan, </journal> <volume> 21(1) </volume> <pages> 1-14, </pages> <month> March </month> <year> 1978. </year>
Reference-contexts: This is not necessarily true for the infinite-horizon discounted value function; it remains convex [30], but may have infinitely many facets. Still, the optimal infinite-horizon discounted value function can be approximated arbitrarily closely by a finite horizon value function for a sufficiently long horizon <ref> [27, 22] </ref>. <p> At this point, we have an approximately optimal value function for the infinite-horizon discounted problem. This pomdp has the property that the optimal infinite-horizon value function has a finite number of linear segments. An associated optimal policy has a finite description and is called finitely transient <ref> [26, 22] </ref>. pomdps with optimal finitely transient policies can sometimes be solved in finite time using value iteration. In other pomdps, the infinite-horizon value function has an infinite number of segments; on these problems the sets V t grow with each iteration.
Reference: [23] <author> Alexander Schrijver. </author> <title> Theory of Linear and Integer Programming. </title> <publisher> Wiley-Interscience, </publisher> <year> 1986. </year>
Reference-contexts: Note that we must assume that the number of bits of precision used in specifying the model is polynomial in these quantities since the polynomial running time of linear programming is expressed as a function of the input precision <ref> [23] </ref>. A more detailed derivation of the running time is available [13]. 5 Understanding Policies In this section we introduce a very simple example and use it to illustrate some properties of pomdp policies.
Reference: [24] <author> Satinder Pal Singh, Tommi Jaakkola, and Michael I. Jordan. </author> <title> Model-free reinforcement learning for non-Markovian decision problems. </title> <booktitle> In Machine Learning: Proceedings of the Eleventh International Conference, </booktitle> <pages> pages 284-292, </pages> <address> San Francisco, California, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In our hallway navigation example, this amounts to performing the same action in every location that looks the same|hardly a promising approach. Somewhat better results can be obtained by adding randomness to the agent's behavior: a policy can be a mapping from observations to probability distributions over actions <ref> [24] </ref>. In order to behave truly effectively in a partially observable world, it is necessary to use memory of previous actions and observations to aid in the disambiguation of the states of the world.
Reference: [25] <author> Richard D. Smallwood and Edward J. Sondik. </author> <title> The optimal control of partially observable Markov processes over a finite horizon. </title> <journal> Operations Research, </journal> <volume> 21 </volume> <pages> 1071-1088, </pages> <year> 1973. </year>
Reference-contexts: Much of the content of this paper is a recapitulation of work in the operations-research literature <ref> [15, 18, 25, 27, 31] </ref>. We have developed new ways of viewing the problem that are, perhaps, more consistent with the AI perspective. We begin by introducing the theory of Markov decision processes (mdps) and pomdps. <p> If we could do this, we might be able to reach a computation time per iteration that is polynomial in jSj, jAj, jj, jV t1 j, and jV t j. Cheng [5] and Smallwood and Sondik <ref> [25] </ref> also try to avoid generating all of V + t by constructing V t directly. However, their algorithms still have worst-case running times exponential in at least one of the problem parameters [13].
Reference: [26] <author> Edward J. Sondik. </author> <title> The Optimal Control of Partially Observable Markov Processes. </title> <type> PhD thesis, </type> <institution> Stanford University, Stanford, California, </institution> <year> 1971. </year>
Reference-contexts: At this point, we have an approximately optimal value function for the infinite-horizon discounted problem. This pomdp has the property that the optimal infinite-horizon value function has a finite number of linear segments. An associated optimal policy has a finite description and is called finitely transient <ref> [26, 22] </ref>. pomdps with optimal finitely transient policies can sometimes be solved in finite time using value iteration. In other pomdps, the infinite-horizon value function has an infinite number of segments; on these problems the sets V t grow with each iteration.
Reference: [27] <author> Edward J. Sondik. </author> <title> The optimal control of partially observable Markov processes over the infinite horizon: Discounted costs. </title> <journal> Operations Research, </journal> <volume> 26(2) </volume> <pages> 282-304, </pages> <year> 1978. </year>
Reference-contexts: Much of the content of this paper is a recapitulation of work in the operations-research literature <ref> [15, 18, 25, 27, 31] </ref>. We have developed new ways of viewing the problem that are, perhaps, more consistent with the AI perspective. We begin by introducing the theory of Markov decision processes (mdps) and pomdps. <p> This belief mdp is such that an optimal policy for it, coupled with the correct state estimator, will give rise to optimal behavior (in the discounted infinite-horizon sense) for the 9 original pomdp <ref> [27, 1] </ref>. The remaining problem, then, is to solve this mdp. <p> This is not necessarily true for the infinite-horizon discounted value function; it remains convex [30], but may have infinitely many facets. Still, the optimal infinite-horizon discounted value function can be approximated arbitrarily closely by a finite horizon value function for a sufficiently long horizon <ref> [27, 22] </ref>. <p> In many cases, it is possible to encode the policy in a graph that can be used to select actions without any explicit representation of the belief state <ref> [27] </ref>; we refer to such graphs as policy graphs. Recall Figure 14, in which the algorithm has nearly converged upon an infinite-horizon policy for the tiger problem.
Reference: [28] <author> Andreas Stolcke and Stephen Omohundro. </author> <title> Hidden Markov model induction by Bayesian model merging. </title> <editor> In Stephen Jose Hanson, Jack D. Cowan, and C. Lee Giles, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 11-18. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California, </address> <year> 1993. </year>
Reference-contexts: We are optimistic and hope to extend these techniques (and others) to get good solutions to large problems. 24 Another area that is not addressed in this paper is the acquisition of a world model. One approach is to extend techniques for learning hidden Markov models <ref> [21, 28] </ref> to learn pomdp models. Then, we could apply algorithms of the type described in this paper to the learned models. Another approach is to combine the learning of the model with the computation of the policy.
Reference: [29] <author> Paul Tseng. </author> <title> Solving H-horizon, stationary Markov decision problems in time proportional to log(H). </title> <journal> Operations Research Letters, </journal> <volume> 9(5) </volume> <pages> 287-297, </pages> <year> 1990. </year>
Reference-contexts: The algorithm terminates when the maximum difference between two successive value functions (known as the Bellman error magnitude) is less than some *. It can be shown <ref> [29] </ref> that there exists a t fl , polynomial in jSj, jAj, the magnitude of the largest value of R (s; a), and 1=(1 fl), such that the greedy policy with respect to V t fl is equal to the optimal infinite-horizon policy, fl .
Reference: [30] <author> C. C. White and D. Harrington. </author> <title> Application of Jensen's inequality for adaptive suboptimal design. </title> <journal> J. Optim. Theory Appl., </journal> <volume> 32 </volume> <pages> 89-100, </pages> <year> 1980. </year>
Reference-contexts: This is not necessarily true for the infinite-horizon discounted value function; it remains convex <ref> [30] </ref>, but may have infinitely many facets. Still, the optimal infinite-horizon discounted value function can be approximated arbitrarily closely by a finite horizon value function for a sufficiently long horizon [27, 22].
Reference: [31] <author> Chelsea C. White, III. </author> <title> Partially observed Markov decision processes: A survey. </title> <journal> Annals of Operations Research, </journal> <volume> 32, </volume> <year> 1991. </year>
Reference-contexts: Much of the content of this paper is a recapitulation of work in the operations-research literature <ref> [15, 18, 25, 27, 31] </ref>. We have developed new ways of viewing the problem that are, perhaps, more consistent with the AI perspective. We begin by introducing the theory of Markov decision processes (mdps) and pomdps.
Reference: [32] <author> Ronald J. Williams and Leemon C. Baird, III. </author> <title> Tight performance bounds on greedy policies based on imperfect value functions. </title> <type> Technical Report NU-CCS-93-13, </type> <institution> Northeastern University, College of Computer Science, </institution> <address> Boston, MA, </address> <month> November </month> <year> 1993. </year> <month> 27 </month>
Reference-contexts: Rather than calculating a bound on t fl in advance and running value iteration for that long, we instead use the following result regarding the Bellman error magnitude, due to Williams and Baird <ref> [32] </ref> in order to terminate with a near-optimal policy. If jV t (s) V t1 (s)j &lt; * for all s, then the value of the greedy policy with respect to V t does not differ from V fl by more than 2*fl=(1 fl) at any state.
References-found: 32

