URL: http://www.cs.brown.edu/people/lpk/general1.ps
Refering-URL: http://www.cs.brown.edu/people/lpk/
Root-URL: 
Title: Associative Reinforcement Learning: Functions in k-DNF  
Author: LESLIE PACK KAELBLING 
Keyword: reinforcement learning, generalization, k-DNF  
Address: Box  Providence, RI 02912-1910 USA  
Affiliation: Computer Science Department  Brown University  
Note: Small Journal Name, 1-20 c Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  
Email: lpk@cs.brown.edu  Editor:  
Date: 1910  
Abstract: An agent that must learn to act in the world by trial and error faces the reinforcement learning problem, which is quite different from standard concept learning. Although good algorithms exist for this problem in the general case, they are often quite inefficient and do not exhibit generalization. One strategy is to find restricted classes of action policies that can be learned more efficiently. This paper pursues that strategy by developing algorithms that can efficiently learn action maps that are expressible in k-DNF. The algorithms are compared with existing methods in empirical trials and are shown to have very good performance. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Anderson, C. W. </author> <year> (1986). </year> <title> Learning and Problem Solving with Multilayer Connectionist Systems. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, Amherst, Massachusetts. </institution>
Reference: <author> Barto, A. G., Sutton, R. S., & Brouwer, P. S. </author> <year> (1981). </year> <title> Associative search network: A reinforcement learning associative memory. </title> <journal> Biological Cybernetics, </journal> <volume> 40, </volume> <year> 201211. </year>
Reference: <author> Berry, D. A. & Fristedt, B. </author> <year> (1985). </year> <title> Bandit Problems: Sequential Allocation of Experiments. </title> <publisher> London: Chapman and Hall. </publisher>
Reference: <author> Enderton, H. B. </author> <year> (1972). </year> <title> A Mathematical Introduction to Logic. </title> <address> New York, New York: </address> <publisher> Academic Press. </publisher>
Reference-contexts: A formula is said to be in disjunctive normal form (DNF) if it is syntactically organized into a disjunction of purely conjunctive terms; there is a simple algorithmic method for converting any formula into DNF <ref> (Enderton, 1972) </ref>. A formula is in the class k-DNF if and only if its representation in DNF contains only conjunctive terms of length k or less. There is no restriction on the number of conjunctive termsjust their length.
Reference: <author> Gibbons, J. D. </author> <year> (1985). </year> <title> Nonparametric Statistical Inference. </title> <address> New York and Basel: </address> <publisher> Marcel Dekker, Inc. </publisher>
Reference-contexts: The mean and the median are both good estimators for the centers of distributions; the difficulty usually lies in deciding how to estimate the spread. I have found that standard non-parametric techniques <ref> (Gibbons, 1985) </ref> work reasonably well, but can be computationally expensive; careful application of the techniques for normal distributions can also be effective, even when the distribution is non-normal. If the individual reinforcement values are not independent given the input and action, then the statistical methods are no longer theoretically grounded.
Reference: <author> Gluck, M. A. </author> <year> (1991). </year> <title> Stimulus generalization and representation in adaptive network models of category learning. </title> <journal> Psychological Science, </journal> <volume> 1(1), </volume> <pages> 5055. </pages>
Reference-contexts: studying the bias of k-DNF, in particular, is that empirical studies of concept learning in humans have shown that the number of conjunctive items that must be ASSOCIATIVE REINFORCEMENT LEARNING 9 taken together (that is, the k in k-DNF) is highly indicative of the difficulty of the concept learning task <ref> (Gluck, 1991) </ref>. These results do not argue that k-DNF is the best bias for technical reasons, but do argue for its plausibility. Valiant developed an algorithm, shown below, for learning functions in k-DNF from input-output pairs, which only uses the input-output pairs with output 0.
Reference: <author> Hertz, J., Krogh, A., & Palmer, R. G. </author> <year> (1991). </year> <title> Introduction to the Theory of Neural Computation. </title> <address> Redwood City, California: </address> <publisher> Addison Wesley. </publisher>
Reference-contexts: Error Backpropagation To remedy the limitations of the linear-associator approach, multi-layer neural-network learning methods have been adapted to reinforcement learning. Anderson (1986), Wer-bos (1988), and Munro (1987), among others, have used error back-propagation methods <ref> (Hertz et al., 1991) </ref> with hidden units in order to allow reinforcement-learning systems to learn more complex action mappings. Williams (1988) presents an analysis of the use of back-propagation in associative reinforcement-learning systems.
Reference: <author> Kaelbling, L. P. </author> <year> (1993a). </year> <title> Associative reinforcement learning: A generate and test algorithm. </title> <journal> Machine Learning. </journal> <note> To appear. 20 KAELBLING Kaelbling, </note> <author> L. P. </author> <year> (1993b). </year> <title> Learning in Embedded Systems. </title> <address> Cambridge, Massachusetts: </address> <note> The MIT Press. Also available as a PhD Thesis from Stanford University, </note> <year> 1990. </year>
Reference-contexts: Another restriction on Boolean functions is simply to limit the syntactic complexity (in terms of depth or number of symbols) of the simplest propositional formula expressing the function. This restriction is explored in a companion paper <ref> (Kaelbling, 1993a) </ref>. The artificial neural network community has also considered methods for learning restricted classes of functions. The simplest restriction is that of linear separability.
Reference: <author> Larsen, R. J. & Marx, M. L. </author> <year> (1986). </year> <title> An Introduction to Mathematical Statistics and Its Applications. </title> <address> Englewood Cliffs, New Jersey: </address> <publisher> Prentice-Hall. </publisher>
Reference-contexts: As ff increases, more instances of reinforcement value 0 are required to drive down the upper bound of the confidence intervals, causing more weight to be placed on acting to gain information. By the DeMoivre-Laplace theorem <ref> (Larsen & Marx, 1986) </ref>, these bounds will converge, in the limit, to the true underlying probability values, and, hence, if each action is continually attempted, this algorithm will converge to the optimal strategy. This algorithm is discussed in much greater detail elsewhere (Kaelbling, 1993b). <p> two-sided test at the fi level of significance that rejects the hypothesis that the parameters are equal whenever s 0 s 1 ( s 0 +s 1 n 0 +n 1 )(n 0 +n 1 ) is either 8 : or ; where z fi=2 is a standard normal deviate <ref> (Larsen & Marx, 1986) </ref>. Because sample size is important for this test, the algorithm is slightly modified to ensure that, at the beginning of a run, each action is chosen a minimum number of times. This parameter will be referred to as fi min . <p> Notes 1. This is a somewhat more complex form than usual, designed to give good results for small values of n <ref> (Larsen & Marx, 1986) </ref>. 2. The choice of L is not relevant to our reinforcement-learning scenariothe details are described by Valiant (1984; 1985). 3.
Reference: <author> Minsky, M. L. & Papert, S. </author> <year> (1969). </year> <title> Perceptrons: An Introduction to Computational Geometry. </title> <address> Cambridge, Massachusetts: </address> <publisher> The MIT Press. </publisher>
Reference-contexts: This algorithm works well on simple problems, but algorithms of this type are incapable of learning functions that are not linearly separable <ref> (Minsky & Papert, 1969) </ref>, and do not necessarily succeed on all separable problems. 4.2. Error Backpropagation To remedy the limitations of the linear-associator approach, multi-layer neural-network learning methods have been adapted to reinforcement learning. <p> This may indicate that with more training instances it would eventually converge to optimal performance. The LARC algorithm performed poorly on both tasks 1 and 2. This poor performance was expected on task 2, because linear associators are known to be unable to learn non-linearly-separable functions <ref> (Minsky & Papert, 1969) </ref>. Task 2 is difficult for LARC because, during the execution of the algorithm, the evaluation function can be too complex to be learned by the simple linear associator, even though the action function is linearly separable.
Reference: <author> Munro, P. </author> <year> (1987). </year> <title> A dual back-propagation scheme for scalar reward learning. </title> <booktitle> In Proceedings of the Ninth Conference of the Cognitive Science Society (pp. 165176). </booktitle> <address> Seattle, Washington. </address>
Reference: <author> Narendra, K. & Thathachar, M. A. L. </author> <year> (1989). </year> <title> Learning Automata: An Introduction. </title> <address> Englewood Cliffs, New Jersey: </address> <publisher> Prentice-Hall. </publisher>
Reference-contexts: This keeps the program from slowing down over time which would, effectively, change the dynamics of the world for the agent. 2. Complexity, Efficiency, and Generalization There are a number of good algorithms for the reinforcement-learning scenario we are interested in, including learning-automata algorithms <ref> (Narendra & Thathachar, 1989) </ref>, Sutton's reinforcement-comparison methods (1984), and Kaelbling's interval-estimation methods (1993b). These algorithms were originally developed for the case in which the agent has no inputs other than reinforcement and merely needs to decide which action it should take all the time.
Reference: <author> Rosenblatt, F. </author> <year> (1961). </year> <title> Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms. </title> <address> Wash-ington, DC: </address> <publisher> Spartan Press. </publisher>
Reference-contexts: However, we can eliminate all elements that contain both an atom and its negation, yielding a set of size 2 k M combined algorithm, called LARCKDNF, is described formally in figure 4 and schematically in figure 5. This method is reminiscent of Rosenblatt's original approach <ref> (Rosenblatt, 1961) </ref> of building networks with a large set of random fixed features in the first layer.
Reference: <author> Sutton, R. S. </author> <year> (1984). </year> <title> Temporal Credit Assignment in Reinforcement Learning. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, Amherst, Massachusetts. </institution>
Reference: <author> Sutton, R. S. </author> <year> (1988). </year> <title> Learning to predict by the method of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3(1), </volume> <pages> 944. </pages>
Reference: <author> Valiant, L. G. </author> <year> (1984). </year> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11), </volume> <pages> 11341142. </pages>
Reference: <author> Valiant, L. G. </author> <year> (1985). </year> <title> Learning disjunctions of conjunctions. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <pages> volume 1 (pp. 560566). </pages> <address> Los Angeles, California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Watkins, C. J. C. H. </author> <year> (1989). </year> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> King's College, </institution> <address> Cambridge. </address>
Reference: <author> Werbos, P. J. </author> <year> (1988). </year> <title> Generalization of backpropagation with application to a recurrent gas market model. </title> <booktitle> Neural Networks, </booktitle> <volume> 1, </volume> <pages> 339356. </pages>
Reference: <author> Widrow, B. & Hoff, M. E. </author> <year> (1960). </year> <title> Adaptive switching circuits. </title> <booktitle> In IRE WESCON Convention Record New York, </booktitle> <address> New York. </address> <note> Reprinted in Neurocomputing: Foundations of Research, </note> <editor> James A. Anderson and Edward Rosenfeld, editors, </editor> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1988. </year>
Reference-contexts: Sutton (1984) gives methods for converting standard reinforcement-learning algorithms to work in an associative setting, allowing an agent to learn efficiently and to generalize across input states. He uses a version of the Widrow-Hoff or Adaline <ref> (Widrow & Hoff, 1960) </ref> weight-update algorithm to associate different internal state values with different input situations. This approach is illustrated by the LARC (linear-associator reinforcement-comparison) algorithm shown in figure 2. The inputs to the algorithm are represented as M + 1-dimensional vectors, with the input functioning as an adjustable threshold.
Reference: <author> Williams, R. J. </author> <year> (1988). </year> <title> On the use of backpropagation in associative reinforcement learning. </title> <booktitle> In Proceedings of the IEEE International Conference on Neural Networks San Diego, </booktitle> <address> California. </address>
Reference: <author> Williams, R. J. </author> <year> (1992). </year> <title> Simple statistical gradient-following algorithms for connectionist reinforcement learning. </title> <journal> Machine Learning, </journal> <volume> 8(3), </volume> <pages> 229256. </pages>
Reference: <author> Wolpert, D. H. </author> <year> (1993). </year> <title> On Overfitting Avoidance as Bias. </title> <type> Technical Report 93-03-016, </type> <institution> Santa Fe Institute, </institution> <address> Santa Fe, New Mexico. </address>
Reference-contexts: There is no basis for choosing a bias a priori unless something is known about the the functions to be learned <ref> (Wolpert, 1993) </ref>. Thus, it is important to investigate and develop algorithms for a variety of different biases, then to be careful, when applying them, to choose appropriately.
References-found: 23

