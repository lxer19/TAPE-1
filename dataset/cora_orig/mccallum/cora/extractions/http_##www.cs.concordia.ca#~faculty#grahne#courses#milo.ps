URL: http://www.cs.concordia.ca/~faculty/grahne/courses/milo.ps
Refering-URL: http://www.cs.concordia.ca/~faculty/grahne/courses/
Root-URL: http://www.cs.concordia.ca
Email: milo@math.tau.ac.il  sagit@math.tau.ac.il  
Title: Using Schema Matching to Simplify Heterogeneous Data Translation  
Author: Tova Milo Sagit Zohar 
Date: 1998  
Address: New York, USA,  
Note: Proceedings of the 24th VLDB Conference  
Affiliation: Computer Science Dept., Tel-Aviv University  
Abstract: A broad spectrum of data is available on the Web in distinct heterogeneous sources, and stored under different formats. As the number of systems that utilize this heterogeneous data grows, the importance of data translation and conversion mechanisms increases greatly. In this paper we present a new translation system, based on schema-matching, aimed at simplifying the intricate task of data conversion. We observe that in many cases the schema of the data in the source system is very similar to that of the target system. In such cases, much of the translation work can be done automatically, based on the schemas similarity. This saves a lot of effort for the user, limiting the amount of programming needed. We define common schema and data models, in which schemas and data (resp.) from many common models can be represented. Using a rule-based method, the source schema is compared with the target one, and each component in the source schema is matched with a corresponding component in the target schema. Then, based on the matching achieved, data instances of the source schema can be translated to instances of the target schema. We show that our schema-based translation system allows a convenient specification and customization of data conversions, and can be easily combined with the traditional data-based translation languages. Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the VLDB copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Very Large Data Base Endowment. To copy otherwise, or to republish, requires a fee and/or special permission from the Endowment. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Abiteboul. </author> <title> Querying semi-structured data. </title> <booktitle> In Proc. </booktitle> <volume> ICDT 97, </volume> <pages> pages 1-18, </pages> <year> 1997. </year>
Reference-contexts: For example, databases use schemas to model database instances; structured documents often obey some grammar (e.g. Document Type Definition - DTD in SGML and HTML); in other models such a definition may be partial (e.g. in semi-structured data <ref> [1] </ref>). The observation is that, in many translations, the schema of the target system is closely related to that of the source system both schemas aim to represent the same data. <p> vertex of this type are ordered or not (6) if some of the component types are optional (this is useful for describing union types and optional attributes), (7) if the sub-tree rooted at a node of this type is allowed to have an arbitrary structure (useful to describe semi-structured data <ref> [1] </ref>), and (8) whether vertices of this type actually appear in the data graph or are just "virtual".
Reference: [2] <author> S. Abiteboul, S. Cluet, and T. Milo. </author> <title> A database interface for files update. </title> <booktitle> In Proc. of the ACM SIGMOD Conf. on Management of Data, </booktitle> <address> San Jose, California, </address> <year> 1995. </year>
Reference-contexts: Their integration is a very active field of research (see for instance, for a very small sample, <ref> [15, 10, 13, 12, 23, 20, 14, 2, 3] </ref>). A key observation is that, often, the application programs used by organizations can only handle data of a specific format. (e.g. Web browsers, like Netscape, expect files in HTML format, and relational databases expect relations). <p> Many works on data translation focus on the translation of specific formats. Some examples are the LaTeX to HTML translators or the HTML to text translators, and mappings between structured documents and object oriented databases in <ref> [2] </ref> and [15]. Some works [3, 16, 14, 5, 25] generalize this approach and consider mappings between various data models. However most of them rely on the data and not on the schema.
Reference: [3] <author> S. Abiteboul, S. Cluet, and T. Milo. </author> <title> Correspondence and translation for heterogeneous data. </title> <booktitle> In Proc. </booktitle> <volume> ICDT 97, </volume> <pages> pages 351-363, </pages> <year> 1997. </year>
Reference-contexts: Their integration is a very active field of research (see for instance, for a very small sample, <ref> [15, 10, 13, 12, 23, 20, 14, 2, 3] </ref>). A key observation is that, often, the application programs used by organizations can only handle data of a specific format. (e.g. Web browsers, like Netscape, expect files in HTML format, and relational databases expect relations). <p> Writing such a program is typically a non trivial task which is often complicated by numerous technical aspects of the specific data sources that are not really relevant to the translation process (e.g. HTML or SGML parsing, or specific database access protocol). Recent works <ref> [3, 16] </ref> consider a more general framework which enables a more flexible translation between various models. The solution is based on using a common data model to which the source/target data is mapped, and providing a common translation language which enables the specification and customization of the translation task. <p> This makes the introduction of new translations easier, but very often still requires considerable programming effort whenever a new translation is to be defined <ref> [3] </ref>. The goal of this work is to design a mechanism for simplifying the specification of translations. <p> Then, based on the user's input, the matching process is completed and the translation is enabled. Note that the purpose of the schema-based data translation method that we propose is not to replace the programming languages for data translation proposed in <ref> [3, 16] </ref>, but rather to complement them. <p> For implementation reasons we used in the prototype Java as the rule definition language, but if desired, the user can use declarative rule languages in the style of <ref> [3, 16] </ref>, thus enabling logic-based inference of the properties and correctness of rules. This is beyond the scope of this paper. Handling data and schemas from different models requires a common framework in which the different schema and data formats can be presented. <p> For that we defined a middleware schema and data models in which the matching process and the data translation are performed. The schema model consists of graphs, and the data model consists of labeled forests and is similar to the one introduced in <ref> [3] </ref> and to the OEM and the tree models of [24, 10]. The difference with the OEM model is that we allow some nodes to be ordered. This is crucial for modeling data that might be ordered (e.g. structured documents). <p> Furthermore our system includes several import/export programs for some common data models (e.g. relational, OO, HTML, SGML, etc.) that can be used by the data sources. 3.1 The Data Model The data model that we use is similar to that of <ref> [3] </ref>, and to the OEM and the tree models of [24, 10]. Data is represented by a forest with labeled nodes. A particularity here is that we allow an order to be defined on the children of some of the nodes. <p> Supporting order as part of the data model enables a natural representation of data coming from such sources <ref> [3] </ref>. As in [3, 24, 10] the labels on vertices can be used to represent schematic information and data values. To represent cyclic structures, leaves can have values that are the ids of other vertices in the forest, in which case the leaf basically describes a "pointer" to the vertex. <p> Supporting order as part of the data model enables a natural representation of data coming from such sources [3]. As in <ref> [3, 24, 10] </ref> the labels on vertices can be used to represent schematic information and data values. To represent cyclic structures, leaves can have values that are the ids of other vertices in the forest, in which case the leaf basically describes a "pointer" to the vertex. <p> To obtain a "real" forest, the virtual nodes are glued to their parents (as in Definition 3.1). Finally, the resulting data instance is exported to target application. We conclude this section with two remarks: Combining Schema- and Data-based Translation: Recent works <ref> [3, 16] </ref> propose specialized programming languages, targeted for specifying data translations. The schema-based approach that we present here is not aimed at replacing these languages but rather at complementing them. <p> Many works on data translation focus on the translation of specific formats. Some examples are the LaTeX to HTML translators or the HTML to text translators, and mappings between structured documents and object oriented databases in [2] and [15]. Some works <ref> [3, 16, 14, 5, 25] </ref> generalize this approach and consider mappings between various data models. However most of them rely on the data and not on the schema. The input data is converted to some middleware model, where it is transformed or integrated with some target models. <p> The input data is converted to some middleware model, where it is transformed or integrated with some target models. This is often done using some translation language. The language should be powerful enough to capture a variety of translations, and may be quite complex. <ref> [3] </ref>, for example, uses datalog-style rules to do this. In [16] the model is more general, and allows the representation of schemas, but still, the translation program should be written manually, and the translation language is intricate.
Reference: [4] <author> R. Abu-Hamdeh, J.R. Cordy, and T.P. Martin. </author> <title> Schema translation using structural transformation. </title> <booktitle> In CASCON'94, IBM Centre for Advanced Studies 1994 Conference, </booktitle> <pages> pages 202-215, </pages> <month> Novem-ber </month> <year> 1994. </year>
Reference-contexts: See [7] for a survey of schema merging and translation techniques. Several works, e.g. [11, 5], consider aspects of merging schemas of source databases. Others, e.g. <ref> [6, 4] </ref>, consider translation of schemas from one model to another. A target schema is created by a series of manipulations on the source schema. [6] for example, introduces a meta-schema model, in which many schemas can be presented.
Reference: [5] <author> R. Ahmed, P. De Smedt, W. Du, W. Kent, M.A. Ketabchi, W. Litwin, A. Rafii, and M.C. Shan. </author> <title> The pegasus heterogeneous multidatabase system. </title> <journal> IEEE Computer, </journal> <volume> 24(12) </volume> <pages> 19-27, </pages> <year> 1991. </year>
Reference-contexts: Many works on data translation focus on the translation of specific formats. Some examples are the LaTeX to HTML translators or the HTML to text translators, and mappings between structured documents and object oriented databases in [2] and [15]. Some works <ref> [3, 16, 14, 5, 25] </ref> generalize this approach and consider mappings between various data models. However most of them rely on the data and not on the schema. The input data is converted to some middleware model, where it is transformed or integrated with some target models. <p> A related subject is schema transformation. Works in this area mainly concentrated on the restructuring of source schemas into target ones (and not on the conversion of data instances of the schemas). See [7] for a survey of schema merging and translation techniques. Several works, e.g. <ref> [11, 5] </ref>, consider aspects of merging schemas of source databases. Others, e.g. [6, 4], consider translation of schemas from one model to another.
Reference: [6] <author> P. Atzeni and R. Torlone. </author> <title> Schema translation between heterogeneous data models in a lattice framework. </title> <booktitle> In Sixth IFIP TC-2 Working Conference on Data Semantics (DS-6), </booktitle> <address> Atlanta, Geor-gia, </address> <year> 1995. </year>
Reference-contexts: See [7] for a survey of schema merging and translation techniques. Several works, e.g. [11, 5], consider aspects of merging schemas of source databases. Others, e.g. <ref> [6, 4] </ref>, consider translation of schemas from one model to another. A target schema is created by a series of manipulations on the source schema. [6] for example, introduces a meta-schema model, in which many schemas can be presented. <p> Several works, e.g. [11, 5], consider aspects of merging schemas of source databases. Others, e.g. [6, 4], consider translation of schemas from one model to another. A target schema is created by a series of manipulations on the source schema. <ref> [6] </ref> for example, introduces a meta-schema model, in which many schemas can be presented. Using the schema meta-model and a rule-based method, the source schema is restructured to become a schema in the target model. The output can then be mapped to the external "real world" format. <p> These works and others, e.g. [21, 22], address the problem of information capacity, namely determining whether it is possible to represent instances of the source schema by instances of the target schema, in a unique way, and vice versa. <ref> [6] </ref> proves that some schema transformations preserve information capacity. Note, however, that in our context, the user may sometimes need to export data to a specific target schema that does not preserve information capacity. Most of the works in this area do not consider the underlying data.
Reference: [7] <author> C. Batini, M. Lenzerini, and S.B. Navathe. </author> <title> A comparative analysis of methodologies for database schema integration. </title> <journal> ACM Computing Surverys, </journal> <volume> 18(4) </volume> <pages> 323-364, </pages> <month> Dec. </month> <year> 1986. </year>
Reference-contexts: For that one can use, for example, the languages mentioned above. A related subject is schema transformation. Works in this area mainly concentrated on the restructuring of source schemas into target ones (and not on the conversion of data instances of the schemas). See <ref> [7] </ref> for a survey of schema merging and translation techniques. Several works, e.g. [11, 5], consider aspects of merging schemas of source databases. Others, e.g. [6, 4], consider translation of schemas from one model to another.
Reference: [8] <author> C. Beeri, G. Elber, T. Milo, Y. Sagiv, N. Tishby, D. Konopniki, and P. Mogilevski. </author> <title> Websuite a tool suite for harnessing web data. </title> <note> In To appear in WebDB'98, </note> <year> 1998. </year>
Reference-contexts: The interface can also display the input/target data forests and the typing computed for their nodes. * an extendible library of import/export programs for connecting to external sources and importing/exporting data and schemas to the system. The TranScm system is a part of a larger project, WWWDAG <ref> [8] </ref>, that aims at developing tools for the utilization of digital libraries available through the Web. Our system is used to translate data found on the Web to the formats expected by the applications that are part of WWWDAG.
Reference: [9] <author> C. Beeri and T. Milo. </author> <title> Schemas for semi-structured data. </title> <type> Technical report. </type>
Reference-contexts: It is possible to show that in the worst case the process can take time exponential in the size of the input (the problem is NP com-plete), but for a large class of schemas, covering most common data models, a polynomial algo rithm exists <ref> [9] </ref>, and this is what we use here. * a graphical user interface that can display the two schemas and the set of matches determined by the system rules (and the problems, if any, encountered in the matching process), and assists the user in adding/disabling/modifying/overriding rules to obtain the desired matching
Reference: [10] <author> P. Buneman, S. Davidson, G. Hillebrand, and D. Suciu. </author> <title> A query language and optimization techniques for unstructured data. </title> <booktitle> In Proc. of the ACM SIGMOD Conf. on Management of Data, </booktitle> <address> San Diego, </address> <year> 1996. </year>
Reference-contexts: Their integration is a very active field of research (see for instance, for a very small sample, <ref> [15, 10, 13, 12, 23, 20, 14, 2, 3] </ref>). A key observation is that, often, the application programs used by organizations can only handle data of a specific format. (e.g. Web browsers, like Netscape, expect files in HTML format, and relational databases expect relations). <p> The schema model consists of graphs, and the data model consists of labeled forests and is similar to the one introduced in [3] and to the OEM and the tree models of <ref> [24, 10] </ref>. The difference with the OEM model is that we allow some nodes to be ordered. This is crucial for modeling data that might be ordered (e.g. structured documents). <p> our system includes several import/export programs for some common data models (e.g. relational, OO, HTML, SGML, etc.) that can be used by the data sources. 3.1 The Data Model The data model that we use is similar to that of [3], and to the OEM and the tree models of <ref> [24, 10] </ref>. Data is represented by a forest with labeled nodes. A particularity here is that we allow an order to be defined on the children of some of the nodes. Order in an inherent component of some data structures, e.g. ordered tuples and lists. <p> Supporting order as part of the data model enables a natural representation of data coming from such sources [3]. As in <ref> [3, 24, 10] </ref> the labels on vertices can be used to represent schematic information and data values. To represent cyclic structures, leaves can have values that are the ids of other vertices in the forest, in which case the leaf basically describes a "pointer" to the vertex.
Reference: [11] <author> P. Buneman, S. Davidson, and Anthony Kosky. </author> <title> Theoretical aspects of schema merging. </title> <booktitle> In Proc. Extending Database Technology, </booktitle> <year> 1992. </year>
Reference-contexts: A related subject is schema transformation. Works in this area mainly concentrated on the restructuring of source schemas into target ones (and not on the conversion of data instances of the schemas). See [7] for a survey of schema merging and translation techniques. Several works, e.g. <ref> [11, 5] </ref>, consider aspects of merging schemas of source databases. Others, e.g. [6, 4], consider translation of schemas from one model to another.
Reference: [12] <author> M.J. Carey et al. </author> <title> Towards heterogeneous multimedia information systems : The Garlic approach. </title> <type> Technical Report RJ 9911, </type> <institution> IBM Almaden Research Center, </institution> <year> 1994. </year>
Reference-contexts: Their integration is a very active field of research (see for instance, for a very small sample, <ref> [15, 10, 13, 12, 23, 20, 14, 2, 3] </ref>). A key observation is that, often, the application programs used by organizations can only handle data of a specific format. (e.g. Web browsers, like Netscape, expect files in HTML format, and relational databases expect relations).
Reference: [13] <author> T.-P. Chang and R. Hull. </author> <title> Using witness generators to support bi-directional update between object-based databases. </title> <booktitle> In Proc. ACM SIG-MOD/SIGACT Conf. on Princ. of Database Syst. (PODS), </booktitle> <address> San Jose, California, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: Their integration is a very active field of research (see for instance, for a very small sample, <ref> [15, 10, 13, 12, 23, 20, 14, 2, 3] </ref>). A key observation is that, often, the application programs used by organizations can only handle data of a specific format. (e.g. Web browsers, like Netscape, expect files in HTML format, and relational databases expect relations).
Reference: [14] <author> S. Chawathe, H. Garcia-Molina, J. Hammer, K. Ireland, Y. Papakonstantinou, J. Ullman, and J. Widom. </author> <title> The tsimmis project: Integration of heterogeneous information sources. </title> <booktitle> In Proceedings of IPSJ Conference, </booktitle> <pages> pages 7-18, </pages> <address> Tokyo, Japan, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: Their integration is a very active field of research (see for instance, for a very small sample, <ref> [15, 10, 13, 12, 23, 20, 14, 2, 3] </ref>). A key observation is that, often, the application programs used by organizations can only handle data of a specific format. (e.g. Web browsers, like Netscape, expect files in HTML format, and relational databases expect relations). <p> Many works on data translation focus on the translation of specific formats. Some examples are the LaTeX to HTML translators or the HTML to text translators, and mappings between structured documents and object oriented databases in [2] and [15]. Some works <ref> [3, 16, 14, 5, 25] </ref> generalize this approach and consider mappings between various data models. However most of them rely on the data and not on the schema. The input data is converted to some middleware model, where it is transformed or integrated with some target models.
Reference: [15] <author> V. Christophides, S. Abiteboul, S. Cluet, and M. Scholl. </author> <title> From structured documents to novel query facilities. </title> <booktitle> In Proc. of the ACM SIGMOD Conf. on Management of Data, </booktitle> <address> Minneapolis, Min-nesota, </address> <year> 1994. </year>
Reference-contexts: Their integration is a very active field of research (see for instance, for a very small sample, <ref> [15, 10, 13, 12, 23, 20, 14, 2, 3] </ref>). A key observation is that, often, the application programs used by organizations can only handle data of a specific format. (e.g. Web browsers, like Netscape, expect files in HTML format, and relational databases expect relations). <p> We demonstrate the above process with an example. We assume below some basic knowledge of SGML [19] and OODBs, and consider the translation of data between these formats. The example we use is a simplified version of the example described in <ref> [15] </ref>. (The full example can be handled similarly; the simplification is only for space reasons.) We ignore for now the representation of these formats in the middleware models (this will be considered in the next section) and concentrate on the matching and translation steps. <p> Many works on data translation focus on the translation of specific formats. Some examples are the LaTeX to HTML translators or the HTML to text translators, and mappings between structured documents and object oriented databases in [2] and <ref> [15] </ref>. Some works [3, 16, 14, 5, 25] generalize this approach and consider mappings between various data models. However most of them rely on the data and not on the schema. The input data is converted to some middleware model, where it is transformed or integrated with some target models.
Reference: [16] <author> S. Cluet, C. Delobel, J. Simeon, and K. Smaga. </author> <title> Your mediators need data conversion! In SIG-MOD'98, </title> <note> to appear, </note> <year> 1998. </year>
Reference-contexts: Writing such a program is typically a non trivial task which is often complicated by numerous technical aspects of the specific data sources that are not really relevant to the translation process (e.g. HTML or SGML parsing, or specific database access protocol). Recent works <ref> [3, 16] </ref> consider a more general framework which enables a more flexible translation between various models. The solution is based on using a common data model to which the source/target data is mapped, and providing a common translation language which enables the specification and customization of the translation task. <p> Then, based on the user's input, the matching process is completed and the translation is enabled. Note that the purpose of the schema-based data translation method that we propose is not to replace the programming languages for data translation proposed in <ref> [3, 16] </ref>, but rather to complement them. <p> For implementation reasons we used in the prototype Java as the rule definition language, but if desired, the user can use declarative rule languages in the style of <ref> [3, 16] </ref>, thus enabling logic-based inference of the properties and correctness of rules. This is beyond the scope of this paper. Handling data and schemas from different models requires a common framework in which the different schema and data formats can be presented. <p> To obtain a "real" forest, the virtual nodes are glued to their parents (as in Definition 3.1). Finally, the resulting data instance is exported to target application. We conclude this section with two remarks: Combining Schema- and Data-based Translation: Recent works <ref> [3, 16] </ref> propose specialized programming languages, targeted for specifying data translations. The schema-based approach that we present here is not aimed at replacing these languages but rather at complementing them. <p> Many works on data translation focus on the translation of specific formats. Some examples are the LaTeX to HTML translators or the HTML to text translators, and mappings between structured documents and object oriented databases in [2] and [15]. Some works <ref> [3, 16, 14, 5, 25] </ref> generalize this approach and consider mappings between various data models. However most of them rely on the data and not on the schema. The input data is converted to some middleware model, where it is transformed or integrated with some target models. <p> This is often done using some translation language. The language should be powerful enough to capture a variety of translations, and may be quite complex. [3], for example, uses datalog-style rules to do this. In <ref> [16] </ref> the model is more general, and allows the representation of schemas, but still, the translation program should be written manually, and the translation language is intricate. The closest to our approach is the one presented in [17], and demonstrated by the WOL language of [18].
Reference: [17] <author> Susan Davidson, Peter Buneman, and Anthony Kosky. </author> <title> Semantics of database transformations. </title> <type> Technical Report MS-CIS-95-25, </type> <institution> University of Pennsylvania, </institution> <year> 1995. </year>
Reference-contexts: In [16] the model is more general, and allows the representation of schemas, but still, the translation program should be written manually, and the translation language is intricate. The closest to our approach is the one presented in <ref> [17] </ref>, and demonstrated by the WOL language of [18]. This work also considers schema-based data translations. However, in their approach, the translation program depends on the specific characteristics of the input schema (e.g. specific labels and typing in the schema), and every two schemas should be assigned mappings manually.
Reference: [18] <author> Susan B. Davidson and Anthony S. Kosky. </author> <title> Wol : A language for database transformations and constrains. </title> <booktitle> In Proc. of the 13th Int. Conf. on Data Engineering, </booktitle> <pages> pages 55-65, </pages> <month> April </month> <year> 1997. </year>
Reference-contexts: In [16] the model is more general, and allows the representation of schemas, but still, the translation program should be written manually, and the translation language is intricate. The closest to our approach is the one presented in [17], and demonstrated by the WOL language of <ref> [18] </ref>. This work also considers schema-based data translations. However, in their approach, the translation program depends on the specific characteristics of the input schema (e.g. specific labels and typing in the schema), and every two schemas should be assigned mappings manually.
Reference: [19] <author> C.F. Goldfarb. </author> <title> The SGML Handbook. </title> <publisher> Calendon Press, Oxford, </publisher> <year> 1990. </year>
Reference-contexts: The resulting elements are "glued" together to form a valid instance of the target schema. Finally, the translated data is exported to the target application. We demonstrate the above process with an example. We assume below some basic knowledge of SGML <ref> [19] </ref> and OODBs, and consider the translation of data between these formats.
Reference: [20] <author> A. Levy, A. Rajaraman, and J. Ordille. </author> <title> Querying heterogeneous information sources using source descriptions. </title> <booktitle> In VLDB, </booktitle> <year> 1996. </year>
Reference-contexts: Their integration is a very active field of research (see for instance, for a very small sample, <ref> [15, 10, 13, 12, 23, 20, 14, 2, 3] </ref>). A key observation is that, often, the application programs used by organizations can only handle data of a specific format. (e.g. Web browsers, like Netscape, expect files in HTML format, and relational databases expect relations).
Reference: [21] <author> R.J. Miller, S. Y.E. Ioannidis, and R. Ramakrish-nan. </author> <title> The use of information capacity in schema integration and translation. </title> <booktitle> In VLDB, </booktitle> <year> 1993. </year>
Reference-contexts: Using the schema meta-model and a rule-based method, the source schema is restructured to become a schema in the target model. The output can then be mapped to the external "real world" format. These works and others, e.g. <ref> [21, 22] </ref>, address the problem of information capacity, namely determining whether it is possible to represent instances of the source schema by instances of the target schema, in a unique way, and vice versa. [6] proves that some schema transformations preserve information capacity.
Reference: [22] <author> R.J. Miller, S. Y.E. Ioannidis, and R. Ramakrish-nan. </author> <title> Schema equivalence in heterogeneous systems: Bridging theory and practice. </title> <journal> Information Systems, </journal> <volume> 19 </volume> <pages> 3-31, </pages> <year> 1994. </year>
Reference-contexts: Using the schema meta-model and a rule-based method, the source schema is restructured to become a schema in the target model. The output can then be mapped to the external "real world" format. These works and others, e.g. <ref> [21, 22] </ref>, address the problem of information capacity, namely determining whether it is possible to represent instances of the source schema by instances of the target schema, in a unique way, and vice versa. [6] proves that some schema transformations preserve information capacity.
Reference: [23] <author> Y. Papakonstantinou, H. Garcia-Molina, and J. Ullman. Medmaker: </author> <title> A mediation system based on declarative specifications. </title> <note> Available by anonymous ftp at db.stanford.edu as the file ~ /pub/papakonstantinou/1995/medmaker.ps. </note>
Reference-contexts: Their integration is a very active field of research (see for instance, for a very small sample, <ref> [15, 10, 13, 12, 23, 20, 14, 2, 3] </ref>). A key observation is that, often, the application programs used by organizations can only handle data of a specific format. (e.g. Web browsers, like Netscape, expect files in HTML format, and relational databases expect relations).
Reference: [24] <author> Y. Papakonstantinou, H. Garcia-Molina, and J. Widom. </author> <title> Object exchange across heterogeneous information sources. </title> <booktitle> In Int. Conference on Data Engineering, </booktitle> <year> 1995. </year>
Reference-contexts: The schema model consists of graphs, and the data model consists of labeled forests and is similar to the one introduced in [3] and to the OEM and the tree models of <ref> [24, 10] </ref>. The difference with the OEM model is that we allow some nodes to be ordered. This is crucial for modeling data that might be ordered (e.g. structured documents). <p> our system includes several import/export programs for some common data models (e.g. relational, OO, HTML, SGML, etc.) that can be used by the data sources. 3.1 The Data Model The data model that we use is similar to that of [3], and to the OEM and the tree models of <ref> [24, 10] </ref>. Data is represented by a forest with labeled nodes. A particularity here is that we allow an order to be defined on the children of some of the nodes. Order in an inherent component of some data structures, e.g. ordered tuples and lists. <p> Supporting order as part of the data model enables a natural representation of data coming from such sources [3]. As in <ref> [3, 24, 10] </ref> the labels on vertices can be used to represent schematic information and data values. To represent cyclic structures, leaves can have values that are the ids of other vertices in the forest, in which case the leaf basically describes a "pointer" to the vertex.
Reference: [25] <author> G. Wiederhold. </author> <title> Forward : Intelligent integration of information. </title> <journal> Journal of Intelligent Information Systems, </journal> 6(2/3):281-291, May 1996. 
Reference-contexts: Many works on data translation focus on the translation of specific formats. Some examples are the LaTeX to HTML translators or the HTML to text translators, and mappings between structured documents and object oriented databases in [2] and [15]. Some works <ref> [3, 16, 14, 5, 25] </ref> generalize this approach and consider mappings between various data models. However most of them rely on the data and not on the schema. The input data is converted to some middleware model, where it is transformed or integrated with some target models.
Reference: [26] <author> Sagit Zohar. </author> <title> Schema-based data translation, 1997. </title> <type> M.Sc Thesis, </type> <institution> Tel-Aviv University. </institution>
Reference-contexts: data from different sources is naturally represented in the middleware model we consider the representation of the SGML document and the OODB discussed in the previous section. (A formal definition of the model and additional examples of the representation of data from various sources in it can be found in <ref> [26] </ref>.) An SGML document is basically represented by its parse tree, so the document in Figure 2 is described by the tree in Figure 5. <p> Item (8) in the labeling is used to reflect this fact. To illustrate things we present below a few examples. (A formal definition of the schema model and additional examples of the representation of various schemas can be found in <ref> [26] </ref>; due to lack of space it is omitted here.) The schema graph of the OODB database is presented in Figure 7, and the schema graph of the SGML document from Figure 6 is pre representation sented in Figure 8. 1 The empty circles represent "virtual" elements (i.e. elements that do <p> to vertices (types) in G, s.t. each vertex v 0 2 F 0 satisfies the requirements of its assigned type, as described by the labeling of h (v 0 ) in G, and in particular v 0 is virtual iff h (v 0 ) is. (For a formal definition see <ref> [26] </ref>.) The explicit version F 0 of a data forest F and its type assignment h are used to determine the data translation, as explained in the next section. <p> Our system contains a large set of built-in rules for which correctness, in the above sense, has been verified <ref> [26] </ref>. <p> composed of five main components: * a rule base consisting of a large set of predefined rules covering all the above cases and many other common cases we encountered in our experiments and in the literature on data translation. (A full list of the available rules can be found in <ref> [26, 27] </ref>.) * a matching module in charge of the matching of the input and output schemas w.r.t to the current set of rules.
Reference: [27] <author> Sagit Zohar. </author> <title> The transcm system, </title> <note> 1997. http://www.math.tau.ac.il/~sagit/tranScm/. </note>
Reference-contexts: composed of five main components: * a rule base consisting of a large set of predefined rules covering all the above cases and many other common cases we encountered in our experiments and in the literature on data translation. (A full list of the available rules can be found in <ref> [26, 27] </ref>.) * a matching module in charge of the matching of the input and output schemas w.r.t to the current set of rules.
References-found: 27

