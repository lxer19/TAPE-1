URL: http://www.cs.jhu.edu/~salzberg/committees.ps
Refering-URL: http://www.cs.jhu.edu/~salzberg/cs661.html
Root-URL: 
Email: lastname@cs.jhu.edu  
Title: Committees of Decision Trees  
Author: David Heath, Simon Kasif, and Steven Salzberg 
Address: Baltimore, MD 21218 U.S.A.  
Affiliation: Department of Computer Science The Johns Hopkins University  
Abstract: Many intelligent systems are designed to sift through a mass of evidence and arrive at a decision. Certain pieces of evidence may be given more weight than others, and this may affect the final decision significantly. When than one intelligent agent is available to make a decision, we can form a committee of experts. By combining the different opinions of these experts, the committee approach can sometimes outperform any individual expert. In this paper, we show how to exploit randomized learning algorithms in order to develop committees of experts. By using the majority vote of these experts to make decisions, we are able to improve the performance of the original learning algorithm. More precisely, we have developed a randomized decision tree induction algorithm, which generates different decision trees every time it is run. Each tree represents a different expert decision-maker. We combine these trees using a majority voting scheme in order to overcome small errors that appear in individual trees. We have tested our idea with several real data sets, and found that accuracy consistently improved when compared to the decision made by a single expert. We have developed some analytical results that explain why this effect occurs. Our experiments also show that the majority voting technique outperforms at least some alternative strategies for exploiting randomization. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bennett, Kristin and Olvi Mangasarian, </author> <year> 1992. </year> <title> Robust linear programming discrimination of two linearly inseparable sets. </title> <journal> Optimization Methods and Software, </journal> <volume> 1 </volume> <pages> 23-34. </pages>
Reference-contexts: The suggestion is that our majority voting scheme is obtaining very close to the maximal accuracy possible for this data. 5.2 Applying k-DT to cancer diagnosis For our second experiment, we chose a dataset that has been the subject of experiments that classified the data using oblique hyperplanes <ref> (Bennett and Mangasarian, 1992) </ref>. This dataset contains 470 examples of patients with breast cancer, and the diagnostic task is to determine whether the cancer is benign or malignant.
Reference: <author> Breiman, Leo, Jerome Friedman, R. Olshen, and C. Stone, </author> <year> 1984. </year> <title> Classification and Regression Trees. </title> <type> Belmont, </type> <institution> Massachusetts: Wadsworth International Group. </institution> <note> 16 Heath, </note> <author> David, </author> <year> 1992. </year> <title> A Geometric Framework for Machine Learning. </title> <type> Ph.D. thesis, </type> <institution> Johns Hopkins University, Baltimore, Maryland. </institution>
Reference: <author> Holte, Robert, </author> <year> 1993. </year> <title> Very simple classification rules perform well on most commonly used datasets. </title> <journal> Machine Learning, </journal> <volume> 11 </volume> <pages> 63-90. </pages>
Reference-contexts: Rate of Trees MM 5:7 4:1 28% 4:1 9 IG 5:5 4:8 13% 4:8 5 Table 1: Iris results 5 Experiments 5.1 Classifying irises For our first experiment, we ran k-DT on Fisher's iris data, a well known dataset that has been the subject of numerous other machine learning studies <ref> (see Holte, 1993 for a recent summary) </ref>. The data consists of 150 examples, 50 each of three different types of irises: setosa, versacolor, and virginica. Each example is described by numeric measurements of width and length of the petals and sepals. We performed thirty-five 10-fold cross validation trials using SADT.
Reference: <author> Jacobs, R., M. Jordan, S. Nowlan, and G. Hinton, </author> <year> 1991. </year> <title> Adaptive mixtures of local experts. </title> <journal> Neural Computation, </journal> <volume> 3 </volume> <pages> 79-87. </pages>
Reference: <author> Lincoln, W. and J. Skrzypek, </author> <year> 1990. </year> <title> Synergy of clustering multiple back propagation networks. </title> <editor> In David S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <pages> pages 650-657. </pages> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Murthy, Sreerama, Simon Kasif, and Steven Salzberg, </author> <year> 1994. </year> <title> A system for induction of oblique decision trees. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2 </volume> <pages> 1-33. </pages>
Reference: <author> Odewahn, </author> <title> S.C., E.B. Stockwell, R.L. Pennington, R.M. </title> <type> Humphreys, </type> <institution> and W.A. Zumach, </institution> <year> 1992. </year> <title> Automated star-galaxy discrimination with neural networks. </title> <journal> Astronomical Journal, </journal> <volume> 103(1) </volume> <pages> 318-331. </pages> <note> 17 Quinlan, </note> <author> J. Ross, </author> <year> 1986. </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106. </pages>
Reference: <author> Quinlan, J. Ross, </author> <year> 1987. </year> <title> Generating production rules from decision trees. </title> <booktitle> In Proceedings of Tenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 304-307. </pages> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: A number of standard techniques have been 1 developed in the machine learning community, most notably Quinlan's C4.5 algorithm (1993) and Breiman et al.'s CART algorithm (1984). Since the introduction of these algorithms, numerous variations and improvements have been put forward, including new pruning strategies <ref> (e.g., Quinlan, 1987) </ref> and incremental versions of the algorithms (Utgoff, 1989). Many of these refinements have been designed to produce better decision trees; i.e., trees that were either more accurate classifiers, or smaller trees, or both.
Reference: <author> Quinlan, J. Ross, </author> <year> 1993. </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Salzberg, Steven, </author> <year> 1991. </year> <title> A nearest hyperrectangle learning method. </title> <journal> Machine Learning, </journal> <volume> 6 </volume> <pages> 251-276. </pages>
Reference: <author> Weiss, Sholom, and I. Kapouleas, </author> <year> 1989. </year> <title> An empirical comparison of pattern recognition, neural nets, and machine learning classification methods. </title> <booktitle> In Proceedings of the International Joint Conference of Artificial Intelligence, </booktitle> <pages> pages 781-787. </pages> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Wolpert, David, </author> <year> 1992. </year> <title> Stacked generalization. </title> <booktitle> Neural Networks, </booktitle> <volume> 5 </volume> <pages> 241-259. </pages>
Reference: <author> Zhang, Xiru, Jill Mesirov, and David Waltz, </author> <year> 1992. </year> <title> A hybrid system for protein secondary structure prediction. </title> <journal> Journal of Molecular Biology, </journal> <volume> 225 </volume> <pages> 1049-1063. </pages>
References-found: 13

