URL: http://www.ai.mit.edu/~mjones/model-based.ps
Refering-URL: http://www.ai.mit.edu/~mjones/mjones.html
Root-URL: 
Title: Model-Based Matching of Line Drawings by Linear Combinations of Prototypes  
Author: Michael J. Jones and Tomaso Poggio 
Date: 1559 December 1995  128  
Affiliation: MASSACHUSETTS INSTITUTE OF TECHNOLOGY ARTIFICIAL INTELLIGENCE LABORATORY and CENTER FOR BIOLOGICAL INFORMATION PROCESSING WHITAKER COLLEGE  
Pubnum: A.I. Memo No.  C.B.I.P. Paper No.  
Abstract: Copyright c fl1995 by The Institute of Electrical and Electronics Engineers, Inc. This paper also appeared in the Proceedings of the Fifth International Conference on Computer Vision Abstract We describe a technique for finding pixelwise correspondences between two images by using models of objects of the same class to guide the search. The object models are "learned" from example images (also called prototypes) of an object class. The models consist of a linear combination of prototypes. The flow fields giving pixelwise correspondences between a base prototype and each of the other prototypes must be given. A novel image of an object of the same class is matched to a model by minimizing an error between the novel image and the current guess for the closest model image. Currently, the algorithm applies to line drawings of objects. An extension to real grey level images is discussed. c fl Massachusetts Institute of Technology, 1996 This paper describes research done at the Artificial Intelligence Laboratory and within the Center for Biological and Computational Learning in the Department of Brain and Cognitive Sciences at the Massachusetts Institute of Technology. This research is sponsored by grants from ONR under contract N00014-93-1-0385 and from ARPA-ONR under contract N00014-92-J-1879; and by a grant from the National Science Foundation under contract ASC-9217041 (this award includes funds from ARPA provided under the HPCC program). Additional support is provided by the North Atlantic Treaty Organization, ATR Audio and Visual Perception Research Laboratories, Mitsubishi Electric Corporation, Sumitomo Metal Industries, Kodak, Daimler-Benz and Siemens AG. Support for the A.I. Laboratory's artificial intelligence research is provided by ARPA-ONR contract N00014-91-J-4038. Tomaso Poggio is supported by the Uncas and Helen Whitaker Chair at MIT's Whitaker College. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> James R. Bergen, P. Anandan, Keith J. Hanna and Rajesh Hingorani. </author> <title> Hierarchical Model-Based Motion Estimation. </title> <booktitle> In Second European Conference on Computer Vision, </booktitle> <publisher> Springer-Verlag, </publisher> <address> Santa Margherita Liguere, Italy, </address> <month> May </month> <year> 1992, </year> <pages> pp. 237-252. </pages>
Reference-contexts: Also, Cootes and Taylor match shape models (which are basically line drawings) to real images whereas we match line drawings to line drawings and also describe a method for matching real image models to real images. Another group of researchers, Bergen, Anandan, Hanna and Hingorani <ref> [1] </ref>, have described a framework for grey-level motion estimation. Their work is based on defining an error function which must be minimized to find the optimal flow field between two images. <p> The need for prior knowledge in the form of object models comes from the fact that optical flow is an underconstrained problem although other ways of adding constraints have of course been used (see for example <ref> [1, 13] </ref>). The linear combination of prototypes model that we described has several advantages. It is a simple learning-from-examples model that only requires 2D views as opposed to a 3D model.
Reference: [2] <author> David Beymer. </author> <title> Vectorizing Face Images by Interleaving Shape and Texture Computations. </title> <note> To be published as AI Memo, </note> <institution> AI Laboratory, MIT 1995. </institution>
Reference-contexts: Furthermore, many object transformations such as 3D rotations of a rigid object and changing expression of a face can be approximated by linear transformations, that can be learned from a small number of examples. The same motivation underlies the work of Beymer <ref> [2] </ref> who describes an alternative approach, also based on a linear combination of prototypes, to vectorize grey-level images. 3 Model-based matching using proto types 3.1 The model We would like the models used for model-based matching to be learned from examples as opposed to being hardwired. <p> In this case, in addition to modeling the shape of objects, we also model the texture of objects. We model texture analogously to the way we modeled shape as a linear combination of the grey level values (texture) of the prototype images (see also <ref> [2] </ref>, for an alternative approach to the same problem). A rather general justification of models of shape and texture consisting of linear combinations of prototypical shapes and textures is the following.
Reference: [3] <author> D. Beymer, A. Shashua and T. Poggio. </author> <title> Example Based Image Analysis and Synthesis. AI Memo No. </title> <type> 1431, </type> <institution> AI Laboratory, MIT 1993. </institution>
Reference-contexts: We define a model in this framework to be a linear combination of vectorized prototypes or equivalently a linear combination of example flow fields (see also <ref> [10, 3] </ref>). 1 To write the models mathematically, we must first introduce some notation. Let I 0 be the base prototype image to which all the correspondences reference. Let N be the total number of prototypes. <p> Under weak assumptions, one can prove that if any network can learn to synthesize shape or texture from examples then the desired shape or texture must be well approximated by a linear combination of the examples (see <ref> [3, 8] </ref>). Let fI j g be the set of prototype images where I 0 4 is the base image. <p> By image analysis we mean the problem of determining certain parameters describing an image such as the pose or expression parameters of an image of a face for example. Our approach to image analysis is to learn a mapping from images to their corresponding parameters (see <ref> [3] </ref>). The representation used for the images is critical in this approach. For example, trying to find a mapping from the raw grey level matrix of an image to its associated parameters would not result in a mapping which generalized to new images. <p> After matching, the resulting correspondences are used to create the vectorized representation for the novel image. The parameters of the novel image are then calculated by applying the previously learned mapping to the vec-torized representation of the novel image as described in <ref> [3] </ref>. As described briefly in section 3.5, we have written a system for analyzing line drawings such as those in figure 1. The system learns to analyze sketches from a user who trains the system with prototype examples. <p> The model parameters which are found by the matching can be used as the analysis parameters for the image. Alternatively, the model parameters can be mapped by an approximation network to a possibly higher level set of analysis parameters (see <ref> [3] </ref>. Examples of the higher level parameters would be given with each prototype. 5.2 Man-machine interface Image analysis can be used to build a general man-machine interface or a gesture recognition system ([3]). <p> It has a quite deep motivation since the linear combination of prototypes model is intimately related to general properties of a very broad class of synthesis networks of the type 5 described by <ref> [3] </ref>. A new model is fairly simple to cre-ate since all that is required are a number of example views of the object class and the pixelwise correspondences for each. Most importantly, the matching algorithm works well in practice.
Reference: [4] <author> Peter J. Burt. </author> <title> The Pyramid as a Structure for Efficient Computation. in Multi-Resolution Image Processing and Analysis. </title> <editor> ed. Rosenfield. </editor> <publisher> Springer-Verlag, </publisher> <year> 1984, </year> <pages> pp. 6-37. </pages>
Reference: [5] <author> T.F. Cootes and C.J. Taylor. </author> <title> Active Shape Models - "Smart Snakes". </title> <booktitle> In Proceedings of the British Machine Vision Conference. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1992, </year> <pages> pp. 266-275. </pages>
Reference: [6] <author> T.F. Cootes, C.J. Taylor, D.H. Cooper and J. Graham. </author> <title> Training Models of Shape from Sets of Examples. </title> <booktitle> In Proc. of the British Machine Vision Conference. </booktitle> <publisher> Springer Verlag, </publisher> <year> 1992, </year> <pages> pp. 9-18. </pages>
Reference: [7] <author> T.F. Cootes and C.J. Taylor. </author> <title> Using Grey-Level Models to Improve Active Shape Model Search. </title> <booktitle> In International Conference on Pattern Recognition, </booktitle> <volume> vol 1. </volume> <publisher> IEEE Computer Society Press, </publisher> <year> 1994, </year> <pages> pp 63-67. </pages>
Reference: [8] <author> Federico Girosi, Michael Jones and Tomaso Pog-gio. </author> <title> Priors, Stabilizers and Basis Functions: from regularization to radial, tensor and additive splines. AI Memo No. </title> <type> 1430, </type> <institution> AI Lab, MIT 1993. </institution>
Reference-contexts: Under weak assumptions, one can prove that if any network can learn to synthesize shape or texture from examples then the desired shape or texture must be well approximated by a linear combination of the examples (see <ref> [3, 8] </ref>). Let fI j g be the set of prototype images where I 0 4 is the base image.
Reference: [9] <author> Nikos K. Logothetis, Thomas Vetter, Anya Hurl-bert and Tomaso Poggio. </author> <title> View-based Models of 3D Object Recognition and Class-specific Invari-ances. AI Memo No. </title> <type> 1472, </type> <institution> AI Lab, MIT 1992. </institution>
Reference-contexts: The main difference between their work and ours is the type of model used. Our models are learned from examples and are specific to a particular object class. The main motivation for our work is the linear class concept of Poggio and Vetter <ref> [11, 9] </ref> that justifies modeling an object in terms of a linear combination of prototypes. Poggio and Vetter showed that linear transformations can be learned exactly from a small set of examples in the case of linear object classes.
Reference: [10] <author> Tomaso Poggio and Roberto Brunelli. </author> <title> A Novel Approach to Graphics. AI Memo No. </title> <type> 1354, </type> <institution> AI Laboratory, MIT 1992. </institution>
Reference-contexts: We define a model in this framework to be a linear combination of vectorized prototypes or equivalently a linear combination of example flow fields (see also <ref> [10, 3] </ref>). 1 To write the models mathematically, we must first introduce some notation. Let I 0 be the base prototype image to which all the correspondences reference. Let N be the total number of prototypes.
Reference: [11] <author> Tomaso Poggio and Thomas Vetter. </author> <title> Recognition and Structure from one 2D Model View: Observations on Prototypes, Object Classes and Symmetries. AI Memo No. </title> <type> 1347, </type> <institution> AI Laboratory, MIT 1992. </institution>
Reference-contexts: The main difference between their work and ours is the type of model used. Our models are learned from examples and are specific to a particular object class. The main motivation for our work is the linear class concept of Poggio and Vetter <ref> [11, 9] </ref> that justifies modeling an object in terms of a linear combination of prototypes. Poggio and Vetter showed that linear transformations can be learned exactly from a small set of examples in the case of linear object classes.
Reference: [12] <author> William H. Press, Brian P. Flannery, Saul A. Teukolsky and William T. Vetterling. </author> <title> Numerical Recipes in C: </title> <booktitle> The Art of Scientific Computing. </booktitle> <publisher> Cambridge University Press, </publisher> <address> Cambridge, Eng-land, </address> <note> second edition, </note> <year> 1992. </year>
Reference: [13] <author> Richard Szeliski and James Coughlan. </author> <title> Spline-Based Image Registration. </title> <type> Technical Report 94/1, </type> <institution> Digital Equipment Corporation, Cambridge Research Lab, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: Minimizing the error yields the model image which best fits the novel image. In order to minimize the error function, the Levenberg-Marquardt algorithm ([12]) is used (a similar use of Levenberg-Marquardt is described in <ref> [13] </ref>). This algorithm requires the derivative of the error with respect to each parameter. <p> The need for prior knowledge in the form of object models comes from the fact that optical flow is an underconstrained problem although other ways of adding constraints have of course been used (see for example <ref> [1, 13] </ref>). The linear combination of prototypes model that we described has several advantages. It is a simple learning-from-examples model that only requires 2D views as opposed to a 3D model.
Reference: [14] <author> Paul A. Viola. </author> <title> Alignment by Maximization of Mutual Information AI Technical Report 1548, AI Laboratory, </title> <publisher> MIT 1995. </publisher> <pages> 6 </pages>
Reference-contexts: The coarse-to-fine approach also significantly improves the robustness of the matching. When combined with blurring, the matching algorithm works well for a large range of settings of the initial parameters. A stochastic gradient minimization algorithm (described in <ref> [14] </ref>) has also been tried in place of Levenberg-Marquardt. It was found to be much faster (around 25 times) and more robust in that it got caught in local minima less frequently.
References-found: 14

