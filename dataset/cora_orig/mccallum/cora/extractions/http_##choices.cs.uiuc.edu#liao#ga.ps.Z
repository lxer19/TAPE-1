URL: http://choices.cs.uiuc.edu/liao/ga.ps.Z
Refering-URL: http://choices.cs.uiuc.edu/liao/home.html
Root-URL: http://www.cs.uiuc.edu
Email: w-liao@uiuc.edu  
Title: Performance Comparison of a Similarity-Based Learner, a Genetic Classifier System, and a Hybrid Learning System  
Author: Willy S. Liao 
Address: 1304 W. Springfield Urbana, IL 61801  
Affiliation: Department of Computer Science University of Illinois at Urbana-Champaign  
Abstract: The Hybrid Learning System (HLS) inductively learns concepts and is a hybrid of a genetic classifier system and a similarity-based learner. Its basic structure is a genetic algorithm that applies a local search operator to population members. This study compares ID3, a standard genetic classifer system, and HLS for accuracy and concept conciseness on a variety of problem domains with and without noise. The results show that the genetic learners have superior accuracy and conciseness, except on randomly constructed artificial concepts. Although the genetic learners perform similarly, the local search operator of HLS gives its genetic algorithm search faster convergence, allowing it to obtain similar or better results than an ordinary genetic search with less relative effort.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Breiman, L., Friedman, J.H., Olshen, R.A., and Stone, C.J. </author> <title> Classification and Regression Trees, </title> <booktitle> Wadsworth International Group, </booktitle> <year> 1984. </year>
Reference-contexts: Each path to a positive node is converted directly into a bitstring rule; attributes not encountered in the path have all their bits set to one. 3.3 Testing Procedure For most of the data sets used, n-fold cross-validation is employed <ref> [1] </ref>. This technique partitions the entire data set into n independent sets. Then each combination of n 1 partitions is used as a training set with the remaining partition used as a testing set.
Reference: [2] <author> Goldberg, D.E. </author> <title> Genetic Algorithms in Search, Optimization and Machine Learning, </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: GAs are weak methods requiring little background knowledge and are capable of globally searching hypothesis space, examining various disparate regions without being caught in local optima as hill-climbing methods often are <ref> [2] </ref>. The latter feature in particular is a function of the crossover and mutation operators used for deriving new hypotheses from old ones. However, the randomized nature of these operators makes it difficult for the population of hypotheses to achieve precise overlap with the actual target concept. <p> A loose interpretation of some theoretical analysis of GA properties is that GAs are excellent at searching through and determining the fitness of short "building blocks" (schemata) in the hypothesis language, but do not tend to preserve longer orderings of such blocks (see discussion of the Schema Theorem in <ref> [2] </ref>). One idea for overcoming this problem is to couple an SBL algorithm optimized for local search to a GA, thus improving the GA's local search capability. This study takes such an approach by introducing a new operator to perform local search. <p> This rule is added to the classifier rule set and all positive events covered by the rule are removed. GLS repeats this rule generation process until no positive events remain in the training set. The genetic algorithm is a standard implementation based on <ref> [2] </ref>. The sole difference between GLS and HLS is that the GA in HLS performs local search on each child after mating. The representation of the GA search space of classifier rules is a bitstring, since all attributes in the tested data are nominal-valued. <p> Inversion is allowed since it is not known if conditions might arise which are unfavorable to simple crossover without reordering. The selection scheme is stochastic remainder selection without replacement, the implementation of which comes from <ref> [2] </ref>. 3 2.3 Local Search Operator in HLS The local search performed by HLS is based on ID3's use of information theory to determine the attribute used to split a node [4].
Reference: [3] <author> Holland, J.H. </author> <title> "Escaping Brittleness: The possibilities of General-Purpose Learning Algorithms Applied to Parallel Rule-Based Systems," </title> <booktitle> in Machine Learning: An Artificial Intelligence Approach (Vol. </booktitle> <volume> 2), </volume> <editor> R. Michalski, J. Carbonell and T. Mitchell (Eds.), </editor> <publisher> Morgan Kaufmann, </publisher> <year> 1986. </year>
Reference-contexts: GLS and HLS are very similar and the following discussion of GLS applies to both unless otherwise stated. GLS follows the so-called Michigan approach to GA-based classifier systems, exemplified by Holland's classifer system <ref> [3] </ref>. The Michigan approach treats individual rules as population members. The alternative Pittsburgh approach uses genetic search on a population of rule sets instead of individual rules. An example Pittsburgh GA learner is given in [13].
Reference: [4] <author> Quinlan, J.R. </author> <title> "Learning Efficient Classification Procedures and Their Application to Chess End Games" in Machine Learning: An Artificial Intelligence Approach, </title> <publisher> Tioga, </publisher> <year> 1983. </year>
Reference-contexts: The gross structure of the new hybrid system in this study is similar to PLS2, in that an SBL routine resides within a genetic algorithm control structure. 2 Description of Algorithms 2.1 ID3: a Similarity-Based Learner A simple version of the decision-tree program ID3 <ref> [4] </ref> is employed in this study. The stopping criterion is a minimum sample size; if the number of events at a node falls below a certain threshold, the program stops splitting. <p> The selection scheme is stochastic remainder selection without replacement, the implementation of which comes from [2]. 3 2.3 Local Search Operator in HLS The local search performed by HLS is based on ID3's use of information theory to determine the attribute used to split a node <ref> [4] </ref>. The idea behind the hybrid operator is to do a little local search in hypothesis space using an individual rule as a starting point; if there exists a better rule nearby, it should replace the original individual.
Reference: [5] <author> Quinlan, J.R. </author> <title> "Induction of Decision Trees," </title> <booktitle> in Machine Learning (Vol. 1), 1986, pg. </booktitle> <pages> 81-106. </pages>
Reference-contexts: Similarity-based learning (SBL) algorithms operate on a prototype assumption or localization bias regarding class membership in feature space. One group of SBL algorithms operates by specialization, partitioning feature space into regions which are homogeneous with respect to class membership. Decision-tree programs such as ID3 <ref> [5] </ref> belong to this group. Such algorithms tend to be more suitable for concepts with a low concentration, or few peaks in instance space. 1 They tend to be less accurate in data domains exhibiting more disjuncts [9]. <p> The effect of minimum splitting size is most apparent when noise is introduced, as is discussed later. This version of ID3 does not use the chi-squared test for determining the appropriateness of a split <ref> [5] </ref>. Using this test generally produces more concise trees that are often extremely shallow. This is a problem with certain domains such as the parity functions.
Reference: [6] <author> Quinlan, J.R. </author> <title> "An Empirical Comparison of Genetic and Decision-Tree Classifiers," </title> <booktitle> in Proceedings of the Fifth International Conference on Machine Learning, 1988, pg. </booktitle> <pages> 135-141. </pages>
Reference: [7] <author> Rendell, L.A. </author> <title> "A Doubly Layered, Genetic Penetrance Learning System," </title> <booktitle> in Proceedings of the Third National Conference on Artificial Intelligence, </booktitle> <year> 1983. </year>
Reference: [8] <author> Rendell, L.A. </author> <title> "Genetic Plans and the Probabilistic Learning System: Synthesis and Results," </title> <booktitle> in Proceedings of the Fifth International Conference on Genetic Algorithms, </booktitle> <year> 1985. </year>
Reference-contexts: This study takes such an approach by introducing a new operator to perform local search. There exist a few demonstrations of the efficacy of GA-SBL hybrid algorithms in the literature. For example, PLS2 <ref> [8] </ref> employs a genetic algorithm which manipulates populations of regions derived by PLS1, an SBL algorithm.
Reference: [9] <author> Rendell, L.A. </author> <title> "Empirical Learning as a Function of Concept Character," </title> <booktitle> in Machine Learning (Vol. 3), 1990, pg. </booktitle> <pages> 267-298. </pages>
Reference-contexts: Efficacy of learning is tested with a separate testing set of events. Concepts can be characterized in several different ways. In particular, there are a number of factors believed to affect concept learning (these are more fully explored in <ref> [9] </ref>). For example, the number of peaks of class membership in instance space, can be used as a measure of a concept's "concentration." This is directly related to the entropy of the feature space, or its complexity. <p> Decision-tree programs such as ID3 [5] belong to this group. Such algorithms tend to be more suitable for concepts with a low concentration, or few peaks in instance space. 1 They tend to be less accurate in data domains exhibiting more disjuncts <ref> [9] </ref>. These algorithms examine individual features in a well-defined, non-randomized manner, and so it is expected that they possess good local search capabilities.
Reference: [10] <author> Rendell, L.A. </author> <title> "Induction as Optimization," </title> <journal> in IEEE Transactions on Systems, Man, and Cybernetics (Vol. </journal> <volume> 20), </volume> <year> 1990, </year> <month> pg. </month> <pages> 326-338. </pages>
Reference-contexts: PLS2 can learn an evaluation function for game tree search in the fifteen-puzzle and produce a function superior in accuracy to one derived by SBL and roughly equal to one from a standalone GA <ref> [10] </ref>. PLS2's relative cost to learn this function is cheaper than the cost incurred by the standalone GA. <p> The lower relative effort required by HLS compared to GLS is consistent with the results of a comparison of PLS2 to a conventional GA <ref> [10] </ref>. 5.2 Why Isn't HLS Performance Better? The above discussion extolling the virtues of the HLS local search operator raises this question: If the GA search of HLS is superior to that of GLS, why doesn't HLS as a whole learn better than GLS? The main faults lie in the fitness
Reference: [11] <author> Rendell, L.A. and Ragavan, H. </author> <title> "Improving the Design of Induction Methods by Analyzing Algorithm Functionality and Data-Based Concept Complexity," </title> <booktitle> Proceedings of the Thirteenth International Joint Conference on AI, </booktitle> <year> 1993. </year>
Reference: [12] <author> Sikora, R. and Shaw, M.J. </author> <title> "A Double-Layered Learning Approach to Acquiring Rules for Classification: Integrating Genetic Algorithms with Similarity-Based Learning," </title> <note> in ORSA Journal On Computing, Spring 1994. </note>
Reference-contexts: Even the simple use of an SBL program to provide the starting population for a GA, rather than a random initial population as is generally used, produces populations which converge more rapidly and concepts which are superior in accuracy in the business domain <ref> [12] </ref>. <p> An example Pittsburgh GA learner is given in [13]. This study uses the Michigan approach since it is easiest to apply the HLS hybrid operator to an individual rule instead of an an entire rule set. The control structure of GLS is based on the approach in <ref> [12] </ref>. Figure 1 gives a pictorial representation of the high-level controlling algorithm which invokes the genetic algorithm. GLS uses a GA search on the 2 training set to find the fittest rule which covers the most positive events.
Reference: [13] <author> Spears, W.M. </author> <title> "Using Genetic Algorithms For Supervised Concept Learning," </title> <booktitle> in Proceedings of the Second International IEEE Conference on Tools for AI, 1990, pg. </booktitle> <pages> 335-341. </pages>
Reference-contexts: The Michigan approach treats individual rules as population members. The alternative Pittsburgh approach uses genetic search on a population of rule sets instead of individual rules. An example Pittsburgh GA learner is given in <ref> [13] </ref>. This study uses the Michigan approach since it is easiest to apply the HLS hybrid operator to an individual rule instead of an an entire rule set. The control structure of GLS is based on the approach in [12]. <p> This representation is similar to the one used in the GA concept learner of <ref> [13] </ref>. <p> For examining the effects of noise, only the training set is corrupted since the testing data must be noise-free to assess accurately the effects of learning in the presence of noise as opposed to classifying noisy events. This study includes randomly-generated artificial concepts. As in <ref> [13] </ref>, concepts are crudely characterized by specifying the number of disjuncts and the number of conjuncts per disjunct. The concepts produced were over a four-attribute instance space, with each attribute having four possible values. The number of disjuncts is the number of classifier rules needed to specify a concept.
Reference: [14] <author> Thrun, S. </author> <title> "The MONK's Problems: A Performance Comparison of Different Learning Algorithms," </title> <institution> Carnegie-Mellon University Technical Report CMU-CS-91-197, </institution> <year> 1991. </year> <month> 12 </month>
Reference-contexts: The data sets used for testing are the 8-bit parity concept, the MONK's problems <ref> [14] </ref>, the tic-tac-toe data set, and a set of randomly generated artificial concepts. It should be noted that the entire instance space is used for the parity and the random artificial concepts. The remaining datasets do not enumerate all of instance space. <p> This study uses 2-conjunct, 3-conjunct, and 4-conjunct random concepts. The number of disjuncts produces concepts with concept coverage between 10 and 60% of instance space. The three MONK's problems <ref> [14] </ref> are used for external validation. This is a suite of artificially constructed test concepts for comparing machine learning algorithms. <p> The genetic learners are more accurate than ID3 in the presence of class noise but all three systems are the same with heavy attribute noise. The MONK's problems results (Table 3) are given to demonstrate external validity and performance relative to an outside implementation of ID3 tested in <ref> [14] </ref>. The MONK results show that all three algorithms perform at an acceptable level. The parity concept and tic-tac-toe data sets both exhibit feature interaction, and GLS and HLS are more accurate both on parity and noiseless tic-tac-toe (Figure 4).
References-found: 14

