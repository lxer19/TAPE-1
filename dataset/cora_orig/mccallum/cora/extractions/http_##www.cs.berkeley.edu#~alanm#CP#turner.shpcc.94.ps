URL: http://www.cs.berkeley.edu/~alanm/CP/turner.shpcc.94.ps
Refering-URL: http://www.cs.berkeley.edu/~alanm/CP/bib.html
Root-URL: 
Title: Cluster-C*: Understanding the Performance Limits  
Author: Charles J. Turner David Mosberger Larry L. Peterson 
Address: Tucson, AZ 85721  
Affiliation: Department of Computer Science University of Arizona  
Abstract: Data parallel languages are gaining interest as it becomes clear that they support a wider range of computation than previously believed. With improved network technology, it is now feasible to build data parallel supercomputers using traditional RISC-based workstations connected by a high-speed network. This paper presents an in-depth look at the communication behavior of nine C fl programs. It also compares the performance of these programs on both a cluster of 8 HP 720 workstations and a 32 node (128 Vector Unit) CM-5. The result is that under some conditions, the cluster is faster on an absolute scale, and that on a relative, per-node scale, the cluster delivers superior performance in all cases. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Banks and M. Prudence. </author> <title> A high-performance network architecture for a PA-RISC workstation. </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> 11(2) </volume> <pages> 191-202, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: For example, local windowing operations have lower physical communication requirements than global transformations like matrix transpose. 3 System Architecture Cluster-C fl consists of eight HP 9000/720 workstations (PA-RISC at 50 MHz). Each machine is equipped with 32 MB RAM and a Medusa FDDI controller <ref> [1] </ref>. We have measured this hardware's host-to-host throughput and latency to be 98.3Mbps and 47.8sec, respectively. A minimal OS runs on this platform. It provides a framework for running the three key pieces of the system: the network layer, the C fl run-time system (RTS), and the application programs.
Reference: [2] <author> A. Beguelin, J. Dongarra, A. Geist, R. Manchek, and V. Sunderam. </author> <title> A users' guide to PVMParallel Virtual Machine. </title> <type> Technical Report ORNL/TM-11826, </type> <institution> Oak Ridge National Laboratory, Oak Ridge, </institution> <address> TN 37831, USA, </address> <year> 1991. </year>
Reference-contexts: as having intrinsic merit in their own right: the data parallel model offers a high potential for concurrency and is generally considered easy to program [6], and loosely-coupled fl This work supported in part by ARPA Contracts DABT63-91-C-0030 and DACA76-93-C-0026. workstation clusters offer an attractive cost-performance alternative to tightly-coupled multiprocessors <ref> [3, 2] </ref>. Addressing the intersection of these two topics, therefore, has a high potential for payoff. The problem, of course, is that the fine-grained parallelism common in data parallel programs is assumed to be inappropriate for workstation clusters because of the associated high latency and low bandwidth of the network. <p> We exploit this regularity to define a message indexing scheme that is highly efficient for both the network interface and the language runtime system. This is in contrast to using a general-purpose communication substrate like PVM <ref> [2] </ref>. We use message vectorization [7] to aggregate multiple virtual processor messages destined for the same physical processor. The network protocols also implement implicit and explicit ACKs and eager retransmission. Fourth, we use several low-level optimizations to minimize the latency of the communication operations.
Reference: [3] <author> J. B. Carter, J. K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Symposium on Operating System Principles, </booktitle> <pages> pages 152-164, </pages> <year> 1991. </year>
Reference-contexts: as having intrinsic merit in their own right: the data parallel model offers a high potential for concurrency and is generally considered easy to program [6], and loosely-coupled fl This work supported in part by ARPA Contracts DABT63-91-C-0030 and DACA76-93-C-0026. workstation clusters offer an attractive cost-performance alternative to tightly-coupled multiprocessors <ref> [3, 2] </ref>. Addressing the intersection of these two topics, therefore, has a high potential for payoff. The problem, of course, is that the fine-grained parallelism common in data parallel programs is assumed to be inappropriate for workstation clusters because of the associated high latency and low bandwidth of the network.
Reference: [4] <author> J. L. Frankel. </author> <title> A reference description of the C* language. </title> <type> Technical Report TR-253, </type> <institution> Thinking Machines Corporation, </institution> <address> Cambridge, MA, USA, </address> <month> May </month> <year> 1991. </year>
Reference: [5] <author> P. J. Hatcher, M. J. Quinn, A. J. Lapadula, B. K. Seev-ers, R. J. Anderson, and R. R. Jones. </author> <title> Data-parallel programming on MIMD computers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 377-383, </pages> <year> 1991. </year>
Reference-contexts: First, all nodes in the cluster execute a copy of the same program, maintaining duplicate copies of all scalar variables and equal portions of all parallel variables in a manner similar to <ref> [5, 7] </ref>. This is in contrast to other multiprocessor-based implementations of C fl including both the CM-2 and CM-5in which a single front-end control processor is connected by a communication tree to an array of processing elements.
Reference: [6] <author> W. D. Hillis and L. W. Tucker. </author> <title> The CM-5 connection machine: A scalable supercomputer. </title> <journal> Communications of the ACM, </journal> <volume> 36(11), </volume> <year> 1993. </year>
Reference-contexts: Both aspects of this question are widely recognized as having intrinsic merit in their own right: the data parallel model offers a high potential for concurrency and is generally considered easy to program <ref> [6] </ref>, and loosely-coupled fl This work supported in part by ARPA Contracts DABT63-91-C-0030 and DACA76-93-C-0026. workstation clusters offer an attractive cost-performance alternative to tightly-coupled multiprocessors [3, 2]. Addressing the intersection of these two topics, therefore, has a high potential for payoff. <p> and briefly describing the communication substrate that underlies our implementation of C fl on a workstation cluster, this paper presents the results of a comprehensive study of the performance of the C fl test suite running on both an HP workstation cluster and a more tightly-coupled multiprocessor architecture, the CM-5 <ref> [6] </ref>. <p> All execution times were measured with either the PA-RISC 20 nanosecond cycle counter on the cluster, or the CM timer facility on the CM-5. The CM-5 architecture <ref> [6] </ref> consists of a front-end SPARC2 that runs all sequential portions of a C fl program, and controls a back-end processing array consisting of 32 SPARC2 processors, each equipped with four vector units (VUs), for a total of 128 VU's.
Reference: [7] <author> S. Hiranandani, K. Kennedy, and C. W. Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8), </volume> <year> 1992. </year>
Reference-contexts: When applied to a pvar, the scan operator computes all partial products of that operator, moving along one dimension of the shape, with optional segment masking. Note that most previous work involving data parallel languages has focused on Fortran D <ref> [7] </ref>. This is significant because C fl emphasizes a much richer interconnection of parallel data. <p> First, all nodes in the cluster execute a copy of the same program, maintaining duplicate copies of all scalar variables and equal portions of all parallel variables in a manner similar to <ref> [5, 7] </ref>. This is in contrast to other multiprocessor-based implementations of C fl including both the CM-2 and CM-5in which a single front-end control processor is connected by a communication tree to an array of processing elements. <p> In a cluster environment, any non-uniform identification of such a front-end processor introduces load imbalances and added communication overheads. Second, we have structured the C fl communication operators to support the overlap of communication and computation through message pipelining <ref> [7] </ref>. Each communication operator is decomposed into two distinct phases: a sending phase, during which local results are computed and sent to the other cluster nodes, and a receiving phase, during which the local processor receives intermediate results or messages from the other cluster nodes. <p> We exploit this regularity to define a message indexing scheme that is highly efficient for both the network interface and the language runtime system. This is in contrast to using a general-purpose communication substrate like PVM [2]. We use message vectorization <ref> [7] </ref> to aggregate multiple virtual processor messages destined for the same physical processor. The network protocols also implement implicit and explicit ACKs and eager retransmission. Fourth, we use several low-level optimizations to minimize the latency of the communication operations.
Reference: [8] <author> D. Mosberger, L. Peterson, and C. Turner. </author> <title> Exploiting highly reliable networks with careful protocols. </title> <type> Technical Report TR94-14, </type> <institution> University of Arizona, Department of Computer Science, </institution> <address> Tucson, AZ, </address> <month> March </month> <year> 1994. </year>
Reference-contexts: Our design involves five main ideas, which we now summarize; more detail can be found in a companion paper <ref> [8] </ref>. First, all nodes in the cluster execute a copy of the same program, maintaining duplicate copies of all scalar variables and equal portions of all parallel variables in a manner similar to [5, 7].
Reference: [9] <author> T. von Eicken, D. E. Culler, S. C. Goldstein, and K. E. Schauser. </author> <title> Active messages: a mechanism for integrated communication and computation. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: The network protocols also implement implicit and explicit ACKs and eager retransmission. Fourth, we use several low-level optimizations to minimize the latency of the communication operations. One optimization is similar to active messages <ref> [9] </ref>, in that as much work as possible is done at interrupt time, thereby minimizing context switch overhead. Interactions between the interrupt handler and the main C* thread are all accomplished without the aid of heavy-weight semaphores.
References-found: 9

