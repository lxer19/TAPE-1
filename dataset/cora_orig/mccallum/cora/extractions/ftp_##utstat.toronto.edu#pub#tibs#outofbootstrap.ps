URL: ftp://utstat.toronto.edu/pub/tibs/outofbootstrap.ps
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00296.html
Root-URL: 
Title: The out-of-bootstrap method for model averaging and selection  
Author: J. Sunil Rao and Robert Tibshirani 
Note: c flUniversity of Toronto  
Date: May 29, 1997  
Affiliation: Cleveland Clinic and University of Toronto  
Abstract: We propose a bootstrap-based method for model averaging and selection that focuses on training points that are left out of individual bootstrap samples. This information can be used to estimate optimal weighting factors for combining estimates from different bootstrap samples, and also for finding the best subsets the linear model setting. These proposals provide alternatives to Bayesian approaches to model averaging and selection, requiring less computation and fewer subjective choices. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Breiman, L. </author> <year> (1996a), </year> <title> `Bagging predictors', </title> <booktitle> Machine Learning. </booktitle>
Reference: <author> Breiman, L. </author> <year> (1996b), </year> <title> Bias, variance and arcing classifiers, </title> <type> Technical report, </type> <institution> University of California, Berkeley. </institution>
Reference: <author> Breiman, L. </author> <year> (1996c), </year> <title> Out-of-bag estimation, </title> <type> Technical report, </type> <institution> University of California, Berkeley. </institution>
Reference: <author> Breiman, L. </author> <year> (1997), </year> <note> `Stacked regressions', Machine Learning. 21 Breiman, </note> <author> L., Friedman, J., Olshen, R. & Stone, C. </author> <year> (1984), </year> <title> Classification and Regression Trees, </title> <publisher> Wadsworth. </publisher>
Reference: <author> Efron, B. </author> <year> (1979), </year> <title> `Bootstrap methods: another look at the jackknife', </title> <journal> Annals of Statistics 7, </journal> <pages> 1-26. </pages>
Reference: <author> Efron, B. </author> <year> (1983), </year> <title> `Estimating the error rate of a prediction rule: some improvements on cross-validation', </title> <journal> J. Amer. Statist. Assoc. </journal> <volume> 78, </volume> <pages> 316-331. </pages>
Reference: <author> Efron, B. & Tibshirani, R. </author> <year> (1997), </year> <title> `Improvements on cross-validation: the 632+ bootstrap: method', </title> <journal> J. Amer. Statist. </journal> <note> Assoc. </note> <author> p. ? Freedman, D. </author> <year> (1983), </year> <title> `A note on screening regression equations', </title> <journal> Amer. Statist. </journal> <pages> pp. 152-155. </pages>
Reference: <author> Gill, P., Murray, W. & Saunders, M. </author> <year> (1990), </year> <title> Lssol users guide 1.0, </title> <type> Technical report, </type> <institution> Stanford University. </institution>
Reference: <author> Harrison, D. & Rubinfeld, D. </author> <year> (1978), </year> <title> `Hedonic prices and the demand for clean air', </title> <editor> J. Environ. </editor> <booktitle> Economics and Management 5, </booktitle> <pages> 81-102. </pages>
Reference: <author> Kass, R. & Wasserman, L. </author> <year> (1995), </year> <title> A reference bayesian test for nested hypotheses and its rel ationship to the schwarz criterion, </title> <type> Technical report, </type> <institution> Carnegie Mellon Univ. </institution>
Reference: <author> Madigan, D. & Raftery, A. </author> <year> (1994), </year> <title> `Model selection and accounting for model uncertainty using occam's window.', </title> <journal> J. Amer. Statist. Assoc. </journal> <pages> pp. 1535-46. </pages>
Reference: <author> Raftery, A., Madigan, D. & Hoeting, J. </author> <year> (1993), </year> <title> Model selection and accounting for model uncertainty in linear regression models, </title> <type> Technical report, </type> <institution> Univ. of Washington. </institution>
Reference: <author> Rubin, D. </author> <year> (1981), </year> <title> `The bayesian bootstrap', </title> <journal> Ann. Statist. </journal> <pages> pp. 130-134. </pages>
Reference: <author> Shao, J. </author> <year> (1992), </year> <title> `Linear model selection by cross-validation', </title> <journal> J. Amer. Statist. Assoc. </journal> <volume> 88, </volume> <pages> 486-494. </pages>
Reference: <author> Shao, J. </author> <year> (1996), </year> <title> `Boostrap model selection', </title> <journal> J. Amer. Statist. Assoc. </journal> <volume> 91, </volume> <pages> 655-665. </pages>
Reference: <author> Shapire, R., Freund, Y., Bartlett, P. & Lee, W. </author> <year> (1997), </year> <title> Boosting the margin: a new explanation for the effectiveness of voting methods, </title> <booktitle> Snowbird 1997 conference. </booktitle>
Reference: <author> Stone, M. </author> <year> (1974), </year> <title> `Cross-validatory choice and assessment of statistical predictions', </title> <journal> J. Roy. Statist. Soc. </journal> <volume> 36, </volume> <pages> 111-147. </pages>
Reference: <author> Tibshirani, R. </author> <year> (1996), </year> <title> Bias, variance and prediction error for classification rules, </title> <type> Technical report, </type> <institution> University of Toronto. </institution>
Reference: <author> Wolpert, D. </author> <year> (1992), </year> <title> `Stacked generalization', </title> <booktitle> Neural Networks 5, </booktitle> <pages> 241-259. </pages>
Reference: <author> Zhang, P. </author> <year> (1993), </year> <title> `Model selection via multifold cross-validation', </title> <journal> Ann. Statist. </journal> <volume> 21, </volume> <pages> 299-311. 23 </pages>
References-found: 20

