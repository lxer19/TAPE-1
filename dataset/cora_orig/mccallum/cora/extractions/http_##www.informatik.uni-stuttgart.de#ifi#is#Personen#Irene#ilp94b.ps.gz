URL: http://www.informatik.uni-stuttgart.de/ifi/is/Personen/Irene/ilp94b.ps.gz
Refering-URL: http://www.informatik.uni-stuttgart.de/ifi/is/Personen/weber.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: stahl@informatik.uni-stuttgart.de weberi@informatik.uni-stuttgart.de  
Title: The Arguments of Newly Invented Predicates in ILP  
Author: Irene Stahl Irene Weber 
Address: Stuttgart, Breitwiesenstr. 20-22, D-70565 Stuttgart  
Affiliation: Institut fur Informatik, Universitat  
Abstract: In this paper we investigate the problem of choosing arguments for a new predicate. We identify the relevant terms to be considered as arguments, and propose methods to choose among them based on propositional minimisation.
Abstract-found: 1
Intro-found: 1
Reference: [BM92] <author> M. Bain and S. Muggleton. </author> <title> Non-monotonic learning. </title> <editor> In S. Muggleton, editor, </editor> <booktitle> Inductive Logic Programming. </booktitle> <publisher> Academic Press, </publisher> <year> 1992. </year>
Reference-contexts: Over-specific hypotheses need not be considered because they can be corrected without new predicates. The chosen over-general hypothesis provides constraints for the otherwise totally unconstrained PI task. Thus, specializing over-general clauses consistent is the situation encountered when PI is used as bias shift operation, e.g., <ref> [WO91, KNS92, LLM93, BM92, Mug93] </ref>. Then, PI consists of three basic steps. First, the over-general clauses to be corrected with new predicates need to be selected. Then, suitable arguments for the new predicates have to be determined. In a last step a definition of the new predicates is induced. <p> In a last step a definition of the new predicates is induced. The first step chooses among the clauses used in successful proofs of negative examples. The aim is to cut off all proofs of negative examples by specializing the selected clauses. A simple approach realized in the CWS-method <ref> [BMS94, BM92] </ref> is to take all clauses that directly resolve with negative examples. However, this might lead to correcting at the surface while the culprit lies deeper within the proofs. <p> In case it is necessary to specialize several clauses, each clause might be corrected with a separate new predicate as in <ref> [BMS94, BM92] </ref>. Alternatively, an incremental procedure might be used that specializes one over-general clause with one new predicate and then continues learning with the augmented vocabulary until learning succeeds or the next failure occurs [Wro93a]. So formally, the PI-situation in the bias shift setting is described as follows. <p> Choosing all variables of C always leads to a complete discrimination provided that the initial instantiations of C, C and C , are not contradictory. As we assume that C and C are disjoint, this is always the case in our setting. This simple approach is taken, e.g., in <ref> [BM92, Mug93, LeB94] </ref>. However, the complexity of inducing a definition for the new predicate grows exponentially with the number of arguments. Therefore, it is desirable to use a minimal set of variables enabling discrimination.
Reference: [BMS94] <author> M. Bain, S. Muggleton, and A. Srinivasan. </author> <title> Closed-world specialisation in incremental learning. </title> <publisher> Forthcoming, </publisher> <year> 1994. </year>
Reference-contexts: In a last step a definition of the new predicates is induced. The first step chooses among the clauses used in successful proofs of negative examples. The aim is to cut off all proofs of negative examples by specializing the selected clauses. A simple approach realized in the CWS-method <ref> [BMS94, BM92] </ref> is to take all clauses that directly resolve with negative examples. However, this might lead to correcting at the surface while the culprit lies deeper within the proofs. <p> In case it is necessary to specialize several clauses, each clause might be corrected with a separate new predicate as in <ref> [BMS94, BM92] </ref>. Alternatively, an incremental procedure might be used that specializes one over-general clause with one new predicate and then continues learning with the augmented vocabulary until learning succeeds or the next failure occurs [Wro93a]. So formally, the PI-situation in the bias shift setting is described as follows.
Reference: [KNS92] <author> B. Kijsirikul, M. Numao, and M. Shimura. </author> <title> Discrimination-based constructive induction of logic programs. </title> <booktitle> In Proc. of the 10th National Conference on AI, </booktitle> <year> 1992. </year>
Reference-contexts: Over-specific hypotheses need not be considered because they can be corrected without new predicates. The chosen over-general hypothesis provides constraints for the otherwise totally unconstrained PI task. Thus, specializing over-general clauses consistent is the situation encountered when PI is used as bias shift operation, e.g., <ref> [WO91, KNS92, LLM93, BM92, Mug93] </ref>. Then, PI consists of three basic steps. First, the over-general clauses to be corrected with new predicates need to be selected. Then, suitable arguments for the new predicates have to be determined. In a last step a definition of the new predicates is induced. <p> This leads to a propositional minimization problem: given tuples of attribute values classified as and , find a minimal set of attributes that allows for complete discrimination. 4.2 A Simple Solution to the Minimization Problem In their discrimination-based constructive induction (DBC) method <ref> [KNS92] </ref>, Ki-jsirikul et.al. solve the above stated minimization problem by a greedy removal procedure. Starting with the complete set of attributes, redundant ones are removed one by one. More formally, let (X 1 ; :::; X n ) be the variables of C without regarding order.
Reference: [LeB94] <author> Guillaume LeBlanc. </author> <title> BMWk revisited. Generaliztion and formalization of an algorithm for detecting recursive relations in term sequences. </title> <booktitle> In Machine Learning: ECML-94, European Conference on Machine Learning, Catania, </booktitle> <address> Italy. </address> <publisher> Springer, </publisher> <year> 1994. </year>
Reference-contexts: Choosing all variables of C always leads to a complete discrimination provided that the initial instantiations of C, C and C , are not contradictory. As we assume that C and C are disjoint, this is always the case in our setting. This simple approach is taken, e.g., in <ref> [BM92, Mug93, LeB94] </ref>. However, the complexity of inducing a definition for the new predicate grows exponentially with the number of arguments. Therefore, it is desirable to use a minimal set of variables enabling discrimination.
Reference: [LLM93] <author> S. Lapointe, C. Ling, and S. Matwin. </author> <title> Constructive inductive logic programming. </title> <booktitle> In Proc. of the IJCAI-93. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: Over-specific hypotheses need not be considered because they can be corrected without new predicates. The chosen over-general hypothesis provides constraints for the otherwise totally unconstrained PI task. Thus, specializing over-general clauses consistent is the situation encountered when PI is used as bias shift operation, e.g., <ref> [WO91, KNS92, LLM93, BM92, Mug93] </ref>. Then, PI consists of three basic steps. First, the over-general clauses to be corrected with new predicates need to be selected. Then, suitable arguments for the new predicates have to be determined. In a last step a definition of the new predicates is induced.
Reference: [Llo84] <author> J.W. Lloyd. </author> <title> Foundations of Logic Programming. </title> <publisher> Springer, </publisher> <address> Berlin, New York, 2nd edition, </address> <year> 1984. </year>
Reference-contexts: In case the language bias is too strong for the learning task, it needs to be shifted. PI is one potential approach to shift the language bias. Throughout the paper, we will use the standard logic programming terminology as described in <ref> [Llo84] </ref>. Investigating the search for the argument structure of a new predicate requires some additional notation. First, for a clause C, head (C) denotes the head and body (C) the body literals of C. Then, we must be able to refer to the variables of clauses and literals.
Reference: [MDR94] <author> S. Muggleton and L. De Raedt. </author> <title> Inductive logic programming: Theory and methods. </title> <journal> Journal of Logic Programming, Special Issue on 10 Years of Logic Programming, </journal> <year> 1994. </year>
Reference-contexts: ground facts E and E as positive and negative examples, a logic program B as background knowledge, and a hypothesis language L using predicate and function symbols from B, E and E , the system is to find a logic program H 2 L such that the following conditions hold <ref> [MDR94] </ref>: Prior Satisfiability: B 6j= E Posterior Satisfiability: B [ H 6j= E Prior Necessity: B 6j= E Posterior Sufficiency: B [ H j= E The quadruple (E ; E ; B; L) is called the learning task.
Reference: [Mic83] <author> R. S. Michalski. </author> <title> A theory and methodology of inductive learning. In R.S. </title> <editor> Michalski, J.G. Carbonell, and T.M. Mitchell, editors, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach. </booktitle> <publisher> Tioga, </publisher> <year> 1983. </year>
Reference-contexts: In order to reduce the effort in tailoring the representation to the requirements of the learning procedure, the idea of adjusting the representation automatically has come up very early in Machine Learning. Constructive learning, the capacity to extend a fixed initial representation by adding new terms <ref> [Mic83, MR89] </ref>, is the most prominent form of representation adjustment. In Inductive Logic Programming (ILP) [Mug92] that applies inductive learning to a first order Horn logic framework, the choice of an appropriate representation is particularly important. The first order framework of ILP requires very strong biases.
Reference: [MR89] <author> C. J. Matheus and L. A. Rendell. </author> <title> Constructive induction on decision trees. </title> <booktitle> In Proc. of the 11th International Joint Conference on AI, </booktitle> <year> 1989. </year>
Reference-contexts: In order to reduce the effort in tailoring the representation to the requirements of the learning procedure, the idea of adjusting the representation automatically has come up very early in Machine Learning. Constructive learning, the capacity to extend a fixed initial representation by adding new terms <ref> [Mic83, MR89] </ref>, is the most prominent form of representation adjustment. In Inductive Logic Programming (ILP) [Mug92] that applies inductive learning to a first order Horn logic framework, the choice of an appropriate representation is particularly important. The first order framework of ILP requires very strong biases.
Reference: [Mug92] <author> S. Muggleton. </author> <title> Inductive Logic Programming. </title> <publisher> Academic Press, </publisher> <year> 1992. </year>
Reference-contexts: Constructive learning, the capacity to extend a fixed initial representation by adding new terms [Mic83, MR89], is the most prominent form of representation adjustment. In Inductive Logic Programming (ILP) <ref> [Mug92] </ref> that applies inductive learning to a first order Horn logic framework, the choice of an appropriate representation is particularly important. The first order framework of ILP requires very strong biases. Using an unsuitable or overly restrictive bias might exclude the target description from the hypothesis space.
Reference: [Mug93] <author> S. Muggleton. </author> <title> Predicate invention and utility. </title> <journal> Journal of Experimental and Theoretical AI, </journal> <volume> (1), </volume> <year> 1993. </year>
Reference-contexts: Over-specific hypotheses need not be considered because they can be corrected without new predicates. The chosen over-general hypothesis provides constraints for the otherwise totally unconstrained PI task. Thus, specializing over-general clauses consistent is the situation encountered when PI is used as bias shift operation, e.g., <ref> [WO91, KNS92, LLM93, BM92, Mug93] </ref>. Then, PI consists of three basic steps. First, the over-general clauses to be corrected with new predicates need to be selected. Then, suitable arguments for the new predicates have to be determined. In a last step a definition of the new predicates is induced. <p> Choosing all variables of C always leads to a complete discrimination provided that the initial instantiations of C, C and C , are not contradictory. As we assume that C and C are disjoint, this is always the case in our setting. This simple approach is taken, e.g., in <ref> [BM92, Mug93, LeB94] </ref>. However, the complexity of inducing a definition for the new predicate grows exponentially with the number of arguments. Therefore, it is desirable to use a minimal set of variables enabling discrimination.
Reference: [MWKE93] <author> K. Morik, S. Wrobel, J. Kietz, and W. Emde. </author> <title> Knowledge Aquisition and Machine Learning: Theory Methods and Applications. </title> <publisher> Academic Press, </publisher> <year> 1993. </year>
Reference-contexts: However, this might lead to correcting at the surface while the culprit lies deeper within the proofs. Therefore, the more sophisticated minimal base revision approach [Wro93b] selects among the minimal sets of clauses required to cut off all successful proofs of negative examples. In MOBAL <ref> [MWKE93] </ref>, the selection is controlled by a two-tiered confidence model. In MILES [WT94], this model has been extended to a flexible three-tiered confidence model.
Reference: [PWZ88] <author> Z. Pawlak, S. K. M. Wong, and W. Ziarko. </author> <title> Rough sets: probabilistic versus deterministic approach. </title> <journal> Int. J. Man-Machine Studies, </journal> <volume> 11 </volume> <pages> 81-95, </pages> <year> 1988. </year>
Reference-contexts: The following definitions are adapted from rough set theory <ref> [PWZ88] </ref>. Definition 1: Let A be a set of attributes. A 2 A is dispensable (or redundant) in A with respect to a classification K if omitting A does not change the classification of any object. A is independent if any A 2 A is indispensable, else A is dependent.
Reference: [Qui86] <author> J. R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: For example, a simple heuristic always selects the attribute leading to the smallest intersection between and tuples. However, the algorithm in figure 2 would also allow for any other method of attribute selection, e.g., information gain <ref> [Qui86] </ref> or the data flow in the clause. In contrast to the DBC approach, the heuristic decisions are made explicit. 4.4 Minimality Both DBC and the method we proposed are designed to produce a minimal set of attributes sufficient for discrimination.
Reference: [WO91] <author> R. Wirth and P. O'Rorke. </author> <title> Constraints on predicate invention. </title> <booktitle> In Eighth International Conference on Machine Learning. </booktitle> <publisher> Morgan Kauf-mann, </publisher> <year> 1991. </year>
Reference-contexts: Over-specific hypotheses need not be considered because they can be corrected without new predicates. The chosen over-general hypothesis provides constraints for the otherwise totally unconstrained PI task. Thus, specializing over-general clauses consistent is the situation encountered when PI is used as bias shift operation, e.g., <ref> [WO91, KNS92, LLM93, BM92, Mug93] </ref>. Then, PI consists of three basic steps. First, the over-general clauses to be corrected with new predicates need to be selected. Then, suitable arguments for the new predicates have to be determined. In a last step a definition of the new predicates is induced.
Reference: [Wro93a] <author> S. Wrobel. </author> <title> Concept Formation and Knowledge Revision. A Demand-Driven Approach to Representation Change. </title> <type> PhD thesis, </type> <institution> Universitat Dortmund, </institution> <year> 1993. </year>
Reference-contexts: Alternatively, an incremental procedure might be used that specializes one over-general clause with one new predicate and then continues learning with the augmented vocabulary until learning succeeds or the next failure occurs <ref> [Wro93a] </ref>. So formally, the PI-situation in the bias shift setting is described as follows. The input consists of a logic program T , the current theory, and the over-general clause C 2 T .
Reference: [Wro93b] <author> S. Wrobel. </author> <title> On the proper definition of minimality in specialization and theory revision. </title> <booktitle> In Machine Learning: ECML-93, European Conference on Machine Learning, </booktitle> <address> Wien, Austria. </address> <publisher> Springer, </publisher> <year> 1993. </year>
Reference-contexts: A simple approach realized in the CWS-method [BMS94, BM92] is to take all clauses that directly resolve with negative examples. However, this might lead to correcting at the surface while the culprit lies deeper within the proofs. Therefore, the more sophisticated minimal base revision approach <ref> [Wro93b] </ref> selects among the minimal sets of clauses required to cut off all successful proofs of negative examples. In MOBAL [MWKE93], the selection is controlled by a two-tiered confidence model. In MILES [WT94], this model has been extended to a flexible three-tiered confidence model.
Reference: [WT94] <author> I. Weber and B. Tausend. </author> <title> A three-tiered confidence model for revising logical theories. </title> <booktitle> In Proc. of ILP-94, </booktitle> <year> 1994. </year>
Reference-contexts: Therefore, the more sophisticated minimal base revision approach [Wro93b] selects among the minimal sets of clauses required to cut off all successful proofs of negative examples. In MOBAL [MWKE93], the selection is controlled by a two-tiered confidence model. In MILES <ref> [WT94] </ref>, this model has been extended to a flexible three-tiered confidence model. For investigating argument choice mechanisms in the bias shift setting, it suffices to consider the case that only one over-general clause is to be specialized with a new predicate.
References-found: 18

