URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-95-1262/CS-TR-95-1262.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-95-1262/
Root-URL: http://www.cs.wisc.edu
Email: wwt@cs.wisc.edu  
Title: Tempest: A Substrate for Portable Parallel Programs  
Author: Mark D. Hill, James R. Larus, and David A. Wood 
Address: 1210 West Dayton St. Madison, WI 53706 USA  
Affiliation: Computer Sciences Department University of WisconsinMadison  
Date: Spring 95, March 1995.  
Note: To appear: COMPCON  
Abstract: This paper describes Tempest, a collection of mechanisms for communication and synchronization in parallel programs. With these mechanisms, authors of compilers, libraries, and application programs can exploitacross a wide range of hardware platformsthe best of shared memory, message passing, and hybrid combinations of the two. Because Tempest provides mechanisms, not policies, programmers can tailor communication to a programs sharing pattern and semantics, rather than restructuring the program to run with the limited communication options offered by existing parallel machines. And since the mechanisms are easily supported on different machines, Tempest provides a portable interface across platforms. This paper describes the Tempest mechanisms, briey explains how they are used, outlines several implementations on both custom and stock hardware, and presents preliminary performance results that demonstrate the benefits of this approach. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> John B. Carter, John K. Bennett, and Willy Zwaenepoel. </author> <title> Implementation and Performance of Munin. </title> <booktitle> In Proc. of the Thirteenth ACM Symposium on Operating System Principles (SOSP), </booktitle> <pages> pp. 152164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: PVM [7] is a widely-used, coarse-grain message-passing system. Berkeleys Active Messages [23] provides a portable interface for fine-grain messages, but, unlike Tempest, no support for transparent caching. DSM systems, such as Rices Munin <ref> [1] </ref> and Treadmarks [10], support shared memory, but since their coherence is limited to page granularity, they require more complex semantic models to mitigate the adverse effects of false sharing. Tempests fine-grain access control avoids page-level false sharing. <p> Tempests fine-grain access control avoids page-level false sharing. Several other systems also support custom protocols, including MIT Alewife [2], Rice Munin <ref> [1] </ref>, and Stanford FLASH [12]. We are not aware, however, of another system that gives a user complete, protected control over protocols. Some Tempest protocols have predecessors.
Reference: [2] <author> David Chaiken, John Kubiatowicz, and Anant Agarwal. </author> <title> LimitLESS Directories: A Scalable Cache Coherence Scheme. </title> <booktitle> In Proc. of the Fourth Inter. Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV), </booktitle> <pages> pp. 224234, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Tempests fine-grain access control avoids page-level false sharing. Several other systems also support custom protocols, including MIT Alewife <ref> [2] </ref>, Rice Munin [1], and Stanford FLASH [12]. We are not aware, however, of another system that gives a user complete, protected control over protocols. Some Tempest protocols have predecessors.
Reference: [3] <author> Satish Chandra, James R. Larus, and Anne Rogers. </author> <title> Where is Time Spent in Message-Passing and Shared-Memory Programs? In Proc. </title> <booktitle> of the Sixth Inter. Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VI), </booktitle> <pages> pp. 6175, </pages> <month> October </month> <year> 1994. </year>
Reference: [4] <author> D. E. Culler, A. Dusseau, S. C. Goldstein, A. Krishnamurthy, S. Lumetta, T. von Eicken, and K. Yelick. </author> <title> Parallel Programming in Split-C. </title> <booktitle> In Proc. of Supercomputing 93, </booktitle> <pages> pp. 262273, </pages> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: With its mechanisms, coarse-grain message passing (e.g., PVM [7]) or NUMA (no caching) shared memory (e.g., Split-C <ref> [4] </ref>) are easily implemented. Active Messages Bulk Data Transfer Virtual Memory Mgmt. Fine Grain Access Control Message Passing X X Data Parallelism X X NUMA Shared Memory X Coherent Shared Memory X X X Hybrid X X X X TABLE 1. Use of Tempest mechanisms.
Reference: [5] <author> William J. Dally and D. Scott Wills. </author> <title> Universal Mechanism for Concurrency. </title> <booktitle> In PARLE 89: Parallel Architectures and Languages Europe. </booktitle> <publisher> Springer-Verlag, </publisher> <month> June </month> <year> 1989. </year>
Reference-contexts: LOADs of Invalid blocks and STOREs to Invalid or Read-Only blocks invoke user-level handlers. This mechanism enables Tempest to support coherence at the same granularity as hardware shared-memory systems [17]. Tempest provides mechanisms to implement programming paradigms, but leaves policy to user-level code <ref> [5] </ref>. Table 1 summarizes the Tempest mechanisms that support different programming paradigms. This code may reside in unprivileged libraries, be generated by a compiler, or be written specifically for an application. <p> Several machines share features with Tempest implementations. The MIT J-Machine shares Tempests goal of providing mechanisms, not policy, but uses a custom processor <ref> [5] </ref>. Stanford FLASH is similar in many respects to Typhoon. FLASH, however, uses a custom memory controller, rather than a snooping device, runs handlers on all hardware caches misses, and runs protocols in privileged mode without address translation.
Reference: [6] <author> Babak Falsafi, Alvin Lebeck, Steven Reinhardt, Ioannis Schoin-as, Mark D. Hill, James Larus, Anne Rogers, and David Wood. </author> <title> Application-Specific Protocols for User-Level Shared Memory. </title> <booktitle> In Proc. of Supercomputing 94, </booktitle> <pages> pp. 380389, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: In particular, Tempest supports custom shared-memory coherence protocols that provide an appli-This paper is a summary of research performed by the Wisconsin Wind Tunnel project. Many ideas described above were previously introduced in other papers: Tempest, Typhoon, and Stache [21], Blizzard [22], custom protocols <ref> [6] </ref>, and Loosely Coherent Memory [15]. Abstracts [9] and information on our papers can be found at URL: http://www.cs.wisc.edu/~wwt This work is supported in part by Wright Laboratory Avionics Directorate, Air Force Material Command, USAF, under grant #F33615-94-1-1525 and ARPA order no. <p> To demonstrate these ideas, we developed custom update protocols for three applications: NAS Appbt, Berkeley EM3D, and SPLASH Barnes <ref> [6] </ref>. The three protocols differ substantially in how they detect sharing. Appbts protocol exploits the applications static and predictable sharing pattern to send updates directly. Barnes dynamic and changeable sharing requires updates to be forwarded through a home node that maintains a sharing list. <p> More recent versions of Blizzard-S closed this gap to 1.5X and run some programs faster than Blizzard-Ewhen high miss rates makes Blizzard-Ss lower miss overhead more important that its higher lookup overhead at each access. Finally, Falsafi et al. <ref> [6] </ref> demonstrate the enormous potential of custom coherence protocols. They improved the 32-processor Blizzard-E performance of NAS Appbt, Berkeley EM3D, and SPLASH Barnes by factors of 5.7, 16.0 and 1.4over optimized shared memory versions by changing the coherence protocols, as described in Section 3.
Reference: [7] <author> Al Geist, Adam Beguelin, Jack Dongarra, Weicheng Jiang, Rob-ert Manchek, and Vaidy Sunderam. </author> <title> PVM 3 userss Guide and Reference Manual. </title> <type> Technical Report ORNL/TM-12187, </type> <institution> Oak Ridge National Laboratory, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: With its mechanisms, coarse-grain message passing (e.g., PVM <ref> [7] </ref>) or NUMA (no caching) shared memory (e.g., Split-C [4]) are easily implemented. Active Messages Bulk Data Transfer Virtual Memory Mgmt. Fine Grain Access Control Message Passing X X Data Parallelism X X NUMA Shared Memory X Coherent Shared Memory X X X Hybrid X X X X TABLE 1. <p> On the CM-5, the shared-memory EM3D ran as fast as a native message-passing version. 6 Related Work Several interfaces share Tempests goal of providing portability among parallel machines. PVM <ref> [7] </ref> is a widely-used, coarse-grain message-passing system. Berkeleys Active Messages [23] provides a portable interface for fine-grain messages, but, unlike Tempest, no support for transparent caching.
Reference: [8] <author> Erik Hagersten. </author> <title> Toward Scalable Cache Only Memory Architectures. </title> <institution> The Royal Institute of Technology Swedish Institute of Computer Science, </institution> <month> Oct. </month> <year> 1992. </year> <institution> Stockholm, Sweden. </institution> <type> Ph.D. Thesis, </type> <institution> Swedish Institute of Computer Science Dissertation Series 08. </institution>
Reference-contexts: Some Tempest protocols have predecessors. In particular, Stache is similar to a DSM protocol extended to cache-sized blocks and to a software implementation of the hardware COMA protocols of the Data Diffusion Machine <ref> [8] </ref> and Kendall Square KSR-1 [11]. Several machines share features with Tempest implementations. The MIT J-Machine shares Tempests goal of providing mechanisms, not policy, but uses a custom processor [5]. Stanford FLASH is similar in many respects to Typhoon.
Reference: [9] <author> Mark D. Hill, James R. Larus, and David A. Wood. </author> <title> The Wiscon-sin Wind Tunnel Project: An Annotated Bibliography. Computer Architecture News, </title> <address> 22(5):1926, </address> <month> December </month> <year> 1994. </year> <note> (Mosaic location is: http://www.cs.wisc.edu/p/wwt/Mosaic/wwt.html). </note>
Reference-contexts: Many ideas described above were previously introduced in other papers: Tempest, Typhoon, and Stache [21], Blizzard [22], custom protocols [6], and Loosely Coherent Memory [15]. Abstracts <ref> [9] </ref> and information on our papers can be found at URL: http://www.cs.wisc.edu/~wwt This work is supported in part by Wright Laboratory Avionics Directorate, Air Force Material Command, USAF, under grant #F33615-94-1-1525 and ARPA order no.
Reference: [10] <author> Pete Keleher, Sandhya Dwarkadas, Alan Cox, and Willy Zwaene-poel. TreadMarks: </author> <title> Distributed Shared Memory on Standard Workstations and Operating Systems. </title> <type> Technical Report 93-214, </type> <institution> Department of Computer Science, Rice University, </institution> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: PVM [7] is a widely-used, coarse-grain message-passing system. Berkeleys Active Messages [23] provides a portable interface for fine-grain messages, but, unlike Tempest, no support for transparent caching. DSM systems, such as Rices Munin [1] and Treadmarks <ref> [10] </ref>, support shared memory, but since their coherence is limited to page granularity, they require more complex semantic models to mitigate the adverse effects of false sharing. Tempests fine-grain access control avoids page-level false sharing.
Reference: [11] <institution> Kendall Square Research. Kendall Square Research Technical Summary, </institution> <year> 1992. </year>
Reference-contexts: Some Tempest protocols have predecessors. In particular, Stache is similar to a DSM protocol extended to cache-sized blocks and to a software implementation of the hardware COMA protocols of the Data Diffusion Machine [8] and Kendall Square KSR-1 <ref> [11] </ref>. Several machines share features with Tempest implementations. The MIT J-Machine shares Tempests goal of providing mechanisms, not policy, but uses a custom processor [5]. Stanford FLASH is similar in many respects to Typhoon.
Reference: [12] <author> Jeffrey Kuskin et al. </author> <title> The Stanford FLASH Multiprocessor. </title> <booktitle> In Proc. of the 21st Annual Inter. Symposium on Computer Architecture, </booktitle> <pages> pp. 302313, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Tempests fine-grain access control avoids page-level false sharing. Several other systems also support custom protocols, including MIT Alewife [2], Rice Munin [1], and Stanford FLASH <ref> [12] </ref>. We are not aware, however, of another system that gives a user complete, protected control over protocols. Some Tempest protocols have predecessors.
Reference: [13] <author> James R. Larus. </author> <title> C**: a Large-Grain, Object-Oriented, Data-Parallel Programming Language. </title> <editor> In Utpal Banerjee, David Gelernt-er, Alexandru Nicolau, and David Padua, editors, </editor> <booktitle> Languages And Compilers for Parallel Computing (5th Inter. Workshop), </booktitle> <pages> pp. 326341. </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1993. </year>
Reference-contexts: Custom protocols can also help support high-level parallel programming languages, which offer semantically attractive constructs that can be difficult to implement efficiently on parallel machines. An example is the copy-in, copy-out semantics that Fortran 90 provides for some data structures and built-in functions. The C** data parallel programming language <ref> [13] </ref> offers this semantics for general routines and data structures. We used Tempest to assist a compiler in efficiently supporting this language semantics.
Reference: [14] <author> James R. Larus. </author> <title> Compiling for Shared-Memory and Message-Passing Computers. </title> <journal> ACM Letters on Programming Languages and Systems, </journal> <volume> 2(14):165180, </volume> <month> MarchDecember </month> <year> 1994. </year>
Reference: [15] <author> James R. Larus, Brad Richards, and Guhan Viswanathan. </author> <title> LCM: Memory System Support for Parallel Language Implementation. </title> <booktitle> In Proc. of the Sixth Inter. Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VI), </booktitle> <pages> pp. </pages> <address> 208218, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: In particular, Tempest supports custom shared-memory coherence protocols that provide an appli-This paper is a summary of research performed by the Wisconsin Wind Tunnel project. Many ideas described above were previously introduced in other papers: Tempest, Typhoon, and Stache [21], Blizzard [22], custom protocols [6], and Loosely Coherent Memory <ref> [15] </ref>. Abstracts [9] and information on our papers can be found at URL: http://www.cs.wisc.edu/~wwt This work is supported in part by Wright Laboratory Avionics Directorate, Air Force Material Command, USAF, under grant #F33615-94-1-1525 and ARPA order no. <p> The C** data parallel programming language [13] offers this semantics for general routines and data structures. We used Tempest to assist a compiler in efficiently supporting this language semantics. Loosely Coherent Memory (LCM) <ref> [15] </ref> implements fine-grain copy-on-write operations, which allows C** programs to run correctly, even when compiler cannot analyze their sharing pattern because of pointers or function calls. 4 Implementing Tempest To develop and demonstrate the Tempest interface, we implemented it on several platforms with different levels of hardware support.
Reference: [16] <author> James R. Larus and Eric Schnarr. EEL: </author> <title> Machine-Independent Executable Editing, </title> <note> 1994. Submitted for publication. </note>
Reference-contexts: Blizzard-E, however, will not work on processors that do not allow restartable exceptions on ECC errors. To increase portability, we developed the all-software Blizzard-S. Blizzard-S modifies executable programs (a.out files) with a tool based on EEL <ref> [16] </ref> to add an explicit tag check before all LOADs and STOREs that could access the shared segment. The current version uses several optimizations to reduce the frequency of tests and implement them in five instructions, in the best case.
Reference: [17] <author> Daniel Lenoski, James Laudon, Truman Joe, David Nakahira, Luis Stevens, Anoop Gupta, and John Hennessy. </author> <title> The DASH Prototype: Logic Overhead and Performance. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(1):4161, </volume> <month> January </month> <year> 1993. </year>
Reference-contexts: The tags are Invalid, Read-Only, and Read-Write. LOADs of Invalid blocks and STOREs to Invalid or Read-Only blocks invoke user-level handlers. This mechanism enables Tempest to support coherence at the same granularity as hardware shared-memory systems <ref> [17] </ref>. Tempest provides mechanisms to implement programming paradigms, but leaves policy to user-level code [5]. Table 1 summarizes the Tempest mechanisms that support different programming paradigms. This code may reside in unprivileged libraries, be generated by a compiler, or be written specifically for an application. <p> The numbers, unfortunately, are not directly comparable, because that they come from different systems (simulation or implementation), different Tempest implementations, different benchmarks, and different protocols. Reinhardt et al. [21] used simulations on the Wisconsin Wind Tunnel [20] to compare Typhoon against a CC-NUMA machine modeled after the Stanford DASH <ref> [17] </ref>. The results showed that Typhoon performs very closely to the all-hardware implementation when both systems ran their base coherence protocols. Typhoon per Data ECC Trap Application Blizzard Run-Time Protocol Software ref Cache miss Memory FIGURE 5.
Reference: [18] <author> Kai Li and Paul Hudak. </author> <title> Memory Coherence in Shared Virtual Memory Systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4):321359, </volume> <month> November </month> <year> 1989. </year>
Reference: [19] <author> Steven K. Reinhardt, Babak Falsafi, and David A. Wood. </author> <title> Kernel Support for the Wisconsin Wind Tunnel. </title> <booktitle> In Proc. of the Usenix Symposium on Microkernels and Other Kernel Architectures, </booktitle> <month> September </month> <year> 1993. </year>
Reference-contexts: The CM-5 provides no support for shared memory but does fit the machine model depicted in Figure 3. 2 The CM-5s network interface is mapped into a user programs address space and provides fast messages. The Tempest virtual memory management mechanisms are provided by an extended CM-5 node kernel <ref> [19] </ref>. Blizzard implements fine-grain access control through two alternative methods. First, Blizzard-E uses a CM-5 diagnostic mode to intentionally set double-bit ECC errors in Invalid blocks. As depicted in Figure 4, a LOAD or STORE that misses in the CM-5s hardware cache goes to memory for a cache-line fill.
Reference: [20] <author> Steven Reinhardt, Mark D. Hill, James R. Larus, Alvin R. Leb-eck, James C. Lewis, and David A. Wood. </author> <title> The Wisconsin Wind Tunnel: Virtual Prototyping of Parallel Computers. </title> <booktitle> In Proc. of the 1993 ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pp. 4860, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: The numbers, unfortunately, are not directly comparable, because that they come from different systems (simulation or implementation), different Tempest implementations, different benchmarks, and different protocols. Reinhardt et al. [21] used simulations on the Wisconsin Wind Tunnel <ref> [20] </ref> to compare Typhoon against a CC-NUMA machine modeled after the Stanford DASH [17]. The results showed that Typhoon performs very closely to the all-hardware implementation when both systems ran their base coherence protocols. Typhoon per Data ECC Trap Application Blizzard Run-Time Protocol Software ref Cache miss Memory FIGURE 5. <p> FLASH, however, uses a custom memory controller, rather than a snooping device, runs handlers on all hardware caches misses, and runs protocols in privileged mode without address translation. Blizzards kernel interface and ECC use come from its ancestor, the Wisconsin Wind Tunnel <ref> [20] </ref>. 6 7 Summary The Tempest mechanisms provide a substrate for portable and efficient parallel programs. A programmer or compiler writer can use these mechanisms to implement an efficient parallel program through the time-proven process of successive refinement.
Reference: [21] <author> Steven K. Reinhardt, James R. Larus, and David A. Wood. Tempest and Typhoon: </author> <title> User-Level Shared Memory. </title> <booktitle> In Proc. of the 21st Annual Inter. Symposium on Computer Architecture, </booktitle> <pages> pp. 325337, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: In particular, Tempest supports custom shared-memory coherence protocols that provide an appli-This paper is a summary of research performed by the Wisconsin Wind Tunnel project. Many ideas described above were previously introduced in other papers: Tempest, Typhoon, and Stache <ref> [21] </ref>, Blizzard [22], custom protocols [6], and Loosely Coherent Memory [15]. Abstracts [9] and information on our papers can be found at URL: http://www.cs.wisc.edu/~wwt This work is supported in part by Wright Laboratory Avionics Directorate, Air Force Material Command, USAF, under grant #F33615-94-1-1525 and ARPA order no. <p> As a common denominator, Tempest assumes a distributed memory hardware base constructed from P processing nodes (see Figure 3) <ref> [21] </ref>. To simplify the exposition, this paper assumes a single program multiple data (SPMD) programming model with one processor per node and one computation thread per processor. Each thread runs in a private address space augmented by an optional shared segment. <p> A parallel machine built from these nodes connects them with a point-to-point network that is accessed through a network interface (NI). Typhoon implements Tempest through the network interface chip depicted in Figure 4 <ref> [21] </ref>. Typhoons Network Interface (NI) includes a reverse translation lookaside buffer (RTLB) to implement fine-grain access control, a processor to run user-level handlers, DMA logic to support block transfers, and the network interface itself. <p> The fill succeeds for valid tags, but the ECC error for an Invalid tag causes a trap, which Blizzard-E vectors to a user-level handler. The 1. RTLB misses delay the processor while the NI loads the entry from memory. Special mappings treat private memory as Read-Write <ref> [21] </ref>. 2. Blizzard does not use the CM-5 vector units. RTLB Data Instr. Cache Network I/F Block Xfer Unit Dis patch Ctrl Integer Processor B B L MBus Interface Cache FIGURE 4. Typhoons Network Interface. 5 Read-Only state is synthesized with page-level protection. <p> The numbers, unfortunately, are not directly comparable, because that they come from different systems (simulation or implementation), different Tempest implementations, different benchmarks, and different protocols. Reinhardt et al. <ref> [21] </ref> used simulations on the Wisconsin Wind Tunnel [20] to compare Typhoon against a CC-NUMA machine modeled after the Stanford DASH [17]. The results showed that Typhoon performs very closely to the all-hardware implementation when both systems ran their base coherence protocols.
Reference: [22] <author> Ioannis Schoinas, Babak Falsafi, Alvin R. Lebeck, Steven K. Re-inhardt, James R. Larus, and David A. Wood. </author> <title> Fine-grain Access Control for Distributed Shared Memory. </title> <booktitle> In Proc. of the Sixth Inter. Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VI), </booktitle> <pages> pp. 297307, </pages> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: In particular, Tempest supports custom shared-memory coherence protocols that provide an appli-This paper is a summary of research performed by the Wisconsin Wind Tunnel project. Many ideas described above were previously introduced in other papers: Tempest, Typhoon, and Stache [21], Blizzard <ref> [22] </ref>, custom protocols [6], and Loosely Coherent Memory [15]. Abstracts [9] and information on our papers can be found at URL: http://www.cs.wisc.edu/~wwt This work is supported in part by Wright Laboratory Avionics Directorate, Air Force Material Command, USAF, under grant #F33615-94-1-1525 and ARPA order no. <p> In all cases, the NI follows the buss snooping protocol and appears to be another processor. In some sense, the NI is the agent for other nodes in the system that helps achieve global coherence with only locally-coherent hardware. Blizzard implements Tempest on a CM-5 <ref> [22] </ref>. The CM-5 provides no support for shared memory but does fit the machine model depicted in Figure 3. 2 The CM-5s network interface is mapped into a user programs address space and provides fast messages. <p> However, Typhoon performed up to 35% better for EM3D when running a custom update protocol that would be difficult to implement in hardware. Schoinas et al. <ref> [22] </ref> present early measurements for Blizzard running on a 32-node CM-5. The results show that Blizzard-S is a viable implementation that runs than two times slower than Blizzard-E, in the worst case.
Reference: [23] <author> Thorsten von Eicken, David E. Culler, Seth Copen Goldstein, and Klaus Erik Schauser. </author> <title> Active Messages: a Mechanism for Integrating Communication and Computation. </title> <booktitle> In Proc. of the 19th Annual Inter. Symposium on Computer Architecture, </booktitle> <pages> pp. 256 266, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Each thread runs in a private address space augmented by an optional shared segment. Shared-memory and hybrid applications can use Tempest mechanisms (or Tempest shared-memory libraries) to manage the shared address space. The four types of Tempest mechanism are: Active messages are short, low-latency messages <ref> [23] </ref>. They are useful for sending control, synchronization, or short data messages. Upon receipt of an active message, the system invokes the handler specified by the message and passes two arguments: the senders processor number and the message length. The handler reads the message body from the incoming message queue. <p> On the CM-5, the shared-memory EM3D ran as fast as a native message-passing version. 6 Related Work Several interfaces share Tempests goal of providing portability among parallel machines. PVM [7] is a widely-used, coarse-grain message-passing system. Berkeleys Active Messages <ref> [23] </ref> provides a portable interface for fine-grain messages, but, unlike Tempest, no support for transparent caching.
Reference: [24] <author> William A Wulf. </author> <booktitle> Compilers and Computer Architecture. IEEE Computer, </booktitle> <address> 14(7):4147, </address> <month> July </month> <year> 1981. </year>
Reference-contexts: This code may reside in unprivileged libraries, be generated by a compiler, or be written specifically for an application. By separating policy from mechanism, Tempest avoids the pitfalls inherent in system-level policies that are too general and expensive or too specific and incomplete <ref> [24] </ref>. 3 Using Tempest Perhaps the best way to understand Tempest is to see how it is used. With its mechanisms, coarse-grain message passing (e.g., PVM [7]) or NUMA (no caching) shared memory (e.g., Split-C [4]) are easily implemented. Active Messages Bulk Data Transfer Virtual Memory Mgmt.
References-found: 24

