URL: ftp://ftp.speech.sri.com/pub/people/ees/papers/icslp96-segmentation.ps
Refering-URL: http://www.speech.sri.com/people/ees/publications.html
Root-URL: 
Email: stolcke@speech.sri.com ees@speech.sri.com  
Title: AUTOMATIC LINGUISTIC SEGMENTATION OF CONVERSATIONAL SPEECH  
Author: Andreas Stolcke Elizabeth Shriberg 
Address: Menlo Park, CA 94025  
Affiliation: Speech Technology and Research Laboratory SRI International,  
Abstract: As speech recognition moves toward more unconstrained domains such as conversational speech, we encounter a need to be able to segment (or resegment) waveforms and recognizer output into linguistically meaningful units, such a sentences. Toward this end, we present a simple automatic segmenter of transcripts based on N-gram language modeling. We also study the relevance of several word-level features for segmentation performance. Using only word-level information, we achieve 85% recall and 70% precision on linguistic boundary detection. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. </author> <title> A maximum entropy approach to natural language processing. </title> <booktitle> Computational Linguistics, </booktitle> <address> 22(1):3971, </address> <year> 1996. </year>
Reference-contexts: The views herein are those of the authors and should not be interpreted as representing the policies of DARPA or the NSF. 3 Such an integration can be achieved in a language model using the maximum entropy paradigm <ref> [1] </ref>, but this would make the estimation process considerably more expensive.
Reference: 2. <author> E. Brill. </author> <title> Some advances in transformation-based part of speech tagging. </title> <booktitle> In Proceedings of the 12th National Conference on Artificial Intelligence, </booktitle> <address> Seattle, WA, 1994. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Conceptually, segmentation is just another classification problem, in which each word transition must be la-beled as either a segment boundary or a within-segment transition. Two natural choices for alternative approaches are decision trees and a transformation-based, error-driven classifier of the type developed by Eric Brill for other tagging problems <ref> [2] </ref>.
Reference: 3. <author> K. W. Church. </author> <title> A stochastic parts program and noun phrase parser for unrestricted text. </title> <booktitle> In Second Conference on Applied Natural Language Processing, </booktitle> <pages> pages 136143, </pages> <address> Austin, Texas, </address> <year> 1988. </year>
Reference-contexts: We already mentioned that if POS information is to be used for segmentation, an automatic tagging step is required. This presents somewhat of a chicken-and-egg problem, in that taggers typically rely on segmentations. An appealing solution to this problem in the statistical tagging framework <ref> [3] </ref> would be to model both segmentation and tag assignment as a single hidden Markov process. 6.3. Other Features for Segmentation All of our experiments were based on lexical information only.
Reference: 4. <author> J. J. Godfrey, E. C. Holliman, and J. McDaniel. </author> <title> SWITCHBOARD: Telephone speech corpus for research and development. </title> <booktitle> In Proceedings IEEE Conference on Acoustics, Speech and Signal Processing, </booktitle> <volume> volume I, </volume> <pages> pages 517520, </pages> <address> San Fran-cisco, </address> <month> March </month> <year> 1992. </year>
Reference-contexts: We therefore chose to eliminate the problem of inaccurate speech recognition and tested our algorithms on hand-transcribed word-level transcripts of spontaneous speech from the Switchboard corpus <ref> [4] </ref>. An additional benefit of this approach is that the models employed by the segmentation algorithms can also be directly used as language models for speech recognizers for the same type of data, an application we are pursuing as well.
Reference: 5. <author> F. Jelinek. </author> <title> Self-organized language modeling for speech recognition. </title> <editor> In A. Waibel and K.-F. Lee, editors, </editor> <booktitle> Readings in Speech Recognition. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, Ca., </address> <year> 1990. </year>
Reference-contexts: THE MODEL The language models used were of the N-gram type commonly used in speech recognition <ref> [5] </ref>. In N-gram models, a word w n from a n 1 word history w 1 : : : w n1 . If the history contains a segment boundary &lt;s&gt;, it is truncated before that location.
Reference: 6. <author> S. M. Katz. </author> <title> Estimation of probabilities from sparse data for the language model component of a speech recognizer. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> 35(3):400401, </volume> <month> March </month> <year> 1987. </year>
Reference-contexts: Longer N-grams can be used if more state information is kept. The local N-gram probabilities are estimated from the training data by using Katz backoff with Good-Turing discounting <ref> [6] </ref>. 5. RESULTS 5.1. Baseline Segmentation Model The first model we looked at models only plain words and segment boundaries in the manner described. It was applied to the concatenation of all turns of a conversation side, with no additional contextual cues supplied.
Reference: 7. <author> M. Meteer et al. </author> <title> Dysfluency annotation stylebook for the Switchboard corpus. Distributed by LDC, </title> <month> February </month> <year> 1995. </year> <note> Revised June 1995 by Ann Taylor. </note>
Reference-contexts: The resulting segmentations were then evaluated along a number of metrics. As training data, we used 1.4 million words of Switchboard transcripts annotated for linguistic segmentations by the UPenn Tree-bank project <ref> [7] </ref>, comprising a total of 193,000 segments. One half of the standard Switchboard development test set, totaling 10,000 words and 1,300 segments, was used for testing.
Reference: 8. <author> M. Meteer and R. Iyer. </author> <title> Modeling conversational speech for speech recognition. </title> <booktitle> In Proceedings of the Conference on Empirical Methods in Natural Language Processing, </booktitle> <address> Philadelphia, PA, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: Our main motivation for the work reported here comes from speech language modeling. Experiments at the 1995 Johns Hopkins Language Modeling Workshop showed that the quality of a language model (LM) can be improved if both training and test data are segmented linguistically, rather than acoustically <ref> [8] </ref>. We showed in [10] and [9] that proper modeling of filled pauses requires knowledge of linguistic segment boundaries. We found for example that segment-internal filled pauses condition the following words quite differently from segment-initial filled pauses. Finally, recent efforts in language modeling for conversational speech, such as [8], attempt to <p> than acoustically <ref> [8] </ref>. We showed in [10] and [9] that proper modeling of filled pauses requires knowledge of linguistic segment boundaries. We found for example that segment-internal filled pauses condition the following words quite differently from segment-initial filled pauses. Finally, recent efforts in language modeling for conversational speech, such as [8], attempt to capitalize on the internal structure of utterances and turns. Such models are formulated in terms of linguistic units and therefore require linguistic segmentations to be applicable. 3. <p> This language model is a full implementation of the model approximated in <ref> [8] </ref>. The hidden disfluency model of [10] has a similar structure. As indicated in the formulae above, we currently use at most two words of history in the local conditional probabilities p (j). Longer N-grams can be used if more state information is kept.
Reference: 9. <author> E. Shriberg and A. Stolcke. </author> <title> Word predictability after hesitations: A corpus-based study. </title> <booktitle> In Proceedings International Conference on Spoken Language Processing, </booktitle> <address> Philadelphia, PA, </address> <month> October </month> <year> 1996. </year>
Reference-contexts: Experiments at the 1995 Johns Hopkins Language Modeling Workshop showed that the quality of a language model (LM) can be improved if both training and test data are segmented linguistically, rather than acoustically [8]. We showed in [10] and <ref> [9] </ref> that proper modeling of filled pauses requires knowledge of linguistic segment boundaries. We found for example that segment-internal filled pauses condition the following words quite differently from segment-initial filled pauses.
Reference: 10. <author> A. Stolcke and E. Shriberg. </author> <title> Statistical language modeling for speech disfluencies. </title> <booktitle> In Proceedings IEEE Conference on Acoustics, Speech and Signal Processing, </booktitle> <volume> volume I, </volume> <pages> pages 405 408, </pages> <address> Atlanta, GA, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: Experiments at the 1995 Johns Hopkins Language Modeling Workshop showed that the quality of a language model (LM) can be improved if both training and test data are segmented linguistically, rather than acoustically [8]. We showed in <ref> [10] </ref> and [9] that proper modeling of filled pauses requires knowledge of linguistic segment boundaries. We found for example that segment-internal filled pauses condition the following words quite differently from segment-initial filled pauses. <p> This language model is a full implementation of the model approximated in [8]. The hidden disfluency model of <ref> [10] </ref> has a similar structure. As indicated in the formulae above, we currently use at most two words of history in the local conditional probabilities p (j). Longer N-grams can be used if more state information is kept.
References-found: 10

