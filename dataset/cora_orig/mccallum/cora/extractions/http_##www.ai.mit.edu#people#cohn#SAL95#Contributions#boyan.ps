URL: http://www.ai.mit.edu/people/cohn/SAL95/Contributions/boyan.ps
Refering-URL: http://www.ai.mit.edu/people/cohn/SAL95/papers.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: jab@cs.cmu.edu  
Title: Active Learning for Optimal Control in Acyclic Domains  
Author: Justin A. Boyan 
Web: http://www.cs.cmu.edu/~jab/  
Address: Pittsburgh, PA 15213  
Affiliation: Computer Science Department Carnegie Mellon University  
Abstract-found: 0
Intro-found: 1
Reference: <author> Boyan, J. A., and Moore, A. W. </author> <year> 1995a. </year> <title> Dynamic programming and function approximation. </title> <note> Technical Report (to appear), </note> <institution> Carnegie Mellon University. </institution>
Reference-contexts: We compared ROUT with TD (0) and TD (1) on several toy problems and two medium-scale problems: a two-player children's dice game ("Pig") and a k-armed bandit problem. A full accounting of the results will appear elsewhere <ref> (Boyan & Moore 1995a) </ref>.
Reference: <author> Boyan, J. A., and Moore, A. W. </author> <year> 1995b. </year> <title> Generalization in reinforcement learning: Safely approximating the value function. </title> <editor> In Tesauro, G.; Touretzky, D. S.; and Leen, T. K., eds., </editor> <booktitle> Advances in Neural Information Processing Systems 7. </booktitle> <publisher> MIT Press. </publisher>
Reference-contexts: In real-world tasks with huge state spaces, this value function must be represented compactly by a function approximator. Our research has focused on how to generate these approximations robustly and efficiently <ref> (Boyan & Moore 1995b) </ref>; active learning can play a crucial role. Active learning of value functions presents quite a different challenge than active learning in the supervised learning context.
Reference: <author> Cormen, T. H.; Leiserson, C. E.; and Rivest, R. L. </author> <year> 1990. </year> <title> Introduction to Algorithms. </title> <publisher> MIT Press. </publisher>
Reference-contexts: When the state space is of tractable size, the optimal value function for such a problem may be computed most efficiently by the algorithm Directed-Acyclic-Graph-Shortest-Paths (DAG-SP) <ref> (Cormen, Leiserson, & Rivest 1990) </ref>. We present ROUT, an active learning version of DAG-SP which applies to huge state spaces and approximated value functions.
Reference: <author> Watkins, C., and Dayan, P. </author> <year> 1992. </year> <note> Technical note: Q-Learning. Machine Learning 8(3/4). </note>
Reference-contexts: On the other hand, since the function being learned in the control case is a value function, it must satisfy the Bellman equation and other constraining properties (see e.g. <ref> (Watkins & Dayan 1992) </ref>). In this work we restrict ourselves to the class of acyclic stochastic Markov decision problems: those for which no legal trajectory can pass through the same state twice.
Reference: <author> Zhang, W., and Dietterich, T. G. </author> <year> 1995. </year> <title> A reinforcement learning approach to job-shop scheduling. </title> <booktitle> In Proceedings of IJCAI-95. </booktitle>
Reference-contexts: In this work we restrict ourselves to the class of acyclic stochastic Markov decision problems: those for which no legal trajectory can pass through the same state twice. Many domains naturally have this property (e.g. games like tic-tac-toe and Connect-Four; the job-shop scheduling formulation of <ref> (Zhang & Diet-terich 1995) </ref>; any finite-horizon problem for which time is a component of the state). When the state space is of tractable size, the optimal value function for such a problem may be computed most efficiently by the algorithm Directed-Acyclic-Graph-Shortest-Paths (DAG-SP) (Cormen, Leiserson, & Rivest 1990).
References-found: 5

