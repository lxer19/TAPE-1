URL: http://vibes.cs.uiuc.edu/Publications/Papers/GridChapter.ps.gz
Refering-URL: http://vibes.cs.uiuc.edu/Publications/publications.htm
Root-URL: http://www.cs.uiuc.edu
Title: Chapter 13 Performance Analysis and Visualization 13.1 Introduction  
Abstract: Despite the ubiquity of scalable parallel systems, the increasing availability of high-performance clusters, and the emergence of the grid as a wide area meta-computing infrastructure, achieving substantial fractions of peak performance on these systems for a wide range of applications remains an elusive goal. Performance measurement techniques like those described in Chapter ?? provide the raw data needed to identify and correct performance problems. However, data alone is not insight | performance visualization, correlation, evaluation, and interaction tools must process raw performance data, correlate it with appropriate network and computation components, both hardware and software, and highlight performance problems in meaningful ways. This processing and correlation is a prerequisite for interactive exploration and optimization of grid applications and their support infrastructure. Moreover, it is the basis for creation of adaptive resource control systems that automatically adjust grid software execution parameters to maximize performance. As grid applications and their software systems become increasingly heterogeneous and dynamic, with programs developed using high-level programming models aggressively transformed by compilers, effective performance correlation and data presentation will become increasingly critical. This chapter builds on the discussion of instrumentation technology in Chapter ?? and compiler technology in Chapter ?? to illustrate how the performance of high-performance 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Marc Abrams, N. Doraswamy, and Anup Mathur. Chitra: </author> <title> Visual Analysis of Parallel and Distributed Programs in the Time, Event, and Frequency Domain. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(6) </volume> <pages> 672-685, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: As one of the first of a new generation of intelligent performance tools, Paradyn presents performance data in an application context. It also moves the field closer to the goal of automatic performance tuning during wide-area application execution. Chitra Chitra <ref> [1] </ref> was developed to support both the analysis of performance data and the construction of models of program execution.
Reference: [2] <author> Marc Abrams, T. Lee, H. Cadiz, and K. Ganugapati. </author> <title> Beyond Software Performance Visualization. </title> <journal> Concurrency Practice and Experience, </journal> <volume> 7(8) </volume> <pages> 737-764, </pages> <month> Dec </month> <year> 1995. </year>
Reference-contexts: clustering [41] and principal component analysis to 1 Emerging systems like Paradyn [35] and Autopilot [45] replace a posteriori analysis with dynamic activation of performance instrumentation during application execution. 6 Performance Analysis and Visualization identify the most important metrics and to reduce data volume via the identification of equivalence classes <ref> [2] </ref>, and * performance data immersion that exploits haptics, spatial audio cues, and three-dimensional graphics to increase presentation and interaction modalities (e.g., Avatar's [48, 27, 44] time tunnels and Virtue's hierarchical graphs [45]).
Reference: [3] <author> Marc Abrams, Randy Ribler, and Anup Mathur. </author> <title> Two Performance Tool Design Issues and Chitra's Solutions. </title> <booktitle> In Proceedings of SPDT'96: SIGMET-RICS Symposium on Parallel and Distributed Tools, </booktitle> <pages> pages 98-107, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: The visualization of categorical data and its correlation with ordinal data is a relatively unexplored regime. Because categorical data lacks a total ordering, it cannot be directly mapped to the same visual attributes as ordinal data without introducing implied orders <ref> [3] </ref>.
Reference: [4] <author> Vikram S. Adve, John Mellor-Crummey, Mark Anderson, Ken Kennedy, Jhy-chun Wang, and Daniel A. Reed. </author> <title> Integrating Compilation and Performance Analysis for Data-Parallel Programs. </title> <editor> In Margaret L. Simmons, Ann H. Hayes, Jeffrey S. Brown, and Daniel A. Reed, editors, </editor> <booktitle> Proceedings of the Workshop on Debugging and Performance Tuning for Parallel Computing Systems. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <year> 1996. </year>
Reference-contexts: x13.6 identifies the key problems in visualization and analysis of heterogeneous, dynamic, and adaptive applications executing on the nascent computational grid. 13.2 Grid Performance Problems Optimizing the performance of parallel applications on homogeneous parallel systems remains a challenging research problem, with open questions related to analysis of data parallel codes <ref> [4] </ref>, characterization of input/output behavior [54], and management of memory locality. However, heterogeneous grid applications pose even more daunting performance problems. <p> As we shall see later in the chapter, successful performance analysis and visualization tools rely on a modest number of common techniques, including * correlation of performance metrics with application and library source code using data on compile-time program transformations (.e.g., as in svPablo <ref> [4] </ref> and Paradyn [23]), together with symbolic performance scalability prediction and artificial intelligence techniques to identify common perfor mance problems, * visualization of metric subsets using a standard set of dynamic graphics techniques (e.g., communication traffic matrices in Seecube [11], processor busy periods in ParaGraph [21], or dynamic profiles in Pablo <p> Although originally developed to support distributed memory parallel systems, portions of the Pablo infrastructure were later augmented to support analysis of data parallel programs. This effort, derived from a joint project between the University of Illinois and the Rice Center for Research on Parallel Computation, resulted in SvPablo <ref> [5, 4, 13] </ref>. As shown in Figure 13.7, SvPablo exploits data on compile-time program transformations to relate hardware and software performance data to application source code using simple color codes.
Reference: [5] <author> Vikram S. Adve, John Mellor-Crummey, Mark Anderson, Ken Kennedy, Jhy-chun Wang, and Daniel A. Reed. </author> <title> An Integrated Compilation and Performance Analysis Environment for Data Parallel Programs. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: In practice, this means * exploiting metadata to correlate and display performance data with source code, focusing developer attention on performance problems at a semantic level where they can effect change (e.g., HPF performance data correlation in svPablo <ref> [5, 13] </ref>), * correlating architectural, software, and performance metric representations of hardware and software behavior (e.g., by showing geographic displays of network traffic [39, 8] that can be correlated with communication metric and processor utilization displays and that can be related to the computation/communication ratios experienced by the execution of specific <p> As examples, both Paradyn [35] and SvPablo [13] concurrently display source code and derived performance metrics. Reflecting the tighter integration of performance measurement and compilation technology, these tools exploit knowledge of compile-time transformations <ref> [5] </ref> to relate dynamic performance data to source code constructs. <p> Although originally developed to support distributed memory parallel systems, portions of the Pablo infrastructure were later augmented to support analysis of data parallel programs. This effort, derived from a joint project between the University of Illinois and the Rice Center for Research on Parallel Computation, resulted in SvPablo <ref> [5, 4, 13] </ref>. As shown in Figure 13.7, SvPablo exploits data on compile-time program transformations to relate hardware and software performance data to application source code using simple color codes.
Reference: [6] <author> Edgar Anderson. </author> <title> A Semigraphical Method for the Analysis of Complex Problems. </title> <journal> Technometrics, </journal> <volume> 2(30) </volume> <pages> 387-391, </pages> <month> August </month> <year> 1960. </year>
Reference-contexts: Even when more detailed analysis is needed to uncover problems, the first step always consists of scanning a standard set of metrics and displays. Although a wide variety of univariate techniques have been proposed, and many displays have been built (e.g., glyphs <ref> [6] </ref>, leds, dot plots, stem and leaf plots, and box plots [10]), simple bargraphs and dials are among the most commonly used. These dynamic visualizations draw on common experience with physical measurement devices, can be read quickly, and readily highlight extrema.
Reference: [7] <author> Peter Beckman and Dennis Gannon. </author> <title> A Portable Run-Time System for Object-Parallel Systems. </title> <booktitle> In Proceedings, IEEE International Parallel Processing Symposium, 1996. </booktitle> <volume> 29 30 BIBLIOGRAPHY </volume>
Reference-contexts: Moreover, they may generate code for an execution model different from the programming model (e.g., translating HPF code to communicating MPI tasks or data parallel C ++ to interactions among CORBA <ref> [7] </ref> objects); see Chapter ?? for additional details on these translation problems. To be effective, both performance and debugging tools [34, 36, 26] must present data in the context of user-written source code.
Reference: [8] <author> Mun Choon Chan, Giovanni Pacifici, and Rolf Stadler. </author> <title> Managing Real-Time Services in Multimedia Networks Using Dynamic Visualization and High-Level Controls. </title> <booktitle> In Third ACM International Conference on Multimedia, </booktitle> <month> November </month> <year> 1995. </year>
Reference-contexts: performance data with source code, focusing developer attention on performance problems at a semantic level where they can effect change (e.g., HPF performance data correlation in svPablo [5, 13]), * correlating architectural, software, and performance metric representations of hardware and software behavior (e.g., by showing geographic displays of network traffic <ref> [39, 8] </ref> that can be correlated with communication metric and processor utilization displays and that can be related to the computation/communication ratios experienced by the execution of specific source code fragments), and * supporting interactive "drill down" of hierarchical software and hardware representations to explore successively more detailed performance data and <p> Several avenues are now being explored to meet these new challenges. Principal among them are the use of graph and geographic network representations <ref> [39, 8] </ref> for analysis of network traffic, immersive environments [48]) for richer data presentation and interaction, and automated and semi-automated performance analysis.
Reference: [9] <author> Christian Clemen~con, Akiyoshi Endo, Josef Fritscher, Roland Ruhl, and Brian J. N. Wylie. Annai: </author> <title> An Integrated Parallel Programming Environment for Multicomputers. </title> <editor> In Amr Zaky and Ted Lewis, editors, </editor> <booktitle> Tools and Environments for Parallel and Distributed Systems, </booktitle> <pages> pages 33-59. </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1996. </year>
Reference-contexts: Like Seecube, Hyperview's implementation was dependent on vendor-specific performance instrumentation and interfaces. ParaGraph [21] relaxed this restriction, creating a simple user interface and access to a large number of network topology-centric visualizations for distributed memory program optimization. More recently, Annai <ref> [9] </ref> integrated an extended High-Performance Fortran (HPF) compiler, a parallel debugger, performance instrumentation software, and ParaGraph visualization under a common user interface. Concurrently, Traceview [30] was developed as a general-purpose, portable tool for visualizing event traces.
Reference: [10] <author> William S. Cleveland and Marylin E. MiGill, </author> <title> editors. Dynamic Graphics for Statistics. </title> <publisher> Wadsworth & Brooks/Cole, </publisher> <year> 1988. </year>
Reference-contexts: The dynamic ranges of different types of ordinal data can vary multiple orders of magnitude, and the exact ranges are often unknown prior to measurement. Although dynamic data scaling is often used to compare metrics whose ranges differ, experiments in statistical graphics <ref> [10] </ref> have shown that the choice of scaling algorithms and visual attribute mapping strongly influences the ability of users to identify patterns and correlations. Hence, performance and debugging tool builders must choose mappings with care, emphasizing representations that are likely to highlight common problems. <p> Orthogonally, performance presentation techniques are also distinguished by the number of metrics they can simultaneously display and correlate with application or library software. At one extreme, simple univariate techniques (e.g., leds, dials, or meters) show a single value, whereas multivariate techniques like scatterplot matrices <ref> [10, 43] </ref> display the relations among a large number of 10 Performance Analysis and Visualization values. The relative utility of multiple univariate and single multivariate displays depends on the relative importance of the metrics. <p> Although a wide variety of univariate techniques have been proposed, and many displays have been built (e.g., glyphs [6], leds, dot plots, stem and leaf plots, and box plots <ref> [10] </ref>), simple bargraphs and dials are among the most commonly used. These dynamic visualizations draw on common experience with physical measurement devices, can be read quickly, and readily highlight extrema. When greater historical context is needed than the simple extrema provided by dials and bargraphs, strip charts are often used. <p> In the figure, the variates are plotted in the same order on both the x and y axes. Hence, the scatterplots on the diagonal each plot a variate against itself. The scatterplots in the main diagonal have been replaced with box and whisker plots <ref> [10] </ref> that show attributes of the bivariate data distribution. Finally, by symmetry, the scatterplots on either side of the main diagonal contain redundant information, but are often plotted to simplify visual analysis. Generalization to three-dimensions leads to scattercube matrices that display all trivariate projections.
Reference: [11] <author> Alva Couch. </author> <title> Graphical Representations of Program Performance on Hypercube Message-Passing Multiprocessors. </title> <type> PhD thesis, </type> <institution> Tufts University, Department of Computer Science, </institution> <year> 1988. </year>
Reference-contexts: code using data on compile-time program transformations (.e.g., as in svPablo [4] and Paradyn [23]), together with symbolic performance scalability prediction and artificial intelligence techniques to identify common perfor mance problems, * visualization of metric subsets using a standard set of dynamic graphics techniques (e.g., communication traffic matrices in Seecube <ref> [11] </ref>, processor busy periods in ParaGraph [21], or dynamic profiles in Pablo [43]), * dynamic statistical clustering [41] and principal component analysis to 1 Emerging systems like Paradyn [35] and Autopilot [45] replace a posteriori analysis with dynamic activation of performance instrumentation during application execution. 6 Performance Analysis and Visualization identify <p> Architectural Mappings Architectural mappings are a local variant of geographic mappings; they project performance information onto an abstraction of a hardware or software architecture. Early performance displays of hypercube communication traffic <ref> [11, 21, 32] </ref> were among the most widely used architectural displays. <p> Current Performance Analysis Tools 19 new generation of parallel performance tools. 4 Of these, Seecube <ref> [11] </ref> was one of the first to exploit inexpensive workstation graphics for parallel performance animation. As the name suggests, Seecube displayed multiple, dynamic representations of the hypercube topology and allowed users to choose several color coded mappings of metrics to hypercube animation.
Reference: [12] <author> C. Cruz-Neira, D.J.Sandin, and T.A. DeFanti. </author> <title> Surround-Screen Projection-Based Virtual Reality: </title> <booktitle> The Design and Implementation of the CAVE. In SIGGRAPH '93 Proceedings. Association for Computing Machinery, </booktitle> <month> August </month> <year> 1993. </year>
Reference-contexts: As in Paradyn, an interactive query interface allows users to "drill down" from high-level performance summaries to more detailed, quantitative data on specific metrics and processors. Avatar Avatar is a virtual reality framework for analyzing complex, time varying, multivariate data [48]. Using the CAVE <ref> [12] </ref>, a head-mounted display, or a workstation and stereo glasses, Avatar immerses the data analyst in a virtual world of performance data. Avatar supports three domain-independent display metaphors: scattercubes, the three-dimensional generalization of scatterplot matrices described in x13.4.2, a geographic display, and a "time tunnel," all with data sonification.
Reference: [13] <author> Luiz A. DeRose, Ying Zhang, Ruth A. Aydt, and Daniel A. Reed. svPablo: </author> <title> HPF Performance Tuning and Visualization. In HPF Users Group Meeting, </title> <month> February </month> <year> 1997. </year>
Reference-contexts: In practice, this means * exploiting metadata to correlate and display performance data with source code, focusing developer attention on performance problems at a semantic level where they can effect change (e.g., HPF performance data correlation in svPablo <ref> [5, 13] </ref>), * correlating architectural, software, and performance metric representations of hardware and software behavior (e.g., by showing geographic displays of network traffic [39, 8] that can be correlated with communication metric and processor utilization displays and that can be related to the computation/communication ratios experienced by the execution of specific <p> Others reached the same conclusion, engendering a third generation of performance analysis and visualization tools that focused on semantically rich displays and tight coupling to application programming models. As examples, both Paradyn [35] and SvPablo <ref> [13] </ref> concurrently display source code and derived performance metrics. Reflecting the tighter integration of performance measurement and compilation technology, these tools exploit knowledge of compile-time transformations [5] to relate dynamic performance data to source code constructs. <p> Although originally developed to support distributed memory parallel systems, portions of the Pablo infrastructure were later augmented to support analysis of data parallel programs. This effort, derived from a joint project between the University of Illinois and the Rice Center for Research on Parallel Computation, resulted in SvPablo <ref> [5, 4, 13] </ref>. As shown in Figure 13.7, SvPablo exploits data on compile-time program transformations to relate hardware and software performance data to application source code using simple color codes.
Reference: [14] <author> Steven G. Eick, M. C. Nelson, and J. D. Schmidt. </author> <title> Graphical Analysis of Computer Log Files. </title> <journal> Communications of the ACM, </journal> <volume> 37(12) </volume> <pages> 50-56, </pages> <month> December </month> <year> 1994. </year>
Reference-contexts: However, several techniques can be used to visualize categorical data and its relations to ordinal data, including * using symbols or colors to identify individual categorical values (e.g., colors to denote software module or network states by Eick et al <ref> [14] </ref> or two and three-dimensional graphs to show task state transitions as in Paragraph [21] and Avatar [44]), * arbitrarily mapping categorical values to numerical values and then ex ploiting ordinal visualization techniques, * the indirect visualization of categorical data through the visualization of ordinal relationships within the data set.
Reference: [15] <author> S. E. Fienberg. </author> <title> Graphical Methods in Statistics. </title> <booktitle> American Statisticians, </booktitle> <pages> pages 165-178, </pages> <year> 1979. </year>
Reference-contexts: In practice, a small number of behaviors dominate, and the data from most processors lie in one of the associated clusters. Often, understanding the reasons for behavioral outliers is the key to improving performance. Kiviat Diagrams Kiviat diagrams <ref> [15] </ref> attempt to succinctly capture multi-variate correlations. For N variables, a Kiviat diagram is formed by N line seg 12 Performance Analysis and Visualization ments that emanate from a single point in the center of the graph.
Reference: [16] <author> Ian Foster and Carl Kesselman. Globus: </author> <title> A Metacomputing Infrasturcture Toolkit. </title> <journal> International Journal of Supercomputing Applications, </journal> <note> to appear. </note>
Reference-contexts: This view of adaptive runtime systems is buttressed by recent experiences with an infrastructure for flexible input/output management [29, 28, 45] and by adaptive runtime systems for wide area computing <ref> [16, 50] </ref>.
Reference: [17] <author> Nahum Gershon and Stephen G. Eick. </author> <title> Information Visualization. </title> <journal> IEEE Computer Graphics and Applications, </journal> <pages> pages 29-31, </pages> <month> July/August </month> <year> 1997. </year>
Reference-contexts: In contrast, computer system performance data lacks natural physical referents | mapping cache hit ratios to a microprocessor photomicrograph is unlikely to suggest hardware or software changes that would alleviate a performance bottleneck. Hence, performance visualization and analysis are more akin to the nascent field of information visualization <ref> [17] </ref> than to scientific visualization.
Reference: [18] <author> S.L. Graham, P.B. Kessler, </author> <title> and M.K. McKusick. gprof: A Call Graph Execution Profiler. </title> <booktitle> In Proceedings of the SIGPLAN '82 Symposium on Compiler Construction, </booktitle> <pages> pages 120-126, </pages> <address> Boston, MA, </address> <month> June </month> <year> 1982. </year> <title> Association for Computing Machinery. BIBLIOGRAPHY 31 </title>
Reference-contexts: Static techniques provides a single, unchanging view of grid application performance; a program execution profile <ref> [18] </ref>, shown as a table or pie chart, is the most common example of a static display. In contrast, dynamic techniques update the performance presentation to reflect changing values of the associated variables (e.g., as in a processor load average or network latency stripchart).
Reference: [19] <author> Steven T. Hackstadt and Allen D. Malony. </author> <title> Visualizing Parallel Programs and Performance. </title> <journal> IEEE Computer Graphics Applications, </journal> <volume> 15(4) </volume> <pages> 12-14, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: In the figure, the light octagon denotes the maximum value encountered on the eight processors, while the darker section shows the current utilization. Although Kiviat diagrams can be animated to show current conditions and maxima, they cannot show comprehensive history. The Kiviat tube <ref> [19] </ref> was developed to provide this history. Figure 13.3b shows a Kiviat tube formed by juxtaposing a sequence of Kiviat diagrams along a third, time axis. To reduce clutter and emphasize recent behavior, older diagrams are rendered in gray scale. 13.4.
Reference: [20] <editor> Michael T. Heath, editor. </editor> <booktitle> Hypercube Multiprocessors 1986. Society of Industrial and Applied Mathematics, 1986. Proceedings for the First Conference on Hypercube Multiprocessors. </booktitle>
Reference-contexts: bears many similarities to the 1980's emergence of performance tools for distributed memory parallel systems, it is instructive to examine the lessons gleaned from this evolutionary path. 13.5.1 Performance Analysis History The explosion of interest in parallel computing in the 1980's and the appearance of highly parallel distributed memory systems <ref> [20] </ref> stimulated development of a 3 We will return to this topic in x13.6. 13.5. Current Performance Analysis Tools 19 new generation of parallel performance tools. 4 Of these, Seecube [11] was one of the first to exploit inexpensive workstation graphics for parallel performance animation.
Reference: [21] <author> Michael T. Heath and Jennifer A. Etheridge. </author> <title> Visualizing the Performance of Parallel Programs. </title> <journal> IEEE Software, </journal> <pages> pages 29-39, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: transformations (.e.g., as in svPablo [4] and Paradyn [23]), together with symbolic performance scalability prediction and artificial intelligence techniques to identify common perfor mance problems, * visualization of metric subsets using a standard set of dynamic graphics techniques (e.g., communication traffic matrices in Seecube [11], processor busy periods in ParaGraph <ref> [21] </ref>, or dynamic profiles in Pablo [43]), * dynamic statistical clustering [41] and principal component analysis to 1 Emerging systems like Paradyn [35] and Autopilot [45] replace a posteriori analysis with dynamic activation of performance instrumentation during application execution. 6 Performance Analysis and Visualization identify the most important metrics and to <p> be used to visualize categorical data and its relations to ordinal data, including * using symbols or colors to identify individual categorical values (e.g., colors to denote software module or network states by Eick et al [14] or two and three-dimensional graphs to show task state transitions as in Paragraph <ref> [21] </ref> and Avatar [44]), * arbitrarily mapping categorical values to numerical values and then ex ploiting ordinal visualization techniques, * the indirect visualization of categorical data through the visualization of ordinal relationships within the data set. Such relationships include pe 13.4. <p> Architectural Mappings Architectural mappings are a local variant of geographic mappings; they project performance information onto an abstraction of a hardware or software architecture. Early performance displays of hypercube communication traffic <ref> [11, 21, 32] </ref> were among the most widely used architectural displays. <p> It was complemented by operating system and compiler instrumentation and a hardware data capture subsystem [31]. In addition, it supported a hierarchical performance browser that showed causal relations among message transmission and receipt events [47]. Like Seecube, Hyperview's implementation was dependent on vendor-specific performance instrumentation and interfaces. ParaGraph <ref> [21] </ref> relaxed this restriction, creating a simple user interface and access to a large number of network topology-centric visualizations for distributed memory program optimization. More recently, Annai [9] integrated an extended High-Performance Fortran (HPF) compiler, a parallel debugger, performance instrumentation software, and ParaGraph visualization under a common user interface. <p> ParaGraph ParaGraph, originally developed by at Oak Ridge National Laboratory and later extended at the University of Illinois <ref> [21] </ref>, uses execution traces produced by the Portable Instrumented Communication Library (PICL) to visualize processor utilization, interprocessor communication, and general task information. ParaGraph exemplifies the early class of trace-based dynamic visual 13.5.
Reference: [22] <author> HPFF. </author> <title> High-Performance Fortran Language Specfication, version 1.1. </title> <type> Technical report, </type> <institution> High Performance Fortran Forum, </institution> <month> November </month> <year> 1994. </year>
Reference-contexts: Now suppose the portion of the grid application code on the SP-2 is based on MPI, while the portion of the code on the Origin is written in High-Performance Fortran (HPF) <ref> [22] </ref>. On the Origin, the Nexus/Globus runtime system manages HPF threads, and on both systems, the NCSA Hierarchical Data Format (HDF) [40] library provides high-level access to data via an implementation atop MPI-IO. In this context, now consider the following performance analysis scenario. <p> For example, compilers for sequential languages now transform loops to increase cache locality and maximize the utilization of superscalar functional units. More perniciously, aggressive compiler transformations of data parallel C ++ [24] and High-Performance Fortran (HPF) <ref> [22] </ref> often result in execution behavior that can differ markedly from what application developers anticipated.
Reference: [23] <author> R. Bruce Irvin and Barton P. Miller. </author> <title> Mapping Performance Data for High-Level and Data Views of Parallel Program Performance. </title> <booktitle> In International Conference on Supercomputing, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: As we shall see later in the chapter, successful performance analysis and visualization tools rely on a modest number of common techniques, including * correlation of performance metrics with application and library source code using data on compile-time program transformations (.e.g., as in svPablo [4] and Paradyn <ref> [23] </ref>), together with symbolic performance scalability prediction and artificial intelligence techniques to identify common perfor mance problems, * visualization of metric subsets using a standard set of dynamic graphics techniques (e.g., communication traffic matrices in Seecube [11], processor busy periods in ParaGraph [21], or dynamic profiles in Pablo [43]), * dynamic
Reference: [24] <author> Elizabeth Johnson, Peter Beckman, and Dennis Gannon. </author> <title> Portable Parallel Programming in HPC++. </title> <booktitle> In Proceedings, ICPP International Workshop on Challenges for Parallel Processing, </booktitle> <year> 1996. </year>
Reference-contexts: For example, compilers for sequential languages now transform loops to increase cache locality and maximize the utilization of superscalar functional units. More perniciously, aggressive compiler transformations of data parallel C ++ <ref> [24] </ref> and High-Performance Fortran (HPF) [22] often result in execution behavior that can differ markedly from what application developers anticipated.
Reference: [25] <author> Doug Kimelman, Bryan Rosenburg, and Tova Roth. </author> <title> Strata-Various :-) Multi-Layer Visualization of Dynamics in Software System Behavior. </title> <booktitle> In Visualization '94, </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: In particular, task, process, thread, and procedure call graphs all illustrate the dynamic execution of individual software components by coloring or animating graphs of software structure. As an example, Figure 13.5 shows a snapshot of the process graph display from the IBM Program Visualizer (PV) <ref> [25] </ref>. The PV image in the center of the figure is a process hierarchy display. Each process is colored based on its recent execution activity A strip chart below the process graph shows the corresponding history of process activations by the process scheduler.
Reference: [26] <author> Krishna Kunchithapadam and Barton P. Miller. </author> <title> Integrating a Debugger and a Performance Tool for Steering. </title> <editor> In M. L Simmons, A. H. Hayes, J. S. Brown, and D. A. Reed, editors, </editor> <booktitle> Debugging and Performance Tools for Parallel Computing Systems, </booktitle> <pages> pages 53-64. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1996. </year>
Reference-contexts: To be effective, both performance and debugging tools <ref> [34, 36, 26] </ref> must present data in the context of user-written source code. Conversely, performance instrumentation and debugging runtime systems must monitor the dynamic behavior of the executing code, even if in a different execution model than the application source code.
Reference: [27] <author> Stephen Lamm, Daniel A. Reed, and Will H. Scullin. </author> <title> Real-time Geographic Visualization of World Wide Web Traffic. </title> <booktitle> In Proceedings of the Fifth International World Wide Web Conference, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: of performance instrumentation during application execution. 6 Performance Analysis and Visualization identify the most important metrics and to reduce data volume via the identification of equivalence classes [2], and * performance data immersion that exploits haptics, spatial audio cues, and three-dimensional graphics to increase presentation and interaction modalities (e.g., Avatar's <ref> [48, 27, 44] </ref> time tunnels and Virtue's hierarchical graphs [45]). As parallel systems evolve from homogeneous collections of processors to include the heterogeneous, distributed, and parallel systems that compose the computational grid, both the dimensionality of the performance metric space and the complexity of data correlation and analysis increase dramatically. <p> Similar geographic network visualization tools are available from a variety of public and commercial sources (e.g., Tivoli). Finally, some network performance visualization tools are now incorporating virtual reality technologies to provide immersive geographic visualizations. For example, Avatar <ref> [27] </ref> displays a three-dimensional globe that appears to float in space before the analyst. The analyst can rotate the globe, view traffic on a worldwide basis or select individual network nodes to obtain detailed information.
Reference: [28] <author> Tara M. Madhyastha, Christopher L. Elford, and Daniel A. Reed. </author> <title> Optimizing Input/Output Using Adaptive File System Policies. </title> <booktitle> In Proceedings of the Fifth Goddard Conference on Mass Storage Systems and Technologies, </booktitle> <month> September </month> <year> 1996. </year>
Reference-contexts: This view of adaptive runtime systems is buttressed by recent experiences with an infrastructure for flexible input/output management <ref> [29, 28, 45] </ref> and by adaptive runtime systems for wide area computing [16, 50].
Reference: [29] <author> Tara M. Madhyastha and Daniel A. Reed. </author> <title> Intelligent, Adaptive File System Policy Selection. </title> <booktitle> In Proceedings of Frontiers '96, </booktitle> <month> October </month> <year> 1996. </year> <note> 32 BIBLIOGRAPHY </note>
Reference-contexts: This view of adaptive runtime systems is buttressed by recent experiences with an infrastructure for flexible input/output management <ref> [29, 28, 45] </ref> and by adaptive runtime systems for wide area computing [16, 50].
Reference: [30] <author> Allen D. Malony, David H. Hammerslag, and David J.Jablonowski. Trace-view: </author> <title> A Trace Visualization Tool. </title> <journal> IEEE Software, </journal> <volume> 8(5) </volume> <pages> 19-28, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: More recently, Annai [9] integrated an extended High-Performance Fortran (HPF) compiler, a parallel debugger, performance instrumentation software, and ParaGraph visualization under a common user interface. Concurrently, Traceview <ref> [30] </ref> was developed as a general-purpose, portable tool for visualizing event traces. Traceview isolated the analysis and visualization interface from the idiosyncrasies of machine architecture or performance event semantics by extracting all information from a configurable event trace file.
Reference: [31] <author> Allen D. Malony and Daniel A. Reed. </author> <title> A Hardware-Based Performance Monitor for the Intel iPSC/2 Hypercube. </title> <booktitle> In 1990 ACM International Conference on Supercomputing, </booktitle> <pages> pages 213-216. </pages> <institution> Association for Computing Machinery, </institution> <month> June </month> <year> 1990. </year>
Reference-contexts: Hyperview [32, 33] generalized the mapping of distributed memory performance metrics to visualizations, supporting configurable display of any member of the cross-product of a set of performance metrics and univariate and multivariate visualization techniques. It was complemented by operating system and compiler instrumentation and a hardware data capture subsystem <ref> [31] </ref>. In addition, it supported a hierarchical performance browser that showed causal relations among message transmission and receipt events [47]. Like Seecube, Hyperview's implementation was dependent on vendor-specific performance instrumentation and interfaces.
Reference: [32] <author> Allen D. Malony, Daniel A. Reed, James W. Arendt, Ruth A. Aydt, Do-minique Grabas, and Brian K. Totty. </author> <title> An Integrated Performance Data Collection Analysis, and Visualization System. </title> <booktitle> In Proceedings of the Fourth Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 229-236, </pages> <address> Monterey, CA, </address> <month> March </month> <year> 1989. </year> <institution> Association for Computing Machinery. </institution>
Reference-contexts: Architectural Mappings Architectural mappings are a local variant of geographic mappings; they project performance information onto an abstraction of a hardware or software architecture. Early performance displays of hypercube communication traffic <ref> [11, 21, 32] </ref> were among the most widely used architectural displays. <p> As such, Seecube proved to be very useful, but it was tightly integrated with instrumentation from a particular research operating system (SIMPLEX) and a specific architecture, hypercube. Tantalized by possibilities suggested by Seecube, several other groups quickly began building performance visualization tools. Hyperview <ref> [32, 33] </ref> generalized the mapping of distributed memory performance metrics to visualizations, supporting configurable display of any member of the cross-product of a set of performance metrics and univariate and multivariate visualization techniques. It was complemented by operating system and compiler instrumentation and a hardware data capture subsystem [31].
Reference: [33] <author> Allen D. Malony, Daniel A. Reed, and David C. Rudolph. </author> <title> Integrating Performance Data Collection, Analysis, and Visualization. </title> <editor> In Margaret Simmons, Rebecca Koskela, and Ingrid Bucher, editors, </editor> <booktitle> Parallel Computer Systems: Performance Instrumentation and Visualization, </booktitle> <pages> pages 73-97. </pages> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1990. </year>
Reference-contexts: As such, Seecube proved to be very useful, but it was tightly integrated with instrumentation from a particular research operating system (SIMPLEX) and a specific architecture, hypercube. Tantalized by possibilities suggested by Seecube, several other groups quickly began building performance visualization tools. Hyperview <ref> [32, 33] </ref> generalized the mapping of distributed memory performance metrics to visualizations, supporting configurable display of any member of the cross-product of a set of performance metrics and univariate and multivariate visualization techniques. It was complemented by operating system and compiler instrumentation and a hardware data capture subsystem [31].
Reference: [34] <author> Charles E. McDowell and David P. Helmbold. </author> <title> Debugging Concurrent Programs. </title> <journal> ACM Computing Surveys, </journal> <volume> 24(4), </volume> <month> December </month> <year> 1989. </year>
Reference-contexts: To be effective, both performance and debugging tools <ref> [34, 36, 26] </ref> must present data in the context of user-written source code. Conversely, performance instrumentation and debugging runtime systems must monitor the dynamic behavior of the executing code, even if in a different execution model than the application source code.
Reference: [35] <author> Barton P. Miller, Mark D. Callaghan, Jonathan M. Cargille, Jeffrey K. Hollingsworth, R. Bruce Irwin, Karen L. Karavanic, Krishna Kunchitka-padam, and Tia Newhall. </author> <title> The Paradyn Parallel Performance Measurement Tools. </title> <journal> IEEE Computer, </journal> <volume> 28(11), </volume> <month> November </month> <year> 1995. </year>
Reference-contexts: perfor mance problems, * visualization of metric subsets using a standard set of dynamic graphics techniques (e.g., communication traffic matrices in Seecube [11], processor busy periods in ParaGraph [21], or dynamic profiles in Pablo [43]), * dynamic statistical clustering [41] and principal component analysis to 1 Emerging systems like Paradyn <ref> [35] </ref> and Autopilot [45] replace a posteriori analysis with dynamic activation of performance instrumentation during application execution. 6 Performance Analysis and Visualization identify the most important metrics and to reduce data volume via the identification of equivalence classes [2], and * performance data immersion that exploits haptics, spatial audio cues, and <p> Others reached the same conclusion, engendering a third generation of performance analysis and visualization tools that focused on semantically rich displays and tight coupling to application programming models. As examples, both Paradyn <ref> [35] </ref> and SvPablo [13] concurrently display source code and derived performance metrics. Reflecting the tighter integration of performance measurement and compilation technology, these tools exploit knowledge of compile-time transformations [5] to relate dynamic performance data to source code constructs. <p> Principal among them are the use of graph and geographic network representations [39, 8] for analysis of network traffic, immersive environments [48]) for richer data presentation and interaction, and automated and semi-automated performance analysis. The latter, exemplified by the Paradyn <ref> [35] </ref> and Autopilot projects [50], relies on real-time performance measurement for interactive performance debugging and dynamic optimization. 13.5.2 Representative Performance Tool Examples During the past fifteen years, a plethora of performance analysis and visualization tools for parallel and distributed systems have appeared. <p> The hierarchy is represented as a tree, where the root is the program, and branches represent machines and processes. Paradyn <ref> [35] </ref>, a direct descendant of IPS-2, builds on the IPS-2 exploratory model of bottleneck identification by supporting real-time insertion and removal of measurement probes during application execution. <p> This approach would allow interactive steering of applications (see Chapter ?? for an explanation of computational steering) and their behavior based on real-time data streams and dynamic visual representations (e.g., using dynamic graphics <ref> [35] </ref> or virtual environments [45]). Moreover, it would enable migration from a model of static resource optimization based on a posteriori measurements to closed loop adaptive control using real-time data streams.
Reference: [36] <author> Barton P. Miller and Jong-Doek Choi. </author> <title> Breakpoints and Halting in Distributed Programs. </title> <booktitle> In Proceedings of the Eighth International Conference on Distributed Computing Systems, </booktitle> <pages> pages 316-323, </pages> <year> 1988. </year>
Reference-contexts: To be effective, both performance and debugging tools <ref> [34, 36, 26] </ref> must present data in the context of user-written source code. Conversely, performance instrumentation and debugging runtime systems must monitor the dynamic behavior of the executing code, even if in a different execution model than the application source code.
Reference: [37] <author> Barton P. Miller, M. Clark, Jeffrey Hollingsworth, S. Kierstead, S. Lim, and T. Torzewski. IPS-2: </author> <title> The Second Generation of a Parallel Program Measurement System. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(2) </volume> <pages> 206-217, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: In the wide-area grid computing model, these assumptions no longer hold. This realization has driven development of a new generation of tools. IPS-2 and Paradyn IPS-2, a performance instrumentation and visualization tool developed at the University of Wisconsin <ref> [38, 37] </ref>, relies on a hierarchical approach, allowing users to describe and collect performance information on both parallel and distributed applications. The hierarchy is represented as a tree, where the root is the program, and branches represent machines and processes.
Reference: [38] <author> Barton P. Miller and C. Q. Yang. IPS: </author> <title> An Interactive and Automatic Performance Measurement Tool for Parallel and Distributed Programs. </title> <booktitle> In Proceedings of the 7th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 482-489, </pages> <month> September </month> <year> 1987. </year> <note> BIBLIOGRAPHY 33 </note>
Reference-contexts: In the wide-area grid computing model, these assumptions no longer hold. This realization has driven development of a new generation of tools. IPS-2 and Paradyn IPS-2, a performance instrumentation and visualization tool developed at the University of Wisconsin <ref> [38, 37] </ref>, relies on a hierarchical approach, allowing users to describe and collect performance information on both parallel and distributed applications. The hierarchy is represented as a tree, where the root is the program, and branches represent machines and processes.
Reference: [39] <author> Tamara Munzner, Eric Hoffman, K. Claffy, and Bill Fenner. </author> <title> Visualizing the Global Topology of the MBone. </title> <booktitle> In Proceedings of the 1996 IEEE Symposium on Information Visualization, </booktitle> <pages> pages 85-92, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: performance data with source code, focusing developer attention on performance problems at a semantic level where they can effect change (e.g., HPF performance data correlation in svPablo [5, 13]), * correlating architectural, software, and performance metric representations of hardware and software behavior (e.g., by showing geographic displays of network traffic <ref> [39, 8] </ref> that can be correlated with communication metric and processor utilization displays and that can be related to the computation/communication ratios experienced by the execution of specific source code fragments), and * supporting interactive "drill down" of hierarchical software and hardware representations to explore successively more detailed performance data and <p> Several avenues are now being explored to meet these new challenges. Principal among them are the use of graph and geographic network representations <ref> [39, 8] </ref> for analysis of network traffic, immersive environments [48]) for richer data presentation and interaction, and automated and semi-automated performance analysis.
Reference: [40] <author> NCSA. </author> <title> NCSA HDF User's Guide, Version 4.1. </title> <booktitle> National Center for Supercomputing Applications, </booktitle> <month> May </month> <year> 1997. </year>
Reference-contexts: On the Origin, the Nexus/Globus runtime system manages HPF threads, and on both systems, the NCSA Hierarchical Data Format (HDF) <ref> [40] </ref> library provides high-level access to data via an implementation atop MPI-IO. In this context, now consider the following performance analysis scenario. The Origin issues an HDF input/output request to a remote data archive.
Reference: [41] <author> Oleg Y. Nickolayev, Philip C. Roth, and Daniel A. Reed. </author> <title> Real-time Statistical Clustering for Event Trace Reduction. </title> <journal> International Journal of Supercomputing Applications, </journal> <volume> 11(2), </volume> <year> 1997. </year>
Reference-contexts: symbolic performance scalability prediction and artificial intelligence techniques to identify common perfor mance problems, * visualization of metric subsets using a standard set of dynamic graphics techniques (e.g., communication traffic matrices in Seecube [11], processor busy periods in ParaGraph [21], or dynamic profiles in Pablo [43]), * dynamic statistical clustering <ref> [41] </ref> and principal component analysis to 1 Emerging systems like Paradyn [35] and Autopilot [45] replace a posteriori analysis with dynamic activation of performance instrumentation during application execution. 6 Performance Analysis and Visualization identify the most important metrics and to reduce data volume via the identification of equivalence classes [2], and
Reference: [42] <author> Daniel A. Reed. </author> <title> Experimental Performance Analysis of Parallel Systems: Techniques and Open Problems. </title> <booktitle> In Proceedings of the 7th International Conference on Modelling Techniques and Tools for Computer Performance Evaluation, </booktitle> <pages> pages 25-51, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: Chitra provides several visualizations designed specifically to analyzing categorical data [49]. These techniques support the visualization of event periodicity, event correlation, stationarity, and pattern recurrence. 22 Performance Analysis and Visualization Pablo and SvPablo Pablo is a portable, extensible performance instrumentation and analysis toolkit developed at the University of Illinois <ref> [43, 42] </ref>. The design focus of Pablo was on performance analysis extensibility via a coarse-grained graphical data flow programming model for assembling performance analyses. Although originally developed to support distributed memory parallel systems, portions of the Pablo infrastructure were later augmented to support analysis of data parallel programs.
Reference: [43] <author> Daniel A. Reed, Ruth A. Aydt, Roger J. Noe, Phillip C. Roth, Keith A. Shields, Bradley W. Schwartz, and Luis F. Tavera. </author> <title> Scalable Performance Analysis: The Pablo Performance Analysis Environment. </title> <editor> In Anthony Skjel-lum, editor, </editor> <booktitle> Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <pages> pages 104-113. </pages> <publisher> IEEE Computer Society, </publisher> <year> 1993. </year>
Reference-contexts: and Paradyn [23]), together with symbolic performance scalability prediction and artificial intelligence techniques to identify common perfor mance problems, * visualization of metric subsets using a standard set of dynamic graphics techniques (e.g., communication traffic matrices in Seecube [11], processor busy periods in ParaGraph [21], or dynamic profiles in Pablo <ref> [43] </ref>), * dynamic statistical clustering [41] and principal component analysis to 1 Emerging systems like Paradyn [35] and Autopilot [45] replace a posteriori analysis with dynamic activation of performance instrumentation during application execution. 6 Performance Analysis and Visualization identify the most important metrics and to reduce data volume via the identification <p> Orthogonally, performance presentation techniques are also distinguished by the number of metrics they can simultaneously display and correlate with application or library software. At one extreme, simple univariate techniques (e.g., leds, dials, or meters) show a single value, whereas multivariate techniques like scatterplot matrices <ref> [10, 43] </ref> display the relations among a large number of 10 Performance Analysis and Visualization values. The relative utility of multiple univariate and single multivariate displays depends on the relative importance of the metrics. <p> This isolation allowed Traceview to concurrently support multiple visualizations from different segments of the event trace. In response to repeated user requests for greater flexibility, adaptability, and extensibility, the Pablo toolkit <ref> [43, 46] </ref> completely decoupled performance data semantics from its structural representation via a performance data metaformat. In turn, this decoupling enabled construction of a coarse-grained data flow model for graphical programming of performance analyses and visualizations. <p> Chitra provides several visualizations designed specifically to analyzing categorical data [49]. These techniques support the visualization of event periodicity, event correlation, stationarity, and pattern recurrence. 22 Performance Analysis and Visualization Pablo and SvPablo Pablo is a portable, extensible performance instrumentation and analysis toolkit developed at the University of Illinois <ref> [43, 42] </ref>. The design focus of Pablo was on performance analysis extensibility via a coarse-grained graphical data flow programming model for assembling performance analyses. Although originally developed to support distributed memory parallel systems, portions of the Pablo infrastructure were later augmented to support analysis of data parallel programs.
Reference: [44] <author> Daniel A. Reed, Christopher L. Elford, Tara Madhyastha, Will H. Scullin, Ruth A. Aydt, and Evgenia Smirni. </author> <title> I/O, Performance Analysis, and Performance Data Immersion. </title> <booktitle> In MASCOTS '96, </booktitle> <pages> pages 1-12, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: of performance instrumentation during application execution. 6 Performance Analysis and Visualization identify the most important metrics and to reduce data volume via the identification of equivalence classes [2], and * performance data immersion that exploits haptics, spatial audio cues, and three-dimensional graphics to increase presentation and interaction modalities (e.g., Avatar's <ref> [48, 27, 44] </ref> time tunnels and Virtue's hierarchical graphs [45]). As parallel systems evolve from homogeneous collections of processors to include the heterogeneous, distributed, and parallel systems that compose the computational grid, both the dimensionality of the performance metric space and the complexity of data correlation and analysis increase dramatically. <p> visualize categorical data and its relations to ordinal data, including * using symbols or colors to identify individual categorical values (e.g., colors to denote software module or network states by Eick et al [14] or two and three-dimensional graphs to show task state transitions as in Paragraph [21] and Avatar <ref> [44] </ref>), * arbitrarily mapping categorical values to numerical values and then ex ploiting ordinal visualization techniques, * the indirect visualization of categorical data through the visualization of ordinal relationships within the data set. Such relationships include pe 13.4.
Reference: [45] <author> Daniel A. Reed, Christopher L. Elford, Tara Madhyastha, Evgenia Smirni, and Stephen L. </author> <title> Lamm. The Next Frontier: Interactive and Closed Loop Performance Steering. </title> <booktitle> In Proceedings of the 1996 International Conference on Parallel Processing Workshop, </booktitle> <pages> pages 20-31, </pages> <month> August </month> <year> 1996. </year>
Reference-contexts: * visualization of metric subsets using a standard set of dynamic graphics techniques (e.g., communication traffic matrices in Seecube [11], processor busy periods in ParaGraph [21], or dynamic profiles in Pablo [43]), * dynamic statistical clustering [41] and principal component analysis to 1 Emerging systems like Paradyn [35] and Autopilot <ref> [45] </ref> replace a posteriori analysis with dynamic activation of performance instrumentation during application execution. 6 Performance Analysis and Visualization identify the most important metrics and to reduce data volume via the identification of equivalence classes [2], and * performance data immersion that exploits haptics, spatial audio cues, and three-dimensional graphics to <p> and Visualization identify the most important metrics and to reduce data volume via the identification of equivalence classes [2], and * performance data immersion that exploits haptics, spatial audio cues, and three-dimensional graphics to increase presentation and interaction modalities (e.g., Avatar's [48, 27, 44] time tunnels and Virtue's hierarchical graphs <ref> [45] </ref>). As parallel systems evolve from homogeneous collections of processors to include the heterogeneous, distributed, and parallel systems that compose the computational grid, both the dimensionality of the performance metric space and the complexity of data correlation and analysis increase dramatically. <p> This approach would allow interactive steering of applications (see Chapter ?? for an explanation of computational steering) and their behavior based on real-time data streams and dynamic visual representations (e.g., using dynamic graphics [35] or virtual environments <ref> [45] </ref>). Moreover, it would enable migration from a model of static resource optimization based on a posteriori measurements to closed loop adaptive control using real-time data streams. <p> This view of adaptive runtime systems is buttressed by recent experiences with an infrastructure for flexible input/output management <ref> [29, 28, 45] </ref> and by adaptive runtime systems for wide area computing [16, 50].
Reference: [46] <author> Daniel A. Reed, Robert D. Olson, Ruth A. Aydt, Tara M. Madhyastha, Thomas Birkett, David W. Jensen, Bobby A. A. Nazief, and Brian K. Totty. </author> <title> Scalable Performance Environments for Parallel Systems. </title> <booktitle> In Proceedings of the Sixth Distributed Memory Computing Conference. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <year> 1991. </year>
Reference-contexts: This isolation allowed Traceview to concurrently support multiple visualizations from different segments of the event trace. In response to repeated user requests for greater flexibility, adaptability, and extensibility, the Pablo toolkit <ref> [43, 46] </ref> completely decoupled performance data semantics from its structural representation via a performance data metaformat. In turn, this decoupling enabled construction of a coarse-grained data flow model for graphical programming of performance analyses and visualizations.
Reference: [47] <author> Daniel A. Reed and David C. Rudolph. </author> <title> Experiences with Hypercube Operating System Instrumentation. </title> <journal> International Journal of High-Speed Computing, </journal> <pages> pages 517-542, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: It was complemented by operating system and compiler instrumentation and a hardware data capture subsystem [31]. In addition, it supported a hierarchical performance browser that showed causal relations among message transmission and receipt events <ref> [47] </ref>. Like Seecube, Hyperview's implementation was dependent on vendor-specific performance instrumentation and interfaces. ParaGraph [21] relaxed this restriction, creating a simple user interface and access to a large number of network topology-centric visualizations for distributed memory program optimization.
Reference: [48] <author> Daniel A. Reed, Keith A. Shields, Luis F. Tavera, Will H. Scullin, and Christopher L. Elford. </author> <title> Virtual Reality and Parallel Systems Performance Analysis. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 57-67, </pages> <month> November </month> <year> 1995. </year> <note> 34 BIBLIOGRAPHY </note>
Reference-contexts: of performance instrumentation during application execution. 6 Performance Analysis and Visualization identify the most important metrics and to reduce data volume via the identification of equivalence classes [2], and * performance data immersion that exploits haptics, spatial audio cues, and three-dimensional graphics to increase presentation and interaction modalities (e.g., Avatar's <ref> [48, 27, 44] </ref> time tunnels and Virtue's hierarchical graphs [45]). As parallel systems evolve from homogeneous collections of processors to include the heterogeneous, distributed, and parallel systems that compose the computational grid, both the dimensionality of the performance metric space and the complexity of data correlation and analysis increase dramatically. <p> Several avenues are now being explored to meet these new challenges. Principal among them are the use of graph and geographic network representations [39, 8] for analysis of network traffic, immersive environments <ref> [48] </ref>) for richer data presentation and interaction, and automated and semi-automated performance analysis. <p> As in Paradyn, an interactive query interface allows users to "drill down" from high-level performance summaries to more detailed, quantitative data on specific metrics and processors. Avatar Avatar is a virtual reality framework for analyzing complex, time varying, multivariate data <ref> [48] </ref>. Using the CAVE [12], a head-mounted display, or a workstation and stereo glasses, Avatar immerses the data analyst in a virtual world of performance data.
Reference: [49] <author> Randy L. Ribler. </author> <title> Visualizing Categorical Time Series Data with Applications to Computer and Communications Network Traces. </title> <type> PhD thesis, </type> <institution> Virginia Tech, </institution> <month> April </month> <year> 1997. </year> <note> http://scholar.lib.vt.edu/theses/public. </note>
Reference-contexts: Such relationships include pe 13.4. Grid Performance Analysis Techniques 9 riodicity, rate of occurrence, stationarity, and the locations of repeating patterns within the data set <ref> [49] </ref>. In summary, the complexity of the computational grid is reflected in the diversity of the performance data needed to understand and rectify performance problems. High-dimensional, multivariate data must be correlated across multiple semantic levels using system metadata. <p> Chitra provides several visualizations designed specifically to analyzing categorical data <ref> [49] </ref>. These techniques support the visualization of event periodicity, event correlation, stationarity, and pattern recurrence. 22 Performance Analysis and Visualization Pablo and SvPablo Pablo is a portable, extensible performance instrumentation and analysis toolkit developed at the University of Illinois [43, 42].
Reference: [50] <author> Randy L. Ribler, Huseyin Simitci, and Daniel A. Reed. </author> <title> The Autopilot Performance-Directed Adaptive Control System. </title> <booktitle> In Proceedings of the International Conference on Supercomputing Workshop on Performance Data Mining, </booktitle> <month> July </month> <year> 1997. </year>
Reference-contexts: Principal among them are the use of graph and geographic network representations [39, 8] for analysis of network traffic, immersive environments [48]) for richer data presentation and interaction, and automated and semi-automated performance analysis. The latter, exemplified by the Paradyn [35] and Autopilot projects <ref> [50] </ref>, relies on real-time performance measurement for interactive performance debugging and dynamic optimization. 13.5.2 Representative Performance Tool Examples During the past fifteen years, a plethora of performance analysis and visualization tools for parallel and distributed systems have appeared. <p> This view of adaptive runtime systems is buttressed by recent experiences with an infrastructure for flexible input/output management [29, 28, 45] and by adaptive runtime systems for wide area computing <ref> [16, 50] </ref>.
Reference: [51] <author> Larry Rosenblum, R. A. Earnshaw, J. Encarnacao, H. Hagen, A. Kaufman, S. Klimenko, G. Nielson, F. Post, and D. Thalmann. </author> <title> Scientific Visualization. </title> <publisher> Academic Press, </publisher> <year> 1994. </year>
Reference-contexts: In turn, this means enabling them to understand the dynamic behavior of the software-hardware aggregate. Simply put, the goal is to put the right picture in the right mind at the right time, highlighting proximate and subsidiary performance and correctness problems and suggesting possible solutions. Scientific visualization <ref> [51] </ref> has proven invaluable as an analysis tool in the experimental and computational sciences, mapping scalar, vector, and tensor field data to visual attributes of real and simulated objects (e.g., rendering stresses as color gradients on a model of an airplane wing).
Reference: [52] <editor> Margaret Simmons, Rebecca Koskela, and Ingrid Bucher, editors. </editor> <title> Parallel Computer Systems: Performance Instrumentation and Visualization. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1990. </year>
Reference-contexts: Space does not permit a detailed description of all such tools, but a representative set are briefly described below, together with their applicability to grid applications; see <ref> [52, 53] </ref> for more exhaustive surveys. ParaGraph ParaGraph, originally developed by at Oak Ridge National Laboratory and later extended at the University of Illinois [21], uses execution traces produced by the Portable Instrumented Communication Library (PICL) to visualize processor utilization, interprocessor communication, and general task information.
Reference: [53] <author> Margaret L. Simmons, Ann H. Hayes, Jeffrey S. Brown, and Daniel A. Reed, </author> <title> editors. Debugging and Performance Tuning for Parallel Computing Systems. </title> <publisher> IEEE Computer Society Press, </publisher> <year> 1996. </year>
Reference-contexts: Space does not permit a detailed description of all such tools, but a representative set are briefly described below, together with their applicability to grid applications; see <ref> [52, 53] </ref> for more exhaustive surveys. ParaGraph ParaGraph, originally developed by at Oak Ridge National Laboratory and later extended at the University of Illinois [21], uses execution traces produced by the Portable Instrumented Communication Library (PICL) to visualize processor utilization, interprocessor communication, and general task information.
Reference: [54] <author> Evgenia Smirni and Daniel A. Reed. </author> <title> Workload Characterization of Input/Output Intensive Parallel Applications. </title> <booktitle> In Proceedings of the 9th International Conference on Modelling Techniques and Tools for Computer Performance Evaluation, </booktitle> <month> June </month> <year> 1997. </year> <title> BIBLIOGRAPHY 35 36 BIBLIOGRAPHY </title>
Reference-contexts: in visualization and analysis of heterogeneous, dynamic, and adaptive applications executing on the nascent computational grid. 13.2 Grid Performance Problems Optimizing the performance of parallel applications on homogeneous parallel systems remains a challenging research problem, with open questions related to analysis of data parallel codes [4], characterization of input/output behavior <ref> [54] </ref>, and management of memory locality. However, heterogeneous grid applications pose even more daunting performance problems.
References-found: 54

