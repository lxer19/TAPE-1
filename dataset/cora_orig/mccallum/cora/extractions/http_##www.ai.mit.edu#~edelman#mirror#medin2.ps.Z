URL: http://www.ai.mit.edu/~edelman/mirror/medin2.ps.Z
Refering-URL: http://www.ai.mit.edu/~edelman/archive.html
Root-URL: 
Email: edelman@wisdom.weizmann.ac.il  nin@math.tau.ac.il  
Title: Learning as extraction of low-dimensional representations  
Author: Shimon Edelman Nathan Intrator 
Date: November 18, 1996  
Address: Rehovot 76100, Israel  Tel Aviv 69978, Israel  
Affiliation: Dept. of Applied Mathematics and Computer Science The Weizmann Institute of Science  School of Mathematical Sciences Sackler Faculty of Exact Sciences Tel Aviv University  
Abstract: Psychophysical findings accumulated over the past several decades indicate that perceptual tasks such as similarity judgment tend to be performed on a low-dimensional representation of the sensory data. Low dimensionality is especially important for learning, as the number of examples required for attaining a given level of performance grows exponentially with the dimensionality of the underlying representation space. In this chapter, we argue that, whereas many perceptual problems are tractable precisely because their intrinsic dimensionality is low, the raw dimensionality of the sensory data is normally high, and must be reduced by a nontrivial computational process, which, in itself, may involve learning. Following a survey of computational techniques for dimensionality reduction, we show that it is possible to learn a low-dimensional representation that captures the intrinsic low-dimensional nature of certain classes of visual objects, thereby facilitating further learning of tasks involving those objects. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Amari, S., Cichocki, A., and Yang, H. H. </author> <year> (1996). </year> <title> A new learning algorithm for blind signal sepa ration. </title> <editor> In Tesauro, G., Touretzky, D., and Leen, T., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 8. </volume> <publisher> MIT Press. </publisher>
Reference-contexts: This actually leads to a minimization of higher order correlations, in addition to the second-order correlation of the PCA. It is yet unclear whether this formulation is appropriate for dimensionality reduction, although an attempt to extend the formulation to a dimensionality reduction method was recently presented <ref> (Amari et al., 1996) </ref>. 2.6 Topology-preserving dimensionality reduction We now turn to the discussion of topology-preserving methods; these can be especially useful for representing data for which an a priori pattern of similarities is given, and which are known to reside in an intrinsically low-dimensional space (embedded in a high-dimensional measurement
Reference: <author> Atick, J. J., Griffin, P. A., and Redlich, A. N. </author> <year> (1996). </year> <title> The vocabulary of shape: principal shapes for probing perception and neural response. </title> <journal> Network, </journal> <volume> 7 </volume> <pages> 1-5. </pages>
Reference-contexts: stimuli (closed contours, parameterized by two orthogonal variables), conducted by Shepard and Cermak (1973), showed that human 12 The essence of Exploratory Projection Pursuit (Friedman, 1987) is to seek projections so that the projected distribution is far from Gaussian. 13 An exception here is the space of human head shapes <ref> (Atick et al., 1996) </ref>; see also section 3.2. 13 subjects judge shape similarity as if they represent the shapes as points in a two-dimensional space, whose placement is correct in the sense of being isomorphic (with respect to shape similarity) to the original parameter space used to generate the shapes. 3.1
Reference: <author> Barlow, H. B. </author> <year> (1959). </year> <title> Sensory mechanisms, the reduction of redundancy, </title> <booktitle> and intelligence. In The mechanisation of thought processes, </booktitle> <pages> pages 535-539. </pages> <address> H.M.S.O., London. </address>
Reference: <author> Barlow, H. B. </author> <year> (1990). </year> <title> Conditions for versatile learning, Helmholtz's unconscious inference, and the task of perception. </title> <journal> Vision Research, </journal> <volume> 30 </volume> <pages> 1561-1571. </pages>
Reference: <author> Barlow, H. B. </author> <year> (1994). </year> <title> What is the computational goal of the neocortex? In Koch, </title> <editor> C. and Davis, J. L., editors, </editor> <title> Large-scale neuronal theories of the brain, </title> <booktitle> chapter 1, </booktitle> <pages> pages 1-22. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Baxter, J. </author> <year> (1995). </year> <title> The canonical metric for vector quantization. </title> <institution> NeuroCOLT NC-TR-95-047, University of London. </institution>
Reference-contexts: Another potential problem with vector quantization is its reliance on the raw (measurement-space) distances between data points, which, in many cases, are inappropriate. 7 In principle, this problem may be approached by incorporating knowledge about the task into the definition of the distance function <ref> (Baxter, 1995) </ref>, although the practical value of this approach is as yet unclear. 2.2 Discriminant analysis Given a number of independent features (dimensions) relative to which data are described, discriminant analysis (Fisher, 1936) creates a linear combination of these which yields the largest mean differences between the desired classes (clusters).
Reference: <author> Beck, J. </author> <year> (1972). </year> <title> Surface Color Perception. </title> <publisher> Cornell University Press, </publisher> <address> Ithaca, NY. </address>
Reference-contexts: The infinite dimensionality of these functions seems to suggest, further, that no set of measurements (short of an infinite and therefore an infeasible one) would suffice to support the recovery of surface reflectance. Nevertheless, human vision exhibits color constancy under a wide range of conditions <ref> (Beck, 1972) </ref>, despite the small dimensionality of the neural color coding space (De Valois and De Valois, 1978); moreover, the dimensionality of the psychological (perceived) color space is also small (Boynton, 1978).
Reference: <author> Bell, A. J. and Sejnowski, T. J. </author> <year> (1995). </year> <title> An information-maximisation approach to blind separation and blind deconvolution. </title> <journal> Neural Computation, </journal> <volume> 7(6) </volume> <pages> 1129-1159. </pages>
Reference: <author> Bellman, R. E. </author> <year> (1961). </year> <title> Adaptive Control Processes. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ. </address>
Reference: <author> Bienenstock, E., Cooper, L., and Munro, P. W. </author> <year> (1982). </year> <title> Theory for the development of neural selectivity: orientation specificity and binocular interaction in visual cortex. </title> <journal> J. of Neuroscience, </journal> <volume> 2 </volume> <pages> 32-48. </pages>
Reference: <author> Borg, I. and Lingoes, J. </author> <year> (1987). </year> <title> Multidimensional Similarity Structure Analysis. </title> <publisher> Springer, </publisher> <address> Berlin. </address>
Reference-contexts: In all the experiments, the parameter-space configurations according to which the stimuli had been arranged (such as the Star configuration in Figure 4, left) were easily recognizable in the MDS plots. Procrustes analysis <ref> (Borg and Lingoes, 1987) </ref> indicated that the similarity between the MDS-derived and the objective configurations was significantly above chance, as estimated by bootstrap (Efron and Tibshirani, 1993).
Reference: <author> Boynton, R. M. </author> <year> (1978). </year> <title> Color, hue, </title> <editor> and wavelength. In Carterette, E. C. and Friedman, M. P., editors, </editor> <booktitle> Handbook of Perception, volume V, </booktitle> <pages> pages 301-347. </pages> <publisher> Academic Press, </publisher> <address> New York, NY. </address> <note> 22 Bridle, </note> <author> J. S. and MacKay, D. J. C. </author> <year> (1992). </year> <title> Unsupervised classifiers, mutual information and `Phan tom Targets'. </title> <editor> In Moody, J., Hanson, S., and Lippmann, R., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 4, </volume> <pages> pages 1096-1101. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: Nevertheless, human vision exhibits color constancy under a wide range of conditions (Beck, 1972), despite the small dimensionality of the neural color coding space (De Valois and De Valois, 1978); moreover, the dimensionality of the psychological (perceived) color space is also small <ref> (Boynton, 1978) </ref>. In fact, both these color spaces are two-dimensional. 1 1.1.1 Low-dimensional physiological color space In human vision, there are three kinds of different retinal cone types (R, G, B; in addition, there are the rods, whose spectral selectivity resembles that of the R cones).
Reference: <author> Buckheit, J. and Donoho, D. L. </author> <year> (1995). </year> <title> Improved linear discrimination using time-frequency dictio naries. </title> <institution> Stanford university technical report. </institution>
Reference: <author> Cohen, J. </author> <year> (1964). </year> <title> Dependency of the spectral reflectance curves of the Munsell color chips. </title> <journal> Psycho nomic Sciences, </journal> <volume> 1 </volume> <pages> 369-370. </pages>
Reference: <author> Comon, P. </author> <year> (1994). </year> <title> Independent component analysis, a new concept? Signal Processing, </title> <booktitle> 36 </booktitle> <pages> 287-314. </pages>
Reference: <author> Cortese, J. M. and Dyre, B. P. </author> <year> (1996). </year> <title> Perceptual similarity of shapes generated from Fourier Descriptors. </title> <journal> Journal of Experimental Psychology: Human Perception and Performance, </journal> <volume> 22 </volume> <pages> 133-143. </pages>
Reference: <author> Cottrell, G. W., Munro, P., and Zipser, D. </author> <year> (1987). </year> <title> Learning internal representations from gray-scale images: An example of extensional programming. </title> <booktitle> In Ninth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 462-473, </pages> <address> Hillsdale. </address> <publisher> Erlbaum. </publisher>
Reference: <author> Cutzu, F. and Edelman, S. </author> <year> (1996). </year> <title> Faithful representation of similarities among three-dimensional shapes in human vision. </title> <booktitle> Proceedings of the National Academy of Science, </booktitle> <volume> 93 </volume> <pages> 12046-12050. </pages>
Reference-contexts: The response time and error rate data from each experiment were entered into proximity tables, as described in <ref> (Cutzu and Edelman, 1996) </ref>, and were submitted to MDS. In all the experiments, the parameter-space configurations according to which the stimuli had been arranged (such as the Star configuration in Figure 4, left) were easily recognizable in the MDS plots.
Reference: <author> De Valois, R. L. and De Valois, K. K. </author> <year> (1978). </year> <title> Neural coding of color. </title> <editor> In Carterette, E. C. and Friedman, M. P., editors, </editor> <booktitle> Handbook of Perception, volume V, </booktitle> <pages> pages 117-166. </pages> <publisher> Academic Press, </publisher> <address> New York, NY. </address>
Reference-contexts: Nevertheless, human vision exhibits color constancy under a wide range of conditions (Beck, 1972), despite the small dimensionality of the neural color coding space <ref> (De Valois and De Valois, 1978) </ref>; moreover, the dimensionality of the psychological (perceived) color space is also small (Boynton, 1978).
Reference: <author> Demartines, P. and Herault, J. </author> <year> (1996). </year> <title> Curvilinear component analysis: a self-organizing neural network for non linear mapping of data sets. Submitted to IEEE Transaction on Neural Networks. </title>
Reference: <author> DeMers, D. and Cottrell, G. </author> <year> (1993). </year> <title> Nonlinear dimensionality reduction. </title> <editor> In Hanson, S. J., Cowan, J. D., and Giles, C. L., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 580-587. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Desimone, R. and Ungerleider, L. </author> <year> (1989). </year> <title> Neural mechanisms of visual processing in monkeys. </title> <editor> In Boler, F. and Grafman, J., editors, </editor> <booktitle> Handbook of Neuropsychology, </booktitle> <volume> volume 2, </volume> <pages> pages 267-299. </pages> <publisher> Elsevier, Amsterdam. </publisher>
Reference-contexts: Why are subsequent populations needed?" <ref> (Desimone and Ungerleider, 1989, p.268) </ref>. 6 We now know that this approach to representation is untenable, as far as learning to recognize objects from examples is concerned.
Reference: <author> Diaconis, P. and Freedman, D. </author> <year> (1984). </year> <title> Asymptotics of graphical projection pursuit. </title> <journal> Annals of Statistics, </journal> <volume> 12 </volume> <pages> 793-815. </pages> <note> 23 Duda, </note> <author> R. O. and Hart, P. E. </author> <year> (1973). </year> <title> Pattern classification and scene analysis. </title> <publisher> Wiley, </publisher> <address> New York. </address>
Reference: <author> D'Zmura, M. and Iverson, G. </author> <year> (1996). </year> <title> A formal approach to color constancy: the recovery of surface and light source spectral properties using bilinear models. </title> <editor> In Dowling, C., Roberts, F., and Theuns, P., editors, </editor> <booktitle> Recent Progress in Mathematical Psychology. </booktitle> <publisher> Erlbaum, </publisher> <address> Hillsdale, NJ. </address>
Reference: <author> Edelman, S. </author> <year> (1995a). </year> <title> Representation of similarity in 3D object discrimination. </title> <journal> Neural Computation, </journal> <volume> 7 </volume> <pages> 407-422. </pages>
Reference: <author> Edelman, S. </author> <year> (1995b). </year> <title> Representation, Similarity, and the Chorus of Prototypes. </title> <journal> Minds and Ma chines, </journal> <volume> 5 </volume> <pages> 45-68. </pages>
Reference-contexts: Thus, a simple image-based representation (which may be considered roughly analogous to an initial stage of processing in the primate visual system, such as the primary visual area V1), could not reproduce the results observed with human subjects. The second model, which we call the Chorus of Prototypes <ref> (Edelman, 1995b) </ref>, corresponded to a higher stage of object processing, in which nearly viewpoint-invariant representations of familiar object classes are available; a rough analogy is to the inferotemporal visual area IT (Young and Yamane, 1992; Logothetis et al., 1995).
Reference: <author> Edelman, S., Cutzu, F., and Duvdevani-Bar, S. </author> <year> (1996). </year> <title> Similarity to reference shapes as a basis for shape representation. </title> <editor> In Cottrell, G. W., editor, </editor> <booktitle> Proceedings of 18th Annual Conf. of the Cognitive Science Society, </booktitle> <pages> pages 260-265, </pages> <address> San Diego, CA. </address>
Reference-contexts: A recent proposal for a reconciliation of the feature contrast theory derived from these results with the metric perceptual scaling theory is described in <ref> (Edelman et al., 1996) </ref>. 3 This pattern has not escaped the attention of theoretical psychologists. <p> The main problem with MDS, if it is considered as a method for massive dimensionality reduction rather than as a tool for exploration of experimental data in applied sciences (Shepard, 1980; Siedlecki et al., 1988), is its poor scaling with dimensionality <ref> (Intrator and Edelman, 1996) </ref>. In the context of learning, a number of methods for topology-preserving dimensionality reduction have been derived from the idea of a self-supervised auto-associative network (Elman and Zipser, 1988; DeMers and Cottrell, 1993; Demartines and Herault, 1996). <p> The response time and error rate data from each experiment were entered into proximity tables, as described in <ref> (Cutzu and Edelman, 1996) </ref>, and were submitted to MDS. In all the experiments, the parameter-space configurations according to which the stimuli had been arranged (such as the Star configuration in Figure 4, left) were easily recognizable in the MDS plots. <p> perceived similarity; if the saliency of individual classifiers in distinguishing between various stimuli is kept track of and is taken into consideration depending on the task at hand, then similarity between stimuli in the representation space can be made asymmetrical and non-transitive, in accordance with Tversky's (1977) general contrast model <ref> (Edelman et al., 1996) </ref>. Surprisingly, Chorus shares its most valuable feature | the ability to make explicit, with a minimal distortion, the low-dimensional pattern formed by a collection of stimuli that reside in an extremely high-dimensional measurement space | with an entire class of other methods. <p> representation of a space of human head shapes emerges following training on a classification task unrelated to similarity preservation, in an architecture that is unrelated to that of the multiple-classifier scheme described above. 3.2 Low-dimensional representation as a substrate for the transfer of learning Our next case study, taken from <ref> (Intrator and Edelman, 1996) </ref>, is intended to demonstrate (1) that a low-dimensional representation is an efficient means for supporting the development of versatile categorization performance through learning, and (2) that topographically faithful representations can emerge through a process of learning, even when the latter is guided by considerations other than the <p> To do that, the learning system must be biased towards solutions that possess the desirable properties | a task that is highly nontrivial in a high-dimensional space, because of the curse of dimensionality. The method for dimensionality reduction described in <ref> (Intrator and Edelman, 1996) </ref> effectively biases the learning system by combining multiple constraints via an extensive use of class labels. <p> This was done by starting with a set of nine 3D laser scans of human heads, and by embedding the 3 fi 6 grid in the 2D space spanned by the two leading "eigenheads" obtained 16 <ref> (Intrator and Edelman, 1996) </ref> applied their method also to another data set, consisting of parameterized fractal images. 18 by placing a 3 fi 6 grid in the space of the two leading principal components of the original nine heads. <p> to be a good substrate for solving classification tasks on which the system has not been trained: the error rate on a random nonlinear dichotomy involving the 18 classes was 0:02, compared to 0:07 obtained by a system trained specifically on that dichotomy, but using the raw multidimensional representation; see <ref> (Intrator and Edelman, 1996) </ref> for details. Right: results for a 5-layer bottleneck MLP with 2 hidden units in the middle hidden layer, trained on the 18-way classification task.
Reference: <author> Edelman, S. and Duvdevani-Bar, S. </author> <year> (1997). </year> <title> Similarity, connectionism, and the problem of represen tation in vision. Neural Computation, </title> <publisher> 9:-. in press. </publisher>
Reference-contexts: pattern of similarities that must be observed (e.g., pink should be represented as closer to red than to green), and the objective color spaces are low-dimensional, as we have seen in section 1.1. 11 A discussion of such quasiconformal mappings in the context of shape representation can be found in <ref> (Edelman and Duvdevani-Bar, 1997) </ref>. 11 pairwise distances between those points. MDS can serve to reduce dimensionality if the points are embedded into a space of fewer dimensions than the original space in which interpoint distances were measured. <p> For a discussion of the issue of different possible parameterizations, see <ref> (Edelman and Duvdevani-Bar, 1997) </ref>). The circles mark the true shape-space locations of the seven objects; the fi's the locations determined by MDS; lines connect corresponding points. <p> label to each distal stimulus, and (3) can be made to ignore irrelevant dimensions of variation in the data (e.g., downplay variation in viewpoint relative to variation in shape), is likely to support a faithful low-dimensional representation of all members of the category from which its training data are chosen <ref> (Edelman and Duvdevani-Bar, 1997) </ref>. <p> 3.1, and the other outlined in the present section) suggest that topography-preserving dimensionality reduction may be less elusive than previously thought, and, in fact, may be a generic property of systems that realize a broad class of mappings between the world and their internal representation space, 18 as proposed in <ref> (Edelman and Duvdevani-Bar, 1997) </ref>. 4 Summary and conclusions To paraphrase the title of E. Wigner's (1960) paper, the unreasonable effectiveness of living representational systems may seem to suggest, at first, that there must be something special about such systems that allows them to harbor representations of the world.
Reference: <author> Efron, B. and Tibshirani, R. </author> <year> (1993). </year> <title> An introduction to the bootstrap. </title> <publisher> Chapman and Hall, London. </publisher>
Reference-contexts: Procrustes analysis (Borg and Lingoes, 1987) indicated that the similarity between the MDS-derived and the objective configurations was significantly above chance, as estimated by bootstrap <ref> (Efron and Tibshirani, 1993) </ref>. Notably, the parameter-space configurations of the stimuli were also recovered in the long-term memory experiments, in which the subjects could not rely on immediate percepts or short-term memory representations of the stimuli (cf. Shepard and Chipman, 1970). and Edelman, 1996) (see section 3.1).
Reference: <author> Elman, J. L. and Zipser, D. </author> <year> (1988). </year> <title> Learning the hidden structure of speech. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 4(83). </volume>
Reference: <author> Field, D. J. </author> <year> (1994). </year> <title> What is the goal of sensory coding? Neural Computation, </title> <booktitle> 6 </booktitle> <pages> 559-601. </pages>
Reference: <author> Fisher, R. A. </author> <year> (1936). </year> <title> The use of multiple measurements in taxonomic problems. </title> <journal> Annals of Eugenics, </journal> <volume> 7 </volume> <pages> 179-188. </pages>
Reference-contexts: principle, this problem may be approached by incorporating knowledge about the task into the definition of the distance function (Baxter, 1995), although the practical value of this approach is as yet unclear. 2.2 Discriminant analysis Given a number of independent features (dimensions) relative to which data are described, discriminant analysis <ref> (Fisher, 1936) </ref> creates a linear combination of these which yields the largest mean differences between the desired classes (clusters).
Reference: <author> Friedman, J. H. </author> <year> (1987). </year> <title> Exploratory projection pursuit. </title> <journal> Journal of the American Statistical Asso ciation, </journal> <volume> 82 </volume> <pages> 249-266. </pages>
Reference-contexts: An early study involving such stimuli (closed contours, parameterized by two orthogonal variables), conducted by Shepard and Cermak (1973), showed that human 12 The essence of Exploratory Projection Pursuit <ref> (Friedman, 1987) </ref> is to seek projections so that the projected distribution is far from Gaussian. 13 An exception here is the space of human head shapes (Atick et al., 1996); see also section 3.2. 13 subjects judge shape similarity as if they represent the shapes as points in a two-dimensional space,
Reference: <author> Geman, S., Bienenstock, E., and Doursat, R. </author> <year> (1992). </year> <title> Neural networks and the bias-variance dilemma. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 1-58. </pages>
Reference-contexts: This emphasizes the choice of the "right" prior, as a means to improve the bias/variance tradeoff <ref> (Geman et al., 1992) </ref>.
Reference: <author> Gutfinger, D. and Sklansky, J. </author> <year> (1991). </year> <title> Robust classifiers by mixed adaptation. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 13 </volume> <pages> 552-567. </pages>
Reference: <author> Hastie, T., Tibshirani, R., and Buja, A. </author> <year> (1994). </year> <title> Flexible discriminant analysis by optimal scoring. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 89 </volume> <pages> 1255-1270. </pages>
Reference-contexts: As far as network implementation is concerned, linear discrimination is efficiently learned by a single-layer Perceptron (Rosenblatt, 1958). Some recent nonlinear extensions of discriminant analysis are discussed in <ref> (Hastie et al., 1994) </ref>. cluster is different in each direction (B). 2.3 Principal components and maximum information preservation In the reduction of dimensionality by principal component analysis (PCA), data are projected onto the leading eigenvectors of their covariance matrix, corresponding to the directions of maximum variance.
Reference: <author> Huber, P. J. </author> <year> (1985). </year> <title> Projection pursuit (with discussion). </title> <journal> The Annals of Statistics, </journal> <volume> 13 </volume> <pages> 435-475. </pages> <address> 24 Intrator, N. </address> <year> (1990). </year> <title> A neural network for feature extraction. </title> <editor> In Touretzky, D. S. and Lippmann, R. P., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 2, </volume> <pages> pages 719-726. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: One possible answer here is provided by the Projection Pursuit (PP) methods <ref> (Huber, 1985) </ref>. These seek features emphasizing the non-Gaussian nature of the data, which may be exhibited by (semi) linear projections.
Reference: <author> Intrator, N. </author> <year> (1993). </year> <title> Combining exploratory projection pursuit and projection pursuit regression with application to neural networks. </title> <journal> Neural Computation, </journal> <volume> 5(3) </volume> <pages> 443-455. </pages>
Reference-contexts: Both these methods make an explicit assumption about the structure of the weight space, but disregard the structure of the input space. As described in the context of projection pursuit regression <ref> (Intrator, 1993) </ref>, a penalty term may be added to the cost function minimized by error back propagation, for the purpose of measuring directly the goodness of the projections 12 (see Figure 3).
Reference: <author> Intrator, N. and Cooper, L. N. </author> <year> (1992). </year> <title> Objective function formulation of the BCM theory of visual cortical plasticity: Statistical connections, stability conditions. </title> <booktitle> Neural Networks, </booktitle> <volume> 5 </volume> <pages> 3-17. </pages>
Reference-contexts: The current version of this rule, its mathematical properties, statistical motivation and network extensions are discussed in <ref> (Intrator and Cooper, 1992) </ref>. 10 2.5 Independent component analysis Independent component analysis (ICA) (Comon, 1994; Bell and Sejnowski, 1995) attempts to find an affine transformation of the input data so that in the new coordinate system, the different dimensions are statistically independent.
Reference: <author> Intrator, N. and Edelman, S. </author> <year> (1996). </year> <title> Learning low dimensional representations of visual objects with extensive use of prior knowledge. </title> <booktitle> Machine Learning, </booktitle> <pages> pages -. submitted. </pages>
Reference-contexts: The main problem with MDS, if it is considered as a method for massive dimensionality reduction rather than as a tool for exploration of experimental data in applied sciences (Shepard, 1980; Siedlecki et al., 1988), is its poor scaling with dimensionality <ref> (Intrator and Edelman, 1996) </ref>. In the context of learning, a number of methods for topology-preserving dimensionality reduction have been derived from the idea of a self-supervised auto-associative network (Elman and Zipser, 1988; DeMers and Cottrell, 1993; Demartines and Herault, 1996). <p> representation of a space of human head shapes emerges following training on a classification task unrelated to similarity preservation, in an architecture that is unrelated to that of the multiple-classifier scheme described above. 3.2 Low-dimensional representation as a substrate for the transfer of learning Our next case study, taken from <ref> (Intrator and Edelman, 1996) </ref>, is intended to demonstrate (1) that a low-dimensional representation is an efficient means for supporting the development of versatile categorization performance through learning, and (2) that topographically faithful representations can emerge through a process of learning, even when the latter is guided by considerations other than the <p> To do that, the learning system must be biased towards solutions that possess the desirable properties | a task that is highly nontrivial in a high-dimensional space, because of the curse of dimensionality. The method for dimensionality reduction described in <ref> (Intrator and Edelman, 1996) </ref> effectively biases the learning system by combining multiple constraints via an extensive use of class labels. <p> This was done by starting with a set of nine 3D laser scans of human heads, and by embedding the 3 fi 6 grid in the 2D space spanned by the two leading "eigenheads" obtained 16 <ref> (Intrator and Edelman, 1996) </ref> applied their method also to another data set, consisting of parameterized fractal images. 18 by placing a 3 fi 6 grid in the space of the two leading principal components of the original nine heads. <p> to be a good substrate for solving classification tasks on which the system has not been trained: the error rate on a random nonlinear dichotomy involving the 18 classes was 0:02, compared to 0:07 obtained by a system trained specifically on that dichotomy, but using the raw multidimensional representation; see <ref> (Intrator and Edelman, 1996) </ref> for details. Right: results for a 5-layer bottleneck MLP with 2 hidden units in the middle hidden layer, trained on the 18-way classification task.
Reference: <author> Intrator, N., Reisfeld, D., and Yeshurun, Y. </author> <year> (1996). </year> <title> Face recognition using a hybrid super vised/unsupervised neural network. </title> <journal> Pattern Recognition Letters, </journal> <volume> 17 </volume> <pages> 67-76. </pages>
Reference-contexts: The main problem with MDS, if it is considered as a method for massive dimensionality reduction rather than as a tool for exploration of experimental data in applied sciences (Shepard, 1980; Siedlecki et al., 1988), is its poor scaling with dimensionality <ref> (Intrator and Edelman, 1996) </ref>. In the context of learning, a number of methods for topology-preserving dimensionality reduction have been derived from the idea of a self-supervised auto-associative network (Elman and Zipser, 1988; DeMers and Cottrell, 1993; Demartines and Herault, 1996). <p> representation of a space of human head shapes emerges following training on a classification task unrelated to similarity preservation, in an architecture that is unrelated to that of the multiple-classifier scheme described above. 3.2 Low-dimensional representation as a substrate for the transfer of learning Our next case study, taken from <ref> (Intrator and Edelman, 1996) </ref>, is intended to demonstrate (1) that a low-dimensional representation is an efficient means for supporting the development of versatile categorization performance through learning, and (2) that topographically faithful representations can emerge through a process of learning, even when the latter is guided by considerations other than the <p> To do that, the learning system must be biased towards solutions that possess the desirable properties | a task that is highly nontrivial in a high-dimensional space, because of the curse of dimensionality. The method for dimensionality reduction described in <ref> (Intrator and Edelman, 1996) </ref> effectively biases the learning system by combining multiple constraints via an extensive use of class labels. <p> This was done by starting with a set of nine 3D laser scans of human heads, and by embedding the 3 fi 6 grid in the 2D space spanned by the two leading "eigenheads" obtained 16 <ref> (Intrator and Edelman, 1996) </ref> applied their method also to another data set, consisting of parameterized fractal images. 18 by placing a 3 fi 6 grid in the space of the two leading principal components of the original nine heads. <p> to be a good substrate for solving classification tasks on which the system has not been trained: the error rate on a random nonlinear dichotomy involving the 18 classes was 0:02, compared to 0:07 obtained by a system trained specifically on that dichotomy, but using the raw multidimensional representation; see <ref> (Intrator and Edelman, 1996) </ref> for details. Right: results for a 5-layer bottleneck MLP with 2 hidden units in the middle hidden layer, trained on the 18-way classification task.
Reference: <author> Jacobs, D. W. </author> <year> (1996). </year> <title> The space requirements of indexing under perspective projections. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 18 </volume> <pages> 330-333. </pages>
Reference: <author> Jacobs, R. A., Jordan, M. I., Nowlan, S. J., and Hinton, G. E. </author> <year> (1991). </year> <title> Adaptive mixtures of local experts. </title> <journal> Neural Computation, </journal> <volume> 3(1) </volume> <pages> 79-87. </pages>
Reference-contexts: Second, cluster memberships are updated based on the new means. Many variations on this approach are possible. A statistically relevant formal framework here is that of fitting the data with a mixture of Gaussians, for which the estimation of the parameters is guided by the maximum likelihood principle <ref> (Jacobs et al., 1991) </ref>.
Reference: <author> Judd, D. B., MacAdam, D. L., and Wyszecki, G. </author> <year> (1964). </year> <title> Spectral distribution of typical daylight as a function of correlated color temperature. </title> <journal> Journal of the Optical Society of America, </journal> <volume> 54 </volume> <pages> 1031-1040. </pages>
Reference-contexts: The space of illuminations likely to be encountered in nature appears to be equally low-dimensional: a principal component analysis of 622 measurements of daylight illumination (carried out at different times of day) showed that over 99% of the variance can be accounted for by as few as three principal components <ref> (Judd et al., 1964) </ref>.
Reference: <author> Kambhatla, N. and Leen, T. K. </author> <year> (1994). </year> <title> Fast non-linear dimension reduction. </title> <editor> In Cowan, J. D., Tesauro, G., and Alspector, J., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 6. </volume> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: This approach, which resembles MDS, suffers from the same poor scaling with the dimensionality. A recent technique that combines PCA and clustering <ref> (Kambhatla and Leen, 1994) </ref> attempts to first cluster the input space and then perform bottleneck dimensionality reduction in different regions separately. In this way, they attempt to overcome the drawback of PCA, namely, its ability to find only linear structure. <p> combining multiple constraints via an extensive use of class labels is an effective way to impose bias on a learning system whose goal is to find a good LDR. 17 In particular, the use of multiple class labels 17 A series of control experiments with a 5-layer nonlinear bottleneck autoencoder <ref> (Kambhatla and Leen, 1994) </ref> showed that self-supervised dimensionality reduction cannot recover a good LDR in the present case, illustrating the 19 the plots show the locations of the 18 fi 3 test stimuli in the space spanned by the activities of the units residing in a hidden layer (18 faces times
Reference: <author> Kammen, D. and Yuille, A. </author> <year> (1988). </year> <title> Spontaneous symmetry-breaking energy functions and the emergence of orientation selective cortical cells. </title> <journal> Biological Cybernetics, </journal> <volume> 59 </volume> <pages> 23-31. </pages>
Reference: <author> Koontz, W. L. G. and Fukunaga, K. </author> <year> (1972). </year> <title> A nonlinear feature extraction algorithm using distance information. </title> <journal> IEEE Trans. Comput., </journal> <volume> 21 </volume> <pages> 56-63. </pages>
Reference-contexts: Because these methods are unsupervised, they extract representations that are not orthogonal to the irrelevant dimensions of the input space. An interesting approach that combines supervised feature extraction with topology preservation was proposed in <ref> (Koontz and Fukunaga, 1972) </ref>, whose dimensionality reduction algorithms explicitly optimize a joint measure of class separation and (input-space) distance preservation (see also Webb, 1995). This approach, which resembles MDS, suffers from the same poor scaling with the dimensionality.
Reference: <author> Kruskal, J. B. </author> <year> (1964). </year> <title> Multidimensional scaling by optimizing goodness of fit to a nonmetric hy pothesis. </title> <journal> Psychometrika, </journal> <volume> 29(1) </volume> <pages> 1-27. </pages>
Reference: <author> Kruskal, J. B. and Wish, M. </author> <year> (1978). </year> <title> Multidimensional Scaling. </title> <publisher> Sage Piblications, </publisher> <address> Beverly Hills, CA. </address>
Reference-contexts: The proximity table for each parameter-space configuration was constructed by computing the Euclidean distances between the views, encoded by 14 Provided that the MDS stress is small <ref> (Kruskal and Wish, 1978) </ref>, as it was in the above experiments. 15 the activities of the receptive fields. In the MDS-derived view-wise configurations, views of different objects were grouped together by object orientation, not by object identity.
Reference: <author> Le Cun, Y., Boser, B., Denker, J., Henderson, D., Howard, R., Hubbard, W., and Jackel, L. </author> <year> (1989). </year> <title> Backpropagation applied to handwritten zip code recognition. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 541-551. </pages> <note> 25 Leen, </note> <author> T. K. and Kambhatla, N. </author> <year> (1994). </year> <title> Fast non-linear dimension reduction. </title> <editor> In Cowan, J. D., Tesauro, G., and Alspector, J., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 6. </volume> <publisher> Morgan Kauffman, </publisher> <address> San Francisco, CA. </address>
Reference-contexts: Why are subsequent populations needed?" <ref> (Desimone and Ungerleider, 1989, p.268) </ref>. 6 We now know that this approach to representation is untenable, as far as learning to recognize objects from examples is concerned.
Reference: <author> Linsker, R. </author> <year> (1986). </year> <title> From basic network principles to neural architecture. </title> <booktitle> Proceedings of the National Academy of Sciences, USA, </booktitle> <volume> 83 </volume> <pages> 7508-7512, 8390-8394, 8779-8783. </pages>
Reference: <author> Logothetis, N. K., Pauls, J., and Poggio, T. </author> <year> (1995). </year> <title> Shape recognition in the inferior temporal cortex of monkeys. </title> <booktitle> Current Biology, </booktitle> <volume> 5 </volume> <pages> 552-563. </pages>
Reference: <author> Miller, K. D., Keller, J., and Stryker, M. P. </author> <year> (1989). </year> <title> Ocular dominance column development: Analysis and simulation. </title> <journal> Science, </journal> <volume> 240 </volume> <pages> 605-615. </pages>
Reference: <author> Millikan, R. </author> <year> (1984). </year> <title> Language, Thought, and Other Biological Categories. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: It seems to be more likely, however, that the phenomenon of representation may be yet another natural category, which developed under evolutionary pressure in response to certain traits of the world with which the system interacts <ref> (cf. Millikan, 1984) </ref>. No doubt, some of the relevant properties of the world contribute more than others in any given case of successful representation.
Reference: <author> Moody, J. and Darken, C. </author> <year> (1989). </year> <title> Fast learning in networks of locally tuned processing units. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 281-289. </pages>
Reference-contexts: Network versions of clustering algorithms frequently involve familiar learning rules, such as the Hebbian rule of synaptic modification <ref> (Moody and Darken, 1989) </ref>. The basic idea behind these methods is two-phase iterative optimization. Given the required or expected number of clusters, the algorithm first adjusts the means of the candidate clusters so as to reflect the cluster membership of each observation.
Reference: <author> Moses, Y., Adini, Y., , and Ullman, S. </author> <year> (1994). </year> <title> Face recognition: the problem of compensating for illumination changes. </title> <editor> In Eklundh, J.-O., editor, </editor> <booktitle> Proc. ECCV-94, </booktitle> <pages> pages 286-296. </pages> <publisher> Springer-Verlag. </publisher>
Reference-contexts: In other words, discriminant analysis seeks those 7 E.g., in the pixel space, the distance between two images of the same face taken under different illuminations is likely to be larger than the distance between images of two different faces, taken under similar illuminations <ref> (Moses et al., 1994) </ref>. projections that minimize intra-class variance while maximizing inter-class variance.
Reference: <author> Nosofsky, R. M. </author> <year> (1988). </year> <title> Exemplar-based accounts of relations between classification, recognition, and typicality. </title> <journal> Journal of Experimental Psychology: Learning, Memory and Cognition, </journal> <volume> 14 </volume> <pages> 700-708. </pages>
Reference: <author> Nowlan, S. J. and Hinton, G. E. </author> <year> (1992). </year> <title> Simplifying neural networks by soft weight-sharing. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 473-493. </pages>
Reference-contexts: An extension of this idea is the "soft" weight sharing, which favors irregularities in the weight distribution in the form of multimodality <ref> (Nowlan and Hinton, 1992) </ref>. This penalty has been shown to improve generalization results obtained by hard weight elimination, under which a weight whose value becomes smaller than a predefined threshold is set to zero.
Reference: <author> Oja, E. </author> <year> (1982). </year> <title> A simplified neuron model as a principal component analyzer. </title> <journal> Journal of Mathe matical Biology, </journal> <volume> 15 </volume> <pages> 267-273. </pages>
Reference: <author> Poggio, T. and Edelman, S. </author> <year> (1990). </year> <title> A network that learns to recognize three-dimensional objects. </title> <journal> Nature, </journal> <volume> 343 </volume> <pages> 263-266. </pages>
Reference-contexts: Such a representation of a 3D object can be relatively easily formed, given several views of the object (Ullman and Basri, 1991), e.g., by training a radial basis function (RBF) network to interpolate a characteristic function for the object in the space of all views of all objects <ref> (Poggio and Edelman, 1990) </ref>. In the simulations, an RBF network was trained to recognize each of a number of reference objects (in the Star configuration, illustrated in Figure 4, the three corner objects were used as reference).
Reference: <author> Poincare, H. (1913/1963). </author> <title> Mathematics and Science: Last Essays. </title> <publisher> Dover, </publisher> <address> New York. </address> <note> translated by J. W. Bolduc. </note>
Reference-contexts: typically describe visual perception as the extraction of information from the two-dimensional retinal image, completely ignoring the fact that the imme 5 diate successor of the retinal space in the processing hierarchy is, in primates, a million-dimensional space spanned by the activities of the individual axons in the optic nerve <ref> (cf. the discussion on the dimensionality of space in Poincare, 1913) </ref>. Obviously, the million numbers available at any given moment at the point of entry to the visual system must be somehow combined together if the dimensionality of the signal is to be reduced.
Reference: <author> Rolls, E. T. and Tovee, M. J. </author> <year> (1995). </year> <title> Sparseness of the neuronal representation of stimuli in the primate temporal visual cortex. </title> <journal> J. of Neurophysiology, </journal> <volume> 73 </volume> <pages> 713-726. </pages>
Reference: <author> Rose, K., Gurewitz, E., and Fox, C. </author> <year> (1992). </year> <title> Vector quantization by deterministic annealing. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 38 </volume> <pages> 1249-1257. </pages> <note> 26 Rosenblatt, </note> <author> F. </author> <year> (1958). </year> <title> The perceptron: A probabilistic model for information storage and orga nization in the brain. </title> <journal> Psych. Rev., </journal> 65:386-407. (Reprinted in Neurocomputing (MIT Press, 1988).). 
Reference-contexts: In general, clustering techniques tend to be very sensitive to the dimensionality of the data, leading to large quantization distortions and to problems associated with local minima of the optimization criterion; to alleviate these problems, recently proposed global vector quantization methods use optimization by simulated annealing <ref> (Rose et al., 1992) </ref>.
Reference: <author> Sanger, T. </author> <year> (1989). </year> <title> Optimal unsupervised learning in feedforward neural networks. </title> <type> AI Lab TR 1086, </type> <institution> MIT. </institution>
Reference: <author> Sejnowski, T. J. </author> <year> (1977). </year> <title> Storing covariance with nonlinearly interacting neurons. </title> <journal> Journal of Math ematical Biology, </journal> <volume> 4 </volume> <pages> 303-321. </pages>
Reference: <author> Shepard, R. N. </author> <year> (1962). </year> <title> The analysis of proximities: Multidimensional scaling with unknown distance function. part i. </title> <journal> Psychometrika, </journal> <volume> 27(2) </volume> <pages> 125-140. </pages>
Reference: <author> Shepard, R. N. </author> <year> (1966). </year> <title> Metric structures in ordinal data. </title> <journal> J. Math. Psychology, </journal> <volume> 3 </volume> <pages> 287-315. </pages>
Reference: <author> Shepard, R. N. </author> <year> (1980). </year> <title> Multidimensional scaling, tree-fitting, and clustering. </title> <journal> Science, </journal> <volume> 210 </volume> <pages> 390-397. </pages>
Reference-contexts: a large variety of perceptual scaling experiments: the subject's performance in tasks involving similarity judgment or perception can be accounted for to a substantial degree by postulating that the perceived similarity directly reflects the metric structure of an underlying perceptual space, in which the various stimuli are represented as points <ref> (Shepard, 1980) </ref>. 4 4 The metric model of the system of internal representations is not always directly applicable, as shown by asymmetry and lack of transitivity of similarity judgments that can be obtained under a range of conditions (Tversky, 1977). <p> match those of mechanics, Shepard (1987) proposed a law of generalization that tied the likelihood of two stimuli evoking the same response to the proximity of the stimuli in a psychological representation space | the same space that so persistently turned out to be low-dimensional in the experiments surveyed in <ref> (Shepard, 1980) </ref>. The significance of Shepard's insight is twofold.
Reference: <author> Shepard, R. N. </author> <year> (1987). </year> <title> Toward a universal law of generalization for psychological science. </title> <journal> Science, </journal> <volume> 237 </volume> <pages> 1317-1323. </pages>
Reference-contexts: We contend, however, that it is possible to identify certain core properties that any representation of the world must possess to be able to support efficient learning and learning-related behavior. Specifically, we believe that representations aimed at capturing similarity | itself the basis for generalization in learning <ref> (Shepard, 1987) </ref> | must be low-dimensional. The link between the issues of similarity and of low-dimensional representations (LDRs) becomes apparent when one considers problems that arise in visual psychophysics. By definition, such problems involve a relationship between the physical characteristics of a stimulus and the perceptual event it evokes.
Reference: <author> Shepard, R. N. and Cermak, G. W. </author> <year> (1973). </year> <title> Perceptual-cognitive explorations of a toroidal set of free-form stimuli. </title> <journal> Cognitive Psychology, </journal> <volume> 4 </volume> <pages> 351-377. </pages>
Reference: <author> Shepard, R. N. and Chipman, S. </author> <year> (1970). </year> <title> Second-order isomorphism of internal representations: Shapes of states. </title> <journal> Cognitive Psychology, </journal> <volume> 1 </volume> <pages> 1-17. </pages>
Reference-contexts: Notably, the parameter-space configurations of the stimuli were also recovered in the long-term memory experiments, in which the subjects could not rely on immediate percepts or short-term memory representations of the stimuli <ref> (cf. Shepard and Chipman, 1970) </ref>. and Edelman, 1996) (see section 3.1). The inset shows one of the shapes, at about 1/3 of its actual screen size, as seen by the subjects in a typical experiment.
Reference: <author> Siedlecki, W., Siedlecka, K., and Sklansky, J. </author> <year> (1988). </year> <title> An overview of mapping techniques for exploratory pattern analysis. </title> <journal> Pattern Recognition, </journal> <volume> 21 </volume> <pages> 411-429. </pages>
Reference: <author> Stone, C. J. </author> <year> (1982). </year> <title> Optimal global rates of convergence for nonparametric regression. </title> <journal> Annals of statistics, </journal> <volume> 10 </volume> <pages> 1040-1053. </pages>
Reference: <author> Tversky, A. </author> <year> (1977). </year> <title> Features of similarity. </title> <journal> Psychological Review, </journal> <volume> 84 </volume> <pages> 327-352. </pages>
Reference-contexts: underlying perceptual space, in which the various stimuli are represented as points (Shepard, 1980). 4 4 The metric model of the system of internal representations is not always directly applicable, as shown by asymmetry and lack of transitivity of similarity judgments that can be obtained under a range of conditions <ref> (Tversky, 1977) </ref>. A recent proposal for a reconciliation of the feature contrast theory derived from these results with the metric perceptual scaling theory is described in (Edelman et al., 1996). 3 This pattern has not escaped the attention of theoretical psychologists.
Reference: <author> Ullman, S. and Basri, R. </author> <year> (1991). </year> <title> Recognition by linear combinations of models. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 13 </volume> <pages> 992-1005. </pages>
Reference-contexts: Such a representation of a 3D object can be relatively easily formed, given several views of the object <ref> (Ullman and Basri, 1991) </ref>, e.g., by training a radial basis function (RBF) network to interpolate a characteristic function for the object in the space of all views of all objects (Poggio and Edelman, 1990).
Reference: <author> Waibel, A., Hanazawa, T., Hinton, G., Shikano, K., and Lang, K. </author> <year> (1989). </year> <title> Phoneme recognition using time-delay neural networks. </title> <journal> IEEE Transactions on ASSP, </journal> <volume> 37 </volume> <pages> 328-339. </pages>
Reference: <author> Webb, A. R. </author> <year> (1995). </year> <title> Multidimensional-scaling by iterative majorization using radial basis functions. </title> <journal> Pattern Recognition, </journal> <volume> 28 </volume> <pages> 753-759. </pages>
Reference-contexts: An interesting approach that combines supervised feature extraction with topology preservation was proposed in (Koontz and Fukunaga, 1972), whose dimensionality reduction algorithms explicitly optimize a joint measure of class separation and (input-space) distance preservation <ref> (see also Webb, 1995) </ref>. This approach, which resembles MDS, suffers from the same poor scaling with the dimensionality. A recent technique that combines PCA and clustering (Kambhatla and Leen, 1994) attempts to first cluster the input space and then perform bottleneck dimensionality reduction in different regions separately.
Reference: <author> Wigner, E. P. </author> <year> (1960). </year> <title> The unreasonable effectiveness of mathematics in the natural sciences. </title> <journal> Comm. Pure Appl. Math., XIII:1-14. </journal> <volume> 27 Yamac, </volume> <editor> M. </editor> <year> (1969). </year> <title> Can we do better by combining `supervised' and `nonsupervised' machine learning for pattern analysis. </title> <type> Ph.D. dissertation, </type> <institution> Brown University. </institution>
Reference: <author> Young, G. and Householder, A. S. </author> <year> (1938). </year> <title> Discussion of a set of points in terms of their mutual distances. </title> <journal> Psychometrika, </journal> <volume> 3 </volume> <pages> 19-22. </pages>
Reference-contexts: This technique is derived from the observation that the knowledge of distances among several points constrains the possible locations of the points (relative to each other) to a sufficient degree as to allow the recovery of the locations (i.e., the coordinates of the points) by a numerical procedure <ref> (Young and Householder, 1938) </ref>.
Reference: <author> Young, M. P. and Yamane, S. </author> <year> (1992). </year> <title> Sparse population coding of faces in the inferotemporal cortex. </title> <journal> Science, </journal> <volume> 256 </volume> <pages> 1327-1331. 28 </pages>
References-found: 80

