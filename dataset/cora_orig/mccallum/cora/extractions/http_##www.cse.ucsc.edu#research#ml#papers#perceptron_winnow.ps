URL: http://www.cse.ucsc.edu/research/ml/papers/perceptron_winnow.ps
Refering-URL: http://www.cse.ucsc.edu/research/ml/publications.html
Root-URL: http://www.cse.ucsc.edu
Title: The Perceptron algorithm vs. Winnow: linear vs. logarithmic mistake bounds when few input variables are relevant  
Author: Jyrki Kivinen Manfred K. Warmuth 
Note: Supported by the Academy of Finland. This work was done while the author was visiting  Supported by NSF grant IRI-9123692. Address:  
Address: Santa Cruz, CA 95064 USA  California, Santa Cruz. Address:  P.O. Box 26 (Teollisuuskatu 23), FIN-00014  Helsinki, Finland,  Santa Cruz, Santa Cruz, CA 95064, USA,  
Affiliation: Baskin Center for Computer Engineering Information Sciences University of California, Santa Cruz  University of  Department of Computer Science,  University of  Computer and Information Sciences, University of California,  
Pubnum: UCSC-CRL-95-44  
Email: jkivinen@cs.helsinki.fi.  manfred@cse.ucsc.edu.  
Date: October 6, 1995  
Abstract: We give an adversary strategy that forces the Perceptron algorithm to make (N k + 1)=2 mistakes when learning k-literal disjunctions over N variables. Experimentally we see that even for simple random data, the number of mistakes made by the Perceptron algorithm grows almost linearly with N, even if the number k of relevant variable remains a small constant. In contrast, Littlestone's algorithm Winnow makes at most O(k log N) mistakes for the same problem. Both algorithms use thresholded linear functions as their hypotheses. However, Winnow does multiplicative updates to its weight vector instead of the additive updates of the Perceptron algorithm. 
Abstract-found: 1
Intro-found: 1
Reference: [AW95] <author> P. Auer and M. K. Warmuth. </author> <title> Tracking the best disjunction. </title> <booktitle> In Proc. 36th Symposium on the Foundations of Comp. </booktitle> <publisher> Sci. IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1995. </year> <note> References 13 </note>
Reference-contexts: Bounds on the worst-case number of mistakes have earlier been obtained for both the Perceptron algorithm and Winnow. Both of these upper bound proofs can be interpreted as using amortized analysis with a potential function. Different potential functions are used: a generalized version of the entropy function for Winnow <ref> [Lit91, AW95] </ref> and the squared Euclidean distance in the Perceptron Convergence Theorem [DH73]. Our observations are analogous to the case of on-line linear regression. Again, there are two algorithms, and worst-case loss bounds for them can be proved using the two potential functions [KW94]. <p> If the instances or outcomes are corrupted by noise, such a target does not usually exist. The noisy case has been considered by Littlestone [Lit91] and more recently by Auer and Warmuth <ref> [AW95] </ref>. Auer and Warmuth also prove worst-case loss bounds for the situation in which the target may change over time.
Reference: [BEHW89] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. </author> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> J. ACM, </journal> <volume> 36(4) </volume> <pages> 929-965, </pages> <year> 1989. </year>
Reference: [DH73] <author> R. O. Duda and P. E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <year> 1973. </year>
Reference-contexts: There are several other algorithms that make multiplicative weight updates and achieve similar mistake bounds [Lit89]. The best upper bound we know for the Perceptron algorithm comes from the Perceptron Convergence Theorem given, e.g, by Duda and Hart <ref> [DH73, pp. 142-145] </ref>. Assuming that the target is a monotone k-literal disjunctions and the instances x t 2 f 0; 1 g N satisfy P i x t;i X for some value X, the bound is O (kX) mistakes. Note that always X N . <p> Both of these upper bound proofs can be interpreted as using amortized analysis with a potential function. Different potential functions are used: a generalized version of the entropy function for Winnow [Lit91, AW95] and the squared Euclidean distance in the Perceptron Convergence Theorem <ref> [DH73] </ref>. Our observations are analogous to the case of on-line linear regression. Again, there are two algorithms, and worst-case loss bounds for them can be proved using the two potential functions [KW94].
Reference: [Kha79] <author> L. G. Khachiyan. </author> <title> A polynomial algorithm in linear programming (in Russian). </title> <journal> Doklady Akademii Nauk SSSR, </journal> <volume> 244 </volume> <pages> 1093-1096, </pages> <year> 1979. </year> <title> (English translation: </title> <journal> Soviet Mathematics Doklady </journal> 20:191-194, 1979.) 
Reference-contexts: Note that always X N . As Maass and Turan [MT94] have pointed out, several linear programming methods can be transformed into efficient linear on-line prediction algorithms. Most notably, this applies to Khachiyan's ellipsoid algorithm <ref> [Kha79] </ref> and to a newer algorithm due to Vaidya [Vai89]. Vaidya's algorithm achieves an upper bound of O (N 2 log N ) mistakes for an arbitrary linear classifier as the target when the instances are from f 0; 1 g N .
Reference: [KW94] <author> J. Kivinen and M. K. Warmuth. </author> <title> Exponentiated gradient versus gradient descent for linear predictors. </title> <type> Report UCSC-CRL-94-16, </type> <institution> University of California, Santa Cruz, </institution> <month> June </month> <year> 1994. </year> <note> An extended abstract appeared in STOC '95. </note>
Reference-contexts: 1. Introduction 3 dense targets is similar to the situation in on-line linear regression <ref> [KW94] </ref>. In the regression problem, the classical Gradient Descent algorithm makes Perceptron-style additive updates, and a new family of Exponentiated Gradient algorithms makes multiplicative Winnow-style updates. Again, the Exponentiated Gradient algorithms win for sparse targets and dense instances, while the gradient descent algorithm wins for dense targets and sparse instances. <p> Our observations are analogous to the case of on-line linear regression. Again, there are two algorithms, and worst-case loss bounds for them can be proved using the two potential functions <ref> [KW94] </ref>. In the case of linear regression it is even possible to derive the updates from the two potential functions in addition to using them in a worst-case analysis. It is an open problem to devise a framework for deriving updates from the potential functions in the linear classification case. <p> If only few variables are relevant then additive algorithms seem to use all the dimensions in a futile search for a good predictor. A cleaner comparison between the related algorithms has been obtained in the linear regression case <ref> [KW94] </ref>. When the targets are small disjunctions, the ellipsoid method also exhibits similar linear growth of its number of mistakes as the computationally trivial Perceptron algorithm. We do not know whether Vaidya's algorithm for linear programming exhibits the linear growth.
Reference: [Lit88] <author> N. Littlestone. </author> <title> Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. </title> <journal> Mach. Learning, </journal> <volume> 2 </volume> <pages> 285-318, </pages> <year> 1988. </year>
Reference-contexts: For this simple artificial data the Perceptron algorithm actually outperformed Winnow that was tuned according to the upper bound theorems of Littlestone <ref> [Lit88] </ref>. 4 2. The prediction model and algorithms We introduce the details of the on-line prediction model and the algorithms we consider in Section 2. Section 3 gives our adversarial lower bound constructions for the class of additive algorithms. <p> rate , the update of the Perceptron algorithm can be written componentwise as w t+1;i = w t;i t x t;i (2:1) and the update of Winnow as w t+1;i = w t;i e t x t;i : (2:2) Note that this basic version of Winnow (the algorithm Winnow2 of <ref> [Lit88] </ref>) only uses positive weights (assuming that the initial weights are positive). The algorithm can be generalized for negative weights by a simple reduction [Lit88]. See Littlestone [Lit89] for a discussion on the learning rates and other parameters of Winnow. <p> update of Winnow as w t+1;i = w t;i e t x t;i : (2:2) Note that this basic version of Winnow (the algorithm Winnow2 of <ref> [Lit88] </ref>) only uses positive weights (assuming that the initial weights are positive). The algorithm can be generalized for negative weights by a simple reduction [Lit88]. See Littlestone [Lit89] for a discussion on the learning rates and other parameters of Winnow. Here we just point out the standard method of allowing the threshold to be fixed to 0 at the cost of increasing the dimensionality of the problem by one. <p> This useful technique gives a method for effectively updating the threshold together with the components of the weight vector. It is known that if the target is a monotone k-literal disjunction, Winnow makes O (k log N ) mistakes <ref> [Lit88] </ref>. There are several other algorithms that make multiplicative weight updates and achieve similar mistake bounds [Lit89]. The best upper bound we know for the Perceptron algorithm comes from the Perceptron Convergence Theorem given, e.g, by Duda and Hart [DH73, pp. 142-145].
Reference: [Lit89] <author> N. Littlestone. </author> <title> Mistake Bounds and Logarithmic Linear-threshold Learning Algorithms. </title> <type> PhD thesis, Report UCSC-CRL-89-11, </type> <institution> University of California Santa Cruz, </institution> <year> 1989. </year>
Reference-contexts: The algorithm can be generalized for negative weights by a simple reduction [Lit88]. See Littlestone <ref> [Lit89] </ref> for a discussion on the learning rates and other parameters of Winnow. Here we just point out the standard method of allowing the threshold to be fixed to 0 at the cost of increasing the dimensionality of the problem by one. <p> It is known that if the target is a monotone k-literal disjunction, Winnow makes O (k log N ) mistakes [Lit88]. There are several other algorithms that make multiplicative weight updates and achieve similar mistake bounds <ref> [Lit89] </ref>. The best upper bound we know for the Perceptron algorithm comes from the Perceptron Convergence Theorem given, e.g, by Duda and Hart [DH73, pp. 142-145].
Reference: [Lit91] <author> N. Littlestone. </author> <title> Redundant noisy attributes, attribute errors, and linear threshold learning using Winnow. </title> <booktitle> In Proc. 4th Workshop on Comput. Learning Theory, </booktitle> <pages> pages 147-156. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: With this tuning Winnow is guaranteed to make at most O (k + log (N=k)) mistakes if the target is a k-literal monotone disjunction, and the algorithm is even robust against noise <ref> [Lit91] </ref>. For the Perceptron algorithm we have used zero initial weights and eliminated the threshold by the transformation given in Section 2. In this case the choice of the learning rate of the Perceptron algorithm makes no difference. As we see, in this sparse case Winnow made clearly fewer mistakes. <p> Bounds on the worst-case number of mistakes have earlier been obtained for both the Perceptron algorithm and Winnow. Both of these upper bound proofs can be interpreted as using amortized analysis with a potential function. Different potential functions are used: a generalized version of the entropy function for Winnow <ref> [Lit91, AW95] </ref> and the squared Euclidean distance in the Perceptron Convergence Theorem [DH73]. Our observations are analogous to the case of on-line linear regression. Again, there are two algorithms, and worst-case loss bounds for them can be proved using the two potential functions [KW94]. <p> In this paper we considered only the case in which there is a target disjunction that is consistent with the trial sequence. If the instances or outcomes are corrupted by noise, such a target does not usually exist. The noisy case has been considered by Littlestone <ref> [Lit91] </ref> and more recently by Auer and Warmuth [AW95]. Auer and Warmuth also prove worst-case loss bounds for the situation in which the target may change over time.
Reference: [Lit95] <author> N. Littlestone. </author> <title> Comparing several linear-threshold learning algorithms on tasks involving superfluous attributes. </title> <booktitle> In Proc. 12th International Machine Learning Conference (ML-95), </booktitle> <pages> pages 353-361, </pages> <year> 1995. </year>
Reference-contexts: Studying this behavior will lead to a deeper understanding of how high dimensionality hurts the Perceptron algorithm and other additive algorithms. A more extensive experimental comparison of various on-line algorithms for learning disjunctions in the presence of attribute noise has recently been done by Littlestone <ref> [Lit95] </ref>. Bounds on the worst-case number of mistakes have earlier been obtained for both the Perceptron algorithm and Winnow. Both of these upper bound proofs can be interpreted as using amortized analysis with a potential function.
Reference: [LLW91] <author> N. Littlestone, P. M. Long, and M. K. Warmuth. </author> <title> On-line learning of linear functions. </title> <booktitle> In Proc. 23rd ACM Symposium on Theory of Computing, </booktitle> <pages> pages 465-475, </pages> <year> 1991. </year>
Reference-contexts: Similar proofs have been devised for linear regression <ref> [LLW91] </ref>. The proofs presented here for the case of linear classification are slightly more involved. We also wish to point out that the behavior similar to that predicted by the worst-case lower bounds already takes place on random data.
Reference: [MT94] <author> W. Maass and G. Turan. </author> <title> How fast can a threshold gate learn. </title> <booktitle> In Computational Learning Theory and Natural Learning Systems, </booktitle> <volume> Volume I, </volume> <pages> pages 381-414. </pages> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: Assuming that the target is a monotone k-literal disjunctions and the instances x t 2 f 0; 1 g N satisfy P i x t;i X for some value X, the bound is O (kX) mistakes. Note that always X N . As Maass and Turan <ref> [MT94] </ref> have pointed out, several linear programming methods can be transformed into efficient linear on-line prediction algorithms. Most notably, this applies to Khachiyan's ellipsoid algorithm [Kha79] and to a newer algorithm due to Vaidya [Vai89].
Reference: [Ros58] <author> F. Rosenblatt. </author> <title> The perceptron: A probabilistic model for information storage and organization in the brain. </title> <journal> Psych. Rev., </journal> <volume> 65 </volume> <pages> 386-407, </pages> <year> 1958. </year> <note> (Reprinted in Neurocomputing (MIT Press, 1988).). </note>
Reference: [SST91] <author> H. Sompolinsky, H. S. Seung, and N. Tishby. </author> <title> Learning curves in large neural networks. </title> <booktitle> In Proc. 4th Workshop on Comput. Learning Theory, </booktitle> <pages> pages 112-127. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: Open problems 5 Open problems So far the evaluation of our algorithms on random data is only experimental. However, it seems possible to obtain closed formulas for the expected total number of mistakes of the Perceptron algorithm on some thermodynamic limit (see, e.g, <ref> [SST91, WRB93] </ref>). We wish to study how these closed formulas relate to the worst-case upper bounds and the adversary lower bounds. Studying this behavior will lead to a deeper understanding of how high dimensionality hurts the Perceptron algorithm and other additive algorithms.
Reference: [Vai89] <author> P. M. Vaidya. </author> <title> A new algorithm for minimizing convex functions over convex sets. </title> <booktitle> In Proc. 30th Symposium on Foundations of Computer Science, </booktitle> <pages> pages 338-343. </pages> <publisher> IEEE Computer Society, </publisher> <year> 1989. </year>
Reference-contexts: Note that always X N . As Maass and Turan [MT94] have pointed out, several linear programming methods can be transformed into efficient linear on-line prediction algorithms. Most notably, this applies to Khachiyan's ellipsoid algorithm [Kha79] and to a newer algorithm due to Vaidya <ref> [Vai89] </ref>. Vaidya's algorithm achieves an upper bound of O (N 2 log N ) mistakes for an arbitrary linear classifier as the target when the instances are from f 0; 1 g N .
Reference: [Val84] <author> L. G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Commun. ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <year> 1984. </year>
Reference: [VC71] <author> V. N. Vapnik and A. Y. Chervonenkis. </author> <title> On the uniform convergence of relative frequencies of events to their probabilities. </title> <journal> Theory of Probab. and its Applications, </journal> <volume> 16(2) </volume> <pages> 264-280, </pages> <year> 1971. </year>
Reference: [WRB93] <author> T. L. H. Watkin, A. Rau, and M. Biehl. </author> <title> The statistical mechanics of learning a rule. </title> <journal> Rev. Mod. Phys., </journal> <volume> 65 </volume> <pages> 499-556, </pages> <year> 1993. </year>
Reference-contexts: Open problems 5 Open problems So far the evaluation of our algorithms on random data is only experimental. However, it seems possible to obtain closed formulas for the expected total number of mistakes of the Perceptron algorithm on some thermodynamic limit (see, e.g, <ref> [SST91, WRB93] </ref>). We wish to study how these closed formulas relate to the worst-case upper bounds and the adversary lower bounds. Studying this behavior will lead to a deeper understanding of how high dimensionality hurts the Perceptron algorithm and other additive algorithms.
References-found: 17

