URL: http://www.cs.utah.edu/~cs686/Previous/a97/prefetch-rice.ps
Refering-URL: http://www.cs.utah.edu/~cs686/Previous/a97/
Root-URL: 
Email: fparthas|vijaypai|shafi|saritag@rice.edu  
Title: The Interaction of Software Prefetching with ILP Processors in Shared-Memory Systems  
Author: Parthasarathy Ranganathan, Vijay S. Pai, Hazim Abdel-Shafi, Sarita V. Adve 
Address: Houston, Texas 77005  
Affiliation: Department of Electrical and Computer Engineering Rice University  
Date: (June, 1997)  
Note: To appear in Proceedings of ISCA-24  
Abstract: This paper provides the first study of the effectiveness of software-controlled non-binding prefetching in shared-memory multiprocessors built of state-of-the-art ILP-based processors. We find that software prefetching results in significant reductions in execution time (12% to 31%) for three out of five applications on an ILP system. However, compared to previous-generation systems, software prefetching is significantly less effective in reducing the memory stall component of execution time on an ILP system. Consequently, even after adding software prefetching, memory stall time accounts for over 30% of the total execution time in four out of five applications on our ILP system. This paper also investigates the interaction of software prefetching with memory consistency models on ILP-based multiprocessors. In particular, we seek to determine whether software prefetching can equalize the performance of sequential consistency (SC) and release consistency (RC). We find that even with software prefetching, for three out of five applications, RC provides a significant reduction in execution time (15% to 40%) compared to SC. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. Abdel-Shafi, J. Hall, S. V. Adve, and V. S. Adve. </author> <title> An Evaluation of Fine-Grain Producer-Initiated Communication in Cache-Coherent Multiprocessors. </title> <booktitle> In Proceedings of the 3rd International Symposium on High-Performance Computer Architecture, </booktitle> <year> 1997. </year>
Reference-contexts: These results show that further techniques are needed to alleviate the effects of late prefetches and resource contention. Latency-reducing techniques such as producer-initiated communication primitives <ref> [1, 15, 29] </ref> appear promising. <p> Latency-reducing techniques such as producer-initiated communication primitives [1, 15, 29] appear promising. A recent study with simple processors has shown that such primitives can interact positively with software prefetching to reduce the effects of both late prefetches and resource contention <ref> [1] </ref>. 6 Interaction of Prefetching with Consistency Models on ILP Systems This section evaluates the performance benefits of software prefetching with sequential consistency on ILP multiprocessors and also determines if software prefetching can equalize the performance of sequential consistency (SC) and release consistency (RC).
Reference: [2] <author> J. E. Bennett and M. J. Flynn. </author> <title> Latency Tolerance for Dynamic Processors. </title> <institution> Stanford University, CSL-TR-96-687, </institution> <year> 1996. </year>
Reference-contexts: Luk and Mowry proposed software prefetching algorithms for pointer-based data structures and evaluated their algorithms for an ILP processor similar to the MIPS R10000 [19]. Bennett and Flynn compared stream buffers (a form of hardware-controlled prefetching), hardware stride-based prefetching, and victim caches for state-of-the-art ILP processors <ref> [2] </ref>. They found that stream buffers and stride-based prefetch-ing do not have much impact on most SPEC92 programs, as these techniques stress bus bandwidth. In another study, Bennett and Flynn proposed a prediction cache that dynamically adapts between stream buffer and victim cache functionality based on program miss patterns [3].
Reference: [3] <author> J. E. Bennett and M. J. Flynn. </author> <title> Reducing Cache Miss Rates Using Prediction Caches. </title> <institution> Stanford University, CSL-TR-96-707, </institution> <year> 1996. </year>
Reference-contexts: They found that stream buffers and stride-based prefetch-ing do not have much impact on most SPEC92 programs, as these techniques stress bus bandwidth. In another study, Bennett and Flynn proposed a prediction cache that dynamically adapts between stream buffer and victim cache functionality based on program miss patterns <ref> [3] </ref>. The prediction cache dynamically adjusts the stream buffer prefetching distance based on the late prefetch pattern of the program.
Reference: [4] <author> D. Callahan, K. Kennedy, and A. Porterfield. </author> <title> Software Prefetching. </title> <booktitle> In Proceedings of the 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <year> 1991. </year>
Reference-contexts: With this technique, the compiler or programmer schedules an explicit prefetch instruction for a location that will be accessed by the processor at a later time, with the goal of bringing the location into the processor's cache before it issues a demand memory access <ref> [4] </ref>. Previous studies have shown that software-controlled non-binding prefetching can eliminate a large fraction of memory stall time in shared-memory multiprocessors [24, 33]. However, all such studies used previous-generation processors with single-issue, static scheduling, and blocking reads. <p> They assumed a small miss latency of 16 cycles. They found that both non-blocking reads and stream buffers improve performance; the best performance is achieved when both are combined. In the context of previous-generation systems, several studies have proposed algorithms for software-controlled non-binding prefetching <ref> [4, 16, 23, 24, 25] </ref>. Of these, Mowry et al. developed the most sophisticated algorithm and compiler implementation, with a comprehensive evaluation for both uniprocessors and shared-memory multiprocessors [23, 24, 25]. This algorithm is described in Section 2.1. Tullsen and Eggers evaluated software-controlled non-binding prefetching on a bus-based system [33].
Reference: [5] <author> T.-F. Chen and J.-L. Baer. </author> <title> Reducing Memory Latency via Non-Blocking and Prefetching Caches. </title> <booktitle> In Proceedings of the 5th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <year> 1992. </year>
Reference-contexts: Further, they assume a small read latency of 10 cycles. Their results show prefetching to be effective with multiple issue uniprocessors. Chen and Baer compared the performance of non-blocking reads and hardware prefetching on a statically-scheduled single-issue uniprocessor <ref> [5] </ref>. They found that their hardware prefetching scheme generally performed better than non-blocking reads and is less sensitive to memory latency. They also studied the combination of non-blocking reads and hardware prefetching for two applications and found it to perform better than either technique alone.
Reference: [6] <author> W. Y. Chen et al. </author> <title> Data Access Microarchitectures for Superscalar Processors with Compiler-Assisted Data Prefetching. </title> <booktitle> In Proceedings of the 24th Annual International Symposisum on Microarchitecture, </booktitle> <year> 1991. </year>
Reference-contexts: Other previous studies on prefetching with ILP unipro-cessors examine statically-scheduled processors. Chen et al. investigated the use of software-controlled prefetching on statically-scheduled multiple-issue uniprocessors for non-numeric applications <ref> [6] </ref>. This work seems to allow only one outstanding read (but several outstanding prefetches). Further, they assume a small read latency of 10 cycles. Their results show prefetching to be effective with multiple issue uniprocessors.
Reference: [7] <author> K. Farkas, N. Jouppi, and P. Chow. </author> <title> How Useful are Non-Blocking Loads, </title> <booktitle> Stream Buffers and Speculative Execution in Multiple Issue Processors? In Proceedings of the 1st International Conference on High-Performance Computer Architecture, </booktitle> <year> 1995. </year>
Reference-contexts: They also studied the combination of non-blocking reads and hardware prefetching for two applications and found it to perform better than either technique alone. Farkas et al. studied the impact of non-blocking reads and stream buffers with statically-scheduled multiple-issue uniprocessors <ref> [7] </ref>. They assumed a small miss latency of 16 cycles. They found that both non-blocking reads and stream buffers improve performance; the best performance is achieved when both are combined. In the context of previous-generation systems, several studies have proposed algorithms for software-controlled non-binding prefetching [4, 16, 23, 24, 25].
Reference: [8] <author> K. Fletcher. </author> <title> Compiler-hardware cooperation in prefetching for shared-memory multiprocessors. </title> <type> Ph.D. Thesis Proposal, </type> <institution> Rice University, </institution> <month> September </month> <year> 1995. </year>
Reference-contexts: The prefetch-ing algorithm can be modified to determine the prefetch distance for each access based on the predicted latency of the access (e.g., by using information about the data layout). Such an approach has been discussed in other studies for software and hardware prefetching <ref> [8, 13, 21] </ref>.
Reference: [9] <author> K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> Performance Evaluation of Memory Consistency Models for Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the 4th Inerna-tional Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <year> 1991. </year>
Reference-contexts: These systems are identical in every respect except that Simple uses previous-generation processors while ILP uses state-of-the-art ILP processors. For this part, we assume RC for both systems since RC has been shown to have the best performance (compared to SC or processor consistency) for these systems <ref> [9, 11, 28] </ref>. We find that for three out of our five applications, current software prefetching methods achieve a significant reduction in total execution time (12% to 31%) on ILP. <p> For our second goal, we compare the performance benefits of prefetching on ILP multiprocessors that implement SC and RC. For SC, we consider a straightforward implementation as well as an optimized implementation incorporating write buffering, speculative reads, and hardware prefetching from the instruction window <ref> [9, 10, 28] </ref>. We find that even with software prefetching, for three applications, RC provides significant reductions in execution time (15% to 40%) compared to the most optimized version of SC. The effectiveness of software prefetching in SC implementations is limited for reasons similar to the RC system. <p> of L1 MSHRs 8 L2 cache (off-chip) 4-way associative, 64 KB L2 request ports 1 L2 hit time 8 cycles, pipelined Number of L2 MSHRs 8 Memory parameters Memory access time 18 cycles (60 ns) Memory transfer bandwidth 16 bytes/cycle Memory interleaving 4-way speculative reads [10, 28], and write buffering <ref> [9] </ref>. The first two techniques, hardware prefetching and speculative reads, exploit the instruction-lookahead window and speculation support available in ILP processors. The hardware prefetch-ing technique issues a non-binding prefetch for a decoded memory instruction in the instruction issue window once the address of the instruction is computed. <p> This section uses RC for both ILP and Simple, since RC has been shown to provide better performance than SC for both types of systems <ref> [9, 11, 28] </ref>. Figures 4 and 5 present the key results from our experiments. Throughout Figure 4, +PF indicates the addition of software prefetching. Figure 4 (a) shows the execution times for each application on Simple and ILP, both with and without software prefetching.
Reference: [10] <author> K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> Two Techniques to Enhance the Performance of Memory Consistency Models. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <year> 1991. </year>
Reference-contexts: Studies on previous-generation multiprocessors have shown that software prefetching can improve the performance of both SC and RC [14]. Systems with ILP processors, however, can incorporate hardware optimizations such as speculative reads and hardware prefetch-ing from the instruction window to improve the performance of SC <ref> [10] </ref>. A recent study has shown that such techniques substantially narrow the performance gap between SC and RC, but a significant gap remains for some applications [28]. <p> For our second goal, we compare the performance benefits of prefetching on ILP multiprocessors that implement SC and RC. For SC, we consider a straightforward implementation as well as an optimized implementation incorporating write buffering, speculative reads, and hardware prefetching from the instruction window <ref> [9, 10, 28] </ref>. We find that even with software prefetching, for three applications, RC provides significant reductions in execution time (15% to 40%) compared to the most optimized version of SC. The effectiveness of software prefetching in SC implementations is limited for reasons similar to the RC system. <p> SCplain is a naive implementation that enforces memory ordering by stalling the issue of a memory operation until the previous memory operation of that processor has completed. SCopt is a more aggressive implementation that improves performance through three hardware techniques, hardware prefetching of writes from the instruction window <ref> [10, 28] </ref>, 2 To appear in Proceedings of ISCA-24 (June, 1997) ILP Processor Processor speed 300MHz Maximum fetch/decode/retire rate 4 (instructions per cycle) Instruction window 64 entries Functional units 2 integer arithmetic 2 floating point 2 address generation Simultaneous speculated branches 8 Maximum instructions in memory queue 32 Network parameters Network <p> hit time 1 cycle Number of L1 MSHRs 8 L2 cache (off-chip) 4-way associative, 64 KB L2 request ports 1 L2 hit time 8 cycles, pipelined Number of L2 MSHRs 8 Memory parameters Memory access time 18 cycles (60 ns) Memory transfer bandwidth 16 bytes/cycle Memory interleaving 4-way speculative reads <ref> [10, 28] </ref>, and write buffering [9]. The first two techniques, hardware prefetching and speculative reads, exploit the instruction-lookahead window and speculation support available in ILP processors. <p> As with software prefetch-ing, we do not drop hardware prefetches in SCopt even if their issue is blocked due to resource constraints. The SCopt processor includes a speculative load buffer to monitor outstanding speculative reads <ref> [10] </ref> and employs a mechanism similar to that used in the MIPS R10000 [22] to recover when possible consistency violations are detected. 3 To appear in Proceedings of ISCA-24 (June, 1997) 3.1.2 Memory Hierarchy and Multiprocessor Configura tion We simulate a hardware cache-coherent, non-uniform memory access (CC-NUMA) shared-memory multiprocessor using an
Reference: [11] <author> K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> Hiding Memory Latency Using Dynamic Scheduling in Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <year> 1992. </year>
Reference-contexts: These systems are identical in every respect except that Simple uses previous-generation processors while ILP uses state-of-the-art ILP processors. For this part, we assume RC for both systems since RC has been shown to have the best performance (compared to SC or processor consistency) for these systems <ref> [9, 11, 28] </ref>. We find that for three out of our five applications, current software prefetching methods achieve a significant reduction in total execution time (12% to 31%) on ILP. <p> This section uses RC for both ILP and Simple, since RC has been shown to provide better performance than SC for both types of systems <ref> [9, 11, 28] </ref>. Figures 4 and 5 present the key results from our experiments. Throughout Figure 4, +PF indicates the addition of software prefetching. Figure 4 (a) shows the execution times for each application on Simple and ILP, both with and without software prefetching.
Reference: [12] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory Consistency and Event Ordering in Scalable Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <year> 1990. </year>
Reference-contexts: Write buffering allows multiple writes to be retired before issue. A read following such writes, however, must wait for these writes to complete before the read can retire from the instruction window. The release consistency model (RC) <ref> [12] </ref> allows more overlap and reordering of memory operations than SC, albeit at the cost of greater programming complexity. RC distinguishes between data and synchronization operations, allowing data operations to be reordered with respect to one another. We only study a straightforward implementation of RC.
Reference: [13] <author> E. H. Gornish. </author> <title> Adaptive and Integrated Data Cache Prefetching for Shared-Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1995. </year>
Reference-contexts: The prefetch-ing algorithm can be modified to determine the prefetch distance for each access based on the predicted latency of the access (e.g., by using information about the data layout). Such an approach has been discussed in other studies for software and hardware prefetching <ref> [8, 13, 21] </ref>. <p> Gornish evaluated binding prefetching for both single-issue and multiple-issue statically-scheduled multiprocessors with non-blocking reads <ref> [13] </ref>. The binding prefetches rely on software cache-coherence and require complete cache flushes before and after each parallel loop. Processor pipelines and functional-unit contention are not modeled. This work integrates hardware and software prefetching support, dynamically adapting hardware prefetching distance according to the latency of each reference.
Reference: [14] <author> A. Gupta, J. Hennessy, K. Gharachorloo, T. Mowry, and W.- D. Weber. </author> <title> Comparative Evaluation of Latency Reducing and Tolerating Techniques. </title> <booktitle> In Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <year> 1991. </year>
Reference-contexts: Relaxed memory consistency models such as release consistency (RC) can potentially tolerate more memory latency than the simple model of sequential consistency (SC), but mandate a more complex programming model. Studies on previous-generation multiprocessors have shown that software prefetching can improve the performance of both SC and RC <ref> [14] </ref>. Systems with ILP processors, however, can incorporate hardware optimizations such as speculative reads and hardware prefetch-ing from the instruction window to improve the performance of SC [10]. <p> We found this to be true for our ILP-based system as well (Section 5.1). Gupta et al. evaluated the interaction between software-controlled prefetching and straightforward implementations of consistency models for systems with simple proces sors <ref> [14] </ref>. They found that prefetching significantly improved the performance of both SC and RC.
Reference: [15] <author> M. D. Hill, J. R. Larus, S. K. Reinhardt, and D. A. Wood. </author> <title> Cooperative Shared Memory: Software and Hardware Support for Scalable Multiprocessors. </title> <booktitle> In Proceedings of the 5th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <year> 1992. </year>
Reference-contexts: These results show that further techniques are needed to alleviate the effects of late prefetches and resource contention. Latency-reducing techniques such as producer-initiated communication primitives <ref> [1, 15, 29] </ref> appear promising.
Reference: [16] <author> A. C. Klaiber and H. M. Levy. </author> <title> An Architecture for Software-Controlled Data Prefetching. </title> <booktitle> In Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <year> 1991. </year>
Reference-contexts: They assumed a small miss latency of 16 cycles. They found that both non-blocking reads and stream buffers improve performance; the best performance is achieved when both are combined. In the context of previous-generation systems, several studies have proposed algorithms for software-controlled non-binding prefetching <ref> [4, 16, 23, 24, 25] </ref>. Of these, Mowry et al. developed the most sophisticated algorithm and compiler implementation, with a comprehensive evaluation for both uniprocessors and shared-memory multiprocessors [23, 24, 25]. This algorithm is described in Section 2.1. Tullsen and Eggers evaluated software-controlled non-binding prefetching on a bus-based system [33].
Reference: [17] <author> D. Kroft. </author> <title> Lockup-Free Instruction Fetch/Prefetch Cache Organization. </title> <booktitle> In Proceedings of the 8th International Symposium on Computer Architecture, </booktitle> <year> 1981. </year>
Reference-contexts: A split-transaction bus connects the network interface, directory controller, and the rest of the system node. The nodes are connected using a two-dimensional mesh network. Figure 1 summarizes the memory system parameters. Both caches are non-blocking with 8 miss status holding registers (MSHRs) <ref> [17] </ref> each. The MSHRs store information about the misses and coalesce multiple requests to the same cache line.
Reference: [18] <author> L. Lamport. </author> <title> How to Make a Multiprocessor Computer that Correctly Executes Multiprocess Programs. </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-28(9):690-691, </volume> <year> 1979. </year>
Reference-contexts: Sequential consistency (SC) <ref> [18] </ref> guarantees that all memory operations appear to execute in program order and hence offers a simple and intuitive programming model. We examine two implementations of sequential consistency - SCplain and SCopt.
Reference: [19] <author> C.-K. Luk and T. C. Mowry. </author> <title> Compiler-Based Prefetching for Recursive Data Structures. </title> <booktitle> In Proceedings of the 7th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <year> 1996. </year>
Reference-contexts: Three recent studies on prefetching in uniprocessors examine state-of-the-art uniprocessors with multiple issue, dynamic scheduling, and non-blocking reads. Luk and Mowry proposed software prefetching algorithms for pointer-based data structures and evaluated their algorithms for an ILP processor similar to the MIPS R10000 <ref> [19] </ref>. Bennett and Flynn compared stream buffers (a form of hardware-controlled prefetching), hardware stride-based prefetching, and victim caches for state-of-the-art ILP processors [2]. They found that stream buffers and stride-based prefetch-ing do not have much impact on most SPEC92 programs, as these techniques stress bus bandwidth.
Reference: [20] <author> N. McIntosh. </author> <type> Private communication. </type> <institution> Rice University, </institution> <month> February </month> <year> 1997. </year>
Reference-contexts: Second, adding outer-loop prefetching introduces inter-block cache conflicts. 5.2 Reducing Resource Contention Although the base architecture used in Section 4 is aggressive in processor, network, and memory system resources, some applications still observe reduced benefits 1 McIntosh has simultaneously developed a similar algorithm for a High Performance Fortran compiler <ref> [20] </ref>. from prefetching on ILP due to increased resource contention. We next re-examine two assumptions in our prefetch strategy that can affect resource contention with ILP. L2 prefetching. The prefetch strategy used in Section 4 prefetched data into the L1 cache.
Reference: [21] <author> N. McIntosh, K. Fletcher, K. Cooper, and K. Kennedy. </author> <title> Compiler Techniques for Software Prefetching on Cache-Coherent Shared-Memory Multiprocessors. Center for Research on Parallel Computation, </title> <institution> Rice University, CRPC-TR96675-S, </institution> <year> 1997. </year>
Reference-contexts: The prefetch-ing algorithm can be modified to determine the prefetch distance for each access based on the predicted latency of the access (e.g., by using information about the data layout). Such an approach has been discussed in other studies for software and hardware prefetching <ref> [8, 13, 21] </ref>.
Reference: [22] <institution> MIPS Technologies, Inc. </institution> <note> R10000 Microprocessor User's Manual, Version 1.1, </note> <year> 1996. </year>
Reference-contexts: Our implementation of the processor core resembles the MIPS R10000 microarchitecture <ref> [22] </ref>, but also includes aggressive features from other architectures. Default parameters for the processor are listed in Figure 1. The Simple system uses previous-generation statically-scheduled, single-issue processors with blocking reads. Simple processors rely on compilers to schedule instructions to hide functional unit latencies. <p> As with software prefetch-ing, we do not drop hardware prefetches in SCopt even if their issue is blocked due to resource constraints. The SCopt processor includes a speculative load buffer to monitor outstanding speculative reads [10] and employs a mechanism similar to that used in the MIPS R10000 <ref> [22] </ref> to recover when possible consistency violations are detected. 3 To appear in Proceedings of ISCA-24 (June, 1997) 3.1.2 Memory Hierarchy and Multiprocessor Configura tion We simulate a hardware cache-coherent, non-uniform memory access (CC-NUMA) shared-memory multiprocessor using an invalidation-based, three-state directory coherence protocol.
Reference: [23] <author> T. Mowry. </author> <title> Tolerating Latency through Software-Controlled Data Prefetching. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1994. </year>
Reference-contexts: We insert prefetches in the applications by hand, following the currently best known compiler prefetching algorithm <ref> [23] </ref>. We run the applications on a detailed execution-driven simulator modeling shared-memory multiprocessors with state-of-the-art ILP processors. For our first goal, we compare the performance benefits of software prefetching on two multiprocessor systems referred to as Simple and ILP. <p> Section 8 concludes the paper. 2 Background Sections 2.1 and 2.2 respectively describe the base software prefetching algorithm and memory consistency implementations we use in this study. 2.1 Software Prefetching Algorithm The best known software prefetch insertion algorithm implemented in a compiler is by Mowry et al. <ref> [23, 24] </ref>. We describe this algorithm below. Section 3.4 describes how we use it to insert prefetches in our applications. The algorithm is loop-based, and consists of an analysis phase and a scheduling phase. <p> Prefetches are not dropped even if resource constraints block their issue, and prefetched lines are brought into the highest level of the memory hierarchy. These two assumptions follow previous work <ref> [23] </ref>, and are re-evaluated in Section 5. Both processor models support RC, using the SPARC V9 MEMBAR fence instructions to impose ordering at synchronization points [32]. The ILP processor additionally supports the two implementations of sequential consistency described in Section 2.2 - SCplain and SCopt. <p> Prefetch drop strategy. Following previous work, our base prefetch strategy does not drop prefetches even when resource constraints block their issue <ref> [23] </ref>. We examined an alternate strategy that drops prefetches when the L1 cache MSHRs are saturated. Our results show less than 2% difference in execution time on all our applications. <p> Our results show less than 2% difference in execution time on all our applications. Dropping prefetches does not give greater performance improvements because any advantages in reducing resource contention are offset by greater latencies seen by subsequent demand misses, as in studies with previous-generation processors <ref> [23] </ref>. 5.3 Summary and Implications The results of this section show that straightforward modifications to the prefetching algorithm to reduce late prefetches do not necessarily translate to more effective prefetching in 9 To appear in Proceedings of ISCA-24 (June, 1997) our applications because of increased early prefetches, inadequate computation, and/or increased <p> They assumed a small miss latency of 16 cycles. They found that both non-blocking reads and stream buffers improve performance; the best performance is achieved when both are combined. In the context of previous-generation systems, several studies have proposed algorithms for software-controlled non-binding prefetching <ref> [4, 16, 23, 24, 25] </ref>. Of these, Mowry et al. developed the most sophisticated algorithm and compiler implementation, with a comprehensive evaluation for both uniprocessors and shared-memory multiprocessors [23, 24, 25]. This algorithm is described in Section 2.1. Tullsen and Eggers evaluated software-controlled non-binding prefetching on a bus-based system [33]. <p> In the context of previous-generation systems, several studies have proposed algorithms for software-controlled non-binding prefetching [4, 16, 23, 24, 25]. Of these, Mowry et al. developed the most sophisticated algorithm and compiler implementation, with a comprehensive evaluation for both uniprocessors and shared-memory multiprocessors <ref> [23, 24, 25] </ref>. This algorithm is described in Section 2.1. Tullsen and Eggers evaluated software-controlled non-binding prefetching on a bus-based system [33]. They characterized bandwidth needs of their applications, and found that the benefits of prefetching degrade as bandwidth needs increase.
Reference: [24] <author> T. Mowry and A. Gupta. </author> <title> Tolerating Latency Through Software-Controlled Prefetching in Shared-Memory Multiprocessors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 12(2) </volume> <pages> 87-106, </pages> <year> 1991. </year>
Reference-contexts: Previous studies have shown that software-controlled non-binding prefetching can eliminate a large fraction of memory stall time in shared-memory multiprocessors <ref> [24, 33] </ref>. However, all such studies used previous-generation processors with single-issue, static scheduling, and blocking reads. Consequently, such studies do not account for the interactions between software prefetching and the other latency-tolerating techniques already incorporated in ILP-based multiprocessors. <p> Section 8 concludes the paper. 2 Background Sections 2.1 and 2.2 respectively describe the base software prefetching algorithm and memory consistency implementations we use in this study. 2.1 Software Prefetching Algorithm The best known software prefetch insertion algorithm implemented in a compiler is by Mowry et al. <ref> [23, 24] </ref>. We describe this algorithm below. Section 3.4 describes how we use it to insert prefetches in our applications. The algorithm is loop-based, and consists of an analysis phase and a scheduling phase. <p> They assumed a small miss latency of 16 cycles. They found that both non-blocking reads and stream buffers improve performance; the best performance is achieved when both are combined. In the context of previous-generation systems, several studies have proposed algorithms for software-controlled non-binding prefetching <ref> [4, 16, 23, 24, 25] </ref>. Of these, Mowry et al. developed the most sophisticated algorithm and compiler implementation, with a comprehensive evaluation for both uniprocessors and shared-memory multiprocessors [23, 24, 25]. This algorithm is described in Section 2.1. Tullsen and Eggers evaluated software-controlled non-binding prefetching on a bus-based system [33]. <p> In the context of previous-generation systems, several studies have proposed algorithms for software-controlled non-binding prefetching [4, 16, 23, 24, 25]. Of these, Mowry et al. developed the most sophisticated algorithm and compiler implementation, with a comprehensive evaluation for both uniprocessors and shared-memory multiprocessors <ref> [23, 24, 25] </ref>. This algorithm is described in Section 2.1. Tullsen and Eggers evaluated software-controlled non-binding prefetching on a bus-based system [33]. They characterized bandwidth needs of their applications, and found that the benefits of prefetching degrade as bandwidth needs increase.
Reference: [25] <author> T. C. Mowry, M. S. Lam, and A. Gupta. </author> <title> Design and Evaluation of a Compiler Algorithm for Prefetching. </title> <booktitle> In Proceedings of the 5th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <year> 1992. </year>
Reference-contexts: They assumed a small miss latency of 16 cycles. They found that both non-blocking reads and stream buffers improve performance; the best performance is achieved when both are combined. In the context of previous-generation systems, several studies have proposed algorithms for software-controlled non-binding prefetching <ref> [4, 16, 23, 24, 25] </ref>. Of these, Mowry et al. developed the most sophisticated algorithm and compiler implementation, with a comprehensive evaluation for both uniprocessors and shared-memory multiprocessors [23, 24, 25]. This algorithm is described in Section 2.1. Tullsen and Eggers evaluated software-controlled non-binding prefetching on a bus-based system [33]. <p> In the context of previous-generation systems, several studies have proposed algorithms for software-controlled non-binding prefetching [4, 16, 23, 24, 25]. Of these, Mowry et al. developed the most sophisticated algorithm and compiler implementation, with a comprehensive evaluation for both uniprocessors and shared-memory multiprocessors <ref> [23, 24, 25] </ref>. This algorithm is described in Section 2.1. Tullsen and Eggers evaluated software-controlled non-binding prefetching on a bus-based system [33]. They characterized bandwidth needs of their applications, and found that the benefits of prefetching degrade as bandwidth needs increase.
Reference: [26] <author> V. S. Pai, P. Ranganathan, and S. V. Adve. RSIM: </author> <title> An Execution-Driven Simulator for ILP-Based Shared-Memory Multiprocessors and Uniprocessors. </title> <booktitle> In Proceedings of the 3rd Workshop on Computer Architecture Education, </booktitle> <year> 1997. </year>
Reference-contexts: 8 Water 512 molecules 16 Radix 1024 radix, 512K keys, max 512K 8 result in the replacement of a line needed by a demand access as damaging; all prefetch types other than unnecessary prefetches can also be damaging. 3.3 Simulation Environment We use the Rice Simulator for ILP Multiprocessors (RSIM) <ref> [26] </ref> to model the Simple and ILP systems described in the previous sections. RSIM models the processors, memory system, and interconnection network in detail, including contention at all resources. Specifically, unlike current direct-execution simulators, we accurately model the details of the processor pipelines.
Reference: [27] <author> V. S. Pai, P. Ranganathan, and S. V. Adve. </author> <title> The Impact of Instruction Level Parallelism on Multiprocessor Performance and Simulation Methodology. </title> <booktitle> In Proceedings of the 3rd International Symposium on High Performance Computer Architecture, </booktitle> <year> 1997. </year>
Reference-contexts: Request permissions from Publications Dept, ACM Inc., fax +1 (212) 869-0481, or permissions@acm.org. while these features significantly improve the performance of computation, memory system performance remains a key bottleneck in multiprocessors <ref> [27] </ref>. To reduce memory stall time, many current processors support software-controlled non-binding prefetching. <p> We divide the execution time into three components - CPU, data memory, and synchronization. We use the following convention to account for stall cycles <ref> [27, 28, 30] </ref>. All cycles where the processor retires the maximum number of instructions allowed by the architecture are considered busy time. Otherwise, we charge the cycle to the stall time component of the first instruction that could not be retired that cycle. <p> The second version, LUopt, additionally uses loop-interchange transformations to move read misses closer to each other. This increases the number of read misses that overlap with each other, better exploiting the support for non-blocking reads in ILP <ref> [27] </ref>. LUopt achieves higher performance than LUorig on ILP without prefetching, but is outperformed by LUorig with the addition of prefetching. We therefore report the results of both versions in Section 4; subsequent sections use only LUorig. Prefetching in LU covers all loop nests in the application. <p> ILP features like multiple issue and dynamic scheduling provide greater improvements for computation time than for the memory stall time on all our applications <ref> [27] </ref>. As a result, memory stall time contributes far more to execution time in ILP than in Simple. Thus, even a smaller fraction of memory stall time reduced in ILP can lead to a reduction in overall execution time similar to or greater than that seen in Simple.
Reference: [28] <author> V. S. Pai, P. Ranganathan, S. V. Adve, and T. Harton. </author> <title> An Evaluation of Memory Consistency Models for Shared-Memory Systems with ILP Processors. </title> <booktitle> In Proceedings of the 7th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <year> 1996. </year>
Reference-contexts: A recent study has shown that such techniques substantially narrow the performance gap between SC and RC, but a significant gap remains for some applications <ref> [28] </ref>. Since software prefetching targets the same latencies as RC, it is important to determine if software prefetching can eliminate the remaining gap between SC and RC on current multiprocessors, allowing the high performance of RC with the simple programming model of SC. <p> These systems are identical in every respect except that Simple uses previous-generation processors while ILP uses state-of-the-art ILP processors. For this part, we assume RC for both systems since RC has been shown to have the best performance (compared to SC or processor consistency) for these systems <ref> [9, 11, 28] </ref>. We find that for three out of our five applications, current software prefetching methods achieve a significant reduction in total execution time (12% to 31%) on ILP. <p> For our second goal, we compare the performance benefits of prefetching on ILP multiprocessors that implement SC and RC. For SC, we consider a straightforward implementation as well as an optimized implementation incorporating write buffering, speculative reads, and hardware prefetching from the instruction window <ref> [9, 10, 28] </ref>. We find that even with software prefetching, for three applications, RC provides significant reductions in execution time (15% to 40%) compared to the most optimized version of SC. The effectiveness of software prefetching in SC implementations is limited for reasons similar to the RC system. <p> SCplain is a naive implementation that enforces memory ordering by stalling the issue of a memory operation until the previous memory operation of that processor has completed. SCopt is a more aggressive implementation that improves performance through three hardware techniques, hardware prefetching of writes from the instruction window <ref> [10, 28] </ref>, 2 To appear in Proceedings of ISCA-24 (June, 1997) ILP Processor Processor speed 300MHz Maximum fetch/decode/retire rate 4 (instructions per cycle) Instruction window 64 entries Functional units 2 integer arithmetic 2 floating point 2 address generation Simultaneous speculated branches 8 Maximum instructions in memory queue 32 Network parameters Network <p> hit time 1 cycle Number of L1 MSHRs 8 L2 cache (off-chip) 4-way associative, 64 KB L2 request ports 1 L2 hit time 8 cycles, pipelined Number of L2 MSHRs 8 Memory parameters Memory access time 18 cycles (60 ns) Memory transfer bandwidth 16 bytes/cycle Memory interleaving 4-way speculative reads <ref> [10, 28] </ref>, and write buffering [9]. The first two techniques, hardware prefetching and speculative reads, exploit the instruction-lookahead window and speculation support available in ILP processors. <p> We only study a straightforward implementation of RC. We do not consider a corresponding RCopt implementation, as previous work has shown that hardware prefetching and speculative reads do not significantly affect the performance of RC for our applications <ref> [28] </ref>, and RC already implements a superset of the write buffering optimization. <p> We divide the execution time into three components - CPU, data memory, and synchronization. We use the following convention to account for stall cycles <ref> [27, 28, 30] </ref>. All cycles where the processor retires the maximum number of instructions allowed by the architecture are considered busy time. Otherwise, we charge the cycle to the stall time component of the first instruction that could not be retired that cycle. <p> This section uses RC for both ILP and Simple, since RC has been shown to provide better performance than SC for both types of systems <ref> [9, 11, 28] </ref>. Figures 4 and 5 present the key results from our experiments. Throughout Figure 4, +PF indicates the addition of software prefetching. Figure 4 (a) shows the execution times for each application on Simple and ILP, both with and without software prefetching. <p> for the remaining memory stall time in SC systems after adding software prefetching and the reasons for the remaining performance difference between SC and RC. 6.2 Memory Stall Time Reduction in SC Systems Without software prefetching, the SC implementations see larger memory stall time than RC due to consistency constraints <ref> [28] </ref>. Software prefetching can address much of this additional time and thus can lead to larger reductions in memory stall time than in RC.
Reference: [29] <author> D. Poulsen. </author> <title> Memory Latency Reduction via Data Prefetch-ing and Data Forwarding in Shared-Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1994. </year>
Reference-contexts: These results show that further techniques are needed to alleviate the effects of late prefetches and resource contention. Latency-reducing techniques such as producer-initiated communication primitives <ref> [1, 15, 29] </ref> appear promising.
Reference: [30] <author> M. Rosenblum, E. Bugnion, S. A. Herrod, E. Witchel, and A. Gupta. </author> <title> The Impact of Architectural Trends on Operating System Performance. </title> <booktitle> In Proceedings of the 15th ACM Symposium on Operating Systems Principles, </booktitle> <year> 1995. </year>
Reference-contexts: We divide the execution time into three components - CPU, data memory, and synchronization. We use the following convention to account for stall cycles <ref> [27, 28, 30] </ref>. All cycles where the processor retires the maximum number of instructions allowed by the architecture are considered busy time. Otherwise, we charge the cycle to the stall time component of the first instruction that could not be retired that cycle.
Reference: [31] <author> J. P. Singh, W.-D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <year> 1992. </year>
Reference-contexts: However, we do model contention due to private data accesses at various processor and cache resources. 3.4 Applications and Prefetching Methodology We use five applications in this study Radix, LU, and FFT from the SPLASH-2 suite [34], and Water and Mp3d from the SPLASH suite <ref> [31] </ref>. Since we do not have a compiler that implements software-prefetching for C programs, we insert prefetches by hand, following the algorithm by Mowry et al. (Section 2.1) for all applications except Water. Prefetching for Water is described further below.
Reference: [32] <author> Sparc International. </author> <title> The SPARC Architecture Manual, </title> <type> Version 9, </type> <year> 1993. </year>
Reference-contexts: These two assumptions follow previous work [23], and are re-evaluated in Section 5. Both processor models support RC, using the SPARC V9 MEMBAR fence instructions to impose ordering at synchronization points <ref> [32] </ref>. The ILP processor additionally supports the two implementations of sequential consistency described in Section 2.2 - SCplain and SCopt. As with software prefetch-ing, we do not drop hardware prefetches in SCopt even if their issue is blocked due to resource constraints.
Reference: [33] <author> D. Tullsen and S. Eggers. </author> <title> Effective Cache Prefetching on Bus-Based Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 13(1) </volume> <pages> 57-88, </pages> <year> 1995. </year>
Reference-contexts: Previous studies have shown that software-controlled non-binding prefetching can eliminate a large fraction of memory stall time in shared-memory multiprocessors <ref> [24, 33] </ref>. However, all such studies used previous-generation processors with single-issue, static scheduling, and blocking reads. Consequently, such studies do not account for the interactions between software prefetching and the other latency-tolerating techniques already incorporated in ILP-based multiprocessors. <p> Many prefetches arrive at the cache much before the demand access. These are vulnerable to cache replacements or invalidations for a longer time, as also observed in studies of previous-generation multiprocessors <ref> [33] </ref>. In Mp3d, these early prefetches hurt performance because they prematurely invalidate other processors' cache lines (due to false and true sharing). In Radix, most early prefetches are replaced prefetches that do not adversely affect other processors. <p> Of these, Mowry et al. developed the most sophisticated algorithm and compiler implementation, with a comprehensive evaluation for both uniprocessors and shared-memory multiprocessors [23, 24, 25]. This algorithm is described in Section 2.1. Tullsen and Eggers evaluated software-controlled non-binding prefetching on a bus-based system <ref> [33] </ref>. They characterized bandwidth needs of their applications, and found that the benefits of prefetching degrade as bandwidth needs increase. They also found that increasing the prefetch distance can reduce late prefetches but can also increase early prefetches, and thus does not significantly improve performance.
Reference: [34] <author> S. C. Woo, M. Ohara, E. Torrie, J. P. Singh, and A. Gupta. </author> <title> The SPLASH-2 Programs: Characterization and Methodological Considerations. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <year> 1995. </year> <month> 13 </month>
Reference-contexts: The L2 cache is a fully pipelined, write-allocate write-back cache. The L1 cache size is 16 KB and the L2 cache size is 64 KB. These sizes are chosen based on the input sizes of our applications (described in Section 3.4), following the methodology described by Woo et al. <ref> [34] </ref>. The primary working sets for our applications fit in the L1 cache, while the secondary working sets do not fit in the L2 cache. <p> However, we do model contention due to private data accesses at various processor and cache resources. 3.4 Applications and Prefetching Methodology We use five applications in this study Radix, LU, and FFT from the SPLASH-2 suite <ref> [34] </ref>, and Water and Mp3d from the SPLASH suite [31]. Since we do not have a compiler that implements software-prefetching for C programs, we insert prefetches by hand, following the algorithm by Mowry et al. (Section 2.1) for all applications except Water. Prefetching for Water is described further below.
References-found: 34

