URL: ftp://ftp.cs.rochester.edu/pub/papers/robotics/97.tr647.Plenoptic_image_editing.ps
Refering-URL: http://www.cs.wisc.edu/computer-vision/pubs.html
Root-URL: 
Title: Plenoptic Image Editing  
Author: Steven M. Seitz Kiriakos N. Kutulakos 
Address: Madison, WI53706  Rochester, NY14607  
Affiliation: Computer Sciences Department University of Wisconsin-Madison  Department of Computer Science University of Rochester  
Abstract: This paper presents a new class of interactive image editing operations designed to maintain consistency between multiple images of a physical 3D object. The distinguishing feature of these operations is that edits to any one image propagate automatically to all other images as if the (unknown) 3D object had itself been modified. The approach is useful first as a power-assist that enables a user to quickly modify many images by editing just a few, and second as a means for constructing and editing image-based scene representations by manipulating a set of photographs. The approach works by extending operations like image painting, scissoring, and morphing so that they alter an object's plenoptic function in a physically-consistent way, thereby affecting object appearance from all viewpoints simultaneously. A key element in realizing these operations is a new volumetric decomposition technique for reconstructing an object's plenoptic function from an incomplete set of camera viewpoints. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. H. Adelson and J. R. Bergen, </author> <title> The plenoptic function and the elements of early vision, in Computation Models of Visual Processing (M. </title> <editor> Landy and J. A. Movshon, eds.), </editor> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: We use the adjective plenoptic to describe image editing operations of this type that globally affect an object's appearance under changes in viewpoint and lighting. Plenop-tic image operations can be best understood in terms of the plenoptic function <ref> [1, 2] </ref>, which provides a complete description of an object's appearance as a function of viewpoint. Within this framework, our goal is to determine how an image editing operation such as painting, scissoring, or morphing should modify an object's plenoptic function. <p> The plenoptic function of a 3D scene describes the flow of light along every oriented ray in space, and encodes the scene's appearance from every direction <ref> [1, 5, 6] </ref>. While the plenoptic function is determined uniquely by the 3D surfaces in a scene and their reflectance properties, we can generate the same plenoptic function by combining many different shapes and radiance functions (Figure 2).
Reference: [2] <author> L. McMillan and G. Bishop, </author> <title> Plenoptic modeling: An image-based rendering system, </title> <booktitle> in Proc. SIG-GRAPH'95, </booktitle> <pages> pp. 39-46, </pages> <year> 1995. </year>
Reference-contexts: We use the adjective plenoptic to describe image editing operations of this type that globally affect an object's appearance under changes in viewpoint and lighting. Plenop-tic image operations can be best understood in terms of the plenoptic function <ref> [1, 2] </ref>, which provides a complete description of an object's appearance as a function of viewpoint. Within this framework, our goal is to determine how an image editing operation such as painting, scissoring, or morphing should modify an object's plenoptic function. <p> A primary advantage of these models is that they use elegant ray-based formulations to avoid difficult pixel correspondence problems that arise in other approaches <ref> [2, 3, 7] </ref>. Performing image editing operations within a distributed representation is difficult, however, for two reasons. First, local image modifications can affect object appearance from disparate views and may therefore require global changes to a distributed representation. <p> Our decomposition technique has significant advantages over previous correspondence-based approaches to image-based rendering. Previous approaches have relied on stereo vision techniques <ref> [2, 3, 7, 9, 10] </ref> or volume intersection [6, 11] to derive pixel correspondence information. Both types of methods have serious weaknessesexisting stereo techniques require that the input cameras be close together and minimize visibility changes (e.g., occlusions, changes in field of view). <p> Figure 4 (b) shows a strategy for the case of outward-facing cameras. This type of camera geometry is useful for acquiring panoramic scene visualizations, as in <ref> [2, 13] </ref>. One consistent set of layers corresponds to a series of rectangles radiating outward from the camera volume. Layer 0 is the axis-aligned bounding box of the camera centers and the rest of the layers are determined by uniformly expanding the box one unit at a time.
Reference: [3] <author> S. Laveau and O. Faugeras, </author> <title> 3-D scene representation as a collection of images, </title> <booktitle> in Proc. Int. Conf. on Pattern Recognition, </booktitle> <pages> pp. 689-691, </pages> <year> 1994. </year>
Reference-contexts: A primary advantage of these models is that they use elegant ray-based formulations to avoid difficult pixel correspondence problems that arise in other approaches <ref> [2, 3, 7] </ref>. Performing image editing operations within a distributed representation is difficult, however, for two reasons. First, local image modifications can affect object appearance from disparate views and may therefore require global changes to a distributed representation. <p> Our decomposition technique has significant advantages over previous correspondence-based approaches to image-based rendering. Previous approaches have relied on stereo vision techniques <ref> [2, 3, 7, 9, 10] </ref> or volume intersection [6, 11] to derive pixel correspondence information. Both types of methods have serious weaknessesexisting stereo techniques require that the input cameras be close together and minimize visibility changes (e.g., occlusions, changes in field of view).
Reference: [4] <author> S. E. Chen, </author> <title> QuicktimeVR an image-based approach to virtual environment navigation, </title> <booktitle> in Proc. SIGGRAPH'95, </booktitle> <pages> pp. 29-38, </pages> <year> 1995. </year>
Reference: [5] <author> M. Levoy and P. Hanrahan, </author> <title> Light field rendering, </title> <booktitle> in Proc. SIGGRAPH '96, </booktitle> <pages> pp. 31-42, </pages> <year> 1996. </year>
Reference-contexts: Indeed, this work has a new image-based rendering approach as a chief component. However, the ability to perform plenoptic editing operations adds new constraints on how the plenoptic function is represented. A direction in image-based rendering has been toward distributed plenoptic representations, in particular the Light Field <ref> [5] </ref> and Lumigraph [6], in which light rays are represented separately. A primary advantage of these models is that they use elegant ray-based formulations to avoid difficult pixel correspondence problems that arise in other approaches [2, 3, 7]. <p> The plenoptic function of a 3D scene describes the flow of light along every oriented ray in space, and encodes the scene's appearance from every direction <ref> [1, 5, 6] </ref>. While the plenoptic function is determined uniquely by the 3D surfaces in a scene and their reflectance properties, we can generate the same plenoptic function by combining many different shapes and radiance functions (Figure 2). <p> Unfortunately, in most plenoptic editing situations the true shape of the scene will be unknown. Representing the shape of a 3D scene as a flat hologram or a holographic cube <ref> [5, 6] </ref> leads to the other extreme of the shape-radiance complexity tradeoff: this choice does not involve any a priori assumptions about a scene's 3D shape but leads to decompositions that generate counter-intuitive correspondences across frames and possess incredibly complex radiance functionsin essense, it is virtually impossible to predict how the
Reference: [6] <author> S. J. Gortler, R. Grzeszczuk, R. Szeliski, and M. F. Cohen, </author> <booktitle> The lumigraph, in Proc. SIGGRAPH'96, </booktitle> <pages> pp. 43-54, </pages> <year> 1996. </year>
Reference-contexts: However, the ability to perform plenoptic editing operations adds new constraints on how the plenoptic function is represented. A direction in image-based rendering has been toward distributed plenoptic representations, in particular the Light Field [5] and Lumigraph <ref> [6] </ref>, in which light rays are represented separately. A primary advantage of these models is that they use elegant ray-based formulations to avoid difficult pixel correspondence problems that arise in other approaches [2, 3, 7]. Performing image editing operations within a distributed representation is difficult, however, for two reasons. <p> Our decomposition technique has significant advantages over previous correspondence-based approaches to image-based rendering. Previous approaches have relied on stereo vision techniques [2, 3, 7, 9, 10] or volume intersection <ref> [6, 11] </ref> to derive pixel correspondence information. Both types of methods have serious weaknessesexisting stereo techniques require that the input cameras be close together and minimize visibility changes (e.g., occlusions, changes in field of view). <p> The plenoptic function of a 3D scene describes the flow of light along every oriented ray in space, and encodes the scene's appearance from every direction <ref> [1, 5, 6] </ref>. While the plenoptic function is determined uniquely by the 3D surfaces in a scene and their reflectance properties, we can generate the same plenoptic function by combining many different shapes and radiance functions (Figure 2). <p> Unfortunately, in most plenoptic editing situations the true shape of the scene will be unknown. Representing the shape of a 3D scene as a flat hologram or a holographic cube <ref> [5, 6] </ref> leads to the other extreme of the shape-radiance complexity tradeoff: this choice does not involve any a priori assumptions about a scene's 3D shape but leads to decompositions that generate counter-intuitive correspondences across frames and possess incredibly complex radiance functionsin essense, it is virtually impossible to predict how the
Reference: [7] <author> P. E. Debevec, C. J. Taylor, and J. Malik, </author> <title> Modeling and rendering architecture from photographs: A hybrid geometry- and image-based approach, </title> <booktitle> in Proc. SIGGRAPH'96, </booktitle> <pages> pp. 11-20, </pages> <year> 1996. </year>
Reference-contexts: A primary advantage of these models is that they use elegant ray-based formulations to avoid difficult pixel correspondence problems that arise in other approaches <ref> [2, 3, 7] </ref>. Performing image editing operations within a distributed representation is difficult, however, for two reasons. First, local image modifications can affect object appearance from disparate views and may therefore require global changes to a distributed representation. <p> Our decomposition technique has significant advantages over previous correspondence-based approaches to image-based rendering. Previous approaches have relied on stereo vision techniques <ref> [2, 3, 7, 9, 10] </ref> or volume intersection [6, 11] to derive pixel correspondence information. Both types of methods have serious weaknessesexisting stereo techniques require that the input cameras be close together and minimize visibility changes (e.g., occlusions, changes in field of view). <p> In this section we describe how this feature is implemented using a constraint framework. Image constraints provided through user-interaction have proven very powerful in solving difficult correspondence tasks. Debevec et al. <ref> [7] </ref> combined user-assisted placement of 3D shape primitives with camera calibration and stereo techniques to enable reconstructions of architectural scenes. Similarly, image morphing methods [14, 15] exploit user-provided feature correspondence information to derive dense pixel correspondence maps. <p> Recovering a model for the radiance of a voxel then involves solving a linear system of equations for the coefficients m ij . 8 5.2 Modeling Residuals In plenoptic decomposition, radiance residuals are used to ensure that local radiance variations are approximated accurately for views close to the input images <ref> [7] </ref>. Residual modeling is an instance of scattered data approximation on the sphere [16]a rich literature on the topic exists, partly motivated by the problem of BRDF estimation [17].
Reference: [8] <author> A. Katayama, K. Tanaka, T. Oshino, and H. Tamura, </author> <title> A viewpoint dependent stereoscopic display using interpolation of multi-viewpoint images, </title> <booktitle> in Proc. SPIE Vol. 2409A, </booktitle> <pages> pp. 21-30, </pages> <year> 1995. </year>
Reference-contexts: Consequently, we propose an alternate, more centralized representation of the plenoptic function that is easily modified in response to image editing operations. Despite being complex, the plenoptic function for a static scene is highly structured. This structure has been noted before, for instance by Katayama et al. <ref> [8] </ref> who used 1 line-detection strategies to extract correspondences within a stack of images. To exploit this structure, we advocate a decomposition of the plenoptic function into a shape and a radiance component. <p> Notice that for any two points P and Q, P can occlude Q from one of the camera viewpoints only if Q is in a higher layer than P . The simplification of visibility relationships for this special case has previously been noted by Katayama et al. <ref> [8] </ref> who used it to find correspondences for cameras arranged on a line. The linear case is easily generalized for any set of cameras satisfying the ordinal visibility constraint, i.e., when the scene lies entirely outside of the camera volume.
Reference: [9] <author> S. M. Seitz and C. R. Dyer, </author> <title> Physically-valid view synthesis by image interpolation, </title> <booktitle> in Proc. IEEE Workshop on Representations of Visual Scenes, </booktitle> <pages> pp. 18-25, </pages> <year> 1995. </year>
Reference-contexts: Our decomposition technique has significant advantages over previous correspondence-based approaches to image-based rendering. Previous approaches have relied on stereo vision techniques <ref> [2, 3, 7, 9, 10] </ref> or volume intersection [6, 11] to derive pixel correspondence information. Both types of methods have serious weaknessesexisting stereo techniques require that the input cameras be close together and minimize visibility changes (e.g., occlusions, changes in field of view).
Reference: [10] <author> T. Kanade, P. J. Narayanan, and P. W. Rander, </author> <title> Visualized reality: Concepts and early results, </title> <booktitle> in Proc. Workshop on Representations of Visual Scenes, </booktitle> <pages> pp. 69-76, </pages> <year> 1995. </year>
Reference-contexts: Our decomposition technique has significant advantages over previous correspondence-based approaches to image-based rendering. Previous approaches have relied on stereo vision techniques <ref> [2, 3, 7, 9, 10] </ref> or volume intersection [6, 11] to derive pixel correspondence information. Both types of methods have serious weaknessesexisting stereo techniques require that the input cameras be close together and minimize visibility changes (e.g., occlusions, changes in field of view).
Reference: [11] <author> S. Moezzi, A. Katkere, D. Y. Kuramura, and R. Jain, </author> <title> Reality modeling and visualization from multiple video sequences, </title> <journal> IEEE Computer Graphics and Applications, </journal> <volume> vol. 16, no. 6, </volume> <pages> pp. 58-63, </pages> <year> 1996. </year>
Reference-contexts: Our decomposition technique has significant advantages over previous correspondence-based approaches to image-based rendering. Previous approaches have relied on stereo vision techniques [2, 3, 7, 9, 10] or volume intersection <ref> [6, 11] </ref> to derive pixel correspondence information. Both types of methods have serious weaknessesexisting stereo techniques require that the input cameras be close together and minimize visibility changes (e.g., occlusions, changes in field of view).
Reference: [12] <author> S. Benton, </author> <title> Survey of holographic stereograms, in Processing and Display of Three-Dimensional Data, </title> <booktitle> Proc. SPIE, </booktitle> <year> 1983. </year> <month> 10 </month>
Reference-contexts: While the plenoptic function is determined uniquely by the 3D surfaces in a scene and their reflectance properties, we can generate the same plenoptic function by combining many different shapes and radiance functions (Figure 2). Holographic imaging <ref> [12] </ref> is one notable application where this ambiguity is put into practical use: it relies on our inability to distinguish views of flat holographic images from views of objects that are truly 3D.
Reference: [13] <author> S. B. Kang and R. Szeliski, </author> <title> 3-D scene data recov-ery using omnidirectional multibaseline stereo, </title> <booktitle> in Proc. Computer Vision and Pattern Recognition Conf., </booktitle> <pages> pp. 364-370, </pages> <year> 1996. </year>
Reference-contexts: Figure 4 (b) shows a strategy for the case of outward-facing cameras. This type of camera geometry is useful for acquiring panoramic scene visualizations, as in <ref> [2, 13] </ref>. One consistent set of layers corresponds to a series of rectangles radiating outward from the camera volume. Layer 0 is the axis-aligned bounding box of the camera centers and the rest of the layers are determined by uniformly expanding the box one unit at a time.
Reference: [14] <author> T. Beier and S. Neely, </author> <title> Feature-based image metamorphosis, </title> <booktitle> in Proc. SIGGRAPH 92, </booktitle> <pages> pp. 35-42, </pages> <year> 1992. </year>
Reference-contexts: Image constraints provided through user-interaction have proven very powerful in solving difficult correspondence tasks. Debevec et al. [7] combined user-assisted placement of 3D shape primitives with camera calibration and stereo techniques to enable reconstructions of architectural scenes. Similarly, image morphing methods <ref> [14, 15] </ref> exploit user-provided feature correspondence information to derive dense pixel correspondence maps. Constraints are used in plenoptic image editing to control the propagation of image edits. They are specified as follows: any user editing operation changes a set of pixels in one image. <p> While the motion of rays is determined, the motion of voxels along rays is not. Our implementation of plenoptic image morphing fixed this variable by constraining voxels to move parallel to the image plane. We use an implementation of the Beier and Neely method <ref> [14] </ref> to generate image warps. The voxel warp is computed by inverse-mapping each voxel in the warped scene to determine the corresponding voxel (if any) in the unwarped scene.
Reference: [15] <author> S. M. Seitz and C. R. Dyer, </author> <title> View morphing, </title> <booktitle> in Proc. SIGGRAPH 96, </booktitle> <pages> pp. 21-30, </pages> <year> 1996. </year>
Reference-contexts: Image constraints provided through user-interaction have proven very powerful in solving difficult correspondence tasks. Debevec et al. [7] combined user-assisted placement of 3D shape primitives with camera calibration and stereo techniques to enable reconstructions of architectural scenes. Similarly, image morphing methods <ref> [14, 15] </ref> exploit user-provided feature correspondence information to derive dense pixel correspondence maps. Constraints are used in plenoptic image editing to control the propagation of image edits. They are specified as follows: any user editing operation changes a set of pixels in one image.
Reference: [16] <author> G. Nielson, </author> <title> Scattered data modeling, </title> <journal> IEEE Computer Graphics and Applications, </journal> <pages> pp. 60-70, </pages> <year> 1993. </year>

References-found: 16

