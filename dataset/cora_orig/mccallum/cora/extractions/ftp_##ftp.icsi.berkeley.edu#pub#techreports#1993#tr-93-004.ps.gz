URL: ftp://ftp.icsi.berkeley.edu/pub/techreports/1993/tr-93-004.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/techreports/1993.html
Root-URL: http://www.icsi.berkeley.edu
Email: utans@icsi.berkeley.edu  
Title: Mixture Models and the EM Algorithm for Object Recognition within Compositional Hierarchies Part 1: Recognition  
Phone: 1-510-642-4274 FAX 1-510-643-7684  
Author: Joachim Utans 
Date: January 1993  
Address: I 1947 Center Street Suite 600 Berkeley, California 94704  
Affiliation: INTERNATIONAL COMPUTER SCIENCE INSTITUTE  
Pubnum: TR-93-004  
Abstract: We apply the Expectation Maximization (EM) algorithm to an assignment problem where in addition to binary assignment variables analog parameters must be estimated. As an example, we use the problem of part labelling in the context of model based object recognition where models are stored in from of a compositional hierarchy. This problem has been formulated previously as a graph matching problem and stated in terms of minimizing an objective function that a recurrent neural network solves [11, 12, 5, 8, 22]. Mjolsness [9, 10] has introduced a stochastic visual grammar as a model for this problem; there the matching problem arises from an index renumbering operation via a permutation matrix. The optimization problem w.r.t the match variables is difficult and Mean Field Annealing techniques are used to solve it. Here we propose to model the part labelling problem in terms of a mixture of distributions, each describing the parameters of a part. Under this model, the match variables correspond to the a posteriori estimates of the mixture coefficients. The parts in the input image are unlabelled, this problem can be stated as missing data problem and the EM algorithm can be used to recover the labels and estimate parameters. The resulting update equations are identical to the Elastic Net equations; however, the update dynamics differ. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Daniel J. Amit. </author> <title> Modeling Brain Function. </title> <publisher> Cambridge University Press, </publisher> <year> 1989. </year>
Reference-contexts: The most straightforward implementation via a Hopfield network [6, 7] easily gets stuck in local minima of the objective function. Recently, results from statistical physics have been successfully applied to the problem of optimizing objective functions with neural networks <ref> [1, 15] </ref>. The Mean Field Approximation and deterministic simulated annealing allow the design of neural networks that can avoid spurious local minima. The governing probability distribution is assumed Gibbsian. The sum in the partition function Z runs over all possible configurations that are accessible to the system. <p> The remaining constraint can be implemented via a penalty term (or is sometimes ignored since solutions that conform to one constraint but not the other are deemed of high cost and therefore most likely avoided by the network). These methods have been described by Amit et al <ref> [2, 1] </ref>, Peterson and Soederberg [14, 15], Simic [18, 17] and Yuille [23]. 2.1 Saddle Point Approximation The Mean Field trick employed here consists of rewriting the sum in the partition function as an integral and evaluating the integral at the saddlepoint, at which point, certain constraint terms can be evaluated <p> 1], Peterson and Soederberg [14, 15], Simic [18, 17] and Yuille [23]. 2.1 Saddle Point Approximation The Mean Field trick employed here consists of rewriting the sum in the partition function as an integral and evaluating the integral at the saddlepoint, at which point, certain constraint terms can be evaluated <ref> [2, 1, 14, 15, 18, 17] </ref>.
Reference: [2] <author> Daniel J. Amit, Hanoch Gutfreund, and H. Sompolinsky. </author> <title> Spin-glass models of neural networks. </title> <journal> Physical Review A, </journal> <volume> 32(2) </volume> <pages> 1007-1018, </pages> <month> August </month> <year> 1985. </year>
Reference-contexts: The remaining constraint can be implemented via a penalty term (or is sometimes ignored since solutions that conform to one constraint but not the other are deemed of high cost and therefore most likely avoided by the network). These methods have been described by Amit et al <ref> [2, 1] </ref>, Peterson and Soederberg [14, 15], Simic [18, 17] and Yuille [23]. 2.1 Saddle Point Approximation The Mean Field trick employed here consists of rewriting the sum in the partition function as an integral and evaluating the integral at the saddlepoint, at which point, certain constraint terms can be evaluated <p> 1], Peterson and Soederberg [14, 15], Simic [18, 17] and Yuille [23]. 2.1 Saddle Point Approximation The Mean Field trick employed here consists of rewriting the sum in the partition function as an integral and evaluating the integral at the saddlepoint, at which point, certain constraint terms can be evaluated <ref> [2, 1, 14, 15, 18, 17] </ref>.
Reference: [3] <author> A. P. Dempster, N. M. Laird, and D. B. Rubin. </author> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> J. Royal Statist. Soc. B, </journal> <volume> 39 </volume> <pages> 1-39, </pages> <year> 1977. </year>
Reference-contexts: The EM algorithm has been described in detail by Demster et al. <ref> [3] </ref> and Redner and Walker [16] (see also Nowlan [13]). The EM algorithm attempts to compute the most likely values for the parameters of the component density functions and the mixture coefficients from the observable data alone. <p> For the mixture density from equation (16), the log-likelihood for the complete data pairs can be written as <ref> [3, 13] </ref>: log P (x; Mjfi) = X X m ji (log P i (x j j i ) + log P (m ji jfi)) (19) where P i (x j j i ) = p e 2 2 jx j (x+d i )j 2 and denotes the prior density for
Reference: [4] <author> R. Durbin and D. Willshaw. </author> <title> An analog approach to the travelling salesman problem using an elastic net method. </title> <journal> Nature, </journal> <volume> 326 </volume> <pages> 689-691, </pages> <year> 1987. </year>
Reference-contexts: and deterministic annealing is used by gradually lowering the temperature T . 2.2 Elastic Net If the network contains both real-valued variables and binary match variables and the problem would be simplified if the match variables could be eliminated entirely (the term "Elastic Net" was introduced by Durbin and Wilshaw <ref> [4] </ref> to distinguish their formulation of the Traveling Salesman problem from one using explicit assignment variables).
Reference: [5] <author> G. Gindi, E. Mjolsness, and P. Anandan. </author> <title> Neural networks for model based recognition. </title> <booktitle> In Neural Networks: Concepts, Applications and Implementations, </booktitle> <pages> pages 144-173. </pages> <publisher> Prentice-Hall, </publisher> <year> 1991. </year>
Reference-contexts: This problem has been formulated previously as a graph matching problem and stated in terms of minimizing an objective function that a recurrent neural network solves <ref> [11, 12, 5, 8, 22] </ref>. Mjolsness [9, 10] has introduced a stochastic visual grammar as a forward (generative) model that describes how an object is build up from parts.
Reference: [6] <author> J. J. </author> <title> Hopfield. Neurons with graded response have collective computational properties like those of two-state neurons. </title> <booktitle> Proceedings of the National Academy of Sciences USA, </booktitle> <volume> vol. 81 </volume> <pages> 3088-3092, </pages> <month> May </month> <year> 1984. </year>
Reference-contexts: The most straightforward implementation via a Hopfield network <ref> [6, 7] </ref> easily gets stuck in local minima of the objective function. Recently, results from statistical physics have been successfully applied to the problem of optimizing objective functions with neural networks [1, 15].
Reference: [7] <author> J. J. Hopfield and D. W. Tank. </author> <title> `Neural' computation of decisions in optimization problems. </title> <journal> Biological Cybernetics, </journal> <volume> vol. 52 </volume> <pages> 141-152, </pages> <year> 1985. </year>
Reference-contexts: The most straightforward implementation via a Hopfield network <ref> [6, 7] </ref> easily gets stuck in local minima of the objective function. Recently, results from statistical physics have been successfully applied to the problem of optimizing objective functions with neural networks [1, 15].
Reference: [8] <author> Christoph von der Malsburg. </author> <title> Pattern recognition by labeled graph matching. </title> <booktitle> Neural Networks, </booktitle> <volume> 1 </volume> <pages> 141-148, </pages> <year> 1988. </year>
Reference-contexts: This problem has been formulated previously as a graph matching problem and stated in terms of minimizing an objective function that a recurrent neural network solves <ref> [11, 12, 5, 8, 22] </ref>. Mjolsness [9, 10] has introduced a stochastic visual grammar as a forward (generative) model that describes how an object is build up from parts.
Reference: [9] <author> E. Mjolsness. </author> <title> Bayesian inference on visual grammars by neural nets that optimize. </title> <type> Technical Report YALEU-DCS-TR-854, </type> <institution> Yale University, Dept. of Computer Science, </institution> <year> 1991. </year>
Reference-contexts: This problem has been formulated previously as a graph matching problem and stated in terms of minimizing an objective function that a recurrent neural network solves [11, 12, 5, 8, 22]. Mjolsness <ref> [9, 10] </ref> has introduced a stochastic visual grammar as a forward (generative) model that describes how an object is build up from parts. The description mirrors the representation in form of a compositional hierarchy; at each stage the description becomes more detailed as more parts are added. <p> This gives the probability of observing such an object prior to the arrival of the data. The problem can then be stated as Bayesian inference problem <ref> [9] </ref>. The optimization problem w.r.t the match variables is difficult and Mean Field Annealing techniques are used to solve it. 1.1 Example: Stochastic Model for Dot Configurations The model places the dot-cluster center x assuming a uniform distribution. <p> For the recognition or inference problem the dots observed in the image are unlabeled and the recovery of the labels is an intrinsic part of the recognition problem. Previous work by Mjolsness <ref> [10, 9] </ref> models the unordering of parts via a index permutation (renumbering). For example, let the parts in the model description be numbered fx 1 ; x 2 ; x 3 g and the parts observed in the image as fx a ; x b ; x c g. <p> This matrix permutes the indices of the parts. For the example above, M 1a = 1 would let x 1 appear in the image as x a . First a permutation matrix M is chosen with probability P (M). The final joint probability distribution becomes <ref> [10, 9] </ref> P (M; fx j g; x) = C P (M) e 2 2 i j M ij jx j (x+d i )j 2 (2) where i indexes the unscrambled model parts and j indexes the parts observed in the image (note that the model parts fx i g have <p> in equation (2) with respect to the unknown variables M and x, subject to the constraint that M is a permutation matrix. 2 Neural Network based on the Mean Field Approximation The maximization problem (4) has been implemented using a recurrent neural network to solve the optimization problem (see Mjolsness <ref> [9, 10] </ref>). The most straightforward implementation via a Hopfield network [6, 7] easily gets stuck in local minima of the objective function. Recently, results from statistical physics have been successfully applied to the problem of optimizing objective functions with neural networks [1, 15]. <p> In both cases, deterministic annealing is performed by slowly decreasing T = 1=fi (but only down to 1 to preserve the ratio of possibly different variances for different parts of the object <ref> [9] </ref>). 3 Mixture Model for Labelling Data An alternative to the index permutation is to model the unordering step by describing the distribution of parameters of parts in form of a mixture density. Mixture models have been used in the context for learning (see for example Nowlan [13]). <p> one way can exist to incorporate a given constraint, allowing a choice that best maps the constraint onto a neural network architecture (i.e. choosing the functional form that is optimized most easily). 3.4 Simulation Results The EM algorithm has been implemented for a 3-level hierarchical model for dot configurations (see <ref> [9] </ref>). In an intermediate step, dot cluster centers x c are placed, the final dot position are taken relative to these centers. The parameters to be estimated are x and the fx c g.
Reference: [10] <author> E. Mjolsness. </author> <title> Visual grammars and their neural nets. </title> <editor> In R.P. Lippmann J.E. Moody, S.J. Han-son, editor, </editor> <booktitle> Advances in Neural Information Processing Systems 4. </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1992. </year>
Reference-contexts: This problem has been formulated previously as a graph matching problem and stated in terms of minimizing an objective function that a recurrent neural network solves [11, 12, 5, 8, 22]. Mjolsness <ref> [9, 10] </ref> has introduced a stochastic visual grammar as a forward (generative) model that describes how an object is build up from parts. The description mirrors the representation in form of a compositional hierarchy; at each stage the description becomes more detailed as more parts are added. <p> For the recognition or inference problem the dots observed in the image are unlabeled and the recovery of the labels is an intrinsic part of the recognition problem. Previous work by Mjolsness <ref> [10, 9] </ref> models the unordering of parts via a index permutation (renumbering). For example, let the parts in the model description be numbered fx 1 ; x 2 ; x 3 g and the parts observed in the image as fx a ; x b ; x c g. <p> This matrix permutes the indices of the parts. For the example above, M 1a = 1 would let x 1 appear in the image as x a . First a permutation matrix M is chosen with probability P (M). The final joint probability distribution becomes <ref> [10, 9] </ref> P (M; fx j g; x) = C P (M) e 2 2 i j M ij jx j (x+d i )j 2 (2) where i indexes the unscrambled model parts and j indexes the parts observed in the image (note that the model parts fx i g have <p> in equation (2) with respect to the unknown variables M and x, subject to the constraint that M is a permutation matrix. 2 Neural Network based on the Mean Field Approximation The maximization problem (4) has been implemented using a recurrent neural network to solve the optimization problem (see Mjolsness <ref> [9, 10] </ref>). The most straightforward implementation via a Hopfield network [6, 7] easily gets stuck in local minima of the objective function. Recently, results from statistical physics have been successfully applied to the problem of optimizing objective functions with neural networks [1, 15].
Reference: [11] <author> Eric Mjolsness, Gene Gindi, and P. Anandan. </author> <title> Optimization in model matching and perceptual organization: A first look. </title> <institution> Research report yaleu/dcs/rr-634, Yale University, Department of Computer Science, </institution> <year> 1988. </year>
Reference-contexts: This problem has been formulated previously as a graph matching problem and stated in terms of minimizing an objective function that a recurrent neural network solves <ref> [11, 12, 5, 8, 22] </ref>. Mjolsness [9, 10] has introduced a stochastic visual grammar as a forward (generative) model that describes how an object is build up from parts.
Reference: [12] <author> Eric Mjolsness, Gene R. Gindi, and P. Anandan. </author> <title> Optimization in model matching and perceptual organization. </title> <journal> Neural Computation, </journal> <volume> vol. 1, no. 2, </volume> <year> 1989. </year>
Reference-contexts: This problem has been formulated previously as a graph matching problem and stated in terms of minimizing an objective function that a recurrent neural network solves <ref> [11, 12, 5, 8, 22] </ref>. Mjolsness [9, 10] has introduced a stochastic visual grammar as a forward (generative) model that describes how an object is build up from parts.
Reference: [13] <author> Steven J. Nowlan. </author> <title> Soft Competitive Adaptation: Neural Network Learning Algorithms based on Fitting Statistical Mixtures. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA 15213, </address> <year> 1991. </year> <month> 11 </month>
Reference-contexts: Mixture models have been used in the context for learning (see for example Nowlan <ref> [13] </ref>). There, the task is to estimate the parameters of the mixture density from data. Ultimately, the goal in the context of the object recognition problem is to learn the model base, its structure and parameters, from data. <p> The EM algorithm has been described in detail by Demster et al. [3] and Redner and Walker [16] (see also Nowlan <ref> [13] </ref>). The EM algorithm attempts to compute the most likely values for the parameters of the component density functions and the mixture coefficients from the observable data alone. The algorithm estimates the statistics of the unobservable indicator variables and uses these estimates to improve the parameter estimates. <p> For the mixture density from equation (16), the log-likelihood for the complete data pairs can be written as <ref> [3, 13] </ref>: log P (x; Mjfi) = X X m ji (log P i (x j j i ) + log P (m ji jfi)) (19) where P i (x j j i ) = p e 2 2 jx j (x+d i )j 2 and denotes the prior density for
Reference: [14] <author> Carsten Peterson and James R. Anderson. </author> <title> Neural networks and NP-complete optimization problems: A performance study on the graph bisection problem. </title> <type> TR MCC-EI-287-87, </type> <institution> Microelectronics and Computer Technology Corporation, </institution> <address> 3500 West Balcones Drive, Austin, TX 78759, </address> <year> 1987. </year>
Reference-contexts: These methods have been described by Amit et al [2, 1], Peterson and Soederberg <ref> [14, 15] </ref>, Simic [18, 17] and Yuille [23]. 2.1 Saddle Point Approximation The Mean Field trick employed here consists of rewriting the sum in the partition function as an integral and evaluating the integral at the saddlepoint, at which point, certain constraint terms can be evaluated [2, 1, 14, 15, 18, <p> 1], Peterson and Soederberg [14, 15], Simic [18, 17] and Yuille [23]. 2.1 Saddle Point Approximation The Mean Field trick employed here consists of rewriting the sum in the partition function as an integral and evaluating the integral at the saddlepoint, at which point, certain constraint terms can be evaluated <ref> [2, 1, 14, 15, 18, 17] </ref>.
Reference: [15] <author> Carsten Peterson and Bo Soderberg. </author> <title> A new method for mapping optimization problems onto neural networks. </title> <journal> International Journal of Neural Systems, </journal> <volume> 1(1) </volume> <pages> 3-22, </pages> <year> 1989. </year>
Reference-contexts: The most straightforward implementation via a Hopfield network [6, 7] easily gets stuck in local minima of the objective function. Recently, results from statistical physics have been successfully applied to the problem of optimizing objective functions with neural networks <ref> [1, 15] </ref>. The Mean Field Approximation and deterministic simulated annealing allow the design of neural networks that can avoid spurious local minima. The governing probability distribution is assumed Gibbsian. The sum in the partition function Z runs over all possible configurations that are accessible to the system. <p> These methods have been described by Amit et al [2, 1], Peterson and Soederberg <ref> [14, 15] </ref>, Simic [18, 17] and Yuille [23]. 2.1 Saddle Point Approximation The Mean Field trick employed here consists of rewriting the sum in the partition function as an integral and evaluating the integral at the saddlepoint, at which point, certain constraint terms can be evaluated [2, 1, 14, 15, 18, <p> 1], Peterson and Soederberg [14, 15], Simic [18, 17] and Yuille [23]. 2.1 Saddle Point Approximation The Mean Field trick employed here consists of rewriting the sum in the partition function as an integral and evaluating the integral at the saddlepoint, at which point, certain constraint terms can be evaluated <ref> [2, 1, 14, 15, 18, 17] </ref>.
Reference: [16] <author> Richard A. Redner and Homer F. Walker. </author> <title> Mixture densities, maximum likelihood and the EM algorithm. </title> <journal> SIAM Review, </journal> <volume> 26(2) </volume> <pages> 195-239, </pages> <month> April </month> <year> 1984. </year>
Reference-contexts: The EM algorithm has been described in detail by Demster et al. [3] and Redner and Walker <ref> [16] </ref> (see also Nowlan [13]). The EM algorithm attempts to compute the most likely values for the parameters of the component density functions and the mixture coefficients from the observable data alone.
Reference: [17] <author> Petar D. Simic. </author> <title> "Constrained nets" for graph matching and other quadratic assignment problems. </title> <type> TR CALT-68-1672, </type> <institution> California Institute of Technology, Pasadena, </institution> <address> CA 91125, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: These methods have been described by Amit et al [2, 1], Peterson and Soederberg [14, 15], Simic <ref> [18, 17] </ref> and Yuille [23]. 2.1 Saddle Point Approximation The Mean Field trick employed here consists of rewriting the sum in the partition function as an integral and evaluating the integral at the saddlepoint, at which point, certain constraint terms can be evaluated [2, 1, 14, 15, 18, 17]. <p> 1], Peterson and Soederberg [14, 15], Simic [18, 17] and Yuille [23]. 2.1 Saddle Point Approximation The Mean Field trick employed here consists of rewriting the sum in the partition function as an integral and evaluating the integral at the saddlepoint, at which point, certain constraint terms can be evaluated <ref> [2, 1, 14, 15, 18, 17] </ref>.
Reference: [18] <author> Petar D. Simic. </author> <title> Statistical mechanics as the underlying theory of "elastic net" and "neural" optimization. </title> <journal> Network, </journal> <volume> 1 </volume> <pages> 89-103, </pages> <year> 1990. </year>
Reference-contexts: These methods have been described by Amit et al [2, 1], Peterson and Soederberg [14, 15], Simic <ref> [18, 17] </ref> and Yuille [23]. 2.1 Saddle Point Approximation The Mean Field trick employed here consists of rewriting the sum in the partition function as an integral and evaluating the integral at the saddlepoint, at which point, certain constraint terms can be evaluated [2, 1, 14, 15, 18, 17]. <p> 1], Peterson and Soederberg [14, 15], Simic [18, 17] and Yuille [23]. 2.1 Saddle Point Approximation The Mean Field trick employed here consists of rewriting the sum in the partition function as an integral and evaluating the integral at the saddlepoint, at which point, certain constraint terms can be evaluated <ref> [2, 1, 14, 15, 18, 17] </ref>.
Reference: [19] <author> J. Utans and G. R. Gindi. </author> <title> A neural network approach to object recognition and image partitioning within a resolution hierarchy. </title> <booktitle> In SPIE Intelligent Information Systems OR'92, </booktitle> <address> Orlando, FL, </address> <month> April 20-24 </month> <year> 1992. </year> <pages> SPIE. </pages>
Reference-contexts: On the other hand, the assignment coefficients avoid local minima by allowing the parameters to stay close to their optimal value. In the context of the mixture density model and the EM algorithm, the Bootstrap method described by Utans and Gindi <ref> [20, 19, 21] </ref> can be interpreted as the first (or first few steps) of the EM algorithm. There, it was proposed to improve convergence in hierarchical matching networks by initializing the parameter estimates from a coarse scale version of the input image.
Reference: [20] <author> J. Utans and G. R. Gindi. </author> <title> A neural network performs context guided segmentation of shapes via bayesian inference. </title> <booktitle> In presented at Neural Networks for Computing, </booktitle> <address> Snowbird, Utah, </address> <month> April 7-10 </month> <year> 1992. </year>
Reference-contexts: On the other hand, the assignment coefficients avoid local minima by allowing the parameters to stay close to their optimal value. In the context of the mixture density model and the EM algorithm, the Bootstrap method described by Utans and Gindi <ref> [20, 19, 21] </ref> can be interpreted as the first (or first few steps) of the EM algorithm. There, it was proposed to improve convergence in hierarchical matching networks by initializing the parameter estimates from a coarse scale version of the input image.
Reference: [21] <author> Joachim Utans and Gene R. Gindi. </author> <title> Improving convergence in hierarchical matching networks for object recognition. </title> <editor> In Stephen J. Hanson, Jack Cowan, and Lee Giles, editors, </editor> <booktitle> Advances in Neural Information Processing 5. </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: On the other hand, the assignment coefficients avoid local minima by allowing the parameters to stay close to their optimal value. In the context of the mixture density model and the EM algorithm, the Bootstrap method described by Utans and Gindi <ref> [20, 19, 21] </ref> can be interpreted as the first (or first few steps) of the EM algorithm. There, it was proposed to improve convergence in hierarchical matching networks by initializing the parameter estimates from a coarse scale version of the input image.
Reference: [22] <author> Joachim Utans, Gene R. Gindi, Eric Mjolsness, and P. Anandan. </author> <title> Neural networks for object recognition within compositional hierarchies: Initial experiments. </title> <type> Technical report 8903, </type> <institution> Yale University, Center for Systems Science, Department Electrical Engineering, </institution> <year> 1989. </year>
Reference-contexts: This problem has been formulated previously as a graph matching problem and stated in terms of minimizing an objective function that a recurrent neural network solves <ref> [11, 12, 5, 8, 22] </ref>. Mjolsness [9, 10] has introduced a stochastic visual grammar as a forward (generative) model that describes how an object is build up from parts.
Reference: [23] <author> A. L. Yuille. </author> <title> Generalized deformable models, statistical physics, and matching problems. </title> <journal> Neural Computation, </journal> <volume> 2(2) </volume> <pages> 1-24, </pages> <year> 1990. </year>
Reference-contexts: These methods have been described by Amit et al [2, 1], Peterson and Soederberg [14, 15], Simic [18, 17] and Yuille <ref> [23] </ref>. 2.1 Saddle Point Approximation The Mean Field trick employed here consists of rewriting the sum in the partition function as an integral and evaluating the integral at the saddlepoint, at which point, certain constraint terms can be evaluated [2, 1, 14, 15, 18, 17]. <p> Yuille <ref> [23] </ref> has shown how to derive the elastic net formulation from the one containing match variables by means of summing over all possible assignments in the partition function (or equiva lently computing the marginal distribution). <p> Again, the partition function is modified to restrict the space of possible solutions to the ones satisfying the column (or row) constraint for the permutation matrix. Unfortunately, summing over all permutation matrices does not lead to a simplified expression for Z <ref> [23] </ref>.
Reference: [24] <author> Alan Yuille, Paul Stolorz, and Joachim Utans. </author> <title> Statistical physics, mixtures of distributions and the EM algorithm. </title> <note> to appear, 1993. 12 </note>
References-found: 24

