URL: http://www.cs.tamu.edu/research/robotics/Amato/Papers/costmodel-sgipc.tr.ps.gz
Refering-URL: http://www.cs.tamu.edu/faculty/amato/dsmft/publications.html
Root-URL: http://www.cs.tamu.edu
Title: A Cost Model for Communication on a Symmetric MultiProcessor  
Author: Nancy M. Amato Andrea Pietracaprina Geppino Pucci Lucia K. Dale Jack Perdue 
Note: This research was supported in part by NATO CRG 961243 Bulk Synchronous Computational Geometry, and by the National Center for Supercomputing Applications under CCR970010N (utilizing the SGI Power ChallengeArray at NCSA,  The work at Texas A&M was also supported by the NSF by CAREER award CCR-9624315 and grant IRI-9619850.  
Date: January 26, 1998  
Address: Texas A&M Univerity  College Station, TX, USA.  
Affiliation: Department of Computer Science  University of Illinois at Urbana-Champaign).  Department of Computer Science, Texas A&M University,  Dipartimento di Matematica Pura e Applicata, Universita di Padova, Italy. Dipartimento di Elettronica e Informatica, Universita di Padova, Italy.  
Pubnum: Technical Report 98-004  
Abstract: In this paper we conduct an in-depth study of the communication costs of programs when run on a typical Symmetric MultiProcessor, the SGI Power Challenge, characterized by powerful off-the-shelf microprocessors communicating through a shared memory via a shared-bus interconnect. Our study is based on an extensive set of experiments designed to assess the relative impact of a number of parameters on the cost of shared memory accesses. We provide evidence that interaction with the memory hierarchy affects communication in such a substantial way that none of the models previously considered in the literature can guarantee a reasonable level of accuracy since they do not take this interaction into account. We then determine two prediction functions that are very accurate predictors of best and worst performance with respect to the memory hierarchy. These functions provide a prediction interval that can be employed to obtain lower and upper bounds on the actual communication cost of an application, and to evaluate the degree of locality of the memory access patterns involved. 
Abstract-found: 1
Intro-found: 1
Reference: [ACS87] <author> A. Aggarwal, A.K. Chandra, and M. Snir. </author> <title> Hierarchical memory with block transfer. </title> <booktitle> In Proc. of the 28th IEEE Symp. on Foundations of Computer Science, </booktitle> <pages> pages 204-216, </pages> <year> 1987. </year>
Reference-contexts: While the first two factors can be easily quantified as numerical parameters, the effects of the memory hierarchy are more complex and hard to capture in a quantitative fashion. In fact, the cost of a sequence of global reads/writes is highly dependent on the locality of reference, <ref> [ACS87] </ref>, which in turn relates to the level of the hierarchy being accessed, the contiguity of consecutive accesses, and the write conflicts among the processors, which activate the invalidation mechanism of the cache coherence protocol. Good and Bad Access Pattern Families.
Reference: [BDM95] <editor> A. Baumker, W. Dittrich, and F. Meyer auf der Heide. </editor> <title> Truly efficient parallel algorithms: c-optimal multisearch for and extension of the BSP model. </title> <booktitle> In Proc. of the 3rd European Symposium on Algorithms, </booktitle> <pages> pages 17-30, </pages> <year> 1995. </year>
Reference-contexts: The BSP opened the way to a rich line of research which resulted in the definition of a number of variants that maintain the basic bulk-synchronous approach to parallel programming but try to enhance the predictive quality of the associated cost model. Among these, we mention the BSP* <ref> [BDM95] </ref> and the E-BSP [JW96], which extend the BSP cost model to account for block transfer and unbalanced communication, respectively. Both these models target distributed-memory architectures, where communication is realized via message passing.
Reference: [BHP + 96] <author> G. Bilardi, K.T. Herley, A. Pietracaprina, G. Pucci and P. Spirakis. </author> <title> BSP vs. LogP. </title> <booktitle> In Proc. of the 8th ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pages 25-32, </pages> <month> June </month> <year> 1996. </year>
Reference-contexts: based on different programming paradigms, are the CG Model [DFRC93], which focuses on algorithmic rather than architectural issues, and the LOGP model [CKP + 96], a fully asynchronous model whose performance predictions tend to be more accurate than those provided by BSP but are still subject to the same limitations <ref> [BHP + 96, BGMZ97] </ref>. The BSP opened the way to a rich line of research which resulted in the definition of a number of variants that maintain the basic bulk-synchronous approach to parallel programming but try to enhance the predictive quality of the associated cost model.
Reference: [BGMZ97] <author> G.E. Blelloch, P.B. Gibbons, Y. Matias, and M. Zagha. </author> <title> Accounting for memory bank contention and delay in high-bandwidth multiprocessors. </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> 8(9) </volume> <pages> 943-958, </pages> <year> 1997. </year>
Reference-contexts: However, as has been often observed, the simple BSP cost model offers only a coarse level of predictivity, since it disregards architectural features of real machines which may have a dramatic impact on performance <ref> [BGMZ97, JW96] </ref>. <p> based on different programming paradigms, are the CG Model [DFRC93], which focuses on algorithmic rather than architectural issues, and the LOGP model [CKP + 96], a fully asynchronous model whose performance predictions tend to be more accurate than those provided by BSP but are still subject to the same limitations <ref> [BHP + 96, BGMZ97] </ref>. The BSP opened the way to a rich line of research which resulted in the definition of a number of variants that maintain the basic bulk-synchronous approach to parallel programming but try to enhance the predictive quality of the associated cost model. <p> Both these models target distributed-memory architectures, where communication is realized via message passing. Bulk-synchronous models specifically tailored for shared-memory systems are the (d; x)-BSP <ref> [BGMZ97] </ref> and the QSM [GMR97], whose cost functions account for some aspects of memory contention, namely, the maximum number of concurrent accesses to the same memory location (QSM), and the maximum number of concurrent accesses to the same memory bank ((d; x)-BSP). <p> In this respect, our objective differs from those that motivated other works concerned with modeling shared-memory systems <ref> [GMR97, BGMZ97] </ref>. The QSM model proposed in [GMR97] attempts to strike a balance between descriptivity and generality, with the intent to enhance the effectiveness of algorithm design while maintaining a high level view of a machine. <p> Therefore, QSM predictions for machines with very few processors (e.g., many SMPs) tend to be no more accurate than those provided by BSP. On the other hand, the (d; x)-BSP <ref> [BGMZ97] </ref> aims at a higher level of descriptivity for a certain class of shared-memory machines, characterized by numerous but slow memory banks and by the provision of latency-hiding features to compensate for bank delays, as is the case, for example, of vector multiprocessors.
Reference: [BLM + 91] <author> G. E. Blelloch, C. E. Leiserson, B. M. Maggs, C. G. Plaxton, S. J. Smith, and M. Zagha. </author> <title> A comparison of sorting algorithms for the Connection Machine CM-2. </title> <booktitle> In Annual ACM Symp. Paral. Algor. Arch., </booktitle> <pages> pages 3-16, </pages> <year> 1991. </year>
Reference-contexts: The application programs implement three different sorting algorithms: sample sort [FM70, HC83, RV87, WS88], column sort [L85], and a parallel version of radix sort <ref> [BLM + 91] </ref>. These algorithms were chosen because they are well-understood parallel algorithms that exhibit a variety of communication patterns. 5.1 Sorting algorithm implementations All algorithms were coded in accordance with the programming model described in Section 3. <p> Summaries of each algorithm's supersteps are shown in Tables 9, 10, and 11. Radix sort Suppose each element is represented by b bits. The simplest way to parallelize radix sort is to use a parallel version of counting sort (e.g., <ref> [CLR90, BLM + 91] </ref>) as the internal stable sort. In superstep 1, each processor counts the number of its n=p elements with each possible value in [0; 2 r ), where r is the number of bits considered in each iteration.
Reference: [CLR90] <author> T. H. Cormen, C. E. Leiserson, and R. L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> McGraw-Hill, </publisher> <year> 1990. </year>
Reference-contexts: Summaries of each algorithm's supersteps are shown in Tables 9, 10, and 11. Radix sort Suppose each element is represented by b bits. The simplest way to parallelize radix sort is to use a parallel version of counting sort (e.g., <ref> [CLR90, BLM + 91] </ref>) as the internal stable sort. In superstep 1, each processor counts the number of its n=p elements with each possible value in [0; 2 r ), where r is the number of bits considered in each iteration.
Reference: [CKP + 96] <author> D.E. Culler, R. Karp, D. Patterson, A. Sahay, E. Santos, K.E. Schauser, R. Subramonian, and T.V. Eicken. </author> <title> LogP: a practical model of computation. </title> <journal> Communications of the ACM, </journal> <volume> 39(11) </volume> <pages> 78-85, </pages> <month> Nov </month> <year> 1996. </year>
Reference-contexts: Similar in spirit to BSP, but based on different programming paradigms, are the CG Model [DFRC93], which focuses on algorithmic rather than architectural issues, and the LOGP model <ref> [CKP + 96] </ref>, a fully asynchronous model whose performance predictions tend to be more accurate than those provided by BSP but are still subject to the same limitations [BHP + 96, BGMZ97].
Reference: [DFRC93] <author> F. Dehne, A. Fabri, and A. Rau-Chaplin. </author> <title> Scalable parallel geometric algorithms for coarse-grained multicomputers. </title> <booktitle> In Proc. of the ACM Conference on Computational Geometry, </booktitle> <pages> page 298-307, </pages> <year> 1993. </year>
Reference-contexts: Similar in spirit to BSP, but based on different programming paradigms, are the CG Model <ref> [DFRC93] </ref>, which focuses on algorithmic rather than architectural issues, and the LOGP model [CKP + 96], a fully asynchronous model whose performance predictions tend to be more accurate than those provided by BSP but are still subject to the same limitations [BHP + 96, BGMZ97].
Reference: [FM70] <author> W. D. Frazer and A. C. McKellar. Samplesort: </author> <title> A sampling approach to minimal storage tree sorting. </title> <journal> J. ACM, </journal> <volume> 17(3) </volume> <pages> 496-507, </pages> <year> 1970. </year>
Reference-contexts: For example, if the execution time approaches the Bad prediction, then the application is likely experiencing a substantial amount of congestion/contention in its interaction with the memory hierarchy. The application programs implement three different sorting algorithms: sample sort <ref> [FM70, HC83, RV87, WS88] </ref>, column sort [L85], and a parallel version of radix sort [BLM + 91]. <p> Sample sort There have been many sorting algorithms proposed that use a random sample of the input elements to partition the input into distinct subproblems that can be sorted independently (see, e.g., <ref> [FM70, HC83, RV87, WS88] </ref>). The basic version we implemented consists of 5 supersteps.
Reference: [GHM + 96] <author> M. Goudreau, J.M.D. Hill, W. McColl, S. Rao, D.C. Stefanescu, T. Suel, and T. Tsantilas. </author> <title> A proposal for the BSP worldwide standard library. </title> <type> Technical report, </type> <institution> Oxford University Computing Laboratory, </institution> <address> Wolfson Building, Parks Rd., Oxford OX1 3QD, UK, </address> <year> 1996. </year>
Reference-contexts: The large body of work this model has generated has proved its suitability for the development of portable software (see e.g., <ref> [GHM + 96] </ref>). However, as has been often observed, the simple BSP cost model offers only a coarse level of predictivity, since it disregards architectural features of real machines which may have a dramatic impact on performance [BGMZ97, JW96].
Reference: [GMR97] <author> P.B. Gibbons, Y. Matias, and V. Ramachandran. </author> <title> Can a shared-memory model serve as a bridging-model for parallel computation? In Proc. </title> <booktitle> of the 9th ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pages 72-83, </pages> <year> 1997. </year>
Reference-contexts: Both these models target distributed-memory architectures, where communication is realized via message passing. Bulk-synchronous models specifically tailored for shared-memory systems are the (d; x)-BSP [BGMZ97] and the QSM <ref> [GMR97] </ref>, whose cost functions account for some aspects of memory contention, namely, the maximum number of concurrent accesses to the same memory location (QSM), and the maximum number of concurrent accesses to the same memory bank ((d; x)-BSP). <p> In order to study communication in isolation, we consider a bulk-synchronous programming model, similar in spirit to the one adopted in <ref> [GMR97] </ref>, in which a program is organized as a sequence of supersteps, where in a superstep, local computation and reads/writes to the shared memory are made in distinct phases separated by barriers. <p> In this respect, our objective differs from those that motivated other works concerned with modeling shared-memory systems <ref> [GMR97, BGMZ97] </ref>. The QSM model proposed in [GMR97] attempts to strike a balance between descriptivity and generality, with the intent to enhance the effectiveness of algorithm design while maintaining a high level view of a machine. <p> In this respect, our objective differs from those that motivated other works concerned with modeling shared-memory systems [GMR97, BGMZ97]. The QSM model proposed in <ref> [GMR97] </ref> attempts to strike a balance between descriptivity and generality, with the intent to enhance the effectiveness of algorithm design while maintaining a high level view of a machine. <p> As a consequence, accesses to 3 local variables do not involve processor interaction, while communication is realized only through global accesses. As mentioned in the introduction, we will conduct our study referring to a bulk-synchronous programming style <ref> [Val90, GMR97] </ref>. More specifically, we regard a program as made of a sequence of supersteps, where each superstep consists of three consecutive phases: copy-in, local computation and copy-out. In the copy-in phase, each processor transfers the contents of the global variables needed for its computation into local ones.
Reference: [HP96] <author> J.L. Hennessy and D.A. Patterson. </author> <title> Computer Architecture A Quantitative Approach (Second Ed.). </title> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA, </address> <year> 1996. </year> <month> 11 </month>
Reference-contexts: let hr i (resp., hw i ) denote the number of global reads (resp., writes) performed by Processor i in 1 We remark that it is outside the scope of the paper to provide an accurate prediction model for the time required by local computation, and refer the reader to <ref> [HP96] </ref> for an extensive treatment of this topic. 4 that superstep, with 1 i p.
Reference: [HC83] <author> J. S. Huang and Y. C. Chow. </author> <title> Parallel sorting and data partitioning by sampling. </title> <booktitle> In Proceedings of the IEEE Computer Society's Seventh International Computer Software and Applications Conference, </booktitle> <pages> pages 627-631, </pages> <year> 1983. </year>
Reference-contexts: For example, if the execution time approaches the Bad prediction, then the application is likely experiencing a substantial amount of congestion/contention in its interaction with the memory hierarchy. The application programs implement three different sorting algorithms: sample sort <ref> [FM70, HC83, RV87, WS88] </ref>, column sort [L85], and a parallel version of radix sort [BLM + 91]. <p> Sample sort There have been many sorting algorithms proposed that use a random sample of the input elements to partition the input into distinct subproblems that can be sorted independently (see, e.g., <ref> [FM70, HC83, RV87, WS88] </ref>). The basic version we implemented consists of 5 supersteps.
Reference: [JW96] <author> B.H.H. Juurlink and H.A.G. Wijshoff. </author> <title> A quantitative comparison of parallel computation models. </title> <booktitle> In Proc. of the 8th ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pages 13-24, </pages> <month> June </month> <year> 1996. </year>
Reference-contexts: However, as has been often observed, the simple BSP cost model offers only a coarse level of predictivity, since it disregards architectural features of real machines which may have a dramatic impact on performance <ref> [BGMZ97, JW96] </ref>. <p> Among these, we mention the BSP* [BDM95] and the E-BSP <ref> [JW96] </ref>, which extend the BSP cost model to account for block transfer and unbalanced communication, respectively. Both these models target distributed-memory architectures, where communication is realized via message passing. <p> We provide evidence that the most consistently accurate functions are those which account separately for reads and writes, and include a measure of the overall communication volume. (A similar conclusion was drawn in <ref> [JW96] </ref> for parallel systems based on message-passing.) * Prediction Interval.
Reference: [L85] <author> T. Leighton. </author> <title> Tight bounds on the complexity of parallel sorting. </title> <journal> IEEE Trans. Comput., </journal> <volume> c-34(4):344-354, </volume> <year> 1985. </year>
Reference-contexts: For example, if the execution time approaches the Bad prediction, then the application is likely experiencing a substantial amount of congestion/contention in its interaction with the memory hierarchy. The application programs implement three different sorting algorithms: sample sort [FM70, HC83, RV87, WS88], column sort <ref> [L85] </ref>, and a parallel version of radix sort [BLM + 91]. These algorithms were chosen because they are well-understood parallel algorithms that exhibit a variety of communication patterns. 5.1 Sorting algorithm implementations All algorithms were coded in accordance with the programming model described in Section 3.
Reference: [Ncs97] <institution> NCSA webpage for SGI Power Challenge architecture and configuration. </institution> <note> http://www.ncsa.uiuc.edu/SCD/Hardware/PCA/Doc/Arch.html. </note>
Reference-contexts: Furthermore, as acknowledged by the authors, the (d; x)-BSP does not capture memory hierarchy effects, which crucially affect performance on SMPs. 2 The Architecture The SGI Power Challenge (SGI PC) [Sil95] is a shared-memory multiprocessor architecture. The NCSA system used in our study <ref> [Ncs97] </ref> is based on the MIPS superscalar RISC R10000 chip, which is a 64-bit processor that uses the MIPS IV instruction set.
Reference: [RV87] <author> J. H. Reif and L. G. Valiant. </author> <title> A logarithmic time sort for linear size networks. </title> <journal> Journal of the ACM, </journal> <volume> 34(1) </volume> <pages> 60-76, </pages> <year> 1987. </year>
Reference-contexts: For example, if the execution time approaches the Bad prediction, then the application is likely experiencing a substantial amount of congestion/contention in its interaction with the memory hierarchy. The application programs implement three different sorting algorithms: sample sort <ref> [FM70, HC83, RV87, WS88] </ref>, column sort [L85], and a parallel version of radix sort [BLM + 91]. <p> Sample sort There have been many sorting algorithms proposed that use a random sample of the input elements to partition the input into distinct subproblems that can be sorted independently (see, e.g., <ref> [FM70, HC83, RV87, WS88] </ref>). The basic version we implemented consists of 5 supersteps.
Reference: [Sil95] <institution> Silicon Graphics Corporation 1995. </institution> <note> SGI Power Challenge: User's Guide, </note> <year> 1995. </year>
Reference-contexts: Furthermore, as acknowledged by the authors, the (d; x)-BSP does not capture memory hierarchy effects, which crucially affect performance on SMPs. 2 The Architecture The SGI Power Challenge (SGI PC) <ref> [Sil95] </ref> is a shared-memory multiprocessor architecture. The NCSA system used in our study [Ncs97] is based on the MIPS superscalar RISC R10000 chip, which is a 64-bit processor that uses the MIPS IV instruction set.
Reference: [SRG94] <author> J.P. Singh, E. Rothberg and A. Gupta. </author> <title> Modeling communication in parallel algorithms: a fruitful interaction between theory and systems? In Proc. </title> <booktitle> of the 6th ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pages 189-199, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Thus, the prediction interval may be employed as a profiling tool to gain valuable insights into the application's interaction with the memory hierarchy, which may in turn lead to better algorithm design <ref> [SRG94] </ref>. Our conclusions are based on an extensive set of experiments designed to identify the quantities that affect the cost of communication and to assess the relative impact of these quantities.
Reference: [Val90] <author> L.G. Valiant. </author> <title> A bridging model for parallel computation. </title> <journal> Communications of the ACM, </journal> <volume> 33(8) </volume> <pages> 103-111, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: In particular, we study the impact of the memory hierarchy on performance and determine under which conditions simple cost functions can guarantee accurate performance predictions. Bulk-Synchronous Models One of the first and most popular attempts to define a bridging model was made by Valiant <ref> [Val90] </ref>, who introduced the Bulk-Synchronous Parallel (BSP) model. The model abstracts a parallel machine as a set of processors with local memory, connected through a communication medium of limited bandwidth. The computation is organized as a sequence of supersteps separated by barrier synchronizations, where processors operate asynchronously in a superstep. <p> As a consequence, accesses to 3 local variables do not involve processor interaction, while communication is realized only through global accesses. As mentioned in the introduction, we will conduct our study referring to a bulk-synchronous programming style <ref> [Val90, GMR97] </ref>. More specifically, we regard a program as made of a sequence of supersteps, where each superstep consists of three consecutive phases: copy-in, local computation and copy-out. In the copy-in phase, each processor transfers the contents of the global variables needed for its computation into local ones.

References-found: 20

