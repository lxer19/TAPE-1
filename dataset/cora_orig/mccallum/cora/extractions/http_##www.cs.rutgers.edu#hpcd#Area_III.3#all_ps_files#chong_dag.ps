URL: http://www.cs.rutgers.edu/hpcd/Area_III.3/all_ps_files/chong_dag.ps
Refering-URL: http://www.cs.rutgers.edu/hpcd/Area_III.3/index.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Parallel Processing Letters: Special Issue on Partitioning and Scheduling, December 1995. c World Scientific Publishing
Author: Frederic T. Chong Shamik D. Sharmay, Eric A. Brewerz, and Joel Saltzy Communicated by Gerasoulos and Yang 
Keyword: irregular, fine grain, directed acyclic graphs, sparse matrix, scheduling, com munication  
Date: Received February 1995 Revised August 1995  
Address: 545 Technology Square, Cambridge, Massachusetts 02139 USA  College Park, Maryland 20742 USA  Berkeley, California 94720 USA  
Affiliation: Laboratory for Computer Science, Massachusetts Institute of Technology  yDepartment of Computer Science, University of Maryland  zComputer Science Division, University of California at Berkeley  
Abstract: We examine multiprocessor runtime support for fine-grained, irregular directed acyclic graphs (DAGs) such as those that arise from sparse-matrix triangular solves. We conduct our experiments on the CM-5, whose lower latencies and active-message support allow us to achieve unprecedented speedups for a general multiprocessor. Where as previous implementations have maximum speedups of less than 4 on even simple banded matrices, we are able to obtain scalable performance on extremely small and irregular problems. On a matrix with only 5300 rows, we are able to achieve scalable performance with a speedup of 34 for 128 processors, resulting in an absolute performance of over 33 million double-precision floating point operations per second. We achieve these speedups with non-matrix-specific methods which are applicable to any DAG. We compare a range of run-time preprocessed and dynamic approaches on matrices from the Harwell-Boeing benchmark set. Although precomputed data distributions and execution schedules produce the best performance, we find that it is challenging to keep their cost low enough to make them worthwhile on small, fine-grained problems. Additionally, we find that a policy of frequent network polling can reduce communication overhead by a factor of three over the standard CM-5 policies. We present a detailed study of runtime overheads and demonstrate that send and receive processor overhead still dominate these applications on the CM-5. We conclude that these applications would highly benefit from architectural support for low-overhead communication. Contact: ftchong@lcs.mit.edu, http://www.ai.mit.edu/people/ftchong/ Funding: Fred Chong is supported in part by ARPA contract N00014-94-I-09885, NSF Experimental Systems grant MIP-9012773, and an NSF PYI Award to Anant Agarwal. Shamik Sharma and Joel Saltz are supported in part by NSF/NASA grant ASC9213821, NASA/ARPA grant NAG-1-1485, ONR/ARPA grant N00014-94-1-0907, and EPRI grant RP3103-06. Eric Brewer is supported in part by TITAN and Mammoth, NSF grants CDA-8722788 and CDA-9401156. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> J. H. Saltz, R. Mirchandaney, and K. Crowley, </author> <title> "Run-time parallelization and scheduling of loops," </title> <journal> IEEE Transactions on Computers, </journal> <pages> pp. 603-611, </pages> <year> 1991. </year>
Reference: 2. <author> T. v. Eicken et al., </author> <title> "Active messages: a mechanism for integrated communication and computation," </title> <booktitle> in ISCA, </booktitle> <month> May </month> <year> 1992. </year>
Reference: 3. <author> Arvind, D. E. Culler, and G. K. Maa, </author> <title> "Assessing the benefits of fine-grained parallelism in dataflow programs," </title> <booktitle> in Supercomputing `88, IEEE, </booktitle> <year> 1988. </year>
Reference: 4. <author> M. T. Heath and P. Raghavan, </author> <title> "Distributed solution of sparse linear systems," </title> <institution> Tech Rpt UIUCDCS-R-93-1793, Dept of Comp Sci, U of IL, Urbana, </institution> <address> IL 61801, </address> <month> Feb </month> <year> 1993. </year>
Reference: 5. <author> S. A. Moyer, </author> <title> "Performance of the iPSC/860 node architecture," </title> <type> Tech Rep IPC-TR-91-007, </type> <institution> Inst for Par Comp, U of VA, </institution> <address> Charlottesville, VA 22903, </address> <month> May </month> <year> 1991. </year>
Reference: 6. <author> R. F. Lucas, T. Blank, and J. J. Tiemann, </author> <title> "A parallel solution method for large sparse systems of equations," </title> <journal> IEEE Trans on CAD, </journal> <pages> pp. 981-991, </pages> <month> Nov </month> <year> 1987. </year>
Reference: 7. <author> F. Alvarado and R. Schreiber, </author> <title> "Optimal parallel solution of sparse triangular systems," </title> <journal> SIAM Journal of Scientific and Statistical Computation, </journal> <year> 1992. </year>
Reference: 8. <author> F. T. Chong and R. Schreiber, </author> <title> "Parallel sparse triangular solution with partitioned inverses and prescheduled DAGs," in Workshop on Solving Irregular Problems on Distributed Memory Machines, </title> <address> (Santa Barbara, California), </address> <month> April </month> <year> 1995. </year> <title> 9. "Analysis of performance accelerator running ETMSP," </title> <institution> Tech Rpt TR-102856, Performance Processors Inc, </institution> <address> Palo Alto, CA 94301, </address> <month> October </month> <year> 1993. </year> <month> Proj 8010-31. </month>
Reference: 10. <institution> Thinking Machines Corp, Cambridge, MA, </institution> <type> CM-5 Technical Summary, </type> <month> Nov </month> <year> 1993. </year>
Reference: 11. <author> C. E. Leiserson et al., </author> <title> "The network architecture of the connection machine CM-5," </title> <booktitle> in SPAA, </booktitle> <address> (San Diego, California), </address> <pages> pp. 272-285, </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1992. </year>
Reference: 12. <author> E. A. Brewer and R. D. Blumofe, "Strata: </author> <title> A high-performance communications library." </title> <type> Technical Report. </type> <institution> MIT Laboratory for Computer Science, </institution> <month> January </month> <year> 1994. </year>
Reference: 13. <institution> Thinking Machines Corp, </institution> <address> Cambridge, MA, </address> <note> CMMD Reference Manual V3.0, </note> <year> 1993. </year>
Reference: 14. <author> A. Gerasoulis and T. Yang, </author> <title> "A comparison of clustering heuristics for scheduling directed acyclic graphs on multiprocessors," </title> <journal> JPDC, </journal> <volume> vol. 16, </volume> <pages> pp. 276-291, </pages> <year> 1992. </year>
Reference: 15. <author> V. Sarkar, </author> <title> Partitioning and Scheduling Parallel Programs for Execution on Multiprocessors. </title> <publisher> MIT Press, </publisher> <year> 1989. </year>
Reference: 16. <author> J. H. Saltz, </author> <title> "Aggregation methods for solving sparse triangular systems on multiprocessors," </title> <journal> SIAM J. of Sci. and Stat. Comp., </journal> <volume> vol. 11, </volume> <pages> pp. 123-144, </pages> <month> Jan </month> <year> 1990. </year>
Reference: 17. <author> I. S. Duff, R. G. Grimes, and J. G. Lewis, </author> <title> "User's guide for the Harwell-Boeing sparse matrix collection," </title> <type> Technical Report TR/PA/92/86, CERFACS, 42 Ave G. Coriolis, </type> <address> 31057 Toulouse Cedex, France, </address> <month> October </month> <year> 1992. </year>
Reference: 18. <author> E. A. Brewer and B. C. Kuszmaul, </author> <title> "How to get good performance from the CM-5 data network," </title> <booktitle> in IPPS, </booktitle> <month> April </month> <year> 1994. </year>

References-found: 17

