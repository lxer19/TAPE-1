URL: http://www.cm.deakin.edu.au/~zijian/Papers/sascb-tr-C98-12.ps.gz
Refering-URL: http://www.cm.deakin.edu.au/~zijian/publications.html
Root-URL: 
Title: Integrating Boosting and Stochastic Attribute Selection Committees for Further Improving the Performance of Decision Tree Learning  
Author: Zijian Zheng, Geoffrey I. Webb, and Kai Ming Ting 
Keyword: Boosting, Decision tree learning, Inductive learning, Machine learning, Data mining.  
Address: Geelong Victoria 3217, Australia  
Affiliation: School of Computing and Mathematics Deakin University,  
Pubnum: Technical Report (TR C98/12)  
Email: fzijian,webb,kmtingg@deakin.edu.au  
Date: May, 1998  
Abstract: Techniques for constructing classifier committees including Boosting and Bagging have demonstrated great success, especially Boosting for decision tree learning. This type of technique generates several classifiers to form a committee by repeated application of a single base learning algorithm. The committee members vote to decide the final classification. Boosting and Bagging create different classifiers by modifying the distribution of the training set. Sasc (Stochastic Attribute Selection Committees) uses an alternative approach to generating classifier committees by stochastic manipulation of the set of attributes considered at each node during tree induction, but keeping the distribution of the training set unchanged. In this paper, we propose a method for improving the performance of Boosting. This technique combines Boosting and Sasc. It builds classifier committees by manipulating both the distribution of the training set and the set of attributes available during induction. In the synergy, Sasc effectively increases the model diversity of Boosting. Experiments with a representative collection of natural domains show that, on average, the combined technique outperforms both Boosting and Sasc in terms of reducing the error rate of decision tree learning. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Ali, K.M. </author> <year> 1996. </year> <title> Learning Probabilistic Relational Concept Descriptions. </title> <type> Ph.D. </type> <institution> diss., Dept of Info. and Computer Science, Univ. of California, Irvine. </institution>
Reference: <author> Bauer, E. and Kohavi, R. </author> <year> 1998. </year> <title> An Empirical Comparison of Voting Classification Algorithms: Bagging, Boosting, and Variants. </title> <note> Submitted to Machine Learning (available at: http://reality.sgi.com/ronnyk/vote.ps.gz). </note>
Reference: <author> Breiman, L. </author> <year> 1996a. </year> <title> Bagging Predictors. </title> <booktitle> Machine Learning 24: </booktitle> <pages> 123-140. </pages> <note> 16 Breiman, </note> <author> L. </author> <year> 1996b. </year> <note> Arcing Classifiers. Technical Report (available at: </note> <institution> http://www.stat.Berkeley.EDU/users/breiman/). Department of Statistics, University of California, Berkeley, </institution> <address> CA. </address>
Reference-contexts: If classifiers in a committee partition the instance space differently, and most points in the instance space are correctly covered by the majority of the committee, then the committee has a lower error rate than the individual classifiers. Bagging <ref> (Breiman 1996a) </ref> and Boosting (Schapire 1990; Freund 1996; Fre-und and Schapire 1996a; 1996b; Schapire et al. 1997), as two representative methods of this type, can significantly decrease the error rate of decision tree learning (Quinlan 1996; Freund and Schapire 1996b; Bauer and Kohavi 1998) with Boosting being generally better than Bagging
Reference: <author> Chan, P., Stolfo, S., and Wolpert, D. </author> <year> 1996. </year> <title> Working Notes of AAAI Workshop on Integrating Multiple Learned Models for Improving and Scaling Machine Learning Algorithms (available at http://www.cs.fit.edu/~imlm/papers.html), Portland, </title> <address> Oregon. </address>
Reference: <author> Dietterich, T.G. and Bakiri, G. </author> <year> 1995. </year> <title> Solving Multiclass Learning Problems via Error-correcting Output Codes. </title> <journal> Journal of Artificial Intelligence Research 2: </journal> <pages> 263-286. </pages>
Reference-contexts: Our analysis suggests that the improvement is achieved through increasing model diversity by Sasc, in addition to that by Boosting. There are some other classifier committee learning approaches such as generating multiple trees by manually changing learning parameters (Kwok and Carter 1990), error-correcting output codes <ref> (Dietterich and Bakiri 1995) </ref>, and generating different classifiers by randomizing the base learning process (Di-etterich and Kong 1995; Ali 1996) which is similar to Sasc (Zheng and Webb 1998). Reviews of related methods are provided in Dietterich (1997) and Ali (1996).
Reference: <author> Dietterich, T.G. and Kong, E.B. </author> <year> 1995. </year> <title> Machine Learning Bias, Statistical Bias, and Statistical Variance of Decision Tree Algorithms. </title> <type> Technical Report, </type> <institution> Dept of Computer Science, Oregon State University, Corvallis, Oregon (available at ftp://ftp.cs.orst.edu/pub/tgd/papers/tr-bias.ps.gz). </institution>
Reference-contexts: Our analysis suggests that the improvement is achieved through increasing model diversity by Sasc, in addition to that by Boosting. There are some other classifier committee learning approaches such as generating multiple trees by manually changing learning parameters (Kwok and Carter 1990), error-correcting output codes <ref> (Dietterich and Bakiri 1995) </ref>, and generating different classifiers by randomizing the base learning process (Di-etterich and Kong 1995; Ali 1996) which is similar to Sasc (Zheng and Webb 1998). Reviews of related methods are provided in Dietterich (1997) and Ali (1996).
Reference: <author> Dietterich, T.G. </author> <year> 1997. </year> <journal> Machine Learning Research. AI Magazine 18: </journal> <pages> 97-136. </pages>
Reference-contexts: The main difference between Bagging and Boosting is that the latter adaptively changes the distribution of the training set based on the performance of previously created classifiers and uses a 1 Committees are also referred to as ensembles <ref> (Dietterich 1997) </ref>. 2 Breiman (1996b) refers to Boosting as the arcing (adaptively resample and combine) method. 2 function of the performance of a classifier as the weight for voting, while the former uses equal weight voting.
Reference: <author> Domingos, P. </author> <year> 1997. </year> <title> Why does Bagging Work? a Bayesian Account and its Implications. </title> <booktitle> In Proceedings of the Third International Conference on Knowledge Discovery and Data Mining, </booktitle> <pages> 155-158. </pages> <publisher> AAAI Press. </publisher>
Reference: <author> Freund, Y. </author> <year> 1996. </year> <title> Boosting a Weak Learning Algorithm by Majority. </title> <booktitle> Information and Computation 121(2): </booktitle> <pages> 256-285. </pages>
Reference: <author> Freund, Y. and Schapire, R.E. </author> <year> 1996a. </year> <title> A Decision-theoretic Generalization of Online Learning and an Application to Boosting. </title> <note> Unpublished manuscript (available at: http://www.research.att.com/~yoav). </note>
Reference: <author> Freund, Y. and Schapire, R.E. </author> <year> 1996b. </year> <title> Experiments with a New Boosting Algorithm. </title> <booktitle> In Proceedings of the Thirteenth International Conference on Machine Learning, </booktitle> <pages> 148-156. </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kohavi, R. </author> <year> 1995. </year> <title> A Study of Cross-validation and Bootstrap for Accuracy Estimation and Model Selection. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> 1137-1143. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This test suite covers a wide variety of different domains with respect to dataset size, the number of classes, the number of attributes, and types of attributes. In every domain, two stratified 10-fold cross-validations <ref> (Kohavi 1995) </ref> were carried out for each algorithm. The result reported for each algorithm in each domain is an average value over 20 trials. All the algorithms are run on the same training and test set partitions with their default option settings.
Reference: <author> Kwok, S.W. and Carter, C. </author> <year> 1990. </year> <title> Multiple Decision Trees. </title> <editor> In Schachter, R.D., Levitt, T.S., Kanal, L.N., and Lemmer, J.F. (eds.), </editor> <booktitle> Uncertainty in Artificial Intelligence, </booktitle> <pages> 327-335. </pages> <publisher> Elsevier Science. </publisher>
Reference-contexts: Our analysis suggests that the improvement is achieved through increasing model diversity by Sasc, in addition to that by Boosting. There are some other classifier committee learning approaches such as generating multiple trees by manually changing learning parameters <ref> (Kwok and Carter 1990) </ref>, error-correcting output codes (Dietterich and Bakiri 1995), and generating different classifiers by randomizing the base learning process (Di-etterich and Kong 1995; Ali 1996) which is similar to Sasc (Zheng and Webb 1998). Reviews of related methods are provided in Dietterich (1997) and Ali (1996).
Reference: <author> Merz, C.J. and Murphy, P.M. </author> <year> 1997. </year> <note> UCI Repository of machine learning databases [http://www.ics.uci.edu/~mlearn/MLRepository.html]. Irvine, </note> <institution> CA: Univ of Cal-ifornia, Dept of Info and Computer Science. </institution>
Reference-contexts: We also explore whether SascB can outperform Boost and Sasc in terms of lower error rate. SascB is compared with C4.5, Boost, and Sasc, using error rate as the primary performance metric. 4.1 Experimental Domains and Methods Forty natural domains from the UCI machine learning repository <ref> (Merz and Murphy 1997) </ref> are used. They include all the domains used by Quinlan (1996) for studying Boosting. Table 1 summarizes the characteristics of these domains, including dataset size, the number of classes, the number of numeric attributes, and the number of discrete attributes.
Reference: <author> Quinlan, J.R. </author> <year> 1993. </year> <title> C4.5: Program for Machine Learning. </title> <address> San Mateo, CA: </address> <publisher> Mor-gan Kaufmann. 17 Quinlan, </publisher> <editor> J.R. </editor> <booktitle> 1996. Bagging, Boosting, and C4.5. In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> 725-730. </pages> <publisher> AAAI Press. </publisher>
Reference-contexts: At the classification stage, the committee members vote to make the final decision. Given a training set described using a set of attributes, conventional classifier learning algorithms such as decision tree learning algorithms <ref> (Quinlan 1993) </ref> build one classifier. Usually, the classifier is correct for most parts of the instance space, but incorrect for some small parts of the instance space. <p> d (x) = 1 if H t correctly classifies x and d (x) = 0 otherwise. 2.2 S ASC During the growth of a decision tree, at each decision node, a decision tree learning algorithm searches for the best attribute to form a test based on some test selection functions <ref> (Quinlan 1993) </ref>. The key idea of Sasc is to vary the 3 To make the algorithm efficient, this step is limited to 10 fi T times. 4 members of a decision tree committee by stochastic manipulation of the set of attributes available for selection at decision nodes. <p> In order to have a good quality tree in the sense that it can correctly cover most parts of the instance space, the tests used at decision nodes should be as good as possible with respect to the test selection function employed. We use C4.5 <ref> (Quinlan 1993) </ref> with the modifications described below as the base classifier learning algorithm in Sasc. When building a decision node, by default C4.5 uses the information gain ratio to search for the best attribute to form a test (Quinlan 1993). <p> We use C4.5 <ref> (Quinlan 1993) </ref> with the modifications described below as the base classifier learning algorithm in Sasc. When building a decision node, by default C4.5 uses the information gain ratio to search for the best attribute to form a test (Quinlan 1993). To force C4.5 to generate different trees using the same training set, we modified C4.5 by stochastically restricting the set of attributes available for selection at a decision node. <p> This is performed by tracing the example down to a leaf of the tree. The class distribution for the example is estimated using the proportion of the training examples of each class at the leaf, if the leaf is not empty. This is the same as C4.5 <ref> (Quinlan 1993) </ref>. When the leaf contains no training examples, C4.5 produces a class distribution with the labeled class of the leaf having the probability 1, and all other classes having the probability 0. In this case, Boost and Sasc are different from C4.5.
Reference: <author> Schapire, R.E. </author> <year> 1990. </year> <title> The Strength of Weak Learnability. </title> <booktitle> Machine Learning 5: </booktitle> <pages> 197-227. </pages>
Reference: <author> Schapire, R.E., Freund, Y., Bartlett, P., and Lee, W.S. </author> <year> 1997. </year> <title> Boosting the Margin: </title>
References-found: 17

