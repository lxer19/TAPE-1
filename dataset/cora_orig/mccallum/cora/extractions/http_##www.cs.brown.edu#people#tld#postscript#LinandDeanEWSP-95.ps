URL: http://www.cs.brown.edu/people/tld/postscript/LinandDeanEWSP-95.ps
Refering-URL: http://www.cs.brown.edu/people/tld/
Root-URL: 
Email: Email: fshl,tldg@cs.brown.edu  
Phone: Phone: (401) 863-7600 Fax: (401) 863-7657  
Title: Generating Optimal Policies for High-level Plans with Conditional Branches and Loops  
Author: Shieu-Hong Lin Thomas Dean 
Keyword: planning, action representation, uncertainty, stochastic domains, decision-theoretic planning, Markov decision processes  
Address: 115 Waterman Street Providence, RI 02906, USA  
Affiliation: Brown University Department of Computer Science  
Abstract: We are concerned with generating optimal policies for Markov decision processes that are represented as high-level plans with conditional branches and loops. Often complex planning processes can be broken down into elementary plan steps with associated restricted sets of actions. These plan steps can be combined to form high-level plans using a simple programming language specifying conditionals, loops, and sequences involving the plan steps as primitive statements. It is infeasible to directly generate and solve the underlying Markov decision process, since the size of the state space is exponential in the size of a high-level plan. We address the problem of efficiently computing an optimal policy by taking advantage of locality structure in the high-level plan. The main result is the specification and analysis of an algorithm that takes as input a high-level plan and provides as output an optimal policy for the underlying Markov decision process.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Corman, T. H., Leiserson, C. E., and Rivest, R. L., </author> <title> Algorithms, </title> <publisher> (MIT Press, </publisher> <address> Cambridge, Mas-sachusetts, </address> <year> 1991). </year>
Reference-contexts: Output: An optimal policy for the plan. 1. Identify the coherent fragments and derive the fragment graph from the high-level plan H by finding strongly-connected components in H <ref> [1] </ref>. 2. Identify (i) the sets of active variables for coherent fragments, (ii) the sets of terminal variables, starting variables, and coupling variables among coherent fragments by scanning through the plan steps in H. 3. Repeat Step 4 and Step 5 until all coherent fragments are processed. 4.
Reference: [2] <author> Dean, Thomas and Kanazawa, Keiji, </author> <title> A Model for Reasoning About Persistence and Causation, </title> <booktitle> Computational Intelligence, </booktitle> <month> 5(3) </month> <year> (1989) </year> <month> 142-150. </month>
Reference-contexts: Locality in the cause-and-effect relationship can enable us to represent the costs and conditional probabilities associated with actions and action outcomes more compactly as probabilistic state-space operators as in [4] or as temporal probabilistic networks as in <ref> [2] </ref>.
Reference: [3] <author> Dean, Thomas and Lin, Shieu-Hong, </author> <title> Decomposition Techniques for Planning in Stochastic Domains, </title> <type> Technical Report CS-95-, </type> <institution> Brown University Department of Computer Science, </institution> <year> 1995. </year>
Reference-contexts: The general idea of restricting reachability in state space by enabling some actions and disabling others is common in control theory. See Ramadge and Wonhan [9] for 13 an survey of work on discrete event systems that concerns the synthesis of supervisory systems. Dean and Lin <ref> [3] </ref> describe a family of techniques for decomposing the computations required to solve large Markov decision processes. This family of techniques complements the representations and algorithms investigated in this paper.
Reference: [4] <author> Hanks, Steve and McDermott, Drew V., </author> <title> Modeling a Dynamic and Uncertain World I: Symbolic and Probabilistic Reasoning About Change, </title> <journal> Artificial Intelligence, </journal> <year> (1994). </year>
Reference-contexts: Locality in the cause-and-effect relationship can enable us to represent the costs and conditional probabilities associated with actions and action outcomes more compactly as probabilistic state-space operators as in <ref> [4] </ref> or as temporal probabilistic networks as in [2].
Reference: [5] <author> Lansky, Amy L., </author> <title> Localized Event-Based Reasoning for Multiagent Domains, </title> <booktitle> Computational Intelligence, </booktitle> <month> 4(4) </month> <year> (1988). </year>
Reference-contexts: The particular methods for compiling fragment graphs are adapted from the work of Lin and Dean [6] on expediting temporal inference, which in turn builds on the work of Lansky <ref> [5] </ref>, Tenenberg [11] and others in exploiting locality planning. The general idea of restricting reachability in state space by enabling some actions and disabling others is common in control theory.
Reference: [6] <author> Lin, Shieu-Hong and Dean, Thomas, </author> <title> Exploiting Locality in Temporal Reasoning, </title> <editor> Sandewall, E. and Backstrom, C., (Eds.), </editor> <booktitle> Current Trends in AI Planning, </booktitle> <address> Amsterdam, </address> <publisher> IOS Press, </publisher> <year> 1994. </year>
Reference-contexts: The algorithm is loosely based on the work of Lin and Dean <ref> [6] </ref> for answering queries in temporal reasoning problems. Assuming a reasonable amount of locality embedded in a high-level plan, our analysis shows that this algorithm can compute an optimal policy in time polynomial in the size of the high-level plan. The remainder of this paper is organized as follows. <p> Related Work The particular representation of Markov processes in terms of high-level plans with conditional branches and loops that serves as inspiration for this paper is due to Smith and Williamson [10]. The particular methods for compiling fragment graphs are adapted from the work of Lin and Dean <ref> [6] </ref> on expediting temporal inference, which in turn builds on the work of Lansky [5], Tenenberg [11] and others in exploiting locality planning. The general idea of restricting reachability in state space by enabling some actions and disabling others is common in control theory.
Reference: [7] <author> Papadimitriou, Christos H. and Tsitsiklis, John N., </author> <title> The Complexity of Markov Chain Decision Processes, </title> <institution> Mathematics of Operations Research, </institution> <month> 12(3) </month> <year> (1987) </year> <month> 441-450. </month>
Reference-contexts: A high-level plan as described above represents a Markov decision process [10]. There exist standard techniques that can determine an optimal policy for a Markov decision process in time polynomial in the size of the given state space <ref> [7] </ref> [8]. However, the size of the state space of the underlying Markov decision process is exponential in the number of state variables appearing in the high-level plan. <p> Deriving Optimal Policies of Plans The following algorithm exploit locality structure embedded in a high-level plan to expedite the derivation of an optimal policy. 6 This can be achieved by applying standard techniques to solve Markov decision processes <ref> [7] </ref> [8]. 11 Input: (i) the domain dynamics hV; Space (V ); A ; Pr; Costi, (ii) a high-level plan H = (N H ; E H ), (iii) a function of final reward P X i 2V r i X i , and (iv) the initial values of the variables
Reference: [8] <author> Puterman, Martin L., </author> <title> Markov Decision Processes, </title> <publisher> (John Wiley & Sons, </publisher> <address> New York, </address> <year> 1994). </year>
Reference-contexts: A high-level plan as described above represents a Markov decision process [10]. There exist standard techniques that can determine an optimal policy for a Markov decision process in time polynomial in the size of the given state space [7] <ref> [8] </ref>. However, the size of the state space of the underlying Markov decision process is exponential in the number of state variables appearing in the high-level plan. <p> Deriving Optimal Policies of Plans The following algorithm exploit locality structure embedded in a high-level plan to expedite the derivation of an optimal policy. 6 This can be achieved by applying standard techniques to solve Markov decision processes [7] <ref> [8] </ref>. 11 Input: (i) the domain dynamics hV; Space (V ); A ; Pr; Costi, (ii) a high-level plan H = (N H ; E H ), (iii) a function of final reward P X i 2V r i X i , and (iv) the initial values of the variables in
Reference: [9] <author> Ramadge, Peter and Wonham, Murray, </author> <title> The Control of Discrete Event Systems, </title> <booktitle> Proceedings of the IEEE, </booktitle> <month> 77(1) </month> <year> (1989) </year> <month> 81-98. </month>
Reference-contexts: By disabling some actions and enabling others in different steps of the decision making process, reachability in the search space can be considerably localized. The work on supervisory controllers in the study of discrete event systems is predicated on this basic idea <ref> [9] </ref>. For example, if I tell you to travel to Chicago from Providence, the search space for constructing a travel plan is quite large. <p> The general idea of restricting reachability in state space by enabling some actions and disabling others is common in control theory. See Ramadge and Wonhan <ref> [9] </ref> for 13 an survey of work on discrete event systems that concerns the synthesis of supervisory systems. Dean and Lin [3] describe a family of techniques for decomposing the computations required to solve large Markov decision processes. This family of techniques complements the representations and algorithms investigated in this paper.
Reference: [10] <author> Smith, David E. and Williamson, Mike, </author> <title> Representation and Evaluation of Plans with Loops, </title> <booktitle> To appear in the working notes for the 1994 Stanford Spring Symposium on Extended Theories of Action, </booktitle> <year> 1995. </year>
Reference-contexts: Appropriate languages for specifying loops and conditional include Petri nets and more restricted task network formalisms such as the one proposed in <ref> [10] </ref>. In this paper, we assume that for each initial state and action possible in that state we have a probability distribution governing the next state, and a cost function indicating the cost incurred for each possible next state. <p> A high-level plan as described above represents a Markov decision process <ref> [10] </ref>. There exist standard techniques that can determine an optimal policy for a Markov decision process in time polynomial in the size of the given state space [7] [8]. <p> Related Work The particular representation of Markov processes in terms of high-level plans with conditional branches and loops that serves as inspiration for this paper is due to Smith and Williamson <ref> [10] </ref>. The particular methods for compiling fragment graphs are adapted from the work of Lin and Dean [6] on expediting temporal inference, which in turn builds on the work of Lansky [5], Tenenberg [11] and others in exploiting locality planning.
Reference: [11] <author> Tenenberg, Josh D., </author> <title> Abstraction in Planning, </title> <editor> Allen, J. F., Kautz, H. A., Pelavin, R. N., and Tenenberg, J. D., (Eds.), </editor> <title> Reasoning about Plans, </title> <publisher> (Morgan Kaufmann, </publisher> <address> San Francisco, California, </address> <year> 1991), </year> <pages> 213-280. </pages>
Reference-contexts: The particular methods for compiling fragment graphs are adapted from the work of Lin and Dean [6] on expediting temporal inference, which in turn builds on the work of Lansky [5], Tenenberg <ref> [11] </ref> and others in exploiting locality planning. The general idea of restricting reachability in state space by enabling some actions and disabling others is common in control theory. See Ramadge and Wonhan [9] for 13 an survey of work on discrete event systems that concerns the synthesis of supervisory systems.
References-found: 11

