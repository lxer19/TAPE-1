URL: http://www.cs.berkeley.edu/~remzi/Postscript/masters.ps.gz
Refering-URL: http://http.cs.berkeley.edu/~remzi/papers.html
Root-URL: 
Title: Communication Behavior of a Distributed Operating System  
Author: Remzi H. Arpaci 
Affiliation: Department of Electrical Engineering and Computer Science Computer Science Division University of California, Berkeley  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> T. Anderson, M. Dahlin, J. Neefe, D. Patterson, and R. Wang. </author> <title> Serverless Network File Systems. </title> <booktitle> In Fifteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 109-26, </pages> <address> Copper Mountain Resort, CO, USA, </address> <month> Dec </month> <year> 1995. </year>
Reference-contexts: We will investigate this further in the next section by studying a simple request-response example. One final observation that arises from the experiments pertains to distributed system design. The benefits 30 of a more advanced striping file system are apparent <ref> [1, 13] </ref>. In Solaris MC, certain nodes can easily become hot-spots, and thus performance bottlenecks, simply because they serve the files for the current workload. A system that stripes blocks across a set of the nodes would load balance the system automatically.
Reference: [2] <author> T. E. Anderson, S. S. Owicki, J. Saxe, and C. Thacker. </author> <title> High-speed Switch Scheduling for Local-Area Networks. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(4) </volume> <pages> 319-352, </pages> <month> Nov </month> <year> 1993. </year>
Reference-contexts: Not surprisingly, many researchers have focused their efforts on design and implementation of fast communication protocols [4, 7, 9, 14]. The recent arrival of high-speed, switch-based local-area networks has improved communication performance by an order of magnitude <ref> [2, 6] </ref>. The bandwidth of many of these networks is in the 100 MB/s range, and, by making use of lightweight communication protocols, one-way end-to-end times are in the range of 10 to 100 microseconds [18, 32].
Reference: [3] <author> M. Baker, J. Hartman, M. Kupfer, K. Shirriff, and J. Ousterhout. </author> <title> Measurements of a Distributed File System. </title> <booktitle> In Proceedings of the Thirteenth Symposium on Operating Systems Principles, </booktitle> <pages> pages 198-212, </pages> <address> Pacific Grove, CA, USA, </address> <month> oct </month> <year> 1991. </year>
Reference-contexts: Of course, message sizes in this case are directly influenced by file sizes; previous studies of file sizes in Unix environments have shown that these have a similar most files small, most data in large files distribution <ref> [3] </ref>. In Figure 4.4, we see that the message sizes for the web workload are much more regular than the make workload; there are only about 10 different message sizes sent throughout the lifetime of the experiment.
Reference: [4] <author> F. Baskett and J. H. Howard. </author> <title> Task communicaton in DEMOS. </title> <booktitle> In Proceedings of the ACM Symposium on Operating System Principles, </booktitle> <pages> pages 23-32, </pages> <address> Reading, MA, USA, </address> <year> 1977. </year>
Reference-contexts: Along these lines, early work often indicated that achieving high network performance was paramount to attaining good overall distributed system performance [11, 31]. Not surprisingly, many researchers have focused their efforts on design and implementation of fast communication protocols <ref> [4, 7, 9, 14] </ref>. The recent arrival of high-speed, switch-based local-area networks has improved communication performance by an order of magnitude [2, 6].
Reference: [5] <author> J. Bernebeu, V. Matena, and Y. Khalidi. </author> <title> Extending a Traditional OS Using Object-Oriented Techniques. </title> <note> To appear in COOTS 96, </note> <year> 1996. </year>
Reference-contexts: In Solaris MC, the ORB also provides other features such as reference counting on objects as well as support for one-way communication. For a full description of the ORB, see <ref> [5] </ref>. 9 2.2 Experimental Method This section describes our experimental method. First, we describe the hardware and software platform used for the experiments. Then we explain how we instrumented Solaris MC, and the performance effects that arose from the instrumentation.
Reference: [6] <author> N. Boden, D. Cohen, R. Felderman, A. Kulawik, and C. Seitz. Myrinet: </author> <title> A Gigabit-per-second Local Area Network. </title> <journal> IEEE Micro, </journal> <volume> 15(1) </volume> <pages> 29-36, </pages> <month> Feb </month> <year> 1995. </year>
Reference-contexts: Not surprisingly, many researchers have focused their efforts on design and implementation of fast communication protocols [4, 7, 9, 14]. The recent arrival of high-speed, switch-based local-area networks has improved communication performance by an order of magnitude <ref> [2, 6] </ref>. The bandwidth of many of these networks is in the 100 MB/s range, and, by making use of lightweight communication protocols, one-way end-to-end times are in the range of 10 to 100 microseconds [18, 32]. <p> Most experiments are performed on an 8-CPU, 4-node cluster of SPARC-10s, each with 64 MB of memory. Each machine has an Ethernet connection to the outside world, a connection to the fast intra-cluster network, the Myrinet local-area network <ref> [6] </ref>, and possibly an extra disk which acts as part of the global file system.
Reference: [7] <author> D. R. Cheriton. VMTP: </author> <title> A Transport Protocol for the Next Generation of Communication Systems. </title> <booktitle> In Proceedings of ACM SIGCOMM, </booktitle> <pages> pages 406-415, </pages> <address> Stowe, VT, </address> <month> Aug. </month> <year> 1986. </year>
Reference-contexts: Along these lines, early work often indicated that achieving high network performance was paramount to attaining good overall distributed system performance [11, 31]. Not surprisingly, many researchers have focused their efforts on design and implementation of fast communication protocols <ref> [4, 7, 9, 14] </ref>. The recent arrival of high-speed, switch-based local-area networks has improved communication performance by an order of magnitude [2, 6].
Reference: [8] <author> D. R. Cheriton. </author> <title> The V Distributed System. </title> <journal> Communications of the ACM, </journal> <volume> 31(3) </volume> <pages> 12-28, </pages> <month> Mar. </month> <year> 1988. </year>
Reference-contexts: Introduction "The known is finite, the unknown infinite; intellectually we stand on an islet in the midst of an illimitable ocean of inexplicability. Our business in every generation is to reclaim a little more land." -Thomas H. Huxley Distributed systems have long been an active area of research <ref> [8, 12, 22, 24, 25] </ref>. A distributed system is comprised of many components, including process management, networking, and file systems. Although the requirements of these various services are eclectic, they have a common need: to communicate.
Reference: [9] <author> D. R. Cheriton and C. L. Williamson. </author> <title> Network Measurement of the VMTP Request-Response Protocol in the V Distributed System. </title> <booktitle> In Proceedings of ACM SIGCOMM, </booktitle> <pages> pages 216-225, </pages> <address> Stowe, VT, </address> <month> Aug. </month> <year> 1987. </year>
Reference-contexts: Along these lines, early work often indicated that achieving high network performance was paramount to attaining good overall distributed system performance [11, 31]. Not surprisingly, many researchers have focused their efforts on design and implementation of fast communication protocols <ref> [4, 7, 9, 14] </ref>. The recent arrival of high-speed, switch-based local-area networks has improved communication performance by an order of magnitude [2, 6].
Reference: [10] <author> J. Chu. </author> <title> Zero-copy TCP in Solaris. </title> <booktitle> In Proceedings of the 1996 Usenix Technical Conference, </booktitle> <pages> pages 253-64, </pages> <address> San Diego, CA, USA, </address> <month> Jan. </month> <year> 1996. </year>
Reference-contexts: In addition, nodes that serve as file servers are more likely to send large blocks of data. Thus it is especially important to optimize large message sends from these nodes, perhaps with the support of zero-copy message layers <ref> [10] </ref>.
Reference: [11] <author> F. Douglis, J. K. Ousterhout, M. F. Kaashoek, and A. S. Tanenbaum. </author> <title> A Comparison of Two Distributed Systems: Amoeba and Sprite. </title> <journal> Computing Systems, </journal> <volume> 4(4) </volume> <pages> 353-383, </pages> <month> Fall </month> <year> 1991. </year>
Reference-contexts: Although the requirements of these various services are eclectic, they have a common need: to communicate. Along these lines, early work often indicated that achieving high network performance was paramount to attaining good overall distributed system performance <ref> [11, 31] </ref>. Not surprisingly, many researchers have focused their efforts on design and implementation of fast communication protocols [4, 7, 9, 14]. The recent arrival of high-speed, switch-based local-area networks has improved communication performance by an order of magnitude [2, 6].
Reference: [12] <author> R. Finkel and M. Scott. </author> <title> Experience with Charlotte: Simplicity and function in a distributed operating system. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 15 </volume> <pages> 676-686, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: Introduction "The known is finite, the unknown infinite; intellectually we stand on an islet in the midst of an illimitable ocean of inexplicability. Our business in every generation is to reclaim a little more land." -Thomas H. Huxley Distributed systems have long been an active area of research <ref> [8, 12, 22, 24, 25] </ref>. A distributed system is comprised of many components, including process management, networking, and file systems. Although the requirements of these various services are eclectic, they have a common need: to communicate.
Reference: [13] <author> J. Hartman and J. Ousterhout. </author> <title> The Zebra Striped Network File System. </title> <booktitle> In Proceedings of the Fourteenth Symposium on Operating Systems Principles, </booktitle> <pages> pages 29-43, </pages> <address> Ashville, NC, USA, </address> <month> Dec. </month> <year> 1993. </year> <month> 42 </month>
Reference-contexts: We will investigate this further in the next section by studying a simple request-response example. One final observation that arises from the experiments pertains to distributed system design. The benefits 30 of a more advanced striping file system are apparent <ref> [1, 13] </ref>. In Solaris MC, certain nodes can easily become hot-spots, and thus performance bottlenecks, simply because they serve the files for the current workload. A system that stripes blocks across a set of the nodes would load balance the system automatically.
Reference: [14] <author> M. F. Kaashoek, R. van Renesse, H. van Staveren, and A. S. Tanenbaum. </author> <title> FLIP: an Internetwork Protocol for Supporting Distributed Systems. </title> <type> Technical report, </type> <institution> Department of Mathematics and Computer Science, Vrije Universiteit, </institution> <month> July </month> <year> 1991. </year>
Reference-contexts: Along these lines, early work often indicated that achieving high network performance was paramount to attaining good overall distributed system performance [11, 31]. Not surprisingly, many researchers have focused their efforts on design and implementation of fast communication protocols <ref> [4, 7, 9, 14] </ref>. The recent arrival of high-speed, switch-based local-area networks has improved communication performance by an order of magnitude [2, 6].
Reference: [15] <author> Y. Khalidi, J. Bernebeu, V. Matena, K. Shirriff, and M. Thadani. </author> <title> Solaris MC: A Multi-Computer OS. </title> <booktitle> In 1996 Winter Usenix, </booktitle> <pages> pages 205-218. </pages> <publisher> USENIX, </publisher> <month> Jan. </month> <year> 1996. </year>
Reference-contexts: The system under scrutiny is Solaris MC, a prototype cluster operating system <ref> [15] </ref>. MC is novel in a number of ways: it extends a real, commercial kernel (Solaris) into a distributed system; further, it does so by building on top of a distributed object system based on CORBA [30]. <p> First, we give a high-level overview of the concepts and philosophy behind the system. Then, we outline the particulars of the experimental environment. 2.1 Solaris MC Solaris MC <ref> [15] </ref> is a prototype multi-computer operating system, where a multi-computer is a cluster of homogeneous computers connected via a high-speed interconnect. Solaris MC provides a single-system image, constructing the illusion of a single machine to users, applications, and the external network.
Reference: [16] <author> Y. A. Khalidi and M. N. Nelson. </author> <title> Extensible File Systems in Spring. </title> <booktitle> In Proceedings of the Fourteenth Symposium on Operating Systems Principles, </booktitle> <address> Ashville, NC, USA, </address> <month> Dec </month> <year> 1993. </year>
Reference-contexts: Further, all nodes see a single path hierarchy for all accessible files. To ensure UNIX file system semantics, coherency protocols are employed. For performance, PXFS makes use of extensive caching based on 8 techniques found in Spring <ref> [16] </ref>, and will eventually provide zero-copy bulk I/O of large data objects. The second major subsystem is process management. In Solaris MC, process management is globalized such that the location of a process is transparent to the user.
Reference: [17] <author> A. M. Mainwaring. </author> <title> Active Message Application Programming Interface and Communication Subsystem Organization. </title> <type> Master's thesis, </type> <institution> University of California, Berkeley, </institution> <year> 1995. </year>
Reference-contexts: The existence of buffer chains implies that the message layer must either support a gather interface, or perform an extra copy and buffer allocation before sending the data. However, a gathering interface is not present in some of today's fast messaging layers <ref> [17, 33] </ref>. The only alternative is to significantly restructure the MC code to always allocate extra space within each object in anticipation of the object system's demands. The gather cost is directly proportional to memory copy costs, plus the the cost of buffer allocation.
Reference: [18] <author> R. P. Martin. HPAM: </author> <title> An Active Message Layer for a Network of Workstations. </title> <booktitle> In Proceedings of Hot Interconnects II, </booktitle> <month> July </month> <year> 1994. </year>
Reference-contexts: The bandwidth of many of these networks is in the 100 MB/s range, and, by making use of lightweight communication protocols, one-way end-to-end times are in the range of 10 to 100 microseconds <ref> [18, 32] </ref>. This provides a quantum leap over the shared-medium 10 Mb/s Ethernet upon which many previous systems were designed. With this radical change underway, it is important to understand and characterize how modern distributed systems make use of new communication technologies.
Reference: [19] <author> V. Matena, J. Bernabeu, and Y. A. Khalidi. </author> <title> High Availability Support in Solaris MC. </title> <type> Technical report, </type> <institution> Sun Microsystem Laboratories, Moutain View, CA, </institution> <note> To appear in 1996. </note>
Reference-contexts: Solaris MC provides a single-system image, constructing the illusion of a single machine to users, applications, and the external network. The existing Solaris API/ABI is preserved, such that existing Solaris 2.x applications and device drivers run without modification. Finally, MC provides support for high availability <ref> [19] </ref>. Solaris MC is comprised of four major subsystems: the file system, process management, networking, and I/O. Extensions were made to each of these components of a normal Solaris kernel in order to attain the aforementioned goals.
Reference: [20] <author> L. McVoy and C. Staelin. lmbench: </author> <title> Portable Tools for Performance Analysis. </title> <booktitle> In Proceedings of the 1996 Winter USENIX, </booktitle> <month> Jan. </month> <year> 1996. </year>
Reference-contexts: The make, web, and database workloads average 1334, 2850, and 2851 context switches per second. One interesting point about the context switch rate is its intensity; if a context switch takes about 30 s <ref> [20] </ref>, 3000 switches per second implies 3000 30s = 90; 000s = 90ms of time spent switching per second. 90 milliseconds of switch time every second is almost 10% of all time. 3.4 Summary In this section, we have used a number of measures of system usage - CPU utilization, system
Reference: [21] <author> J. Mogul, R. Rashid, and M. Accetta. </author> <title> The Packet Filter: An Efficient Mechanism For User-Level Network Code. </title> <booktitle> In Proceedings of the Eleventh Symposium on Operating Systems Principles, </booktitle> <pages> pages 39-51, </pages> <address> Austin, TX, USA, </address> <month> nov </month> <year> 1987. </year>
Reference-contexts: All requests are sent to M C 2 which is connected to an Ethernet network interface. After a request is received, the networking subsystem redirects the traffic for this connection to one of the nodes in the system, similar to the Mach packet filter <ref> [21] </ref>. All responses are forwarded from the responding cluster node to M C 2 and then back to the original source. The disk containing the HTTP files is connected to M C 2 .
Reference: [22] <author> S. J. Mullender, G. V. Rossum, A. S. Tanenbaum, R. V. Renesse, and H. van Staveren. </author> <title> Amoeba A Distributed Operating System for the 1990's. </title> <journal> IEEE Computer Magazine, </journal> <volume> 33(12) </volume> <pages> 46-63, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Introduction "The known is finite, the unknown infinite; intellectually we stand on an islet in the midst of an illimitable ocean of inexplicability. Our business in every generation is to reclaim a little more land." -Thomas H. Huxley Distributed systems have long been an active area of research <ref> [8, 12, 22, 24, 25] </ref>. A distributed system is comprised of many components, including process management, networking, and file systems. Although the requirements of these various services are eclectic, they have a common need: to communicate.
Reference: [23] <institution> NCSA HTTPd Development Team. </institution> <note> NCSA HTTPd. http://hoohoo.ncsa.uiuc.edu, Apr. </note> <year> 1996. </year>
Reference-contexts: In this benchmark, an external stream of HTTP requests is generated and sent to the MC cluster, each node of which is running a copy of the NCSA httpd server <ref> [23] </ref>. All requests are sent to M C 2 which is connected to an Ethernet network interface. After a request is received, the networking subsystem redirects the traffic for this connection to one of the nodes in the system, similar to the Mach packet filter [21].
Reference: [24] <author> J. K. Ousterhout, A. R. Cherenson, F. Douglis, M. N. Nelson, and B. B. Welch. </author> <title> The Sprite Network Operating System. </title> <journal> IEEE Computer, </journal> <volume> 21(2) </volume> <pages> 23-36, </pages> <month> Feb. </month> <year> 1988. </year>
Reference-contexts: Introduction "The known is finite, the unknown infinite; intellectually we stand on an islet in the midst of an illimitable ocean of inexplicability. Our business in every generation is to reclaim a little more land." -Thomas H. Huxley Distributed systems have long been an active area of research <ref> [8, 12, 22, 24, 25] </ref>. A distributed system is comprised of many components, including process management, networking, and file systems. Although the requirements of these various services are eclectic, they have a common need: to communicate.
Reference: [25] <editor> G. J. Popek and B. J. Walker, editors. </editor> <booktitle> The LOCUS Distributed System Architecture, </booktitle> <pages> pages 73-89. </pages> <booktitle> Computer Systems Series. </booktitle> <publisher> The MIT Press, </publisher> <year> 1985. </year>
Reference-contexts: Introduction "The known is finite, the unknown infinite; intellectually we stand on an islet in the midst of an illimitable ocean of inexplicability. Our business in every generation is to reclaim a little more land." -Thomas H. Huxley Distributed systems have long been an active area of research <ref> [8, 12, 22, 24, 25] </ref>. A distributed system is comprised of many components, including process management, networking, and file systems. Although the requirements of these various services are eclectic, they have a common need: to communicate.
Reference: [26] <author> D. M. Ritchie and K. Thompson. </author> <title> The UNIX Time-Sharing System. </title> <journal> Communications of the ACM, </journal> <volume> 17(7) </volume> <pages> 365-75, </pages> <month> July </month> <year> 1974. </year>
Reference-contexts: We now briefly explain each component of MC, as well as the underlying object substrate. The Solaris MC file system, PXFS (the proxy file system), extends the local Unix file system to a distributed environment <ref> [26] </ref>. In PXFS, all file access is location transparent. A process anywhere in the system can open a file located upon any disk in the system. PXFS accomplishes this by interposing at the vnode layer [34], where PXFS can intercept file operations and forward them to the correct physical node.
Reference: [27] <author> D. Schmidt and T. Suda. </author> <title> Transport System Architecture Services For High-performance Communications Systems. </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> 11(4) </volume> <pages> 489-506, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Beneath the ORB is the transport layer. The transport provides a reliable send of an arbitrary buffer chain to any node in the system. Currently, it is implemented as a STREAMS module <ref> [27] </ref> that provides a reliable message transport on top of any STREAMS-based device driver. <p> on the potential areas a communication layer or network interface should optimize for, and also attempt to understand how much overhead the object system requires, in terms of bytes sent. 6.1 Buffer Chains buffers, similar to the structure of mbufs found in older Unix systems, or mblks in STREAMS vernacular <ref> [27] </ref>. Though a message may have an arbitrarily large number of buffers in it, our first measurement shows that this is not the case. Table 6.1 shows the percent of messages that are sent as buffer chains consisting of 1, 2, and 3 buffers for each of the workloads.
Reference: [28] <institution> SPEC Design Committee. </institution> <address> SPECweb. http://www.specbench.org/osg/web/, Apr. </address> <year> 1996. </year>
Reference-contexts: The disk holding the source tree is attached to M C 3 . This workload stresses remote execution facilities as well as PXFS. 10 The second workload, web, is based on the Spec WWW benchmark <ref> [28] </ref>. In this benchmark, an external stream of HTTP requests is generated and sent to the MC cluster, each node of which is running a copy of the NCSA httpd server [23]. All requests are sent to M C 2 which is connected to an Ethernet network interface.
Reference: [29] <institution> SunSoft Technical Writers. TNF Manual Page. Solaris AnswerBook, </institution> <year> 1996. </year>
Reference-contexts: In Section 5, the cluster configuration is slightly different; it is explained further therein. 2.2.2 Tracing To trace the communication behavior of Solaris MC, we make extensive use of the TNF tracing facility in Solaris 2.5 <ref> [29] </ref>. This utility allows events in the kernel to be time-stamped and logged to a kernel buffer, which can then easily be extracted and analyzed. Each call to log an event takes roughly 7 microseconds; therefore, insertion of many such calls along a critical path can seriously alter results.
Reference: [30] <author> F. Tibbitts. </author> <title> CORBA: A Common Touch For Distributed Applications. </title> <journal> Data Communications International, </journal> <volume> 24(7) </volume> <pages> 71-5, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: The system under scrutiny is Solaris MC, a prototype cluster operating system [15]. MC is novel in a number of ways: it extends a real, commercial kernel (Solaris) into a distributed system; further, it does so by building on top of a distributed object system based on CORBA <ref> [30] </ref>. MC extends the file system, process management, networking, and I/O subsystems of Unix to provide users with a single-system image. Although MC provides an interesting measurement testbed, it is currently in the early stages of development. Many aspects of the system have not yet been optimized.
Reference: [31] <author> R. van Rennesse and A. Tanenbaum. </author> <title> Amoeba: The World's Fastest Operating System. </title> <booktitle> In Operating Systems Review, </booktitle> <month> Dec </month> <year> 1989. </year> <month> 43 </month>
Reference-contexts: Although the requirements of these various services are eclectic, they have a common need: to communicate. Along these lines, early work often indicated that achieving high network performance was paramount to attaining good overall distributed system performance <ref> [11, 31] </ref>. Not surprisingly, many researchers have focused their efforts on design and implementation of fast communication protocols [4, 7, 9, 14]. The recent arrival of high-speed, switch-based local-area networks has improved communication performance by an order of magnitude [2, 6].
Reference: [32] <author> T. von Eicken, A. Basu, V. Buch, and W. Vogels. U-net: </author> <title> A user-level network interface for parallel and distributed computing. </title> <booktitle> In Proceedings of the Fifteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 40-53, </pages> <address> Copper Mountain Resort, CO, USA, </address> <year> 1995. </year>
Reference-contexts: The bandwidth of many of these networks is in the 100 MB/s range, and, by making use of lightweight communication protocols, one-way end-to-end times are in the range of 10 to 100 microseconds <ref> [18, 32] </ref>. This provides a quantum leap over the shared-medium 10 Mb/s Ethernet upon which many previous systems were designed. With this radical change underway, it is important to understand and characterize how modern distributed systems make use of new communication technologies.
Reference: [33] <author> T. von Eicken, D. E. Culler, S. C. Goldstein, and K. E. Schauser. </author> <title> Active Messages: a Mechanism for Integrated Communication and Computation. </title> <booktitle> In Proc. of the 19th International Symposium on Computer Architecture, </booktitle> <pages> pages 256-266, </pages> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: One implication for a message layer is that support for medium-sized messages between the sizes of 68 and 256 bytes is important; note that the 20-byte message size supported by early active message layers <ref> [33] </ref> is of no use in this environment. Though it is difficult to deem a certain workload typical, all the workloads made extensive use of these medium-sized messages. Specifically, during the make, web, and database workloads, 80% of messages were between 68 and 256 bytes. <p> The existence of buffer chains implies that the message layer must either support a gather interface, or perform an extra copy and buffer allocation before sending the data. However, a gathering interface is not present in some of today's fast messaging layers <ref> [17, 33] </ref>. The only alternative is to significantly restructure the MC code to always allocate extra space within each object in anticipation of the object system's demands. The gather cost is directly proportional to memory copy costs, plus the the cost of buffer allocation.
Reference: [34] <author> B. Welch. </author> <title> A Comparison of Three Distributed File System Architectures: Vnode, Sprite, and Plan 9. </title> <journal> Computing Systems, </journal> <volume> 7(2) </volume> <pages> 175-99, </pages> <month> Spring </month> <year> 1994. </year> <month> 44 </month>
Reference-contexts: In PXFS, all file access is location transparent. A process anywhere in the system can open a file located upon any disk in the system. PXFS accomplishes this by interposing at the vnode layer <ref> [34] </ref>, where PXFS can intercept file operations and forward them to the correct physical node. Further, all nodes see a single path hierarchy for all accessible files. To ensure UNIX file system semantics, coherency protocols are employed.
References-found: 34

