URL: http://www.cs.berkeley.edu/~nir/Papers/Fr1.ps
Refering-URL: http://http.cs.berkeley.edu/~nir/publications.html
Root-URL: 
Email: nir@cs.berkeley.edu  
Title: Learning Belief Networks in the Presence of Missing Values and Hidden Variables  
Author: Nir Friedman 
Address: 387 Soda Hall  Berkeley, CA 94720  
Affiliation: Computer Science Division,  University of California,  
Abstract: In recent years there has been a flurry of works on learning probabilistic belief networks. Current state of the art methods have been shown to be successful for two learning scenarios: learning both network structure and parameters from complete data, and learning parameters for a fixed network from incomplete datathat is, in the presence of missing values or hidden variables. However, no method has yet been demonstrated to effectively learn network structure from incomplete data. In this paper, we propose a new method for learning network structure from incomplete data. This method is based on an extension of the Expectation-Maximization (EM) algorithm for model selection problems that performs search for the best structure inside the EM procedure. We prove the convergence of this algorithm, and adapt it for learning belief networks. We then describe how to learn networks in two scenarios: when the data contains missing values, and in the presence of hidden variables. We provide experimental results that show the effectiveness of our procedure in both scenarios.
Abstract-found: 1
Intro-found: 1
Reference: <author> Beinlich, I., G. Suermondt, R. Chavez, and G. </author> <title> Cooper (1989). The ALARM monitoring system: A case study with two probabilistic inference techniques for belief networks. </title> <booktitle> In Proc. 2'nd European Conf. on AI and Medicine. </booktitle> <address> Berlin: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Binder, J., D. Koller, S. Russell, and K. </author> <title> Kanazawa (1997). Adaptive probabilistic networks with hidden variables. </title> <booktitle> Machine Learning this volume. </booktitle>
Reference: <author> Cheeseman, P., J. Kelly, M. Self, J. Stutz, W. Taylor, and D. </author> <title> Freeman (1988). Autoclass: a Bayesian classification system. </title> <note> In ML '88. </note>
Reference: <author> Chickering, D. M. and D. </author> <title> Heckerman (1996). Efficient approximations for the marginal likelihood of incomplete data given a Bayesian network. </title> <booktitle> In Proc. Twelfth Conference on Uncertainty in Artificial Intelligence (UAI '96), </booktitle> <pages> pp. 158-168. </pages>
Reference: <author> Cooper, G. F. and E. </author> <title> Herskovits (1992). A Bayesian method for the induction of probabilistic networks from data. </title> <booktitle> Machine Learning 9, </booktitle> <pages> 309-347. </pages>
Reference: <author> Dempster, A. P., N. M. Laird, and D. B. </author> <title> Rubin (1977). Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society B 39, </journal> <pages> 1-39. </pages>
Reference: <author> Heckerman, D. </author> <year> (1995). </year> <title> A tutorial on learning Bayesian networks. </title> <type> Technical Report MSR-TR-95-06, </type> <institution> Mi-crosoft Research. </institution>
Reference: <author> Heckerman, D., D. Geiger, and D. M. </author> <title> Chickering (1995). Learning Bayesian networks: The combination of knowledge and statistical data. </title> <booktitle> Machine Learning 20, </booktitle> <pages> 197-243. </pages>
Reference: <author> Heckerman, D., A. Mamdani, and M. P. </author> <title> Wellman (1995). Real-world applications of Bayesian networks. </title> <journal> Communications of the ACM 38. </journal>
Reference: <author> Lam, W. and F. </author> <title> Bacchus (1994). Learning Bayesian belief networks. An approach based on the MDL principle. </title> <booktitle> Computational Intelligence 10, </booktitle> <pages> 269-293. </pages>
Reference: <author> Lauritzen, S. L. </author> <year> (1995). </year> <title> The EM algorithm for graphical association models with missing data. </title> <journal> Computational Statistics and Data Analysis 19, </journal> <pages> 191-201. </pages>
Reference: <author> Lauritzen, S. L. and D. J. </author> <month> Spiegelhalter </month> <year> (1988). </year> <title> Local computations with probabilities on graphical structures and their application to expert systems. </title> <journal> Journal of the Royal Statistical Society B 50(2), </journal> <pages> 157-224. </pages>
Reference: <author> McLachlan, G. J. and T. </author> <title> Krishnan (1997). The EM Algorithm and Extensions. </title> <publisher> Wiley Interscience. </publisher>
Reference: <author> Murphy, P. M. and D. W. </author> <note> Aha (1995). UCI repository of machine learning databases. http://www.ics. uci.edu/mlearn/MLRepository.html. </note>
Reference: <author> Pearl, J. </author> <year> (1988). </year> <title> Probabilistic Reasoning in Intelligent Systems. </title> <address> San Francisco, Calif.: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Schwarz, G. </author> <year> (1978). </year> <title> Estimating the dimension of a model. </title> <journal> Annals of Statistics 6, </journal> <pages> 461-464. </pages>
Reference: <author> Tanner, M. A. </author> <year> (1993). </year> <title> Tools for Statistical Inference. </title> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
References-found: 17

