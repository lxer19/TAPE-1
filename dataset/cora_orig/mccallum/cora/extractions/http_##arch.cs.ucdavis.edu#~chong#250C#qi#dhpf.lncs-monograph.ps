URL: http://arch.cs.ucdavis.edu/~chong/250C/qi/dhpf.lncs-monograph.ps
Refering-URL: http://arch.cs.ucdavis.edu/~chong/250C/qi/
Root-URL: http://www.cs.ucdavis.edu
Title: Advanced Code Generation for High Performance Fortran  
Author: Vikram Adve and John Mellor-Crummey 
Address: Houston, Texas, USA  
Affiliation: Department of Computer Science and Center For Research on Parallel Computation, Rice University,  
Abstract: Summary. For data-parallel languages such as High Performance Fortran to achieve wide acceptance, parallelizing compilers must be able to provide consistently high performance for a broad spectrum of scientific applications. Although compilation of regular data-parallel applications for message-passing systems have been widely studied, current state-of-the-art compilers implement only a small number of key optimizations, and the implementations generally focus on optimizing programs using a "case-based" approach. For these reasons, current compilers are unable to provide consistently high levels of performance. In this paper, we describe techniques developed in the Rice dHPF compiler to address key code generation challenges that arise in achieving high performance for regular applications on message-passing systems. We focus on techniques required to implement advanced optimizations and to achieve consistently high performance with existing optimizations. Many of the core communication analysis and code generation algorithms in dHPF are expressed in terms of abstract equations manipulating integer sets. This approach enables general and yet simple implementations of sophisticated optimizations, making it more practical to include a comprehensive set of optimizations in data-parallel compilers. It also enables the compiler to support much more aggressive computation partitioning algorithms than in previous compilers. We therefore believe this approach can provide higher and more consistent levels of performance than are available today. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> V. Adve, J. Mellor-Crummey, and A. Sethi. </author> <title> HPF analysis and code generation using integer sets. </title> <type> Technical Report CS-TR97-275, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> April </month> <year> 1997. </year>
Reference-contexts: Optimizations we have formulated in this manner include message vectorization [14], message coalescing [43], recognizing in-place communication <ref> [1] </ref>, code generation for our general CP model [1], non-local index set splitting [32], control-flow simplification [34], and generalized loop-peeling for improving parallelism. <p> Optimizations we have formulated in this manner include message vectorization [14], message coalescing [43], recognizing in-place communication <ref> [1] </ref>, code generation for our general CP model [1], non-local index set splitting [32], control-flow simplification [34], and generalized loop-peeling for improving parallelism. By formulating these algorithms in terms of operations on integer sets, we are able to abstract away the details of the CPs, references, and data layouts for each problem instance. <p> Our aim in this chapter is to motivate and provide an overview of the techniques we use to address the challenges described above. The algorithms underlying these techniques are described and evaluated in detail elsewhere <ref> [1, 34] </ref>. In the following section, we use an example to describe 4 Vikram Adve and John Mellor-Crummey the basic steps in generating an explicit message-passing parallel program for HPF. <p> It appears much easier and more intuitive to express complex optimizations directly in terms of sequences of abstract set operations on integer sets, as shown in <ref> [1] </ref>. There is also a large body of work on techniques to enumerate communication sets and iteration sets in the presence of cyclic (k) distributions (e.g., [12, 17, 28, 35, 45]). <p> The details of our extensions to handle a symbolic number of processors are given in <ref> [1] </ref>. 4. Computation Partitioning A computation partitioning (CP) for a statement is a precise specification of which processor or processors must execute each dynamic instance of the statement. The CPs chosen by the compiler play a fundamental role in determining the performance of the resulting code. <p> This allows us to to Advanced Code Generation for High Performance Fortran 19 renumber the label definitions and any matching label references. The details of the renumbering scheme are described elsewhere <ref> [1] </ref>. The second key issue we address with the framework is controlling the compile-time cost of using expensive algorithms such as mmcodegen, while still ensuring that guard overhead is minimized. There are two features in the framework that ensure efficiency. <p> We then generate code from these sets directly. The integer set equations used to compute the communication sets for each logical communication event are described in detail elsewhere <ref> [1] </ref>. We briefly describe the key aspects of the algorithm here. <p> Similarly, if the CP for a write reference is replicated, a similar step ensures that only one writer sends the data back to each owner. To avoid communication bottlenecks, we ensure that all the owners (or writers) participate by providing data to different groups of destination processors <ref> [1] </ref>. For the case of coarse-grain pipelining, we use another additional step to account for the blocking of communication (i.e., the granularity of the pipeline). <p> These sets are computed using a sequence of integer set equations, taking as input the basic sets and maps of Figure 3.1 and the non-local data set, nlDataAccessed (m), computed as an intermediate result for the communication sets as described in Section 5.1. The detailed equations are explained in <ref> [1] </ref>. The key step is computing the iterations that access non-local data for each given reference. The non-local data set for the reference describes the non-local data accessed by the processor myid. Composing this map with Ref M ap 1 gives the iterations that access these non-local elements.
Reference: 2. <author> S. Amarasinghe and M. Lam. </author> <title> Communication optimization and code generation for distributed memory machines. </title> <booktitle> In Proceedings of the SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <address> Albuquerque, NM, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: Three groups have used a more abstract and general approach based on linear inequalities to support code generation for communication and iteration sets <ref> [2, 3, 4, 5] </ref>. In this approach, each code generation or communication optimization problem is described by a collection of linear inequalities representing integer sets or mappings. <p> In the SUIF compiler, these techniques were also applied to carry out specific optimizations including message vectorization, message coalescing (limited to certain combinations of references) and redundant message elimination <ref> [2] </ref>. The advantage of using linear inequalities over case-based approaches is that each optimization or code generation problem can be expressed and solved in abstract terms, independent of the specific forms of references, data layouts, and computation partitionings. Furthermore, Fourier-Motzkin elimination is applicable to arbitrary affine references and data layouts. <p> This formulation is made explicit in dHPF and in the SUIF compiler <ref> [2] </ref>, and similar formulations have been discussed elsewhere [32, 23]. The integer set framework in dHPF includes the representation of these primitive quantities as integer tuple sets and mappings, together with the operations to manipulate them and generate code from them. <p> Any compiler for a data-parallel language based on data distributions operates primarily on three types of tuple spaces, and the three pairwise mappings between these tuple spaces <ref> [32, 23, 2] </ref>. <p> The owner-computes rule amounts to a simple heuristic choice for partitioning a computation. It is straightforward to show that this approach is not optimal in general [12]. An alternate partitioning Advanced Code Generation for High Performance Fortran 13 strategy used by SUIF <ref> [2] </ref> and Barua, Kranz & Agarwal [6] requires a CP to be described by a single affine mapping of iterations to processors, and assigns a single CP to an entire loop iteration and not to individual statements in a loop. <p> The important patterns (particularly reductions and broadcast) have been supported in most data-parallel compilers. Message coalescing combines messages for multiple non-local references to the same or different variables, in order to reduce the total number of messages and to eliminate redundant communication. Previous implementations in Fortran 77D [24], SUIF <ref> [2] </ref>, Paradigm [5], and IBM's pHPF [11, 16] have some significant limitations. In particular, coalescing can produce fairly complex data sets from the union of data sets for individual references. <p> SUIF uses array dataflow analysis to communicate data directly from a processor executing a non-local write to the next processor executing a non-local read <ref> [2] </ref>, whereas dHPF must use an extra message to send the data first to the owner and from there to the reader. The former two optimizations can be directly added to the current implementation of dHPF.
Reference: 3. <author> C. Ancourt, F. Coelho, F. Irigoin, and R. Keryell. </author> <title> A linear algebra framework for static HPF code distribution. </title> <booktitle> In Proceedings of the Fourth Workshop on Compilers for Parallel Computers, </booktitle> <address> Delft, The Netherlands, </address> <month> December </month> <year> 1993. </year>
Reference-contexts: Three groups have used a more abstract and general approach based on linear inequalities to support code generation for communication and iteration sets <ref> [2, 3, 4, 5] </ref>. In this approach, each code generation or communication optimization problem is described by a collection of linear inequalities representing integer sets or mappings. <p> Code generation then amounts to generating loops to iterate over these index ranges. In the PIPS and Paradigm compilers, these techniques were primarily used for code generation for communication and iteration sets <ref> [3, 5] </ref>. In the SUIF compiler, these techniques were also applied to carry out specific optimizations including message vectorization, message coalescing (limited to certain combinations of references) and redundant message elimination [2].
Reference: 4. <author> C. Ancourt and F. Irigoin. </author> <title> Scanning polyhedra with do loops. </title> <booktitle> In Proceedings of the Third ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Williamsburg, VA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Three groups have used a more abstract and general approach based on linear inequalities to support code generation for communication and iteration sets <ref> [2, 3, 4, 5] </ref>. In this approach, each code generation or communication optimization problem is described by a collection of linear inequalities representing integer sets or mappings.
Reference: 5. <author> P. Banerjee, J. Chandy, M. Gupta, E. Hodges, J. Holm, A. Lain, D. Palermo, S. Ramaswamy, and E. Su. </author> <title> The Paradigm compiler for distributed-memory multicomputers. </title> <journal> IEEE Computer, </journal> <volume> 28(10) </volume> <pages> 37-47, </pages> <month> October </month> <year> 1995. </year>
Reference-contexts: Three groups have used a more abstract and general approach based on linear inequalities to support code generation for communication and iteration sets <ref> [2, 3, 4, 5] </ref>. In this approach, each code generation or communication optimization problem is described by a collection of linear inequalities representing integer sets or mappings. <p> Code generation then amounts to generating loops to iterate over these index ranges. In the PIPS and Paradigm compilers, these techniques were primarily used for code generation for communication and iteration sets <ref> [3, 5] </ref>. In the SUIF compiler, these techniques were also applied to carry out specific optimizations including message vectorization, message coalescing (limited to certain combinations of references) and redundant message elimination [2]. <p> In such cases, compilers add statement guards to implement the CPs and, except for Paradigm, don't reduce loop bounds. The Paradigm compiler reduces loop bounds to the convex hull of the iteration spaces of statements inside the loop, in order to reduce the number of guards executed <ref> [5] </ref>. Second, fragmenting a loop nest into a sequence of separate loops over individual statements can (a) significantly reduce reuse of cached values between statements, and (b) significantly increase the contribution of loop overhead to overall execution time. <p> Message coalescing combines messages for multiple non-local references to the same or different variables, in order to reduce the total number of messages and to eliminate redundant communication. Previous implementations in Fortran 77D [24], SUIF [2], Paradigm <ref> [5] </ref>, and IBM's pHPF [11, 16] have some significant limitations. In particular, coalescing can produce fairly complex data sets from the union of data sets for individual references. <p> It is an important optimization for effectively implementing parallelism in such loop nests because the only alternative may be to perform a full array redistribution, which can be much more expensive. To our knowledge, this optimization has been implemented in a few research compilers <ref> [24, 5] </ref> and one commercial one [16]. Optimizations to overlap communication with computation - Dataflow-based communication placement attempts to hide the latency of communication by placing message sends early and receives late so as to overlap messages with unrelated computation.
Reference: 6. <author> R. Barua, D. Kranz, and A. Agarwal. </author> <title> Communication-minimal partitioning of parallel loops and data arrays for cache-coherent distributed-memory multiprocessors. </title> <booktitle> In Proceedings of the Ninth Workshop on Languages and Compilers for Parallel Computing. </booktitle> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1996. </year>
Reference-contexts: The owner-computes rule amounts to a simple heuristic choice for partitioning a computation. It is straightforward to show that this approach is not optimal in general [12]. An alternate partitioning Advanced Code Generation for High Performance Fortran 13 strategy used by SUIF [2] and Barua, Kranz & Agarwal <ref> [6] </ref> requires a CP to be described by a single affine mapping of iterations to processors, and assigns a single CP to an entire loop iteration and not to individual statements in a loop.
Reference: 7. <author> S. Benkner, B. Chapman, and H. Zima. </author> <title> Vienna Fortran 90. </title> <booktitle> In Proceedings of the 1992 Scalable High Performance Computing Conference, </booktitle> <address> Williamsburg, VA, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: Code Generation To compute the above sets and to generate code using them, the primary approach in most previous research and commercial compilers has been to focus on individual common cases and to precompute the iteration and communication sets symbolically for specific forms of references, data layouts and computation partitionings <ref> [7, 10, 15, 16, 17, 19, 24, 32, 33, 42, 46] </ref>.
Reference: 8. <author> W. Blume and R. Eigenmann. </author> <title> Demand-driven symbolic range propagation. </title> <booktitle> In Proceedings of the Eighth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 141-160, </pages> <address> Columbus, OH, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: The foundation for control flow simplification in dHPF is an algorithm for globally propagating symbolic constraints on the values of variables imposed by loops, conditional branches, assertions, and integer computations. Several previous systems have supported strategies for computing and exploiting range information about variables <ref> [20, 8, 9, 25, 44] </ref>. Two key differences that distinguish our work are that we handle more general logical combinations of constraints on variables (not just ranges) and we use these constraints to simplify control flow.
Reference: 9. <author> Fran~cois Bourdoncle. </author> <title> Abstract debugging of higher-order imperative languages. </title> <booktitle> In Proceedings of the SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 46-55, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: The foundation for control flow simplification in dHPF is an algorithm for globally propagating symbolic constraints on the values of variables imposed by loops, conditional branches, assertions, and integer computations. Several previous systems have supported strategies for computing and exploiting range information about variables <ref> [20, 8, 9, 25, 44] </ref>. Two key differences that distinguish our work are that we handle more general logical combinations of constraints on variables (not just ranges) and we use these constraints to simplify control flow.
Reference: 10. <author> Z. Bozkus, L. Meadows, S. Nakamoto, V. Schuster, and M. Young. </author> <title> Compiling High Performance Fortran. </title> <booktitle> In Proceedings of the Seventh SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 704-709, </pages> <address> San Francisco, CA, </address> <month> February </month> <year> 1995. </year>
Reference-contexts: Code Generation To compute the above sets and to generate code using them, the primary approach in most previous research and commercial compilers has been to focus on individual common cases and to precompute the iteration and communication sets symbolically for specific forms of references, data layouts and computation partitionings <ref> [7, 10, 15, 16, 17, 19, 24, 32, 33, 42, 46] </ref>. <p> A loop fusion pass that attempts to recover cache locality and reduce loop bound overhead is possible, but complex. Both the IBM and the Portland Group HPF compilers use a loop fusion pass, but apply it only to the simplest cases, namely conformant loops <ref> [10, 16] </ref>. Kelly, Pugh & Rosser describe an aggressive algorithm to generate efficient code without relying on loop distribution, for a loop nest containing multiple statements with different iteration spaces [26].
Reference: 11. <author> S. Chakrabarti, M. Gupta, and J-D. Choi. </author> <title> Global communication analysis and optimization. </title> <booktitle> In Proceedings of the SIGPLAN '96 Conference on Programming Language Design and Implementation, </booktitle> <address> Philadelphia, PA, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: Message coalescing combines messages for multiple non-local references to the same or different variables, in order to reduce the total number of messages and to eliminate redundant communication. Previous implementations in Fortran 77D [24], SUIF [2], Paradigm [5], and IBM's pHPF <ref> [11, 16] </ref> have some significant limitations. In particular, coalescing can produce fairly complex data sets from the union of data sets for individual references. <p> This is useful, for example, in stencil computations that access diagonal neighbors such as a nine-point stencil over a two-dimensional array. Chakrabarti et al. describe a powerful communication placement algorithm that can be used to maximize opportunities for message coalescing or to balance message coalescing with communication overlap <ref> [11] </ref>. SUIF uses array dataflow analysis to communicate data directly from a processor executing a non-local write to the next processor executing a non-local read [2], whereas dHPF must use an extra message to send the data first to the owner and from there to the reader.
Reference: 12. <author> S. Chatterjee, J. Gilbert, R. Schreiber, and S. Teng. </author> <title> Optimal evaluation of array expressions on massively parallel machines. </title> <type> Technical Report CSL-92-11, </type> <institution> Xerox Corporation, </institution> <month> December </month> <year> 1992. </year> <title> Advanced Code Generation for High Performance Fortran 41 </title>
Reference-contexts: There is also a large body of work on techniques to enumerate communication sets and iteration sets in the presence of cyclic (k) distributions (e.g., <ref> [12, 17, 28, 35, 45] </ref>). <p> The owner-computes rule amounts to a simple heuristic choice for partitioning a computation. It is straightforward to show that this approach is not optimal in general <ref> [12] </ref>.
Reference: 13. <author> R. Cytron, J. Ferrante, B. Rosen, M. Wegman, and K. Zadeck. </author> <title> Efficiently computing static single assignment form and the control dependence graph. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(4) </volume> <pages> 451-490, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: These quantities are constructed from a powerful symbolic representation used in dHPF, namely global value numbering. A value number in dHPF is a handle for a symbolic expression tree. Value numbers are constructed from dataflow analysis of the program based on its Static Single Assignment (SSA) form <ref> [13] </ref>, such that any two subexpressions that are known to have identical runtime values are assigned the same value number [21]. Their construction subsumes expression simplification, constant propagation, auxiliary induction variable recognition, and computing range information for expressions of loop index variables. <p> The goal of our algorithm is to collect and propagate these constraints globally through the code and use this information to simplify the control flow. Our algorithm combines three key program analysis technologies, namely, the control dependence graph (CDG) <ref> [13] </ref>, global value numbering based on thinned gated single assignment form [21], and simplification of integer constraints specified as Presburger formulae [27].
Reference: 14. <author> M. Gerndt. </author> <title> Updating distributed variables in local computations. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 2(3) </volume> <pages> 171-193, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: Robust algorithms for communication and code generation: The core communication analysis, optimization, and code generation algorithms in dHPF are expressed in terms of abstract equations manipulating integer sets rather than as a collection of strategies for different cases. Optimizations we have formulated in this manner include message vectorization <ref> [14] </ref>, message coalescing [43], recognizing in-place communication [1], code generation for our general CP model [1], non-local index set splitting [32], control-flow simplification [34], and generalized loop-peeling for improving parallelism. <p> In a preliminary evaluation, the algorithm has proven highly effective at eliminating excess control-flow in the generated code. Furthermore, we find that the general purpose control-flow simplification algorithm provides some or all of the benefits of special-purpose optimizations such as vector-message pipelining [43] and overlap areas <ref> [14] </ref>. Our aim in this chapter is to motivate and provide an overview of the techniques we use to address the challenges described above. The algorithms underlying these techniques are described and evaluated in detail elsewhere [1, 34]. <p> As mentioned above, non-local index set splitting was implemented in a limited form in Kali. Overlap areas for shift communication are extra boundary elements added to the local sections of arrays involved in shift communication <ref> [14] </ref>. They permit local and non-local data to be referenced uniformly, thus avoiding the need for the access checks (or alternatives) mentioned above. Generally, interprocedural analysis has to be used to determine the size of required overlap areas globally for each array. <p> An interesting outcome we observed in our experiments is that the general purpose control-flow simplification algorithm (in combination with our CP code generation algorithm and loop splitting) provides some or all of the benefits of much more specialized optimizations such as vector-message pipelining [43] and overlap areas <ref> [14] </ref>. In particular, the pipelined shift communication pattern for Erlebacher shown in Fig. 6.3 is exactly what vector message-pipelining aims to produce, but the latter is a narrow optimization and complex to implement, as explained in [34]. In dHPF, however, it results naturally from loop-splitting, bounds reduction, and control-flow simplification.
Reference: 15. <author> M. Gupta and P. Banerjee. </author> <title> A methodology for high-level synthesis of communication for multicomputers. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <address> Washington, DC, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Code Generation To compute the above sets and to generate code using them, the primary approach in most previous research and commercial compilers has been to focus on individual common cases and to precompute the iteration and communication sets symbolically for specific forms of references, data layouts and computation partitionings <ref> [7, 10, 15, 16, 17, 19, 24, 32, 33, 42, 46] </ref>.
Reference: 16. <author> M. Gupta, S. Midkiff, E. Schonberg, V. Seshadri, D. Shields, K. Wang, W. Ching, and T. Ngo. </author> <title> An HPF compiler for the IBM SP2. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <address> San Diego, CA, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: Even within this class of applications, state-of-the-art commercial and research compilers do not consistently achieve performance competitive with hand-written code <ref> [16, 24] </ref>. <p> Code Generation To compute the above sets and to generate code using them, the primary approach in most previous research and commercial compilers has been to focus on individual common cases and to precompute the iteration and communication sets symbolically for specific forms of references, data layouts and computation partitionings <ref> [7, 10, 15, 16, 17, 19, 24, 32, 33, 42, 46] </ref>. <p> For loops containing multiple statements with different CPs, it is important that each processor execute as few guards as possible to determine which statement instances it must execute in each iteration. Previous compilers, such as IBM's pHPF compiler <ref> [16] </ref>, rely on loop distribution to construct separate loop nests containing statements with identical CPs, so as to avoid the need for run-time guards. There are two drawbacks to using loop distribution in this manner. First, loop distribution may be impossible because of cyclic data dependences in the loop. <p> A loop fusion pass that attempts to recover cache locality and reduce loop bound overhead is possible, but complex. Both the IBM and the Portland Group HPF compilers use a loop fusion pass, but apply it only to the simplest cases, namely conformant loops <ref> [10, 16] </ref>. Kelly, Pugh & Rosser describe an aggressive algorithm to generate efficient code without relying on loop distribution, for a loop nest containing multiple statements with different iteration spaces [26]. <p> This is implemented by virtually all data-parallel compilers, but in case-based compilers it is usually restricted to specific reference patterns for which the compiler can derive (or conservatively approximate) the data sets to be communicated <ref> [16, 24, 32] </ref>. Exploiting collective communication is essential for achieving good speedup in important cases such as reductions, broadcasts, and array redistribution [33]. On certain systems, collective communication primitives may also provide significant benefits for other patterns such as shift communication. <p> Message coalescing combines messages for multiple non-local references to the same or different variables, in order to reduce the total number of messages and to eliminate redundant communication. Previous implementations in Fortran 77D [24], SUIF [2], Paradigm [5], and IBM's pHPF <ref> [11, 16] </ref> have some significant limitations. In particular, coalescing can produce fairly complex data sets from the union of data sets for individual references. <p> To our knowledge, this optimization has been implemented in a few research compilers [24, 5] and one commercial one <ref> [16] </ref>. Optimizations to overlap communication with computation - Dataflow-based communication placement attempts to hide the latency of communication by placing message sends early and receives late so as to overlap messages with unrelated computation. <p> Even the latter may not need access checks if all non-local references now access only non-local data. The alternative to this transformation is to copy local and non-local data into a common buffer (as done in the IBM pHPF compiler <ref> [16] </ref>), which can be costly in time and memory usage. As mentioned above, non-local index set splitting was implemented in a limited form in Kali. Overlap areas for shift communication are extra boundary elements added to the local sections of arrays involved in shift communication [14]. <p> Some other specific communication optimizations have been implemented in other compilers but are not included in dHPF. IBM's pHPF coalesces diagonal shift communication into 24 Vikram Adve and John Mellor-Crummey two messages <ref> [16] </ref>, whereas dHPF requires three. This is useful, for example, in stencil computations that access diagonal neighbors such as a nine-point stencil over a two-dimensional array.
Reference: 17. <author> S. K. S. Gupta, S. D. Kaushik, C.-H. Huang, and P. Sadayappan. </author> <title> Compiling array expressions for efficient execution on distributed-memory machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 32(2) </volume> <pages> 155-172, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: Code Generation To compute the above sets and to generate code using them, the primary approach in most previous research and commercial compilers has been to focus on individual common cases and to precompute the iteration and communication sets symbolically for specific forms of references, data layouts and computation partitionings <ref> [7, 10, 15, 16, 17, 19, 24, 32, 33, 42, 46] </ref>. <p> There is also a large body of work on techniques to enumerate communication sets and iteration sets in the presence of cyclic (k) distributions (e.g., <ref> [12, 17, 28, 35, 45] </ref>).
Reference: 18. <author> R. v. Hanxleden. </author> <title> Compiler Support for Machine-Independent Parallelization of Irregular Problems. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> December </month> <year> 1994. </year>
Reference-contexts: A fourth problem (not directly related to a general CP model) is that static code generation techniques will not be useful for a code with irregular or complex partitionings. Such cases require runtime strategies such as the inspector-executor approach (e.g., <ref> [32, 18, 40] </ref>). However, regular and irregular partitionings may coexist in the same program, and perhaps even in a single loop nest. This raises the need for a flexible code generation framework that allows each part of the source program to be partitioned using the most efficient strategy applicable.
Reference: 19. <author> J. Harris, J. Bircsak, M. R. Bolduc, J. A. Diewald, I. Gale, N. Johnson, S. Lee, C. A. Nelson, and C. Offner. </author> <title> Compiling High Performance Fortran for distributed-memory systems. </title> <journal> Digital Technical Journal of Digital Equipment Corp., </journal> <volume> 7(3) </volume> <pages> 5-23, </pages> <month> Fall </month> <year> 1995. </year>
Reference-contexts: Code Generation To compute the above sets and to generate code using them, the primary approach in most previous research and commercial compilers has been to focus on individual common cases and to precompute the iteration and communication sets symbolically for specific forms of references, data layouts and computation partitionings <ref> [7, 10, 15, 16, 17, 19, 24, 32, 33, 42, 46] </ref>. <p> Some compilers simply replicate the execution of control-flow statements on all processors, which is a simple method to ensure correctness but can substantially reduce parallelism in loops containing control flow <ref> [19] </ref>. Many other compilers ignore control flow because loop bounds reduction and ownership guards will enforce the appropriate CP for enclosed statements. However, this approach sacrifices potential parallelism for the sake of simplifying code generation.
Reference: 20. <author> W. H. Harrison. </author> <title> Compiler analysis of the value ranges for variables. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-3(3):243-250, </volume> <month> May </month> <year> 1977. </year>
Reference-contexts: The foundation for control flow simplification in dHPF is an algorithm for globally propagating symbolic constraints on the values of variables imposed by loops, conditional branches, assertions, and integer computations. Several previous systems have supported strategies for computing and exploiting range information about variables <ref> [20, 8, 9, 25, 44] </ref>. Two key differences that distinguish our work are that we handle more general logical combinations of constraints on variables (not just ranges) and we use these constraints to simplify control flow.
Reference: 21. <author> Paul Havlak. </author> <title> Interprocedural Symbolic Analysis. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> May </month> <year> 1994. </year> <note> Also available as CRPC-TR94451 from the Center for Research on Parallel Computation and CS-TR94-228 from the Rice Department of Computer Science. </note>
Reference-contexts: Value numbers are constructed from dataflow analysis of the program based on its Static Single Assignment (SSA) form [13], such that any two subexpressions that are known to have identical runtime values are assigned the same value number <ref> [21] </ref>. Their construction subsumes expression simplification, constant propagation, auxiliary induction variable recognition, and computing range information for expressions of loop index variables. A value number can be reconstituted back into an equivalent code fragment that represents the value. for an example HPF code fragment. <p> Our algorithm combines three key program analysis technologies, namely, the control dependence graph (CDG) [13], global value numbering based on thinned gated single assignment form <ref> [21] </ref>, and simplification of integer constraints specified as Presburger formulae [27]. The former two enable us to derive an efficient, non-iterative algorithm for propagating constraints along control dependences, while the latter enables simplification of logical constraints and code generation from the simplified constraints.
Reference: 22. <author> High Performance Fortran Forum. </author> <title> High Performance Fortran language specification. </title> <booktitle> Scientific Programming, </booktitle> <address> 2(1-2):1-170, </address> <year> 1993. </year>
Reference-contexts: To permit a symbolic number of processors or cyclic (k) distribution with symbolic k, we use a virtual processor (VP) model that naturally matches the semantics of templates in HPF <ref> [22] </ref>. The VP model uses a virtual processor array for each physical processor array, using template indices (i.e., ignoring the distribute directive) in dimensions where the block size or number of processors is unknown, but using physical processor indices in all other dimensions.
Reference: 23. <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng. </author> <title> Compiler support for machine-independent parallel programming in Fortran D. </title> <editor> In J. Saltz and P. Mehro-tra, editors, </editor> <title> Languages, Compilers, and Run-Time Environments for Distributed Memory Machines. </title> <publisher> North-Holland, </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1992. </year>
Reference-contexts: The Fortran 77D compiler also handled the same classes of references and distributions, but computed special-case expressions for the iteration sets for "interior" and "boundary" processors [24]. (In fact, both these groups have described basic compilation steps in terms of abstract set operations <ref> [23, 32] </ref>; however, this was used only as a pedagogical abstraction and the corresponding compilers were implemented using case-based analysis.) Li and Chen describe algorithms to classify communication caused by more general reference patterns (assuming aligned arrays and the owner-computes rule), and generate code to realize these patterns efficiently on a <p> This formulation is made explicit in dHPF and in the SUIF compiler [2], and similar formulations have been discussed elsewhere <ref> [32, 23] </ref>. The integer set framework in dHPF includes the representation of these primitive quantities as integer tuple sets and mappings, together with the operations to manipulate them and generate code from them. The core optimizations in dHPF are implemented directly on this framework. <p> Any compiler for a data-parallel language based on data distributions operates primarily on three types of tuple spaces, and the three pairwise mappings between these tuple spaces <ref> [32, 23, 2] </ref>.
Reference: 24. <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng. </author> <title> Preliminary experiences with the Fortran D compiler. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <address> Portland, OR, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: Even within this class of applications, state-of-the-art commercial and research compilers do not consistently achieve performance competitive with hand-written code <ref> [16, 24] </ref>. <p> Furthermore, even for these optimizations, most research and commercial data-parallel compilers to date [7, 10, 15, 16, 17, 19, 24, 32, 33, 35, 42, 45, 46] (including the Rice Fortran 77D compiler <ref> [24] </ref>) perform communication analysis and code generation for specific combinations of the form of references, data layouts and computation partitionings. While such "case-based" approaches can provide excellent performance where they apply, they will provide poor performance for cases that have not been explicitly considered. <p> Code Generation To compute the above sets and to generate code using them, the primary approach in most previous research and commercial compilers has been to focus on individual common cases and to precompute the iteration and communication sets symbolically for specific forms of references, data layouts and computation partitionings <ref> [7, 10, 15, 16, 17, 19, 24, 32, 33, 42, 46] </ref>. <p> The Fortran 77D compiler also handled the same classes of references and distributions, but computed special-case expressions for the iteration sets for "interior" and "boundary" processors <ref> [24] </ref>. (In fact, both these groups have described basic compilation steps in terms of abstract set operations [23, 32]; however, this was used only as a pedagogical abstraction and the corresponding compilers were implemented using case-based analysis.) Li and Chen describe algorithms to classify communication caused by more general reference patterns <p> This is implemented by virtually all data-parallel compilers, but in case-based compilers it is usually restricted to specific reference patterns for which the compiler can derive (or conservatively approximate) the data sets to be communicated <ref> [16, 24, 32] </ref>. Exploiting collective communication is essential for achieving good speedup in important cases such as reductions, broadcasts, and array redistribution [33]. On certain systems, collective communication primitives may also provide significant benefits for other patterns such as shift communication. <p> The important patterns (particularly reductions and broadcast) have been supported in most data-parallel compilers. Message coalescing combines messages for multiple non-local references to the same or different variables, in order to reduce the total number of messages and to eliminate redundant communication. Previous implementations in Fortran 77D <ref> [24] </ref>, SUIF [2], Paradigm [5], and IBM's pHPF [11, 16] have some significant limitations. In particular, coalescing can produce fairly complex data sets from the union of data sets for individual references. <p> It is an important optimization for effectively implementing parallelism in such loop nests because the only alternative may be to perform a full array redistribution, which can be much more expensive. To our knowledge, this optimization has been implemented in a few research compilers <ref> [24, 5] </ref> and one commercial one [16]. Optimizations to overlap communication with computation - Dataflow-based communication placement attempts to hide the latency of communication by placing message sends early and receives late so as to overlap messages with unrelated computation.
Reference: 25. <author> Harold Johnson. </author> <title> Data flow analysis of `intractable' imbedded system software. </title> <booktitle> In Proceedings of the SIGPLAN '86 Symposium on Compiler Construction, </booktitle> <pages> pages 109-117, </pages> <year> 1986. </year>
Reference-contexts: The foundation for control flow simplification in dHPF is an algorithm for globally propagating symbolic constraints on the values of variables imposed by loops, conditional branches, assertions, and integer computations. Several previous systems have supported strategies for computing and exploiting range information about variables <ref> [20, 8, 9, 25, 44] </ref>. Two key differences that distinguish our work are that we handle more general logical combinations of constraints on variables (not just ranges) and we use these constraints to simplify control flow.
Reference: 26. <author> W. Kelly, W. Pugh, and E. Rosser. </author> <title> Code generation for multiple mappings. </title> <booktitle> In Frontiers '95: The 5th Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <address> McLean, VA, </address> <month> February </month> <year> 1995. </year>
Reference-contexts: In particular, the library provides two key capabilities: it supports a general class of integer set operations including set union, and it provides an algorithm to generate efficient code that enumerates points in a given sequence of iteration spaces associated with a sequence of statements in a loop <ref> [26] </ref>. (Appendix A describes this code generation capability.) These capabilities are an invaluable asset for implementing set-based versions of the core HPF compiler optimizations as well as enabling a variety of interesting new optimizations, described in later sections. <p> Previous compilers use simple approaches for code generation and do not solve this problem in its general form (as described briefly below), but Kelly, Pugh & Rosser have developed an aggressive algorithm for "multiple-mappings code generation" which directly tackles this problem <ref> [26] </ref>. A third and related difficulty, however, is that good algorithms for generating efficient code (like that of Kelly, Pugh & Rosser) will be inherently expensive because of the potential complexity of the iteration spaces and the resulting code. <p> Kelly, Pugh & Rosser describe an aggressive algorithm to generate efficient code without relying on loop distribution, for a loop nest containing multiple statements with different iteration spaces <ref> [26] </ref>. Given a sequence of (possibly non-convex) iteration spaces, the algorithm, mmcodegen, synthesizes a code fragment to enumerate the points in the iteration spaces in lexicographic order, tiling the loops as necessary to lift guards out of one or more levels of loops.
Reference: 27. <author> Wayne Kelly, Vadim Maslov, William Pugh, Evan Rosser, Tatiana Shpeisman, and David Wonnacott. </author> <title> The Omega Library Interface Guide. </title> <type> Technical report, </type> <institution> Dept. of Computer Science, Univ. of Maryland, College Park, </institution> <month> April </month> <year> 1996. </year>
Reference-contexts: We use the Omega library developed by Pugh et al. at the University of Maryland for this purpose <ref> [27] </ref>. The library operations use powerful algorithms based on Fourier-Motzkin elimination for manipulating integer tuple sets represented by Presburger formulae [37]. <p> Our algorithm combines three key program analysis technologies, namely, the control dependence graph (CDG) [13], global value numbering based on thinned gated single assignment form [21], and simplification of integer constraints specified as Presburger formulae <ref> [27] </ref>. The former two enable us to derive an efficient, non-iterative algorithm for propagating constraints along control dependences, while the latter enables simplification of logical constraints and code generation from the simplified constraints.
Reference: 28. <author> K. Kennedy, N. Nedeljkovic, and A. Sethi. </author> <title> A linear-time algorithm for computing the memory access sequence in data-parallel programs. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: There is also a large body of work on techniques to enumerate communication sets and iteration sets in the presence of cyclic (k) distributions (e.g., <ref> [12, 17, 28, 35, 45] </ref>). <p> These are not supported by our frame 12 Vikram Adve and John Mellor-Crummey work, and would have to fall back on more expensive run-time techniques such as a finite-state-machine approach for computing communication and iteration sets (for example, <ref> [28] </ref>), or an inspector-executor approach. To permit a symbolic number of processors or cyclic (k) distribution with symbolic k, we use a virtual processor (VP) model that naturally matches the semantics of templates in HPF [22].
Reference: 29. <author> Ken Kennedy and Charles Koelbel. </author> <title> The High Performance Fortran 2.0 Language, </title> <booktitle> chapter 1. Lecture Notes in Computer Science Series. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1997. </year> <note> 42 Vikram Adve and John Mellor-Crummey </note>
Reference-contexts: 1. Introduction Data-parallel languages such as High-Performance Fortran (HPF) <ref> [29, 31] </ref> aim to make parallel scientific computing accessible to a much wider audi ence by providing a simple, portable, abstract programming model applica ble to a wide variety of parallel computing systems. <p> Background: The Code Generation Problem For HPF The High Performance Fortran standard describes a number of extensions to Fortran 90 to guide compiler parallelization for parallel systems. The language is discussed in some detail in an earlier chapter <ref> [29] </ref>, and we assume the reader is familiar with the major aspects of the language (particularly, the data distribution directives).
Reference: 30. <author> Ken Kennedy and Ajay Sethi. </author> <title> Resource-based communication placement analysis. </title> <booktitle> In Proceedings of the Ninth Workshop on Languages and Compilers for Parallel Computing. </booktitle> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1996. </year>
Reference-contexts: The second step uses a combination of dependence and dataflow analysis to choose the placement of communication so as to determine how far each message can be vectorized out of enclosing loops, and to optionally move communication calls early or late to hide communication latency <ref> [30] </ref>. The third step uses the algorithms of Li and Chen [33] to to determine if specialized collective communication primitives such as a broadcast could be exploited. (Reductions are recognized using separate algorithms.) Otherwise, the compiler directly implements the communication using pairwise point-to-point communication.
Reference: 31. <author> C. Koelbel, D. Loveman, R. Schreiber, G. Steele, Jr., and M. Zosel. </author> <title> The High Performance Fortran Handbook. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: 1. Introduction Data-parallel languages such as High-Performance Fortran (HPF) <ref> [29, 31] </ref> aim to make parallel scientific computing accessible to a much wider audi ence by providing a simple, portable, abstract programming model applica ble to a wide variety of parallel computing systems. <p> For clarity, we use different set variables to denote points in different tuple spaces. The construction of the Layout mapping follows the two steps used to describe an array layout in HPF <ref> [31] </ref>, 1 We use names with lower-case initial letters for tuple sets and upper-case letters for mappings respectively. 2 A set of rank 0, f [ ] : f (v 1 ; : : : ; v n )g, should be interpreted as a boolean that takes the values true or
Reference: 32. <author> C. Koelbel and P. Mehrotra. </author> <title> Compiling global name-space parallel loops for distributed execution. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 440-451, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Optimizations we have formulated in this manner include message vectorization [14], message coalescing [43], recognizing in-place communication [1], code generation for our general CP model [1], non-local index set splitting <ref> [32] </ref>, control-flow simplification [34], and generalized loop-peeling for improving parallelism. By formulating these algorithms in terms of operations on integer sets, we are able to abstract away the details of the CPs, references, and data layouts for each problem instance. <p> Code Generation To compute the above sets and to generate code using them, the primary approach in most previous research and commercial compilers has been to focus on individual common cases and to precompute the iteration and communication sets symbolically for specific forms of references, data layouts and computation partitionings <ref> [7, 10, 15, 16, 17, 19, 24, 32, 33, 42, 46] </ref>. <p> For example, to implement the Kali compiler <ref> [32] </ref>, the authors pre-compute symbolically the iteration sets and communication sets for subscripts of the form c, i + c and c i, where i is a loop index variable for BLOCK and CYCLIC distributions. <p> The Fortran 77D compiler also handled the same classes of references and distributions, but computed special-case expressions for the iteration sets for "interior" and "boundary" processors [24]. (In fact, both these groups have described basic compilation steps in terms of abstract set operations <ref> [23, 32] </ref>; however, this was used only as a pedagogical abstraction and the corresponding compilers were implemented using case-based analysis.) Li and Chen describe algorithms to classify communication caused by more general reference patterns (assuming aligned arrays and the owner-computes rule), and generate code to realize these patterns efficiently on a <p> This formulation is made explicit in dHPF and in the SUIF compiler [2], and similar formulations have been discussed elsewhere <ref> [32, 23] </ref>. The integer set framework in dHPF includes the representation of these primitive quantities as integer tuple sets and mappings, together with the operations to manipulate them and generate code from them. The core optimizations in dHPF are implemented directly on this framework. <p> Any compiler for a data-parallel language based on data distributions operates primarily on three types of tuple spaces, and the three pairwise mappings between these tuple spaces <ref> [32, 23, 2] </ref>. <p> A fourth problem (not directly related to a general CP model) is that static code generation techniques will not be useful for a code with irregular or complex partitionings. Such cases require runtime strategies such as the inspector-executor approach (e.g., <ref> [32, 18, 40] </ref>). However, regular and irregular partitionings may coexist in the same program, and perhaps even in a single loop nest. This raises the need for a flexible code generation framework that allows each part of the source program to be partitioned using the most efficient strategy applicable. <p> This is implemented by virtually all data-parallel compilers, but in case-based compilers it is usually restricted to specific reference patterns for which the compiler can derive (or conservatively approximate) the data sets to be communicated <ref> [16, 24, 32] </ref>. Exploiting collective communication is essential for achieving good speedup in important cases such as reductions, broadcasts, and array redistribution [33]. On certain systems, collective communication primitives may also provide significant benefits for other patterns such as shift communication. <p> Communication can be overlapped with the local iterations by first executing send operations for the non-local data required in the loop, then the local iterations, then the receives, and finally the nonlocal iterations. Loop splitting was implemented in Kali <ref> [32] </ref>, albeit with significant limitations as described in Section 5.3. Optimizations to minimize data buffering and access costs Minimizing buffer copying overhead is essential to minimize the overall cost of communication. This can be achieved in multiple ways. <p> We can also reduce the number of ownership guards executed before non-local references by eliminating the guards in the local iterations, and perhaps some in the non-local iterations as well. The only implementation of loop-splitting we know of is in Kali <ref> [32] </ref>, where the authors used set equations to explain the optimization but used case-based analysis to derive the iteration sets (during compiler development) for a few special cases restricted to one-dimensional data distributions. <p> This 30 Vikram Adve and John Mellor-Crummey approach is only practical for a small number of special cases. We have extended the equations in <ref> [32] </ref> to apply to an arbitrary number of non-local references with any regular data layouts and any CP in our CP model, using the sets and mappings described in previous sections.
Reference: 33. <author> J. Li and M. Chen. </author> <title> Compiling communication-efficient programs for massively parallel machines. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 361-376, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Code Generation To compute the above sets and to generate code using them, the primary approach in most previous research and commercial compilers has been to focus on individual common cases and to precompute the iteration and communication sets symbolically for specific forms of references, data layouts and computation partitionings <ref> [7, 10, 15, 16, 17, 19, 24, 32, 33, 42, 46] </ref>. <p> this was used only as a pedagogical abstraction and the corresponding compilers were implemented using case-based analysis.) Li and Chen describe algorithms to classify communication caused by more general reference patterns (assuming aligned arrays and the owner-computes rule), and generate code to realize these patterns efficiently on a target machine <ref> [33] </ref>. In general, these compilers focus on providing specific optimizations aimed at cases that are considered to be the most common and most important. <p> Exploiting collective communication is essential for achieving good speedup in important cases such as reductions, broadcasts, and array redistribution <ref> [33] </ref>. On certain systems, collective communication primitives may also provide significant benefits for other patterns such as shift communication. The important patterns (particularly reductions and broadcast) have been supported in most data-parallel compilers. <p> The third step uses the algorithms of Li and Chen <ref> [33] </ref> to to determine if specialized collective communication primitives such as a broadcast could be exploited. (Reductions are recognized using separate algorithms.) Otherwise, the compiler directly implements the communication using pairwise point-to-point communication. The fourth step chooses references whose communication can be combined.
Reference: 34. <author> J. Mellor-Crummey and V. Adve. </author> <title> Simplifying control flow in compiler-generated parallel code. </title> <type> Technical Report CS-TR97-278, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> May </month> <year> 1997. </year>
Reference-contexts: Optimizations we have formulated in this manner include message vectorization [14], message coalescing [43], recognizing in-place communication [1], code generation for our general CP model [1], non-local index set splitting [32], control-flow simplification <ref> [34] </ref>, and generalized loop-peeling for improving parallelism. By formulating these algorithms in terms of operations on integer sets, we are able to abstract away the details of the CPs, references, and data layouts for each problem instance. <p> We motivate and briefly describe a powerful algorithm for constraint propagation and control flow simplification used in the dHPF compiler <ref> [34] </ref>. In a preliminary evaluation, the algorithm has proven highly effective at eliminating excess control-flow in the generated code. Furthermore, we find that the general purpose control-flow simplification algorithm provides some or all of the benefits of special-purpose optimizations such as vector-message pipelining [43] and overlap areas [14]. <p> Our aim in this chapter is to motivate and provide an overview of the techniques we use to address the challenges described above. The algorithms underlying these techniques are described and evaluated in detail elsewhere <ref> [1, 34] </ref>. In the following section, we use an example to describe 4 Vikram Adve and John Mellor-Crummey the basic steps in generating an explicit message-passing parallel program for HPF. <p> In this section, we present an example that illustrates how a sequence of code generation steps results in superfluous loops and conditionals and then provide a brief overview of our control-flow simplification technique. Our algorithm is described and evaluated in detail in <ref> [34] </ref>. To illustrate some of the principal sources of excess control-flow in dHPF, Fig. 6.1 shows source code for a loop from the Erlebacher benchmark, including the CPs and the initial placement of communication chosen by the compiler. <p> These assertions provide a mechanism for communicating information that a later compiler pass may be unable to infer directly from the code. We refer the reader to <ref> [34] </ref> for the details of the overall algorithm, including the handling of iterative constructs and assertions. 6.3 Evaluation and Discussion shown in Figure 6.2. In Figure 6.3, we see that all infeasible and tautological guards have been eliminated, the latter being replaced by their enclosed code. <p> In Figure 6.3, we see that all infeasible and tautological guards have been eliminated, the latter being replaced by their enclosed code. We also evaluated the algorithm for 3 benchmarks (Tomcatv from the Spec92 benchmark suite, Erlebacher, and Jacobi) <ref> [34] </ref>. The algorithm eliminated between 31% and 81% of guards introduced by the compiler for these programs, yielding between 1% and 15% improvement in execution time on an IBM SP-2. <p> In particular, the pipelined shift communication pattern for Erlebacher shown in Fig. 6.3 is exactly what vector message-pipelining aims to produce, but the latter is a narrow optimization and complex to implement, as explained in <ref> [34] </ref>. In dHPF, however, it results naturally from loop-splitting, bounds reduction, and control-flow simplification. As a second example, overlap areas are specifically designed to simplify 38 Vikram Adve and John Mellor-Crummey referencing non-local data in shift communication patterns, but a comprehensive implementation of overlap areas requires interprocedural analysis. <p> In fact, the code for Tom-catv without overlap areas slightly outperformed the code with overlap areas (by about 3%) because overlap areas required an extra unpacking operation <ref> [34] </ref>. Overall, these results and the evaluation described above indicate that the control-flow simplification algorithm can be a useful general-purpose optimization for parallelizing compilers. 7.
Reference: 35. <author> S. Midkiff. </author> <title> Local iteration set computation for block-cyclic distributions. </title> <booktitle> In Proceedings of the 24th International Conference on Parallel Processing, </booktitle> <address> Oconomowoc, WI, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: There is also a large body of work on techniques to enumerate communication sets and iteration sets in the presence of cyclic (k) distributions (e.g., <ref> [12, 17, 28, 35, 45] </ref>).
Reference: 36. <author> D. Oppen. </author> <title> A 2 2 s pn upper bound on the complexity of Presburger arithmetic. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 16(3) </volume> <pages> 323-332, </pages> <month> July </month> <year> 1978. </year>
Reference-contexts: One potentially significant disadvantage of using such a general representation is the compile-time cost of the algorithms used in Omega. In particular, simplification of formulae in Presburger arithmetic can be extremely costly in the worst-case <ref> [36] </ref>. Pugh has shown, however, that when the underlying algorithms in Omega (for Fourier-Motzkin elimination) are applied to dependence analysis, the execution time is quite small even for complex constraints with coupled subscripts and also for synthetic problems known to cause poor performance [37].
Reference: 37. <author> W. Pugh. </author> <title> A practical algorithm for exact array dependence analysis. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 102-114, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: We use the Omega library developed by Pugh et al. at the University of Maryland for this purpose [27]. The library operations use powerful algorithms based on Fourier-Motzkin elimination for manipulating integer tuple sets represented by Presburger formulae <ref> [37] </ref>. <p> Pugh has shown, however, that when the underlying algorithms in Omega (for Fourier-Motzkin elimination) are applied to dependence analysis, the execution time is quite small even for complex constraints with coupled subscripts and also for synthetic problems known to cause poor performance <ref> [37] </ref>. These experimental results at least provide evidence that the basic techniques could be practical for use in a compiler. In dHPF, the Omega library has already proved to be a powerful tool for proto-typing advanced optimizations based on the integer set framework.
Reference: 38. <author> J. Ramanujam. </author> <title> Integer Lattice Based Method for Local Address Generation for Block-Cyclic Distributions, </title> <booktitle> chapter 17. Lecture Notes in Computer Science Series. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1997. </year>
Reference-contexts: If a static mapping is not computable at compile time, alternative 16 Vikram Adve and John Mellor-Crummey strategies that can be used include run-time resolution [39], the inspector-executor approach, and run-time techniques for handling cyclic (k) parti-tionings. The latter two approaches are described in other chapters within this volume <ref> [40, 38] </ref>. It is relatively straightforward to partition a simple loop nest that contains a single statement or a sequence of statements with the same CP.
Reference: 39. <author> A. Rogers and K. Pingali. </author> <title> Process decomposition through locality of reference. </title> <booktitle> In Proceedings of the SIGPLAN '89 Conference on Programming Language Design and Implementation, </booktitle> <address> Portland, OR, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: Although many important optimizations for such systems have been proposed by previous researchers, current compilers implement only a small fraction of these optimizations, generally focusing on the most fundamental ones such as static loop partitioning based on the "owner-computes" rule <ref> [39] </ref>, moving messages out of loops, reducing the number of data copies, and exploiting collective communication. <p> The communication analysis techniques supporting the model are described in Section 5. 4.1 Computation Partitioning Models Most research and commercial compilers for HPF to date primarily use the owner-computes rule <ref> [39] </ref> to partition a computation. This rule specifies that each value assigned by a statement is computed by the owner (i.e., the "home") of the location being assigned the value, e.g., the left hand side (LHS) in an assignment. <p> If a static mapping is not computable at compile time, alternative 16 Vikram Adve and John Mellor-Crummey strategies that can be used include run-time resolution <ref> [39] </ref>, the inspector-executor approach, and run-time techniques for handling cyclic (k) parti-tionings. The latter two approaches are described in other chapters within this volume [40, 38]. It is relatively straightforward to partition a simple loop nest that contains a single statement or a sequence of statements with the same CP.
Reference: 40. <author> J. Saltz. </author> <title> Runtime Support for Irregular Problems, </title> <booktitle> chapter 17. Lecture Notes in Computer Science Series. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1997. </year>
Reference-contexts: A fourth problem (not directly related to a general CP model) is that static code generation techniques will not be useful for a code with irregular or complex partitionings. Such cases require runtime strategies such as the inspector-executor approach (e.g., <ref> [32, 18, 40] </ref>). However, regular and irregular partitionings may coexist in the same program, and perhaps even in a single loop nest. This raises the need for a flexible code generation framework that allows each part of the source program to be partitioned using the most efficient strategy applicable. <p> If a static mapping is not computable at compile time, alternative 16 Vikram Adve and John Mellor-Crummey strategies that can be used include run-time resolution [39], the inspector-executor approach, and run-time techniques for handling cyclic (k) parti-tionings. The latter two approaches are described in other chapters within this volume <ref> [40, 38] </ref>. It is relatively straightforward to partition a simple loop nest that contains a single statement or a sequence of statements with the same CP.
Reference: 41. <author> A. Schrijver. </author> <title> Theory of Linear and Integer Programming. </title> <publisher> John Wiley and Sons, </publisher> <address> Chichester, Great Britain, </address> <year> 1986. </year>
Reference-contexts: In this approach, each code generation or communication optimization problem is described by a collection of linear inequalities representing integer sets or mappings. Fourier-Motzkin elimination <ref> [41] </ref> is used to simplify the resulting inequalities, and to compute a range of values for individual index variables that together enumerate the integer points described by these inequalities. Code generation then amounts to generating loops to iterate over these index ranges.
Reference: 42. <author> J. Stichnoth, D. O'Hallaron, and T. Gross. </author> <title> Generating communication for array statements: Design, implementation, and evaluation. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 21(1) </volume> <pages> 150-159, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Code Generation To compute the above sets and to generate code using them, the primary approach in most previous research and commercial compilers has been to focus on individual common cases and to precompute the iteration and communication sets symbolically for specific forms of references, data layouts and computation partitionings <ref> [7, 10, 15, 16, 17, 19, 24, 32, 33, 42, 46] </ref>.
Reference: 43. <author> C.-W. Tseng. </author> <title> An Optimizing Fortran D Compiler for MIMD Distributed-Memory Machines. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: Optimizations we have formulated in this manner include message vectorization [14], message coalescing <ref> [43] </ref>, recognizing in-place communication [1], code generation for our general CP model [1], non-local index set splitting [32], control-flow simplification [34], and generalized loop-peeling for improving parallelism. <p> In a preliminary evaluation, the algorithm has proven highly effective at eliminating excess control-flow in the generated code. Furthermore, we find that the general purpose control-flow simplification algorithm provides some or all of the benefits of special-purpose optimizations such as vector-message pipelining <ref> [43] </ref> and overlap areas [14]. Our aim in this chapter is to motivate and provide an overview of the techniques we use to address the challenges described above. The algorithms underlying these techniques are described and evaluated in detail elsewhere [1, 34]. <p> An interesting outcome we observed in our experiments is that the general purpose control-flow simplification algorithm (in combination with our CP code generation algorithm and loop splitting) provides some or all of the benefits of much more specialized optimizations such as vector-message pipelining <ref> [43] </ref> and overlap areas [14]. In particular, the pipelined shift communication pattern for Erlebacher shown in Fig. 6.3 is exactly what vector message-pipelining aims to produce, but the latter is a narrow optimization and complex to implement, as explained in [34].
Reference: 44. <author> Peng Tu and David Padua. </author> <title> Gated SSA-based demand-driven symbolic analysis for parallelizing compilers. </title> <booktitle> In Proceedings of the 1995 ACM International Conference on Supercomputing, </booktitle> <address> Barcelona, Spain, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: The foundation for control flow simplification in dHPF is an algorithm for globally propagating symbolic constraints on the values of variables imposed by loops, conditional branches, assertions, and integer computations. Several previous systems have supported strategies for computing and exploiting range information about variables <ref> [20, 8, 9, 25, 44] </ref>. Two key differences that distinguish our work are that we handle more general logical combinations of constraints on variables (not just ranges) and we use these constraints to simplify control flow. <p> The information we collect is closely related to the concept of "iterated control dependence" path conditions described by Tu and Padua <ref> [44] </ref>. Rather than computing constraints on variables directly, we compute constraints on value numbers representing unique values of variables. This allows us to avoid invalidating constraints after redefinitions and SSA merge points since each of these points simply yields a new value number for the variable.
Reference: 45. <author> Kees van Reeuwijk, Will Denissen, Henk Sips, and Edwin Paalvast. </author> <title> An implementation framework for hpf distributed arrays on message-passing parallel computer systems. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 7(8) </volume> <pages> 897-914, </pages> <month> September </month> <year> 1996. </year>
Reference-contexts: There is also a large body of work on techniques to enumerate communication sets and iteration sets in the presence of cyclic (k) distributions (e.g., <ref> [12, 17, 28, 35, 45] </ref>).

References-found: 45

