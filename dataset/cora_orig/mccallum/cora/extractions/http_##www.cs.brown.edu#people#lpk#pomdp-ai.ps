URL: http://www.cs.brown.edu/people/lpk/pomdp-ai.ps
Refering-URL: http://www.cs.brown.edu/research/ai/
Root-URL: http://www.cs.brown.edu
Title: Planning and Acting in Partially Observable Stochastic Domains problem of a robot navigating in a
Author: Leslie Pack Kaelbling ; Michael L. Littman Anthony R. Cassandra 
Keyword: Key words: planning, uncertainty, partially observable Markov decision processes  
Note: Consider the  
Address: Box 1910 Providence, RI 02912-1910, USA  Durham, NC 27708-0129, USA  3500 West Balcones Center Drive Austin, TX 78759-5398, USA  
Affiliation: Computer Science Department Brown University,  Department of Computer Science Duke University  Microelectronics and Computer Technology Corporation (MCC)  
Abstract: In this paper, we bring techniques from operations research to bear on the problem of choosing optimal actions in partially observable stochastic domains. We begin by introducing the theory of Markov decision processes (mdps) and partially observable mdps (pomdps). We then outline a novel algorithm for solving pomdps off line and show how, in some cases, a finite-memory controller can be extracted from the solution to a pomdp. We conclude with a discussion of how our approach relates to previous work, the complexity of finding exact solutions to pomdps, and of some possibilities for finding approximate solutions. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. J. Astrom. </author> <title> Optimal control of Markov decision processes with incomplete state estimation. </title> <journal> Journal of Mathematical Analysis and Applications, </journal> <volume> 10 </volume> <pages> 174-205, </pages> <year> 1965. </year>
Reference: [2] <author> Fahiem Bacchus, Craig Boutilier, and Adam Grove. </author> <title> Rewarding behaviors. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 1160-1167. </pages> <publisher> AAAI Press/The MIT Press, </publisher> <year> 1996. </year> <month> 39 </month>
Reference-contexts: Haddawy et al. [21] looked at a broad family of decision-theoretic objectives that make it possible to specify trade-offs between partially satisfying goals quickly and satisfying them completely. Bacchus, Boutilier, and Grove <ref> [2] </ref> show how some richer objectives based on evaluations of sequences of actions can actually be converted to total-reward problems.
Reference: [3] <author> Dimitri P. Bertsekas. </author> <title> Dynamic Programming and Optimal Control. </title> <publisher> Athena Scientific, </publisher> <address> Belmont, Massachusetts, </address> <year> 1995. </year> <note> Volumes 1 and 2. </note>
Reference: [4] <author> Avrim L. Blum and Merrick L. Furst. </author> <title> Fast planning through planning graph analysis. </title> <journal> Artificial Intelligence, </journal> <volume> 90(1-2):279-298, </volume> <year> 1997. </year>
Reference-contexts: Traditional plans are simple sequences of actions. They are sufficient when the initial state is known and all actions are deterministic. A slightly more elaborate structure is the partially ordered plan (generated, for example, by snlp and ucpop), or the parallel plan <ref> [4] </ref>. In this type of plan, actions can be left unordered if all orderings are equivalent under the performance metric. When actions are stochastic, partially ordered plans can still be used (as in Buridan), but contingent plans can be more effective.
Reference: [5] <author> Jim Blythe. </author> <title> Planning with external events. </title> <booktitle> In Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 94-101, </pages> <year> 1994. </year>
Reference: [6] <author> Craig Boutilier and David Poole. </author> <title> Computing optimal policies for partially observable decision processes using compact representations. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 1168-1175. </pages> <publisher> AAAI Press/The MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: However, this work has served as a substrate for development of algorithms for more complex and efficient representations <ref> [6] </ref>. Section 6 describes the relation between the present approach and prior research in more detail. One important facet of the pomdp approach is that there is no distinction drawn between actions taken to change the state of the world and actions taken to gain information.
Reference: [7] <author> Anthony Cassandra, Michael L. Littman, and Nevin L. Zhang. </author> <title> Incremental Pruning: A simple, fast, exact method for partially observable Markov decision processes. </title> <booktitle> In Proceedings of the Thirteenth Annual Conference on Uncertainty in Artificial Intelligence (UAI-97), </booktitle> <pages> pages 54-61, </pages> <address> San Francisco, CA, 1997. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: In higher dimensions, more complex algorithms are needed and the number of corners is often exponential in the dimensionality. Thus, the geometric approaches are useful only in pomdps with extremely small state spaces. Zhang and Liu [67] describe the incremental-pruning algorithm, later generalized by Cassandra, Littman, and Zhang <ref> [7] </ref>. This algorithm is simple to implement and empirically faster than the witness algorithm, while sharing its good worst-case complexity in terms of P t j.
Reference: [8] <author> Anthony R. Cassandra, Leslie Pack Kaelbling, and Michael L. Littman. </author> <title> Acting optimally in partially observable stochastic domains. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pages 1023-1028, </pages> <address> Seattle, WA, </address> <year> 1994. </year>
Reference-contexts: Other examples are explored in an earlier paper <ref> [8] </ref>. 5.1 The Tiger Problem Imagine an agent standing in front of two closed doors. Behind one of the doors is a tiger and behind the other is a large reward.
Reference: [9] <author> Anthony Rocco Cassandra. </author> <title> Exact and Approximate Algorithms for Partially Observable Markov Decision Problems. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Brown University, </institution> <month> May </month> <year> 1998. </year>
Reference-contexts: is, R (ff; V) = fb j b ff &gt; b ~ff; for all ~ff 2 V ff and b 2 Bg : It is relatively easy, using a linear program, to find a point in R (ff; V) if one exists, or to determine that the region is empty <ref> [9] </ref>. The simplest pruning strategy, proposed by Sondik [58,42], is to test R (ff; ~ V) for every ff in ~ V and remove those ff that are nowhere dominant. <p> A much more efficient pruning method was proposed by Lark and White [64] and is described in detail by Littman [35] and by Cassandra <ref> [9] </ref>. Because it has many subtle technical details, it is not described here. 4.3 One Step of Value Iteration The value function for a pomdp can be computed using value iteration, with the same basic structure as for the discrete mdp case. <p> In the process of motivating the one-pass algorithm, Sondik [58] applies the same ideas to finding Q-functions instead of the complete value function. The resulting algorithm might be called the two-pass algorithm <ref> [9] </ref>, and, its form is much like the witness algorithm because it first constructs each separate Q-function, then combines the Q-functions together to create the optimal value function. <p> Although it appears that the algorithm attracted no attention and was never implemented in over 25 years after the completion of Sondik's dissertation, it was recently implemented and found to be faster than any of the algorithms that predated the witness algorithm <ref> [9] </ref>. As pointed out in Section 4, value functions in belief space have a natural geometric interpretation. For small state spaces, algorithms that exploit this geometry are quite efficient [16]. An excellent example of this is Cheng's linear support algorithm [10].
Reference: [10] <author> Hsien-Te Cheng. </author> <title> Algorithms for Partially Observable Markov Decision Processes. </title> <type> PhD thesis, </type> <institution> University of British Columbia, British Columbia, Canada, </institution> <year> 1988. </year>
Reference-contexts: If we could do this, we might be able to reach a computation time per iteration that is polynomial in jSj, jAj, jj, jV t1 j, and jV t j. Cheng <ref> [10] </ref> and Smallwood and Sondik [56] also try to avoid generating all of V + t by constructing V t directly. However, their algorithms still have worst-case running times exponential in at least one of the problem parameters [34]. <p> Although the algorithm is sophisticated and, in principle, avoids exhaustively enumerating the set of possibly useful policy trees at each iteration, it appears to run more slowly than the simpler enumeration methods in practice, at least for problems with small state spaces <ref> [10] </ref>. In the process of motivating the one-pass algorithm, Sondik [58] applies the same ideas to finding Q-functions instead of the complete value function. <p> As pointed out in Section 4, value functions in belief space have a natural geometric interpretation. For small state spaces, algorithms that exploit this geometry are quite efficient [16]. An excellent example of this is Cheng's linear support algorithm <ref> [10] </ref>. This algorithm can be viewed as a variation of the witness algorithm in which witness points are sought at the corners of regions of the approximate value function defined by the algorithm's equivalent of the set U .
Reference: [11] <author> Lonnie Chrisman. </author> <title> Reinforcement learning with perceptual aliasing: The perceptual distinctions approach. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 183-188, </pages> <address> San Jose, California, 1992. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: This approach has the potential significant advantage of being able to learn a model that is complex enough to support optimal (or good) behavior without making irrelevant distinctions; this idea has been pursued by Chrisman <ref> [11] </ref> and McCallum [40,41]. A Appendix Theorem 1 Let U a be a non-empty set of useful policy trees, and Q a t be the complete set of useful policy trees.
Reference: [12] <author> Anne Condon. </author> <title> The complexity of stochastic games. </title> <journal> Information and Computation, </journal> <volume> 96(2) </volume> <pages> 203-224, </pages> <month> February </month> <year> 1992. </year>
Reference: [13] <author> Thomas Dean, Leslie Pack Kaelbling, Jak Kirman, and Ann Nicholson. </author> <title> Planning under time constraints in stochastic domains. </title> <journal> Artificial Intelligence, </journal> <volume> 76(1-2):35-74, </volume> <year> 1995. </year>
Reference: [14] <author> Denise Draper, Steve Hanks, and Dan Weld. </author> <title> Probabilistic planning with information gathering and contingent execution. </title> <type> Technical Report 93-12-04, </type> <institution> University of Washington, </institution> <address> Seattle, WA, </address> <month> December </month> <year> 1993. </year>
Reference-contexts: The most closely related work to our own is that of Kushmerick, Hanks, and Weld [30] on the Buridan system, and Draper, Hanks and Weld <ref> [14] </ref> on the C-Buridan system. 31 6.1 Imperfect Knowledge Plans generated using standard mdp algorithms and classical planning algorithms 10 assume that the underlying state of the process will be known with certainty during plan execution.
Reference: [15] <author> Mark Drummond and John Bresina. </author> <title> Anytime synthetic projection: Maximizing the probability of goal satisfaction. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pages 138-144. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year> <month> 40 </month>
Reference-contexts: Bacchus, Boutilier, and Grove [2] show how some richer objectives based on evaluations of sequences of actions can actually be converted to total-reward problems. Other objectives considered in planning systems, aside from simple goals of achievement, include goals of maintenance and goals of prevention <ref> [15] </ref>; these types of goals can typically be represented using immediate rewards as well. 35 6.6 Representation of Problems The propositional representations most often used in planning have a number of advantages over the flat state-space representations associated with mdps and pomdps.
Reference: [16] <author> James N. Eagle. </author> <title> The optimal search for a moving target when the search path is constrained. </title> <journal> Operations Research, </journal> <volume> 32(5) </volume> <pages> 1107-1115, </pages> <year> 1984. </year>
Reference-contexts: As pointed out in Section 4, value functions in belief space have a natural geometric interpretation. For small state spaces, algorithms that exploit this geometry are quite efficient <ref> [16] </ref>. An excellent example of this is Cheng's linear support algorithm [10]. This algorithm can be viewed as a variation of the witness algorithm in which witness points are sought at the corners of regions of the approximate value function defined by the algorithm's equivalent of the set U .
Reference: [17] <author> Emmanuel Fernandez-Gaucherand, Aristotle Arapostathis, and Steven I. Marcus. </author> <title> On the average cost optimality equation and the structure of optimal policies for partially observable Markov processes. </title> <journal> Annals of Operations Research, </journal> <volume> 29 </volume> <pages> 471-512, </pages> <year> 1991. </year>
Reference-contexts: Given the inter-representability results between goal-probability problems and discounted-optimality problems, it is hard to make technical sense of this difference. In fact, many pomdp models should probably be addressed in an average-reward context <ref> [17] </ref>. Using a discounted-optimal policy in a truly infinite-duration setting is a convenient approximation, similar to the use of a situation-action mapping from a finite-horizon policy in receding horizon control.
Reference: [18] <author> Robert P. Goldman and Mark S. Boddy. </author> <title> Conditional linear planning. </title> <editor> In Kristian Hammond, editor, </editor> <booktitle> The Second International Conference on Artificial Intelligence Planning Systems, </booktitle> <pages> pages 80-85. </pages> <publisher> The AAAI Press / The MIT Press, </publisher> <year> 1994. </year>
Reference: [19] <author> Robert P. Goldman and Mark S. Boddy. </author> <title> Epsilon-safe planning. </title> <booktitle> In Proceedings of the 10th Conference on Uncertainty in Artificial Intelligence (UAI94), </booktitle> <pages> pages 253-261, </pages> <address> Seattle, WA, </address> <year> 1994. </year>
Reference: [20] <author> Robert P. Goldman and Mark S. Boddy. </author> <title> Representing uncertainty in simple planners. </title> <booktitle> In Proceedings of the 4th International Conference on Principles of Knowledge Representation and Reasoning (KR94), </booktitle> <pages> pages 238-245, </pages> <year> 1994. </year>
Reference-contexts: This representation for imperfect knowledge is only appropriate when the designer of the system knows, in advance, what aspects of the state will be known and unknown. It is insufficient for multiple agents reasoning about each others' knowledge and for representing certain types of correlated uncertainty <ref> [20] </ref>. Formulating knowledge as predicate values that are either known or unknown makes it impossible to reason about gradations of knowledge.
Reference: [21] <author> Peter Haddawy and Steve Hanks. </author> <title> Utility models for goal-directed decision-theoretic planners. </title> <type> Technical Report 93-06-04, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: Koenig and Simmons [28] examine risk-sensitive planning and showed how planners for the total-reward criterion could be used to optimize risk-sensitive behavior. Haddawy et al. <ref> [21] </ref> looked at a broad family of decision-theoretic objectives that make it possible to specify trade-offs between partially satisfying goals quickly and satisfying them completely. Bacchus, Boutilier, and Grove [2] show how some richer objectives based on evaluations of sequences of actions can actually be converted to total-reward problems.
Reference: [22] <author> Eric A. Hansen. </author> <title> Cost-effective sensing during plan execution. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pages 1029-1035. </pages> <publisher> AAAI Press/The MIT Press, </publisher> <year> 1994. </year>
Reference: [23] <author> Eric A. Hansen. </author> <title> An improved policy iteration algorithm for partially observable MDPs. </title> <booktitle> In Advances in Neural Information Processing Systems 10, </booktitle> <year> 1998. </year> <note> In press. </note>
Reference-contexts: From the approximate value function we can extract a stationary policy that is approximately optimal. Sondik [59] and Hansen <ref> [23] </ref> have shown how to use algorithms like witness algorithm that perform exact dynamic-programming backups in pomdps in a policy-iteration algorithm to find exact solutions to many infinite-horizon problems. 5 Understanding Policies In this section we introduce a very simple example and use it to illustrate some properties of pomdp policies. <p> The simpler policy 28 Fig. 15. Edges can be rearranged to form a stationary policy. iteration algorithm due to Hansen <ref> [23] </ref> has not been proven to converge for all such pomdps. 9 5.4 Plan Graphs One drawback of the pomdp approach is that the agent must maintain a belief state and use it to select an optimal action on every step; if the underlying state space or that V is large,
Reference: [24] <author> Ronald A. Howard. </author> <title> Dynamic Programming and Markov Processes. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1960. </year>
Reference-contexts: Howard <ref> [24] </ref> showed that there exists a stationary policy, fl , that is optimal for every starting state.
Reference: [25] <author> Ronald A. Howard. </author> <title> Information value theory. </title> <journal> IEEE Transactions on Systems Science and Cybernetics, </journal> <volume> SSC-2(1):22-26, </volume> <month> August </month> <year> 1966. </year>
Reference-contexts: In low-entropy belief states, which are near the corners of the simplex, the agent can take actions more likely to be appropriate for the current state of the world and, so, gain more reward. This has some connection to the notion of "value of information," <ref> [25] </ref> where an agent can incur a cost to move it from a high-entropy to a low-entropy state; this is only worthwhile when the value of the information (the difference in value between the two states) exceeds the cost of gaining the information.
Reference: [26] <author> R. E. </author> <title> Kalman. A new approach to linear filtering and prediction problems. </title> <journal> Transactions of the American Society of Mechanical Engineers, Journal of Basic Engineering, </journal> <volume> 82 </volume> <pages> 35-45, </pages> <month> March </month> <year> 1960. </year>
Reference: [27] <author> Sven Koenig. </author> <title> Optimal probabilistic and decision-theoretic planning using Markovian decision theory. </title> <type> Technical Report UCB/CSD 92/685, </type> <institution> Berkeley, </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: The exponentially discounted sum of these rewards over the execution of a plan (finite or infinite horizon) constitutes the value of the plan. This objective is used extensively in most work with mdps and pomdps, including ours. 34 Several authors (for example, Koenig <ref> [27] </ref>) have pointed out that, given a completely observable problem stated as one of goal achievement, reward functions can be constructed so that a policy that maximizes reward can be used to maximize the probability of goal attainment in the original problem.
Reference: [28] <author> Sven Koenig and Reid G. Simmons. </author> <title> Risk-sensitive planning with probabilistic decision graphs. </title> <booktitle> In Proceedings of the 4th International Conference on Principles of Knowledge Representation and Reasoning, </booktitle> <pages> pages 363-373, </pages> <year> 1994. </year>
Reference-contexts: Littman [35] catalogs some alternatives to the total-reward criterion, all of which are based on the idea that the objective value for a plan is based on a summary of immediate rewards over the duration of a run. Koenig and Simmons <ref> [28] </ref> examine risk-sensitive planning and showed how planners for the total-reward criterion could be used to optimize risk-sensitive behavior. Haddawy et al. [21] looked at a broad family of decision-theoretic objectives that make it possible to specify trade-offs between partially satisfying goals quickly and satisfying them completely.
Reference: [29] <author> John R. Koza. </author> <title> Genetic Programming: On the Programming of Computers by Means of Natural Selection. </title> <publisher> The MIT Press, </publisher> <year> 1992. </year> <month> 41 </month>
Reference: [30] <author> Nicholas Kushmerick, Steve Hanks, and Daniel S. Weld. </author> <title> An algorithm for probabilistic planning. </title> <journal> Artificial Intelligence, </journal> <volume> 76(1-2):239-286, </volume> <month> September </month> <year> 1995. </year>
Reference-contexts: Our comparison focuses on issues of imperfect knowledge, uncertainty in initial state, the transition model, the observation model, the objective of planning, the representation of domains, and plan structures. The most closely related work to our own is that of Kushmerick, Hanks, and Weld <ref> [30] </ref> on the Buridan system, and Draper, Hanks and Weld [14] on the C-Buridan system. 31 6.1 Imperfect Knowledge Plans generated using standard mdp algorithms and classical planning algorithms 10 assume that the underlying state of the process will be known with certainty during plan execution.
Reference: [31] <author> Shieu-Hong Lin and Thomas Dean. </author> <title> Generating optimal policies for high-level plans with conditional branches and loops. </title> <booktitle> In Proceedings of the Third European Workshop on Planning, </booktitle> <pages> pages 205-218, </pages> <year> 1995. </year>
Reference: [32] <author> Michael L. Littman. </author> <title> Memoryless policies: Theoretical limitations and practical results. </title> <editor> In Dave Cliff, Philip Husbands, Jean-Arcady Meyer, and Stewart W. Wilson, editors, </editor> <booktitle> From Animals to Animats 3: Proceedings of the Third International Conference on Simulation of Adaptive Behavior, </booktitle> <address> Cambridge, MA, 1994. </address> <publisher> The MIT Press. </publisher>
Reference-contexts: Randomness effectively allows the agent to sometimes choose different actions in different locations with the same appearance, increasing the probability that it might choose a good action; in practice deterministic observation-action mappings are prone to getting trapped in deterministic loops <ref> [32] </ref>. In order to behave truly effectively in a partially observable world, it is necessary to use memory of previous actions and observations to aid in the disambiguation of the states of the world.
Reference: [33] <author> Michael L. Littman, Anthony R. Cassandra, and Leslie Pack Kaelbling. </author> <title> Learning policies for partially observable environments: Scaling up. </title> <editor> In Armand Prieditis and Stuart Russell, editors, </editor> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pages 362-370. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA, </address> <year> 1995. </year> <note> Reprinted in Readings in Agents, </note> <editor> M. H. Huhns and M. P. Singh, eds., </editor> <publisher> Morgan Kaufmann, </publisher> <year> 1998. </year>
Reference-contexts: Our current work explores the use of function-approximation methods for representing value functions and the use of simulation in order to concentrate the approximations on the frequently visited parts of the belief space <ref> [33] </ref>. The results of this work are encouraging and have allowed us to get a very good solution to an 89-state, 16-observation instance of a hallway navigation problem similar to the one described in the introduction.
Reference: [34] <author> Michael L. Littman, Anthony R. Cassandra, and Leslie Pack Kaelbling. </author> <title> Efficient dynamic-programming updates in partially observable Markov decision processes. </title> <type> Technical Report CS-95-19, </type> <institution> Brown University, Providence, RI, </institution> <year> 1996. </year>
Reference-contexts: Cheng [10] and Smallwood and Sondik [56] also try to avoid generating all of V + t by constructing V t directly. However, their algorithms still have worst-case running times exponential in at least one of the problem parameters <ref> [34] </ref>. In fact, the existence of an algorithm that runs in time polynomial in jSj, jAj, jj, jV t1 j, and jV t j would settle the long-standing complexity-theoretic question "Does NP=RP?" in the affirmative [34], so we will pursue a slightly different approach. <p> algorithms still have worst-case running times exponential in at least one of the problem parameters <ref> [34] </ref>. In fact, the existence of an algorithm that runs in time polynomial in jSj, jAj, jj, jV t1 j, and jV t j would settle the long-standing complexity-theoretic question "Does NP=RP?" in the affirmative [34], so we will pursue a slightly different approach. Instead of computing V t directly, we will compute, for each action a, a set Q a t of t-step policy trees that have action a at their root. <p> In what sense is the witness algorithm superior to previous algorithms for solving pomdps, then? Experiments indicate that the witness algorithm is faster in practice over a wide range of problem sizes <ref> [34] </ref>. The primary complexity-theoretic difference is that the witness algorithm runs in polynomial time in the number of policy trees in Q a t . <p> Now we can state the witness theorem <ref> [34] </ref>: The true Q-function, Q a t , differs from the approximate Q-function, ^ Q a t , if and only if there is some p 2 U a , o 2 , and p 0 2 V t1 for which there is some b such that V p new (b) <p> It gives a uniform treatment of action to gain information and action to change the world. Although they are derived through the domain of continuous belief spaces, elegant finite-state controllers may sometimes be constructed using algorithms such as the witness algorithm. However, experimental results <ref> [34] </ref> suggest that even the witness algorithm becomes impractical for problems of modest size (jSj &gt; 15 and jj &gt; 15).
Reference: [35] <author> Michael Lederman Littman. </author> <title> Algorithms for Sequential Decision Making. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Brown University, </institution> <month> February </month> <year> 1996. </year> <note> Also Technical Report CS-96-09. </note>
Reference-contexts: The simplest pruning strategy, proposed by Sondik [58,42], is to test R (ff; ~ V) for every ff in ~ V and remove those ff that are nowhere dominant. A much more efficient pruning method was proposed by Lark and White [64] and is described in detail by Littman <ref> [35] </ref> and by Cassandra [9]. Because it has many subtle technical details, it is not described here. 4.3 One Step of Value Iteration The value function for a pomdp can be computed using value iteration, with the same basic structure as for the discrete mdp case. <p> In fact, many pomdp models should probably be addressed in an average-reward context [17]. Using a discounted-optimal policy in a truly infinite-duration setting is a convenient approximation, similar to the use of a situation-action mapping from a finite-horizon policy in receding horizon control. Littman <ref> [35] </ref> catalogs some alternatives to the total-reward criterion, all of which are based on the idea that the objective value for a plan is based on a summary of immediate rewards over the duration of a run.
Reference: [36] <author> William S. Lovejoy. </author> <title> A survey of algorithmic methods for partially observable Markov decision processes. </title> <journal> Annals of Operations Research, </journal> <volume> 28(1) </volume> <pages> 47-65, </pages> <year> 1991. </year>
Reference: [37] <author> Stephen M. Majercik and Michael L. Littman. MAXPLAN: </author> <title> A new approach to probabilistic planning. </title> <type> Technical Report CS-1998-01, </type> <institution> Department of Computer Science, Duke University, </institution> <year> 1998. </year> <note> Submitted for review. </note>
Reference-contexts: If observations reveal the precise identity of the current state, the planning model is called "completely observable." The mdp model, as well as some planning systems such as cnlp and Plinth [18,19] assume complete observ-ability. Other systems, such as Buridan and maxplan <ref> [37] </ref>, have no observation model and can attack "completely unobservable" problems. Classical planning systems typically have no observation model, but the fact that the initial state is known and operators are deterministic means that they can also be thought of as solving completely observable problems.
Reference: [38] <author> T. M. Mansell. </author> <title> A method for planning given uncertain and incomplete information. </title> <booktitle> In Proceedings of the 9th Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 350-358. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <month> July </month> <year> 1993. </year>
Reference-contexts: An exception is the U-Plan <ref> [38] </ref> system, which creates a separate plan for each possible initial state with the aim of making these plans easy to merge to form a single plan. Conditional planners typically have some aspects of the initial state unknown.
Reference: [39] <author> David McAllester and David Rosenblitt. </author> <title> Systematic nonlinear planning. </title> <booktitle> In Proceedings of the 9th National Conference on Artificial Intelligence, </booktitle> <pages> pages 634-639, </pages> <year> 1991. </year>
Reference-contexts: In the mdp framework, the agent is informed of the current state each time it takes an action. In many classical planners (e.g., snlp <ref> [39] </ref>, ucpop [45]), the current state can be calculated trivially from the known initial state and knowledge of the deterministic operators. The assumption of perfect knowledge is not valid in many domains.
Reference: [40] <author> R. Andrew McCallum. </author> <title> Overcoming incomplete perception with utile distinction memory. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pages 190-196, </pages> <address> Amherst, Massachusetts, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [41] <author> R. Andrew McCallum. </author> <title> Instance-based utile distinctions for reinforcement learning with hidden state. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pages 387-395, </pages> <address> San Francisco, CA, 1995. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 42 </pages>
Reference: [42] <author> George E. Monahan. </author> <title> A survey of partially observable Markov decision processes: Theory, models, and algorithms. </title> <journal> Management Science, </journal> <volume> 28(1) </volume> <pages> 1-16, </pages> <month> January </month> <year> 1982. </year>
Reference: [43] <author> Robert C. Moore. </author> <title> A formal theory of knowledge and action. </title> <editor> In Jerry R. Hobbs and Robert C. Moore, editors, </editor> <booktitle> Formal Theories of the Commonsense World, </booktitle> <pages> pages 319-358. </pages> <publisher> Ablex Publishing Company, </publisher> <address> Norwood, New Jersey, </address> <year> 1985. </year>
Reference-contexts: This is essentially a planning problem: given a complete and correct model of the world dynamics and a reward structure, find an optimal way to behave. In the artificial intelligence (AI) literature, a deterministic version of this problem has been addressed by adding knowledge preconditions to traditional planning systems <ref> [43] </ref>. Because we are interested in stochastic domains, however, we must depart from the traditional AI planning model.
Reference: [44] <author> L. Morgenstern. </author> <title> Knowledge preconditions for actions and plans. </title> <booktitle> In Proceedings of the 10th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 867-874, </pages> <year> 1987. </year>
Reference: [45] <author> J. S. Penberthy and D. Weld. UCPOP: </author> <title> A sound, complete, partial order planner for ADL. </title> <booktitle> In Proceedings of the third international conference on principles of knowledge representation and reasoning, </booktitle> <pages> pages 103-114, </pages> <year> 1992. </year>
Reference-contexts: In the mdp framework, the agent is informed of the current state each time it takes an action. In many classical planners (e.g., snlp [39], ucpop <ref> [45] </ref>), the current state can be calculated trivially from the known initial state and knowledge of the deterministic operators. The assumption of perfect knowledge is not valid in many domains.
Reference: [46] <author> Mark A. Peot and David E. Smith. </author> <title> Conditional nonlinear planning. </title> <booktitle> In Proceedings of the First International Conference on Artificial Intelligence Planning Systems, </booktitle> <pages> pages 189-197, </pages> <year> 1992. </year>
Reference-contexts: A step towards building a working planning system that reasons about knowledge is to relax the generality of the logic-based schemes. The approach of cnlp <ref> [46] </ref> uses three-valued propositions where, in addition to true and false, there is a value unknown, which represents the state when the truth of the proposition is not known. <p> Many domains are not easily modeled with deterministic actions, since an action can have different results, even when applied in exactly the same state. Extensions to classical planning, such as cnlp <ref> [46] </ref> and Cassandra [48] have considered operators with nondeterministic effects. For each operator, there is a set of possible next states that could occur. A drawback of this approach is that it gives no information about the relative likelihood of the possible outcomes.
Reference: [47] <author> Loren K. Platzman. </author> <title> A feasible computational approach to infinite-horizon partially-observed Markov decision problems. </title> <type> Technical report, </type> <institution> Georgia Institute of Technology, </institution> <address> Atlanta, GA, </address> <month> January </month> <year> 1981. </year>
Reference: [48] <author> Louise Pryor and Gregg Collins. </author> <title> Planning for contingencies: A decision-based approach. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4 </volume> <pages> 287-339, </pages> <year> 1996. </year>
Reference-contexts: Many domains are not easily modeled with deterministic actions, since an action can have different results, even when applied in exactly the same state. Extensions to classical planning, such as cnlp [46] and Cassandra <ref> [48] </ref> have considered operators with nondeterministic effects. For each operator, there is a set of possible next states that could occur. A drawback of this approach is that it gives no information about the relative likelihood of the possible outcomes.
Reference: [49] <author> Martin L. Puterman. </author> <title> Markov Decision Processes|Discrete Stochastic Dynamic Programming. </title> <publisher> John Wiley & Sons, Inc., </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference-contexts: It is often the case that V t = fl long before V t is near V fl ; tighter bounds may be obtained using the span semi-norm on the value function <ref> [49] </ref>. 3 Partial Observability For mdps we can compute the optimal policy and use it to act by simply executing (s) for current state s.
Reference: [50] <author> Lawrence R. Rabiner. </author> <title> A tutorial on hidden Markov models and selected applications in speech recognition. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 77(2) </volume> <pages> 257-286, </pages> <month> February </month> <year> 1989. </year>
Reference: [51] <author> Katsushige Sawaki and Akira Ichikawa. </author> <title> Optimal control for partially observable Markov decision processes over an infinite horizon. </title> <journal> Journal of the Operations Research Society of Japan, </journal> <volume> 21(1) </volume> <pages> 1-14, </pages> <month> March </month> <year> 1978. </year>
Reference: [52] <author> R. B. Scherl and H. J. Levesque. </author> <title> The frame problem and knowledge-producing actions. </title> <booktitle> In Proceedings of the 11th National Conference on Artificial Intelligence, </booktitle> <pages> pages 689-697, </pages> <year> 1993. </year>
Reference: [53] <author> Marcel J. Schoppers. </author> <title> Universal plans for reactive robots in unpredictable environments. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence 10, </booktitle> <pages> pages 1039-1046, </pages> <year> 1987. </year>
Reference-contexts: For completely observable problems with a high branching factor, a more convenient representation is a policy, which maps the current state (situation) to a choice of action. Because there is an action choice specified for all possible initial states, policies are also called universal plans <ref> [53] </ref>. This representation is not appropriate for pomdps, since the underlying state is not fully observable. However, pomdp policies can be viewed as universal plans over belief space. It is interesting to note that there are infinite-horizon pomdps for which no finite-state plan is sufficient.
Reference: [54] <author> Alexander Schrijver. </author> <title> Theory of Linear and Integer Programming. </title> <publisher> Wiley-Interscience, </publisher> <address> New York, NY, </address> <year> 1986. </year>
Reference-contexts: Note that we must assume that the number of bits of precision used in specifying the model is polynomial in these quantities since the polynomial running time of linear programming is 22 expressed as a function of the input precision <ref> [54] </ref>. 4.5 Alternative Approaches The witness algorithm is by no means the only exact algorithm for solving finite-horizon pomdps. The first such algorithm was described by Sondik [58,56]. The one-pass works by identifying linear regions of the value function one at a time.
Reference: [55] <author> Satinder Pal Singh, Tommi Jaakkola, and Michael I. Jordan. </author> <title> Model-free reinforcement learning for non-Markovian decision problems. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 284-292, </pages> <address> San Francisco, California, 1994. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 43 </pages>
Reference-contexts: In our hallway navigation example, this amounts to performing the same action in every location that looks the same|hardly a promising approach. Somewhat better results can be obtained by adding randomness to the agent's behavior: a policy can be a mapping from observations to probability distributions over actions <ref> [55] </ref>. Randomness effectively allows the agent to sometimes choose different actions in different locations with the same appearance, increasing the probability that it might choose a good action; in practice deterministic observation-action mappings are prone to getting trapped in deterministic loops [32].
Reference: [56] <author> Richard D. Smallwood and Edward J. Sondik. </author> <title> The optimal control of partially observable Markov processes over a finite horizon. </title> <journal> Operations Research, </journal> <volume> 21 </volume> <pages> 1071-1088, </pages> <year> 1973. </year>
Reference-contexts: If we could do this, we might be able to reach a computation time per iteration that is polynomial in jSj, jAj, jj, jV t1 j, and jV t j. Cheng [10] and Smallwood and Sondik <ref> [56] </ref> also try to avoid generating all of V + t by constructing V t directly. However, their algorithms still have worst-case running times exponential in at least one of the problem parameters [34].
Reference: [57] <author> David E. Smith and Mike Williamson. </author> <title> Representation and evaluation of plans with loops. </title> <booktitle> Working notes for the 1995 Stanford Spring Symposium on Extended Theories of Action, </booktitle> <year> 1995. </year>
Reference: [58] <author> Edward Sondik. </author> <title> The Optimal Control of Partially Observable Markov Processes. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1971. </year>
Reference-contexts: In the process of motivating the one-pass algorithm, Sondik <ref> [58] </ref> applies the same ideas to finding Q-functions instead of the complete value function. <p> The only finite-time algorithm that has been described for solving pomdps with finitely transient optimal policies over the infinite horizon is a version of policy iteration described by Sondik <ref> [58] </ref>. The simpler policy 28 Fig. 15.
Reference: [59] <author> Edward J. Sondik. </author> <title> The optimal control of partially observable Markov processes over the infinite horizon: Discounted costs. </title> <journal> Operations Research, </journal> <volume> 26(2) </volume> <pages> 282-304, </pages> <year> 1978. </year>
Reference-contexts: From the approximate value function we can extract a stationary policy that is approximately optimal. Sondik <ref> [59] </ref> and Hansen [23] have shown how to use algorithms like witness algorithm that perform exact dynamic-programming backups in pomdps in a policy-iteration algorithm to find exact solutions to many infinite-horizon problems. 5 Understanding Policies In this section we introduce a very simple example and use it to illustrate some properties <p> In many cases, it is possible to encode the policy in a graph that can be used to select actions without any explicit representation of the belief state <ref> [59] </ref>; we refer to such graphs as plan graphs. Recall Figure 14, in which the algorithm has nearly converged upon an infinite-horizon policy for the tiger problem.
Reference: [60] <author> Andreas Stolcke and Stephen Omohundro. </author> <title> Hidden Markov model induction by Bayesian model merging. </title> <editor> In Stephen Jose Hanson, Jack D. Cowan, and C. Lee Giles, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 11-18, </pages> <address> San Mateo, California, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [61] <author> Jonathan Tash and Stuart Russell. </author> <title> Control strategies for a stochastic planner. </title> <booktitle> In Proceedings of the 12th National Conference on Artificial Intelligence, </booktitle> <pages> pages 1079-1085, </pages> <year> 1994. </year>
Reference: [62] <author> Paul Tseng. </author> <title> Solving H-horizon, stationary Markov decision problems in time proportional to log(H). </title> <journal> Operations Research Letters, </journal> <volume> 9(5) </volume> <pages> 287-297, </pages> <year> 1990. </year>
Reference-contexts: The algorithm terminates when the maximum dif ference between two successive value functions (known as the Bellman error magnitude) is less than some *. It can be shown <ref> [62] </ref> that there exists a t fl , polynomial in jSj, jAj, the magnitude of the largest value of R (s; a), and 1=(1 fl), such that the greedy policy with respect to V t fl is equal to the optimal infinite-horizon policy, fl .
Reference: [63] <author> C. C. White and D. Harrington. </author> <title> Application of Jensen's inequality for adaptive suboptimal design. </title> <journal> Journal of Optimization Theory and Applications, </journal> <volume> 32(1) </volume> <pages> 89-99, </pages> <year> 1980. </year>
Reference-contexts: This is not necessarily true for the infinite-horizon discounted value function; it remains convex <ref> [63] </ref>, but may have infinitely many facets. Still, the optimal infinite-horizon discounted value function can be approximated arbitrarily closely by a finite-horizon value function for a sufficiently long horizon [59,51].
Reference: [64] <author> Chelsea C. White, III. </author> <title> Partially observed Markov decision processes: A survey. </title> <journal> Annals of Operations Research, </journal> <volume> 32, </volume> <year> 1991. </year>
Reference-contexts: The simplest pruning strategy, proposed by Sondik [58,42], is to test R (ff; ~ V) for every ff in ~ V and remove those ff that are nowhere dominant. A much more efficient pruning method was proposed by Lark and White <ref> [64] </ref> and is described in detail by Littman [35] and by Cassandra [9].
Reference: [65] <author> Chelsea C. White, III and William T. Scherer. </author> <title> Solution procedures for partially observed Markov decision processes. </title> <journal> Operations Research, </journal> <volume> 37(5) </volume> <pages> 791-797, </pages> <month> September-October </month> <year> 1989. </year>
Reference-contexts: As a result, compared to exhaustive enumeration, very few non--useful policy trees are considered and the algorithm runs extremely quickly. White and Scherer <ref> [65] </ref> propose an alternative approach in which the reward function is changed so that all of the algorithms discussed in this chapter will tend to run more efficiently.
Reference: [66] <author> Ronald J. Williams and Leemon C. Baird, III. </author> <title> Tight performance bounds on greedy policies based on imperfect value functions. </title> <type> Technical Report NU-CCS-93-14, </type> <institution> Northeastern University, College of Computer Science, </institution> <address> Boston, MA, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: Rather than calculating a bound on t fl in advance and running value iteration for that long, we instead use the following result regarding the Bellman error magnitude <ref> [66] </ref> in order to terminate with a near-optimal policy. If jV t (s)V t1 (s)j &lt; * for all s, then the value of the greedy policy with respect to V t does not differ from V fl by more than 2*fl=(1 fl) at any state.
Reference: [67] <author> Nevin L. Zhang and Wenju Liu. </author> <title> Planning in stochastic domains: Problem characteristics and approximation. </title> <type> Technical Report HKUST-CS96-31, </type> <institution> Department of Computer Science, Hong Kong University of Science and Technology, </institution> <year> 1996. </year>
Reference-contexts: In higher dimensions, more complex algorithms are needed and the number of corners is often exponential in the dimensionality. Thus, the geometric approaches are useful only in pomdps with extremely small state spaces. Zhang and Liu <ref> [67] </ref> describe the incremental-pruning algorithm, later generalized by Cassandra, Littman, and Zhang [7]. This algorithm is simple to implement and empirically faster than the witness algorithm, while sharing its good worst-case complexity in terms of P t j.
Reference: [68] <author> Jieyu Zhao and Jurgen H. Schmidhuber. </author> <title> Incremental self-improvement for lifetime multi-agent reinforcement learning. </title> <editor> In Pattie Maes, Maja J. Mataric, Jean-Arcady Meyer, Jordan Pollack, and Stewart W. Wilson, editors, </editor> <booktitle> From Animals to Animats: Proceedings of the Fourth International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> pages 516-525. </pages> <publisher> The MIT Press, </publisher> <year> 1996. </year> <month> 44 </month>
Reference: [69] <author> Uri Zwick and Mike Paterson. </author> <title> The complexity of mean payoff games on graphs. </title> <booktitle> Theoretical Computer Science, </booktitle> <address> 158(1-2):343-359, </address> <year> 1996. </year> <month> 45 </month>
References-found: 69

