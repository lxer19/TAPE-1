URL: http://www.cs.utoronto.ca/~revow/papers/bCart.ps.Z
Refering-URL: http://www.cs.utoronto.ca/~revow/
Root-URL: 
Email: hinton@cs.toronto.edu  revow@cs.toronto.edu  
Title: Using Pairs of Data-Points to Define Splits for Decision Trees  
Author: Geoffrey E. Hinton Michael Revow 
Address: Toronto, Ontario, M5S 1A4, Canada  Toronto, Ontario, M5S 1A4, Canada  
Affiliation: Department of Computer Science University of Toronto  Department of Computer Science University of Toronto  
Abstract: Conventional binary classification trees such as CART either split the data using axis-aligned hyperplanes or they perform a compu-tationally expensive search in the continuous space of hyperplanes with unrestricted orientations. We show that the limitations of the former can be overcome without resorting to the latter. For every pair of training data-points, there is one hyperplane that is orthogonal to the line joining the data-points and bisects this line. Such hyperplanes are plausible candidates for splits. In a comparison on a suite of 12 datasets we found that this method of generating candidate splits outperformed the standard methods, particularly when the training sets were small.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Breiman, J. H. Freidman, R. A. Olshen, and C. J. Stone. </author> <title> Classification and regression trees. </title> <booktitle> Wadsworth international Group, </booktitle> <address> Belmont, California, </address> <year> 1984. </year>
Reference-contexts: There are two commonly used methods of picking a projection direction. The simplest method is to restrict the allowable directions to the k axes defined by the data. This is the default method used in CART <ref> [1] </ref>. If this set of directions is too restrictive, the usual alternative is to search general directions in the full k-dimensional space or general directions in a space defined by a subset of the k axes. <p> We can see no reason to expect strong interactions between the method of building the tree and the method of generating the candidate hyperplanes, but to minimize confounding effects we always use exactly the same method of building the decision tree. We faithfully followed the method described in <ref> [1] </ref>, except for a small modification where the code that was kindly supplied by Leo Breiman used a slightly different method for determining the amount of pruning. Training a decision tree involves two distinct stages. <p> Except as noted in the appendix, the datasets were used exactly in the form of the distribution as of June 1993. All datasets have only continuous attributes and there are no missing values. 5 The synthetic "waves" example <ref> [1] </ref> was added as a twelfth dataset. Table 1 gives a brief description of the datasets. Datasets are identified by a two letter abbreviation along the top. The rows in the table give the total number of instances, number of classes and number of attributes for each dataset. <p> BC Breast cancer database from the University of Wisconsin Hospitals. GL Glass identification database. In these experiments we only considered the classification into float/nonfloat processed glass, ignoring other types of glass. VW Vowel recognition. WN Wine recognition. VH Vehicle silhouettes. WV Waveform example, the synthetic example from <ref> [1] </ref>. IS - Johns Hopkins University Ionosphere database. SN Sonar- mines versus rocks discrimination. We did not control for aspect-angle.
Reference: [2] <author> J. L. Fleiss. </author> <title> Statistical methods for rates and proportions. Second edition. </title> <publisher> Wiley, </publisher> <year> 1981. </year>
Reference-contexts: While these results are interesting, they do not provide any measure of confidence that one method performs better or worse than another. Since all methods were trained and tested on the same data, we can perform a two-tailed McNemar test <ref> [2] </ref> on the predictions for pairs of methods. The resulting P-values are given in table 3.
References-found: 2

