URL: http://www.isi.edu/sims/papers/94-sims-ML.ps
Refering-URL: http://www.isi.edu/~knoblock/
Root-URL: 
Email: fchunnan,knoblockg@isi.edu  
Title: Rule Induction for Semantic Query Optimization  
Author: Chun-Nan Hsu and Craig A. Knoblock 
Address: 4676 Admiralty Way, Marina del Rey, CA 90292  
Affiliation: Information Sciences Institute and Department of Computer Science University of Southern California  
Abstract: Semantic query optimization can dramatically speed up database query answering by knowledge intensive reformulation. But the problem of how to learn required semantic rules has not previously been solved. This paper describes an approach using an inductive learning algorithm to solve the problem. In our approach, learning is triggered by user queries and then the system induces semantic rules from the information in databases. The inductive learning algorithm used in this approach can select an appropriate set of relevant attributes from a potentially huge number of attributes in real-world databases. Experimental results demonstrate that this approach can learn sufficient background knowledge to reformulate queries and provide a 57 percent average performance improvement.
Abstract-found: 1
Intro-found: 1
Reference: <author> Almuallim, H., and Dietterich, T. G. </author> <year> (1991). </year> <title> Learning with many irrelevant features. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> 547-552. </pages>
Reference: <author> Arens, Y.; Chee, C. Y.; Hsu, C.-N.; and Knoblock, C. A. </author> <year> (1993). </year> <title> Retrieving and integrating data from multiple information sources. </title> <journal> International Journal on Intelligent and Cooperative Information Systems 2(2) </journal> <pages> 127-159. </pages>
Reference: <author> Brachman, R., and Schmolze, J. </author> <year> (1985). </year> <title> An overview of the KL-ONE knowledge representation system. </title> <booktitle> Cognitive Science 9(2) </booktitle> <pages> 171-216. </pages>
Reference-contexts: In these information sources, data objects are clustered into conceptual units. For example, the conceptual unit of a relational databases is a relation, or simply a table. For object-based databases, it is a class. In description-logic knowledge bases <ref> (Brachman & Schmolze 1985) </ref>, data instances are clustered into concepts. Each con ceptual unit has attributes that describe the relevant features. Most inductive learning systems, such as ID3, assume that relevant attributes are given. Consider a database with three relations: car, person, and company.
Reference: <author> Cai, Y.; Cercone, N.; and Han, J. </author> <year> (1991). </year> <title> Learning in relational databases: An attribute-oriented approach. </title> <booktitle> Computational Intelligence 7(3) </booktitle> <pages> 119-132. </pages>
Reference-contexts: His paper provides an information-theoretic motivation of the heuristic. <ref> (Cai, Cercone, & Han 1991) </ref> present an attribute-oriented learning approach designed to learn from relational databases. The approach learns conjunctive rules by generalizing instances of a single relation.
Reference: <author> Chvatal, V. </author> <year> (1979). </year> <title> A greedy heuristic for the set covering problem. </title> <note> Mathematics of Operations Research 4. </note>
Reference-contexts: The motivation of this formula is also from the generalized minimum set covering problem. The gain/cost heuristic has been proved to generate a set cover within a small ratio bound (ln jnj + 1) of the optimal set cov ering cost <ref> (Chvatal 1979) </ref>, where n is the number of input sets. However, in this problem, the cost of a set is a constant and the total cost of the entire set covers is the sum of the cost of each set.
Reference: <author> Cormen, T. H.; Leiserson, C. E.; and Rivest, R. L. </author> <year> (1989). </year> <title> Introduction To Algorithms. </title> <address> Cambridge, MA: </address> <publisher> The MIT Press/McGraw-Hill Book Co. </publisher>
Reference-contexts: These rules are then returned immediately and learned by the system. If the proposed rule has more than one antecedent literal, such as rule (1), then the system can use the greedy minimum set cover algorithm <ref> (Cormen, Leis-erson, & Rivest 1989) </ref> to eliminate unnecessary constraints. The problem of minimum set cover is to find a subset from a given collection of sets such that the union of the sets in the subset is equal to the union of all sets.
Reference: <author> Haussler, D. </author> <year> (1988). </year> <title> Quantifying inductive bias: AI learning algorithms and Valiant's learning framework. </title> <booktitle> Artificial Intelligence 36 </booktitle> <pages> 177-221. </pages>
Reference-contexts: Our inductive learning algorithm is extended from the greedy algorithm that learns internal disjunctions proposed by <ref> (Haussler 1988) </ref>. Of the many inductive learning algorithms, Haussler's was chosen because its hypothesis description language is the most similar to ours. His algorithm starts from an empty hypothesis of the target concept description to be learned.
Reference: <author> Hsu, C.-N., and Knoblock, C. A. </author> <year> (1993). </year> <title> Reformulating query plans for multidatabase systems. </title> <booktitle> In Proceedings of the Second International Conference on Information and Knowledge Management. </booktitle>
Reference-contexts: We have developed an efficient reformulation algorithm that is polynomial in terms of the number of applicable rules. We also extended this algorithm to reformulate multi-database query access plans and showed that the reformulations produce substantial performance improvements <ref> (Hsu & Knoblock 1993) </ref>. We conclude this section with the following observations on semantic query optimization. 1. Semantic query optimization can reduce query ex ecution cost substantially. 2. Semantic query optimization is not a tautolog ical transformation from the given query; it requires nontrivial, domain-specific background knowledge. <p> This overhead is very small compared to the total query processing time. On average, the system fires rules 5 times for reformulation. Note that the same rule may be fired more than once during the reformulation procedure (see <ref> (Hsu & Knoblock 1993) </ref> for more detailed descriptions).
Reference: <author> King, J. J. </author> <year> (1981). </year> <title> Query Optimization by Semantic Reasoning. </title> <type> Ph.D. Dissertation, </type> <institution> Stanford University, Department of Computer Science. </institution>
Reference-contexts: In this paper, we present an approach in which inductively learned knowledge is used for semantic query optimization to speed up query answering for data/knowledge-based systems. The principle of semantic query optimization <ref> (King 1981) </ref> is to use semantic rules, such as all Tunisian seaports have railroad access, to reformulate a query into a less expensive but equivalent query, so as to reduce the query evaluation cost. <p> A critical issue of semantic query optimization is how to encode useful background knowledge for reformulation. Most of the previous work in semantic query optimization in the database community assume that the knowledge is given. <ref> (King 1981) </ref> proposed using semantic integrity constraints for reformulation to address the knowledge acquisition problem. Examples of semantic integrity constraints are The salary of an employee is always less than his manager's, and Only female patients can be pregnant.
Reference: <author> Minton, S. </author> <year> (1988). </year> <title> Learning Effective Search Control Knowledge: An Explanation-Based Approach. </title> <type> Ph.D. Dissertation, </type> <institution> Carnegie Mellon University, School of Computer Science. </institution>
Reference-contexts: Since the execution cost of a query is dependent on the properties of the contents of information sources being queried, the utility of a semantic rule is also dependent on these properties. 4. The overhead of reformulation is determined by the number of applicable rules. Therefore, the utility problem <ref> (Minton 1988) </ref> is likely to arise and the learning must be selective. 3 OVERVIEW OF THE LEARNING APPROACH This section presents an overview of our learning approach to address the knowledge acquisition problem of semantic query optimization.
Reference: <editor> Muggleton, S. editor. </editor> <year> (1994). </year> <title> Special section on inductive logic programming. </title> <journal> SIGART Bulletin 5(1) </journal> <pages> 5-49. </pages>
Reference-contexts: The problem of inductive learning from a database with multiple relations shares many issues with research work in inductive logic programming (ILP) <ref> (Muggleton et al. 1994) </ref>, especially the issue of when to introduce new relations. The main difference be tween our approach and ILP is that we also consider the cost of the learned concept description.
Reference: <author> Nunez, M. </author> <year> (1991). </year> <title> The use of background knowledge in decision tree induction. </title> <booktitle> Machine Learning 6 </booktitle> <pages> 231-250. </pages>
Reference-contexts: This function is similar to our function gain=evaluation cost. While there is no theoretic analysis about the general performance of the heuristic I 2 =C for decision-tree learning, our function is derived from approximation heuristics for minimum set cover problems. <ref> (Nunez 1991) </ref> defined another similar heuristic (2 I 1)=C for cost-sensitive decision-tree learning. His paper provides an information-theoretic motivation of the heuristic. (Cai, Cercone, & Han 1991) present an attribute-oriented learning approach designed to learn from relational databases. The approach learns conjunctive rules by generalizing instances of a single relation.
Reference: <author> Pazzani, M. J., and Kibler, D. </author> <year> (1992). </year> <title> The utility of knowledge in inductive learning. </title> <booktitle> Machine Learning 9 </booktitle> <pages> 57-94. </pages>
Reference-contexts: In this way, the system can tolerate incorrect and incomplete prior knowledge. This usage of prior knowledge follows the general spirit of FOCL <ref> (Pazzani & Kibler 1992) </ref>. 5 Experimental Results Our experiments are performed on the SIMS knowledge-based information server (Arens et al. 1993; Hsu & Knoblock 1993). SIMS allows users to access different kinds of remote databases and knowledge bases as if they were using a single system.
Reference: <author> Russell, S. J. </author> <year> (1989). </year> <title> The Use of Knowledge in Analogy and Induction. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The algorithm can be further enhanced by including prior knowledge to reduce the search space. The idea is to use prior knowledge, such as determinations pro posed by <ref> (Russell 1989) </ref>, to sort candidate constraints by their comparative relevance, and then test their gain/cost ratio in this sorted order.
Reference: <author> Shekhar, S.; Hamidzadeh, B.; Kohli, A.; and Coyle, M. </author> <year> (1993). </year> <title> Learning transformation rules for semantic query optimization: A data-driven approach. </title> <journal> IEEE Transactions on Knowledge and Data Engineering 5(6) </journal> <pages> 950-964. </pages>
Reference-contexts: Time saved 30.99 39.14 1.31 125.46 % Gain of total elapsed time 57.1% 87.8% 12.9% 59.1% Average overhead 0.08 0.07 0.07 0.11 Times rule fired 5.00 6.00 4.18 7.00 6 RELATED WORK Previously, two systems that learn background knowledge for semantic query optimization were proposed by (Siegel 1988) and by <ref> (Shekhar et al. 1993) </ref>. Siegel's system uses predefined heuristics to drive learning by an example query. This approach is limited because the heuristics are unlikely to be comprehensive enough to detect missing rules for various queries and databases.
Reference: <author> Shekhar, S.; Srivastava, J.; and Dutta, S. </author> <year> (1988). </year> <title> A formal model of trade-off between optimization and execution costs in semantic query optimization. </title> <booktitle> In Proceedings of the 14th VLDB Conference. </booktitle>
Reference: <author> Shen, W.-M. </author> <year> (1992). </year> <title> Discovering regularities from knowledge bases. </title> <journal> International Journal of Intelligent Systems 7 </journal> <pages> 623-635. </pages>
Reference-contexts: The operationalization component in our learning approach can be enhanced with an EBL-like explainer to filter out low utility rules and generalize rules. A similar "induction-first then EBL" approach can be found in <ref> (Shen 1992) </ref>. Shen's system uses general heuristics to guide the inductive learning for regularities expressed in a rule template P (x; y) ^ R (y; z) ) Q (x; z).
Reference: <author> Shenoy, S. T., and Ozsoyoglu, Z. M. </author> <year> (1989). </year> <title> Design and implementation of a semantic query optimizer. </title> <journal> IEEE Trans. Knowledge and Data Engineering I(3):344-361. </journal>
Reference: <author> Siegel, M. D. </author> <year> (1988). </year> <title> Automatic rule derivation for semantic query optimization. </title> <editor> In Kerschberg, L., ed., </editor> <booktitle> Proceedings of the Second International Conference on Expert Database Systems. </booktitle> <institution> Fairfax, VA: George Mason Foundation. </institution> <month> 371-385. </month>
Reference-contexts: 23.28 5.45 8.79 86.78 Time saved 30.99 39.14 1.31 125.46 % Gain of total elapsed time 57.1% 87.8% 12.9% 59.1% Average overhead 0.08 0.07 0.07 0.11 Times rule fired 5.00 6.00 4.18 7.00 6 RELATED WORK Previously, two systems that learn background knowledge for semantic query optimization were proposed by <ref> (Siegel 1988) </ref> and by (Shekhar et al. 1993). Siegel's system uses predefined heuristics to drive learning by an example query. This approach is limited because the heuristics are unlikely to be comprehensive enough to detect missing rules for various queries and databases.
Reference: <author> Tan, M. </author> <year> (1993). </year> <title> Cost-sensitive learning of classification knowledge and its application in robotics. </title> <booktitle> Machine Learning 13 </booktitle> <pages> 7-33. </pages>
Reference-contexts: Our approach can learn from large databases because it also uses the knowledge underlying the database design. Tan's cost-sensitive learning <ref> (Tan 1993) </ref> is an inductive learning algorithm that also takes the cost of the learned description into account. His algorithm tries to learn minimum-cost decision trees from examples in a robot object-recognition domain. The algorithm selects a minimum number of attributes to construct a decision tree for recognition.
Reference: <author> Ullman, J. D. </author> <year> (1988). </year> <title> Principles of Database and Knowledge-base Systems, </title> <booktitle> volume I,II. </booktitle> <address> Palo Alto, CA: </address> <publisher> Computer Science Press. </publisher>
Reference-contexts: There are two types of literals. The first type corresponds to a relation stored in a database. The second type consists of built-in predicates, such as &gt; and member. Sometimes they are referred to as extensional and intentional relations, respectively (see <ref> (Ullman 1988) </ref>). We do not consider negative literals and recursion in this paper. Semantic rules for query optimization are also expressed in terms of Horn-clause rules. Semantic rules must be consistent with the contents of a database. <p> To estimate the actual cost of a constraint is very expensive. We therefore use an approximation heuristic here. The evaluation cost of individual constraints can be estimated using standard database query optimization techniques <ref> (Ullman 1988) </ref> as follows.
References-found: 21

