URL: http://www.cm.deakin.edu.au/~zijian/Papers/icycs95.ps.gz
Refering-URL: http://www.cm.deakin.edu.au/~zijian/publications.html
Root-URL: 
Email: (Email: zijian@cs.su.oz.au)  
Title: Continuous-valued Xof-N Attributes Versus Nominal Xof-N Attributes for Constructive Induction: A Case Study  
Author: Zijian Zheng 
Address: Sydney, NSW 2006, Australia  
Affiliation: Basser Department of Computer Science The University of  
Note: In Proceedings of the Fourth International Conference for Young Computer Scientists (ICYCS-95), Peking University Press, 566-573, 1995.  
Abstract: An Xof-N is a set containing one or more attribute-value pairs. For a given instance, its value corresponds to the number of its attribute-value pairs that are true. In this paper, we explore the characteristics and performance of continuous-valued Xof-N attributes versus nominal Xof-N attributes for constructive induction. Nominal Xof-Ns are more representationally powerful than continuous-valued Xof-Ns, but the former suffer the "fragmentation" problem, although some mechanisms such as subsetting can help to solve the problem. Two approaches to constructive induction using continuous-valued Xof-Ns are described. Continuous-valued Xof-Ns perform better than nominal ones on domains that need Xof-Ns with only one cut point. On domains that need Xof-N representations with more than one cut point, nominal Xof-Ns perform better than continuous-valued ones. Experimental results on a set of artificial and real-world domains support these statements. 
Abstract-found: 1
Intro-found: 1
Reference: [Bloedorn et al., 1993] <author> E. Bloedorn, R.S. Michalski, and J. Wnek, </author> <title> Multistrategy constructive induction: </title> <booktitle> AQ17-MCI. Proceedings of the Second International Workshop on Multistrategy Learning (MSL-93), </booktitle> <pages> 188-203, </pages> <year> 1993. </year>
Reference-contexts: Some researchers construct new continuous-values attributes by using attribute counting operators <ref> [Michalski, 1978; Bloedorn et al., 1993] </ref> or mathematical operators such as multiplication and division [Michalski, 1978; Langley et al., 1987]. We proposed a new approach to constructive induction that creates Xof-N representations and uses them as new nominal attributes [Zheng, 1995]. <p> ID2-of-3 uses two different operators to construct M-of-N representations as binary attributes, while LFC generates conjunctions as binary attributes by using a directed lookahead search. Instead of building decision trees, MoN [Ting, 1994] generates M-of-N rules. The production rule learning algorithms INDUCE [Michalski, 1978], AQ17-DCI, and AQ17-MCI <ref> [Bloedorn et al., 1993] </ref> use the attribute counting operator #VarEQ (x) 9 to construct new attributes that count the number of attributes in an instance which take the value x.
Reference: [Breiman et al., 1984] <author> L. Breiman, J.H. Friedman, R.A. Olshen, and C.J. Stone, </author> <title> Classification And Regression Trees, </title> <address> Belmont, CA: </address> <publisher> Wadsworth, </publisher> <year> 1984. </year>
Reference-contexts: Following the method used in [Dietterich et al., 1990] we generate data sets by using a window of length 7, but use the 1000 most common English words. For each real-world domain, a 10-fold cross-validation <ref> [Breiman et al., 1984] </ref> is conducted on the entire data set.
Reference: [Dietterich et al., 1990] <author> T.G. Dietterich, H. Hild, and G. Bakiri, </author> <title> A comparative study of ID3 and backpropagation for English text-to-speech mapping. </title> <booktitle> Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <pages> 24-31, </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: Nettalk (Phoneme) and Nettalk (Stress) are two basic subproblems of the Nettalk domain. Nettalk (Letter) is a combination of them. Their tasks are mapping a letter in an English word into a phoneme, a stress, and a phoneme-stress pair respectively. Following the method used in <ref> [Dietterich et al., 1990] </ref> we generate data sets by using a window of length 7, but use the 1000 most common English words. For each real-world domain, a 10-fold cross-validation [Breiman et al., 1984] is conducted on the entire data set.
Reference: [Langley et al., 1987] <author> P. Langley, H.A. Simon, G.L. Bradshaw, and J.M. Zytkow, </author> <title> Scientific Discovery: Computational Explorations of the Creative Processes, </title> <publisher> Cambridge: MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: Some researchers construct new continuous-values attributes by using attribute counting operators [Michalski, 1978; Bloedorn et al., 1993] or mathematical operators such as multiplication and division <ref> [Michalski, 1978; Langley et al., 1987] </ref>. We proposed a new approach to constructive induction that creates Xof-N representations and uses them as new nominal attributes [Zheng, 1995]. Because they have ordered discrete values, they can also be treated as new continuous-valued attributes.
Reference: [Matheus and Rendell, 1989] <author> C.J. Matheus and L.A. Rendell, </author> <title> Constructive induction on decision trees. </title> <booktitle> Proceedings of the 11th International Joint Conference on Artificial Intelligence, </booktitle> <pages> 645-650, </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1989. </year>
Reference-contexts: 1. Introduction A wide variety of methods has been explored to construct new binary attributes by using, as constructive operators, logical AND, NOT, OR <ref> [Matheus and Rendell, 1989; Pa-gallo, 1990; Ragavan and Rendell, 1993] </ref>, and M-of-N [Murphy and Pazzani, 1991]. Some researchers construct new continuous-values attributes by using attribute counting operators [Michalski, 1978; Bloedorn et al., 1993] or mathematical operators such as multiplication and division [Michalski, 1978; Langley et al., 1987]. <p> Most hypothesis-driven constructive induction systems such as FRINGE [Pagallo, 1990], CITRE <ref> [Matheus and Rendell, 1989] </ref>, and CI [Zheng, 1992] construct and/or select a set of new attributes based on the entire training set. <p> This strategy has a shortcoming: new attributes that have high evaluation function values on the entire training set might have lower values than other unselected new attributes on a training subset after a part of a decision tree has been created <ref> [Matheus and Rendell, 1989] </ref>. To overcome this, XofN, XofN (c), and XofN (cc) construct one new attribute using the local training set for each decision node. Therefore, the new attribute constructed at each decision node is the best one in the algorithms' search space in terms of the evaluation function.
Reference: [Michalski, 1978] <author> R.S. Michalski, </author> <title> Pattern recognition as knowledge-guided computer induction. </title> <type> TR: 927, </type> <institution> Dept of Computer Science, The University of Illinois, Urbana, </institution> <year> 1978. </year>
Reference-contexts: Some researchers construct new continuous-values attributes by using attribute counting operators <ref> [Michalski, 1978; Bloedorn et al., 1993] </ref> or mathematical operators such as multiplication and division [Michalski, 1978; Langley et al., 1987]. We proposed a new approach to constructive induction that creates Xof-N representations and uses them as new nominal attributes [Zheng, 1995]. <p> Some researchers construct new continuous-values attributes by using attribute counting operators [Michalski, 1978; Bloedorn et al., 1993] or mathematical operators such as multiplication and division <ref> [Michalski, 1978; Langley et al., 1987] </ref>. We proposed a new approach to constructive induction that creates Xof-N representations and uses them as new nominal attributes [Zheng, 1995]. Because they have ordered discrete values, they can also be treated as new continuous-valued attributes. <p> ID2-of-3 uses two different operators to construct M-of-N representations as binary attributes, while LFC generates conjunctions as binary attributes by using a directed lookahead search. Instead of building decision trees, MoN [Ting, 1994] generates M-of-N rules. The production rule learning algorithms INDUCE <ref> [Michalski, 1978] </ref>, AQ17-DCI, and AQ17-MCI [Bloedorn et al., 1993] use the attribute counting operator #VarEQ (x) 9 to construct new attributes that count the number of attributes in an instance which take the value x.
Reference: [Murphy and Aha, 1994] <author> P.M. Murphy and D.W. Aha, </author> <title> UCI Repository of machine learning databases [Machine-readable data repository]. </title> <address> Irvine, CA: </address> <institution> University of California, Department of Information and Computer Science, </institution> <note> Available by anonymous ftp at ics.uci.edu in the pub/machine-learning-databases directory, </note> <year> 1994. </year>
Reference-contexts: All of them can be accessed by anonymous ftp at ics.uci.edu in the pub/machine-learning-databases directory <ref> [Murphy and Aha, 1994] </ref>. In all the experiments reported here, all algorithms are run with their default option settings, and are run on the same partitions. 4.2. Experimental results Table 2 gives results of the four algorithms on two artificial and ten real-world domains.
Reference: [Murphy and Pazzani, 1991] <author> P.M. Murphy and M.J. Pazzani, ID2-of-3: </author> <title> Constructive induction of M-of-N concepts for discriminators in decision trees. </title> <booktitle> Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <pages> 183-187, </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: 1. Introduction A wide variety of methods has been explored to construct new binary attributes by using, as constructive operators, logical AND, NOT, OR [Matheus and Rendell, 1989; Pa-gallo, 1990; Ragavan and Rendell, 1993], and M-of-N <ref> [Murphy and Pazzani, 1991] </ref>. Some researchers construct new continuous-values attributes by using attribute counting operators [Michalski, 1978; Bloedorn et al., 1993] or mathematical operators such as multiplication and division [Michalski, 1978; Langley et al., 1987]. <p> These results support our statement that nominal Xof-N is more representationally powerful than continuous-valued Xof-N but nominal Xof-N suffers the "fragmentation" problem. 5. Related Work Like XofN, XofN (c), and XofN (cc), ID2-of-3 <ref> [Murphy and Pazzani, 1991] </ref> and LFC [Ra-gavan and Rendell, 1993] are also data-driven constructive induction algorithms. They also create one new attribute at each decision node when building a tree.
Reference: [Oliver et al., 1992] <author> J.J. Oliver, D.L. Dowe, and C.S. Wallace, </author> <title> Inferring decision graphs using the minimum message length principle. </title> <booktitle> Proceedings of the 5th Australian Joint Conference on Artificial Intelligence, </booktitle> <pages> 361-367, </pages> <publisher> World Scientific, </publisher> <year> 1992. </year>
Reference-contexts: If target concepts are not complex or huge data sets are available, it is not a problem. In addition, a simple method that helps to solve the problem is subset-ting [Quinlan, 1993]. Two other plausible solutions are subranging and decision graphs <ref> [Oliver et al., 1992] </ref>. 3. Algorithms [Zheng, 1995] gives a constructive induction algorithm XofN that builds trees with nominal Xof-N attributes. This section briefly describes the main idea of XofN, and then discusses methods of creating and using continuous-valued Xof-N attributes.
Reference: [Pagallo, 1990] <author> G. Pagallo, </author> <title> Adaptive Decision Tree Algorithms for Learning from Examples, </title> <type> Ph.D. thesis, </type> <institution> University of California at Santa Cruz, </institution> <year> 1990. </year>
Reference-contexts: On the Tic-Tac-Toe domain, it is 0.2, 25.7, 49.9, and 64.4 seconds respectively. 4.1. Experimental domains and methods Two artificial domains used here are from <ref> [Pagallo, 1990] </ref>. One is a randomly generated DNF concept: DNF4. It has ten terms with 64 binary attributes. Twenty-nine of them are irrelevant. The other is an even parity function with 5 relevant and 27 irrelevant attributes: Parity5. We use the same experimental method given in [Pagallo, 1990]. <p> used here are from <ref> [Pagallo, 1990] </ref>. One is a randomly generated DNF concept: DNF4. It has ten terms with 64 binary attributes. Twenty-nine of them are irrelevant. The other is an even parity function with 5 relevant and 27 irrelevant attributes: Parity5. We use the same experimental method given in [Pagallo, 1990]. For each experiment, a training set and a test set are independently drawn from the uniform distribution. The size of the test sets is 2000. The sizes of training sets are 2640 for DNF4 and 4000 for Parity5. <p> They are the results of a VC dimension analysis for finding the number of examples which would suffice for an ideal learning algorithm to create a consistent hypothesis with an error rate less than 10% on any test set (see <ref> [Pagallo, 1990] </ref> for details). On each concept, experiments are repeated ten times using different training and test sets. In addition, we use ten real-world domains on which M-of-N like concepts are expected to be found [Spackman, 1988]. <p> Most hypothesis-driven constructive induction systems such as FRINGE <ref> [Pagallo, 1990] </ref>, CITRE [Matheus and Rendell, 1989], and CI [Zheng, 1992] construct and/or select a set of new attributes based on the entire training set.
Reference: [Quinlan, 1993] <author> J.R. Quinlan, C4.5: </author> <title> Programs for Machine Learning, </title> <address> San Meteo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: If target concepts are not complex or huge data sets are available, it is not a problem. In addition, a simple method that helps to solve the problem is subset-ting <ref> [Quinlan, 1993] </ref>. Two other plausible solutions are subranging and decision graphs [Oliver et al., 1992]. 3. Algorithms [Zheng, 1995] gives a constructive induction algorithm XofN that builds trees with nominal Xof-N attributes. <p> Algorithms [Zheng, 1995] gives a constructive induction algorithm XofN that builds trees with nominal Xof-N attributes. This section briefly describes the main idea of XofN, and then discusses methods of creating and using continuous-valued Xof-N attributes. XofN uses C4.5 <ref> [Quinlan, 1993] </ref> as its selective induction component. At each decision node when building a tree, it creates one nominal Xof-N attribute by using a simple greedy search in the instance space defined by primitive attributes. XofN considers reusing new attributes constructed previously.
Reference: [Ragavan and Rendell, 1993] <author> H. Ragavan and L. Rendell, </author> <title> Lookahead feature construction for learning hard concepts. </title> <booktitle> Proceedings of the 10th International Conference on Machine Learning, </booktitle> <pages> 252-259, </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: 1. Introduction A wide variety of methods has been explored to construct new binary attributes by using, as constructive operators, logical AND, NOT, OR <ref> [Matheus and Rendell, 1989; Pa-gallo, 1990; Ragavan and Rendell, 1993] </ref>, and M-of-N [Murphy and Pazzani, 1991]. Some researchers construct new continuous-values attributes by using attribute counting operators [Michalski, 1978; Bloedorn et al., 1993] or mathematical operators such as multiplication and division [Michalski, 1978; Langley et al., 1987].
Reference: [Spackman, 1988] <author> K.A. Spackman, </author> <title> Learning categorical decision criteria in biomedical domains. </title> <booktitle> Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <pages> 36-46, </pages> <publisher> Morgan Kauf-mann, </publisher> <year> 1988. </year>
Reference-contexts: On each concept, experiments are repeated ten times using different training and test sets. In addition, we use ten real-world domains on which M-of-N like concepts are expected to be found <ref> [Spackman, 1988] </ref>. They are five medical domains: Cleveland Heart Disease, Hepatitis, Liver Disorders, Pima Indians Diabetes, Wisconsin Breast Cancer, one molecular biology domain: Promoters, three linguistics domains: Nettalk (Phoneme), Nettalk (Stress), Nettalk (Letter), and one game domain: Tic-Tac-Toe.
Reference: [Thrun et al., 1991] <author> S.B. Thrun, J. Bala, E. Bloedorn, I. Bratko, B. Cestnik, J. Cheng, K. De Jong, S. Dzeroski, S.E. Fahlman, D. Fisher, R. Hamann, K. Kaufman, S. Keller, I. Kononenko, J. Kreuziger, R.S. Michalski, T. Mitchell, P. Pachowicz, Y. Reich, H. Vafaie, W. Van de Welde, W. Wenzel, J. Wnek, and J. Zhang, </author> <title> The MONK's problems a performance comparison of different learning algorithms. </title> <type> Tech. Report: </type> <institution> CMU-CD-91-197, Carnegie Mellon University, </institution> <year> 1991. </year>
Reference-contexts: Ting and N. Indurkhya for comments that improve the ideas and earlier drafts. Finally, P.M. Murphy and D. Aha are gratefully acknowledged for creating and managing the UCI Repository of machine learning databases. 10 Generated rules have the form like (#VarEQ (1) &gt;= 3) <ref> [Thrun et al., 1991, p11] </ref>. 11 On the DNF4 domain, XofN needs the subsetting mechanism. 7
Reference: [Ting, 1994] <author> K.M. Ting, </author> <title> An M-of-N rule induction algorithm and its application to DNA domain. </title> <booktitle> Proceedings of the 27th Annual Hawaii International Conference on System Sciences, Volume V: Biotechnology Computing, </booktitle> <pages> 133-140, </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1994. </year>
Reference-contexts: They also create one new attribute at each decision node when building a tree. ID2-of-3 uses two different operators to construct M-of-N representations as binary attributes, while LFC generates conjunctions as binary attributes by using a directed lookahead search. Instead of building decision trees, MoN <ref> [Ting, 1994] </ref> generates M-of-N rules. The production rule learning algorithms INDUCE [Michalski, 1978], AQ17-DCI, and AQ17-MCI [Bloedorn et al., 1993] use the attribute counting operator #VarEQ (x) 9 to construct new attributes that count the number of attributes in an instance which take the value x.
Reference: [Zheng, 1992] <author> Z. Zheng, </author> <title> Constructing conjunctive tests for decision trees. </title> <booktitle> Proceedings of Australian Joint Conference on Artificial Intelligence, </booktitle> <pages> 355-360, </pages> <publisher> World Scientific, </publisher> <year> 1992. </year>
Reference-contexts: Most hypothesis-driven constructive induction systems such as FRINGE [Pagallo, 1990], CITRE [Matheus and Rendell, 1989], and CI <ref> [Zheng, 1992] </ref> construct and/or select a set of new attributes based on the entire training set.
Reference: [Zheng, 1995] <author> Z. Zheng, </author> <title> Constructing nominal X-of-N attributes. </title> <booktitle> Proceedings of the 14th International Joint Conference on Artificial Intelligence, </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year> <month> 8 </month>
Reference-contexts: We proposed a new approach to constructive induction that creates Xof-N representations and uses them as new nominal attributes <ref> [Zheng, 1995] </ref>. Because they have ordered discrete values, they can also be treated as new continuous-valued attributes. <p> Then we evaluate the performance of continuous-valued Xof-N attributes versus nominal Xof-N attributes using a set of artificial and real-world domains. 2. Xof-N Representations and Their Characteristics A definition of the Xof-N representation is given in <ref> [Zheng, 1995] </ref>. In short, an Xof-N is a set containing N + attribute-value pairs with N different attributes. N + , the size of the Xof-N representation, is greater than or equal to N . The value of an Xof-N can be any integer between 0 and N . <p> If target concepts are not complex or huge data sets are available, it is not a problem. In addition, a simple method that helps to solve the problem is subset-ting [Quinlan, 1993]. Two other plausible solutions are subranging and decision graphs [Oliver et al., 1992]. 3. Algorithms <ref> [Zheng, 1995] </ref> gives a constructive induction algorithm XofN that builds trees with nominal Xof-N attributes. This section briefly describes the main idea of XofN, and then discusses methods of creating and using continuous-valued Xof-N attributes. XofN uses C4.5 [Quinlan, 1993] as its selective induction component. <p> The test for a decision node is the best one of the Xof-N created for the node, all Xof-Ns generated previously, and all primitive attributes. The evaluation function for new attributes is a combination of the information gain ratio 2 and the coding cost of new attributes <ref> [Zheng, 1995] </ref>. Evaluations are done on the local training set of the current decision node. The simple greedy search currently used starts from an empty set. It alternatively applies two operators: adding one possible attribute-value pair and deleting one possible attribute-value pair.
References-found: 17

