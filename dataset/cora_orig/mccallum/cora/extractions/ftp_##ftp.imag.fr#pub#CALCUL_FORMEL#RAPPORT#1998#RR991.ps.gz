URL: ftp://ftp.imag.fr/pub/CALCUL_FORMEL/RAPPORT/1998/RR991.ps.gz
Refering-URL: http://www-lmc.imag.fr/CF/publi.html
Root-URL: http://www.imag.fr
Email: Gilles.Villard@imag.fr  
Title: Block solution of sparse linear systems over GF(q): the singular case  
Author: G. Villard 
Address: B.P. 53 F38041 GRENOBLE cedex 9  
Affiliation: LMC-IMAG,  
Abstract: 2N matrix times vector products and O(n 2+o(1) ) additional arithmetic operations. Only the block Wiedemann algorithm as given by Kaltofen, has the least number (1 + *)N + O(1) of matrix-vector products of any known algorithm. We extend its analysis to the case of singular matrices A. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Coppersmith, D. </author> <title> Solving linear equations over GF(2): block Lanczos algorithm. </title> <booktitle> Linear Algebra and its Applications 192 (1993), </booktitle> <pages> 33-60. </pages>
Reference-contexts: It is well known that these latter methods are strongly related to each other. For a unified theory in the finite field case, the reader may refer to [7]. Typical in numerical analysis, these techniques are not yet fully analyzed over algebraic domains. Proposed and experimented as heuristics in <ref> [6, 1, 2, 8] </ref>, they lead to empirical data, which point out the lack of theoretical explanations for various problems. Attempts of probability analysis are made in [8, 7] but only for random inputs. The Wiedemann approach is the best understood technique.
Reference: [2] <author> Coppersmith, D. </author> <title> Solving homogeneous linear equations over GF(2) via block Wiedemann algorithm. </title> <journal> Math. Comp. </journal> <volume> 62, 205 (1994), </volume> <pages> 333-350. </pages>
Reference-contexts: It is well known that these latter methods are strongly related to each other. For a unified theory in the finite field case, the reader may refer to [7]. Typical in numerical analysis, these techniques are not yet fully analyzed over algebraic domains. Proposed and experimented as heuristics in <ref> [6, 1, 2, 8] </ref>, they lead to empirical data, which point out the lack of theoretical explanations for various problems. Attempts of probability analysis are made in [8, 7] but only for random inputs. The Wiedemann approach is the best understood technique. <p> We refer to [12] for the non block method and to [4, 9, 11] for the block method of Coppersmith <ref> [2] </ref>. Note that, using the unified approach of [7], these results may be extended to the conjugate gradient and to the Lanczos non block methods. These studies lead to the following cost analysis. <p> In this short note we show that some ideas used to obtain the two latter results, for general non homogeneous systems, can be applied to the block Wiedemann algorithm of Coppersmith <ref> [2] </ref>. To use the block approach will allow us to reduce the number of required matrix-vector products. Precisely, this will lead to the least known number (given above) of products for such methods. <p> To limit the length of the presentation, we assume the reader is a little bit familiar with the papers [12, 4] and [11]. 1 Block solution of non homogeneous systems We adapt the Coppersmith's version <ref> [2] </ref> of Wiedemann's algorithm to the solution of a non homogeneous system Ax = b. We use the terminology proposed in [11]. The algorithm picks up two random matrices X in M m;N (K) and Y in M N;n1 (K), then computes Z = [b; AY ]. <p> The overall number of matrix-vector products is thus (1 + n=m + 2=(n 1))N , with O ((1 + n=m)N 2 log N log log N ) additional operations. A matrix generating polynomial D Z () is computed in O ((m + n)N 2 arithmetic operations <ref> [2, 4, 10] </ref>. From there G Z () is obtained at a lower cost.
Reference: [3] <author> Eberly, W., and Kaltofen, E. </author> <title> On randomized Lanczos algorithms. </title> <booktitle> In International Symposium on Symbolic and Algebraic Computation, </booktitle> <address> Maui, Hawaii, USA (July 1997), </address> <publisher> ACM Press. </publisher>
Reference-contexts: The solution proposed by Wiedemann involves O (N log N ) matrix-vector products over any field [12]. This can be improved to O (N ) products using the randomization in [5] and to 2N products using the results in <ref> [3] </ref>. This latter paper is about the Lanczos algorithm, but its conclusions carry over to the Wiedemann one. Any system Ax = b over a large cardinality field, can thus be solved with high probability using 2N matrix-vector products and O (N 2 ) additional arithmetic operations.
Reference: [4] <author> Kaltofen, E. </author> <title> Analysis of Coppersmith's block Wiedemann algorithm for the parallel solution of sparse linear systems. </title> <journal> Math. Comp. </journal> <volume> 64, 210 (1995), </volume> <pages> 777-806. </pages>
Reference-contexts: To find dependencies i.e. to solve Ax = 0 or to solve nonsingular systems Ax = b, the corresponding algorithms are rigorously proven to work with probability strictly greater than zero over any field. We refer to [12] for the non block method and to <ref> [4, 9, 11] </ref> for the block method of Coppersmith [2]. Note that, using the unified approach of [7], these results may be extended to the conjugate gradient and to the Lanczos non block methods. These studies lead to the following cost analysis. <p> Over sufficiently large fields, the least number of matrix-vector products, required to solve a system Ax = b for any known algorithm, is (1 + *)N + O (1), for any arbitrarily small positive constant *. It can be arrived at, only using the block method in <ref> [4] </ref>. Unfortunately its full analysis was restricted to invertible matrices. Indeed, the case Ax = b with A singular is a little bit more difficult. The solution proposed by Wiedemann involves O (N log N ) matrix-vector products over any field [12]. <p> The result is established in x2, we begin in x1 with a short presentation of the algorithm. To limit the length of the presentation, we assume the reader is a little bit familiar with the papers <ref> [12, 4] </ref> and [11]. 1 Block solution of non homogeneous systems We adapt the Coppersmith's version [2] of Wiedemann's algorithm to the solution of a non homogeneous system Ax = b. We use the terminology proposed in [11]. <p> The bound on ffi l directly comes from corollary 6.5 in [9]. 2 We can now prove the final result. This is an extension of the corollary of page 795 in <ref> [4] </ref> for general matrices. Theorem 1 Let A be an N fiN matrix over K =GF (q). Let * &gt; 0 be fixed. Suppose that X with m rows and Y with n1 columns are chosen at random over K. Let b be in the range space of A. <p> To ensure that (det D Z ())(0) 6= 0 we apply lemma 1. The condition on A is satisfied by preconditioning A using Toeplitz matrices (theorem 2 in [5]) and a diagonal matrix (lemma 2 in [5]) as done in <ref> [4] </ref>. The condition on Z is satisfied by construction and by assumption on b. Next we ensure that the left projection using X does not modify the matrix generating polynomials. <p> Next we ensure that the left projection using X does not modify the matrix generating polynomials. We use the result given by lemma 2 in the generic case and get the target probability for random X and Y using either theorem 6 in <ref> [4] </ref> or theorem 5 in [11]. The condition on m is satisfied since the above preconditioning ensures that OE fl = 1 [5]. We now detail the cost of the algorithm. <p> We now detail the cost of the algorithm. Including the cost of the preconditioning, multiplying A by a vector requires one product of a matrix times a vector of dimensions N plus O (log N log log N ) arithmetic operations in K <ref> [4] </ref>. <p> of A times a vector are required to generate the sequence fH i g ffi l +ffi r 1 i=0 ; N=(n 1) additional multiplications plus O (N 2 ) operations in K are needed to compute the solution x from a generating polynomial of degree less than N=(n 1) <ref> [4] </ref>. The overall number of matrix-vector products is thus (1 + n=m + 2=(n 1))N , with O ((1 + n=m)N 2 log N log log N ) additional operations. A matrix generating polynomial D Z () is computed in O ((m + n)N 2 arithmetic operations [2, 4, 10]. <p> The overall number of matrix-vector products is thus (1 + n=m + 2=(n 1))N , with O ((1 + n=m)N 2 log N log log N ) additional operations. A matrix generating polynomial D Z () is computed in O ((m + n)N 2 arithmetic operations <ref> [2, 4, 10] </ref>. From there G Z () is obtained at a lower cost. <p> A matrix generating polynomial D Z () is computed in O ((m + n)N 2 arithmetic operations [2, 4, 10]. From there G Z () is obtained at a lower cost. Following <ref> [4] </ref> and formulating these costs in terms of * = n=m + 2=(n 1), a quantity that can be made arbitrarily close to zero for well chosen values of m and n, the theorem is proven. 2 Remark 1 The preconditioning used for A, actually ensures stronger properties for the spectral
Reference: [5] <author> Kaltofen, E., and Saunders, B. </author> <title> On Wiedemann's method of solving sparse linear systems. </title> <booktitle> In Proc. AAECC-9 (1991), </booktitle> <publisher> LNCS 539, Springer Verlag, </publisher> <pages> pp. 29-38. </pages>
Reference-contexts: Indeed, the case Ax = b with A singular is a little bit more difficult. The solution proposed by Wiedemann involves O (N log N ) matrix-vector products over any field [12]. This can be improved to O (N ) products using the randomization in <ref> [5] </ref> and to 2N products using the results in [3]. This latter paper is about the Lanczos algorithm, but its conclusions carry over to the Wiedemann one. <p> A (h (A)b + g 2 (A)Y 2 + : : : + g n (A)Y n ) : The algorithm may thus output as a solution x for the system Ax = b: The algorithm is randomized concerning two points: to satisfy (2) we rely below on results in <ref> [12, 5] </ref>. To ensure that the left projection is correct we use our previous studies of the algorithm in the homogeneous case [9, 11]. 2 Probabilistic analysis We first give the necessary conditions to satisfy (2). <p> Proof. Let us first show that the algorithm produces a solution with the claimed probability. To ensure that (det D Z ())(0) 6= 0 we apply lemma 1. The condition on A is satisfied by preconditioning A using Toeplitz matrices (theorem 2 in <ref> [5] </ref>) and a diagonal matrix (lemma 2 in [5]) as done in [4]. The condition on Z is satisfied by construction and by assumption on b. Next we ensure that the left projection using X does not modify the matrix generating polynomials. <p> To ensure that (det D Z ())(0) 6= 0 we apply lemma 1. The condition on A is satisfied by preconditioning A using Toeplitz matrices (theorem 2 in <ref> [5] </ref>) and a diagonal matrix (lemma 2 in [5]) as done in [4]. The condition on Z is satisfied by construction and by assumption on b. Next we ensure that the left projection using X does not modify the matrix generating polynomials. <p> We use the result given by lemma 2 in the generic case and get the target probability for random X and Y using either theorem 6 in [4] or theorem 5 in [11]. The condition on m is satisfied since the above preconditioning ensures that OE fl = 1 <ref> [5] </ref>. We now detail the cost of the algorithm. Including the cost of the preconditioning, multiplying A by a vector requires one product of a matrix times a vector of dimensions N plus O (log N log log N ) arithmetic operations in K [4].
Reference: [6] <author> LaMacchia, B., and Odlyzko, A. </author> <title> Solving large sparse linear systems over finite fields. </title> <booktitle> In Advances in Cryptology - CRYPTO'90, </booktitle> <publisher> Springer LNCS 537 (1991), </publisher> <editor> A. Menezes and S. Vanstone, </editor> <booktitle> Eds., </booktitle> <pages> pp. 109-133. </pages>
Reference-contexts: It is well known that these latter methods are strongly related to each other. For a unified theory in the finite field case, the reader may refer to [7]. Typical in numerical analysis, these techniques are not yet fully analyzed over algebraic domains. Proposed and experimented as heuristics in <ref> [6, 1, 2, 8] </ref>, they lead to empirical data, which point out the lack of theoretical explanations for various problems. Attempts of probability analysis are made in [8, 7] but only for random inputs. The Wiedemann approach is the best understood technique.
Reference: [7] <author> Lambert, R. </author> <title> Computational aspects of discrete logarithms. </title> <type> PhD thesis, </type> <institution> University of Waterloo, </institution> <address> Ontario, Canada, </address> <year> 1996. </year>
Reference-contexts: Another approach is to apply either the conjugate gradient, the Lanczos or the Krylov / Wiede-mann method. It is well known that these latter methods are strongly related to each other. For a unified theory in the finite field case, the reader may refer to <ref> [7] </ref>. Typical in numerical analysis, these techniques are not yet fully analyzed over algebraic domains. Proposed and experimented as heuristics in [6, 1, 2, 8], they lead to empirical data, which point out the lack of theoretical explanations for various problems. <p> Typical in numerical analysis, these techniques are not yet fully analyzed over algebraic domains. Proposed and experimented as heuristics in [6, 1, 2, 8], they lead to empirical data, which point out the lack of theoretical explanations for various problems. Attempts of probability analysis are made in <ref> [8, 7] </ref> but only for random inputs. The Wiedemann approach is the best understood technique. To find dependencies i.e. to solve Ax = 0 or to solve nonsingular systems Ax = b, the corresponding algorithms are rigorously proven to work with probability strictly greater than zero over any field. <p> We refer to [12] for the non block method and to [4, 9, 11] for the block method of Coppersmith [2]. Note that, using the unified approach of <ref> [7] </ref>, these results may be extended to the conjugate gradient and to the Lanczos non block methods. These studies lead to the following cost analysis.
Reference: [8] <author> Montgomery, P. </author> <title> A block Lanczos algorithm for finding dependencies over GF(2). </title> <booktitle> In EUROCRYPT'95, </booktitle> <address> Heidelberg, Germany. </address> <publisher> Springer LNCS 921 (1995), </publisher> <pages> pp. 106-120. </pages>
Reference-contexts: It is well known that these latter methods are strongly related to each other. For a unified theory in the finite field case, the reader may refer to [7]. Typical in numerical analysis, these techniques are not yet fully analyzed over algebraic domains. Proposed and experimented as heuristics in <ref> [6, 1, 2, 8] </ref>, they lead to empirical data, which point out the lack of theoretical explanations for various problems. Attempts of probability analysis are made in [8, 7] but only for random inputs. The Wiedemann approach is the best understood technique. <p> Typical in numerical analysis, these techniques are not yet fully analyzed over algebraic domains. Proposed and experimented as heuristics in [6, 1, 2, 8], they lead to empirical data, which point out the lack of theoretical explanations for various problems. Attempts of probability analysis are made in <ref> [8, 7] </ref> but only for random inputs. The Wiedemann approach is the best understood technique. To find dependencies i.e. to solve Ax = 0 or to solve nonsingular systems Ax = b, the corresponding algorithms are rigorously proven to work with probability strictly greater than zero over any field.
Reference: [9] <author> Villard, G. </author> <title> A study of Coppersmith's block Wiedemann algorithm using matrix polynomials, </title> <month> Feb. </month> <year> 1997. </year> <institution> RR 975-I-M IMAG Grenoble, France. </institution>
Reference-contexts: To find dependencies i.e. to solve Ax = 0 or to solve nonsingular systems Ax = b, the corresponding algorithms are rigorously proven to work with probability strictly greater than zero over any field. We refer to [12] for the non block method and to <ref> [4, 9, 11] </ref> for the block method of Coppersmith [2]. Note that, using the unified approach of [7], these results may be extended to the conjugate gradient and to the Lanczos non block methods. These studies lead to the following cost analysis. <p> To ensure that the left projection is correct we use our previous studies of the algorithm in the homogeneous case <ref> [9, 11] </ref>. 2 Probabilistic analysis We first give the necessary conditions to satisfy (2). Then we precise which values of ffi l and ffi r should be chosen to characterize to required sequence length in the algorithm. We conclude with the main result giving the overall cost. <p> Then we precise which values of ffi l and ffi r should be chosen to characterize to required sequence length in the algorithm. We conclude with the main result giving the overall cost. Next two lemmata are consequences of corollary 1 in [11] or of corollary 6.5 in <ref> [9] </ref>. The first lemma can also be directly shown by classical means of linear algebra, we omit its proof. Lemma 1 Suppose A in M N (K) with rank r is similar to a block-diagonal matrix diag (B; 0) where B is invertible and has rank r as well. <p> The bound on ffi l directly comes from corollary 6.5 in <ref> [9] </ref>. 2 We can now prove the final result. This is an extension of the corollary of page 795 in [4] for general matrices. Theorem 1 Let A be an N fiN matrix over K =GF (q). Let * &gt; 0 be fixed.
Reference: [10] <author> Villard, G. </author> <title> Computing minimum generating matrix polynomials, 1997. </title> <type> Preprint IMAG Grenoble, </type> <institution> France. </institution>
Reference-contexts: The overall number of matrix-vector products is thus (1 + n=m + 2=(n 1))N , with O ((1 + n=m)N 2 log N log log N ) additional operations. A matrix generating polynomial D Z () is computed in O ((m + n)N 2 arithmetic operations <ref> [2, 4, 10] </ref>. From there G Z () is obtained at a lower cost.
Reference: [11] <author> Villard, G. </author> <title> Further analysis of Coppersmith's block Wiede-mann algorithm for the solution of sparse linear systems. </title> <booktitle> In International Symposium on Symbolic and Algebraic Computation, </booktitle> <address> Maui, Hawaii, USA (July 1997), </address> <publisher> ACM Press. </publisher>
Reference-contexts: To find dependencies i.e. to solve Ax = 0 or to solve nonsingular systems Ax = b, the corresponding algorithms are rigorously proven to work with probability strictly greater than zero over any field. We refer to [12] for the non block method and to <ref> [4, 9, 11] </ref> for the block method of Coppersmith [2]. Note that, using the unified approach of [7], these results may be extended to the conjugate gradient and to the Lanczos non block methods. These studies lead to the following cost analysis. <p> The result is established in x2, we begin in x1 with a short presentation of the algorithm. To limit the length of the presentation, we assume the reader is a little bit familiar with the papers [12, 4] and <ref> [11] </ref>. 1 Block solution of non homogeneous systems We adapt the Coppersmith's version [2] of Wiedemann's algorithm to the solution of a non homogeneous system Ax = b. We use the terminology proposed in [11]. <p> presentation, we assume the reader is a little bit familiar with the papers [12, 4] and <ref> [11] </ref>. 1 Block solution of non homogeneous systems We adapt the Coppersmith's version [2] of Wiedemann's algorithm to the solution of a non homogeneous system Ax = b. We use the terminology proposed in [11]. The algorithm picks up two random matrices X in M m;N (K) and Y in M N;n1 (K), then computes Z = [b; AY ]. <p> For two integers ffi l and ffi r to be determined in next section, one next generate the matrix sequence H i = XA Z; i = 0; : : : ; ffi l + ffi r 1: The solution is computed from a matrix generating polynomial <ref> [11] </ref> for the sequence fH i g i . <p> To ensure that the left projection is correct we use our previous studies of the algorithm in the homogeneous case <ref> [9, 11] </ref>. 2 Probabilistic analysis We first give the necessary conditions to satisfy (2). Then we precise which values of ffi l and ffi r should be chosen to characterize to required sequence length in the algorithm. We conclude with the main result giving the overall cost. <p> Then we precise which values of ffi l and ffi r should be chosen to characterize to required sequence length in the algorithm. We conclude with the main result giving the overall cost. Next two lemmata are consequences of corollary 1 in <ref> [11] </ref> or of corollary 6.5 in [9]. The first lemma can also be directly shown by classical means of linear algebra, we omit its proof. <p> Let A fl be a restriction of A to its range space. Suppose that the Frobenius form of A fl has OE fl companion blocks. Let fl be the sum of the dimensions of the first minfOE fl ; ng blocks. From <ref> [11] </ref>, with a right blocking matrix with n columns, we know that the degree of the minimal polynomial depends on fl , we deduce that: Lemma 2 Let Z = AY and Z b = [b; Z] and suppose A satisfies the conditions of lemma 1. <p> Its degree is less than ffi r = d fl =(n 1)e. It can be computed from the sequence fX A i Z b g ffi l +ffi r 1 i=0 where ffi l = dN=me. Proof. Applying proposition 2 of <ref> [11] </ref> on the left we know that the two minimal polynomials are equal. We now bound the degree of the minimal polynomial for fX A i Z b g 1 i=0 from a bound for the one for fX A i Zg 1 i=0 . From corollary 1 in [11], the <p> of <ref> [11] </ref> on the left we know that the two minimal polynomials are equal. We now bound the degree of the minimal polynomial for fX A i Z b g 1 i=0 from a bound for the one for fX A i Zg 1 i=0 . From corollary 1 in [11], the minimal polynomial for fA i Zg 1 i=0 is equal to the one for fX A i Zg 1 i=0 . <p> Next we ensure that the left projection using X does not modify the matrix generating polynomials. We use the result given by lemma 2 in the generic case and get the target probability for random X and Y using either theorem 6 in [4] or theorem 5 in <ref> [11] </ref>. The condition on m is satisfied since the above preconditioning ensures that OE fl = 1 [5]. We now detail the cost of the algorithm.
Reference: [12] <author> Wiedemann, D. </author> <title> Solving sparse linear equations over finite fields. </title> <journal> IEEE Transf. Inform. Theory 32 (1986), </journal> <pages> 54-62. </pages>
Reference-contexts: The Wiedemann approach is the best understood technique. To find dependencies i.e. to solve Ax = 0 or to solve nonsingular systems Ax = b, the corresponding algorithms are rigorously proven to work with probability strictly greater than zero over any field. We refer to <ref> [12] </ref> for the non block method and to [4, 9, 11] for the block method of Coppersmith [2]. Note that, using the unified approach of [7], these results may be extended to the conjugate gradient and to the Lanczos non block methods. These studies lead to the following cost analysis. <p> Unfortunately its full analysis was restricted to invertible matrices. Indeed, the case Ax = b with A singular is a little bit more difficult. The solution proposed by Wiedemann involves O (N log N ) matrix-vector products over any field <ref> [12] </ref>. This can be improved to O (N ) products using the randomization in [5] and to 2N products using the results in [3]. This latter paper is about the Lanczos algorithm, but its conclusions carry over to the Wiedemann one. <p> The result is established in x2, we begin in x1 with a short presentation of the algorithm. To limit the length of the presentation, we assume the reader is a little bit familiar with the papers <ref> [12, 4] </ref> and [11]. 1 Block solution of non homogeneous systems We adapt the Coppersmith's version [2] of Wiedemann's algorithm to the solution of a non homogeneous system Ax = b. We use the terminology proposed in [11]. <p> A (h (A)b + g 2 (A)Y 2 + : : : + g n (A)Y n ) : The algorithm may thus output as a solution x for the system Ax = b: The algorithm is randomized concerning two points: to satisfy (2) we rely below on results in <ref> [12, 5] </ref>. To ensure that the left projection is correct we use our previous studies of the algorithm in the homogeneous case [9, 11]. 2 Probabilistic analysis We first give the necessary conditions to satisfy (2).
References-found: 12

