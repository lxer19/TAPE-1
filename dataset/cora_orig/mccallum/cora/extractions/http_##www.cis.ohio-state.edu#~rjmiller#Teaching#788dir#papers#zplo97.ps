URL: http://www.cis.ohio-state.edu/~rjmiller/Teaching/788dir/papers/zplo97.ps
Refering-URL: http://www.cis.ohio-state.edu/~rjmiller/Teaching/788dir/projects.html
Root-URL: 
Email: fzaki,srini,wei,ogiharag@cs.rochester.edu  
Title: Evaluation of Sampling for Data Mining of Association Rules  
Author: Mohammed Javeed Zaki, Srinivasan Parthasarathy, Wei Li, Mitsunori Ogihara 
Address: Rochester, Rochester NY 14627  
Affiliation: Computer Science Department, University of  
Abstract: Discovery of association rules is a prototypical problem in data mining. The current algorithms proposed for data mining of association rules make repeated passes over the database to determine the commonly occurring itemsets (or set of items). For large databases, the I/O overhead in scanning the database can be extremely high. In this paper we show that random sampling of transactions in the database is an effective method for finding association rules. Sampling can speed up the mining process by more than an order of magnitude by reducing I/O costs and drastically shrinking the number of transaction to be considered. We may also be able to make the sampled database resident in main-memory. Furthermore, we show that sampling can accurately represent the data patterns in the database with high confidence. We experimentally evaluate the effectiveness of sampling on different databases, and study the relationship between the performance, and the accuracy and confidence of the chosen sample. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Agrawal, T. Imielinski, and A. Swami. </author> <title> Mining association rules between sets of items in large databases. </title> <booktitle> In ACM SIGMOD Intl. Conf. Management of Data, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: It combines research in machine learning, statistics and databases. In this paper we will concentrate on the discovery of association rules. The problem of mining association rules over basket data was introduced in <ref> [1] </ref>. Basket data usually consists of a record per customer with a transaction date, along with items bought by the customer. <p> The effectiveness of sampling is experimentally analyzed in section 4, and section 6 presents our conclusions. 2. Data mining for association rules We now present the formal statement of the problem of mining association rules over basket data. The discussion 1 below closely follows that in <ref> [1, 3] </ref>. Let I = fi 1 ; i 2 ; ; i m g be a set of m distinct attributes, also called items. A set of items is called an itemset, and an itemset with k items is called a k-itemset. <p> The second step consists of forming implication rules among the large itemsets [3]. In this paper we only deal with the computationally intensive first step. Many algorithms for finding large itemsets have been proposed in the literature <ref> [1, 7, 3, 10, 12, 6, 13, 2] </ref>. In this paper we will use the Apriori algorithm [2] to evaluate the effectiveness of sampling for data mining. We chose Apriori since it fast and has excellent scale-up properties. <p> The distribution of the support of the itemsets in the original database also influences the sampling quality. 5. Related Work Many algorithms for finding large itemsets have been proposed in the literature since the introduction of this problem in <ref> [1] </ref> (AIS algorithm). The Apriori algorithm [2] reduces the search space effectively, by using the property that any subset of a large itemset must itself be large.
Reference: [2] <author> R. Agrawal, H. Mannila, R. Srikant, H. Toivonen, and A. I. Verkamo. </author> <title> Fast discovery of association rules. In Advances in Knowledge Discovery and Data Mining, </title> <editor> U. Fayyad, G. Piatetsky-Shapiro, P. Smyth, R. Uthurusamy (Eds.). </editor> <publisher> AAAI Press, </publisher> <address> Melo Park, CA, </address> <year> 1996. </year>
Reference-contexts: The second step consists of forming implication rules among the large itemsets [3]. In this paper we only deal with the computationally intensive first step. Many algorithms for finding large itemsets have been proposed in the literature <ref> [1, 7, 3, 10, 12, 6, 13, 2] </ref>. In this paper we will use the Apriori algorithm [2] to evaluate the effectiveness of sampling for data mining. We chose Apriori since it fast and has excellent scale-up properties. <p> In this paper we only deal with the computationally intensive first step. Many algorithms for finding large itemsets have been proposed in the literature [1, 7, 3, 10, 12, 6, 13, 2]. In this paper we will use the Apriori algorithm <ref> [2] </ref> to evaluate the effectiveness of sampling for data mining. We chose Apriori since it fast and has excellent scale-up properties. We would like to observe that our results are about sampling, and as such independent of the mining algorithm used. 2.1. <p> Let L k denote the set of Large k-itemsets and C k the set of candidate k-itemsets. The general structure of the algorithm is given in figure 1. We refer the reader to <ref> [2] </ref> for more detail on Apriori, and its performance characteristics. We now present a simple example of how Apriori works. <p> The distribution of the support of the itemsets in the original database also influences the sampling quality. 5. Related Work Many algorithms for finding large itemsets have been proposed in the literature since the introduction of this problem in [1] (AIS algorithm). The Apriori algorithm <ref> [2] </ref> reduces the search space effectively, by using the property that any subset of a large itemset must itself be large. The DHP algorithm [12] uses a hash table in pass k to do efficient pruning of (k + 1)-itemsets to further reduce the candidate set. <p> Algorithms using only general-purpose DBMS systems and relational algebra operations have also been proposed [6, 7]. A theoretical analysis of sampling (using Chernoff bounds) for association rules was presented in <ref> [2, 10] </ref>. We look at this problem in more detail empirically, and compare theory and experimentation. In [8] the authors compare sample selection schemes for data mining. They make a claim for collecting the sample dynamically in the context of the subsequent mining algorithm to be applied.
Reference: [3] <author> R. Agrawal and R. Srikant. </author> <title> Fast algorithms for mining association rules. </title> <booktitle> In 20th VLDB Conference, </booktitle> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: The effectiveness of sampling is experimentally analyzed in section 4, and section 6 presents our conclusions. 2. Data mining for association rules We now present the formal statement of the problem of mining association rules over basket data. The discussion 1 below closely follows that in <ref> [1, 3] </ref>. Let I = fi 1 ; i 2 ; ; i m g be a set of m distinct attributes, also called items. A set of items is called an itemset, and an itemset with k items is called a k-itemset. <p> The data mining task for association rules can be broken into two steps. The first step consists of finding all large itemsets, i.e., itemsets that occur in the database with a certain user-specified frequency, called minimum support. The second step consists of forming implication rules among the large itemsets <ref> [3] </ref>. In this paper we only deal with the computationally intensive first step. Many algorithms for finding large itemsets have been proposed in the literature [1, 7, 3, 10, 12, 6, 13, 2]. <p> The second step consists of forming implication rules among the large itemsets [3]. In this paper we only deal with the computationally intensive first step. Many algorithms for finding large itemsets have been proposed in the literature <ref> [1, 7, 3, 10, 12, 6, 13, 2] </ref>. In this paper we will use the Apriori algorithm [2] to evaluate the effectiveness of sampling for data mining. We chose Apriori since it fast and has excellent scale-up properties. <p> For T 10:I4:D250K, jDj = 250000, jT j = 10, jIj = 4. For both databases the number of maximal potentially large itemsets jLj = 2000, and the number of items N = 1000. We refer the reader to <ref> [3] </ref> for more detail on the database generation. * ENROLL: This is a database of student enrollments for a particular graduating class. Each transaction consists of a student ID followed by information on the college, major, department, semester, and a list of courses taken during that semester.
Reference: [4] <author> W. G. Cochran. </author> <title> Sampling Techniques. </title> <publisher> John Wiley & Sons, </publisher> <year> 1977. </year>
Reference-contexts: The random variable X giving the number of transactions in the sample containing the itemset I, has a binomial distribution of n trials, with the probability of success t (note: the correct distribution for finite populations is the Hypergeometric distribution, although the Binomial distribution is a satisfactory approximation <ref> [4] </ref>).
Reference: [5] <author> T. Hagerup and C. Rub. </author> <title> A guided tour of chernoff bounds. </title> <booktitle> In Information Processing Letters, </booktitle> <pages> pages 305-308. </pages> <publisher> North-Holland, 1989/90. </publisher>
Reference-contexts: For any positive constant, 0 * 1, the Chernoff bounds <ref> [5] </ref> state that P (X (1 *)nt ) e * 2 nt=2 (1) Chernoff bounds provide information on how close is the actual occurrence of an itemset in the sample, as compared to the expected count in the sample.
Reference: [6] <author> M. Holsheimer, M. Kersten, H. Mannila, and H. Toivonen. </author> <title> A perspective on databases and data mining. </title> <booktitle> In 1st Intl. Conf. Knowledge Discovery and Data Mining, </booktitle> <month> Aug. </month> <year> 1995. </year>
Reference-contexts: The second step consists of forming implication rules among the large itemsets [3]. In this paper we only deal with the computationally intensive first step. Many algorithms for finding large itemsets have been proposed in the literature <ref> [1, 7, 3, 10, 12, 6, 13, 2] </ref>. In this paper we will use the Apriori algorithm [2] to evaluate the effectiveness of sampling for data mining. We chose Apriori since it fast and has excellent scale-up properties. <p> The Partition algorithm [13] minimizes I/O by scanning the database only twice. In the first pass it generates the set of all potentially large itemsets, and in the second pass their support is obtained. Algorithms using only general-purpose DBMS systems and relational algebra operations have also been proposed <ref> [6, 7] </ref>. A theoretical analysis of sampling (using Chernoff bounds) for association rules was presented in [2, 10]. We look at this problem in more detail empirically, and compare theory and experimentation. In [8] the authors compare sample selection schemes for data mining.
Reference: [7] <author> M. Houtsma and A. Swami. </author> <title> Set-oriented mining of association rules. In RJ 9567. </title> <institution> IBM Almaden, </institution> <month> Oct. </month> <year> 1993. </year>
Reference-contexts: The second step consists of forming implication rules among the large itemsets [3]. In this paper we only deal with the computationally intensive first step. Many algorithms for finding large itemsets have been proposed in the literature <ref> [1, 7, 3, 10, 12, 6, 13, 2] </ref>. In this paper we will use the Apriori algorithm [2] to evaluate the effectiveness of sampling for data mining. We chose Apriori since it fast and has excellent scale-up properties. <p> The Partition algorithm [13] minimizes I/O by scanning the database only twice. In the first pass it generates the set of all potentially large itemsets, and in the second pass their support is obtained. Algorithms using only general-purpose DBMS systems and relational algebra operations have also been proposed <ref> [6, 7] </ref>. A theoretical analysis of sampling (using Chernoff bounds) for association rules was presented in [2, 10]. We look at this problem in more detail empirically, and compare theory and experimentation. In [8] the authors compare sample selection schemes for data mining.
Reference: [8] <author> G. John and P. Langley. </author> <title> Static versus dynamic sampling for data mining. </title> <booktitle> In 2nd Intl. Conf. Knowledge Discovery and Data Mining, </booktitle> <month> Aug. </month> <year> 1996. </year>
Reference-contexts: Algorithms using only general-purpose DBMS systems and relational algebra operations have also been proposed [6, 7]. A theoretical analysis of sampling (using Chernoff bounds) for association rules was presented in [2, 10]. We look at this problem in more detail empirically, and compare theory and experimentation. In <ref> [8] </ref> the authors compare sample selection schemes for data mining. They make a claim for collecting the sample dynamically in the context of the subsequent mining algorithm to be applied. A recent paper [14] presents an association rule mining algorithm using sampling.
Reference: [9] <author> D. E. Knuth. </author> <booktitle> The Art of Computer Programming. Volume 2. Seminumerical Algorithms. </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1981. </year>
Reference-contexts: If m records have been chosen from the first t records, then the next record will be chosen with the probability (n m)=(N t). This algorithm, called Method S <ref> [9] </ref>, generates O (N ) random variates, and also runs in O (N ) time. Method A significantly speeds up the sampling process by efficiently determining the number of records to be skipped over before the next one is chosen for the sample.
Reference: [10] <author> H. Mannila, H. Toivonen, and I. Verkamo. </author> <title> Efficient algorithms for discovering association rules. </title> <note> In AAAI Wkshp. Knowledge Discovery in Databases, </note> <month> July </month> <year> 1994. </year>
Reference-contexts: The second step consists of forming implication rules among the large itemsets [3]. In this paper we only deal with the computationally intensive first step. Many algorithms for finding large itemsets have been proposed in the literature <ref> [1, 7, 3, 10, 12, 6, 13, 2] </ref>. In this paper we will use the Apriori algorithm [2] to evaluate the effectiveness of sampling for data mining. We chose Apriori since it fast and has excellent scale-up properties. <p> Algorithms using only general-purpose DBMS systems and relational algebra operations have also been proposed [6, 7]. A theoretical analysis of sampling (using Chernoff bounds) for association rules was presented in <ref> [2, 10] </ref>. We look at this problem in more detail empirically, and compare theory and experimentation. In [8] the authors compare sample selection schemes for data mining. They make a claim for collecting the sample dynamically in the context of the subsequent mining algorithm to be applied.
Reference: [11] <author> F. Olken and D. Rotem. </author> <title> Random sampling from database files a survey. </title> <booktitle> In 5th Intl. Conf. Statistical and Scientific Database Management, </booktitle> <month> Apr. </month> <year> 1990. </year>
Reference-contexts: Random sampling from databases has been successfully used in query size estimation. Such information can be used for statistical analyses of databases, where approximate answers would suffice. It may also be used to estimate selectivities or intermediate result sizes for query optimization <ref> [11] </ref>. In the context of association rules, sampling can be utilized to gather quick preliminary rules. This may help the user to direct the data mining process by refining the criterion for interesting rules.
Reference: [12] <author> J. S. Park, M. Chen, and P. S. Yu. </author> <title> An effective hash based algorithm for mining association rules. </title> <booktitle> In ACM SIGMOD Intl. Conf. Management of Data, </booktitle> <month> May </month> <year> 1995. </year>
Reference-contexts: The second step consists of forming implication rules among the large itemsets [3]. In this paper we only deal with the computationally intensive first step. Many algorithms for finding large itemsets have been proposed in the literature <ref> [1, 7, 3, 10, 12, 6, 13, 2] </ref>. In this paper we will use the Apriori algorithm [2] to evaluate the effectiveness of sampling for data mining. We chose Apriori since it fast and has excellent scale-up properties. <p> The Apriori algorithm [2] reduces the search space effectively, by using the property that any subset of a large itemset must itself be large. The DHP algorithm <ref> [12] </ref> uses a hash table in pass k to do efficient pruning of (k + 1)-itemsets to further reduce the candidate set. The Partition algorithm [13] minimizes I/O by scanning the database only twice.
Reference: [13] <author> A. Savasere, E. Omiecinski, and S. Navathe. </author> <title> An efficient algorithm for mining association rules in large databases. </title> <booktitle> In 21st VLDB Conference, </booktitle> <year> 1995. </year>
Reference-contexts: The second step consists of forming implication rules among the large itemsets [3]. In this paper we only deal with the computationally intensive first step. Many algorithms for finding large itemsets have been proposed in the literature <ref> [1, 7, 3, 10, 12, 6, 13, 2] </ref>. In this paper we will use the Apriori algorithm [2] to evaluate the effectiveness of sampling for data mining. We chose Apriori since it fast and has excellent scale-up properties. <p> The DHP algorithm [12] uses a hash table in pass k to do efficient pruning of (k + 1)-itemsets to further reduce the candidate set. The Partition algorithm <ref> [13] </ref> minimizes I/O by scanning the database only twice. In the first pass it generates the set of all potentially large itemsets, and in the second pass their support is obtained. Algorithms using only general-purpose DBMS systems and relational algebra operations have also been proposed [6, 7].
Reference: [14] <author> H. Toivonen. </author> <title> Sampling large databases for association rules. </title> <booktitle> In 22nd VLDB Conference, </booktitle> <year> 1996. </year>
Reference-contexts: We look at this problem in more detail empirically, and compare theory and experimentation. In [8] the authors compare sample selection schemes for data mining. They make a claim for collecting the sample dynamically in the context of the subsequent mining algorithm to be applied. A recent paper <ref> [14] </ref> presents an association rule mining algorithm using sampling. A sample of the database is obtained and all association rules in the sample are found. These results are then verified against the entire database. The results are thus exact and not approximations based on the sample.
Reference: [15] <author> J. S. Vitter. </author> <title> An efficient algorithm for sequential random sampling. </title> <journal> In ACM Trans. Mathematical Software, </journal> <volume> volume 13(1), </volume> <pages> pages 58-67, </pages> <month> Mar. 87. </month>
Reference-contexts: Sampling algorithm For generating samples of the database, we use the Method A Algorithm presented in <ref> [15] </ref>, which is simple and very efficient for large sample size, n. A simple algorithm for sampling generates an independent uniform random variate for each record to determine whether that record should be chosen for the sample.
References-found: 15

