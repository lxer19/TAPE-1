URL: file://ftp.cs.utexas.edu/pub/qsim/papers/Pierce+Kuipers-aij-97.ps.Z
Refering-URL: http://net.cs.utexas.edu/users/qr/robotics/abstracts-robotics.html
Root-URL: 
Title: Map Learning with Uninterpreted Sensors and Effectors  
Author: David Pierce and Benjamin Kuipers 
Keyword: spatial semantic hierarchy, map learning, cognitive maps, feature learning, abstract interfaces, action models, changes of representation.  
Note: To appear, Artificial Intelligence Journal,  
Address: Austin, Austin, TX 78712 USA  
Affiliation: Department of Computer Sciences University of Texas at  
Email: dmpierce@cs.utexas.edu, kuipers@cs.utexas.edu  
Date: 1997.  
Abstract: This paper presents a set of methods by which a learning agent can learn a sequence of increasingly abstract and powerful interfaces to control a robot whose sensorimotor apparatus and environment are initially unknown. The result of the learning is a rich hierarchical model of the robot's world (its sensorimotor apparatus and environment). The learning methods rely on generic properties of the robot's world such as almost-everywhere smooth effects of motor control signals on sensory features. At the lowest level of the hierarchy, the learning agent analyzes the effects of its motor control signals in order to define a new set of control signals, one for each of the robot's degrees of freedom. It uses a generate-and-test approach to define sensory features that capture important aspects of the environment. It uses linear regression to learn models that characterize context-dependent effects of the control signals on the learned features. It uses these models to define high-level control laws for finding and following paths defined using constraints on the learned features. The agent abstracts these control laws, which interact with the continuous environment, to a finite set of actions that implement discrete state transitions. At this point, the agent has abstracted the robot's continuous world to a finite-state world and can use existing methods to learn its structure. The learning agent's methods are evaluated on several simulated robots with different sensorimotor systems and environments. fl This work has taken place in the Qualitative Reasoning Group at the Artificial Intelligence Laboratory, The University of Texas at Austin. Research of the Qualitative Reasoning Group is supported in part by NSF grants IRI-9216584 and IRI-9504138, by NASA grants NCC 2-760 and NAG 2-994, and by the Texas Advanced Research Program under grant no. 003658-242. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Dana Angluin. </author> <title> On the complexity of minimum inference of regular sets. </title> <journal> Information and Control, </journal> <volume> 39 </volume> <pages> 337-350, </pages> <year> 1978. </year>
Reference-contexts: In the implementation, the frequency distributions use 50 subintervals uniformly distributed over the range <ref> [-1, 1] </ref>. <p> In the case that the learning agent is passively given examples of the environment's input/output behavior, it has been shown that finding the smallest automaton consistent with the behavior is NP-complete <ref> [1, 9] </ref>. With active learning, in which the agent actively chooses its actions, the problem becomes tractable. Kuipers [17] describes the TOUR model, a method for understanding discrete spatial worlds based on a theory of cognitive maps.
Reference: [2] <author> Dana Angluin. </author> <title> Learning regular sets from queries and counterexamples. </title> <journal> Information and Computation, </journal> <volume> 75 </volume> <pages> 87-106, </pages> <month> November </month> <year> 1987. </year>
Reference-contexts: Kuipers [17] describes the TOUR model, a method for understanding discrete spatial worlds based on a theory of cognitive maps. Dudek, et al. [8] generalize Kuipers and Byun's [18, 19] strategy for topological map-learning and provide algorithms for discriminating perceptually identical states. Angluin <ref> [2] </ref> gives a polynomial-time algorithm using active experimentation and passively received counterexamples. Rivest & Schapire [38] improve on Angluin's algorithm and give a version that does not require the reset operation (returning to the start state after each experiment).
Reference: [3] <author> David Chapman and Leslie Pack Kaelbling. </author> <title> Learning from delayed reinforcement in a complex domain. </title> <type> Tech. Report TR-90-11, </type> <institution> Teleos Research, Palo Alto, California, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: The network could then serve as the static action model and could be used to find base actions for path-following behaviors. Reinforcement learning. It may be possible to use reinforcement learning <ref> [3, 23, 36, 42, 45] </ref> to learn homing and path-following behaviors without the need for the primitive actions or explicit action models. An advantage of such an approach is that it does not presume that a particular model of the sensorimotor apparatus has been learned.
Reference: [4] <author> Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> MIT Press/McGraw-Hill Book Company, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: Computing the relation ~ for i and j given the relation is straightforward (e.g., <ref> [4] </ref>). An equivalence class of the relation ~, if not a singleton, is described as a group feature of s. For the example robot, the raw sensory feature has 29 elements.
Reference: [5] <author> Thomas Dean, Dana Angluin, Kenneth Basye, Sean Engelson, Leslie Kaelbling, Evangelos Kokkevis, and Oded Maron. </author> <title> Inferring finite automata with stochastic output functions and an application to map learning. </title> <booktitle> In Proceedings, Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 208-214, </pages> <address> San Jose, CA, July 1992. </address> <publisher> AAAI Press/MIT Press. </publisher>
Reference-contexts: Angluin [2] gives a polynomial-time algorithm using active experimentation and passively received counterexamples. Rivest & Schapire [38] improve on Angluin's algorithm and give a version that does not require the reset operation (returning to the start state after each experiment). Dean et al. <ref> [5] </ref> have extended Rivest and Schapire's theory to handle stochastic FSA's. They assume that actions are deterministic but that the output function mapping states to senses is probabilistic. The key to their method is "going in circles" until the uncertainty washes out.
Reference: [6] <author> Thomas Dean, Kenneth Basye, and Leslie Kaelbling. </author> <title> Uncertainty in graph-based map learning. </title> <editor> In Jonathan H. Connell and Sridhar Mahadevan, editors, </editor> <booktitle> Robot Learning, </booktitle> <pages> pages 171-192. </pages> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, </address> <year> 1993. </year>
Reference-contexts: Dean et al. [5] have extended Rivest and Schapire's theory to handle stochastic FSA's. They assume that actions are deterministic but that the output function mapping states to senses is probabilistic. The key to their method is "going in circles" until the uncertainty washes out. Dean, Basye, and Kaelbling <ref> [6] </ref> give a good review of learning techniques for a variety of stochastic automata. Drescher's schema mechanism [7] employs a statistical learning method called marginal attribution. Schemas emphasize sensory effects of actions rather than state transitions and are ideal for representing partial knowledge in stochastic worlds.
Reference: [7] <author> Gary L. Drescher. </author> <title> Made-Up Minds: A Constructivist Approach to Artificial Intelligence. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1991. </year>
Reference-contexts: that x is a good predictor for the effect of u j on y i , a correlator can be used to determine u j 's effect on y i for each context defined by the predicate z ij = k. 14 This approach is analogous to Drescher's marginal attribution <ref> [7] </ref>. 27 Testing each of a large set of features to see if they improve the predictability of a control signal's effect is expensive. Heuristics can be used to guide the search for relevant features to use in defining contexts. <p> The key to their method is "going in circles" until the uncertainty washes out. Dean, Basye, and Kaelbling [6] give a good review of learning techniques for a variety of stochastic automata. Drescher's schema mechanism <ref> [7] </ref> employs a statistical learning method called marginal attribution. Schemas emphasize sensory effects of actions rather than state transitions and are ideal for representing partial knowledge in stochastic worlds. Wei-Min Shen's LIVE system [40] learns the structure of a finite-state environment from experience (and experimentation) within it.
Reference: [8] <author> Gregory Dudek, Michael Jenkin, Evangelos Milios, and David Wilkes. </author> <title> Robotic exploration as graph construction. </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> 7(6) </volume> <pages> 859-865, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: With active learning, in which the agent actively chooses its actions, the problem becomes tractable. Kuipers [17] describes the TOUR model, a method for understanding discrete spatial worlds based on a theory of cognitive maps. Dudek, et al. <ref> [8] </ref> generalize Kuipers and Byun's [18, 19] strategy for topological map-learning and provide algorithms for discriminating perceptually identical states. Angluin [2] gives a polynomial-time algorithm using active experimentation and passively received counterexamples.
Reference: [9] <author> E. Mark Gold. </author> <title> Complexity of automaton identification from given data. </title> <journal> Information and Control, </journal> <volume> 37 </volume> <pages> 302-320, </pages> <year> 1978. </year>
Reference-contexts: In the case that the learning agent is passively given examples of the environment's input/output behavior, it has been shown that finding the smallest automaton consistent with the behavior is NP-complete <ref> [1, 9] </ref>. With active learning, in which the agent actively chooses its actions, the problem becomes tractable. Kuipers [17] describes the TOUR model, a method for understanding discrete spatial worlds based on a theory of cognitive maps.
Reference: [10] <author> Kazuo Hiraki. </author> <title> Abstraction of sensory-motor features. </title> <booktitle> In Proceedings of the Sixteenth Annual Conference of the Cognitive Science Society, </booktitle> <address> Hillsdale, NJ, 1994. </address> <publisher> Lawrence Erlbaum Associates. </publisher> <pages> 57 </pages>
Reference-contexts: When the environment is only partially observable, LIVE uses locally distinguishing experiments to test the hypothesized properties of unobserved state variables. A primary focus of the work of Shen and other constructive inductionists <ref> [10, 28, 39, 41] </ref> is the learning of new features. At this level of description, our approach and Shen's are similar. However, in terms of the actual methods used and the domains of applicability, the two approaches are very different and are in fact complementary.
Reference: [11] <author> B. K. P. Horn. </author> <title> Robot Vision. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: This motion results in a change in intensity. A point in the image with a large gradient will, in the presence of motion, also have a large temporal derivative. This is an informal motivation for the optical flow constraint equation <ref> [11] </ref>, which defines the optical flow at a point in an image to have magnitude E t =k ~ E p k and direction ~ E p : v = k ~ E p k k ~ E p k E t ~ E p Here, k ~ E p k
Reference: [12] <author> Michael I. Jordan and David E. Rumelhart. </author> <title> Forward models: Supervised learning with a distal teacher. </title> <journal> Cognitive Science, </journal> <volume> 16 </volume> <pages> 307-354, </pages> <year> 1992. </year>
Reference-contexts: Another approach would be to use a neural network <ref> [12] </ref> to learn to predict the context-dependent effects of arbitrary actions. The network could then serve as the static action model and could be used to find base actions for path-following behaviors. Reinforcement learning.
Reference: [13] <author> Teuvo Kohonen. </author> <title> Self-Organization and Associative Memory. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <note> second edition, </note> <year> 1988. </year>
Reference-contexts: This type of decomposition may be performed using principal component analysis (PCA). (See Mardia et al. [25] for an introduction. Oja [29] discusses how a neural network can function as a Principal Component Analyzer. Ritter et al. [37] show that self-organizing maps <ref> [13] </ref> can be seen as a generalization of PCA.) Principal component analysis of a set of values for a variable y produces a set of orthogonal unit vectors fv i g, called eigenvectors, that may be viewed as a basis set for the variable y.
Reference: [14] <author> David Kortenkamp and Terry Weymouth. </author> <title> Topological mapping for mobile robots using a combination of sonar and vision sensing. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI-94), </booktitle> <year> 1994. </year>
Reference-contexts: NX's distinctive places correspond to discrete states and its local control strategies correspond to state transitions. These constructs have to be manually redesigned in order to apply to a robot with a different sensorimotor apparatus. Mataric [26, 27] and Kortenkamp & Weymouth <ref> [14] </ref> have engineered similar solutions on physical robots. Lin and Hanson [24] use a physical robot, called Ratbot, with 16 sonar sensors and 16 infrared sensors to demonstrate learning of a topological map of locally distinctive places.
Reference: [15] <author> W. J. Krzanowski. </author> <title> Principles of Multivariate Analysis: A User's Perspective. </title> <booktitle> Oxford Statistical Science Series. </booktitle> <publisher> Clarendon Press, Oxford, </publisher> <year> 1988. </year>
Reference-contexts: For example, we use a fairly sophisticated method called principal component analysis <ref> [15] </ref> as a feature identification method. <p> Thus, to satisfy the constraints, n position vectors of dimension n 1 are required. Solving for the position vectors given the distance constraints can be done using a technique called metric scaling <ref> [15] </ref>. 7 The problem remains that n points of dimension n 1 are inconvenient to use, if not meaningless, for large n. In general, sensory arrays are 1-, 2-, or 3-dimensional objects. <p> The method for learning path-following behaviors would be improved if the static action model could predict the effects of arbitrary motor control vectors, not just the primitive actions. With a more comprehensive static action model, more path-following behaviors could be defined. For example, 24 See, for example, <ref> [15, p. 415] </ref>. 45 in the circular room, a path-following behavior could be based on a motor control vector with a large advancing component and a small turning component.
Reference: [16] <author> B. J. Kuipers. </author> <title> An ontological hierarchy for spatial knowledge. </title> <booktitle> In Proc. 10th Int. Workshop on Qualitative Reasoning About Physical Systems, </booktitle> <address> Fallen Leaf Lake, California, USA, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: Instead of trying to make predictions based on the raw sense vector, it needs to learn high-level features and behaviors. Understanding the world thus requires a hierarchy of features, behaviors, and accompanying descriptions. The hierarchy that the learning agent uses is called the spatial semantic hierarchy <ref> [18, 19, 20, 16] </ref>. 1.1 The spatial semantic hierarchy The spatial semantic hierarchy (SSH) is a hierarchical structure for a substantial body of com-monsense knowledge, showing how a cognitive map can be built on sensorimotor interaction with the world.
Reference: [17] <author> Benjamin J. Kuipers. </author> <title> Modeling spatial knowledge. </title> <journal> Cognitive Science, </journal> <volume> 2 </volume> <pages> 129-153, </pages> <year> 1978. </year>
Reference-contexts: In the case that the learning agent is passively given examples of the environment's input/output behavior, it has been shown that finding the smallest automaton consistent with the behavior is NP-complete [1, 9]. With active learning, in which the agent actively chooses its actions, the problem becomes tractable. Kuipers <ref> [17] </ref> describes the TOUR model, a method for understanding discrete spatial worlds based on a theory of cognitive maps. Dudek, et al. [8] generalize Kuipers and Byun's [18, 19] strategy for topological map-learning and provide algorithms for discriminating perceptually identical states.
Reference: [18] <author> Benjamin J. Kuipers and Yung-Tai Byun. </author> <title> A robust, qualitative method for robot spatial learning. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence (AAAI-88), </booktitle> <pages> pages 774-779, </pages> <year> 1988. </year>
Reference-contexts: Instead of trying to make predictions based on the raw sense vector, it needs to learn high-level features and behaviors. Understanding the world thus requires a hierarchy of features, behaviors, and accompanying descriptions. The hierarchy that the learning agent uses is called the spatial semantic hierarchy <ref> [18, 19, 20, 16] </ref>. 1.1 The spatial semantic hierarchy The spatial semantic hierarchy (SSH) is a hierarchical structure for a substantial body of com-monsense knowledge, showing how a cognitive map can be built on sensorimotor interaction with the world. <p> The abstract interface gives the current view and the set of currently applicable actions. The contribution of this paper is a set of methods for learning these first three levels. This paper's work is complementary to the work done by Kuipers and Byun <ref> [18, 19] </ref> in which all levels of the descriptive ontology were engineered by hand, and the focus of the learning agent was on learning the structure of the environment. <p> With active learning, in which the agent actively chooses its actions, the problem becomes tractable. Kuipers [17] describes the TOUR model, a method for understanding discrete spatial worlds based on a theory of cognitive maps. Dudek, et al. [8] generalize Kuipers and Byun's <ref> [18, 19] </ref> strategy for topological map-learning and provide algorithms for discriminating perceptually identical states. Angluin [2] gives a polynomial-time algorithm using active experimentation and passively received counterexamples. <p> Kuipers and Byun <ref> [18, 19] </ref> demonstrate an engineered solution to the continuous-to-discrete abstraction problem for the NX robot. NX's distinctive places correspond to discrete states and its local control strategies correspond to state transitions. These constructs have to be manually redesigned in order to apply to a robot with a different sensorimotor apparatus.
Reference: [19] <author> Benjamin J. Kuipers and Yung-Tai Byun. </author> <title> A robot exploration and mapping strategy based on a semantic hierarchy of spatial representations. </title> <journal> Journal of Robotics and Autonomous Systems, </journal> <volume> 8 </volume> <pages> 47-63, </pages> <year> 1991. </year>
Reference-contexts: Instead of trying to make predictions based on the raw sense vector, it needs to learn high-level features and behaviors. Understanding the world thus requires a hierarchy of features, behaviors, and accompanying descriptions. The hierarchy that the learning agent uses is called the spatial semantic hierarchy <ref> [18, 19, 20, 16] </ref>. 1.1 The spatial semantic hierarchy The spatial semantic hierarchy (SSH) is a hierarchical structure for a substantial body of com-monsense knowledge, showing how a cognitive map can be built on sensorimotor interaction with the world. <p> The abstract interface gives the current view and the set of currently applicable actions. The contribution of this paper is a set of methods for learning these first three levels. This paper's work is complementary to the work done by Kuipers and Byun <ref> [18, 19] </ref> in which all levels of the descriptive ontology were engineered by hand, and the focus of the learning agent was on learning the structure of the environment. <p> With active learning, in which the agent actively chooses its actions, the problem becomes tractable. Kuipers [17] describes the TOUR model, a method for understanding discrete spatial worlds based on a theory of cognitive maps. Dudek, et al. [8] generalize Kuipers and Byun's <ref> [18, 19] </ref> strategy for topological map-learning and provide algorithms for discriminating perceptually identical states. Angluin [2] gives a polynomial-time algorithm using active experimentation and passively received counterexamples. <p> Kuipers and Byun <ref> [18, 19] </ref> demonstrate an engineered solution to the continuous-to-discrete abstraction problem for the NX robot. NX's distinctive places correspond to discrete states and its local control strategies correspond to state transitions. These constructs have to be manually redesigned in order to apply to a robot with a different sensorimotor apparatus.
Reference: [20] <author> Benjamin J. Kuipers and Tod S. Levitt. </author> <title> Navigation and mapping in large-scale space. </title> <journal> AI Magazine, </journal> <volume> 9(2) </volume> <pages> 25-43, </pages> <year> 1988. </year>
Reference-contexts: Instead of trying to make predictions based on the raw sense vector, it needs to learn high-level features and behaviors. Understanding the world thus requires a hierarchy of features, behaviors, and accompanying descriptions. The hierarchy that the learning agent uses is called the spatial semantic hierarchy <ref> [18, 19, 20, 16] </ref>. 1.1 The spatial semantic hierarchy The spatial semantic hierarchy (SSH) is a hierarchical structure for a substantial body of com-monsense knowledge, showing how a cognitive map can be built on sensorimotor interaction with the world.
Reference: [21] <author> Benjamin C. Kuo. </author> <title> Automatic Control Systems. </title> <publisher> Prentice-Hall, Inc., </publisher> <address> Englewood Cliffs, N.J., 4 edition, </address> <year> 1982. </year>
Reference-contexts: The output is given by a proportional-integral (PI) control law with parameters = 1:0, ! = 0:05 (see <ref> [21] </ref>) that minimizes the difference between y i and y fl i . The behavior is done when this difference is close to zero. <p> The equation z = k defines a context in which u fi maintains y constant according to the static action model. The values of m ijkffin and r ijkffin are taken from the dynamic action model. Simple PI and PD (proportional-derivative) controllers are used (see <ref> [21] </ref>) depending on whether the primary effect of u ffi is on _y i or y i , respectively.
Reference: [22] <author> Douglas B. Lenat. </author> <title> On automated scientific theory formation: A case study using the AM program. </title> <editor> In J. E. Hayes, D. Michie, and L. I. Mikulich, editors, </editor> <booktitle> Machine Intelligence 9, </booktitle> <pages> pages 251-286. </pages> <publisher> Halsted Press, </publisher> <address> New York, </address> <year> 1977. </year>
Reference-contexts: First, it shows how a heterogeneous set of learning methods can be used to construct a deep hierarchy of sensory features and control laws. Only a very few previous learning methods such as AM <ref> [22] </ref> (see also [39]) have constructed similarly deep concept hierarchies. Second, the knowledge contained in this hierarchy shows how a foundational domain of symbolic commonsense knowledge can be grounded in continuous sensorimotor interaction with a continuous world. 3 We did not always follow this restriction in the implementation itself. <p> An interesting open problem is to define a general set of feature generators appropriate to learning mobile robots, analogous to the small and general set of functional transformations used by Shen [39] to replicate the performance of AM <ref> [22] </ref>.
Reference: [23] <author> Long-Ji Lin. </author> <title> Reinforcement Learning for Robots Using Neural Networks. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <year> 1993. </year>
Reference-contexts: The network could then serve as the static action model and could be used to find base actions for path-following behaviors. Reinforcement learning. It may be possible to use reinforcement learning <ref> [3, 23, 36, 42, 45] </ref> to learn homing and path-following behaviors without the need for the primitive actions or explicit action models. An advantage of such an approach is that it does not presume that a particular model of the sensorimotor apparatus has been learned. <p> It would be interesting to combine our approach with that used by Lin & Hanson's Ratbot to produce a learning method that uses neither domain-dependent knowledge nor a knowledge of control theory. 25 The reinforcement-learning algorithm is a neural-network version of Q learning <ref> [43, 23] </ref>. 26 In earlier work we explored the use of reinforcement learning to learn homing behaviors [33]. 50 The error signals would be defined as for our learning agent and a neural-net version of Q learning would be used to learn the local control strategies based on those error signals.
Reference: [24] <author> Long-Ji Lin and Stephen Jose Hanson. </author> <title> On-line learning for indoor navigation: Preliminary results with RatBot. In NIPS93 Robot Learning Workshop, </title> <year> 1993. </year>
Reference-contexts: While it would be possible to use reinforcement-learning methods to learn a homing behavior given an error signal <ref> [33, 24] </ref>, most of the relevant learning has already been done. The homing behavior can be defined as an instance of the generic, domain-independent control law in Figure 13, drawing on the knowledge in the static action model. <p> These constructs have to be manually redesigned in order to apply to a robot with a different sensorimotor apparatus. Mataric [26, 27] and Kortenkamp & Weymouth [14] have engineered similar solutions on physical robots. Lin and Hanson <ref> [24] </ref> use a physical robot, called Ratbot, with 16 sonar sensors and 16 infrared sensors to demonstrate learning of a topological map of locally distinctive places.
Reference: [25] <author> K. V. Mardia, J. T. Kent, and J. M. Bibby. </author> <title> Multivariate Analysis. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1979. </year>
Reference-contexts: Each line segment represents the position, direction, and magnitude of one of these average local motion vectors. by linear combination. This type of decomposition may be performed using principal component analysis (PCA). (See Mardia et al. <ref> [25] </ref> for an introduction. Oja [29] discusses how a neural network can function as a Principal Component Analyzer.
Reference: [26] <editor> Maja J. Mataric. </editor> <title> Navigating with a rat brain: A neurobiologically-inspired model for robot spatial representation. </title> <editor> In J.-A. Meyer and S. W. Wilson, editors, </editor> <booktitle> From Animals to Animats: Proceedings of The First International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> pages 169-175, </pages> <address> Cambridge, MA, 1991. </address> <publisher> MIT Press/Bradford Books. </publisher>
Reference-contexts: NX's distinctive places correspond to discrete states and its local control strategies correspond to state transitions. These constructs have to be manually redesigned in order to apply to a robot with a different sensorimotor apparatus. Mataric <ref> [26, 27] </ref> and Kortenkamp & Weymouth [14] have engineered similar solutions on physical robots. Lin and Hanson [24] use a physical robot, called Ratbot, with 16 sonar sensors and 16 infrared sensors to demonstrate learning of a topological map of locally distinctive places.
Reference: [27] <editor> Maja J. Mataric. </editor> <title> Integration of representation into goal-driven behavior-based robots. </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> 8(3) </volume> <pages> 304-312, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: NX's distinctive places correspond to discrete states and its local control strategies correspond to state transitions. These constructs have to be manually redesigned in order to apply to a robot with a different sensorimotor apparatus. Mataric <ref> [26, 27] </ref> and Kortenkamp & Weymouth [14] have engineered similar solutions on physical robots. Lin and Hanson [24] use a physical robot, called Ratbot, with 16 sonar sensors and 16 infrared sensors to demonstrate learning of a topological map of locally distinctive places.
Reference: [28] <author> Christopher J. Matheus. </author> <title> The need for constructive induction. </title> <editor> In Lawrence A. Birnbaum and Gregg C. Collins, editors, </editor> <booktitle> Machine Learning: Proceedings of the Eighth International Workshop (ML91), </booktitle> <pages> pages 173-177, </pages> <address> San Mateo, CA, June 1991. </address> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference-contexts: When the environment is only partially observable, LIVE uses locally distinguishing experiments to test the hypothesized properties of unobserved state variables. A primary focus of the work of Shen and other constructive inductionists <ref> [10, 28, 39, 41] </ref> is the learning of new features. At this level of description, our approach and Shen's are similar. However, in terms of the actual methods used and the domains of applicability, the two approaches are very different and are in fact complementary.
Reference: [29] <author> Erkki Oja. </author> <title> A simplified neuron model as a principal component analyzer. </title> <journal> Journal of Mathematical Biology, </journal> <volume> 15 </volume> <pages> 267-273, </pages> <year> 1982. </year>
Reference-contexts: For example, we use a fairly sophisticated method called principal component analysis [15] as a feature identification method. However, principal component analysis may be implemented as a neural network <ref> [29] </ref>. 4 Real sonar sensors may not satisfy this requirement due to specular reflection, a property of sonar sensors that makes them difficult to use, even in systems that are engineered by hand. 6 2 Learning a model of the sensory apparatus The learning agent's first step is to learn a <p> Each line segment represents the position, direction, and magnitude of one of these average local motion vectors. by linear combination. This type of decomposition may be performed using principal component analysis (PCA). (See Mardia et al. [25] for an introduction. Oja <ref> [29] </ref> discusses how a neural network can function as a Principal Component Analyzer.
Reference: [30] <author> David Pierce. </author> <title> Learning a set of primitive actions with an uninterpreted sensorimotor apparatus. </title> <editor> In Lawrence A. Birnbaum and Gregg C. Collins, editors, </editor> <booktitle> Machine Learning: Proceedings of the Eighth International Workshop (ML91), </booktitle> <pages> pages 338-342, </pages> <address> San Mateo, CA, June 1991. </address> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference-contexts: The target behaviors (e.g., corridor following) are specified by a human teacher. For example, when learning the corridor-following behavior, the robot is rewarded when it moves along the corridor without running into obstacles. Our approach <ref> [30, 31, 32, 34] </ref> is complementary to that of Lin and Hanson. They specify the desirable behaviors by defining appropriate reward signals and then letting the robot learn on its own how to gain the rewards.
Reference: [31] <author> David Pierce. </author> <title> Learning turn and travel actions with an uninterpreted sensorimotor apparatus. </title> <booktitle> In Proceedings IEEE International Conference on Robotics and Automation, </booktitle> <pages> pages 246-251, </pages> <address> Los Alamitos, CA, April 1991. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: The details of that experiment are given in <ref> [31] </ref>. The synchro-drive and tank-style robots demonstrate two different motor apparatuses with identical capabilities. <p> The target behaviors (e.g., corridor following) are specified by a human teacher. For example, when learning the corridor-following behavior, the robot is rewarded when it moves along the corridor without running into obstacles. Our approach <ref> [30, 31, 32, 34] </ref> is complementary to that of Lin and Hanson. They specify the desirable behaviors by defining appropriate reward signals and then letting the robot learn on its own how to gain the rewards.
Reference: [32] <author> David Pierce. </author> <title> Map Learning with Uninterpreted Sensors and Effectors. </title> <type> PhD thesis, </type> <institution> University of Texas at Austin, </institution> <month> May </month> <year> 1995. </year> <note> (http: //ftp.cs.utexas.edu /pub /qsim /papers /Pierce-PhD-95.ps.Z). </note>
Reference-contexts: The target behaviors (e.g., corridor following) are specified by a human teacher. For example, when learning the corridor-following behavior, the robot is rewarded when it moves along the corridor without running into obstacles. Our approach <ref> [30, 31, 32, 34] </ref> is complementary to that of Lin and Hanson. They specify the desirable behaviors by defining appropriate reward signals and then letting the robot learn on its own how to gain the rewards.
Reference: [33] <author> David Pierce and Benjamin Kuipers. </author> <title> Learning hill-climbing functions as a strategy for generating behaviors in a mobile robot. </title> <editor> In J.-A. Meyer and S. W. Wilson, editors, </editor> <booktitle> From Animals to Animats: Proceedings of The First International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> pages 327-336, </pages> <address> Cambridge, MA, </address> <year> 1991. </year> <institution> MIT Press/Bradford Books. Also University of Texas at Austin, AI Laboratory TR AI91-137. </institution>
Reference-contexts: While it would be possible to use reinforcement-learning methods to learn a homing behavior given an error signal <ref> [33, 24] </ref>, most of the relevant learning has already been done. The homing behavior can be defined as an instance of the generic, domain-independent control law in Figure 13, drawing on the knowledge in the static action model. <p> used by Lin & Hanson's Ratbot to produce a learning method that uses neither domain-dependent knowledge nor a knowledge of control theory. 25 The reinforcement-learning algorithm is a neural-network version of Q learning [43, 23]. 26 In earlier work we explored the use of reinforcement learning to learn homing behaviors <ref> [33] </ref>. 50 The error signals would be defined as for our learning agent and a neural-net version of Q learning would be used to learn the local control strategies based on those error signals. The control laws would be implemented as mappings from sensory inputs to motor control signals.
Reference: [34] <author> David Pierce and Benjamin Kuipers. </author> <title> Learning to explore and build maps. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence (AAAI-94), </booktitle> <address> Cambridge, MA, 1994. </address> <publisher> AAAI/MIT Press. </publisher>
Reference-contexts: The target behaviors (e.g., corridor following) are specified by a human teacher. For example, when learning the corridor-following behavior, the robot is rewarded when it moves along the corridor without running into obstacles. Our approach <ref> [30, 31, 32, 34] </ref> is complementary to that of Lin and Hanson. They specify the desirable behaviors by defining appropriate reward signals and then letting the robot learn on its own how to gain the rewards.
Reference: [35] <author> William H. Press, Saul A. Teukolsky, William T. Vetterling, and Brian P. Flannery. </author> <title> Numerical Recipes in C. </title> <publisher> Cambridge University Press, </publisher> <year> 1988. </year>
Reference-contexts: In practice, y may be approximated as a linear combination of the first few eigenvectors while throwing the remaining ones away. 10 Principal component analysis may be performed using a technique called singular value decomposition <ref> [35] </ref>, which identifies the eigenvectors and computes the standard deviation of each principal component. The relative magnitudes of the standard deviations tell how important each eigenvector is for the purposes of approximating the sample values for y.
Reference: [36] <author> Mark Ring. </author> <title> Continual Learning in Reinforcement Environments. </title> <type> PhD thesis, </type> <institution> University of Texas at Austin, </institution> <month> August </month> <year> 1994. </year>
Reference-contexts: The network could then serve as the static action model and could be used to find base actions for path-following behaviors. Reinforcement learning. It may be possible to use reinforcement learning <ref> [3, 23, 36, 42, 45] </ref> to learn homing and path-following behaviors without the need for the primitive actions or explicit action models. An advantage of such an approach is that it does not presume that a particular model of the sensorimotor apparatus has been learned.
Reference: [37] <author> H. J. Ritter, T. Martinez, and K. J. Schulten. </author> <title> Neural Computation and Self-Organizing Maps: An Introduction. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1992. </year> <month> 59 </month>
Reference-contexts: This type of decomposition may be performed using principal component analysis (PCA). (See Mardia et al. [25] for an introduction. Oja [29] discusses how a neural network can function as a Principal Component Analyzer. Ritter et al. <ref> [37] </ref> show that self-organizing maps [13] can be seen as a generalization of PCA.) Principal component analysis of a set of values for a variable y produces a set of orthogonal unit vectors fv i g, called eigenvectors, that may be viewed as a basis set for the variable y.
Reference: [38] <author> Ronald L. Rivest and Robert E. Schapire. </author> <title> Inference of finite automata using homing sequences. </title> <journal> Information and Computation, </journal> <volume> 103(2) </volume> <pages> 299-347, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: Dudek, et al. [8] generalize Kuipers and Byun's [18, 19] strategy for topological map-learning and provide algorithms for discriminating perceptually identical states. Angluin [2] gives a polynomial-time algorithm using active experimentation and passively received counterexamples. Rivest & Schapire <ref> [38] </ref> improve on Angluin's algorithm and give a version that does not require the reset operation (returning to the start state after each experiment). Dean et al. [5] have extended Rivest and Schapire's theory to handle stochastic FSA's.
Reference: [39] <author> Wei-Min Shen. </author> <title> Functional transformations in AI discovery systems. </title> <journal> Artificial Intelligence, </journal> <volume> 41(3) </volume> <pages> 257-272, </pages> <year> 1990. </year>
Reference-contexts: First, it shows how a heterogeneous set of learning methods can be used to construct a deep hierarchy of sensory features and control laws. Only a very few previous learning methods such as AM [22] (see also <ref> [39] </ref>) have constructed similarly deep concept hierarchies. Second, the knowledge contained in this hierarchy shows how a foundational domain of symbolic commonsense knowledge can be grounded in continuous sensorimotor interaction with a continuous world. 3 We did not always follow this restriction in the implementation itself. <p> Our feature generators are essentially a special case of the functional transformations of <ref> [39] </ref>. These feature generators are appropriate for a robot with a rich sensorimotor apparatus and are, as we will demonstrate, sufficient for a particular set of environments and sensorimotor apparatuses. <p> An interesting open problem is to define a general set of feature generators appropriate to learning mobile robots, analogous to the small and general set of functional transformations used by Shen <ref> [39] </ref> to replicate the performance of AM [22]. <p> When the environment is only partially observable, LIVE uses locally distinguishing experiments to test the hypothesized properties of unobserved state variables. A primary focus of the work of Shen and other constructive inductionists <ref> [10, 28, 39, 41] </ref> is the learning of new features. At this level of description, our approach and Shen's are similar. However, in terms of the actual methods used and the domains of applicability, the two approaches are very different and are in fact complementary.
Reference: [40] <author> Wei-Min Shen. </author> <title> Autonomous Learning from the Environment. </title> <editor> W. H. </editor> <publisher> Freeman and Company, </publisher> <year> 1994. </year>
Reference-contexts: Drescher's schema mechanism [7] employs a statistical learning method called marginal attribution. Schemas emphasize sensory effects of actions rather than state transitions and are ideal for representing partial knowledge in stochastic worlds. Wei-Min Shen's LIVE system <ref> [40] </ref> learns the structure of a finite-state environment from experience (and experimentation) within it. His complementary discrimination learning algorithm exploits observed counterexamples to a hypothesized concept definition to refine the boundary between positive and negative examples of the concept.
Reference: [41] <author> Wei-Min Shen and Herbert A. Simon. </author> <title> Rule creation and rule learning through environmental exploration. </title> <booktitle> In Proceedings IJCAI-89, </booktitle> <pages> pages 675-680, </pages> <year> 1989. </year>
Reference-contexts: When the environment is only partially observable, LIVE uses locally distinguishing experiments to test the hypothesized properties of unobserved state variables. A primary focus of the work of Shen and other constructive inductionists <ref> [10, 28, 39, 41] </ref> is the learning of new features. At this level of description, our approach and Shen's are similar. However, in terms of the actual methods used and the domains of applicability, the two approaches are very different and are in fact complementary.
Reference: [42] <author> R. S. Sutton. </author> <title> Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <editor> In B. W. Porter and R. J. Mooney, editors, </editor> <booktitle> Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <pages> pages 216-224. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1990. </year>
Reference-contexts: The network could then serve as the static action model and could be used to find base actions for path-following behaviors. Reinforcement learning. It may be possible to use reinforcement learning <ref> [3, 23, 36, 42, 45] </ref> to learn homing and path-following behaviors without the need for the primitive actions or explicit action models. An advantage of such an approach is that it does not presume that a particular model of the sensorimotor apparatus has been learned.
Reference: [43] <author> C.J.C.H. Watkins. </author> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> King's College, </institution> <address> Cambridge, </address> <year> 1989. </year>
Reference-contexts: It would be interesting to combine our approach with that used by Lin & Hanson's Ratbot to produce a learning method that uses neither domain-dependent knowledge nor a knowledge of control theory. 25 The reinforcement-learning algorithm is a neural-network version of Q learning <ref> [43, 23] </ref>. 26 In earlier work we explored the use of reinforcement learning to learn homing behaviors [33]. 50 The error signals would be defined as for our learning agent and a neural-net version of Q learning would be used to learn the local control strategies based on those error signals.
Reference: [44] <author> Steven Whitehead, Jonas Karlsson, and Josh Tenenberg. </author> <title> Learning multiple goal behavior via task decomposition and dynamic policy merging. </title> <editor> In Jonathan H. Connell and Sridhar Mahadevan, editors, </editor> <booktitle> Robot Learning, </booktitle> <pages> pages 45-78. </pages> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, </address> <year> 1993. </year>
Reference-contexts: An advantage of such an approach is that it does not presume that a particular model of the sensorimotor apparatus has been learned. A disadvantage is that it is difficult to train more than one behavior at a time <ref> [44] </ref> whereas it is possible to learn action models for a large number of features simultaneously. Learning composite primitive actions. Consider a robot that is capable of rotating and advancing and that has a ring of distance sensors that is always oriented in the same direction.
Reference: [45] <author> Ronald J. Williams. </author> <title> Reinforcement-learning connectionist systems. </title> <type> Technical Report NU-CCS-87-3, </type> <institution> College of Computer Science, Northeastern University, </institution> <month> February </month> <year> 1987. </year> <month> 60 </month>
Reference-contexts: The network could then serve as the static action model and could be used to find base actions for path-following behaviors. Reinforcement learning. It may be possible to use reinforcement learning <ref> [3, 23, 36, 42, 45] </ref> to learn homing and path-following behaviors without the need for the primitive actions or explicit action models. An advantage of such an approach is that it does not presume that a particular model of the sensorimotor apparatus has been learned.
References-found: 45

