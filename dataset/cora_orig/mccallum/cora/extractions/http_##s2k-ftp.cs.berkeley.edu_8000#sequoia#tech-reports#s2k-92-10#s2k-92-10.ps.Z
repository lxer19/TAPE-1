URL: http://s2k-ftp.cs.berkeley.edu:8000/sequoia/tech-reports/s2k-92-10/s2k-92-10.ps.Z
Refering-URL: http://s2k-ftp.cs.berkeley.edu:8000/sequoia/tech-reports/s2k-92-10/
Root-URL: http://www.cs.berkeley.edu
Email: grefen@cs.pitt.edu marti@cs.berkeley.edu  
Title: A Method for Refining Automatically-Discovered Lexical Relations:  
Address: 210 MIB 571 Evans Hall  Pittsburg, PA 15232 Berkeley,CA 94720  
Affiliation: Department of Computer Science Computer Science Division  University of Pittsburgh University of California, Berkeley  
Abstract: Combining Weak Techniques for Stronger Results Abstract Knowledge-poor corpus-based approaches to natural language processing are attractive in that they do not incur the difficulties associated with complex knowledge bases and real-world inferences. However, these kinds of language processing techniques in isolation often do not suffice for a particular task; for this reason we are interested in finding ways to combine various techniques and improve their results. Accordingly, we conducted experiments to refine the results of an automatic lexical discovery technique by making use of a statistically-based syntactic similarity measure. The discovery program uses lexico-syntactic patterns to find instances of the hyponymy relation in large text bases. Once relations of this sort are found, they should be inserted into an existing lexicon or thesaurus. However, the terms in the relation may have multiple senses, thus hampering automatic placement. In order to address this problem we tried to make a term-similarity determination technique choose where, in an existing thesaurus, to install a lexical relation. The union of these two corpus-based methods is promising, although only partially successful in the experiments run so far. Here we report some preliminary results, and make suggestions for how to improve the technique in future. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Brent, M. R. </author> <year> (1991). </year> <title> Automatic acquisition of subcategorization frames from untagged, free-text corpora. </title> <booktitle> In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics. </booktitle>
Reference-contexts: The union of these two corpus-based methods is promising, although only partially successful in the experiments run so far. These ideas are related to other recent work in several ways. We make use of restricted syntactic information as do Brent's <ref> (Brent 1991) </ref> verb subcategorization frame recognition technique and Smadja's (Smadja & McKeown 1990) collocation acquisition algorithm.
Reference: <author> Calzolari, N. & R. </author> <month> Bindi </month> <year> (1990). </year> <title> Acquisition of lexical information from a large textual italian corpus. </title> <booktitle> In Proceedings of the Thirteenth International Conference on Computational Linguistics, </booktitle> <publisher> Helsinki. </publisher>
Reference-contexts: similarity among terms based on the contexts they tend to occur in; (Church & Hanks 1990) uses frequency of co-occurrence of content words to create clusters of semantically similar words, (Hindle 1990) uses both simple syntactic frames and frequency of occurrence of content words to determine similarity among nouns, and <ref> (Calzolari & Bindi 1990) </ref> use corpus-based statistical association ratios to determine lexical information such as prepositional complementation relations, modification relations, and significant compounds.
Reference: <author> Church, K. & P. </author> <title> Hanks (1990). Word association norms, mutual information, and lexicography. </title> <journal> American Journal of Computational Linguistics, </journal> <volume> 16(1) </volume> <pages> 22-29. </pages>
Reference-contexts: We make use of restricted syntactic information as do Brent's (Brent 1991) verb subcategorization frame recognition technique and Smadja's (Smadja & McKeown 1990) collocation acquisition algorithm. The work reported here attempts to find semantic similarity among terms based on the contexts they tend to occur in; <ref> (Church & Hanks 1990) </ref> uses frequency of co-occurrence of content words to create clusters of semantically similar words, (Hindle 1990) uses both simple syntactic frames and frequency of occurrence of content words to determine similarity among nouns, and (Calzolari & Bindi 1990) use corpus-based statistical association ratios to determine lexical information
Reference: <author> Evans, D. A., S. K. Henderson, R. G. Lefferts, & I. A. </author> <month> Monarch </month> <year> (1991). </year> <title> A summary of the CLARIT project. </title> <type> Technical Report CMU-LCL-91-2, </type> <institution> Laboratory for Computational Linguistics, Carnegie-Mellon University. </institution>
Reference-contexts: Zwingli's Sixty-seven Articles (1523) for disputation became a basic doctrinal document for the Swiss reformed church. 3.2 Syntactic Analysis of the Corpus These sentences were grammatically analyzed using a robust system developed for CLARIT <ref> (Evans et al. 1991) </ref>. <p> `genius', `Einstein' was found to be closest to `Albert' and `theory'. (This was also another case in which there were no terms at the correct level of description names of individual geniuses do not currently appear in the network.) We are hoping to integrate the phrase extraction mechanisms of CLARIT <ref> (Evans et al. 1991) </ref> in order to treat these properly. Third, the uses of only syntactic contexts gives an interesting but restricted idea of what a word means.
Reference: <author> Grefenstette, G. </author> <year> (1992a). </year> <title> SEXTANT: extracting semantics from raw text implementation details. </title> <type> Technical Report CS92-05, </type> <institution> University of Pitts-burgh, Computer Science Dept. </institution>
Reference-contexts: The SEXTANT system takes this output and uses a number of simple robust algorithms <ref> (Grefenstette 1992a) </ref> to divide the sentences of the corpus into complex noun phrases (NP) and verb phrases (VP) as shown below: NP both NP the form of the letter and its position at the begin of the alphabet VP be NP from the latin alphabet -- , which VP derive NP <p> We use a weighted version of the Jaccard coefficient; each attribute is weighted between 0 and 1 as a function of how many different words it modifies. See <ref> (Grefenstette 1992a) </ref> for details. 4 Several Examples 4.1 Example 1 As described in the preceeding section, we processed 4 megabytes of text for the `Harvard' example, performing morphological analysis, dictionary look-up, rough grammatical disambiguation, division into noun and verb phrases, parsing, and extraction of lexical attributes for each noun in the
Reference: <author> Grefenstette, G. </author> <year> (1992b). </year> <title> Use of syntactic context to produce term association lists for text retrieval. </title> <booktitle> In Proceedings of SIGIR '92, </booktitle> <address> Copenhagen, Denmark. Grolier (1990). </address> <publisher> Academic American Encyclopedia. Grolier Electronic Publishing, Danbury, Connecti-cut. </publisher>
Reference-contexts: Furthermore, these contexts can be determined from the contexts that surround the child subtrees of each sense of Y. <ref> (Grefenstette 1992b) </ref> has developed a weak technique, embodied in a program called SEXTANT, which, given a target word, determines which other words in a corpus are most similar to it based on their syntactic usage patterns.
Reference: <author> Hearst, M. A. </author> <year> (1992). </year> <title> Automatic acquisition of hy--ponyms from large text corpora. </title> <booktitle> In Proceedings of the Fourteenth International Conference on Computational Linguistics, </booktitle> <address> Nantes, France. </address>
Reference-contexts: Section 5 sketches some solutions to the difficulties mentioned in the previous section and discusses one aspect of the integration of statistical techniques and a thesaurus, and Section 7 presents a brief conclusion. 2 The Problem: Integration of Lexical Relations <ref> (Hearst 1992) </ref> reports a method for the automatic acquisition of the hyponymy lexical relation from unrestricted text. In this method, the text is scanned for instances of distinguished lexico-syntactic patterns that indicate the relation of interest. For example, consider the lexico-syntactic pattern ... <p> The results of determining syntactic similarity for several examples appear in the following section. 3.1 Generating the corpus One of the relations extracted from an on-line encyclopedia (Grolier 1990) using the technique described in <ref> (Hearst 1992) </ref>, is hyponym (Harvard, institution). As described above, if we wish to insert this relation into a hierarchical structure such as WordNet, we have to decide which sense of `institution' is appropriate.
Reference: <author> Hindle, D. </author> <year> (1990). </year> <title> Noun classification from predicate-argument structures. </title> <booktitle> Proceedings of the 28th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 268-275. </pages>
Reference-contexts: The work reported here attempts to find semantic similarity among terms based on the contexts they tend to occur in; (Church & Hanks 1990) uses frequency of co-occurrence of content words to create clusters of semantically similar words, <ref> (Hindle 1990) </ref> uses both simple syntactic frames and frequency of occurrence of content words to determine similarity among nouns, and (Calzolari & Bindi 1990) use corpus-based statistical association ratios to determine lexical information such as prepositional complementation relations, modification relations, and significant compounds.
Reference: <author> Miller, G. A., R. Beckwith, C. Fellbaum, D. Gross, & K. J. </author> <title> Miller (1990). Introduction to wordnet: An on-line lexical database. </title> <journal> Journal of Lexicography, </journal> <volume> 3(4) </volume> <pages> 235-244. </pages>
Reference-contexts: We want to develop a means to correctly insert an instance of the hyponymy relation into an existing hyponymically-structured network (hyponymy is reflexive, symmetric and transitive). For our experiments we use the manually constructed thesaurus WordNet <ref> (Miller et al. 1990) </ref>. In WordNet, word forms with synonymous meanings are grouped into sets, called synsets. This allows a distinction to be made between senses of homographs.
Reference: <author> Romesburg, H. C. </author> <year> (1984). </year> <title> Cluster Analysis for Researchers. Lifetime Learning Publications, </title> <address> Bel-mont, CA. </address>
Reference-contexts: Using measures derived in the social sciences for comparing two individuals, each described by a number of attributes <ref> (Romesburg 1984) </ref>, we compare each of the nouns in the corpus derived from the words in the list above. The attributes of each word are those other words found in syntactic relation to it by the simple syntactic processing of SEXTANT.
Reference: <author> Smadja, F. A. & K. R. </author> <title> McKeown (1990). Automatically extracting and representing collocations for language generation. </title> <booktitle> In Proceedings of the 28th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 252-259. </pages>
Reference-contexts: The union of these two corpus-based methods is promising, although only partially successful in the experiments run so far. These ideas are related to other recent work in several ways. We make use of restricted syntactic information as do Brent's (Brent 1991) verb subcategorization frame recognition technique and Smadja's <ref> (Smadja & McKeown 1990) </ref> collocation acquisition algorithm.
Reference: <author> Wilks, Y. </author> <year> (1990). </year> <title> Combining weak methods in large-scale text processing. </title> <editor> In P. S. Jacobs, editor, </editor> <title> Text-Based Intelligent Systems: Current Research in Text Analysis, Information Extraction, and Retrieval. GE Research & Development Center, </title> <type> TR 90CRD198. </type>
Reference-contexts: This paper presents an attempt to combine knowledge-poor techniques; <ref> (Wilks 1990) </ref> discusses the potential power behind combining weak methods, and in (Wilks et al. 1990) describes advances achieved using this paradigm. The next section describes in detail the problem being addressed and the two existing coarse-level language processing techniques that are to be combined. <p> This paper presents an attempt to combine knowledge-poor techniques; (Wilks 1990) discusses the potential power behind combining weak methods, and in <ref> (Wilks et al. 1990) </ref> describes advances achieved using this paradigm. The next section describes in detail the problem being addressed and the two existing coarse-level language processing techniques that are to be combined.
Reference: <author> Wilks, Y. A., D. C. Fass, C. ming Guo, J. E. McDon-ald, T. Plate, & B. M. </author> <month> Slator </month> <year> (1990). </year> <title> Providing machine tractable dictionary tools. </title> <journal> Journal of Machine Translation, </journal> <volume> 2. </volume>
Reference-contexts: This paper presents an attempt to combine knowledge-poor techniques; <ref> (Wilks 1990) </ref> discusses the potential power behind combining weak methods, and in (Wilks et al. 1990) describes advances achieved using this paradigm. The next section describes in detail the problem being addressed and the two existing coarse-level language processing techniques that are to be combined. <p> This paper presents an attempt to combine knowledge-poor techniques; (Wilks 1990) discusses the potential power behind combining weak methods, and in <ref> (Wilks et al. 1990) </ref> describes advances achieved using this paradigm. The next section describes in detail the problem being addressed and the two existing coarse-level language processing techniques that are to be combined.
References-found: 13

