URL: ftp://info.mcs.anl.gov/pub/tech_reports/plassman/NOWs.ps.gz
Refering-URL: http://www.mcs.anl.gov/sumaa3d/Papers/papers.html
Root-URL: http://www.mcs.anl.gov
Title: UNSTRUCTURED MESH COMPUTATIONS ON NETWORKS OF WORKSTATIONS  
Author: MARK T. JONES AND PAUL E. PLASSMANN 
Keyword: Key words. Finite Elements, Networks of Workstations, Parallel Computing, Scalable Algorithms, Unstructured Meshes  
Abstract: Unstructured mesh technology can be used to create highly efficient scientific and engineering application software. Networks of workstations (NOWs) are a cost-effective platform for the timely solution of large problems in science and engineering. The performance of unstructured mesh computations on NOWs is investigated in this paper. Several parallel unstructured mesh algorithms are informally shown to have computation and communication characteristics similar to those of parallel sparse matrix-by-vector multiplication. These characteristics are discussed, and the requirements they place on an interconnection network are described. Experimental data are given to summarize the communication parameters of four different NOW configurations. Finally, extensive empirical results are given to characterize the performance of unstructured mesh computations on the four NOWs. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. J. Berger and S. H. Bokhari, </author> <title> A partitioning strategy for nonuniform problems on multiprocessors, </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36 (1987), </volume> <pages> pp. 570-580. </pages>
Reference-contexts: Note that every triangle that contains a vertex owned by P0 is stored on P0. We have chosen to first partition the vertices using a geometric partitioning scheme described in [8]. This scheme is an extension of the orthogonal recursive bisection <ref> [1, 16] </ref> and has been shown experimentally to have desirable properties that scale with the problem size [11]. This partitioning algorithm is itself a parallel algorithm. The runtime of the implementation used in this paper increases as the number of processors increases linearly with the problem size. 2.2. Mesh Refinement.
Reference: [2] <author> M. A. Blumrich, K. Li, R. Alpert, C. Dubnicki, E. W. Felten, and J. Sandberg, </author> <title> Virtual memory mapped network interface for the shrimp multicomputer, </title> <booktitle> in Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <month> April </month> <year> 1994, </year> <pages> pp. 142-153. </pages>
Reference-contexts: Of the parallel algorithms employed, only the performance of the partitioning algorithm was found to be wanting. The authors are investigating a novel implementation of this algorithm to increase its scalability. New technologies for NOWs, such as active messages [12] and the SHRIMP project <ref> [2] </ref>, offer the promise of increasing the effective bandwidth available to applications.
Reference: [3] <author> I. S. Duff, </author> <title> Parallel implementation of multifrontal schemes, </title> <booktitle> Parallel Computing, 3 (1986), </booktitle> <pages> pp. 193-204. </pages>
Reference-contexts: The efficiency of the algorithm we have used is similar to the alternative algorithm; the number of redundant element evaluations is proportional to the number of elements on the boundary of a processor. 2.4. Matrix Solution. Parallel solution of sparse linear systems of equations has been widely studied <ref> [3, 5, 6, 9, 14] </ref>. The iterative method used in this paper is a parallel conjugate gradient method preconditioned by an incomplete factorization implemented in the BlockSolve95 software package [10]. The parallel incomplete factorization method is based on graph coloring and is described in [7, 9].
Reference: [4] <author> D. Epema, M. Livny, R. van Dantzig, X. Evers, and J. Pruyne, </author> <title> A worldwide flock of condors: Load sharing among workstation clusters, </title> <journal> Journal on Future Generations of Computers Systems, </journal> <month> 12 </month> <year> (1996). </year>
Reference-contexts: The authors are investigating a novel implementation of this algorithm to increase its scalability. New technologies for NOWs, such as active messages [12] and the SHRIMP project [2], offer the promise of increasing the effective bandwidth available to applications. Advances in operating systems for NOWs such as Condor <ref> [4] </ref> will improve the perfor mance of applications during times when some workstations on the network are being used intensively. 21 5 10 15 20 25 0 4 8 12 16 Number of processors Mflops per processor LE3 130449 unk.;linear elements NOW2 (155Mbps ATM) 130449 unk.;linear elements NOW1 (100Mbps Ethernet) 166173
Reference: [5] <author> A. George, M. T. Heath, and J. Liu, </author> <title> Parallel Cholesky factorization on a shared-memory multiprocessor, Linear Algebra and its Applications, </title> <booktitle> 77 (1986), </booktitle> <pages> pp. 165-187. </pages>
Reference-contexts: The efficiency of the algorithm we have used is similar to the alternative algorithm; the number of redundant element evaluations is proportional to the number of elements on the boundary of a processor. 2.4. Matrix Solution. Parallel solution of sparse linear systems of equations has been widely studied <ref> [3, 5, 6, 9, 14] </ref>. The iterative method used in this paper is a parallel conjugate gradient method preconditioned by an incomplete factorization implemented in the BlockSolve95 software package [10]. The parallel incomplete factorization method is based on graph coloring and is described in [7, 9].
Reference: [6] <author> M. Heath, E. Ng, and B. Peyton, </author> <title> Parallel algorithms for sparse linear systems, </title> <journal> SIAM Review, </journal> <volume> 33 (1991), </volume> <pages> pp. 420-460. 22 </pages>
Reference-contexts: The efficiency of the algorithm we have used is similar to the alternative algorithm; the number of redundant element evaluations is proportional to the number of elements on the boundary of a processor. 2.4. Matrix Solution. Parallel solution of sparse linear systems of equations has been widely studied <ref> [3, 5, 6, 9, 14] </ref>. The iterative method used in this paper is a parallel conjugate gradient method preconditioned by an incomplete factorization implemented in the BlockSolve95 software package [10]. The parallel incomplete factorization method is based on graph coloring and is described in [7, 9].
Reference: [7] <author> M. T. Jones and P. E. Plassmann, </author> <title> A parallel graph coloring heuristic, </title> <journal> SIAM Journal on Scientific Computing, </journal> <volume> 14 (1993), </volume> <pages> pp. </pages> <month> 654-669. </month> <title> [8] , Computational results for parallel unstructured mesh computations, </title> <booktitle> Computing Systems in Engineering, 5 (1994), </booktitle> <pages> pp. </pages> <month> 297-309. </month> <title> [9] , Scalable iterative solution of sparse linear systems, </title> <booktitle> Parallel Computing, 20 (1994), </booktitle> <pages> pp. </pages> <month> 753-773. </month> <title> [10] , BlockSolve95 users manual: Scalable library software for the parallel solution of sparse linear systems, </title> <type> ANL Report ANL-95/48, </type> <institution> Argonne National Laboratory, Argonne, Ill., </institution> <month> De-cember </month> <year> 1995. </year> <title> [11] , Parallel algorithms for adaptive mesh refinement, </title> <journal> SIAM Journal on Scientific Computing, </journal> <volume> 18 (1997), </volume> <pages> pp. 686-708. </pages>
Reference-contexts: The iterative method used in this paper is a parallel conjugate gradient method preconditioned by an incomplete factorization implemented in the BlockSolve95 software package [10]. The parallel incomplete factorization method is based on graph coloring and is described in <ref> [7, 9] </ref>. BlockSolve95 analyzes and takes advantage of the local structure of the mesh; it achieves high execution rates for problems with multiple unknowns per vertex and for higher order elements.
Reference: [12] <author> L. T. Liu and D. E. Culler, </author> <title> Evaluation of the Intel Paragon on active message communication, </title> <booktitle> in Proceedings of the Intel Supercomputer Users Group Conference, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: Of the parallel algorithms employed, only the performance of the partitioning algorithm was found to be wanting. The authors are investigating a novel implementation of this algorithm to increase its scalability. New technologies for NOWs, such as active messages <ref> [12] </ref> and the SHRIMP project [2], offer the promise of increasing the effective bandwidth available to applications.
Reference: [13] <author> Message Passing Interface Forum, </author> <title> MPI: A message-passing interface standard, </title> <journal> International Journal of Supercomputing Applications, </journal> <month> 8 </month> <year> (1994). </year>
Reference-contexts: To characterize the performance of these five configurations, we experimentally examined two quantities of interest: latency and bandwidth as a function of the number of processors and the message length. We wrote a short C program that sent and received messages between pairs of processors using MPI <ref> [13] </ref>. We timed 100 repetitions of that task for varying message sizes and for varying numbers of processor pairs operating simultaneously. A barrier function was used to synchronize the processors. <p> A brief description of each of the problems is given in Table 5.1. On each configuration we used an identical set of application programs. Each program is written primarily in C and uses the Message Passing Interface (MPI) standard for interprocessor communication <ref> [13] </ref>. Table 5.1 Information on the smallest instance of each of the problem types. Element No. of No. of No. of Name Problem Type Dim.
Reference: [14] <author> D. P. O'Leary and R. White, </author> <title> Multi-splittings of matrices and parallel solution of linear systems, </title> <journal> SIAM Journal of Algebraic Discrete Methods, </journal> <volume> 6 (1985), </volume> <pages> pp. 630-640. </pages>
Reference-contexts: The efficiency of the algorithm we have used is similar to the alternative algorithm; the number of redundant element evaluations is proportional to the number of elements on the boundary of a processor. 2.4. Matrix Solution. Parallel solution of sparse linear systems of equations has been widely studied <ref> [3, 5, 6, 9, 14] </ref>. The iterative method used in this paper is a parallel conjugate gradient method preconditioned by an incomplete factorization implemented in the BlockSolve95 software package [10]. The parallel incomplete factorization method is based on graph coloring and is described in [7, 9].
Reference: [15] <author> M.-C. Rivara, </author> <title> Mesh refinement processes based on the generalized bisection of simplices, </title> <journal> SIAM Journal of Numerical Analysis, </journal> <volume> 21 (1984), </volume> <pages> pp. 604-613. </pages>
Reference-contexts: This partitioning algorithm is itself a parallel algorithm. The runtime of the implementation used in this paper increases as the number of processors increases linearly with the problem size. 2.2. Mesh Refinement. The mesh refinement algorithm is based on the bisection of triangles (or tetrahedra) <ref> [15] </ref>. A set of triangles (tetrahedra) is marked for refinement based on local error estimates. The parallel refinement algorithm selects independent subsets of triangles (tetrahedra) from this marked set that can be bisected simultaneously while still maintaining the integrity of the mesh data structures [11].
Reference: [16] <author> R. D. Williams, </author> <title> Performance of dynamic load balancing algorithms for unstructured mesh calculations, </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 3 (1991), </volume> <pages> pp. 457-481. 23 </pages>
Reference-contexts: Note that every triangle that contains a vertex owned by P0 is stored on P0. We have chosen to first partition the vertices using a geometric partitioning scheme described in [8]. This scheme is an extension of the orthogonal recursive bisection <ref> [1, 16] </ref> and has been shown experimentally to have desirable properties that scale with the problem size [11]. This partitioning algorithm is itself a parallel algorithm. The runtime of the implementation used in this paper increases as the number of processors increases linearly with the problem size. 2.2. Mesh Refinement.
References-found: 12

