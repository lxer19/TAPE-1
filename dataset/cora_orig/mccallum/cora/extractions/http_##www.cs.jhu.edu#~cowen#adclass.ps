URL: http://www.cs.jhu.edu/~cowen/adclass.ps
Refering-URL: http://www.cs.jhu.edu/~cowen/
Root-URL: http://www.cs.jhu.edu
Title: Approximate Distance Classification  
Author: Adam H. Cannon Lenore J. Cowen Carey E. Priebe 
Address: Baltimore, MD, 21218  
Affiliation: Department of Mathematical Sciences The Johns Hopkins University  
Abstract: We investigate the use of a class of nonlinear projections from a high-dimensional Euclidean space to a low-dimensional space in a classification (supervised learning) context. The projections developed by Cowen and Priebe approximately preserve interclass distances. Projected data was obtained from data sets that have been extensively studied in the machine learning and statistical pattern recognition communities, and analyzed in the projected space using standard statistical techniques. A simple implementation involving no pre-processing or data dependent adjustments produced results that are near-competitive with the best known established classification rates on these benchmark data sets. Thus even in moderate dimensional spaces the utility and robustness of classification schemes based on these projections is demonstrated. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Asimov. </author> <title> The grand tour: A tool for viewing multidimensional data. </title> <journal> SIAM J. Sci. Statist. Comp., </journal> <volume> 6 </volume> <pages> 128-143, </pages> <year> 1985. </year>
Reference-contexts: Instead of searching for clustering structure of the original, high-dimensional observations, it is common practice to employ dimension reduction methods. The question "How to project?" naturally arises. Some success has been achieved in moderately high-dimensional spaces using linear projections combined with projection pursuit methods (see Huber [8] and Asimov <ref> [1] </ref>), but finding useful linear projections in very high dimensions frequently remains a barrier. In 1997, Cowen and Priebe [4] introduced a class of nonlinear projections that is easy to construct and has been demonstrated to preserve clustering structure in high-dimensional data sets that strongly cluster.
Reference: [2] <author> P.J. Bickel and K.A. Doksum. </author> <title> Mathematical Statistics. </title> <publisher> Prentice Hall, </publisher> <year> 1977. </year>
Reference: [3] <author> G. Casella and R.L. Berger. </author> <title> Statistical Inference. </title> <publisher> Wadsworth, Inc., </publisher> <year> 1990. </year>
Reference: [4] <author> L.J. Cowen and C.E. Priebe. </author> <title> Randomized nonlinear projectons uncover high-dimensional structure. </title> <journal> Advances in Applied Math, </journal> <volume> 19 </volume> <pages> 319-331, </pages> <year> 1997. </year>
Reference-contexts: The question "How to project?" naturally arises. Some success has been achieved in moderately high-dimensional spaces using linear projections combined with projection pursuit methods (see Huber [8] and Asimov [1]), but finding useful linear projections in very high dimensions frequently remains a barrier. In 1997, Cowen and Priebe <ref> [4] </ref> introduced a class of nonlinear projections that is easy to construct and has been demonstrated to preserve clustering structure in high-dimensional data sets that strongly cluster. The motivation behind their work is to reduce dimensionality while approximately preserving intercluster distances. <p> The motivation behind their work is to reduce dimensionality while approximately preserving intercluster distances. Consequently the classification and clustering techniques based on them are referred to as Approximate Distance Classification and Clustering methods or ADC methods for short. The ADC projections Cowen and Priebe present in <ref> [4] </ref> and [10] are a family of projections to low-dimensional space, each indexed by a subset of the observations (called the witness set ). <p> * Recognition Problem: How do we distinguish the "best" (or most useful) projections from the rest? * Resolution Problem: How do we classify an observation if our analysis would give conflicting class labels for different projections? The first of these problems is addressed to some degree by Cowen and Priebe <ref> [4] </ref>. In the sequel we explore possible solutions to the second and third problems and test their merits experimentally on known benchmark data sets. <p> That is, Y i 2 f0; 1g. 2.2 ADC Projections Given a set of observations in a high-dimensional space we first seek a projection of the data into a lower-dimensional space for which approximate intercluster distances are maintained. In this paper, we map to &lt; 1 .(see <ref> [4] </ref> for the general definition of the ADC map.) Definition 1 Let S = fx 1 ; x 2 ; : : : ; x n g be a collection of n vectors in &lt; d . Let D S, and k k denote the L 2 norm. <p> However, as the number, C, of classes grows, this may not be the best approach. To find projections that will be good for multiple classes simultaneously, Cowen and Priebe's <ref> [4] </ref> treatment of the multiple cluster case suggests that it will be necessary to consider the j-dimensional ADC projections where j &gt; 1 grows as a function of the number of classes.
Reference: [5] <author> Luc Devroye, Laszlo Gyorfi, </author> <note> and Gabor Lugosi. </note>
Reference-contexts: of classes on the other hand, will probably require projections to j-dimensional space (for j &gt; 1), rather than just the 1-dimensional projections. 2 The Method 2.1 Problem Formulation The classification or supervised learning problem may be described in the following way using the notation of Devroye, Gyorfi and Lugosi <ref> [5] </ref>. Suppose we are given D n , a collection of n labelled observations in &lt; d , where for each observation X i 2 &lt; d , we are also given an associated class label Y i 2 f1; : : : ; Cg, where C is finite. <p> There are many existing methods to measure the quality of a projection (see Devroye, Gyorfi and Lugosi <ref> [5] </ref>, Huber [8], Diaconis and Freedman [6]). Here the quality of a projection generated by a witness set D was measured based on how well a subclassifier would perform on a observation from the training set if instead that observation had been deleted from D n . <p> More details on the deleted estimate can be found in Devroye, Gyorfi and Lugosi <ref> [5] </ref>, chapter 24. 2.4 Filtering and Combining Results So far the procedure is as follows. First, we sample w witness sets from the set of all size s subsets of the training data in class 1. Then, the deleted estimate evaluates each witness set that is chosen.
References-found: 5

