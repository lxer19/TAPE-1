URL: http://www.cs.indiana.edu/l/www/pub/leake/leake/p-95-07.ps.Z
Refering-URL: http://www.cs.indiana.edu/l/www/pub/leake/leake/
Root-URL: http://www.cs.indiana.edu
Email: leake@cs.indiana.edu  
Phone: 812-855-9756  
Title: Abduction, Experience, and Goals: A Model of Everyday Abductive Explanation*  
Author: David B. Leake 
Note: Accepted by The Journal of Experimental and Theoretical Artificial Intelligence Running head: Abduction, Experience, and Goals *This research was supported in part by the National Science Foundation under Grant No. IRI-9409348.  
Date: February 5, 1995  
Address: Lindley Hall 215  Bloomington, IN 47405  
Affiliation: Computer Science Department  Indiana University  
Abstract-found: 0
Intro-found: 1
Reference: <author> Baker, L. & Anderson, R. </author> <year> (1982). </year> <title> Effects of inconsistent information on text processing: evidence for comprehension monitoring. </title> <journal> Reading Research Quarterly, </journal> <volume> 17, </volume> <pages> 281-294. </pages>
Reference: <author> Bylander, T., Allemang, D., Tanner, C., & Josephson, J. </author> <year> (1991). </year> <title> The computational complexity of abduction. </title> <journal> Artificial Intelligence, </journal> <volume> 49, </volume> <pages> 25-60. </pages>
Reference: <author> Chandrasekaran, B. </author> <year> (1994). </year> <title> Functional representation and causal processes. </title> <editor> In Yovits, M. (Ed.), </editor> <booktitle> Advances in Computers. </booktitle> <publisher> Academic Press, </publisher> <address> New York. </address>
Reference: <author> Charniak, E. </author> <year> (1978). </year> <title> On the use of framed knowledge in language comprehension. </title> <journal> Artificial Intelligence, </journal> <volume> 11 (3), </volume> <pages> 225-265. </pages>
Reference: <author> Charniak, E. </author> <year> (1986). </year> <title> A neat theory of marker passing. </title> <booktitle> In Proceedings of the Fifth National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 584-588 Philadelphia, PA. </address> <publisher> AAAI. </publisher>
Reference-contexts: Structural comparisons are neutral to the content of the explanation, focusing instead on factors such as the lengths of the chains involved in the explanation (favoring the shortest chains) (e.g., Wilensky, 1983) or the number of abductive assumptions they require <ref> (e.g., Charniak, 1986) </ref>. Structural comparisons can also be aimed at measuring the "coherence" of explanations (Ng & Mooney, 1990; Thagard, 1989). When structural methods are used as the sole criteria for evaluating candidate explanations, two problems result.
Reference: <author> Charniak, E. & Goldman, R. </author> <year> (1991). </year> <title> A probabilistic model of plan recognition. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 160-165 Anaheim, CA. </address> <publisher> AAAI. </publisher>
Reference: <author> Charniak, E. & McDermott, D. </author> <year> (1987). </year> <title> Introduction to Artificial Intelligence. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA. </address>
Reference-contexts: 151] characterized abductive inference with the following framework: The surprising fact, C, is observed; But if A were true, C would be a matter of course, Hence, there is reason to suspect that A is true. 4 This process is often treated as a form of "deduction applied in reverse" <ref> (Charniak & McDermott, 1987) </ref>. 4 The result of this process is a deductive proof whose premises, rather than being definitively known, may include "abductive assumptions" that are generated during the explanation process.
Reference: <author> Charniak, E. & Shomony, S. </author> <year> (1994). </year> <title> Cost-based abduction and MAP explanation. </title> <journal> Artificial Intelligence, </journal> <volume> 66, </volume> <pages> 345-374. </pages>
Reference: <author> Chien, S. </author> <year> (1989). </year> <title> Using and refining simplifications: explanation-based learning of plans in intractable domains. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 590-595 Detroit, MI. IJCAI. </address>
Reference: <author> Chinn, C. & Brewer, W. </author> <year> (1993). </year> <title> Factors that influence how people respond to anomalous data. </title> <booktitle> In Proceedings of the Fifteenth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pp. </pages> <address> 318-323 Boulder, </address> <publisher> CO. Cognitive Science Society. </publisher>
Reference: <author> Cullingford, R. </author> <year> (1978). </year> <title> Script Application: Computer Understanding of Newspaper Stories. </title> <type> Ph.D. thesis, </type> <institution> Yale University. </institution> <note> Computer Science Department Technical Report 116. </note> <author> de Kleer, J. & Williams, B. </author> <year> (1989). </year> <title> Diagnosis with behavioral modes. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 1324-1330 Detroit, MI. IJCAI. </address>
Reference: <author> DeJong, G. </author> <year> (1979). </year> <title> Skimming Stories in Real Time: An Experiment in Integrated Understanding. </title> <type> Ph.D. thesis, </type> <institution> Yale University. Computer Science Department Technical Report 158. </institution>
Reference: <author> DeJong, G. & Mooney, R. </author> <year> (1986). </year> <title> Explanation-based learning: an alternative view. </title> <journal> Machine Learning, </journal> <volume> 1 (1), </volume> <pages> 145-176. </pages>
Reference: <author> Dietterich, T. & Flann, N. </author> <year> (1988). </year> <title> An inductive approach to solving the imperfect theory problem. </title> <booktitle> In Proceedings of the 1988 AAAI Spring Symposium on Explanation-based Learning, </booktitle> <pages> pp. </pages> <address> 42-46 Stanford, CA. </address> <publisher> AAAI. 20 Falkenhainer, B. </publisher> <year> (1990). </year> <title> Abduction as similarity-driven explanation. </title> <editor> In O'Rorke, P. (Ed.), </editor> <booktitle> Working Notes of the 1990 Spring Symposium on Automated Abduction, </booktitle> <pages> pp. 135-139. </pages> <institution> AAAI. </institution> <type> Technical Report 90-32, </type> <institution> Department of Information and Computer Science, University of California, Irvine. </institution>
Reference: <author> Garner, R. </author> <year> (1981). </year> <title> Metacognition and Reading Comprehension, </title> <journal> chap. </journal> <volume> 3. </volume> <publisher> Ablex, </publisher> <address> Norwood, NJ. </address>
Reference: <author> Hale, C. & Barsalou, L. </author> <title> (In press). Explanation content and construction during system learning and troubleshooting. </title> <journal> The Journal of the Learning Sciences. </journal>
Reference: <author> Hammond, K. </author> <year> (1989). </year> <title> Case-Based Planning: Viewing Planning as a Memory Task. </title> <publisher> Academic Press, </publisher> <address> San Diego. </address>
Reference: <author> Harman, G. </author> <year> (1965). </year> <title> The inference to the best explanation. </title> <journal> Philosophical Review, </journal> <volume> 74, </volume> <pages> 88-95. </pages>
Reference-contexts: 1 Introduction Abductive inference is the pattern of reasoning involved in forming and accepting explanatory hypotheses (Peirce, 1948). In general, many competing hypotheses may be generated for any phenomenon, requiring the explainer to choose between competing alternatives; as a result, abduction is often characterized as "inference to the best explanation" <ref> (Harman, 1965) </ref>. Modeling the abductive explanation process requires addressing fundamental questions of what constitutes an explanation, how candidate explanations are generated, and what constitutes the "best" explanation. Many models of abduction address these questions as follows.
Reference: <author> Hobbs, J., Stickel, M., Appelt, D., & Martin, P. </author> <year> (1993). </year> <title> Interpretation as abduction. </title> <journal> Artificial Intelligence, </journal> <volume> 63 (1-2), </volume> <pages> 69-142. </pages>
Reference-contexts: In response to this problem, many ideas have been proposed for reducing chaining cost during the search for explanations, such as combining of top-down and bottom-up processing (Wilensky, 1983), limiting the maximum chain length (Mooney, 1990), using heuristics to limit the branching factor of search <ref> (Hobbs et al., 1993) </ref>, using marker-passing to propose candidate paths (Charniak, 1986; Norvig, 1989), making simplifying assumptions about the explanations (Chien, 1989; Tadepalli, 1989), and using plausibility estimates to guide the choice of which explanations to pursue (de Kleer & Williams, 1989; Ng & Mooney, 1990).
Reference: <author> Josephson, J. & Josephson, S. </author> <year> (1994). </year> <title> Abductive Inference: Computation, Philosophy, Technology. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, England. </address>
Reference-contexts: facets of a situation on which to focus to attempt to explain them all. 5 According to report of a spoken communication with Harry Pople, anomalies are also used as focus for explanation in Pople's EAGOL system for power-plant diagnosis, but no description of that system has yet been published <ref> (Josephson & Josephson, 1994, p. 265) </ref>. 7 In general, unless the explainer focuses on resolving the anomaly, rather than simply accepting any derivation of the anomalous event, there is no guarantee that the explanation will be useful for repairing flawed understanding. <p> models of abduction, the "best" explanation is simply the "most plausible." Although it has been pointed out in the abduction literature that pragmatic 15 factors determine the level of certainty to require in an explanation, as when a doctor requires a diagnosis with high certainty before attempting a risky operation <ref> (Josephson & Josephson, 1994) </ref>, pragmatic factors usually do not enter elsewhere into the explanation evaluation process.
Reference: <author> Kass, A. </author> <year> (1986). </year> <title> Modifying explanations to understand stories. </title> <booktitle> In Proceedings of the Eighth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pp. </pages> <address> 691-696 Amherst, MA. </address> <publisher> Cognitive Science Society. </publisher>
Reference: <author> Kass, A. </author> <year> (1990). </year> <title> Developing Creative Hypotheses by Adapting Explanations. </title> <type> Ph.D. thesis, </type> <institution> Yale University. Northwestern University Institute for the Learning Sciences, </institution> <type> Technical Report 6. </type>
Reference-contexts: Case-based explanation for understanding has been investigated in an ongoing series of projects beginning with the SWALE system (Kass, 1986; Leake & Owens, 1986; Schank & Leake, 1989; Schank et al., 1994) and continuing with SWALE's descendents ABE <ref> (Kass, 1990, 1992, 1994) </ref> and ACCEPTER (Leake, 1992, 1994a). The approach is now being extended to explanation in the context of diagnosis of device failures (Leake, 1994b; Sooriamurthi & Leake, 1994). In SWALE, ABE, and ACCEPTER, abductive explanation is used to explain anomalous events in news stories. <p> Substantiation of the methods used is beyond the scope of this article but is provided elsewhere: problem characterization and retrieval issues are addressed in (Leake, 1991b, 1992, 1994a), explanation evaluation issues are addressed in (Leake, 1991a, 1992, 1994a), and adaptation issues are addressed in <ref> (Kass, 1990, 1992, 1994) </ref>. 2 3 Six fundamental issues for abductive reasoning Comparing models of abductive reasoning depends on first identifying fundamental issues to serve as points for comparison. <p> If it is necessary to adapt an explanation to a new situation, the strategies allowed to revise the explanation are more powerful than explanation-based generalization alone. During adaptation, parts of the explanation can not only be generalized but also added or substituted <ref> (Kass, 1990, 1992, 1994) </ref>. This allows case-based explanation to apply prior explanations to a wider range of circumstances than is possible for explanation-based generalization. <p> Thus the case-based approach uses experience to suggest alternatives even in situations that are not straightforwardly subsumed by generalizations of prior explanations, allowing more flexible reuse of the results of prior explanation construction. This process depends on having effective strategies for guiding adaptation, and such strategies are described in <ref> (Kass, 1990, 1992, 1994) </ref>. 8 Criteria for the "best" explanation In order to choose the explanation to accept, an abductive explanation system requires criteria for what constitutes the "best" explanation. <p> Case-based explanation uses its evaluation of goodness both to identify promising partial explanations and to pinpoint particular aspects of those explanations that need to be fixed. Based on the description of the problem, an adaptation rule tailored to fixing the problem is selected and applied to repair that problem <ref> (Kass, 1990, 1992, 1994) </ref>. This gives more precise guidance.
Reference: <author> Kass, A. </author> <year> (1992). </year> <title> Question asking, </title> <booktitle> artificial intelligence, and human creativity. </booktitle> <editor> In Lauer, T., Peacock, E., & Graesser, A. (Eds.), </editor> <title> Questions and Information Processing. </title> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ. </address>
Reference: <author> Kass, A. </author> <year> (1994). </year> <title> Tweaker: adapting old explanations to new situations. </title> <editor> In Schank, R., Riesbeck, C., & Kass, A. (Eds.), </editor> <title> Inside Case-Based Explanation, </title> <journal> chap. </journal> <volume> 8, </volume> <pages> pp. 263-295. </pages> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference: <author> Kautz, H. & Allen, J. </author> <year> (1986). </year> <title> Generalized plan recognition. </title> <booktitle> In Proceedings of the Fifth National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 32-37 Philadelphia, PA. </address> <publisher> AAAI. </publisher>
Reference: <author> Keller, R. </author> <year> (1988). </year> <title> Defining operationality for explanation-based learning. </title> <journal> Artificial Intelligence, </journal> <volume> 35 (2), </volume> <pages> 227-241. </pages>
Reference-contexts: In this way, the role of goal--based considerations in EBL is limited to deciding when to stop a goal-neutral chaining process or when to accept a complete explanation <ref> (Keller, 1988) </ref>. Case-based explanation uses goals to decide which paths to follow and how to follow them while making decisions about how to repair a flawed explanation.
Reference: <author> Kolodner, J. </author> <year> (1993). </year> <title> Case-Based Reasoning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Konolige, K. </author> <year> (1990). </year> <title> A general theory of abduction. </title> <editor> In O'Rorke, P. (Ed.), </editor> <booktitle> Working Notes of the 1990 Spring Symposium on Automated Abduction. </booktitle> <institution> AAAI. </institution> <type> Technical Report 90-32, </type> <institution> Department of Information and Computer Science, University of California, Irvine. </institution>
Reference: <author> Koton, P. </author> <year> (1988). </year> <title> Reasoning about evidence in causal explanations. </title> <booktitle> In Proceedings of the Seventh National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 256-261 Minneapolis, MN. </address> <publisher> AAAI, Morgan Kaufmann Publishers, Inc. 21 Krulwich, </publisher> <editor> B., Birnbaum, L., & Collins, G. </editor> <year> (1990). </year> <title> Goal-directed diagnosis of expectation failures. </title> <editor> In O'Rorke, P. (Ed.), </editor> <booktitle> Working Notes of the 1990 Spring Symposium on Automated Abduction, </booktitle> <pages> pp. 116-119. </pages> <institution> AAAI. </institution> <type> Technical Report 90-32, </type> <institution> Department of Information and Computer Science, University of California, Irvine. </institution>
Reference-contexts: In general, a case-based explanation system's initial explanation library could be provided by external sources (e.g., by reading about explained episodes) or built up by chaining methods <ref> (Koton, 1988) </ref>. 3 An alternative view of the nature of explanations is presented by set-covering models (e.g., Peng & Reggia, 1990), in which explanations are sets of factors that provide a covering for a set of findings, according to pre-defined associational links, rather than derivational chains built up from more primitive
Reference: <author> Lalljee, M. & Abelson, R. </author> <year> (1983). </year> <title> The organization of explanations. </title> <editor> In Hewstone, M. (Ed.), </editor> <title> Attribution Theory: Social and Functional Extensions. </title> <publisher> Blackwell, Oxford. </publisher>
Reference: <author> Leake, D. </author> <year> (1988). </year> <title> Evaluating explanations. </title> <booktitle> In Proceedings of the Seventh National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 251-255 Minneapolis, MN. </address> <publisher> AAAI, Morgan Kaufmann Publishers, Inc. </publisher>
Reference: <author> Leake, D. </author> <year> (1991a). </year> <title> Goal-based explanation evaluation. </title> <journal> Cognitive Science, </journal> <volume> 15 (4), </volume> <pages> 509-545. </pages>
Reference-contexts: Substantiation of the methods used is beyond the scope of this article but is provided elsewhere: problem characterization and retrieval issues are addressed in (Leake, 1991b, 1992, 1994a), explanation evaluation issues are addressed in <ref> (Leake, 1991a, 1992, 1994a) </ref>, and adaptation issues are addressed in (Kass, 1990, 1992, 1994). 2 3 Six fundamental issues for abductive reasoning Comparing models of abductive reasoning depends on first identifying fundamental issues to serve as points for comparison. <p> In general, in any multi-task system the only way to assure useful explanations is to explicitly evaluate their goodness according to current system goals <ref> (Leake, 1991a, 1992) </ref>. However, as observed previously, models of abductive explanation seldom consider the effect of different possible uses of explanations on which explanation to favor.
Reference: <author> Leake, D. </author> <year> (1991b). </year> <title> An indexing vocabulary for case-based explanation. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 10-15 Anaheim, CA. </address> <publisher> AAAI. </publisher>
Reference-contexts: The viability of this process depends on having effective methods to perform each of these steps. Substantiation of the methods used is beyond the scope of this article but is provided elsewhere: problem characterization and retrieval issues are addressed in <ref> (Leake, 1991b, 1992, 1994a) </ref>, explanation evaluation issues are addressed in (Leake, 1991a, 1992, 1994a), and adaptation issues are addressed in (Kass, 1990, 1992, 1994). 2 3 Six fundamental issues for abductive reasoning Comparing models of abductive reasoning depends on first identifying fundamental issues to serve as points for comparison. <p> the explanation library of the program ACCEPTER are organized and retrieved using an indexing scheme in which the indexing vocabulary reflects both which features of a situation were anomalous and why they were anomalous, so that the process for retrieving stored explanations can focus on explanations relevant to both points <ref> (Leake, 1991b, 1992) </ref>. To illustrate ACCEPTER's indexing vocabulary, we describe some of the categories that could apply to the ATM robbery example.
Reference: <author> Leake, D. </author> <year> (1992). </year> <title> Evaluating Explanations: A Content Theory. </title> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ. </address>
Reference-contexts: Case-based explanation for understanding has been investigated in an ongoing series of projects beginning with the SWALE system (Kass, 1986; Leake & Owens, 1986; Schank & Leake, 1989; Schank et al., 1994) and continuing with SWALE's descendents ABE (Kass, 1990, 1992, 1994) and ACCEPTER <ref> (Leake, 1992, 1994a) </ref>. The approach is now being extended to explanation in the context of diagnosis of device failures (Leake, 1994b; Sooriamurthi & Leake, 1994). In SWALE, ABE, and ACCEPTER, abductive explanation is used to explain anomalous events in news stories. <p> The method used to estimate the reasonableness of the assumptions and rules in an explanatory chain is to compare them to standard stereotypes. When conflicts occur, those conflicts are flagged as plausibility problems <ref> (Leake, 1992, 1994a) </ref>. No inference is done to evaluate how their ramifications interact. The motivation for estimating likelihood by similarity to stereotyped patterns, rather than using formal probability calculations, is the need to decide plausibility even when probabilities are unavailable.
Reference: <author> Leake, D. </author> <year> (1993). </year> <title> Focusing construction and selection of abductive hypotheses. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 24-29 Chambery, France. IJCAI. </address>
Reference: <author> Leake, D. </author> <year> (1994a). </year> <title> ACCEPTER: evaluating explanations. </title> <editor> In Schank, R., Riesbeck, C., & Kass, A. (Eds.), </editor> <title> Inside Case-Based Explanation, </title> <journal> chap. </journal> <volume> 6, </volume> <pages> pp. 167-206. </pages> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference: <author> Leake, D. </author> <year> (1994b). </year> <title> Issues in goal-driven explanation. </title> <editor> In Ram, A. & desJardins, M. (Eds.), </editor> <booktitle> Proceedings of the 1994 AAAI Spring Symposium on Goal-Driven Learning, </booktitle> <pages> pp. </pages> <address> 72-79 Stanford, CA. </address> <publisher> AAAI. </publisher>
Reference: <author> Leake, D. & Owens, C. </author> <year> (1986). </year> <title> Organizing memory for explanation. </title> <booktitle> In Proceedings of the Eighth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pp. </pages> <address> 710-715 Amherst, MA. </address> <publisher> Cognitive Science Society. </publisher>
Reference: <author> Lebowitz, M. </author> <year> (1980). </year> <title> Generalization and Memory in an Integrated Understanding System. </title> <type> Ph.D. thesis, </type> <institution> Yale University. Computer Science Department Technical Report 186. </institution>
Reference: <author> Levesque, H. </author> <year> (1989). </year> <title> A knowledge-level account of abduction. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 1061-1067 Detroit, MI. IJCAI. </address>
Reference: <author> Lin, D. </author> <year> (1992). </year> <title> Obvious Abduction. </title> <type> Ph.D. thesis, </type> <institution> University of Alberta. </institution>
Reference: <author> Mackie, J. </author> <year> (1965). </year> <title> Causes and conditions. </title> <journal> American Philosophical Quarterly, </journal> <volume> 2 (4), </volume> <pages> 245-264. </pages>
Reference: <author> McCarthy, J. </author> <year> (1980). </year> <title> Circumscription|a form of non-monotonic reasoning. </title> <journal> Artificial Intelligence, </journal> <volume> 13 (1 & 2), </volume> <pages> 27-39. </pages>
Reference-contexts: Unfortunately, however, rules describing the everyday world can never be perfect. For example, even if a reasoner has perfect knowledge of the world, rules can never include all the factors that may potentially be relevant in a situation. This problem, known as the qualification problem <ref> (McCarthy, 1980) </ref>, is illustrated by McCarthy's famous observation that any rule stating that a car will start when the ignition is turned on must in principle depend on an infinite set of conditions such as "there isn't a potato in the tail pipe." Consequently, it is useful to guide the choice
Reference: <author> McKoon, G. & Ratcliff, R. </author> <year> (1992). </year> <title> Inference during reading. </title> <journal> Psychological Review, </journal> <volume> 99 (3), </volume> <pages> 440-466. </pages>
Reference: <author> Miller, R., Pople, H., & Meyers, J. </author> <year> (1982). </year> <title> Internist-i, an experimental computer-based diagnostic consultant for general internal medicine. </title> <journal> New England Journal of Medicine, </journal> <volume> 307 (8), </volume> <pages> 468-476. </pages> <note> 22 Minsky, </note> <author> M. </author> <year> (1975). </year> <title> A framework for representing knowledge. </title> <editor> In Winston, P. (Ed.), </editor> <booktitle> The Psychology of Computer Vision, chap. </booktitle> <volume> 6, </volume> <pages> pp. 211-277. </pages> <publisher> McGraw-Hill, </publisher> <address> New York. </address>
Reference: <author> Mitchell, T., Keller, R., & Kedar-Cabelli, S. </author> <year> (1986). </year> <title> Explanation-based generalization: a unifying view. </title> <journal> Machine Learning, </journal> <volume> 1 (1), </volume> <pages> 47-80. </pages>
Reference-contexts: In explanation-based generalization, goal-based criteria are used to test whether an explanatory chain is sufficient (i.e., whether the leaves of an explanation are operational) but not to guide the choice of which alternatives to 18 pursue when adding to an explanatory chain <ref> (Mitchell et al., 1986) </ref>. In this way, the role of goal--based considerations in EBL is limited to deciding when to stop a goal-neutral chaining process or when to accept a complete explanation (Keller, 1988).
Reference: <author> Mooney, R. </author> <year> (1990). </year> <title> A General Explanation-based Learning Mechanism and its Application to Narrative Understanding. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo. </address>
Reference-contexts: In response to this problem, many ideas have been proposed for reducing chaining cost during the search for explanations, such as combining of top-down and bottom-up processing (Wilensky, 1983), limiting the maximum chain length <ref> (Mooney, 1990) </ref>, using heuristics to limit the branching factor of search (Hobbs et al., 1993), using marker-passing to propose candidate paths (Charniak, 1986; Norvig, 1989), making simplifying assumptions about the explanations (Chien, 1989; Tadepalli, 1989), and using plausibility estimates to guide the choice of which explanations to pursue (de Kleer &
Reference: <author> Ng, H. & Mooney, R. </author> <year> (1990). </year> <title> On the role of coherence in abductive explanation. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 337-342 Boston, MA. </address> <publisher> AAAI. </publisher>
Reference-contexts: In general, a case-based explanation system's initial explanation library could be provided by external sources (e.g., by reading about explained episodes) or built up by chaining methods (Koton, 1988). 3 An alternative view of the nature of explanations is presented by set-covering models <ref> (e.g., Peng & Reggia, 1990) </ref>, in which explanations are sets of factors that provide a covering for a set of findings, according to pre-defined associational links, rather than derivational chains built up from more primitive rules.
Reference: <author> Norvig, P. </author> <year> (1989). </year> <title> Marker passing as a weak method for text inferencing. </title> <journal> Cognitive Science, </journal> <volume> 13 (4), </volume> <pages> 569-620. </pages>
Reference: <author> Norvig, P. & Wilensky, R. </author> <year> (1990). </year> <title> Problems with abductive language understanding models. </title> <editor> In O'Rorke, P. (Ed.), </editor> <booktitle> Working Notes of the 1990 Spring Symposium on Automated Abduction, </booktitle> <pages> pp. 18-22. </pages> <institution> AAAI. </institution> <type> Technical Report 90-32, </type> <institution> Department of Information and Computer Science, University of California, Irvine. </institution> <month> O'Rorke </month> <year> (1994). </year> <title> Abduction and explanation-based learning: case studies in diverse domains. </title> <journal> Computational Intelligence, </journal> <volume> 10 (3), </volume> <pages> 295-330. </pages>
Reference: <author> O'Rorke, P. </author> <year> (1989). </year> <title> Coherence and abduction. </title> <journal> The Behavioral and Brain Sciences, </journal> <volume> 12 (3), 484. </volume> <month> Otero & Campanario </month> <year> (1990). </year> <title> Comprehension evaluation and regulation in learning from science texts. </title> <journal> Journal of Research in Science Teaching, </journal> <volume> 27, </volume> <pages> 447-460. </pages>
Reference: <author> Ourston, D. & Mooney, R. J. </author> <year> (1994). </year> <title> Theory refinement combining analytical and empirical methods. </title> <journal> Artificial Intelligence, </journal> <volume> 66, </volume> <pages> 273-309. </pages>
Reference: <author> Pazzani, M. </author> <year> (1990). </year> <title> Creating a Memory of Causal Relationships: An Integration of Empirical and Explanation-Based Methods. </title> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ. </address>
Reference: <author> Pearl, J. </author> <year> (1988). </year> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo. </address>
Reference: <author> Peirce, C. </author> <year> (1948). </year> <title> Abduction and induction. </title> <editor> In Buchler, J. (Ed.), </editor> <booktitle> The Philosophy of Peirce: Selected Writings, chap. </booktitle> <volume> 11. </volume> <publisher> Harcourt, Brace and Company, </publisher> <address> New York. </address>
Reference-contexts: 1 Introduction Abductive inference is the pattern of reasoning involved in forming and accepting explanatory hypotheses <ref> (Peirce, 1948) </ref>. In general, many competing hypotheses may be generated for any phenomenon, requiring the explainer to choose between competing alternatives; as a result, abduction is often characterized as "inference to the best explanation" (Harman, 1965).
Reference: <author> Peng, Y. & Reggia, J. </author> <year> (1990). </year> <title> Abductive Inference Models for Diagnostic Problem Solving. </title> <publisher> Springer Verlag, </publisher> <address> New York. </address>
Reference-contexts: In general, a case-based explanation system's initial explanation library could be provided by external sources (e.g., by reading about explained episodes) or built up by chaining methods (Koton, 1988). 3 An alternative view of the nature of explanations is presented by set-covering models <ref> (e.g., Peng & Reggia, 1990) </ref>, in which explanations are sets of factors that provide a covering for a set of findings, according to pre-defined associational links, rather than derivational chains built up from more primitive rules.
Reference: <author> Poole, D. </author> <year> (1989). </year> <title> Normality and faults in logic-based diagnosis. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 1304-1310 Detroit, MI. IJCAI. </address>
Reference: <author> Poule, D. </author> <year> (1993). </year> <title> Probabilistic horn abduction and bayesian networks. </title> <journal> Artificial Intelligence, </journal> <volume> 64, </volume> <pages> 81-129. </pages>
Reference: <author> Rajamoney, S. </author> <year> (1993). </year> <title> Designing experiments to extend the domain theory. </title> <editor> In DeJong, G. (Ed.), </editor> <title> Investigating Explanation-Based Learning, </title> <journal> chap. </journal> <volume> 5, </volume> <pages> pp. 166-189. </pages> <publisher> Kluwer. 23 Ram, </publisher> <editor> A. & Leake, D. </editor> <year> (1991). </year> <title> Evaluation of explanatory hypotheses. </title> <booktitle> In Proceedings of the Thirteenth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pp. </pages> <address> 867-871 Chicago, IL. </address> <publisher> Cognitive Science Society. </publisher>
Reference: <author> Read, S. & Cesa, I. </author> <year> (1991). </year> <title> This reminds me of the time when : : : : expectation failures in reminding and explanation. </title> <journal> Journal of Experimental Social Psychology, </journal> <volume> 27, </volume> <pages> 1-25. </pages>
Reference-contexts: This example and similar informally-collected accounts helped to suggest that explanation generation could be facilitated by applying case-based reasoning. Later psychological experiments have supported the psychological validity of this reminding-based explanation process and the tendency of people to favor explanations that are based on prior explanations of similar episodes <ref> (Read & Cesa, 1991) </ref>. 2 2.1 The case-based explanation algorithm The algorithm used in the SWALE system and its descendents can be summarized as follows: * Problem characterization: Generate a description of what must be explained, i.e., the information that a good explanation must provide. * Explanation retrieval: Use the results
Reference: <author> Rieger, C. </author> <year> (1975). </year> <title> Conceptual memory and inference. In Conceptual Information Processing. </title> <publisher> North-Holland, Amsterdam. </publisher>
Reference: <author> Riesbeck, C. & Schank, R. </author> <year> (1989). </year> <title> Inside Case-Based Reasoning. </title> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ. </address>
Reference: <author> Rymon, R. </author> <year> (1993). </year> <title> Diagnostic Reasoning and Planning in Exploratory-Corrective Domains. </title> <type> Ph.D. thesis, </type> <institution> The University of Pennsylvania. </institution>
Reference-contexts: The influence of intended uses for explanations on explanation generation has begun to be investigated in abductive diagnosis for tasks such as integrating medical diagnosis and response <ref> (Rymon, 1993) </ref>, and performing medical diagnosis within a planning framework (Turner, 1994).
Reference: <author> Sanford, A. </author> <year> (1990). </year> <title> On the nature of text-driven inference. </title> <editor> In Balota, D., d'Arcais, G. F., & Rayner, K. (Eds.), </editor> <title> Comprehension processes in reading, </title> <journal> chap. </journal> <volume> 24. </volume> <publisher> Lawrence Erlbaum, </publisher> <address> Hillsdale, NJ. </address>
Reference: <author> Schank, R. </author> <year> (1982). </year> <title> Dynamic Memory: A Theory of Learning in Computers and People. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, England. </address>
Reference: <author> Schank, R. </author> <year> (1986). </year> <title> Explanation Patterns: Understanding Mechanically and Creatively. </title> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ. </address>
Reference-contexts: Consequently, the case-based model explicitly treats explanations as plausible inference chains rather than deductive proofs. In the case-based explanation model, explanations are represented as explanation patterns (XPs) <ref> (Schank, 1986) </ref>. Explanation patterns encode chains of belief dependencies showing how belief in a conclusion follows from belief in a set of premises. Syntactically these are like deductive explanations, but the derivation in an explanation pattern is not considered to entail the chain's consequent.
Reference: <author> Schank, R. & Abelson, R. </author> <year> (1977). </year> <title> Scripts, Plans, Goals and Understanding. </title> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ. </address>
Reference: <author> Schank, R. & Leake, D. </author> <year> (1989). </year> <title> Creativity and learning in a case-based explainer. </title> <journal> Artificial Intelligence, </journal> <volume> 40 (1-3), </volume> <pages> 353-385. </pages> <note> Also in Carbonell, </note> <editor> J., editor, </editor> <title> Machine Learning: Paradigms and Methods, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference: <editor> Schank, R., Riesbeck, C., & Kass, A. (Eds.). </editor> <year> (1994). </year> <title> Inside Case-Based Explanation. </title> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale New Jersey. </address>
Reference: <author> Selman, B. & Levesque, H. J. </author> <year> (1990). </year> <title> Abductive and default reasoning: a computational core. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 343-348 Boston, MA. </address> <publisher> AAAI. </publisher>
Reference: <author> Snyder, C., Higgens, R., & Stucky, R. </author> <year> (1983). </year> <title> Excuses: Masquerades in Search of Grace. </title> <publisher> Wiley, </publisher> <address> New York. </address>
Reference-contexts: For example, subjects attempting to absolve themselves of blame will focus on different features of a situation from those stressed by subjects without that goal <ref> (Snyder et al., 1983) </ref>. In general, in any multi-task system the only way to assure useful explanations is to explicitly evaluate their goodness according to current system goals (Leake, 1991a, 1992).
Reference: <author> Sooriamurthi, R. & Leake, D. </author> <year> (1994). </year> <title> Towards situated explanation. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <address> p. 1492 Seattle, WA. </address>
Reference: <author> Stern, C. & Luger, G. </author> <year> (1992). </year> <title> A model for abductive problem-solving based on explanations templates and lazy evaluation. </title> <journal> International Journal of Expert Systems, </journal> <volume> 5 (3), </volume> <pages> 249-265. </pages>
Reference: <author> Tadepalli, P. </author> <year> (1989). </year> <title> Lazy explanation-based learning: a solution to the intractable theory problem. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 694-700 Detroit, MI. IJCAI. </address>
Reference: <author> Thagard, P. </author> <year> (1989). </year> <title> Explanatory coherence. </title> <journal> The Behavioral and Brain Sciences, </journal> <volume> 12 (3), </volume> <pages> 435-502. </pages> <note> 24 Tuhrim, </note> <author> S., Reggia, J., & Goodall, S. </author> <year> (1991). </year> <title> An experimental study of criteria for hypothesis plausibility. </title> <journal> Journal of Experimental and Theoretical Artificial Intelligence, </journal> <volume> 3, </volume> <pages> 129-144. </pages>
Reference: <author> Turner, R. M. </author> <year> (1994). </year> <title> Adaptive Reasoning for Real-World Problems: A Schema-Based Approach. </title> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ. </address>
Reference-contexts: The influence of intended uses for explanations on explanation generation has begun to be investigated in abductive diagnosis for tasks such as integrating medical diagnosis and response (Rymon, 1993), and performing medical diagnosis within a planning framework <ref> (Turner, 1994) </ref>.
Reference: <author> Van Fraassen, B. </author> <year> (1980). </year> <title> The Scientific Image, </title> <journal> chap. </journal> <volume> 5. </volume> <publisher> Clarendon Press, Oxford. </publisher>
Reference: <author> Vonk, W. & Noordman, L. </author> <year> (1990). </year> <title> On the control of inferences in text understanding. </title> <editor> In Balota, D., d'Arcais, G. F., & Rayner, K. (Eds.), </editor> <title> Comprehension processes in reading, </title> <journal> chap. </journal> <volume> 21. </volume> <publisher> Lawrence Erlbaum, </publisher> <address> Hillsdale, NJ. </address>
Reference: <author> Wilensky, R. </author> <year> (1983). </year> <title> Planning and Understanding. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA. </address>
Reference-contexts: In response to this problem, many ideas have been proposed for reducing chaining cost during the search for explanations, such as combining of top-down and bottom-up processing <ref> (Wilensky, 1983) </ref>, limiting the maximum chain length (Mooney, 1990), using heuristics to limit the branching factor of search (Hobbs et al., 1993), using marker-passing to propose candidate paths (Charniak, 1986; Norvig, 1989), making simplifying assumptions about the explanations (Chien, 1989; Tadepalli, 1989), and using plausibility estimates to guide the choice of <p> Structural comparisons are neutral to the content of the explanation, focusing instead on factors such as the lengths of the chains involved in the explanation (favoring the shortest chains) <ref> (e.g., Wilensky, 1983) </ref> or the number of abductive assumptions they require (e.g., Charniak, 1986). Structural comparisons can also be aimed at measuring the "coherence" of explanations (Ng & Mooney, 1990; Thagard, 1989). When structural methods are used as the sole criteria for evaluating candidate explanations, two problems result.
Reference: <author> Zadrozny, W. </author> <year> (1994). </year> <title> Is there a prototypical rule of abduction? (Yes, e.g. in proximity based explanations). </title> <journal> Journal of Experimental and Theoretical Artificial Intelligence, </journal> <volume> 6, </volume> <pages> 147-162. 25 </pages>
References-found: 80

