URL: http://www.cs.cmu.edu/~seth/papers/poplpp.ps
Refering-URL: http://www.cs.cmu.edu/~seth/papers.html
Root-URL: 
Email: schauser@cs.ucsb.edu  fculler,sethgg@cs.berkeley.edu  
Title: Separation Constraint Partitioning ANew Algorithm for Partitioning Non-strict Programs into Sequential Threads  
Author: Klaus E. Schauser David E. Culler, Seth C. Goldstein 
Address: Santa Barbara, CA 93106  Berkeley, CA 94720  
Affiliation: Department of Computer Science University of California, Santa Barbara  Computer Science Division University of California, Berkeley  
Abstract: In this paper we present substantially improved thread partitioning algorithms for modern implicitly parallel languages. We present a new block partitioning algorithm, separation constraint partitioning, which is both more powerful and more flexible than previous algorithms. Our algorithm is guaranteed to derive maximal threads. We present a theoretical framework for proving the correctness of our partitioning approach, and we show how separation constraint partitioning makes interprocedural partitioning viable. We have implemented the partitioning algorithms in an Id90 compiler for workstations and parallel machines. Using this experimental platform, we quantify the effectiveness of different partitioning schemes on whole applications. 
Abstract-found: 1
Intro-found: 1
Reference: [AA89] <author> Z. Ariola and Arvind. P-TAC: </author> <title> A parallel intermediate language. </title> <booktitle> In Proceedings of the 1989 Conference on Functional Programming Languages and Computer Architecture, </booktitle> <pages> pages 230-242, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: Conditionals are handled similarly to function calls. A conditional with two arms can be viewed as a function call, where, depending on the result of the predicate, one of two blocks are called <ref> [AA89] </ref>. This representation simplifies the partitioning process, as we can use the same unified mechanism to deal with function calls and conditionals.
Reference: [ACI + 83] <author> Arvind, D. E. Culler, R. A. Iannucci, V. Kathail, K. Pingali, and R. E. Thomas. </author> <title> The Tagged Token 11 Dataflow Architecture. </title> <type> Technical report, </type> <institution> MIT Lab for Comp. Sci., </institution> <month> August </month> <year> 1983. </year>
Reference-contexts: Dynamic scheduling is expensive on commodity microprocessors, incurring a high cost for context switching. Therefore, these languages have been accompanied by the development of specialized computer architectures, e.g., graph reduction machines [PCSH87, Kie87], dataflow machines <ref> [ACI + 83, GKW85, SYH + 89, PC90] </ref>, and multithreaded architectures [Jor83, NPA92]. Much research has been done in compiling lenient languages for dataflow architectures [ACI + 83, Tra86, AN90, GKW85, Cul90]. <p> Therefore, these languages have been accompanied by the development of specialized computer architectures, e.g., graph reduction machines [PCSH87, Kie87], dataflow machines [ACI + 83, GKW85, SYH + 89, PC90], and multithreaded architectures [Jor83, NPA92]. Much research has been done in compiling lenient languages for dataflow architectures <ref> [ACI + 83, Tra86, AN90, GKW85, Cul90] </ref>. As a clearer separation of language and architecture has been obtained, attention has shifted to compilation aspects of these languages for commodity processors [Tra91, SCvE91, Nik93].
Reference: [AN90] <author> Arvind and R. S. Nikhil. </author> <title> Executing a Program on the MIT Tagged-Token Dataflow Architecture. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 39(3) </volume> <pages> 300-318, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: Therefore, these languages have been accompanied by the development of specialized computer architectures, e.g., graph reduction machines [PCSH87, Kie87], dataflow machines [ACI + 83, GKW85, SYH + 89, PC90], and multithreaded architectures [Jor83, NPA92]. Much research has been done in compiling lenient languages for dataflow architectures <ref> [ACI + 83, Tra86, AN90, GKW85, Cul90] </ref>. As a clearer separation of language and architecture has been obtained, attention has shifted to compilation aspects of these languages for commodity processors [Tra91, SCvE91, Nik93].
Reference: [BH87] <author> A. Bloss and P. Hudak. </author> <title> Path Semantics. </title> <booktitle> In Mathematical Foundations of Programming Language Semantics (LNCS 298). </booktitle> <publisher> Springer-Verlag, </publisher> <month> April </month> <year> 1987. </year>
Reference-contexts: Partitioning goes a step further as it may derive that arguments can be evaluated together even if a function is not strict in them [Tra91]. Path analysis <ref> [BH87] </ref> detects the order in which arguments are evaluated, which may result in a cheaper representation of thunks and reduce the cost of forcing and updating them. Serial combinators by 2 The complete proof can be found in [Sch94].
Reference: [CGSvE93] <author> D. E. Culler, S. C. Goldstein, K. E. Schauser, and T. von Eicken. </author> <title> TAM | A Compiler Controlled Threaded Abstract Machine. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 18 </volume> <pages> 347-370, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: The partitioned graphs are used to generate code for TAM, a threaded abstract machine <ref> [CGSvE93] </ref>. The TAM code is then translated to the target machine. Our translation path uses C as a portable "intermediate form" and is producing code for the CM-5, as well as for various standard sequential machines [Gol94]. We used this implementation for statistics collection and measurements. <p> All of the programs are compiled for parallel execution. As they run, lots parallelism is exposed. However in order to factor out a broad family of issues unrelated to partitioning, such as load balancing and locality, we present data here from runs on a single processor. See <ref> [CGSvE93, SGS + 93] </ref> for data and discussion on running these programs on parallel machines. We use six benchmark programs, shown in Table 1, ranging up to 1,100 source code lines. <p> The programs are described in <ref> [CGSvE93] </ref>. to dataflow partitioning). ALU includes integer and floating-point arithmetic, messages includes instructions executed to handle messages, heap includes global I-structure and M-structure accesses, and control represents all control-flow instructions including moves to initialize synchronization counters.
Reference: [Coo94] <author> S. R. Coorg. </author> <title> Partitioning Non-strict Languages for Multi-threaded Code Generation. </title> <type> Master's thesis, </type> <institution> Dept. of EECS, MIT, </institution> <address> Cambridge, MA, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: Schauser et al. extended the two basic partitioning schemes with local "merge up" and "merge down" rules, thus achieving essentially the same degree of grouping as iterated partitioning [SCvE91]. Traub et al. [TCS92] extended iterated partitioning with interprocedural analysis to obtain larger threads. Recently, <ref> [Coo94] </ref> and [Sch94] independently developed extensions to the interprocedural algorithm to handle recursive functions.
Reference: [CPJ85] <author> C. Clack and S. L. Peyton-Jones. </author> <title> Strictness Analysis </title>
Reference-contexts: Finally, Section 6 contains the summary and conclusions. A short sketch of the proof of correctness for the partitioning algorithm appears in Appendix A. 1.2 Related Work Partitioning is similar in spirit to compilation techniques for lazy functional languages [SNvGP91, Pey92]. Strictness analysis <ref> [Myc80, CPJ85] </ref> tries to determine which arguments can be evaluated before invoking the body of a function, thus avoiding the creation of expensive thunks for strict arguments.
References-found: 7

