URL: ftp://ftp.cis.ufl.edu/cis/tech-reports/tr91/tr91-007.ps
Refering-URL: http://www.cis.ufl.edu/tech-reports/tech-reports/tr91-abstracts.html
Root-URL: http://www.cis.ufl.edu
Author: Theodore Johnson 
Date: August 18, 1991  
Abstract: A Highly Concurrent Priority Queue Based on the B-link Tree University of Florida, Department of CIS Electronic Tech Report #007-91 Abstract We present a highly concurrent priority queue algorithm based on the B-link tree, which is a B + -tree in which every node has a pointer to its right sibling. The algorithm is built on the concurrent B-link tree algorithms proposed by Lehman and Yao [15] and Sagiv [19]. Since the priority queue is based on highly concurrent search structure algorithms, a large number of insert operations can execute concurrently with little or no interference. We present two algorithms for executing the deletemin operation. The first algorithm executes deletemin operations serially, but we show that it can support a higher throughput that previous shared memory concurrent priority queue algorithms because most deletemin operations execute very quickly. The second deletemin algorithm uses the fetch-and-add operation to allow several deletemin operations to execute concurrently, and can support a much higher throughput.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Aho, J. Hopcroft, and J. Ullman. </author> <title> Data Structures and Algorithms. </title> <publisher> Addison-Wesley, </publisher> <year> 1983. </year>
Reference-contexts: Biswas and Brown [4] propose a concurrent priority queue based on a nearly complete heap-structured binary tree. Their algorithm, which allows concurrent inserts and deletemins, is a based on the usual priority 1 queue algorithm <ref> [1] </ref>. The authors use a special locking protocol to prevent deadlock between descending deletemin restructuring operations and ascending insert restructuring operations. Rao and Kumar [18] propose an algorithm similar to the one in [4], except that the insert operations restructure top-down.
Reference: [2] <author> R. Anani. Lr-algorithm: </author> <title> Concurrent operations on priority queues. </title> <booktitle> In Proceedings of the Second IEEE Symposium on Parallel and Distributed Processing, </booktitle> <pages> pages 22-25, </pages> <year> 1990. </year>
Reference-contexts: The authors use a special locking protocol to prevent deadlock between descending deletemin restructuring operations and ascending insert restructuring operations. Rao and Kumar [18] propose an algorithm similar to the one in [4], except that the insert operations restructure top-down. Anani <ref> [2] </ref> proposes an extension to Rao and Kumar's algorithm that evenly hashes insert operations to the external nodes. Jones [12] proposes a priority queue based on the skew heap [21],a self-adjusting heap-ordered binary tree. <p> The Biswas-Browne priority queue algorithm [4] serializes deletemin operations at the root, and requires that during the serialization period, three locks be placed (including one on the root) and that the root restructured on every deletemin operation. The Rao-Kumar algorithm [18] (improved upon by Anani <ref> [2] </ref>) serializes both inserts and deletemins at the root, requires that four locks be placed (including one on the root), and that the root be restructured on every deletemin operation.
Reference: [3] <author> R. Baeza-Yates. </author> <title> Expected behavior of B + -trees under random inserts. </title> <journal> Acta Informatica, </journal> <volume> 27, </volume> <year> 1989. </year>
Reference-contexts: Second, we assume that the rate of insert operations is equal to the rate of deletemin operations, so that the size of the priority queue is stable. 5.1 Analysis of the Serialized-deletemin Algorithm In order to analyze the algorithms, we need to characterize the data structure. Baeza-Yates <ref> [3] </ref> has shown that the expected utilization of a B + -tree is 69% if the tree is created from a sequence of random insertions.
Reference: [4] <author> J. Biswas and J.C. Browne. </author> <title> Simultaneous update of priority structures. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages 124-131, </pages> <year> 1987. </year>
Reference-contexts: As a result, the root must have the lowest key in the entire tree, so a deletemin operation always removes the root. Quinn and Yoo [17] describe a parallel algorithm for removing an element from a heap structured tree. Biswas and Brown <ref> [4] </ref> propose a concurrent priority queue based on a nearly complete heap-structured binary tree. Their algorithm, which allows concurrent inserts and deletemins, is a based on the usual priority 1 queue algorithm [1]. <p> The authors use a special locking protocol to prevent deadlock between descending deletemin restructuring operations and ascending insert restructuring operations. Rao and Kumar [18] propose an algorithm similar to the one in <ref> [4] </ref>, except that the insert operations restructure top-down. Anani [2] proposes an extension to Rao and Kumar's algorithm that evenly hashes insert operations to the external nodes. Jones [12] proposes a priority queue based on the skew heap [21],a self-adjusting heap-ordered binary tree. <p> Fan and Cheng's algorithm [6] can provide better performance, but it is a synchronous VLSI implementation, so it isn't directly comparable. The Biswas-Browne priority queue algorithm <ref> [4] </ref> serializes deletemin operations at the root, and requires that during the serialization period, three locks be placed (including one on the root) and that the root restructured on every deletemin operation.
Reference: [5] <author> D. Comer. </author> <title> The ubiquitious B-tree. </title> <journal> ACM Comp. Surveys, </journal> <volume> 11 </volume> <pages> 121-137, </pages> <year> 1979. </year>
Reference-contexts: To avoid the serialization bottleneck, we propose a concurrent priority queue based on a different data structure, the B-link tree. A B-link tree is a B + -tree in which each node has a pointer to its right neighbor <ref> [5] </ref>. The leftmost child of a B-tree will always contain the smallest key. Thus, the leaf level of a B-link tree is an ordered list, and the internal nodes of the B-link tree form an index into the list.
Reference: [6] <author> A. Fan and K. Cheng. </author> <title> A simultaneous access priority queue. </title> <booktitle> In Proceedings of the international Conference on Parallel Processing, </booktitle> <pages> pages I95-I99, </pages> <year> 1989. </year>
Reference-contexts: This algorithm admits a simple lock-coupling algorithm because both insert and deletemin operations restructure the priority queue top-down. Herlihy [8] proposes a wait-free concurrent priority queue algorithm based on the skew heap. In addition to the shared memory concurrent priority queues, Fan and Cheng <ref> [6] </ref> propose a pipelined VLSI priority queue that uses a sorting and a merging network. An obvious problem with concurrent priority queues based on a heap-structured binary tree is that the root is a serialization bottleneck. <p> Fan and Cheng's algorithm <ref> [6] </ref> can provide better performance, but it is a synchronous VLSI implementation, so it isn't directly comparable.
Reference: [7] <author> A. Gottlieb, B. D. Lubachevsky, and L. Rudolph. </author> <title> Coordinating large numbers of processors. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing. IEEE, </booktitle> <year> 1981. </year>
Reference-contexts: Mark and unmark can be implemented using the fetch-and-add operation <ref> [7] </ref>. 2. findchild (node,v,leftmost,ischild) : Determines the next node to visit while searching for key v. The procedure will return the address of the right sibling if v isn't in the range of node, or if the node is deleted. <p> If the third option for notification is used (keeping the notifications in the tree), then the notification list of the the child should be appended to the root's list. 4 The Concurrent-Deletemin Algorithm A priority queue algorithm that allows concurrent deletemin operations can be implemented if the fetch-and-add operation <ref> [7] </ref> is available. The idea is that deletemin operations can concurrently perform a fetch-and-add on the number of remaining items in the leftmost node. Each deletemin operation receives a different number which corresponds to the item in the leftmost node which it returns.
Reference: [8] <author> M. Herlihy. </author> <title> A methodology for implementing highly concurrent data structures. </title> <booktitle> In Proceeding of the Second ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 197-206. </pages> <publisher> ACM, </publisher> <year> 1989. </year>
Reference-contexts: Jones [12] proposes a priority queue based on the skew heap [21],a self-adjusting heap-ordered binary tree. This algorithm admits a simple lock-coupling algorithm because both insert and deletemin operations restructure the priority queue top-down. Herlihy <ref> [8] </ref> proposes a wait-free concurrent priority queue algorithm based on the skew heap. In addition to the shared memory concurrent priority queues, Fan and Cheng [6] propose a pipelined VLSI priority queue that uses a sorting and a merging network.
Reference: [9] <author> T. Johnson. </author> <title> The Performance of Concurrent Data Structure Algorithms. </title> <type> PhD thesis, </type> <institution> NYU Dept. of Computer Science, </institution> <year> 1990. </year>
Reference-contexts: These algorithms have been found to allow the highest throughput and to cause the least serialization delays among the existing concurrent B + -tree algorithms <ref> [10, 9] </ref>, a further indication that the B-link tree is a promising data structure. The idea behind the B-link tree algorithms is the use of the half-split operation (see figure 1). <p> Therefore, it is primarily insert operations that block other insert operations. The insert operations follow the Sagiv's protocol, which Johnson and Shasha have shown results in very little interference <ref> [10, 9] </ref>. Since we assume that the rate of the insert operations equals the rate of the deletemin operations, the throughput of the priority queue will be limited by the maximum rate at which deletemin operations can be processed. <p> This type of queue is analyzed in <ref> [9] </ref>, where it is shown that L (ln ( R = R ) + fl)= R where L is the time to place a W lock, fl :5721 is Euler's constant, R locks arrive at rate R and are served at rate R .
Reference: [10] <author> T. Johnson and D. Shasha. </author> <title> A framework for the performance analysis of concurrent B-tree algorithms. </title> <booktitle> In ACM SIGACT/SIGMOD/SIGART Symposium on Principles of Database Systems, </booktitle> <pages> pages 273-287, </pages> <year> 1990. </year>
Reference-contexts: These algorithms have been found to allow the highest throughput and to cause the least serialization delays among the existing concurrent B + -tree algorithms <ref> [10, 9] </ref>, a further indication that the B-link tree is a promising data structure. The idea behind the B-link tree algorithms is the use of the half-split operation (see figure 1). <p> Therefore, it is primarily insert operations that block other insert operations. The insert operations follow the Sagiv's protocol, which Johnson and Shasha have shown results in very little interference <ref> [10, 9] </ref>. Since we assume that the rate of the insert operations equals the rate of the deletemin operations, the throughput of the priority queue will be limited by the maximum rate at which deletemin operations can be processed.
Reference: [11] <author> D. Jones. </author> <title> An emperical comparison of priority-queue and event-set implementations. </title> <journal> Communications of the ACM, </journal> <volume> 29(4) </volume> <pages> 300-311, </pages> <year> 1986. </year>
Reference-contexts: First, we assume that the keys of the insert operations tend to increase This assumption holds for many applications, such as discrete event simulators, and is the working assumption made when empirically comparing priority queue algorithms <ref> [11, 13] </ref>. Second, we assume that the rate of insert operations is equal to the rate of deletemin operations, so that the size of the priority queue is stable. 5.1 Analysis of the Serialized-deletemin Algorithm In order to analyze the algorithms, we need to characterize the data structure.
Reference: [12] <author> D. Jones. </author> <title> Concurrent operations on priority queues. </title> <journal> Communications of the ACM, </journal> <volume> 32(1) </volume> <pages> 132-137, </pages> <year> 1989. </year>
Reference-contexts: Rao and Kumar [18] propose an algorithm similar to the one in [4], except that the insert operations restructure top-down. Anani [2] proposes an extension to Rao and Kumar's algorithm that evenly hashes insert operations to the external nodes. Jones <ref> [12] </ref> proposes a priority queue based on the skew heap [21],a self-adjusting heap-ordered binary tree. This algorithm admits a simple lock-coupling algorithm because both insert and deletemin operations restructure the priority queue top-down. Herlihy [8] proposes a wait-free concurrent priority queue algorithm based on the skew heap. <p> The Rao-Kumar algorithm [18] (improved upon by Anani [2]) serializes both inserts and deletemins at the root, requires that four locks be placed (including one on the root), and that the root be restructured on every deletemin operation. Jones' algorithm <ref> [12] </ref> serializes insert and deletemin operations at the root, and requires that the deletemin operations place two locks, including one on the root, and and that the root be restructured. 15 All of the above algorithms allow many deletemin operations to be active in the queue concurrently.
Reference: [13] <author> J.H. Kingston. </author> <title> Analysis of tree algorithms for the simulation event list. </title> <journal> Acta Informatica, </journal> <pages> pages 15-33, </pages> <year> 1985. </year>
Reference-contexts: First, we assume that the keys of the insert operations tend to increase This assumption holds for many applications, such as discrete event simulators, and is the working assumption made when empirically comparing priority queue algorithms <ref> [11, 13] </ref>. Second, we assume that the rate of insert operations is equal to the rate of deletemin operations, so that the size of the priority queue is stable. 5.1 Analysis of the Serialized-deletemin Algorithm In order to analyze the algorithms, we need to characterize the data structure.
Reference: [14] <author> V. Lanin and D. Shasha. </author> <title> A symmetric concurrent B-tree algorithm. </title> <booktitle> In 1986 Fall Joint Computer Conference, </booktitle> <pages> pages 380-389, </pages> <year> 1986. </year>
Reference-contexts: Many inserts of low-priority items can occur simultaneously with a deletemin operation with little or no interference. 2 The Concurrent Priority Queue As a starting point for a concurrent priority queue algorithm based on a B-link tree we use the concurrent algorithms that have been proposed for the B-link tree <ref> [19, 15, 14] </ref>. These algorithms have been found to allow the highest throughput and to cause the least serialization delays among the existing concurrent B + -tree algorithms [10, 9], a further indication that the B-link tree is a promising data structure. <p> A priority queue requires deletemin operations in addition to the insert operations. Merging underfull nodes due to delete operations is difficult to implement in a B-link tree, because it is difficult to determine when the space used by the deleted node may be reclaimed <ref> [14, 19] </ref>. The deletemin operation has more structure than the delete operation, so that the space used by deleted nodes can be easily reclaimed.
Reference: [15] <author> P.L. Lehman and S.B. Yao. </author> <title> Efficient locking for concurrent operations on B-trees. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 6(4) </volume> <pages> 650-670, </pages> <year> 1981. </year>
Reference-contexts: Many inserts of low-priority items can occur simultaneously with a deletemin operation with little or no interference. 2 The Concurrent Priority Queue As a starting point for a concurrent priority queue algorithm based on a B-link tree we use the concurrent algorithms that have been proposed for the B-link tree <ref> [19, 15, 14] </ref>. These algorithms have been found to allow the highest throughput and to cause the least serialization delays among the existing concurrent B + -tree algorithms [10, 9], a further indication that the B-link tree is a promising data structure. <p> If an operation is searching for a larger key, it visits the node's right sibling. 2 Our concurrent priority queue algorithm builds upon Sagiv's [19], which is an improvement of the Lehman-Yao algorithm <ref> [15] </ref>. In Sagiv's algorithm, no operation holds more than one lock at a time.
Reference: [16] <author> M. Quinn and N. Deo. </author> <title> Parallel graph algorithms. </title> <journal> Computing Surveys, </journal> <volume> 16(3) </volume> <pages> 319-348, </pages> <year> 1984. </year>
Reference-contexts: The operations on a concurrent priority queue should satisfy a serializability condition such as strict serializability or decisive operation serializability [20]. Concurrent priority queues can be useful in parallel processor scheduling (especially real-time processor scheduling). In addition, concurrent priority queues are useful for a variety of parallel algorithms <ref> [17, 18, 16] </ref>. Many concurrent priority queues have been proposed, usually based on a heap structured tree. In a heap structured tree, the key in a child is greater than the key in the parent.
Reference: [17] <author> M. Quinn and Y. Yoo. </author> <title> Data structures for the efficient solution of graph theoretic problems on tightly-coupled computers. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages 431-438, </pages> <year> 1984. </year>
Reference-contexts: The operations on a concurrent priority queue should satisfy a serializability condition such as strict serializability or decisive operation serializability [20]. Concurrent priority queues can be useful in parallel processor scheduling (especially real-time processor scheduling). In addition, concurrent priority queues are useful for a variety of parallel algorithms <ref> [17, 18, 16] </ref>. Many concurrent priority queues have been proposed, usually based on a heap structured tree. In a heap structured tree, the key in a child is greater than the key in the parent. <p> In a heap structured tree, the key in a child is greater than the key in the parent. As a result, the root must have the lowest key in the entire tree, so a deletemin operation always removes the root. Quinn and Yoo <ref> [17] </ref> describe a parallel algorithm for removing an element from a heap structured tree. Biswas and Brown [4] propose a concurrent priority queue based on a nearly complete heap-structured binary tree. Their algorithm, which allows concurrent inserts and deletemins, is a based on the usual priority 1 queue algorithm [1].
Reference: [18] <author> V. Rao and V. Kumar. </author> <title> Concurrent access of priority queues. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 37(12) </volume> <pages> 1657-1665, </pages> <year> 1988. </year>
Reference-contexts: The operations on a concurrent priority queue should satisfy a serializability condition such as strict serializability or decisive operation serializability [20]. Concurrent priority queues can be useful in parallel processor scheduling (especially real-time processor scheduling). In addition, concurrent priority queues are useful for a variety of parallel algorithms <ref> [17, 18, 16] </ref>. Many concurrent priority queues have been proposed, usually based on a heap structured tree. In a heap structured tree, the key in a child is greater than the key in the parent. <p> Their algorithm, which allows concurrent inserts and deletemins, is a based on the usual priority 1 queue algorithm [1]. The authors use a special locking protocol to prevent deadlock between descending deletemin restructuring operations and ascending insert restructuring operations. Rao and Kumar <ref> [18] </ref> propose an algorithm similar to the one in [4], except that the insert operations restructure top-down. Anani [2] proposes an extension to Rao and Kumar's algorithm that evenly hashes insert operations to the external nodes. <p> The Biswas-Browne priority queue algorithm [4] serializes deletemin operations at the root, and requires that during the serialization period, three locks be placed (including one on the root) and that the root restructured on every deletemin operation. The Rao-Kumar algorithm <ref> [18] </ref> (improved upon by Anani [2]) serializes both inserts and deletemins at the root, requires that four locks be placed (including one on the root), and that the root be restructured on every deletemin operation.
Reference: [19] <author> Y. Sagiv. </author> <title> Concurrent operations on B fl -trees with overtaking. </title> <booktitle> In Fourth Annual ACM SIGAC-T/SIGMOD Symposium on the Principles of Database Systems, </booktitle> <pages> pages 28-37. </pages> <publisher> ACM, </publisher> <year> 1985. </year>
Reference-contexts: Many inserts of low-priority items can occur simultaneously with a deletemin operation with little or no interference. 2 The Concurrent Priority Queue As a starting point for a concurrent priority queue algorithm based on a B-link tree we use the concurrent algorithms that have been proposed for the B-link tree <ref> [19, 15, 14] </ref>. These algorithms have been found to allow the highest throughput and to cause the least serialization delays among the existing concurrent B + -tree algorithms [10, 9], a further indication that the B-link tree is a promising data structure. <p> Each node stores the value of the largest key that might be found in the subtree rooted at that node. If an operation is searching for a larger key, it visits the node's right sibling. 2 Our concurrent priority queue algorithm builds upon Sagiv's <ref> [19] </ref>, which is an improvement of the Lehman-Yao algorithm [15]. In Sagiv's algorithm, no operation holds more than one lock at a time. <p> A priority queue requires deletemin operations in addition to the insert operations. Merging underfull nodes due to delete operations is difficult to implement in a B-link tree, because it is difficult to determine when the space used by the deleted node may be reclaimed <ref> [14, 19] </ref>. The deletemin operation has more structure than the delete operation, so that the space used by deleted nodes can be easily reclaimed. <p> If none of the nodes involved in the insert operation are deleted while the insert operation is active, then the insert operation is correct because it uses the same protocol as Sagiv's algorithm <ref> [19] </ref>. If a node that is used in the insert operation is deleted while the insert operation is active, there are two possibilities. First, the node might be deleted before the last time that the insert operation accesses it.
Reference: [20] <author> D. Shasha and N. Goodman. </author> <title> Concurrent search structure algorithms. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 13(1) </volume> <pages> 53-90, </pages> <year> 1988. </year>
Reference-contexts: A concurrent priority queue handles many insert and deletemin requests simultaneously. The operations on a concurrent priority queue should satisfy a serializability condition such as strict serializability or decisive operation serializability <ref> [20] </ref>. Concurrent priority queues can be useful in parallel processor scheduling (especially real-time processor scheduling). In addition, concurrent priority queues are useful for a variety of parallel algorithms [17, 18, 16]. Many concurrent priority queues have been proposed, usually based on a heap structured tree. <p> This algorithm is decisive operation serializable (The concurrent execution is equivalent to a serial execution in which operations are ordered according to when their decisive actions occurred <ref> [20] </ref>). <p> If the fetch-and-decrement instruction returns a negative value, the block is empty and the deletemin operation must try again. This implementation is simple and efficient, but it has the problem that it is can produce executions that aren't strict serializable <ref> [20] </ref> that is, it is possible for that an insert operation I finishes before deletemin operation D starts, I inserts k I , D deletes k D , k I &lt; k D , and k I is still in the priority queue when D finishes. <p> The non-strict serializable implementation may be acceptable, especially if there is a high rate of deletemin operations and inserts into the leftmost leaf are rare (i.e., keys of the insert operations tend to increase), but some applications might require a strict serializable or a decisive operation serializable <ref> [20] </ref> priority queue. We next present a decisive operation serializable (and thus a strict serializable) concurrent-deletemin priority queue.
Reference: [21] <author> D. Sleator and R. Tarjan. </author> <title> Self adjusting heaps. </title> <journal> Siam Journal of Computing, </journal> <volume> 15(1) </volume> <pages> 52-59, </pages> <year> 1986. </year> <month> 19 </month>
References-found: 21

