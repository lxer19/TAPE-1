URL: http://www.cs.jhu.edu/~brill/ACL_NBEST.ps
Refering-URL: http://www.cs.jhu.edu/~brill/acadpubs.html
Root-URL: http://www.cs.jhu.edu
Email: -brill,rflorian,jhndrsn,lidia-@cs.jhu.edu  
Title: Beyond N-Grams: Can Linguistic Sophistication Improve Language Modeling?  
Author: Eric Brill, Radu Florian, John C. Henderson, Lidia Mangu 
Address: Baltimore, Md. 21218 USA  
Affiliation: Department of Computer Science Johns Hopkins University  
Abstract: It seems obvious that a successful model of natural language would incorporate a great deal of both linguistic and world knowledge. Interestingly, state of the art language models for speech recognition are based on a very crude linguistic model, namely conditioning the probability of a word on a small fixed number of preceding words. Despite many attempts to incorporate more sophisticated information into the models, the n-gram model remains the state of the art, used in virtually all speech recognition systems. In this paper we address the question of whether there is hope in improving language modeling by incorporating more sophisticated linguistic and world knowledge, or whether the n-grams are already capturing the majority of the information that can be employed.
Abstract-found: 1
Intro-found: 1
Reference: <author> Brill E, Harris D, Lowe S, Luo X, Rao P, Ristad E and Roukos S. </author> <year> (1996). </year> <title> A hidden tag model for language. In "Research Notes", Center for Language and speech processing. </title> <institution> The Johns Hopkins University. </institution> <note> Chapter 2. </note>
Reference: <author> Chelba C, Eagle D, Jelinek F, Jimenez V, Khudanpur S, Mangu L, Printz H, Ristad E, Rosenfeld R, Stolcke A and Wu D. </author> <title> (1997) Structure and Performance of a Dependency Language Model. </title> <booktitle> In Eurospeech 97. </booktitle> <address> Rhodes, Greece. </address>
Reference: <author> Chomsky N. </author> <title> (1956) Three models for the description of language. </title> <journal> IRE Trans. On Inform. Theory. </journal> <volume> IT-2, </volume> <pages> 113-124. </pages>
Reference: <author> Della Pietra S, Della Pietra V, Gillett J, Lafferty J, Printz H and Ures L. </author> <title> (1994) Inference and Estimation of a Long-Range Trigram Model. </title> <booktitle> In Proceedings of the Second International Colloquium on Grammatical Inference. </booktitle> <address> Alicante, Spain. </address>
Reference: <author> Fong E and Wu D. </author> <title> (1995) Learning restricted probabilistic link grammars. </title> <booktitle> IJCAI Workshop on New Approaches to Learning for Natural Language Processing, </booktitle> <address> Montreal. </address>
Reference: <author> Golding A and Roth D. </author> <title> (1996) Applying Winnow to ContextSensitive Spelling Correction. </title> <booktitle> In Proceedings of ICML 96. </booktitle>
Reference: <author> Jelinek F, Lafferty J.D. and Mercer R.L. </author> <title> (1992) Basic Methods of Probabilistic Context-Free Grammars. In "Speech Recognition and Understanding. Recent Advances, Trends, </title> <journal> and Applications", </journal> <volume> Volume F75, </volume> <pages> 345-360. </pages> <publisher> Berlin:Springer Verlag. </publisher>
Reference: <author> Jurafsky D., Wooters C, Segal J, Stolcke A, Fosler E, Tajchman G and Morgan N. </author> <title> (1995) Using a stochastic context-free grammar as a language model for speech recognition. </title> <booktitle> In ICASSP 95. </booktitle>
Reference: <author> Knight K and Chandler I. </author> <year> (1994). </year> <title> Automated Postediting of Documents. </title> <booktitle> Proceedings, Twelfth National Conference on Artificial Intelligence. </booktitle>
Reference: <author> Seymore K. and Rosenfeld R. </author> <title> (1997) Using Story Topics for Language Model Adaptation. </title> <booktitle> In Eurospeech 97. </booktitle> <address> Rhodes, Greece. </address>

References-found: 10

