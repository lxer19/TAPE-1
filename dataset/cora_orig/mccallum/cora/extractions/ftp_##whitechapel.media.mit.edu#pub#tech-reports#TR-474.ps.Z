URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-474.ps.Z
Refering-URL: http://www-white.media.mit.edu/cgi-bin/tr_pagemaker/
Root-URL: http://www.media.mit.edu
Email: fpinhanez,bobickg@media.mit.edu  
Title: Control a Reactive Computer Graphics Character in a Theater Play  
Author: Claudio Pinhanez Aaron Bobick 
Address: Ames St. Room E15-368C Cambridge, MA 02139  
Affiliation: 20  
Note: Using Computer Vision to  
Abstract: M.I.T Media Laboratory Perceptual Computing Section Technical Report No. 474 (to appear in Proc. ICVS'99) - October 1998 Abstract It/I is a two-character theater play where the human character I is taunted and played by a vision-based, autonomous computerized character | called It | which controls computer-graphics, sound, and stage lights. Unlike previous immersive interactive systems, the computer vision system recognizes the human character's actions by considering not only tracking and gestural information and the character's internal variables but also the context provided by the current situation in the story. This paper focuses on a methodology to represent and recognize the human and computer characters' actions that is based on interval scripts, a paradigm that uses Allen's temporal primitives to describe the relationships among the different actions and reactions. The system was tested in six public performances held at the MIT Media Laboratory in 1997, when the computer graphics character ran automatically during the 30-minute duration of the play.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> James F. Allen. </author> <title> Towards a general theory of action and time. </title> <journal> Artificial Intelligence, </journal> <volume> 23 </volume> <pages> 123-154, </pages> <year> 1984. </year>
Reference-contexts: In other words, actions | and thus, interaction | can not be fully described neither by events (as Director does), nor by simple tree-forking structures as proposed in [18, 2], nor by straight encapsulation such as suggested by structured programming. We adopted Allen's interval algebra <ref> [1] </ref> as the temporal model of interval scripts. Temporal relationships between intervals can be described as disjunctions of Allen's primitives and easily incorporated into an interval script. <p> Events, tree-structures, and encapsulated actions and other basic elements from other scripting languages are subsumed by Allen's algebra temporal relationships <ref> [1, 16] </ref>. Therefore, with explicit declaration of temporal constraints, the interval script paradigm allows the description of complex relations that occur in real interaction, like parallel and mutually exclusive actions, and even causality. During the compilation of the interval script, those temporal constraints are pre-processed using Allen's path-consistency algorithm [1]. <p> Therefore, with explicit declaration of temporal constraints, the interval script paradigm allows the description of complex relations that occur in real interaction, like parallel and mutually exclusive actions, and even causality. During the compilation of the interval script, those temporal constraints are pre-processed using Allen's path-consistency algorithm <ref> [1] </ref>. But to achieve speed during run-time constraint propagation, the resulting network is converted into a PNF-valued constraint network called a PNF-network.
Reference: [2] <author> Joseph Bates, A. Bryan Loyall, and W. Scott Reilly. </author> <title> An architecture for action, emotion, and social behavior. </title> <booktitle> In Proceedings of the Fourth European Workshop on Modeling Autonomous Agents in a Multi-Agent World, </booktitle> <editor> S. Martino al Cimino, </editor> <address> Italy, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: In other words, actions | and thus, interaction | can not be fully described neither by events (as Director does), nor by simple tree-forking structures as proposed in <ref> [18, 2] </ref>, nor by straight encapsulation such as suggested by structured programming. We adopted Allen's interval algebra [1] as the temporal model of interval scripts. Temporal relationships between intervals can be described as disjunctions of Allen's primitives and easily incorporated into an interval script.
Reference: [3] <author> Bruce Blumberg. </author> <title> Old Tricks, New Dogs: Ethology and Interactive Creatures. </title> <type> PhD thesis, </type> <institution> M.I.T. Media Arts and Sciences Program, </institution> <year> 1996. </year>
Reference-contexts: Also, unlike some previous reactive autonomous characters like Silas <ref> [3] </ref> or NeuroBaby [25], the interaction script of It/I contains the story of the play and It's role in it. In this sense, It is more a computer-actor (as defined in [19]) than an autonomous creature. <p> In this sense, It is more a computer-actor (as defined in [19]) than an autonomous creature. Specifically, It is pro-active in relation to a story, instead of reactive to human interaction or to internalized, creature-like goals <ref> [3] </ref>. Pro-active computer characters require scripting methods that allow representation of complex story patterns, including parallel actions, multiple paths, and some sense of cause and consequence. 5.1 Interval Scripts An interval script associates a temporal interval with every action in the script.
Reference: [4] <author> Aaron F. Bobick. </author> <title> Movement, activity, and action: The role of knowledge in the perception of motion. </title> <journal> Phil. Trans. Royal Society London B, </journal> <volume> 352 </volume> <pages> 1257-1265, </pages> <year> 1997. </year>
Reference-contexts: The main focus of this paper is on a paradigm called interval scripts to represent both the contextual elements required to recognize high-level actions (as defined by Bo-bick in <ref> [4] </ref>) and the behavior of the computer character. <p> How can this information be used by an interactive system based on a story script whose primitives refer to high-level actions such as "paying attention", "refusing to play with the machine", "posing for a picture"? The fundamental problem has been addressed by Bobick <ref> [4] </ref> who distinguishes three categories of problems in recognizing human motion, according to an increasing use of knowledge: movements, activities, and actions. Basically, a movement is "... a space-time trajectory in some configuration space." (as defined in [4]) whose recognition is independent of contextual information, except for viewing conditions. <p> machine", "posing for a picture"? The fundamental problem has been addressed by Bobick <ref> [4] </ref> who distinguishes three categories of problems in recognizing human motion, according to an increasing use of knowledge: movements, activities, and actions. Basically, a movement is "... a space-time trajectory in some configuration space." (as defined in [4]) whose recognition is independent of contextual information, except for viewing conditions. An activity is a sequence of movements that can be recognized based on the statistical properties of their temporal relationships.
Reference: [5] <author> Aaron F. Bobick, Stephen Intille, Jim Davis, Freedom Baird, Claudio Pinhanez, Lee Campbell, Yuri Ivanov, Arjan Schutte, and Andy Wilson. </author> <title> The KidsRoom: A perceptually-based interactive and immersive story environment. </title> <type> Technical Report 398, </type> <institution> M.I.T. Media Laboratory Perceptual Computing Section, </institution> <month> November </month> <year> 1996. </year>
Reference-contexts: In this particular application | as well as in other interactive, immersive spaces like <ref> [5] </ref> | the context is largely set up by the actions of the computerized characters, justifying thus that the context for action recognition and the computer behavior to be unified in a sole structure or script. <p> This is an considerable improvement over vision systems based on background subtraction used before in many previous interactive environments <ref> [14, 5, 24] </ref>, since it enables lighting change, an important dramatic element. The segmentation part of the vision system runs currently at 15 frames per second in a SGI Indy 5000 (although during the performances of It/I it ran at 8 frames/second). <p> Because the recognition of this type of gesture has already been demonstrated in computer vision (see [7]) and in real-time environments <ref> [5] </ref>, we believe that there are no real technical obstacles for a fully autonomous run of the play. 4 From Tracking to Action Recognition As described above, the vision module outputs only the position of the actor and the occurrence of some pre-defined gestures. <p> Basically, our approach relies on setting up a situation where only some actions are expected, each of them with different spatial-temporal signatures. The idea has been used before in, among others, The KidsRoom <ref> [5] </ref>, an interactive story-based bedroom for children. <p> We see It/I as part of a continuing work of understanding and developing technology for story-based, interactive, immersive environments. which started with SingSong [23], followed by The KidsRoom <ref> [5] </ref>, and after It/I, by PAT, a virtual aerobics personal trainer [8]. To our knowledge, It/I is the first play ever produced involving a character automatically controlled by a computer that was truly interactive.
Reference: [6] <author> M. Cecelia Buchanan and Polle T. Zellweger. </author> <title> Automatic temporal layout mechanisms. </title> <booktitle> In Proc. of ACM Multimedia'93, </booktitle> <pages> pages 341-350, </pages> <address> Ahaheim, California, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: Interval scripts have been first proposed by Pin-hanez, Mase, and Bobick in [23], but in It/I we employed a new, revised, and improved version of the concept. Previous scripting languages (for example, Director [13], or <ref> [6, 9] </ref>) lack appropriate ways to represent the duration and complexity of human action in immersive environments: hidden in the structure is an assumption that actions are pin-point events in time (coming from the typical point-and-click interfaces those languages are designed for) or a simple sequence of basic commands.
Reference: [7] <author> James W. Davis and A. Bobick. </author> <title> The representation and recognition of human movement using temporal templates. </title> <booktitle> In Proc. of CVPR'97, </booktitle> <pages> pages 928-934, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: In many ways, the understanding of the world by the computer character | named It | reflects the state-of-art of real-time automatic vision: the character's reaction is mostly based on tracking I's movements and position and on the recognition of some specific gestures (using techniques similar to <ref> [7] </ref>). However, the result of the low-level tracking system is considerably augmented by incorporating knowledge about the expected actions to happen in each moment of the play. Unlike other vision-based interactive systems (e.g. [14]), the actor's position and gestures are interpreted according to the current context provided by the script. <p> Smaller blobs are labeled as blocks only when isolated from the person's silhouette. The vision system is trained to recognize the five different gestures shown in fig. 5. To perform gesture recognition we employ a simplification of the technique described by Davis and Bobick in <ref> [7] </ref>. During the performances we also employed manual detection of some of the gestures that could not be detected by the system with enough reliability. <p> Because the recognition of this type of gesture has already been demonstrated in computer vision (see <ref> [7] </ref>) and in real-time environments [5], we believe that there are no real technical obstacles for a fully autonomous run of the play. 4 From Tracking to Action Recognition As described above, the vision module outputs only the position of the actor and the occurrence of some pre-defined gestures.
Reference: [8] <author> James W. Davis and Aaron F. Bobick. </author> <title> Virtual PAT: a virtual personal aerobics trainer. </title> <type> Technical Report 436, </type> <institution> M.I.T. Media Laboratory Perceptual Computing Section, </institution> <month> January </month> <year> 1998. </year>
Reference-contexts: We see It/I as part of a continuing work of understanding and developing technology for story-based, interactive, immersive environments. which started with SingSong [23], followed by The KidsRoom [5], and after It/I, by PAT, a virtual aerobics personal trainer <ref> [8] </ref>. To our knowledge, It/I is the first play ever produced involving a character automatically controlled by a computer that was truly interactive. We believe that this was possible only because a flexible scripting system was developed to describe the story interplay and the computer and human characters' actions.
Reference: [9] <author> Rei Hamakawa and Jun Rekimoto. </author> <title> Object composition and playback models for handling multimedia data. </title> <booktitle> In Proc. of ACM Multimedia'93, </booktitle> <pages> pages 273-281, </pages> <address> Ahaheim, California, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: Interval scripts have been first proposed by Pin-hanez, Mase, and Bobick in [23], but in It/I we employed a new, revised, and improved version of the concept. Previous scripting languages (for example, Director [13], or <ref> [6, 9] </ref>) lack appropriate ways to represent the duration and complexity of human action in immersive environments: hidden in the structure is an assumption that actions are pin-point events in time (coming from the typical point-and-click interfaces those languages are designed for) or a simple sequence of basic commands.
Reference: [10] <author> Yuri Ivanov, Aaron Bobick, and John Liu. </author> <title> Fast lighting independent background subtraction. </title> <booktitle> In Proc. of the IEEE Workshop on Visual Surveillance - VS'98, </booktitle> <pages> pages 49-55, </pages> <address> Bombay, India, </address> <month> January </month> <year> 1998. </year>
Reference-contexts: All the computer character's reactions during the play were based on the human actor's actions as detected by a 3-camera visual segmentation system invariant to lighting changes (based on <ref> [10] </ref>). The play It/I was written considering the sensory limitations of computer vision. The actions of the human character | called I | were restricted to those that the computer could recognize automatically through image processing and computer vision techniques. <p> In the performance setup we employed a frontal 3-camera stereo system able to segment the actor and the blocks and to compute a silhouette image that is used to track and recognize gestures. The stereo system, based on <ref> [10] </ref>, constructs off-line a depth map of the background | stage, backdrops, and screens. Based on the depth map, it is possible to determine in real-time whether a pixel in the central camera image belongs to the background or to the foreground, in spite of lighting or background screen changes.
Reference: [11] <author> Myron W. Krueger. </author> <title> Artificial Reality II. </title> <publisher> Addison-Wesley, </publisher> <year> 1990. </year>
Reference-contexts: In addition, It/I has a clear dramatic structure not present in most previous interactive, immersive systems as, for instance, the spaces designed by Krueger <ref> [11] </ref> and Sommerer and Mignonneau [24]. 2 System Architecture upper layer contains information specific to the computer character and the bottom layer is comprised of modules directly interacting with the actual input and output devices.
Reference: [12] <author> A. K. Mackworth. </author> <title> Consistency in networks of relations. </title> <journal> Artificial Intelligence, </journal> <volume> 8(1) </volume> <pages> 99-118, </pages> <year> 1977. </year>
Reference-contexts: In [21] the PNF-theory and algorithms are explained in detail in the context of action detection. 5.3 Run-Time Processing During run-time, the control cycle starts by gathering the state of all sensors and the previous PNF-state state of all intervals. An arc-consistency algorithm <ref> [12] </ref> is then run on the PNF-network, what, according to [21] determines an approximation of the minimal domain of the constraint network.
Reference: [13] <author> MacroMind Inc. Director's User Manual. </author> <year> 1990. </year>
Reference-contexts: Interval scripts have been first proposed by Pin-hanez, Mase, and Bobick in [23], but in It/I we employed a new, revised, and improved version of the concept. Previous scripting languages (for example, Director <ref> [13] </ref>, or [6, 9]) lack appropriate ways to represent the duration and complexity of human action in immersive environments: hidden in the structure is an assumption that actions are pin-point events in time (coming from the typical point-and-click interfaces those languages are designed for) or a simple sequence of basic commands.
Reference: [14] <author> Pattie Maes, Trevor Darrell, Bruce Blumberg, and Alex Pentland. </author> <title> The ALIVE system: Full-body interaction with autonomous agents. </title> <booktitle> In Proc. of the Computer Animation '95 Conference, </booktitle> <address> Geneva, Switzer-land, </address> <month> April </month> <year> 1995. </year>
Reference-contexts: However, the result of the low-level tracking system is considerably augmented by incorporating knowledge about the expected actions to happen in each moment of the play. Unlike other vision-based interactive systems (e.g. <ref> [14] </ref>), the actor's position and gestures are interpreted according to the current context provided by the script. <p> The above segment exemplifies the complexity of the interaction in a typical scene of It/I. The scenes have a complexity level that are quite beyond previous full-body interactive systems. For example, in ALIVE <ref> [14] </ref>, although the main character (the dog-like CG-character Silus) had quite a complex internal structure, the meaning of the user's gestures remains constant as the interaction proceeds. <p> This is an considerable improvement over vision systems based on background subtraction used before in many previous interactive environments <ref> [14, 5, 24] </ref>, since it enables lighting change, an important dramatic element. The segmentation part of the vision system runs currently at 15 frames per second in a SGI Indy 5000 (although during the performances of It/I it ran at 8 frames/second).
Reference: [15] <author> R. Mann, A. Jepson, and Jeffrey Siskind. </author> <title> The computational perception of scene dynamics. </title> <booktitle> In Proc. of Fourth European Conference in Computer Vision, </booktitle> <month> April </month> <year> 1996. </year>
Reference-contexts: Most attempts of performing action recognition have relied in choosing very restricted domains where the needed contextual knowledge can be codified without explicit inclusion of knowledge (for example, <ref> [15, 17] </ref>). Our approach in the It/I project differs from those because the action context is obtained from the story: different movements are recognized as different actions according to when they happen.
Reference: [16] <author> Itay Meiri. </author> <title> Combining qualitative and quantitative constraints in temporal reasoning. </title> <journal> Artificial Intelligence, </journal> <volume> 87(1-2):343-385, </volume> <month> November </month> <year> 1996. </year>
Reference-contexts: Events, tree-structures, and encapsulated actions and other basic elements from other scripting languages are subsumed by Allen's algebra temporal relationships <ref> [1, 16] </ref>. Therefore, with explicit declaration of temporal constraints, the interval script paradigm allows the description of complex relations that occur in real interaction, like parallel and mutually exclusive actions, and even causality. During the compilation of the interval script, those temporal constraints are pre-processed using Allen's path-consistency algorithm [1].
Reference: [17] <author> Hans-Hellmut Nagel. </author> <title> A vision of `vision and language' comprises action: An example from road traffic. </title> <journal> Artificial Intelligence Review, </journal> <volume> 8 </volume> <pages> 189-214, </pages> <year> 1995. </year>
Reference-contexts: Most attempts of performing action recognition have relied in choosing very restricted domains where the needed contextual knowledge can be codified without explicit inclusion of knowledge (for example, <ref> [15, 17] </ref>). Our approach in the It/I project differs from those because the action context is obtained from the story: different movements are recognized as different actions according to when they happen.
Reference: [18] <author> Ken Perlin and Athomas Goldberg. Improv: </author> <title> A system for scripting interactive actors in virtual worlds. </title> <booktitle> In Proc. of SIGGRAPH'96, </booktitle> <month> August </month> <year> 1996. </year>
Reference-contexts: In other words, actions | and thus, interaction | can not be fully described neither by events (as Director does), nor by simple tree-forking structures as proposed in <ref> [18, 2] </ref>, nor by straight encapsulation such as suggested by structured programming. We adopted Allen's interval algebra [1] as the temporal model of interval scripts. Temporal relationships between intervals can be described as disjunctions of Allen's primitives and easily incorporated into an interval script.
Reference: [19] <author> Claudio S. Pinhanez. </author> <booktitle> Computer theater. In Proc. of the Eighth International Symposium on Electronic Arts (ISEA'97), </booktitle> <address> Chicago, Illinois, </address> <month> September </month> <year> 1997. </year>
Reference-contexts: Pinhanez <ref> [19] </ref> details the concept of computer theater, the origins of the term, and related works. Our research in computer theater has concentrated on building automatic, story-aware computer-actors able to interact with human actors on camera-monitored stages. <p> Also, unlike some previous reactive autonomous characters like Silas [3] or NeuroBaby [25], the interaction script of It/I contains the story of the play and It's role in it. In this sense, It is more a computer-actor (as defined in <ref> [19] </ref>) than an autonomous creature. Specifically, It is pro-active in relation to a story, instead of reactive to human interaction or to internalized, creature-like goals [3].
Reference: [20] <author> Claudio S. Pinhanez and Aaron F. Bobick. </author> <title> Fast constraint propagation on specialized Allen networks and its application to action recognition and control. </title> <type> Technical report # 456, </type> <institution> M.I.T. Media Laboratory Perceptual Computing Section, </institution> <month> January </month> <year> 1998. </year>
Reference-contexts: As shown in fig. 3, the computer character control system is composed of one active element, the interaction manager, that process the interaction script (described in detail in <ref> [20] </ref>).
Reference: [21] <author> Claudio S. Pinhanez and Aaron F. Bobick. </author> <title> Human action detection using PNF propagation of temporal constraints. </title> <booktitle> In Proc. of CVPR'98, </booktitle> <pages> pages 898-904, </pages> <address> Santa Barbara, California, </address> <month> June </month> <year> 1998. </year>
Reference-contexts: During the compilation of the interval script, those temporal constraints are pre-processed using Allen's path-consistency algorithm [1]. But to achieve speed during run-time constraint propagation, the resulting network is converted into a PNF-valued constraint network called a PNF-network. In <ref> [21] </ref> the PNF-theory and algorithms are explained in detail in the context of action detection. 5.3 Run-Time Processing During run-time, the control cycle starts by gathering the state of all sensors and the previous PNF-state state of all intervals. <p> An arc-consistency algorithm [12] is then run on the PNF-network, what, according to <ref> [21] </ref> determines an approximation of the minimal domain of the constraint network. <p> The core mechanism is Pin-hanez and Bobick's PNF-propagation algorithm for action recognition that can be viewed as a fast specialization of general temporal constraint propagation (see <ref> [21] </ref>). The concept was tested in real performances of a computer theater play.
Reference: [22] <author> Claudio S. Pinhanez and Aaron F. Bobick. "It/I": </author> <title> A theater play featuring an autonomous computer graphics character. </title> <type> Technical report # 455, </type> <institution> M.I.T. Media Laboratory Perceptual Computing Section, </institution> <month> January </month> <year> 1998. </year>
Reference-contexts: This control model is a simplification of a more elaborate 3-layered architecture, called story-character-device architecture, or SCD, which we are developing considering more difficult problems of controlling stories in immersive environments (see <ref> [22] </ref>). As shown in fig. 3, the computer character control system is composed of one active element, the interaction manager, that process the interaction script (described in detail in [20]). <p> A complete description of the syntax and semantics of interval scripts is beyond the scope of this paper (see <ref> [22] </ref>). We opted instead to present here some examples which illustrate some of the main features of the paradigm, followed by a basic description of the temporal formalism used in interval scripts and its run-time processing. in the beginning of the first scene of It/I.
Reference: [23] <author> Claudio S. Pinhanez, Kenji Mase, and Aaron F. Bo-bick. </author> <title> Interval scripts: A design paradigm for story-based interactive systems. </title> <booktitle> In CHI'97, </booktitle> <pages> pages 287-294, </pages> <address> Atlanta, Georgia, </address> <month> March </month> <year> 1997. </year>
Reference-contexts: Interval scripts have been first proposed in <ref> [23] </ref>, and constitute a scripting paradigm rooted in the AI technique of temporal constraint propagation to enhance action recognition and context switching. <p> The 30-minute interaction between It and I is written as an interval script, a language for interaction based on the concept of time intervals and temporal relationships. Interval scripts have been first proposed by Pin-hanez, Mase, and Bobick in <ref> [23] </ref>, but in It/I we employed a new, revised, and improved version of the concept. <p> Moreover, we have also run successfully the system with non-actors from the audience who had quite diverse ways to move and act on the stage. We see It/I as part of a continuing work of understanding and developing technology for story-based, interactive, immersive environments. which started with SingSong <ref> [23] </ref>, followed by The KidsRoom [5], and after It/I, by PAT, a virtual aerobics personal trainer [8]. To our knowledge, It/I is the first play ever produced involving a character automatically controlled by a computer that was truly interactive.
Reference: [24] <author> Christa Sommerer and Laurent Mignonneau. </author> <title> Art as a living system. </title> <journal> Leonardo, </journal> <volume> 30(5), </volume> <month> October </month> <year> 1997. </year>
Reference-contexts: In addition, It/I has a clear dramatic structure not present in most previous interactive, immersive systems as, for instance, the spaces designed by Krueger [11] and Sommerer and Mignonneau <ref> [24] </ref>. 2 System Architecture upper layer contains information specific to the computer character and the bottom layer is comprised of modules directly interacting with the actual input and output devices. <p> This is an considerable improvement over vision systems based on background subtraction used before in many previous interactive environments <ref> [14, 5, 24] </ref>, since it enables lighting change, an important dramatic element. The segmentation part of the vision system runs currently at 15 frames per second in a SGI Indy 5000 (although during the performances of It/I it ran at 8 frames/second).
Reference: [25] <author> Naoko Tosa, Hideki Hashimoto, Kaoru Sezaki, Ya-suharu Kunii, Toyotoshi Yamada, Kotaro Sabe, Ryosuke Nishino, Hiroshi Harashima, and Fumio Ha-rashima. </author> <title> Network-based neuro-baby with robotic hand. </title> <booktitle> In Proc. of IJCAI'95 Workshop on Entertainment and AI/Alife, </booktitle> <address> Montreal, Canada, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: Also, unlike some previous reactive autonomous characters like Silas [3] or NeuroBaby <ref> [25] </ref>, the interaction script of It/I contains the story of the play and It's role in it. In this sense, It is more a computer-actor (as defined in [19]) than an autonomous creature.
References-found: 25

