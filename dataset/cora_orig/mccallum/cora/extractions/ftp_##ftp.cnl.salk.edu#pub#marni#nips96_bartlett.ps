URL: ftp://ftp.cnl.salk.edu/pub/marni/nips96_bartlett.ps
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00243.html
Root-URL: 
Email: marni@salk.edu  terry@salk.edu  
Title: Viewpoint invariant face recognition using independent component analysis and attractor networks  
Author: Marian Stewart Bartlett Terrence J. Sejnowski 
Address: La Jolla, CA 92037  La Jolla, CA 92037  
Affiliation: University of California San Diego The Salk Institute  University of California San Diego Howard Hughes Medical Institute The Salk Institute,  
Note: In:Neural Information Processing Systems-Natural and Synthetic, Vol. 9, M. Mozer, M. Jordan, T. Petsche, eds. MIT Press, Cambridge, MA. 1997. p. 817-823.  
Abstract: We have explored two approaches to recognizing faces across changes in pose. First, we developed a representation of face images based on independent component analysis (ICA) and compared it to a principal component analysis (PCA) representation for face recognition. The ICA basis vectors for this data set were more spatially local than the PCA basis vectors and the ICA representation had greater invariance to changes in pose. Second, we present a model for the development of viewpoint invariant responses to faces from visual experience in a biological system. The temporal continuity of natural visual experience was incorporated into an attractor network model by Hebbian learning following a lowpass temporal filter on unit activities. When combined with the temporal filter, a basic Hebbian update rule became a generalization of Griniasty et al. (1993), which associates temporally proximal input patterns into basins of attraction. The system acquired rep resentations of faces that were largely independent of pose.
Abstract-found: 1
Intro-found: 1
Reference: <author> Bartlett, M. Stewart, & Sejnowski, T., </author> <year> 1996. </year> <title> Unsupervised learning of invariant representations of faces through temporal association. </title> <journal> Computational Neuroscience: </journal> <note> Int. Rev. Neurobio. Suppl. 1 J.M Bower, </note> <editor> Ed., </editor> <publisher> Academic Press, </publisher> <address> San Diego, CA:317-322. </address>
Reference-contexts: The feedforward and lateral connections were trained successively. 2.1 Competitive Hebbian learning of temporal relationships The Competitive Learning Algorithm (Rumelhart & Zipser, 1985) was extended to include a temporal lowpass filter on output unit activities <ref> (Bartlett & Sejnowski, 1996) </ref>.
Reference: <author> Beymer, D. </author> <year> 1994. </year> <title> Face recognition under varying pose. </title> <booktitle> In Proceedings of the 1994 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. </booktitle> <address> Los Alami-tos, CA: </address> <publisher> IEEE Comput. Soc. Press: </publisher> <pages> 756-61. </pages>
Reference: <author> Bell, A. & Sejnowski, T., </author> <year> (1997). </year> <title> The independent components of natural scenes are edge filters. </title> <booktitle> Advances in Neural Information Processing Systems 9. </booktitle>
Reference: <author> Bell, A., & Sejnowski, T., </author> <year> 1995. </year> <title> An information Maximization approach to blind separation and blind deconvolution. </title> <journal> Neural Comp. </journal> <volume> 7: </volume> <pages> 1129-1159. </pages>
Reference-contexts: The sources are recovered by a matrix of learned filters, W , which produce statistically independent outputs, U . The weight matrix, W , was found through an unsupervised learning algorithm that maximizes the mutual information between the input and the output of a nonlinear transformation <ref> (Bell & Sejnowski, 1995) </ref>. This algorithm has proven successful for separating randomly mixed auditory signals (the cocktail party problem), and has recently been applied to EEG signals (Makeig et al., 1996) and natural scenes (see Bell & Sejnowski, this volume).
Reference: <author> Comon, P. </author> <year> 1994. </year> <title> Independent component analysis anew concept? Signal Processing 36 </title> <type> 287-314. Cottrell & Metcalfe, </type> <year> 1991. </year> <title> Face, gender and emotion recognition using Holons. </title> <booktitle> In Advances in Neural Information Processing Systems 3, </booktitle> <editor> D. Touretzky, (Ed.), </editor> <publisher> Morgan Kauf-man, </publisher> <address> San Mateo, CA: 564 - 571. </address>
Reference-contexts: Independent component analysis (ICA) is a generalization of principal component analysis (PCA), which decorrelates the higher-order moments of the input <ref> (Comon, 1994) </ref>. In a task such as face recogni tion, much of the important information is contained in the high-order statistics of the images.
Reference: <author> Foldiak, P. </author> <year> 1991. </year> <title> Learning invariance from transformation sequences. </title> <journal> Neural Comp. </journal> <volume> 3 </volume> <pages> 194-200. </pages>
Reference: <author> Griniasty, M., Tsodyks, M., & Amit, D. </author> <year> 1993. </year> <title> Conversion of temporal correlations between stimuli to spatial correlations between attractors. </title> <journal> Neural Comp. </journal> <volume> 5 </volume> <pages> 1-17. </pages>
Reference-contexts: This learning rule is a generalization of an attractor network learning rule that has been shown to associate random input patterns into basins of attraction based on serial position in the input sequence <ref> (Griniasty, Tsodyks & Amit, 1993) </ref>. <p> The following update rule was used for the activation V of unit i at time t from the lateral inputs <ref> (Griniasty, Tsodyks, & Amit, 1993) </ref>: V i (t + ffit) = h X i Where is a neural threshold and (x) = 1 for x &gt; 0, and 0 otherwise.
Reference: <author> Hasselmo M. Rolls E. Baylis G. & Nalwa V. </author> <year> 1989. </year> <title> Object-centered encoding by face-selective neurons in the cortex in the superior temporal sulcus of the monkey. </title> <journal> Experimental Brain Research 75(2) </journal> <pages> 417-29. </pages>
Reference-contexts: of the nonlinearity provides a measure of the variance of the original source (Tony Bell, personal communication). 2 Unsupervised Learning of Viewpoint Invariant Representations of Faces in an Attractor Network Cells in the primate inferior temporal lobe have been reported that respond selectively to faces despite substantial changes in viewpoint <ref> (Hasselmo, Rolls, Baylis, & Nalwa, 1989) </ref>. Some cells responded independently of viewing angle, whereas other cells gave intermediate responses between a viewer-centered and an object centered representation. This section addresses how a system can acquire such invariance to viewpoint from visual experience.
Reference: <author> Heeger, D. </author> <year> (1991). </year> <title> Nonlinear model of neural responses in cat visual cortex. Computational Models of Visual Processing, </title> <editor> M. Landy & J. Movshon, Eds. </editor> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Images were presented to the model in sequential order as the subject changed pose from left to right (Figure ??). The first layer is an energy model related to the output of V1 complex cells <ref> (Heeger, 1991) </ref>. The images were filtered by a set of sine and cosine Gabor filters at 4 spatial scales and 4 orientations at 255 spatial locations. Sine and cosine outputs were squared and summed.
Reference: <author> Makeig, S, Bell, AJ, Jung, T-P, and Sejnowski, </author> <title> TJ 1996. Independent component analysis of Electroencephalographic data, </title> <booktitle> In: Advances in Neural Information Processing Systems 8, </booktitle> <pages> 145-151. </pages>
Reference-contexts: This algorithm has proven successful for separating randomly mixed auditory signals (the cocktail party problem), and has recently been applied to EEG signals <ref> (Makeig et al., 1996) </ref> and natural scenes (see Bell & Sejnowski, this volume). The independent component images contained in the rows of U are shown in Figure ??. In contrast to the principal components, all 200 independent components were spatially local.
Reference: <author> Rhodes, P. </author> <year> 1992. </year> <title> The long open time of the NMDA channel facilitates the self-organization of invariant object responses in cortex. </title> <publisher> Soc. </publisher> <address> Neurosci. Abst. 18:740. </address>
Reference-contexts: Such lowpass temporal filters have been related to the time course of the modifiable state of a neuron based on the open time of the NMDA channel for calcium influx <ref> (Rhodes, 1992) </ref>. <p> We showed that a lowpass temporal filter on unit activities, which has been related to the time course of the modifiable state of a neuron <ref> (Rhodes, 1992) </ref>, cooperates with Hebbian learning to (1) increase the viewpoint invariance of responses to faces in a feedforward system, and (2) create basins of attraction in an attractor network which associate temporally proximal inputs.
Reference: <author> Rumelhart, D. & Zipser, D. </author> <year> 1985. </year> <title> Feature discovery by competitive learning. </title> <booktitle> Cognitive Science 9: </booktitle> <pages> 75-112. </pages>
Reference-contexts: The third stage of the model was an attractor network produced by lateral interconnections among all of the complex pattern units. The feedforward and lateral connections were trained successively. 2.1 Competitive Hebbian learning of temporal relationships The Competitive Learning Algorithm <ref> (Rumelhart & Zipser, 1985) </ref> was extended to include a temporal lowpass filter on output unit activities (Bartlett & Sejnowski, 1996).
Reference: <author> Turk, M., & Pentland, A. </author> <year> 1991. </year> <title> Eigenfaces for Recognition. </title> <journal> J. Cog. Neurosci. </journal> <volume> 3(1) </volume> <pages> 71-86. </pages>
Reference-contexts: Eigenfaces We compared the performance of the ICA representation to that of the PCA representation for recognizing faces across changes in pose. The PCA representation of a face consisted of its component coefficients, which was equivalent to the "Eigenface" principal components. representation <ref> (Turk & Pentland, 1991) </ref>. A test image was recognized by assigning it the label of the nearest of the other 199 images in Euclidean distance. Classification error rates for the ICA and PCA representations and for the original graylevel images are presented in Table ??.
Reference: <author> Wallis, G. & Rolls, E. </author> <year> 1996. </year> <title> A model of invariant object recognition in the visual system. </title> <type> Technical Report, </type> <institution> Oxford University Department of Experimental Psychology. </institution>
References-found: 14

