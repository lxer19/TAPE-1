URL: http://syseng.anu.edu.au/~jon/papers/knightcap.ps.gz
Refering-URL: http://forum.swarthmore.edu/~jay/learn-game/indexes/other-papers.html
Root-URL: 
Email: fJon.Baxter,Andrew.Tridgell,Lex.Weaverg@anu.edu.au  
Title: KnightCap: A chess program that learns by combining TD() with minimax search  
Author: Jonathan Baxter Andrew Tridgell Lex Weaver 
Date: November 27, 1997  
Address: Canberra 0200, Australia  Canberra 0200, Australia  Canberra 0200, Australia  
Affiliation: Department of Systems Engineering Australian National University  Department of Computer Science Australian National University  Department of Computer Science Australian National University  
Abstract: In this paper we present TDLeaf(), a variation on the TD() algorithm that enables it to be used in conjunction with minimax search. We present some experiments in which our chess program, KnightCap, used TDLeaf() to learn its evaluation function while playing on the Free Ineternet Chess Server (FICS, fics.onenet.net). It improved from a 1650 rating to a 2100 rating in just 308 games and 3 days of play. We discuss some of the reasons for this success and also the relationship between our results and Tesauro's results in backgammon. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. F. Beal and M. C. Smith. </author> <title> Learning Piece values Using Temporal Differences. </title> <journal> Journal of The International Computer Chess Association, </journal> <year> 1997. </year> <month> 15 </month>
Reference-contexts: KnightCap is an ongoing project with new features being added to its evaluation function continually. We use TDLeaf () and Internet play to tune the coefficients of these features As this paper was going to press we discovered <ref> [1] </ref> in which the same idea was used to tune the material values in a chess program that only operated with material values. The learnt values came out very close to the traditional chess values of 1:3:3;5:9 for pawn,knight,bishop,rook,queen, but were learnt by self-play rather than on-line play. <p> With KnightCap, we found self-play to be worse than on-line learning (see section 4), but KnightCap's performance was being measured against the on-line players, not against a fixed program as in <ref> [1] </ref>. The remainder of this paper is organised as follows. In section 2 we describe the TD () algorithm as it applies to games. The TDLeaf () algorithm is described in section 3. Some details of Knightcap are given in section 4.1. <p> After 600 games (twice as many as in the original FICS experiment), we played the resulting version against the good version that learnt on FICS for a further 100 games with the weight values fixed. The self-play version scored only 11% against the good FICS version. In <ref> [1] </ref> positive results using essentially TDLeaf and self-play (with some random move choice) were reported for only learning the material weights.
Reference: [2] <author> D. P. Bertsekas and J. N. Tsitsiklis. </author> <title> Neuro-Dynamic Programming. </title> <publisher> Athena Sci--entific, </publisher> <year> 1996. </year>
Reference-contexts: Unfortunately, there is no such guarantee if ~ J (; w) is non-linear [14], or if a (x t ) depends on w <ref> [2] </ref>. However, despite the lack of theoretical guarantees there have been many successful applications of the TD () algorithm [3]. 3 Minimax Search and TD () For most games, any action a taken in state x will lead to predetermined state which we will denote by x 0 a .
Reference: [3] <author> L. P. Kaelbling, M. L. Littman, and A. W. Moore. </author> <title> Reinforcement Learning: A Survey. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4:237285, </volume> <year> 1996. </year>
Reference-contexts: Unfortunately, there is no such guarantee if ~ J (; w) is non-linear [14], or if a (x t ) depends on w [2]. However, despite the lack of theoretical guarantees there have been many successful applications of the TD () algorithm <ref> [3] </ref>. 3 Minimax Search and TD () For most games, any action a taken in state x will lead to predetermined state which we will denote by x 0 a .
Reference: [4] <author> D. Levy and M. Newborn. </author> <title> How Computers Play Chess. </title> <editor> W. H. </editor> <publisher> Freeman and Co., </publisher> <year> 1990. </year>
Reference-contexts: Space limitations prevent a full explanation of all of the described features, an interested reader should be able find explanations in the widely available computer chess literature (see for example [5] and <ref> [4] </ref>) or by examining the source code: http://wwwsyseng.anu.edu.au/lsg. 4.1.1 Board representation This is where KnightCap differs most from other chess programs. The principal board representation used in KnightCap is the topieces array. This is an array of 32 bit words with one word for each square on the board.
Reference: [5] <author> T. A. Marsland and J. Schaeffer. </author> <title> Computers, Chess and Cognition. </title> <publisher> Springer Verlag, </publisher> <year> 1990. </year>
Reference-contexts: Space limitations prevent a full explanation of all of the described features, an interested reader should be able find explanations in the widely available computer chess literature (see for example <ref> [5] </ref> and [4]) or by examining the source code: http://wwwsyseng.anu.edu.au/lsg. 4.1.1 Board representation This is where KnightCap differs most from other chess programs. The principal board representation used in KnightCap is the topieces array.
Reference: [6] <author> A. Plaat, J. Schaeffer, W. Pijls, and A. de Bruin. </author> <title> Best-First Fixed-Depth Minmax Algorithms. </title> <journal> Artificial Intelligence, </journal> <volume> 87:255293, </volume> <year> 1996. </year>
Reference-contexts: The topieces array is also used to drive the move generator and obviates the need for a standard move generation function. 4.1.2 Search algorithm The basis of the search algorithm used in KnightCap is MTD (f) <ref> [6] </ref>. MTD (f) is a logical extension of the minimal-window alpha-beta search that formalises the placement of the minimal search window to produce what is in effect a bisection search over the evaluation space.
Reference: [7] <author> J. Pollack, A. Blair, and M. Land. </author> <title> Coevolution of a Backgammon Player. </title> <booktitle> In Proceedings of the Fifth Artificial Life Conference, </booktitle> <address> Nara, Japan, </address> <year> 1996. </year>
Reference-contexts: Many authors have discussed the peculiarities of backgammon that make it particularly suitable for Temporal Difference learning with self-play <ref> [10, 8, 7] </ref>.
Reference: [8] <author> N. Schraudolph, P. Dayan, and T. Sejnowski. </author> <title> Temporal Difference Learning of Position Evaluation in the Game of Go. </title> <editor> In J. Cowan, G. Tesauro, and J. Alspector, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <address> San Fransisco, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The parameters of the neural network were updated according to the TD () algorithm after each game. Although the results with backgammon are quite striking, there is lingering disappointment that despite several attempts, they have not been repeated for other board games such as othello, Go and chess <ref> [12, 15, 8] </ref>. Many authors have discussed the peculiarities of backgammon that make it particularly suitable for Temporal Difference learning with self-play [10, 8, 7]. <p> Many authors have discussed the peculiarities of backgammon that make it particularly suitable for Temporal Difference learning with self-play <ref> [10, 8, 7] </ref>. <p> If we assume that the agent chooses its actions according to some function a (x) of the current state 1 Randomizing move choice is another way of avoiding problems associated with self-play (this approach has been tried in Go <ref> [8] </ref>), but the advantage of the Internet is that more information is provided by the opponents play. 3 x (so that a (x) 2 A x ), the expected reward from each state x 2 S is given by J fl (x) := E x N jx r (x N );
Reference: [9] <author> R. Sutton. </author> <title> Learning to Predict by the Method of Temporal Differences. </title> <booktitle> Machine Learning, </booktitle> <address> 3:944, </address> <year> 1988. </year>
Reference-contexts: 1 Introduction Temporal Difference learning or TD (), first introduced by Sutton <ref> [9] </ref>, is an elegant algorithm for approximating the expected long term future cost (or cost-to-go) of a stochastic dynamical system as a function of the current state. The mapping from states to future cost is implemented by a parameterized function approximator such as a neural network.
Reference: [10] <author> G. Tesauro. </author> <title> Practical Issues in Temporal Difference Learning. </title> <booktitle> Machine Learning, </booktitle> <address> 8:257278, </address> <year> 1992. </year>
Reference-contexts: Many authors have discussed the peculiarities of backgammon that make it particularly suitable for Temporal Difference learning with self-play <ref> [10, 8, 7] </ref>.
Reference: [11] <author> G. Tesauro. </author> <title> TD-Gammon, a self-teaching backgammon program, achieves master-level play. </title> <booktitle> Neural Computation, </booktitle> <address> 6:215219, </address> <year> 1994. </year>
Reference-contexts: Perhaps the most remarkable success of TD () is Tesauro's TD-Gammon, a neural network backgammon player that was trained from scratch using TD () and simulated self-play. TD-Gammon is competitive with the best human backgammon players <ref> [11] </ref>. In TD-Gammon the neural network played a dual role, both as a predictor of the expected cost-to-go of the position and as a means to select moves.
Reference: [12] <author> S. Thrun. </author> <title> Learning to Play the Game of Chess. </title> <editor> In G. Tesauro, D. Touretzky, and T. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <address> San Fransisco, 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The parameters of the neural network were updated according to the TD () algorithm after each game. Although the results with backgammon are quite striking, there is lingering disappointment that despite several attempts, they have not been repeated for other board games such as othello, Go and chess <ref> [12, 15, 8] </ref>. Many authors have discussed the peculiarities of backgammon that make it particularly suitable for Temporal Difference learning with self-play [10, 8, 7].
Reference: [13] <author> A. Tridgell. </author> <title> KnightCapA parallel chess program on the AP1000+. </title> <booktitle> In Proceedings of the Seventh Fujitsu Parallel Computing Workshop, </booktitle> <address> Canberra, Asu-tralia, </address> <year> 1997. </year> <title> ftp://samba.anu.edu.au/tridge/knightcap pcw97.ps.gz. source code: </title> <address> http://wwwsysneg.anu.edu.au/lsg. </address>
Reference-contexts: Some details on the methodology used and parallelism results obtained are available in <ref> [13] </ref>. The results given in this paper were obtained using a single CPU machine. 4.1.9 Evaluation function The heart of any chess program is its evaluation function. KnightCap uses a quite slow evaluation function that evaluates a number of quite computationally expensive features.
Reference: [14] <author> J. N. Tsitsikilis and B. V. Roy. </author> <title> An Analysis of Temporal Difference Learning with Function Approximation. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 42(5):674 690, </volume> <year> 1997. </year>
Reference-contexts: Provided the actions a (x t ) are independent of the parameter vector w, it can be shown that for linear ~ J (; w), the TD () algorithm converges to a near-optimal parameter vector <ref> [14] </ref>. Unfortunately, there is no such guarantee if ~ J (; w) is non-linear [14], or if a (x t ) depends on w [2]. <p> Provided the actions a (x t ) are independent of the parameter vector w, it can be shown that for linear ~ J (; w), the TD () algorithm converges to a near-optimal parameter vector <ref> [14] </ref>. Unfortunately, there is no such guarantee if ~ J (; w) is non-linear [14], or if a (x t ) depends on w [2]. <p> We are currently investigating how well TDLeaf () works for Backgammon, with a view to understanding the differences between chess and backgammon and how they affect learning. On the theoretical side, it has recently been shown that TD () converges for linear evaluation functions <ref> [14] </ref>. An interesting avenue for further investigation would be to determine whether TDLeaf () has similar convergence properties.
Reference: [15] <author> S. Walker, R. Lister, and T. Downs. </author> <title> On Self-Learning Patterns in the Othello Board Game by the Method of Temporal Differences. </title> <editor> In C. Rowles, H. liu, and N. Foo, editors, </editor> <booktitle> Proceedings of the 6th Australian Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 328333, </pages> <address> Melbourne, 1993. </address> <publisher> World Scientific. </publisher> <pages> 16 </pages>
Reference-contexts: The parameters of the neural network were updated according to the TD () algorithm after each game. Although the results with backgammon are quite striking, there is lingering disappointment that despite several attempts, they have not been repeated for other board games such as othello, Go and chess <ref> [12, 15, 8] </ref>. Many authors have discussed the peculiarities of backgammon that make it particularly suitable for Temporal Difference learning with self-play [10, 8, 7].
References-found: 15

