URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P398.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts93.htm
Root-URL: http://www.mcs.anl.gov
Title: The Message Passing Version of the Parallel Community Climate Model  
Author: J. B. Drake, R. E. Flanery, D. W. Walker, P. H. Worley I. T. Foster, J. G. Michalakes, R. L. Stevens J. J. Hack, D. L. Williamson 
Address: 1  2  3  
Affiliation: Oak Ridge National Laboratory  Argonne National Laboratory  National Center for Atmospheric Research  
Date: 1993, 500-513.  
Note: in Parallel Supercomputing in Atmospheric Science, eds. G-R. Hoff- man and T. Kauranne, World Scientific,  
Abstract: This paper is a brief overview of a parallel version of the NCAR Community Climate Model, CCM2, implemented for MIMD massively parallel computers using a message-passing programming paradigm. The parallel implementation was developed on an Intel iPSC/860 with 128 processors and on the Intel Delta with 512 processors, and the initial target platform for the production version of the code is the Intel Paragon with 2048 processors. Because the implementation uses a standard, portable message-passing library, the code can be easily ported to other multiprocessors supporting a message-passing programming paradigm, or run on machines distributed across a network. The parallelization strategy used is to decompose the problem domain into geographical patches and assign each processor to do the computation associated with a distinct subset of the patches. With this decomposition, the physics calculations involve only grid points and data local to a processor and are performed in parallel. Using parallel algorithms developed for the semi-Lagrangian transport, the fast Fourier transform and the Legendre transform, both physics and dynamics are computed in parallel with minimal data movement and modest change to the original CCM2 source code. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Anthes, </author> <title> 1986 summary of workshop on the NCAR Community Climate/Forecast models, </title> <journal> Bull. Amer. Meteor. Soc., </journal> <volume> 67 (1986), </volume> <pages> pp. 194-198. </pages>
Reference-contexts: Because of its widespread use, the model was designated a Community Climate Model (CCM). The advantages of the community model concept, in which many scientists use the same basic model for a variety of scientific studies, were demonstrated in workshops held at NCAR in July 1985 <ref> [1] </ref>, July 1987 [14], and July 1990 [15]. Fundamental strengths and weaknesses of the model have been identified at these workshops through the presentation of a diverse number of applications of the CCM. <p> To evaluate the spectral coefficients numerically, a fast Fourier transform (FFT) is used to find m () for any given . The Legendre transform is approximated using a Gaussian quadrature rule. Denoting the Gauss points in <ref> [1; 1] </ref> by j and the Gauss weights by w j , m J X m ( j )P m Here J is the number of Gauss points. (For simplicity, we will henceforth refer to (3) as the forward Legendre transform.) The point values are recovered from the spectral coefficients by
Reference: [2] <author> L. J. Bath, J. Rosinski, and J. Olson, </author> <title> Users' guide to NCAR CCM2, </title> <type> NCAR Tech. </type> <institution> Note NCAR/TN-379+IA, National Center for Atmospheric Research, Boulder, Colo., </institution> <year> 1992. </year>
Reference-contexts: The algorithms of the model are described in [7] and details of the implementation on the CRAY YMP are provided in a Users' guide <ref> [2] </ref>. The bulk of the effort in the NCAR Climate Modeling Section over the last several years has been to improve the physical representation of a wide range of key climate processes in the CCM, including clouds and radiation, moist convection, the planetary boundary layer, and transport. <p> First, however, we will briefly outline the performance of the code on the CRAY Y-MP. Reference will be made to some specific aspects of the model code and parameters. Rather than describing them in detail here, the reader is referred to the Users' Guide <ref> [2] </ref>. The hardware performance monitor on the CRAY Y-MP reports that the CCM2 running on a single processor averages 154 Mflops and can complete 0.25 timesteps per CPU second. When multitasked on 8 processors, it completes 1.86 timesteps per wallclock second.
Reference: [3] <author> Department of Energy, </author> <title> Building an advanced climate model: Progress plan for the CHAMMP climate modeling program, </title> <type> DOE Tech. Report DOE/ER-0479T, U.S. </type> <institution> Department of Energy, </institution> <address> Washington, D.C., </address> <month> December </month> <year> 1990. </year>
Reference-contexts: 1 Introduction The Computer Hardware, Advanced Mathematics, and Model Physics (CHAMMP) program <ref> [3] </ref> seeks to provide climate researchers with an advanced modeling capability for the study of global change issues. Current general circulation models are generally applied at coarse spatial resolution with minimal coupling between ocean, atmosphere and biosphere.
Reference: [4] <author> E. Eliasen, B. Machenhauer, and E. Rasmussen, </author> <title> On a numerical method for integration of the hydrodynamical equations with a spectral representation of the horizontal fields, </title> <type> Rep. No. 2, </type> <institution> Institut for Teoretisk Meteorologi, Kobenhavns Universitet, Denmark, </institution> <year> 1970. </year>
Reference-contexts: The new methods will be incorporated in future releases and versions of the model as seems appropriate for computer efficiency and the requirement for increased capabilities. 3 Parallel Algorithms Overview There are two major dynamics algorithms in the CCM2 code, the spectral transform method <ref> [4] </ref>, [8], [9] and a semi-Lagrangian transport method [13]. Aside from the dynamics there are numerous calculations that incorporate other physical processes such as clouds and radiation, moist convection, the planetary boundary layer and surface processes.
Reference: [5] <author> G. A. Geist, M. T. Heath, B. W. Peyton, and P. H. Worley, PICL: </author> <title> a portable instrumented communication library, C reference manual, </title> <type> Tech. Report ORNL/TM-11130, </type> <institution> Oak Ridge National Laboratory, Oak Ridge, TN, </institution> <month> July </month> <year> 1990. </year> <title> [6] , A users' guide to PICL: a portable instrumented communication library, </title> <type> Tech. Report ORNL/TM-11616, </type> <institution> Oak Ridge National Laboratory, Oak Ridge, TN, </institution> <month> August </month> <year> 1990. </year>
Reference-contexts: While the target machine for the parallel code is the Intel Paragon, the code uses PICL, a portable instrumented communication library <ref> [5] </ref>, [6], to implement interprocessor communication, providing a degree of portability and the ability to collect performance data. An experimental version of the code has also been implemented using PVM, the Parallel Virtual Machine, and has been used to run the code across a network of workstations.
Reference: [7] <author> J. J. Hack, B. A. Boville, B. P. Briegleb, J. T. Kiehl, P. J. Rasch, and D. L. Williamson, </author> <title> Description of the NCAR Community Climate Model (CCM2), </title> <type> NCAR Tech. </type> <institution> Note NCAR/TN-382+STR, National Center for Atmospheric Research, Boulder, Colo., </institution> <year> 1992. </year>
Reference-contexts: The most recent version of the CCM, CCM2, which was released to the NSF research community in October 1992, incorporates the most ambitious set of changes to date. The algorithms of the model are described in <ref> [7] </ref> and details of the implementation on the CRAY YMP are provided in a Users' guide [2].
Reference: [8] <author> B. Machenhauer, </author> <title> The spectral method, in Numerical Methods Used in Atmospheric Models, vol. II of GARP Pub. </title> <journal> Ser. </journal> <volume> No. 17. </volume> <publisher> JOC, World Meteorological Organization, </publisher> <address> Geneva, Switzerland, </address> <year> 1979, </year> <journal> ch. </journal> <volume> 3, </volume> <pages> pp. 121-275. </pages> <note> 14 DRAKE, ET AL </note>
Reference-contexts: The new methods will be incorporated in future releases and versions of the model as seems appropriate for computer efficiency and the requirement for increased capabilities. 3 Parallel Algorithms Overview There are two major dynamics algorithms in the CCM2 code, the spectral transform method [4], <ref> [8] </ref>, [9] and a semi-Lagrangian transport method [13]. Aside from the dynamics there are numerous calculations that incorporate other physical processes such as clouds and radiation, moist convection, the planetary boundary layer and surface processes.
Reference: [9] <author> S. A. Orszag, </author> <title> Transform method for calculation of vector-coupled sums: Application to the spectral form of the vorticity equation, </title> <institution> J. Atmos. Sci., </institution> <month> 27 </month> <year> (1970), </year> <pages> pp. 890-895. </pages>
Reference-contexts: The new methods will be incorporated in future releases and versions of the model as seems appropriate for computer efficiency and the requirement for increased capabilities. 3 Parallel Algorithms Overview There are two major dynamics algorithms in the CCM2 code, the spectral transform method [4], [8], <ref> [9] </ref> and a semi-Lagrangian transport method [13]. Aside from the dynamics there are numerous calculations that incorporate other physical processes such as clouds and radiation, moist convection, the planetary boundary layer and surface processes. These other processes share the common feature that they are coupled horizontally only through the dynamics. <p> To allow exact, unaliased transforms of quadratic terms the following relations are sufficient: J (3M + 1)=2, I = 2J , and N (m) = M <ref> [9] </ref>. Using N (m) = M is called a triangular truncation because the (m; n) indices of the spectral coefficients make up a triangular array. The examples in the rest of this section will assume a triangular truncation is used.
Reference: [10] <author> I. A. </author> <title> Stegun, Legendre functions, in Handbook of Mathematical Functions, </title> <editor> M. Abramowitz and I. A. Stegun, eds., </editor> <publisher> Dover Publications, </publisher> <address> New York, </address> <year> 1972, </year> <journal> ch. </journal> <volume> 8, </volume> <pages> pp. 332-353. </pages>
Reference-contexts: represent longitude and latitude. 6 DRAKE, ET AL Representations of the state variables in spectral space are the coefficients of an expansion in terms of complex exponentials and associated Legendre polynomials, (; ) = m=M n=jmj n P m where P m n () is the (normalized) associated Legendre function <ref> [10] </ref> and i = p The spectral coefficients are then determined by the equation m Z 1 2 0 n ()d j 1 n ()d (2) since the spherical harmonics P m n ()e im form an orthonormal basis for square integrable functions on the sphere.
Reference: [11] <author> R. A. van de Geijn, </author> <title> On global combine operations, </title> <note> LAPACK Working Note 29, </note> <institution> Computer Science Department, University of Tennessee, </institution> <address> Knoxville, TN 37996, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: But there are sophisticated techniques for calculating the vector sum that do a good job of minimizing both the communication cost and the associated parallel computation cost. Currently we use a variant of the recursive halving algorithm <ref> [11] </ref>. For the inverse transform, calculation of m ( j ) requires only spectral coefficients associated with wavenumber m, all of which are local to every processor in the corresponding processor column. Thus, no interprocessor communication is required in the inverse transform.
Reference: [12] <author> D. W. Walker, P. H. Worley, and J. B. Drake, </author> <title> Parallelizing the spectral transform method. Part II, </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 4 (1992), </volume> <pages> pp. 509-531. </pages>
Reference-contexts: Unlike the ring-pipeline algorithm, the vector sum algorithm, as implemented, does not permit the overlapping of communication and computation. But it is still fast and is more easily implemented in the context of CCM2 than is the ring-pipeline algorithm. The FFT can also be performed effectively in parallel <ref> [12] </ref> by exploiting the fact that there are multiple gridlines to be transformed at any one time, making it possible to hide some of the communication cost by overlapping the communication in one FFT with the computation in another. <p> The rest of this section describes in more detail the data decomposition and parallel algorithms for the Legendre transform and the semi-Lagrangian transport. (A detailed description of the parallel FFT can be found in <ref> [12] </ref>.) To motivate this discussion, important attributes of the spectral transform method are also reviewed. 3.1 The Spectral Transform Algorithm The spectral transform method is based on a dual representation of the scalar fields in terms of a truncated series of spherical harmonic functions and in terms of values on a <p> The Fourier domain can be regarded as a wavenumber-latitude grid, so, like the physical domain, the Fourier domain is two-dimensional. However, a different decomposition is used. The differences arise because of the way in which the FFT algorithm permutes the ordering of the output Fourier coefficients <ref> [12] </ref>. But, modulo this reordering, the wavenumber "dimension" is partitioned into P sets of consecutive wavenumbers, with each set being assigned to a different processor column. The partitioning function in the latitude direction is the same as in the physical domain. See Fig. 1 for an example decomposition.
Reference: [13] <author> D. L. Williamson and P. J. Rasch, </author> <title> Two-dimensional semi-Lagrangian transport with shape-preserving interpolation, </title> <journal> Mon. Wea. Rev., </journal> <volume> 117 (1989), </volume> <pages> pp. 102-129. </pages>
Reference-contexts: methods will be incorporated in future releases and versions of the model as seems appropriate for computer efficiency and the requirement for increased capabilities. 3 Parallel Algorithms Overview There are two major dynamics algorithms in the CCM2 code, the spectral transform method [4], [8], [9] and a semi-Lagrangian transport method <ref> [13] </ref>. Aside from the dynamics there are numerous calculations that incorporate other physical processes such as clouds and radiation, moist convection, the planetary boundary layer and surface processes. These other processes share the common feature that they are coupled horizontally only through the dynamics. <p> this redundant work has not proved to be an issue, and the vector sum has proved to be a viable parallel algorithm for PCCM2. 3.4 Semi-Lagrangian Transport As mentioned previously, the advection of moisture is done in CCM2 using a semi-Lagrangian transport (SLT) method in conjunction with shape preserving interpolation <ref> [13] </ref>. The method updates the value of the moisture field at a grid point (the arrival point, A) by first establishing a trajectory through which the particle arriving at A has moved during the current timestep (2t).
Reference: [14] <author> D. L. Williamson, Ed., </author> <title> Report of the second workshop on the community climate model, </title> <type> NCAR Tech Note NCAR/TN-310+PROC, </type> <institution> National Center for Atmospheric Research, Boulder, </institution> <address> CO 80307, </address> <year> 1988. </year> <note> [15] , CCM progress report - July 1990, NCAR Tech Note NCAR/TN-351+PPR, </note> <institution> National Center for Atmospheric Research, Boulder, CO, </institution> <month> July </month> <year> 1990. </year>
Reference-contexts: Because of its widespread use, the model was designated a Community Climate Model (CCM). The advantages of the community model concept, in which many scientists use the same basic model for a variety of scientific studies, were demonstrated in workshops held at NCAR in July 1985 [1], July 1987 <ref> [14] </ref>, and July 1990 [15]. Fundamental strengths and weaknesses of the model have been identified at these workshops through the presentation of a diverse number of applications of the CCM.
Reference: [16] <author> P. H. Worley and J. B. Drake, </author> <title> Parallelizing the spectral transform method, </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 4 (1992), </volume> <pages> pp. 269-291. </pages>
Reference-contexts: Earlier research on the spectral transform method established that efficient Legendre transforms could be performed using at least a couple of methods. For example, a highly efficient ring-pipeline algorithm was proposed <ref> [16] </ref> that overlaps communication and computation, giving a "scalable" parallel algorithm. For the initial implementation of PCCM2, a vector sum algorithm (described below) is used to calculate the Legendre transform. Unlike the ring-pipeline algorithm, the vector sum algorithm, as implemented, does not permit the overlapping of communication and computation.
References-found: 14

