URL: ftp://ftp.dcs.ex.ac.uk/pub/parallel/models/unify.ps.Z
Refering-URL: http://www.dcs.ex.ac.uk/reports/reports.html
Root-URL: 
Email: e-mail: dca@dcs.exeter.ac.uk  
Phone: Tel: +44 392 264063  
Title: Towards a Unified Parallel Architecture Class  
Author: Duncan K. G. Campbell 
Date: January 1994  
Address: Road Exeter EX4 4PT United Kingdom  
Affiliation: Department of Computer Science University of Exeter Prince of Wales  
Pubnum: Research Report 294  
Abstract-found: 0
Intro-found: 1
Reference: [BKTJ92] <author> Henri E. Bal, M. Frans Kaashoek, Andrew S. Tanenbaum, and Jack Jansen. </author> <title> Replication techniques for speeding up parallel applications on distributed systems. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 4(5) </volume> <pages> 337-355, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: What is required to achieve congruent simulation is a means of restricting the amount of bandwidth required, that is, restricting the amount (distance and/or frequency) of communication. An established technique for reducing communication traffic for DSM (hence improving its performance) is that of data-replication <ref> [MV84, BKTJ92, SZ90, NL91] </ref>. This technique is based around the replication of data on different processor-memory nodes.
Reference: [Cok91] <author> R. S. Cok. </author> <title> Parallel Programs for Transputers. </title> <publisher> Prentice Hall, </publisher> <year> 1991. </year>
Reference-contexts: the time of execution on the MIMD machine T 0 = the time of execution on the SIMD machine c = the maximum slowdown for the simulation 2.1.2 SIMD on MIMD The emulation of SIMD on MIMD was thought not to be particularly contentious and, in fact, be fairly straightforward <ref> [HLQA92, HS86, Cok91] </ref>. It is basically achieved by using the SPMD 9 style which is very common on MIMD machines. The SIMD program is replicated across the MIMD machine's processors and the data divided up equally between the processors. SIMD machines are (generally) synchronous while MIMD machines are (generally) asynchronous. <p> So, in order to compare similar machines differing only in memory, it was necessary to unify the tc xIMD and hypercuboid lc xIMD architecture classes because the hypercuboid architecture has logarithmic diameter. 2.2.1 SADM Congruently simulating tightly-coupled machines on loosely-coupled machines uses the technique known as Distributed Shared Memory <ref> [MV84, SZ90, RK91, NL91, Cok91, Rai92a, Rai92b, Rai90, RWC90] </ref>. The two basic global memory operations for a shared-memory machine are: global read and global write. These two operations form the basis of any other operations used to operate on the shared-memory.
Reference: [Col91] <author> Robert J. Collins. </author> <title> Multiple Instruction Multiple Data Emulation on the Connection Machine. </title> <type> Master's thesis, </type> <institution> Computer Science Department, UCLA, </institution> <month> February </month> <year> 1991. </year> <month> 8 </month>
Reference-contexts: was achieved by the technique of SPMD 7 . 2.1.1 MIMD on SIMD Danny Hillis [Hil85, HS86], and others, say that it is possible to emulate MIMD machines on SIMD machines by executing the MIMD instructions stored as data on the SIMD machine, while incurring only a constant overhead (see <ref> [Col91] </ref>). David Skillicorn accepts that an SIMD machine can simulate a MIMD machine with only constant slowdown, however, he does claim there to be some flaws with the process of executing instructions stored as data. The author, however, is inclined to agree with Danny Hillis on this point. <p> According to Marcus Wloka [Wlo91], a MIMD program of time complexity T can be simulated by a SIMD program in time T 0 &lt; cT , where c is a small constant. Robert Collins provides a fairly detailed description of the actual simulation in his Master's Thesis <ref> [Col91] </ref>. The simulation is performed by the MIMD machine being implemented by an interpreter that simulates one MIMD processor on each SIMD processing element (PE), so executing MIMD instructions on every MIMD processor in parallel.
Reference: [Hil85] <author> W. D. Hillis. </author> <title> The Connection Machine. </title> <publisher> MIT Press, </publisher> <year> 1985. </year>
Reference-contexts: The congruent simulation of MIMD on SIMD was achieved through the technique of having the SIMD machine interpret the MIMD machine's execution. The congruent simulation of SIMD on MIMD was achieved by the technique of SPMD 7 . 2.1.1 MIMD on SIMD Danny Hillis <ref> [Hil85, HS86] </ref>, and others, say that it is possible to emulate MIMD machines on SIMD machines by executing the MIMD instructions stored as data on the SIMD machine, while incurring only a constant overhead (see [Col91]).
Reference: [HLQA92] <author> P. J. Hatcher, A. J. Lapadula, M. J. Quinn, and R. J. Anderson. </author> <title> Compiling Data-Parallel Programs for MIMD Architectures. </title> <editor> In W. Joosen and E. Milgrom, editors, </editor> <title> Parallel Computing: from theory to sound practice. </title> <publisher> IOS Press, </publisher> <year> 1992. </year>
Reference-contexts: the time of execution on the MIMD machine T 0 = the time of execution on the SIMD machine c = the maximum slowdown for the simulation 2.1.2 SIMD on MIMD The emulation of SIMD on MIMD was thought not to be particularly contentious and, in fact, be fairly straightforward <ref> [HLQA92, HS86, Cok91] </ref>. It is basically achieved by using the SPMD 9 style which is very common on MIMD machines. The SIMD program is replicated across the MIMD machine's processors and the data divided up equally between the processors. SIMD machines are (generally) synchronous while MIMD machines are (generally) asynchronous.
Reference: [HQ91] <author> P. J. Hatcher and M. J. Quinn. </author> <title> Data-Parallel Programming on MIMD Computers. </title> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: In fact, the synchronisation can also be combined into the communication, thus hiding its costs. However, there is a single case where synchronisation must be introduced and can not be hidden. This is when there is a parallel loop containing an interaction point <ref> [HQ91] </ref>. The number of iterations of the loop 9 Single Program, Multiple Data stream. 4 may proceed independently on each processor, so some may complete before others.
Reference: [HS86] <author> W. D. Hillis and G. L. Steele. </author> <title> Data parallel algorithms. </title> <journal> Communications of the ACM, </journal> <volume> 29(2) </volume> <pages> 1170-1183, </pages> <year> 1986. </year>
Reference-contexts: The congruent simulation of MIMD on SIMD was achieved through the technique of having the SIMD machine interpret the MIMD machine's execution. The congruent simulation of SIMD on MIMD was achieved by the technique of SPMD 7 . 2.1.1 MIMD on SIMD Danny Hillis <ref> [Hil85, HS86] </ref>, and others, say that it is possible to emulate MIMD machines on SIMD machines by executing the MIMD instructions stored as data on the SIMD machine, while incurring only a constant overhead (see [Col91]). <p> the time of execution on the MIMD machine T 0 = the time of execution on the SIMD machine c = the maximum slowdown for the simulation 2.1.2 SIMD on MIMD The emulation of SIMD on MIMD was thought not to be particularly contentious and, in fact, be fairly straightforward <ref> [HLQA92, HS86, Cok91] </ref>. It is basically achieved by using the SPMD 9 style which is very common on MIMD machines. The SIMD program is replicated across the MIMD machine's processors and the data divided up equally between the processors. SIMD machines are (generally) synchronous while MIMD machines are (generally) asynchronous.
Reference: [MV84] <author> Kurt Mehlhorn and Uzi Vishkin. </author> <title> Randomized and deterministic simulations of PRAMs by parallel machines with restricted granularity of parallel memories. </title> <journal> Acta Informatica, </journal> <volume> 21 </volume> <pages> 339-374, </pages> <year> 1984. </year>
Reference-contexts: So, in order to compare similar machines differing only in memory, it was necessary to unify the tc xIMD and hypercuboid lc xIMD architecture classes because the hypercuboid architecture has logarithmic diameter. 2.2.1 SADM Congruently simulating tightly-coupled machines on loosely-coupled machines uses the technique known as Distributed Shared Memory <ref> [MV84, SZ90, RK91, NL91, Cok91, Rai92a, Rai92b, Rai90, RWC90] </ref>. The two basic global memory operations for a shared-memory machine are: global read and global write. These two operations form the basis of any other operations used to operate on the shared-memory. <p> What is required to achieve congruent simulation is a means of restricting the amount of bandwidth required, that is, restricting the amount (distance and/or frequency) of communication. An established technique for reducing communication traffic for DSM (hence improving its performance) is that of data-replication <ref> [MV84, BKTJ92, SZ90, NL91] </ref>. This technique is based around the replication of data on different processor-memory nodes.
Reference: [NL91] <author> B. Nitzberg and V. Lo. </author> <title> Distributed Shared Memory: a survey of issues and algorithms. </title> <journal> Computer, </journal> <volume> 24(8) </volume> <pages> 52-60, </pages> <year> 1991. </year>
Reference-contexts: So, in order to compare similar machines differing only in memory, it was necessary to unify the tc xIMD and hypercuboid lc xIMD architecture classes because the hypercuboid architecture has logarithmic diameter. 2.2.1 SADM Congruently simulating tightly-coupled machines on loosely-coupled machines uses the technique known as Distributed Shared Memory <ref> [MV84, SZ90, RK91, NL91, Cok91, Rai92a, Rai92b, Rai90, RWC90] </ref>. The two basic global memory operations for a shared-memory machine are: global read and global write. These two operations form the basis of any other operations used to operate on the shared-memory. <p> What is required to achieve congruent simulation is a means of restricting the amount of bandwidth required, that is, restricting the amount (distance and/or frequency) of communication. An established technique for reducing communication traffic for DSM (hence improving its performance) is that of data-replication <ref> [MV84, BKTJ92, SZ90, NL91] </ref>. This technique is based around the replication of data on different processor-memory nodes.
Reference: [Rai90] <author> Sanjay Raina. </author> <title> Software controlled shared virtual memory management on a Transputer based multiprocessor. </title> <editor> In D. L. Fielding, editor, </editor> <booktitle> Transputer Research and Applications 4, </booktitle> <pages> pages 143-152. </pages> <publisher> IOS Press, </publisher> <year> 1990. </year>
Reference-contexts: So, in order to compare similar machines differing only in memory, it was necessary to unify the tc xIMD and hypercuboid lc xIMD architecture classes because the hypercuboid architecture has logarithmic diameter. 2.2.1 SADM Congruently simulating tightly-coupled machines on loosely-coupled machines uses the technique known as Distributed Shared Memory <ref> [MV84, SZ90, RK91, NL91, Cok91, Rai92a, Rai92b, Rai90, RWC90] </ref>. The two basic global memory operations for a shared-memory machine are: global read and global write. These two operations form the basis of any other operations used to operate on the shared-memory.
Reference: [Rai92a] <author> Sanjay Raina. </author> <title> Performance study of the Data Diffusion Machine via Transputer emulation. </title> <type> Technical Report CSTR-92-35, </type> <institution> Department of Computer Science, University of Bristol, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: So, in order to compare similar machines differing only in memory, it was necessary to unify the tc xIMD and hypercuboid lc xIMD architecture classes because the hypercuboid architecture has logarithmic diameter. 2.2.1 SADM Congruently simulating tightly-coupled machines on loosely-coupled machines uses the technique known as Distributed Shared Memory <ref> [MV84, SZ90, RK91, NL91, Cok91, Rai92a, Rai92b, Rai90, RWC90] </ref>. The two basic global memory operations for a shared-memory machine are: global read and global write. These two operations form the basis of any other operations used to operate on the shared-memory.
Reference: [Rai92b] <author> Sanjay Raina. </author> <title> Virtual shared memory: A survey of techniques and systems. </title> <type> Technical Report CSTR-92-36, </type> <institution> Department of Computer Science, University of Bristol, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: classes of parallel architecture which now need to be addressed: tc xIMD 10 , hypercuboid lc xIMD, and sparse lc xIMD. 2.2 Unification of Memory This area concerned the simulation of tightly-coupled architectures on loosely-coupled architectures, and vice-versa, which is known as SADM and DASM, respectively, according to Sanjay Raina <ref> [Rai92b] </ref> who proposed the following taxomomy: SASM Single Address space, Shared Memory SADM Single Address space, Distributed Memory DADM Distributed Address space, Distributed Memory DASM Distributed Address space, Shared Memory. 10 Single or Multiple Instruction streams, Multiple Data streams. 5 Current switching technology means that tightly-coupled systems have switches of logarithmic <p> So, in order to compare similar machines differing only in memory, it was necessary to unify the tc xIMD and hypercuboid lc xIMD architecture classes because the hypercuboid architecture has logarithmic diameter. 2.2.1 SADM Congruently simulating tightly-coupled machines on loosely-coupled machines uses the technique known as Distributed Shared Memory <ref> [MV84, SZ90, RK91, NL91, Cok91, Rai92a, Rai92b, Rai90, RWC90] </ref>. The two basic global memory operations for a shared-memory machine are: global read and global write. These two operations form the basis of any other operations used to operate on the shared-memory.
Reference: [RK91] <author> U. Ramachandran and M. Y. A. Khalidi. </author> <title> An implementation of Distributed Shared Memory. </title> <journal> Software Practice and Experience, </journal> <volume> 21(5) </volume> <pages> 443-464, </pages> <year> 1991. </year>
Reference-contexts: So, in order to compare similar machines differing only in memory, it was necessary to unify the tc xIMD and hypercuboid lc xIMD architecture classes because the hypercuboid architecture has logarithmic diameter. 2.2.1 SADM Congruently simulating tightly-coupled machines on loosely-coupled machines uses the technique known as Distributed Shared Memory <ref> [MV84, SZ90, RK91, NL91, Cok91, Rai92a, Rai92b, Rai90, RWC90] </ref>. The two basic global memory operations for a shared-memory machine are: global read and global write. These two operations form the basis of any other operations used to operate on the shared-memory.
Reference: [RWC90] <author> Sanjay Raina, David H. D. Warren, and James Cowrie. </author> <title> Shared virtual memory management on Transputers via the Data Diffusion Machine. </title> <editor> In H. Zedan, editor, </editor> <booktitle> Real-Time Systems with Transputers, </booktitle> <pages> pages 322-330. </pages> <publisher> IOS Press, </publisher> <year> 1990. </year>
Reference-contexts: So, in order to compare similar machines differing only in memory, it was necessary to unify the tc xIMD and hypercuboid lc xIMD architecture classes because the hypercuboid architecture has logarithmic diameter. 2.2.1 SADM Congruently simulating tightly-coupled machines on loosely-coupled machines uses the technique known as Distributed Shared Memory <ref> [MV84, SZ90, RK91, NL91, Cok91, Rai92a, Rai92b, Rai90, RWC90] </ref>. The two basic global memory operations for a shared-memory machine are: global read and global write. These two operations form the basis of any other operations used to operate on the shared-memory.
Reference: [Ski90a] <author> D. B. Skillicorn. </author> <title> Architecture-independent parallel computation. </title> <type> Technical Report ISSN-0836-0227-90-268, </type> <institution> Department of Computing and Information Science, Queen's University, Kingston, </institution> <address> Ontario, </address> <month> March </month> <year> 1990. </year>
Reference-contexts: 1 Introduction David Skillicorn's four classes of parallel architecture (see <ref> [Ski90a, Ski90b, Ski91] </ref>) were taken as the basic parallel architecture classes for the unification process. These architectures are: SIMD 1 , tc 2 MIMD 3 , hypercuboid 4 lc 5 MIMD and sparse 6 lc MIMD.
Reference: [Ski90b] <author> D. B. Skillicorn. </author> <title> Practical concurrent programming for parallel machines. </title> <type> Technical Report ISSN-0836-0227-90-282, </type> <institution> Department of Computing and Information Science, Queen's University, Kingston, </institution> <address> Ontario, </address> <month> July </month> <year> 1990. </year>
Reference-contexts: 1 Introduction David Skillicorn's four classes of parallel architecture (see <ref> [Ski90a, Ski90b, Ski91] </ref>) were taken as the basic parallel architecture classes for the unification process. These architectures are: SIMD 1 , tc 2 MIMD 3 , hypercuboid 4 lc 5 MIMD and sparse 6 lc MIMD.
Reference: [Ski91] <author> D. B. Skillicorn. </author> <title> Practical parallel computation. </title> <type> Technical Report ISSN-0836-0227-91-312, </type> <institution> Department of Computing and Information Science, Queen's University, Kingston, </institution> <address> Ontario, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: 1 Introduction David Skillicorn's four classes of parallel architecture (see <ref> [Ski90a, Ski90b, Ski91] </ref>) were taken as the basic parallel architecture classes for the unification process. These architectures are: SIMD 1 , tc 2 MIMD 3 , hypercuboid 4 lc 5 MIMD and sparse 6 lc MIMD. <p> The SIMD class is traditionally programmed in a data-parallel style, MIMD in a control-parallel style, tc using shared variables, lc using message passing, etc. Two of his requirements for a model of parallel computation were also adopted; these are: architecture-independence and congruence <ref> [Ski92a, Ski92b, Ski91] </ref>. A model is said to be architecture-independent if it is general enough to model a range of architecture types [Ski92a, Ski92b, Ski91]. So, the source code written in terms of a particular parallel programming paradigm is implementable on various parallel architecture classes without modification. <p> Two of his requirements for a model of parallel computation were also adopted; these are: architecture-independence and congruence <ref> [Ski92a, Ski92b, Ski91] </ref>. A model is said to be architecture-independent if it is general enough to model a range of architecture types [Ski92a, Ski92b, Ski91]. So, the source code written in terms of a particular parallel programming paradigm is implementable on various parallel architecture classes without modification. For a model to be congruent it must reflect the real costs of execution at the model level [Ski92b].
Reference: [Ski92a] <author> D. B. Skillicorn. </author> <title> The Bird-Meertens Formalism as a parallel model. </title> <institution> Department of Computing and Information Science, Queen's University, Kingston, </institution> <month> June </month> <year> 1992. </year>
Reference-contexts: The SIMD class is traditionally programmed in a data-parallel style, MIMD in a control-parallel style, tc using shared variables, lc using message passing, etc. Two of his requirements for a model of parallel computation were also adopted; these are: architecture-independence and congruence <ref> [Ski92a, Ski92b, Ski91] </ref>. A model is said to be architecture-independent if it is general enough to model a range of architecture types [Ski92a, Ski92b, Ski91]. So, the source code written in terms of a particular parallel programming paradigm is implementable on various parallel architecture classes without modification. <p> Two of his requirements for a model of parallel computation were also adopted; these are: architecture-independence and congruence <ref> [Ski92a, Ski92b, Ski91] </ref>. A model is said to be architecture-independent if it is general enough to model a range of architecture types [Ski92a, Ski92b, Ski91]. So, the source code written in terms of a particular parallel programming paradigm is implementable on various parallel architecture classes without modification. For a model to be congruent it must reflect the real costs of execution at the model level [Ski92b].
Reference: [Ski92b] <author> D. B. Skillicorn. </author> <title> Parallelism and the Bird-Meertens Formalism. </title> <institution> Department of Computing and Information Science, Queen's University, Kingston, </institution> <month> April </month> <year> 1992. </year>
Reference-contexts: The SIMD class is traditionally programmed in a data-parallel style, MIMD in a control-parallel style, tc using shared variables, lc using message passing, etc. Two of his requirements for a model of parallel computation were also adopted; these are: architecture-independence and congruence <ref> [Ski92a, Ski92b, Ski91] </ref>. A model is said to be architecture-independent if it is general enough to model a range of architecture types [Ski92a, Ski92b, Ski91]. So, the source code written in terms of a particular parallel programming paradigm is implementable on various parallel architecture classes without modification. <p> Two of his requirements for a model of parallel computation were also adopted; these are: architecture-independence and congruence <ref> [Ski92a, Ski92b, Ski91] </ref>. A model is said to be architecture-independent if it is general enough to model a range of architecture types [Ski92a, Ski92b, Ski91]. So, the source code written in terms of a particular parallel programming paradigm is implementable on various parallel architecture classes without modification. For a model to be congruent it must reflect the real costs of execution at the model level [Ski92b]. <p> So, the source code written in terms of a particular parallel programming paradigm is implementable on various parallel architecture classes without modification. For a model to be congruent it must reflect the real costs of execution at the model level <ref> [Ski92b] </ref>. That is, for a model to be congruent over an architecture the execution cost visible to the programmer should be (asymptotically) the execution cost on that architecture.
Reference: [SZ90] <author> M. Stumm and S. Zhou. </author> <title> Algorithms implementing Distributed Shared Memory. </title> <journal> Computer, </journal> <volume> 23(5) </volume> <pages> 54-64, </pages> <year> 1990. </year>
Reference-contexts: So, in order to compare similar machines differing only in memory, it was necessary to unify the tc xIMD and hypercuboid lc xIMD architecture classes because the hypercuboid architecture has logarithmic diameter. 2.2.1 SADM Congruently simulating tightly-coupled machines on loosely-coupled machines uses the technique known as Distributed Shared Memory <ref> [MV84, SZ90, RK91, NL91, Cok91, Rai92a, Rai92b, Rai90, RWC90] </ref>. The two basic global memory operations for a shared-memory machine are: global read and global write. These two operations form the basis of any other operations used to operate on the shared-memory. <p> What is required to achieve congruent simulation is a means of restricting the amount of bandwidth required, that is, restricting the amount (distance and/or frequency) of communication. An established technique for reducing communication traffic for DSM (hence improving its performance) is that of data-replication <ref> [MV84, BKTJ92, SZ90, NL91] </ref>. This technique is based around the replication of data on different processor-memory nodes.
Reference: [Wlo91] <author> M. G. Wloka. </author> <title> Parallel VLSI Synthesis. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Brown University, </institution> <year> 1991. </year> <month> 9 </month>
Reference-contexts: This issue is discussed later, after the case without communication is introduced. According to Marcus Wloka <ref> [Wlo91] </ref>, a MIMD program of time complexity T can be simulated by a SIMD program in time T 0 &lt; cT , where c is a small constant. Robert Collins provides a fairly detailed description of the actual simulation in his Master's Thesis [Col91].
References-found: 21

