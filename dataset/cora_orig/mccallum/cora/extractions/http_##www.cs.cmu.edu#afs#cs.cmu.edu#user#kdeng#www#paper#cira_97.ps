URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/kdeng/www/paper/cira_97.ps
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/kdeng/www/papers.html
Root-URL: 
Email: kdeng@ri.cmu.edu awm@ri.cmu.edu nechyba@ri.cmu.edu  
Title: Learning to Recognize Time Series: Combining ARMA models with Memory-based Learning  
Author: Kan Deng Andrew W. Moore Michael C. Nechyba 
Address: Pittsburgh, PA 15213  
Affiliation: The Robotics Institute Carnegie Mellon University  
Abstract: For a given time series observation sequence, we can estimate the parameters of the AutoRegression Moving Average (ARMA) model, thereby representing a potentially long time series by a limited dimensional vector. In many applications, these parameter vectors will be separable into different groups, due to the different underlying mechanisms that generate differing time series. We can then use classification algorithms to predict the class of a new, uncategorized time series. For the purposes of a highly autonomous system, our approach to this classification uses memory-based learning and intensive cross-validation for feature and kernel selection. In an example application, we distinguish between driving data of a skilled, sober driver vs. a drunk driver, by calculating the ARMA model for the respective time series. In this paper, we first give a brief introduction to the theory of time series. We then discuss in detail our approach to time series recognition, using the ARMA model, and finish with experimental results. 
Abstract-found: 1
Intro-found: 1
Reference: [Akaike, 1971] <author> Akaike, H., </author> <title> Autoregressive model fitting for control, </title> <journal> Ann. Inst. Statist. Math. </journal> <volume> 23. </volume> <year> 1971. </year>
Reference: [Akaike, 1976] <author> Akaike, H., </author> <title> Canonical correlation analysis of time series and the use of an information criterion, in System Identification: Advances and Case Studies, </title> <editor> R.K.Mehra and D.G.Lainiotis, Eds., </editor> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1976. </year>
Reference-contexts: best for a certain domain consists of three steps, 1. selecting several typical time series samples from this domain; 2. for each possible combination of p and q (usually both p and q are no bigger than 10), calculating the value of AIC, which is a selection criteria proposed by <ref> [Akaike, 1976] </ref>, based on the time series samples selected; 3. The best p and q should correspond to the minimum AIC value. [Choi, 1992] To reconfirm the validation of ARMA (p,q) with specified p and q for a certain domain, we can use Portmanteau testing method, referring to [S-plus]. 2.
Reference: [Atkeson, Moore & Schaal, 1997] <author> Atkeson, C. G., Moore, A. W., and Schaal, S., </author> <title> Locally Weighted Learning, </title> <note> to appear in AI Review, </note> <year> 1997. </year>
Reference-contexts: Besides, it is costly to enumerate the distances from the query to all the data points in the memory. Therefore, we need more advanced techniques, such as k-nearest neighbor, kernel regression <ref> [Atkeson, Moore & Schaal, 1997] </ref>, locally weighted logistic regression [Deng & Moore, 1997], logistic regression-based classifier, etc., and more efficient memory retrieval mechanism. 4. Related models In this section, we discuss two other methods for time series classification: (1) Hidden Markov Models (HMMs) and (2) recurrent networks.
Reference: [Basseville & Nikiforov, 1993] <author> Basseville, M., and Nikiforov, I. </author> <title> Detection of Abrupt Changes: </title> <booktitle> Theory and Applications, </booktitle> <year> 1993. </year>
Reference-contexts: Non-parametric recognition researches include [Vandewalle & Moor, 1988], [Dellaert, Polzin & Waibel, 1996] and many more. In this paper, we explore the parametric approach using a well-established model called AutoRegression Moving Average, or briefly ARMA (p,q). ARMA model has been tried previously in the field of signal processing <ref> [Basseville & Nikiforov, 1993] </ref>, especially speech recognition [Makhoul, 1975]. Compared with HMM, neural networks and recurrent networks, ARMA (p,q) is specially good at modeling stationary and Gaussian-distributed time series. And since ARMA (p,q) is a linear model, it can require vastly less computation to estimate the coefficients of the model.
Reference: [Brockwell & Davis, 1991] <author> Brockwell, P.J, and Davis, R.A., </author> <title> Time Series: Theory and Methods, </title> <note> second edition, </note> <year> 1991. </year>
Reference-contexts: There are many cases of the violation of the stationarity and invertibility restrictions. Some of them can be easily pre-eliminated, and ARMA model is then still useful. Trend and seasonality are two examples, referring to <ref> [Brockwell & Davis, 1991] </ref> in the chapters about ARIMA model. More advanced explorations about non-stationary time series are summarized in [Tong, 1990]. Most music time series are not stationary and invertible, and they are more complicated than trend and seasonality. <p> Therefore, it is reasonable to represent a time series by its corresponding ARMA model parameters and . The estimation of and is often done by Maximum Likelihood method <ref> [Brockwell & Davis, 1991] </ref>. If several time series samples shares the same and , it means these samples are homogeneous. Suppose we have several car position time series samples, all the samples are generated by the same driver, the same car with the same road condition.
Reference: [Choi, 1992] <author> Choi, B. S., </author> <title> ARMA Model Identification, </title> <publisher> by Springer-Verlag New York, Inc. </publisher> <year> 1992. </year>
Reference-contexts: The best p and q should correspond to the minimum AIC value. <ref> [Choi, 1992] </ref> To reconfirm the validation of ARMA (p,q) with specified p and q for a certain domain, we can use Portmanteau testing method, referring to [S-plus]. 2. Collection of time series samples We collect numerous time series samples from this domain.
Reference: [Dellaert, Polzin & Waibel, 1996] <author> Dellaert, F., Polzin, T., and Waibel, A., </author> <title> Recognizing Emotion in Speech. </title> <booktitle> in ICSLP96 Conference Proceedings, </booktitle> <year> 1996. </year>
Reference-contexts: Non-parametric methods do not use models. Instead, given some time series samples, they extract the features of these samples, such as the mean values, variances, correlations and frequencies, and do the recognition job based on these features. Non-parametric recognition researches include [Vandewalle & Moor, 1988], <ref> [Dellaert, Polzin & Waibel, 1996] </ref> and many more. In this paper, we explore the parametric approach using a well-established model called AutoRegression Moving Average, or briefly ARMA (p,q). ARMA model has been tried previously in the field of signal processing [Basseville & Nikiforov, 1993], especially speech recognition [Makhoul, 1975].
Reference: [Dempster, Laird & Rubin, 1977] <author> Dempster, A. P., Laird, N.M., and Ru-bin, D. B., </author> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> in Journal of the Royal Statistical Society, </journal> <volume> B 39 (1), </volume> <year> 1977. </year>
Reference: [Deng & Moore, 1997] <author> Deng, K., and Moore, A.W., </author> <title> Locally Weighted Logistics for Classification, </title> <note> (in preparation). </note>
Reference-contexts: Besides, it is costly to enumerate the distances from the query to all the data points in the memory. Therefore, we need more advanced techniques, such as k-nearest neighbor, kernel regression [Atkeson, Moore & Schaal, 1997], locally weighted logistic regression <ref> [Deng & Moore, 1997] </ref>, logistic regression-based classifier, etc., and more efficient memory retrieval mechanism. 4. Related models In this section, we discuss two other methods for time series classification: (1) Hidden Markov Models (HMMs) and (2) recurrent networks.
Reference: [Elman, 1990] <author> Elman, J.L, </author> <title> Finding Structure in Time, </title> <booktitle> in Cognitive Science 14, </booktitle> <pages> pp. 179-211, </pages> <year> 1990. </year>
Reference: [Hannaford & Lee, 1991] <author> Hannaford, B., and Lee, P., </author> <title> Hidden Markov Model Analysis of Force/Torque Information in Telemanipulation, </title> <journal> in the International Journal of Robotics Research., </journal> <volume> Vol. 10, No. 5, </volume> <month> October </month> <year> 1991. </year>
Reference-contexts: We divide the time series recognition approaches into two classes: non-parametric and parametric ones. Parametric methods assume there is a model underlying the generation of the time series. The recognition of a time series is equivalent to the classification of the underlying models. HMM [Rabiner, 1989], <ref> [Hannaford & Lee, 1991] </ref> is such a model, [Nechyba & Xu, 1997] and [Pook & Ballard, 1993] used this model to do the time series recognitions in different domains.
Reference: [Kohn & Ansley, 1985] <author> Kohn, R., and Ansley, </author> <title> C.F. Estimation, prediction and interpolation for ARIMA models with missing data, </title> <type> technical report, </type> <institution> Graduate School of Business, University of Chicago. </institution> <year> 1985. </year>
Reference: [Makhoul, 1975] <author> Makhoul, J. </author> <title> Linear Prediction, </title> <booktitle> in Proceedings of IEEE, </booktitle> <volume> vol. 63, No. 4, </volume> <month> April </month> <year> 1975. </year>
Reference-contexts: In this paper, we explore the parametric approach using a well-established model called AutoRegression Moving Average, or briefly ARMA (p,q). ARMA model has been tried previously in the field of signal processing [Basseville & Nikiforov, 1993], especially speech recognition <ref> [Makhoul, 1975] </ref>. Compared with HMM, neural networks and recurrent networks, ARMA (p,q) is specially good at modeling stationary and Gaussian-distributed time series. And since ARMA (p,q) is a linear model, it can require vastly less computation to estimate the coefficients of the model.
Reference: [Nechyba & Xu, 1997] <author> Nechyba, M.C., and Xu, Y., </author> <title> Human Control Strategy: Abstraction, Verification and Replication, </title> <note> to appear in IEEE Control Systems Magazine, </note> <month> April, </month> <year> 1997. </year>
Reference-contexts: Parametric methods assume there is a model underlying the generation of the time series. The recognition of a time series is equivalent to the classification of the underlying models. HMM [Rabiner, 1989], [Hannaford & Lee, 1991] is such a model, <ref> [Nechyba & Xu, 1997] </ref> and [Pook & Ballard, 1993] used this model to do the time series recognitions in different domains. Other popular parametric models for time series are neural networks and recurrent networks [El-man, 1990], [Feldkamp, 1994], [Seawell & Kalman, 1995], [Nikovski, 1995]. Non-parametric methods do not use models.
Reference: [Nikovski, 1995] <author> Nikovski, D.N., </author> <title> Adaptive Computation Techniques for Time Series Analysis, A Master Degree Thesis, </title> <institution> Dept. Computer Science, Southern Illinois University. </institution> <month> July </month> <year> 1995. </year>
Reference-contexts: Other popular parametric models for time series are neural networks and recurrent networks [El-man, 1990], [Feldkamp, 1994], [Seawell & Kalman, 1995], <ref> [Nikovski, 1995] </ref>. Non-parametric methods do not use models. Instead, given some time series samples, they extract the features of these samples, such as the mean values, variances, correlations and frequencies, and do the recognition job based on these features.
Reference: [Pomerleau, 1995] <author> Pomerleau, D., RALPH: </author> <title> Rapidly Adapting Lateral Position Handler in 1995 IEEE Symposium on Intelligent Vehicle, </title> <address> De-troit, Michigan, U.S.A. </address> <year> 1995. </year>
Reference-contexts: Time series module, which does the estimation of the ARMA model and recognizes the time series based on the classification of the ARMA parameters. 3. Signal module, which gives the driver a proper signal, such as a blinking lamp, and/or a microphone. <ref> [Pomerleau, 1995] </ref> has done some impressive work in autonomous driving. We can use their navigation system to build our calibration module. Our main contribution will be the time series module. Finally, the signal module can be done by engineers in Detroit. The overall system is illustrated in Figure 3.
Reference: [Pook & Ballard, 1993] <author> Pook, P. K., and Ballard, D.H., </author> <title> Recognizing Te-leoperated Manipulations, </title> <journal> in IEEE Control Systems Magazine, </journal> <year> 1993. </year>
Reference-contexts: Parametric methods assume there is a model underlying the generation of the time series. The recognition of a time series is equivalent to the classification of the underlying models. HMM [Rabiner, 1989], [Hannaford & Lee, 1991] is such a model, [Nechyba & Xu, 1997] and <ref> [Pook & Ballard, 1993] </ref> used this model to do the time series recognitions in different domains. Other popular parametric models for time series are neural networks and recurrent networks [El-man, 1990], [Feldkamp, 1994], [Seawell & Kalman, 1995], [Nikovski, 1995]. Non-parametric methods do not use models.
Reference: [Puskorius & Feldkamp, 1994] <author> Puskorius, G.V., and Feldkamp, L.A., </author> <title> Neurocontrol of Nonlinear Dynamical Systems with Kalman Filter Trained Recurrent Networks, </title> <journal> in IEEE Transactions on Neural Networks, </journal> <volume> Vol. 5, No. 2, </volume> <month> March </month> <year> 1994. </year>
Reference: [Rabiner, 1989] <author> Rabiner, </author> <title> L.R., A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition, </title> <booktitle> in Proceedings of the IEEE, </booktitle> <volume> Vol. 77, No. 2, </volume> <month> Feb. </month> <year> 1989. </year>
Reference-contexts: We divide the time series recognition approaches into two classes: non-parametric and parametric ones. Parametric methods assume there is a model underlying the generation of the time series. The recognition of a time series is equivalent to the classification of the underlying models. HMM <ref> [Rabiner, 1989] </ref>, [Hannaford & Lee, 1991] is such a model, [Nechyba & Xu, 1997] and [Pook & Ballard, 1993] used this model to do the time series recognitions in different domains.
Reference: [Seawell & Kalman, 1995] <author> Seawell, </author> <title> T.I., and Kalman, B.L., Time Variability While Training a Parallel Neural Net Network, </title> <type> TR WUCS-95-27, </type> <institution> Dept. Computer Science, Washington University, </institution> <month> August </month> <year> 1995. </year>
Reference-contexts: HMM [Rabiner, 1989], [Hannaford & Lee, 1991] is such a model, [Nechyba & Xu, 1997] and [Pook & Ballard, 1993] used this model to do the time series recognitions in different domains. Other popular parametric models for time series are neural networks and recurrent networks [El-man, 1990], [Feldkamp, 1994], <ref> [Seawell & Kalman, 1995] </ref>, [Nikovski, 1995]. Non-parametric methods do not use models. Instead, given some time series samples, they extract the features of these samples, such as the mean values, variances, correlations and frequencies, and do the recognition job based on these features.
Reference: [S-plus] <institution> Statistical Sciences, </institution> <note> S-plus Guide to Statistical & Mathematical Analysis, Version 3.3, </note> <institution> Seattle: StatSci, a division of MathSoft, Inc. </institution> <year> 1995 </year>
Reference-contexts: The best p and q should correspond to the minimum AIC value. [Choi, 1992] To reconfirm the validation of ARMA (p,q) with specified p and q for a certain domain, we can use Portmanteau testing method, referring to <ref> [S-plus] </ref>. 2. Collection of time series samples We collect numerous time series samples from this domain. For each of them, we estimate the parameters and involved in the ARMA (p,q) model (the values of p and q are fixed for all the samples from a certain domain).
Reference: [Stoffer, 1986] <author> Stoffer, </author> <title> D.S., Estimation and Identification of Space-Time ARMAX Models in the Presence of Missing data. </title> <journal> Journal of the American Statistical Association, Sept. 1986, </journal> <volume> Vol. 81, No. 395, </volume> <booktitle> Theory and Methods. </booktitle>
Reference: [Tong, 1990] <author> Tong, H., </author> <title> Non-linear Time Series, A Dynamic System Approach, </title> <publisher> published by Clarendon Press Oxford, </publisher> <year> 1990. </year>
Reference-contexts: Some of them can be easily pre-eliminated, and ARMA model is then still useful. Trend and seasonality are two examples, referring to [Brockwell & Davis, 1991] in the chapters about ARIMA model. More advanced explorations about non-stationary time series are summarized in <ref> [Tong, 1990] </ref>. Most music time series are not stationary and invertible, and they are more complicated than trend and seasonality. Hence, we dont plan to apply our new approach to music recognition, though it is a very appealing topic. We started from distinguishing sober and drunk drivers. 3.
Reference: [Vandewalle & Moor, 1988] <author> Vandewalle, J.,and De Moor B., </author> <title> On the use of the singular value decomposition in identification and signal processing. </title> <booktitle> in the Proceeding of the workshop of NATO Advanced Study Institute on Numerical Linear Algebra, Digital Signal Processing and Parallel Algorithms, </booktitle> <address> Leuven, Belgium, </address> <month> Aug. </month> <year> 1988. </year> <title> 3. Signal module 2. Time series module 1. Calibration module Fig. 3: Driving performance monitoring and warning system. Fig. 4: ARMA(4,4) parameter clusters which distinguish driving performances from sober ones. Fig. 5: The same as Figure 6, except that the drunken series have been scaled to have equal variances as the sober ones. </title>
Reference-contexts: Non-parametric methods do not use models. Instead, given some time series samples, they extract the features of these samples, such as the mean values, variances, correlations and frequencies, and do the recognition job based on these features. Non-parametric recognition researches include <ref> [Vandewalle & Moor, 1988] </ref>, [Dellaert, Polzin & Waibel, 1996] and many more. In this paper, we explore the parametric approach using a well-established model called AutoRegression Moving Average, or briefly ARMA (p,q).
References-found: 24

