URL: http://www.cs.yale.edu/users/rasmussen/lib/papers/cvpr97.ps.gz
Refering-URL: http://www.cs.yale.edu/users/rasmussen/research.html
Root-URL: http://www.cs.yale.edu
Email: rasmuss@powered.cs.yale.edu, hager@cs.yale.edu  
Title: An Adaptive Model for Tracking Objects by Color  
Author: Christopher Rasmussen and Gregory D. Hager 
Address: 51 Prospect Street New Haven, CT 06520-8285  
Affiliation: Department of Computer Science Yale University  
Abstract: This paper discusses an adaptive approach to tracking objects based primarily on their color. By limiting our attention to areas of approximately uniform reflectance on nearly Lambertian surfaces, we can assume that pixels corresponding to surface patches on the object form a linear cluster in color space. Performing principal components analysis on user-selected sample pixels parametrizes an ellipsoidal model of this distribution that can be used to quickly and robustly track many objects through a range of orientations and scales. The apparent color and intensity of an object may change as it moves, though; we present a technique for tracking an object's color distribution through color space while tracking it in the image. Finally, by using weak spatial information we demonstrate successful tracking in the presence of similar-colored background distractors. In combination with a tracking failure recovery procedure, the basic technique has been successfully applied to tasks as diverse as head and hand tracking, following juggled and thrown balls, and constructing vision-based input devices. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. Allen, A. Timcenko, B. Yoshimi, and P. Michelman. </author> <title> Automated Tracking and Grasping of a Moving Object with a Robotic Hand-Eye System. </title> <journal> In IEEE Trans. Robotics and Automation, </journal> <volume> Vol. 9, No. 2, </volume> <pages> pp. 152-165, </pages> <year> 1993. </year>
Reference-contexts: This has led to it becoming the preferred method of performing tracking for many practical applications. Traditionally, blob tracking has been implemented on gray-scale images using thresholding or simple functions of gray-scale value <ref> [1, 2, 4, 12] </ref>. These techniques usually rely on a structured environment (e.g., a black backdrop) so that the target (e.g., a white ping-pong ball) is unique. With the advent of inexpensive color cameras and digitizers, it seems likely that color-based thresholding techniques will provide a practical and robust alternative.
Reference: [2] <author> R. Andersson. </author> <title> Understanding and Applying a Robot Ping-Pong Player's Expert Controller. </title> <booktitle> In ICRA, </booktitle> <pages> pp. 1284-1289, </pages> <year> 1989. </year>
Reference-contexts: This has led to it becoming the preferred method of performing tracking for many practical applications. Traditionally, blob tracking has been implemented on gray-scale images using thresholding or simple functions of gray-scale value <ref> [1, 2, 4, 12] </ref>. These techniques usually rely on a structured environment (e.g., a black backdrop) so that the target (e.g., a white ping-pong ball) is unique. With the advent of inexpensive color cameras and digitizers, it seems likely that color-based thresholding techniques will provide a practical and robust alternative. <p> Dynamic, dextrous robots that use visual input typically assume a simplified visual environment in order to achieve robustness and speed. The juggling robot of [12] and the ping-pong playing robot of <ref> [2] </ref>, for example, require a well-illuminated white ball and a black background for accurate segmentation. This limits the robots to performing only in specially-constructed workspaces with fixed camera views that minimize distraction.
Reference: [3] <author> M. Black and A. Jepson. EigenTracking: </author> <title> Robust Matching and Tracking of Articulated Objects Using a View-Based Representation. </title> <booktitle> In ECCV, </booktitle> <pages> pp. 329-342, </pages> <year> 1996. </year>
Reference-contexts: Geometry is prone to the problems (multiple object orientations and scales) that we are avoiding by using just color, of course, but some work has been done on this <ref> [3, 10] </ref>.
Reference: [4] <author> P. Corke. </author> <title> Visual Servoing. In Visual Control of Robot Manipulators A Review, </title> <editor> K. Hashimoto, ed., </editor> <publisher> World Scientific, </publisher> <year> 1993. </year>
Reference-contexts: This has led to it becoming the preferred method of performing tracking for many practical applications. Traditionally, blob tracking has been implemented on gray-scale images using thresholding or simple functions of gray-scale value <ref> [1, 2, 4, 12] </ref>. These techniques usually rely on a structured environment (e.g., a black backdrop) so that the target (e.g., a white ping-pong ball) is unique. With the advent of inexpensive color cameras and digitizers, it seems likely that color-based thresholding techniques will provide a practical and robust alternative.
Reference: [5] <author> Y. Du and J. Crisman. </author> <title> A Color Projection for Fast Generic Target Tracking. </title> <booktitle> In IROS, </booktitle> <pages> pp. 360-365, </pages> <year> 1995. </year>
Reference-contexts: With the advent of inexpensive color cameras and digitizers, it seems likely that color-based thresholding techniques will provide a practical and robust alternative. To date, little has been published on practical color tracking algorithms. A simple color tracking technique is presented in Du and Crisman <ref> [5] </ref>. They pick a set of arbitrary "categorical" colors in RGB space and construct membership volumes for each one based on nearest neighbor calculation. Objects are characterized by their histograms over these volumes, permitting multi-colored regions to be tracked.
Reference: [6] <author> G. Finlayson, B. Funt, and K. Barnard. </author> <title> Color Constancy Under Varying Illumination. </title> <booktitle> In ICCV, </booktitle> <pages> pp. 720-725, </pages> <year> 1995. </year>
Reference-contexts: Research on color constancy is addressed toward solving this problem, but has not advanced far enough to be applicable to most real-world situations <ref> [6] </ref>. We can often overcome this problem by exploit-ing an attribute of the tracking domain: as an object moves, its apparent color varies more or less continuously. This property is very different from how most color constancy problems are posed, as discontinuities in light composition falling on a fixed scene.
Reference: [7] <author> G. Hager. </author> <title> The `X-Vision' System: A General-Purpose Substrate for Vision-Based Robotics. </title> <booktitle> In Workshop on Vision for Robotics, </booktitle> <year> 1995. </year>
Reference-contexts: Pfinder depends on a static background for accurate segmentation, ruling out camera movement. In this paper, we describe a general-purpose technique for color blob tracking within the XVision real-time vision software package <ref> [7] </ref>. The primary attributes of our approach are that it does not depend on a fixed camera, it uses an empirical sampling of the color space and a physically plausible model of color variation, and it exhibits fast performance on standard workstations.
Reference: [8] <author> R. Kahn, M. Swain, P. Prokopowicz, and R. Firby. </author> <title> Gesture Recognition Using the Perseus Architecture. </title> <note> To appear in CVPR, </note> <year> 1996. </year>
Reference-contexts: Objects are characterized by their histograms over these volumes, permitting multi-colored regions to be tracked. It is unclear what update function is used to determine where to move the tracking window, and the results are based only on synthetic, abstract image sequences. Pfinder [17] and Perseus <ref> [8] </ref> are specialized systems for tracking people. Pfinder uses a statistical characterization of color variation in a static image to perform color-based change detection; Perseus uses color histograms (as one of a suite of "feature maps").
Reference: [9] <author> G. Klinker, S. Shafer, and T. Kanade. </author> <title> A Physical Approach to Color Image Understanding. </title> <journal> Int. Journal of Computer Vision, </journal> <volume> Vol. 4, </volume> <pages> pp. 7-38, </pages> <year> 1990. </year>
Reference-contexts: It is acceptable to be conservative and only classify part of the object as the object, as long as a large enough fraction is found to be distinguishable from noise. The theoretical basis for our segmentation algorithm is given by Klinker et al. <ref> [9] </ref>. They present a thorough analysis of the physics of object color properties in the context of an off-line approach to segmenting unknown colors in a scene.
Reference: [10] <author> H. Murase and S. Nayar. </author> <title> Visual Learning and Recognition of 3-D Objects from Appearance. </title> <journal> Int. Journal of Computer Vision, </journal> <volume> Vol. 14, </volume> <pages> pp. 5-24, </pages> <year> 1995. </year>
Reference-contexts: Geometry is prone to the problems (multiple object orientations and scales) that we are avoiding by using just color, of course, but some work has been done on this <ref> [3, 10] </ref>.
Reference: [11] <author> E. Oja. </author> <title> Subspace Methods of Pattern Recognition, </title> <publisher> Research Studies Press, </publisher> <year> 1983. </year>
Reference-contexts: However, by choosing objects with minimal specularity or choosing color sample locations on them away from specularities, we have found that a single tubal cluster often captures color variation very well. We model this cluster by doing principal components analysis (PCA) <ref> [11] </ref> on a user-chosen sample color distribution. The results give the tracker a bounding ellipsoid for determining pixel membership in the object's dominant color. This approach can easily be extended to handle objects composed of multiple uniform color patches.
Reference: [12] <author> A. Rizzi, L. Whitcomb, and D. Koditschek. </author> <title> Distributed Real-Time Control of a Spatial Robot Juggler. </title> <journal> IEEE Computer, </journal> <volume> Vol. 25, No. 5, </volume> <pages> pp. 12-23, </pages> <month> May, </month> <year> 1992. </year>
Reference-contexts: This has led to it becoming the preferred method of performing tracking for many practical applications. Traditionally, blob tracking has been implemented on gray-scale images using thresholding or simple functions of gray-scale value <ref> [1, 2, 4, 12] </ref>. These techniques usually rely on a structured environment (e.g., a black backdrop) so that the target (e.g., a white ping-pong ball) is unique. With the advent of inexpensive color cameras and digitizers, it seems likely that color-based thresholding techniques will provide a practical and robust alternative. <p> Dynamic, dextrous robots that use visual input typically assume a simplified visual environment in order to achieve robustness and speed. The juggling robot of <ref> [12] </ref> and the ping-pong playing robot of [2], for example, require a well-illuminated white ball and a black background for accurate segmentation. This limits the robots to performing only in specially-constructed workspaces with fixed camera views that minimize distraction.
Reference: [13] <author> J. Shi and C. Tomasi. </author> <title> Good Features to Track. </title> <booktitle> In CVPR, </booktitle> <pages> pp. 593-600, </pages> <year> 1994. </year>
Reference-contexts: No explicit information about the scale or orientation of O i is generated as a byproduct of the update cycle, as is the case with many correlation-based methods <ref> [13] </ref>. An approximation to such information can in principle be calculated by fitting an ellipse to CS i , but it is not guaranteed to be meaningful because of O i 's three-dimensionality.
Reference: [14] <author> D. Terzopoulos and T. Rabie. </author> <title> Animat Vision: </title> <booktitle> Active Vision in Artificial Animals. In ICCV, </booktitle> <pages> pp. 801-808, </pages> <year> 1995. </year>
Reference: [15] <author> K. Toyama and G. Hager. </author> <title> Incremental Focus of Attention for Robust Visual Tracking. </title> <note> To appear in CVPR, </note> <year> 1996. </year>
Reference-contexts: We call O i "lost" when the size of CS i falls below an aggregate image area threshold ff (ff = 1 here). A tracker can often recover from such events by resampling the entire image at a lower resolution and performing a global search <ref> [15] </ref>. Using 8 x 8 sub-sampling, a global search incurs about 5 times the computation of a normal tracking cycle. Though this causes the update rate to dip, we have found this to be a rare and therefore acceptable cost.
Reference: [16] <author> J. Tsotsos, S. Culhane, W. Wai, Y. Lai, N. Davis, F. Nuflo. </author> <title> Modeling Visual-Attention Via Selective Tuning. </title> <journal> AI, </journal> <volume> Vol. 78, No. </volume> <pages> 1-2, pp. 507-545, </pages> <month> October, </month> <year> 1995. </year>
Reference-contexts: As input to higher level processes, any of the weak-geometry color trackers that we have presented is well suited to being an attentional mechanism <ref> [16] </ref> that suggests where to perform more sophisticated analyses. The ADC algorithm, with its use of connected components, points toward what form such analyses might take.
Reference: [17] <author> C. Wren, A. Azarbayejani, T. Darrell, and A. Pentland. Pfinder: </author> <title> Real-Time Tracking of the Human Body. </title> <booktitle> In SPIE, </booktitle> <volume> Vol. 2615, </volume> <year> 1995. </year>
Reference-contexts: Objects are characterized by their histograms over these volumes, permitting multi-colored regions to be tracked. It is unclear what update function is used to determine where to move the tracking window, and the results are based only on synthetic, abstract image sequences. Pfinder <ref> [17] </ref> and Perseus [8] are specialized systems for tracking people. Pfinder uses a statistical characterization of color variation in a static image to perform color-based change detection; Perseus uses color histograms (as one of a suite of "feature maps"). <p> Those pixels which match the model are interpreted as belonging to the object and are called its support after <ref> [17] </ref>; the tracking window is repositioned to be centered on them. 3.1 Initialization Initial information about an object O i is supplied to the tracker through human agency. <p> ADC works well for tracking faces in many indoor lighting situations, both from fixed and mobile camera platforms (Pfinder <ref> [17] </ref> does not allow camera movement). We have had good results with mounting the camera on a pan-tilt unit, allowing the tracker to follow the subject's face over a wide range of poses, positions, and distances in a room.
References-found: 17

