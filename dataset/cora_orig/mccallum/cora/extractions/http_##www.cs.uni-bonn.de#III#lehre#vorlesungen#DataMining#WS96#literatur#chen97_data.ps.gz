URL: http://www.cs.uni-bonn.de/III/lehre/vorlesungen/DataMining/WS96/literatur/chen97:data.ps.gz
Refering-URL: http://www.cs.uni-bonn.de/III/lehre/vorlesungen/DataMining/WS96/
Root-URL: http://cs.uni-bonn.de
Title: Data Mining: An Overview from Database Perspective  
Author: Ming-Syan Chen Jiawei Han Philip S. Yu 
Keyword: Index Terms Data mining, knowledge discovery, association rules, classification, data clustering, pattern matching algorithms, data generalization and characterization, data cubes, multiple-dimensional databases.  
Note: J. Han was supported in part by the research grant NSERC-A3723 from the Natural Sciences and Engineering Research Council of Canada, the research grant NCE:IRIS/Precarn-HMI5 from the Networks of Centres of Excellence of Canada, and research grants from MPR Teltech Ltd. and Hughes Research Laboratories.  
Address: Taipei, Taiwan, ROC  B.C. V5A 1S6, Canada  Ctr. P.O.Box 704 Yorktown, NY 10598, U.S.A.  
Affiliation: Elect. Eng. Department National Taiwan Univ.  School of Computing Sci. Simon Fraser University  IBM T.J. Watson Res.  
Abstract: Mining information and knowledge from large databases has been recognized by many researchers as a key research topic in database systems and machine learning, and by many industrial companies as an important area with an opportunity of major revenues. Researchers in many different fields have shown great interest in data mining. Several emerging applications in information providing services, such as data warehousing and on-line services over the Internet, also call for various data mining techniques to better understand user behavior, to improve the service provided, and to increase the business opportunities. In response to such a demand, this article is to provide a survey, from a database researcher's point of view, on the data mining techniques developed recently. A classification of the available data mining techniques is provided and a comparative study of such techniques is presented. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Agrawal, C. Faloutsos, and A. Swami. </author> <title> Efficient Similarity Search in Sequence Databases. </title> <booktitle> Proceedings of the 4th Intl. conf. on Foundations of Data Organization and Algorithms, </booktitle> <address> Octo-ber, </address> <year> 1993. </year>
Reference-contexts: Examples of this type of database include: financial database for stock price index, medical databases, and multimedia databases, to name a few. Searching for similar patterns in a temporal or spatial-temporal database is essential in many data mining operations <ref> [1, 3, 56] </ref> in order to discover and predict the risk, causality, and trend associated with a specific pattern. <p> Significant progress has recently been made in sequence matching for temporal databases <ref> [1, 5, 28, 29, 54, 57] </ref> and for speech recognition techniques such as dynamic time warping [81]. <p> Significant progress has recently been made in sequence matching for temporal databases [1, 5, 28, 29, 54, 57] and for speech recognition techniques such as dynamic time warping [81]. Two types of similarity queries for temporal data have emerged thus far: whole matching <ref> [1] </ref> in which the target sequence and the sequences in the database have the same length; subsequence matching [29] in which the target sequence could be shorter than the sequences in the database and the match can occur at any arbitrary point. <p> Finally, different techniques have been explored to reduce the number of comparisons or search space during mining. 7.1 Similarity measures Different similarity measures have been considered, mainly the Euclidean distance <ref> [1, 29, 28] </ref> and the correlation [54]. The Euclidean distance between two sequences is defined as follows. Let fx i g be the target sequence and fy i g be a sequence in the database. <p> The denominators of equations (7.2) and (7.3) are equal in virtue of Parseval's Theorem <ref> [1] </ref>. If both fx i g and fy i g are properly normalized, the value of correlation coefficient c i is a similarity measure of two sequences and ranges from 1 to 1, where 1 indicates a perfect match. <p> In [46], the concept of mapping an object to a point in the feature space and then applying multidimensional indexing method to perform similarity search is explored. A fast whole matching method generalizing this idea to sequence matching is proposed in <ref> [1] </ref>, where the similarity between a stored sequence in the database and a target sequence is measured by the Euclidean distance between the features extracted from these two sequences in the Fourier domain.
Reference: [2] <author> R. Agrawal, S. Ghosh, T. Imielinski, B. Iyer, and A. Swami. </author> <title> An Interval Classifier for Database Mining Applications. </title> <booktitle> Proceedings of the 18th International Conference on Very Large Data Bases, </booktitle> <pages> pages 560-573, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Linear regression and linear discriminant analysis techniques are classical statistical models [26]. Methods have also been studied for scaling machine learning algorithms by combining base classifiers from partitioned data sets [18]. There have also 20 been some studies of classification techniques in the context of large databases <ref> [2, 10] </ref>. An interval classifier has been proposed in [2] to reduce the cost of decision tree generation. <p> Methods have also been studied for scaling machine learning algorithms by combining base classifiers from partitioned data sets [18]. There have also 20 been some studies of classification techniques in the context of large databases [2, 10]. An interval classifier has been proposed in <ref> [2] </ref> to reduce the cost of decision tree generation. The neural network approach for classification and rule extraction in databases has also been studied recently [55]. 5.2 Methods for performance improvement Most of the techniques developed in machine learning and statistics may encounter the problem of scaling-up. <p> For example, the interval classifier proposed in <ref> [2] </ref> uses database indices to improve only the efficiency of data retrieval but not the efficiency of classification since the classification algorithm itself is essentially an ID-3 algorithm. A direct integration of attribute-oriented induction with the ID-3 algorithm may help discovery of classification rules at high abstraction levels [40].
Reference: [3] <author> R. Agrawal, T. Imielinski, and A. Swami. </author> <title> Database Mining: A Performance Perspective. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <pages> pages 914-925, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: Examples of this type of database include: financial database for stock price index, medical databases, and multimedia databases, to name a few. Searching for similar patterns in a temporal or spatial-temporal database is essential in many data mining operations <ref> [1, 3, 56] </ref> in order to discover and predict the risk, causality, and trend associated with a specific pattern.
Reference: [4] <author> R. Agrawal, T. Imielinski, and A. Swami. </author> <title> Mining Association Rules between Sets of Items in Large Databases. </title> <booktitle> Proceedings of ACM SIGMOD, </booktitle> <pages> pages 207-216, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: In general, the kinds of knowledge which can be discovered in a database are categorized as follows. Mining association rules in transactional or relational databases has recently attracted a lot of attention in database communities <ref> [4, 7, 39, 57, 66, 73, 78] </ref>. <p> A mathematical model was proposed in <ref> [4] </ref> to address the problem of mining association rules. Let I=fi 1 , i 2 , :::, i m g be a set of literals, called items. Let D be a set of transactions, where each transaction T is a set of items such that T I. <p> It is often desirable to pay attention to only those rules which may have reasonably large support. Such rules with high confidence and strong support are referred to as strong rules in <ref> [4, 68] </ref>. The task of mining association rules is essentially to discover strong association rules in large databases. In [4, 7, 66], the problem of mining association rules is decomposed into the following two steps: 1. <p> Such rules with high confidence and strong support are referred to as strong rules in [4, 68]. The task of mining association rules is essentially to discover strong association rules in large databases. In <ref> [4, 7, 66] </ref>, the problem of mining association rules is decomposed into the following two steps: 1. Discover the large itemsets, i.e., the sets of itemsets that have transaction support above a pre-determined minimum support s. 2. Use the large itemsets to generate the association rules for the database. <p> Then, the resulting maximal reference sequences are AD; BG; ABE; and CGH. A maximal reference sequence corresponds to a frequently accessed pattern in an information providing service. It is noted that the problem of finding large reference sequences is similar to that of finding large itemsets for association rules <ref> [4] </ref> where a large itemset is a set of items appearing in a sufficient 31 number of transactions.
Reference: [5] <author> R. Agrawal, K.-I. Lin, H.S. Sawhney, and K. Shim. </author> <title> Fast Similarity Search in the Presence of Noise, Scaling, and Translation in Time-Series Databases. </title> <booktitle> Proceedings of the 21th International Conference on Very Large Data Bases, </booktitle> <pages> pages 490-501, </pages> <month> September </month> <year> 1995. </year>
Reference-contexts: Significant progress has recently been made in sequence matching for temporal databases <ref> [1, 5, 28, 29, 54, 57] </ref> and for speech recognition techniques such as dynamic time warping [81]. <p> This process is iterated until all of the features are exhausted. Compared to the method proposed in [29], HierarchyScan performs a hierarchical scan instead of using a tree structure for indexing. Different transformations were considered in [54]. In <ref> [5] </ref>, another approach is introduced to determine all similar sequences in a set of sequences. It is also applicable to find all subsequences similar to a target sequence. The similarity measure considered is the Euclidean distance between the sequences and the matching is performed in the time domain. <p> Quest is a data mining system developed at the IBM Almaden Research Center by Agrawal, et al. [6], which discovers various kinds of knowledge in large databases, including association rules [7, 78, 79], sequential patterns [8], classification rules [59], pattern matching and analysis <ref> [5] </ref>, etc. KEFIR is a knowledge discovery system developed at the GTE Labs by Piatetsky-Shapiro, et al. [68, 58] for the analysis of health care data.
Reference: [6] <author> R. Agrawal, M. Mehta, J. Shafer, R. Srikant, A. Arning, and T. Bollinger. </author> <title> The Quest data mining system. </title> <booktitle> In Proc. 1996 Int'l Conf. on Data Mining and Knowledge Discovery (KDD'96), </booktitle> <address> Portland, Oregon, </address> <month> August </month> <year> 1996. </year>
Reference-contexts: However, this introduction is by no means complete. Appendices are welcome, and a comprehensive overview of such systems is necessary. Quest is a data mining system developed at the IBM Almaden Research Center by Agrawal, et al. <ref> [6] </ref>, which discovers various kinds of knowledge in large databases, including association rules [7, 78, 79], sequential patterns [8], classification rules [59], pattern matching and analysis [5], etc.
Reference: [7] <author> R. Agrawal and R. Srikant. </author> <title> Fast Algorithms for Mining Association Rules in Large Databases. </title> <booktitle> Proceedings of the 20th International Conference on Very Large Data Bases, </booktitle> <pages> pages 478-499, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: In general, the kinds of knowledge which can be discovered in a database are categorized as follows. Mining association rules in transactional or relational databases has recently attracted a lot of attention in database communities <ref> [4, 7, 39, 57, 66, 73, 78] </ref>. <p> Such rules with high confidence and strong support are referred to as strong rules in [4, 68]. The task of mining association rules is essentially to discover strong association rules in large databases. In <ref> [4, 7, 66] </ref>, the problem of mining association rules is decomposed into the following two steps: 1. Discover the large itemsets, i.e., the sets of itemsets that have transaction support above a pre-determined minimum support s. 2. Use the large itemsets to generate the association rules for the database. <p> After the large itemsets are identified, the corresponding association rules can be derived in a straightforward manner. Efficient counting of large itemsets is thus the focus of most prior work. Here, algorithms Apriori and DHP, developed in <ref> [7] </ref> and [66] respectively, are described to illustrate the nature of this problem. 3.1 Algorithm Apriori and DHP Consider an example transaction database given in Figure 1 1 . <p> In each iteration (or each pass), Apriori constructs a candidate set of large itemsets, counts the number of occurrences of each candidate itemset, and then determines large itemsets based on a pre-determined minimum support <ref> [7] </ref>. In the first iteration, Apriori simply scans all the transactions to count the number of occurrences for each item. The set of candidate 1-itemsets, C 1 , obtained is shown in Figure 2. Assuming 1 This example database is extracted from [7]. 8 Scan D C 1 Itemset Sup. fAg <p> large itemsets based on a pre-determined minimum support <ref> [7] </ref>. In the first iteration, Apriori simply scans all the transactions to count the number of occurrences for each item. The set of candidate 1-itemsets, C 1 , obtained is shown in Figure 2. Assuming 1 This example database is extracted from [7]. 8 Scan D C 1 Itemset Sup. fAg 2 fCg 3 fEg 3 Itemset Sup. fAg 2 fCg 3 C 2 Itemset fA Bg fA Eg fB Eg Scan D C 2 Itemset Sup. fA Bg 1 fA Eg 1 fB Eg 3 L 2 Itemset Sup. fA Cg 2 <p> one or a set of classification attributes can be derived by applying a decision tree classifier [72] on the generalized data [42]; and association rules which associate a set of generalized attribute properties in a logic implication rule by integration of attribute-oriented induction and the methods for mining association rules <ref> [7, 39, 66, 78] </ref>. Moreover, statistical pattern discovery can also be performed using attribute-oriented induction [24]. <p> Appendices are welcome, and a comprehensive overview of such systems is necessary. Quest is a data mining system developed at the IBM Almaden Research Center by Agrawal, et al. [6], which discovers various kinds of knowledge in large databases, including association rules <ref> [7, 78, 79] </ref>, sequential patterns [8], classification rules [59], pattern matching and analysis [5], etc. KEFIR is a knowledge discovery system developed at the GTE Labs by Piatetsky-Shapiro, et al. [68, 58] for the analysis of health care data.
Reference: [8] <author> R. Agrawal and R. Srikant. </author> <title> Mining Sequential Patterns. </title> <booktitle> Proceedings of the 11th International Conference on Data Engineering, </booktitle> <pages> pages 3-14, </pages> <month> March </month> <year> 1995. </year> <month> 34 </month>
Reference-contexts: We note that both DHP and Apriori are iterative algorithms on the large itemset size in the sense that the large k-itemsets are derived from the large (k 1)-itemsets. These large itemset counting techniques are in fact applicable to dealing with other data mining problems <ref> [8, 19] </ref>. 3.2 Mining generalized and multiple-level association rules In many applications, interesting associations among data items often occur at a relatively high concept level. <p> Appendices are welcome, and a comprehensive overview of such systems is necessary. Quest is a data mining system developed at the IBM Almaden Research Center by Agrawal, et al. [6], which discovers various kinds of knowledge in large databases, including association rules [7, 78, 79], sequential patterns <ref> [8] </ref>, classification rules [59], pattern matching and analysis [5], etc. KEFIR is a knowledge discovery system developed at the GTE Labs by Piatetsky-Shapiro, et al. [68, 58] for the analysis of health care data.
Reference: [9] <author> K. K. Al-Taha, R. T. Snodgrass, and M. D. Soo. </author> <title> Bibliography on spatiotemporal databases. </title> <journal> ACM SIGMOD Record, </journal> <volume> 22(1) </volume> <pages> 59-67, </pages> <month> March </month> <year> 1993. </year>
Reference-contexts: Such clustering may facilitate taxonomy formation, which means the organization of observations into a hierarchy of classes that group similar events together. Temporal or spatial-temporal data constitutes a large portion of data stored in computers <ref> [9, 80] </ref>. Examples of this type of database include: financial database for stock price index, medical databases, and multimedia databases, to name a few.
Reference: [10] <author> T.M. Anwar, H.W. Beck, and S.B. Navathe. </author> <title> Knowledge Mining by Imprecise Querying: A Classification-Based Approach. </title> <booktitle> Proceedings of the 8th International Conference on Data Engineering, </booktitle> <pages> pages 622-630, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: Linear regression and linear discriminant analysis techniques are classical statistical models [26]. Methods have also been studied for scaling machine learning algorithms by combining base classifiers from partitioned data sets [18]. There have also 20 been some studies of classification techniques in the context of large databases <ref> [2, 10] </ref>. An interval classifier has been proposed in [2] to reduce the cost of decision tree generation.
Reference: [11] <author> N. Beckmann, H.-P. Kriegel, R. Schneider, and B. Seeger. </author> <title> The R*-tree: An efficient and robust access method for points and rectangles. </title> <booktitle> In Proc. 1990 ACM-SIGMOD Int. Conf. Management of Data, </booktitle> <pages> pages 322-331, </pages> <address> Atlantic City, NJ, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Data clustering identifies the sparse and the crowded places, and hence discovers the overall distribution patterns of the data set. Data clustering has been studied in statistics [18, 47], machine learning [31, 32], spatial database <ref> [11] </ref>, and data mining [18, 27, 62, 85] areas with different emphases. As a branch of statistics, clustering analysis has been studied extensively for many years, mainly focused on distance-based clustering analysis. <p> First, CLARANS assumes that the objects to be clustered are all stored in main memory. This assumption may not be valid for large databases and disk-based methods could be required. This drawback is alleviated by integrating CLARANS with efficient spatial access methods, like R*-tree <ref> [11] </ref>. R*-tree supports the focusing techniques that Ester et al. proposed to reduce the cost of implementing CLARANS. Ester et al. showed that the most computationally expensive step of CLARANS is calculating the total distances between the two clusters.
Reference: [12] <author> M. Bieber and J. Wan. </author> <title> Backtracking in a Multiple-Window Hypertext Environment. </title> <booktitle> ACM European Conf. on Hypermedia Technology, </booktitle> <pages> pages 158-166, </pages> <year> 1994. </year>
Reference-contexts: Capturing user access patterns in such environments is referred to as mining path traversal patterns. Note that although some efforts have been elaborated upon analyzing the user behavior <ref> [12, 15, 16] </ref>, mining path traversal patterns is still in its infancy.
Reference: [13] <author> R. Brachman and T. Anand. </author> <title> The process of knowledge discovery in databases: A human-centered approach. In U.M. </title> <editor> Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 37-58. </pages> <publisher> AAAI/MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: INLEN is a system, developed by Michalski, et al. [61], which integrates multiple learning paradigms in data mining. Explora is a multipattern and multistrategy discovery assistant developed by Klosgen [52]. IMACS is a data mining system developed at AT&T Laboratory by Brachman et al. <ref> [13] </ref>, using sophisticated knowledge representation techniques. DataMine is system exploring interactive ad-hoc query-directed data mining, developed by Imielinski, et al. [45]. IDEA, developed at AT&T Laboratory by Selfridge, et al. [74], performs interactive data explorations and analysis.
Reference: [14] <author> L. Breiman, J. Friedman, R. Olshen, and C. Stone. </author> <title> Classification of Regression Trees. </title> <publisher> Wadsworth, </publisher> <year> 1984. </year>
Reference-contexts: There are many other evaluation functions, such as Gini index, chi-square test, and so forth <ref> [14, 52, 68, 82] </ref>. For example, for Gini index [14, 59], if a data set T contains examples from n classes, gini (T ) is defined as, gini (T ) = 1 p 2 i : where p i is the relative frequency of class i in T . <p> There are many other evaluation functions, such as Gini index, chi-square test, and so forth [14, 52, 68, 82]. For example, for Gini index <ref> [14, 59] </ref>, if a data set T contains examples from n classes, gini (T ) is defined as, gini (T ) = 1 p 2 i : where p i is the relative frequency of class i in T .
Reference: [15] <author> E. Caramel, S. Crawford, and H. Chen. </author> <title> Browsing in Hypertext: A Cognitive Study. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 22(5) </volume> <pages> 865-883, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: Capturing user access patterns in such environments is referred to as mining path traversal patterns. Note that although some efforts have been elaborated upon analyzing the user behavior <ref> [12, 15, 16] </ref>, mining path traversal patterns is still in its infancy.
Reference: [16] <author> L. D. Catledge and J. E. Pitkow. </author> <title> Characterizing browsing strategies in the world-wide web. </title> <booktitle> Proceedings of the 3rd WWW Conference, </booktitle> <month> April </month> <year> 1995. </year>
Reference-contexts: Capturing user access patterns in such environments is referred to as mining path traversal patterns. Note that although some efforts have been elaborated upon analyzing the user behavior <ref> [12, 15, 16] </ref>, mining path traversal patterns is still in its infancy.
Reference: [17] <author> P. K. Chan and S. J. Stolfo. </author> <title> Learning arbiter and combiner trees from partitioned data for scaling machine learning. </title> <booktitle> Proc. 1st Int. Conf. on Knowledge Discovery and Data Mining (KDD'95), </booktitle> <pages> pages 39-44, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: The combination of these techniques enables it to scale for large data sets and classify data sets irrespective of the number of classes, attributes, and examples. An approach, called meta-learning, was proposed in <ref> [17] </ref>. In [17], methods to learn how to combine several base classifiers, which are learned from subsets of data, were developed. Efficient scaling-up to larger learning problems can hence be achieved. <p> The combination of these techniques enables it to scale for large data sets and classify data sets irrespective of the number of classes, attributes, and examples. An approach, called meta-learning, was proposed in <ref> [17] </ref>. In [17], methods to learn how to combine several base classifiers, which are learned from subsets of data, were developed. Efficient scaling-up to larger learning problems can hence be achieved.
Reference: [18] <author> P. Cheeseman and J. Stutz. </author> <title> Bayesian classification (AutoClass): Theory and results. In U.M. </title> <editor> Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 153-180. </pages> <publisher> AAAI/MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: Moreover, there are also approaches for tranform-ing decision trees into rules [72] and transforming rules and trees into comprehensive knowledge structures [34]. There have been many other approaches on data classification, including statistical approaches <ref> [18, 26, 68] </ref>, rough sets approach [87], etc. Linear regression and linear discriminant analysis techniques are classical statistical models [26]. Methods have also been studied for scaling machine learning algorithms by combining base classifiers from partitioned data sets [18]. <p> Linear regression and linear discriminant analysis techniques are classical statistical models [26]. Methods have also been studied for scaling machine learning algorithms by combining base classifiers from partitioned data sets <ref> [18] </ref>. There have also 20 been some studies of classification techniques in the context of large databases [2, 10]. An interval classifier has been proposed in [2] to reduce the cost of decision tree generation. <p> Given a large set of multidimensional data points, the data space is usually not uniformly occupied by the data points. Data clustering identifies the sparse and the crowded places, and hence discovers the overall distribution patterns of the data set. Data clustering has been studied in statistics <ref> [18, 47] </ref>, machine learning [31, 32], spatial database [11], and data mining [18, 27, 62, 85] areas with different emphases. As a branch of statistics, clustering analysis has been studied extensively for many years, mainly focused on distance-based clustering analysis. <p> Data clustering identifies the sparse and the crowded places, and hence discovers the overall distribution patterns of the data set. Data clustering has been studied in statistics [18, 47], machine learning [31, 32], spatial database [11], and data mining <ref> [18, 27, 62, 85] </ref> areas with different emphases. As a branch of statistics, clustering analysis has been studied extensively for many years, mainly focused on distance-based clustering analysis. <p> As a branch of statistics, clustering analysis has been studied extensively for many years, mainly focused on distance-based clustering analysis. Systems based on statistical classification methods, such as AutoClass <ref> [18] </ref> which uses a Bayesian classification method, have been used in clustering in real world databases with reported success. The distance-based approaches assume that all the data points are given in advance and can be scanned frequently. They are global or semi-global methods at the granularity of data points.
Reference: [19] <author> M.-S. Chen, J.-S. Park, and P. S. Yu. </author> <title> Data Mining for Path Traversal Patterns in a Web Environment. </title> <booktitle> Proceedings of the 16th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 385-392, </pages> <month> May 27-30 </month> <year> 1996. </year>
Reference-contexts: We note that both DHP and Apriori are iterative algorithms on the large itemset size in the sense that the large k-itemsets are derived from the large (k 1)-itemsets. These large itemset counting techniques are in fact applicable to dealing with other data mining problems <ref> [8, 19] </ref>. 3.2 Mining generalized and multiple-level association rules In many applications, interesting associations among data items often occur at a relatively high concept level. <p> This technique is called scan-reduction. In <ref> [19] </ref>, the technique of scan-reduction was utilized and shown to result in prominent performance improvement. Clearly, such a technique is applicable to both Apriori and DHP. 12 3.4.2 Sampling: Mining with adjustable accuracy Several applications require mining the transaction data to capture the customer behavior in a very frequent basis. <p> Note that, as pointed out in <ref> [19] </ref>, since users are traveling along the information providing services to search for the desired information, some objects are visited because of their locations rather than their content, showing the very difference between the traversal pattern problem and others which are mainly based on customer transactions. <p> However, as these information providing services are becoming increasingly popular nowadays, there is a growing demand for capturing user travelling behavior and improving the quality of such services. In response to this demand, the problem of mining traversal patterns was explored in <ref> [19] </ref>, where the proposed solution procedure consists of two steps. First, an algorithm was devised to convert the original sequence of log data into a set of traversal subsequences. Each traversal subsequence represents a maximal forward reference from the starting point of a user access.
Reference: [20] <author> M.-S. Chen and P. S. Yu. </author> <title> Using Multi-Attribute Predicates for Mining Classification Rules. </title> <institution> IBM Research Report, </institution> <year> 1995. </year>
Reference-contexts: This is particularly true in the presence of those attributes that have strong inference among themselves. In view of this, a two-phase method for multi 21 attribute extraction was devised in <ref> [20] </ref> to improve the efficiency of deriving classification rules in a large training dataset. A feature that is useful in inferring the group identity of a data tuple is said to have a good inference power to that group identity.
Reference: [21] <author> D.W. Cheung, J. Han, V. Ng, and C.Y. Wong. </author> <title> Maintenance of discovered association rules in large databases: An incremental updating technique. </title> <booktitle> In Proc. 1996 Int'l Conf. on Data Engineering, </booktitle> <address> New Orleans, Louisiana, </address> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: A database may allow frequent or occasional updates and such updates may not only invalidate some existing strong association rules but also turn some weak rules into strong ones. Thus it is nontrivial to maintain such discovered rules in large databases. An incremental updating technique is developed in <ref> [21] </ref> for efficient maintenance of discovered association rules in transaction databases with data insertion.
Reference: [22] <author> C. Clifton and D. Marks. </author> <title> Security and privacy implications of data mining. </title> <booktitle> In Proc. 1996 SIG-MOD'96 Workshop on Research Issues on Data Mining and Knowledge Discovery (DMKD'96), </booktitle> <pages> pages 15-20, </pages> <address> Montreal, Canada, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: For example, the handling of different types of data are confined to relational and transactional data, and the methods for protection of privacy and data security are not addressed (some discussions could be found elsewhere, such as <ref> [22, 63] </ref>).
Reference: [23] <author> J. December and N. Randall. </author> <title> The World Wide Web Unleashed. </title> <publisher> SAMS Publishing, </publisher> <year> 1994. </year> <month> 35 </month>
Reference-contexts: It also permits non-matching gaps in the matching subsequences. 8 Mining Path Traversal Patterns In a distributed information providing environment, documents or objects are usually linked together to facilitate interactive access. Examples for such information providing environments include World Wide Web (WWW) <ref> [23] </ref> and on-line services, such as Prodigy, CompuServe and America Online, where users, when seeking for information of interest, travel from one object to another via the corresponding facilities (i.e., hyperlinks and URL addresses) provided.
Reference: [24] <author> V. Dhar and A. Tuzhilin. </author> <title> Abstract-Driven Pattern Discovery in Databases. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <pages> pages 926-938, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: Moreover, statistical pattern discovery can also be performed using attribute-oriented induction <ref> [24] </ref>. <p> For example, in the schema item (id; name; category; producer; date made; cost; price), "fcategory, producer, date madeg ae fcategory, date madeg" indicates the former forms a lower level concept than the latter. Moreover, rules and view definitions can also be used as the definitions of concept hierarchies <ref> [24] </ref>. Conceptual hierarchies for numerical or ordered attributes can be generated automatically based on the analysis of data distributions in the set of relevant data [38]. Moreover, a given hierarchy may not be best suited for a particular data mining task.
Reference: [25] <author> S. Dzeroski. </author> <title> Inductive logic programming and knowledge discovery. In U.M. </title> <editor> Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 117-152. </pages> <publisher> AAAI/MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: Besides the work done by database researchers, there have been fruitful results on data mining and knowledge discovery reported in many others fields. For example, researchers in statistics have developed many techniques which may benefit data mining [26]. Inductive logic programming <ref> [25] </ref> 33 is a fast growing subfield in logic programming which is closely related to data mining. There have also been active studies on data mining using data visualization techniques [49] and visualization of data mining results [51].
Reference: [26] <author> J. Elder IV and D. </author> <title> Pregibon. A statistical perspective on knowledge discovery in databases. In U.M. </title> <editor> Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 83-115. </pages> <publisher> AAAI/MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: Moreover, there are also approaches for tranform-ing decision trees into rules [72] and transforming rules and trees into comprehensive knowledge structures [34]. There have been many other approaches on data classification, including statistical approaches <ref> [18, 26, 68] </ref>, rough sets approach [87], etc. Linear regression and linear discriminant analysis techniques are classical statistical models [26]. Methods have also been studied for scaling machine learning algorithms by combining base classifiers from partitioned data sets [18]. <p> There have been many other approaches on data classification, including statistical approaches [18, 26, 68], rough sets approach [87], etc. Linear regression and linear discriminant analysis techniques are classical statistical models <ref> [26] </ref>. Methods have also been studied for scaling machine learning algorithms by combining base classifiers from partitioned data sets [18]. There have also 20 been some studies of classification techniques in the context of large databases [2, 10]. <p> Besides the work done by database researchers, there have been fruitful results on data mining and knowledge discovery reported in many others fields. For example, researchers in statistics have developed many techniques which may benefit data mining <ref> [26] </ref>. Inductive logic programming [25] 33 is a fast growing subfield in logic programming which is closely related to data mining. There have also been active studies on data mining using data visualization techniques [49] and visualization of data mining results [51].
Reference: [27] <author> M. Ester, H.-P. Kriegel, and X. Xu. </author> <title> Knowledge discovery in large spatial databases: Focusing techniques for efficient class identification. </title> <booktitle> In Proc. 4th Int. Symp. on Large Spatial Databases (SSD'95), </booktitle> <pages> pages 67-82, </pages> <address> Portland, Maine, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: Data clustering identifies the sparse and the crowded places, and hence discovers the overall distribution patterns of the data set. Data clustering has been studied in statistics [18, 47], machine learning [31, 32], spatial database [11], and data mining <ref> [18, 27, 62, 85] </ref> areas with different emphases. As a branch of statistics, clustering analysis has been studied extensively for many years, mainly focused on distance-based clustering analysis. <p> CLARANS has been experimentally shown to be more effective than both PAM and CLARA. The computational complexity of every iteration in CLARANS is basically linearly proportional to the number of objects <ref> [27, 62] </ref>. It should be mentioned that CLARANS can be used to find the most natural number of clusters k nat . A heuristic is adopted in [62] to determine k nat , which uses silhouette coefficients 3 , introduced by Kaufman and Rousseeuw [48]. <p> can be used to cluster reasonably large data sets, such as houses in the Vancouver area, and the CLARAN algorithm outperforms PAM and CLARA. 3 It is a property of an object that specifies how much the object truly belongs to the cluster. 24 6.2 Focusing methods Ester et al. <ref> [27] </ref> pointed out some drawbacks of the CLARANS clustering algorithm [62] and proposed new techniques to improve its performance. First, CLARANS assumes that the objects to be clustered are all stored in main memory. This assumption may not be valid for large databases and disk-based methods could be required.
Reference: [28] <author> C. Faloutsos and K.-I. Lin. </author> <title> FastMap: A Fast Algorithm for Indexing, Data-Mining and Visualization of Tranditional and Multimedia Datasets. </title> <booktitle> Proceedings of ACM SIGMOD, </booktitle> <pages> pages 163-174, </pages> <month> May, </month> <year> 1995. </year>
Reference-contexts: Significant progress has recently been made in sequence matching for temporal databases <ref> [1, 5, 28, 29, 54, 57] </ref> and for speech recognition techniques such as dynamic time warping [81]. <p> Finally, different techniques have been explored to reduce the number of comparisons or search space during mining. 7.1 Similarity measures Different similarity measures have been considered, mainly the Euclidean distance <ref> [1, 29, 28] </ref> and the correlation [54]. The Euclidean distance between two sequences is defined as follows. Let fx i g be the target sequence and fy i g be a sequence in the database. <p> A fast heuristic algorithm which approximates this dimensionality reduction process is proposed in <ref> [28] </ref>. Even with the significant dimensionality reduction resulting from the algorithm proposed in [28] and the compression of the representations of the subsequences in the feature space using the method proposed in [29], generating all subsequences from each time series is still a daunting task for a database of long sequences. <p> A fast heuristic algorithm which approximates this dimensionality reduction process is proposed in <ref> [28] </ref>. Even with the significant dimensionality reduction resulting from the algorithm proposed in [28] and the compression of the representations of the subsequences in the feature space using the method proposed in [29], generating all subsequences from each time series is still a daunting task for a database of long sequences.
Reference: [29] <author> C. Faloutsos, M. Ranganathan, and Y. Manolopoulos. </author> <title> Fast Subsequence Matching in Time-Series Databases. </title> <booktitle> Proceedings of ACM SIGMOD, </booktitle> <address> Minneapolis, MN, </address> <pages> pages 419-429, </pages> <month> May, </month> <year> 1994. </year>
Reference-contexts: Significant progress has recently been made in sequence matching for temporal databases <ref> [1, 5, 28, 29, 54, 57] </ref> and for speech recognition techniques such as dynamic time warping [81]. <p> Two types of similarity queries for temporal data have emerged thus far: whole matching [1] in which the target sequence and the sequences in the database have the same length; subsequence matching <ref> [29] </ref> in which the target sequence could be shorter than the sequences in the database and the match can occur at any arbitrary point. Various approaches proposed in the literature differ in the following aspects. The first one is the similarity measure chosen. <p> Finally, different techniques have been explored to reduce the number of comparisons or search space during mining. 7.1 Similarity measures Different similarity measures have been considered, mainly the Euclidean distance <ref> [1, 29, 28] </ref> and the correlation [54]. The Euclidean distance between two sequences is defined as follows. Let fx i g be the target sequence and fy i g be a sequence in the database. <p> Extending the above concept, an innovative approach is proposed in <ref> [29] </ref> to match subsequences by generating the first few Fourier coefficients of all possible subsequences of a given length for each stored sequence in the database. The idea is to match each sequence into a small set of multidimensional rectangles in the feature space. <p> A fast heuristic algorithm which approximates this dimensionality reduction process is proposed in [28]. Even with the significant dimensionality reduction resulting from the algorithm proposed in [28] and the compression of the representations of the subsequences in the feature space using the method proposed in <ref> [29] </ref>, generating all subsequences from each time series is still a daunting task for a database of long sequences. In [54], an enhancement is proposed of the feature extraction and matching method discussed in [29]. This new approach on subsequence matching is referred to as HierarchyScan. <p> compression of the representations of the subsequences in the feature space using the method proposed in <ref> [29] </ref>, generating all subsequences from each time series is still a daunting task for a database of long sequences. In [54], an enhancement is proposed of the feature extraction and matching method discussed in [29]. This new approach on subsequence matching is referred to as HierarchyScan. It uses the correlation coefficient as an alternative similarity measure between the target sequence and the stored sequences, and performs an adaptive scan on the extracted features of the stored sequences based on the target sequence. <p> Only a small fraction of the sequences is expected to pass the test. Then the next most discriminating set of features is used for matching. This process is iterated until all of the features are exhausted. Compared to the method proposed in <ref> [29] </ref>, HierarchyScan performs a hierarchical scan instead of using a tree structure for indexing. Different transformations were considered in [54]. In [5], another approach is introduced to determine all similar sequences in a set of sequences. It is also applicable to find all subsequences similar to a target sequence.
Reference: [30] <author> U. M. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy. </author> <title> Advances in Knowledge Discovery and Data Mining. </title> <publisher> AAAI/MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: This explosive growth in data and databases has generated an urgent need for new techniques and tools that can intelligently and automatically transform the processed data into useful information and knowledge. Consequently, data mining has become a research area with increasing importance <ref> [30, 70, 76] </ref>. Data mining, which is also referred to as knowledge discovery in databases, means a process of nontrivial extraction of implicit, previously unknown and potentially useful information (such as knowledge rules, constraints, regularities) from data in databases [70]. <p> Applications of classification include medical diagnosis, performance prediction, selective marketing, to name a few. Data classification has been studied substantially in statistics, machine learning, neural net works, and expert systems [82] and is an important theme in data mining <ref> [30] </ref>. 19 5.1 Classification based on decision trees A decision-tree-based classification method, such as [71, 72], has been influcential in machine learning studies. It is a supervised learning method that constructs decision trees from a set of examples. <p> Researchers and developers in many fields have contributed to the state-of-the-art of data mining <ref> [30, 70] </ref>. Therefore, it is a challenging task to provide a comprehensive overview of the data mining methods within a short article. This article is an attempt to provide a reasonably comprehensive survey, from a database researcher's point of view, on the data mining techniques developed recently.
Reference: [31] <author> D. Fisher. </author> <title> Improving inference through conceptual clustering. </title> <booktitle> In Proc. 1987 AAAI Conf., </booktitle> <pages> pages 461-465, </pages> <address> Seattle, Washington, </address> <month> July </month> <year> 1987. </year>
Reference-contexts: Data clustering identifies the sparse and the crowded places, and hence discovers the overall distribution patterns of the data set. Data clustering has been studied in statistics [18, 47], machine learning <ref> [31, 32] </ref>, spatial database [11], and data mining [18, 27, 62, 85] areas with different emphases. As a branch of statistics, clustering analysis has been studied extensively for many years, mainly focused on distance-based clustering analysis. <p> Classes are defined as collections of objects whose intraclass similarity is high and interclass similarity is low. The method of clustering analysis in conceptual clustering is mainly based on probability analysis. Such approaches, represented by <ref> [31, 32] </ref>, make the assumption that probability distributions on separate attributes are statistically independent of each other. This assumption is, however, not always true since correlation between attributes often exists. Moreover, the probability distribution representation of clusters makes it very expensive to update and store the clusters. <p> This is especially so when the attributes have a large number of values since their time and space complexities depend not only on the number of attributes, but also on the number of values for each attribute. Furthermore, the probability-based tree (such as <ref> [31] </ref>) that is built to identify clusters is not height-balanced for skewed input data, which may cause the time and space complexity to degrade dramatically.
Reference: [32] <author> D. Fisher. </author> <title> Optimization and simplification of hierarchical clusterings. </title> <booktitle> In Proc. 1st Int. Conf. on Knowledge Discovery and Data Mining (KDD'95), </booktitle> <pages> pages 118-123, </pages> <address> Montreal, Canada, </address> <month> Aug. </month> <year> 1995. </year>
Reference-contexts: Data clustering identifies the sparse and the crowded places, and hence discovers the overall distribution patterns of the data set. Data clustering has been studied in statistics [18, 47], machine learning <ref> [31, 32] </ref>, spatial database [11], and data mining [18, 27, 62, 85] areas with different emphases. As a branch of statistics, clustering analysis has been studied extensively for many years, mainly focused on distance-based clustering analysis. <p> Classes are defined as collections of objects whose intraclass similarity is high and interclass similarity is low. The method of clustering analysis in conceptual clustering is mainly based on probability analysis. Such approaches, represented by <ref> [31, 32] </ref>, make the assumption that probability distributions on separate attributes are statistically independent of each other. This assumption is, however, not always true since correlation between attributes often exists. Moreover, the probability distribution representation of clusters makes it very expensive to update and store the clusters.
Reference: [33] <author> Y. Fu and J. Han. </author> <title> Meta-rule-guided mining of association rules in relational databases. </title> <booktitle> In Proc. 1st Int'l Workshop on Integration of Knowledge Discovery with Deductive and Object-Oriented Databases (KDOOD'95), </booktitle> <pages> pages 39-46, </pages> <address> Singapore, </address> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: In [78], methods for mining associations at generalized abstraction level are studied by extension of the Apriori algorithm. Besides mining multiple-level and generalized association rules, the mining of quantitative association rules [79] and meta-rule guided mining of association rules in relational databases <ref> [33, 75] </ref> are also studied recently, with efficient algorithms developed. 3.3 Interestingness of discovered association rules Notice that not all the discovered strong association rules (i.e., passing the minimum support and minimum confidence thresholds) are interesting enough to present.
Reference: [34] <author> B. R. </author> <title> Gains. Tranforming rules and trees into comprehensive knowledge structures. In U.M. </title> <editor> Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 205-228. </pages> <publisher> AAAI/MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: Moreover, there are also approaches for tranform-ing decision trees into rules [72] and transforming rules and trees into comprehensive knowledge structures <ref> [34] </ref>. There have been many other approaches on data classification, including statistical approaches [18, 26, 68], rough sets approach [87], etc. Linear regression and linear discriminant analysis techniques are classical statistical models [26].
Reference: [35] <author> A. Gupta, V. Harinarayan, and D. Quass. </author> <title> Aggregate-query processing in data warehousing environment. </title> <booktitle> In Proc. 21st Int. Conf. Very Large Data Bases, </booktitle> <pages> pages 358-369, </pages> <address> Zurich, Switzer-land, </address> <month> Sept. </month> <year> 1995. </year>
Reference-contexts: Data generalization is a process which abstracts a large set of relevant data in a database from a low concept level to relatively high ones. The methods for efficient and flexible generalization of large data sets can be categorized into two approaches: (1) data cube approach <ref> [35, 43, 83, 84] </ref>, and (2) attribute-oriented induction approach [37, 40]. 4.1 Data cube approach The data cube approach has a few alternative names or a few variants, such as, "multidimensional databases," "materialized views," and "OLAP (On-Line Analytical Processing)." The general idea of the approach is to materialize certain expensive computations
Reference: [36] <author> J. Han. </author> <title> Mining knowledge at multiple concept levels. </title> <booktitle> In Proc. 4th Int. Conf. on Information and Knowledge Management, </booktitle> <pages> pages 19-24, </pages> <address> Baltimore, Maryland, </address> <month> Nov. </month> <year> 1995. </year>
Reference-contexts: For example, drill-down or roll-up operations can be performed to view data at multiple abstraction levels <ref> [36] </ref>; the generalized relation can be mapped into summarization tables, charts, curves, etc., for presentation and visualization; characteristic rules which summarize the generalized data characteristics in quantitative rule forms can be extracted; discriminant rules which contrast different classes of data at multiple abstraction levels can be extracted by grouping the set <p> The roll-up of a generalized relation may simply start with this relation; however, the drill-down of the relation may start with the minimal generalized relation and perform data generalization to the corresponding abstraction levels <ref> [36] </ref>. The essential background knowledge applied in attribute-oriented induction is concept hierarchy (or lattice) associated with each attribute [37]. Most concept hierarchies are stored implicitly in databases.
Reference: [37] <author> J. Han, Y. Cai, and N. Cercone. </author> <title> Data-driven discovery of quantitative rules in relational databases. </title> <journal> IEEE Trans. Knowledge and Data Engineering, </journal> <volume> 5 </volume> <pages> 29-40, </pages> <year> 1993. </year>
Reference-contexts: The methods for efficient and flexible generalization of large data sets can be categorized into two approaches: (1) data cube approach [35, 43, 83, 84], and (2) attribute-oriented induction approach <ref> [37, 40] </ref>. 4.1 Data cube approach The data cube approach has a few alternative names or a few variants, such as, "multidimensional databases," "materialized views," and "OLAP (On-Line Analytical Processing)." The general idea of the approach is to materialize certain expensive computations that are frequently inquired, es 14 pecially those involving <p> An alternative, online, generalization-based data analysis technique, is called attribute-oriented induction approach <ref> [37, 40] </ref>. The approach takes a data mining query expressed in an SQL-like data mining query language and collects the set of relevant data in a database. <p> The approach takes a data mining query expressed in an SQL-like data mining query language and collects the set of relevant data in a database. Data generalization is then performed on the set of relevant data by applying a set of data generalization techniques <ref> [37, 40, 60] </ref>, including attribute-removal, concept-tree climbing, attribute-threshold control, propagation of counts and other aggregate function values, etc. <p> The essential background knowledge applied in attribute-oriented induction is concept hierarchy (or lattice) associated with each attribute <ref> [37] </ref>. Most concept hierarchies are stored implicitly in databases. For example, a set of attributes in address (number; street; city; province; country) in a database schema represents the concept hierarchies of the attribute address. <p> The generalized data may also be stored in a database, in the form of a generalized relation or a generalized cube, and be updated incrementally upon database updates <ref> [37] </ref>. The approach has been implemented in a data mining system, DBMiner, and been experimented in several large relational databases [40, 42]. The approach can also be extended to generalization-based data mining in object-oriented databases [41], spatial databases [53, 56], and other kinds of databases. <p> CLARANS also enables the detection of outliers, e.g., points that do not belong to any cluster. Based upon CLARANS, two spatial data mining algorithms were developed in a fashion similar to the attribute-oriented induction algorithms developed for spatial data mining <ref> [56, 37] </ref>: spatial dominant approach, SD (CLARANS) and non-spatial dominant approach, NSD (CLARANS). Both algorithms assume that the user specifies the type of the rule to be mined and relevant data through a learning request in a similar way as DBMiner [40]. <p> SKICAT is a knowledge discovery system, developed at Jet Propulsion Laboratory, which automatically detects and classifies sky objects from image data resulting from a major astronomical sky survey. DBMiner is a relational data mining system developed at Simon Fraser University by Han, et al. <ref> [37, 39, 40] </ref>, for mining multiple kinds of rules at multiple abstraction levels, including characteristic rules, discriminant rules, association rules, classification rules, etc.
Reference: [38] <author> J. Han and Y. Fu. </author> <title> Dynamic generation and refinement of concept hierarchies for knowledge discovery in databases. </title> <booktitle> In Proc. AAAI'94 Workshop on Knowledge Discovery in Databases (KDD'94), </booktitle> <pages> pages 157-168, </pages> <address> Seattle, WA, </address> <month> July </month> <year> 1994. </year> <month> 36 </month>
Reference-contexts: Moreover, rules and view definitions can also be used as the definitions of concept hierarchies [24]. Conceptual hierarchies for numerical or ordered attributes can be generated automatically based on the analysis of data distributions in the set of relevant data <ref> [38] </ref>. Moreover, a given hierarchy may not be best suited for a particular data mining task. Therefore, such hierarchies should be adjusted dynamically in many cases based on the analysis of data distributions of the corresponding set of data [38]. <p> the analysis of data distributions in the set of relevant data <ref> [38] </ref>. Moreover, a given hierarchy may not be best suited for a particular data mining task. Therefore, such hierarchies should be adjusted dynamically in many cases based on the analysis of data distributions of the corresponding set of data [38]. An an example, one may use data mining facilities to study the general characteristics of NSERC (Natural Science and Engineering Research Council of Canada) research grant database.
Reference: [39] <author> J. Han and Y. Fu. </author> <title> Discovery of Multiple-Level Association Rules from Large Databases. </title> <booktitle> Proceedings of the 21th International Conference on Very Large Data Bases, </booktitle> <pages> pages 420-431, </pages> <month> September </month> <year> 1995. </year>
Reference-contexts: In general, the kinds of knowledge which can be discovered in a database are categorized as follows. Mining association rules in transactional or relational databases has recently attracted a lot of attention in database communities <ref> [4, 7, 39, 57, 66, 73, 78] </ref>. <p> Therefore, it is important to study mining association rules at a generalized abstraction level [78] or at multiple levels <ref> [39] </ref>. Information about multiple abstraction levels may exist in database organizations. For example, a class hierarchy [50] may be implied by a combination of database attributes, such as day, month, year. It may also be given explicitly by users or experts, such as Alberta ae Prairies. <p> The association relationship in the latter statement is expressed at a lower concept level but often carries more specific and concrete information than that in the former. Therefore, it is important to mine association rules at a generalized abstraction level or at multiple concept levels. In <ref> [39] </ref>, the notion of mining multiple-level association rules are introduced: low level associations will be examined only when their high level parents are large at their corresponding levels, and different levels may adopt different minimum support thresholds. <p> one or a set of classification attributes can be derived by applying a decision tree classifier [72] on the generalized data [42]; and association rules which associate a set of generalized attribute properties in a logic implication rule by integration of attribute-oriented induction and the methods for mining association rules <ref> [7, 39, 66, 78] </ref>. Moreover, statistical pattern discovery can also be performed using attribute-oriented induction [24]. <p> SKICAT is a knowledge discovery system, developed at Jet Propulsion Laboratory, which automatically detects and classifies sky objects from image data resulting from a major astronomical sky survey. DBMiner is a relational data mining system developed at Simon Fraser University by Han, et al. <ref> [37, 39, 40] </ref>, for mining multiple kinds of rules at multiple abstraction levels, including characteristic rules, discriminant rules, association rules, classification rules, etc.
Reference: [40] <author> J. Han and Y. Fu. </author> <title> Exploration of the power of attribute-oriented induction in data mining. In U.M. </title> <editor> Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 399-421. </pages> <publisher> AAAI/MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: The methods for efficient and flexible generalization of large data sets can be categorized into two approaches: (1) data cube approach [35, 43, 83, 84], and (2) attribute-oriented induction approach <ref> [37, 40] </ref>. 4.1 Data cube approach The data cube approach has a few alternative names or a few variants, such as, "multidimensional databases," "materialized views," and "OLAP (On-Line Analytical Processing)." The general idea of the approach is to materialize certain expensive computations that are frequently inquired, es 14 pecially those involving <p> An alternative, online, generalization-based data analysis technique, is called attribute-oriented induction approach <ref> [37, 40] </ref>. The approach takes a data mining query expressed in an SQL-like data mining query language and collects the set of relevant data in a database. <p> The approach takes a data mining query expressed in an SQL-like data mining query language and collects the set of relevant data in a database. Data generalization is then performed on the set of relevant data by applying a set of data generalization techniques <ref> [37, 40, 60] </ref>, including attribute-removal, concept-tree climbing, attribute-threshold control, propagation of counts and other aggregate function values, etc. <p> The generalized data is expressed in the form of a generalized relation on which many other operations or transformations can be performed to transform generalized data into different kinds of knowledge or map them into different forms <ref> [40] </ref>. <p> the data cube is dense), or O (n log (p)), if a generalized relation is used (desirable when the corresponding cube is rather sparse), where n is the number of tuples in the set of relevant data and p is the size (i.e., number of tuples) of the generalized relation <ref> [40] </ref>. <p> The generalized data may also be stored in a database, in the form of a generalized relation or a generalized cube, and be updated incrementally upon database updates [37]. The approach has been implemented in a data mining system, DBMiner, and been experimented in several large relational databases <ref> [40, 42] </ref>. The approach can also be extended to generalization-based data mining in object-oriented databases [41], spatial databases [53, 56], and other kinds of databases. The approach is designed for generalization-based data mining. <p> A direct integration of attribute-oriented induction with the ID-3 algorithm may help discovery of classification rules at high abstraction levels <ref> [40] </ref>. It, though efficient, may reduce the classification accuracy since the classification interval may have been generalized to a rather high level. <p> Both algorithms assume that the user specifies the type of the rule to be mined and relevant data through a learning request in a similar way as DBMiner <ref> [40] </ref>. <p> SKICAT is a knowledge discovery system, developed at Jet Propulsion Laboratory, which automatically detects and classifies sky objects from image data resulting from a major astronomical sky survey. DBMiner is a relational data mining system developed at Simon Fraser University by Han, et al. <ref> [37, 39, 40] </ref>, for mining multiple kinds of rules at multiple abstraction levels, including characteristic rules, discriminant rules, association rules, classification rules, etc.
Reference: [41] <author> J. Han, S. Nishio, and H. Kawano. </author> <title> Knowledge discovery in object-oriented and active databases. </title> <editor> In F. Fuchi and T. Yokoi, editors, </editor> <booktitle> Knowledge Building and Knowledge Sharing, </booktitle> <pages> pages 221-230. </pages> <publisher> Ohmsha, Ltd. and IOS Press, </publisher> <year> 1994. </year>
Reference-contexts: The approach has been implemented in a data mining system, DBMiner, and been experimented in several large relational databases [40, 42]. The approach can also be extended to generalization-based data mining in object-oriented databases <ref> [41] </ref>, spatial databases [53, 56], and other kinds of databases. The approach is designed for generalization-based data mining.
Reference: [42] <author> J. Han, Y. Fu, W. Wang, J. Chiang, W. Gong, K. Koperski, D. Li, Y. Lu, A. Rajan, N. Ste-fanovic, B. Xia, and O. R. Zaiane. </author> <title> DBMiner: A system for mining knowledge in large relational databases. </title> <booktitle> In Proc. 1996 Int'l Conf. on Data Mining and Knowledge Discovery (KDD'96), </booktitle> <address> Portland, Oregon, </address> <month> August </month> <year> 1996. </year>
Reference-contexts: abstraction levels can be extracted by grouping the set of comparison data into contrasting classes before data generalization; classification rules which classify data at different abstraction levels according to one or a set of classification attributes can be derived by applying a decision tree classifier [72] on the generalized data <ref> [42] </ref>; and association rules which associate a set of generalized attribute properties in a logic implication rule by integration of attribute-oriented induction and the methods for mining association rules [7, 39, 66, 78]. Moreover, statistical pattern discovery can also be performed using attribute-oriented induction [24]. <p> To compare the research grants between `British Columbia' and `Alberta' (two neighbor provinces in Western Canada) in the discipline of `Computer (Science)' according to the attributes: disc code (discipline code) and grant category, the following data mining query can be specified in a data mining query language DMQL <ref> [42] </ref> as follows. 17 use NSERC95 mine discriminant rule for `BC Grants' where O.Province = `British Columbia' in contrast to `Alberta Grants' where O.Province = `Alberta' from Award A, Organization O, Grant type G where A.grant code = G.grant code and A.OrgID = O.OrgID and A.disc code = `Computer' related to <p> The generalized data may also be stored in a database, in the form of a generalized relation or a generalized cube, and be updated incrementally upon database updates [37]. The approach has been implemented in a data mining system, DBMiner, and been experimented in several large relational databases <ref> [40, 42] </ref>. The approach can also be extended to generalization-based data mining in object-oriented databases [41], spatial databases [53, 56], and other kinds of databases. The approach is designed for generalization-based data mining. <p> A multiple-level classification technique and a level adjustment and merge technique have been developed in DBMiner to improve the classification accuracy in large databases by the integration of attribute-oriented induction and classification methods <ref> [42] </ref>. Recently, Mehta et al. [59] has developed a fast data classifier, called SLIQ (Supervised Learning In QUEST), for mining classification rules in large databases. SLIQ is a decision tree classifier that can handle both numeric and categorical attributes. It uses a novel pre-sorting technique in the tree growing phase.
Reference: [43] <author> V. Harinarayan, J. D. Ullman, and A. Rajaraman. </author> <title> Implementing data cubes efficiently. </title> <booktitle> In Proc. 1996 ACM-SIGMOD Int. Conf. Management of Data, </booktitle> <address> Montreal, Canada, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: Data generalization is a process which abstracts a large set of relevant data in a database from a low concept level to relatively high ones. The methods for efficient and flexible generalization of large data sets can be categorized into two approaches: (1) data cube approach <ref> [35, 43, 83, 84] </ref>, and (2) attribute-oriented induction approach [37, 40]. 4.1 Data cube approach The data cube approach has a few alternative names or a few variants, such as, "multidimensional databases," "materialized views," and "OLAP (On-Line Analytical Processing)." The general idea of the approach is to materialize certain expensive computations <p> For example, a relation with the schema "sales (part; supplier; customer; sale price)" can be materialized into a set of eight views as shown in Figure 4 (extracted from <ref> [43] </ref>), where psc indicates a view consisting of aggregate function values (such as total sales) computed by grouping three attributes part, supplier, and customer, p indicates a view consisting of the corresponding aggregate function values computed by grouping part alone, etc. <p> The problem of materialization of a selected subset of a very large number of views can be modeled as a lattice of views. A recent study <ref> [43] </ref> has shown that a greedy algorithm, which, given what views have already been materialized, selects for materializing those views that offer the most improvement in average response time, is able to lead to results within 63% of those generated by the optimal algorithm in all cases.
Reference: [44] <author> IBM. </author> <title> Scalable POWERparallel Systems. </title> <type> Technical Report GA23-2475-02, </type> <month> February </month> <year> 1995. </year>
Reference-contexts: For example, in a shared nothing type parallel environment like SP2 <ref> [44] </ref>, each node can directly collect information only from its local database partition, and the process to reach a global decision from partial knowledge collected at individual nodes could itself be complicated and communication intensive. An algorithm for parallel data mining, called PDM, was reported in [67].
Reference: [45] <author> T. Imielinski and A. Virmani. </author> <title> DataMine application programming interface and query language for kdd applications. </title> <booktitle> In Proc. 1996 Int'l Conf. on Data Mining and Knowledge Discovery (KDD'96), </booktitle> <address> Portland, Oregon, </address> <month> August </month> <year> 1996. </year>
Reference-contexts: Explora is a multipattern and multistrategy discovery assistant developed by Klosgen [52]. IMACS is a data mining system developed at AT&T Laboratory by Brachman et al. [13], using sophisticated knowledge representation techniques. DataMine is system exploring interactive ad-hoc query-directed data mining, developed by Imielinski, et al. <ref> [45] </ref>. IDEA, developed at AT&T Laboratory by Selfridge, et al. [74], performs interactive data explorations and analysis. There have been many other data mining systems reported by machine learning and statistics researchers.
Reference: [46] <author> H.V. Jagadish. </author> <title> A Retrieval Technique for Similar Shapes. </title> <booktitle> Proceedings of ACM SIGMOD, </booktitle> <pages> pages 208-217, </pages> <year> 1991. </year>
Reference-contexts: With noisy signals, the correlation value is always smaller than one and the peaks of the sequence fc i g give the locations of possible matches. 28 7.2 Alternative approaches The straightforward approach for whole matching is to consider all of the data points of a sequence simultaneously. In <ref> [46] </ref>, the concept of mapping an object to a point in the feature space and then applying multidimensional indexing method to perform similarity search is explored.
Reference: [47] <author> A. K. Jain and R. C. Dubes. </author> <title> Algorithms for Clustering Data. </title> <publisher> Printice Hall, </publisher> <year> 1988. </year>
Reference-contexts: Given a large set of multidimensional data points, the data space is usually not uniformly occupied by the data points. Data clustering identifies the sparse and the crowded places, and hence discovers the overall distribution patterns of the data set. Data clustering has been studied in statistics <ref> [18, 47] </ref>, machine learning [31, 32], spatial database [11], and data mining [18, 27, 62, 85] areas with different emphases. As a branch of statistics, clustering analysis has been studied extensively for many years, mainly focused on distance-based clustering analysis.
Reference: [48] <author> L. Kaufman and P. J. Rousseeuw. </author> <title> Finding Groups in Data: an Introduction to Cluster Analysis. </title> <publisher> John Wiley & Sons, </publisher> <year> 1990. </year>
Reference-contexts: in the database community. 6.1 Clustering large applications based upon randomized search Ng and Han presented a clustering algorithm, CLARANS (Clustering Large Applications based upon RANdomized Search) [62], based on randomized search and originated from two clustering algorithms used in statistics, PAM (Partitioning Around Medoids) and CLARA (Clustering LARge Applications) <ref> [48] </ref>. The PAM algorithm [48] finds k clusters in n objects by first finding a representative object for each cluster. Such a representative, which is the most centrally located point in a cluster, is called a medoid. <p> 6.1 Clustering large applications based upon randomized search Ng and Han presented a clustering algorithm, CLARANS (Clustering Large Applications based upon RANdomized Search) [62], based on randomized search and originated from two clustering algorithms used in statistics, PAM (Partitioning Around Medoids) and CLARA (Clustering LARge Applications) <ref> [48] </ref>. The PAM algorithm [48] finds k clusters in n objects by first finding a representative object for each cluster. Such a representative, which is the most centrally located point in a cluster, is called a medoid. <p> The best choice of points in one iteration is chosen as the medoids for the next iteration. The cost of a single iteration is O (k (n k) 2 ). It is therefore computationally inefficient for large values of n and k. The CLARA algorithm <ref> [48] </ref> accomplishes the same task whereas it utilizes the technique of sampling. Only a small portion of the real data is chosen as a representative of the data and medoids 23 are chosen from this sample using PAM. <p> It should be mentioned that CLARANS can be used to find the most natural number of clusters k nat . A heuristic is adopted in [62] to determine k nat , which uses silhouette coefficients 3 , introduced by Kaufman and Rousseeuw <ref> [48] </ref>. CLARANS also enables the detection of outliers, e.g., points that do not belong to any cluster.
Reference: [49] <author> D. Keim, H. Kriegel, and T. Seidl. </author> <title> Supporting data mining of large databases by visual feedback queries. </title> <booktitle> In Proc. 10th of Int. Conf. on Data Engineering, </booktitle> <pages> pages 302-313, </pages> <address> Houston, TX, </address> <month> Feb. </month> <year> 1994. </year>
Reference-contexts: For example, researchers in statistics have developed many techniques which may benefit data mining [26]. Inductive logic programming [25] 33 is a fast growing subfield in logic programming which is closely related to data mining. There have also been active studies on data mining using data visualization techniques <ref> [49] </ref> and visualization of data mining results [51]. For lack of space, the results and issues in these related fields cannot be presented in this short article. An overview of data mining and knowledge discovery in a broader spectrum is left as a future exercise.
Reference: [50] <author> W. Kim. </author> <title> Introduction to Objected-Oriented Databases. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massa-chusetts, </address> <year> 1990. </year>
Reference-contexts: Therefore, it is important to study mining association rules at a generalized abstraction level [78] or at multiple levels [39]. Information about multiple abstraction levels may exist in database organizations. For example, a class hierarchy <ref> [50] </ref> may be implied by a combination of database attributes, such as day, month, year. It may also be given explicitly by users or experts, such as Alberta ae Prairies. Consider the class hierarchy in Figure 3 for example.
Reference: [51] <author> M. Klemettinen, H. Mannila, P. Ronkainen, H. Toivonen, and A. I. Verkamo. </author> <title> Finding interesting rules from large sets of discovered association rules. </title> <booktitle> In Proc. 3rd Int'l Conf. on Information and Knowledge Management, </booktitle> <pages> pages 401-408, </pages> <address> Gaithersburg, Maryland, </address> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: Inductive logic programming [25] 33 is a fast growing subfield in logic programming which is closely related to data mining. There have also been active studies on data mining using data visualization techniques [49] and visualization of data mining results <ref> [51] </ref>. For lack of space, the results and issues in these related fields cannot be presented in this short article. An overview of data mining and knowledge discovery in a broader spectrum is left as a future exercise.
Reference: [52] <author> W. Klosgen. Explora: </author> <title> a multipattern and multistrategy discovery assistant. In U.M. </title> <editor> Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 249-271. </pages> <publisher> AAAI/MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: There are many other evaluation functions, such as Gini index, chi-square test, and so forth <ref> [14, 52, 68, 82] </ref>. For example, for Gini index [14, 59], if a data set T contains examples from n classes, gini (T ) is defined as, gini (T ) = 1 p 2 i : where p i is the relative frequency of class i in T . <p> INLEN is a system, developed by Michalski, et al. [61], which integrates multiple learning paradigms in data mining. Explora is a multipattern and multistrategy discovery assistant developed by Klosgen <ref> [52] </ref>. IMACS is a data mining system developed at AT&T Laboratory by Brachman et al. [13], using sophisticated knowledge representation techniques. DataMine is system exploring interactive ad-hoc query-directed data mining, developed by Imielinski, et al. [45].
Reference: [53] <author> K. Koperski and J. Han. </author> <title> Discovery of spatial association rules in geographic information databases. </title> <booktitle> In Proc. 4th Int'l Symp. on Large Spatial Databases (SSD'95), </booktitle> <pages> pages 47-66, </pages> <address> Portland, Maine, </address> <month> Aug. </month> <year> 1995. </year> <month> 37 </month>
Reference-contexts: The approach has been implemented in a data mining system, DBMiner, and been experimented in several large relational databases [40, 42]. The approach can also be extended to generalization-based data mining in object-oriented databases [41], spatial databases <ref> [53, 56] </ref>, and other kinds of databases. The approach is designed for generalization-based data mining. <p> The approach is designed for generalization-based data mining. It is not suitable for mining specific patterns at primitive concept levels although it may help guiding such data mining by first finding some traces at high concept levels and then progressively deepening the data mining process to lower abstraction levels <ref> [53] </ref>. 18 class discipline grant category amount category support% comparison% B.C. 2.00 66.67 Computer Infrastructure Grant 40Ks-60Ks Alberta 1.72 33.33 B.C. 2.00 66.67 Computer Other 20Ks-40Ks Alberta 1.72 33.33 B.C. 2.00 50.00 Computer Other 60Ks-Alberta 3.45 50.00 B.C. 38.00 63.33 Computer Research Grant: Individual 0-20Ks Alberta 37.93 36.67 B.C. 28.00 56.00
Reference: [54] <author> C-S. Li, P.S. Yu, and V. Castelli. HierarchyScan: </author> <title> A Hierarchical Similarity Search Algorithm for Databases of Long Sequences. </title> <booktitle> Proceedings of the 12th International Conference on Data Engineering, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: Significant progress has recently been made in sequence matching for temporal databases <ref> [1, 5, 28, 29, 54, 57] </ref> and for speech recognition techniques such as dynamic time warping [81]. <p> Finally, different techniques have been explored to reduce the number of comparisons or search space during mining. 7.1 Similarity measures Different similarity measures have been considered, mainly the Euclidean distance [1, 29, 28] and the correlation <ref> [54] </ref>. The Euclidean distance between two sequences is defined as follows. Let fx i g be the target sequence and fy i g be a sequence in the database. <p> The Euclidean distance is only meaningful for measuring the distance between two vectors with the same dimension. Another possible similarity measure is the correlation between two sequences as considered in <ref> [54] </ref>. This measure not only gives the relative similarity as a function of location but also eliminates the need to generate all the subsequences of given length n of each time series in the database. <p> In <ref> [54] </ref>, an enhancement is proposed of the feature extraction and matching method discussed in [29]. This new approach on subsequence matching is referred to as HierarchyScan. <p> Then the next most discriminating set of features is used for matching. This process is iterated until all of the features are exhausted. Compared to the method proposed in [29], HierarchyScan performs a hierarchical scan instead of using a tree structure for indexing. Different transformations were considered in <ref> [54] </ref>. In [5], another approach is introduced to determine all similar sequences in a set of sequences. It is also applicable to find all subsequences similar to a target sequence. The similarity measure considered is the Euclidean distance between the sequences and the matching is performed in the time domain.
Reference: [55] <author> H. Lu, R. Setiono, and H. Liu. NeuroRule: </author> <title> A Connectionist Approach to Data Mining. </title> <booktitle> Proceedings of the 21th International Conference on Very Large Data Bases, </booktitle> <pages> pages 478-489, </pages> <month> September </month> <year> 1995. </year>
Reference-contexts: An interval classifier has been proposed in [2] to reduce the cost of decision tree generation. The neural network approach for classification and rule extraction in databases has also been studied recently <ref> [55] </ref>. 5.2 Methods for performance improvement Most of the techniques developed in machine learning and statistics may encounter the problem of scaling-up.
Reference: [56] <author> W. Lu, J. Han, and B. C. Ooi. </author> <title> Knowledge discovery in large spatial databases. </title> <booktitle> In Far East Workshop on Geographic Information Systems, </booktitle> <pages> pages 275-289, </pages> <address> Singapore, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: Examples of this type of database include: financial database for stock price index, medical databases, and multimedia databases, to name a few. Searching for similar patterns in a temporal or spatial-temporal database is essential in many data mining operations <ref> [1, 3, 56] </ref> in order to discover and predict the risk, causality, and trend associated with a specific pattern. <p> The approach has been implemented in a data mining system, DBMiner, and been experimented in several large relational databases [40, 42]. The approach can also be extended to generalization-based data mining in object-oriented databases [41], spatial databases <ref> [53, 56] </ref>, and other kinds of databases. The approach is designed for generalization-based data mining. <p> CLARANS also enables the detection of outliers, e.g., points that do not belong to any cluster. Based upon CLARANS, two spatial data mining algorithms were developed in a fashion similar to the attribute-oriented induction algorithms developed for spatial data mining <ref> [56, 37] </ref>: spatial dominant approach, SD (CLARANS) and non-spatial dominant approach, NSD (CLARANS). Both algorithms assume that the user specifies the type of the rule to be mined and relevant data through a learning request in a similar way as DBMiner [40].
Reference: [57] <author> H. Mannila, H. Toivonen, and A. Inkeri Verkamo. </author> <title> Efficient Algorithms for Discovering Association Rules. </title> <booktitle> Proceedings of AAAI Workshop on Knowledge Discovery in Databases, </booktitle> <pages> pages 181-192, </pages> <month> July, </month> <year> 1994. </year>
Reference-contexts: In general, the kinds of knowledge which can be discovered in a database are categorized as follows. Mining association rules in transactional or relational databases has recently attracted a lot of attention in database communities <ref> [4, 7, 39, 57, 66, 73, 78] </ref>. <p> As a means to improve efficiency, sampling has been used in [78] for determining the cut-off level in the class hierarchy of items to collect occurrence counts in mining generalized association rules. Sampling was discussed in <ref> [57] </ref> as a justification for devising algorithms and conducting experiments with data sets of small sizes. 3.4.3 Incremental updating of discovered association rules Since it is costly to find the association rules in large databases, incremental updating techniques should be developed for maintenance of the discovered association rules to avoid redoing <p> Significant progress has recently been made in sequence matching for temporal databases <ref> [1, 5, 28, 29, 54, 57] </ref> and for speech recognition techniques such as dynamic time warping [81].
Reference: [58] <author> C.J. Matheus, G. Piatetsky-Shapiro, and D. McNeil. </author> <title> Selecting and reporting what is interesting: The KEFIR application to healthcare data. In U.M. </title> <editor> Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 495-516. </pages> <publisher> AAAI/MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: KEFIR is a knowledge discovery system developed at the GTE Labs by Piatetsky-Shapiro, et al. <ref> [68, 58] </ref> for the analysis of health care data. SKICAT is a knowledge discovery system, developed at Jet Propulsion Laboratory, which automatically detects and classifies sky objects from image data resulting from a major astronomical sky survey.
Reference: [59] <author> M. Mehta, R. Agrawal, and J. Rissanen. SLIQ: </author> <title> A fast scalable classifier for data mining. </title> <booktitle> In Proc. 1996 Int. Conference on Extending Database Technology (EDBT'96), </booktitle> <address> Avignon, France, </address> <month> March </month> <year> 1996. </year>
Reference-contexts: There are many other evaluation functions, such as Gini index, chi-square test, and so forth [14, 52, 68, 82]. For example, for Gini index <ref> [14, 59] </ref>, if a data set T contains examples from n classes, gini (T ) is defined as, gini (T ) = 1 p 2 i : where p i is the relative frequency of class i in T . <p> A multiple-level classification technique and a level adjustment and merge technique have been developed in DBMiner to improve the classification accuracy in large databases by the integration of attribute-oriented induction and classification methods [42]. Recently, Mehta et al. <ref> [59] </ref> has developed a fast data classifier, called SLIQ (Supervised Learning In QUEST), for mining classification rules in large databases. SLIQ is a decision tree classifier that can handle both numeric and categorical attributes. It uses a novel pre-sorting technique in the tree growing phase. <p> Quest is a data mining system developed at the IBM Almaden Research Center by Agrawal, et al. [6], which discovers various kinds of knowledge in large databases, including association rules [7, 78, 79], sequential patterns [8], classification rules <ref> [59] </ref>, pattern matching and analysis [5], etc. KEFIR is a knowledge discovery system developed at the GTE Labs by Piatetsky-Shapiro, et al. [68, 58] for the analysis of health care data.
Reference: [60] <author> R. S. Michalski. </author> <title> A theory and methodology of inductive learning. </title> <editor> In Michalski et al., editor, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, </booktitle> <volume> Vol. 1, </volume> <pages> pages 83-134. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1983. </year>
Reference-contexts: The approach takes a data mining query expressed in an SQL-like data mining query language and collects the set of relevant data in a database. Data generalization is then performed on the set of relevant data by applying a set of data generalization techniques <ref> [37, 40, 60] </ref>, including attribute-removal, concept-tree climbing, attribute-threshold control, propagation of counts and other aggregate function values, etc.
Reference: [61] <author> R. S. Michalski, L. Kerschberg, K. A. Kaufman, and J.S. Ribeiro. </author> <title> Mining for knowledge in databases: The INLEN architecture, initial implementation and first results. </title> <journal> J. Int. Info. Systems, </journal> <volume> 1 </volume> <pages> 85-114, </pages> <year> 1992. </year>
Reference-contexts: KnowledgeMiner is a data mining system, developed by Shen et al. [75], which integrates data mining with deductive database techniques and using meta-rule to guide the data mining process. INLEN is a system, developed by Michalski, et al. <ref> [61] </ref>, which integrates multiple learning paradigms in data mining. Explora is a multipattern and multistrategy discovery assistant developed by Klosgen [52]. IMACS is a data mining system developed at AT&T Laboratory by Brachman et al. [13], using sophisticated knowledge representation techniques.
Reference: [62] <author> R. Ng and J. Han. </author> <title> Efficient and effective clustering method for spatial data mining. </title> <booktitle> In Proc. 1994 Int. Conf. Very Large Data Bases, </booktitle> <pages> pages 144-155, </pages> <address> Santiago, Chile, </address> <month> September </month> <year> 1994. </year>
Reference-contexts: Data clustering identifies the sparse and the crowded places, and hence discovers the overall distribution patterns of the data set. Data clustering has been studied in statistics [18, 47], machine learning [31, 32], spatial database [11], and data mining <ref> [18, 27, 62, 85] </ref> areas with different emphases. As a branch of statistics, clustering analysis has been studied extensively for many years, mainly focused on distance-based clustering analysis. <p> Clustering analysis in large databases has been studied recently in the database community. 6.1 Clustering large applications based upon randomized search Ng and Han presented a clustering algorithm, CLARANS (Clustering Large Applications based upon RANdomized Search) <ref> [62] </ref>, based on randomized search and originated from two clustering algorithms used in statistics, PAM (Partitioning Around Medoids) and CLARA (Clustering LARge Applications) [48]. The PAM algorithm [48] finds k clusters in n objects by first finding a representative object for each cluster. <p> As expected, CLARA can deal with larger data sets than PAM. However, since CLARA searches for the best k medoids among the selected sample of the data set, it cannot find the best clustering if any sampled medoid is not among the best k medoids. The CLARANS algorithm <ref> [62] </ref> tries to integrate PAM and CLARA by searching only the subset of the data set but not confining itself to any sample at any given time. <p> CLARANS has been experimentally shown to be more effective than both PAM and CLARA. The computational complexity of every iteration in CLARANS is basically linearly proportional to the number of objects <ref> [27, 62] </ref>. It should be mentioned that CLARANS can be used to find the most natural number of clusters k nat . A heuristic is adopted in [62] to determine k nat , which uses silhouette coefficients 3 , introduced by Kaufman and Rousseeuw [48]. <p> The computational complexity of every iteration in CLARANS is basically linearly proportional to the number of objects [27, 62]. It should be mentioned that CLARANS can be used to find the most natural number of clusters k nat . A heuristic is adopted in <ref> [62] </ref> to determine k nat , which uses silhouette coefficients 3 , introduced by Kaufman and Rousseeuw [48]. CLARANS also enables the detection of outliers, e.g., points that do not belong to any cluster. <p> as houses in the Vancouver area, and the CLARAN algorithm outperforms PAM and CLARA. 3 It is a property of an object that specifies how much the object truly belongs to the cluster. 24 6.2 Focusing methods Ester et al. [27] pointed out some drawbacks of the CLARANS clustering algorithm <ref> [62] </ref> and proposed new techniques to improve its performance. First, CLARANS assumes that the objects to be clustered are all stored in main memory. This assumption may not be valid for large databases and disk-based methods could be required.
Reference: [63] <author> D. E. O'Leary. </author> <title> Knowledge discovery as a threat to database security. </title> <editor> In G. Piatetsky-Shapiro and W. J. Frawley, editors, </editor> <booktitle> Knowledge Discovery in Databases, </booktitle> <pages> pages 507-516. </pages> <publisher> AAAI/MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: For example, the handling of different types of data are confined to relational and transactional data, and the methods for protection of privacy and data security are not addressed (some discussions could be found elsewhere, such as <ref> [22, 63] </ref>).
Reference: [64] <author> A. Papoulis. </author> <title> Probability, Random Variable, and Stochastic Process. </title> <publisher> McGraw Hills, </publisher> <address> New York, </address> <year> 1984. </year>
Reference-contexts: Fourier transformation is by no means the best method of feature extraction. It is known that the a priori relative importance of the features can be optimally determined from the singular value decomposition (SVD) or the Karhunen-Loeve transformation on the covariance matrix of the collection of the time sequences <ref> [64] </ref>. A fast heuristic algorithm which approximates this dimensionality reduction process is proposed in [28].
Reference: [65] <author> J.-S. Park, M.-S. Chen, and P. S. Yu. </author> <title> Mining Association Rules with Adjustable Accuracy. </title> <institution> IBM Research Report, </institution> <year> 1995. </year>
Reference-contexts: Allowing imprecise results can in fact significantly improve the efficiency of the mining algorithms. As the database size increases nowadays, sampling appears to be an attractive approach to data mining. A technique of relaxing the support factor based on the sampling size is devised in <ref> [65] </ref> to achieve the desired level of accuracy. As shown in [65], the relaxation factor, as well as the sample size, can be properly adjusted so as to improve the result accuracy while minimizing the corresponding execution time, thereby allowing us to effectively achieve a design trade-off between accuracy and efficiency <p> As the database size increases nowadays, sampling appears to be an attractive approach to data mining. A technique of relaxing the support factor based on the sampling size is devised in <ref> [65] </ref> to achieve the desired level of accuracy. As shown in [65], the relaxation factor, as well as the sample size, can be properly adjusted so as to improve the result accuracy while minimizing the corresponding execution time, thereby allowing us to effectively achieve a design trade-off between accuracy and efficiency with two control parameters.
Reference: [66] <author> J.-S. Park, M.-S. Chen, and P. S. Yu. </author> <title> An Effective Hash Based Algorithm for Mining Association Rules. </title> <booktitle> Proceedings of ACM SIGMOD, </booktitle> <pages> pages 175-186, </pages> <month> May, </month> <year> 1995. </year>
Reference-contexts: In general, the kinds of knowledge which can be discovered in a database are categorized as follows. Mining association rules in transactional or relational databases has recently attracted a lot of attention in database communities <ref> [4, 7, 39, 57, 66, 73, 78] </ref>. <p> Such rules with high confidence and strong support are referred to as strong rules in [4, 68]. The task of mining association rules is essentially to discover strong association rules in large databases. In <ref> [4, 7, 66] </ref>, the problem of mining association rules is decomposed into the following two steps: 1. Discover the large itemsets, i.e., the sets of itemsets that have transaction support above a pre-determined minimum support s. 2. Use the large itemsets to generate the association rules for the database. <p> After the large itemsets are identified, the corresponding association rules can be derived in a straightforward manner. Efficient counting of large itemsets is thus the focus of most prior work. Here, algorithms Apriori and DHP, developed in [7] and <ref> [66] </ref> respectively, are described to illustrate the nature of this problem. 3.1 Algorithm Apriori and DHP Consider an example transaction database given in Figure 1 1 . <p> Apriori then scans all the transactions and discovers the large 3-itemsets L 3 in Figure 2. Since there is no candidate 4-itemset to be constituted from L 3 , Apriori ends the process of discovering large itemsets. Similar to Apriori, DHP in <ref> [66] </ref> also generates candidate k-itemsets from L k1 . <p> one or a set of classification attributes can be derived by applying a decision tree classifier [72] on the generalized data [42]; and association rules which associate a set of generalized attribute properties in a logic implication rule by integration of attribute-oriented induction and the methods for mining association rules <ref> [7, 39, 66, 78] </ref>. Moreover, statistical pattern discovery can also be performed using attribute-oriented induction [24].
Reference: [67] <author> J.-S. Park, M.-S. Chen, and P. S. Yu. </author> <title> Efficient Parallel Data Mining for Association Rules. </title> <booktitle> Proceedings of the 4th Intern'l Conf. on Information and Knowledge Management, </booktitle> <pages> pages 31-36, </pages> <month> Nov. 29 - Dec. 3, </month> <year> 1995. </year> <month> 38 </month>
Reference-contexts: An algorithm for parallel data mining, called PDM, was reported in <ref> [67] </ref>. Under PDM, with the entire transaction database being partitioned across all nodes, each node will employ a hashing method to identify candidate k-itemsets (i.e., itemsets consisting of k items) from its local database. To reduce the communication cost incurred, a clue-and-poll technique was devised in [67] to resolve the uncertainty <p> PDM, was reported in <ref> [67] </ref>. Under PDM, with the entire transaction database being partitioned across all nodes, each node will employ a hashing method to identify candidate k-itemsets (i.e., itemsets consisting of k items) from its local database. To reduce the communication cost incurred, a clue-and-poll technique was devised in [67] to resolve the uncertainty due to the partial knowledge collected at each node by judiciously selecting a small fraction of the itemsets for the information exchange among nodes. 4 Multi-level Data Generalization, Summarization, and Characterization Data and objects in databases often contain detailed information at primitive concept levels.
Reference: [68] <author> G. Piatetsky-Shapiro. </author> <title> Discovery, analysis, and presentation of strong rules. </title> <editor> In G. Piatetsky--Shapiro and W. J. Frawley, editors, </editor> <booktitle> Knowledge Discovery in Databases, </booktitle> <pages> pages 229-238. </pages> <publisher> AAAI/MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: It is often desirable to pay attention to only those rules which may have reasonably large support. Such rules with high confidence and strong support are referred to as strong rules in <ref> [4, 68] </ref>. The task of mining association rules is essentially to discover strong association rules in large databases. In [4, 7, 66], the problem of mining association rules is decomposed into the following two steps: 1. <p> Clearly, the factor of statistical dependence among various user behaviors analyzed has to be taken into consideration for the determination of the usefulness of association rules. There have been some interesting studies on the interestingness or usefulness of discovered rules, such as <ref> [68, 78, 77] </ref>. The notion of interestingness on discovered generalized association rules is introduced in [78]. <p> There are many other evaluation functions, such as Gini index, chi-square test, and so forth <ref> [14, 52, 68, 82] </ref>. For example, for Gini index [14, 59], if a data set T contains examples from n classes, gini (T ) is defined as, gini (T ) = 1 p 2 i : where p i is the relative frequency of class i in T . <p> Moreover, there are also approaches for tranform-ing decision trees into rules [72] and transforming rules and trees into comprehensive knowledge structures [34]. There have been many other approaches on data classification, including statistical approaches <ref> [18, 26, 68] </ref>, rough sets approach [87], etc. Linear regression and linear discriminant analysis techniques are classical statistical models [26]. Methods have also been studied for scaling machine learning algorithms by combining base classifiers from partitioned data sets [18]. <p> KEFIR is a knowledge discovery system developed at the GTE Labs by Piatetsky-Shapiro, et al. <ref> [68, 58] </ref> for the analysis of health care data. SKICAT is a knowledge discovery system, developed at Jet Propulsion Laboratory, which automatically detects and classifies sky objects from image data resulting from a major astronomical sky survey.
Reference: [69] <author> G. Piatetsky-Shapiro, U. Fayyad, and P. Smith. </author> <title> From data mining to knowledge discovery: An overview. In U.M. </title> <editor> Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 1-35. </pages> <publisher> AAAI/MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: This article is an attempt to provide a reasonably comprehensive survey, from a database researcher's point of view, on the data mining techniques developed recently. An overview of data mining and knowledge discovery, from some data mining and machine learning researchers, has been performed recently <ref> [69] </ref>. The 32 major difference of our survey from theirs is the focus of this survey is on the techniques developed by database researchers, with an emphasis on efficient methods for data mining in very large databases.
Reference: [70] <author> G. Piatetsky-Shapiro and W. J. Frawley. </author> <title> Knowledge Discovery in Databases. </title> <publisher> AAAI/MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: This explosive growth in data and databases has generated an urgent need for new techniques and tools that can intelligently and automatically transform the processed data into useful information and knowledge. Consequently, data mining has become a research area with increasing importance <ref> [30, 70, 76] </ref>. Data mining, which is also referred to as knowledge discovery in databases, means a process of nontrivial extraction of implicit, previously unknown and potentially useful information (such as knowledge rules, constraints, regularities) from data in databases [70]. <p> Data mining, which is also referred to as knowledge discovery in databases, means a process of nontrivial extraction of implicit, previously unknown and potentially useful information (such as knowledge rules, constraints, regularities) from data in databases <ref> [70] </ref>. There are also many other terms, appearing in some articles and documents, carrying a similar or slightly different meaning, such as knowledge mining from databases, knowledge extraction, data archaeology, data dredging, data analysis, and so on. <p> Researchers and developers in many fields have contributed to the state-of-the-art of data mining <ref> [30, 70] </ref>. Therefore, it is a challenging task to provide a comprehensive overview of the data mining methods within a short article. This article is an attempt to provide a reasonably comprehensive survey, from a database researcher's point of view, on the data mining techniques developed recently.
Reference: [71] <author> J. R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: Data classification has been studied substantially in statistics, machine learning, neural net works, and expert systems [82] and is an important theme in data mining [30]. 19 5.1 Classification based on decision trees A decision-tree-based classification method, such as <ref> [71, 72] </ref>, has been influcential in machine learning studies. It is a supervised learning method that constructs decision trees from a set of examples. The quality (function) of a tree depends on both the classification accuracy and the size of the tree. <p> The eventual outcome is a tree in which each leaf carries a class name, and each interior node specifies an attribute with a branch corresponding to each possible value of that attribute. A typical decision tree learning system, ID-3 <ref> [71] </ref>, adopts a top-down irrevocable strategy that searches only part of the search space. It guarantees that a simple, but not necessarily the simplest, tree is found. ID-3 uses an information-theoretic approach aimed at minimizing the expected number of tests to classify an object. <p> An information-based heuristic selects the attribute providing the highest information gain, i.e., the attribute which minimizes the information needed in the resulting subtrees to classify the elements. An extension to ID-3, C4.5 [72], extends the domain of classification from categorical attributes to numerical ones. The ID-3 system <ref> [71] </ref> uses information gain as the evaluation functions for classification, with the following evaluation function, i = (p i ln (p i )); where p i is the probability that an object is in class i.
Reference: [72] <author> J. R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: classes of data at multiple abstraction levels can be extracted by grouping the set of comparison data into contrasting classes before data generalization; classification rules which classify data at different abstraction levels according to one or a set of classification attributes can be derived by applying a decision tree classifier <ref> [72] </ref> on the generalized data [42]; and association rules which associate a set of generalized attribute properties in a logic implication rule by integration of attribute-oriented induction and the methods for mining association rules [7, 39, 66, 78]. Moreover, statistical pattern discovery can also be performed using attribute-oriented induction [24]. <p> Data classification has been studied substantially in statistics, machine learning, neural net works, and expert systems [82] and is an important theme in data mining [30]. 19 5.1 Classification based on decision trees A decision-tree-based classification method, such as <ref> [71, 72] </ref>, has been influcential in machine learning studies. It is a supervised learning method that constructs decision trees from a set of examples. The quality (function) of a tree depends on both the classification accuracy and the size of the tree. <p> An information-based heuristic selects the attribute providing the highest information gain, i.e., the attribute which minimizes the information needed in the resulting subtrees to classify the elements. An extension to ID-3, C4.5 <ref> [72] </ref>, extends the domain of classification from categorical attributes to numerical ones. <p> Moreover, there are also approaches for tranform-ing decision trees into rules <ref> [72] </ref> and transforming rules and trees into comprehensive knowledge structures [34]. There have been many other approaches on data classification, including statistical approaches [18, 26, 68], rough sets approach [87], etc. Linear regression and linear discriminant analysis techniques are classical statistical models [26].
Reference: [73] <author> A. Savasere, E. Omiecinski, and S. Navathe. </author> <title> An Efficient Algorithm for Mining Association Rules in Large Databases. </title> <booktitle> Proceedings of the 21th International Conference on Very Large Data Bases, </booktitle> <pages> pages 432-444, </pages> <month> September </month> <year> 1995. </year>
Reference-contexts: In general, the kinds of knowledge which can be discovered in a database are categorized as follows. Mining association rules in transactional or relational databases has recently attracted a lot of attention in database communities <ref> [4, 7, 39, 57, 66, 73, 78] </ref>.
Reference: [74] <author> P. G. Selfridge, D. Srivastava, and L. O. Wilson. </author> <title> IDEA: Interactive data exploration and analysis. </title> <booktitle> In Proc. 1996 ACM-SIGMOD Int. Conf. Management of Data, </booktitle> <address> Montreal, Canada, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: IMACS is a data mining system developed at AT&T Laboratory by Brachman et al. [13], using sophisticated knowledge representation techniques. DataMine is system exploring interactive ad-hoc query-directed data mining, developed by Imielinski, et al. [45]. IDEA, developed at AT&T Laboratory by Selfridge, et al. <ref> [74] </ref>, performs interactive data explorations and analysis. There have been many other data mining systems reported by machine learning and statistics researchers.
Reference: [75] <author> W. Shen, K. Ong, B. Mitbander, and C. Zaniolo. </author> <title> Metaqueries for data mining. In U.M. </title> <editor> Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 375-398. </pages> <publisher> AAAI/MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: In [78], methods for mining associations at generalized abstraction level are studied by extension of the Apriori algorithm. Besides mining multiple-level and generalized association rules, the mining of quantitative association rules [79] and meta-rule guided mining of association rules in relational databases <ref> [33, 75] </ref> are also studied recently, with efficient algorithms developed. 3.3 Interestingness of discovered association rules Notice that not all the discovered strong association rules (i.e., passing the minimum support and minimum confidence thresholds) are interesting enough to present. <p> KnowledgeMiner is a data mining system, developed by Shen et al. <ref> [75] </ref>, which integrates data mining with deductive database techniques and using meta-rule to guide the data mining process. INLEN is a system, developed by Michalski, et al. [61], which integrates multiple learning paradigms in data mining. Explora is a multipattern and multistrategy discovery assistant developed by Klosgen [52].
Reference: [76] <author> A. Silberschatz, M. Stonebraker, and J. D. Ullman. </author> <title> Database research: Achievements and opportunities into the 21st century. </title> <booktitle> In Report of an NSF Workshop on the Future of Database Systems Research, </booktitle> <month> May </month> <year> 1995. </year>
Reference-contexts: This explosive growth in data and databases has generated an urgent need for new techniques and tools that can intelligently and automatically transform the processed data into useful information and knowledge. Consequently, data mining has become a research area with increasing importance <ref> [30, 70, 76] </ref>. Data mining, which is also referred to as knowledge discovery in databases, means a process of nontrivial extraction of implicit, previously unknown and potentially useful information (such as knowledge rules, constraints, regularities) from data in databases [70].
Reference: [77] <author> A. Silberschatz and A. Tuzhilin. </author> <title> On subjective measure of interestingness in knowledge discovery. </title> <booktitle> In Proc. 1st Int. Conf. on Knowledge Discovery and Data Mining (KDD'95), </booktitle> <pages> pages 275-281, </pages> <address> Montreal, Canada, </address> <month> Aug. </month> <year> 1995. </year>
Reference-contexts: Clearly, the factor of statistical dependence among various user behaviors analyzed has to be taken into consideration for the determination of the usefulness of association rules. There have been some interesting studies on the interestingness or usefulness of discovered rules, such as <ref> [68, 78, 77] </ref>. The notion of interestingness on discovered generalized association rules is introduced in [78]. <p> There have been some interesting studies on the interestingness or usefulness of discovered rules, such as [68, 78, 77]. The notion of interestingness on discovered generalized association rules is introduced in [78]. The subjective measure of interestingness in knowledge discovery is studied in <ref> [77] </ref>. 3.4 Improving the efficiency of mining association rules Since the amount of the processed data in mining association rules tends to be huge, it is important to devise efficient algorithms to conduct mining on such data.
Reference: [78] <author> R. Srikant and R. Agrawal. </author> <title> Mining Generalized Association Rules. </title> <booktitle> Proceedings of the 21th International Conference on Very Large Data Bases, </booktitle> <pages> pages 407-419, </pages> <month> September </month> <year> 1995. </year>
Reference-contexts: In general, the kinds of knowledge which can be discovered in a database are categorized as follows. Mining association rules in transactional or relational databases has recently attracted a lot of attention in database communities <ref> [4, 7, 39, 57, 66, 73, 78] </ref>. <p> Therefore, it is important to study mining association rules at a generalized abstraction level <ref> [78] </ref> or at multiple levels [39]. Information about multiple abstraction levels may exist in database organizations. For example, a class hierarchy [50] may be implied by a combination of database attributes, such as day, month, year. <p> Four algorithms are developed for efficient mining of association rules based on different ways of sharing of multiple-level mining 10 processes and reduction of the encoded transaction tables. In <ref> [78] </ref>, methods for mining associations at generalized abstraction level are studied by extension of the Apriori algorithm. <p> Clearly, the factor of statistical dependence among various user behaviors analyzed has to be taken into consideration for the determination of the usefulness of association rules. There have been some interesting studies on the interestingness or usefulness of discovered rules, such as <ref> [68, 78, 77] </ref>. The notion of interestingness on discovered generalized association rules is introduced in [78]. <p> There have been some interesting studies on the interestingness or usefulness of discovered rules, such as [68, 78, 77]. The notion of interestingness on discovered generalized association rules is introduced in <ref> [78] </ref>. The subjective measure of interestingness in knowledge discovery is studied in [77]. 3.4 Improving the efficiency of mining association rules Since the amount of the processed data in mining association rules tends to be huge, it is important to devise efficient algorithms to conduct mining on such data. <p> As a means to improve efficiency, sampling has been used in <ref> [78] </ref> for determining the cut-off level in the class hierarchy of items to collect occurrence counts in mining generalized association rules. <p> one or a set of classification attributes can be derived by applying a decision tree classifier [72] on the generalized data [42]; and association rules which associate a set of generalized attribute properties in a logic implication rule by integration of attribute-oriented induction and the methods for mining association rules <ref> [7, 39, 66, 78] </ref>. Moreover, statistical pattern discovery can also be performed using attribute-oriented induction [24]. <p> Appendices are welcome, and a comprehensive overview of such systems is necessary. Quest is a data mining system developed at the IBM Almaden Research Center by Agrawal, et al. [6], which discovers various kinds of knowledge in large databases, including association rules <ref> [7, 78, 79] </ref>, sequential patterns [8], classification rules [59], pattern matching and analysis [5], etc. KEFIR is a knowledge discovery system developed at the GTE Labs by Piatetsky-Shapiro, et al. [68, 58] for the analysis of health care data.
Reference: [79] <author> R. Srikant and R. Agrawal. </author> <title> Mining quantitative association rules in large relational tables. </title> <booktitle> In Proc. 1996 ACM-SIGMOD Int. Conf. Management of Data, </booktitle> <address> Montreal, Canada, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: In [78], methods for mining associations at generalized abstraction level are studied by extension of the Apriori algorithm. Besides mining multiple-level and generalized association rules, the mining of quantitative association rules <ref> [79] </ref> and meta-rule guided mining of association rules in relational databases [33, 75] are also studied recently, with efficient algorithms developed. 3.3 Interestingness of discovered association rules Notice that not all the discovered strong association rules (i.e., passing the minimum support and minimum confidence thresholds) are interesting enough to present. <p> Appendices are welcome, and a comprehensive overview of such systems is necessary. Quest is a data mining system developed at the IBM Almaden Research Center by Agrawal, et al. [6], which discovers various kinds of knowledge in large databases, including association rules <ref> [7, 78, 79] </ref>, sequential patterns [8], classification rules [59], pattern matching and analysis [5], etc. KEFIR is a knowledge discovery system developed at the GTE Labs by Piatetsky-Shapiro, et al. [68, 58] for the analysis of health care data.
Reference: [80] <author> R. Stam and R. Snodgrass. </author> <title> A bibliography on temporal databases. </title> <journal> IEEE Bulletin on Data Engineering, </journal> <volume> 11(4), </volume> <month> December </month> <year> 1988. </year>
Reference-contexts: Such clustering may facilitate taxonomy formation, which means the organization of observations into a hierarchy of classes that group similar events together. Temporal or spatial-temporal data constitutes a large portion of data stored in computers <ref> [9, 80] </ref>. Examples of this type of database include: financial database for stock price index, medical databases, and multimedia databases, to name a few.
Reference: [81] <author> Y. Stettiner, D. Malah, and D. Chazan. </author> <title> Dynamic Time Warping with Path Control and Non local Cost. </title> <booktitle> Proceedings of 12th IAPR Intern'l Conf. on Pattern Recognition, </booktitle> <pages> pages 174-177, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Significant progress has recently been made in sequence matching for temporal databases [1, 5, 28, 29, 54, 57] and for speech recognition techniques such as dynamic time warping <ref> [81] </ref>.
Reference: [82] <author> S. M. Weiss and C. A. </author> <title> Kulikowski. Computer Systems that Learn: Classification and Prediction Methods from Statistics, Neural Nets, Machine Learning, and Expert Systems. </title> <publisher> Morgan Kaufman, </publisher> <year> 1991. </year>
Reference-contexts: Applications of classification include medical diagnosis, performance prediction, selective marketing, to name a few. Data classification has been studied substantially in statistics, machine learning, neural net works, and expert systems <ref> [82] </ref> and is an important theme in data mining [30]. 19 5.1 Classification based on decision trees A decision-tree-based classification method, such as [71, 72], has been influcential in machine learning studies. It is a supervised learning method that constructs decision trees from a set of examples. <p> There are many other evaluation functions, such as Gini index, chi-square test, and so forth <ref> [14, 52, 68, 82] </ref>. For example, for Gini index [14, 59], if a data set T contains examples from n classes, gini (T ) is defined as, gini (T ) = 1 p 2 i : where p i is the relative frequency of class i in T .
Reference: [83] <author> J. Widom. </author> <title> Research problems in data warehousing. </title> <booktitle> In Proc. 4th Int. Conf. on Information and Knowledge Management, </booktitle> <pages> pages 25-30, </pages> <address> Baltimore, Maryland, </address> <month> Nov. </month> <year> 1995. </year> <month> 39 </month>
Reference-contexts: Data generalization is a process which abstracts a large set of relevant data in a database from a low concept level to relatively high ones. The methods for efficient and flexible generalization of large data sets can be categorized into two approaches: (1) data cube approach <ref> [35, 43, 83, 84] </ref>, and (2) attribute-oriented induction approach [37, 40]. 4.1 Data cube approach The data cube approach has a few alternative names or a few variants, such as, "multidimensional databases," "materialized views," and "OLAP (On-Line Analytical Processing)." The general idea of the approach is to materialize certain expensive computations <p> As a matter of fact, in many realistic cases, the difference between the greedy and optimal solutions is essentially negligible. Data cube approach is an interesting technique with many applications <ref> [83] </ref>. Techniques for 15 indexing multiple dimensional data cubes and for incremental updating of data cubes at database updates have also been studied [83, 86]. Notice that data cubes could be quite sparse in many cases because not every cell in each dimension may have corresponding data in the database. <p> Data cube approach is an interesting technique with many applications [83]. Techniques for 15 indexing multiple dimensional data cubes and for incremental updating of data cubes at database updates have also been studied <ref> [83, 86] </ref>. Notice that data cubes could be quite sparse in many cases because not every cell in each dimension may have corresponding data in the database. Techniques should be developed to handle sparse cubes efficiently.
Reference: [84] <author> W. P. Yan and P. Larson. </author> <title> Eager aggregation and lazy aggregation. </title> <booktitle> In Proc. 21st Int. Conf. Very Large Data Bases, </booktitle> <pages> pages 345-357, </pages> <address> Zurich, Switzerland, </address> <month> Sept. </month> <year> 1995. </year>
Reference-contexts: Data generalization is a process which abstracts a large set of relevant data in a database from a low concept level to relatively high ones. The methods for efficient and flexible generalization of large data sets can be categorized into two approaches: (1) data cube approach <ref> [35, 43, 83, 84] </ref>, and (2) attribute-oriented induction approach [37, 40]. 4.1 Data cube approach The data cube approach has a few alternative names or a few variants, such as, "multidimensional databases," "materialized views," and "OLAP (On-Line Analytical Processing)." The general idea of the approach is to materialize certain expensive computations
Reference: [85] <author> T. Zhang, R. Ramakrishnan, and M. Livny. </author> <title> BIRCH: an efficient data clustering method for very large databases. </title> <booktitle> In Proc. 1996 ACM-SIGMOD Int. Conf. Management of Data, </booktitle> <address> Montreal, Canada, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: Data clustering identifies the sparse and the crowded places, and hence discovers the overall distribution patterns of the data set. Data clustering has been studied in statistics [18, 47], machine learning [31, 32], spatial database [11], and data mining <ref> [18, 27, 62, 85] </ref> areas with different emphases. As a branch of statistics, clustering analysis has been studied extensively for many years, mainly focused on distance-based clustering analysis. <p> Zhang et. al. <ref> [85] </ref> presented another algorithm, BIRCH (Balanced Iterative Reducing and Clustering), for clustering large sets of points. The method they presented is an incremental one with the possibility of 25 adjustment of memory requirements to the size of memory that is available. Two concepts, Clustering Feature and CF tree, are introduced. <p> Zhang et. al. claimed that any clustering algorithm, including CLARANS can be used with CF trees. The CPU and I/O costs of the BIRCH algorithm are of order O (N ). A good number of experiments reported in <ref> [85] </ref> show linear scalability of the algorithm with respect to the number of points, insensibility to the input order, and good quality of clustering of the data. 26 7 Pattern-based Similarity Search Next, we discuss data mining techniques based on pattern-based similarity search.
Reference: [86] <author> Y. Zhuge, H. Garcia-Molina, J. Hammer, and J. Widom. </author> <title> View maintenance in a warehousing environment. </title> <booktitle> In Proc. 1995 ACM-SIGMOD Int. Conf. Management of Data, </booktitle> <pages> pages 316-327, </pages> <address> San Jose, CA, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: Data cube approach is an interesting technique with many applications [83]. Techniques for 15 indexing multiple dimensional data cubes and for incremental updating of data cubes at database updates have also been studied <ref> [83, 86] </ref>. Notice that data cubes could be quite sparse in many cases because not every cell in each dimension may have corresponding data in the database. Techniques should be developed to handle sparse cubes efficiently.
Reference: [87] <author> W. Ziarko. </author> <title> Rough Sets, Fuzzy Sets and Knowledge Discovery. </title> <publisher> Springer-Verlag, </publisher> <year> 1994. </year> <month> 40 </month>
Reference-contexts: Moreover, there are also approaches for tranform-ing decision trees into rules [72] and transforming rules and trees into comprehensive knowledge structures [34]. There have been many other approaches on data classification, including statistical approaches [18, 26, 68], rough sets approach <ref> [87] </ref>, etc. Linear regression and linear discriminant analysis techniques are classical statistical models [26]. Methods have also been studied for scaling machine learning algorithms by combining base classifiers from partitioned data sets [18].
References-found: 87

