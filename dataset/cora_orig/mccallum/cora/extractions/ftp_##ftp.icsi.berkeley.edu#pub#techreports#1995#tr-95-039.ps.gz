URL: ftp://ftp.icsi.berkeley.edu/pub/techreports/1995/tr-95-039.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/techreports/1995.html
Root-URL: http://www.icsi.berkeley.edu
Email: albers@icsi.berkeley.edu  mitzen@cs.berkeley.edu  
Title: Average Case Analyses of List Update Algorithms, with Applications to Data Compression  
Author: Susanne Albers Michael Mitzenmacher 
Note: Supported in part by an Otto Hahn Medal Award of the Max Planck Society.  
Address: 1947 Center Street, Berkeley, CA 94708;  Berkeley, Berkeley, CA 94720.  
Affiliation: International Computer Science Institute,  and Max Planck Institute for Computer Science, Germany.  Computer Science Division, University of California at  
Pubnum: TR-95-039  
Abstract: We study the performance of the Timestamp(0) (TS(0)) algorithm for self-organizing sequential search on discrete memoryless sources. We demonstrate that TS(0) is better than Move-to-front on such sources, and determine performance ratios for TS(0) against the optimal o*ine and static adversaries in this situation. Previous work on such sources compared online algorithms only to static adversaries. One practical motivation for our work is the use of the Move-to-front heuristic in various compression algorithms. Our theoretical results suggest that in many cases using TS(0) in place of Move-to-front in schemes that use the latter should improve compression. Tests using implementations on a standard corpus of test documents demonstrate that TS(0) leads to improved compression. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Albers. </author> <title> Improved randomized on-line algorithms for the list update problem. </title> <booktitle> In Proc. of the 6th Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 412-419, </pages> <year> 1995. </year>
Reference-contexts: Previous results have shown that MTF is such an algorithm, whereas algorithms T and FC are not. Our main contribution is to show that there is an algorithm that has an even better overall performance than MTF. The algorithm we analyze belongs to the Timestamp (p) family of algorithms <ref> [1] </ref> that were introduced in the context of randomized online algorithms and are defined for any real number p 2 [0; 1]. <p> The algorithm we analyze belongs to the Timestamp (p) family of algorithms [1] that were introduced in the context of randomized online algorithms and are defined for any real number p 2 <ref> [0; 1] </ref>. For p = 0, the algorithm is deterministic and can be formulated as follows: * Algorithm TS (0): Insert the requested item, say x, in front of the first item in the list that has been requested at most once since the last request to x. <p> If x has not been requested so far, leave the position of x unchanged. In <ref> [1] </ref> it was shown that the algorithm achieves a competitive ratio of 2 on any request sequence, as does Move-to-front [14]. <p> These statements were shown in <ref> [1] </ref>. Therefore, when TS (0) serves the request to x j in the subsequence x i x i x j , then x j does not move in front of x i . <p> In the same way we can estimate OPT's cost on a request sequence . This technique of evaluating cost by considering pairs of items was also used in <ref> [2, 10, 1] </ref>. In the second part of the analysis we show that, for each pair fx; yg, the asymptotic expected cost paid by TS (0) on a phase is at most 3 2 times the asymptotic expected cost incurred by OPT. <p> Let C T S ( xy ) be the cost incurred by TS (0) if it serves xy on a two item list that consists of only x and y. In <ref> [1] </ref> it was shown that if TS (0) serves on the long list, then the relative position of x and y changes in the same way as if TS (0) serves xy on the two item list. <p> In the following we concentrate on one particular pair fx; yg of items x 6= y. We partition the request sequence xy into phases, as in <ref> [1] </ref>. The first phase starts with the first request in xy and ends when, for the first time, there are two requests to the same item and the next request is different. <p> In particular, we have shown that on all distributions, the expected cost incurred by TS (0) is at most 1.5 times the expected cost incurred by the optimal (dynamic) o*ine algorithm. We note that a similar analysis can also be used to study the Timestamp (p) algorithms <ref> [1] </ref>, but TS (0) yields the best competitive ratio against distributions. Also, the techniques we used can be extended to the case that a request sequence is generated by a Markov chain, but for general Markov chains, we cannot do better than a competitive ratio of 2.
Reference: [2] <author> J.L. Bentley and C.C. McGeoch. </author> <title> Amortized analyses of self-organizing sequential search heuristics. </title> <journal> Communication of the ACM, </journal> <volume> 28 </volume> <pages> 404-411, </pages> <year> 1985. </year>
Reference-contexts: In the same way we can estimate OPT's cost on a request sequence . This technique of evaluating cost by considering pairs of items was also used in <ref> [2, 10, 1] </ref>. In the second part of the analysis we show that, for each pair fx; yg, the asymptotic expected cost paid by TS (0) on a phase is at most 3 2 times the asymptotic expected cost incurred by OPT.
Reference: [3] <author> J.L. Bentley, D.S. Sleator, R.E. Tarjan and V.K. Wei. </author> <title> A locally adaptive data compression scheme. </title> <journal> Communication of the ACM, </journal> <volume> 29 </volume> <pages> 320-330, </pages> <year> 1986. </year>
Reference-contexts: Since our results show that TS (0) performs better than MTF on distributions, we consider applying the algorithm in the context of data compression, where MTF has been used to develop a locally adaptive data compression scheme <ref> [3] </ref>. Here we prove that for all distributions ~p = (p 1 ; p 2 ; : : : ; p n ), 1 the expected number of bits needed by a TS (0)-based encoding scheme to encode one symbol is linear in the entropy of the source. <p> The string S consists of symbols, where each symbol is an element in the alphabet = fx 1 ; x 2 ; : : : ; x n g. Each symbol is equal to x i with probability p i . Bentley et al. <ref> [3] </ref> showed how a list update algorithm can be used to develop a data compression scheme. The idea is to convert the string S of symbols into a string I of integers. <p> It also suggests that perhaps the bound could be improved by avoiding this technical difficulty. 3 Analyses and simulations for data compression The MTF algorithm has proved useful in the development of the locally adaptive compression scheme of <ref> [3] </ref>. Motivated by this result, we consider a similar algorithm based on TS (0). We assume the reader is somewhat familiar with the system of [3], which was briefly described in the introduction. 10 3.1 Theoretical results Let B T S (~p) be the expected number of bits that TS (0) <p> technical difficulty. 3 Analyses and simulations for data compression The MTF algorithm has proved useful in the development of the locally adaptive compression scheme of <ref> [3] </ref>. Motivated by this result, we consider a similar algorithm based on TS (0). We assume the reader is somewhat familiar with the system of [3], which was briefly described in the introduction. 10 3.1 Theoretical results Let B T S (~p) be the expected number of bits that TS (0) needs to encode one symbol in an input sequence that is generated by ~p = (p 1 ; p 2 ; : : :; p <p> In order to analyze B T S (~p), we have to specify how an integer j should be encoded. We use a variable length prefix code by Elias [6] which encodes the integer j using 1 + blog jc + 2blog (1 + log j)c bits. Bentley et al. <ref> [3] </ref> showed that, using this prefix code, the expected number of bits needed by the MTF algorithm is B MT F (~p) 1 + H (~p) + 2 log (1 + H (~p)); for all ~p. <p> Similarly, let A MT F (S) be the average number of bits needed by the MTF algorithm. Again, we assume that an integer j is encoded by means of the Elias encoding that requires 1 + blog jc+ 2blog (1 + log j)c bits. Bentley et al. <ref> [3] </ref> show that for any input sequence S, A MT F (S) 1 + H (S) + 2 log (1 + H (S)) where H (S) = P n m i m i ) is the "empirical entropy" of S. <p> For convenience, we placed no memory limitation on the compressor or decompressor; that is, the length of the list was allowed to grow as large as necessary. In practice one might wish to devise a more memory-efficient scheme, using the list as a cache as in <ref> [3] </ref>. The results of Table 2 reflect the compression achieved 1 , including only the token stream and not the dictionary. As one might expect, the gains from TS in this situation are less dramatic; however, they do reach 1% on large documents.
Reference: [4] <author> M. Burows and D.J. Wheeler. </author> <title> A block-sorting lossless data compression algorithm. </title> <note> DEC SRC Research Report 124, </note> <year> 1994. </year>
Reference-contexts: They implemented the new compression scheme but their simulations do not show an explicit comparison between MTF and MTF with secondary lists. Also recently, a fast and efficient compression scheme that uses MTF encoding as a subroutine has been developed <ref> [4] </ref>. <p> Surprisingly TS performs worse than MTF in one instance, underscoring an important point: TS is not guaranteed to perform better than MTF. We have also attempted to use TS (0) encoding in place of MTF encoding in the data compression algorithm recently presented by Burrows and Wheeler <ref> [4] </ref>. Unfortunately, the results here show less 1 Because the current implementation handles only ASCII characters, we do not have results for all files. 13 File TS (0) MTF Original Bytes % Orig. Bytes % Orig.
Reference: [5] <author> F.R.K. Chung, </author> <title> D.J. Hajela and P.D. Seymour. Self-organizing sequential search and Hilbert's inequality. </title> <booktitle> Proc. 17th Annual Symposium on the Theory of Computing, </booktitle> <pages> pages 217-223, </pages> <year> 1985. </year>
Reference-contexts: Rivest [12] showed that for all ~p, E F C (~p)=E ST AT (~p) = 1. However, the algorithm FC has the drawback that it adapts very slowly to changing probability distributions. Chung et al. <ref> [5] </ref> analyzed the MTF rule and proved E MT F (~p)=E ST AT (~p) 2 1:5708 for all ~p. <p> ; p n ) is X X p i p 3 j p 2 (p i + p j ) 3 = i;j i + 6p i p j + p 2 2 (p i + p j ) 3 + 2 We can now adapt the techniques presented in <ref> [5] </ref> to bound the ratio between E T S (~p) and E ST AT (~p). We assume p 1 p 2 : : : p n . <p> R 1 0 G (x; y)f (x)f (y)dxdy R 1 R 1 0 min (x; y)f (x)f (y)dxdy 0 Proof: Set F (x) = R x 1 f (x)dx: Then R 1 R 1 0 min (x; y)f (x)f (y)dxdy = R 1 0 F 2 (x)dx. [Lemma 2 of <ref> [5] </ref>] Similarly, Z 1 Z 1 G (x; y)f (x)f (y)dxdy = Z 1 f (x)dx G (x; y)F (y)j 1 Z 1 @G F (y)dy = 0 0 @y f (x)F (y)dxdy = 0 0 @x@y F (x)F (y)dxdy = 0 0 H (x; y)F (x)F (y)dxdy 0 0 H
Reference: [6] <author> P. Elias. </author> <title> Universal codeword sets and the representation of the integers. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 21 </volume> <pages> 194-203, </pages> <year> 1975. </year>
Reference-contexts: We assume p i &gt; 0 for all i. In order to analyze B T S (~p), we have to specify how an integer j should be encoded. We use a variable length prefix code by Elias <ref> [6] </ref> which encodes the integer j using 1 + blog jc + 2blog (1 + log j)c bits. <p> The compression is performed by turning the document into a token stream. The tokens are then encoded by their position on the list using standard variable-length prefix encodings given by Elias <ref> [6] </ref>; each integer i requires 1 + 2blog ic bits. We can compare the compression of MTF and TS compression by varying the adaptive discipline of the list.
Reference: [7] <author> G.H. Gonnet, J.I. Munro and H. Suwanda. </author> <title> Towards self-organizing linear search. </title> <booktitle> In Proc. 19th Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 169-174, </pages> <year> 1979. </year> <month> 15 </month>
Reference-contexts: However, the algorithm FC has the drawback that it adapts very slowly to changing probability distributions. Chung et al. [5] analyzed the MTF rule and proved E MT F (~p)=E ST AT (~p) 2 1:5708 for all ~p. This bound is tight because Gonnet et al. <ref> [7] </ref> showed that one can find ~p 0 with E MT F (~p 0 )=E ST AT (~p 0 ) ff for any ff arbitrarily close to 2 .
Reference: [8] <author> D. Grinberg, S. Rajagopalan, R. Venkatesan and V.K. Wei. </author> <title> Splay trees for data compression. </title> <booktitle> In Proc. of the 6th Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 522-530, </pages> <year> 1995. </year>
Reference-contexts: By Shannon's source coding theorem, this is optimal, up to a constant factor. Recently, Grinberg et al. <ref> [8] </ref> proposed a modification of the MTF encoding, which they call MTF encoding with secondary lists. They implemented the new compression scheme but their simulations do not show an explicit comparison between MTF and MTF with secondary lists.
Reference: [9] <author> G.H. Hardy, J.E. Littlewood and G. Polya. </author> <title> Inequalities. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, England, </address> <year> 1967. </year>
Reference-contexts: For j = 1; : : : ; n, let q ij be the asymptotic probability that x i is at position j in TS (0)'s list. The expected number of bits to encode the symbol x i is P n j=1 q ij f (j), which, by Jensen's <ref> [9] </ref> inequality, is at most f ( j=1 q ij j). Jensen's inequality states that for any concave function f and any set fw 1 ; w 2 ; : : :; w n g of positive reals, P n P n i=1 w i y i ).
Reference: [10] <author> S. Irani. </author> <title> Two results on the list update problem. </title> <journal> Information Processing Letters, </journal> <volume> 38 </volume> <pages> 301-306, </pages> <year> 1991. </year>
Reference-contexts: In the same way we can estimate OPT's cost on a request sequence . This technique of evaluating cost by considering pairs of items was also used in <ref> [2, 10, 1] </ref>. In the second part of the analysis we show that, for each pair fx; yg, the asymptotic expected cost paid by TS (0) on a phase is at most 3 2 times the asymptotic expected cost incurred by OPT.
Reference: [11] <author> R. Karp and P. Raghavan. </author> <title> From a personal communication cited in [13]. </title>
Reference-contexts: Sleator and Tarjan proved that the MTF algorithm is 2-competitive. They also showed that the algorithms T and FC are not c-competitive for any constant c. The competitive ratio of 2 is the best ratio that a deterministic online algorithm for the list update problem can achieve <ref> [11] </ref>. In classical data compression, it is often assumed that a discrete memoryless source generates a string S to be compressed. The string S consists of symbols, where each symbol is an element in the alphabet = fx 1 ; x 2 ; : : : ; x n g.
Reference: [12] <author> R. Rivest. </author> <title> On self-organizing sequential search heuristics. </title> <journal> Communication of the ACM, </journal> <volume> 19 </volume> <pages> 63-67, </pages> <year> 1976. </year>
Reference-contexts: For any algorithm A, let E A (~p) denote the asymptotic expected cost incurred by algorithm A in serving one request in a request sequence generated by the distribution ~p. Rivest <ref> [12] </ref> showed that for all ~p, E F C (~p)=E ST AT (~p) = 1. However, the algorithm FC has the drawback that it adapts very slowly to changing probability distributions. <p> p i p j (p i p j ) 2 ! Corollary 1 For any probability distribution ~p = (p 1 ; p 2 ; : : : ; p n ), E MT F (~p) E T S (~p) = 1ijn (p i p j ) 2 Proof: Rivest <ref> [12] </ref> showed E MT F (~p) = P 2p i p j p i +p j : Using part b) of Theorem 1, the result follows immediately. 2.2 Performance against dynamic o*ine algorithms Theorem 2 For any probability distribution ~p = (p 1 ; p 2 ; : : : ;
Reference: [13] <author> N. Reingold, J. Westbrook and D.D. Sleator. </author> <title> Randomized competitive algorithms for the list update problem. </title> <journal> Algorithmica, </journal> <volume> 11(1) </volume> <pages> 15-32, </pages> <year> 1994. </year>
Reference: [14] <author> D.D. Sleator and R.E. Tarjan. </author> <title> Amortized efficiency of list update and paging rules. </title> <journal> Communication of the ACM, </journal> <volume> 28 </volume> <pages> 202-208, </pages> <year> 1985. </year>
Reference-contexts: If x has not been requested so far, leave the position of x unchanged. In [1] it was shown that the algorithm achieves a competitive ratio of 2 on any request sequence, as does Move-to-front <ref> [14] </ref>. Here we demonstrate that TS (0) performs better on distributions, both by developing a formula for the expected cost per request, and by comparing TS (0) with the optimal static and dynamic o*ine algorithms. <p> More recent research on the list update problem was inspired by Sleator and Tarjan <ref> [14] </ref> who suggested to compare the performance of an online algorithm to that of an optimal o*ine algorithm. An optimal o*ine algorithm knows the entire request sequence in advance and can serve it with minimum cost.
Reference: [15] <author> B. Teia. </author> <title> A lower bound for randomized list update algorithms. </title> <journal> Information Processing Letters, </journal> <volume> 47 </volume> <pages> 5-9, </pages> <year> 1993. </year>
Reference-contexts: It is worthwhile to notice that 1.5 is the best lower bound currently known on the competitiveness that can be achieved by randomized list update algorithms against the oblivious adversary <ref> [15] </ref>. Thus, the performance ratio of TS (0) on distributions is at least as good as the performance ratio of randomized algorithms on any input. Finally we evaluate TS (0) against the optimal static ordering and show, for all ~p, E T S (~p)=E ST AT (~p) 1:34.
Reference: [16] <author> I.H. Witten and T. Bell. </author> <title> The Calgary/Canterbury text compression corpus. </title> <note> Anonymous ftp from ftp.cpsc.ucalgary.ca : /pub/text.compression/corpus/text.compression.corpus.tar.Z. 16 </note>
References-found: 16

