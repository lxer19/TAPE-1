URL: http://www.cs.colostate.edu/~vision/ps/1997/tr97-122.ps.gz
Refering-URL: http://www.cs.colostate.edu/~vision/html/publications.html
Root-URL: 
Email: ross@cs.colostate.edu  
Phone: Phone: (970) 491-5792 Fax: (970) 491-2466  
Title: Users Guide  
Author: J. Ross Beveridge 
Web: WWW: http://www.cs.colostate.edu  
Address: Fort Collins, CO 80523-1873  
Date: November 20, 1997  
Affiliation: Colorado State University  Computer Science Department Colorado State University  
Note: LiME  This work was sponsored by the Defense Advanced Research Projects Agency (ARPA) through the Topographic Engineering Center administered by the U.S. Army Research Office Scientific Services Program and monitored by Battelle. (Grant DAAL03 91-C-0034, TCN 96188 D.O. #1958)  
Pubnum: Technical Report CS-97-122  
Abstract: Computer Science Technical Report 
Abstract-found: 1
Intro-found: 1
Reference: [AF86] <author> N. Ayache and O. D. Faugeras. </author> <title> Hyper: A new approach for the recognition and positioning of 2-d objects. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 8(1):44 - 54, </volume> <month> January </month> <year> 1986. </year>
Reference-contexts: This extension appeared in [BWR90], and while we did not realize this at the time, we soon discovered our fitting procedure is an extension of similar work by Ayache <ref> [AF86] </ref>. LiME has two different local search algorithms available, Hamming-distance 1 steepest descent and subset-convergent. An early comparison of the two appeared in [Bev92] and a more thorough comparison appears as part of [JRBG97].
Reference: [Bev92] <author> J. Ross Beveridge. </author> <title> Comparing Subset-convergent and Variable-depth Local Search on Perspective Sensitive Landmark Recognition Problems. </title> <booktitle> In Proceedings: SPIE Intelligent Robots and Computer Vision XI: Algorithms, Techniques, and Active Vision, </booktitle> <volume> volume 1825, </volume> <pages> pages 168 - 179. SPIE, </pages> <month> November </month> <year> 1992. </year> <month> 26 </month>
Reference-contexts: LiME has two different local search algorithms available, Hamming-distance 1 steepest descent and subset-convergent. An early comparison of the two appeared in <ref> [Bev92] </ref> and a more thorough comparison appears as part of [JRBG97]. This latter paper addresses the basic question of how run-time scales as a function of problem size. Both the algorithms and data used in this paper come as part of the LiME distribution.
Reference: [Bev93] <author> J. Ross Beveridge. </author> <title> Local Search Algorithms for Geometric Object Recognition: Optimal Correspondence and Pose. </title> <type> PhD thesis, </type> <institution> University of Massachusetts at Amherst, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: During local search, LiME can be instructed to draw each new best-fit configuration of the model relative to the data. Fitting and the associated match error are fully described in <ref> [Bev93] </ref>. Our work on optimal line segment matching began with the random starts local search algorithms and first considered rigid 2D objects [BWR89]. We quickly discovered that fitting 2D models to data is becomes simpler, not more complex, when scale is allowed to vary. <p> Take care when using a value of less than 1:5 for the Scale Range. All of the match error parameters correspond to parameters described in Beveridge's Thesis <ref> [Bev93] </ref>.
Reference: [BHR86] <author> J. B. Burns, A. R. Hanson, and E. M. Riseman. </author> <title> Extracting straight lines. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-8(4):425 - 456, </volume> <month> July </month> <year> 1986. </year>
Reference-contexts: Data is also represented simply as a set of straight line segments. In our own past work with LiME, line segments have been extracted from imagery using the Burns Algorithm <ref> [BHR86] </ref>. However, a variety of straight line extraction algorithms have been developed and might be used [NB80, LB82]. If you are using LiME as part of the IUE (DARPA Image Understanding Environment, see Section 1.2 below) then you will also have access to our implementation of the Burns algorithm.
Reference: [BWR89] <author> J. Ross Beveridge, Rich Weiss, and Edward M. Riseman. </author> <title> Optimization of 2-dimensional model matching. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 815 - 830, </pages> <address> Los Altos, CA, </address> <month> June </month> <year> 1989. </year> <title> DARPA, </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Fitting and the associated match error are fully described in [Bev93]. Our work on optimal line segment matching began with the random starts local search algorithms and first considered rigid 2D objects <ref> [BWR89] </ref>. We quickly discovered that fitting 2D models to data is becomes simpler, not more complex, when scale is allowed to vary.
Reference: [BWR90] <author> J. Ross Beveridge, Rich Weiss, and Edward M. Riseman. </author> <title> Combinatorial Optimization Applied to Variable Scale 2D Model Matching. </title> <booktitle> In Proceedings of the IEEE International Conference on Pattern Recognition 1990, Atlantic City, </booktitle> <pages> pages 18 - 23. </pages> <publisher> IEEE, </publisher> <month> June </month> <year> 1990. </year>
Reference-contexts: Our work on optimal line segment matching began with the random starts local search algorithms and first considered rigid 2D objects [BWR89]. We quickly discovered that fitting 2D models to data is becomes simpler, not more complex, when scale is allowed to vary. This extension appeared in <ref> [BWR90] </ref>, and while we did not realize this at the time, we soon discovered our fitting procedure is an extension of similar work by Ayache [AF86]. LiME has two different local search algorithms available, Hamming-distance 1 steepest descent and subset-convergent.
Reference: [DWG97] <author> C. Guerra-Salcedo D. Whitley, J. R. Beveridge and C. Graves. </author> <title> Messy Genetic Algorithms for Subset Feature Selection. </title> <booktitle> In Proc. 1997 International Conference on Genetic Algorithms, </booktitle> <pages> pages 568 - 575, </pages> <month> July </month> <year> 1997. </year>
Reference-contexts: The other two algorithms have been developed more recently. The key-feature algorithm is explained and compared to random starts local search in [JRBS97]. The messy genetic algorithm is explained and compared to random starts local search in <ref> [DWG97] </ref>. The key-feature algorithm begins by finding spatially proximate triples of model and data line segments and ranking these from best to worst according the match error. Then a simplified local search is used to fill-out a match from this initial triple.
Reference: [GDKH93] <author> David E. Goldberg, Kalyanmoy Deb, Hillol Kargupta, and Georges Harik. </author> <title> Rapid, accurate optimization of difficult problems using fast messy genetic algorithms. </title> <editor> In Stephanie Forrest, editor, </editor> <booktitle> Proc. 5th International Conference on Genetic Algorithms, </booktitle> <pages> pages 56-64. </pages> <publisher> Morgan-Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: However, rather than attempt to build matches independently off the k best triples, it places these triples into a population and uses a type of genetic algorithm, a Messy GA <ref> [GDKH93] </ref>, to iteratively rank, select and recombine elements from the population until larger and better matches emerge. It has been our own experience to date that the messy genetic algorithm appears to best combine speed with robust performance.
Reference: [JRBG97] <author> Edward M. Riseman J. Ross Beveridge and Christopher R. Graves. </author> <title> How Easy is Matching 2D Line Models Using Local Search? IEEE Trans. </title> <journal> on Pattern Analysis and Machine Intelligence, </journal> <volume> 19(6):564 - 579, </volume> <month> June </month> <year> 1997. </year>
Reference-contexts: LiME has two different local search algorithms available, Hamming-distance 1 steepest descent and subset-convergent. An early comparison of the two appeared in [Bev92] and a more thorough comparison appears as part of <ref> [JRBG97] </ref>. This latter paper addresses the basic question of how run-time scales as a function of problem size. Both the algorithms and data used in this paper come as part of the LiME distribution. The other two algorithms have been developed more recently.
Reference: [JRBS97] <author> Christopher R. Graves J. Ross Beveridge and Jim Steinborn. </author> <title> Comparing Random-Starts Local Search with Key-Feature matching. </title> <booktitle> In Proc. 1997 International Joint Conference on Artificial Intelligence, page (to appear), </booktitle> <month> August </month> <year> 1997. </year>
Reference-contexts: Both the algorithms and data used in this paper come as part of the LiME distribution. The other two algorithms have been developed more recently. The key-feature algorithm is explained and compared to random starts local search in <ref> [JRBS97] </ref>. The messy genetic algorithm is explained and compared to random starts local search in [DWG97]. The key-feature algorithm begins by finding spatially proximate triples of model and data line segments and ranking these from best to worst according the match error.
Reference: [LB82] <author> David G. Lowe and T. O. Binford. </author> <title> Segmentation and Aggregation: An Approach To Figure Ground Phenomena. </title> <booktitle> In Proc. ARPA Image Understanding Workshop, </booktitle> <year> 1982. </year>
Reference-contexts: Data is also represented simply as a set of straight line segments. In our own past work with LiME, line segments have been extracted from imagery using the Burns Algorithm [BHR86]. However, a variety of straight line extraction algorithms have been developed and might be used <ref> [NB80, LB82] </ref>. If you are using LiME as part of the IUE (DARPA Image Understanding Environment, see Section 1.2 below) then you will also have access to our implementation of the Burns algorithm.
Reference: [NB80] <author> R. Nevatia and R Babu. </author> <title> Linear feature extraction and description. Computer Vision, </title> <journal> Graphics, and Image Processing, </journal> <volume> 13:257 - 269, </volume> <year> 1980. </year>
Reference-contexts: Data is also represented simply as a set of straight line segments. In our own past work with LiME, line segments have been extracted from imagery using the Burns Algorithm [BHR86]. However, a variety of straight line extraction algorithms have been developed and might be used <ref> [NB80, LB82] </ref>. If you are using LiME as part of the IUE (DARPA Image Understanding Environment, see Section 1.2 below) then you will also have access to our implementation of the Burns algorithm.
Reference: [WS90] <author> D. Whitley and T. Starkweather. </author> <title> Genitor ii: A distributed genetic algorithm. </title> <journal> Journal of Experimental and Theoretical Artificial Intelligence, </journal> <volume> 2(3):189 - 214, </volume> <year> 1990. </year> <month> 27 </month>
Reference-contexts: However, in this case it is expected that a larger value should be used: it is better to include more triples/doubles rather than fewer in the initial population. The last parameter for the Messy GA is the maximum number of generations to run. The Messy GA uses the Genitor <ref> [WS90] </ref> selection strategy and therefore the definition of a generation may strike some as counter intuitive. In Genitor, a generation involves the selection and recombination of a single pair of parents. The number of generations should typically be in the thousands.
References-found: 13

