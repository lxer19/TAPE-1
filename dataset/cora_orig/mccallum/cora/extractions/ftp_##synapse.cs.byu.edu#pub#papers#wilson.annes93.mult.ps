URL: ftp://synapse.cs.byu.edu/pub/papers/wilson.annes93.mult.ps
Refering-URL: ftp://synapse.cs.byu.edu/pub/papers/details.html
Root-URL: 
Email: email: randy@axon.cs.byu.edu  email: martinez@cs.byu.edu  
Title: The Importance of Using Multiple Styles of Generalization  
Author: D. Randall Wilson Tony R. Martinez 
Address: Provo, Utah 84602, U.S.A.  
Affiliation: Computer Science Department Brigham Young University,  
Date: 54-57, November 1993.  
Note: Proceedings of the First New Zealand International Conference on Artificial Neural Networks and Expert Systems (ANNES), pp.  
Abstract: There are many ways for a learning system to generalize from training set data. There is likely no one style of generalization which will solve all problems better than any other style, for different styles will work better on some applications than others. This paper presents several styles of generalization and uses them to suggest that a collection of such styles can provide more accurate generalization than any one style by itself. Empirical results of generalizing on several real-world applications are given, and comparisons are made on the generalization accuracy of each style of generalization. The empirical results support the hypothesis that using multiple generalization styles can improve generalization accuracy. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Lippmann, Richard P., </author> <title> An Introduction to Computing with Neural Nets, </title> <journal> IEEE ASSP Magazine, </journal> <volume> 3, no. 4, </volume> <pages> pp. 4-22, </pages> <month> April </month> <year> 1987. </year>
Reference: [2] <author> Rumelhart, D. E., and J. L. McClelland, </author> <title> Parallel Distributed Processing, </title> <publisher> MIT Press, Ch. </publisher> <pages> 8, pp. 318-362, </pages> <year> 1986. </year>
Reference: [3] <author> Widrow, Bernard, and Michael A. Lehr, </author> <title> 30 Years of Adaptive Neural Networks: Perceptron, Madaline, and Backpropagation, </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 78, no. 9, </volume> <pages> pp. 1415-1441, </pages> <month> September </month> <year> 1990. </year>
Reference: [4] <author> Duda, R. O., and P. E. </author> <title> Hart (1973). Pattern Classification and Scene Analysis. </title> <address> New York, NY: </address> <publisher> Wiley. </publisher>
Reference-contexts: Euclidean distance is similar to the State Difference scheme, except that the individual distances are squared before they are summed. The State Difference and Euclidean Distance measures have been called the Minkowskian metric with r=1 and r=2, respectively <ref> [4] </ref>. Regardless of the distance metric used, there will often be several prototypes which are the same distance from the new input. Instead of arbitrarily choosing one of the several equally close prototypes to be the winner, the prototypes can vote for their output.
Reference: [5] <author> Martinez, Tony R., </author> <title> Adaptive Self-Organizing Concurrent Systems, </title> <booktitle> Progress in Neural Networks, </booktitle> <volume> 1, ch. 5, </volume> <pages> pp. 105-126, </pages> <editor> O. Omidvar (Ed), </editor> <publisher> Ablex Publishing, </publisher> <year> 1990. </year>
Reference-contexts: Each feature has only the output and one input variable asserted, and the rest of the input variables are dont 5 care variables (denoted by an asterisk, *). Such features are similar to rule-based instances with only one variable asserted <ref> [5] </ref>. It is possible to use combinations of variables instead of only one variable at a time and thus create higher-order features, but such methods run into exponential factors in both storage and computation time, and are thus not pursued in this research.
Reference: [6] <author> Murphy, P., and D. W. Aha, </author> <title> UCI Repository of Machine Learning Databases [Machine-readable data repository at ics.uci.edu], </title> <address> Irvine, CA: </address> <institution> University of California, Department of Information and Computer Science, </institution> <year> 1993. </year>
Reference-contexts: In each case, a features voting power equals its count, which is the number of instances it represents. 3. Empirical Results Each of the generalization styles described above was implemented and tested on several well-known training sets from the collection of Machine Learning Databases at University of California Irvine <ref> [6] </ref>. Out of n instances in each database, n-1 instances were used in the training set, 6 and n instances were used in the test set, using a leave-one-out method.
References-found: 6

