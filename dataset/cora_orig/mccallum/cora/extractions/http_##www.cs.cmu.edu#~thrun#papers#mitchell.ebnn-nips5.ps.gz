URL: http://www.cs.cmu.edu/~thrun/papers/mitchell.ebnn-nips5.ps.gz
Refering-URL: http://www.cs.cmu.edu/~thrun/papers/full.html
Root-URL: 
Email: E-mail: mitchell@cs.cmu.edu  thrun@uran.informatik.uni-bonn.de  
Title: Explanation-Based Neural Network Learning for Robot Control  
Author: C.L. Giles, S.J. Hanson, and J.D. Cowan (eds.) Tom M. Mitchell Sebastian B. Thrun 
Affiliation: School of Computer Science Carnegie Mellon University  University of Bonn Institut fur Informatik III  
Date: 1992  
Address: San Mateo, CA,  Pittsburgh, PA 15213  Romerstr. 164, D-5300 Bonn, Germany  
Note: To appear in: Advances in Neural Information Processing Systems 5  Morgan Kaufmann,  
Abstract: How can artificial neural nets generalize better from fewer examples? In order to generalize successfully, neural network learning methods typically require large training data sets. We introduce a neural network learning method that generalizes rationally from many fewer data points, relying instead on prior knowledge encoded in previously learned neural networks. For example, in robot control learning tasks reported here, previously learned networks that model the effects of robot actions are used to guide subsequent learning of robot control functions. For each observed training example of the target function (e.g. the robot control policy), the learner explains the observed example in terms of its prior knowledge, then analyzes this explanation to infer additional information about the shape, or slope, of the target function. This shape knowledge is used to bias generalization in the learned target function. Results are presented applying this approach to a simulated robot task based on reinforcement learning.
Abstract-found: 1
Intro-found: 1
Reference: [ Barto et al., 1991 ] <author> Andy G. Barto, Steven J. Bradtke, and Satinder P. Singh. </author> <title> Real-time learning and control using asynchronous dynamic programming. </title> <type> Technical Report COINS 91-57, </type> <institution> Department of Computer Science, University of Massachusetts, </institution> <address> MA, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: for the evaluation function Q: b Q (s 1 ; a 1 ) := R b Q (s 2 ; a 2 ) := R b Q (s 3 ; a 3 ) := R 1 This approach to learning a policy is adopted from recent research on reinforcement learning <ref> [ Barto et al., 1991 ] </ref> . produce the final reward R. The domain knowledge represented by neural network action models is used to post-facto predict and analyze each step of the observed episode.
Reference: [ Baum and Haussler, 1989 ] <author> Eric Baum and David Haussler. </author> <title> What size net gives valid generalization? Neural Computation, </title> <booktitle> 1(1) </booktitle> <pages> 151-160, </pages> <year> 1989. </year>
Reference-contexts: 1 Introduction Neural network learning methods generalize from observed training data to new cases based on an inductive bias that is similar to smoothly interpolating between observed training points. Theoretical results [ Valiant, 1984 ] , <ref> [ Baum and Haussler, 1989 ] </ref> on learnability, as well as practical experience, show that such purely inductive methods require dramatically increasing numbers of training examples to learn functions of increasing complexity.
Reference: [ DeJong and Mooney, 1986 ] <author> Gerald DeJong and Raymond Mooney. </author> <title> Explanation-based learning: An alternative view. </title> <journal> Machine Learning, </journal> <volume> 1(2) </volume> <pages> 145-176, </pages> <year> 1986. </year>
Reference-contexts: Explanation-based neural network learning (EBNN) is a method that generalizes from fewer training examples, relying instead on prior knowledge encoded in previously learned networks that encode domain knowledge. EBNN is a neural network analogue to symbolic explanation-based learning methods (EBL) <ref> [ DeJong and Mooney, 1986 ] </ref> , [ Mitchell et al., 1986 ] . Symbolic EBL methods generalize based upon pre-specified domain knowledge represented by collections of symbolic rules.
Reference: [ Jordan and Rumelhart, 1990 ] <author> Michael I. Jordan and David E. Rumelhart. </author> <title> Forward models: Supervised learning with a distal teacher. </title> <note> submitted to Cognitive Science, </note> <year> 1990. </year>
Reference-contexts: A major difference from other model-based approaches to robot learning, such as Sutton's DYNA architecture [ Sutton, 1990 ] or Jordan/Rumelhart's distal teacher method <ref> [ Jordan and Rumelhart, 1990 ] </ref> , is the ability of EBNN to operate across the spectrum of strong to weak domain theories (using LOB*). EBNN has been found to degrade gracefully as the accuracy of the domain theory decreases.
Reference: [ Lin, 1991 ] <author> Long-Ji Lin. </author> <title> Programming robots using reinforcement learning and teaching. </title> <booktitle> In Proceedings of AAAI-91, </booktitle> <address> Menlo Park, CA, July 1991. </address> <publisher> AAAI Press / The MIT Press. </publisher>
Reference-contexts: We found empirically that this technique outperformed Tangent Prop in the domain at hand. 3 We also applied an experience replay technique proposed by Lin <ref> [ Lin, 1991 ] </ref> in order to optimally exploit the information given by the observed training episodes.
Reference: [ Mitchell et al., 1986 ] <author> Tom M. Mitchell, Rich Keller, and Smadar Kedar-Cabelli. </author> <title> Explanation-based generalization: A unifying view. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 47-80, </pages> <year> 1986. </year>
Reference-contexts: Explanation-based neural network learning (EBNN) is a method that generalizes from fewer training examples, relying instead on prior knowledge encoded in previously learned networks that encode domain knowledge. EBNN is a neural network analogue to symbolic explanation-based learning methods (EBL) [ DeJong and Mooney, 1986 ] , <ref> [ Mitchell et al., 1986 ] </ref> . Symbolic EBL methods generalize based upon pre-specified domain knowledge represented by collections of symbolic rules.
Reference: [ Pratt, 1993 ] <author> Lori Y. Pratt. </author> <title> Discriminability-based transfer between neural networks. Same volume. </title>
Reference: [ Rumelhart et al., 1986 ] <author> David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In D. E. Rumelhart and J. L. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing. </booktitle> <volume> Vol. I + II. </volume> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: In EBNN, this domain knowledge is represented by real-valued neural networks. By using neural network representations, it becomes possible to learn this domain knowledge using training algorithms such as the Backpropagation algorithm <ref> [ Rumelhart et al., 1986 ] </ref> .
Reference: [ Shavlik and Towell, 1989 ] <author> Jude W. Shavlik and G.G. Towell. </author> <title> An approach to combining explanation-based and neural learning algorithms. </title> <journal> Connection Science, </journal> <volume> 1(3) </volume> <pages> 231-253, </pages> <year> 1989. </year>
Reference-contexts: Because the learned action models M i are independent of the particular control task (reward function), this knowledge acquired during one task transfers directly to other tasks. EBNN differs from other approaches to knowledge-based neural network learning, such as Shavlik/Towell's KBANNs <ref> [ Shavlik and Towell, 1989 ] </ref> , in that the domain knowledge and the target function are strictly separated, and that both are learned from scratch.
Reference: [ Simard et al., 1992 ] <author> Patrice Simard, Bernard Victorri, Yann LeCun, and John Denker. </author> <title> Tangent prop a formalism for specifying selected invariances in an adaptive network. </title> <editor> In J. E. Moody, S. J. Hanson, and R. P. Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 895-903, </pages> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In EBNN, the evaluation function is learned by a real-valued function approximator that fits both the target values and target slopes. If this approximator is a neural network, an extended version of the Backpropagation algorithm can be employed to fit these slope constraints as well, as originally shown by <ref> [ Simard et al., 1992 ] </ref> . Their algorithm Tangent Prop extends the Backpropagation error function by a second term measuring the mean square error of the slopes. Gradient descent in slope space is then combined with Backpropagation to minimize both error functions.
Reference: [ Sutton, 1988 ] <author> Richard S. Sutton. </author> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <year> 1988. </year>
Reference-contexts: question, due to the heuristic nature of the assumption LOB*. 2.4 EBNN and Reinforcement Learning To make EBNN applicable to robot learning, we extend it here to a more sophisticated scheme for learning the evaluation function Q, namely Watkins' Q-Learning [ Watkins, 1989 ] combined with Sutton's temporal difference methods <ref> [ Sutton, 1988 ] </ref> . The reason for doing so is the problem of suboptimal action choices in robot learning: Robots must explore their environment, i.e., they must select non-optimal actions. <p> Sutton's TD () <ref> [ Sutton, 1988 ] </ref> can be used to combine both Watkins' Q-Learning and the non-recursive Q-estimation scheme underlying the previous section.
Reference: [ Sutton, 1990 ] <author> Richard S. Sutton. </author> <title> Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <month> June </month> <year> 1990, </year> <pages> pages 216-224, </pages> <year> 1990. </year>
Reference-contexts: A major difference from other model-based approaches to robot learning, such as Sutton's DYNA architecture <ref> [ Sutton, 1990 ] </ref> or Jordan/Rumelhart's distal teacher method [ Jordan and Rumelhart, 1990 ] , is the ability of EBNN to operate across the spectrum of strong to weak domain theories (using LOB*). EBNN has been found to degrade gracefully as the accuracy of the domain theory decreases.
Reference: [ Valiant, 1984 ] <author> Leslie G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27 </volume> <pages> 1134-1142, </pages> <year> 1984. </year>
Reference-contexts: 1 Introduction Neural network learning methods generalize from observed training data to new cases based on an inductive bias that is similar to smoothly interpolating between observed training points. Theoretical results <ref> [ Valiant, 1984 ] </ref> , [ Baum and Haussler, 1989 ] on learnability, as well as practical experience, show that such purely inductive methods require dramatically increasing numbers of training examples to learn functions of increasing complexity.
Reference: [ Watkins, 1989 ] <author> Chris J. C. H. Watkins. </author> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> King's College, </institution> <address> Cambridge, England, </address> <year> 1989. </year>
Reference-contexts: are promising, the generality of this approach is an open question, due to the heuristic nature of the assumption LOB*. 2.4 EBNN and Reinforcement Learning To make EBNN applicable to robot learning, we extend it here to a more sophisticated scheme for learning the evaluation function Q, namely Watkins' Q-Learning <ref> [ Watkins, 1989 ] </ref> combined with Sutton's temporal difference methods [ Sutton, 1988 ] . The reason for doing so is the problem of suboptimal action choices in robot learning: Robots must explore their environment, i.e., they must select non-optimal actions. <p> Such non-optimal actions can have a negative impact on the final reward of an episode which results in both underestimating target values and misleading slope estimates. Watkins' Q-Learning <ref> [ Watkins, 1989 ] </ref> permits non-optimal actions during the course of learning Q.
References-found: 14

