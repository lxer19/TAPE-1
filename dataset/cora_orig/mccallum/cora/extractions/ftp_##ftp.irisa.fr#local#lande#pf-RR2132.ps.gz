URL: ftp://ftp.irisa.fr/local/lande/pf-RR2132.ps.gz
Refering-URL: http://www.irisa.fr/lande/fradet/Fradet.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Compilation of Head and Strong Reduction  PROGRAMME 2 Calcul Symbolique, Programmation et  
Author: Pascal Fradet N Gnie Logiciel 
Date: December 1993  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> E. Astesiano and G. Costa. </author> <title> Languages with reducing reexive types. </title> <booktitle> In 7th Coll. on Automata, Languages and Programming, </booktitle> <volume> LNCS Vol. 85, </volume> <pages> pp. 38-50, </pages> <year> 1980. </year>
Reference-contexts: We consider only head reduction ; typing does not seem to simplify the compilation of strong reduction. Simply typed l-calculus would suit our purposes but would harshly restrict the class of expressions. More exible typing systems are sufficient. One candidate is reexive reducing typing <ref> [1] </ref> which has already been used in [8] to determine the functionality of expressions. It is shown in [1] that we can restrict a language to reexive reducing types without weakening its expressive power. <p> Simply typed l-calculus would suit our purposes but would harshly restrict the class of expressions. More exible typing systems are sufficient. One candidate is reexive reducing typing <ref> [1] </ref> which has already been used in [8] to determine the functionality of expressions. It is shown in [1] that we can restrict a language to reexive reducing types without weakening its expressive power.
Reference: [2] <author> H.P. Barendregt. </author> <title> The Lambda Calculus. Its Syntax and Semantics. </title> <publisher> North-Holland, </publisher> <year> 1981. </year>
Reference: [3] <author> H.P. Barendregt, J.R. Kennaway, J.W. </author> <title> Klop and M.R. Sleep. Needed reduction and spine strategies for the lambda calculus. </title> <journal> Information and Computation, </journal> <volume> Vol. 75, </volume> <pages> pp. 191-231, </pages> <year> 1987. </year>
Reference-contexts: If the expression to reduce is (lx.E)F we know that the whnf of the body E will be needed and so it is safe to reduce E prior to the b-reduction. This computation rule belongs to the so-called spine-strategies <ref> [3] </ref>. It never takes more reductions than normal order and may save duplication of work.
Reference: [4] <author> P. Crgut. </author> <title> An abstract machine for the normalization of l-terms. </title> <booktitle> In Proc. of the ACM Conf. on Lisp and Functional Programming, </booktitle> <pages> pp. 333-340, </pages> <year> 1990. </year>
Reference-contexts: This example indicates the cost of reconstructing expressions. This cost is acceptable when the final goal is to implement symbolic evaluation. When the goal is to evaluate whnfs more efficiently by sharing hnfs then such examples should be avoided using analyses. In <ref> [4] </ref> Crgut gives a function for which the reduction takes n 2 steps when sharing hnfs whereas it takes only n steps using standard wh-reduction. It is also shown that this is the worst case. <p> It is also shown that this is the worst case. Conclusion 15 7 Conclusion Implementation of head and strong reduction has also been studied by Crgut <ref> [4] </ref> and Nadathur and Wilson [13]. Crguts abstract machine is based on De Bruijns notation. Two versions have been developed. The first one evaluates the head or full normal form of the global expression. The second one implements a spine strategy and shares head normal forms. <p> The cps expression is just applied to a special continuation and an index to keep track of the binder length. We presented a way to get rid of this index and suggested applications for our technique. Compared to <ref> [4] </ref> and [13] we do not have a second index or special structures like formal variables. But the main difference is that we proceed by program transformations and stay within the functional framework. Used as a preliminary step our technique allows a standard compiler to evaluate under ls.
Reference: [5] <author> M. J. Fischer. </author> <title> Lambda-calculus schemata. </title> <booktitle> In Proc. of the ACM Conf. on Proving Properties about Programs, Sigplan Notices, </booktitle> <volume> Vol. 7(1), </volume> <pages> pp. </pages> <year> 104-109,1972. </year>
Reference: [6] <author> P. Fradet and D. Le Mtayer. </author> <title> Compilation of functional languages by program transformation. </title> <journal> ACM Trans. on Prog. Lang. and Sys., </journal> <volume> 13(1), </volume> <pages> pp. 21-51, </pages> <year> 1991. </year>
Reference-contexts: Used as a preliminary step our technique allows a standard compiler to evaluate under ls. Thus we can take advantage of all the classical compiling tools like analyses, transformations or simplifications. As already emphasized in <ref> [6] </ref>, another advantage of this approach is that we do not have to introduce an abstract machine which makes correctness proofs simpler. Furthermore, optimizations of this compilation step can be easily expressed and justified in the functional framework.
Reference: [7] <author> G.S. Frandsen and C. Sturtivant. </author> <title> What is an efficient implementation of the l-calculus? In Proc. </title> <booktitle> of the ACM Conf. on Functional Prog. Languages and Comp. Arch., </booktitle> <volume> LNCS Vol. 523, </volume> <pages> pp. 289-312, </pages> <year> 1991. </year>
Reference-contexts: This computation rule belongs to the so-called spine-strategies [3]. It never takes more reductions than normal order and may save duplication of work. A revealing example, taken from <ref> [7] </ref>, is the reduction of A n I where the family of l-expressions A i is defined by A 0 = lx.x I and A n = lh.(lw.w h (w w)) A n-1 .
Reference: [8] <author> M. Georgeff. </author> <title> Transformations and reduction strategies for typed lambda expressions. </title> <journal> ACM Trans. on Prog. Lang. and Sys., </journal> <volume> 6(4), </volume> <pages> pp. 603-631, </pages> <year> 1984. </year>
Reference-contexts: Simply typed l-calculus would suit our purposes but would harshly restrict the class of expressions. More exible typing systems are sufficient. One candidate is reexive reducing typing [1] which has already been used in <ref> [8] </ref> to determine the functionality of expressions. It is shown in [1] that we can restrict a language to reexive reducing types without weakening its expressive power.
Reference: [9] <author> R.J.M. Hughes. Supercombinators, </author> <title> a new implementation method for applicative languages. </title> <booktitle> In Proc. of the ACM Conf. on Lisp and Functional Programming, </booktitle> <pages> pp. 1-10, </pages> <year> 1982. </year>
Reference-contexts: For example, in lazy graph reduction, the implementation of b-reduction (lx.E)F fi b E [F/x] implies making a copy of the body E before the substitution. It is well known that this may lose sharing and work may be duplicated [17]. Program transformations, such as fully lazy lambda-lifting <ref> [9] </ref>, aim at maximizing sharing but duplication of work can still occur. Another approach used to avoid recomputation is to consider alternative evaluation strategies. <p> I x) should have been (l c y. H 0 ) and the enclosing evaluation of (l c x. ) can continue. The problem comes from free variables already instantiated when a new head reduction begins. The simplest solution would be to transform the l-abstractions into supercombinators using l-lifting <ref> [9] </ref>. The supercombinators (not already in hnf) will be applied to A W 0 and their hnfs will be shared naturally. However, it is not clear whether we share the same computations by (spine-)reducing an expression and its supercombinator form.
Reference: [10] <author> D. Kranz, R. Kelsey, J. Rees, P. Hudak, J. Philbin and N. Adams. </author> <title> Orbit: An optimizing compiler for scheme. </title> <booktitle> In proc. of 1986 ACM SIGPLAN Symp. on Comp. Construction, </booktitle> <pages> 219-233, </pages> <year> 1986. </year>
Reference: [11] <author> J. Lamping. </author> <title> An algorithm for lambda calculus optimal reductions. </title> <booktitle> In Proc. of the ACM Conf. on Princ. of Prog. Lang., </booktitle> <pages> pp. 16-30, </pages> <year> 1990. </year>
Reference: [12] <institution> J.-J. Lvy. Rductions correctes et optimales dans le lambda calcul. Thse de doctorat dtat, Paris VII, </institution> <year> 1978. </year>
Reference: [13] <author> G. Nadathur and D.S.Wilson. </author> <title> A representation of lambda terms suitable for operations on their intensions, </title> <booktitle> In Proc. of the ACM Conf. on Lisp and Functional Programming, </booktitle> <pages> pp. 341-348, </pages> <year> 1990. </year>
Reference-contexts: It is also shown that this is the worst case. Conclusion 15 7 Conclusion Implementation of head and strong reduction has also been studied by Crgut [4] and Nadathur and Wilson <ref> [13] </ref>. Crguts abstract machine is based on De Bruijns notation. Two versions have been developed. The first one evaluates the head or full normal form of the global expression. The second one implements a spine strategy and shares head normal forms. <p> One plays the role of our binder level as in section 3 and 4, the other one is needed (only in the second version of the machine) to deal with the problem of free variables in subexpressions exposed in section 6.2. The algorithm presented in <ref> [13] </ref> was motivated by the implementation of lProlog [14]. It evaluates terms to hnf and expressed as an abstract machine, this technique resembles Crguts. It is also based on De Bruijn notation and the machine state includes two indexes. <p> The cps expression is just applied to a special continuation and an index to keep track of the binder length. We presented a way to get rid of this index and suggested applications for our technique. Compared to [4] and <ref> [13] </ref> we do not have a second index or special structures like formal variables. But the main difference is that we proceed by program transformations and stay within the functional framework. Used as a preliminary step our technique allows a standard compiler to evaluate under ls.
Reference: [14] <author> G. Nadathur and D. Miller. </author> <title> An overview of lProlog. </title> <booktitle> In Proc. of the 5th Int. Conf. on Logic Prog., </booktitle> <publisher> MIT Press, </publisher> <pages> pp. 810-827, </pages> <year> 1988. </year>
Reference-contexts: However, there are cases where one would like to reduce under ls to get head normal forms (hnf) or even (strong) normal forms (nf). Specifically, head/strong reduction can be of interest in: program transformations (like partial evaluation) which need to reduce under ls, higher order logic programming like l-prolog <ref> [14] </ref> where unification involves reducing l-terms to normal forms, evaluating data structures coded in l-expressions, compiling more efficient evaluation strategies. A well known tool used to compile (weak) evaluation strategies of functional programs is continuation-passing style (cps) conversion [5][15]. This program transformation makes the evaluation ordering explicit. <p> The algorithm presented in [13] was motivated by the implementation of lProlog <ref> [14] </ref>. It evaluates terms to hnf and expressed as an abstract machine, this technique resembles Crguts. It is also based on De Bruijn notation and the machine state includes two indexes. We described in this paper how to use cps conversion to compile head and strong reduction.
Reference: [15] <author> G.D. Plotkin. </author> <title> Call-by-name, call-by-value and the l-calculus. </title> <booktitle> Theoretical Computer Science, </booktitle> <pages> pp. 125-159, </pages> <year> 1975. </year>
Reference-contexts: Closed normal forms are of the form lx 1 . lx n .x i N 1 N p with 1in and with N = x j | lx 1 . lx n .x k N 1 N p N stands for the standard cps conversion associated with call-by-name <ref> [15] </ref> and is defined in Figure 1. N (lx.E) = lc.c (lx.N (E)) Variables c and f are supposed not to occur in the source term. <p> noted a fi w , for example: Head Reduction 3 N ((lx.E) F) I (lc.(lc.c (lx.N (E))) (lf.f N (F) c)) I a fi w (lx.N (E)) N (F) I The following property states that evaluation of cps expressions simulates the reduction of source expressions ; it is proved in <ref> [15] </ref>. Property 1 If E * fi w W then N (E) I * fi w X a w N (W) I and if W is a whnf then X is a whnf. Further more E does not have a whnf iff N (E) I does not have a whnf.
Reference: [16] <author> J. </author> <title> Staples. A graph-like lambda calculus for which leftmost-outermost reduction is optimal. In Graph Grammars and their Application, </title> <booktitle> LNCS vol. </booktitle> <volume> 73, </volume> <pages> pp. 440-455, </pages> <year> 1978. </year>
Reference-contexts: The complexity of the evaluation drops from exponential to linear. Of course this strategy alone is not optimal (optimal reduction of l-expressions is more complex [11][12]) and work can still be duplicated. But in <ref> [16] </ref> Staples proposes a similar evaluation strategy with the additional rule that substitutions are not carried out inside redexes (they are suspended until the redex is needed and reduced to hnf). <p> Computation can still be duplicated by performing substitutions inside redexes. It would be inter esting to extend our work to compile Staples method <ref> [16] </ref> which avoids this loss of sharing. We did a few experiments using the trivial way (i.e. transforming source expressions before giving them to our compiler). We transformed the family of expressions A n defined in section 6.1 into supercombinators and into cps form.

References-found: 16

