URL: http://www.cs.rice.edu/~adve/CS615/jade-hetero.sc92.ps
Refering-URL: http://www.cs.rice.edu/~adve/comp615.html
Root-URL: 
Title: Heterogeneous Parallel Programming in Jade  
Author: Martin C. Rinard, Daniel J. Scales and Monica S. Lam 
Address: CA 94305  
Affiliation: Computer Systems Laboratory Stanford University,  
Abstract: This paper presents Jade, a high-level parallel programming language for managing coarse-grain concur-rency. Jade simplifies programming by providing the programmer with the abstractions of sequential execution and a shared address space. Jade programmers augment sequential, imperative programs with constructs that declare how parts of the program access data; the Jade implementation dynamically interprets this information to execute the program in parallel. This parallel execution preserves the serial semantics of the original program. Jade has been implemented as an extension to C on shared-memory multiprocessors, a message-passing machine, networks of heterogeneous workstations, and systems with special-purpose functional units. Programs written in Jade run on all of these platforms without modification. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Henri E. Bal, M. Frans Kaashoek, and Andrew S. Tanen-baum. Orca: </author> <title> A Language for Parallel Programming of Distributed Systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 18(3), </volume> <month> March </month> <year> 1992. </year>
Reference-contexts: Each parallel application written in Linda contains an application-specific synchronization algorithm built using the low-level tuple-space primitives. 6.3 Parallel Object-Oriented Languages There are a variety of distributed programming languages that take an object-oriented approach to programming message-passing machines or networks of workstations <ref> [4, 6, 1] </ref>. These languages typically make each object a unit of communication and each method invocation a unit of synchronization. Each method that modifies an object obtains exclusive access to that object; in some systems objects may be replicated to allow concurrent read access.
Reference: [2] <author> John Bennett, John B. Carter, and Willy Zwaenepoel. Munin: </author> <title> Distributed Shared Memory Based on Type-specific Memory Coherence. </title> <booktitle> In Proceedings of the Second ACM/SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 168-176, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: We consider three alternatives in turn: distributed shared memory, Linda, and object-oriented parallel languages. 6.1 Distributed Shared Memory The difficulties associated with managing the movement and replication of data have prompted researchers to look for ways to provide a shared-memory interface to a collection of message passing machines <ref> [2, 11, 15] </ref>. The typical approach is to use a cache-coherence algorithm implemented in software, with pages taking the place of cache lines and the page fault hardware detecting attempts to access remote or invalid pages.
Reference: [3] <author> A. A. </author> <title> Berlin. Partial Evaluation Applied to Numerical Computation. </title> <booktitle> In Proceedings of the 1990 ACM Conference on LISP and Functional Programming, </booktitle> <pages> pages 139-150, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: The Jade implementation uses the excess con-currency present in the computation to hide the latency of fetching remote data. The execution of a Jade program can be viewed as a process of creating and executing a parallel task graph. Some systems <ref> [3, 12] </ref> separate the task generation and execution phases of a computation. However, the Jade implementation overlaps the creation and execution of the task graph. Overlapping task generation and execution allows the machine to get an early start on the computation.
Reference: [4] <author> A. Black, N. Hutchison, E. Jul, and H. Levy. </author> <title> Object Structure in the Emerald System. </title> <booktitle> In Proceedings of the ACM Conference on Object-Oriented Programming Systems, Languages and Applications, </booktitle> <pages> pages 78-86, </pages> <year> 1986. </year>
Reference-contexts: Each parallel application written in Linda contains an application-specific synchronization algorithm built using the low-level tuple-space primitives. 6.3 Parallel Object-Oriented Languages There are a variety of distributed programming languages that take an object-oriented approach to programming message-passing machines or networks of workstations <ref> [4, 6, 1] </ref>. These languages typically make each object a unit of communication and each method invocation a unit of synchronization. Each method that modifies an object obtains exclusive access to that object; in some systems objects may be replicated to allow concurrent read access.
Reference: [5] <author> N. Carriero and D. Gelernter. </author> <title> How to Write Parallel Programs: A Guide to the Perplexed. </title> <journal> ACM Computing Surveys, </journal> <volume> 21(3) </volume> <pages> 323-357, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: One advantage of the distributed shared memory approach over Jade is that the shared memory abstraction is provided completely transparently to the programmer. In Jade, the programmer must specify the granularity of shared objects and declare which variables and data structures reference shared objects. 6.2 Linda Linda <ref> [5] </ref> is an explicitly parallel language that supports the concept of a global tuple space. Linda programmers create parallel processes that interact via asynchronous operations that insert, read and remove tuples from tuple space.
Reference: [6] <author> Jeffrey S. Chase, Franz G. Amador, Edward D. Lazowska, Henry M. Levy, and Richard J. Littlefield. </author> <title> The Amber System: Parallel Programming on a Network of Multiprocessors. </title> <booktitle> In Proceedings of the Twelfth ACM Symposium on Operating Systems, </booktitle> <pages> pages 147-158, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: Each parallel application written in Linda contains an application-specific synchronization algorithm built using the low-level tuple-space primitives. 6.3 Parallel Object-Oriented Languages There are a variety of distributed programming languages that take an object-oriented approach to programming message-passing machines or networks of workstations <ref> [4, 6, 1] </ref>. These languages typically make each object a unit of communication and each method invocation a unit of synchronization. Each method that modifies an object obtains exclusive access to that object; in some systems objects may be replicated to allow concurrent read access.
Reference: [7] <author> Jyh-Herng Chow and William Ludwell Harrison III. </author> <title> Compile-Time Analysis of Parallel Programs that Share Memory. </title> <booktitle> In Record of the Nineteenth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 130-141, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: Jade therefore promotes a hierarchical model of parallel computation in which a high-level programming language allows programmers to exploit coarse-grain, data-dependent concurrency while the compiler exploits fine-grain, concurrency statically available within tasks. The difficulty of applying compiler optimizations to explicitly parallel code <ref> [7, 17] </ref> limits the amount of concurrency that programmers can exploit using explicitly parallel languages. We have implemented Jade in a wide variety of computational environments, from tightly-coupled shared mem ory multiprocessors through networks of workstations to heterogeneous systems with special-purpose accelerators connected by high-speed networks.
Reference: [8] <author> D. E. Culler and Arvind. </author> <title> Resource Requirements of Dataflow Programs. </title> <booktitle> In 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 141-150, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: In this case, the implementation can suspend the original task and execute the InternalUpdate to column 1. In many explicitly parallel languages, suppressing excessive task creation may create deadlock <ref> [8, 13] </ref>. In Jade, on the other hand, the implementation can suppress task creation in the face of excess concurrency without risking deadlock. The serial semantics guarantees that a task never waits for a subsequent task (in the original serial order).
Reference: [9] <author> M. S. Lam and M. C. Rinard. </author> <title> Coarse-Grain Parallel Programming in Jade. </title> <booktitle> In Proceedings of the Third ACM SIG-PLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 94-105, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: We have implemented Jade as an extension to C on shared memory multiprocessors, a homogeneous message passing machine (the Intel iPSC/860), a network of heterogeneous workstations, and a heterogeneous system containing special-purpose graphics accelerators. A previous paper <ref> [9] </ref> describes a preliminary design of Jade that applied only to shared-memory machines. [16] contains a formal treatment of the equivalence between the serial and parallel executions of Jade programs.
Reference: [10] <author> D. Lenoski, K. Gharachorloo, J. Laudon, A. Gupta, J. Hen-nessy, M. Horowitz, and M. Lam. </author> <title> The Stanford DASH Multiprocessor. </title> <journal> Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Jade supports the abstraction of a single shared memory that all tasks can access; each piece of data (statically or dynamically) allocated in this memory is called a shared object. Shared objects are identified in a Jade program via the shared type qualifier. For example: double shared A <ref> [10] </ref>; double shared *B; The first declaration defines a statically allocated vector of doubles, while the second declaration defines a reference (pointer) to a dynamically allocated vector of doubles. Jade programmers use the withonly-do construct to identify a task and to specify how that task will access data. <p> There are no source code modifications required to port Jade applications between these platforms. Our shared-memory implementation of Jade runs on the Silicon Graphics 4D/240S multiprocessor and on the Stanford DASH multiprocessor <ref> [10] </ref>. The workstation implementation of Jade uses PVM [18] as a reliable, typed transport protocol. Currently, this implementation of Jade runs on the SPARC-based Sun workstations and on MIPS-based systems, including the DECSta-tion 3100 and 5000 machines, the Silicon Graphics workstations, and the Stanford DASH multiprocessor.
Reference: [11] <author> Kai Li. IVY: </author> <title> A Shared Virtual Memory System for Parallel Computing. </title> <booktitle> In Proceedings of the 1988 International Conference on Parallel Processing, </booktitle> <pages> pages II 94-101, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: We consider three alternatives in turn: distributed shared memory, Linda, and object-oriented parallel languages. 6.1 Distributed Shared Memory The difficulties associated with managing the movement and replication of data have prompted researchers to look for ways to provide a shared-memory interface to a collection of message passing machines <ref> [2, 11, 15] </ref>. The typical approach is to use a cache-coherence algorithm implemented in software, with pages taking the place of cache lines and the page fault hardware detecting attempts to access remote or invalid pages.
Reference: [12] <author> R. Mirchandaney, J. H. Saltz, R. M. Smith, D. M. Nicol, and K. Crowley. </author> <title> Principles of Runtime Support for Parallel Processors. </title> <booktitle> In Proceedings of the 1988 International Conference on Supercomputing, </booktitle> <pages> pages 140-152, </pages> <year> 1988. </year>
Reference-contexts: The Jade implementation uses the excess con-currency present in the computation to hide the latency of fetching remote data. The execution of a Jade program can be viewed as a process of creating and executing a parallel task graph. Some systems <ref> [3, 12] </ref> separate the task generation and execution phases of a computation. However, the Jade implementation overlaps the creation and execution of the task graph. Overlapping task generation and execution allows the machine to get an early start on the computation.
Reference: [13] <author> E. Mohr, D. A. Kranz, and R. H. Halstead. </author> <title> Lazy Task Creation: a Technique for Increasing the Granularity of Parallel Programs. </title> <booktitle> In Proceedings of the 1990 ACM Conference on LISP and Functional Programming, </booktitle> <pages> pages 185-197, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: In this case, the implementation can suspend the original task and execute the InternalUpdate to column 1. In many explicitly parallel languages, suppressing excessive task creation may create deadlock <ref> [8, 13] </ref>. In Jade, on the other hand, the implementation can suppress task creation in the face of excess concurrency without risking deadlock. The serial semantics guarantees that a task never waits for a subsequent task (in the original serial order).
Reference: [14] <author> J. Duane Northcutt, Gerard A. Wall, James G. Hanko, and Eugene M. Kuerner. </author> <title> A High Resolution Video Workstation. Signal Processing: </title> <journal> Image Communication, </journal> <volume> 4(4 </volume> & 5):445-455, 1992. 
Reference-contexts: Jade applications can use any combination of these kinds of workstations in the course of a single execution. We have also implemented Jade on the Intel iPSC/860 using the native message passing system. Jade also runs on the High Resolution Video (HRV) Workstation <ref> [14] </ref> from Sun Mi-crosystems Laboratories. The machine consists of several SPARC and Intel i860 processors; its functional units are capable of digitizing and displaying video at up to full HDTV rates.
Reference: [15] <author> U. Ramachandran, M. Yousef, and A. Khalidi. </author> <title> An Implementation of Distributed Shared Memory. </title> <journal> Software - Practice and Experience, </journal> <volume> 21(5) </volume> <pages> 443-464, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: We consider three alternatives in turn: distributed shared memory, Linda, and object-oriented parallel languages. 6.1 Distributed Shared Memory The difficulties associated with managing the movement and replication of data have prompted researchers to look for ways to provide a shared-memory interface to a collection of message passing machines <ref> [2, 11, 15] </ref>. The typical approach is to use a cache-coherence algorithm implemented in software, with pages taking the place of cache lines and the page fault hardware detecting attempts to access remote or invalid pages.
Reference: [16] <author> M. C. Rinard and M. S. Lam. </author> <booktitle> Semantic foundations of Jade. In Record of the Nineteenth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 105-118, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: A previous paper [9] describes a preliminary design of Jade that applied only to shared-memory machines. <ref> [16] </ref> contains a formal treatment of the equivalence between the serial and parallel executions of Jade programs. This paper describes the Jade language and provides a detailed example of how to use Jade to parallelize a sparse Cholesky factorization algorithm.
Reference: [17] <author> H. Srinivasan and M. Wolfe. </author> <title> Analyzing Programs with Explicit Parallelism. </title> <editor> In Utpal Banerjee, David Gelernter, Alexandru Nicolau, and David Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 405-419. </pages> <year> 1992. </year>
Reference-contexts: Jade therefore promotes a hierarchical model of parallel computation in which a high-level programming language allows programmers to exploit coarse-grain, data-dependent concurrency while the compiler exploits fine-grain, concurrency statically available within tasks. The difficulty of applying compiler optimizations to explicitly parallel code <ref> [7, 17] </ref> limits the amount of concurrency that programmers can exploit using explicitly parallel languages. We have implemented Jade in a wide variety of computational environments, from tightly-coupled shared mem ory multiprocessors through networks of workstations to heterogeneous systems with special-purpose accelerators connected by high-speed networks.
Reference: [18] <author> V.S. Sunderam. </author> <title> PVM: a framework for parallel distributed computing. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 2(4) </volume> <pages> 315-339, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: There are no source code modifications required to port Jade applications between these platforms. Our shared-memory implementation of Jade runs on the Silicon Graphics 4D/240S multiprocessor and on the Stanford DASH multiprocessor [10]. The workstation implementation of Jade uses PVM <ref> [18] </ref> as a reliable, typed transport protocol. Currently, this implementation of Jade runs on the SPARC-based Sun workstations and on MIPS-based systems, including the DECSta-tion 3100 and 5000 machines, the Silicon Graphics workstations, and the Stanford DASH multiprocessor.
Reference: [19] <author> Songnian Zhou, Michael Stumm, Kai Li, and David Wort-man. </author> <title> Heterogeneous Distributed Shared Memory. </title> <type> Technical Report CSRI-244, </type> <institution> Computer Systems Research Institute, University of Toronto, </institution> <month> September </month> <year> 1990. </year>
Reference-contexts: Each parallel program must therefore have the same address space on all the different machines on which it executes. This restriction has so far limited distributed shared memory systems to homogeneous collections of machines (except for a prototype described in <ref> [19] </ref>). The Jade implementation can do the necessary conversions in a heterogeneous environment because it knows the types of all shared objects. One advantage of the distributed shared memory approach over Jade is that the shared memory abstraction is provided completely transparently to the programmer.
References-found: 19

