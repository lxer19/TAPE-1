URL: http://www.cs.umn.edu/Research/Agassiz/Paper/tsai.phd.ps.Z
Refering-URL: http://www.cs.umn.edu/Research/Agassiz/agassiz_pubs.html
Root-URL: http://www.cs.umn.edu
Title: c  
Author: flCopyright by JENN-YUAN TSAI 
Date: 1998  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> A. Agarwal, D. Chaiken, G. D'Souza, K Johnson, D. Kranz, J. Kubiatowicz, K. Kurihara, B. H. Lim, G. Maa, D. Nussbaum, M. Parkin, and D. A. Yeung. </author> <title> The mit alewife machine: A large-scale distributed-memory multiprocessor. </title> <booktitle> In Proceedings of Workshop on Multithreaded Computers, Supercomputing 91, </booktitle> <month> November </month> <year> 1991. </year>
Reference-contexts: Systems that adopt multithreading for hiding latency include HEP [17], Horizon [22], Tera [2], and Alewife <ref> [1] </ref>. To tolerate memory latency, HEP, Horizon and Tera can accommodate more than one hundred threads per processor and allow fine-grained and fast context switching at every cycle. These machines superpipeline both the memory and ALU operations to increase bandwidth, but they do not use data caches.
Reference: [2] <author> Robert Alverson, David Callahan, Daniel Cummings, Brian Koblenz, Allan Porterfield, and Burton Smith. </author> <title> The Tera computer system. </title> <booktitle> In Conference Proceedings, 1990 International Conference on Supercomputing, </booktitle> <pages> pages 1-6, </pages> <month> June 11-15, </month> <year> 1990. </year>
Reference-contexts: There are several major obstacles which limit the amount of parallelism that can be exploited. Most state-of-the-art microprocessors use single-threaded superscalar architecture to exploit instruction-level parallelism. Its parallelism is limited by the size of the single instruction window. Multithreaded architectures, such as Tera <ref> [2] </ref> and M-Machine [10], rely totally on par-allelizing compilers to partition a program into multiple threads for context switching. <p> Systems that adopt multithreading for hiding latency include HEP [17], Horizon [22], Tera <ref> [2] </ref>, and Alewife [1]. To tolerate memory latency, HEP, Horizon and Tera can accommodate more than one hundred threads per processor and allow fine-grained and fast context switching at every cycle. These machines superpipeline both the memory and ALU operations to increase bandwidth, but they do not use data caches.
Reference: [3] <author> Scott E. Breach, T. N. Vijaykumar, and Gurindar S. Sohi. </author> <title> The anatomy of the register file in a multiscalar processor. </title> <booktitle> In Proceedings of the 27th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 181-190, </pages> <month> November 30-December 2, </month> <year> 1994. </year> <note> [4] .C. </note> <author> Burger and T. M. Austin. </author> <title> The simplescalar tool set, version 2.0. </title> <journal> In Computer Architecture News, </journal> <volume> volume 25 (3), </volume> <pages> pages 13-25, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: Queue registers are also attached with full/empty bits to support data synchronization. Multiscalar allows a thread (task) to forward register data to its successor task through the unidirectional ring connection between processing units. To facilitate register data forwarding and synchronization, the compiler of multiscalar generates a create mask <ref> [3] </ref> for each task which indicates the register values the task may produce. In addition, the last instruction in a task that produces the value is attached with a forward bit to inform the processing element to forward the result of the instruction to its successor tasks.
Reference: [5] <author> Michael Butler, Tse-Yu Yeh, Yale Patt, Mitch Alsup, Hunter Scales, and Michael Shebanow. </author> <title> Single instruction stream parallelism is greater than two. </title> <booktitle> In Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 276-286, </pages> <month> May 27-30, </month> <year> 1991. </year> <month> 108 </month>
Reference-contexts: However, it is known to be very difficult to extract enough parallelism with a single thread of control even for a small number of functional units <ref> [5, 23, 49] </ref>. A single-threaded sequencing mechanism has several major limitations. For example, independent instructions from different basic blocks need to be grouped together in a single instruction stream.
Reference: [6] <author> Ding-Kai Chen and Pen-Chung Yew. </author> <title> Statement reordering for doacross loops. </title> <booktitle> In Proceedings of International Conference on Parallel Processing, </booktitle> <volume> volume Vol. II, </volume> <pages> pages 24-28, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: In the thread pipelining, the compiler will try to increase the execution overlap of concurrent threads by minimizing the stall caused by data dependences between threads. To do this, we can perform statement reordering <ref> [6] </ref> and schedule target stores as early as possible, or schedule load instructions that may be data dependent on the target stores of some predecessor threads as late as possible. 41 5.4 Advanced Compiler Techniques for Superthreading 5.4.1 Conversion of Data Speculation to Control Speculation Some concurrent multiple-threaded architectures, such as
Reference: [7] <author> S. Cho, J.-Y. Tsai, Y. Song, B. Zheng, S. J. Schwinn, X. Wang, Q. Zhao, Z. Li, D. J. Lilja, and P.-C. Yew. </author> <title> High-level information an approach for integrating front-end and back-end compilers. </title> <type> Technical report, </type> <institution> Department of Computer Science and Engineering, University of Minnesota, </institution> <month> February </month> <year> 1998. </year> <type> Technical Report #98-008. </type>
Reference-contexts: The compiler system includes a parallelizing front-end compiler for thread partitioning and thread pipelining and a optimizing back-end compiler for instruction scheduling and register allocation. The front-end compiler and the back-end compiler are integrated with a High-Level Information (HLI) interface <ref> [7] </ref>. With more dedicated compiler support for C program analyses and transformations, we expected the complete implementation of the superthreading compiler can effectively parallelize both C and Fortran programs and generate more efficient superthreaded codes.
Reference: [8] <author> Pradeep K. Dubey, Kevin O'Brien, Kathryn O'Brien, and Charles Barton. </author> <title> Single-program speculative multithreading (SPSM) architecture: Compiler-assisted fine-grained multithread-ing. </title> <booktitle> In Proceedings of the IFIP WG 10.3 Working Conference on Parallel Architectures and Compilation Techniques, PACT '95, </booktitle> <pages> pages 109-121, </pages> <month> June 27-29, </month> <year> 1995. </year>
Reference-contexts: After a slave thread completes its execution, it is merged into the master thread and then retires. Concurrent multithreaded architectures that fall into this category include M-machine [10, 19] and SPSM (Single-Program Speculative Multithreading) architecture <ref> [8] </ref>. In the M-machine, a home H-thread can create concurrent H-threads by executing a special fork operation. Each H-thread executes on a cluster (processing element) and has its own register file. While running, H-threads communicate through registers and can write data to the register file of other H-threads. <p> For programs with a lot of maybe data dependences between threads, the performance will suffer if those ambiguous data dependences do not actually occur at run-time. Some models <ref> [11, 36, 8, 38, 20] </ref> allow the compiler to speculate on those maybe data dependences with run-time data speculation support. We will discuss this approach in the next section in more detail. Another approach is to perform run-time data synchronization based on the history of previous execution [30, 48]. <p> In this section, we review the thread-level speculative execution support for control and data dependences. 2.3.1 Control Speculation Some concurrent multiple-threaded models such as Elementary Multithreading [15], Multiscalar [11, 36], SPSM <ref> [8] </ref>, Traces processors [32, 34], and others [20, 28, 29] support thread-level control speculation. A thread which is control dependent on some conditions determined by earlier threads can be initiated and speculatively executed before the conditions are known. <p> For example, in the address resolution buffer (ARB) of a Multiscalar processor, each active task has its own value field in every entry to save its store data before it is committed. 2.3.2 Data Speculation Multiscalar [11, 36], SPSM <ref> [8] </ref>, Traces processors [32, 34], and others [38, 20, 28, 29] also support speculation on the maybe data dependences with the help of run-time address disambiguation hardware. <p> early as possible, or schedule load instructions that may be data dependent on the target stores of some predecessor threads as late as possible. 41 5.4 Advanced Compiler Techniques for Superthreading 5.4.1 Conversion of Data Speculation to Control Speculation Some concurrent multiple-threaded architectures, such as Multiscalar [11, 36] and SPSM <ref> [8] </ref>, provide hardware support for data speculation. With such hardware support, the compiler can speculate on potential data dependences between threads by assuming that they would not be violated, and rely on the hardware to detect violations at run-time. <p> To achieve high instruction issue rate, the XIMD [52], Elementary Multithreading [15], M-machine [10], Simultaneous Multithreading [47], Multiscalar [11, 36], and SPSM <ref> [8] </ref> machines allow instructions from different threads to be issued and executed concurrently. The Simultaneous Mul-tithreading and SPSM machines support only independent parallel threads, such as parallel loops without cross-iteration dependences (i.e. do-all loops).
Reference: [9] <author> Matthew K. Farrens and Andrew R. Pleszkun. </author> <title> Strategies for achieving improved processor throughput. </title> <booktitle> In Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 362-369, </pages> <month> May 27-30, </month> <year> 1991. </year>
Reference-contexts: Many concurrent multiple-threaded processor architectures have been proposed and studied [2, 8, 9, 10, 11, 14, 15, 17, 18, 19, 31, 36, 47, 46, 52, 32, 38, 28, 29, 20, 21]. Some of them 4 <ref> [9, 14, 18, 31, 47, 46] </ref> are primarily for increasing system throughput by allowing multiple programs (e.g. one program on each thread) to be run concurrently. In this thesis, we focus on models that are primarily for speeding up the execution of one single program.
Reference: [10] <author> Marco Fillo, Stephen W. Keckler, William J. Dally, Nicholas P. Carter, Andrew Chang, Yevgeny Gurevich, and Whay S. Lee. </author> <title> The m-machine multicomputer. </title> <booktitle> In Proceedings of the 28th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 146-156, </pages> <month> November 29-December 1, </month> <year> 1995. </year>
Reference-contexts: The master thread and the slave threads execute in parallel on different processing elements or funational units. After a slave thread completes its execution, it is merged into the master thread and then retires. Concurrent multithreaded architectures that fall into this category include M-machine <ref> [10, 19] </ref> and SPSM (Single-Program Speculative Multithreading) architecture [8]. In the M-machine, a home H-thread can create concurrent H-threads by executing a special fork operation. Each H-thread executes on a cluster (processing element) and has its own register file. <p> This section describes the data communication mechanisms used in some concurrent multithreaded architectures. 2.2.1 Register Data Communication Some concurrent multithreaded architectures such as XIMD [52], Elementary Multithreading [15], M-machine <ref> [10, 19] </ref>, Multiscalar [11, 36], Trace Processor [32, 34] allow concurrent threads to communicate or share register data with each other. In the XIMD architecture, all threads (instruction streams) share a global register file. This approach allows data communication between threads to be very straightforward. <p> There are several major obstacles which limit the amount of parallelism that can be exploited. Most state-of-the-art microprocessors use single-threaded superscalar architecture to exploit instruction-level parallelism. Its parallelism is limited by the size of the single instruction window. Multithreaded architectures, such as Tera [2] and M-Machine <ref> [10] </ref>, rely totally on par-allelizing compilers to partition a program into multiple threads for context switching. Program structures such as pointer aliases, loops with early exits, or complicated dependences across loop iterations cannot be analyzed at compile time, which often resulting in a huge loss in "exploitable" program parallelism. <p> Although its coarse-grained multithreading requires more overhead to switch from one thread to another, the Alewife seeks to avoid memory latency by using a data cache in addition to hiding it. To achieve high instruction issue rate, the XIMD [52], Elementary Multithreading [15], M-machine <ref> [10] </ref>, Simultaneous Multithreading [47], Multiscalar [11, 36], and SPSM [8] machines allow instructions from different threads to be issued and executed concurrently. The Simultaneous Mul-tithreading and SPSM machines support only independent parallel threads, such as parallel loops without cross-iteration dependences (i.e. do-all loops).
Reference: [11] <author> Manoj Franklin and Gurindar S. Sohi. </author> <title> The expandable split window paradigm for exploiting fine-grained parallelism. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 58-67, </pages> <month> May 19-21, </month> <year> 1992. </year> <month> 109 </month>
Reference-contexts: This section describes the data communication mechanisms used in some concurrent multithreaded architectures. 2.2.1 Register Data Communication Some concurrent multithreaded architectures such as XIMD [52], Elementary Multithreading [15], M-machine [10, 19], Multiscalar <ref> [11, 36] </ref>, Trace Processor [32, 34] allow concurrent threads to communicate or share register data with each other. In the XIMD architecture, all threads (instruction streams) share a global register file. This approach allows data communication between threads to be very straightforward. <p> For programs with a lot of maybe data dependences between threads, the performance will suffer if those ambiguous data dependences do not actually occur at run-time. Some models <ref> [11, 36, 8, 38, 20] </ref> allow the compiler to speculate on those maybe data dependences with run-time data speculation support. We will discuss this approach in the next section in more detail. Another approach is to perform run-time data synchronization based on the history of previous execution [30, 48]. <p> In this section, we review the thread-level speculative execution support for control and data dependences. 2.3.1 Control Speculation Some concurrent multiple-threaded models such as Elementary Multithreading [15], Multiscalar <ref> [11, 36] </ref>, SPSM [8], Traces processors [32, 34], and others [20, 28, 29] support thread-level control speculation. A thread which is control dependent on some conditions determined by earlier threads can be initiated and speculatively executed before the conditions are known. <p> For example, in the address resolution buffer (ARB) of a Multiscalar processor, each active task has its own value field in every entry to save its store data before it is committed. 2.3.2 Data Speculation Multiscalar <ref> [11, 36] </ref>, SPSM [8], Traces processors [32, 34], and others [38, 20, 28, 29] also support speculation on the maybe data dependences with the help of run-time address disambiguation hardware. <p> schedule target stores as early as possible, or schedule load instructions that may be data dependent on the target stores of some predecessor threads as late as possible. 41 5.4 Advanced Compiler Techniques for Superthreading 5.4.1 Conversion of Data Speculation to Control Speculation Some concurrent multiple-threaded architectures, such as Multiscalar <ref> [11, 36] </ref> and SPSM [8], provide hardware support for data speculation. With such hardware support, the compiler can speculate on potential data dependences between threads by assuming that they would not be violated, and rely on the hardware to detect violations at run-time. <p> Although its coarse-grained multithreading requires more overhead to switch from one thread to another, the Alewife seeks to avoid memory latency by using a data cache in addition to hiding it. To achieve high instruction issue rate, the XIMD [52], Elementary Multithreading [15], M-machine [10], Simultaneous Multithreading [47], Multiscalar <ref> [11, 36] </ref>, and SPSM [8] machines allow instructions from different threads to be issued and executed concurrently. The Simultaneous Mul-tithreading and SPSM machines support only independent parallel threads, such as parallel loops without cross-iteration dependences (i.e. do-all loops).
Reference: [12] <author> Manoj Franklin and Gurindar S. Sohi. </author> <title> Register traffic analysis for streamlining inter-operation communication in fine-grain parallel processors. </title> <booktitle> In Proceedings of the 25th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 236-245, </pages> <month> December 1-4, </month> <year> 1992. </year>
Reference-contexts: Second, the sequential 7 thread execution order allows it to use a very simple and fast unidirectional ring to connect all processing elements, since data always flow from predecessor threads to successor threads. Concurrent multithreaded architectures falling into this category include Multiscalar <ref> [12, 36] </ref> and the superthreaded architecture presented in this thesis. In the multiscalar paradigm, the control flow graph of a program is transformed into a task graph by the compiler.
Reference: [13] <author> Manoj Franklin and Gurindar S. Sohi. Arb: </author> <title> A hardware mechanism for dynamic reordering of memory references. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 45 </volume> (6):552-571, May 1996. 
Reference-contexts: Tasks execute on the multiple processing elements in parallel but complete and retire in the program sequential order. Each processing element has it own register file and own stage in the shared Address Resolution Buffer (ARB) <ref> [13] </ref> to maintain its speculation state corresponding to register and memory values. When a task is assigned to a processing element, the live register values from its predecessor tasks will be forwarded to its register file through the unidirectional ring.
Reference: [14] <author> Robert H. Halstead, Jr. and Tetsuya Fujita. MASA: </author> <title> A multithreaded processor architecture for parallel symbolic computing. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 443-451, </pages> <month> May 30-June 2, </month> <year> 1988. </year>
Reference-contexts: Many concurrent multiple-threaded processor architectures have been proposed and studied [2, 8, 9, 10, 11, 14, 15, 17, 18, 19, 31, 36, 47, 46, 52, 32, 38, 28, 29, 20, 21]. Some of them 4 <ref> [9, 14, 18, 31, 47, 46] </ref> are primarily for increasing system throughput by allowing multiple programs (e.g. one program on each thread) to be run concurrently. In this thesis, we focus on models that are primarily for speeding up the execution of one single program.
Reference: [15] <author> Hiroaki Hirata, Kozo Kimura, Satoshi Nagamine, Yoshiyuki Mochizuki, Akio Nishimura, Yoshimori Nakase, and Teiji Nishizawa. </author> <title> An elementary processor architecture with simultaneous instruction issuing from multiple threads. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 136-145, </pages> <month> May 19-21, </month> <year> 1992. </year>
Reference-contexts: This section describes the data communication mechanisms used in some concurrent multithreaded architectures. 2.2.1 Register Data Communication Some concurrent multithreaded architectures such as XIMD [52], Elementary Multithreading <ref> [15] </ref>, M-machine [10, 19], Multiscalar [11, 36], Trace Processor [32, 34] allow concurrent threads to communicate or share register data with each other. In the XIMD architecture, all threads (instruction streams) share a global register file. This approach allows data communication between threads to be very straightforward. <p> In this section, we review the thread-level speculative execution support for control and data dependences. 2.3.1 Control Speculation Some concurrent multiple-threaded models such as Elementary Multithreading <ref> [15] </ref>, Multiscalar [11, 36], SPSM [8], Traces processors [32, 34], and others [20, 28, 29] support thread-level control speculation. A thread which is control dependent on some conditions determined by earlier threads can be initiated and speculatively executed before the conditions are known. <p> Although its coarse-grained multithreading requires more overhead to switch from one thread to another, the Alewife seeks to avoid memory latency by using a data cache in addition to hiding it. To achieve high instruction issue rate, the XIMD [52], Elementary Multithreading <ref> [15] </ref>, M-machine [10], Simultaneous Multithreading [47], Multiscalar [11, 36], and SPSM [8] machines allow instructions from different threads to be issued and executed concurrently. The Simultaneous Mul-tithreading and SPSM machines support only independent parallel threads, such as parallel loops without cross-iteration dependences (i.e. do-all loops).
Reference: [16] <author> Mike Johnson. </author> <title> Superscalar Microprocessor Design. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1991. </year>
Reference-contexts: Instruction-level specula 1 tive execution with branch prediction is needed to move independent instructions across basic block boundaries. However, a large instruction window size requires the compiler or hardware to perform more levels of branch prediction and speculation. The accuracy of branch prediction at deeper levels will suffer quickly <ref> [16] </ref>. Also, in VLIW architectures, the code size will expand exponentially because a single-threaded code needs to include all possible combinations of branch conditions. This problem is especially serious when a compiler attempts to software pipeline a loop with many conditional branches [50]. <p> In superscalar architectures, the processor needs to perform run-time dependence checking for both register and memory accesses. The hardware overhead for such dependence checking is very high and can grow quadratically as the size of the instruction window increases <ref> [16] </ref>. In VLIW architectures, the sub-operations of each long-word are executed in a lock-step fashion to enforce the dependences between instructions.
Reference: [17] <author> Harry F. Jordan. </author> <title> Performance measurements on HEP | a pipelined MIMD computer. </title> <booktitle> In Proceedings of the 10th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 207-212, </pages> <month> June 13-17, </month> <year> 1983. </year>
Reference-contexts: Systems that adopt multithreading for hiding latency include HEP <ref> [17] </ref>, Horizon [22], Tera [2], and Alewife [1]. To tolerate memory latency, HEP, Horizon and Tera can accommodate more than one hundred threads per processor and allow fine-grained and fast context switching at every cycle.
Reference: [18] <author> George E. Daddis Jr. and H.C. Torng. </author> <title> The concurrent execution of multiple instruction streams on superscalar processors. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <pages> pages I:76-83, </pages> <month> August, </month> <year> 1991. </year> <month> 110 </month>
Reference-contexts: Many concurrent multiple-threaded processor architectures have been proposed and studied [2, 8, 9, 10, 11, 14, 15, 17, 18, 19, 31, 36, 47, 46, 52, 32, 38, 28, 29, 20, 21]. Some of them 4 <ref> [9, 14, 18, 31, 47, 46] </ref> are primarily for increasing system throughput by allowing multiple programs (e.g. one program on each thread) to be run concurrently. In this thesis, we focus on models that are primarily for speeding up the execution of one single program.
Reference: [19] <author> Stephen W. Keckler and William J. Dally. </author> <title> Processor coupling: Integrating compile time and runtime scheduling for parallelism. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 202-213, </pages> <month> May 19-21, </month> <year> 1992. </year>
Reference-contexts: The master thread and the slave threads execute in parallel on different processing elements or funational units. After a slave thread completes its execution, it is merged into the master thread and then retires. Concurrent multithreaded architectures that fall into this category include M-machine <ref> [10, 19] </ref> and SPSM (Single-Program Speculative Multithreading) architecture [8]. In the M-machine, a home H-thread can create concurrent H-threads by executing a special fork operation. Each H-thread executes on a cluster (processing element) and has its own register file. <p> This section describes the data communication mechanisms used in some concurrent multithreaded architectures. 2.2.1 Register Data Communication Some concurrent multithreaded architectures such as XIMD [52], Elementary Multithreading [15], M-machine <ref> [10, 19] </ref>, Multiscalar [11, 36], Trace Processor [32, 34] allow concurrent threads to communicate or share register data with each other. In the XIMD architecture, all threads (instruction streams) share a global register file. This approach allows data communication between threads to be very straightforward.
Reference: [20] <author> Venkata Krishnan and Josep Torrellas. </author> <title> Executing sequential binaries on a multithreaded architecture with speculation support. </title> <booktitle> In Proceedings of the Workshop on Multithreaded Execution Architecture and Compilation, </booktitle> <month> January 27-28, </month> <year> 1998. </year>
Reference-contexts: For programs with a lot of maybe data dependences between threads, the performance will suffer if those ambiguous data dependences do not actually occur at run-time. Some models <ref> [11, 36, 8, 38, 20] </ref> allow the compiler to speculate on those maybe data dependences with run-time data speculation support. We will discuss this approach in the next section in more detail. Another approach is to perform run-time data synchronization based on the history of previous execution [30, 48]. <p> In this section, we review the thread-level speculative execution support for control and data dependences. 2.3.1 Control Speculation Some concurrent multiple-threaded models such as Elementary Multithreading [15], Multiscalar [11, 36], SPSM [8], Traces processors [32, 34], and others <ref> [20, 28, 29] </ref> support thread-level control speculation. A thread which is control dependent on some conditions determined by earlier threads can be initiated and speculatively executed before the conditions are known. After the earlier threads have determined the conditions, the speculative thread is committed or squashed, accordingly. <p> For example, in the address resolution buffer (ARB) of a Multiscalar processor, each active task has its own value field in every entry to save its store data before it is committed. 2.3.2 Data Speculation Multiscalar [11, 36], SPSM [8], Traces processors [32, 34], and others <ref> [38, 20, 28, 29] </ref> also support speculation on the maybe data dependences with the help of run-time address disambiguation hardware. In these models, the compiler or the processor speculates on maybe data dependences between threads without generating explicit data synchronization instructions to enforce them.
Reference: [21] <author> Venkata Krishnan and Josep Torrellas. </author> <title> A clustered approach to multithreaded processors. </title> <booktitle> In Proceedings of the International Parallel Processing Symposium, </booktitle> <month> March </month> <year> 1998. </year>
Reference: [22] <author> J. T. Kuehn and B. J. Smith. </author> <title> The horizon supercomputing system: </title> <booktitle> Architecture and software. In Proceedings of Supercomputing 1988, </booktitle> <month> November </month> <year> 1988. </year>
Reference-contexts: Systems that adopt multithreading for hiding latency include HEP [17], Horizon <ref> [22] </ref>, Tera [2], and Alewife [1]. To tolerate memory latency, HEP, Horizon and Tera can accommodate more than one hundred threads per processor and allow fine-grained and fast context switching at every cycle.
Reference: [23] <author> Monica S. Lam and Robert P. Wilson. </author> <title> Limits of control flow on parallelism. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 46-57, </pages> <month> May 19-21, </month> <year> 1992. </year>
Reference-contexts: However, it is known to be very difficult to extract enough parallelism with a single thread of control even for a small number of functional units <ref> [5, 23, 49] </ref>. A single-threaded sequencing mechanism has several major limitations. For example, independent instructions from different basic blocks need to be grouped together in a single instruction stream.
Reference: [24] <author> J. K. F. Lee and A. J. Smith. </author> <title> Branch prediction strategies and branch target buffer design. </title> <journal> IEEE Computer, </journal> <volume> 17(1) </volume> <pages> 6-22, </pages> <month> January </month> <year> 1984. </year>
Reference-contexts: To provide sufficient instruction bandwidth, each thread processing element has its own first-level (L1) instruction cache. The instruction fetch logic is also equipped with a branch target buffer <ref> [24] </ref> to eliminate branch delay and to support branch prediction and instruction-level speculative 29 execution. Instructions fetched from the instruction cache are stored in the instruction queue before they are decoded and dispatched. <p> In case of a tie,the instruction fetch logic will choose the oldest y-thread. To allow instructions fetch beyond a conditional branch, the instruction fetch logic can use a conventional branch target buffer <ref> [24] </ref> to support instruction-level control speculation within each y-thread. In every cycle, the instruction dispatch unit examines the top instructions in the instruction queues of all thread slots. If the required resources are available, the dispatch unit will send the instructions to the reservation stations of the appropriate functional units.
Reference: [25] <author> Zhiyuan Li. </author> <title> Array privatization for parallel execution of loops. </title> <booktitle> In Proceedings of the 6th ACM International Conference on Supercomputing, </booktitle> <pages> pages 313-322, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: The compiler need not generate target store instructions for the update of the private variables because they will not cause read-after-write data dependence across threads. For this purpose, the compiler can perform analysis (on both scalars and arrays) to identify privatizable variables <ref> [53, 25] </ref>. * Loop unrolling/interchange: Loop unrolling and loop interchange are common techniques used in parallelizing compilers to increase the granularity of parallel loops.
Reference: [26] <author> Zhiyuan Li, Jenn-Yuan Tsai, Xin Wang, Pen-Chung Yew, and Bixia Zheng. </author> <title> Compiler techniques for concurrent multithreading with hardware speculation support. </title> <booktitle> In Proceedings of the 9th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <volume> LNCS #1239, </volume> <pages> pages 175-191, </pages> <month> August </month> <year> 1996. </year> <month> 111 </month>
Reference-contexts: Chapter 2 gives the background of concurrent multi-threaded models. In Chapter 3, we introduce the superthreaded execution model [43, 39]. Chapter 4 describes the microarchitecture of a superthreaded processor. The compilation techniques <ref> [41, 42, 26] </ref> or the superthreaded model is presented in Chapter 5. Chapter 6 evaluates the performance of the superthreaded architecture by using hand compiled codes [40].
Reference: [27] <author> Mikko H. Lipsati and John Paul Shen. </author> <title> Exceeding the dataflow limit via value prediction. </title> <booktitle> In Proceedings of the 29th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 226-237, </pages> <month> December 2-4, </month> <year> 1996. </year>
Reference-contexts: During execution, a trace accesses global register file for live-in register values. If a live-in register value is not available, the trace will wait for the value produced by predecessor traces by monitoring the global result bus, or use the predicted value <ref> [27] </ref> of the register based on the history of the previous traces.
Reference: [28] <author> Pedro Marcuello and Antonio Gonzales. </author> <title> Control and data dependence speculation in multi-threaded processor. </title> <booktitle> In Proceedings of the Workshop on Multithreaded Execution Architecture and Compilation, </booktitle> <month> January 27-28, </month> <year> 1998. </year>
Reference-contexts: In this section, we review the thread-level speculative execution support for control and data dependences. 2.3.1 Control Speculation Some concurrent multiple-threaded models such as Elementary Multithreading [15], Multiscalar [11, 36], SPSM [8], Traces processors [32, 34], and others <ref> [20, 28, 29] </ref> support thread-level control speculation. A thread which is control dependent on some conditions determined by earlier threads can be initiated and speculatively executed before the conditions are known. After the earlier threads have determined the conditions, the speculative thread is committed or squashed, accordingly. <p> For example, in the address resolution buffer (ARB) of a Multiscalar processor, each active task has its own value field in every entry to save its store data before it is committed. 2.3.2 Data Speculation Multiscalar [11, 36], SPSM [8], Traces processors [32, 34], and others <ref> [38, 20, 28, 29] </ref> also support speculation on the maybe data dependences with the help of run-time address disambiguation hardware. In these models, the compiler or the processor speculates on maybe data dependences between threads without generating explicit data synchronization instructions to enforce them.
Reference: [29] <author> Pedro Marcuello, Antonio Gonzales, and Jordi Tubella. </author> <title> Speculative multithreaded processors. </title> <booktitle> In Proceedings of the 1998 International Conference on Supercomputing, </booktitle> <month> July 13 - 17, </month> <year> 1998. </year>
Reference-contexts: When a task is assigned to a processing element, the live register values from its predecessor tasks will be forwarded to its register file through the unidirectional ring. Dynamic threading: Recently, a couple of concurrent multithreaded execution models <ref> [32, 34, 45, 29] </ref> have been proposed to identify concurrent threads or traces at run time. These models do not rely on the compiler to identify concurrent threads from a sequential program and to generate multithreaded code. <p> In this section, we review the thread-level speculative execution support for control and data dependences. 2.3.1 Control Speculation Some concurrent multiple-threaded models such as Elementary Multithreading [15], Multiscalar [11, 36], SPSM [8], Traces processors [32, 34], and others <ref> [20, 28, 29] </ref> support thread-level control speculation. A thread which is control dependent on some conditions determined by earlier threads can be initiated and speculatively executed before the conditions are known. After the earlier threads have determined the conditions, the speculative thread is committed or squashed, accordingly. <p> For example, in the address resolution buffer (ARB) of a Multiscalar processor, each active task has its own value field in every entry to save its store data before it is committed. 2.3.2 Data Speculation Multiscalar [11, 36], SPSM [8], Traces processors [32, 34], and others <ref> [38, 20, 28, 29] </ref> also support speculation on the maybe data dependences with the help of run-time address disambiguation hardware. In these models, the compiler or the processor speculates on maybe data dependences between threads without generating explicit data synchronization instructions to enforce them.
Reference: [30] <author> Andreas Moshovos, Scott E. Breach, T.N. Vijaykumar, and Gurindar S. Sohi. </author> <title> Dynamic speculation and synchronization of data dependences. </title> <booktitle> In Proceedings of the 24rd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 181-193, </pages> <month> June 2-4, </month> <year> 1997. </year>
Reference-contexts: Some models [11, 36, 8, 38, 20] allow the compiler to speculate on those maybe data dependences with run-time data speculation support. We will discuss this approach in the next section in more detail. Another approach is to perform run-time data synchronization based on the history of previous execution <ref> [30, 48] </ref>. However, this approach may require a lot of storage to keep all previous data dependence history. In this thesis, we propose a run-time data dependence checking mechanism to detect and synchronize inter-thread memory data dependences at run time with the help of the compiler.
Reference: [31] <author> R. Guru Prasadh and Chuang lin Wu. </author> <title> A benchmark evaluation of a multi-thread risc processor architecture. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <pages> pages I:84-91, </pages> <month> August, </month> <year> 1991. </year>
Reference-contexts: Many concurrent multiple-threaded processor architectures have been proposed and studied [2, 8, 9, 10, 11, 14, 15, 17, 18, 19, 31, 36, 47, 46, 52, 32, 38, 28, 29, 20, 21]. Some of them 4 <ref> [9, 14, 18, 31, 47, 46] </ref> are primarily for increasing system throughput by allowing multiple programs (e.g. one program on each thread) to be run concurrently. In this thesis, we focus on models that are primarily for speeding up the execution of one single program.
Reference: [32] <author> E. Rotenberg, Q. Jacobson, Y. Sazeides, and J. Smith. </author> <title> Trace processors. </title> <booktitle> In Proceedings of the 30th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 138-148, </pages> <month> December 1-3, </month> <year> 1997. </year>
Reference-contexts: When a task is assigned to a processing element, the live register values from its predecessor tasks will be forwarded to its register file through the unidirectional ring. Dynamic threading: Recently, a couple of concurrent multithreaded execution models <ref> [32, 34, 45, 29] </ref> have been proposed to identify concurrent threads or traces at run time. These models do not rely on the compiler to identify concurrent threads from a sequential program and to generate multithreaded code. <p> This section describes the data communication mechanisms used in some concurrent multithreaded architectures. 2.2.1 Register Data Communication Some concurrent multithreaded architectures such as XIMD [52], Elementary Multithreading [15], M-machine [10, 19], Multiscalar [11, 36], Trace Processor <ref> [32, 34] </ref> allow concurrent threads to communicate or share register data with each other. In the XIMD architecture, all threads (instruction streams) share a global register file. This approach allows data communication between threads to be very straightforward. <p> In this section, we review the thread-level speculative execution support for control and data dependences. 2.3.1 Control Speculation Some concurrent multiple-threaded models such as Elementary Multithreading [15], Multiscalar [11, 36], SPSM [8], Traces processors <ref> [32, 34] </ref>, and others [20, 28, 29] support thread-level control speculation. A thread which is control dependent on some conditions determined by earlier threads can be initiated and speculatively executed before the conditions are known. <p> For example, in the address resolution buffer (ARB) of a Multiscalar processor, each active task has its own value field in every entry to save its store data before it is committed. 2.3.2 Data Speculation Multiscalar [11, 36], SPSM [8], Traces processors <ref> [32, 34] </ref>, and others [38, 20, 28, 29] also support speculation on the maybe data dependences with the help of run-time address disambiguation hardware. In these models, the compiler or the processor speculates on maybe data dependences between threads without generating explicit data synchronization instructions to enforce them.
Reference: [33] <author> J. E. Smith and A. R. Pleszkun. </author> <title> Implementation of precise interrupts in pipelined processors. </title> <booktitle> In Proceedings of the 12th International Symposium on Computer Architecture, </booktitle> <pages> pages 36-44, </pages> <month> June </month> <year> 1985. </year> <month> 112 </month>
Reference-contexts: Instructions can then be executed out of order when their operands are available. To support speculative execution and in-order instruction completion, the instruction dispatch and completion unit uses an reorder buffer <ref> [33] </ref> to buffer instruction results before they are committed. The reorder buffer also serves as an rename buffer to provide later instructions with uncommitted results which they are flow (read-after-write) dependent on. In the superscalar execution model, instructions can be executed out of order provided their operands are available. <p> Our scheduling policy is to execute instructions from earlier threads as soon as possible, since they may produce results needed by later threads. Although instructions of a thread can be executed out of order, their execution results are completed in-order by using a reorder buffer <ref> [33] </ref> as in a conventional superscalar processor. 8.4.2 Memory Buffers The primary and the secondary memory buffers of a thread processing element have four major functions: * Performing run-time data dependence checking for data synchronization and data speculation, * Providing a thread with dependent store data from predecessor y-threads or x-threads,
Reference: [34] <author> James E. Smith and Sriram Vajapeyam. </author> <title> Trace processors: Moving to fouth-generation mi--croarchitectures. </title> <journal> IEEE Computer, </journal> <volume> 30(9) </volume> <pages> 68-74, </pages> <month> September </month> <year> 1997. </year>
Reference-contexts: When a task is assigned to a processing element, the live register values from its predecessor tasks will be forwarded to its register file through the unidirectional ring. Dynamic threading: Recently, a couple of concurrent multithreaded execution models <ref> [32, 34, 45, 29] </ref> have been proposed to identify concurrent threads or traces at run time. These models do not rely on the compiler to identify concurrent threads from a sequential program and to generate multithreaded code. <p> This section describes the data communication mechanisms used in some concurrent multithreaded architectures. 2.2.1 Register Data Communication Some concurrent multithreaded architectures such as XIMD [52], Elementary Multithreading [15], M-machine [10, 19], Multiscalar [11, 36], Trace Processor <ref> [32, 34] </ref> allow concurrent threads to communicate or share register data with each other. In the XIMD architecture, all threads (instruction streams) share a global register file. This approach allows data communication between threads to be very straightforward. <p> In this section, we review the thread-level speculative execution support for control and data dependences. 2.3.1 Control Speculation Some concurrent multiple-threaded models such as Elementary Multithreading [15], Multiscalar [11, 36], SPSM [8], Traces processors <ref> [32, 34] </ref>, and others [20, 28, 29] support thread-level control speculation. A thread which is control dependent on some conditions determined by earlier threads can be initiated and speculatively executed before the conditions are known. <p> For example, in the address resolution buffer (ARB) of a Multiscalar processor, each active task has its own value field in every entry to save its store data before it is committed. 2.3.2 Data Speculation Multiscalar [11, 36], SPSM [8], Traces processors <ref> [32, 34] </ref>, and others [38, 20, 28, 29] also support speculation on the maybe data dependences with the help of run-time address disambiguation hardware. In these models, the compiler or the processor speculates on maybe data dependences between threads without generating explicit data synchronization instructions to enforce them.
Reference: [35] <author> M. D. Smith. </author> <title> Tracing with pixie. </title> <type> Technical report, </type> <institution> Stanford University, Stanford, </institution> <address> California 94305, </address> <month> November </month> <year> 1991. </year> <note> Technical Report CSL-TR-91-497. </note>
Reference-contexts: In the transformed superthreaded programs, special superthreading instructions, such as fork and store ts, are represented as function calls to specific subroutines. The transformed superthreaded program is compiled by the SGI C compiler. The program is then instrumented by Pixie <ref> [35] </ref> to generate instruction and memory reference traces. The simulator executes the instrumented program on the host SGI machine and collects the traces generated by the program. During the trace collection phase, the function calls which represent the superthreading instructions will be converted to the actual superthreading instructions for simulation.
Reference: [36] <author> Gurindar S. Sohi, Scott E. Breach, and T. N. Vijaykumar. </author> <title> Multiscalar processors. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 414-425, </pages> <month> June 22-24, </month> <year> 1995. </year>
Reference-contexts: Second, the sequential 7 thread execution order allows it to use a very simple and fast unidirectional ring to connect all processing elements, since data always flow from predecessor threads to successor threads. Concurrent multithreaded architectures falling into this category include Multiscalar <ref> [12, 36] </ref> and the superthreaded architecture presented in this thesis. In the multiscalar paradigm, the control flow graph of a program is transformed into a task graph by the compiler. <p> This section describes the data communication mechanisms used in some concurrent multithreaded architectures. 2.2.1 Register Data Communication Some concurrent multithreaded architectures such as XIMD [52], Elementary Multithreading [15], M-machine [10, 19], Multiscalar <ref> [11, 36] </ref>, Trace Processor [32, 34] allow concurrent threads to communicate or share register data with each other. In the XIMD architecture, all threads (instruction streams) share a global register file. This approach allows data communication between threads to be very straightforward. <p> For programs with a lot of maybe data dependences between threads, the performance will suffer if those ambiguous data dependences do not actually occur at run-time. Some models <ref> [11, 36, 8, 38, 20] </ref> allow the compiler to speculate on those maybe data dependences with run-time data speculation support. We will discuss this approach in the next section in more detail. Another approach is to perform run-time data synchronization based on the history of previous execution [30, 48]. <p> In this section, we review the thread-level speculative execution support for control and data dependences. 2.3.1 Control Speculation Some concurrent multiple-threaded models such as Elementary Multithreading [15], Multiscalar <ref> [11, 36] </ref>, SPSM [8], Traces processors [32, 34], and others [20, 28, 29] support thread-level control speculation. A thread which is control dependent on some conditions determined by earlier threads can be initiated and speculatively executed before the conditions are known. <p> For example, in the address resolution buffer (ARB) of a Multiscalar processor, each active task has its own value field in every entry to save its store data before it is committed. 2.3.2 Data Speculation Multiscalar <ref> [11, 36] </ref>, SPSM [8], Traces processors [32, 34], and others [38, 20, 28, 29] also support speculation on the maybe data dependences with the help of run-time address disambiguation hardware. <p> schedule target stores as early as possible, or schedule load instructions that may be data dependent on the target stores of some predecessor threads as late as possible. 41 5.4 Advanced Compiler Techniques for Superthreading 5.4.1 Conversion of Data Speculation to Control Speculation Some concurrent multiple-threaded architectures, such as Multiscalar <ref> [11, 36] </ref> and SPSM [8], provide hardware support for data speculation. With such hardware support, the compiler can speculate on potential data dependences between threads by assuming that they would not be violated, and rely on the hardware to detect violations at run-time. <p> Although its coarse-grained multithreading requires more overhead to switch from one thread to another, the Alewife seeks to avoid memory latency by using a data cache in addition to hiding it. To achieve high instruction issue rate, the XIMD [52], Elementary Multithreading [15], M-machine [10], Simultaneous Multithreading [47], Multiscalar <ref> [11, 36] </ref>, and SPSM [8] machines allow instructions from different threads to be issued and executed concurrently. The Simultaneous Mul-tithreading and SPSM machines support only independent parallel threads, such as parallel loops without cross-iteration dependences (i.e. do-all loops).
Reference: [37] <author> Gurindar S. Sohi and Manoj Franklin. </author> <title> High-bandwidth data memory systems for superscalar processors. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 53-62, </pages> <month> April 8-11, </month> <year> 1991. </year>
Reference-contexts: Techniques to improve cache bandwidth include non-blocking, multi-port, and interleaved multi-bank <ref> [37] </ref>. Non-blocking improves the cache performance by reducing the bandwidth degradation due to misses. Multi-port (duplicated ports) and multi-bank caches can serve multiple requests per cycle. In general, a multi-port cache is more effective than a multi-bank cache because it has less port contention.
Reference: [38] <author> J. Gregory Steffan and Todd Mowry. </author> <title> The potential for using thread-level data speculation to facilitate automatic parallelization. </title> <booktitle> In Proceedings of the Fourth International Symposium On High-Performance Computer Architecture, </booktitle> <pages> pages 2-13, </pages> <month> January 31-February 4, </month> <year> 1998. </year>
Reference-contexts: For programs with a lot of maybe data dependences between threads, the performance will suffer if those ambiguous data dependences do not actually occur at run-time. Some models <ref> [11, 36, 8, 38, 20] </ref> allow the compiler to speculate on those maybe data dependences with run-time data speculation support. We will discuss this approach in the next section in more detail. Another approach is to perform run-time data synchronization based on the history of previous execution [30, 48]. <p> For example, in the address resolution buffer (ARB) of a Multiscalar processor, each active task has its own value field in every entry to save its store data before it is committed. 2.3.2 Data Speculation Multiscalar [11, 36], SPSM [8], Traces processors [32, 34], and others <ref> [38, 20, 28, 29] </ref> also support speculation on the maybe data dependences with the help of run-time address disambiguation hardware. In these models, the compiler or the processor speculates on maybe data dependences between threads without generating explicit data synchronization instructions to enforce them.
Reference: [39] <author> J.-Y. Tsai, Z. Jiang, Z. Li, D.J. Lilja, X. Wang, P.-C. Yew, B. Zheng, and S. Schwinn. Su-perthreading: </author> <title> Integrating compilation technology and processor architecture for cost-effective concurrent multithreading. </title> <journal> In Journal of Information Science and Engineering, </journal> <volume> volume Vol 12, No.1, </volume> <month> March </month> <year> 1998. </year>
Reference-contexts: These features allow the compiler to exploit more potential thread-level parallelism in general-purpose applications. 1.1 Thesis Organization This thesis is organized as follows. Chapter 2 gives the background of concurrent multi-threaded models. In Chapter 3, we introduce the superthreaded execution model <ref> [43, 39] </ref>. Chapter 4 describes the microarchitecture of a superthreaded processor. The compilation techniques [41, 42, 26] or the superthreaded model is presented in Chapter 5. Chapter 6 evaluates the performance of the superthreaded architecture by using hand compiled codes [40].
Reference: [40] <author> Jenn-Yuan Tsai, Zhenzhen Jiang, Eric Ness, and Pen-Chung Yew. </author> <title> Performance study of a concurrent multithreaded processor. </title> <booktitle> In Proceedings of the Fourth International Symposium On High-Performance Computer Architecture, </booktitle> <pages> pages 24-35, </pages> <month> January 31-February 4, </month> <year> 1998. </year> <month> 113 </month>
Reference-contexts: In Chapter 3, we introduce the superthreaded execution model [43, 39]. Chapter 4 describes the microarchitecture of a superthreaded processor. The compilation techniques [41, 42, 26] or the superthreaded model is presented in Chapter 5. Chapter 6 evaluates the performance of the superthreaded architecture by using hand compiled codes <ref> [40] </ref>. Chapter 7 presents the framework of a superthreading compiler prototype based on the SUIF compiler, and shows the speedups of superthreaded codes generated by the compiler.
Reference: [41] <author> Jenn-Yuan Tsai, Zhenzhen Jiang, and Pen-Chung Yew. </author> <title> Program optimization for concurrent multithreaded architecture. </title> <booktitle> In Proceedings of the 10th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1997. </year>
Reference-contexts: Chapter 2 gives the background of concurrent multi-threaded models. In Chapter 3, we introduce the superthreaded execution model [43, 39]. Chapter 4 describes the microarchitecture of a superthreaded processor. The compilation techniques <ref> [41, 42, 26] </ref> or the superthreaded model is presented in Chapter 5. Chapter 6 evaluates the performance of the superthreaded architecture by using hand compiled codes [40].
Reference: [42] <author> Jenn-Yuan Tsai, Zhenzhen Jiang, and Pen-Chung Yew. </author> <title> Compiler techniques for the su-perthreaded architectures. </title> <note> To appear in International Journal of Parallel Programming - Special Issue on Languages and Compilers for Parallel Computing, </note> <month> June </month> <year> 1998. </year>
Reference-contexts: Chapter 2 gives the background of concurrent multi-threaded models. In Chapter 3, we introduce the superthreaded execution model [43, 39]. Chapter 4 describes the microarchitecture of a superthreaded processor. The compilation techniques <ref> [41, 42, 26] </ref> or the superthreaded model is presented in Chapter 5. Chapter 6 evaluates the performance of the superthreaded architecture by using hand compiled codes [40].
Reference: [43] <author> Jenn-Yuan Tsai and Pen-Chung Yew. </author> <title> The superthreaded architecture: Thread pipelining with run-time data dependence checking and control speculation. </title> <booktitle> In Proceedings of the 1996 Conference on Parallel Architectures and Compilation Techniques, PACT '96, </booktitle> <pages> pages 35-46, </pages> <month> October 20-23, </month> <year> 1996. </year>
Reference-contexts: With the above mentioned limitations in the single-threaded execution model, it is important to consider other possible models for future microprocessors which will have more functional units and higher issue rates. In this thesis, we present a concurrent multiple-threaded model, called superthreading <ref> [43] </ref>, as an alternative. The superthreaded architecture integrates compilation techniques and run-time hardware support to exploit both thread-level and instruction-level parallelism in programs. It uses a thread pipelining execution model to enhance the overlapping between threads. <p> These features allow the compiler to exploit more potential thread-level parallelism in general-purpose applications. 1.1 Thesis Organization This thesis is organized as follows. Chapter 2 gives the background of concurrent multi-threaded models. In Chapter 3, we introduce the superthreaded execution model <ref> [43, 39] </ref>. Chapter 4 describes the microarchitecture of a superthreaded processor. The compilation techniques [41, 42, 26] or the superthreaded model is presented in Chapter 5. Chapter 6 evaluates the performance of the superthreaded architecture by using hand compiled codes [40]. <p> Therefore, the compiler must generate threads at the appropriate level where the maximum combined performance gains can be achieved from both superthreading and superscalar. In addition, the size of a thread cannot be too large in order to avoid overflowing the memory buffer <ref> [43] </ref>. To generate threads at the right level, the compiler examines data dependences between contiguous execution portions of a program and estimates the amount of thread-level parallelism at each level with respect to the data dependences.
Reference: [44] <author> Jenn-Yuan Tsai, Bixia Zheng, and Pen-Chung Yew. </author> <title> Improving instruction throughput and memory latency using two-dimensional superthreading. </title> <booktitle> To appear in Proceedings of The Second European Parallel and Distributed Systems Conference, </booktitle> <month> July </month> <year> 1998. </year>
Reference-contexts: Chapter 7 presents the framework of a superthreading compiler prototype based on the SUIF compiler, and shows the speedups of superthreaded codes generated by the compiler. In chapter 8, we present an extension of the superthreaded architecture, called two-dimensional superthreading <ref> [44] </ref>, which can support memory latency hiding and improve instruction throughput.
Reference: [45] <author> Jordi Tubella and Antonio Gonzalez. </author> <title> Control speculation in multithreaded processors through dynamic loop detection. </title> <booktitle> In Proceedings of the Fourth International Symposium On High-Performance Computer Architecture, </booktitle> <pages> pages 14-23, </pages> <month> January 31-February 4, </month> <year> 1998. </year>
Reference-contexts: When a task is assigned to a processing element, the live register values from its predecessor tasks will be forwarded to its register file through the unidirectional ring. Dynamic threading: Recently, a couple of concurrent multithreaded execution models <ref> [32, 34, 45, 29] </ref> have been proposed to identify concurrent threads or traces at run time. These models do not rely on the compiler to identify concurrent threads from a sequential program and to generate multithreaded code.
Reference: [46] <author> Dean M. Tullsen, Susan J. Eggers, Joel S. Emer, Henry M. Levy, Jack L. Lo, and Rebecca L. Stamm. </author> <title> Exploiting choice: Instruction fetch and issue on an implementable simultaneous multithreading processor. </title> <booktitle> In Proceedings of the 23rd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 191-202, </pages> <month> May 22-24, </month> <year> 1996. </year> <month> 114 </month>
Reference-contexts: Many concurrent multiple-threaded processor architectures have been proposed and studied [2, 8, 9, 10, 11, 14, 15, 17, 18, 19, 31, 36, 47, 46, 52, 32, 38, 28, 29, 20, 21]. Some of them 4 <ref> [9, 14, 18, 31, 47, 46] </ref> are primarily for increasing system throughput by allowing multiple programs (e.g. one program on each thread) to be run concurrently. In this thesis, we focus on models that are primarily for speeding up the execution of one single program.
Reference: [47] <author> Dean M. Tullsen, Susan J. Eggers, and Henry M. Levy. </author> <title> Simultaneous multithreading: Max--imizing on-chip parallelism. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 392-403, </pages> <month> June 22-24, </month> <year> 1995. </year>
Reference-contexts: Many concurrent multiple-threaded processor architectures have been proposed and studied [2, 8, 9, 10, 11, 14, 15, 17, 18, 19, 31, 36, 47, 46, 52, 32, 38, 28, 29, 20, 21]. Some of them 4 <ref> [9, 14, 18, 31, 47, 46] </ref> are primarily for increasing system throughput by allowing multiple programs (e.g. one program on each thread) to be run concurrently. In this thesis, we focus on models that are primarily for speeding up the execution of one single program. <p> Although its coarse-grained multithreading requires more overhead to switch from one thread to another, the Alewife seeks to avoid memory latency by using a data cache in addition to hiding it. To achieve high instruction issue rate, the XIMD [52], Elementary Multithreading [15], M-machine [10], Simultaneous Multithreading <ref> [47] </ref>, Multiscalar [11, 36], and SPSM [8] machines allow instructions from different threads to be issued and executed concurrently. The Simultaneous Mul-tithreading and SPSM machines support only independent parallel threads, such as parallel loops without cross-iteration dependences (i.e. do-all loops).
Reference: [48] <author> Gary S. Tyson and Todd M. Austin. </author> <title> Improving the accuracy and performance of memory communication through renaming. </title> <booktitle> In Proceedings of the 30th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 218-227, </pages> <month> December 1-3, </month> <year> 1997. </year>
Reference-contexts: Some models [11, 36, 8, 38, 20] allow the compiler to speculate on those maybe data dependences with run-time data speculation support. We will discuss this approach in the next section in more detail. Another approach is to perform run-time data synchronization based on the history of previous execution <ref> [30, 48] </ref>. However, this approach may require a lot of storage to keep all previous data dependence history. In this thesis, we propose a run-time data dependence checking mechanism to detect and synchronize inter-thread memory data dependences at run time with the help of the compiler.
Reference: [49] <author> David W. Wall. </author> <title> Limits of instruction-level parallelism. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 176-188, </pages> <month> April 8-11, </month> <year> 1991. </year>
Reference-contexts: However, it is known to be very difficult to extract enough parallelism with a single thread of control even for a small number of functional units <ref> [5, 23, 49] </ref>. A single-threaded sequencing mechanism has several major limitations. For example, independent instructions from different basic blocks need to be grouped together in a single instruction stream.
Reference: [50] <author> Nancy J. Warter, Grant E. Haab, John W. Bockhaus, and Krishna Subramanian. </author> <title> Enhanced modulo scheduling for loops with conditional branches. </title> <booktitle> In Proceedings of the 25th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 170-179, </pages> <month> December 1-4, </month> <year> 1992. </year>
Reference-contexts: Also, in VLIW architectures, the code size will expand exponentially because a single-threaded code needs to include all possible combinations of branch conditions. This problem is especially serious when a compiler attempts to software pipeline a loop with many conditional branches <ref> [50] </ref>. In superscalar architectures, the processor needs to perform run-time dependence checking for both register and memory accesses. The hardware overhead for such dependence checking is very high and can grow quadratically as the size of the instruction window increases [16].
Reference: [51] <author> Robert P. Wilson, Robert S. French, Christopher S. Wilson, Samam P. Amarasinghe, Jen-nifer M. Anderson, Steven W.K. Tjiang, Shi-Wei Liao, Chau-Wen Tseng, Mary W. Hall, Monica S. Lam, and John L. Hennessy. </author> <title> Suif: An infrastructure for research on parallelizing and optimizing compilers. </title> <journal> In ACM SIGPLAN Notices, </journal> <volume> volume 29(12), </volume> <month> December </month> <year> 1994. </year>
Reference-contexts: To test the ability of an existing parallelizing compiler for generating superthreaded code and to study new compiler techniques for the complete implementation of the superthreading compiler, we have built a superthreading compiler prototype based on the SUIF parallelizing compiler (baseparsuif-1.0.0.beta.1) <ref> [51] </ref>.
Reference: [52] <author> Andrew Wolfe and John P. Shen. </author> <title> A variable instruction stream extension to the VLIW architecture. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 2-14, </pages> <month> April 8-11, </month> <year> 1991. </year>
Reference-contexts: Those parallel execution models can be classified into four categories as described below: Variable instruction streams: This model is proposed by Wolfe and Shenused in their XIMD (Variable Instruction Stream, Multiple Data Stream Processor) architecture <ref> [52] </ref>. The XIMD architecture, which is proposed as an extension to the VLIW model, can split its functional units among one or more instruction streams. Furthermore, the number of instruction streams can vary dynamically at run time based on a scenario generated by the compiler. <p> This section describes the data communication mechanisms used in some concurrent multithreaded architectures. 2.2.1 Register Data Communication Some concurrent multithreaded architectures such as XIMD <ref> [52] </ref>, Elementary Multithreading [15], M-machine [10, 19], Multiscalar [11, 36], Trace Processor [32, 34] allow concurrent threads to communicate or share register data with each other. In the XIMD architecture, all threads (instruction streams) share a global register file. This approach allows data communication between threads to be very straightforward. <p> Although its coarse-grained multithreading requires more overhead to switch from one thread to another, the Alewife seeks to avoid memory latency by using a data cache in addition to hiding it. To achieve high instruction issue rate, the XIMD <ref> [52] </ref>, Elementary Multithreading [15], M-machine [10], Simultaneous Multithreading [47], Multiscalar [11, 36], and SPSM [8] machines allow instructions from different threads to be issued and executed concurrently. The Simultaneous Mul-tithreading and SPSM machines support only independent parallel threads, such as parallel loops without cross-iteration dependences (i.e. do-all loops).
Reference: [53] <author> Michael J. Wolfe. </author> <title> Optimizing supercompilers for supercomputers. </title> <type> Technical report, </type> <institution> University of Illinois, </institution> <month> October </month> <year> 1982. </year> <note> Technical Report UIUCDCS-R-82-1105. 115 </note>
Reference-contexts: The compiler need not generate target store instructions for the update of the private variables because they will not cause read-after-write data dependence across threads. For this purpose, the compiler can perform analysis (on both scalars and arrays) to identify privatizable variables <ref> [53, 25] </ref>. * Loop unrolling/interchange: Loop unrolling and loop interchange are common techniques used in parallelizing compilers to increase the granularity of parallel loops.
References-found: 52

