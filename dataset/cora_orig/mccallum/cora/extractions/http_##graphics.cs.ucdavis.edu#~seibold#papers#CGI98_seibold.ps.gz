URL: http://graphics.cs.ucdavis.edu/~seibold/papers/CGI98_seibold.ps.gz
Refering-URL: http://graphics.cs.ucdavis.edu/~seibold/
Root-URL: http://www.cs.ucdavis.edu
Email: wseibold@atlas.otago.ac.nz,  geoff@otago.ac.nz,  
Title: Towards an Understanding of Surfaces through Polygonization  
Author: Wolfgang Seibold Geoff Wyvill 
Web: http://heffalump.otago.ac.nz/wseibold  http://atlas.otago.ac.nz:800/graphics/Geoff.html  
Address: Otago, Dunedin, New Zealand  
Affiliation: Computer Science Department, Graphics Research Laboratory University of  
Abstract: This paper identifies some of the major problems in poly-gonizing parametric surfaces. We are interested in associating an error with a polygonization which tells us, how well we have done, rather than judging a surface by its looks alone. We introduce an error measurement which is more intuitive and helps to understand the surface. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Garland and P. Heckbert. </author> <title> Fast polygonal approximation of terrains and height fields. </title> <institution> CMU-CS-95-181, School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA 16213, </address> <year> 1995. </year>
Reference-contexts: The next thing we have to be aware of is that the result will strongly depend on how we connect the vertices to form triangles. Using Delaunay triangulation or data dependent triangulation <ref> [1] </ref>, will give a different triangulation for example. If the algorithm only inserts points or only deletes them without moving, (refinement and decimation [2]), then our result will depend on the starting set of triangles. <p> The resulting algorithm follows: while error &gt; " or triangles &lt; max find triangle with worst error divide triangle at point of worst error make triangulation Delaunay We implemented two versions of this algorithm. One using the analytic error <ref> [1] </ref> and the other one using the geometric one. To get a better feeling for polygonization we performed three experiments. Experiment 1: Choose fifty random points in parametric space and triangulate till the number of triangles equals one thousand.
Reference: [2] <author> P. Heckbert and M. </author> <title> Garland. Survey of polygonal surface simplification algorithms. Multiresolution Surface Modeling Course, </title> <booktitle> SIGGRAPH, </booktitle> <year> 1997. </year>
Reference-contexts: The approximation is the only data we have. There are many publications on triangulation from captured data, CT scans, implicit and parametrically defined surfaces. But the main emphasis has been speed of triangulation and ability to represent the surface with reasonable economy. Techniques like decimation <ref> [2] </ref> start with a very fine mesh and then develop an approximation with fewer triangles. The value of the result is mostly demonstrated by the extent to which the approximation `looks' like the original. <p> Using Delaunay triangulation or data dependent triangulation [1], will give a different triangulation for example. If the algorithm only inserts points or only deletes them without moving, (refinement and decimation <ref> [2] </ref>), then our result will depend on the starting set of triangles. In case of the refinement method the original vertices have to be a subset of the optimal vertex set and in the case of the decimation approach they have to be a superset. <p> The method of polygonizing a model by hand is normally used in combination with a real model, where we can physically stick vertices on, but has also been used for polygon models for flight simulators <ref> [2] </ref>. The idea is to use our visual system to detect good places to put vertices. This works well because we can see the model. To hand polygonize a virtual model like an abstract mathematical function, we need some information about the actual shape. <p> Experiments The last section dealt with some measures of error. Each of them returns an error for a given triangle. A common method in polygonization is greedy insertion, <ref> [2] </ref>. The algorithm starts out with a coarse mesh and refines it by adding vertices at the points of highest error. In our approach we use incremental Delaunay triangulation in parametric space to make sure that the mesh does not have cracks [5]. <p> It has some advantages over the geometric algorithm at the beginning, but does not make a difference in the analytic case. Generally for triangulations with very few triangles the hand polygonization does better <ref> [2] </ref>. In Section 5 we pointed out that the analytic error overestimates the error at places of a high gradient. The volcano has high gradient inside the crater.
Reference: [3] <author> E. </author> <title> Kreyszig. Introduction to differential geometry and rie-mannian geometry. </title> <booktitle> Mathematical Exposition 16, </booktitle> <publisher> University of Toronto Press, </publisher> <pages> pages 8488, </pages> <year> 1968. </year>
Reference-contexts: It was chosen because it turned out to be a problem case in the history modeler [8] and it contains elliptic, hyperbolic and parabolic points, for definitions see <ref> [3] </ref>. To polygonize this surface we used Delaunay triangulation in parametric space. As stated earlier this has the advantage of being adaptive without the problem of T-junctions. Furthermore the Delaunay triangulation can be constructed incrementally [8]. To experiment with the surface we need certain tools to work with. <p> Should we be measuring n or m? In 3D yet another problem occurs. There will be cases where the normal distance is not defined for every point on the mesh. Imagine a surface with a saddle point. At the 5 D. saddle point there are two principle directions <ref> [3] </ref>. These indicate the directions of maximum and minimum curvature. Figure 13 shows a saddle with two triangles. Looking from the right side along the shared edge and taking a cross section at the center line through the triangles results in Figure 14a.
Reference: [4] <author> C. Lawson. </author> <title> Software for c1 surface interpolation. </title> <booktitle> Mathematical Software III, </booktitle> <pages> pages 161194, </pages> <year> 1997. </year>
Reference-contexts: This results in a gap shown in Figure 4b. But how do we fit them together without creating gaps between them? One way to avoid cracks in the mesh of the polygonized they produce gaps in the surface. model is to use a Delaunay triangulation of the parametric space <ref> [4] </ref>. A Delaunay triangulation will connect triangles of different sizes yet cannot contain T-junctions. 2.4. Choice of Representation A parametric surface can be described as a mapping f : P ! M where P is a parametric space and M is a 2 model space.
Reference: [5] <author> V. Vlassopoulos. </author> <title> Adaptive polygonization of parametric surfaces. </title> <journal> The Visual Computer, </journal> <volume> 6(5), </volume> <year> 1990. </year>
Reference-contexts: But what happens when we use a different measure? Obviously we will only get the optimal triangulation if we choose the best measure of error. 3. Related Work Most published work on polygonization of parametric surfaces simply presents algorithms. A good example is Vlassopoulos <ref> [5] </ref>. He divides the parametric space until some geometric criteria are satisfied. These criteria are calculated as in Section 5.1, entirely based on the mesh points. The error estimate is based on how close to convergence we are rather than on a calculated deviation from the surface. <p> The algorithm starts out with a coarse mesh and refines it by adding vertices at the points of highest error. In our approach we use incremental Delaunay triangulation in parametric space to make sure that the mesh does not have cracks <ref> [5] </ref>. The resulting algorithm follows: while error &gt; " or triangles &lt; max find triangle with worst error divide triangle at point of worst error make triangulation Delaunay We implemented two versions of this algorithm. One using the analytic error [1] and the other one using the geometric one.
Reference: [6] <author> G. Wyill, D. McRobie, and M. Gigante. </author> <title> Modelling with features. </title> <journal> IEEE Computer Graphics and Application, </journal> <volume> 17(5):40 46, </volume> <month> September/October </month> <year> 1997. </year>
Reference-contexts: give as good a visual result but with only a fraction of the number of polygons? Over the last five years, members of our group have produced a series of surface modelers based on the idea of mapping a complicated shape from a single sphere, treated as a parametric surface <ref> [7, 8, 9, 6] </ref>. We elected to use a refinement approach because it seemed wasteful to throw away a vertex once you had calculated it. But if the presence of that vertex actually increases the error of polygonization, it is better to remove it anyway.
Reference: [7] <author> G. Wyvill and D. McRobie. </author> <title> Local and global control of Cao En surfaces. </title> <booktitle> Communicating with Virtual Worlds (Proceedings of CGI'93, </booktitle> <pages> pages 216227, </pages> <year> 1993. </year>
Reference-contexts: give as good a visual result but with only a fraction of the number of polygons? Over the last five years, members of our group have produced a series of surface modelers based on the idea of mapping a complicated shape from a single sphere, treated as a parametric surface <ref> [7, 8, 9, 6] </ref>. We elected to use a refinement approach because it seemed wasteful to throw away a vertex once you had calculated it. But if the presence of that vertex actually increases the error of polygonization, it is better to remove it anyway.
Reference: [8] <author> G. Wyvill and D. McRobie. </author> <title> Adaptive polygonization of Cao En surfaces. </title> <booktitle> Proceedings of CGI'94, </booktitle> <pages> pages 250256, </pages> <year> 1994. </year>
Reference-contexts: give as good a visual result but with only a fraction of the number of polygons? Over the last five years, members of our group have produced a series of surface modelers based on the idea of mapping a complicated shape from a single sphere, treated as a parametric surface <ref> [7, 8, 9, 6] </ref>. We elected to use a refinement approach because it seemed wasteful to throw away a vertex once you had calculated it. But if the presence of that vertex actually increases the error of polygonization, it is better to remove it anyway. <p> We elected to use a refinement approach because it seemed wasteful to throw away a vertex once you had calculated it. But if the presence of that vertex actually increases the error of polygonization, it is better to remove it anyway. In <ref> [8] </ref>, we established the effectiveness of using Delau-nay triangles on the spherical parametric space and made some very good models, particularly of human faces. Further experience with these modelers, however, has shown that some shapes are handled very badly. <p> It was chosen because it turned out to be a problem case in the history modeler <ref> [8] </ref> and it contains elliptic, hyperbolic and parabolic points, for definitions see [3]. To polygonize this surface we used Delaunay triangulation in parametric space. As stated earlier this has the advantage of being adaptive without the problem of T-junctions. Furthermore the Delaunay triangulation can be constructed incrementally [8]. <p> the history modeler <ref> [8] </ref> and it contains elliptic, hyperbolic and parabolic points, for definitions see [3]. To polygonize this surface we used Delaunay triangulation in parametric space. As stated earlier this has the advantage of being adaptive without the problem of T-junctions. Furthermore the Delaunay triangulation can be constructed incrementally [8]. To experiment with the surface we need certain tools to work with. First of all a suitable display tool has to be found. For the implementation we are using a combination of OpenGL and Tcl/Tk.
Reference: [9] <author> G. Wyvill, D. McRobie, C. Haig, and C. McNaughton. </author> <title> Free form modeling with history. </title> <booktitle> International Journal of Shape Modeling, </booktitle> <year> 1996. </year>
Reference-contexts: give as good a visual result but with only a fraction of the number of polygons? Over the last five years, members of our group have produced a series of surface modelers based on the idea of mapping a complicated shape from a single sphere, treated as a parametric surface <ref> [7, 8, 9, 6] </ref>. We elected to use a refinement approach because it seemed wasteful to throw away a vertex once you had calculated it. But if the presence of that vertex actually increases the error of polygonization, it is better to remove it anyway. <p> It works well for smooth surfaces without high curvature but depends on a fairly tight starting triangulation <ref> [9] </ref>. A refinement method based on this criterion will in general converge to the surface because as the triangles get smaller in relation to the local curvature, the divergence angle tends to zero. But before convergence is reached, its is possible that we get the case of Figure 8b.
Reference: [10] <author> R. Zabih. </author> <title> The hausdorff distance. </title> <address> http:// si-mon.cs.cornell.edu/ Info/ People/ rdz/ MM95/ node11.html, </address> <year> 1995. </year> <title> using the analytic error with 1000 triangles. using the geometric error with 1000 triangles. 9 sets of the analytic error.(Logarithmic scale) scale) 10 </title>
Reference-contexts: This method gives us two, probably different, measures. To get the right one we take the bigger one. The method described above is related to the Hausdorff distance which is defined as follows <ref> [10] </ref>. Definition 2: Let A and B be two point sets the Haus-dorff distance h is defined by h (A; B) = max min ka bk The problem with the Hausdorff distance is that it will search for the globally closest point.
References-found: 10

