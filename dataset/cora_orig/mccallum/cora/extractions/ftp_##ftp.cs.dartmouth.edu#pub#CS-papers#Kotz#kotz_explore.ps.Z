URL: ftp://ftp.cs.dartmouth.edu/pub/CS-papers/Kotz/kotz:explore.ps.Z
Refering-URL: http://www.cs.dartmouth.edu/iopads95/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: fdfk,tcaig@cs.dartmouth.edu  
Title: Exploring the use of I/O Nodes for Computation in a MIMD Multiprocessor  
Author: David Kotz and Ting Cai 
Address: Hanover, NH 03755  
Affiliation: Department of Computer Science Dartmouth College  
Web: URL ftp://ftp.cs.dartmouth.edu/TR/TR95-253.ps.Z.  
Note: Appeared at the Workshop for I/O in Parallel and Distributed Systems at IPPS '95, pp. 78-89. Available at  
Abstract: As parallel systems move into the production scientific-computing world, the emphasis will be on cost-effective solutions that provide high throughput for a mix of applications. Cost-effective solutions demand that a system make effective use of all of its resources. Many MIMD multiprocessors today, however, distinguish between "compute" and "I/O" nodes, the latter having attached disks and being dedicated to running the file-system server. This static division of responsibilities simplifies system management but does not necessarily lead to the best performance in workloads that need a different balance of computation and I/O. Of course, computational processes sharing a node with a file-system service may receive less CPU time, network bandwidth, and memory bandwidth than they would on a computation-only node. In this paper we begin to examine this issue experimentally. We found that high-performance I/O does not necessarily require substantial CPU time, leaving plenty of time for application computation. There were some complex file-system requests, however, which left little CPU time available to the application. (The impact on network and memory bandwidth still needs to be determined.) For applications (or users) that cannot tolerate an occasional interruption, we recommend that they continue to use only compute nodes. For tolerant applications needing more cycles than those provided by the compute nodes, we recommend that they take full advantage of both compute and I/O nodes for computation, and that operating systems should make this possible. 
Abstract-found: 1
Intro-found: 1
Reference: [ALBL91] <author> Thomas E. Anderson, Henry M. Levy, Brian N. Bershad, and Ed-ward D. Lazowska. </author> <title> The interaction of architecture and operating system design. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 108-120, </pages> <year> 1991. </year>
Reference-contexts: Proteus itself has been validated against real message-passing machines [BDCW91]. We configured Proteus using the parameters listed in Table 1. These parameters are not meant to reflect any particular machine, but a generic machine of current technology. 1 This is a moderate context-switch time <ref> [ALBL91] </ref>, even when cache effects are considered. In any case, preliminary experiments showed that our results were not sensitive to this parameter. 5 Table 1: Parameters for simulator.
Reference: [BBH95] <author> Sandra Johnson Baylor, Caroline B. Benveniste, and Yarson Hsu. </author> <title> Performance evaluation of a parallel I/O architecture. </title> <booktitle> In Proceedings of the 9th ACM International Conference on Supercomputing, </booktitle> <month> July </month> <year> 1995. </year> <note> To appear. </note>
Reference-contexts: This paper should only be considered a starting point, as we have only considered the impact of I/O service on the CPU utilization of an I/O node. File-I/O traffic may also substantially impact the communication performance of a computation-only application <ref> [BBH95] </ref>. Filesystem activity will also compete with a computation for memory bandwidth and cache space. Finally, efficient system software would be needed to provide the flexibility that we propose. Nonetheless, we feel that the issue is worth further exploration. An implementation, and experimentation with a real workload, are necessary.
Reference: [BBS + 94] <author> Robert Bennett, Kelvin Bryant, Alan Sussman, Raja Das, and Joel Saltz. Jovian: </author> <title> A framework for optimizing parallel I/O. </title> <booktitle> In Proceedings of the 1994 Scalable Parallel Libraries Conference, </booktitle> <pages> pages 10-20. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> Oc-tober </month> <year> 1994. </year>
Reference-contexts: Increasingly common, however, are specialized interfaces to support multidimensional matrices [CFPB93, SW94, GL91, GGL93, BdC93, BBS + 94, Mas92, SCJ + 95], and interfaces that support collective I/O <ref> [GGL93, BdC93, BBS + 94, Mas92] </ref>. With a collective-I/O interface, all processes make a single joint request to the file system, rather than numerous independent requests.
Reference: [BdC93] <author> Rajesh Bordawekar, Juan Miguel del Rosario, and Alok Choudhary. </author> <title> Design and evaluation of primitives for parallel I/O. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 452-461, </pages> <year> 1993. </year>
Reference-contexts: Increasingly common, however, are specialized interfaces to support multidimensional matrices [CFPB93, SW94, GL91, GGL93, BdC93, BBS + 94, Mas92, SCJ + 95], and interfaces that support collective I/O <ref> [GGL93, BdC93, BBS + 94, Mas92] </ref>. With a collective-I/O interface, all processes make a single joint request to the file system, rather than numerous independent requests.
Reference: [BDCW91] <author> Eric A. Brewer, Chrysanthos N. Del-larocas, Adrian Col-brook, and William E. Weihl. Pro-teus: </author> <title> A high-performance parallel-architecture simulator. </title> <type> Technical Report MIT/LCS/TR-516, </type> <institution> MIT, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: For each interruption, therefore, we deducted 50 sec. 1 Idle intervals shorter than 50 sec were therefore useless to the computation, and so were not counted. 3.5 Simulator Our traces were collected from the STARFISH parallel file-system simulator [Kot94], which ran on top of the Proteus parallel-architecture simulator <ref> [BDCW91] </ref>, which in turn ran on a DEC-5000 workstation. Proteus itself has been validated against real message-passing machines [BDCW91]. We configured Proteus using the parameters listed in Table 1. <p> therefore useless to the computation, and so were not counted. 3.5 Simulator Our traces were collected from the STARFISH parallel file-system simulator [Kot94], which ran on top of the Proteus parallel-architecture simulator <ref> [BDCW91] </ref>, which in turn ran on a DEC-5000 workstation. Proteus itself has been validated against real message-passing machines [BDCW91]. We configured Proteus using the parameters listed in Table 1. These parameters are not meant to reflect any particular machine, but a generic machine of current technology. 1 This is a moderate context-switch time [ALBL91], even when cache effects are considered.
Reference: [CF94] <author> Peter F. Corbett and Dror G. Fei-telson. </author> <title> Design and implementation of the Vesta parallel file system. </title> <booktitle> In Proceedings of the Scalable High-Performance Computing Conference, </booktitle> <pages> pages 63-70, </pages> <year> 1994. </year> <month> 10 </month>
Reference: [CFPB93] <author> Peter F. Corbett, Dror G. Feitelson, Jean-Pierre Prost, and Sandra John-son Baylor. </author> <title> Parallel access to files in the Vesta file system. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 472-481, </pages> <year> 1993. </year>
Reference: [DdR92] <author> Erik DeBenedictis and Juan Miguel del Rosario. </author> <title> nCUBE parallel I/O software. </title> <booktitle> In Eleventh Annual IEEE International Phoenix Conference on Computers and Communications (IPCCC), </booktitle> <pages> pages 0117-0124, </pages> <month> April </month> <year> 1992. </year>
Reference: [Dib90] <author> Peter C. Dibble. </author> <title> A Parallel Interleaved File System. </title> <type> PhD thesis, </type> <institution> University of Rochester, </institution> <month> March </month> <year> 1990. </year>
Reference: [DSE88] <author> Peter Dibble, Michael Scott, and Carla Ellis. </author> <title> Bridge: A high-performance file system for parallel processors. </title> <booktitle> In Proceedings of the Eighth International Conference on Distributed Computer Systems, </booktitle> <pages> pages 154-161, </pages> <month> June </month> <year> 1988. </year>
Reference: [FPD93] <author> James C. French, Terrence W. Pratt, and Mriganka Das. </author> <title> Performance measurement of the Concurrent File System of the Intel iPSC/2 hypercube. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> 17(1-2):115-121, January and February 1993. </note>
Reference: [GGL93] <author> N. Galbreath, W. Gropp, and D. Levine. </author> <title> Applications-driven parallel I/O. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 462-471, </pages> <year> 1993. </year>
Reference-contexts: Increasingly common, however, are specialized interfaces to support multidimensional matrices [CFPB93, SW94, GL91, GGL93, BdC93, BBS + 94, Mas92, SCJ + 95], and interfaces that support collective I/O <ref> [GGL93, BdC93, BBS + 94, Mas92] </ref>. With a collective-I/O interface, all processes make a single joint request to the file system, rather than numerous independent requests.
Reference: [GL91] <author> Andrew S. Grimshaw and Ed-mond C. Loyot, Jr. </author> <title> ELFS: object-oriented extensible file systems. </title> <type> Technical Report TR-91-14, </type> <institution> Univ. of Virginia Computer Science Department, </institution> <month> July </month> <year> 1991. </year>
Reference: [HdC95] <author> Michael Harry, Juan Miguel del Rosario, and Alok Choudhary. </author> <title> VIP-FS: A virtual, parallel file system for high performance parallel and distributed computing. </title> <booktitle> In Proceedings of the Ninth International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1995. </year> <note> To appear. </note>
Reference: [HER + 95] <author> Jay Huber, Christopher L. Elford, Daniel A. Reed, Andrew A. Chien, and David S. Blumenthal. </author> <title> PPFS: A high performance portable parallel file system. </title> <type> Technical Report UIUCDCS-R-95-1903, </type> <institution> University of Illinois at Urbana Champaign, </institution> <month> Jan-uary </month> <year> 1995. </year>
Reference: [HPF93] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Language Specification, </title> <address> 1.0 edition, </address> <month> May 3 </month> <year> 1993. </year>
Reference-contexts: Thus, 10 MB was a compromise to save simulation time. The file was striped, block by block, across the 16 disks attached to the computational application's processors. The matrix was distributed across the 16 memories of the I/O application according to one of the HPF distributions <ref> [HPF93] </ref>, as shown in Figure 1. Each matrix element was either 8 bytes or 8 Kbytes.
Reference: [Kot94] <author> David Kotz. </author> <title> Disk-directed I/O for MIMD multiprocessors. </title> <booktitle> In Proceedings of the 1994 Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 61-74, </pages> <note> Novem-ber 1994. Updated as Dartmouth TR PCS-TR94-226 on November 8, </note> <year> 1994. </year>
Reference-contexts: With a collective-I/O interface, all processes make a single joint request to the file system, rather than numerous independent requests. Disk-directed I/O is a promising new technique that takes advantage of a collective-I/O interface, and leads to much better performance than file systems based on traditional caching strategies <ref> [Kot94] </ref>. With disk-directed I/O, compute nodes make a collective request to the file system, which forwards the request to all I/O nodes. <p> Data transfers between compute nodes and I/O nodes use low-overhead "Memput" and "Memget" messages that move data directly to and from the application buffer. The experiments in <ref> [Kot94] </ref> show that disk-directed I/O obtains nearly the peak disk bandwidth across many data distributions and system configurations. There have been no previous studies of CPU activity on the I/O nodes of multiprocessors. A ten-year old study of diskless workstations [LZCZ86] found that file-server CPU load can be extremely high. <p> Disk-directed I/O is a new technique that takes advantage of a collective-I/O interface, and leads to much better performance than traditional caching <ref> [Kot94] </ref>. As described above, it works by giving control over the order and pace of data transfer to the I/O nodes, who optimize the transfer for maximum disk performance. <p> For each interruption, therefore, we deducted 50 sec. 1 Idle intervals shorter than 50 sec were therefore useless to the computation, and so were not counted. 3.5 Simulator Our traces were collected from the STARFISH parallel file-system simulator <ref> [Kot94] </ref>, which ran on top of the Proteus parallel-architecture simulator [BDCW91], which in turn ran on a DEC-5000 workstation. Proteus itself has been validated against real message-passing machines [BDCW91]. We configured Proteus using the parameters listed in Table 1. <p> The CPU overhead of traditional caching does not seem to be so bad, but this was again partially due to the poor I/O performance spreading out the overhead over many cycles. When we added barrier synchronizations to the computational application, the I/O activity 2 In <ref> [Kot94] </ref>, the easy patterns are called rc and wc with 8-KB records, and the hard patterns are called rbc and wbc with 8-byte records. 3 We suspect the latter may be improved with a gather/scatter message-passing mechanism. 6 Table 2: Percent of CPU time available to the computational application (100% is
Reference: [Kri94] <author> Orran Krieger. </author> <title> HFS: A flexible file system for shared-memory multiprocessors. </title> <type> PhD thesis, </type> <institution> University of Toronto, </institution> <month> October </month> <year> 1994. </year>
Reference: [KTR94] <author> David Kotz, Song Bac Toh, and Sriram Radhakrishnan. </author> <title> A detailed simulation model of the HP 97560 disk drive. </title> <type> Technical Report PCS-TR94-220, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: (one per IOP) 16 I/O bus type SCSI I/O bus peak bandwidth 10 Mbytes/s Interconnect topology 6 fi 6 torus Interconnect bandwidth 200 fi 10 6 bytes/s bidirectional Interconnect latency 20 ns per router Routing wormhole We added a disk model, a reimplementation of Ruemmler and Wilkes' HP 97560 model <ref> [RW94, KTR94] </ref>. We validated our model against disk traces provided by HP, using the same technique and measure as Ruemmler and Wilkes.
Reference: [LIN + 93] <author> Susan J. LoVerso, Marshall Is-man, Andy Nanopoulos, William Nesheim, Ewan D. Milne, and Richard Wheeler. sfs: </author> <title> A parallel file system for the CM-5. </title> <booktitle> In Proceedings of the 1993 Summer USENIX Conference, </booktitle> <pages> pages 291-305, </pages> <year> 1993. </year>
Reference: [LZCZ86] <author> Edward D. Lazowska, John Za-horjan, David R. Cheriton, and 11 Willy Zwaenepoel. </author> <title> File access performance of diskless workstations. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 4(3) </volume> <pages> 238-268, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: The experiments in [Kot94] show that disk-directed I/O obtains nearly the peak disk bandwidth across many data distributions and system configurations. There have been no previous studies of CPU activity on the I/O nodes of multiprocessors. A ten-year old study of diskless workstations <ref> [LZCZ86] </ref> found that file-server CPU load can be extremely high. To be able to provide high performance during periods of intense I/O activity, however, a balanced multiprocessor spreads its disks across many I/O nodes so that the I/O-node CPUs will not be a performance bottleneck.
Reference: [Mas92] <institution> Parallel file I/O routines. MasPar Computer Corporation, </institution> <year> 1992. </year>
Reference-contexts: Increasingly common, however, are specialized interfaces to support multidimensional matrices [CFPB93, SW94, GL91, GGL93, BdC93, BBS + 94, Mas92, SCJ + 95], and interfaces that support collective I/O <ref> [GGL93, BdC93, BBS + 94, Mas92] </ref>. With a collective-I/O interface, all processes make a single joint request to the file system, rather than numerous independent requests.
Reference: [MCD + 91] <author> Evangelos Markatos, Mark Crovella, Prakash Das, Cezary Dubnicki, and Tom LeBlanc. </author> <title> The effects of multiprogramming on barrier synchronization. </title> <booktitle> In Proceedings of the 1991 IEEE Symposium on Parallel and Distributed Processing, </booktitle> <pages> pages 662-669, </pages> <year> 1991. </year>
Reference-contexts: These interruptions slowed the computational application in two ways. First, every cycle spent servicing the I/O request was another cycle delay for the interrupted application. Second, delaying one process in the computational application indirectly delayed other processes that waited for the process at a future synchronization point <ref> [MCD + 91] </ref>. In our experiments we used two different kinds of computational applications, 36 different kinds of I/O applications, and two different kinds of file systems, all on a parallel file-system simulator. 3.1 Computational applications Our two computational applications did nothing but computation.
Reference: [MS94] <author> Steven A. Moyer and V. S. Sun-deram. </author> <title> PIOUS: a scalable parallel I/O system for distributed computing environments. </title> <booktitle> In Proceedings of the Scalable High-Performance Computing Conference, </booktitle> <pages> pages 71-78, </pages> <year> 1994. </year>
Reference: [Nit94] <author> Bill Nitzberg. </author> <title> Time between barriers. </title> <type> Personal communication, </type> <year> 1994. </year>
Reference-contexts: Similarly we chose a tight 1 msec interval to represent a challenging case (several NASA benchmarks on the Intel Paragon and an SGI cluster were measured with inter-barrier times of 6, 17, or 64 msec <ref> [Nit94] </ref>). Note that our barrier experiment also represents a computational application that is running on many processors, only some of which are involved in serving I/O, while others are left to run at full speed.
Reference: [Pie89] <author> Paul Pierce. </author> <title> A concurrent file system for a highly parallel mass storage system. </title> <booktitle> In Fourth Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 155-160. </pages> <publisher> Golden Gate Enterprises, </publisher> <address> Los Altos, CA, </address> <month> March </month> <year> 1989. </year>
Reference: [Roy93] <author> Paul J. Roy. </author> <title> Unix file access and caching in a multicomputer environment. </title> <booktitle> In Proceedings of the Usenix Mach III Symposium, </booktitle> <pages> pages 21-37, </pages> <year> 1993. </year>
Reference: [RW94] <author> Chris Ruemmler and John Wilkes. </author> <title> An introduction to disk drive modeling. </title> <journal> IEEE Computer, </journal> <volume> 27(3) </volume> <pages> 17-28, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: (one per IOP) 16 I/O bus type SCSI I/O bus peak bandwidth 10 Mbytes/s Interconnect topology 6 fi 6 torus Interconnect bandwidth 200 fi 10 6 bytes/s bidirectional Interconnect latency 20 ns per router Routing wormhole We added a disk model, a reimplementation of Ruemmler and Wilkes' HP 97560 model <ref> [RW94, KTR94] </ref>. We validated our model against disk traces provided by HP, using the same technique and measure as Ruemmler and Wilkes.
Reference: [SCJ + 95] <author> K. E. Seamons, Y. Chen, P. Jones, J. Jozwiak, and M. Winslett. </author> <title> Server-directed collective I/O in Panda. </title> <note> Submitted to Supercomputing '95, </note> <month> March </month> <year> 1995. </year>
Reference: [SW94] <author> K. E. Seamons and M. Winslett. </author> <title> An efficient abstract interface for multidimensional array I/O. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <pages> pages 650-659, </pages> <month> November </month> <year> 1994. </year>
Reference: [Wal94] <author> D. W. Walker. </author> <title> The design of a standard message passing interface for distributed memory concurrent computers. </title> <journal> Parallel Computing, </journal> <volume> 20(4) </volume> <pages> 657-673, </pages> <month> April </month> <year> 1994. </year> <month> 12 </month>
Reference-contexts: Faster processors, which would be found in any substantial parallel machine, should experience even less impact. Given the heavyweight nature of this operating system and the NFS file system, these results corroborate those in the previous section. 4 For more information see http://www.cs.dartmouth.edu/research/fleet/. 5 We used MPI <ref> [Wal94] </ref> for the communication support. Table 4: Execution time of a synthetic parallel computation, in seconds. In the "No I/O" case, this application runs alone, and represents the ideal execution time for this application.
References-found: 31

