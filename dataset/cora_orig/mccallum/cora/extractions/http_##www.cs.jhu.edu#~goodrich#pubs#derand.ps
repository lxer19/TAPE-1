URL: http://www.cs.jhu.edu/~goodrich/pubs/derand.ps
Refering-URL: http://www.cs.jhu.edu/~goodrich/pubs/index.html
Root-URL: http://www.cs.jhu.edu
Email: goodrich@cs.jhu.edu ramose@dimacs.rutgers.edu  
Title: Bounded-Independence Derandomization of Geometric Partitioning with Applications to Parallel Fixed-Dimensional Linear Programming  
Author: Michael T. Goodrich Edgar A. Ramos 
Address: Baltimore, MD 21218 Piscataway, NJ 08855-1179  
Affiliation: Center for Geometric Computing DIMACS Johns Hopkins University Rutgers University  
Abstract: We give fast and efficient methods for constructing *-nets and *-approximations for range spaces with bounded VC-exponent. These combinatorial structures have wide applicability to geometric partitioning problems, which are often used in divide-and-conquer constructions in computational geometry algorithms. In addition, we introduce a new deterministic set approximation for range spaces with bounded VC-exponent, which we call the ffi-relative *-approximation, and we show how such approximations can be efficiently constructed in parallel. To demonstrate the utility of these constructions we show how they can be used to solve the linear programming problem in IR d deterministically in O((log log n) d ) time using linear work in the PRAM model of computation, for any fixed constant d. Our method is developed for the CRCW variant of the PRAM parallel computation model, and can be easily implemented to run in O(log n(log log n) d1 ) time using linear work on an EREW PRAM.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. K. Agarwal. </author> <title> Partitioning arrangements of lines: I. An efficient deterministic algorithm. </title> <journal> Discrete Comput. Geom., </journal> <volume> 5 </volume> <pages> 449-483, </pages> <year> 1990. </year>
Reference-contexts: 1 Introduction The study of randomized algorithms and methods for reducing the amount of perfect randomness needed for geometric algorithms has proven to be a very rich area of research (e.g., see <ref> [1, 2, 4, 5, 14, 15, 22, 42, 58, 57] </ref>). <p> Nevertheless, as shown by Rompel [62] (see also Schmidt, Siegel and Srinivasan [64]), we may derive something analogous: Lemma 2.1 [62]: Let X (k) be the sum of n k-wise independent random variables taking on values in the range <ref> [0; 1] </ref>, with = E (X (k) ), where k is a positive even integer.
Reference: [2] <author> P. K. Agarwal. </author> <title> Geometric partitioning and its applications. </title> <editor> In J. E. Goodman, R. Pollack, and W. Steiger, editors, </editor> <booktitle> Computational Geometry: Papers from the DIMACS special year. </booktitle> <publisher> Amer. Math. Soc., </publisher> <year> 1991. </year>
Reference-contexts: 1 Introduction The study of randomized algorithms and methods for reducing the amount of perfect randomness needed for geometric algorithms has proven to be a very rich area of research (e.g., see <ref> [1, 2, 4, 5, 14, 15, 22, 42, 58, 57] </ref>). <p> One general type of geometric structure that has motivated much of the derandomization research, and one that motivated the development of the *-approximation and *-net notions for computational geometry, is the geometric partition (e.g., see <ref> [2, 49] </ref>). In this problem, one is given a collection X of n hyperplanes in IR d , and a parameter r, and one wishes to construct a partition of IR d into O (r d ) constant-sized cells so that each cell intersects as few hyperplanes as possible.
Reference: [3] <author> M. Ajtai and N. Megiddo. </author> <title> A deterministic poly(log log n)-time n-processor algorithm for linear programming in fixed dimension. </title> <booktitle> In Proc. 24th Annu. ACM Sympos. Theory Comput., </booktitle> <pages> 327-338, </pages> <year> 1992. </year>
Reference-contexts: The existing deterministic parallel algorithms are not as efficient, however. Ajtai and Megiddo <ref> [3] </ref> give a deterministic O ((log log n) d ) time method, but it has a sub-optimal fi (n (log log n) d ) work bound and it is defined for the very powerful parallel model that only counts "comparison" steps [67]. <p> Without loss of generality, we may additionally assume that ~v = (0; 0; : : :; 0; 1), i.e., we are interested in the "lowest" vertex in P . Our method for finding p is inspired by the methods of Ajtai and Megiddo <ref> [3] </ref> and Dyer [26], but is nevertheless quite different. We find the optimal solution p by calling the following recursive procedure as ParLP d (X; 2n). Procedure ParLP d (X; w): Output: An optimal solution p for X (using work that is O (w)). 1. Let n = jXj.
Reference: [4] <author> N. Alon and N. Megiddo. </author> <title> Parallel linear programming in fixed dimension almost surely in constant time. </title> <booktitle> In Proc. 31st Annu. IEEE Sympos. </booktitle> <institution> Found. Comput. Sci., </institution> <month> 574-582, </month> <year> 1990. </year>
Reference-contexts: 1 Introduction The study of randomized algorithms and methods for reducing the amount of perfect randomness needed for geometric algorithms has proven to be a very rich area of research (e.g., see <ref> [1, 2, 4, 5, 14, 15, 22, 42, 58, 57] </ref>). <p> Alternatively, in the weaker EREW PRAM model processors may not concurrently access the same memory location. 3 see [16, 19, 21, 28, 39, 50, 65]) or on building data structures for linear programming queries (e.g., see [30, 53]). In the parallel domain, Alon and Megiddo <ref> [4] </ref> give analogous results, showing that through the use of randomization one can solve a fixed-dimensional linear program in O (1) time with very high probability using n processors in a randomized CRCW PRAM model. The existing deterministic parallel algorithms are not as efficient, however.
Reference: [5] <author> N. Alon and J. Spencer. </author> <title> The probabilistic method. </title> <editor> J. </editor> <publisher> Wiley & Sons, </publisher> <year> 1993. </year>
Reference-contexts: 1 Introduction The study of randomized algorithms and methods for reducing the amount of perfect randomness needed for geometric algorithms has proven to be a very rich area of research (e.g., see <ref> [1, 2, 4, 5, 14, 15, 22, 42, 58, 57] </ref>). <p> We perform this derandomization using the bounded independence derandomization technique <ref> [5, 41, 44, 45, 64] </ref>, which assumes our algorithm uses random variables that are only k-wise independent. <p> We perform this derandomization using the bounded independence derandomization technique [5, 41, 44, 45, 64], which assumes our algorithm uses random variables that are only k-wise independent. Thus, before we give our methods, let us review these concepts (see also <ref> [5, 57] </ref>). 2.1 Random sampling Since the probabilistic algorithm we wish to derandomize is based upon random sampling, let us begin by saying a few words about this technique. <p> Note that in this notation X (n) = X; hence, we may omit the superscript if the underlying random variables are mutually independent. Unfortunately, restricting our attention to k-wise independent indicator random variables prevents us from directly using the well-known and powerful Chernoff bounds <ref> [5, 17, 35, 57] </ref> for bounding the tail of the distribution of their sum. <p> Then there is a fixed constant c &gt; 0 such that Pr (jX (k) j ) c k + k 2 ! k=2 for any &gt; 0. 2.3 Derandomization via bounded independence We are now ready to review the bounded independence technique for derandomizing a probabilistic algorithm <ref> [5, 41, 44, 45] </ref>. We use the parallel formulation of Luby [44], which is based upon a combinatorial construction of Joffe [38] (see also Karloff and Mansour [41]).
Reference: [6] <author> N. M. Amato, M. T. Goodrich, and E. A. Ramos. </author> <title> Parallel algorithms for higher-dimensional convex hulls. </title> <booktitle> In Proc. 35th Annu. IEEE Sympos. </booktitle> <institution> Found. Comput. Sci., </institution> <month> 683-694, </month> <year> 1994. </year>
Reference-contexts: Our specific interest in this paper is in the design of fast and efficient deterministic methods for constructing small-sized ffi-relative *-approximations in parallel and applying these methods to fixed-dimensional linear programming. Our methods have other applications as well, including fixed-dimensional convex hull and geometric partition construction <ref> [6, 7] </ref>, but these are beyond the scope of this paper. 1.1 Previous work on derandomizing geometric algorithms Before we describe our results, however, let us review some related previous work. <p> Interestingly, Amato, Goodrich, and Ramos <ref> [6, 7] </ref> have shown how to use such methods to derive efficient parallel algorithms for d-dimensional convex hull construction, planar segment intersection computation, (1=r)-cutting construction, and d-dimensional point location. We suspect that there may be other applications, as well.
Reference: [7] <author> N. M. Amato, M. T. Goodrich, and E. A. Ramos. </author> <title> Computing faces in segment and simplex arrangements. </title> <booktitle> In Proc. 27th Annu. ACM Sympos. Theory Comput., </booktitle> <pages> 672-682, </pages> <year> 1995. </year>
Reference-contexts: Our specific interest in this paper is in the design of fast and efficient deterministic methods for constructing small-sized ffi-relative *-approximations in parallel and applying these methods to fixed-dimensional linear programming. Our methods have other applications as well, including fixed-dimensional convex hull and geometric partition construction <ref> [6, 7] </ref>, but these are beyond the scope of this paper. 1.1 Previous work on derandomizing geometric algorithms Before we describe our results, however, let us review some related previous work. <p> Interestingly, Amato, Goodrich, and Ramos <ref> [6, 7] </ref> have shown how to use such methods to derive efficient parallel algorithms for d-dimensional convex hull construction, planar segment intersection computation, (1=r)-cutting construction, and d-dimensional point location. We suspect that there may be other applications, as well.
Reference: [8] <author> P. Assouad. </author> <title> Densite et dimension. </title> <journal> Ann. Inst. Fourier, Grenoble, </journal> <volume> 3 </volume> <pages> 232-282, </pages> <year> 1983. </year>
Reference-contexts: A related and simpler notion, however, is based upon the shatter function, R (m) = fjRj A j: A X; jAj = mg: In particular, we say that (X; R) has VC-exponent <ref> [8, 13] </ref> bounded by e if R (m) is O (m e ).
Reference: [9] <author> P. Beame and J. Hastad. </author> <title> Optimal bounds for decision problems on the CRCW PRAM. </title> <journal> Journal of the ACM, </journal> <volume> 36(3) </volume> <pages> 643-670, </pages> <year> 1989. </year>
Reference-contexts: cannot immediately derive Poly (log log n)-time methods for the CRCW PRAM from the above analysis, for checking if a given Y satisfies the condition for being a (1=r)-approximation requires (log n= log log n) time using a polynomial number of processor, by a simple reduction from the parity problem <ref> [9] </ref>. We can avoid this lower bound, however, by checking this condition approximately rather than exactly. To do this we use a fast method for -approximate counting [31, 33], where one wishes to compute the sum of an array of n bits with a relative error of .
Reference: [10] <author> K. P. Bennett. </author> <title> Decision tree construction via linear programming. </title> <booktitle> In Proceedings of the 4th Midwest Artificial Intelligence and Cognitive Science Society Conference, </booktitle> <pages> 97-101, </pages> <year> 1992. </year>
Reference-contexts: Of particular interest is the case when the dimensionality, d (corresponding to the number of variables), is fixed, as occurs, for example, in several applications of linear programming in geometric computing (e.g., see [16, 21, 29, 54, 55, 60]) and machine learning (e.g., see <ref> [10, 11] </ref>).
Reference: [11] <editor> K.P. Bennett and O.L. Mangasarian. </editor> <title> Multicategory discrimination via linear programming. </title> <journal> Optimization Methods and Software, </journal> <volume> 3 </volume> <pages> 29-39, </pages> <year> 1994. </year>
Reference-contexts: Of particular interest is the case when the dimensionality, d (corresponding to the number of variables), is fixed, as occurs, for example, in several applications of linear programming in geometric computing (e.g., see [16, 21, 29, 54, 55, 60]) and machine learning (e.g., see <ref> [10, 11] </ref>).
Reference: [12] <author> B. Berger, J. Rompel, and P. W. Shor. </author> <title> Efficient NC algorithms for set cover with applications to learning and geometry. </title> <booktitle> In Proc. 30th Annu. IEEE Sympos. Found. Comput. Sci., </booktitle> <volume> volume 30, </volume> <pages> 54-59, </pages> <year> 1989. </year> <note> Also in J. </note> <institution> Comput. Syst. Sci. </institution> <month> 49, 454-477 </month> <year> (1994). </year>
Reference-contexts: Chazelle and Friedman [15] show that one can in fact construct such a partitioning with * = 1=r deterministically in polynomial time, and Berger, Rompel, and Shor <ref> [12] </ref> and Motwani, Naor, and Naor [56] show that one can construct similar geometric partitions for * = log r=r in N C. (Recall that NC denotes the class of problems solvable in polylogarithmic time using a polynomial number of processors [37, 43].) Unfortunately, the running time of Chazelle and Friedman's <p> In this paper we assume such a sample is chosen by defining, for each element x i in X in parallel, a random variable X i that is 1 with probability s=n; we use the rule that x i 2 Y if X i = 1 <ref> [12] </ref>.
Reference: [13] <author> H. Bronnimann and M. T. Goodrich. </author> <title> Almost optimal set covers in finite VC-dimension. </title> <journal> Discrete and Computational Geometry, </journal> <volume> 14, </volume> <year> 1995, </year> <pages> 463-479. 19 </pages>
Reference-contexts: A related and simpler notion, however, is based upon the shatter function, R (m) = fjRj A j: A X; jAj = mg: In particular, we say that (X; R) has VC-exponent <ref> [8, 13] </ref> bounded by e if R (m) is O (m e ).
Reference: [14] <author> Herve Bronnimann, Bernard Chazelle, and Jir Matousek. </author> <title> Product range spaces, sensitive sampling, </title> <booktitle> and derandomization. In Proc. 34th Annu. IEEE Sympos. Found. Comput. Sci. (FOCS 93), </booktitle> <pages> 400-409, </pages> <year> 1993. </year>
Reference-contexts: 1 Introduction The study of randomized algorithms and methods for reducing the amount of perfect randomness needed for geometric algorithms has proven to be a very rich area of research (e.g., see <ref> [1, 2, 4, 5, 14, 15, 22, 42, 58, 57] </ref>). <p> if, for each range R 2 R, fi fi jY " Rj jX j fi fi ffi jX j This notion is a combined measure of the absolute and relative error between jY " Rj=jY j and jRj=jX j, and it is somewhat similar to a notion Bronnimann et al. <ref> [14] </ref> refer to as a "sensitive" *-approximation 1 . Note that this notion also subsumes that of an *-net, for any ffi-relative *-approximation is automatically an (*=(1 ffi))-net. <p> A general framework for geometric partitioning emerges from the framework when a range space (X; R) has constant Vapnik-Chervonenkis [68] (VC)-dimension. Letting Rj A denote the set fA"R : R 2 1 Bronnimann et al. <ref> [14] </ref> call a subset A X a sensitive *-approximation if jjA"Rj=jAjjRj=jXjj (*=2)(*+ p 2 Rg, the VC-dimension of (X; R) is defined as the maximum size of a subset A of X such that Rj A = 2 A (e.g., see [49]). <p> There are several recent results that show that one can construct a (1=r)-approximation of size O (r 2 log r) for any range space with VC-exponent bounded by e in time O (nr c ) for some constant c depending on e (e.g., see <ref> [14, 16, 48, 47, 52, 51] </ref>).
Reference: [15] <author> B. Chazelle and J. Friedman. </author> <title> A deterministic view of random sampling and its use in geometry. </title> <journal> Combinatorica, </journal> <volume> 10(3) </volume> <pages> 229-249, </pages> <year> 1990. </year>
Reference-contexts: 1 Introduction The study of randomized algorithms and methods for reducing the amount of perfect randomness needed for geometric algorithms has proven to be a very rich area of research (e.g., see <ref> [1, 2, 4, 5, 14, 15, 22, 42, 58, 57] </ref>). <p> One can apply random sampling to construct such a partitioning so that each cell intersects at most *n hyperplanes, for * = log r=r [22, 36]. Chazelle and Friedman <ref> [15] </ref> show that one can in fact construct such a partitioning with * = 1=r deterministically in polynomial time, and Berger, Rompel, and Shor [12] and Motwani, Naor, and Naor [56] show that one can construct similar geometric partitions for * = log r=r in N C. (Recall that NC denotes <p> Compute the intersection of the halfspaces in Y and a canonical triangulation T <ref> [15] </ref> of this polyhedral region (with the origin as base apex), using a "brute force" method that uses O (r c ) 17 work. (In a CRCW implementation this can be done in O (log log r) time; an EREW implement-ation takes O (log r) time.
Reference: [16] <author> B. Chazelle and J. Matousek. </author> <title> On linear-time deterministic algorithms for optimization problems in fixed dimension. </title> <booktitle> In Proc. 4th ACM-SIAM Sympos. Discrete Algorithms, </booktitle> <pages> 281-290, </pages> <year> 1993. </year>
Reference-contexts: There are several recent results that show that one can construct a (1=r)-approximation of size O (r 2 log r) for any range space with VC-exponent bounded by e in time O (nr c ) for some constant c depending on e (e.g., see <ref> [14, 16, 48, 47, 52, 51] </ref>). <p> Chazelle and Matousek <ref> [16] </ref> give slower NC algorithms using O (nr c ) work 2 that construct such sets of size O (r 2+ff ) for any fixed constant ff &gt; 0. 1.2 Our results on parallel geometric derandomization We give fast and efficient efficient parallel algorithms for constructing *-nets and ffi-relative *- approximations. <p> Thus, our methods improve the previous size bounds from those achieved previously by the author [32] while also improving the time bounds from those achieved previously by Chazelle and Matousek <ref> [16] </ref>. We also derive similar bounds for constructing (1=r)-nets. To demonstrate the utility of this result, we show how it can be used to design a new efficient parallel method for fixed-dimensional linear programming. 1.3 Fixed-Dimensional Linear Programming The linear programming problem is central in the study of discrete algorithms. <p> Of particular interest is the case when the dimensionality, d (corresponding to the number of variables), is fixed, as occurs, for example, in several applications of linear programming in geometric computing (e.g., see <ref> [16, 21, 29, 54, 55, 60] </ref>) and machine learning (e.g., see [10, 11]). <p> Alternatively, in the weaker EREW PRAM model processors may not concurrently access the same memory location. 3 see <ref> [16, 19, 21, 28, 39, 50, 65] </ref>) or on building data structures for linear programming queries (e.g., see [30, 53]).
Reference: [17] <author> H. Chernoff. </author> <title> A measure of asymptotic efficiency for tests of a hypothesis based on the sum of the observations. </title> <journal> Annals of Math. Stat., </journal> <volume> 23 </volume> <pages> 493-509, </pages> <year> 1952. </year>
Reference-contexts: Note that in this notation X (n) = X; hence, we may omit the superscript if the underlying random variables are mutually independent. Unfortunately, restricting our attention to k-wise independent indicator random variables prevents us from directly using the well-known and powerful Chernoff bounds <ref> [5, 17, 35, 57] </ref> for bounding the tail of the distribution of their sum.
Reference: [18] <author> V. Chvatal. </author> <title> Linear Programming. </title> <editor> W. H. </editor> <publisher> Freeman, </publisher> <address> New York, NY, </address> <year> 1983. </year>
Reference-contexts: It has been applied to a host of combinatorial optimization problems since the first efficient algorithms for solving it were developed in the 1940's (e.g., see <ref> [18, 23, 40, 59] </ref>). Geometrically, it can be viewed as the problem of locating a point that is maximal in a given ~v direction in the polyhedral region P defined by the intersection of n halfspaces in IR d .
Reference: [19] <author> K. L. Clarkson. </author> <title> Linear programming in O(n3 d 2 ) time. </title> <journal> Inform. Process. Lett., </journal> <volume> 22 </volume> <pages> 21-24, </pages> <year> 1986. </year>
Reference-contexts: Alternatively, in the weaker EREW PRAM model processors may not concurrently access the same memory location. 3 see <ref> [16, 19, 21, 28, 39, 50, 65] </ref>) or on building data structures for linear programming queries (e.g., see [30, 53]).
Reference: [20] <author> K. L. Clarkson. </author> <title> New applications of random sampling in computational geometry. </title> <journal> Discrete Comput. Geom., </journal> <volume> 2 </volume> <pages> 195-222, </pages> <year> 1987. </year>
Reference-contexts: The study of random sampling in the design of efficient computational geometry methods really began in earnest with some outstanding early work of Clarkson <ref> [20] </ref>, Haussler and Welzl [36], and Clarkson and Shor [22]. One general type of geometric structure that has motivated much of the derandomization research, and one that motivated the development of the *-approximation and *-net notions for computational geometry, is the geometric partition (e.g., see [2, 49]). <p> give applications of these methods to fixed-dimensional linear programming in Section 5, and we conclude in Section 6. 2 Probabilistic Preliminaries Our approach to constructing small-sized (1=r)-nets and (1=r)-approximations of range spaces with bounded VC-exponent is to derandomize a straightforward probabilistic algorithm that is based upon the random sampling technique <ref> [20] </ref>. We perform this derandomization using the bounded independence derandomization technique [5, 41, 44, 45, 64], which assumes our algorithm uses random variables that are only k-wise independent. <p> This approach to constructing small-sized approximations and nets of range spaces with bounded VC-exponent is to derandomize a straightforward probabilistic algorithm, Approx, which is based upon the random sampling technique <ref> [20] </ref>. 3.1 Geometric random samples Let (X; R) be a given range space with VC-exponent bounded by e, for some constant e &gt; 0.
Reference: [21] <author> K. L. Clarkson. </author> <title> A Las Vegas algorithm for linear programming when the dimension is small. </title> <booktitle> In Proc. 29th Annu. IEEE Sympos. </booktitle> <institution> Found. Comput. Sci., </institution> <month> 452-456, </month> <year> 1988. </year> <note> Also in JACM 42, </note> <month> 488-499 </month> <year> (1995). </year>
Reference-contexts: Of particular interest is the case when the dimensionality, d (corresponding to the number of variables), is fixed, as occurs, for example, in several applications of linear programming in geometric computing (e.g., see <ref> [16, 21, 29, 54, 55, 60] </ref>) and machine learning (e.g., see [10, 11]). <p> Alternatively, in the weaker EREW PRAM model processors may not concurrently access the same memory location. 3 see <ref> [16, 19, 21, 28, 39, 50, 65] </ref>) or on building data structures for linear programming queries (e.g., see [30, 53]).
Reference: [22] <author> K. L. Clarkson and P. W. Shor. </author> <title> Applications of random sampling in computational geometry, II. </title> <journal> Discrete Comput. Geom., </journal> <volume> 4 </volume> <pages> 387-421, </pages> <year> 1989. </year>
Reference-contexts: 1 Introduction The study of randomized algorithms and methods for reducing the amount of perfect randomness needed for geometric algorithms has proven to be a very rich area of research (e.g., see <ref> [1, 2, 4, 5, 14, 15, 22, 42, 58, 57] </ref>). <p> The study of random sampling in the design of efficient computational geometry methods really began in earnest with some outstanding early work of Clarkson [20], Haussler and Welzl [36], and Clarkson and Shor <ref> [22] </ref>. One general type of geometric structure that has motivated much of the derandomization research, and one that motivated the development of the *-approximation and *-net notions for computational geometry, is the geometric partition (e.g., see [2, 49]). <p> One can apply random sampling to construct such a partitioning so that each cell intersects at most *n hyperplanes, for * = log r=r <ref> [22, 36] </ref>.
Reference: [23] <author> G. B. Dantzig. </author> <title> Linear Programming and Extensions. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ, </address> <year> 1963. </year>
Reference-contexts: It has been applied to a host of combinatorial optimization problems since the first efficient algorithms for solving it were developed in the 1940's (e.g., see <ref> [18, 23, 40, 59] </ref>). Geometrically, it can be viewed as the problem of locating a point that is maximal in a given ~v direction in the polyhedral region P defined by the intersection of n halfspaces in IR d .
Reference: [24] <author> X. Deng. </author> <title> An optimal parallel algorithm for linear programming in the plane. </title> <journal> Inform. Process. Lett., </journal> <volume> 35 </volume> <pages> 213-217, </pages> <year> 1990. </year>
Reference-contexts: The only work-optimal deterministic PRAM result we are familiar with is a method by Deng <ref> [24] </ref> for 2-dimensional linear programming that runs in O (log n) time using O (n) work on a CRCW PRAM.
Reference: [25] <author> M. Dietzfelbinger. </author> <title> Universal hashing and k-wise independent random variables via integer arithmetic without primes. </title> <booktitle> In Proc. 13th Annual Symp. on Theoretical Aspects of Computer Science, 1996. Lecture Notes in Computer Science 1046, </booktitle> <pages> 569-580, </pages> <publisher> Springer 1996. </publisher>
Reference-contexts: Of course, we desire these "error functions" to be as small as possible. The next lemma explores how well a random Y achieves this goal when Y is defined using k-wise independent random variables. 5 Recently, Dietzfelbinger <ref> [25] </ref> has given an alternative construction that does make use of the availability of a prime q. 6 Lemma 3.2: Let (X; R) be a range space.
Reference: [26] <author> M. E. Dyer. </author> <title> A parallel algorithm for linear programming in fixed dimension. </title> <booktitle> In Proc. 11th ACM Symp. on Computational Geometry, </booktitle> <year> 1995. </year>
Reference-contexts: The only work-optimal deterministic PRAM result we are familiar with is a method by Deng [24] for 2-dimensional linear programming that runs in O (log n) time using O (n) work on a CRCW PRAM. Recently, Dyer <ref> [26] </ref> has given an O (log n (log log n) d1 ) time method that uses O (n log n (log log n) d1 ) work in the EREW PRAM model. <p> Without loss of generality, we may additionally assume that ~v = (0; 0; : : :; 0; 1), i.e., we are interested in the "lowest" vertex in P . Our method for finding p is inspired by the methods of Ajtai and Megiddo [3] and Dyer <ref> [26] </ref>, but is nevertheless quite different. We find the optimal solution p by calling the following recursive procedure as ParLP d (X; 2n). Procedure ParLP d (X; w): Output: An optimal solution p for X (using work that is O (w)). 1. Let n = jXj.
Reference: [27] <author> M. E. Dyer. </author> <title> Linear time algorithms for two- and three-variable linear programs. </title> <journal> SIAM J. Comput., </journal> <volume> 13 </volume> <pages> 31-45, </pages> <year> 1984. </year>
Reference-contexts: Indeed, a major contribution of computational geometry research has been to show that fixed-dimensional linear programming can be solved in linear time, starting with the seminal work of Dyer <ref> [27] </ref> and Megiddo [54, 55], and following with subsequent work in the sequential domain concentrated primarily on reducing the constant "hiding behind" the big-oh in these results (e.g., 2 Recall that the work done by a parallel algorithm is the total number of operations performed by all processors, and it is
Reference: [28] <author> M. E. Dyer. </author> <title> On a multidimensional search technique and its application to the Euclidean one center problem. </title> <journal> SIAM J. Comput., </journal> <volume> 15 </volume> <pages> 725-738, </pages> <year> 1986. </year>
Reference-contexts: Alternatively, in the weaker EREW PRAM model processors may not concurrently access the same memory location. 3 see <ref> [16, 19, 21, 28, 39, 50, 65] </ref>) or on building data structures for linear programming queries (e.g., see [30, 53]).
Reference: [29] <author> H. Edelsbrunner. </author> <title> Algorithms in Combinatorial Geometry, </title> <booktitle> volume 10 of EATCS Monographs on Theoretical Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Heidelberg, West Germany, </address> <year> 1987. </year>
Reference-contexts: Of particular interest is the case when the dimensionality, d (corresponding to the number of variables), is fixed, as occurs, for example, in several applications of linear programming in geometric computing (e.g., see <ref> [16, 21, 29, 54, 55, 60] </ref>) and machine learning (e.g., see [10, 11]).
Reference: [30] <author> D. Eppstein. </author> <title> Dynamic three-dimensional linear programming. </title> <journal> ORSA J. Comput., </journal> <volume> 4 </volume> <pages> 360-368, </pages> <year> 1992. </year>
Reference-contexts: Alternatively, in the weaker EREW PRAM model processors may not concurrently access the same memory location. 3 see [16, 19, 21, 28, 39, 50, 65]) or on building data structures for linear programming queries (e.g., see <ref> [30, 53] </ref>). In the parallel domain, Alon and Megiddo [4] give analogous results, showing that through the use of randomization one can solve a fixed-dimensional linear program in O (1) time with very high probability using n processors in a randomized CRCW PRAM model.
Reference: [31] <author> T. Goldberg and U. Zwick. </author> <title> Optimal deterministic approximate parallel prefix sums and their applications. </title> <booktitle> In Proc. 4th IEEE Israel Symp. on Theory of Computing and Systems, </booktitle> <pages> 220-228, </pages> <year> 1995. </year>
Reference-contexts: We can avoid this lower bound, however, by checking this condition approximately rather than exactly. To do this we use a fast method for -approximate counting <ref> [31, 33] </ref>, where one wishes to compute the sum of an array of n bits with a relative error of . That is, if x is the number of 1's in the array, then we desire a value x 0 such that x=(1 + ) x 0 (1 + )x. <p> That is, if x is the number of 1's in the array, then we desire a value x 0 such that x=(1 + ) x 0 (1 + )x. Lemma 3.6 <ref> [31] </ref>: Performing -approximate counting of an n-element Boolean array, with = (log N ) b , can be done in O (1) time using O ((n + N ) f (b) ) work on a CRCW PRAM, for any fixed constant b &gt; 0. <p> The work bound we pass to this recursive call is w, unless this level in the recursion is equal to ci+1, for some integer i 1, in which case we pass the work bound w=2 1=c . (To implement this step in the CRCW PRAM model we use -approximate compaction <ref> [31, 34, 46] </ref>, where one is given an array A with m of its locations "occupied" and one wishes to map these m distinguished elements to an array B of size (1 + )m. The time bound is O (log log n) [31] using linear work. <p> The time bound is O (log log n) <ref> [31] </ref> using linear work.
Reference: [32] <author> M. T. Goodrich. </author> <title> Geometric partitioning made easier, even in parallel. </title> <booktitle> In Proc. 9th Annu. ACM Sympos. Comput. Geom., </booktitle> <pages> 73-82, </pages> <year> 1993. </year> <month> 20 </month>
Reference-contexts: Thus, our methods improve the previous size bounds from those achieved previously by the author <ref> [32] </ref> while also improving the time bounds from those achieved previously by Chazelle and Matousek [16]. We also derive similar bounds for constructing (1=r)-nets.
Reference: [33] <author> M. T. Goodrich, Y. Matias, and U. Vishkin. </author> <title> Optimal parallel approximation for prefix sums and integer sorting. </title> <booktitle> In Proc. 5th ACM-SIAM Symp. on Discrete Algorithms, </booktitle> <pages> 241-250, </pages> <year> 1994. </year>
Reference-contexts: We can avoid this lower bound, however, by checking this condition approximately rather than exactly. To do this we use a fast method for -approximate counting <ref> [31, 33] </ref>, where one wishes to compute the sum of an array of n bits with a relative error of . That is, if x is the number of 1's in the array, then we desire a value x 0 such that x=(1 + ) x 0 (1 + )x.
Reference: [34] <author> T. Hagerup. </author> <title> Fast deterministic processor allocation. </title> <booktitle> In 4th ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> 1-10, </pages> <year> 1993. </year>
Reference-contexts: The work bound we pass to this recursive call is w, unless this level in the recursion is equal to ci+1, for some integer i 1, in which case we pass the work bound w=2 1=c . (To implement this step in the CRCW PRAM model we use -approximate compaction <ref> [31, 34, 46] </ref>, where one is given an array A with m of its locations "occupied" and one wishes to map these m distinguished elements to an array B of size (1 + )m. The time bound is O (log log n) [31] using linear work.
Reference: [35] <author> T. Hagerup and C. Rub. </author> <title> A guided tour of Chernoff bounds. </title> <journal> Information Processing Letters, </journal> <volume> 33(10) </volume> <pages> 305-308, </pages> <year> 1990. </year>
Reference-contexts: Note that in this notation X (n) = X; hence, we may omit the superscript if the underlying random variables are mutually independent. Unfortunately, restricting our attention to k-wise independent indicator random variables prevents us from directly using the well-known and powerful Chernoff bounds <ref> [5, 17, 35, 57] </ref> for bounding the tail of the distribution of their sum.
Reference: [36] <author> D. Haussler and E. Welzl. </author> <title> Epsilon-nets and simplex range queries. </title> <journal> Discrete Comput. Geom., </journal> <volume> 2 </volume> <pages> 127-151, </pages> <year> 1987. </year>
Reference-contexts: DIMACS is an NSF Science and Technology Center, funded under contract STC-91-19999; and also receives support from the New Jersey Commission on Science and Technology. Author's current address: Max-Planck-Institut fur Informatik, Im Stadtwald, 66123 Saarbrucken, Germany. E-mail: ramos@mpi-sb.mpg.de 1 and the *-net <ref> [36, 49] </ref>. These concepts are defined for very general frameworks, where one is given a set system (X; R) consisting of a finite ground set, X, and a set, R, of subsets of X. <p> A subset Y is an *-approximation for (X; R) if, for each range R 2 R, fi fi jY " Rj jXj fi fi *: Relaxing this requirement a bit, Y is said to be an *-net <ref> [36, 49] </ref> of (X; R) if Y " R 6= ; for each R 2 R such that jRj &gt; *jXj. This is clearly a weaker notion than that of an *-approximation, for any *-approximation is automatically an *-net, but the converse need not be true. <p> The study of random sampling in the design of efficient computational geometry methods really began in earnest with some outstanding early work of Clarkson [20], Haussler and Welzl <ref> [36] </ref>, and Clarkson and Shor [22]. One general type of geometric structure that has motivated much of the derandomization research, and one that motivated the development of the *-approximation and *-net notions for computational geometry, is the geometric partition (e.g., see [2, 49]). <p> One can apply random sampling to construct such a partitioning so that each cell intersects at most *n hyperplanes, for * = log r=r <ref> [22, 36] </ref>.
Reference: [37] <author> J. JaJa. </author> <title> An Introduction to Parallel Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., </address> <year> 1992. </year>
Reference-contexts: deterministically in polynomial time, and Berger, Rompel, and Shor [12] and Motwani, Naor, and Naor [56] show that one can construct similar geometric partitions for * = log r=r in N C. (Recall that NC denotes the class of problems solvable in polylogarithmic time using a polynomial number of processors <ref> [37, 43] </ref>.) Unfortunately, the running time of Chazelle and Friedman's algorithm is quite high, as are the time and processor bounds of the implied parallel algorithms (they run in O (log 4 n) time using a number of processors proportional to the time bound of Chazelle and Friedman's algorithm). <p> the speed of the algorithm is the same as that used in Random (plus an additional term for performing an "or" on all the results in parallel, which can be done in O (1) time in the CRCW PRAM model and O (log n) time in the EREW PRAM model <ref> [37, 43, 61] </ref>). Having reviewed the necessary probabilistic preliminaries, let us now turn to the problem of constructing (1=r)-approximations and (1=r)-nets. 3 O ((nr) )-Work Approximation Finding Before we describe our work-efficient method, however, we first describe some algorithms for constructing (1=r)-nets and (1=r)-approximations that are fast but not work-efficient. <p> Both implementations are simple applications of parallel minimum finding <ref> [37, 43, 61] </ref> and are left to the reader.) 4. Using ParLP d1 as a subroutine, determine the simplex in T that contains p. <p> The time bound is O (log log n) [31] using linear work. Of course, in the EREW PRAM model this step can easily be implemented in O (log n) time via a parallel prefix computation <ref> [37, 43, 61] </ref>.) Since this method always recurses in a region guaranteed to contain the optimal point and we include in the subproblem all halfspaces whose boundary intersects , we will eventually find the optimal point p.
Reference: [38] <author> A. Joffe. </author> <title> On a set of almost deterministic k-independent random variables. </title> <journal> Annals of Probability, </journal> <volume> 2 </volume> <pages> 161-162, </pages> <year> 1974. </year>
Reference-contexts: We use the parallel formulation of Luby [44], which is based upon a combinatorial construction of Joffe <ref> [38] </ref> (see also Karloff and Mansour [41]). In this formulation, we assume we have a parallel probabilistic algorithm, Random, which is designed so that all the randomization is contained in a single choice step. In addition, we assume the following: 1.
Reference: [39] <author> G. Kalai. </author> <title> A subexponential randomized simplex algorithm. </title> <booktitle> In Proc. 24th Annu. ACM Sympos. Theory Comput., </booktitle> <pages> 475-482, </pages> <year> 1992. </year>
Reference-contexts: Alternatively, in the weaker EREW PRAM model processors may not concurrently access the same memory location. 3 see <ref> [16, 19, 21, 28, 39, 50, 65] </ref>) or on building data structures for linear programming queries (e.g., see [30, 53]).
Reference: [40] <author> H. Karloff. </author> <title> Linear Programming. </title> <publisher> Birkhauser, </publisher> <address> Boston, </address> <year> 1991. </year>
Reference-contexts: It has been applied to a host of combinatorial optimization problems since the first efficient algorithms for solving it were developed in the 1940's (e.g., see <ref> [18, 23, 40, 59] </ref>). Geometrically, it can be viewed as the problem of locating a point that is maximal in a given ~v direction in the polyhedral region P defined by the intersection of n halfspaces in IR d .
Reference: [41] <author> H. Karloff and Y. Mansour. </author> <title> On construction of k-wise independent random variables. </title> <booktitle> In Proc. ACM Sympos. Theory of Computing, </booktitle> <pages> 564-573, </pages> <year> 1994. </year>
Reference-contexts: We perform this derandomization using the bounded independence derandomization technique <ref> [5, 41, 44, 45, 64] </ref>, which assumes our algorithm uses random variables that are only k-wise independent. <p> Then there is a fixed constant c &gt; 0 such that Pr (jX (k) j ) c k + k 2 ! k=2 for any &gt; 0. 2.3 Derandomization via bounded independence We are now ready to review the bounded independence technique for derandomizing a probabilistic algorithm <ref> [5, 41, 44, 45] </ref>. We use the parallel formulation of Luby [44], which is based upon a combinatorial construction of Joffe [38] (see also Karloff and Mansour [41]). <p> We use the parallel formulation of Luby [44], which is based upon a combinatorial construction of Joffe [38] (see also Karloff and Mansour <ref> [41] </ref>). In this formulation, we assume we have a parallel probabilistic algorithm, Random, which is designed so that all the randomization is contained in a single choice step. In addition, we assume the following: 1.
Reference: [42] <author> H. Karloff and P. Raghavan. </author> <title> Randomized algorithms and pseudorandom numbers. </title> <journal> J. ACM, </journal> <volume> 40(3) </volume> <pages> 454-476, </pages> <year> 1993. </year>
Reference-contexts: 1 Introduction The study of randomized algorithms and methods for reducing the amount of perfect randomness needed for geometric algorithms has proven to be a very rich area of research (e.g., see <ref> [1, 2, 4, 5, 14, 15, 22, 42, 58, 57] </ref>).
Reference: [43] <author> R. M. Karp and V. Ramachandran. </author> <title> Parallel algorithms for shared memory machines. </title> <editor> In J. van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer Science, </booktitle> <pages> 869-941. </pages> <publisher> Elsevier/The MIT Press, </publisher> <address> Amsterdam, </address> <year> 1990. </year>
Reference-contexts: deterministically in polynomial time, and Berger, Rompel, and Shor [12] and Motwani, Naor, and Naor [56] show that one can construct similar geometric partitions for * = log r=r in N C. (Recall that NC denotes the class of problems solvable in polylogarithmic time using a polynomial number of processors <ref> [37, 43] </ref>.) Unfortunately, the running time of Chazelle and Friedman's algorithm is quite high, as are the time and processor bounds of the implied parallel algorithms (they run in O (log 4 n) time using a number of processors proportional to the time bound of Chazelle and Friedman's algorithm). <p> the speed of the algorithm is the same as that used in Random (plus an additional term for performing an "or" on all the results in parallel, which can be done in O (1) time in the CRCW PRAM model and O (log n) time in the EREW PRAM model <ref> [37, 43, 61] </ref>). Having reviewed the necessary probabilistic preliminaries, let us now turn to the problem of constructing (1=r)-approximations and (1=r)-nets. 3 O ((nr) )-Work Approximation Finding Before we describe our work-efficient method, however, we first describe some algorithms for constructing (1=r)-nets and (1=r)-approximations that are fast but not work-efficient. <p> Both implementations are simple applications of parallel minimum finding <ref> [37, 43, 61] </ref> and are left to the reader.) 4. Using ParLP d1 as a subroutine, determine the simplex in T that contains p. <p> The time bound is O (log log n) [31] using linear work. Of course, in the EREW PRAM model this step can easily be implemented in O (log n) time via a parallel prefix computation <ref> [37, 43, 61] </ref>.) Since this method always recurses in a region guaranteed to contain the optimal point and we include in the subproblem all halfspaces whose boundary intersects , we will eventually find the optimal point p.
Reference: [44] <author> M. Luby. </author> <title> A simple parallel algorithm for the maximal independent set problem. </title> <journal> SIAM J. Comput., </journal> <volume> 15(4) </volume> <pages> 1036-1053, </pages> <year> 1986. </year>
Reference-contexts: We perform this derandomization using the bounded independence derandomization technique <ref> [5, 41, 44, 45, 64] </ref>, which assumes our algorithm uses random variables that are only k-wise independent. <p> Then there is a fixed constant c &gt; 0 such that Pr (jX (k) j ) c k + k 2 ! k=2 for any &gt; 0. 2.3 Derandomization via bounded independence We are now ready to review the bounded independence technique for derandomizing a probabilistic algorithm <ref> [5, 41, 44, 45] </ref>. We use the parallel formulation of Luby [44], which is based upon a combinatorial construction of Joffe [38] (see also Karloff and Mansour [41]). <p> We use the parallel formulation of Luby <ref> [44] </ref>, which is based upon a combinatorial construction of Joffe [38] (see also Karloff and Mansour [41]). In this formulation, we assume we have a parallel probabilistic algorithm, Random, which is designed so that all the randomization is contained in a single choice step. <p> Of course, such a prime number can easily be found in O (1) time in the CRCW PRAM model using a polynomial number of processors. 4 In our usage each X i will take a value from f0; 1g. 5 Luby <ref> [44] </ref> shows that if Random satisfies all of these conditions, then one may construct a space of q k points so that each point corresponds to an assignment of values to X 1 ; X 2 ; : : : ; X n .
Reference: [45] <author> M. Luby. </author> <title> Removing randomness in parallel computation without a processor penalty. </title> <booktitle> In Proc. 29th IEEE Symp. on Found. Comp. Sci., </booktitle> <pages> 162-173, </pages> <year> 1988. </year>
Reference-contexts: We perform this derandomization using the bounded independence derandomization technique <ref> [5, 41, 44, 45, 64] </ref>, which assumes our algorithm uses random variables that are only k-wise independent. <p> Then there is a fixed constant c &gt; 0 such that Pr (jX (k) j ) c k + k 2 ! k=2 for any &gt; 0. 2.3 Derandomization via bounded independence We are now ready to review the bounded independence technique for derandomizing a probabilistic algorithm <ref> [5, 41, 44, 45] </ref>. We use the parallel formulation of Luby [44], which is based upon a combinatorial construction of Joffe [38] (see also Karloff and Mansour [41]).
Reference: [46] <author> Y. Matias and U. Vishkin. </author> <title> Converting high probability into nearly-constant time|with applications to parallel hashing. </title> <booktitle> In 23rd ACM Symp. on Theory of Computing, </booktitle> <pages> 307-316, </pages> <year> 1991. </year>
Reference-contexts: The work bound we pass to this recursive call is w, unless this level in the recursion is equal to ci+1, for some integer i 1, in which case we pass the work bound w=2 1=c . (To implement this step in the CRCW PRAM model we use -approximate compaction <ref> [31, 34, 46] </ref>, where one is given an array A with m of its locations "occupied" and one wishes to map these m distinguished elements to an array B of size (1 + )m. The time bound is O (log log n) [31] using linear work.
Reference: [47] <author> J. Matousek. </author> <title> Approximations and optimal geometric divide-and-conquer. </title> <booktitle> In Proc. 23rd Annu. ACM Sympos. Theory Comput., </booktitle> <pages> 505-511, </pages> <year> 1991. </year> <note> Also in J. </note> <institution> Comput. Syst. Sci. </institution> <month> 50, 203-208 </month> <year> (1995). </year>
Reference-contexts: There are several recent results that show that one can construct a (1=r)-approximation of size O (r 2 log r) for any range space with VC-exponent bounded by e in time O (nr c ) for some constant c depending on e (e.g., see <ref> [14, 16, 48, 47, 52, 51] </ref>). <p> Proof: The structure of the proof is to apply the previous corollary to recursively refine our approximations to be of a size depending only on r, not n. The main idea of this approach is to take advantage of an observation of Matousek <ref> [47] </ref> on an additive property of *-appromations, which states that an *-approximation of a ffi-approximation of a set X is itself an (* + ffi)-approximation of X. <p> Their work complexities are quite high, however. In this section we show how to reduce this significantly. Let (X; R) be a range space with VC-exponent bounded by e. We need another simple lemma, which is an adaptation of an observation made by Matousek <ref> [47] </ref>.
Reference: [48] <author> J. Matousek. </author> <title> Cutting hyperplane arrangements. </title> <journal> Discrete Comput. Geom., </journal> <volume> 6 </volume> <pages> 385-406, </pages> <year> 1991. </year>
Reference-contexts: There are several recent results that show that one can construct a (1=r)-approximation of size O (r 2 log r) for any range space with VC-exponent bounded by e in time O (nr c ) for some constant c depending on e (e.g., see <ref> [14, 16, 48, 47, 52, 51] </ref>). <p> We achieve this by designing an algorithm, Approx, which almost achieves this goal, in that it has a good work bound, but doesn't quite achieve the size bound (the Approx procedure is a modification of earlier simple divide-and-conquer method of Matousek <ref> [48] </ref>). We can then follow this by a call to Theorem 3.13 to improve the size bound, while keeping the work bound at O (nr O (1) ).
Reference: [49] <author> J. Matousek. </author> <title> Epsilon-nets and computational geometry. </title> <editor> In J. Pach, editor, </editor> <booktitle> New Trends in Discrete and Computational Geometry, volume 10 of Algorithms and Combinatorics, </booktitle> <pages> 69-89. </pages> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: Interestingly, most of the combinatorial properties needed by geometric random samples can be characterized by two notions|the *-approximation <ref> [49, 68] </ref> fl This research was announced in preliminary form in Proc. 9th ACM Symp. on Computational Geometry (SCG), 1993, 73-82, and in Proc. 7th ACM-SIAM Symposium on Discrete Algorithms (SODA), 1996, 132-141. y This research is supported by the National Science Foundation under Grants IRI-9116843, CCR-9300079, and CCR-9625289, and by <p> DIMACS is an NSF Science and Technology Center, funded under contract STC-91-19999; and also receives support from the New Jersey Commission on Science and Technology. Author's current address: Max-Planck-Institut fur Informatik, Im Stadtwald, 66123 Saarbrucken, Germany. E-mail: ramos@mpi-sb.mpg.de 1 and the *-net <ref> [36, 49] </ref>. These concepts are defined for very general frameworks, where one is given a set system (X; R) consisting of a finite ground set, X, and a set, R, of subsets of X. <p> A subset Y is an *-approximation for (X; R) if, for each range R 2 R, fi fi jY " Rj jXj fi fi *: Relaxing this requirement a bit, Y is said to be an *-net <ref> [36, 49] </ref> of (X; R) if Y " R 6= ; for each R 2 R such that jRj &gt; *jXj. This is clearly a weaker notion than that of an *-approximation, for any *-approximation is automatically an *-net, but the converse need not be true. <p> One general type of geometric structure that has motivated much of the derandomization research, and one that motivated the development of the *-approximation and *-net notions for computational geometry, is the geometric partition (e.g., see <ref> [2, 49] </ref>). In this problem, one is given a collection X of n hyperplanes in IR d , and a parameter r, and one wishes to construct a partition of IR d into O (r d ) constant-sized cells so that each cell intersects as few hyperplanes as possible. <p> set fA"R : R 2 1 Bronnimann et al. [14] call a subset A X a sensitive *-approximation if jjA"Rj=jAjjRj=jXjj (*=2)(*+ p 2 Rg, the VC-dimension of (X; R) is defined as the maximum size of a subset A of X such that Rj A = 2 A (e.g., see <ref> [49] </ref>). A related and simpler notion, however, is based upon the shatter function, R (m) = fjRj A j: A X; jAj = mg: In particular, we say that (X; R) has VC-exponent [8, 13] bounded by e if R (m) is O (m e ).
Reference: [50] <author> J. Matousek, M. Sharir, and E. Welzl. </author> <title> A subexponential bound for linear programming. </title> <booktitle> In Proc. 8th Annu. ACM Sympos. Comput. Geom., </booktitle> <pages> 1-8, </pages> <year> 1992. </year> <note> Also in Algorithmica16, </note> <month> 498-516 </month> <year> (1996). </year>
Reference-contexts: Alternatively, in the weaker EREW PRAM model processors may not concurrently access the same memory location. 3 see <ref> [16, 19, 21, 28, 39, 50, 65] </ref>) or on building data structures for linear programming queries (e.g., see [30, 53]).
Reference: [51] <author> J. Matousek, E. Welzl, and L. Wernisch. </author> <title> Discrepancy and "-approximations for bounded VC-dimension. </title> <journal> Combinatorica, </journal> <volume> 13 </volume> <pages> 455-466, </pages> <year> 1993. </year>
Reference-contexts: There are several recent results that show that one can construct a (1=r)-approximation of size O (r 2 log r) for any range space with VC-exponent bounded by e in time O (nr c ) for some constant c depending on e (e.g., see <ref> [14, 16, 48, 47, 52, 51] </ref>).
Reference: [52] <author> J. Matousek. </author> <title> Efficient partition trees. </title> <journal> Discrete Comput. Geom., </journal> <volume> 8 </volume> <pages> 315-334, </pages> <year> 1992. </year> <month> 21 </month>
Reference-contexts: There are several recent results that show that one can construct a (1=r)-approximation of size O (r 2 log r) for any range space with VC-exponent bounded by e in time O (nr c ) for some constant c depending on e (e.g., see <ref> [14, 16, 48, 47, 52, 51] </ref>).
Reference: [53] <author> J. Matousek. </author> <title> Linear optimization queries. </title> <journal> J. Algorithms, </journal> <volume> 14 </volume> <pages> 432-448, </pages> <year> 1993. </year> <title> The results combined with results of O. </title> <note> Schwarzkopf also appear in Proc. 8th ACM Sympos. Comput. Geom., </note> <year> 1992, </year> <pages> 16-25. </pages>
Reference-contexts: Alternatively, in the weaker EREW PRAM model processors may not concurrently access the same memory location. 3 see [16, 19, 21, 28, 39, 50, 65]) or on building data structures for linear programming queries (e.g., see <ref> [30, 53] </ref>). In the parallel domain, Alon and Megiddo [4] give analogous results, showing that through the use of randomization one can solve a fixed-dimensional linear program in O (1) time with very high probability using n processors in a randomized CRCW PRAM model.
Reference: [54] <author> N. Megiddo. </author> <title> Linear-time algorithms for linear programming in R 3 and related problems. </title> <journal> SIAM J. Comput., </journal> <volume> 12 </volume> <pages> 759-776, </pages> <year> 1983. </year>
Reference-contexts: Of particular interest is the case when the dimensionality, d (corresponding to the number of variables), is fixed, as occurs, for example, in several applications of linear programming in geometric computing (e.g., see <ref> [16, 21, 29, 54, 55, 60] </ref>) and machine learning (e.g., see [10, 11]). <p> Indeed, a major contribution of computational geometry research has been to show that fixed-dimensional linear programming can be solved in linear time, starting with the seminal work of Dyer [27] and Megiddo <ref> [54, 55] </ref>, and following with subsequent work in the sequential domain concentrated primarily on reducing the constant "hiding behind" the big-oh in these results (e.g., 2 Recall that the work done by a parallel algorithm is the total number of operations performed by all processors, and it is never more than
Reference: [55] <author> N. Megiddo. </author> <title> Linear programming in linear time when the dimension is fixed. </title> <journal> J. ACM, </journal> <volume> 31 </volume> <pages> 114-127, </pages> <year> 1984. </year>
Reference-contexts: Of particular interest is the case when the dimensionality, d (corresponding to the number of variables), is fixed, as occurs, for example, in several applications of linear programming in geometric computing (e.g., see <ref> [16, 21, 29, 54, 55, 60] </ref>) and machine learning (e.g., see [10, 11]). <p> Indeed, a major contribution of computational geometry research has been to show that fixed-dimensional linear programming can be solved in linear time, starting with the seminal work of Dyer [27] and Megiddo <ref> [54, 55] </ref>, and following with subsequent work in the sequential domain concentrated primarily on reducing the constant "hiding behind" the big-oh in these results (e.g., 2 Recall that the work done by a parallel algorithm is the total number of operations performed by all processors, and it is never more than
Reference: [56] <author> R. Motwani, J. Naor, and M. Naor. </author> <title> The probabilistic method yields deterministic parallel algorithms. </title> <booktitle> In Proc. 30th Annu. IEEE Sympos. </booktitle> <institution> Found. Comput. Sci., </institution> <month> 8-13, </month> <year> 1989. </year> <note> Also in J. </note> <institution> Comput. Syst. Sci. </institution> <month> 49, 478-516 </month> <year> (1994). </year>
Reference-contexts: Chazelle and Friedman [15] show that one can in fact construct such a partitioning with * = 1=r deterministically in polynomial time, and Berger, Rompel, and Shor [12] and Motwani, Naor, and Naor <ref> [56] </ref> show that one can construct similar geometric partitions for * = log r=r in N C. (Recall that NC denotes the class of problems solvable in polylogarithmic time using a polynomial number of processors [37, 43].) Unfortunately, the running time of Chazelle and Friedman's algorithm is quite high, as are
Reference: [57] <author> R. Motwani and P. Raghavan. </author> <title> Randomized Algorithms. </title> <publisher> Cambridge University Press, </publisher> <address> New York, </address> <year> 1995. </year>
Reference-contexts: 1 Introduction The study of randomized algorithms and methods for reducing the amount of perfect randomness needed for geometric algorithms has proven to be a very rich area of research (e.g., see <ref> [1, 2, 4, 5, 14, 15, 22, 42, 58, 57] </ref>). <p> We perform this derandomization using the bounded independence derandomization technique [5, 41, 44, 45, 64], which assumes our algorithm uses random variables that are only k-wise independent. Thus, before we give our methods, let us review these concepts (see also <ref> [5, 57] </ref>). 2.1 Random sampling Since the probabilistic algorithm we wish to derandomize is based upon random sampling, let us begin by saying a few words about this technique. <p> Note that in this notation X (n) = X; hence, we may omit the superscript if the underlying random variables are mutually independent. Unfortunately, restricting our attention to k-wise independent indicator random variables prevents us from directly using the well-known and powerful Chernoff bounds <ref> [5, 17, 35, 57] </ref> for bounding the tail of the distribution of their sum.
Reference: [58] <author> K. Mulmuley. </author> <title> Computational Geometry: An Introduction Through Randomized Algorithms. </title> <publisher> Pren-tice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1993. </year>
Reference-contexts: 1 Introduction The study of randomized algorithms and methods for reducing the amount of perfect randomness needed for geometric algorithms has proven to be a very rich area of research (e.g., see <ref> [1, 2, 4, 5, 14, 15, 22, 42, 58, 57] </ref>).
Reference: [59] <author> C. H. Papadimitriou and K. Steiglitz. </author> <title> Combinatorial Optimization: Algorithms and Complexity. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1982. </year>
Reference-contexts: It has been applied to a host of combinatorial optimization problems since the first efficient algorithms for solving it were developed in the 1940's (e.g., see <ref> [18, 23, 40, 59] </ref>). Geometrically, it can be viewed as the problem of locating a point that is maximal in a given ~v direction in the polyhedral region P defined by the intersection of n halfspaces in IR d .
Reference: [60] <author> F. P. Preparata and M. I. Shamos. </author> <title> Computational Geometry: An Introduction. </title> <publisher> Springer-Verlag, </publisher> <address> New York, NY, </address> <year> 1985. </year>
Reference-contexts: Of particular interest is the case when the dimensionality, d (corresponding to the number of variables), is fixed, as occurs, for example, in several applications of linear programming in geometric computing (e.g., see <ref> [16, 21, 29, 54, 55, 60] </ref>) and machine learning (e.g., see [10, 11]).
Reference: [61] <author> J. H. Reif. </author> <title> Synthesis of Parallel Algorithms. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: the speed of the algorithm is the same as that used in Random (plus an additional term for performing an "or" on all the results in parallel, which can be done in O (1) time in the CRCW PRAM model and O (log n) time in the EREW PRAM model <ref> [37, 43, 61] </ref>). Having reviewed the necessary probabilistic preliminaries, let us now turn to the problem of constructing (1=r)-approximations and (1=r)-nets. 3 O ((nr) )-Work Approximation Finding Before we describe our work-efficient method, however, we first describe some algorithms for constructing (1=r)-nets and (1=r)-approximations that are fast but not work-efficient. <p> Both implementations are simple applications of parallel minimum finding <ref> [37, 43, 61] </ref> and are left to the reader.) 4. Using ParLP d1 as a subroutine, determine the simplex in T that contains p. <p> The time bound is O (log log n) [31] using linear work. Of course, in the EREW PRAM model this step can easily be implemented in O (log n) time via a parallel prefix computation <ref> [37, 43, 61] </ref>.) Since this method always recurses in a region guaranteed to contain the optimal point and we include in the subproblem all halfspaces whose boundary intersects , we will eventually find the optimal point p.
Reference: [62] <author> J. T. Rompel. </author> <title> Techniques for Computing with Low-Independence Randomness. </title> <type> Ph.D. thesis, </type> <institution> Dept. of EECS, M.I.T., </institution> <year> 1990. </year>
Reference-contexts: Unfortunately, restricting our attention to k-wise independent indicator random variables prevents us from directly using the well-known and powerful Chernoff bounds [5, 17, 35, 57] for bounding the tail of the distribution of their sum. Nevertheless, as shown by Rompel <ref> [62] </ref> (see also Schmidt, Siegel and Srinivasan [64]), we may derive something analogous: Lemma 2.1 [62]: Let X (k) be the sum of n k-wise independent random variables taking on values in the range [0; 1], with = E (X (k) ), where k is a positive even integer. <p> Nevertheless, as shown by Rompel <ref> [62] </ref> (see also Schmidt, Siegel and Srinivasan [64]), we may derive something analogous: Lemma 2.1 [62]: Let X (k) be the sum of n k-wise independent random variables taking on values in the range [0; 1], with = E (X (k) ), where k is a positive even integer.
Reference: [63] <author> N. Sauer. </author> <title> On the density of families of sets. </title> <journal> Journal of Combinatorial Theory, </journal> <volume> 13 </volume> <pages> 145-147, </pages> <year> 1972. </year>
Reference-contexts: Interestingly, the VC-exponent definition subsumes that of the VC-dimension, for if (X; R) has VC-dimension e, then it has VC-exponent bounded by e as well <ref> [63, 68] </ref>.
Reference: [64] <author> J. P. Schmidt, A. Siegel, and A. Srinivasan. </author> <title> Chernoff-Hoeffding bounds for applications with limited independence. </title> <booktitle> In Proc. 4th ACM-SIAM Symp. on Discrete Algorithms, </booktitle> <pages> 331-340, </pages> <year> 1993. </year>
Reference-contexts: We perform this derandomization using the bounded independence derandomization technique <ref> [5, 41, 44, 45, 64] </ref>, which assumes our algorithm uses random variables that are only k-wise independent. <p> Unfortunately, restricting our attention to k-wise independent indicator random variables prevents us from directly using the well-known and powerful Chernoff bounds [5, 17, 35, 57] for bounding the tail of the distribution of their sum. Nevertheless, as shown by Rompel [62] (see also Schmidt, Siegel and Srinivasan <ref> [64] </ref>), we may derive something analogous: Lemma 2.1 [62]: Let X (k) be the sum of n k-wise independent random variables taking on values in the range [0; 1], with = E (X (k) ), where k is a positive even integer.
Reference: [65] <author> R. Seidel. </author> <title> Small-dimensional linear programming and convex hulls made easy. </title> <journal> Discrete Comput. Geom., </journal> <volume> 6 </volume> <pages> 423-434, </pages> <year> 1991. </year>
Reference-contexts: Alternatively, in the weaker EREW PRAM model processors may not concurrently access the same memory location. 3 see <ref> [16, 19, 21, 28, 39, 50, 65] </ref>) or on building data structures for linear programming queries (e.g., see [30, 53]). <p> Let us also assume that the origin, o, is contained in P , the polytope defined by the linear constraints. These assumptions can be removed with minor modifications to our method (similar to those used, for example, by Seidel <ref> [65] </ref>). Without loss of generality, we may additionally assume that ~v = (0; 0; : : :; 0; 1), i.e., we are interested in the "lowest" vertex in P .
Reference: [66] <author> S. Sen. </author> <title> A deterministic poly(log log n) time optimal CRCW PRAM algorithm for linear programming in fixed dimension. </title> <type> Technical Report 95-08, </type> <institution> Dept. of Computer Science, University of Newcastle, </institution> <year> 1995. </year>
Reference-contexts: Recently, Dyer [26] has given an O (log n (log log n) d1 ) time method that uses O (n log n (log log n) d1 ) work in the EREW PRAM model. In addition, we have recently learned that Sen <ref> [66] </ref> has independently discovered a CRCW PRAM method that runs in O ((log log n) d+1 ) time using O (n) work. 1.4 Our results for parallel linear programming In this paper we give a deterministic parallel method for fixed dimensional linear programming that runs in O ((log log n) d
Reference: [67] <author> L. Valiant. </author> <title> Parallelism in comparison problems. </title> <journal> SIAM J. Comput., </journal> <volume> 4(3) </volume> <pages> 348-355, </pages> <year> 1975. </year>
Reference-contexts: Ajtai and Megiddo [3] give a deterministic O ((log log n) d ) time method, but it has a sub-optimal fi (n (log log n) d ) work bound and it is defined for the very powerful parallel model that only counts "comparison" steps <ref> [67] </ref>. The only work-optimal deterministic PRAM result we are familiar with is a method by Deng [24] for 2-dimensional linear programming that runs in O (log n) time using O (n) work on a CRCW PRAM.
Reference: [68] <author> V. N. Vapnik and A. Y. Chervonenkis. </author> <title> On the uniform convergence of relative frequencies of events to their probabilities. </title> <journal> Theory Probab. Appl., </journal> <volume> 16 </volume> <pages> 264-280, </pages> <year> 1971. </year> <month> 22 </month>
Reference-contexts: Interestingly, most of the combinatorial properties needed by geometric random samples can be characterized by two notions|the *-approximation <ref> [49, 68] </ref> fl This research was announced in preliminary form in Proc. 9th ACM Symp. on Computational Geometry (SCG), 1993, 73-82, and in Proc. 7th ACM-SIAM Symposium on Discrete Algorithms (SODA), 1996, 132-141. y This research is supported by the National Science Foundation under Grants IRI-9116843, CCR-9300079, and CCR-9625289, and by <p> A general framework for geometric partitioning emerges from the framework when a range space (X; R) has constant Vapnik-Chervonenkis <ref> [68] </ref> (VC)-dimension. <p> Interestingly, the VC-exponent definition subsumes that of the VC-dimension, for if (X; R) has VC-dimension e, then it has VC-exponent bounded by e as well <ref> [63, 68] </ref>.
References-found: 68

