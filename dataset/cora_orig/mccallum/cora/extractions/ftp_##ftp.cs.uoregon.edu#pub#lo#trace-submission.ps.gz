URL: ftp://ftp.cs.uoregon.edu/pub/lo/trace-submission.ps.gz
Refering-URL: http://www.cs.uoregon.edu/research/DistributedComputing/archive.html
Root-URL: http://www.cs.uoregon.edu
Email: kurtw, lo@cs.uoregon.edu  feit@cs.huji.ac.il  nitzberg@nas.nasa.gov  moore@sdsc.edu  
Title: A Comparison of Workload Traces from Two Production Parallel Machines  
Author: Kurt Windisch, Virginia Lo Dror Feitelson Bill Nitzberg Reagan Moore 
Affiliation: University of Oregon  Hebrew University  NASA Ames Research Center  San Diego Supercomputer Center  
Abstract: The analysis of workload traces from real production parallel machines can aid a wide variety of parallel processing research, providing a realistic basis for experimentation in the management of resources over an entire workload. We analyze a five-month workload trace of an Intel Paragon machine supporting a production parallel workload at the San Diego Supercomputer Center (SDSC), comparing and contrasting it with a similar workload study of an Intel iPSC/860 machine at NASA Ames NAS. Our analysis of workload characteristics takes into account the job scheduling policies of the sites and focuses on characteristics such as job size distribution (job parallelism), resource usage, runtimes, submission patterns, and wait times. Despite fundamental differences in the two machines and their respective usage environments, we observe a number of interesting similarities with respect to job size distribution, system utilization, run time distribution, and interarrival time distribution. We hope to gain insight into the potential use of workload traces for evaluating resource management polices at supercomputing sites and for providing both real-world job streams and accurate stochastic workload models for use in simulation analysis of resource management policies. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Intel Corp. </author> <title> Paragon Network Queuing System manual. </title> <month> October </month> <year> 1993. </year>
Reference-contexts: Interactively scheduled jobs are referred to as direct jobs. In addition, the administration of both machines allows for specially arranged and scheduled dedicated use by a single user. The other physical partition of these machines, the batch partition, is governed by the Network 3 Queuing System (NQS) <ref> [1] </ref>. The NQS batch partitioning system supports heterogeneous scheduling queues based on any of several characteristics: maximum runtime, maximum job size, and node type (16M regular nodes or 32M "fat" nodes in the SDSC machine).
Reference: [2] <author> D. Feitelson. </author> <title> Packing schemes for gang scheduling. </title> <booktitle> In Proceedings of the 2nd Workshop on Job Scheduling Strategies for Parallel Processing, IPPS '96, </booktitle> <year> 1996. </year>
Reference-contexts: In <ref> [2] </ref> Feitelson was able to create an accurate workload model based on observed workload charac teristics from three separate supercomputer sites. As a first step in this direction, we compare the characteristics of workload traces from two distinctly different machines. <p> The irregularity of Figures 2 and 3 illustrate that modeling workload job distributions with probabilistic functions is a difficult problem for which simple distribution functions will not suffice. However, using a harmonic distribution for job sizes and then manually emphasizing certain "interesting sizes", Feitelson <ref> [2] </ref> was able to create a surprisingly accurate workload model based, in part, on this SDSC trace. In the SDSC trace, only 6,719 jobs (20.2%) ran on a single node, while 26,564 jobs (79.8%) were parallel.
Reference: [3] <author> D. G. Feitelson and B. Nitzberg. </author> <title> Job characteristics of a production parallel scientific workload on the NASA Ames iPSC/860. </title> <booktitle> In Proceedings of the 1st. Workshop on Job Scheduling Strategies for Parallel Processing, IPPS '95, </booktitle> <month> April </month> <year> 1995. </year>
Reference-contexts: The first is an Intel iPSC/860 at NASA Ames Numerical Aerodynamic Simulation lab (NAS). The NAS machine is a 128-node hypercube. The three-month workload trace, taken from October 1 to December 31, 1993, was analyzed in detail by Feitelson and Nitzberg <ref> [3] </ref> and we use this analysis as a basis for comparison. The second machine in our comparison is an Intel Paragon at the San Diego Supercomputer Center (SDSC), containing 416 mesh-connected nodes. The mesh network, 2 however, has the dimensions 16 fi 28, with 32 mesh slots unfilled. <p> Yet, because of the fundamental differences between the two machines, we hope to gain insight from any commonalities between the two workloads. We have reproduced many of the observations and graphs from <ref> [3] </ref>; however, referring to the full NAS iPSC/860 technical report is helpful in understanding our comparison. This paper is organized similarly to the NAS iPSC/860 paper [3]. Section 2 contains background on the NAS and SDSC scheduling policies. <p> We have reproduced many of the observations and graphs from <ref> [3] </ref>; however, referring to the full NAS iPSC/860 technical report is helpful in understanding our comparison. This paper is organized similarly to the NAS iPSC/860 paper [3]. Section 2 contains background on the NAS and SDSC scheduling policies. Sections 3 through 8 describe job size distributions, resource usage, system utilization, multiprogramming levels, runtimes, and submission characteristics. <p> During NAS prime-time, the only NQS queues serviced were those for small jobs (up to 32 nodes, 1 hour or less) and 64 compute nodes were reserved for interactive use. During non-prime-time, all nodes were available for NQS use <ref> [3] </ref>. In the SDSC trace, prime-time was Monday - Friday, from 9am to 5pm. <p> As with the NAS workload, these peaks are exclusively the result of the NQS queue time limits (1, 4, and 12 hours). A large number of parallel jobs also ran for 3 seconds or less. When analyzing the distribution of runtimes in the NAS trace in <ref> [3] </ref> the authors conclude that the 17 18 runtimes for parallel jobs tend to increase as the parallelism of the jobs increases. Our study of the SDSC trace appears to support this claim, as seen in Table 4. <p> is much more regular than any of the other distributions characterizing the workloads, and could be modeled reasonably well by simpler probabilistic functions. 22 23 9 Conclusions We conducted much the same analysis on the 5 month SDSC Intel Paragon trace as was done for the NASA Ames NAS iPSC/860 <ref> [3] </ref>. Despite many fundamental differences in the two machines and the way in which they were used, there are a surprising number of similarities in their workloads. * The overall job size distributions had many similar characteristics.
Reference: [4] <author> S. Hotovy. </author> <title> Workload evolution on the Cornell Theory Center IBM SP2. </title> <booktitle> In Proceedings of the 2nd Workshop on Job Scheduling Strategies for Parallel Processing, IPPS '96, </booktitle> <year> 1996. </year> <month> 25 </month>
Reference-contexts: The study of workload traces has the potential to: * help administrators of parallel supercomputing sites evaluate and refine the resource management policies and algorithms for these machines. This was done by Hotovy in <ref> [4] </ref>, in which job scheduling policies for an IBM SP-2 at the Cornell Theory Center were modified in response to observed workload trends. * provide real-world job streams for direct use in more realistic simulation analysis of scheduling and allocation algorithms than is possible with purely stochastic workload models. <p> This suggests a changing, maturing workload similar to that reported for the Cornell SP-2 trace <ref> [4] </ref>. 8 All Direct NQS Day Night Weekend Total 33283 22094 11189 11111 13596 8576 Power-of-two 28276 17547 10729 9326 11626 7324 (85.0%) (79.4%) (95.9%) (83.9%) (85.5%) (85.4%) Non-power-of-two 5007 4547 460 1785 1970 1252 (15.0%) (20.6%) (4.1%) (16.1%) (14.5%) (14.6%) Table 3: The number of jobs in SDSC with power-of-two
Reference: [5] <author> K. Li and K. Cheng. </author> <title> A two-dimensional buddy system for dynamic resource allocation in a parti--tionable mesh connected system. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 12 </volume> <pages> 79-83, </pages> <year> 1991. </year>
Reference: [6] <author> R. Moore and M. Wan. </author> <title> Intel Paragon allocation algorithms. </title> <type> Personal communication, </type> <year> 1995. </year>
Reference-contexts: There are also two special low priority queues in the SDSC machine, standby and fstandby which have a maximum time limit of 12 hours <ref> [6] </ref>. Tables 1 and 2 show the queues available on each machine. In both systems, scheduling policies change for non-prime-time periods to better serve the batch-oriented, night-time workloads. In the NAS trace, prime-time scheduling was in effect Monday - Friday, from 6am to 8pm.
Reference: [7] <author> T. Suzuoka, J. Subhlok, and T. Gross. </author> <title> Evaluating job scheduling techniques for highly parallel computers. </title> <type> Technical Report CMU-CS-95-149, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <year> 1995. </year>
Reference-contexts: This was done by Suzuoka, Subholok, and Gross <ref> [7] </ref>, who compared the performance of several scheduling mechanisms executing a set of jobs extracted from a workload trace of an Intel Paragon at ETH Zurich. * provide a basis for the development and validation of more realistic stochastic workload models for use in simulation analysis of scheduling and allocation algorithms.
Reference: [8] <author> M. Wan, R. Moore, G. Kremenek, and K. Steube. </author> <title> A batch scheduler for the Intel Paragon MPP system with a non-contiguous node allocation algorithm. </title> <booktitle> In Proceedings of the 2nd Workshop on Job Scheduling Strategies for Parallel Processing, IPPS '96, </booktitle> <year> 1996. </year>
Reference-contexts: The number of nodes reserved for interactive or batch use remains constant between SDSC prime-time and non-prime-time, however, the set of nodes assigned to each NQS queue is reconfigured for non-prime time (and these sets are not required to be disjoint) <ref> [8] </ref>. At the time of this study, 416 nodes in the SDSC Paragon were partitioned statically: 48 nodes were dedicated to the interactive partition, 352 nodes to the NQS compute partition, 6 nodes to the service partition, and 10 nodes to the I/O partition.

References-found: 8

