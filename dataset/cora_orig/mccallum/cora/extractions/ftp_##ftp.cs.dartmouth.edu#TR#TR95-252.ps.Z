URL: ftp://ftp.cs.dartmouth.edu/TR/TR95-252.ps.Z
Refering-URL: http://www.cs.dartmouth.edu/reports/abstracts/TR95-252/
Root-URL: http://www.cs.dartmouth.edu
Email: E-mail: robert.s.gray@dartmouth.edu  
Title: Content-based Image Retrieval: Color and Edges  
Author: Robert S. Gray 
Address: Hanover, NH 03755  
Affiliation: Department of Computer Science Dartmouth College  
Abstract: One of the tools that will be essential for future electronic publishing is a powerful image retrieval system. The author should be able to search an image database for images that convey the desired information or mood; a reader should be able to search a corpus of published work for images that are relevant to his or her needs. Most commercial image retrieval systems associate keywords or text with each image and require the user to enter a keyword or textual description of the desired image. This text-based approach has numerous drawbacks associating keywords or text with each image is a tedious task; some image features may not be mentioned in the textual description; some features are "nearly impossible to describe with text"; and some features can be described in widely different ways [Na93a]. In an effort to overcome these problems and improve retrieval performance, researchers have focused more and more on content-based image retrieval in which retrieval is accomplished by comparing image features directly rather than textual descriptions of the image features. Features that are commonly used in content-based retrieval include color, shape, texture and edges. In this paper we describe a simple content-based system that retrieves color images on the basis of their color distributions and edge characteristics. The system uses two retrieval techniques that have been described in the literature - i.e. histogram intersection to compare color distributions and sketch comparison to compare edge characteristics. The performance of the system is evaluated and various extensions to the existing techniques are proposed.
Abstract-found: 1
Intro-found: 1
Reference: [CLP94] <author> Tat-Seng Chua, Swee-Kiew Lim, and Hung-Keng Pung. </author> <title> Content-based retrieval of images. </title> <booktitle> In Multimedia 94, </booktitle> <pages> pages 211-218, </pages> <address> San Francisco, Calfiornia, 1994. </address> <publisher> ACM. </publisher>
Reference-contexts: These values were used in [HK92]. Modified values did not produce significant improvements in retrieval performance or edge map quality. 2.2 Color extraction The color extraction module divides each image into non-overlapping subareas as in <ref> [CLP94] </ref> and then constructs a three-axis histogram for the overall image and for each subarea. The module can produce either RGB or CIE-LUV histograms and provides parameters to control the number of histogram buckets along each color axis as well as the number and position of the subareas. <p> This is computationally intractable for large databases or complicated queries. To handle the problem of color localization and the problem of specifying on a color by color basis whether location matters many researchers use segments rather than subareas or a combination of segments and subareas <ref> [CLP94, Na93b, GZCS94] </ref>. Each image is divided into segments such that each segment contains approximately a single uniform color or a single object. The color distribution of each segment is represented as a weighted centroid or as a histogram. <p> Histogram intersection needs to be modified to allow inexact color match. Eventually we will need to incorporate a hierarchical or cluster-based scheme for the sake of search efficiency. In addition we would like to evaluate other color retrieval schemes that have been presented in the literature. For example <ref> [CLP94] </ref> uses the color pair technique and has achieved good results in a small database; QBIC uses a matrix-based technique that takes the product of a difference histogram and a set of perceptual color distances, but unfortunately there is no analysis of retrieval performance [Na93b]; [GZCS94] reduces an entire histogram to
Reference: [FVDFH91] <author> James D. Foley, Andries Van Dam, Steven K. Feiner, and John F. Hughes. </author> <title> Computer Graphics: </title> <booktitle> Principles and Practice. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <note> second edition, </note> <year> 1991. </year>
Reference-contexts: The CIE-LUV color space has the advantage that the distance between two points in the space is approximately proportionally to the perceptual distance between the two corresponding colors (as expressed by human viewers) <ref> [FVDFH91] </ref>. The conversion from the RGB color space to the CIE-LUV color space is straightforward except for the necessity of choosing a prototypical red, green and blue. Details of the conversion can be found in [FVDFH91]. <p> proportionally to the perceptual distance between the two corresponding colors (as expressed by human viewers) <ref> [FVDFH91] </ref>. The conversion from the RGB color space to the CIE-LUV color space is straightforward except for the necessity of choosing a prototypical red, green and blue. Details of the conversion can be found in [FVDFH91]. Our revised edge detection algorithm first converts the RGB image to a CIE-LUV image and then uses the detection algorithm as described above except that the gradients are no longer scaled by the local intensity power.
Reference: [GZCS94] <author> Yihong Gong, Hongjiang Zhang, H. C. Chuan, and M. Sakauchi. </author> <title> An image database system with content capturing and fast image indexing abilities. </title> <booktitle> In Proceedings of the International Conference on Multimedia Computing and Systems, </booktitle> <pages> pages 121-130, </pages> <address> Boston, Massachusetts, 1994. </address> <publisher> IEEE. </publisher>
Reference-contexts: This is computationally intractable for large databases or complicated queries. To handle the problem of color localization and the problem of specifying on a color by color basis whether location matters many researchers use segments rather than subareas or a combination of segments and subareas <ref> [CLP94, Na93b, GZCS94] </ref>. Each image is divided into segments such that each segment contains approximately a single uniform color or a single object. The color distribution of each segment is represented as a weighted centroid or as a histogram. <p> For example [CLP94] uses the color pair technique and has achieved good results in a small database; QBIC uses a matrix-based technique that takes the product of a difference histogram and a set of perceptual color distances, but unfortunately there is no analysis of retrieval performance [Na93b]; <ref> [GZCS94] </ref> reduces an entire histogram to a single integer key by first transforming the histogram into a hyper-polygon and then taking a weighted sum of the angles and edge lengths, but again there is no analysis of retrieval performance. The technique of [GZCS94] will be exceptionally useful if it provides reasonable <p> there is no analysis of retrieval performance [Na93b]; <ref> [GZCS94] </ref> reduces an entire histogram to a single integer key by first transforming the histogram into a hyper-polygon and then taking a weighted sum of the angles and edge lengths, but again there is no analysis of retrieval performance. The technique of [GZCS94] will be exceptionally useful if it provides reasonable retrieval performance since then the first few levels of a hierarchical or cluster-based retrieval scheme would involve comparison of integer pairs rather than histogram pairs. 4.2 Edges 4.2.1 Edge detection The edge detection algorithm does not construct good edge maps for every <p> Unfortunately the authors do not provide an analysis of retrieval performance. A second common feature is the shape of the objects in the images. In the QBIC project [Na93b] a combination of area, circularity, eccentricity, major axis orientation and moment invariants are used as shape features. In <ref> [GZCS94] </ref> only circularity and major axis orientation are used. In QBIC the user draws the desired shape. Then the system computes the features of the query shape and matches the query features against the features of each shape in the images. In [GZCS94] the user does not draw the shape but <p> In <ref> [GZCS94] </ref> only circularity and major axis orientation are used. In QBIC the user draws the desired shape. Then the system computes the features of the query shape and matches the query features against the features of each shape in the images. In [GZCS94] the user does not draw the shape but rather specifies the values of the two shape parameters directly. Unfortunately both systems require that the images be segmented along object boundaries in order to identify the shapes that a user is likely to use when retrieving images. <p> QBIC resorts to a manual approach in which a human manually outlines the desired shapes using an interactive "shrink-wrap" utility [Na93b]. In <ref> [GZCS94] </ref> the images are segmented automatically on the basis of color but a postprocessing step is required to recover from over-segmentation; the postprocessing step is not described but is probably manual. Both authors provide no analysis of retrieval performance.
Reference: [HK92] <author> Kyoji Hirata and Toshikazu Kato. </author> <title> Query by visual example. </title> <booktitle> In Advances in Database Technology EDBT 1992, Third International Conference on Extending Database Technology, </booktitle> <pages> pages 56-71, </pages> <address> Vienna, Austria, 1992. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: The system does not develop any novel retrieval techniques but instead uses existing techniques that have been described in the literature - i.e histogram intersection [SB91, Swa93] is used to compare color distributions and sketch comparison <ref> [HK92, KKOH92] </ref> is used to compare edge characteristics. It is hoped that the system will highlight potential avenues of research and serve as a testbed for future work. To this end, the performance of the system is evaluated and various extensions to the existing retrieval techniques are proposed. <p> The color and edge extraction modules construct a set of histograms and an edge map for each image. No manual intervention is required during the extraction process. The query processing module uses histogram intersection [SB91, Swa93] to compare histograms and sketch comparison <ref> [KKOH92, HK92] </ref> to compare edge maps. The user interface provides a graphical front end. The four modules are shown in figure 1 and described below. 2.1 Edge extraction The edge extraction module originally used the edge detection algorithm from [KKOH92, HK92] which identifies the edges in an RGB image that are <p> uses histogram intersection [SB91, Swa93] to compare histograms and sketch comparison <ref> [KKOH92, HK92] </ref> to compare edge maps. The user interface provides a graphical front end. The four modules are shown in figure 1 and described below. 2.1 Edge extraction The edge extraction module originally used the edge detection algorithm from [KKOH92, HK92] which identifies the edges in an RGB image that are clearly perceptive to a human viewer. First the RGB image is reduced to thumbnail size and median filtered. Then four gradients one for each major orientation - are calculated for each pixel in the thumbnail. <p> This scaling factor is a simple application of the Weber-Fechner law to the RGB color space. The Weber-Fechner law states that the "contrast sensitivity of the human eye is proportional to the log-scale of the intensity value" <ref> [HK92] </ref>). The overall gradient for each pixel is taken to be whichever of the four gradients has the maximum absolute value. These maximal gradients are used to identify the edge pixels. First the algorithm calculates the average and standard deviation of the gradient magnitudes over the entire thumbnail image. <p> The goal of this technique is to extract only those edges that are clearly perceptive within the image as a whole and within their local section of the image. The resulting edge map should be generally similar to a human's impression of the image <ref> [HK92] </ref>. The algorithm was applied to a test collection of forty-eight outdoor scenes that are sold as part of the Microsoft Scenes screen saver; one of the scenes is shown in figure 3. The results were generally poor as the algorithm missed many clearly perceptive edges. <p> The number of edge abstracts that fall into each category is shown at the right. 4 the global edge candidates. These values were used in <ref> [HK92] </ref>. Modified values did not produce significant improvements in retrieval performance or edge map quality. 2.2 Color extraction The color extraction module divides each image into non-overlapping subareas as in [CLP94] and then constructs a three-axis histogram for the overall image and for each subarea. <p> This result should hold when the technique is used to determine the similarity between two equal sized images. The system uses a slightly modified version of sketch comparison <ref> [HK92, KKOH92] </ref> to compute the edge similarity score. The edge extraction module constructs an edge map for each image as described above. The query processing module accepts an edge map as input and compares this map against each of the image maps. <p> It would be more reasonable to have an edge that followed each tree truck. Unfortunately the tree trunks are far less prominent than the leaf clusters on the basis of color alone since the trunks blend into the fog. <ref> [HK92] </ref> and [KKOH92] have cleaner edge maps due to the nature of their images. They use a collection of paintings primarily landscapes and portraits that tend to have far sharper color boundaries than our outdoor photographs. <p> These are the situations that were considered in <ref> [HK92] </ref> and [KKOH92] where sketch comparison was observed to provide reasonable performance. However it clearly does not provide reasonable performance in the case of arbitrary queries against arbitrary image databases. 4 Future work 4.1 Color 4.1.1 Weighting The color retrieval technique has several weaknesses that were discussed in the evaluation. <p> Some of these extraneous edges arise due to "color noise" - i.e. a small group of pixels whose color is sharply different than all their neighbors. More aggressive median filtering and a thinning procedure to strip out the shortest edges <ref> [KKOH92, HK92] </ref> will eliminate some of these extraneous edges. In addition we should experiment with the gradient threshold to see if a higher threshold will give better results - e.g. keep only those edge pixels whose gradient strength is greater than the average plus two standard deviations.
Reference: [KKOH92] <author> Toshikazu Kato, Takio Kurita, Nobuyaki Otsu, and Kyoji Hirata. </author> <title> A sketch retrieval method for full color image databases. </title> <booktitle> In International Conference on Pattern Recognition (ICPR), </booktitle> <pages> pages 530-533, </pages> <address> The Hague, The Netherlands, </address> <year> 1992. </year> <month> IAPR. </month>
Reference-contexts: The system does not develop any novel retrieval techniques but instead uses existing techniques that have been described in the literature - i.e histogram intersection [SB91, Swa93] is used to compare color distributions and sketch comparison <ref> [HK92, KKOH92] </ref> is used to compare edge characteristics. It is hoped that the system will highlight potential avenues of research and serve as a testbed for future work. To this end, the performance of the system is evaluated and various extensions to the existing retrieval techniques are proposed. <p> The color and edge extraction modules construct a set of histograms and an edge map for each image. No manual intervention is required during the extraction process. The query processing module uses histogram intersection [SB91, Swa93] to compare histograms and sketch comparison <ref> [KKOH92, HK92] </ref> to compare edge maps. The user interface provides a graphical front end. The four modules are shown in figure 1 and described below. 2.1 Edge extraction The edge extraction module originally used the edge detection algorithm from [KKOH92, HK92] which identifies the edges in an RGB image that are <p> uses histogram intersection [SB91, Swa93] to compare histograms and sketch comparison <ref> [KKOH92, HK92] </ref> to compare edge maps. The user interface provides a graphical front end. The four modules are shown in figure 1 and described below. 2.1 Edge extraction The edge extraction module originally used the edge detection algorithm from [KKOH92, HK92] which identifies the edges in an RGB image that are clearly perceptive to a human viewer. First the RGB image is reduced to thumbnail size and median filtered. Then four gradients one for each major orientation - are calculated for each pixel in the thumbnail. <p> This result should hold when the technique is used to determine the similarity between two equal sized images. The system uses a slightly modified version of sketch comparison <ref> [HK92, KKOH92] </ref> to compute the edge similarity score. The edge extraction module constructs an edge map for each image as described above. The query processing module accepts an edge map as input and compares this map against each of the image maps. <p> It would be more reasonable to have an edge that followed each tree truck. Unfortunately the tree trunks are far less prominent than the leaf clusters on the basis of color alone since the trunks blend into the fog. [HK92] and <ref> [KKOH92] </ref> have cleaner edge maps due to the nature of their images. They use a collection of paintings primarily landscapes and portraits that tend to have far sharper color boundaries than our outdoor photographs. The third problem is the most critical and has a drastic effect on retrieval performance. <p> These are the situations that were considered in [HK92] and <ref> [KKOH92] </ref> where sketch comparison was observed to provide reasonable performance. However it clearly does not provide reasonable performance in the case of arbitrary queries against arbitrary image databases. 4 Future work 4.1 Color 4.1.1 Weighting The color retrieval technique has several weaknesses that were discussed in the evaluation. <p> Some of these extraneous edges arise due to "color noise" - i.e. a small group of pixels whose color is sharply different than all their neighbors. More aggressive median filtering and a thinning procedure to strip out the shortest edges <ref> [KKOH92, HK92] </ref> will eliminate some of these extraneous edges. In addition we should experiment with the gradient threshold to see if a higher threshold will give better results - e.g. keep only those edge pixels whose gradient strength is greater than the average plus two standard deviations.
Reference: [Na93a] <author> Wayne Niblack and all. </author> <title> The QBIC project: Querying images by content using color, texture and shape. </title> <booktitle> SPIE, </booktitle> 1908 173-187, 1993. 
Reference-contexts: Standard text retrieval techniques are then used to identify the relevant images. Unfortunately this text-based approach to image retrieval has numerous drawbacks <ref> [Na93a] </ref>. Associating keywords or text with each image is a tedious and time-consuming task since it must be done manually or at best semi-automatically; image processing technology is not advanced enough to allow the automatic construction of textual image descriptions except in well-defined and tightly focused domains. <p> Some image features may not be mentioned in the textual description due to design decision or indexer error; these image features do not exist from the standpoint of the retrieval system and any query that mentions them will fail. Some features are "nearly impossible to describe with text" <ref> [Na93a] </ref>; for example many textures and shapes defy easy description. Finally different indexers or even the same indexer may describe the same feature with different terms or different features with the same term; these are the standard text retrieval problems of synonymy and polysemy. <p> The database is searched for images that contain blocks of the desired texture. The approach works well on a collection of synthetic images but has not yet been tested on real-world images [SC94]. 17 The QBIC project <ref> [Na93b, Na93a] </ref> allows texture-based retrieval although it uses different features.
Reference: [Na93b] <author> Wayne Niblack and all. </author> <title> The QBIC project: Querying images by content using color, texture and shape. </title> <type> Research Report RJ 9203 (81511), </type> <institution> IBM Research Divison, Almaden Research Center, </institution> <address> San Jose, California, </address> <year> 1993. </year>
Reference-contexts: This is computationally intractable for large databases or complicated queries. To handle the problem of color localization and the problem of specifying on a color by color basis whether location matters many researchers use segments rather than subareas or a combination of segments and subareas <ref> [CLP94, Na93b, GZCS94] </ref>. Each image is divided into segments such that each segment contains approximately a single uniform color or a single object. The color distribution of each segment is represented as a weighted centroid or as a histogram. <p> For example [CLP94] uses the color pair technique and has achieved good results in a small database; QBIC uses a matrix-based technique that takes the product of a difference histogram and a set of perceptual color distances, but unfortunately there is no analysis of retrieval performance <ref> [Na93b] </ref>; [GZCS94] reduces an entire histogram to a single integer key by first transforming the histogram into a hyper-polygon and then taking a weighted sum of the angles and edge lengths, but again there is no analysis of retrieval performance. <p> This concept is similar to the idea of "active snakes" which are often used in interactive outlining applications <ref> [Na93b] </ref>. A more drastic step would to be to make the edge maps edge-based as well. Then the problem of comparing a query to an image would become a problem of comparing the feature values corresponding to the various edges. <p> The database is searched for images that contain blocks of the desired texture. The approach works well on a collection of synthetic images but has not yet been tested on real-world images [SC94]. 17 The QBIC project <ref> [Na93b, Na93a] </ref> allows texture-based retrieval although it uses different features. <p> Coarse--ness measures the scale of the texture and is computed with moving windows of several sizes; contrast describes the "vividness" of the texture and is calculated from the gray-level histogram; and directionality measures whether the image has a "favored" direction and is computed from the gradient directions in the image <ref> [Na93b] </ref>. The author notes that many other texture features were either too expensive to compute or ill-suited to heterogeneous collections of images [Na93b]. The user queries the database by providing a swatch of the desired texture which is then matched against the images. <p> texture and is calculated from the gray-level histogram; and directionality measures whether the image has a "favored" direction and is computed from the gradient directions in the image <ref> [Na93b] </ref>. The author notes that many other texture features were either too expensive to compute or ill-suited to heterogeneous collections of images [Na93b]. The user queries the database by providing a swatch of the desired texture which is then matched against the images. Unfortunately the authors do not provide an analysis of retrieval performance. A second common feature is the shape of the objects in the images. In the QBIC project [Na93b] a <p> images <ref> [Na93b] </ref>. The user queries the database by providing a swatch of the desired texture which is then matched against the images. Unfortunately the authors do not provide an analysis of retrieval performance. A second common feature is the shape of the objects in the images. In the QBIC project [Na93b] a combination of area, circularity, eccentricity, major axis orientation and moment invariants are used as shape features. In [GZCS94] only circularity and major axis orientation are used. In QBIC the user draws the desired shape. <p> Unfortunately both systems require that the images be segmented along object boundaries in order to identify the shapes that a user is likely to use when retrieving images. QBIC resorts to a manual approach in which a human manually outlines the desired shapes using an interactive "shrink-wrap" utility <ref> [Na93b] </ref>. In [GZCS94] the images are segmented automatically on the basis of color but a postprocessing step is required to recover from over-segmentation; the postprocessing step is not described but is probably manual. Both authors provide no analysis of retrieval performance.
Reference: [SB91] <author> Michael J. Swain and Dana H. Ballard. </author> <title> Color indexing. </title> <journal> International Journal of Computer Vision, </journal> <volume> 7(1) </volume> <pages> 11-32, </pages> <year> 1991. </year>
Reference-contexts: The system does not develop any novel retrieval techniques but instead uses existing techniques that have been described in the literature - i.e histogram intersection <ref> [SB91, Swa93] </ref> is used to compare color distributions and sketch comparison [HK92, KKOH92] is used to compare edge characteristics. It is hoped that the system will highlight potential avenues of research and serve as a testbed for future work. <p> The color and edge extraction modules construct a set of histograms and an edge map for each image. No manual intervention is required during the extraction process. The query processing module uses histogram intersection <ref> [SB91, Swa93] </ref> to compare histograms and sketch comparison [KKOH92, HK92] to compare edge maps. The user interface provides a graphical front end. <p> Then it identifies those images that have similar histograms and edge maps. The module computes color and edge similarity scores for each image and takes a weighted average of the two scores to get an overall similarity score. The system uses histogram intersection <ref> [SB91, Swa93] </ref> to compute the color similarity score. For each image the color extraction module constructs one histogram for the overall image and one histogram for each subarea as described above. <p> Histogram intersection has been shown to be insensitive to "change in image resolution, histogram size, occlusion, depth and viewpoint" [Swa93] and has provided excellent performance when finding images that contain a given object <ref> [SB91] </ref>. This result should hold when the technique is used to determine the similarity between two equal sized images. The system uses a slightly modified version of sketch comparison [HK92, KKOH92] to compute the edge similarity score. <p> In this case there is a much more efficient version of histogram intersection called incremental histogram intersection that intersects the buckets in order of decreasing pixel count and stops 14 if it determines that the similarity between the histograms can not possibly be more than some threshold <ref> [SB91] </ref>. However efficiency of the segment comparison algorithm of which histogram intersection is just a part will be critical when we move to segment-based retrieval. Reasonable efficiency will require an excellent representation for segment features as well as a hierarchical or cluster-based retrieval scheme.
Reference: [SC94] <author> John R. Smith and Shih-Fu Chang. </author> <title> Quad-tree segmentation for texture-based image query. </title> <booktitle> In Multimedia 94, </booktitle> <pages> pages 279-286, </pages> <address> San Francisco, Calfiornia, 1994. </address> <publisher> ACM. </publisher>
Reference-contexts: One common feature that is used in content-based retrieval is texture. <ref> [SC94] </ref> uses quad-tree segmentation to divide an image into blocks of approximately uniform texture. Feature vectors for the textures are computed from mean and variance measures produced by a QMF wavelet decomposition. The user queries the image database by selecting a desired texture from a set of prototypical textures. <p> The user queries the image database by selecting a desired texture from a set of prototypical textures. The database is searched for images that contain blocks of the desired texture. The approach works well on a collection of synthetic images but has not yet been tested on real-world images <ref> [SC94] </ref>. 17 The QBIC project [Na93b, Na93a] allows texture-based retrieval although it uses different features.
Reference: [Swa93] <author> Michael J. Swain. </author> <title> Interactive indexing into image databases. </title> <booktitle> SPIE, </booktitle> 1908 95-103, 1993. <volume> 19 </volume>
Reference-contexts: The system does not develop any novel retrieval techniques but instead uses existing techniques that have been described in the literature - i.e histogram intersection <ref> [SB91, Swa93] </ref> is used to compare color distributions and sketch comparison [HK92, KKOH92] is used to compare edge characteristics. It is hoped that the system will highlight potential avenues of research and serve as a testbed for future work. <p> The color and edge extraction modules construct a set of histograms and an edge map for each image. No manual intervention is required during the extraction process. The query processing module uses histogram intersection <ref> [SB91, Swa93] </ref> to compare histograms and sketch comparison [KKOH92, HK92] to compare edge maps. The user interface provides a graphical front end. <p> Then it identifies those images that have similar histograms and edge maps. The module computes color and edge similarity scores for each image and takes a weighted average of the two scores to get an overall similarity score. The system uses histogram intersection <ref> [SB91, Swa93] </ref> to compute the color similarity score. For each image the color extraction module constructs one histogram for the overall image and one histogram for each subarea as described above. <p> Thus the purpose of the min in the similarity measure is to filter out the background pixels in each image, leaving only those pixels that might belong to the object. Histogram intersection has been shown to be insensitive to "change in image resolution, histogram size, occlusion, depth and viewpoint" <ref> [Swa93] </ref> and has provided excellent performance when finding images that contain a given object [SB91]. This result should hold when the technique is used to determine the similarity between two equal sized images.
References-found: 10

