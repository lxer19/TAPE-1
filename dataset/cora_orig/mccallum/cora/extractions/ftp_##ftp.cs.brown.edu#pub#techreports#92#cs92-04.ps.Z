URL: ftp://ftp.cs.brown.edu/pub/techreports/92/cs92-04.ps.Z
Refering-URL: http://www.cs.brown.edu/publications/techreports/reports/CS-92-04.html
Root-URL: http://www.cs.brown.edu/
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> A. Aggarwal and J. S. Vitter, </author> <title> "The Input/Output Complexity of Sorting and Related Problems," </title> <journal> Communications of the ACM (September 1988), </journal> <pages> 1116-1127, </pages> <booktitle> also appears in Proceedings of 14th Annual International Colloquium on Automata, Languages, and Programming (ICALP), Lecture Notes in Computer Science 267, </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1987. </year>
Reference-contexts: Efficient algorithms for multilevel hierarchical memory are considered in the companion paper [19]. In previous work, Aggarwal and Vitter <ref> [1] </ref> presented optimal upper and lower bounds on the I/O needed for sorting-related problems of size N using a two-level memory model where internal memory can store M records and the secondary memory size is limitless. <p> Their results generalized the groundbreaking work done by Floyd [4], who gave optimal bounds for sorting, realized by standard two-way merge sort, for the special case P = 1 and M = 2B = p N . The model in <ref> [1] </ref> is somewhat unrealistic, however, because secondary storage is usually partitioned into separate physical devices, each capable of transferring only one block per I/O. We are interested in optimal algorithms for realistic two-level storage systems that allow P simultaneous data transfers. <p> The restriction that only one block can be accessed per disk during an I/O is what distinguishes our model from the less realistic model of <ref> [1] </ref>. This distinction is akin to the difference in parallel computation between the MPC (module parallel computer) model and the less realistic PRAM model. However, general PRAM simulation techniques use logarithmic time per step; if they were applied to the algorithms in [1], the resulting algorithms would not be optimal in <p> our model from the less realistic model of <ref> [1] </ref>. This distinction is akin to the difference in parallel computation between the MPC (module parallel computer) model and the less realistic PRAM model. However, general PRAM simulation techniques use logarithmic time per step; if they were applied to the algorithms in [1], the resulting algorithms would not be optimal in terms of I/O. The algorithms we develop on our model use the same number of I/Os as those in [1] for the less realistic model. 5 B records D 1 Disks Tracks 3 Problem Definitions The problems we consider in this paper <p> However, general PRAM simulation techniques use logarithmic time per step; if they were applied to the algorithms in <ref> [1] </ref>, the resulting algorithms would not be optimal in terms of I/O. The algorithms we develop on our model use the same number of I/Os as those in [1] for the less realistic model. 5 B records D 1 Disks Tracks 3 Problem Definitions The problems we consider in this paper have been well described in the literature. Most of the following definitions are those from [1], with suitable modifications. <p> on our model use the same number of I/Os as those in <ref> [1] </ref> for the less realistic model. 5 B records D 1 Disks Tracks 3 Problem Definitions The problems we consider in this paper have been well described in the literature. Most of the following definitions are those from [1], with suitable modifications. Sorting Problem Instance: The internal memory is empty, and the N records are stored in the first N locations of secondary storage. Goal: The internal memory is empty, and the N records are stored in sorted non-decreasing order in the first N locations of secondary storage. <p> The logarithmic factors that multiply the N=P B term in the above expressions indicate the degree of nonlinearity. The I/O lower bounds for Theorems 1-4 follow from the lower bounds proved in <ref> [1] </ref> for the less realistic model in which P tracks can be accessed on the same disk in a single I/O. Since any algorithm in our model automatically applies to the model in [1], the same lower bounds apply. The I/O lower bounds proved in [1] are based only on routing <p> The I/O lower bounds for Theorems 1-4 follow from the lower bounds proved in <ref> [1] </ref> for the less realistic model in which P tracks can be accessed on the same disk in a single I/O. Since any algorithm in our model automatically applies to the model in [1], the same lower bounds apply. The I/O lower bounds proved in [1] are based only on routing concerns and thus hold for an arbitrarily powerful adversary, except in the case of sorting for the extreme case mentioned in Theorem 1 when M and B are extremely small, in which case <p> the lower bounds proved in <ref> [1] </ref> for the less realistic model in which P tracks can be accessed on the same disk in a single I/O. Since any algorithm in our model automatically applies to the model in [1], the same lower bounds apply. The I/O lower bounds proved in [1] are based only on routing concerns and thus hold for an arbitrarily powerful adversary, except in the case of sorting for the extreme case mentioned in Theorem 1 when M and B are extremely small, in which case the comparison model is used. <p> The algorithms, which consist of a series of shu*e-merges, are the ones described in <ref> [1] </ref>, except that the disk placement of the blocks of the merged runs must be done in a staggered way so that the merging in the next pass can be done using full parallelism. <p> The condition (S 1) 2 2M in Lemma 3 is satisfied by the setting S p M=B= ln 2 (M=B) + 1 for Phase 1, and we have pN=M = (S 1)N=4M N=2 (S 1). 6.5 Permuting for Very Small P and B Aggarwal and Vitter <ref> [1] </ref> show in their one-disk model with P block transfers per I/O that the optimal way to permute when P and B are very small is the nave method of moving P records in a single I/O from their inputed positions to the desired final positions.
Reference: [2] <author> M. Blum, R. W. Floyd, V. Pratt, R. Rivest, and R. E. Tarjan, </author> <title> "Time Bounds for Selection," in Complexity of Computer Calculations, </title> <editor> Miller and Thatcher, eds., </editor> <publisher> Plenum, </publisher> <address> NY, </address> <year> 1973, </year> <pages> 105-109. </pages>
Reference-contexts: We pick the median record from each of these sorted sets and find the median of the medians using the linear-time sequential algorithm developed in <ref> [2] </ref>. The number of I/Os required for these operations is O (n=P B + n=M ) = O (n=P B). We use the key value of this median record to partition the n records into two sets.
Reference: [3] <author> J. L. Carter and M. N. Wegman, </author> <title> "Universal Classes of Hash Functions," </title> <journal> Journal of Computer and System Sciences 18 (April 1979), </journal> <pages> 143-154, </pages> <booktitle> also appears in Proceedings of the 9th Annual ACM Symposium on Theory of Computing, </booktitle> <month> (May </month> <year> 1977), </year> <pages> 106-112. 33 </pages>
Reference: [4] <author> R. W. Floyd, </author> <title> "Permuting Information in Idealized Two-Level Storage," in Complexity of Computer Calculations, </title> <editor> R. Miller and J. Thatcher, ed., </editor> <publisher> Plenum, </publisher> <year> 1972, </year> <pages> 105-109. </pages>
Reference-contexts: In their model, an I/O can simultaneously transfer P physical blocks, each consisting of B contiguous records. Their results generalized the groundbreaking work done by Floyd <ref> [4] </ref>, who gave optimal bounds for sorting, realized by standard two-way merge sort, for the special case P = 1 and M = 2B = p N .
Reference: [5] <author> G. Gibson, L. Hellerstein, R. M. Karp, R. H. Katz, and D. A. Patterson, </author> <title> "Coding Techniques for Handling Failures in Large Disk Arrays," </title> <booktitle> Procedings of the Third International Conference on Architectural Support for Programming Languages and Operating Systems (March 1989). </booktitle>
Reference: [6] <author> W. Jilke, </author> <title> "Disk Array Mass Storage Systems: The New Opportunity," </title> <publisher> Amperif Corporation, </publisher> <month> September </month> <year> 1986. </year>
Reference: [7] <author> L. Kleinrock, </author> <title> Queueing Systems, Volume I: Theory, </title> <publisher> Wiley and Sons, </publisher> <address> New York, </address> <year> 1979. </year>
Reference-contexts: To bound (9), we use Chernoff's bound <ref> [7] </ref>: 18 6 EXTERNAL SORTING AND PERMUTING Lemma 1 If X is a nonnegative random variable and r 0 we have PrfX ug E (e rX ) Before we apply Chernoff's bound, we must construct the appropriate scenario.
Reference: [8] <author> D. Knuth, </author> <title> The Art of Computer Programming, Volume 3: Sorting and Searching, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1973. </year>
Reference-contexts: Permutation Networks The problem instance and goal are the same as for the FFT problem, except that the permutation network digraph (see below) is pebbled instead of the FFT digraph. A permutation network is a sorting network <ref> [8] </ref> consisting of comparator modules or switches that can be set by external controls so that any desired permutation of the inputs can be realized at the output level of the network. It consists of J + 1 levels, for some J log N , each containing N nodes. <p> Thus the hard part of sorting in the non-extreme case is the routing of the records, not the determination of the records' order. The well-known technique of key sorting <ref> [8] </ref>, which attempts to reduce sorting to permutation routing by using a special-purpose method of determining the order of the records, is therefore not going to use significantly fewer I/Os than will general sorting algorithms.
Reference: [9] <author> E. E. Lindstrom and J. S. Vitter, </author> <title> "The Design and Analysis of BucketSort for Bubble Memory Secondary Storage," </title> <journal> IEEE Transactions on Computers C-34 (March 1985), </journal> <pages> 218-233. </pages>
Reference: [10] <author> N. B. Maginnis, </author> <title> "Store More, Spend Less: Mid-Range Options Around," </title> <type> Computerworld (November 16, </type> <year> 1986), </year> <month> 71. </month>
Reference: [11] <author> M. H. Nodine and J. S. Vitter, </author> <title> "Optimal Deterministic Sorting on Parallel Memory Hierarchies," </title> <institution> Department of Computer Science, Brown University, </institution> <note> Technical Report , August 1992. </note>
Reference: [12] <author> M. H. Nodine and J. S. Vitter, </author> <title> "Optimal Deterministic Sorting on Parallel Disks," </title> <institution> Department of Computer Science, Brown University, </institution> <note> Technical Report , August 1992. </note>
Reference: [13] <author> M. H. Nodine and J. S. Vitter, </author> <title> "Large-Scale Sorting in Parallel Memories," </title> <booktitle> Proceedings of the 3rd Annual ACM Symposium on Parallel Algorithms and Architectures (July 1991). </booktitle>
Reference: [14] <author> D. A. Patterson, G. Gibson, and R. H. Katz, </author> <title> "A Case for Redundant Arrays of Inexpensive Disks (RAID)," </title> <booktitle> Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data (June 1988), </booktitle> <pages> 109-116. </pages>
Reference: [15] <author> J. Savage and J. S. Vitter, </author> <title> "Parallelism in Space-Time Tradeoffs," </title> <booktitle> in Advances in Computing Research, </booktitle> <volume> Volume 4 , F. </volume> <editor> P. Preparata, ed., </editor> <publisher> JAI Press, </publisher> <year> 1987, </year> <pages> 117-146, </pages> <note> also appears in Proceedings of the International Workshop on Parallel Computing and VLSI, Amalfi, Italy (May 1984), </note> <editor> P. Bertolazzi and F. Luccio, ed., </editor> <publisher> Elsevier Science Press, </publisher> <year> 1985, </year> <pages> 49-58. </pages>
Reference-contexts: The lower bound in Theorem 5 for standard matrix multiplication follows by taking the bound for the case P = 1 in <ref> [15] </ref> and dividing by P . The algorithms that meet the lower bounds of Theorems 1-5 will be described and analyzed in the following sections.
Reference: [16] <author> H. S. Stone, </author> <title> "Parallel Processing with the Perfect Shu*e," </title> <journal> IEEE Transactions on Computers C-20 (February 1971), </journal> <pages> 153-161. </pages>
Reference-contexts: Without loss of generality, we assume for simplicity of exposition that N , M , P , and B are powers of 2. The operation of shu*e-merge consists of performing a perfect shu*e <ref> [16] </ref> on the elements of M=B runs of r records each, and the result is a single shu*ed run of rM=B elements.
Reference: [17] <institution> University of California at Berkeley, "Massive Information Storage, Management, and Use (NSF Institutional Infrastructure Proposal)," </institution> <note> Technical Report No. UCB/CSD 89/493, </note> <month> January </month> <year> 1989. </year>
Reference: [18] <author> J. S. Vitter and Ph. Flajolet, </author> <title> "Average-Case Analysis of Algorithms and Data Structures," </title> <booktitle> in Handbook of Theoretical Computer Science, </booktitle> <editor> Jan van Leeuwen, ed., </editor> <publisher> North-Holland, </publisher> <year> 1990. </year>
Reference-contexts: It uses a hashing approach to distribute the blocks of each bucket among the disks. It works effectively when the "hash function" distributes the records evenly, and by analogy to the maximum bucket occupancy in hashing <ref> [18] </ref>, this happens intuitively when the expected number of blocks per disk for each bucket is at least a logarithmic amount. However, if N is not much larger than M, the distribution using the hashing approach can be quite uneven, resulting in nonoptimal performance.
Reference: [19] <author> J. S. Vitter and E. A. Shriver, </author> <title> "Algorithms for Parallel Memory II: Hierarchical Multilevel Memories," </title> <institution> Department of Computer Science, Brown University, </institution> <type> Technical Report CS-90-22, </type> <month> September </month> <year> 1990. </year> <note> 34 REFERENCES </note>
Reference-contexts: Magnetic disks, for example, provide the functionality needed in our model of secondary storage, so for simplicity we shall refer to secondary storage as disk storage, consisting of one or more disk drives. Efficient algorithms for multilevel hierarchical memory are considered in the companion paper <ref> [19] </ref>. In previous work, Aggarwal and Vitter [1] presented optimal upper and lower bounds on the I/O needed for sorting-related problems of size N using a two-level memory model where internal memory can store M records and the secondary memory size is limitless.
Reference: [20] <author> C. Wu and T. Feng, </author> <title> "The Universality of the Shu*e-Exchange Network," </title> <journal> IEEE Transactions on Computers C-30 (May 1981), </journal> <pages> 324-332. </pages>
Reference-contexts: Permutation Networks and FFT Every permutation of N elements can be realized by three passes through an FFT network, by an appropriate setting of the switches in the FFT network that depends on the permutation <ref> [20] </ref>. So we can get optimal I/O strategies for an FFT-based permutation network by getting optimal I/O strategies for FFT digraphs. The FFT digraph is defined in Section 3. For simplicity, we assume that log M divides log N evenly.
References-found: 20

