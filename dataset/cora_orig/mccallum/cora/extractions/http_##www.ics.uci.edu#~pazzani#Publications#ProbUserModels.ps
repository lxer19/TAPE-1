URL: http://www.ics.uci.edu/~pazzani/Publications/ProbUserModels.ps
Refering-URL: http://www.ics.uci.edu/~mlearn/MLPapers.html
Root-URL: 
Email: -dbillsus, pazzani-@ics.uci.edu  
Title: Learning Probabilistic User Models  
Author: Daniel Billsus and Michael Pazzani 
Address: Irvine, CA 92697-3425, USA  
Affiliation: Department of Information and Computer Science University of California, Irvine  
Abstract: We describe two applications that use rated text documents to induce a model of the user's interests. Based on our experiments with these applications we propose the use of a probabilistic learning algorithm, the Simple Bayesian Classifier (SBC), for user modeling tasks. We discuss the adva n-tages and disadvantages of the SBC and present a novel extension to this algorithm that is specif i-cally geared towards improving predictive accuracy for datasets typically encountered in user mo d-eling and information filtering tasks. Results from an empirical study demonstrate the effectiveness of our approach. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Billsus, D. & Pazzani, M. </author> <title> (1996) Revising User Profiles: The Search for Interesting Web Sites. </title> <booktitle> In Proceedings of the Third International Workshop on Multistrategy Learning, </booktitle> <publisher> AAAI Press. </publisher>
Reference-contexts: The learned model is then used to generate a list of funding o p-portunities sorted according to GrantLearner's relevance prediction. First experiments and feedback from different users suggest that GrantLearner is a helpful tool to locate potentially interesting fun ding opportunities. 3. The Simple Bayesian Classifier In previous work <ref> (Pazzani, Muramatsu, Billsus, 1996) </ref> we compared standard machine learning alg o-rithms, including the SBC, artificial neural networks, decision trees and nearest neighbor approaches, on text classification tasks. Using rated text documents as training examples, we induced Boolean concepts that distinguish between interesting and uninteresting documents. <p> Using this approach, we find the k most informative words of the current set of rated pages. In the experiments discussed in this paper, we use the 96 most informative words, b e--cause previous experiments with the SBC <ref> (Pazzani, Muramatsu, Billsus, 1996) </ref> resulted in the highest average accuracy for this value of k. We convert all text documents in the training set to Boolean feature vectors, which can then be used as training examples for the SBC. <p> In addition, we will perform further experiments with additional knowledge directly obtained from the user (e.g. in form of a more detailed rating scale or features explicitly provided by the user). Experiments reported in <ref> (Billsus, Pazzani, 1996) </ref> suggest that explicit user knowledge can substantially increase predictive accuracy. 7. Conclusions and Summary We have motivated the use of a probabilistic learning algorithm, the Simple Bayesian Classifier (SBC), for user model induction based on rated text documents.
Reference: <author> Domingos, P., and Pazzani, M. </author> <title> (1996) Beyond Independence: Conditions for the Optimality of the Simple Bayesian Classifier. </title> <booktitle> Proceedings of the International Conference on Machine Learning . Bari, </booktitle> <address> Italy. </address>
Reference: <author> Duda, R. & Hart, P. </author> <title> (1973) Pattern classification and scene analysis. </title> <address> New York: </address> <publisher> John Wiley & Sons. </publisher>
Reference-contexts: We convert all text documents in the training set to Boolean feature vectors, which can then be used as training examples for the SBC. The SBC <ref> (Duda & Hart, 1973) </ref> is a probabilistic method for classification. It is used to determine the probability that an exa m-ple belongs to class C j given feature values of the example.
Reference: <author> Good, I. </author> <title> (1965) The Estimation of Probabilities: An Essay on Modern Bayesian Methods . M.I.T. </title> <publisher> Press. </publisher>
Reference-contexts: Kohavi et. al. (1997) compare 8 different estimation methods that prevent pro b-ability estimates from being zero and vary in the strength of their estimation bias. For example, the commonly used probability correction based on Laplaces law of succession <ref> (Good, 1965) </ref> has the following form for two-class problems: p = (N + 1) / (n + 2), where N is the number of matches, i.e. in our context the number of examples from one class that have a certain attribute value, and n is the overall number of examples.
Reference: <author> Kobsa, A., Wahlster, W. (eds.), </author> <title> (1989) User Models in Dialog Systems New York: </title> <publisher> Springer 1989 Kohavi, </publisher> <editor> R., Becker, B., and Sommerfield, D. </editor> <title> (1997) Improving Simple Bayes, </title> <type> ECML-97 Technical Report (ftp://starry.stanford.edu/pub/ronnyk/impSBC.ps.Z). </type>
Reference-contexts: 1. Introduction The acquisition of user models for interactive computer systems has been addressed in many different ways. Most approaches discussed in the user modeling literature <ref> (e.g. work reported in Kobsa, Wahlster 1989) </ref> are based on predefined knowledge about users or groups of users, e.g. in form of stereotypes or inference rules. For tasks that require a detailed model about the users preferences, likes and dislikes, these acquisition techniques are often not flexible enough.
Reference: <author> Lang, K. </author> <year> (1995) </year> <month> NewsWeeder: </month> <title> Learning to filter news. </title> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning. </booktitle> <address> Lake Tahoe, CA. </address>
Reference-contexts: Not all words that appear in a text document are used as features. We use an information-based approach, similar to that used by an early version of the NewsWeeder program <ref> (Lang, 1995) </ref> to determine which words to use as features. Intuitively, one would like to select words that occur frequently in pages rated as interesting, but infrequently on pages rated as uninteresting. This is accomplished by finding the expected information gain (e.g.
Reference: <author> Pazzani, M., Muramatsu J., and Billsus, D. </author> <year> (1996) </year> <month> Syskill & Webert: </month> <title> Identifying interesting web sites. </title> <booktitle> Proceedings of the National Conference on Artificial Intelligence, </booktitle> <address> Portland, OR. </address>
Reference-contexts: The learned model is then used to generate a list of funding o p-portunities sorted according to GrantLearner's relevance prediction. First experiments and feedback from different users suggest that GrantLearner is a helpful tool to locate potentially interesting fun ding opportunities. 3. The Simple Bayesian Classifier In previous work <ref> (Pazzani, Muramatsu, Billsus, 1996) </ref> we compared standard machine learning alg o-rithms, including the SBC, artificial neural networks, decision trees and nearest neighbor approaches, on text classification tasks. Using rated text documents as training examples, we induced Boolean concepts that distinguish between interesting and uninteresting documents. <p> Using this approach, we find the k most informative words of the current set of rated pages. In the experiments discussed in this paper, we use the 96 most informative words, b e--cause previous experiments with the SBC <ref> (Pazzani, Muramatsu, Billsus, 1996) </ref> resulted in the highest average accuracy for this value of k. We convert all text documents in the training set to Boolean feature vectors, which can then be used as training examples for the SBC. <p> In addition, we will perform further experiments with additional knowledge directly obtained from the user (e.g. in form of a more detailed rating scale or features explicitly provided by the user). Experiments reported in <ref> (Billsus, Pazzani, 1996) </ref> suggest that explicit user knowledge can substantially increase predictive accuracy. 7. Conclusions and Summary We have motivated the use of a probabilistic learning algorithm, the Simple Bayesian Classifier (SBC), for user model induction based on rated text documents.
Reference: <author> Quinlan, J.R. </author> <title> (1986) Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 81106. </pages>
Reference-contexts: Intuitively, one would like to select words that occur frequently in pages rated as interesting, but infrequently on pages rated as uninteresting. This is accomplished by finding the expected information gain <ref> (e.g. Quinlan 1986) </ref> that the presence or absence of a word gives toward the classification of elements of a set of documents. Using this approach, we find the k most informative words of the current set of rated pages.
References-found: 8

