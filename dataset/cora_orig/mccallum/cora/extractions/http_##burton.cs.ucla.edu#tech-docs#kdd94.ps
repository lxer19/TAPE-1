URL: http://burton.cs.ucla.edu/tech-docs/kdd94.ps
Refering-URL: http://burton.cs.ucla.edu/tech-docs/kdd94/kdd94.html
Root-URL: http://www.cs.ucla.edu
Title: Abstraction of High Level Concepts from Numerical Values in Databases  
Author: Wesley W. Chu and Kuorong Chiang 
Keyword: Key words: approximate query answering, type abstraction hierarchy, conceptual clustering, discretization, data summarization, knowledge discovery in databases.  
Address: Los Angeles  
Affiliation: Computer Science Department University of California,  
Abstract: A conceptual clustering method is proposed for discovering high level concepts of numerical attribute values from databases. The method considers both frequency and value distributions of data, thus is able to discover relevant concepts from numerical attributes. The discovered knowledge can be used for representing data semantically and for providing approximate answers when exact ones are not available. Our knowledge discovery approach is to partition the data set of one or more attributes into clusters that minimize the relaxation error. An algorithm is developed which finds the best binary partition in O(n) time and generates a concept hierarchy in O(n 2 ) time where n is the number of distinct values of the attribute. The effectiveness of our clustering method is demonstrated by applying it to a large transportation database for approximate query answering. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. R. Anderberg. </author> <title> Cluster Analysis for applications. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: For clustering objects into "neighborhoods," two groups of methods can be used: statistical clustering and numerical taxonomy <ref> [12, 1, 7, 17] </ref>, and conceptual clustering [15, 10, 14]. In statistical clustering and numerical taxonomy, most similarity metrics are defined between pairs of objects. Objects are pair-wise clustered in a bottom up manner until all objects are in a single cluster.
Reference: [2] <author> David K. Y. Chiu, Andrew K. C. Wong, and Benny Cheung. </author> <title> Information discovery through hierarchical maximum entropy discretization and synthesis. </title> <editor> In Gregory Piatetsky-Shapiro and William J. Frawley, editors, </editor> <title> Knowledge Discovery in Databases. </title> <publisher> AAAI Press/The MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: A commonly used measure is the information entropy [16]. It can be shown that the entropy is maximum when the data is partitioned most evenly. (We call this the ME method <ref> [2, 18] </ref>.) However, no semantic meaning is attached to the resultant clusters because the discretization is concerned with the frequency distribution rather than the value distribution in the cluster. Therefore, the ME method is not suitable for abstracting numerical data.
Reference: [3] <author> Wesley W. Chu and Q. Chen. </author> <title> Neighborhood and associative query answering. </title> <journal> Journal of Intelligent Information Systems, </journal> <volume> 1(3/4), </volume> <year> 1992. </year>
Reference-contexts: 1 Introduction Knowledge discovered from databases can be used to abstract the data into high level concepts. The discovered concept, or abstraction, usually implies a certain context, thus providing more information than the raw data. As a result, the abstraction can be used to characterize databases and process queries intelligently <ref> [6, 3] </ref>. In particular, abstraction can be used to derive approximate answers when the objects requested by a query are not available, the query conditions can be relaxed to their corresponding abstraction where "neighborhood objects" can be found and returned as the approximate answers. <p> In Conceptual clustering, the "goodness measures" are usually defined for the overall partitioning of objects instead of for pairs of objects. Clustering methods are designed that maximize the goodness measure. For approximate query answering <ref> [3, 5] </ref>, the goal is to minimizes the relaxation error of the approximate answers where the relaxation error is defined based on fl This work is supported in part by DARPA contract N00174-91-C-0107 y The authors may be reached at fwwc,kuorongg@cs.ucla.edu the overall partitioning of objects.
Reference: [4] <author> Wesley W. Chu and Kuorong Chiang. </author> <title> A distribution sensitive clustering method for numerical values. </title> <type> Technical Report 93-0006, </type> <institution> UCLA Computer Science Department, </institution> <year> 1993. </year>
Reference-contexts: For symmetrical distributions, the best cut is at the median. Therefore, ME and DISC find the same binary cut. (For a formal proof, the interested readers shall refer to <ref> [4] </ref>.) For skewed distributions, which holds for most data in the transportation database, DISC performs better than ME. Empirical results show that the performance improvement of DISC over ME increases as the skewness increases [4]. 7.2 Multiple Attributes In addition to the relaxation error, we shall introduce two additional performance measures, <p> DISC find the same binary cut. (For a formal proof, the interested readers shall refer to <ref> [4] </ref>.) For skewed distributions, which holds for most data in the transportation database, DISC performs better than ME. Empirical results show that the performance improvement of DISC over ME increases as the skewness increases [4]. 7.2 Multiple Attributes In addition to the relaxation error, we shall introduce two additional performance measures, accuracy of the answer and efficiency of the retrieval 4 , to compare the performance of DISC with ME: accuracy of the answer = retrieved relevant answers all relevant answers efficiency of the retrieval
Reference: [5] <author> Wesley W. Chu, M. A. Merzbacher, and L. Berkovich. </author> <booktitle> The design and implementation of CoBase. In Proceedings of ACM SIGMOD, </booktitle> <address> Washington D. C., USA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: In Conceptual clustering, the "goodness measures" are usually defined for the overall partitioning of objects instead of for pairs of objects. Clustering methods are designed that maximize the goodness measure. For approximate query answering <ref> [3, 5] </ref>, the goal is to minimizes the relaxation error of the approximate answers where the relaxation error is defined based on fl This work is supported in part by DARPA contract N00174-91-C-0107 y The authors may be reached at fwwc,kuorongg@cs.ucla.edu the overall partitioning of objects. <p> DISC generates all the numerical TAHs in less than one hour of processing time on a Sun Sparc 10 Workstation. The generated TAHs are used in the Cooperative Database System (CoBase) <ref> [5] </ref> at UCLA for providing approximate answers. The approximate query answers derived from TAHs are empirically evaluated in terms of accuracy, efficiency, and relaxation error.
Reference: [6] <author> F. Cuppens and R. Demoloube. </author> <title> Cooperative answering: a methodology to provide intelligent access to databases. </title> <booktitle> In Proceedings of the 2th International Conference on Expert Database Systems, </booktitle> <address> Virginia, USA, </address> <year> 1988. </year>
Reference-contexts: 1 Introduction Knowledge discovered from databases can be used to abstract the data into high level concepts. The discovered concept, or abstraction, usually implies a certain context, thus providing more information than the raw data. As a result, the abstraction can be used to characterize databases and process queries intelligently <ref> [6, 3] </ref>. In particular, abstraction can be used to derive approximate answers when the objects requested by a query are not available, the query conditions can be relaxed to their corresponding abstraction where "neighborhood objects" can be found and returned as the approximate answers.
Reference: [7] <author> B. Everitt. </author> <title> Cluster Analysis. </title> <publisher> Heinemann Educational Books, </publisher> <address> London, </address> <year> 1980. </year>
Reference-contexts: For clustering objects into "neighborhoods," two groups of methods can be used: statistical clustering and numerical taxonomy <ref> [12, 1, 7, 17] </ref>, and conceptual clustering [15, 10, 14]. In statistical clustering and numerical taxonomy, most similarity metrics are defined between pairs of objects. Objects are pair-wise clustered in a bottom up manner until all objects are in a single cluster.
Reference: [8] <author> D. H. Fisher. </author> <title> Knowledge acquisition via incremental conceptual clustering. </title> <journal> Machine Learning, </journal> <volume> 2(2) </volume> <pages> 139-172, </pages> <year> 1987. </year>
Reference-contexts: Therefore, the ME method is not suitable for abstracting numerical data. COBWEB <ref> [8] </ref>, a conceptual clustering system, uses category utility (CU ) [9] as a quality measure to classify the objects described by a set of attributes into a classification tree.
Reference: [9] <author> M. A. Gluck and J. E. Corter. </author> <title> Information, uncertainty, and the unity of categories. </title> <booktitle> In Proceedings of the 7th Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 283-287, </pages> <address> Irvine, CA, </address> <year> 1985. </year>
Reference-contexts: Therefore, the ME method is not suitable for abstracting numerical data. COBWEB [8], a conceptual clustering system, uses category utility (CU ) <ref> [9] </ref> as a quality measure to classify the objects described by a set of attributes into a classification tree.
Reference: [10] <author> Stephen Jose Hanson and Malcolm Bauer. </author> <title> Conceptual clustering, categorization, </title> <journal> and poly-morphy. Machine Learning, </journal> <volume> 3 </volume> <pages> 343-372, </pages> <year> 1989. </year>
Reference-contexts: For clustering objects into "neighborhoods," two groups of methods can be used: statistical clustering and numerical taxonomy [12, 1, 7, 17], and conceptual clustering <ref> [15, 10, 14] </ref>. In statistical clustering and numerical taxonomy, most similarity metrics are defined between pairs of objects. Objects are pair-wise clustered in a bottom up manner until all objects are in a single cluster.
Reference: [11] <author> Yannis E. Ioannidis, Tomas Saulys, and Andrew J. Whitsitt. </author> <title> Conceptual learning in database design. </title> <journal> ACM Transactions on Information Systems, </journal> <volume> 10(3) </volume> <pages> 265-293, </pages> <year> 1992. </year>
Reference-contexts: i=1 j=1 jx i x j j ) After rearrangement by noticing P n P n j=1 P (x i )P (x j ) = 1, we have G (C) = 1 i=1 j=1 jx i x j j : (6) 2 The same scoring rule is independently proposed in <ref> [11] </ref> for matching two numerical values. Let us define the relaxation error of C, denoted by RE (C), as the normalized expected difference between any two values in C.
Reference: [12] <author> A. K. Jain and R. C. Dubes. </author> <title> Algorithms for Cluster Analysis. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1988. </year>
Reference-contexts: For clustering objects into "neighborhoods," two groups of methods can be used: statistical clustering and numerical taxonomy <ref> [12, 1, 7, 17] </ref>, and conceptual clustering [15, 10, 14]. In statistical clustering and numerical taxonomy, most similarity metrics are defined between pairs of objects. Objects are pair-wise clustered in a bottom up manner until all objects are in a single cluster.
Reference: [13] <author> Maurice G. Kendall and Alan Stuart. </author> <booktitle> The Advanced Theory of Statistics, </booktitle> <volume> volume 1. </volume> <publisher> Hafner Publishing Company, </publisher> <year> 1969. </year>
Reference-contexts: Let n be the number of distinct values in C, f i be the frequency of the value x i , and N be the total number of values in C which equals P n i=1 f i . Based on <ref> [13] </ref>, equation (7) can be transformed to RE (C) = N 2 h=1 where F (h) is the accumulated frequency P h i=1 f i .
Reference: [14] <author> M. Lebowitz. </author> <title> Experiments with incremental conceptual formation. </title> <journal> Machine Learning, </journal> <volume> 2(2) </volume> <pages> 103-138, </pages> <year> 1987. </year>
Reference-contexts: For clustering objects into "neighborhoods," two groups of methods can be used: statistical clustering and numerical taxonomy [12, 1, 7, 17], and conceptual clustering <ref> [15, 10, 14] </ref>. In statistical clustering and numerical taxonomy, most similarity metrics are defined between pairs of objects. Objects are pair-wise clustered in a bottom up manner until all objects are in a single cluster.
Reference: [15] <author> R. S. Michalski and R. E. Stepp. </author> <title> Learning from observation: Conceptual clustering. </title> <editor> In R. S. Michalski, J. G. Carbonell, and T. M. Mitchell, editors, </editor> <booktitle> Machine Learning, </booktitle> <volume> volume 1. </volume> <publisher> Margan Kaufmann Publishers, Inc., </publisher> <year> 1983. </year>
Reference-contexts: For clustering objects into "neighborhoods," two groups of methods can be used: statistical clustering and numerical taxonomy [12, 1, 7, 17], and conceptual clustering <ref> [15, 10, 14] </ref>. In statistical clustering and numerical taxonomy, most similarity metrics are defined between pairs of objects. Objects are pair-wise clustered in a bottom up manner until all objects are in a single cluster.
Reference: [16] <author> C. E. Shannon and W. Weaver. </author> <title> The Mathematical Theory of Communication. </title> <publisher> University of Illinois Press, </publisher> <address> Urbana, Ill, </address> <year> 1964. </year>
Reference-contexts: Finally, we compare the performance of the TAH generated by DISC with the traditional index tree for approximate query answering. 2 Related Work Prior work in discretization aims at decreasing cardinality of data by maximizing/minimizing certain heuristic measures. A commonly used measure is the information entropy <ref> [16] </ref>.
Reference: [17] <author> P. H. A. Sneath and R. R. Sokal. </author> <title> Numerical Taxonomy: The Principles and Practice of Numerical Classification. </title> <editor> W. H. </editor> <publisher> Freeman and Company, </publisher> <address> San Francisco, </address> <year> 1973. </year>
Reference-contexts: For clustering objects into "neighborhoods," two groups of methods can be used: statistical clustering and numerical taxonomy <ref> [12, 1, 7, 17] </ref>, and conceptual clustering [15, 10, 14]. In statistical clustering and numerical taxonomy, most similarity metrics are defined between pairs of objects. Objects are pair-wise clustered in a bottom up manner until all objects are in a single cluster.
Reference: [18] <author> Andrew K. C. Wong and David K. Y. Chiu. </author> <title> Synthesizing statistical knowledge from incomplete mixed-mode data. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 9(6) </volume> <pages> 796-805, </pages> <year> 1987. </year>
Reference-contexts: A commonly used measure is the information entropy [16]. It can be shown that the entropy is maximum when the data is partitioned most evenly. (We call this the ME method <ref> [2, 18] </ref>.) However, no semantic meaning is attached to the resultant clusters because the discretization is concerned with the frequency distribution rather than the value distribution in the cluster. Therefore, the ME method is not suitable for abstracting numerical data.
References-found: 18

