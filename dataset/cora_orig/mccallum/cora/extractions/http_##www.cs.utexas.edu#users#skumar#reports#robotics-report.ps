URL: http://www.cs.utexas.edu/users/skumar/reports/robotics-report.ps
Refering-URL: http://www.cs.utexas.edu/users/skumar/reports.html
Root-URL: 
Title: Simultaneous Learning of Control Laws and Local Environment Representations for Intelligent Navigation Robots  
Author: Shailesh Kumar 
Address: Austin  
Affiliation: Department of Computer Science The University of Texas at  
Abstract: Two issues of an intelligent navigation robot have been addressed in this work. First is the robot's ability to learn a representation of the local environment and use this representation to identify which local environment it is in. This is done by first extracting features from the sensors which are more informative than just distances of obstacles in various directions. Using these features a reduced ring representation (RRR) of the local environment is derived. As the robot navigates, it learns the RRR signatures of all the new environment types it encounters. For purpose of identification, a ring matching criteria is proposed where the robot tries to match the RRR from the sensory input to one of the RRRs in its library. The second issue addressed is that of learning hill climbing control laws in the local environments. Unlike conventional neuro-controllers, a reinforcement learning framework, where the robot first learns a model of the environment and then learns the control law in terms of a neural network is proposed here. The reinforcement function is generated from the sensory inputs of the robot before and after a control action is taken. Three key results shown in this work are that (1) The robot is able to build its library of RRR signatures perfectly even with significant sensor noise for eight different local environ-mets, (2) It was able to identify its local environment with an accuracy of more than 96%, once the library is build, and (3) the robot was able to learn adequate hill climbing control laws which take it to the distinctive state of the local environment for five different environment types.
Abstract-found: 1
Intro-found: 1
Reference: <author> Goetz, P., Kumar, S., and Miikkulainen, R. </author> <year> (1996). </year> <title> On-line adaptation of a signal predistorter through dual reinforcement learning. </title> <booktitle> In Machine Learning: Proceedings of the 13th Annual Conference (Bari, </booktitle> <address> Italy). </address>
Reference-contexts: The environment generates a reinforcement signal obtained by a reinforcement function r (s (t),s (t + 1)) where s (t + 1) is the sensor input of the robot after taking the control action u (t) 4.1 Reinforcement Learning The reinforcement learning framework <ref> (Goetz et al. 1996) </ref> used in this work is shown in figure 13 There are two stages in reinforcement learning. The first neural network marked M in figure 13 tries to learn a model of the environment.
Reference: <author> Lee, W. L. </author> <year> (1996). </year> <title> Spatial semantic hierarchy for physical mobile robot. </title> <type> Master's thesis, </type> <institution> Department of Computer Science, University of Texas at Austin, Austin, TX. </institution>
Reference-contexts: The environment shown in figure 5 has generates the transition map in which the following pairs of environments are neighbors: (&lt;L,CO&gt;,&lt;T,CO&gt;,&lt;C,CO&gt;). Section 3.5 describes the learning of the transition matrix as the navigation takes place. A transition matrix is equivalent to the Finite State Machine used in <ref> (Lee 1996) </ref> and since in this work the possible set of environment grows, the Transition matrix also builds up. <p> There are two kinds of goals that the robot can pursue: (1) to identify the distinctive state of the current local environment or (2) to follow a trajectory to move from one local environment to another. For the first goal as described in <ref> (Lee 1996) </ref>, the robot uses a hill climbing control law where it tries to minimize a distinctive measure (d-measure). The distinctive state of an environment is a unique state or location such that its distance from the all the nearby objects (wall or Convex edge) is equal. <p> Section 4.3 describes the learning of the model in detail. Section 4.4 describes the learning of the controller network. 4.2 Reinforcement Signal As mentioned earlier, in this work only hill climbing control laws are learned. Essentially hill climbing tries to minimize a d-measure <ref> (Lee 1996) </ref>. Thus the reinforcement should be positive if the d-measure related to the sensor s 0 is smaller than that related to the sensor vector s and should be negative otherwise. Moreover, it should be normalized in the range -1 to 1.
Reference: <author> Zalzala, A. M. S., and Morris, A. S. </author> <year> (1996). </year> <title> Neural Networks for Robotic Control. </title> <address> New York: </address> <publisher> Ellis Horwood. </publisher> <pages> 19 </pages>
Reference-contexts: Reinforcement learning is used to learn these control laws. In conventional approaches where a neural network is used to learn a control law, a mapping from sensory inputs to a control vector is learned by a back propagation network <ref> (Zalzala and Morris 1996) </ref>. This approach has two problems. Firstly it requires a desired control action to be compared with the output of the neuro controller in order to compute the error to be back propagated.
References-found: 3

