URL: http://www.ece.cmu.edu/~ganger/papers/cmg95.ps
Refering-URL: http://www.ece.cmu.edu/~ganger/disksim/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: ganger@lcs.mit.edu  
Title: Generating Representative Synthetic Workloads An Unsolved Problem  
Author: Gregory R. Ganger 
Web: http://www.pdos.lcs.mit.edu/~ganger  
Affiliation: M.I.T. Laboratory for Computer Science  
Date: December 1995, pp. 1263-1269.  
Note: Appeared in the Proceedings of the Computer Measurement Group (CMG) Conference,  
Abstract: Synthetic disk request traces are convenient and popular workloads for performance evaluation of storage subsystem designs and implementations. This paper develops an approach for validating synthetic disk request generators. Using this approach, commonly-used simplifying assumptions about workload characteristics (e.g., uniformly-distributed starting addresses and Poisson arrivals) are shown to be inappropriate, often leading to inaccurate performance predictions. Also, workload characteristics that require additional study are identified.
Abstract-found: 1
Intro-found: 1
Reference: [Ebling94] <author> M. Ebling, M. Satyanarayanan, "SynRGen: </author> <title> An Extensible File Reference Generator", </title> <booktitle> ACM SIGMET-RICS Conference, </booktitle> <month> May </month> <year> 1994, </year> <pages> pp. 108-117. </pages>
Reference-contexts: Rather than attempting to recreate the disk activity resulting from particular system activity, one could use the system activity itself to drive models (or instances) of the system. Two promising approaches to synthetic system benchmarks are the SPEC S-det benchmark [Gaede81, Gaede82] and SynRGen <ref> [Ebling94] </ref>, a synthetic file reference generator. Acknowledgements I thank Richard Golding, Yale Patt, John Wilkes and Bruce Worthington for their direct involvement in the development of this work. This work was partially funded by a CMG graduate fellowship.
Reference: [Ferrari84] <author> D. Ferrari, </author> <title> "On the Foundation of Artificial Workload Design", </title> <booktitle> ACM SIGMETRICS Conference, </booktitle> <month> May </month> <year> 1984, </year> <pages> pp. 8-14. </pages>
Reference-contexts: While performance-based comparison more appropriate than other approaches <ref> [Ferrari84] </ref>, it limits the applicability of calibration results to those systems-under-test (SUTs) used. As a result, workload characteristics that are important in other SUTs can be overlooked. For example, this work examines disk workloads on an individual disk-by-disk basis, ignoring correlations among the workloads observed by the different disks.
Reference: [Gaede81] <author> S. </author> <title> Gaede, "Tools for Research in Computer Workload Characterization", Experimental Computer Performance and Evaluation, 1981, </title> <editor> ed. by D. Ferrari and M. </editor> <publisher> Spadoni. </publisher>
Reference-contexts: Rather than attempting to recreate the disk activity resulting from particular system activity, one could use the system activity itself to drive models (or instances) of the system. Two promising approaches to synthetic system benchmarks are the SPEC S-det benchmark <ref> [Gaede81, Gaede82] </ref> and SynRGen [Ebling94], a synthetic file reference generator. Acknowledgements I thank Richard Golding, Yale Patt, John Wilkes and Bruce Worthington for their direct involvement in the development of this work. This work was partially funded by a CMG graduate fellowship.
Reference: [Gaede82] <author> S. </author> <title> Gaede, "A Scaling Technique for Comparing Interactive System Capacities", </title> <booktitle> 13th International Conference on Management and Performance Evaluation of Computer Systems, </booktitle> <year> 1982, </year> <pages> pp. 62-67. </pages>
Reference-contexts: Rather than attempting to recreate the disk activity resulting from particular system activity, one could use the system activity itself to drive models (or instances) of the system. Two promising approaches to synthetic system benchmarks are the SPEC S-det benchmark <ref> [Gaede81, Gaede82] </ref> and SynRGen [Ebling94], a synthetic file reference generator. Acknowledgements I thank Richard Golding, Yale Patt, John Wilkes and Bruce Worthington for their direct involvement in the development of this work. This work was partially funded by a CMG graduate fellowship.
Reference: [Ganger95] <author> G. Ganger, </author> <title> "System-Orented Evaluation of I/O Subsystem Performance", </title> <type> Ph.D. Dissertation, </type> <institution> University of Michigan, </institution> <address> Ann Arbor, </address> <year> 1995. </year>
Reference-contexts: Request arrival patterns are affected by a number of system-specific details. Two important characteristics of arrival patterns are feedback and burstiness. Feedback between individual request response times and subsequent request arrivals can have a significant impact on performance <ref> [Ganger95] </ref>. However, it is not possible to extract feedback information from the disk request traces available for this work, so we do not pursue it. An arrival pattern is bursty if it exhibits interspersed periods of high activity (small inter-arrival times) and low activity (large inter-arrival time). <p> Also, request arrival patterns do not match those generated by a Poisson process. In fact, they are neither independent nor exponentially distributed. An accurate synthetic trace generator may not be the appropriate solution, because standalone I/O subsystem models are too narrow in scope <ref> [Ganger95] </ref>. They tend to treat all requests equally, ignoring differences in how individual requests affect system behavior. As a result, they ignore or over-simplify feedback effects be tween individual request response times and subsequent request arrivals. <p> As a result, they ignore or over-simplify feedback effects be tween individual request response times and subsequent request arrivals. Also, they predict performance changes with I/O subsystem performance metrics, which do not (in general) correlate with changes in overall system performance. <ref> [Ganger95] </ref> demonstrates these problems and proposes a solution, system-level modeling. A system-level approach may also be appropriate for workload synthesis. Rather than attempting to recreate the disk activity resulting from particular system activity, one could use the system activity itself to drive models (or instances) of the system.
Reference: [HP92] <author> Hewlett-Packard Company, </author> <title> "HP C2244/45/46/47 3.5-inch SCSI-2 Disk Drive Technical Reference Manual", Part Number 5960-8346, </title> <address> Edition 3, </address> <month> September </month> <year> 1992. </year>
Reference-contexts: The simulator accurately models zoned recording, spare regions, defect slipping and reallocation, disk buffers and caches, various prefetch algorithms, fast write, bus delays, and control and communication overheads. For this work, the simulator was configured to model the HP C2240 series of disk drives <ref> [HP92] </ref> using the parameters HP C2247 Disk Drive Formatted Capacity 1.05 GB Rotation Speed 5400 RPM Data Surfaces 13 Cylinders 2051 512-Byte Sectors 2054864 Zones 8 Sectors/Track 56-96 Interface SCSI-2 256 KB Cache, 2 Segments Track Sparing/Reallocation Table 2: Default characteristics of the HP C2247. described in the appendix of [Worthington94].
Reference: [Leland93] <author> W. Leland, M. Taqqu, W. Willinger, D. Wilson, </author> <title> "On the Self-Similar Nature of Ethernet Traffic", </title> <booktitle> ACM SIGCOMM Conference, </booktitle> <year> 1993, </year> <pages> pp. 183-193. </pages>
Reference-contexts: Much work is needed before an understanding of disk request arrival patterns will be achieved. One possible solution, self-similarity, has been applied successfully to the characterization of network traffic <ref> [Leland93] </ref>. Arrival Average Total Rand. Synth. Generation Resp. Time Error Error Error expon 14.4 (6.8) 548 0.9 547 actdist 17.7 (15.5) 542 1.8 539 2-dists 18.1 (16.8) 540 1.8 538 3-dists 20.4 (25.4) 534 3.0 530 Table 6: Arrival pattern synthesis for Order.
Reference: [Ramakrishnan92] <author> K. Ramakrishnan, P. Biswas, R. Karelda, </author> <title> "Analysis of File I/O Traces in Commercial Computing Environments", </title> <booktitle> ACM SIGMETRICS Conference, </booktitle> <year> 1992, </year> <pages> pp. 78-90. </pages>
Reference-contexts: We describe the traces only briefly as they have been described elsewhere in more detail <ref> [Ramakrishnan92, Ruemmler93] </ref>. The traced work-loads span a range of user environments, and each is at least a full workshift (8 hours) in length. Some basic characteristics of the traces are given in table 1. They vary widely in read/write ratios, access sizes, arrival rates, locality and burstiness. <p> While these traces are actually two months in length, we report data for a single week-long snapshot (5/30/92 to 6/6/92). The other four traces are from commercial VAX TM systems running the VMS TM operating system <ref> [Ramakrishnan92] </ref>. Air-Rsv is from a transaction processing environment in which approximately 500 travel agents made airline and hotel reservations. Sci-TS is from a scientific time-sharing environment in which analytic modeling software and graphical and statistical packages were used. Order and Report are from a machine parts distribution company.
Reference: [Ruemmler93] <author> C. Ruemmler, J. Wilkes, </author> <title> "UNIX Disk Access Patterns", </title> <booktitle> Winter USENIX Conference, </booktitle> <month> January </month> <year> 1993, </year> <pages> pp. 405-420. </pages>
Reference-contexts: We describe the traces only briefly as they have been described elsewhere in more detail <ref> [Ramakrishnan92, Ruemmler93] </ref>. The traced work-loads span a range of user environments, and each is at least a full workshift (8 hours) in length. Some basic characteristics of the traces are given in table 1. They vary widely in read/write ratios, access sizes, arrival rates, locality and burstiness. <p> Some basic characteristics of the traces are given in table 1. They vary widely in read/write ratios, access sizes, arrival rates, locality and burstiness. Two of the traces come from Hewlett-Packard systems running HP-UX T M , a version of the UNIX TM operating system <ref> [Ruemmler93] </ref>. Cello comes from a server at HP Labs used for program development, simulation, mail, and news. Snake is from a file server at the University of California at Berkeley used primarily for compilation and editing.
Reference: [Ruemmler94] <author> C. Ruemmler, J. Wilkes, </author> <title> "An Introduction to Disk Drive Modeling", </title> <journal> IEEE Computer, </journal> <volume> Vol. 27, No. 3, </volume> <month> March </month> <year> 1994, </year> <pages> pp. 17-28. </pages>
Reference-contexts: The main disadvantage of trace synthesis is the danger of generating a trace that is a poor match to the original in some important way, thereby compromising the validity of the results. Borrowing from recent work in disk model calibration <ref> [Ruemmler94] </ref>, we develop an approach to validating synthetic disk request trace generators. Using this approach, we show that commonly applied assumptions (e.g., uniform starting addresses, Poisson arrivals) are inappropriate and can produce dramatically incorrect results. We also identify specific areas where additional understanding of workload characteristics is needed. <p> Performance measurements from the resulting SUT activity are used to evaluate the closeness of the synthetic trace. While many metrics can be used, we try to encapsulate a number of them in a single value. As in <ref> [Ruemmler94] </ref>, we plot the response time distributions for the two request sequences (traced and synthetic) and use the root-mean-squared (RMS) horizontal distance between the two curves as a metric, which we call the total error. Lower values indicate better matches. <p> The average response times of the actual disk and the simulator match to within 0.8% in all cases. Unpredictable (from the disk's view) host delays partially account for the difference. Greater insight can be achieved by comparing the response time distributions <ref> [Ruemmler94] </ref>. Figure 2 shows distributions of measured and simulated response times for a sample validation workload of 10,000 requests. As with most of our validation results, one can barely see that two curves are present.
Reference: [Worthington94] <author> B. Worthington, G. Ganger, Y. Patt, </author> <title> "Scheduling Algorithms for Modern Disk Drives", </title> <booktitle> ACM SIGMETRICS Conference, </booktitle> <month> May </month> <year> 1994, </year> <pages> pp. </pages> <month> 241-251. </month> <title> Full version: "Scheduling for Modern Disk Drives and NonRandom Workloads", </title> <type> Report CSE-TR-194-94, </type> <institution> University of Michigan, </institution> <address> Ann Arbor, </address> <month> March </month> <year> 1994. </year>
Reference-contexts: [HP92] using the parameters HP C2247 Disk Drive Formatted Capacity 1.05 GB Rotation Speed 5400 RPM Data Surfaces 13 Cylinders 2051 512-Byte Sectors 2054864 Zones 8 Sectors/Track 56-96 Interface SCSI-2 256 KB Cache, 2 Segments Track Sparing/Reallocation Table 2: Default characteristics of the HP C2247. described in the appendix of <ref> [Worthington94] </ref>. Some default specifications for the HP C2247 drive (the 7-platter member of the C2240 line) are listed in table 2. The simulator was validated by exercising an actual HP C2247 and capturing traces of all SCSI activity.
References-found: 11

