URL: http://www.cs.princeton.edu/~dpd/Papers/DobkinGunopolous.ps.Z
Refering-URL: http://www.cs.princeton.edu/~dpd/Research.html
Root-URL: http://www.cs.princeton.edu
Title: Concept Learning with geometric hypotheses  
Author: David P. Dobkin and Dimitrios Gunopulos 
Abstract: We present a general approach to solving the minimizing disagreement problem for geometric hypotheses with finite VC-dimension. These results also imply efficient agnostic-PAC learning of these hypotheses classes. In particular we give an O(n min(ff+1=2;2k1) log n) algorithm that solves the m.d.p. for two-dimensional convex k-gon hypotheses (where ff is the VC dimension of the implied set system, k is constant), and an O(n 3k1 log n) algorithm for convex k-hedra hypotheses in three dimensions. We extend these results to handle unions of k-gons and give an approach to approximation algorithms. 
Abstract-found: 1
Intro-found: 1
Reference: [AL] <author> D. Angluin and P. Laird, </author> <title> Learning from noisy examples. </title> <booktitle> Machine Learning, 2 (1988), </booktitle> <pages> 343-370. </pages>
Reference: [BN] <author> W. Buntine and T. Niblett, </author> <title> A further comparison of splitting rules for decision-tree induction. </title> <booktitle> Machine Learning, 8 (1992), </booktitle> <pages> 75-82. </pages>
Reference: [C] <author> B. Chazelle, </author> <title> Geometric Discrepancy Revisited. </title> <booktitle> 34th IEEE Symp. Foundat. Computer Science (1993). </booktitle>
Reference-contexts: A 10 Let us begin with a technique that was given by <ref> [C] </ref>. He considers the problem of computing the halfspace that maximizes the numerical discrepancy of a two dimensional point set. His technique easily extends to the bichromatic discrepancy, and we briefly explain it here.
Reference: [dB] <author> M. deBerg, </author> <title> Computing Half-Plane and Strip Discrepancy of Planar Point Sets, </title> <note> to appear (1994). </note>
Reference-contexts: Here we provide a general approach to approximation algorithms to the same problems. Our approach is based on the properties of set ranges with bounded VC-dimension. 2 We note here that <ref> [dB] </ref> gives a more complicated algorithm to compute the numerical discrepancy of 1-stripes in O (n 2 2 a (n) log n). A 10 Let us begin with a technique that was given by [C].
Reference: [DE] <author> D. Dobkin and D. Eppstein, </author> <title> Computing the Discrepancy. </title> <booktitle> 9th Ann. Symp. on Comput. Geom. </booktitle> <year> (1993), </year> <pages> 47-52. </pages>
Reference: [DGM] <author> D. Dobkin, D. Gunopulos and W. Maass, </author> <title> Computing the maximum Bichromatic Discrepancy, with applications in Computer Graphics and Machine Learning. </title> <journal> JCSS, </journal> <note> to appear. </note>
Reference-contexts: Weiss and Kulikowski ([WK91]) report that for some real life data sets, hypotheses defined as the disjunction or conjunction of simple rules of the form x i c 1 or x j = c 2 outperform neural nets and decision trees. <ref> [DGM] </ref> present an algorithm to find the optimal hypothesis, when the hypothesis set is restricted to axis aligned boxes. In this work we examine an extention of the linear separator model. We consider hypotheses that are each defined as the intersection of a constant number of halfs-paces. <p> The problem of computing the numerical discrepancy 1 of halfspaces was addressed by Dobkin and Eppstein ([DE]), 1 In the numerical discrepancy model, a point set is compared to a continuous function (e.g. the area function, <ref> [DGM] </ref>). and a dynamic algorithm for the numerical discrepancy is given. It is easy to show the following lemma if we follow their ideas and use the dynamic one-dimensional algorithm (given in [DGM]) that computes the bichro-matic discrepancy of a point set in O (log n) time per insertion or deletion. <p> In the numerical discrepancy model, a point set is compared to a continuous function (e.g. the area function, <ref> [DGM] </ref>). and a dynamic algorithm for the numerical discrepancy is given. It is easy to show the following lemma if we follow their ideas and use the dynamic one-dimensional algorithm (given in [DGM]) that computes the bichro-matic discrepancy of a point set in O (log n) time per insertion or deletion. <p> Then finding the halfspace that maximizes the discrepancy among all halfspaces that pass through p; q is an one dimensional problem, for which an O (log n) dynamic algorithm with O (n) space requirements is given in <ref> [DGM] </ref>. If we build such a structure for every pair of points, after the insertion of a point in S we have to update O (n 2 ) structures and create O (n) new structures. <p> To solve the O (n 2 ) problems efficiently, we sort the m dual points on their x-coordinate, and we sweep the arrangement with a vertical line. Using the dynamic one dimensional algorithm from <ref> [DGM] </ref> (lemma 2), sweeping the arrangement takes O (m 2 log m) time. <p> To extend the algorithm to k-stripes we note that, using the dual transformation, a k-stripe transforms to the union of k intervals. <ref> [DGM] </ref> extend their dynamic algorithm to dynamically compute the maximum union of k intervals in O (k 2 log m) (where m is the number of one dimensional points) per update, and this gives a running time of O (k 2 n 2 log n). 2 5 Approximation algorithms The algorithms
Reference: [DG] <author> D. Dobkin and D. Gunopulos, </author> <title> The maximum discrepancy of simple geometric ranges, </title> <address> TR-480-94, Princeton U., </address> <year> 1994. </year>
Reference: [EG] <author> H. Edelsbrunner and L. J. Guibas, </author> <title> Topologically Sweeping an Arrangement. </title> <journal> JCSS, </journal> <volume> 38, </volume> <month> 165-194 </month> <year> (1989). </year>
Reference-contexts: It is then easy to see that k halfspaces are sufficient to separate any subset of T , and so T is shattered. From the first part we know that a maximal shattered subset has to be convex, and have cardinality at most 2k + 1. <ref> [EG] </ref> give an algorithm that computes the maximum convex subset of a two dimensional point set S in O (jSj 3 ) time. 2 The bound on the shatter function shows that there are at most O (n ff ) sets of points that we have to consider in order to
Reference: [F93] <author> P. Fisher, </author> <title> Learning unions of convex polygons. </title> <booktitle> Proc. of EURO-COLT '93, </booktitle> <year> 1993. </year>
Reference: [F95] <author> P. Fisher, </author> <title> More or Less Efficient Agnostic Learning of Convex Polygons. </title> <note> these procedings (COLT-95), to appear. </note>
Reference-contexts: We consider hypotheses that are each defined as the intersection of a constant number of halfs-paces. Hypotheses are thus piecewise linear and convex, and are still simple enough to offer intuitive solutions that are more accurate compared to hyperplanes. In a similar approach, Fisher ([F93], <ref> [F95] </ref>) considers general convex hypotheses. In addition, the VC dimensions of the geometric hypothesis sets that we consider are finite.
Reference: [Ha] <author> D. Haussler, </author> <title> Decision theoretic generations of the PAC-model for neural nets and other applications. </title> <journal> Inf. and Comp., </journal> <volume> 100 (1992), </volume> <pages> 78-150. </pages>
Reference: [Ho] <author> R.C. Holte, </author> <title> Very simple classification rules perform well on most commonly used datasets. </title> <booktitle> Machine Learning, 11 (1993), </booktitle> <pages> 63-91. </pages>
Reference-contexts: There are a lot of experimental results regarding the performance of various heuristic learning algorithms on a number of "benchmark"- datasets for real world classification problems ([BN], <ref> [Ho] </ref>, [Ma], [WK90], [WGT], [WK91]). fl Department of Computer Science, Princeton University, 35 Olden St., Princeton, NJ 08540, USA, e-mail: dpd@cs.princeton.edu.
Reference: [HU] <author> J.E. Hopcroft and J.D. Ullman, </author> <title> Introduction to Automata Theory, Languages and Computation. </title> <publisher> Addison-Wesley (1979). </publisher>
Reference: [KSS] <author> M. Kearns, R.E. Schapire and L.M. Sellie, </author> <title> Toward efficient agnostic learning. </title> <booktitle> 5th ACM Workshop on Computational Learning Theory (1992) 341-352. </booktitle>
Reference-contexts: As Haussler has shown ([Ha]), if a given hypothesis class H has finite VC-dimension then a polynomial algorithm that solves the minimizing disagreement problem for H is a sufficient condition for efficient agnostic PAC-learning with H (see also <ref> [KSS] </ref> and [V]). So our results show efficient (polynomial-time) agnostic PAC-learning with hypothesis sets of k-gons or k-hedra.
Reference: [Ma] <author> W. Maass, </author> <title> Efficient Agnostic PAC-Learning with Simple Hypotheses. </title> <booktitle> 7th Ann. ACM Conference on Computational Learning Theory (1994), </booktitle> <pages> 67-75. </pages>
Reference-contexts: There are a lot of experimental results regarding the performance of various heuristic learning algorithms on a number of "benchmark"- datasets for real world classification problems ([BN], [Ho], <ref> [Ma] </ref>, [WK90], [WGT], [WK91]). fl Department of Computer Science, Princeton University, 35 Olden St., Princeton, NJ 08540, USA, e-mail: dpd@cs.princeton.edu.
Reference: [Ma2] <author> W. Maass, </author> <title> private communication. </title>
Reference: [MWW] <author> J. Matousek, E. Welzl and L. Wernish, </author> <title> Discrepancy and *-approximations for bounded VC-dimension. </title> <booktitle> 25th ACM Symp. on Theory of Computing (1993), </booktitle> <pages> 424-430. </pages>
Reference: [V] <author> L.G. Valiant, </author> <title> A theory of the learnable. </title> <booktitle> Comm. of the ACM 27 (1984), </booktitle> <pages> 1134-1142. </pages>
Reference-contexts: As Haussler has shown ([Ha]), if a given hypothesis class H has finite VC-dimension then a polynomial algorithm that solves the minimizing disagreement problem for H is a sufficient condition for efficient agnostic PAC-learning with H (see also [KSS] and <ref> [V] </ref>). So our results show efficient (polynomial-time) agnostic PAC-learning with hypothesis sets of k-gons or k-hedra.
Reference: [VC] <author> V.N. Vapnik and A.Ya. Chervonenkis, </author> <title> On the uniform convergence of relative frequencies of events to their probabilities. </title> <journal> Theory Probab. Applic. </journal> <volume> 16 (1971), </volume> <pages> 264-280. </pages>
Reference-contexts: The Vapnik-Chervonenkis dimension, or VC-dimension of the set system (S; R) is the maximum size of a shattered subset of S (<ref> [VC] </ref>). The primary shatter function of a set system (S; R) is defined as follows: R (m) = max AS;jAj=m jfR " AjR 2 Rgj. A result by [VC] gives a bound on the shatter function of a set system with VC-dimension ff: R (m) = O (m ff ). Intuitively the VC they can shatter 7. dimension of a set system shows how complicated it is to describe.
Reference: [WGT] <author> S.M. Weiss, R. Galen and P.V. Tadepalli, </author> <title> Maximizing the predictive value of production rules. </title> <booktitle> Art. Int. 45 (1990), </booktitle> <pages> 47-71. </pages>
Reference-contexts: There are a lot of experimental results regarding the performance of various heuristic learning algorithms on a number of "benchmark"- datasets for real world classification problems ([BN], [Ho], [Ma], [WK90], <ref> [WGT] </ref>, [WK91]). fl Department of Computer Science, Princeton University, 35 Olden St., Princeton, NJ 08540, USA, e-mail: dpd@cs.princeton.edu.
Reference: [WK90] <author> S.M. Weiss and I. Kapouleas, </author> <title> An empirical comparison of pattern recognition, neural nets, and machine learning classification methods. </title> <booktitle> 11th Int. Joint Conf. on Art. Int. (1990), </booktitle> <publisher> Morgan Kauffmann, </publisher> <pages> 781-787. </pages>
Reference-contexts: There are a lot of experimental results regarding the performance of various heuristic learning algorithms on a number of "benchmark"- datasets for real world classification problems ([BN], [Ho], [Ma], <ref> [WK90] </ref>, [WGT], [WK91]). fl Department of Computer Science, Princeton University, 35 Olden St., Princeton, NJ 08540, USA, e-mail: dpd@cs.princeton.edu.
Reference: [WK91] <author> S.M. Weiss and C.A. </author> <title> Kulikowski, Computer Systems that Learn (1991), </title> <publisher> Morgan Kauffmann. </publisher>
Reference-contexts: There are a lot of experimental results regarding the performance of various heuristic learning algorithms on a number of "benchmark"- datasets for real world classification problems ([BN], [Ho], [Ma], [WK90], [WGT], <ref> [WK91] </ref>). fl Department of Computer Science, Princeton University, 35 Olden St., Princeton, NJ 08540, USA, e-mail: dpd@cs.princeton.edu.
Reference: [W] <author> E. Welzl, </author> <type> personal communication. </type>
References-found: 23

