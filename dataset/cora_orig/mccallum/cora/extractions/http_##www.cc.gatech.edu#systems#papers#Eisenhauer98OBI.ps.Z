URL: http://www.cc.gatech.edu/systems/papers/Eisenhauer98OBI.ps.Z
Refering-URL: http://www.cs.gatech.edu/people/home/eisen/
Root-URL: 
Title: An Object-Based Infrastructure for Program Monitoring and Steering  
Author: Greg Eisenhauer and Karsten Schwan 
Address: Atlanta, Georgia 30332  
Affiliation: College of Computing Georgia Institute of Technology  
Abstract: Program monitoring and steering systems can provide invaluable insight into the behavior of complex parallel and distributed applications. But the traditional event-stream-based approach to program monitoring does not scale well with increasing complexity. This paper introduces the Mirror Object Model, a new approach for program monitoring and steering systems. This approach provides a higher-level object-based abstraction that links the producer and the consumer of data and provides a seamless model which integrates monitoring and steering computation. We also introduce the Mirror Object Steering System (MOSS), an implementation of the Mirror Object Model based on CORBA-style objects. This paper demonstrates the advantages of MOSS over traditional event-stream-based monitoring systems in handling complex situations. Additionally, we show that the additional functionality of MOSS can be achieved without significant performance penalty. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> CORBAservices: </author> <title> Common Object Services Specification, </title> <booktitle> chapter 4. Object Management Group, </booktitle> <year> 1997. </year> <note> http://www.omg.org. </note>
Reference-contexts: MOSS uses event channels to transport monitoring information to the mirror objects in the interactivity system. Event channels are a publish-subscribe communication mechanism similar to the artifact of the same name described in the CORBA Common Object Services specification <ref> [1] </ref>. However, unlike object-level implementations of the CORBA Event Services specification, our event channels do not rely upon a central object for event distribution. 2 OTL is described further in [3]. Instead, event communication from source to sink occurs directly, regardless of the number or location of sources and sinks.
Reference: [2] <author> Greg Eisenhauer. </author> <title> Portable Self-Describing Binary Data Streams. </title> <type> Technical Report GIT-CC-94-45, </type> <institution> College of Computing, Georgia Institute of Technology, </institution> <year> 1994. </year> <note> (anon. ftp from ftp.cc.gatech.edu). </note>
Reference-contexts: In each of the three systems, this is accomplished by interposing a null network send routine into the monitoring system infrastructure after allowing the monitoring system to establish a normal connection to an external client. In the case of Audobon and MOSS, both of which use PBIO <ref> [2, 4] </ref> for network transmission, this interposition is accomplished 6 Data Transmit Size MOSS Audobon Autopilot Time 12 14.4 s 26.3 s 24.9 s 4.29 s 1024 14.4 33.5 38.0 14.1 102400 14.4 641. 1260. 1510.
Reference: [3] <author> Greg Eisenhauer. </author> <title> An Object Infrastructure for High Performance Interactive Applications. </title> <type> PhD thesis, </type> <institution> Georgia Institute of Technology, </institution> <month> June </month> <year> 1998. </year>
Reference-contexts: an object-like abstraction does not present significant additional difficulties. /* boundary.idl ** ** An object representation of the particle ** domain boundaries. */ interface boundary - attribute float boundary; attribute long index; float get_val (); void put_val (in float value); -; interface domain - attribute long mol_count; attribute long index <ref> [3] </ref>; -; The implementation of object monitoring and remote invocation depends upon the nature of the object system employed in the application. Many object systems provide for remote object invocation. <p> However, unlike object-level implementations of the CORBA Event Services specification, our event channels do not rely upon a central object for event distribution. 2 OTL is described further in <ref> [3] </ref>. Instead, event communication from source to sink occurs directly, regardless of the number or location of sources and sinks. If there are no sinks for a particular channel, no network traffic occurs.
Reference: [4] <author> Greg Eisenhauer, Beth Plale, and Karsten Schwan. DataExchange: </author> <title> High Performance Communication in Distributed Laboratories. </title> <note> to appear in Journal of Parallel Computing, </note> <year> 1998. </year>
Reference-contexts: Instead, event communication from source to sink occurs directly, regardless of the number or location of sources and sinks. If there are no sinks for a particular channel, no network traffic occurs. Event channels and state marshaling are implemented using DataExchange and PBIO <ref> [4] </ref>, communications infrastructure packages which were developed for the Distributed Laboratory environment. Event channels were developed together with MOSS, but they are in no way dependent upon MOSS or the object system and can be used independently. <p> In each of the three systems, this is accomplished by interposing a null network send routine into the monitoring system infrastructure after allowing the monitoring system to establish a normal connection to an external client. In the case of Audobon and MOSS, both of which use PBIO <ref> [2, 4] </ref> for network transmission, this interposition is accomplished 6 Data Transmit Size MOSS Audobon Autopilot Time 12 14.4 s 26.3 s 24.9 s 4.29 s 1024 14.4 33.5 38.0 14.1 102400 14.4 641. 1260. 1510. <p> The most significant obstacle in achieving these gains automatically is the practical difficulty of moving the Update () method code into the application. One promising approach is the use of dynamic code generation, such as is employed in MDL in the Para-dyn project [10]. As described and measured in <ref> [4] </ref>, PBIO is already capable of using dynamic code generation to create customized data conversion routines. These generated routines must be able to access and store data elements, convert elements between basic types and call subroutines to convert complex subtypes.
Reference: [5] <author> I. Foster, C. Kesselman, and S. Tuecke. </author> <title> The Nexus Approach to Integrating Multithreading and Communication. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <pages> pages 70-82, </pages> <year> 1996. </year>
Reference-contexts: Network transmission times included for reference. by using set interface IOfile () to modify the basic functions through which PBIO accesses the network and supplying a write func which returned a success code. In the case of Autopilot, which uses Nexus <ref> [5] </ref> for network transmission, internal Nexus data structures were traversed and the protocols send rsr function pointer was changed.
Reference: [6] <author> Weiming Gu. </author> <title> On-line Monitoring and Interactive Steering of Large-Scale Parallel and Distributed Applications. </title> <type> PhD thesis, </type> <institution> Georgia Institute of Technology, </institution> <address> At-lanta, GA 30332, </address> <year> 1995. </year>
Reference-contexts: But even for applications written in procedural languages, the mirror object approach is still valid. Any simple procedural program will require modification in order to support application-level monitoring and steering. Systems such as Falcon <ref> [6] </ref> and Progress [18] require the insertion of code snippets into the application at appropriate points. A similar approach which is more agreeable with the object-based nature of the interactivity system is to "objectify" small portions of the application to enable them for steering. <p> For example, Figure 2 depicts different update specifications applied to each attribute in the mirrored object. Attribute update specifications might explore the semantic variations that were done with monitoring sensors in <ref> [6] </ref>, that is, variable-rate tracing sensors and sampling sensors. The analogous techniques in MOSS would involve arranging instrumentation such that object state is reported less frequently than at every change (such as every N th change) or that data values be reported at regular time intervals regardless of change. <p> The analogous techniques in MOSS would involve arranging instrumentation such that object state is reported less frequently than at every change (such as every N th change) or that data values be reported at regular time intervals regardless of change. Because these techniques were evaluated in detail in <ref> [6] </ref>, they are not considered further here. However they are very appropriate for MOSS and may be employed in the future to further reduce monitoring overheads. Implementation. In CORBA, access to object attributes occurs only through get/set routines that are generated by the compiler. <p> Other potentially useful techniques include the buffering of updates, periodic sampling instead of push-style update generation, and the local reduction of data through averaging and other means. These facilities were explored in Falcon <ref> [6] </ref> and are available in Autopilot. It is possible to employ such techniques to the object state update mechanism in MOSS as well. Additionally, one could reduce overheads in MOSS by only transmitting changed attributes when an update occurs.
Reference: [7] <author> Weiming Gu, Greg Eisenhauer, and Karsten Schwan. </author> <title> Falcon: On-line Monitoring and Steering of Parallel Programs. </title> <note> to appear in Concurrency: Practice and Experience. </note>
Reference-contexts: Prior research in program monitoring has developed mechanisms for controlling the overall flow of data. But some data reduction schemes, such as on-off switches on monitoring sensors, do not lend themselves well to situations where there is more than one end-user <ref> [7, 17, 10] </ref>. In any case, such data suppression mechanisms are of little use if the consumers really need all of the data being generated. <p> That is, they treat the application and monitoring clients as processes linked by a stream of data records. Some event-stream-based systems can direct or filter monitoring data records by type, but they generally do not directly support content-based event routing <ref> [7, 17] </ref>. This limitation simplifies the event handling infrastructure, but such systems cannot handle the selective data distribution necessary to do data-parallel monitoring distribution. <p> The approach taken in MOSS is to use a CORBA-style object system. Since remote invocations are part of basic CORBA functionality, this provides us with our steering mechanism. However, if such a system was not available, one could resort to program annotation (such as Falcon <ref> [7] </ref> and Progress [18]), code transformation (as in Pablo [14]) or, if sufficient information was available in the program, dynamic instrumentation techniques (such as those used in Paradyn [10]) to enable monitoring. Our MOSS implementation follows the style common to CORBA Object Resource Broker implementations. <p> In order to address this concern, this section examines the computational costs of a basic monitoring operation in MOSS as well as two other systems which involve monitoring. The first of these external systems is Audobon, a variant of the Falcon <ref> [7] </ref> monitoring system developed at Georgia Tech. Audobon was chosen because it is a basic eventstream-based monitoring system. The second external system chosen for evaluation is Autopilot [16], a program monitoring and steering system recently developed at the University of Illinois.
Reference: [8] <editor> Rick Jones. NetPerf: </editor> <title> A Network Performance Benchmark, Revision 2.1. </title> <address> http://www.cup.hp.com/netperf/ NetperfPage.html, </address> <month> February </month> <year> 1996. </year>
Reference-contexts: Bandwidth measurements were made with NetPerf <ref> [8] </ref>. transport system might significantly reduce this overhead. In Audobon, the additional overhead as data size increases can also be traced to a copy operation. Audobon, like Falcon, has sensors which accept data as subroutine parameters.
Reference: [9] <author> Thomas Kindler, Karsten Schwan, Dilma Silva, Mary Trauner, and Fred Alyea. </author> <title> A Parallel Spectral Model for Atmospheric Transport Processes. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 8(9) </volume> <pages> 639-666, </pages> <month> November </month> <year> 1996. </year>
Reference-contexts: As an illustrative example of the type of application targeted by Distributed Labs, consider a long-running scientific simulation, such as a global climate model described in <ref> [9] </ref>. This atmospheric model achieves high simulation speeds by distributing the simulation of different atmospheric layers across processors on an SMP or network of workstations. In a Distributed Laboratory, the progress of this application can be monitored and controlled by multiple scientists at spatially distributed locations.
Reference: [10] <author> Barton P. Miller, Mark D. Callaghan, Jonathan M. Cargille, Jeffrey K. Hollingsworth, R. Bruce Irvin, Karen L. Karavanic, Krishna Kunchithapadam, and Tia Newhall. </author> <title> The Paradyn Parallel Performance Measurement Tools. </title> <booktitle> IEEE Computer, </booktitle> <year> 1995. </year> <month> 10 </month>
Reference-contexts: Prior research in program monitoring has developed mechanisms for controlling the overall flow of data. But some data reduction schemes, such as on-off switches on monitoring sensors, do not lend themselves well to situations where there is more than one end-user <ref> [7, 17, 10] </ref>. In any case, such data suppression mechanisms are of little use if the consumers really need all of the data being generated. <p> This limitation simplifies the event handling infrastructure, but such systems cannot handle the selective data distribution necessary to do data-parallel monitoring distribution. In addition, some systems draw a sharp distinction between the application and monitoring clients, making it difficult to build multi-level data reduction schemes <ref> [10, 11] </ref>. 2.1 Goals for a New Model Based on the difficulties described above, we established goals for a new model for program monitoring and steering systems: * scale gracefully with increasing complexity in both the application and the monitoring system; * provide for automatic control of monitoring data col lection <p> However, if such a system was not available, one could resort to program annotation (such as Falcon [7] and Progress [18]), code transformation (as in Pablo [14]) or, if sufficient information was available in the program, dynamic instrumentation techniques (such as those used in Paradyn <ref> [10] </ref>) to enable monitoring. Our MOSS implementation follows the style common to CORBA Object Resource Broker implementations. Object interface declarations, such as the example given in Figure 3 are written in IDL. <p> The most significant obstacle in achieving these gains automatically is the practical difficulty of moving the Update () method code into the application. One promising approach is the use of dynamic code generation, such as is employed in MDL in the Para-dyn project <ref> [10] </ref>. As described and measured in [4], PBIO is already capable of using dynamic code generation to create customized data conversion routines. These generated routines must be able to access and store data elements, convert elements between basic types and call subroutines to convert complex subtypes.
Reference: [11] <author> B. Mohr, A. Malony, and J. Cuny. </author> <title> TAU. </title> <editor> In G. Wilson, editor, </editor> <title> Parallel Programming using C++. </title> <publisher> MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: This limitation simplifies the event handling infrastructure, but such systems cannot handle the selective data distribution necessary to do data-parallel monitoring distribution. In addition, some systems draw a sharp distinction between the application and monitoring clients, making it difficult to build multi-level data reduction schemes <ref> [10, 11] </ref>. 2.1 Goals for a New Model Based on the difficulties described above, we established goals for a new model for program monitoring and steering systems: * scale gracefully with increasing complexity in both the application and the monitoring system; * provide for automatic control of monitoring data col lection
Reference: [12] <author> Bodhi Mukherjee and Karsten Schwan. </author> <title> Experiments with a Configurable Lock for Multiprocessors. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <publisher> Michigan, </publisher> <pages> pages 205-208. </pages> <publisher> IEEE, </publisher> <month> Aug. </month> <year> 1993. </year>
Reference-contexts: Similarly, simple algorithmic steering operations may be moved from the interactivity system directly into the application. Because latency can be a significant limitation on the applicability of algorithmic steering, migrating externally-specified steering operations directly into the application can open new domains for program steering. For example, <ref> [12] </ref> describes mutex locks that adapt themselves to the most efficient mode of operation based on their usage pattern.
Reference: [13] <author> Beth Plale, Greg Eisenhauer, Jeremy Heiner, Vernard Martin, Karsten Schwan, and Jeffrey Vetter. </author> <title> From Interactive Applications to Distributed Laboratories. </title> <journal> IEEE Concurrency, </journal> <year> 1998. </year>
Reference-contexts: Additionally, we show that the added functionality of MOSS can be achieved without incurring significant performance penalty. 2 Influences and Goals The design of the Mirror Object Model was influenced by the Distributed Laboratories Project <ref> [13] </ref> at Georgia Tech and its goals of supporting complex and collaborative interaction with long-running distributed and parallel applications. As an illustrative example of the type of application targeted by Distributed Labs, consider a long-running scientific simulation, such as a global climate model described in [9].
Reference: [14] <author> Daniel A. Reed, Ruth A. Aydt, Roger J. Noe, Phillip C. Roth, Keith A. Shields, Bradley Schwartz, and Luis F. Tavera. </author> <title> Scalable Performance Analysis: The Pablo Performance Analysis Environment. </title> <booktitle> In Proceedings of the Scalable Parallel Libraries Conference. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <year> 1993. </year>
Reference-contexts: Since remote invocations are part of basic CORBA functionality, this provides us with our steering mechanism. However, if such a system was not available, one could resort to program annotation (such as Falcon [7] and Progress [18]), code transformation (as in Pablo <ref> [14] </ref>) or, if sufficient information was available in the program, dynamic instrumentation techniques (such as those used in Paradyn [10]) to enable monitoring. Our MOSS implementation follows the style common to CORBA Object Resource Broker implementations.
Reference: [15] <author> Daniel A. Reed, Christopher L. Elford, Tara M Mad-hyastha, Evgenia Smirni, and Stephen E. </author> <title> Lamm. The Next Frontier: Interactive and Closed Loop Performance Steering. </title> <booktitle> In Proceedings of the 1996 ICPP Workshop, </booktitle> <month> august </month> <year> 1996. </year>
Reference-contexts: The Mirror Object Model also allows the implementation of important optimizations which have the potential to open new ground to program monitoring and steering. Additionally, while this paper concentrated on scalability concerns, MOSS has significant advantages in programmability and in targeting the dynamic applications described in <ref> [15] </ref>. Future papers will explore these issues. MOSS runs on a variety of platforms including Sun Sparc SunOS 4.1.3, Sun Sparc Solaris 2.x SGI MIPS IRIX 5.x, SGI MIPS (32 and 64-bit) IRIX 6.x, IBM RS6000 AIX 3.2, x86 Linux, x86 Solaris 2.x, and x86 Windows NT.
Reference: [16] <author> Randy L. Ribler, Huseyin Simitci, and Daniel A. Reed. </author> <title> The Autopilot Performance-Directed Adaptive Control System. Future Generation Computer Systems, special issue (Performance Data Mining), </title> <note> submitted for publication, </note> <month> November </month> <year> 1997. </year>
Reference-contexts: In general, such modifications cannot be safely performed without some application-level synchronization. Several approaches to steering synchronization have been proposed. Progress [17] performs steering actions only at specific execution points. Autopilot <ref> [16] </ref> allows steering updates to be enabled and disabled on an object-by-object basis. <p> In other systems, such as in our initial CORBA-based MOSS implementation, synchronization is entirely up to the application programmer. This flexibility allows MOSS to leverage already-programmed synchronization code where available, as well as enabling it to emulate the synchronization styles of Progress [18] and Autopilot <ref> [16] </ref> with application-level code. 4.2 In the Interactivity System The issues involved in creating clients in the interactivity system are very similar to those described in Section 4.1. <p> The first of these external systems is Audobon, a variant of the Falcon [7] monitoring system developed at Georgia Tech. Audobon was chosen because it is a basic eventstream-based monitoring system. The second external system chosen for evaluation is Autopilot <ref> [16] </ref>, a program monitoring and steering system recently developed at the University of Illinois. Like MOSS, Autopilot is object-based and implements per-object data routing facilities.
Reference: [17] <author> Jeffrey Vetter. </author> <title> Computational Steering Annotated Bibliography. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 36(6), </volume> <year> 1997. </year>
Reference-contexts: Prior research in program monitoring has developed mechanisms for controlling the overall flow of data. But some data reduction schemes, such as on-off switches on monitoring sensors, do not lend themselves well to situations where there is more than one end-user <ref> [7, 17, 10] </ref>. In any case, such data suppression mechanisms are of little use if the consumers really need all of the data being generated. <p> That is, they treat the application and monitoring clients as processes linked by a stream of data records. Some event-stream-based systems can direct or filter monitoring data records by type, but they generally do not directly support content-based event routing <ref> [7, 17] </ref>. This limitation simplifies the event handling infrastructure, but such systems cannot handle the selective data distribution necessary to do data-parallel monitoring distribution. <p> Unlike monitoring, which is implicitly synchronous, steering involves modifying the state of the running application by an external agent. In general, such modifications cannot be safely performed without some application-level synchronization. Several approaches to steering synchronization have been proposed. Progress <ref> [17] </ref> performs steering actions only at specific execution points. Autopilot [16] allows steering updates to be enabled and disabled on an object-by-object basis.
Reference: [18] <author> J.S. Vetter and K. Schwan. </author> <title> Progress: a toolkit for interactive program steering. </title> <booktitle> In Proc. 1995 Int'l Conf. on Parallel Processing, </booktitle> <pages> pages II/139-42, </pages> <year> 1995. </year> <month> 11 </month>
Reference-contexts: But even for applications written in procedural languages, the mirror object approach is still valid. Any simple procedural program will require modification in order to support application-level monitoring and steering. Systems such as Falcon [6] and Progress <ref> [18] </ref> require the insertion of code snippets into the application at appropriate points. A similar approach which is more agreeable with the object-based nature of the interactivity system is to "objectify" small portions of the application to enable them for steering. <p> The approach taken in MOSS is to use a CORBA-style object system. Since remote invocations are part of basic CORBA functionality, this provides us with our steering mechanism. However, if such a system was not available, one could resort to program annotation (such as Falcon [7] and Progress <ref> [18] </ref>), code transformation (as in Pablo [14]) or, if sufficient information was available in the program, dynamic instrumentation techniques (such as those used in Paradyn [10]) to enable monitoring. Our MOSS implementation follows the style common to CORBA Object Resource Broker implementations. <p> In other systems, such as in our initial CORBA-based MOSS implementation, synchronization is entirely up to the application programmer. This flexibility allows MOSS to leverage already-programmed synchronization code where available, as well as enabling it to emulate the synchronization styles of Progress <ref> [18] </ref> and Autopilot [16] with application-level code. 4.2 In the Interactivity System The issues involved in creating clients in the interactivity system are very similar to those described in Section 4.1.
References-found: 18

