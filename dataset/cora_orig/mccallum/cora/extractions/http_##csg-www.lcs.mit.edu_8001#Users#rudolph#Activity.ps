URL: http://csg-www.lcs.mit.edu:8001/Users/rudolph/Activity.ps
Refering-URL: http://csg-www.lcs.mit.edu:8001/Users/rudolph/
Root-URL: 
Email: feit@watson.ibm.com  rudolph@cs.huji.ac.il  
Title: Coscheduling Based on Run-Time Identification of Activity Working Sets  
Author: Dror G. Feitelson P. O. Box Larry Rudolph 
Keyword: coscheduling, gang scheduling, on-line algorithms, activity working set.  
Address: Heights, NY 10598  91904 Jerusalem, Israel  
Affiliation: IBM T. J. Watson Research Center  Yorktown  Institute of Computer Science The Hebrew University of Jerusalem  
Abstract: This paper introduces a method for runtime identification of sets of interacting activities ("working sets") with the purpose of coscheduling them, i.e. scheduling them so that all the activities in the set execute simultaneously on distinct processors. The identification is done by monitoring access rates to shared communication objects: activities that access the same objects at a high rate thereby interact frequently, and therefore would benefit from coscheduling. Simulation results show that coscheduling with our runtime identification scheme can give better performance than uncoordinated scheduling based on a single global activity queue. The finer-grained the interactions among the activities in a working set, the better the performance differential. Moreover, coscheduling based on automatic runtime identification achieves about the same performance as coscheduling based on manual identification of working sets by the programmer. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. L. Black, </author> <title> "Scheduling support for concurrency and parallelism in the Mach operating system". </title> <booktitle> Computer 23(5), </booktitle> <pages> pp. 35-43, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: The Mach scheduler also allows processors to be allocated to jobs dynamically, but performs the scheduling of activities on these processors in the kernel rather than leaving it to the application <ref> [1] </ref>. Like the other dynamic partitioning schemes, it does not require a one-to-one relation between activities and processors, thereby violating the original concepts as introduced by Ousterhout for coscheduling. The term "gang scheduling" has nevertheless been applied to this system as well.
Reference: [2] <author> J. B la_zewicz, M. Drabowski, and J. W~eglarz, </author> <title> "Scheduling multiprocessor tasks to minimize schedule length". </title> <journal> IEEE Trans. Comput. </journal> <volume> C-35(5), </volume> <pages> pp. 389-393, </pages> <month> May </month> <year> 1986. </year>
Reference-contexts: In these systems, gangs are typically defined to include all the activities (or processes in their terminology) in the application. Off-line algorithms to find an optimal gang schedule were presented by B la_zewicz et al. <ref> [2] </ref>. Feitelson and Rudolph proposed a scalable implementation of gang scheduling based on buddy systems [9, 11, 12]. In these works gangs are also predefined, even though they are not necessarily identical to jobs (i.e., a job can have multiple independent gangs).
Reference: [3] <author> R. H. Campbell, N. Islam, and P. Madany, </author> <title> "Choices, frameworks and refinement". </title> <booktitle> Computing Systems 5(5), </booktitle> <pages> pp. 217-257, </pages> <month> Summer </month> <year> 1992. </year>
Reference-contexts: In the sequel, we shall sometimes use "gang" as a synonym for "working set". Coscheduling was introduced by Ousterhout [27], who also implemented it in the Medusa system on CM fl . Since then, gang scheduling was implemented in a number of additional experimental systems <ref> [15, 29, 3, 10, 4] </ref>. Gang scheduling is also provided by at least five commercial systems: the Alliant FX/8 [31], the Connection Machine CM-5 [21], the Meiko CS-2 [8], the Intel Paragon [8], and the Silicon Graphics Challenge.
Reference: [4] <author> E. M. Chaves Filho and V. C. Barbosa, </author> <title> "Time sharing in hypercube multiprocessors". </title> <booktitle> In 4th IEEE Symp. Parallel & Distributed Processing, </booktitle> <pages> pp. 354-359, </pages> <month> Dec </month> <year> 1992. </year>
Reference-contexts: In the sequel, we shall sometimes use "gang" as a synonym for "working set". Coscheduling was introduced by Ousterhout [27], who also implemented it in the Medusa system on CM fl . Since then, gang scheduling was implemented in a number of additional experimental systems <ref> [15, 29, 3, 10, 4] </ref>. Gang scheduling is also provided by at least five commercial systems: the Alliant FX/8 [31], the Connection Machine CM-5 [21], the Meiko CS-2 [8], the Intel Paragon [8], and the Silicon Graphics Challenge.
Reference: [5] <author> E. G. Coffman, Jr., M. R. Garey, and D. S. Johnson, </author> <title> "Approximation algorithms for bin-packing | an updated survey". In Algorithm Design for Computer Systems Design, </title> <editor> G. Ausiello, M. Lucertini, and P. </editor> <booktitle> Serafini (eds.), </booktitle> <pages> pp. 49-106, </pages> <publisher> Springer-Verlag, </publisher> <year> 1984. </year>
Reference-contexts: Two simple optimizations are possible to reduce the fragmentation. First, the working sets can be sorted in decreasing size, leading to first-fit-decreasing bin packing, which is more space-efficient <ref> [5] </ref>. Second, alternative scheduling can be overlapped with coscheduling to the degree possible by PEs that are left over in the coscheduling slots.
Reference: [6] <author> J. Edler, A. Gottlieb, C. P. Kruskal, K. P. McAuliffe, L. Rudolph, M. Snir, P. J. Teller, and J. Wilson, </author> <title> "Issues related to MIMD shared-memory computers: the NYU Ultra-computer approach". </title> <booktitle> In 12th Ann. Intl. Symp. Computer Architecture Conf. Proc., </booktitle> <pages> pp. 126-135, </pages> <year> 1985. </year>
Reference-contexts: a single global activity queue (or workpile) is maintained, and each processor removes the next activity from the workpile, executes it for a whole time quantum, and returns it to the end of the workpile | the scheduling of one activity is uncoordinated with the scheduling of any other activity <ref> [6, 17] </ref>. The subject of this paper is how to identify the activities that constitute a working set. The analogy with uniprocessor memory management is not so helpful here. The memory working set is approximated by using the least recently used (LRU) paradigm [20].
Reference: [7] <author> D. G. Feitelson, </author> <title> Communicators: Object-Based Multiparty Interactions for Parallel Programming. </title> <type> Technical Report 91-12, </type> <institution> Dept. Computer Science, The Hebrew University of Jerusalem, </institution> <month> Nov </month> <year> 1991. </year> <title> Activity Working Sets Last Edit: </title> <address> Dec 9, </address> <year> 1994 </year> <month> 22 </month>
Reference-contexts: The communication objects can be named channels as in Occam [18], named barrier synchronization points, or named multiparty interactions such as scripts [13] or communicators <ref> [7] </ref>. Their role is to help expose the patterns in which activities interact and to identify the activity working set; activities which use or access the same objects are known to interact, so it is safe to conjecture that they are in the same activity working set. <p> In some environments, this assumption is directly valid. For example, the named channels used for message passing in Occam [18], and practically all proposed notations for multiparty interactions, e.g. scripts [13] and communicators <ref> [7] </ref>, are objects known to the system. In systems that provide group communication primitives such as broadcast, multicast, and barrier synchronization, the system calls used to implement these services may be used to identify the interactions (but note that operations in distinct groups must be distinguished) [25].
Reference: [8] <author> D. G. Feitelson, </author> <title> A Survey of Scheduling in Multiprogrammed Parallel Systems. </title> <type> Research Report RC 19790 (87657), </type> <institution> IBM T. J. Watson Research Center, </institution> <month> Oct </month> <year> 1994. </year>
Reference-contexts: Since then, gang scheduling was implemented in a number of additional experimental systems [15, 29, 3, 10, 4]. Gang scheduling is also provided by at least five commercial systems: the Alliant FX/8 [31], the Connection Machine CM-5 [21], the Meiko CS-2 <ref> [8] </ref>, the Intel Paragon [8], and the Silicon Graphics Challenge. In these systems, gangs are typically defined to include all the activities (or processes in their terminology) in the application. Off-line algorithms to find an optimal gang schedule were presented by B la_zewicz et al. [2]. <p> Since then, gang scheduling was implemented in a number of additional experimental systems [15, 29, 3, 10, 4]. Gang scheduling is also provided by at least five commercial systems: the Alliant FX/8 [31], the Connection Machine CM-5 [21], the Meiko CS-2 <ref> [8] </ref>, the Intel Paragon [8], and the Silicon Graphics Challenge. In these systems, gangs are typically defined to include all the activities (or processes in their terminology) in the application. Off-line algorithms to find an optimal gang schedule were presented by B la_zewicz et al. [2].
Reference: [9] <author> D. G. Feitelson and L. Rudolph, </author> <title> "Distributed hierarchical control for parallel processing". </title> <booktitle> Computer 23(5), </booktitle> <pages> pp. 65-77, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: But it has usually been assumed that the working sets are defined by the user [27]. For example, the syntactic structure of the language may be used, by defining the set of activities that are spawned together by a single parallel construct to be a working set (Fig. 1) <ref> [9] </ref>. We suggest an automatic system that can identify the activity working set by using access rates to shared communication objects to gather information about interaction patterns at runtime. <p> Off-line algorithms to find an optimal gang schedule were presented by B la_zewicz et al. [2]. Feitelson and Rudolph proposed a scalable implementation of gang scheduling based on buddy systems <ref> [9, 11, 12] </ref>. In these works gangs are also predefined, even though they are not necessarily identical to jobs (i.e., a job can have multiple independent gangs). This paper is the first to consider the possibility of automatically grouping the activities in a job into gangs at run time. <p> Pairs that block sooner than others must nevertheless wait for those that manage to do more work. This can only be solved by partitioning the machine and doing gang scheduling independently on the partitions, as suggested in the Distributed Hierarchical Control scheme <ref> [9] </ref>. With uncoordinated scheduling, the overhead is constant regardless of the granularity of the interactions. For fine-grain interactions, the relative overhead is very large, leading to poor performance. For coarse-grain interactions, the overhead is acceptable and overall performance is similar to that of coscheduling.
Reference: [10] <author> D. G. Feitelson and L. Rudolph, </author> <title> "Gang scheduling performance benefits for fine-grain synchronization". </title> <journal> J. Parallel & Distributed Comput. </journal> <volume> 16(4), </volume> <pages> pp. 306-318, </pages> <month> Dec </month> <year> 1992. </year>
Reference-contexts: In the sequel, we shall sometimes use "gang" as a synonym for "working set". Coscheduling was introduced by Ousterhout [27], who also implemented it in the Medusa system on CM fl . Since then, gang scheduling was implemented in a number of additional experimental systems <ref> [15, 29, 3, 10, 4] </ref>. Gang scheduling is also provided by at least five commercial systems: the Alliant FX/8 [31], the Connection Machine CM-5 [21], the Meiko CS-2 [8], the Intel Paragon [8], and the Silicon Graphics Challenge. <p> Under certain circumstances, its use can be extended to large distributed-memory machines as well [26, 24]. A number of papers have evaluated the performance implications of coscheduling and gang scheduling, and compared them with dynamic partitioning and other scheduling policies for multiprogrammed multiprocessors <ref> [10, 28, 22, 16, 35] </ref>. The Mach scheduler also allows processors to be allocated to jobs dynamically, but performs the scheduling of activities on these processors in the kernel rather than leaving it to the application [1]. <p> The choice between busy waiting and blocking depends on the granularity of interactions: if the time between interactions is small, so is the expected waiting time, and waiting is better than blocking. Coscheduling is specifically targeted to support such fine-grain interactions <ref> [27, 10] </ref>. An immediate consequence is that coarse-grain interactions do not require coscheduling. For example, if the interval between successive interactions is longer than the scheduling time quantum, the price for blocking is paid anyway.
Reference: [11] <author> D. G. Feitelson and L. Rudolph, </author> <title> "Mapping and scheduling in a shared parallel environment using distributed hierarchical control". </title> <booktitle> In Intl. Conf. Parallel Processing, </booktitle> <volume> vol. I, </volume> <pages> pp. 1-8, </pages> <month> Aug </month> <year> 1990. </year>
Reference-contexts: Off-line algorithms to find an optimal gang schedule were presented by B la_zewicz et al. [2]. Feitelson and Rudolph proposed a scalable implementation of gang scheduling based on buddy systems <ref> [9, 11, 12] </ref>. In these works gangs are also predefined, even though they are not necessarily identical to jobs (i.e., a job can have multiple independent gangs). This paper is the first to consider the possibility of automatically grouping the activities in a job into gangs at run time.
Reference: [12] <author> D. G. Feitelson and L. Rudolph, </author> <title> "Wasted resources in gang scheduling". </title> <booktitle> In 5th Jerusalem Conf. Information Technology, </booktitle> <pages> pp. 127-136, </pages> <publisher> IEEE Computer Society Press, </publisher> <month> Oct </month> <year> 1990. </year>
Reference-contexts: Off-line algorithms to find an optimal gang schedule were presented by B la_zewicz et al. [2]. Feitelson and Rudolph proposed a scalable implementation of gang scheduling based on buddy systems <ref> [9, 11, 12] </ref>. In these works gangs are also predefined, even though they are not necessarily identical to jobs (i.e., a job can have multiple independent gangs). This paper is the first to consider the possibility of automatically grouping the activities in a job into gangs at run time.
Reference: [13] <author> N. Francez, B. Hailpern, and G. Taubenfeld, </author> <title> "Script: a communication abstraction mechanism". </title> <journal> Sci. Comput. Programming 6(1), </journal> <pages> pp. 35-88, </pages> <month> Jan </month> <year> 1986. </year>
Reference-contexts: The communication objects can be named channels as in Occam [18], named barrier synchronization points, or named multiparty interactions such as scripts <ref> [13] </ref> or communicators [7]. Their role is to help expose the patterns in which activities interact and to identify the activity working set; activities which use or access the same objects are known to interact, so it is safe to conjecture that they are in the same activity working set. <p> In some environments, this assumption is directly valid. For example, the named channels used for message passing in Occam [18], and practically all proposed notations for multiparty interactions, e.g. scripts <ref> [13] </ref> and communicators [7], are objects known to the system. In systems that provide group communication primitives such as broadcast, multicast, and barrier synchronization, the system calls used to implement these services may be used to identify the interactions (but note that operations in distinct groups must be distinguished) [25].
Reference: [14] <author> D. Ghosal, G. Serazzi, and S. K. Tripathi, </author> <title> "The processor working set and its use in scheduling multiprocessor systems". </title> <journal> IEEE Trans. Softw. Eng. </journal> <volume> 17(5), </volume> <pages> pp. 443-453, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Despite the identical nomenclature, we shall reserve the terms gang scheduling and coscheduling for policies where the number of processors used is Activity Working Sets Last Edit: Dec 9, 1994 5 equal to the number of activities. Finally, we note that although Ghosal et al. <ref> [14] </ref> suggests multiprocessor scheduling algorithms based on a concept they call the "processor working set", their work is totally unrelated to Ousterhout's coscheduling definitions. 2 Runtime Identification of Activity Working Sets This section describes how our system identifies activity working sets and how they are scheduled.
Reference: [15] <author> B. C. Gorda and E. D. Brooks III, </author> <title> Gang Scheduling a Parallel Machine. </title> <type> Technical Report UCRL-JC-107020, </type> <institution> Lawrence Livermore National Laboratory, </institution> <month> Dec </month> <year> 1991. </year>
Reference-contexts: In the sequel, we shall sometimes use "gang" as a synonym for "working set". Coscheduling was introduced by Ousterhout [27], who also implemented it in the Medusa system on CM fl . Since then, gang scheduling was implemented in a number of additional experimental systems <ref> [15, 29, 3, 10, 4] </ref>. Gang scheduling is also provided by at least five commercial systems: the Alliant FX/8 [31], the Connection Machine CM-5 [21], the Meiko CS-2 [8], the Intel Paragon [8], and the Silicon Graphics Challenge.
Reference: [16] <author> A. Gupta, A. Tucker, and S. Urushibara, </author> <title> "The impact of operating system scheduling policies and synchronization methods on the performance of parallel applications". </title> <booktitle> In SIGMETRICS Conf. Measurement & Modeling of Comput. Syst., </booktitle> <pages> pp. 120-132, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Under certain circumstances, its use can be extended to large distributed-memory machines as well [26, 24]. A number of papers have evaluated the performance implications of coscheduling and gang scheduling, and compared them with dynamic partitioning and other scheduling policies for multiprogrammed multiprocessors <ref> [10, 28, 22, 16, 35] </ref>. The Mach scheduler also allows processors to be allocated to jobs dynamically, but performs the scheduling of activities on these processors in the kernel rather than leaving it to the application [1].
Reference: [17] <author> S. F. Hummel and E. Schonberg, </author> <title> "Low-overhead scheduling of nested parallelism". </title> <journal> IBM J. Res. Dev. </journal> <volume> 35(5/6), </volume> <pages> pp. 743-765, </pages> <month> Sep/Nov </month> <year> 1991. </year>
Reference-contexts: a single global activity queue (or workpile) is maintained, and each processor removes the next activity from the workpile, executes it for a whole time quantum, and returns it to the end of the workpile | the scheduling of one activity is uncoordinated with the scheduling of any other activity <ref> [6, 17] </ref>. The subject of this paper is how to identify the activities that constitute a working set. The analogy with uniprocessor memory management is not so helpful here. The memory working set is approximated by using the least recently used (LRU) paradigm [20].
Reference: [18] <author> INMOS Ltd., </author> <title> Occam Programming Manual. </title> <publisher> Prentice-Hall, </publisher> <year> 1984. </year>
Reference-contexts: We suggest an automatic system that can identify the activity working set by using access rates to shared communication objects to gather information about interaction patterns at runtime. The communication objects can be named channels as in Occam <ref> [18] </ref>, named barrier synchronization points, or named multiparty interactions such as scripts [13] or communicators [7]. <p> We assume that the interactions are mediated by communication objects (CO) that are known to the system, and do not change 1 . In some environments, this assumption is directly valid. For example, the named channels used for message passing in Occam <ref> [18] </ref>, and practically all proposed notations for multiparty interactions, e.g. scripts [13] and communicators [7], are objects known to the system.
Reference: [19] <author> A. R. Karlin, K. Li, M. S. Manasse, and S. Owicki, </author> <title> "Empirical studies of competitive spinning for a shared-memory multiprocessor". </title> <booktitle> In 13th Symp. Operating Systems Principles, </booktitle> <pages> pp. 41-55, </pages> <month> Oct </month> <year> 1991. </year>
Reference-contexts: Only activities that are ready to run are placed in the queue. Each activity is scheduled independently, and receives a full scheduling time quantum for execution. Competitive two-phase blocking is used for the interactions <ref> [27, 19] </ref>. Thus when an activity waits for an interaction, it first busy-waits for a duration equal to the context-switch overhead, and then it suspends execution and yields its processor. If the activity's time quantum expires while it is busy-waiting, it is suspended at that time.
Reference: [20] <author> S. </author> <title> Krakowiak, </title> <booktitle> Principles of Operating Systems. </booktitle> <publisher> MIT Press, </publisher> <year> 1988. </year>
Reference-contexts: The subject of this paper is how to identify the activities that constitute a working set. The analogy with uniprocessor memory management is not so helpful here. The memory working set is approximated by using the least recently used (LRU) paradigm <ref> [20] </ref>. Basically, this assumes that the past is indicative of the future and hence it may be expected that the next pages that the application will need are exactly those that it used most recently.
Reference: [21] <author> C. E. Leiserson, Z. S. Abuhamdeh, D. C. Douglas, C. R. Feynman, M. N. Ganmukhi, J. V. Hill, W. D. Hillis, B. C. Kuszmaul, M. A. St. Pierre, D. S. Wells, M. C. Wong, S-W. Yang, and R. Zak, </author> <title> "The network architecture of the Connection Machine CM-5". </title> <booktitle> In 4th Symp. Parallel Algorithms & Architectures, </booktitle> <pages> pp. 272-285, </pages> <month> Jun </month> <year> 1992. </year> <title> Activity Working Sets Last Edit: </title> <address> Dec 9, </address> <year> 1994 </year> <month> 23 </month>
Reference-contexts: Since then, gang scheduling was implemented in a number of additional experimental systems [15, 29, 3, 10, 4]. Gang scheduling is also provided by at least five commercial systems: the Alliant FX/8 [31], the Connection Machine CM-5 <ref> [21] </ref>, the Meiko CS-2 [8], the Intel Paragon [8], and the Silicon Graphics Challenge. In these systems, gangs are typically defined to include all the activities (or processes in their terminology) in the application. Off-line algorithms to find an optimal gang schedule were presented by B la_zewicz et al. [2].
Reference: [22] <author> S. T. Leutenegger and M. K. Vernon, </author> <title> "The performance of multiprogrammed multiprocessor scheduling policies". </title> <booktitle> In SIGMETRICS Conf. Measurement & Modeling of Comput. Syst., </booktitle> <pages> pp. 226-236, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Under certain circumstances, its use can be extended to large distributed-memory machines as well [26, 24]. A number of papers have evaluated the performance implications of coscheduling and gang scheduling, and compared them with dynamic partitioning and other scheduling policies for multiprogrammed multiprocessors <ref> [10, 28, 22, 16, 35] </ref>. The Mach scheduler also allows processors to be allocated to jobs dynamically, but performs the scheduling of activities on these processors in the kernel rather than leaving it to the application [1].
Reference: [23] <author> C. McCann, R. Vaswani, and J. Zahorjan, </author> <title> "A dynamic processor allocation policy for multiprogrammed shared-memory multiprocessors". </title> <journal> ACM Trans. Comput. Syst. </journal> <volume> 11(2), </volume> <pages> pp. 146-178, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Therefore a two-level scheduling scheme is used: the operating system is responsible for the allocation of processors, while the application itself schedules activities on them <ref> [32, 23] </ref>. Two-level scheduling with dynamic partitioning has been shown to be an effective scheduling scheme for small-scale uniform memory access (UMA) multiprocessors, provided applications are coded in a style that can tolerate dynamic changes in the number of processors at runtime.
Reference: [24] <author> C. McCann and J. Zahorjan, </author> <title> "Processor allocation policies for message passing parallel computers". </title> <booktitle> In SIGMETRICS Conf. Measurement & Modeling of Comput. Syst., </booktitle> <pages> pp. 19-32, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: Under certain circumstances, its use can be extended to large distributed-memory machines as well <ref> [26, 24] </ref>. A number of papers have evaluated the performance implications of coscheduling and gang scheduling, and compared them with dynamic partitioning and other scheduling policies for multiprogrammed multiprocessors [10, 28, 22, 16, 35].
Reference: [25] <author> Message Passing Interface Forum, </author> <title> MPI: A Message-Passing Interface Standard. </title> <month> May </month> <year> 1994. </year>
Reference-contexts: In systems that provide group communication primitives such as broadcast, multicast, and barrier synchronization, the system calls used to implement these services may be used to identify the interactions (but note that operations in distinct groups must be distinguished) <ref> [25] </ref>. In systems that only provide basic point-to-point message passing, each sender-receiver pair identifies an interaction. The identification of working sets is not based directly on a single interaction, e.g. a specific instance of a collective communication operation. Rather, it is based on interaction schemata with the COs.
Reference: [26] <author> V. K. Naik, S. K. Setia, and M. S. Squillante, </author> <title> "Scheduling of large scientific applications on distributed memory multiprocessor systems". </title> <booktitle> In 6th SIAM Conf. Parallel Processing for Scientific Computing, </booktitle> <volume> vol. II, </volume> <pages> pp. 913-922, </pages> <month> Mar </month> <year> 1993. </year>
Reference-contexts: Under certain circumstances, its use can be extended to large distributed-memory machines as well <ref> [26, 24] </ref>. A number of papers have evaluated the performance implications of coscheduling and gang scheduling, and compared them with dynamic partitioning and other scheduling policies for multiprogrammed multiprocessors [10, 28, 22, 16, 35].
Reference: [27] <author> J. K. Ousterhout, </author> <title> "Scheduling techniques for concurrent systems". </title> <booktitle> In 3rd Intl. Conf. Distributed Comput. Syst., </booktitle> <pages> pp. 22-30, </pages> <month> Oct </month> <year> 1982. </year>
Reference-contexts: Some twelve years ago John Ouster-hout noticed an analogy between memory management in multiprogrammed uniprocessor operating systems and processor management in multiprogrammed multiprocessor systems <ref> [27] </ref>. The analogy was based on the observation that parallel applications require a "working set" of processes to execute simultaneously, just like uniprocessor applications require a working set of pages to be memory resident simultaneously. <p> Activity working sets should be defined by the pattern of interactions among the activities. But it has usually been assumed that the working sets are defined by the user <ref> [27] </ref>. For example, the syntactic structure of the language may be used, by defining the set of activities that are spawned together by a single parallel construct to be a working set (Fig. 1) [9]. <p> If any processors are left over when some working sets are scheduled, they are used to schedule any activity that happens to be ready, with no regard to working sets. This is called alternative scheduling <ref> [27, 28] </ref>. In the sequel, we shall sometimes use "gang" as a synonym for "working set". Coscheduling was introduced by Ousterhout [27], who also implemented it in the Medusa system on CM fl . <p> This is called alternative scheduling [27, 28]. In the sequel, we shall sometimes use "gang" as a synonym for "working set". Coscheduling was introduced by Ousterhout <ref> [27] </ref>, who also implemented it in the Medusa system on CM fl . Since then, gang scheduling was implemented in a number of additional experimental systems [15, 29, 3, 10, 4]. <p> The choice between busy waiting and blocking depends on the granularity of interactions: if the time between interactions is small, so is the expected waiting time, and waiting is better than blocking. Coscheduling is specifically targeted to support such fine-grain interactions <ref> [27, 10] </ref>. An immediate consequence is that coarse-grain interactions do not require coscheduling. For example, if the interval between successive interactions is longer than the scheduling time quantum, the price for blocking is paid anyway. <p> This is essentially a dynamic version of Ousterhout's matrix algorithm <ref> [27] </ref>, as the mapping of activities to processors may change in each round. Coscheduled activities that arrive at an interaction retain their processor and busy wait. This is justified because activities in gangs are believed to interact frequently, so the waiting time is expected to be short. <p> Only activities that are ready to run are placed in the queue. Each activity is scheduled independently, and receives a full scheduling time quantum for execution. Competitive two-phase blocking is used for the interactions <ref> [27, 19] </ref>. Thus when an activity waits for an interaction, it first busy-waits for a duration equal to the context-switch overhead, and then it suspends execution and yields its processor. If the activity's time quantum expires while it is busy-waiting, it is suspended at that time.
Reference: [28] <author> M. K. Seager and J. M. Stichnoth, </author> <title> Simulating the Scheduling of Parallel Supercomputer Applications. </title> <type> Technical Report UCRL-102059, </type> <institution> Lawrence Livermore National Laboratory, </institution> <month> Sep </month> <year> 1989. </year>
Reference-contexts: If any processors are left over when some working sets are scheduled, they are used to schedule any activity that happens to be ready, with no regard to working sets. This is called alternative scheduling <ref> [27, 28] </ref>. In the sequel, we shall sometimes use "gang" as a synonym for "working set". Coscheduling was introduced by Ousterhout [27], who also implemented it in the Medusa system on CM fl . <p> Under certain circumstances, its use can be extended to large distributed-memory machines as well [26, 24]. A number of papers have evaluated the performance implications of coscheduling and gang scheduling, and compared them with dynamic partitioning and other scheduling policies for multiprogrammed multiprocessors <ref> [10, 28, 22, 16, 35] </ref>. The Mach scheduler also allows processors to be allocated to jobs dynamically, but performs the scheduling of activities on these processors in the kernel rather than leaving it to the application [1].
Reference: [29] <author> P. </author> <title> Steiner, "Extending multiprogramming to a DMPP". </title> <journal> Future Generation Comput. Syst. </journal> <pages> 8(1-3), pp. 93-109, </pages> <month> Jul </month> <year> 1992. </year>
Reference-contexts: In the sequel, we shall sometimes use "gang" as a synonym for "working set". Coscheduling was introduced by Ousterhout [27], who also implemented it in the Medusa system on CM fl . Since then, gang scheduling was implemented in a number of additional experimental systems <ref> [15, 29, 3, 10, 4] </ref>. Gang scheduling is also provided by at least five commercial systems: the Alliant FX/8 [31], the Connection Machine CM-5 [21], the Meiko CS-2 [8], the Intel Paragon [8], and the Silicon Graphics Challenge.
Reference: [30] <author> H. Sullivan, T. R. Bashkow, and D. Klappholz, </author> <title> "A large scale, homogeneous, fully distributed parallel machine, </title> <booktitle> II ". In 4th Ann. Intl. Symp. Computer Architecture Conf. Proc., </booktitle> <pages> pp. 118-124, </pages> <month> Mar </month> <year> 1977. </year>
Reference-contexts: One basic strategy is to physically partition the parallel machine and statically assign a parallel application job to a partition. Many partitioned systems do not support preemption, effectively limiting the number of jobs that can execute simultaneously <ref> [30, 34, 33] </ref>. Other systems take this partitioning a step further, and perform dynamic re-partitioning of the processors to reflect changes in the system load. Note that if the number of processors allocated to a job is less than the number of activities in it, then they cannot execute simultaneously.
Reference: [31] <author> J. A. </author> <title> Test, "Multi-processor management in the Concentrix operating system". </title> <booktitle> In Proc. Winter USENIX Technical Conf., </booktitle> <pages> pp. 173-182, </pages> <month> Jan </month> <year> 1986. </year>
Reference-contexts: Since then, gang scheduling was implemented in a number of additional experimental systems [15, 29, 3, 10, 4]. Gang scheduling is also provided by at least five commercial systems: the Alliant FX/8 <ref> [31] </ref>, the Connection Machine CM-5 [21], the Meiko CS-2 [8], the Intel Paragon [8], and the Silicon Graphics Challenge. In these systems, gangs are typically defined to include all the activities (or processes in their terminology) in the application.
Reference: [32] <author> A. Tucker and A. Gupta, </author> <title> "Process control and scheduling issues for multiprogrammed shared-memory multiprocessors". </title> <booktitle> In 12th Symp. Operating Systems Principles, </booktitle> <pages> pp. 159-166, </pages> <month> Dec </month> <year> 1989. </year>
Reference-contexts: Therefore a two-level scheduling scheme is used: the operating system is responsible for the allocation of processors, while the application itself schedules activities on them <ref> [32, 23] </ref>. Two-level scheduling with dynamic partitioning has been shown to be an effective scheduling scheme for small-scale uniform memory access (UMA) multiprocessors, provided applications are coded in a style that can tolerate dynamic changes in the number of processors at runtime.
Reference: [33] <author> D. L. Tuomenoksa and H. J. Siegel, </author> <title> "Task scheduling on the PASM parallel processing system". </title> <journal> IEEE Trans. Softw. Eng. </journal> <volume> SE-11(2), </volume> <pages> pp. 145-157, </pages> <month> Feb </month> <year> 1985. </year>
Reference-contexts: One basic strategy is to physically partition the parallel machine and statically assign a parallel application job to a partition. Many partitioned systems do not support preemption, effectively limiting the number of jobs that can execute simultaneously <ref> [30, 34, 33] </ref>. Other systems take this partitioning a step further, and perform dynamic re-partitioning of the processors to reflect changes in the system load. Note that if the number of processors allocated to a job is less than the number of activities in it, then they cannot execute simultaneously.
Reference: [34] <author> A. M. van Tilborg and L. D. Wittie, </author> <title> "Wave scheduling | decentralized scheduling of task forces in multicomputers". </title> <journal> IEEE Trans. Comput. </journal> <volume> C-33(9), </volume> <pages> pp. 835-844, </pages> <month> Sep </month> <year> 1984. </year> <title> Activity Working Sets Last Edit: </title> <address> Dec 9, </address> <year> 1994 </year> <month> 24 </month>
Reference-contexts: One basic strategy is to physically partition the parallel machine and statically assign a parallel application job to a partition. Many partitioned systems do not support preemption, effectively limiting the number of jobs that can execute simultaneously <ref> [30, 34, 33] </ref>. Other systems take this partitioning a step further, and perform dynamic re-partitioning of the processors to reflect changes in the system load. Note that if the number of processors allocated to a job is less than the number of activities in it, then they cannot execute simultaneously.
Reference: [35] <author> J. Zahorjan, E. D. Lazowska, and D. L. Eager, </author> <title> "The effect of scheduling discipline on spin overhead in shared memory parallel systems". </title> <journal> IEEE Trans. Parallel & Distributed Syst. </journal> <volume> 2(2), </volume> <pages> pp. 180-198, </pages> <month> Apr </month> <year> 1991. </year>
Reference-contexts: Under certain circumstances, its use can be extended to large distributed-memory machines as well [26, 24]. A number of papers have evaluated the performance implications of coscheduling and gang scheduling, and compared them with dynamic partitioning and other scheduling policies for multiprogrammed multiprocessors <ref> [10, 28, 22, 16, 35] </ref>. The Mach scheduler also allows processors to be allocated to jobs dynamically, but performs the scheduling of activities on these processors in the kernel rather than leaving it to the application [1].
References-found: 35

