URL: http://polaris.cs.uiuc.edu/reports/1464.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Email: koufaty,xchen,poulsen,torrella@csrd.uiuc.edu  
Title: Data Forwarding in Scalable Shared-Memory Multiprocessors 1  
Author: D. A. Koufaty, X. Chen, D. K. Poulsen and J. Torrellas 
Keyword: Memory latency hiding, forwarding and prefetching, multiprocessor caches, scalable shared-memory multiprocessors, address trace analysis.  
Address: IL 61801  
Affiliation: Center for Supercomputing Research and Development and Computer Science Department University of Illinois at Urbana-Champaign,  
Abstract: Scalable shared-memory multiprocessors are often slowed down by long-latency memory accesses. One way to cope with this problem is to use data forwarding to overlap memory accesses with computation. With data forwarding, when a processor produces a datum, in addition to updating its cache, it sends a copy of the datum to the caches of the processors that the compiler identified as consumers of it. As a result, when the consumer processors access the datum, they find it in their caches. This paper addresses two main issues. First, it presents a framework for a compiler algorithm for forwarding. Second, using address traces, it evaluates the performance impact of different levels of support for forwarding. Our simulations of a 32-processor machine show that a slightly-optimistic support for forwarding speeds up five applications by, on average, 50% for large caches and 30% for small caches. For large caches, many sharing read misses can be eliminated, while for smaller caches, forwarding does not seem to increase the number of conflict misses. Overall, support for forwarding in shared-memory multiprocessors promises to deliver good application speedups. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Adve and M. Hill. </author> <title> Weak Ordering ANew Definition. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 1-13, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: However, while these techniques indeed reduce average latencies significantly, they tend to handle sharing-induced misses poorly. To deal with this last situation, other techniques that overlap memory accesses with computation or other memory accesses are often necessary. These techniques include multithreading [2], relaxed memory consistency models <ref> [1, 6] </ref>, data prefetching [11] and data forwarding [16]. In both prefetching and forwarding, the data is moved close to the consumer processor before it is actually needed. Therefore, when the processor finally accesses the data, it can do so with low latency.
Reference: [2] <author> A. Agarwal. </author> <title> Performance Tradeoffs in Multithreaded Processors. </title> <journal> In IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> volume 3, </volume> <pages> pages 525-539, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: However, while these techniques indeed reduce average latencies significantly, they tend to handle sharing-induced misses poorly. To deal with this last situation, other techniques that overlap memory accesses with computation or other memory accesses are often necessary. These techniques include multithreading <ref> [2] </ref>, relaxed memory consistency models [1, 6], data prefetching [11] and data forwarding [16]. In both prefetching and forwarding, the data is moved close to the consumer processor before it is actually needed. Therefore, when the processor finally accesses the data, it can do so with low latency.
Reference: [3] <author> U. Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer Academic Publishers, Norwell, </publisher> <address> MA, </address> <year> 1988. </year>
Reference-contexts: Clearly, in the presence of procedure calls and aliasing, the compiler analysis can be complex. Such analysis needs to be handled by data dependence algorithms between loops. Discussion of data dependence algorithms <ref> [3] </ref> is beyond the scope of this paper. Instead, we will assume that we have already identified the producer and consumer loops. We now have to compute the forwarding expression in the producer loop. In the following, we will proceed with an example of how to compute the forwarding expression.
Reference: [4] <author> M. Berry et al. </author> <title> The Perfect Club Benchmarks: Effective Performance Evaluation of Supercomputers. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 3(3) </volume> <pages> 5-40, </pages> <month> Fall </month> <year> 1989. </year>
Reference-contexts: could split the producer loop into one that goes from 3 to 5 and another one that goes from 6 to 16. 3.1.1 Examples of Forwarding Expressions Before finishing this section, we show three examples of the resulting forwarding expressions for loops in parallelized versions of the Perfect Club codes <ref> [4, 5] </ref>. The examples show a forward 8 DOALL I = 3, 16 IF (I &gt; 5) THEN A (2I+1) = .... ELSE FW (A (2I+1),(2I-1)/3) = ... <p> For the network, we use the analytical delay model for indirect multistage networks presented in [8]. In our evaluation, we assume that each forward requires 4 assembly instructions in the producer code. 4.2 Applications We trace the parallel versions of the five Perfect Club codes <ref> [4] </ref> presented in Table 2. These versions run with 32 processors and were parallelized using a parallelizing compiler and later by hand [5]. Since the codes take a long time to run, we reduced their time requirements while preserving their parallelism and reference behavior.
Reference: [5] <author> R. Eigenmann, J. Hoeflinger, G. Jaxon, and D. Padua. </author> <title> The Cedar Fortran Project. </title> <type> Technical Report 1262, </type> <institution> Center for Supercomputing Research and Development, </institution> <month> October </month> <year> 1992. </year>
Reference-contexts: could split the producer loop into one that goes from 3 to 5 and another one that goes from 6 to 16. 3.1.1 Examples of Forwarding Expressions Before finishing this section, we show three examples of the resulting forwarding expressions for loops in parallelized versions of the Perfect Club codes <ref> [4, 5] </ref>. The examples show a forward 8 DOALL I = 3, 16 IF (I &gt; 5) THEN A (2I+1) = .... ELSE FW (A (2I+1),(2I-1)/3) = ... <p> They refer to a machine with 32 processors. Forward Broadcast. A simplified version of a pair of producer-consumer loops in TRFD <ref> [5] </ref> where broadcast is required is shown in Figure 5-(a). In the figure, the notation X (1:NUM) = Y (1:NUM) is short for a loop that assigns each of the NUM elements in Y to its corresponding elements in X. <p> Forward from Serial to Parallel Section. One important parallel loop in QCD <ref> [5] </ref> consumes data produced in a serial section. The serial section and the loop are shown in Figure 6-(a). Constant propagation is used to determine all the constants that appear in the loop bounds. <p> These versions run with 32 processors and were parallelized using a parallelizing compiler and later by hand <ref> [5] </ref>. Since the codes take a long time to run, we reduced their time requirements while preserving their parallelism and reference behavior. We did this by reducing the number of iterations 12 Table 2: Applications studied.
Reference: [6] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory Consistency and Event Ordering in Scalable Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: However, while these techniques indeed reduce average latencies significantly, they tend to handle sharing-induced misses poorly. To deal with this last situation, other techniques that overlap memory accesses with computation or other memory accesses are often necessary. These techniques include multithreading [2], relaxed memory consistency models <ref> [1, 6] </ref>, data prefetching [11] and data forwarding [16]. In both prefetching and forwarding, the data is moved close to the consumer processor before it is actually needed. Therefore, when the processor finally accesses the data, it can do so with low latency.
Reference: [7] <author> D. Glasco et al. </author> <title> Update-Based Cache Coherence Protocols for Scalable Shared-Memory Multiprocessors. </title> <type> Technical Report CSL-TR-93-588, </type> <institution> Computer Systems Laboratory, Stanford University, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: Alternatively, the hardware can perform combining if the producer buffers each forward for a few cycles before sending it. If, in the meantime, the processor generates another forward to the same line, the first one is discarded. This approach is used by Glasco et al. <ref> [7] </ref> in update protocols. Forwarding to the secondary cache only. If the primary cache is not big enough to hold the working set plus the forwarded data, forwards may be sent to the secondary cache only.
Reference: [8] <author> C. Kruskal and M. Snir. </author> <title> The Performance of Multistage Interconnection Networks for Multiprocessors. </title> <journal> In IEEE Trans. on Computers, </journal> <pages> pages 1091-98, </pages> <month> December </month> <year> 1983. </year>
Reference-contexts: Since we use the release consistency model, write misses do not stall the processors. We accurately model all the contention in the system. For the network, we use the analytical delay model for indirect multistage networks presented in <ref> [8] </ref>. In our evaluation, we assume that each forward requires 4 assembly instructions in the producer code. 4.2 Applications We trace the parallel versions of the five Perfect Club codes [4] presented in Table 2.
Reference: [9] <author> M. Lam, E. Rothberg, and M. Wolf. </author> <title> The Cache Performance and Optimizations of Blocked Algorithms. </title> <booktitle> In Proceedings of the 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 63-74, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: To decide whether or not to prune a forward, we need to estimate the number of different cache lines that the consumer processor will need in its cache between the arrival and the use of the forwarded datum. This value can be estimated by using locality analysis as in <ref> [9, 17] </ref>, where spatial, temporal and group locality carried by loops are identified. If such value is larger than a threshold value, the forward is pruned; otherwise the forward is left in the code. In a fully associative cache, such threshold can be set to the size of the cache.
Reference: [10] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. S. Lam. </author> <title> The Stanford Dash Multiprocessor. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: It can be thought of as producer-initiated prefetching. Data forwarding and prefetching are complementary techniques that can be combined. In this paper, however, we focus exclusively on data forwarding. There is little previous work on data forwarding. Forwarding was added to the DASH architecture with the Deliver instruction <ref> [10] </ref>. This instruction sends the data to several clusters specified in the instruction with a bit vector. In addition, the directory is updated in parallel. Unfortunately, there is no performance evaluation of the deliver instruction. Poulsen and Yew [14, 16] studied data forwarding alone and in combination with data prefetching. <p> In this case, the originating processor will be forced to retry the transaction. A similar approach is used in the transactions of the DASH cache coherence protocol <ref> [10] </ref>. Even if, in the end, the forward instruction safely deposits the data in the cache of the consumer processors, three problems may occur. First, a consumer processor may request the data before it arrives from the forwarding processor. <p> The write-and-forward instruction as defined restricts data forwarding to happen when the write takes place. To deal with the cases where the forward should be delayed but the write should not, a special forward-only instruction can be used. An example of such instruction is the DASH deliver instruction <ref> [10] </ref>. These forward-only instructions can then be software-pipelined like it is done with prefetches [12]. Software-Based Prefetching. Forwarding is less appropriate when the consumer is unknown or when the consumer will not use the data until much later. In these cases, we can use prefetching.
Reference: [11] <author> T. Mowry and A. Gupta. </author> <title> Tolerating Latency through Software-Controlled Prefetching in Shared-Memory Multiprocessors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 12(2) </volume> <pages> 87-106, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: To deal with this last situation, other techniques that overlap memory accesses with computation or other memory accesses are often necessary. These techniques include multithreading [2], relaxed memory consistency models [1, 6], data prefetching <ref> [11] </ref> and data forwarding [16]. In both prefetching and forwarding, the data is moved close to the consumer processor before it is actually needed. Therefore, when the processor finally accesses the data, it can do so with low latency. <p> Finally, if several consumers want the data, all of them have to execute prefetch instructions. We note in passing that forwarding also has the counterpart to prefetching in exclusive mode <ref> [11] </ref>. In this case, the producer sends the data in exclusive mode and invalidates itself. The compiler support required for data forwarding is more sophisticated than for prefetching. Indeed, if a processor issues prefetches to eliminate conflict misses only, then the compiler analysis can focus exclusively on that processor's code.
Reference: [12] <author> T. Mowry, M. Lam, and A. Gupta. </author> <title> Design and Evaluation of a Compiler Algorithm for Prefetching. </title> <booktitle> 23 In Proceedings of the 5th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 62-73, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: To deal with the cases where the forward should be delayed but the write should not, a special forward-only instruction can be used. An example of such instruction is the DASH deliver instruction [10]. These forward-only instructions can then be software-pipelined like it is done with prefetches <ref> [12] </ref>. Software-Based Prefetching. Forwarding is less appropriate when the consumer is unknown or when the consumer will not use the data until much later. In these cases, we can use prefetching. One hybrid forwarding-prefetching scheme has been studied by Poulsen and Yew [14]. <p> In a fully associative cache, such threshold can be set to the size of the cache. For direct mapped or set associative caches, however, we need to reduce the threshold to account for cache conflicts. Mowry et al. <ref> [12] </ref> suggest a threshold approximately equal to 1/16th of the size of the cache. To understand our approach, consider the example in Figure 8. In the figure, processor P1 writes to variable a at point S1 and processor P2 reads the data at S2 .
Reference: [13] <author> C. D. Polychronopoulos et al. </author> <title> Parafrase-2: An Environment for Parallelizing, Partitioning, Synchronizing, and Scheduling Programs on Multiprocessors. </title> <booktitle> In Proceedings of the 1989 International Conference on Parallel Processing, </booktitle> <volume> volume II, </volume> <pages> pages 39-48, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: Before presenting the results, we now discuss the simulation system and the applications used. 4.1 Simulation System For our simulations, we use the EPG-sim execution-driven simulator [15]. We start by compiling our applications with the Parafrase2 compiler <ref> [13] </ref>. For each array access in the program, the compiler introduces a call to the simulator. Each call takes as arguments the address of the data accessed, the processor ID, the type of access, and the time-stamp.
Reference: [14] <author> D. K. Poulsen. </author> <title> Memory Latency Reduction via Data Prefetching and Data Forwarding in Shared Memory Multiprocessors. </title> <type> Ph.D. thesis, </type> <institution> University of Illinois at Urbana-Champaign. Also Center for Supercomputing Research and Development Report 1377, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: Forwarding was added to the DASH architecture with the Deliver instruction [10]. This instruction sends the data to several clusters specified in the instruction with a bit vector. In addition, the directory is updated in parallel. Unfortunately, there is no performance evaluation of the deliver instruction. Poulsen and Yew <ref> [14, 16] </ref> studied data forwarding alone and in combination with data prefetching. They showed that large gains can be achieved with forwarding alone and in combination with prefetching. However, they only studied large caches, which tend to show the positive side of forwarding only. <p> Software-Based Prefetching. Forwarding is less appropriate when the consumer is unknown or when the consumer will not use the data until much later. In these cases, we can use prefetching. One hybrid forwarding-prefetching scheme has been studied by Poulsen and Yew <ref> [14] </ref>. To see the relationship between forwarding and prefetching, we finish this section with a comparison between the two schemes. 2.2 Comparison to Software-Based Prefetching Data forwarding and data prefetching can be compared in terms of effectiveness, complexity of the compiler and hardware support required, and overhead involved. <p> To determine the working set size of the applications, we plot the miss rate as a function of the cache size. The cache size at the knee of the resulting curve is approximately equal to the working set size <ref> [14] </ref>. In our experiments, we choose a size of 256 Kbytes for a cache larger than the working set size of the applications.
Reference: [15] <author> D. K. Poulsen and P.-C. Yew. </author> <title> Execution Driven Tools for Parallel Simulation of Parallel Architectures and Applications. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 860-869, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: In this execution, whenever we have a write that was detected as forwardable in the previous pass, we replace it by a write-and-forward. Before presenting the results, we now discuss the simulation system and the applications used. 4.1 Simulation System For our simulations, we use the EPG-sim execution-driven simulator <ref> [15] </ref>. We start by compiling our applications with the Parafrase2 compiler [13]. For each array access in the program, the compiler introduces a call to the simulator. Each call takes as arguments the address of the data accessed, the processor ID, the type of access, and the time-stamp.
Reference: [16] <author> D. K. Poulsen and P.-C. Yew. </author> <title> Data Prefetching and Data Forwarding in Shared Memory Multiprocessors. </title> <booktitle> In Proceedings of the 1994 International Conference on Parallel Processing, </booktitle> <volume> volume II, </volume> <pages> pages 276-280, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: To deal with this last situation, other techniques that overlap memory accesses with computation or other memory accesses are often necessary. These techniques include multithreading [2], relaxed memory consistency models [1, 6], data prefetching [11] and data forwarding <ref> [16] </ref>. In both prefetching and forwarding, the data is moved close to the consumer processor before it is actually needed. Therefore, when the processor finally accesses the data, it can do so with low latency. <p> Forwarding was added to the DASH architecture with the Deliver instruction [10]. This instruction sends the data to several clusters specified in the instruction with a bit vector. In addition, the directory is updated in parallel. Unfortunately, there is no performance evaluation of the deliver instruction. Poulsen and Yew <ref> [14, 16] </ref> studied data forwarding alone and in combination with data prefetching. They showed that large gains can be achieved with forwarding alone and in combination with prefetching. However, they only studied large caches, which tend to show the positive side of forwarding only. <p> Data forwarding can be supported efficiently with single or multiple words per cache line. Following Poulsen and Yew's work <ref> [16] </ref>, we support it with a Write-and-Forward assembly instruction. This instruction is inserted by the compiler in lieu of an ordinary write instruction. It uses one more register than an ordinary write instruction: a mask register that contains the processors that should receive the update.
Reference: [17] <author> M. E. Wolf and M. S. </author> <note> Lam. </note>
Reference-contexts: To decide whether or not to prune a forward, we need to estimate the number of different cache lines that the consumer processor will need in its cache between the arrival and the use of the forwarded datum. This value can be estimated by using locality analysis as in <ref> [9, 17] </ref>, where spatial, temporal and group locality carried by loops are identified. If such value is larger than a threshold value, the forward is pruned; otherwise the forward is left in the code. In a fully associative cache, such threshold can be set to the size of the cache.
References-found: 17

