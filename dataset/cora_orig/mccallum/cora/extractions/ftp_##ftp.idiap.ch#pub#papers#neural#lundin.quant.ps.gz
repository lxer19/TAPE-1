URL: ftp://ftp.idiap.ch/pub/papers/neural/lundin.quant.ps.gz
Refering-URL: http://www.idiap.ch/~perry/allpubs.html
Root-URL: http://www.idiap.ch/~perry/allpubs.html
Title: Connectionist Quantization Functions  
Author: T. Lundin, E. Fiesler and P. Moerland 
Keyword: (artificial) neural networks, connectionist systems, weight discretization, quantization, generalization, hardware implementation.  
Address: Geneve, Switzerland,  CP 592, CH-1920 Martigny, Switzerland,  
Affiliation: Computing  IDIAP,  
Note: Published in the Proceedings of the '96 SIPAR-Workshop on Parallel and Distributed  
Email: E-mail: Tomas.Lundin@idiap.ch  
Date: October 4, 1996.  
Abstract: One of the main strengths of connectionist systems, also known as neural networks, is their massive parallelism. However, most neural networks are simulated on serial computers where the advantage of massive parallelism is lost. For large and real-world applications, parallel hardware implementations are therefore essential. Since a discretization or quantization of the neural network parameters is of great benefit for both analog and digital hardware implementations, they are the focus of study in this paper. In 1987 a successful weight discretization method was developed, which is flexible and produces networks with few discretization levels and without significant loss of performance. However, recent studies have shown that the chosen quantization function is not optimal. In this paper, new quantization functions are introduced and evaluated for improving the performance of this flexible weight discretization method. 
Abstract-found: 1
Intro-found: 1
Reference: [Bellido-93] <author> I. Bellido and E. Fiesler. </author> <title> Do Backpropagation Trained Neural Networks have Normal Weight Distributions? Proc. </title> <booktitle> of the Int. Conf. on Artificial Neural Networks, </booktitle> <pages> pp. 772-775, </pages> <publisher> Springer, </publisher> <address> London, </address> <year> 1993. </year>
Reference-contexts: It consists of two different phases and uses both W and W + . In the first phase the interval [W ,0] and [0,W + ] are divided in equidistant levels. The second phase consists of attributing the value zero to the level that is closest to zero. In <ref> [Bellido-93] </ref> it is concluded that the weight distribution in a neural network resembles a Gaussian-like distribution. This means that there are many weights with small values and only few weights with large values.
Reference: [Fiesler-88] <author> E. Fiesler, A. Choudry, and H. J. Caulfield. </author> <title> Weight Discretization in Backward Error Propagation Neural Networks. Neural Networks, </title> <booktitle> special supp. "Abstracts of the First Annual INNS Meeting", vol.1, </booktitle> <address> p.380, </address> <year> 1988. </year>
Reference-contexts: Several weight discretization techniques have been developed to reduce the required accuracy further without deterioration of network performance. One of the earliest and perhaps most successful of these techniques <ref> [Fiesler-88] </ref> is further investigated and improved in this paper. The basic idea of this method is to first train the neural network with the backpropagation algorithm without quantization of the weights. Secondly, the resulting continuous weights are discretized by mapping them to the closest discretization level. <p> The resulting errors, which are based on the difference between the obtained and desired network outputs, are subsequently used to update the continuous weights during the backward pass until satisfactory performance is obtained. In the original paper <ref> [Fiesler-88] </ref> a staircase shaped threshold function with a uniform distribution is used to perform the mapping to equidistant discrete weights. In this paper, five alternative quantization functions are introduced and compared with the original one in a series of experiments on five real-world benchmark problems. <p> The expected value (mean) of the weights calculated over the entire pre-trained continuous network is indicated as E (w). The following quantization functions have been implemented and tested: Symmetrical This is the original quantization function from <ref> [Fiesler-88] </ref>. The resulting discretization levels are symmetric around zero and equidistant, with a step size of one between the weight levels. The intuitive disadvantage of the symmetrical quantization function is that it does not incorporate any knowledge about the weights of the pre-trained continuous network.
Reference: [Holt-93] <author> J. L. Holt and J.-N. Hwang. </author> <title> Finite Error Precision Analysis of Neural Network Hardware Implementations. </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 42, no. 3, </volume> <pages> pp. 1380-1389, </pages> <month> March </month> <year> 1993. </year>
Reference-contexts: Most of the standard algorithms for training neural networks are not suitable for quantized networks because they are based on gradient descent and require a high accuracy of the network parameters: 8 bits are required for on-chip recall and 16 bits for on-chip training <ref> [Holt-93] </ref>. Several weight discretization techniques have been developed to reduce the required accuracy further without deterioration of network performance. One of the earliest and perhaps most successful of these techniques [Fiesler-88] is further investigated and improved in this paper.
Reference: [Lundin-96] <author> T. Lundin, E. Fiesler, and P. Moerland. </author> <title> Quantization Functions for Multilayer Perceptrons. </title> <note> Technical report 96-xx (in preparation), IDIAP, Martigny, Switzerland 1996. </note>
Reference-contexts: Furthermore, a learning rate of 0.5, a momentum term of 0.9, and a flat-spot constant of 0.1 have been used. Additional information about the different benchmarks and the experimental set-up can be found in <ref> [Lundin-96] </ref>. 3.1 Discussion of Results To evaluate the experimental results, two different performance measurements have been used. For the approximation problems (Auto-mpg and Sunspot), the normalized mean square error on the test set is most significant.
Reference: [Marchesi-93] <author> M. Marchesi, G. Orlandi, F. Piazza, and A. Uncini. </author> <title> Fast Neural Networks Without Multipliers. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 4, no. 1, </volume> <pages> pp. 53-62, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: The Power of two W max quantization function has the added advantage that the normalized discrete weight values are restricted to powers-of-two. Therefore, simple shift registers can be employed to substitute the more complex multipliers <ref> [Marchesi-93] </ref>.
Reference: [Thimm-96] <author> G. Thimm and E. Fiesler. </author> <title> Weight Initialization in Higher Order and Multi-Layer Perceptrons. </title> <note> Accepted for publication in IEEE Transactions on Neural Networks, 1996. 5 </note>
Reference-contexts: The desired output values for these two benchmarks are namely real-valued (approximation problem), while the other benchmarks are classification problems with desired output values of 1 and +1. For each run the network was initialized with different random weights in the interval [-0.77,0.77], which has been chosen according to <ref> [Thimm-96] </ref>. Furthermore, a learning rate of 0.5, a momentum term of 0.9, and a flat-spot constant of 0.1 have been used.
References-found: 6

