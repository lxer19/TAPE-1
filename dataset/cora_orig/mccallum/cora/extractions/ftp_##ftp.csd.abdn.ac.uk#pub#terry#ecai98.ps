URL: ftp://ftp.csd.abdn.ac.uk/pub/terry/ecai98.ps
Refering-URL: http://www.csd.abdn.ac.uk/~terry/Publications/pub.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Implicit Feature Selection with the Value Difference Metric  
Author: Terry R. Payne and Peter Edwards 
Abstract: The nearest neighbour paradigm provides an effective approach to supervised learning. However, it is especially susceptible to the presence of irrelevant attributes. Whilst many approaches have been proposed that select only the most relevant attributes within a data set, these approaches involve pre-processing the data in some way, and can often be computationally complex. The Value Difference Metric (VDM) is a symbolic distance metric used by a number of different nearest neighbour learning algorithms. This paper demonstrates how the VDM can be used to reduce the impact of irrelevant attributes on classification accuracy without the need for pre-processing the data. We illustrate how this metric uses simple probabilistic techniques to weight features in the instance space, and then apply this weighting technique to an alternative symbolic distance metric. The resulting distance metrics are compared in terms of classification accuracy, on a number of real-world and artificial data sets. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D.W. Aha, </author> <title> `Tolerating Noisy, Irrelevant and Novel Attributes in Instance-Based Learning Algorithms', </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 36, </volume> <pages> 267-287, </pages> <year> (1992). </year>
Reference-contexts: Many Nearest Neighbour learning algorithms employ weights to modify the effect a specific component has in the resulting classification process <ref> [1, 8, 15, 18] </ref>. For example, PE-BLS [5] and EACH [15] assign a weight to each of the instances (or hyper-rectangles in the case of EACH) and modify this weight according to whether the instances result in correct or incorrect class predictions. <p> The VDM utilises value weights (5) to determine how well a specific value for a given attribute can discriminate between class labels. Other systems utilise weights to augment (or diminish) the effects of relevant (or irrelevant) attributes <ref> [1, 15] </ref>. 3 IRRELEVANT ATTRIBUTES AND FEATURE SELECTION An attribute is irrelevant if it contributes nothing to the target hypothesis, i.e. it makes no meaningful contribution towards the classification task. <p> However, the inclusion of such attributes often also results in a degradation in classification accuracy. Nearest Neighbour algorithms are especially susceptible to the inclusion of irrelevant attributes in the data set, and several studies have shown that the classification accuracy degrades as the number of irrelevant attributes is increased <ref> [1, 10, 18] </ref>. This degradation is due to the fact that irrelevant attributes violate the underlying assumption made by the nearest neighbour paradigm. As the location of the instance is defined by its attributes, this assumption relies on the attributes being relevant to the target hypothesis. <p> The weighted Overlap metric (WOM) and the simple Overlap metric (OM) were included to provide a comparison of the VDM, MVDM and OMVW with other distance metrics. The Weighted Overlap metric (WOM) is similar to the distance metrics used by weighted NN algorithms, such as IB4 <ref> [1] </ref> and EACH [15]. A set of attribute weights are generated by evaluating the training data and updating the attribute weights ! a using the weight function given in (7).
Reference: [2] <author> D.W. Aha, D. Kibler, and M.K. Albert, </author> <title> `Instance-Based Learning Algorithms', </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <pages> 37-66, </pages> <year> (1991). </year>
Reference-contexts: of Computing Science, King's College, University of Aberdeen, Aberdeen, Scotland, AB24 3UE. 2 Department of Computing Science, King's College, University of Aberdeen, Aberdeen, Scotland, AB24 3UE. nearest neighbour theme have also been proposed that represent the induced hypothesis as hyper-rectangles [15], as a set of prototype points or selected instances <ref> [2, 4] </ref>, or as feature projections [3]. Nearest Neighbour learning algorithms determine the class label of an unclassified instance by comparing it to a set of stored, classified instances, and identifying the class label of the nearest neighbour in this set. <p> For this reason, the metrics were evaluated on the 24-attribute Machine Learning and Data Mining 453 T.R.Payne P.Edwards LED display problem. This problem contains seven binary valued attributes (corresponding to the different segments within an LED seven segment numeric display), and an additional seventeen irrelevant attributes <ref> [2] </ref>. If this number of additional attributes is varied, it is possible to observe the effect of irrelevant attributes on different learning algorithms. Data sets were constructed containing 200 randomly generated instances with 10% noise (i.e. each attribute value had a 10% chance of being inverted).
Reference: [3] <author> A. Akku~s and H.A. Guvenir, </author> <title> `K Nearest Neighbor Classification on Feature Projections', </title> <booktitle> in Proceedings of the 13th International Conference on Machine Learning, </booktitle> <pages> pp. 12-19. </pages> <address> San Francisco, </address> <publisher> CA:Morgan Kaufmann, </publisher> <year> (1996). </year>
Reference-contexts: of Aberdeen, Aberdeen, Scotland, AB24 3UE. 2 Department of Computing Science, King's College, University of Aberdeen, Aberdeen, Scotland, AB24 3UE. nearest neighbour theme have also been proposed that represent the induced hypothesis as hyper-rectangles [15], as a set of prototype points or selected instances [2, 4], or as feature projections <ref> [3] </ref>. Nearest Neighbour learning algorithms determine the class label of an unclassified instance by comparing it to a set of stored, classified instances, and identifying the class label of the nearest neighbour in this set.
Reference: [4] <author> Y. Biberman, </author> <booktitle> `The Role of Prototypicality in Exemplar-Based Learning', in Proceedings of the 8th European Conference on Machine Learning, </booktitle> <pages> pp. 77-91. </pages> <address> Berlin, Germany:Springer-Verlag, </address> <year> (1995). </year>
Reference-contexts: of Computing Science, King's College, University of Aberdeen, Aberdeen, Scotland, AB24 3UE. 2 Department of Computing Science, King's College, University of Aberdeen, Aberdeen, Scotland, AB24 3UE. nearest neighbour theme have also been proposed that represent the induced hypothesis as hyper-rectangles [15], as a set of prototype points or selected instances <ref> [2, 4] </ref>, or as feature projections [3]. Nearest Neighbour learning algorithms determine the class label of an unclassified instance by comparing it to a set of stored, classified instances, and identifying the class label of the nearest neighbour in this set.
Reference: [5] <author> S. Cost and S. Salzberg, </author> <title> `A Weighted Nearest Neighbor Algorithm for Learning with Symbolic Features', </title> <journal> Machine Learning, </journal> <volume> 10, </volume> <pages> 57-78, </pages> <year> (1993). </year>
Reference-contexts: Many Nearest Neighbour learning algorithms employ weights to modify the effect a specific component has in the resulting classification process [1, 8, 15, 18]. For example, PE-BLS <ref> [5] </ref> and EACH [15] assign a weight to each of the instances (or hyper-rectangles in the case of EACH) and modify this weight according to whether the instances result in correct or incorrect class predictions. <p> The inclusion of a weight within the VDM has been questioned by a number of studies. PEBLS <ref> [5] </ref> is a NN learning algorithm which uses the Modified Value Difference Metric (MVDM).
Reference: [6] <author> B. V. Dasarathy, </author> <title> Nearest Neighbor(NN) Norms: NN Pattern Classification Techniques, </title> <publisher> Los Alamitos, California:IEEE Computer Society Press, </publisher> <year> 1991. </year>
Reference-contexts: Thus, it is important to identify such attributes automatically and prevent them from influencing the classification process. One of the most common learning paradigms in machine learning and pattern analysis is the Nearest Neighbour (NN) paradigm. This approach to supervised learning has been studied extensively <ref> [6] </ref>, and compared with a variety of other learning approaches, such as Bayesian techniques [14], artificial neural networks [12] and rule induction algorithms [12], and has also been analysed theoretically [10].
Reference: [7] <author> G. John, R. Kohavi, and K. Pfleger, </author> <title> `Irrelevant Features and the Subset Selection Problem', </title> <booktitle> in Proceedings of the 11th International Conference on Machine Learning, </booktitle> <pages> pp. 121-129. </pages> <address> San Francisco, </address> <publisher> CA:Morgan Kaufmann, </publisher> <year> (1994). </year>
Reference-contexts: The wrapper model was proposed as a means of using the bias inherent in the learning algorithm, to select the attribute subset. It has been argued that this model is superior to the filter model, which uses different biases in the attribute selection and the learning stages <ref> [7] </ref>. Both models perform a search within a space of attribute subsets to determine the optimal (or sub-optimal) subset for the classification task. In contrast to these models, a number of nearest neighbour techniques utilise weights to identify irrelevant attributes.
Reference: [8] <author> K. Kira and L.A. Rendell, </author> <title> `The Feature Selection Problem: Traditional Methods and a New Algorithm', </title> <booktitle> in Proceedings of the 10th National Conference on Artificial Intelligence (AAAI-92), </booktitle> <pages> pp. 129-134. </pages> <publisher> MIT Press, </publisher> <year> (1992). </year>
Reference-contexts: Many Nearest Neighbour learning algorithms employ weights to modify the effect a specific component has in the resulting classification process <ref> [1, 8, 15, 18] </ref>. For example, PE-BLS [5] and EACH [15] assign a weight to each of the instances (or hyper-rectangles in the case of EACH) and modify this weight according to whether the instances result in correct or incorrect class predictions. <p> Thus, the contribution of irrelevant attributes to the classification task falls as the contribution of other attributes rises. The resulting weights can be used to determine which attributes should be retained in the attribute subset, and which attributes should be discarded <ref> [8] </ref>. An alternative approach is to use the weights to control the influence that each attribute has on the distance between two instances.
Reference: [9] <author> R. Kohavi, </author> <title> `A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection', </title> <booktitle> in Proceedings of the 14th International Joint Conference on Artificial Intelligence (IJCAI), </booktitle> <pages> pp. 1137-1145. </pages> <address> San Mateo, CA:Morgan Kauf-mann, </address> <year> (1995). </year>
Reference-contexts: Attribute weights are determined by evaluating the NN algorithm on the training data. A vector of attribute weights is generated, which initially gives each attribute an equal weight. The leave-one out cross validation technique <ref> [9] </ref> is then used to predict the class label of each of the instances in the data set. As each instance is evaluated, the weights are adjusted according to whether or not the classification is correct. <p> To evaluate the performance of each of the distance metrics, a 10-fold cross validation <ref> [9] </ref> was performed on a number of different UCI data sets. The results, given in Table 2, list the classification accuracies achieved by each metric for each of the data sets.
Reference: [10] <author> P. Langley and W. Iba, </author> <title> `Average-case Analysis of a Nearest Neighbor Algorithm', </title> <booktitle> in Proceedings of the 13th International Joint Conference on Artificial Intelligence (IJCAI), </booktitle> <pages> pp. 889-894. </pages> <address> San Mateo, </address> <publisher> CA:Morgan Kaufmann, </publisher> <year> (1993). </year>
Reference-contexts: This approach to supervised learning has been studied extensively [6], and compared with a variety of other learning approaches, such as Bayesian techniques [14], artificial neural networks [12] and rule induction algorithms [12], and has also been analysed theoretically <ref> [10] </ref>. <p> However, the inclusion of such attributes often also results in a degradation in classification accuracy. Nearest Neighbour algorithms are especially susceptible to the inclusion of irrelevant attributes in the data set, and several studies have shown that the classification accuracy degrades as the number of irrelevant attributes is increased <ref> [1, 10, 18] </ref>. This degradation is due to the fact that irrelevant attributes violate the underlying assumption made by the nearest neighbour paradigm. As the location of the instance is defined by its attributes, this assumption relies on the attributes being relevant to the target hypothesis.
Reference: [11] <author> C.J. Merz and P.M. Murphy. </author> <title> UCI repository of machine learning databases, </title> <year> 1996. </year>
Reference-contexts: We have investigated the utility of !(i a ), both as a component of the VDM, and when combined with another distance metric. Several symbolic data sets from the UCI Machine Learning Repository <ref> [11] </ref> were used to evaluate the performance of five distance metrics: three of which were based on class conditional probabilities (VDM, MVDM & OMVW); and two which were used for baseline comparisons with other studies.
Reference: [12] <author> Machine Learning, </author> <title> Neural and Statistical Classification, </title> <editor> eds., D. Michie, D.J. Spiegelhalter, and C.C. Taylor, </editor> <publisher> UK:Ellis Hor-wood Ltd., </publisher> <year> 1994. </year>
Reference-contexts: One of the most common learning paradigms in machine learning and pattern analysis is the Nearest Neighbour (NN) paradigm. This approach to supervised learning has been studied extensively [6], and compared with a variety of other learning approaches, such as Bayesian techniques [14], artificial neural networks <ref> [12] </ref> and rule induction algorithms [12], and has also been analysed theoretically [10]. <p> This approach to supervised learning has been studied extensively [6], and compared with a variety of other learning approaches, such as Bayesian techniques [14], artificial neural networks <ref> [12] </ref> and rule induction algorithms [12], and has also been analysed theoretically [10].
Reference: [13] <author> T.R. Payne and P. Edwards, </author> <title> `A Survey of Feature Selection Methods'. </title> <type> Unpublished Draft, </type> <year> 1998. </year>
Reference-contexts: The resulting data set will generally contain fewer irrelevant attributes, and thus the performance of the learning algorithm will increase in terms of either complexity of the target hypothesis, or in terms of accuracy. A number of different techniques have been studied <ref> [13] </ref>, and can be grouped into two broad categories: those that employ the filter model, where the selection technique is independent of the final learning algorithm; and those that employ the wrapper model, where the final learning algorithm is embedded within the selection mechanism.
Reference: [14] <author> J. Rachlin, S. Kasif, S. Salzberg, and D.W. Aha, </author> <title> `Towards a Better Understanding of Memory-Based Reasoning Systems', </title> <booktitle> in Proceedings of the 11th International Machine Learning Conference (ML94), </booktitle> <pages> pp. 242-250. </pages> <address> San Francisco, </address> <publisher> CA:Morgan Kaufmann, </publisher> <year> (1994). </year>
Reference-contexts: One of the most common learning paradigms in machine learning and pattern analysis is the Nearest Neighbour (NN) paradigm. This approach to supervised learning has been studied extensively [6], and compared with a variety of other learning approaches, such as Bayesian techniques <ref> [14] </ref>, artificial neural networks [12] and rule induction algorithms [12], and has also been analysed theoretically [10].
Reference: [15] <author> S. Salzberg, </author> <title> `A Nearest Hyperrectangle Learning Method', </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <pages> 251-276, </pages> <year> (1991). </year>
Reference-contexts: Variants on the 1 Department of Computing Science, King's College, University of Aberdeen, Aberdeen, Scotland, AB24 3UE. 2 Department of Computing Science, King's College, University of Aberdeen, Aberdeen, Scotland, AB24 3UE. nearest neighbour theme have also been proposed that represent the induced hypothesis as hyper-rectangles <ref> [15] </ref>, as a set of prototype points or selected instances [2, 4], or as feature projections [3]. <p> Many Nearest Neighbour learning algorithms employ weights to modify the effect a specific component has in the resulting classification process <ref> [1, 8, 15, 18] </ref>. For example, PE-BLS [5] and EACH [15] assign a weight to each of the instances (or hyper-rectangles in the case of EACH) and modify this weight according to whether the instances result in correct or incorrect class predictions. <p> Many Nearest Neighbour learning algorithms employ weights to modify the effect a specific component has in the resulting classification process [1, 8, 15, 18]. For example, PE-BLS [5] and EACH <ref> [15] </ref> assign a weight to each of the instances (or hyper-rectangles in the case of EACH) and modify this weight according to whether the instances result in correct or incorrect class predictions. <p> The VDM utilises value weights (5) to determine how well a specific value for a given attribute can discriminate between class labels. Other systems utilise weights to augment (or diminish) the effects of relevant (or irrelevant) attributes <ref> [1, 15] </ref>. 3 IRRELEVANT ATTRIBUTES AND FEATURE SELECTION An attribute is irrelevant if it contributes nothing to the target hypothesis, i.e. it makes no meaningful contribution towards the classification task. <p> The weighted Overlap metric (WOM) and the simple Overlap metric (OM) were included to provide a comparison of the VDM, MVDM and OMVW with other distance metrics. The Weighted Overlap metric (WOM) is similar to the distance metrics used by weighted NN algorithms, such as IB4 [1] and EACH <ref> [15] </ref>. A set of attribute weights are generated by evaluating the training data and updating the attribute weights ! a using the weight function given in (7).
Reference: [16] <author> S. Salzberg, </author> <title> `Distance Metrics for Instance-Based Learning', </title> <booktitle> in ISMIS'91 6th International Symposium, Methodologies for Intelligent Systems, </booktitle> <pages> pp. 399-408, </pages> <year> (1991). </year>
Reference-contexts: Class Conditional Probability Values for the symbols in Figure 1. 3 The value of r in (1) varies for the Minkowskian metric, but is equal to 1 for the Overlap metric. 4 A comparison of these two metrics can be found in <ref> [16] </ref> Metric. This process can be illustrated by means of an example. The top three charts in Figure 1 represent the discrete class distributions of three different symbolic values, `X', `Y' and `Z'. Each distribution consists of three class conditional probabilities, represented by the vertical bars.
Reference: [17] <author> C. Stanfill and D. Waltz, </author> <title> `Toward Memory-Based Reasoning', </title> <journal> Communications of the ACM, </journal> <volume> 29(12), </volume> <pages> 1213-1228, </pages> <year> (1986). </year>
Reference-contexts: The Value Difference Metric (VDM) was first proposed as an alternative approach for determination of the distance between two symbolic values <ref> [17] </ref>.
Reference: [18] <author> D. Wettschereck, D.W. Aha, and T. Mohri, </author> <title> `A Review and Empirical Evaluation of Feature Weighting Methods for a Class of Lazy Learning Algorithms', </title> <journal> Artificial Intelligence Review, </journal> <pages> 11(1-5), 273-314, </pages> <year> (1997). </year> <note> Special Issue on Lazy Learning. </note>
Reference-contexts: Many Nearest Neighbour learning algorithms employ weights to modify the effect a specific component has in the resulting classification process <ref> [1, 8, 15, 18] </ref>. For example, PE-BLS [5] and EACH [15] assign a weight to each of the instances (or hyper-rectangles in the case of EACH) and modify this weight according to whether the instances result in correct or incorrect class predictions. <p> However, the inclusion of such attributes often also results in a degradation in classification accuracy. Nearest Neighbour algorithms are especially susceptible to the inclusion of irrelevant attributes in the data set, and several studies have shown that the classification accuracy degrades as the number of irrelevant attributes is increased <ref> [1, 10, 18] </ref>. This degradation is due to the fact that irrelevant attributes violate the underlying assumption made by the nearest neighbour paradigm. As the location of the instance is defined by its attributes, this assumption relies on the attributes being relevant to the target hypothesis. <p> A recent study compared MVDM with the VDM, but concluded that there was no difference in Machine Learning and Data Mining 452 T.R.Payne P.Edwards the classification accuracies of either metric over several data sets <ref> [18] </ref>. We have investigated the utility of !(i a ), both as a component of the VDM, and when combined with another distance metric.
Reference: [19] <author> D.R. Wilson and T.R. Martinez, </author> <title> `Improved Heterogeneous Distance Functions', </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 6, </volume> <pages> 1-34, </pages> <year> (1997). </year> <title> Machine Learning and Data Mining 454 T.R.Payne P.Edwards </title>
Reference-contexts: Hence, if the class of a new instance is unknown, it can be predicted by determining the class of its nearest neighbour within this instance space. To determine the proximity of two instances, a distance metric is required. Although several distance metrics have been proposed <ref> [19] </ref>, the most commonly used metrics are suitable only for either symbolic or numeric attributes. These include the Euclidean and Manhattan distance metrics for numeric attributes, and the Overlap distance metric for symbolic attributes.
References-found: 19

