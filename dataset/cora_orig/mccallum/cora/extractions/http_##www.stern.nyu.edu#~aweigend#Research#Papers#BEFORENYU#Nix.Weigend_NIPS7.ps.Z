URL: http://www.stern.nyu.edu/~aweigend/Research/Papers/BEFORENYU/Nix.Weigend_NIPS7.ps.Z
Refering-URL: http://www.stern.nyu.edu/~aweigend/Research/Papers/BEFORENYU/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: dnix@cs.colorado.edu  andreas@cs.colorado.edu  
Title: ``Learning Local Error Bars for Nonlinear Regression.''  Learning Local Error Bars for Nonlinear Regression  
Author: Nix, D. A., and A. S. Weigend (). G. Tesauro, D. S. Touretzky, and T. K. Leen, p. --. David A. Nix Andreas S. Weigend 
Address: Boulder, CO 80309-0430  Boulder, CO 80309-0430  
Affiliation: Department of Computer Science and Institute of Cognitive Science University of Colorado  Department of Computer Science and Institute of Cognitive Science University of Colorado  
Note: Citation:  In Advances in Neural Information Processing Systems 7 (NIPS*94), edited by  Cambridge, MA: MIT Press.  
Abstract: We present a new method for obtaining local error bars for nonlinear regression, i.e., estimates of the confidence in predicted values that depend on the input. We approach this problem by applying a maximum-likelihood framework to an assumed distribution of errors. We demonstrate our method first on computer-generated data with locally varying, normally distributed target noise. We then apply it to laser data from the Santa Fe Time Series Competition where the underlying system noise is known quantization error and the error bars give local estimates of model misspecification. In both cases, the method also provides a weighted-regression effect that improves generalization performance. 
Abstract-found: 1
Intro-found: 1
Reference: <author> C. Bishop. </author> <title> (1994) Mixture Density Networks. </title> <institution> Neural Computing Research Group Report NCRG/4288, Department of Computer Science, Aston University, Birmingham, UK. </institution>
Reference-contexts: Such limited connectivity can be too constraining on the functional forms for 2 (x) and, in our experience, 1 The case of a single Gaussian to represent a unimodal distribution can also been generalized to a mixture of several Gaussians that allows the modeling of multimodal distributions <ref> (Bishop, 1994) </ref>. 490 produce inferior results. This is a significant difference compared to Bishop's (1994) Gaussian mixture approach in which all output units are directly connected to one set of hidden units.
Reference: <author> W.L. </author> <title> Buntine and A.S. Weigend. (1991) Bayesian Backpropagation. </title> <journal> Complex Systems, </journal> <volume> 5: </volume> <pages> 603-643. </pages>
Reference: <author> A.M. Fraser and A. Dimitriadis. </author> <title> (1994) Forecasting Probability Densities Using Hidden Markov Models with Mixed States. In Time Series Prediction: Forecasting the Future and Understanding the Past, A.S. </title> <editor> Weigend and N.A. Gershenfeld, eds., </editor> <publisher> Addison-Wesley, </publisher> <pages> pp. 265-282. </pages>
Reference-contexts: Such additional information could come from attempting to estimate the entire CTD with connectionist methods (e.g., Mixture Density Networks, Bishop, 1994; fractional binning, Srivastava & Weigend, 1994) or with non-connectionist methods such as a Monte Carlo on a hidden Markov model <ref> (Fraser & Dimitriadis, 1994) </ref>.
Reference: <author> P.T. </author> <title> Kazlas and A.S. Weigend. (1995) Direct Multi-Step Time Series Prediction Using TD(). </title> <booktitle> In Advances in Neural Information Processing Systems 7 (NIPS*94, this volume). </booktitle> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> D.E. Rumelhart, R. Durbin, R. Golden, and Y. Chauvin. </author> <title> (1995) Backpropagation: The Basic Theory. </title>
Reference-contexts: If parameterized, this conditional target distribution (CTD) may also be fl http://www.cs.colorado.edu/~andreas/Home.html . This paper is available with figures in colors as ftp://ftp.cs.colorado.edu/pub/ Time-Series/MyPapers/Nix.Weigend NIPS7.ps.Z . viewed as an error model <ref> (Rumelhart et al., 1995) </ref>. Here, we present a simple method that provides higher-order information about the CTD than simply the mean. <p> Applying Bayes' rule and assuming statistical independence of the errors, we equivalently do gradient descent in the negative log likelihood of the targets d given the inputs and the network model, summed over all patterns i <ref> (see Rumelhart et al., 1995) </ref>: C = P i ln P (d i jx i ; N ): Traditionally, the resulting form of this cost function involves only the estimate y (x i ) of the conditional mean; the variance of the CTD is assumed to be constant for all x
Reference: <editor> In Backpropagation: </editor> <title> Theory, Architectures and Applications, </title> <editor> Y. Chauvin and D.E. Rumelhart, eds., </editor> <publisher> Lawrence Erlbaum, </publisher> <pages> pp. 1-34. </pages>
Reference: <author> T. Sauer. </author> <title> (1994) Time Series Prediction by Using Delay Coordinate Embedding. In Time Series Prediction: Forecasting the Future and Understanding the Past, A.S. </title> <editor> Weigend and N.A. Gershenfeld, eds., </editor> <publisher> Addison-Wesley, </publisher> <pages> pp. 175-193. </pages>
Reference: <author> A.N. Srivastava and A.S. Weigend. </author> <title> (1994) Computing the Probability Density in Connectionist Regression. </title> <booktitle> In Proceedings of the IEEE International Conference on Neural Networks (IEEE-ICNN'94), </booktitle> <address> Orlando, FL, p. 3786-3789. IEEE-Press. </address>
Reference: <author> A.S. Weigend and N.A. Gershenfeld, eds. </author> <title> (1994) Time Series Prediction: Forecasting the Future and Understanding the Past. </title> <publisher> Addison-Wesley. </publisher>
Reference-contexts: While non-parametric estimates of the shape of a CTD require large quantities of data, our less data-hungry method <ref> (Weigend & Nix, 1994) </ref> assumes a specific parameterized form of the CTD (e.g., Gaussian) and gives us the value of the error bar (e.g., the width of the Gaussian) by finding those parameters which maximize the likelihood that the target data was generated by a particular network model.
Reference: <author> A.S. Weigend and B. LeBaron. </author> <title> (1994) Evaluating Neural Network Predictors by Bootstrapping. </title> <booktitle> In Proceedings of the International Conference on Neural Information Processing (ICONIP'94), Seoul, Korea, </booktitle> <pages> pp. 1207-1212. </pages>
Reference-contexts: While non-parametric estimates of the shape of a CTD require large quantities of data, our less data-hungry method <ref> (Weigend & Nix, 1994) </ref> assumes a specific parameterized form of the CTD (e.g., Gaussian) and gives us the value of the error bar (e.g., the width of the Gaussian) by finding those parameters which maximize the likelihood that the target data was generated by a particular network model.
Reference: <author> A.S. Weigend and D.A. Nix. </author> <title> (1994) Predictions with Confidence Intervals (Local Error Bars). </title> <booktitle> In Proceedings of the International Conference on Neural Information Processing (ICONIP'94), </booktitle> <address> Seoul, Korea, p. </address> <pages> 847-852. 496 </pages>
Reference-contexts: While non-parametric estimates of the shape of a CTD require large quantities of data, our less data-hungry method <ref> (Weigend & Nix, 1994) </ref> assumes a specific parameterized form of the CTD (e.g., Gaussian) and gives us the value of the error bar (e.g., the width of the Gaussian) by finding those parameters which maximize the likelihood that the target data was generated by a particular network model.
References-found: 11

