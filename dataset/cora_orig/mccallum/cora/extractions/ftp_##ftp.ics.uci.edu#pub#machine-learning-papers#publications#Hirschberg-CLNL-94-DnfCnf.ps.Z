URL: ftp://ftp.ics.uci.edu/pub/machine-learning-papers/publications/Hirschberg-CLNL-94-DnfCnf.ps.Z
Refering-URL: http://www.ics.uci.edu/AI/ML/MLAbstracts.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: dan@ics.uci.edu pazzani@ics.uci.edu ali@ics.uci.edu  
Phone: (714) 856-5888  
Title: Average Case Analysis of k-CNF and k-DNF learning algorithms  
Author: Daniel S. Hirschberg Michael J. Pazzani Kamal M. Ali 
Address: Irvine, CA 92717, USA  
Affiliation: Department of Information and Computer Science University of California, Irvine  
Date: July 1, 1994  
Abstract: We present average case models of algorithms for learning Conjunctive Normal Form (CNF, i.e., conjunctions of disjunctions) and Disjunctive Normal Form (DNF, i.e., disjunctions of conjunctions). Our goal is to predict the expected error of the learning algorithm as a function of the number n of training examples, averaging over all sequences of n training examples. We show that our average case models accurately predict the expected error and demonstrate that the analysis can lead to insight into the behavior of the algorithm and the factors that affect the error. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Benedek, G., and Itai, A. </author> <year> 1987. </year> <title> Learnability by fixed distributions. </title> <booktitle> In Proceedings of the 1988 Workshop on Computational Learning Theory (pp 81-90). </booktitle> <address> Boston, MA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <editor> Petsche T. et al. </editor> <booktitle> Computational Learning Theory and Natural Learning Systems, </booktitle> <volume> Vol. </volume> <pages> 2. </pages> <note> 13 Blumer, </note> <author> A., Ehrenfeucht, A., Haussler, D., and Warmuth, M. </author> <year> 1989. </year> <title> Learnability and the Vapnik- Chervonenkis dimension. </title> <journal> Journal of the Association of Computing Machinery, </journal> <volume> 36, </volume> <pages> 929-965. </pages>
Reference: <author> Haussler, D. </author> <year> 1987. </year> <title> Applying Valiant's Learning Framework to AI Concept Learning Problems. </title> <type> Technical Report UCSC-CRL-87-11, </type> <institution> University of California, Santa Cruz. </institution>
Reference: <author> Haussler, D. </author> <year> 1987. </year> <title> Bias, version spaces and Valiant's learning framework. </title> <booktitle> In Proceedings of the Fourth International Workshop on Machine Learning (pp. </booktitle> <pages> 324-335). </pages> <address> Irvine, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Haussler, D. </author> <year> 1990. </year> <title> Probably Approximately Correct Learning. </title> <booktitle> In Proceedings of the Fourth International Workshop on Machine Learning, </booktitle> <pages> (pp. </pages> <address> 1101-1108) Boston: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Petsche T. et al. Computational Learning Theory and Natural Learning Systems, Vol. 2. 3 Although the PAC model has been used to predict the accuracy of an algorithm as a function of the number of training examples <ref> (Haussler, 1990) </ref>, it is primarily used to show that certain classes of concepts are not learnable and concerns itself with learnability in the limit.
Reference: <author> Haussler, D. </author> <year> 1986. </year> <title> Quantifying inductive bias in concept learning. </title> <type> Technical Report UCSC-CRL-86-25, </type> <institution> University of California, Santa Cruz. </institution>
Reference: <author> Haussler, D., Littlestone, N. and Warmuth, M. </author> <year> 1990. </year> <title> Predicting 0,1-functions on randomly drawn points. </title> <type> Technical Reports USCS-CRL-90-54, </type> <institution> University of California, Santa Cruz. </institution>
Reference-contexts: Petsche T. et al. Computational Learning Theory and Natural Learning Systems, Vol. 2. 3 Although the PAC model has been used to predict the accuracy of an algorithm as a function of the number of training examples <ref> (Haussler, 1990) </ref>, it is primarily used to show that certain classes of concepts are not learnable and concerns itself with learnability in the limit.
Reference: <author> Hembold, D., Sloan, R., and Warmuth, M. </author> <year> 1990. </year> <title> Learning nested differences of intersection-closed concept classes, </title> <journal> Machine Learning, </journal> <volume> 5, </volume> <pages> 165-196. </pages>
Reference: <author> Hirschberg D., Pazzani M. </author> <year> 1992. </year> <title> Average Case Analysis of Learning k-CNF concepts. </title> <booktitle> In Proceedings of the Ninth International Workshop on Machine Learning, </booktitle> <pages> (pp. </pages> <address> 206-211) Aberdeen, UK: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kearns, M., Li, M., Pitt, L., and Valiant, L. </author> <year> 1987. </year> <title> On the learnability of Boolean formula. </title> <booktitle> In Proceedings of the Nineteenth Annual ACM Symposium on the Theory of Computing (pp. </booktitle> <pages> 285-295). </pages> <address> New York City: NY: </address> <publisher> ACM Press. </publisher>
Reference: <author> Natarajan, B. </author> <year> 1987. </year> <title> On learning Boolean formula. </title> <booktitle> In Proceedings of the Nineteenth Annual ACM Symposium on the Theory of Computing (pp. </booktitle> <pages> 295-304). </pages> <address> New York: </address> <publisher> ACM Press. </publisher>
Reference: <editor> Petsche T. et al. </editor> <booktitle> Computational Learning Theory and Natural Learning Systems, </booktitle> <volume> Vol. 2. 14 Pazzani, </volume> <editor> M., and Sarrett, W. </editor> <year> 1990. </year> <title> Average case analysis of conjunctive learning algorithms. </title> <booktitle> In Proceedings of the Seventh International Workshop on Machine Learning, </booktitle> <address> Austin, TX: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Valiant, L. </author> <year> 1984. </year> <title> A theory of the learnable. </title> <journal> Communications of the Association of Computing Machinery, </journal> <volume> 27, </volume> <pages> 1134-1142. </pages>
Reference: <author> Valiant, L. </author> <year> 1985. </year> <title> Learning disjunctions of conjunctions. </title> <booktitle> In Proceedings of the Ninth International Joint Conference on Artificial Intelligence (pp 560-566). </booktitle> <address> Los Angeles, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Vapnik, V. and Chervonenkis, A. </author> <year> 1971. </year> <title> On the uniform convergence of relative frequencies of events to their probabilities. </title> <journal> Theory of Probability with Applications, </journal> <volume> 16, </volume> <pages> 264-280. </pages>
Reference-contexts: Blumer et al: (1989), for example, give the following upper bound on the mean error V Cdim (h) N where h is the hypothesis space, N is the number of training examples, and V Cdim (h) is the Vapnik-Chervonenkis dimension of that hypothesis space <ref> (Vapnik & Chervonenkis, 1971) </ref>. Other distribution-specific work on bounding the amount of error has been done by Haussler et al: (1990).

Reference: <editor> Petsche T. et al. </editor> <booktitle> Computational Learning Theory and Natural Learning Systems, </booktitle> <volume> Vol. </volume> <month> 2. </month> <title> 18 CAPTIONS, page 2 algorithm when there are a total of 3 (lowest curve), 4 (middle), and 6 features (upper curve). The curves represent mean error rates as predicted by our average case model. The circles represent sample average error rates as determined by empirical tests and the bars are 95 percent confidence intervals around the empirically determined error values. To avoid clutter, some confidence intervals are not shown. </title>
References-found: 16

