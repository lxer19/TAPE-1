URL: http://www-diglib.stanford.edu/diglib/WP/PUBLIC/DOC93.ps
Refering-URL: http://www.public.iastate.edu/~CYBERSTACKS/Aristotle.htm
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Adaptive Web Page Recommendation Service  
Author: Marko Balabanovic 
Affiliation: Department of Computer Science, Stanford University  
Address: Marina del Rey CA  
Date: February 1997,  
Note: Stanford University Digital Libraries Project Working Paper SIDL-WP-1996-0041 Revised version to appear in First International Conference on Autonomous Agents,  An  
Abstract: An adaptive recommendation service seeks to adapt to its users, providing increasingly personalized recommendations over time. In this paper we introduce the "Fab" adaptive web page recommendation service. There has been much research on analyzing document content in order to improve recommendations or search results. More recently researchers have begun to explore how the similarities between users can be exploited to the same ends. The Fab system strikes a balance between these two approaches, taking advantage of the shared interests among users without losing the benefits of the representations provided by content analysis. Running since March 1996, it has been populated with a collection of agents for the collection and selection of web pages, whose interaction fosters emergent collaborative properties. In this paper we explain the design of the system architecture and report the results of our first experiment, evaluating recommendations provided to a group of test users. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Allan, J. </author> <year> 1995. </year> <title> Relevance feedback with too much data. </title> <booktitle> In Proceedings of the 18 th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. </booktitle>
Reference: <author> Armstrong, R.; Freitag, D.; Joachims, T.; and Mitchell, T. </author> <year> 1995. </year> <title> WebWatcher: A learning apprentice for the World-Wide Web. </title> <booktitle> In Proceedings of the AAAI Spring Symposium on Information Gathering from Heterogenous, Distributed Resources. </booktitle>
Reference: <author> Balabanovic, M., and Shoham, Y. </author> <year> 1995. </year> <title> Learning information retrieval agents: Experiments with automated web browsing. </title> <booktitle> In Proceedings of the AAAI Spring Symposium on Information Gathering from Heterogenous, Distributed Resources. </booktitle>
Reference-contexts: Search Agents First described in <ref> (Balabanovic & Shoham 1995) </ref>, these agents perform a best-first search of the web. For each agent the heuristic function used to evaluate a web page w is r (w; m), where m is the agent's search profile.
Reference: <author> Buckley, C.; Salton, G.; Allan, J.; and Singhal, A. </author> <year> 1995. </year> <title> Automatic query expansion using SMART: </title> <booktitle> TREC-3. In Proceedings of the 3 rd Text REtrieval Conference. </booktitle>
Reference: <author> Cox, III, E. P. </author> <year> 1980. </year> <title> The optimal number of response alternatives for a scale: A review. </title> <journal> Journal of Marketing Research 17 </journal> <pages> 407-422. </pages>
Reference: <author> Deerwester, S.; Dumais, S. T.; Furnas, G. W.; Lan-dauer, T. K.; and Harshman, R. </author> <year> 1990. </year> <title> Indexing by latent semantic analysis. </title> <journal> Journal of the American Society for Information Science 41(6) </journal> <pages> 391-407. </pages>
Reference-contexts: To enable the application of ML techniques there has been much interest recently in dimensionality reduction (e.g., <ref> (Deerwester et al. 1990) </ref>). In the IR community variants of relevance feedback have been studied in the context of the routing task at the TREC conferences (Harman 1995), for example (Buck-ley et al. 1995; Allan 1995).
Reference: <author> Foltz, P. W., and Dumais, S. T. </author> <year> 1992. </year> <title> Personalized information delivery: An analysis of information filtering methods. </title> <journal> Communications of the ACM 35(12) </journal> <pages> 51-60. </pages>
Reference: <author> Harman, D. </author> <year> 1995. </year> <booktitle> Overview of the third Text REtrieval Conference (TREC-3). In Proceedings of the 3 rd Text REtrieval Conference. </booktitle>
Reference-contexts: To enable the application of ML techniques there has been much interest recently in dimensionality reduction (e.g., (Deerwester et al. 1990)). In the IR community variants of relevance feedback have been studied in the context of the routing task at the TREC conferences <ref> (Harman 1995) </ref>, for example (Buck-ley et al. 1995; Allan 1995). There have also been numerous comparisons between these variants and non-incremental ML techniques (Schutze, Hull, & Peder-sen 1995; Lang 1995; Pazzani, Muramatsu, & Billsus 1996). <p> This model has been used and studied extensively, forms the basis for commercial web search systems and has been shown to be competitive with alternative IR methods <ref> (Harman 1995) </ref>. In this model documents and queries are represented as vectors. Assume some dictionary vector d, where each element d i is a word. Each document then has a vector w, where element w i is the weight of word d i for that document.
Reference: <author> Lang, K. </author> <year> 1995. </year> <title> Newsweeder: Learning to filter netnews. </title> <booktitle> In Proceedings of the 12 th International Conference on Machine Learning. </booktitle>
Reference: <author> Lesk, M. E., and Salton, G. </author> <year> 1971. </year> <title> Relevance assessments and retrieval system evaluation. In The Smart System|Experiments in Automatic Document Processing. </title> <publisher> Prentice Hall Inc. </publisher> <pages> 506-527. </pages>
Reference-contexts: In contrast to the negative results from user studies mentioned, it has been shown that human judges are good at making relative judgments given a collection of documents, and will be consistent over time and compared to other judges <ref> (Lesk & Salton 1971) </ref>. The ndpm measure only requires relative judgments from users, and since there is no query involved the judgments relate to pertinence not relevance. Thus it is more appropriate than precision and recall, which are based on absolute relevance judgments.
Reference: <author> Lieberman, H. </author> <year> 1995. </year> <title> Letizia: An agent that assists web browsing. </title> <booktitle> In International Joint Conference on Artificial Intelligence. </booktitle>
Reference: <author> Mauldin, M. L., and Leavitt, J. R. </author> <year> 1994. </year> <booktitle> Web-agent related research at the CMT. In Proceedings of the ACM Special Interest Group on Networked Information Discovery and Retrieval (SIGNIDR-94). </booktitle>
Reference-contexts: Selection Next select from the collected items those best for a particular user. Delivery Finally deliver the selected items to the user. Most on-line recommendation or search services can be described in this way. For instance a web index (e.g., <ref> (Mauldin & Leavitt 1994) </ref>) will have an exhaustive collection component, a selection component which is an information retrieval (IR) system to match web pages to user-supplied queries, and will deliver the pages via the web.
Reference: <author> Mitchell, T. </author> <year> 1995. </year> <type> Personal communication. </type>
Reference-contexts: Computer graphics/Game programming Library cataloging and classification Post-industrial music Sports information and gaming Native American culture Cookery 60's music Hiking Evolution There has been criticism of collaborative recommendation services <ref> (Mitchell 1995) </ref> that they have never been shown to do better than the "majority vote" rule, i.e. they would do as well just recommending the most popular items.
Reference: <author> Mittelstaedt, R. A. </author> <year> 1971. </year> <title> Semantic properties of selective evaluative adjectives: Other evidence. </title> <journal> Journal of Marketing Research 8 </journal> <pages> 236-237. </pages>
Reference: <author> Pazzani, M.; Muramatsu, J.; and Billsus, D. </author> <year> 1996. </year> <title> Syskill & Webert: Identifying interesting web sites. </title> <booktitle> In Proccedings of the 13 th National Conference on Artificial Intelligence (to appear). </booktitle>
Reference-contexts: If d i does not occur in the dictionary we set df (i) = 1. In an attempt to avoid over-fitting, and reduce memory and communications load, we use just the 100 highest-weighted words from any document. Recent experiments described in <ref> (Pazzani, Muramatsu, & Bill-sus 1996) </ref> have shown that using too many words leads Excellent Very Good Good Neutral Poor Very Bad Terrible to a decrease in performance when classifying web pages using supervised learning methods.
Reference: <author> Porter, M. </author> <year> 1980. </year> <title> An algorithm for suffix stripping. </title> <booktitle> Program 14(3) </booktitle> <pages> 130-137. </pages>
Reference-contexts: Each document then has a vector w, where element w i is the weight of word d i for that document. If the document does not contain d i then w i = 0. In our formulation we reduce words to their stems using the Porter algorithm <ref> (Porter 1980) </ref>, ignore words from a standard stop list of 571 words, and calculate a TFIDF weight: the weight w i of a word d i in a document W is given by: w i = 0:5 + 0:5 tf max log df (i) where tf (i) is the number of
Reference: <author> Resnick, P.; Iacovou, N.; Suchak, M.; Bergstrom, P.; and Riedl, J. </author> <year> 1994. </year> <title> GroupLens: An open architecture for collaborative filtering of netnews. </title> <booktitle> In Proceedings of the ACM Conference on Computer Supported Cooperative Work. </booktitle>
Reference: <author> Rocchio, Jr., J. </author> <year> 1971. </year> <title> Relevance feedback in information retrieval. In The Smart System|Experiments in Automatic Document Processing. </title> <publisher> Prentice Hall Inc. </publisher> <pages> 313-323. </pages>
Reference-contexts: In order to measure how well a page w matches a profile m, we use a variant of the standard IR cosine measure: r (w; m) = w:m Updating m also corresponds to a normal operation in retrospective IR: relevance feedback <ref> (Rocchio 1971) </ref>. We use a simple update rule: u (w; m; s) = m + sw where s is the user's score for page w (we translate the scale shown in Figure 1 to the integers 3, 2, 1, 0, 1, 2, 3).
Reference: <author> Rorvig, M. E. </author> <year> 1988. </year> <title> Psychometric measurement and information retrieval. </title> <editor> In Williams, M. E., ed., </editor> <booktitle> Annual Review of Information Science and Technology, </booktitle> <volume> volume 23. </volume> <publisher> Elsevier Science Publishers B.V. </publisher> <pages> 157-189. </pages>
Reference-contexts: User studies have shown that users have difficulty making consistent relevance judgments over a long period of time when asked to rate documents on an absolute relevance scale (Lesk & Salton 1971; Saracevic 1975), and in fact this is a general feature of human judgments <ref> (Rorvig 1988) </ref>. There will also be considerable disagreement between the judgments of different users (Saracevic 1995). The usual way to circumvent this problem is to test IR systems on standardized collections of documents and queries, so as to make recall and precision figures comparable.
Reference: <author> Salton, G., and McGill, M. J. </author> <year> 1983. </year> <title> An Introduction to Modern Information Retrieval. </title> <publisher> McGraw-Hill. </publisher>
Reference-contexts: Now we make the further assumption that we can represent the content of a page purely by considering the words contained in the text. We ignore all mark-up tags, images and other multimedia information. The vector-space model of information retrieval <ref> (Salton & McGill 1983) </ref> provides us with an appropriate representation for documents based on their constituent words. This model has been used and studied extensively, forms the basis for commercial web search systems and has been shown to be competitive with alternative IR methods (Harman 1995).
Reference: <author> Saracevic, T. </author> <year> 1975. </year> <title> Relevance: A review of and a framework for the thinking on the notion in information science. </title> <journal> Journal of the American Society for Information Science 26(6) </journal> <pages> 321-343. </pages>
Reference: <author> Saracevic, T. </author> <year> 1995. </year> <title> Evaluation of evaluation in information retrieval. </title> <booktitle> In 18 th International ACM SIGIR Conference on Research and Development in Information Retrieval. </booktitle>
Reference-contexts: There will also be considerable disagreement between the judgments of different users <ref> (Saracevic 1995) </ref>. The usual way to circumvent this problem is to test IR systems on standardized collections of documents and queries, so as to make recall and precision figures comparable. Since our domain is the web, we cannot rely on using a standardized collection.
Reference: <author> Schutze, H.; Hull, D. A.; and Pedersen, J. O. </author> <year> 1995. </year> <title> A comparison of classifiers and document representations for the routing problem. </title> <booktitle> In Proceedings of the 18 th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. </booktitle>
Reference: <author> Shardanand, U., and Maes, P. </author> <year> 1995. </year> <title> Social information filtering: Algorithms for automating "word of mouth". </title> <booktitle> In Conference on Human Factors in Computing Systems (CHI '95). </booktitle>
Reference: <author> Sheth, B., and Maes, P. </author> <year> 1993. </year> <title> Evolving agents for personalized information filtering. </title> <booktitle> In Proceedings of the 9 th IEEE Conference on Artificial Intelligence for Applications. </booktitle>
Reference: <author> Yao, Y. Y. </author> <year> 1995. </year> <title> Measuring retrieval effectiveness based on user preference of documents. </title> <journal> Journal of the American Society for Information Science 46(2) </journal> <pages> 133-145. </pages>
Reference-contexts: user and system rankings gives us a single-valued measure, simpler to interpret than precision-recall graphs (interestingly in the degenerate case where two-level rankings are used, ndpm can be shown to be a composite measure of recall and precision, and is related to several other measures proposed over the years|details in <ref> (Yao 1995) </ref>). Even apart from these considerations, recall is impossible to measure and difficult even to estimate when the document collection is the whole web. An alternative approach would be to use classification accuracy as our performance measure.
References-found: 26

