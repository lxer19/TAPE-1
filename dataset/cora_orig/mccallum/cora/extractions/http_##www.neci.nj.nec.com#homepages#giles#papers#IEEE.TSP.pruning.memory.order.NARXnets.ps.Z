URL: http://www.neci.nj.nec.com/homepages/giles/papers/IEEE.TSP.pruning.memory.order.NARXnets.ps.Z
Refering-URL: http://www.neci.nj.nec.com/homepages/giles/html/CLG_pub.html
Root-URL: 
Phone: 2  3  
Title: A Delay Damage Model Selection Algorithm for NARX Neural Networks  
Author: Tsungnan Lin ;y ,C. Lee Giles ; Bill G. Horne S.Y. Kung 
Keyword: Recurrent neural networks, tapped-delay lines, long-term dependencies, time series, automata, memory, temporal sequences, gradient descent training, latching, NARX networks, auto-regressive, pruning, embedding theory.  
Note: Published in IEEE Transactions on Signal Processing, "Special Issue on Neural Networks," vol. 45, no. 11, p. 2719-2730, 1997. Copyright IEEE. Current address: Epson  
Address: 4 Independence Way, Princeton, NJ 08540  Princeton, NJ 08540  College Park, MD 20742  Palo Alto Laboratory, 3145 Porter Drive, Suite 104, Palo Alto, CA 94304  
Affiliation: 1 NEC Research Institute,  Department of Electrical Engineering, Princeton University,  UMIACS, University of Maryland,  
Abstract: Recurrent neural networks have become popular models for system identification and time series prediction. NARX (Nonlinear AutoRegressive models with eXogenous inputs) neural network models are a popular subclass of recurrent networks and have been used in many applications. Though embedded memory can be found in all recurrent network models, it is particularly prominent in NARX models. We show that using intelligent memory order selection through pruning and good initial heuristics significantly improves the generalization and predictive performance of these nonlinear systems on problems as diverse as grammatical inference and time series prediction. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. Akaike. </author> <title> A new look at the statistical model identification. </title> <journal> IEEE trans. Automatic Control, </journal> <volume> AC 19 </volume> <pages> 716-723, </pages> <year> 1974. </year>
Reference-contexts: Two different representations can be equivalent in terms of expressive power, but may differ dramatically in the efficiency or effectiveness of problem-solving. When there is no prior knowledge about the model of the underling process, traditional statistical tests can be used, for example, Akaike information criterion (AIC) <ref> [1] </ref> and the minimum description length principle (MDL) [53]. Such models are judged on their "goodness-of-fit", which is a function of the likelihood of the data given the hypothesized model and its associated degrees of freedom. Fogel [16] applied the modification of AIC to select a "best" network.
Reference: [2] <author> W. Atmar. </author> <title> Notes on the simulation of evolution. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 5(1) </volume> <pages> 130-148, </pages> <year> 1994. </year>
Reference-contexts: However, the AIC method is complex and can be troubled by imprecision [55, 25]. Such model complexity and regularization methods are readily used for nonlinear models such as neural networks; see for example [24, 42, 67]. Evolutionary Programming <ref> [2, 18] </ref> is another search mechanism. This algorithm operates on a population of models. Offspring models are created by randomly mutating parents models. Competition between offspring models for survival are judged according to the fitness function.
Reference: [3] <author> A.D. Back and A.C. Tsoi. </author> <title> A comparison of discrete-time operator models for nonlinear system identification. </title> <editor> In G. Tesauro, D. Touretzky, and T. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <pages> pages 883-890. </pages> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: A good representation can make useful information explicit and easy to extract. In this work we only explored classic tapped delay memory structures. It would be interesting to see if similar results could be achieved for other memory models <ref> [3, 62] </ref>. However, a minimal representation does not necessarily mean a good representation. Two different representations can be equivalent in terms of expressive power, but may make a great difference in efficiency and effectiveness of problem-solving. 23 6 Acknowledgements We would like to acknowledge useful discussions and suggestions by G.
Reference: [4] <author> U. Bodenhausen and A. Waibel. </author> <title> The tempo 2 algorithm: Adjusting time-delays by supervised learning. </title> <booktitle> In Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 155-161. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: This algorithm iteratively determines the memory of a model based on the gradient information. Originally proposed by Etter, it was used as a "adaptive delay filter", which included variable delays taps as well as variable gains, for modeling several sparse systems [15, 7]. Recently others <ref> [4, 13, 35, 14] </ref> have also extended neural networks to include adaptable time delays. Because the error function of the adaptable time delays depends on the autocorrelation function of input signals [15, 7], the gradient of the delay operator will depend on the derivative of input signals.
Reference: [5] <author> Y. Cauvin. </author> <title> A back-propagation algorithm with optimal use of hidden units. </title> <editor> In D. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems 1, </booktitle> <pages> pages 519-526, </pages> <year> 1989. </year>
Reference-contexts: We also give a brief introduction to the theory of dynamic embedding before discussing the results of time series prediction. In order to also optimize the architecture of the MLP of a NARX network or NSAR, several methods of weight-elimination <ref> [5, 31, 47, 64, 66] </ref> can be incorporated into the training algorithm. In the following experiments, networks are trained using weight decay [31].
Reference: [6] <author> S. Chen, S.A. Billings, and P.M. Grant. </author> <title> Non-linear system identification using neural networks. </title> <journal> International Journal of Control, </journal> <volume> 51(6) </volume> <pages> 1191-1214, </pages> <year> 1990. </year>
Reference-contexts: 1 Introduction NARX (Nonlinear AutoRegressive models with eXogenous inputs) recurrent neural architectures <ref> [6, 44] </ref>, as opposed to other recurrent neural models, have limited feedback architectures which come only from the output neuron instead of from hidden neurons. <p> The success of their algorithm had been implemented in identification of handwritten ZIP-codes by pruning the weights of feedforward networks [11, 10]. 2 NARX Neural Network An important and useful class of discrete-time nonlinear systems is the Nonlinear AutoRegressive with eXogenous inputs (NARX) model <ref> [6, 34, 39, 57, 58] </ref>: y (t) = f u (t D u ); : : : ; u (t 1); u (t); y (t D y ); : : : ; y (t 1) ; (1) where u (t) and y (t) represent input and output of the model at <p> When the function f can be approximated by a Multilayer Perceptron (MLP), the resulting system is called a NARX 4 recurrent neural network <ref> [6, 44] </ref>. Figure 1 shows a NARX networks with input-memory of order 2 and output-memory of order 3. <p> Figure 1 shows a NARX networks with input-memory of order 2 and output-memory of order 3. It has been demonstrated that NARX neural networks are well suited for modeling several nonlinear systems such as heat exchangers <ref> [6] </ref>, waste water treatment plants [57, 58], catalytic reforming systems in a petroleum refinery [58], nonlinear oscillations associated with multi-legged locomotion in biological systems [61], time series [9], and various artificial nonlinear systems [6, 44, 51]. <p> demonstrated that NARX neural networks are well suited for modeling several nonlinear systems such as heat exchangers [6], waste water treatment plants [57, 58], catalytic reforming systems in a petroleum refinery [58], nonlinear oscillations associated with multi-legged locomotion in biological systems [61], time series [9], and various artificial nonlinear systems <ref> [6, 44, 51] </ref>. When the output-memory order of NARX network is zero, a NARX network becomes a Time Delay Neural Network (TDNN) [32, 33, 63], which is simply a tapped delay line input into a MLP.
Reference: [7] <author> Yung-Fu Cheng and Delores M. Etter. </author> <title> Analysis of an adaptive technique for modeling sparse systems. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> 37(2) </volume> <pages> 254-264, </pages> <year> 1989. </year>
Reference-contexts: This algorithm iteratively determines the memory of a model based on the gradient information. Originally proposed by Etter, it was used as a "adaptive delay filter", which included variable delays taps as well as variable gains, for modeling several sparse systems <ref> [15, 7] </ref>. Recently others [4, 13, 35, 14] have also extended neural networks to include adaptable time delays. Because the error function of the adaptable time delays depends on the autocorrelation function of input signals [15, 7], the gradient of the delay operator will depend on the derivative of input signals. <p> which included variable delays taps as well as variable gains, for modeling several sparse systems <ref> [15, 7] </ref>. Recently others [4, 13, 35, 14] have also extended neural networks to include adaptable time delays. Because the error function of the adaptable time delays depends on the autocorrelation function of input signals [15, 7], the gradient of the delay operator will depend on the derivative of input signals. However, a closed form of the derivative of the input signal can not always be determined in general.
Reference: [8] <editor> D.S. Clouse, C.L. Giles, B.G. Horne, and G.W. Cottrell. </editor> <title> Time-delay neural networks: Representation and induction of finite state machines. </title> <journal> IEEE Transactions on Neural Networks. </journal> <note> Accepted. </note>
Reference-contexts: In the following experiments, networks are trained using weight decay [31]. All experiments were trained using Back-Propagation Through Time (BPTT) [68]. 4.1 Grammatical Inference: Learning A 512-state Finite Memory Ma chine NARX networks have been shown to be able to simulate and learn a class of finite state machines <ref> [8, 21] </ref>, called respectively definite and finite memory machines. When being trained on strings which are encoded as temporal sequences, NARX networks are able to "learn" rather large (hundreds to thousands of states) machines provided that they have enough memory and the logic implementation is not too complex.
Reference: [9] <author> J. Connor, L.E. Atlas, and D.R. Martin. </author> <title> Recurrent networks and NARMA modeling. </title> <editor> In J.E. Moody, S.J. Hanson, and R.P. Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 301-308, </pages> <year> 1992. </year>
Reference-contexts: It has been demonstrated that NARX neural networks are well suited for modeling several nonlinear systems such as heat exchangers [6], waste water treatment plants [57, 58], catalytic reforming systems in a petroleum refinery [58], nonlinear oscillations associated with multi-legged locomotion in biological systems [61], time series <ref> [9] </ref>, and various artificial nonlinear systems [6, 44, 51]. When the output-memory order of NARX network is zero, a NARX network becomes a Time Delay Neural Network (TDNN) [32, 33, 63], which is simply a tapped delay line input into a MLP.
Reference: [10] <author> Y. Le Cun, J. S. Denker, and S. A. Solla. </author> <title> Handwritten digit recognition with a backpropagation network. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <volume> volume 2, </volume> <pages> pages 396-404, </pages> <year> 1990. </year>
Reference-contexts: Le Cun et al. [11] originally calculated the "saliency" by estimating the second order derivative for each weight. The success of their algorithm had been implemented in identification of handwritten ZIP-codes by pruning the weights of feedforward networks <ref> [11, 10] </ref>. 2 NARX Neural Network An important and useful class of discrete-time nonlinear systems is the Nonlinear AutoRegressive with eXogenous inputs (NARX) model [6, 34, 39, 57, 58]: y (t) = f u (t D u ); : : : ; u (t 1); u (t); y (t D y
Reference: [11] <author> Y. Le Cun, J. S. Denker, and S. A. Solla. </author> <title> Optimal brain damage. </title> <editor> In D.S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 2, </volume> <pages> pages 598-605, </pages> <address> San Mateo, CA, 1990. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: The sensitive measure of each memory order is calculated by estimating the second order derivative of the error function with respect to each memory order. Le Cun et al. <ref> [11] </ref> originally calculated the "saliency" by estimating the second order derivative for each weight. <p> Le Cun et al. [11] originally calculated the "saliency" by estimating the second order derivative for each weight. The success of their algorithm had been implemented in identification of handwritten ZIP-codes by pruning the weights of feedforward networks <ref> [11, 10] </ref>. 2 NARX Neural Network An important and useful class of discrete-time nonlinear systems is the Nonlinear AutoRegressive with eXogenous inputs (NARX) model [6, 34, 39, 57, 58]: y (t) = f u (t D u ); : : : ; u (t 1); u (t); y (t D y
Reference: [12] <author> G. Cybenko. </author> <title> Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals, </title> <journal> and Systems, </journal> <volume> 2(4) </volume> <pages> 303-314, </pages> <year> 1989. </year> <month> 24 </month>
Reference-contexts: Once an embedding dimension is chosen, one remaining task is to approximate the mapping function f . It has been shown that a feedforward neural network with enough neurons is capable of approximating any nonlinear function to an arbitrary degree of accuracy <ref> [12, 19, 28, 29] </ref>. Neural networks thus can provide a good approximation to the function f . These arguments provide the basic motivation for the use of NARX networks to the nonlinear time series prediction.
Reference: [13] <author> J. E. Dayhoff D.-T. Lin and P. A. Ligomenides. </author> <title> A learning algorithm for adaptive time-delays in a temporal neural network. </title> <type> Technical Report SRC-TR-92-59, </type> <institution> Systems Research Center, University of Maryland, College Park, Maryland, </institution> <year> 1992. </year>
Reference-contexts: This algorithm iteratively determines the memory of a model based on the gradient information. Originally proposed by Etter, it was used as a "adaptive delay filter", which included variable delays taps as well as variable gains, for modeling several sparse systems [15, 7]. Recently others <ref> [4, 13, 35, 14] </ref> have also extended neural networks to include adaptable time delays. Because the error function of the adaptable time delays depends on the autocorrelation function of input signals [15, 7], the gradient of the delay operator will depend on the derivative of input signals.
Reference: [14] <author> Shawn P. Day and Michael R. Davenport. </author> <title> Continuous-time temporal back-propagation with adaptable time delays. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 4(2) </volume> <pages> 348-354, </pages> <year> 1993. </year>
Reference-contexts: This algorithm iteratively determines the memory of a model based on the gradient information. Originally proposed by Etter, it was used as a "adaptive delay filter", which included variable delays taps as well as variable gains, for modeling several sparse systems [15, 7]. Recently others <ref> [4, 13, 35, 14] </ref> have also extended neural networks to include adaptable time delays. Because the error function of the adaptable time delays depends on the autocorrelation function of input signals [15, 7], the gradient of the delay operator will depend on the derivative of input signals.
Reference: [15] <author> D. M. Etter and S. D. Stearns. </author> <title> Adaptive estimation of time delays in sampled data systems. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> ASSP-29(3):582-587, </volume> <year> 1981. </year>
Reference-contexts: This algorithm iteratively determines the memory of a model based on the gradient information. Originally proposed by Etter, it was used as a "adaptive delay filter", which included variable delays taps as well as variable gains, for modeling several sparse systems <ref> [15, 7] </ref>. Recently others [4, 13, 35, 14] have also extended neural networks to include adaptable time delays. Because the error function of the adaptable time delays depends on the autocorrelation function of input signals [15, 7], the gradient of the delay operator will depend on the derivative of input signals. <p> which included variable delays taps as well as variable gains, for modeling several sparse systems <ref> [15, 7] </ref>. Recently others [4, 13, 35, 14] have also extended neural networks to include adaptable time delays. Because the error function of the adaptable time delays depends on the autocorrelation function of input signals [15, 7], the gradient of the delay operator will depend on the derivative of input signals. However, a closed form of the derivative of the input signal can not always be determined in general.
Reference: [16] <author> David B. Fogel. </author> <title> An information criterion for optimal neural network selection. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 2(5) </volume> <pages> 490-497, </pages> <year> 1991. </year>
Reference-contexts: Such models are judged on their "goodness-of-fit", which is a function of the likelihood of the data given the hypothesized model and its associated degrees of freedom. Fogel <ref> [16] </ref> applied the modification of AIC to select a "best" network. However, the AIC method is complex and can be troubled by imprecision [55, 25]. Such model complexity and regularization methods are readily used for nonlinear models such as neural networks; see for example [24, 42, 67].
Reference: [17] <author> David B. Fogel. </author> <title> Using evolutionary programming for modeling: An ocean acoustic example. </title> <journal> IEEE Journal of Oceanic Engineering, </journal> <volume> 17(4) </volume> <pages> 333-340, </pages> <year> 1992. </year>
Reference-contexts: Evolutionary Programming [2, 18] is another search mechanism. This algorithm operates on a population of models. Offspring models are created by randomly mutating parents models. Competition between offspring models for survival are judged according to the fitness function. Fogel <ref> [17] </ref> used evolutionary programming for order selections of linear models in a time series of ocean acoustic data. But the algorithm can be computationally expensive when the underling process is complex and nonlinear.
Reference: [18] <author> David B. Fogel. </author> <title> Evolutionary programming: An introduction and some current directions. </title> <journal> Statistics and Computing, </journal> <volume> 4 </volume> <pages> 113-129, </pages> <year> 1994. </year>
Reference-contexts: However, the AIC method is complex and can be troubled by imprecision [55, 25]. Such model complexity and regularization methods are readily used for nonlinear models such as neural networks; see for example [24, 42, 67]. Evolutionary Programming <ref> [2, 18] </ref> is another search mechanism. This algorithm operates on a population of models. Offspring models are created by randomly mutating parents models. Competition between offspring models for survival are judged according to the fitness function.
Reference: [19] <author> K. Funahashi. </author> <title> On the approximate realization of continuous mappings by neural networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 2(3) </volume> <pages> 183-192, </pages> <year> 1989. </year>
Reference-contexts: Once an embedding dimension is chosen, one remaining task is to approximate the mapping function f . It has been shown that a feedforward neural network with enough neurons is capable of approximating any nonlinear function to an arbitrary degree of accuracy <ref> [12, 19, 28, 29] </ref>. Neural networks thus can provide a good approximation to the function f . These arguments provide the basic motivation for the use of NARX networks to the nonlinear time series prediction.
Reference: [20] <author> C. L. Giles, B. G. Horne, and T. Lin. </author> <title> Learning a class of large finite state machines with a recurrent neural network. </title> <institution> Technical Report UMIACS-TR-94-94 and CS-TR-3328, Institute for Advanced Computer Studies, University of Maryland, College Park, Maryland, </institution> <year> 1994. </year>
Reference-contexts: The training set was 300 strings randomly chosen from the complete set. The complete set, which consists of all strings of length from 1 to d + 1 (10 in this case), are shown to be able to sufficiently identify a finite memory machine with depth d <ref> [20] </ref> . The strings were encoded such that input values of 0s and 1s and target output labels "negative" and "positive" corresponded to floating point values of 0:0 and 1:0 respectively. <p> The networks were trained with Back Propagation Through Time (BPTT) algorithm at the learning rate of 0:1 and weight decay of 0:001. The training time was set to 5000 epochs. For more details, see <ref> [20, 21] </ref>. For each of 50 experiments, the weights were randomly initialized within the range of [0:5; 0:5]. The average training time was approximately 600 epochs. After training, the trained networks were tested on the remaining strings of the complete set.
Reference: [21] <author> C.L. Giles, B.G. Horne, and T. Lin. </author> <title> Learning a class of large finite state machines with a recurrent neural network. </title> <booktitle> Neural Networks, </booktitle> <volume> 8(9) </volume> <pages> 1359-1365, </pages> <year> 1995. </year>
Reference-contexts: In the following experiments, networks are trained using weight decay [31]. All experiments were trained using Back-Propagation Through Time (BPTT) [68]. 4.1 Grammatical Inference: Learning A 512-state Finite Memory Ma chine NARX networks have been shown to be able to simulate and learn a class of finite state machines <ref> [8, 21] </ref>, called respectively definite and finite memory machines. When being trained on strings which are encoded as temporal sequences, NARX networks are able to "learn" rather large (hundreds to thousands of states) machines provided that they have enough memory and the logic implementation is not too complex. <p> The networks were trained with Back Propagation Through Time (BPTT) algorithm at the learning rate of 0:1 and weight decay of 0:001. The training time was set to 5000 epochs. For more details, see <ref> [20, 21] </ref>. For each of 50 experiments, the weights were randomly initialized within the range of [0:5; 0:5]. The average training time was approximately 600 epochs. After training, the trained networks were tested on the remaining strings of the complete set.
Reference: [22] <author> C.L. Giles, T. Lin, and B.G. </author> <title> Horne. Remembering the past: The role of embedded memory in recurrent neural network architectures. </title> <booktitle> In Neural Networks for Signal Processing VII, Proceedings of The 1997 IEEE Workshop, </booktitle> <address> Piscataway, NJ, 1997. </address> <publisher> IEEE Press. In Press. </publisher>
Reference-contexts: Recently, it has been shown that such embedded memory can help gradient-based learning in other recurrent neural network architectures <ref> [22] </ref>. Not only can the embedded memory reduce the sensitivity to long-term dependencies, but it also plays an important role in learning capability and generalization performance [37]. In particular, forecasting performance could be seriously deficient if a model's memory architecture is either either too little or too much memory.
Reference: [23] <author> C.L. Giles and C.W. Omlin. </author> <title> Pruning recurrent neural networks for improved generalization performance. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 5(5) </volume> <pages> 848-851, </pages> <year> 1994. </year>
Reference-contexts: After pruning, the network is retrained. Of course, this procedure can be iterated. This method should be contrasted to other recurrent neural network pruning procedures where recurrent nodes are pruned based on output values <ref> [23] </ref> and where second-order methods are used to prune input taps and single order feedback taps for fully recurrent neural networks [50]. The sensitive measure of each memory order is calculated by estimating the second order derivative of the error function with respect to each memory order.
Reference: [24] <author> F. Girosi and T. Poggio. </author> <title> Networks and the best approximation property. </title> <journal> Biological Cybernetics, </journal> <volume> 63 </volume> <pages> 169-176, </pages> <year> 1990. </year>
Reference-contexts: Fogel [16] applied the modification of AIC to select a "best" network. However, the AIC method is complex and can be troubled by imprecision [55, 25]. Such model complexity and regularization methods are readily used for nonlinear models such as neural networks; see for example <ref> [24, 42, 67] </ref>. Evolutionary Programming [2, 18] is another search mechanism. This algorithm operates on a population of models. Offspring models are created by randomly mutating parents models. Competition between offspring models for survival are judged according to the fitness function.
Reference: [25] <author> E. J. Hannan and B. G. Quinn. </author> <title> The determination of the order of an autoregression. </title> <journal> J. Royal Stat. Soc. B., </journal> <volume> 41 </volume> <pages> 190-195, </pages> <year> 1979. </year>
Reference-contexts: Fogel [16] applied the modification of AIC to select a "best" network. However, the AIC method is complex and can be troubled by imprecision <ref> [55, 25] </ref>. Such model complexity and regularization methods are readily used for nonlinear models such as neural networks; see for example [24, 42, 67]. Evolutionary Programming [2, 18] is another search mechanism. This algorithm operates on a population of models. Offspring models are created by randomly mutating parents models.
Reference: [26] <author> Babak Hassibi and David G. Stork. </author> <title> Second order derivatives for network pruning: Optimal brain surgeon. </title> <editor> In S.J. Hanson, J.D. Cowan, and C.L. Giles, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <address> San Mateo, CA, 1993. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: Several methods for sensitivity calculations have been proposed <ref> [43, 30, 26] </ref>, for details see the survey paper by Reed [52]. Our method of calculating sensitivity is based on evaluating the second order derivative of the cost function with respect to each memory order [50].
Reference: [27] <author> B.G. Horne and C.L. Giles. </author> <title> An experimental comparison of recurrent neural networks. </title> <booktitle> In Advances in Neural Information Processing Systems 7, </booktitle> <pages> pages 697-704. </pages> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: Not only are NARX neural networks computationally powerful in theory, but they have several advantages in practice. For example, it has been reported that gradient-descent learning can be more effective in NARX networks than in other recurrent architectures with "hidden states" <ref> [27] </ref>. Part of the reason can be attributed to the embedded memory of NARX networks.
Reference: [28] <author> K. Hornik, M. Stinchcombe, and H. White. </author> <title> Multilayer feedforward networks are universal approximators. </title> <booktitle> Neural Networks, </booktitle> <volume> 2(5) </volume> <pages> 359-366, </pages> <year> 1989. </year> <month> 25 </month>
Reference-contexts: Once an embedding dimension is chosen, one remaining task is to approximate the mapping function f . It has been shown that a feedforward neural network with enough neurons is capable of approximating any nonlinear function to an arbitrary degree of accuracy <ref> [12, 19, 28, 29] </ref>. Neural networks thus can provide a good approximation to the function f . These arguments provide the basic motivation for the use of NARX networks to the nonlinear time series prediction.
Reference: [29] <author> B. Irie and S. Miyake. </author> <title> Capabilities of three-layered perceptrons. </title> <booktitle> In Proceedings of the IEEE International Conference on Neural Networks, </booktitle> <volume> volume 1, </volume> <pages> pages 641-648, </pages> <address> San Diego, CA, 1988. </address> <publisher> IEEE. </publisher>
Reference-contexts: Once an embedding dimension is chosen, one remaining task is to approximate the mapping function f . It has been shown that a feedforward neural network with enough neurons is capable of approximating any nonlinear function to an arbitrary degree of accuracy <ref> [12, 19, 28, 29] </ref>. Neural networks thus can provide a good approximation to the function f . These arguments provide the basic motivation for the use of NARX networks to the nonlinear time series prediction.
Reference: [30] <author> E.D. Karnin. </author> <title> A simple procedure for pruning back-propagation trained neural networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 1(2) </volume> <pages> 239-242, </pages> <year> 1990. </year>
Reference-contexts: Several methods for sensitivity calculations have been proposed <ref> [43, 30, 26] </ref>, for details see the survey paper by Reed [52]. Our method of calculating sensitivity is based on evaluating the second order derivative of the cost function with respect to each memory order [50].
Reference: [31] <author> A. Krogh and J.A. Hertz. </author> <title> A simple weight decay can improve generalization. </title> <editor> In J.E. Moody, S.J. Hanson, and R.P. Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 950-957, </pages> <year> 1992. </year>
Reference-contexts: In this paper, we propose a pruning-based algorithm, the Delay Damage Algorithm, to determine the optimal memory-order of NARX and input time delay neural networks. This algorithm can also incorporate several useful heuristics, such as weight decay <ref> [31] </ref>, which are used extensively in static networks to optimize the nonlinear function. [For a survey of pruning methods for feedforward neural networks, see [52].] The procedure of the algorithm starts with a NARX network with enough degrees of freedom in both input and output memory or taps, and then delete <p> We also give a brief introduction to the theory of dynamic embedding before discussing the results of time series prediction. In order to also optimize the architecture of the MLP of a NARX network or NSAR, several methods of weight-elimination <ref> [5, 31, 47, 64, 66] </ref> can be incorporated into the training algorithm. In the following experiments, networks are trained using weight decay [31]. <p> In order to also optimize the architecture of the MLP of a NARX network or NSAR, several methods of weight-elimination [5, 31, 47, 64, 66] can be incorporated into the training algorithm. In the following experiments, networks are trained using weight decay <ref> [31] </ref>. All experiments were trained using Back-Propagation Through Time (BPTT) [68]. 4.1 Grammatical Inference: Learning A 512-state Finite Memory Ma chine NARX networks have been shown to be able to simulate and learn a class of finite state machines [8, 21], called respectively definite and finite memory machines.
Reference: [32] <author> K.J. Lang, A.H. Waibel, and G.E. Hinton. </author> <title> A time-delay neural network architecture for isolated word recognition. </title> <booktitle> Neural Networks, </booktitle> <volume> 3(1) </volume> <pages> 23-44, </pages> <year> 1990. </year>
Reference-contexts: When the output-memory order of NARX network is zero, a NARX network becomes a Time Delay Neural Network (TDNN) <ref> [32, 33, 63] </ref>, which is simply a tapped delay line input into a MLP.
Reference: [33] <author> A. Lapedes and R. Farber. </author> <title> Nonlinear signal processing using neural networks: Prediction and signal modeling. </title> <type> Technical Report LA-UR-87-2662, </type> <institution> Los Alamos National Laboratories, </institution> <address> Los Alamos, New Mexico, </address> <year> 1987. </year>
Reference-contexts: When the output-memory order of NARX network is zero, a NARX network becomes a Time Delay Neural Network (TDNN) <ref> [32, 33, 63] </ref>, which is simply a tapped delay line input into a MLP.
Reference: [34] <author> I.J. Leontaritis and S.A. Billings. </author> <title> Input-output parametric models for non-linear systems: Part I: deterministic non-linear systems. </title> <journal> International Journal of Control, </journal> <volume> 41(2) </volume> <pages> 303-328, </pages> <year> 1985. </year>
Reference-contexts: The success of their algorithm had been implemented in identification of handwritten ZIP-codes by pruning the weights of feedforward networks [11, 10]. 2 NARX Neural Network An important and useful class of discrete-time nonlinear systems is the Nonlinear AutoRegressive with eXogenous inputs (NARX) model <ref> [6, 34, 39, 57, 58] </ref>: y (t) = f u (t D u ); : : : ; u (t 1); u (t); y (t D y ); : : : ; y (t 1) ; (1) where u (t) and y (t) represent input and output of the model at
Reference: [35] <author> D.T. Lin, J.E. Dayhoff, </author> <title> and P.A. Ligomenides. Trajectory production with the adaptive time-delay neural network. </title> <booktitle> Neural Networks, </booktitle> <volume> 8(3) </volume> <pages> 447-461, </pages> <year> 1995. </year>
Reference-contexts: This algorithm iteratively determines the memory of a model based on the gradient information. Originally proposed by Etter, it was used as a "adaptive delay filter", which included variable delays taps as well as variable gains, for modeling several sparse systems [15, 7]. Recently others <ref> [4, 13, 35, 14] </ref> have also extended neural networks to include adaptable time delays. Because the error function of the adaptable time delays depends on the autocorrelation function of input signals [15, 7], the gradient of the delay operator will depend on the derivative of input signals.
Reference: [36] <author> T. Lin, B.G. Horne, P. Tino, and C.L. Giles. </author> <title> Learning long-term dependencies in narx recurrent neural networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 7(6) </volume> <pages> 1329-1338, </pages> <year> 1996. </year>
Reference-contexts: This embedded memory will appear as jump-ahead connections which provide shorter paths for propagating gradient information more efficiently when the networks are unfolded in time to backpropagate the error signal and thus reduce the network's sensitivity to the problem of long-term dependencies <ref> [36, 38] </ref>. Recently, it has been shown that such embedded memory can help gradient-based learning in other recurrent neural network architectures [22]. Not only can the embedded memory reduce the sensitivity to long-term dependencies, but it also plays an important role in learning capability and generalization performance [37].
Reference: [37] <author> Tsungnan Lin, B.G. Horne, and C.L. Giles. </author> <title> How memory orders effect the performance of narx networks. </title> <institution> Technical Report UMIACS-TR-96-76 and CS-TR-3706, Institute for Advanced Computer Studies, University of Maryland, College Park, Maryland, </institution> <year> 1996. </year>
Reference-contexts: Recently, it has been shown that such embedded memory can help gradient-based learning in other recurrent neural network architectures [22]. Not only can the embedded memory reduce the sensitivity to long-term dependencies, but it also plays an important role in learning capability and generalization performance <ref> [37] </ref>. In particular, forecasting performance could be seriously deficient if a model's memory architecture is either either too little or too much memory. Therefore, choosing the appropriate memory architectures for a given task is a critical issue in NARX networks. <p> However, the generalization performance and the size of extracted machines are also found to be very sensitive to the memory order selections in NARX networks <ref> [37] </ref>. The purpose of this experiment is to see how the delay damage algorithm can improve the generalization performance of NARX networks with unnecessary memory structures. In this experiment, the finite memory machine has 512 states. The machine has input order of 5 and output order of 4. <p> Note that the performance of NARX networks can be strongly dependent on the selection of memory order <ref> [37] </ref>. In particular the new testing set consisted of 250 positive strings and 250 negative strings from length 20 to 150 in increments of 10.
Reference: [38] <author> Tsungnan Lin, B.G. Horne, P. Tino, and C.L. Giles. </author> <title> Learning long-term dependencies is not as difficult with narx recurrent neural networks. </title> <booktitle> In Advances in Neural Information Processing Systems 8. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1996. </year>
Reference-contexts: This embedded memory will appear as jump-ahead connections which provide shorter paths for propagating gradient information more efficiently when the networks are unfolded in time to backpropagate the error signal and thus reduce the network's sensitivity to the problem of long-term dependencies <ref> [36, 38] </ref>. Recently, it has been shown that such embedded memory can help gradient-based learning in other recurrent neural network architectures [22]. Not only can the embedded memory reduce the sensitivity to long-term dependencies, but it also plays an important role in learning capability and generalization performance [37].
Reference: [39] <author> L. Ljung. </author> <title> System identification : Theory for the user. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1987. </year>
Reference-contexts: The success of their algorithm had been implemented in identification of handwritten ZIP-codes by pruning the weights of feedforward networks [11, 10]. 2 NARX Neural Network An important and useful class of discrete-time nonlinear systems is the Nonlinear AutoRegressive with eXogenous inputs (NARX) model <ref> [6, 34, 39, 57, 58] </ref>: y (t) = f u (t D u ); : : : ; u (t 1); u (t); y (t D y ); : : : ; y (t 1) ; (1) where u (t) and y (t) represent input and output of the model at
Reference: [40] <author> G. A. N. Mbamalu and M. E. El-Hawary. </author> <title> Load forecasting via suboptimal seasonal autoregres-sive models and iteratively reweighted least squares estimation. </title> <journal> IEEE Transactions on Power Systems, </journal> <volume> 8(1) </volume> <pages> 343-348, </pages> <year> 1993. </year>
Reference-contexts: SAR models have demonstrated their long-term prediction capability in various applications <ref> [40, 45] </ref> and can easily be extended into nonlinear models. A nonlinear version of a SAR is a NSAR 2 . A primary problem associated with the nonlinear subset model is how to optimally select the subset orders.
Reference: [41] <author> D.C. Montgomery and E.A. Peck. </author> <title> Introduction to Linear Regression Analysis. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1982. </year>
Reference-contexts: possible so that the information content in these regressors will influence the predicted value of the dependent variable; on the other hand, it is also desired that the model includes as few regressors as possible because the variance of the model's predictions increases along with the increasing number of regressors <ref> [41] </ref>. 2 According to the embedding theorem [48, 54, 60], the memory orders need to be large enough in order to provide a sufficient embedding. The problem of choosing the proper memory architecture corresponds to give a good representation of input data.
Reference: [42] <author> J.E. Moody. </author> <title> The effective number of parameters: An analysis of generalization and regularization in nonlinear learning systems. </title> <editor> In J.E. Moody, S. J. Hanson, and R. P. Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 4, </volume> <pages> pages 847-854. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1992. </year> <month> 26 </month>
Reference-contexts: Fogel [16] applied the modification of AIC to select a "best" network. However, the AIC method is complex and can be troubled by imprecision [55, 25]. Such model complexity and regularization methods are readily used for nonlinear models such as neural networks; see for example <ref> [24, 42, 67] </ref>. Evolutionary Programming [2, 18] is another search mechanism. This algorithm operates on a population of models. Offspring models are created by randomly mutating parents models. Competition between offspring models for survival are judged according to the fitness function.
Reference: [43] <author> M. C. Mozer and P. Smolensky. </author> <title> Skeletonization: A technique for trimming the fat from a network via relevance assessment. </title> <journal> Connection Science, </journal> <volume> 11 </volume> <pages> 3-26, </pages> <year> 1989. </year>
Reference-contexts: Several methods for sensitivity calculations have been proposed <ref> [43, 30, 26] </ref>, for details see the survey paper by Reed [52]. Our method of calculating sensitivity is based on evaluating the second order derivative of the cost function with respect to each memory order [50].
Reference: [44] <author> K.S. Narendra and K. Parthasarathy. </author> <title> Identification and control of dynamical systems using neural networks. </title> <journal> IEEE Trans. on Neural Networks, </journal> <volume> 1(1) </volume> <pages> 4-27, </pages> <year> 1990. </year>
Reference-contexts: 1 Introduction NARX (Nonlinear AutoRegressive models with eXogenous inputs) recurrent neural architectures <ref> [6, 44] </ref>, as opposed to other recurrent neural models, have limited feedback architectures which come only from the output neuron instead of from hidden neurons. <p> When the function f can be approximated by a Multilayer Perceptron (MLP), the resulting system is called a NARX 4 recurrent neural network <ref> [6, 44] </ref>. Figure 1 shows a NARX networks with input-memory of order 2 and output-memory of order 3. <p> demonstrated that NARX neural networks are well suited for modeling several nonlinear systems such as heat exchangers [6], waste water treatment plants [57, 58], catalytic reforming systems in a petroleum refinery [58], nonlinear oscillations associated with multi-legged locomotion in biological systems [61], time series [9], and various artificial nonlinear systems <ref> [6, 44, 51] </ref>. When the output-memory order of NARX network is zero, a NARX network becomes a Time Delay Neural Network (TDNN) [32, 33, 63], which is simply a tapped delay line input into a MLP.
Reference: [45] <author> Gil Nave and Arnon Cohen. </author> <title> Ecg compression using long-term prediction. </title> <journal> IEEE Transactions on Biomedical Engineering, </journal> <volume> 40(9) </volume> <pages> 877-885, </pages> <year> 1993. </year>
Reference-contexts: SAR models have demonstrated their long-term prediction capability in various applications <ref> [40, 45] </ref> and can easily be extended into nonlinear models. A nonlinear version of a SAR is a NSAR 2 . A primary problem associated with the nonlinear subset model is how to optimally select the subset orders. <p> However, these distinctions are not always made nor are standard. For example, NARX networks have also been called NARMA (Nonlinear AutoRegressive Moving Average) networks. It would also be possible to refer to a NSAR as a NAR model. 5 case <ref> [69, 45] </ref>. In designing the nonlinear sparse models, we can determine the memory order by applying the Delay Damage Algorithm, described in the next section.
Reference: [46] <author> R. Ma ne. </author> <title> On the dimension of the compact invariant sets of certain nonlinear maps. </title> <booktitle> In Lecture Notes in Math. </booktitle> <volume> No. 898, </volume> <pages> page 230. </pages> <publisher> Springer-Verlag, </publisher> <year> 1981. </year>
Reference-contexts: The purpose of time-delay embedding is to unfold the projection back to a multivariate state space that is representative of the original system <ref> [46, 49, 60] </ref>. It was shown that if the dynamical system and the observed quantity were generic, then the delay coordinate map from a d-dimensional smooth compact manifold to 2d + 1-dimensional reconstruction space was a diffeomorphism (one-to-one differential mapping).
Reference: [47] <author> S.J. Nowlan and G.E. Hinton. </author> <title> Simplifying neural networks by soft weight-sharing. </title> <journal> Neural Computation, </journal> <volume> 4(4) </volume> <pages> 473-493, </pages> <year> 1992. </year>
Reference-contexts: We also give a brief introduction to the theory of dynamic embedding before discussing the results of time series prediction. In order to also optimize the architecture of the MLP of a NARX network or NSAR, several methods of weight-elimination <ref> [5, 31, 47, 64, 66] </ref> can be incorporated into the training algorithm. In the following experiments, networks are trained using weight decay [31].
Reference: [48] <author> Edward Ott, Tim Sauer, and James Yorke. </author> <title> Coping with Chaos. </title> <publisher> John Wiley and Sons, Inc, </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference-contexts: these regressors will influence the predicted value of the dependent variable; on the other hand, it is also desired that the model includes as few regressors as possible because the variance of the model's predictions increases along with the increasing number of regressors [41]. 2 According to the embedding theorem <ref> [48, 54, 60] </ref>, the memory orders need to be large enough in order to provide a sufficient embedding. The problem of choosing the proper memory architecture corresponds to give a good representation of input data. A good representation can make useful information explicit and easy to extract.
Reference: [49] <author> N. Packard, J. Crutchfield, D. Farmer, and R. Shaw. </author> <title> Geometry from a time series. </title> <journal> Physical Review Letter, </journal> <volume> 45 </volume> <pages> 712-715, </pages> <year> 1980. </year>
Reference-contexts: The purpose of time-delay embedding is to unfold the projection back to a multivariate state space that is representative of the original system <ref> [46, 49, 60] </ref>. It was shown that if the dynamical system and the observed quantity were generic, then the delay coordinate map from a d-dimensional smooth compact manifold to 2d + 1-dimensional reconstruction space was a diffeomorphism (one-to-one differential mapping).
Reference: [50] <author> M.W. Pedersen and L.K. Hansen. </author> <title> Recurrent networks: Second order properties and pruning. </title> <editor> In G. Tesauro, D. Touretzky, and T. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7. </booktitle> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: Of course, this procedure can be iterated. This method should be contrasted to other recurrent neural network pruning procedures where recurrent nodes are pruned based on output values [23] and where second-order methods are used to prune input taps and single order feedback taps for fully recurrent neural networks <ref> [50] </ref>. The sensitive measure of each memory order is calculated by estimating the second order derivative of the error function with respect to each memory order. Le Cun et al. [11] originally calculated the "saliency" by estimating the second order derivative for each weight. <p> Several methods for sensitivity calculations have been proposed [43, 30, 26], for details see the survey paper by Reed [52]. Our method of calculating sensitivity is based on evaluating the second order derivative of the cost function with respect to each memory order <ref> [50] </ref>. We assume the cost function (E) is the mean-squared error: E = 2 p t=t 0 6 where p and t denote the pattern index and time index respectively.
Reference: [51] <author> S.-Z. Qin, H.-T. Su, and T.J. McAvoy. </author> <title> Comparison of four neural net learning methods for dynamic system identification. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 3(1) </volume> <pages> 122-130, </pages> <year> 1992. </year>
Reference-contexts: demonstrated that NARX neural networks are well suited for modeling several nonlinear systems such as heat exchangers [6], waste water treatment plants [57, 58], catalytic reforming systems in a petroleum refinery [58], nonlinear oscillations associated with multi-legged locomotion in biological systems [61], time series [9], and various artificial nonlinear systems <ref> [6, 44, 51] </ref>. When the output-memory order of NARX network is zero, a NARX network becomes a Time Delay Neural Network (TDNN) [32, 33, 63], which is simply a tapped delay line input into a MLP.
Reference: [52] <author> Russel Reed. </author> <title> Pruning algorithms| a survey. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 4(5) </volume> <pages> 740-747, </pages> <year> 1993. </year>
Reference-contexts: This algorithm can also incorporate several useful heuristics, such as weight decay [31], which are used extensively in static networks to optimize the nonlinear function. [For a survey of pruning methods for feedforward neural networks, see <ref> [52] </ref>.] The procedure of the algorithm starts with a NARX network with enough degrees of freedom in both input and output memory or taps, and then delete those memory orders with small sensitivity measure after training. After pruning, the network is retrained. Of course, this procedure can be iterated. <p> Several methods for sensitivity calculations have been proposed [43, 30, 26], for details see the survey paper by Reed <ref> [52] </ref>. Our method of calculating sensitivity is based on evaluating the second order derivative of the cost function with respect to each memory order [50].
Reference: [53] <author> J. Rissanen. </author> <title> Universal coding, information, prediction, and estimation. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-30:629-636, </volume> <year> 1984. </year>
Reference-contexts: When there is no prior knowledge about the model of the underling process, traditional statistical tests can be used, for example, Akaike information criterion (AIC) [1] and the minimum description length principle (MDL) <ref> [53] </ref>. Such models are judged on their "goodness-of-fit", which is a function of the likelihood of the data given the hypothesized model and its associated degrees of freedom. Fogel [16] applied the modification of AIC to select a "best" network.
Reference: [54] <author> T. Sauer, J. Yorke, and M. Casdagli. </author> <title> Embedology. </title> <journal> Journal of Statistical Physics, </journal> <volume> 65 </volume> <pages> 579-616, </pages> <year> 1991. </year>
Reference-contexts: these regressors will influence the predicted value of the dependent variable; on the other hand, it is also desired that the model includes as few regressors as possible because the variance of the model's predictions increases along with the increasing number of regressors [41]. 2 According to the embedding theorem <ref> [48, 54, 60] </ref>, the memory orders need to be large enough in order to provide a sufficient embedding. The problem of choosing the proper memory architecture corresponds to give a good representation of input data. A good representation can make useful information explicit and easy to extract. <p> strings for a NARX neural network trained on the data from the 512 state finite memory machine. 11 4.2 The Theory of Embedding In order to clarify the results of our experiments on time series prediction we give a brief introduction to the theory of embedding, for more details see <ref> [54] </ref>. The state of a deterministic dynamic system is the information necessary to determine the entire future evolution of the system. A time series is a set of measures of an observable quantity of the system over time. <p> It was shown that if the dynamical system and the observed quantity were generic, then the delay coordinate map from a d-dimensional smooth compact manifold to 2d + 1-dimensional reconstruction space was a diffeomorphism (one-to-one differential mapping). The theorem was further refined by Sauer et al. <ref> [54] </ref> such that a measured quantity led to a one-to-one delay coordinate map as long as the reconstruction dimension was greater than twice the box-counting dimension of the attractor.
Reference: [55] <author> R. Shibata. </author> <title> Various model selection techniques in time series analysis. </title> <journal> In Handbood of statistics, </journal> <volume> volume 5, </volume> <pages> pages 179-187. </pages> <editor> Eds. </editor> <publisher> Amsterdam: Elsevier, </publisher> <year> 1985. </year>
Reference-contexts: Fogel [16] applied the modification of AIC to select a "best" network. However, the AIC method is complex and can be troubled by imprecision <ref> [55, 25] </ref>. Such model complexity and regularization methods are readily used for nonlinear models such as neural networks; see for example [24, 42, 67]. Evolutionary Programming [2, 18] is another search mechanism. This algorithm operates on a population of models. Offspring models are created by randomly mutating parents models.
Reference: [56] <author> H.T. Siegelmann, B.G. Horne, and C.L. Giles. </author> <title> Computational capabilities of recurrent narx neural networks. </title> <journal> IEEE Trans. on Systems, Man and Cybernetics Part B: Cybernetics, </journal> <volume> 27(2):208, </volume> <year> 1997. </year>
Reference-contexts: It has been shown that in theory one can use NARX networks, rather than conventional recurrent networks, without any computational loss and that they are at least equivalent to Turing machines <ref> [56] </ref>. Not only are NARX neural networks computationally powerful in theory, but they have several advantages in practice. For example, it has been reported that gradient-descent learning can be more effective in NARX networks than in other recurrent architectures with "hidden states" [27].
Reference: [57] <author> H.-T. Su and T.J. McAvoy. </author> <title> Identification of chemical processes using recurrent networks. </title> <booktitle> In Proceedings of the American Controls Conference, </booktitle> <volume> volume 3, </volume> <pages> pages 2314-2319, </pages> <year> 1991. </year>
Reference-contexts: The success of their algorithm had been implemented in identification of handwritten ZIP-codes by pruning the weights of feedforward networks [11, 10]. 2 NARX Neural Network An important and useful class of discrete-time nonlinear systems is the Nonlinear AutoRegressive with eXogenous inputs (NARX) model <ref> [6, 34, 39, 57, 58] </ref>: y (t) = f u (t D u ); : : : ; u (t 1); u (t); y (t D y ); : : : ; y (t 1) ; (1) where u (t) and y (t) represent input and output of the model at <p> Figure 1 shows a NARX networks with input-memory of order 2 and output-memory of order 3. It has been demonstrated that NARX neural networks are well suited for modeling several nonlinear systems such as heat exchangers [6], waste water treatment plants <ref> [57, 58] </ref>, catalytic reforming systems in a petroleum refinery [58], nonlinear oscillations associated with multi-legged locomotion in biological systems [61], time series [9], and various artificial nonlinear systems [6, 44, 51].
Reference: [58] <author> H.-T. Su, T.J. McAvoy, and P. Werbos. </author> <title> Long-term predictions of chemical processes using recurrent neural networks: A parallel training approach. </title> <journal> Industrial Engineering and Chemical Research, </journal> <volume> 31 </volume> <pages> 1338-1352, </pages> <year> 1992. </year> <month> 27 </month>
Reference-contexts: The success of their algorithm had been implemented in identification of handwritten ZIP-codes by pruning the weights of feedforward networks [11, 10]. 2 NARX Neural Network An important and useful class of discrete-time nonlinear systems is the Nonlinear AutoRegressive with eXogenous inputs (NARX) model <ref> [6, 34, 39, 57, 58] </ref>: y (t) = f u (t D u ); : : : ; u (t 1); u (t); y (t D y ); : : : ; y (t 1) ; (1) where u (t) and y (t) represent input and output of the model at <p> Figure 1 shows a NARX networks with input-memory of order 2 and output-memory of order 3. It has been demonstrated that NARX neural networks are well suited for modeling several nonlinear systems such as heat exchangers [6], waste water treatment plants <ref> [57, 58] </ref>, catalytic reforming systems in a petroleum refinery [58], nonlinear oscillations associated with multi-legged locomotion in biological systems [61], time series [9], and various artificial nonlinear systems [6, 44, 51]. <p> It has been demonstrated that NARX neural networks are well suited for modeling several nonlinear systems such as heat exchangers [6], waste water treatment plants [57, 58], catalytic reforming systems in a petroleum refinery <ref> [58] </ref>, nonlinear oscillations associated with multi-legged locomotion in biological systems [61], time series [9], and various artificial nonlinear systems [6, 44, 51].
Reference: [59] <author> C. Svarer, L. K. Hansen, and J. Larsen. </author> <booktitle> On design and evaluation of tapped-dealy neural architectures. In IEEE International Conf. on Neural Networks, </booktitle> <pages> pages 46-51, </pages> <year> 1992. </year>
Reference-contexts: They were able to reduce a network with 8 hidden nodes to 3. The embedding dimension remained the same. Svarer et al. <ref> [59] </ref> pruned the weights using a second order sensitivity measure. Our approach is to prune the orders of networks directly and use the weight decay technique to optimize the nonlinear mapping function.
Reference: [60] <author> F. Takens. </author> <title> Detecting strange attractors in turbulence. </title> <booktitle> In Lecture Notes in Math. </booktitle> <volume> No. 898, </volume> <pages> pages 366-381. </pages> <publisher> Springer-Verlag, </publisher> <year> 1981. </year>
Reference-contexts: these regressors will influence the predicted value of the dependent variable; on the other hand, it is also desired that the model includes as few regressors as possible because the variance of the model's predictions increases along with the increasing number of regressors [41]. 2 According to the embedding theorem <ref> [48, 54, 60] </ref>, the memory orders need to be large enough in order to provide a sufficient embedding. The problem of choosing the proper memory architecture corresponds to give a good representation of input data. A good representation can make useful information explicit and easy to extract. <p> The purpose of time-delay embedding is to unfold the projection back to a multivariate state space that is representative of the original system <ref> [46, 49, 60] </ref>. It was shown that if the dynamical system and the observed quantity were generic, then the delay coordinate map from a d-dimensional smooth compact manifold to 2d + 1-dimensional reconstruction space was a diffeomorphism (one-to-one differential mapping).
Reference: [61] <author> S.T. Venkataraman. </author> <title> On encoding nonlinear oscillations in neural networks for locomotion. </title> <booktitle> In Proceedings of the Eighth Yale Workshop on Adaptive and Learning Systems, </booktitle> <pages> pages 14-20, </pages> <year> 1994. </year>
Reference-contexts: It has been demonstrated that NARX neural networks are well suited for modeling several nonlinear systems such as heat exchangers [6], waste water treatment plants [57, 58], catalytic reforming systems in a petroleum refinery [58], nonlinear oscillations associated with multi-legged locomotion in biological systems <ref> [61] </ref>, time series [9], and various artificial nonlinear systems [6, 44, 51]. When the output-memory order of NARX network is zero, a NARX network becomes a Time Delay Neural Network (TDNN) [32, 33, 63], which is simply a tapped delay line input into a MLP.
Reference: [62] <author> Bert de Vries and Jose Principe. </author> <title> The gamma model anew neural network for temporal processing. </title> <booktitle> Neural Networks, </booktitle> <volume> 5(4) </volume> <pages> 565-576, </pages> <year> 1992. </year>
Reference-contexts: A good representation can make useful information explicit and easy to extract. In this work we only explored classic tapped delay memory structures. It would be interesting to see if similar results could be achieved for other memory models <ref> [3, 62] </ref>. However, a minimal representation does not necessarily mean a good representation. Two different representations can be equivalent in terms of expressive power, but may make a great difference in efficiency and effectiveness of problem-solving. 23 6 Acknowledgements We would like to acknowledge useful discussions and suggestions by G.
Reference: [63] <author> A. Waibel, T. Hanazawa, G. Hinton, K. Shikano, and K. Lang. </author> <title> Phoneme recognition using time-delay neural networks. </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> 37(3) </volume> <pages> 328-339, </pages> <year> 1989. </year>
Reference-contexts: When the output-memory order of NARX network is zero, a NARX network becomes a Time Delay Neural Network (TDNN) <ref> [32, 33, 63] </ref>, which is simply a tapped delay line input into a MLP.
Reference: [64] <author> Andreas S. Weigend, Bernardo A. Huberman, and David E. Rumelhart. </author> <title> Prediction the future: A connectionist approach. </title> <journal> International Journal of Neural Systems, </journal> <volume> 1(3) </volume> <pages> 193-209, </pages> <year> 1990. </year>
Reference-contexts: We also give a brief introduction to the theory of dynamic embedding before discussing the results of time series prediction. In order to also optimize the architecture of the MLP of a NARX network or NSAR, several methods of weight-elimination <ref> [5, 31, 47, 64, 66] </ref> can be incorporated into the training algorithm. In the following experiments, networks are trained using weight decay [31]. <p> The series is shown in Figure 4 and has served as a benchmark for time series prediction problems. Several researchers have tested the prediction ability of neural networks on this data. For example, Weigend et al. <ref> [64] </ref> trained a NSAR (or TDNN) network with an embedded dimension of 12 with 8 hidden neurons and pruned the weights by adding a complexity term to the cost function. They were able to reduce a network with 8 hidden nodes to 3. The embedding dimension remained the same. <p> Svarer et al. [59] pruned the weights using a second order sensitivity measure. Our approach is to prune the orders of networks directly and use the weight decay technique to optimize the nonlinear mapping function. For comparison we treat the data in the same way as Weigend et al. <ref> [64] </ref>. and partition it into a training set from 1700 through 1920 and a testing set from 1921 to 1979. The data set was scaled to be in the range of [0; 1:0]. We tried various architectures with various number of hidden nodes and different embedding dimensions. <p> This measure removes the data's dependence on dynamic range and size of the data set. To make comparisons with previous work, the variance was normalized with the same value of 1535 <ref> [64] </ref>. <p> To do the long-term prediction, the predicted output is fed back into the neural network. Hence, the inputs consists of predicted value as opposed to actual data of the origin time series. We used the average relative I-times iterated prediction variance proposed by Weigend et al. <ref> [64] </ref> as a performance measure of the long-term behavior of the models.
Reference: [65] <author> A.S. Weigend and N.A. Gershenfeld. </author> <title> Time Series Prediction: Forecasting the future and understanding the past. </title> <publisher> Addison-Wesley, </publisher> <year> 1994. </year>
Reference-contexts: The data set consists of laser intensity collected from a laboratory experiment <ref> [65] </ref>. Although deterministic, its behavior is chaotic as seen in Figure 7. Figure 8 shows the normalized autocorrelation function of the first 1000 training data points. The networks were trained as a one-step ahead predictor using the first 1000 points. The data was scaled to zero mean and unit variance.
Reference: [66] <author> A.S. Weigend, B.A. Huberman, and D.E. Rumelhart. </author> <title> Predicting sunspots and exchange rates with connectionist networks. </title> <editor> In M. Casdagli and S. Eubank, editors, </editor> <booktitle> Nonlinear Modeling and Forecasting, SFI Studies in the Sciences of Complexity, </booktitle> <volume> volume 12. </volume> <publisher> Addison-Wesley, </publisher> <year> 1991. </year>
Reference-contexts: We also give a brief introduction to the theory of dynamic embedding before discussing the results of time series prediction. In order to also optimize the architecture of the MLP of a NARX network or NSAR, several methods of weight-elimination <ref> [5, 31, 47, 64, 66] </ref> can be incorporated into the training algorithm. In the following experiments, networks are trained using weight decay [31].
Reference: [67] <author> A.S. Weigend, D.E. Rumelhart, and B.A. Huberman. </author> <title> Generalization by weight-elimination with application to forecasting. </title> <editor> In R. P. Lippmann, J.E. Moody, and D. S. Touretzky, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 3, </volume> <pages> pages 875-882. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: Fogel [16] applied the modification of AIC to select a "best" network. However, the AIC method is complex and can be troubled by imprecision [55, 25]. Such model complexity and regularization methods are readily used for nonlinear models such as neural networks; see for example <ref> [24, 42, 67] </ref>. Evolutionary Programming [2, 18] is another search mechanism. This algorithm operates on a population of models. Offspring models are created by randomly mutating parents models. Competition between offspring models for survival are judged according to the fitness function.
Reference: [68] <author> R.J. Williams and D. Zipser. </author> <title> Gradient-based learning algorithms for recurrent networks and their computational complexity. </title> <editor> In Y. Chauvin and D. E. Rumelhart, editors, Back-propagation: </editor> <booktitle> Theory, Architectures and Applications, chapter 13, </booktitle> <pages> pages 433-486. </pages> <publisher> Lawrence Erlbaum Publishers, </publisher> <address> Hillsdale, N.J., </address> <year> 1995. </year>
Reference-contexts: In the following experiments, networks are trained using weight decay [31]. All experiments were trained using Back-Propagation Through Time (BPTT) <ref> [68] </ref>. 4.1 Grammatical Inference: Learning A 512-state Finite Memory Ma chine NARX networks have been shown to be able to simulate and learn a class of finite state machines [8, 21], called respectively definite and finite memory machines.
Reference: [69] <author> Gwo-Hsing Yu and Yow-Chano Lin. </author> <title> A methodology for selecting subset autoregressive time series models. </title> <journal> Journal of Time Series Analysis, </journal> <volume> 12(4) </volume> <pages> 363-373, </pages> <year> 1989. </year> <month> 28 </month>
Reference-contexts: However, these distinctions are not always made nor are standard. For example, NARX networks have also been called NARMA (Nonlinear AutoRegressive Moving Average) networks. It would also be possible to refer to a NSAR as a NAR model. 5 case <ref> [69, 45] </ref>. In designing the nonlinear sparse models, we can determine the memory order by applying the Delay Damage Algorithm, described in the next section.
References-found: 69

