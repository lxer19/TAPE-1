URL: http://www.cs.tamu.edu/faculty/rwerger/pubs/i3e.ps.gz
Refering-URL: http://www.cs.tamu.edu/faculty/rwerger/pubs/
Root-URL: http://www.cs.tamu.edu
Title: The LRPD Test: Speculative Run-Time Parallelization of Loops with Privatization and Reduction Parallelization  
Author: Lawrence Rauchwerger and David A. Padua 
Abstract: Current parallelizing compilers cannot identify a significant fraction of parallelizable loops because they have complex or statically insufficiently defined access patterns. As parallelizable loops arise frequently in practice, we advocate a novel framework for their identification: speculatively execute the loop as a doall, and apply a fully parallel data dependence test to determine if it had any cross-iteration dependences; if the test fails, then the loop is re-executed serially. Since, from our experience, a significant amount of the available parallelism in Fortran programs can be exploited by loops transformed through privatization and reduction parallelization, our methods can speculatively apply these transformations and then check their validity at run-time. Another important contribution of this paper is a novel method for reduction recognition which goes beyond syntactic pattern matching: it detects at run-time if the values stored in an array participate in a reduction operation, even if they are transferred through private variables and/or are affected by statically unpredictable control flow. We present experimental results on loops from the PERFECT Benchmarks which substantiate our claim that these techniques can yield significant speedups which are often superior to those obtainable by inspector/executor methods. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Santosh Abraham. </author> <type> Private Communication. </type> <institution> Hewlett Packard Laboratories, </institution> <year> 1994. </year>
Reference-contexts: dependent) the compiler should identify the references that have the potential to be unused and insert code to solve this problem 2 sum returns the number of non-zero elements in A w 3 any returns the "OR" of its vector operand's elements, i.e., any (v [1 : n]) = (v <ref> [1] </ref> _ v [2] _ : : : _ v [n]). 6 shadow arrays PD test 1 2 3 4 tw tm A w 0 1 0 1 3 2 A np 1 1 1 1 A w (:) ^ A np (:) 0 1 0 1 shadow arrays LPD test <p> A.1 A Processor-wise Version of the LPD Test The LPD Test determines whether a loop has any cross-iteration data dependences. It turns out that essentially the same method can be used to test whether the loop, as executed, has any cross-processor data dependences <ref> [1] </ref>. The only difference is that all checks in the test refer to processors rather than to iterations, i.e., replace "iteration" by "processor" in the description of the LPD test so that all iterations assigned to a processor are considered as one "super-iteration" by the test.
Reference: [2] <author> J. R. Allen, K. Kennedy, C. Porterfield, and J. Warren. </author> <title> Conversion of control dependence to data dependence. </title> <booktitle> In Proceedings of the 10th ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 177-189, </pages> <month> January </month> <year> 1983. </year>
Reference-contexts: should identify the references that have the potential to be unused and insert code to solve this problem 2 sum returns the number of non-zero elements in A w 3 any returns the "OR" of its vector operand's elements, i.e., any (v [1 : n]) = (v [1] _ v <ref> [2] </ref> _ : : : _ v [n]). 6 shadow arrays PD test 1 2 3 4 tw tm A w 0 1 0 1 3 2 A np 1 1 1 1 A w (:) ^ A np (:) 0 1 0 1 shadow arrays LPD test 1 2 3 <p> In (c), the procedure in (b) is called. The markredux operations are as described in Fig. 5. methods is a fairly straightforward demand driven forward substitution of all the variables on the RHS, a process by which all control flow dependences are substituted by data dependences as described in <ref> [2] </ref>, [40]. Once this expression of the RHS is obtained it can be analyzed and validated by the methods described in the previous section. In the following we explain by way of example how our new method can identify reductions.
Reference: [3] <author> T. Allen and D. A. Padua. </author> <title> Debugging fortran on a shared-memory machine. </title> <booktitle> In Proceedings of the 1987 International Conference on Parallel Processing, </booktitle> <pages> pages 721-727, </pages> <address> St. Charles, IL, </address> <year> 1987. </year>
Reference-contexts: Race Detection for Parallel Program Debugging A significant amount of work has been invested in the research of hazards (race conditions) and access anomalies for debugging parallel programs. Generally, access anomaly detection techniques seek to identify the point in the parallel execution at which the access anomaly occurred. In <ref> [3] </ref>, [16] the authors discuss methods that statically analyze the source program, and methods that analyze an execution trace of the program.
Reference: [4] <institution> Alliant Computer Systems Corporation. FX/Series Architecture Manual, </institution> <year> 1986. </year>
Reference-contexts: However, if this is not possible, then it may be the case that schedule reuse could be attempted when parallelizing the inner loops. V. Experimental Results In this section we present experimental results obtained on two modestly parallel machines with 8 (Alliant FX/80 <ref> [4] </ref>) and 14 processors (Alliant FX/2800 [5]) using a Fortran implementation of our run-time library. The codes were manually instrumented with calls to the run-time library.
Reference: [5] <institution> Alliant Computer Systems Corporation. Alliant FX/2800 Series System Description, </institution> <year> 1991. </year>
Reference-contexts: However, if this is not possible, then it may be the case that schedule reuse could be attempted when parallelizing the inner loops. V. Experimental Results In this section we present experimental results obtained on two modestly parallel machines with 8 (Alliant FX/80 [4]) and 14 processors (Alliant FX/2800 <ref> [5] </ref>) using a Fortran implementation of our run-time library. The codes were manually instrumented with calls to the run-time library. However, we remark that our results scale with the number of processors and the data size and thus can therefore be extrapolated for massively parallel processors (MPPs).
Reference: [6] <author> R. Ballance, A. Maccabe, and K. Ottenstein. </author> <title> The Program Dependence Web: a Representation Supporting Control Data- and Demand-Driven Interpretation of Imperative Languages. </title> <booktitle> In Proceedings of the SIGPLAN'90 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 257-271, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: The operators "*", "+", and "not" represent logical "and", "or", and "complement" operators, respectively. computed by following all potential paths in the control flow graph. A direct approach uses a gated static single assignment (GSSA) <ref> [6] </ref>, [41] representation of the program. In such a representation, scalar variables are assigned only once.
Reference: [7] <author> U. Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer. </publisher> <address> Boston, MA., </address> <year> 1988. </year>
Reference-contexts: In order to determine whether or not the execution order of the data accesses affects the semantics of the loop, the data dependence relations between the statements in the loop body must be analyzed <ref> [7] </ref>, [20], [29], [43], [50]. There are three possible types of dependences between two statements that access the same memory location: flow (read after write), anti (write after read), and output (write after write). Flow dependences express a fundamental relationship about the data flow in the program.
Reference: [8] <author> M. Berry, D. Chen, P. Koss, D. Kuck, S. Lo, Y. Pang, R. Roloff, A. Sameh, E. Clementi, S. Chin, D. Schneider, G. Fox, P. Messina, D. Walker, C. Hsiung, J. Schwarzmeier, K. Lue, S. Orzag, F. Seidl, O. Johnson, G. Swanson, R. Goodrum, and J. Martin. </author> <title> The PERFECT club benchmarks: Effective performance evaluation of supercomputers. </title> <type> Technical Report CSRD 21 827, </type> <institution> Center for Supercomputing Research and Development, University of Illinois, Urbana, IL, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: The codes were manually instrumented with calls to the run-time library. However, we remark that our results scale with the number of processors and the data size and thus can therefore be extrapolated for massively parallel processors (MPPs). We considered seven do loops from the PERFECT Benchmarks <ref> [8] </ref> that could not be parallelized by any compiler available to us. Our results are summarized in Table I. We have applied the LRPD test in both speculative and inspector/executor mode (with the notable exception of TRACK).
Reference: [9] <author> H. Berryman and J. Saltz. </author> <title> A manual for PARTI runtime primitives. </title> <type> Interim Report 90-13, </type> <institution> ICASE, </institution> <year> 1990. </year>
Reference-contexts: This scheme relies heavily on synchronization, inserts an additional level of indirection into all memory accesses, and calls for dynamic shared memory allocation. The problem of analyzing and scheduling loops at run-time has been studied extensively by Saltz et al. <ref> [9] </ref>, [35], [36], [37], [44]. In most of these methods, the original source loop is transformed into an inspector, which performs some run-time data dependence analysis and constructs a (preliminary) schedule, and an executor, which performs the scheduled work. The original source loop is assumed to have no output dependences.
Reference: [10] <author> W. Blume and R. Eigenmann. </author> <title> Performance Analysis of Parallelizing Compilers on the Perfect Benchmarks TM Programs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(6) </volume> <pages> 643-656, </pages> <month> November </month> <year> 1992. </year>
Reference: [11] <author> M. Burke, R. Cytron, J. Ferrante, and W. Hsieh. </author> <title> Automatic generation of nested, fork-join parallelism. </title> <journal> Journal of Supercomputing, </journal> <pages> pages 71-88, </pages> <year> 1989. </year>
Reference-contexts: Privatization creates, for each processor cooperating on the execution of the loop, private copies of the program variables that give rise to anti or output dependences (see, e.g., <ref> [11] </ref>, [23], [24], [39], [40]).
Reference: [12] <author> W. J. Camp, S. J. Plimpton, B. A. Hendrickson, and R. W. Leland. </author> <title> Massively parallel methods for engineering and science problems. </title> <journal> Comm. ACM, </journal> <volume> 37(4) </volume> <pages> 31-41, </pages> <month> April </month> <year> 1994. </year>
Reference: [13] <author> D. K. Chen, P. C. Yew, and J. Torrellas. </author> <title> An efficient algorithm for the run-time parallelization of doacross loops. </title> <booktitle> In Proceedings of Supercomputing 1994, </booktitle> <pages> pages 518-527, </pages> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: By using separate shadow variables to process the read and write operations, Midkiff and Padua [27] improved this basic method so that concurrent reads from a memory location are allowed in multiple iterations. Recently, Chen, Yew and Torrellas <ref> [13] </ref> proposed another variant of the Zhu and Yew method which improves performance in the presence of hot-spots (i.e., many accesses to the same memory location) by first doing some of the computation in private storage. Xu and Chaudhary [46], [45] improve upon [13] by not serializing on multiple reads to <p> Recently, Chen, Yew and Torrellas <ref> [13] </ref> proposed another variant of the Zhu and Yew method which improves performance in the presence of hot-spots (i.e., many accesses to the same memory location) by first doing some of the computation in private storage. Xu and Chaudhary [46], [45] improve upon [13] by not serializing on multiple reads to the same location. All of the above mentioned methods construct maximal stages in the sense that each iteration is placed in the earliest possible stage, giving a minimal depth schedule, i.e., a minimal number of stages. <p> requires restricts privatizes optimal sequential global type of or finds Method schedule portions synchron loop reductions Rauchwerger/Amato/Padua [31] Yes No No No P,R Zhu/Yew [49] No 1 No Yes 2 No No Midkiff/Padua [27] Yes No Yes 2 No No Krothapalli/Sadayappan [18] No 3 No Yes 2 No P Chen/Yew/Torrellas <ref> [13] </ref> No 1;3 No Yes No No Xu/Chaudary [46], [45] Yes No Yes No No Saltz/Mirchandaney [35] No 3 No Yes Yes 5 No Saltz et al. [37] Yes Yes 4 Yes Yes 5 No Leung/Zahorjan [22] Yes No Yes Yes 5 No Polychronopoulos [30] No No No No No Rauchwerger/Padua <p> Although bootstrapping might not optimally parallelize the inspector (due to the synchronization barriers introduced for each processor), it will produce the same minimum depth schedule as the sequential inspector of Saltz et al. In summary, the previous run-time methods for paral-lelizing loops rely heavily on global synchronizations (communication) <ref> [13] </ref>, [18], [22], [27], [30], [35], [37], [49], are applicable only to restricted types of loops [22], [35], [37], have significant sequential components [30], [35], [37], and/or do not extract the maximum available parallelism (they make conservative assumptions) [13], [22], [30], [35], [37], [49]. <p> previous run-time methods for paral-lelizing loops rely heavily on global synchronizations (communication) <ref> [13] </ref>, [18], [22], [27], [30], [35], [37], [49], are applicable only to restricted types of loops [22], [35], [37], have significant sequential components [30], [35], [37], and/or do not extract the maximum available parallelism (they make conservative assumptions) [13], [22], [30], [35], [37], [49]. A high level comparison of the various methods is given in Table II. VII. Related Work A.
Reference: [14] <author> A. Dinning and E. Schonberg. </author> <title> An empirical comparison of monitoring algorithms for access anomaly detection. </title> <booktitle> In Proc. of 2-nd ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming (PPOPP), </booktitle> <pages> pages 1-10, </pages> <year> 1990. </year>
Reference-contexts: Since not all anomalies can be detected statically, and execution traces can require prohibitive amounts of memory, run-time access anomaly detection methods that minimize memory requirements are desirable [38], <ref> [14] </ref>, [28]. In fact, a run-time anomaly detection method proposed by Snir, and optimized by Schonberg [38] and later by Nudler [28] bears similarities to a simplified version of the LRPD test presented in Section III (i.e., a version without privatization).
Reference: [15] <author> R. Eigenmann, J. Hoeflinger, Z. Li, and D. Padua. </author> <title> Experience in the Automatic Parallelization of Four Perfect-Benchmark Programs. </title> <booktitle> Lecture Notes in Computer Science 589. Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Santa Clara, CA, </address> <pages> pages 65-83, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: It depends only on the access pattern and not on the operation type. Several parallel methods are known for performing reduction operations. One typical method is to transform the do loop into a doall and enclose the access to the reduction variable in an unordered critical section <ref> [15] </ref>, [50]. Drawbacks of this method are that it is not scalable and requires synchronizations which can be very expensive in large multiprocessor systems.
Reference: [16] <author> P. A. Emrath, S. Ghosh, and D. A. Padua. </author> <title> Detecting nondeter-minacy in parallel programs. </title> <journal> IEEE Soft., </journal> <pages> pages 69-77, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: Generally, access anomaly detection techniques seek to identify the point in the parallel execution at which the access anomaly occurred. In [3], <ref> [16] </ref> the authors discuss methods that statically analyze the source program, and methods that analyze an execution trace of the program.
Reference: [17] <author> D. R. Jefferson. </author> <title> Virtual time. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(3) </volume> <pages> 404-425, </pages> <month> July </month> <year> 1985. </year>
Reference-contexts: While the method may not be suitable for performance-oriented parallelization of doall loops, it is a clever technique for debugging arbitrary fork-join parallelism constructs. B. Optimistic Execution A concept related to the speculative approach described in this paper is virtual time first introduced in <ref> [17] </ref> and defined as a " ... paradigm for organizing and synchronizing distributed systems.... [It] provides a flexible abstraction of real time in much the same way that virtual memory provides an abstraction of real memory.
Reference: [18] <author> V. Krothapalli and P. Sadayappan. </author> <title> An approach to synchronization of parallel computing. </title> <booktitle> In Proceedings of the 1988 International Conference on Supercomputing, </booktitle> <pages> pages 573-581, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: The wavefronts can be constructed sequentially by inspecting all the shared variable accesses, or in parallel with the aid of critical sections. Note that since all computations are performed at run-time, it is important for them to be efficiently parallelizable. Krothapalli and Sadayappan <ref> [18] </ref> proposed a run-time scheme for removing anti (write-after-read) and output (write-after-write) dependences from loops. <p> suboptimal schedule since a new synchronization 19 obtains contains requires restricts privatizes optimal sequential global type of or finds Method schedule portions synchron loop reductions Rauchwerger/Amato/Padua [31] Yes No No No P,R Zhu/Yew [49] No 1 No Yes 2 No No Midkiff/Padua [27] Yes No Yes 2 No No Krothapalli/Sadayappan <ref> [18] </ref> No 3 No Yes 2 No P Chen/Yew/Torrellas [13] No 1;3 No Yes No No Xu/Chaudary [46], [45] Yes No Yes No No Saltz/Mirchandaney [35] No 3 No Yes Yes 5 No Saltz et al. [37] Yes Yes 4 Yes Yes 5 No Leung/Zahorjan [22] Yes No Yes Yes 5 <p> Although bootstrapping might not optimally parallelize the inspector (due to the synchronization barriers introduced for each processor), it will produce the same minimum depth schedule as the sequential inspector of Saltz et al. In summary, the previous run-time methods for paral-lelizing loops rely heavily on global synchronizations (communication) [13], <ref> [18] </ref>, [22], [27], [30], [35], [37], [49], are applicable only to restricted types of loops [22], [35], [37], have significant sequential components [30], [35], [37], and/or do not extract the maximum available parallelism (they make conservative assumptions) [13], [22], [30], [35], [37], [49].
Reference: [19] <author> C. Kruskal. </author> <title> Efficient parallel algorithms for graph problems. </title> <booktitle> In Proceedings of the 1986 International Conference on Parallel Processing, </booktitle> <pages> pages 869-876, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: A scalable method can be obtained by noting that a reduction operation is an associative and commutative recurrence and can thus be parallelized using a recursive doubling algorithm <ref> [19] </ref>, [21].
Reference: [20] <author> D. J. Kuck, R. H. Kuhn, D. A. Padua, B. Leasure, and M. Wolfe. </author> <title> Dependence graphs and compiler optimizations. </title> <booktitle> In Proceedings of the 8th ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 207-218, </pages> <month> January </month> <year> 1981. </year>
Reference-contexts: In order to determine whether or not the execution order of the data accesses affects the semantics of the loop, the data dependence relations between the statements in the loop body must be analyzed [7], <ref> [20] </ref>, [29], [43], [50]. There are three possible types of dependences between two statements that access the same memory location: flow (read after write), anti (write after read), and output (write after write). Flow dependences express a fundamental relationship about the data flow in the program.
Reference: [21] <author> F. Thomson Leighton. </author> <title> Introduction to Parallel Algorithms and Architectures: Arrays, Trees, Hypercubes. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: A scalable method can be obtained by noting that a reduction operation is an associative and commutative recurrence and can thus be parallelized using a recursive doubling algorithm [19], <ref> [21] </ref>. <p> The counting in Step 2 (a) can be done in parallel by giving each processor s=p values to add within its private memory, and then summing the p resulting values in global storage, which takes O (s=p + log p) time <ref> [21] </ref>. The comparisons in Step 2 (b) (2 (d)) of A w with A r (with A np and A nx ) take O (s=p + log p) time.
Reference: [22] <author> S. Leung and J. Zahorjan. </author> <title> Improving the performance of runtime parallelization. </title> <booktitle> In 4th PPOPP, </booktitle> <pages> pages 83-91, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Recently, Leung and Zahorjan <ref> [22] </ref> have proposed some other methods of parallelizing the inspector of Saltz et al. These techniques are also restricted to loops with no output dependences. <p> Yes 2 No No Krothapalli/Sadayappan [18] No 3 No Yes 2 No P Chen/Yew/Torrellas [13] No 1;3 No Yes No No Xu/Chaudary [46], [45] Yes No Yes No No Saltz/Mirchandaney [35] No 3 No Yes Yes 5 No Saltz et al. [37] Yes Yes 4 Yes Yes 5 No Leung/Zahorjan <ref> [22] </ref> Yes No Yes Yes 5 No Polychronopoulos [30] No No No No No Rauchwerger/Padua [32], [34] No 6 No No No P,R TABLE II A comparison of run-time parallelization techniques for do loops. <p> In summary, the previous run-time methods for paral-lelizing loops rely heavily on global synchronizations (communication) [13], [18], <ref> [22] </ref>, [27], [30], [35], [37], [49], are applicable only to restricted types of loops [22], [35], [37], have significant sequential components [30], [35], [37], and/or do not extract the maximum available parallelism (they make conservative assumptions) [13], [22], [30], [35], [37], [49]. <p> In summary, the previous run-time methods for paral-lelizing loops rely heavily on global synchronizations (communication) [13], [18], <ref> [22] </ref>, [27], [30], [35], [37], [49], are applicable only to restricted types of loops [22], [35], [37], have significant sequential components [30], [35], [37], and/or do not extract the maximum available parallelism (they make conservative assumptions) [13], [22], [30], [35], [37], [49]. A high level comparison of the various methods is given in Table II. VII. Related Work A. <p> run-time methods for paral-lelizing loops rely heavily on global synchronizations (communication) [13], [18], <ref> [22] </ref>, [27], [30], [35], [37], [49], are applicable only to restricted types of loops [22], [35], [37], have significant sequential components [30], [35], [37], and/or do not extract the maximum available parallelism (they make conservative assumptions) [13], [22], [30], [35], [37], [49]. A high level comparison of the various methods is given in Table II. VII. Related Work A. Race Detection for Parallel Program Debugging A significant amount of work has been invested in the research of hazards (race conditions) and access anomalies for debugging parallel programs.
Reference: [23] <author> Zhiyuan Li. </author> <title> Array privatization for parallel execution of loops. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <pages> pages 313-322, </pages> <year> 1992. </year>
Reference-contexts: Privatization creates, for each processor cooperating on the execution of the loop, private copies of the program variables that give rise to anti or output dependences (see, e.g., [11], <ref> [23] </ref>, [24], [39], [40]).
Reference: [24] <author> D. E. Maydan, S. P. Amarasinghe, and M. S. Lam. </author> <title> Data dependence and data-flow analysis of arrays. </title> <booktitle> In Proceedings 5th Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1992. </year>
Reference-contexts: Privatization creates, for each processor cooperating on the execution of the loop, private copies of the program variables that give rise to anti or output dependences (see, e.g., [11], [23], <ref> [24] </ref>, [39], [40]).
Reference: [25] <author> John Mellor-Crummey. </author> <title> On-the-fly detection of data races for programs with nested fork-join parallelism. </title> <booktitle> In Proceedings of Supercomputing 1991, </booktitle> <pages> pages 24-33, </pages> <address> Albuquerque, NM, </address> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: Viewed in the framework of the LRPD test, a separate shadow array for each iteration in a loop must be maintained. In 1991 Mellor-Crummey <ref> [25] </ref> improved this technique by dramatically reducing the memory requirements; the maximum access history storage is O (N), where N is the maximum level fork-join nesting.
Reference: [26] <author> John Mellor-Crummey. </author> <title> Compile-time support for efficient data race detection in shared-memory parallel programs. </title> <booktitle> In Proc. if the ACM/ONR Workshop on Parallel and Distributed Debugging, </booktitle> <pages> pages 129-139, </pages> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: The execution time overhead is still very high, because every reference monitored has to be logged and checked against the access history in a critical section. In <ref> [26] </ref> an order of magnitude increase in execution time of instrumented codes is reported for experiments on a sequential machine. Even after reducing the shadowed references through compile time analysis, the time expansion factor remains around 5.
Reference: [27] <author> S. Midkiff and D. Padua. </author> <title> Compiler algorithms for synchronization. </title> <journal> IEEE Trans. Comput., </journal> <volume> C-36(12):1485-1495, </volume> <year> 1987. </year>
Reference-contexts: In each phase, the lowest unassigned iteration to access any variable (e.g., array element) is found using atomic compare-and-swap synchronization primitives to record the minimum such iteration in a shadow version of that variable. By using separate shadow variables to process the read and write operations, Midkiff and Padua <ref> [27] </ref> improved this basic method so that concurrent reads from a memory location are allowed in multiple iterations. <p> Thus sectioning will usually produce a suboptimal schedule since a new synchronization 19 obtains contains requires restricts privatizes optimal sequential global type of or finds Method schedule portions synchron loop reductions Rauchwerger/Amato/Padua [31] Yes No No No P,R Zhu/Yew [49] No 1 No Yes 2 No No Midkiff/Padua <ref> [27] </ref> Yes No Yes 2 No No Krothapalli/Sadayappan [18] No 3 No Yes 2 No P Chen/Yew/Torrellas [13] No 1;3 No Yes No No Xu/Chaudary [46], [45] Yes No Yes No No Saltz/Mirchandaney [35] No 3 No Yes Yes 5 No Saltz et al. [37] Yes Yes 4 Yes Yes 5 <p> In summary, the previous run-time methods for paral-lelizing loops rely heavily on global synchronizations (communication) [13], [18], [22], <ref> [27] </ref>, [30], [35], [37], [49], are applicable only to restricted types of loops [22], [35], [37], have significant sequential components [30], [35], [37], and/or do not extract the maximum available parallelism (they make conservative assumptions) [13], [22], [30], [35], [37], [49].
Reference: [28] <author> I. Nudler and L. Rudolph. </author> <title> Tools for the efficient developement of efficient parallel programs. </title> <booktitle> In Proc. 1st Israeli Conference on Computer System Engineering, </booktitle> <year> 1988. </year>
Reference-contexts: Since not all anomalies can be detected statically, and execution traces can require prohibitive amounts of memory, run-time access anomaly detection methods that minimize memory requirements are desirable [38], [14], <ref> [28] </ref>. In fact, a run-time anomaly detection method proposed by Snir, and optimized by Schonberg [38] and later by Nudler [28] bears similarities to a simplified version of the LRPD test presented in Section III (i.e., a version without privatization). <p> Since not all anomalies can be detected statically, and execution traces can require prohibitive amounts of memory, run-time access anomaly detection methods that minimize memory requirements are desirable [38], [14], <ref> [28] </ref>. In fact, a run-time anomaly detection method proposed by Snir, and optimized by Schonberg [38] and later by Nudler [28] bears similarities to a simplified version of the LRPD test presented in Section III (i.e., a version without privatization).
Reference: [29] <author> D. A. Padua and M. J. Wolfe. </author> <title> Advanced compiler optimizations for supercomputers. </title> <journal> Communications of the ACM, </journal> <volume> 29 </volume> <pages> 1184-1201, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: In order to determine whether or not the execution order of the data accesses affects the semantics of the loop, the data dependence relations between the statements in the loop body must be analyzed [7], [20], <ref> [29] </ref>, [43], [50]. There are three possible types of dependences between two statements that access the same memory location: flow (read after write), anti (write after read), and output (write after write). Flow dependences express a fundamental relationship about the data flow in the program.
Reference: [30] <author> C. Polychronopoulos. </author> <title> Compiler Optimizations for Enhancing Parallelism and Their Imp act on Architecture Design. </title> <journal> IEEE Trans. Comput., </journal> <volume> C-37(8):991-1004, </volume> <month> August </month> <year> 1988. </year>
Reference-contexts: All of the above mentioned methods construct maximal stages in the sense that each iteration is placed in the earliest possible stage, giving a minimal depth schedule, i.e., a minimal number of stages. Polychronopoulos <ref> [30] </ref> gives a method that assigns iterations to stages in a different way: each wavefront consists of a maximal set of contiguous iterations which contain no cross-iteration dependences. It is easy to see that this method may not yield a minimum depth schedule. <p> No Yes 2 No P Chen/Yew/Torrellas [13] No 1;3 No Yes No No Xu/Chaudary [46], [45] Yes No Yes No No Saltz/Mirchandaney [35] No 3 No Yes Yes 5 No Saltz et al. [37] Yes Yes 4 Yes Yes 5 No Leung/Zahorjan [22] Yes No Yes Yes 5 No Polychronopoulos <ref> [30] </ref> No No No No No Rauchwerger/Padua [32], [34] No 6 No No No P,R TABLE II A comparison of run-time parallelization techniques for do loops. In the table entries, P and R show that the method identities privatizable and reduction variables, respectively. <p> In summary, the previous run-time methods for paral-lelizing loops rely heavily on global synchronizations (communication) [13], [18], [22], [27], <ref> [30] </ref>, [35], [37], [49], are applicable only to restricted types of loops [22], [35], [37], have significant sequential components [30], [35], [37], and/or do not extract the maximum available parallelism (they make conservative assumptions) [13], [22], [30], [35], [37], [49]. <p> In summary, the previous run-time methods for paral-lelizing loops rely heavily on global synchronizations (communication) [13], [18], [22], [27], <ref> [30] </ref>, [35], [37], [49], are applicable only to restricted types of loops [22], [35], [37], have significant sequential components [30], [35], [37], and/or do not extract the maximum available parallelism (they make conservative assumptions) [13], [22], [30], [35], [37], [49]. A high level comparison of the various methods is given in Table II. VII. Related Work A. <p> methods for paral-lelizing loops rely heavily on global synchronizations (communication) [13], [18], [22], [27], <ref> [30] </ref>, [35], [37], [49], are applicable only to restricted types of loops [22], [35], [37], have significant sequential components [30], [35], [37], and/or do not extract the maximum available parallelism (they make conservative assumptions) [13], [22], [30], [35], [37], [49]. A high level comparison of the various methods is given in Table II. VII. Related Work A. Race Detection for Parallel Program Debugging A significant amount of work has been invested in the research of hazards (race conditions) and access anomalies for debugging parallel programs.
Reference: [31] <author> L. Rauchwerger, N. Amato, and D. Padua. </author> <title> A scalable method for run-time loop parallelization. </title> <journal> IJPP, </journal> <volume> 26(6) </volume> <pages> 537-576, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: Thus sectioning will usually produce a suboptimal schedule since a new synchronization 19 obtains contains requires restricts privatizes optimal sequential global type of or finds Method schedule portions synchron loop reductions Rauchwerger/Amato/Padua <ref> [31] </ref> Yes No No No P,R Zhu/Yew [49] No 1 No Yes 2 No No Midkiff/Padua [27] Yes No Yes 2 No No Krothapalli/Sadayappan [18] No 3 No Yes 2 No P Chen/Yew/Torrellas [13] No 1;3 No Yes No No Xu/Chaudary [46], [45] Yes No Yes No No Saltz/Mirchandaney [35] No
Reference: [32] <author> L. Rauchwerger and D. Padua. </author> <title> The privatizing doall test: A run-time technique for doall loop identification and array priva-tization. </title> <booktitle> In Proceedings of the 1994 International Conference on Supercomputing, </booktitle> <pages> pages 33-43, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: The new algorithms consider only data dependences caused by actual cross-iteration data-flow (a flow of values). Thus, they may potentially qualify more loops as parallel than the method in <ref> [32] </ref> which conservatively considered the dependences due to every memory reference even if no cross-iteration data-flow occurred at run-time. This situation could arise for example when a loop reads a shared variable, but then only uses it conditionally. <p> Since predicates seldom can be evaluated statically, the compiler must be conservative and conclude that the read access causes a dependence in every iteration of the loop. The test given here improves upon the more conservative Privatizing doall (PD) test described in <ref> [32] </ref> by checking only the dynamic data dependences caused by the actual cross-iteration flow of values stored in the shared arrays (the PD test was checking exclusively reference patterns). <p> In the PD test described in detail in <ref> [32] </ref>, all memory references to the array under test are marked in the same control block in which they appear, regardless whether the values stored at those addresses contribute to the global data flow or not. <p> No 1;3 No Yes No No Xu/Chaudary [46], [45] Yes No Yes No No Saltz/Mirchandaney [35] No 3 No Yes Yes 5 No Saltz et al. [37] Yes Yes 4 Yes Yes 5 No Leung/Zahorjan [22] Yes No Yes Yes 5 No Polychronopoulos [30] No No No No No Rauchwerger/Padua <ref> [32] </ref>, [34] No 6 No No No P,R TABLE II A comparison of run-time parallelization techniques for do loops. In the table entries, P and R show that the method identities privatizable and reduction variables, respectively.
Reference: [33] <author> Lawrence Rauchwerger and David A. Padua. </author> <title> Parallelizing WHILE Loops for Multiprocessor Systems. </title> <booktitle> In Proceedings of 9th International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1995. </year>
Reference-contexts: The ideal speedup of loop 40 is not very large since the loop is small, imbalanced between iterations, and 16 Fig. 11. Fig. 13. 17 Fig. 15. traverses a linked list. The linked list traversal was par-allelized using techniques we developed for automatically parallelizing while loops <ref> [33] </ref>. Thus, although the obtained speedup is modest, it represents a significant fraction of the ideal speedup (see Fig. 16). Therefore, since loop 40 is one of the smallest loops in the LOAD subroutine, we expect to obtain better speedups on the larger loops (since they have larger ideal speedups).
Reference: [34] <author> Lawrence Rauchwerger and David A. Padua. </author> <title> The LRPD Test: Speculative Run-Time Parallelization of Loops with Privatiza-tion and Reduction Parallelization. </title> <booktitle> In Proceedings of the SIG-PLAN 1995 Conference on Programming Language Design and Implementation, </booktitle> <address> La Jolla, CA, </address> <pages> pages 218-232, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: 1;3 No Yes No No Xu/Chaudary [46], [45] Yes No Yes No No Saltz/Mirchandaney [35] No 3 No Yes Yes 5 No Saltz et al. [37] Yes Yes 4 Yes Yes 5 No Leung/Zahorjan [22] Yes No Yes Yes 5 No Polychronopoulos [30] No No No No No Rauchwerger/Padua [32], <ref> [34] </ref> No 6 No No No P,R TABLE II A comparison of run-time parallelization techniques for do loops. In the table entries, P and R show that the method identities privatizable and reduction variables, respectively.
Reference: [35] <author> J. Saltz and R. Mirchandaney. </author> <title> The preprocessed doacross loop. In Dr. </title> <editor> H.D. Schwetman, editor, </editor> <booktitle> Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <pages> pages 174-178. </pages> <publisher> CRC Press, Inc., </publisher> <year> 1991. </year> <title> Vol. </title> <booktitle> II Software. </booktitle>
Reference-contexts: This scheme relies heavily on synchronization, inserts an additional level of indirection into all memory accesses, and calls for dynamic shared memory allocation. The problem of analyzing and scheduling loops at run-time has been studied extensively by Saltz et al. [9], <ref> [35] </ref>, [36], [37], [44]. In most of these methods, the original source loop is transformed into an inspector, which performs some run-time data dependence analysis and constructs a (preliminary) schedule, and an executor, which performs the scheduled work. The original source loop is assumed to have no output dependences. <p> The inspector computation (the topological sort) can be par-allelized somewhat using the DOACROSS parallelization technique of Saltz and Mirchandaney <ref> [35] </ref>, in which processors are assigned iterations in a wrapped manner, and busy-waits are used to ensure that values have been produced before they are used (again, this is only possible if the original loop has no output dependences). <p> Rauchwerger/Amato/Padua [31] Yes No No No P,R Zhu/Yew [49] No 1 No Yes 2 No No Midkiff/Padua [27] Yes No Yes 2 No No Krothapalli/Sadayappan [18] No 3 No Yes 2 No P Chen/Yew/Torrellas [13] No 1;3 No Yes No No Xu/Chaudary [46], [45] Yes No Yes No No Saltz/Mirchandaney <ref> [35] </ref> No 3 No Yes Yes 5 No Saltz et al. [37] Yes Yes 4 Yes Yes 5 No Leung/Zahorjan [22] Yes No Yes Yes 5 No Polychronopoulos [30] No No No No No Rauchwerger/Padua [32], [34] No 6 No No No P,R TABLE II A comparison of run-time parallelization techniques <p> In summary, the previous run-time methods for paral-lelizing loops rely heavily on global synchronizations (communication) [13], [18], [22], [27], [30], <ref> [35] </ref>, [37], [49], are applicable only to restricted types of loops [22], [35], [37], have significant sequential components [30], [35], [37], and/or do not extract the maximum available parallelism (they make conservative assumptions) [13], [22], [30], [35], [37], [49]. <p> In summary, the previous run-time methods for paral-lelizing loops rely heavily on global synchronizations (communication) [13], [18], [22], [27], [30], <ref> [35] </ref>, [37], [49], are applicable only to restricted types of loops [22], [35], [37], have significant sequential components [30], [35], [37], and/or do not extract the maximum available parallelism (they make conservative assumptions) [13], [22], [30], [35], [37], [49]. A high level comparison of the various methods is given in Table II. VII. Related Work A. <p> In summary, the previous run-time methods for paral-lelizing loops rely heavily on global synchronizations (communication) [13], [18], [22], [27], [30], <ref> [35] </ref>, [37], [49], are applicable only to restricted types of loops [22], [35], [37], have significant sequential components [30], [35], [37], and/or do not extract the maximum available parallelism (they make conservative assumptions) [13], [22], [30], [35], [37], [49]. A high level comparison of the various methods is given in Table II. VII. Related Work A. <p> for paral-lelizing loops rely heavily on global synchronizations (communication) [13], [18], [22], [27], [30], <ref> [35] </ref>, [37], [49], are applicable only to restricted types of loops [22], [35], [37], have significant sequential components [30], [35], [37], and/or do not extract the maximum available parallelism (they make conservative assumptions) [13], [22], [30], [35], [37], [49]. A high level comparison of the various methods is given in Table II. VII. Related Work A. Race Detection for Parallel Program Debugging A significant amount of work has been invested in the research of hazards (race conditions) and access anomalies for debugging parallel programs.
Reference: [36] <author> J. Saltz, R. Mirchandaney, and K. Crowley. </author> <title> The doconsider loop. </title> <booktitle> In Proceedings of the 1989 International Conference on Supercomputing, </booktitle> <pages> pages 29-40, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: This scheme relies heavily on synchronization, inserts an additional level of indirection into all memory accesses, and calls for dynamic shared memory allocation. The problem of analyzing and scheduling loops at run-time has been studied extensively by Saltz et al. [9], [35], <ref> [36] </ref>, [37], [44]. In most of these methods, the original source loop is transformed into an inspector, which performs some run-time data dependence analysis and constructs a (preliminary) schedule, and an executor, which performs the scheduled work. The original source loop is assumed to have no output dependences.
Reference: [37] <author> J. Saltz, R. Mirchandaney, and K. Crowley. </author> <title> Run-time paral-lelization and scheduling of loops. </title> <journal> IEEE Trans. Comput., </journal> <volume> 40(5), </volume> <month> May </month> <year> 1991. </year>
Reference-contexts: This is a simple illustration of the schedule reuse technique, in which a correct execution schedule is determined once, and subsequently reused if all of the defining conditions remain invariant (see, e.g., Saltz et al. <ref> [37] </ref>). If it can be determined at compile time that the data access pattern is invariant across different executions of the same loop, then no additional computation is required. <p> This scheme relies heavily on synchronization, inserts an additional level of indirection into all memory accesses, and calls for dynamic shared memory allocation. The problem of analyzing and scheduling loops at run-time has been studied extensively by Saltz et al. [9], [35], [36], <ref> [37] </ref>, [44]. In most of these methods, the original source loop is transformed into an inspector, which performs some run-time data dependence analysis and constructs a (preliminary) schedule, and an executor, which performs the scheduled work. The original source loop is assumed to have no output dependences. In [37], the inspector <p> [35], [36], <ref> [37] </ref>, [44]. In most of these methods, the original source loop is transformed into an inspector, which performs some run-time data dependence analysis and constructs a (preliminary) schedule, and an executor, which performs the scheduled work. The original source loop is assumed to have no output dependences. In [37], the inspector constructs stages that respect the flow dependences by performing a sequential topological sort of the accesses in the loop. The executor enforces any anti dependences by using old and new versions of each variable. <p> No Yes 2 No No Midkiff/Padua [27] Yes No Yes 2 No No Krothapalli/Sadayappan [18] No 3 No Yes 2 No P Chen/Yew/Torrellas [13] No 1;3 No Yes No No Xu/Chaudary [46], [45] Yes No Yes No No Saltz/Mirchandaney [35] No 3 No Yes Yes 5 No Saltz et al. <ref> [37] </ref> Yes Yes 4 Yes Yes 5 No Leung/Zahorjan [22] Yes No Yes Yes 5 No Polychronopoulos [30] No No No No No Rauchwerger/Padua [32], [34] No 6 No No No P,R TABLE II A comparison of run-time parallelization techniques for do loops. <p> In summary, the previous run-time methods for paral-lelizing loops rely heavily on global synchronizations (communication) [13], [18], [22], [27], [30], [35], <ref> [37] </ref>, [49], are applicable only to restricted types of loops [22], [35], [37], have significant sequential components [30], [35], [37], and/or do not extract the maximum available parallelism (they make conservative assumptions) [13], [22], [30], [35], [37], [49]. <p> In summary, the previous run-time methods for paral-lelizing loops rely heavily on global synchronizations (communication) [13], [18], [22], [27], [30], [35], <ref> [37] </ref>, [49], are applicable only to restricted types of loops [22], [35], [37], have significant sequential components [30], [35], [37], and/or do not extract the maximum available parallelism (they make conservative assumptions) [13], [22], [30], [35], [37], [49]. A high level comparison of the various methods is given in Table II. VII. Related Work A. <p> In summary, the previous run-time methods for paral-lelizing loops rely heavily on global synchronizations (communication) [13], [18], [22], [27], [30], [35], <ref> [37] </ref>, [49], are applicable only to restricted types of loops [22], [35], [37], have significant sequential components [30], [35], [37], and/or do not extract the maximum available parallelism (they make conservative assumptions) [13], [22], [30], [35], [37], [49]. A high level comparison of the various methods is given in Table II. VII. Related Work A. <p> paral-lelizing loops rely heavily on global synchronizations (communication) [13], [18], [22], [27], [30], [35], <ref> [37] </ref>, [49], are applicable only to restricted types of loops [22], [35], [37], have significant sequential components [30], [35], [37], and/or do not extract the maximum available parallelism (they make conservative assumptions) [13], [22], [30], [35], [37], [49]. A high level comparison of the various methods is given in Table II. VII. Related Work A. Race Detection for Parallel Program Debugging A significant amount of work has been invested in the research of hazards (race conditions) and access anomalies for debugging parallel programs.
Reference: [38] <author> E. Schonberg. </author> <title> On-the-fly detection of access anomalies. </title> <booktitle> In Proceedings of the SIGPLAN 1989 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 285-297, </pages> <address> Portland, Oregon, </address> <year> 1989. </year>
Reference-contexts: Since not all anomalies can be detected statically, and execution traces can require prohibitive amounts of memory, run-time access anomaly detection methods that minimize memory requirements are desirable <ref> [38] </ref>, [14], [28]. In fact, a run-time anomaly detection method proposed by Snir, and optimized by Schonberg [38] and later by Nudler [28] bears similarities to a simplified version of the LRPD test presented in Section III (i.e., a version without privatization). <p> Since not all anomalies can be detected statically, and execution traces can require prohibitive amounts of memory, run-time access anomaly detection methods that minimize memory requirements are desirable <ref> [38] </ref>, [14], [28]. In fact, a run-time anomaly detection method proposed by Snir, and optimized by Schonberg [38] and later by Nudler [28] bears similarities to a simplified version of the LRPD test presented in Section III (i.e., a version without privatization).
Reference: [39] <author> P. Tu and D. Padua. </author> <title> Array privatization for shared and distributed memory machines. </title> <booktitle> In Proceedings 2nd Workshop on Languages, Compilers, and Run-Time Environments for Distributed Memory Machines, </booktitle> <month> September </month> <year> 1992. </year>
Reference-contexts: Privatization creates, for each processor cooperating on the execution of the loop, private copies of the program variables that give rise to anti or output dependences (see, e.g., [11], [23], [24], <ref> [39] </ref>, [40]).
Reference: [40] <author> P. Tu and D. Padua. </author> <title> Automatic array privatization. </title> <booktitle> In Proceedings 6th Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: Privatization creates, for each processor cooperating on the execution of the loop, private copies of the program variables that give rise to anti or output dependences (see, e.g., [11], [23], [24], [39], <ref> [40] </ref>). <p> The markredux operations are as described in Fig. 5. methods is a fairly straightforward demand driven forward substitution of all the variables on the RHS, a process by which all control flow dependences are substituted by data dependences as described in [2], <ref> [40] </ref>. Once this expression of the RHS is obtained it can be analyzed and validated by the methods described in the previous section. In the following we explain by way of example how our new method can identify reductions.
Reference: [41] <author> Peng Tu and David Padua. </author> <title> GSA based demand-driven symbolic analysis. </title> <type> Technical Report 1339, </type> <institution> University of Illinois at Urbana-Champaign, Cntr for Supercomputing Res & Dev, </institution> <month> February </month> <year> 1994. </year>
Reference-contexts: The operators "*", "+", and "not" represent logical "and", "or", and "complement" operators, respectively. computed by following all potential paths in the control flow graph. A direct approach uses a gated static single assignment (GSSA) [6], <ref> [41] </ref> representation of the program. In such a representation, scalar variables are assigned only once.
Reference: [42] <author> A. Vladimirescu. </author> <title> LSI Circuit Simulation on Vector Computers. </title> <type> PhD thesis, </type> <institution> Electronics Research Laboratory, University of Cal-ifornia, Berkeley, </institution> <month> October </month> <year> 1982. </year> <note> Technical Rept. No. UCB/ERL M82/75. </note>
Reference-contexts: It is important to note that the loop in Fig. 8 exemplifies the type of loop found in the SPICE2G6 program (subroutine LOAD) which can account for 70% of the sequential execution time (Its vectorization has been dealt with before <ref> [42] </ref>). Finally we mention that reductions such as min, max, etc., would first have to be syntactically pattern matched, and then substituted by the min and max functions. From this perspective, they are more difficult to recognize than simpler arithmetic reductions.
Reference: [43] <author> M. Wolfe. </author> <title> Optimizing Compilers for Supercomputers. </title> <publisher> The MIT Press, </publisher> <address> Boston, MA, </address> <year> 1989. </year>
Reference-contexts: In order to determine whether or not the execution order of the data accesses affects the semantics of the loop, the data dependence relations between the statements in the loop body must be analyzed [7], [20], [29], <ref> [43] </ref>, [50]. There are three possible types of dependences between two statements that access the same memory location: flow (read after write), anti (write after read), and output (write after write). Flow dependences express a fundamental relationship about the data flow in the program.
Reference: [44] <author> J. Wu, J. Saltz, S. Hiranandani, and H. Berryman. </author> <title> Runtime compilation methods for multicomputers. In Dr. </title> <editor> H.D. Schwet-man, editor, </editor> <booktitle> Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <pages> pages 26-30. </pages> <publisher> CRC Press, Inc., </publisher> <year> 1991. </year> <title> Vol. </title> <booktitle> II Software. </booktitle>
Reference-contexts: This scheme relies heavily on synchronization, inserts an additional level of indirection into all memory accesses, and calls for dynamic shared memory allocation. The problem of analyzing and scheduling loops at run-time has been studied extensively by Saltz et al. [9], [35], [36], [37], <ref> [44] </ref>. In most of these methods, the original source loop is transformed into an inspector, which performs some run-time data dependence analysis and constructs a (preliminary) schedule, and an executor, which performs the scheduled work. The original source loop is assumed to have no output dependences.
Reference: [45] <author> C. Xu. </author> <title> Effects of Parallelism Degree on Runtime Parallelism of Loops. </title> <booktitle> In Proceedings of the 31st Hawaii International Conference on System Sciences, </booktitle> <pages> pages 86-95, </pages> <month> January </month> <year> 1998. </year>
Reference-contexts: Recently, Chen, Yew and Torrellas [13] proposed another variant of the Zhu and Yew method which improves performance in the presence of hot-spots (i.e., many accesses to the same memory location) by first doing some of the computation in private storage. Xu and Chaudhary [46], <ref> [45] </ref> improve upon [13] by not serializing on multiple reads to the same location. All of the above mentioned methods construct maximal stages in the sense that each iteration is placed in the earliest possible stage, giving a minimal depth schedule, i.e., a minimal number of stages. <p> finds Method schedule portions synchron loop reductions Rauchwerger/Amato/Padua [31] Yes No No No P,R Zhu/Yew [49] No 1 No Yes 2 No No Midkiff/Padua [27] Yes No Yes 2 No No Krothapalli/Sadayappan [18] No 3 No Yes 2 No P Chen/Yew/Torrellas [13] No 1;3 No Yes No No Xu/Chaudary [46], <ref> [45] </ref> Yes No Yes No No Saltz/Mirchandaney [35] No 3 No Yes Yes 5 No Saltz et al. [37] Yes Yes 4 Yes Yes 5 No Leung/Zahorjan [22] Yes No Yes Yes 5 No Polychronopoulos [30] No No No No No Rauchwerger/Padua [32], [34] No 6 No No No P,R TABLE
Reference: [46] <author> C. Xu and V. Chaudhary. </author> <title> Time-stamping Algorithms for Par-allelization of Loops at Run-time. </title> <booktitle> In Proceedings of 11th International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1997. </year>
Reference-contexts: Recently, Chen, Yew and Torrellas [13] proposed another variant of the Zhu and Yew method which improves performance in the presence of hot-spots (i.e., many accesses to the same memory location) by first doing some of the computation in private storage. Xu and Chaudhary <ref> [46] </ref>, [45] improve upon [13] by not serializing on multiple reads to the same location. All of the above mentioned methods construct maximal stages in the sense that each iteration is placed in the earliest possible stage, giving a minimal depth schedule, i.e., a minimal number of stages. <p> or finds Method schedule portions synchron loop reductions Rauchwerger/Amato/Padua [31] Yes No No No P,R Zhu/Yew [49] No 1 No Yes 2 No No Midkiff/Padua [27] Yes No Yes 2 No No Krothapalli/Sadayappan [18] No 3 No Yes 2 No P Chen/Yew/Torrellas [13] No 1;3 No Yes No No Xu/Chaudary <ref> [46] </ref>, [45] Yes No Yes No No Saltz/Mirchandaney [35] No 3 No Yes Yes 5 No Saltz et al. [37] Yes Yes 4 Yes Yes 5 No Leung/Zahorjan [22] Yes No Yes Yes 5 No Polychronopoulos [30] No No No No No Rauchwerger/Padua [32], [34] No 6 No No No P,R
Reference: [47] <author> Y. Zhang, L. Rauchwerger, and J. Torrellas. </author> <title> Hardware for Speculative Run-Time Parallelization in Distributed Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of teh 4th International Symposium on High Performance Computer Architecture 1998, (HPCA-4), </booktitle> <pages> pages 162-173, </pages> <month> February </month> <year> 1998. </year>
Reference-contexts: It is conceivable, and we believe desirable, that future machines would include special hardware devices to accelerate the run-time analysis and in this way widen the range of applicability of the techniques and increase potential speedups <ref> [47] </ref>, [48]. II. Preliminaries A loop can be executed in fully parallel form, without synchronization, if and only if the desired outcome of the loop does not depend in any way upon the execution ordering of the data accesses from different iterations. <p> To bias the results even more in our favor, the decision on when to apply the methods should make use of run-time collected information about the fully parallel/not parallel nature of the loop. In addition, specialized hardware features could greatly reduce the overhead introduced by the methods <ref> [47] </ref>. Finally we believe that the true importance of this work is that it breaks the barrier at which automatic paralleliza-tion had stopped: regular, well-behaved programs.
Reference: [48] <author> Y. Zhang, L. Rauchwerger, and J. Torrellas. </author> <title> Speculative Parallel Execution of Loops with Cross-Iteration Dependences in DSM Multiprocessors. </title> <booktitle> In Proceedings of the 5th International Symposium on High-Performance Computer Architecture(HPCA-5), </booktitle> <month> January </month> <year> 1999. </year>
Reference-contexts: It is conceivable, and we believe desirable, that future machines would include special hardware devices to accelerate the run-time analysis and in this way widen the range of applicability of the techniques and increase potential speedups [47], <ref> [48] </ref>. II. Preliminaries A loop can be executed in fully parallel form, without synchronization, if and only if the desired outcome of the loop does not depend in any way upon the execution ordering of the data accesses from different iterations.
Reference: [49] <author> C. Zhu and P. C. Yew. </author> <title> A scheme to enforce data dependence on large multiprocessor systems. </title> <journal> IEEE Trans. Softw. Eng., </journal> <volume> 13(6) </volume> <pages> 726-739, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: Stages formed by a regular pattern of iterations are named wavefronts. They are executed sequentially by placing a synchronization barrier between each pair of consecutive wavefronts. One of the first run-time methods for scheduling partially parallel loops was proposed by Zhu and Yew <ref> [49] </ref>. It computes the stages one after the other in successive phases. <p> Thus sectioning will usually produce a suboptimal schedule since a new synchronization 19 obtains contains requires restricts privatizes optimal sequential global type of or finds Method schedule portions synchron loop reductions Rauchwerger/Amato/Padua [31] Yes No No No P,R Zhu/Yew <ref> [49] </ref> No 1 No Yes 2 No No Midkiff/Padua [27] Yes No Yes 2 No No Krothapalli/Sadayappan [18] No 3 No Yes 2 No P Chen/Yew/Torrellas [13] No 1;3 No Yes No No Xu/Chaudary [46], [45] Yes No Yes No No Saltz/Mirchandaney [35] No 3 No Yes Yes 5 No Saltz <p> In summary, the previous run-time methods for paral-lelizing loops rely heavily on global synchronizations (communication) [13], [18], [22], [27], [30], [35], [37], <ref> [49] </ref>, are applicable only to restricted types of loops [22], [35], [37], have significant sequential components [30], [35], [37], and/or do not extract the maximum available parallelism (they make conservative assumptions) [13], [22], [30], [35], [37], [49]. A high level comparison of the various methods is given in Table II. <p> loops rely heavily on global synchronizations (communication) [13], [18], [22], [27], [30], [35], [37], <ref> [49] </ref>, are applicable only to restricted types of loops [22], [35], [37], have significant sequential components [30], [35], [37], and/or do not extract the maximum available parallelism (they make conservative assumptions) [13], [22], [30], [35], [37], [49]. A high level comparison of the various methods is given in Table II. VII. Related Work A. Race Detection for Parallel Program Debugging A significant amount of work has been invested in the research of hazards (race conditions) and access anomalies for debugging parallel programs.
Reference: [50] <author> H. Zima. </author> <title> Supercompilers for Parallel and Vector Computers. </title> <publisher> ACM Press, </publisher> <address> New York, New York, </address> <year> 1991. </year>
Reference-contexts: In order to determine whether or not the execution order of the data accesses affects the semantics of the loop, the data dependence relations between the statements in the loop body must be analyzed [7], [20], [29], [43], <ref> [50] </ref>. There are three possible types of dependences between two statements that access the same memory location: flow (read after write), anti (write after read), and output (write after write). Flow dependences express a fundamental relationship about the data flow in the program. <p> It depends only on the access pattern and not on the operation type. Several parallel methods are known for performing reduction operations. One typical method is to transform the do loop into a doall and enclose the access to the reduction variable in an unordered critical section [15], <ref> [50] </ref>. Drawbacks of this method are that it is not scalable and requires synchronizations which can be very expensive in large multiprocessor systems. <p> problem has been handled at compile-time by syntactically pattern matching the loop statements with a template of a generic reduction, and then performing a data dependence analysis of the variable under scrutiny to guarantee that it is not used anywhere else 4 in the loop except in the reduction statement <ref> [50] </ref>. III. Speculative Parallel Execution of DO Loops Consider a do loop for which the compiler cannot statically determine the access pattern of a shared array A that is referenced in the loop. <p> So far the problem of reduction variable recognition has been handled at compile-time by syntactically pattern matching the loop statements with a template of a generic reduction, and then performing a data dependence analysis of the variable under scrutiny to validate it as a reduction variable <ref> [50] </ref>. There are two major shortcomings of such pattern matching identification methods. 1. The data dependence analysis necessary to qualify a statement as a reduction cannot be performed statically in the presence of input-dependent access pat terns. 2.
References-found: 50

