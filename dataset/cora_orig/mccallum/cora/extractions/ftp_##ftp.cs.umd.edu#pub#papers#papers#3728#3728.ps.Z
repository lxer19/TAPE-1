URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3728/3728.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Title: Toward Optimizing Distributed Programs Directed By Configurations  
Author: Tae-Hyung Kim 
Degree: Doctor of Philosophy, 1996 Dissertation directed by: Associate Professor James M. Purtilo  
Affiliation: Institute for Advanced Computer Studies and Computer Science Department University of Maryland College Park, Maryland 20742  Department of Computer Science  
Note: Abstract Title of Dissertation:  
Abstract: Networks of workstations are now viable environments for running distributed and parallel applications. Recent advances in software interconnection technology enables programmers to prepare applications to run in dynamically changing environments because module interconnection activity is regarded as an essentially distinct and different intellectual activity so as isolated from that of implementing individual modules. But there remains the question of how to optimize the performance of those applications for a given execution environment: how can developers realize performance gains without paying a high programming cost to specialize their application for the target environment? Interconnection technology has allowed programmers to tailor and tune their applications on distributed environments, but the traditional approach to this process has ignored the performance issue over gracefully seemless integration of various software components. Networks of workstations can be virtual parallel machines. For a distributed and parallel application on such environments, an ability to write performance-literate programs is as important as that to seemlessly integrate distributed modules. Our dissertation research is an effort to extend the plain interconnection technology to that with a variety of performance attributes. The RPC (remote procedure call) paradigm is used at the module programming level because it adopts a widely used and understood procedure call abstraction as the sole 
Abstract-found: 1
Intro-found: 1
Reference: [ABLL92] <author> T. Anderson, B. Bershad, E. Lazowska, and H. Levy. </author> <title> Scheduler activations: Effective kernel support for the user-level management of parallelism. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 10(1) </volume> <pages> 53-79, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: Unfortunately, if users should choose those `right' routines for a proper communication style, the RPC paradigm loses its superiority over message passing style programming with explicit send and receive primitives. Another approach to cure the synchronous nature of RPC is using light-weight threads for RPC calls <ref> [BELL90, BELL89, ABLL92, SB90] </ref>. When an RPC is invoked, a new thread is created to take a waiting burden for the return value, and the calling process continues its execution. Anderson et al. [ABLL92] reported that user-level light-weight process control is more efficient than kernel-level control [ABLL92]. <p> Another approach to cure the synchronous nature of RPC is using light-weight threads for RPC calls [BELL90, BELL89, ABLL92, SB90]. When an RPC is invoked, a new thread is created to take a waiting burden for the return value, and the calling process continues its execution. Anderson et al. <ref> [ABLL92] </ref> reported that user-level light-weight process control is more efficient than kernel-level control [ABLL92]. Special mechanisms need be provided to make RPC possible if servers are replicated, which is an another form of variation. <p> When an RPC is invoked, a new thread is created to take a waiting burden for the return value, and the calling process continues its execution. Anderson et al. <ref> [ABLL92] </ref> reported that user-level light-weight process control is more efficient than kernel-level control [ABLL92]. Special mechanisms need be provided to make RPC possible if servers are replicated, which is an another form of variation. Replicated Distributed System [Coo85], PARPC [MBR87], Marionette [SA89] and MultiRPC [SS86] present mechanisms to call multiple instances of same remote operation in parallel on multiple servers.
Reference: [AL93] <author> S. P. Amarasinghe and M. S. Lam. </author> <title> Communication optimization and code generation for distributed memory machines. </title> <booktitle> In Proceedings of the ACM SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 126-138, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: The communication patterns among processes are expected to be highly structured and often predictable so that the entire data can be decomposed before starting computations for the good performance. Initial data layout and communication optimization <ref> [vHK94, HQ91, AL93] </ref> are hot issues in this kind of computations and they are mutually related. Many practical scientific problems except fairly irregular ones are known to be efficiently computed under this structure [FJL + 88].
Reference: [And91a] <author> G. R. Andrews. </author> <title> Concurrent Programming: </title> <booktitle> Principles and Practice. </booktitle> <publisher> The Benjamin/Cummings Publishing Co., Inc., </publisher> <year> 1991. </year>
Reference-contexts: In other words, the nature of an application determines such an `efficient' form of interactions. Those interactions are summarized by four categories as follows, which have been presented in the previous researches on various parallel execution paradigms <ref> [YBS86, BDZ88, Geh84, Geh86, And91a, CG89, LHG86, SA89, Gen81, Geh90] </ref>. In the rest of this dissertation, we study how those fundamental parallel program structures can be properly handled during the process of automatic adaptation with optimization. 3.1 Client/Server structure a server. This is how conventional synchronous RPC [Gib87, BN84] works. <p> Data pipelining is primarily a network of filters that transforms an input <ref> [And91a] </ref>. It is often used in data reduction or image processing systems. Since this structure represents a network of filter processes, it is naturally suitable to a network of workstations environment. Each filter process accepts two inputs and emits a stream of sorted data. <p> Thus, the dissertation research does not focus on exploiting data parallelism, while those program optimization techniques can be adopted as one of infra-structures in our optimization framework that is aimed at multi-paradigm distributed and parallel applications on workstation clusters. 3.5 Remarks Andrew <ref> [And91a, And91b] </ref> classified distributed and parallel program structures based on the behavioral type of a process component in a program rather than the type of interaction among its components as given in the chapter. The four basic types are `filter', `client', `server' and `peer' processes. <p> It is a shared data object. Processes' efforts to read or write those data are controlled in regards with the critical section problem. The remaining two paradigms are easily constructed through the basic four processes | specialist by filter, client and server and agenda by peer processes. Andrews <ref> [And91a, And91b] </ref> presented typical distributed and parallel applications that can be optimally expressed as one or combination of the four process types. A versatile programming environment for all of the forms of parallelism or process can be provided by message passing paradigms like PVM [Sun90] and MPI [For93]. <p> Indeed, any single paradigm is not enough for high performance. That is why Linda has three paradigms for writing a program based on the data sharing in the form of tuple space. And that is why Andrews <ref> [And91a] </ref> suggests to use different types of processes for different types of applications.
Reference: [And91b] <author> G. R. Andrews. </author> <title> Paradigms for process interaction in distributed programs. </title> <journal> ACM Computing Surveys, </journal> <volume> Vol. 23(1), </volume> <month> March </month> <year> 1991. </year>
Reference-contexts: Thus, the dissertation research does not focus on exploiting data parallelism, while those program optimization techniques can be adopted as one of infra-structures in our optimization framework that is aimed at multi-paradigm distributed and parallel applications on workstation clusters. 3.5 Remarks Andrew <ref> [And91a, And91b] </ref> classified distributed and parallel program structures based on the behavioral type of a process component in a program rather than the type of interaction among its components as given in the chapter. The four basic types are `filter', `client', `server' and `peer' processes. <p> It is a shared data object. Processes' efforts to read or write those data are controlled in regards with the critical section problem. The remaining two paradigms are easily constructed through the basic four processes | specialist by filter, client and server and agenda by peer processes. Andrews <ref> [And91a, And91b] </ref> presented typical distributed and parallel applications that can be optimally expressed as one or combination of the four process types. A versatile programming environment for all of the forms of parallelism or process can be provided by message passing paradigms like PVM [Sun90] and MPI [For93].
Reference: [ARZ92] <author> F. Allen, B. K. Rosen, and K. Zadeck. </author> <title> Optimization in Compilers. </title> <publisher> ACM Press, </publisher> <year> 1992. </year>
Reference-contexts: Node v postdominates node w, denoted by v p w, if v appears on every path from w to Exit. If v postdominates w but v 6= w, then v strictly postdominates w <ref> [ARZ92] </ref>. In a postdominator tree (P DT ), the children of a node v are all immediately postdominated by v. Control Dependence A CF G node w is control dependent on a CF G node v if both of the following hold [FOW87]: 1.
Reference: [ASU86] <author> A. V. Aho, R. Sethi, and J. D. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison Wesley, </publisher> <year> 1986. </year>
Reference-contexts: That is, a du-chain allows us to find all tuples that might use the value assigned a variable at a particular vertex in CF G. A ud-chain (use-def chain) is the set of reaching definitions associated with each use of a variable <ref> [ASU86] </ref>. Dominance and Post-dominance Node v dominates node w, denoted by vw, if v appears on every path from Entry to w. A node always dominates itself. Node v immediately 14 dominates node w iff v dominates w and there is no node x such that vxw. <p> The more the gap is widened, the more statements (including another remote call) can be executed during executing R. Let us discuss the initialization in more details. Node v dominates node w, denoted by vw, if v appears on every path from Entry to w <ref> [ASU86] </ref>. Node v immediately dominates node w iff vw and there is no node x such that vx and xw. In a dominator tree (DT ) of a CF G, the children of a node v are all immediately dominated by v.
Reference: [ATK91] <author> A. L. Ananda, B. H. Tay, and E. K. Koh. </author> <title> Astra An asynchronous remote procedure call facility. </title> <booktitle> In Proceedings of the 11th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 172-179, </pages> <year> 1991. </year>
Reference-contexts: Since the synchronous property of RPC results in hindrance to parallel executions that can increase the total elapse time, various asynchronous RPC mechanisms have been devised to implement RPC in a non-blocking way <ref> [LS88, ATK91, WFN90, GG88] </ref>. Call streaming [LS88] is a pioneering work in an asynchronous RPC implementation. A new data type called a promise which is created at the time of a call so that the caller can continue its execution was designed to support asynchronous calls known as call streaming. <p> An asynchronous call does not block the client, and replies can be received as they are needed. To date, the decision on calling style is not the programmer's (for example, calls may be synchronous only [BN84] or they may be asynchronous only <ref> [ATK91, LS88, WFN90] </ref>), or the decision has to be made at module programming level by use of different library routines [Cor91]. If we let this decision be separate from RPC statement, the modules will remain reusable for different calling styles.
Reference: [Bar78] <author> J. M. Barth. </author> <title> A practical interprocedural data flow analysis algorithm. </title> <journal> Communication of the ACM, </journal> <volume> Vol. 21(9) </volume> <pages> 724-736, </pages> <month> September </month> <year> 1978. </year>
Reference-contexts: An edge without a control predicate means T (true). Since the call tree encompasses all possible call sequences in the program, the control predicates on edges are flow-sensitive information <ref> [Bar78, Cal88, Hal90] </ref>. <p> From the viewpoint of each procedure, the 49 interprocedural data flow equations to this end can be expressed as following recursive forms where the Called (P ) is the set of remote procedures called directly from P <ref> [Bar78] </ref>: U se (P ) = LocalU se (P ) [ Q2Called (P ) U se (Q) (Eq5.5) Def (P ) = LocalDef (P ) [ Q2Called (P ) Def (Q) (Eq5.6) We can rewrite the above equations as the following concrete forms, because (1) call/return is the sole mechanism of
Reference: [BB90] <author> K. P. Belkhale and P. Banerjee. </author> <title> An approximate algorithm for the parti-tionable independent task scheduling problem. </title> <booktitle> In Proceedings of '90 International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1990. </year>
Reference-contexts: If we cannot guarantee the faster processor finishes earlier, the migration paths must be bi-directional as in the following cases. For the latter case, we cannot initialize in the above way as the value of L (i) is unknown. The LPT (Largest Processing Time first) algorithm <ref> [BB90] </ref> is for this class of loop models. The tasks are sorted in descending order based on execution time L (i). Each processor 68 should process the largest task first.
Reference: [BDZ88] <author> P. A. Buhr, Glen Ditchfield, and C. R. Zarke. </author> <title> Adding concurrency to a statically type-safe object-oriented programming language. </title> <booktitle> In Proceedings of the ACM SIGPLAN Workshop on object-based concurrent programming, </booktitle> <pages> pages 18-21, </pages> <month> September </month> <year> 1988. </year> <month> 74 </month>
Reference-contexts: In other words, the nature of an application determines such an `efficient' form of interactions. Those interactions are summarized by four categories as follows, which have been presented in the previous researches on various parallel execution paradigms <ref> [YBS86, BDZ88, Geh84, Geh86, And91a, CG89, LHG86, SA89, Gen81, Geh90] </ref>. In the rest of this dissertation, we study how those fundamental parallel program structures can be properly handled during the process of automatic adaptation with optimization. 3.1 Client/Server structure a server. This is how conventional synchronous RPC [Gib87, BN84] works.
Reference: [BELL89] <author> B. N. Berstad, T. E.Anderson, E. D. Lazowska, and H. M. Levy. </author> <title> Lightweight remote procedure call. </title> <booktitle> In Proceedings of 12th Symposium on Operating Systems Principles, </booktitle> <pages> pages 102-113, </pages> <year> 1989. </year>
Reference-contexts: Unfortunately, if users should choose those `right' routines for a proper communication style, the RPC paradigm loses its superiority over message passing style programming with explicit send and receive primitives. Another approach to cure the synchronous nature of RPC is using light-weight threads for RPC calls <ref> [BELL90, BELL89, ABLL92, SB90] </ref>. When an RPC is invoked, a new thread is created to take a waiting burden for the return value, and the calling process continues its execution. Anderson et al. [ABLL92] reported that user-level light-weight process control is more efficient than kernel-level control [ABLL92].
Reference: [BELL90] <author> B. N. Berstad, T. E.Anderson, E. D. Lazowska, and H. M. Levy. </author> <title> Lightweight remote procedure call. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> Vol. 8(8) </volume> <pages> 37-55, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: The RPC paradigm adopts the model of client-server computing; caller and callee correspond to client and server, respectively. Traditional researches on improving RPC program performance have focused on reducing latency and transmission time within this pairwise form <ref> [JZ93, BELL90, GG88] </ref>. When this simple topology extends to a network of client-server model computing, more advanced optimization other than just efficient pairwise hooking between client and server is called for. Figure 1.1 shows two basic topologies to form a general application. <p> Unfortunately, if users should choose those `right' routines for a proper communication style, the RPC paradigm loses its superiority over message passing style programming with explicit send and receive primitives. Another approach to cure the synchronous nature of RPC is using light-weight threads for RPC calls <ref> [BELL90, BELL89, ABLL92, SB90] </ref>. When an RPC is invoked, a new thread is created to take a waiting burden for the return value, and the calling process continues its execution. Anderson et al. [ABLL92] reported that user-level light-weight process control is more efficient than kernel-level control [ABLL92].
Reference: [bJTF92] <author> Edited by J. T. Feo. </author> <title> A Comparative Study of Parallel Programming Languages: The Salishan Problems. </title> <publisher> North-Holland, </publisher> <year> 1992. </year>
Reference-contexts: Wilson [Wil94] proposes 9 applications that can assess how well a parallel programming system can support large scale software engineering and how easily systems can be learned or how quickly code can be developed. Prior to that Feo <ref> [bJTF92] </ref> suggested Salishan Problems and collected results for the problems from various parallel programming systems | all at the module programming level. The comparison goals in the assessments were two-fold.
Reference: [BL93] <author> R. Butler and E. Lusk. </author> <title> Monitors, messages and clusters: The p4 parallel programming system. </title> <type> Technical Report MCS-P362-0493, </type> <institution> Argonne National Laboratory, Argonne, IL, </institution> <year> 1993. </year>
Reference-contexts: Message passing primitives <ref> [Sun90, For93, BL93, Pur94, Fel79, Coo80] </ref> are expressive enough to program for efficiency; however, they are too low-level to write large distributed programs. Programmers are fully responsible for matching send/receive pairs, allocating buffers, and marshaling/unmarshaling data correctly. <p> It can be analogous to the trade-off between assembler vs. compiler. Higher level message passing systems include PVM [Sun90], MPI [For93], p4 <ref> [BL93] </ref> and Polylith [Pur94]. PVM PVM [Sun90] (Parallel Virtual Machine) is a user-level code and uses rsh commands to initiate daemons on remote machines. The user writes applications as a collection of cooperative tasks. Tasks access PVM resources through a library of standard interface routines. <p> Communication structures include those for sending and receiving data as well as 12 high-level primitives such as broadcast, barrier synchronization and reduction operations. Task (or process) management operations like spawning, killing, initializing are provided as well. p4 Message passing in p4 <ref> [BL93] </ref> system is achieved through similar traditional explicit send and receive primitives as other message passing systems. However, the user is responsible for buffer allocation and management. Broadcast, barrier synchronization, global operations are provided.
Reference: [BLA + 93] <author> M. A. Blumrich, K. Li, R. Alpert, C. Dubnicki, E. W. Felten, and J. Sand-berg. </author> <title> Virtual memory mapped network interface for the shrimp multicom-puter. </title> <type> Technical Report TR-437-93, </type> <institution> Princeton University Computer Science Department, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: Many researches have engaged in providing novel hardware or software solutions to alleviate such an obstacle for workstation clusters to be a viable environments for running parallel applications. Hardware solutions like SHRIMP (Scalable High-performance Really Inexpensive Multiprocessor) <ref> [BLA + 93] </ref> and PAPERS (Purdue's Adaptor for Parallel Execution and Rapid Synchronization) [DMSM94] projects are to develop an add-on interface unit that is connected to each workstation so that the resulting cluster can run at faster communication speeds and even allow fine-grain parallel execution.
Reference: [BMW85] <author> W. C. Brantley, K. P. McAuliffe, and J. Weiss. </author> <title> RP3 processor-memory element. </title> <booktitle> In Proceedings of the 1985 International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1985. </year>
Reference-contexts: However, still there is an order of magnitude difference in the speed of latency and transmission rate from a real parallel machine whether it is a distributed memory machine like Intel Paragon [Hoc94] or shared memory machine like IBM RP3 <ref> [BMW85] </ref>. Many researches have engaged in providing novel hardware or software solutions to alleviate such an obstacle for workstation clusters to be a viable environments for running parallel applications.
Reference: [BN84] <author> A. D. Birrell and B. J. Nelson. </author> <title> Implementing remote procedure calls. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> Vol. 2(1) </volume> <pages> 39-59, </pages> <month> February </month> <year> 1984. </year>
Reference-contexts: The MIL syntax is slightly extended to include performance-related factors for our purpose (Chapter 4). Module level programming is conducted by conventional programming languages like C with a remote procedure call abstraction for non-local interactions among software modules. Conventional stub-generation based RPC implementations <ref> [BN84, Gib87, CP91] </ref> suffer low performance because parallelism is inhibited and communications may be redundant. That we want to circumvent those problems motivates us to have a parallelization phase. This is to develop source-level transformation of RPC-based distributed programs for higher performance. <p> For example, the communication between far distant layers might require a series of communications between a series of adjacent layers. The major problem of traditional stub generation based methods <ref> [BN84, Gib87] </ref> for implementing RPC paradigm is that it just adopts the natural flow of modularization as its actual flow of a program while the ideal flow of it does not conform to that way the program is written. <p> RPC is a convenient paradigm for the sake of writing programs at the sacrifice of performance, if traditional stub generation based approaches <ref> [BN84, Gib87, CP91] </ref> are used. The automatic parallelization technique compensates programmers for the per formance problem. * A new decentralized load balancing scheme has been developed for workstation cluster environments [KP96a]. <p> Finally, we study various load balancing methods and compare them to our dynamic and decentralized load balancing method. 2.1 Remote Procedure Call Remote procedure call <ref> [BN84, Cor91] </ref> is a popular paradigm for distributed programming since it simplifies program construction by abstracting away from details of communication and synchronization. However, these early RPC implementations are synchronous in nature, and hence fail to exploit the inherent parallelism in distributed applications. <p> Optimizing RPC performance has been limited to how to efficiently hook in a pairwise sense between client and server communications as shown in the original work by Birrel and Nelson <ref> [BN84] </ref> and Peregrine high performance RPC system [JZ93]. Since the synchronous property of RPC results in hindrance to parallel executions that can increase the total elapse time, various asynchronous RPC mechanisms have been devised to implement RPC in a non-blocking way [LS88, ATK91, WFN90, GG88]. <p> In the rest of this dissertation, we study how those fundamental parallel program structures can be properly handled during the process of automatic adaptation with optimization. 3.1 Client/Server structure a server. This is how conventional synchronous RPC <ref> [Gib87, BN84] </ref> works. As a typical application in this category, let's consider a disk server that repeatedly handles read and write requests from client processes. A scheduling like SST (Shortest Seek Time first), SCAN, and C-SCAN is often used to optimize the moving distance of disk head. <p> The RPC paradigm adopts a widely used and understood procedure call abstraction as the sole mechanism of remote operations; thus it simplifies distributed programming by abstracting from details of communication and synchronization. Since those conventional implementations of RPC paradigm <ref> [BN84, Gib87] </ref> only allow client and server types of processes, the RPC paradigm has a limited coverage of applications especially for high performance. Indeed, any single paradigm is not enough for high performance. <p> Throughout this section, the expressions enclosed by an oval box denote our extension of the original MIL specification in polygen [CP91] for a performance configuration. Calling Style A synchronous call is a call whereby the client blocks the call until the server completes it <ref> [BN84] </ref>. An asynchronous call does not block the client, and replies can be received as they are needed. To date, the decision on calling style is not the programmer's (for example, calls may be synchronous only [BN84] or they may be asynchronous only [ATK91, LS88, WFN90]), or the decision has to <p> is a call whereby the client blocks the call until the server completes it <ref> [BN84] </ref>. An asynchronous call does not block the client, and replies can be received as they are needed. To date, the decision on calling style is not the programmer's (for example, calls may be synchronous only [BN84] or they may be asynchronous only [ATK91, LS88, WFN90]), or the decision has to be made at module programming level by use of different library routines [Cor91]. If we let this decision be separate from RPC statement, the modules will remain reusable for different calling styles. <p> Each of the execution scenarios shows performance that is comparable to a manually coded counterparts, yet these were achieved without extensive manual intervention on the part of programmers. 39 40 Chapter 5 Source-to-Source Transformation When an RPC is implemented through traditional stub generation based methods <ref> [BN84, CP91, Gib87] </ref>, a stub takes in charge of the three functions: (1) communication - RPC arguments are transmitted to the remote callee, and the result is back to the caller, (2) synchronization the caller is suspended until the result is back, and (3) data conversion machines may have distinct data <p> = LocalU se (P ) [ Q2Called (P ) U se (Q) (Eq5.5) Def (P ) = LocalDef (P ) [ Q2Called (P ) Def (Q) (Eq5.6) We can rewrite the above equations as the following concrete forms, because (1) call/return is the sole mechanism of interactions between remote processes <ref> [BN84] </ref>, and (2) call-by-value semantics is useful enough in general distributed programs [HL82].
Reference: [BR89] <author> L. Bomans and D. Roose. </author> <title> Benchmarking the iPSC/2 hypercube multiprocessor. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> Vol. 1(1) </volume> <pages> 3-18, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: The conventional approach to modeling communication time for transferring a message of m bytes is a simple linear function, i.e. T comm = ff + fim, where ff is startup time and fi is transfer time per byte <ref> [BR89] </ref>. The empirical values for ff and fi under the PVM system [Sun90] at LAN-based clustered workstations are 4:527 msec, 0:0024 msec and 1:661 msec, 0:00157 msec for datagram and stream transmission cases, respectively, which imply ff fi [SS94].
Reference: [BST89] <author> H. E. Bal, J. G. Steiner, and A. S. Tanenbaum. </author> <title> Programming languages for distributed computing systems. </title> <journal> ACM Computing Surveys, </journal> <volume> Vol. 21(3) </volume> <pages> 260-322, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: Writing distributed and/or parallel programs is difficult for programmers, and even more difficult when high performance is required. Many mechanisms to achieve better performance in distributed programming have been proposed <ref> [BST89, Geh86, Geh90, Gen81, LS88] </ref>; however, in practice these mechanisms are hard to utilize, and do not take into account the burden placed on programmers who already encounter difficulty in writing functionally correct programs. <p> The capability to write an efficient program is not enough, unless the writing complexity is controlled in a systematic way. Furthermore, most of these mecha 1 nisms are expressed by special programming language constructs for specifying the exact semantics on communication and synchronization <ref> [BST89] </ref>. Such languages are not good at accommodating the programming skills of those who are already accustomed to conventional programming languages like C.
Reference: [Cal88] <author> D. Callahan. </author> <title> The program summary graph and flow-sensitive interprocedural data flow analysis. </title> <booktitle> In Proceedings of the ACM SIGPLAN '88 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 47-56, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: An edge without a control predicate means T (true). Since the call tree encompasses all possible call sequences in the program, the control predicates on edges are flow-sensitive information <ref> [Bar78, Cal88, Hal90] </ref>.
Reference: [Car93] <author> John B. Carter. </author> <title> Efficient Distributed Shared Memory Based On Multi-Protocol Release Consistency. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> sep </month> <year> 1993. </year>
Reference-contexts: Message passing primitives [Sun90, For93, BL93, Pur94, Fel79, Coo80] are expressive enough to program for efficiency; however, they are too low-level to write large distributed programs. Programmers are fully responsible for matching send/receive pairs, allocating buffers, and marshaling/unmarshaling data correctly. Programming under DSM systems <ref> [Car93] </ref> eases such difficulties, but the resulting programs suffer efficiency due to false sharing and coherence maintaining overhead especially in a distributed-memory machine environments.
Reference: [CFR + 91] <author> R. Cytron, J. Ferrante, B. K. Rosen, M. N. Wegman, and F. K. Zadeck. </author> <title> Efficiently computing static single assignment form and the control dependence graph. </title> <journal> ACM Transactions on Programming Languages and systems, </journal> <volume> 13 </volume> <pages> 451-490, </pages> <month> October </month> <year> 1991. </year> <month> 75 </month>
Reference-contexts: Consider an example program shown in Figure 5.2 (a). 4 From the CFG in Figure 5.2 (b), we can evaluate a control predicate as follows. 4 In the SSA (Static Single Assignment) <ref> [CFR + 91] </ref> representation of the program, a join node for the loop construct is omitted for brevity. 45 main () f 1 ; /* defs on x 1 , y 1 , z 1 */ loop f 2 : y 2 = f 1 (x 1 ); 3 ; 4
Reference: [CG89] <author> N. Carriero and D. Gelernter. </author> <title> How to write parallel programs: A guide to the perplexed. </title> <journal> ACM Computing Surveys, </journal> <volume> Vol. 21(6) </volume> <pages> 322-356, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: Two major limitations are that it is static and that the loop heterogeneity model is linear. In this dissertation, we present a new dynamic load balancing method for parallel loops of more general patterns, since many non-scientific applications such as the DNA sequence search problem <ref> [CG89] </ref> or the Mandelbrot set computation [FvDF + 93], which are good candidate applications for workstation clusters, often do not carry conventional regular loop patterns. The unpredictable patterns can even be detrimental to those improvements [KW85, PK87, TN91], although the pure self-scheduling scheme is orthogonal to the loop patterns. <p> The Polylith system is an implementation of such a software bus that hides compatibility problems from software developers. 2.3 The Linda Linda <ref> [CG89, CG90] </ref> is a unique programming system that is based on a special memory model called tuple space. It consists of a few simple operations and is orthogonal to the base languages in which it is embedded. Linda memory consists of a collection of logical tuples. <p> In other words, the nature of an application determines such an `efficient' form of interactions. Those interactions are summarized by four categories as follows, which have been presented in the previous researches on various parallel execution paradigms <ref> [YBS86, BDZ88, Geh84, Geh86, And91a, CG89, LHG86, SA89, Gen81, Geh90] </ref>. In the rest of this dissertation, we study how those fundamental parallel program structures can be properly handled during the process of automatic adaptation with optimization. 3.1 Client/Server structure a server. This is how conventional synchronous RPC [Gib87, BN84] works. <p> Particularly for irregular loops, we can distinguish between the two cases: predictable vs. unpredictable. For example, the parallel tasks in the DNA sequence search problem <ref> [CG89] </ref> and the Mandelbrot set computation are all irregular, but the tasks in the first problem are predictable while the tasks in the second one are not. <p> loops are predictable (see Section 6.1.1), there are two cases: one is when we know the amount of the required computation exactly, as in Figure 6.1 (a), (b), (c) and sometimes (d), and the other is when we can determine just the orderings, like in the DNA sequence search problem <ref> [CG89] </ref>. For the former case, as L (i) is known in advance, if we distribute proportionately according to each processor's throughput, we can reduce the likelihood of migration. In other words, the processor P i with t i will get t i P P k t k .
Reference: [CG90] <author> N. Carriero and D. Gelernter. </author> <title> How to write parallel programs: A first course. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: The Polylith system is an implementation of such a software bus that hides compatibility problems from software developers. 2.3 The Linda Linda <ref> [CG89, CG90] </ref> is a unique programming system that is based on a special memory model called tuple space. It consists of a few simple operations and is orthogonal to the base languages in which it is embedded. Linda memory consists of a collection of logical tuples. <p> Tuples are either active (process tuples) or passive (data tuples). As explained in Chapter 3, Linda model perceives a parallel programming as three basic paradigms of coordinations: agenda, result and specialist. Linda is a programming model that coordinates those three paradigms. One of its realization, C-Linda <ref> [CG90] </ref> has four basic tuple-space operations as follows. 13 out (t) causes tuple t to be added to tuple space; the executing process continues im-mediately. <p> For example, the types of processes in client and server structure are of client and server. Pipeline is of filter. Master/slave and SPMD are of peer. Another classification on paradigms of parallel computations is given by the Linda <ref> [CG90] </ref> project: result, specialist, and agenda parallelism. Result parallelism focuses on the shape of result in order for process interactions. Specialist parallelism focuses on the makeup of the work crew that are specialists for particular jobs. <p> Length of communication paths: RPC can lengthen communication paths unnecessarily if involved modules form a computation network (like the trellis model in Chapter 8 of <ref> [CG90] </ref>) because of its two-way communication protocol. For instance, in Figure 4.1 (c), an intermediate result in each stage of the compare module must go back to the client first before being delivered to the next stage. An optimization step that eliminates such unnecessary communication paths is called for. <p> Second, the scheme does not allow overlap between communication and computation because the next task can not be issued unless the current one has been finished. To alleviate these problems, water-marking can be used <ref> [CG90] </ref>. The idea is to maintain the number of tasks between an upper and a lower limit. The upper water-mark limits the maximum number of queuing tasks to a particular slave, thus prevents possible overload.
Reference: [CLZ95] <author> M. Cierniak, W. Li, and M. J. Zaki. </author> <title> Loop scheduling for heterogeneity. </title> <booktitle> In Proceedings of the 4th International Symposium on High-Performance Distributed Computing, </booktitle> <month> August </month> <year> 1995. </year>
Reference-contexts: A loop is called a parallel loop (DOALL-loop) if there are no data dependences among all iterations. The question of how to allocate an iteration to a particular processor for minimizing the total execution time is known as a loop scheduling problem <ref> [TY86, KW85, PK87, TN91, CLZ95] </ref>. Networks of workstations are somewhat new environments for loop scheduling problems: the communication delay is longer and the granularity of a sub-task is coarser. <p> Networks of workstations are somewhat new environments for loop scheduling problems: the communication delay is longer and the granularity of a sub-task is coarser. To our knowl 8 edge, the first work on parallel loop scheduling in a network of heterogeneous workstations was done by Cierniak et al. <ref> [CLZ95] </ref>. They considered three aspects of heterogeneity | loop, processor, and network | and developed algorithms for generating optimal and sub-optimal schedules of loops. Two major limitations are that it is static and that the loop heterogeneity model is linear. <p> Load balancing for multiple sub-tasks generated from a single application has been known as "parallel loop scheduling problems" <ref> [TY86, KW85, PK87, TN91, CLZ95] </ref>, which have been researched as a way of loop parallelization in a shared-memory programming model. If there are I uniform-sized iterations, and P identical processors, load can be balanced simply by assigning I=P iterations to each processor. <p> If a size of sub-task as a unit of computation is variable, and even worse, if it cannot be known in 16 advance or the throughputs are variable too, this proportionally allocating approach cannot be effective. Cierniak et al. <ref> [CLZ95] </ref> deals with the very parallel loop scheduling problem in a version of a network of workstations. They present an optimal algorithm that allocates sub-tasks to all involving processors so that the elapse time can be minimized, although their heterogeneity model is limited to linear one. <p> It is also known as administrator and workers structure [Gen81]. This category encompasses a well-known parallel loop problem in parallel processing <ref> [TY86, KW85, PK87, TN91, CLZ95] </ref>. Conventional RPC represents client/server computations. Some modifications are required to utilize the replicated servers. PARPC [MBR87] and MultiRPC [SS86] changed the basic RPC mechanism to be able to deal with multiple servers. <p> Finally, as in Figure 6.1 (d), the workload may be quite irregular. Many non-scientific applications carry parallel loops of this type. The first three cases have been specially considered by conventional loop scheduling methods <ref> [PK87, TN91, CLZ95] </ref> in order to improve on the basic self-scheduling method. Particularly for irregular loops, we can distinguish between the two cases: predictable vs. unpredictable.
Reference: [CMZ92] <author> B. Chapman, P. Mehrotra, and H. Zima. </author> <title> Programming in Vienna Fortran. </title> <journal> Scientific Programming, </journal> <volume> 1(1) </volume> <pages> 31-50, </pages> <year> 1992. </year>
Reference-contexts: If v and w conflict with each other, and if v is reachable from w or w is reachable from v in CF G, we say that v is data dependent on w or w is data dependent on v. 2.4.2 Previous Works Many parallelization compilers <ref> [KLS + 94, HKT92, CMZ92] </ref> have a main target of data parallelism at the loop level. Researches on function-level parallelism are relatively rare partly because many scientific problems contain data parallelism at the loop level as shown in a systematic work by Fox et el. [FJL + 88]. <p> f if (strm1 [i] &lt;= strm2 [j]) outstrm [k++] = strm1 [i++]; else outstm [k++] = strm2 [j++]; g else if (i &lt;= strm1 [0]) outstrm [k++] = strm1 [i++]; else outstrm [k++] = strm2 [j++]; g outstrm [0] = strm1 [0] + strm2 [0]; return (outstrm); g 23 Fortran <ref> [CMZ92] </ref> and HPF (High Performance Fortran) [KLS + 94] are popular. The primary works are concentrated on how to optimize such a Fortran program using program analysis techniques. Many research works have been done for this structure of parallel computations on distributed memory parallel machine environments.
Reference: [Coo80] <author> R. P. Cook. </author> <title> *MOD A language for distributed programming. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> Vol. 6(6), </volume> <month> November </month> <year> 1980. </year>
Reference-contexts: Message passing primitives <ref> [Sun90, For93, BL93, Pur94, Fel79, Coo80] </ref> are expressive enough to program for efficiency; however, they are too low-level to write large distributed programs. Programmers are fully responsible for matching send/receive pairs, allocating buffers, and marshaling/unmarshaling data correctly.
Reference: [Coo85] <author> E. C. Cooper. </author> <title> Replicated Distributed System. </title> <booktitle> In Proceedings of the 10th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 63-78, </pages> <year> 1985. </year>
Reference-contexts: Anderson et al. [ABLL92] reported that user-level light-weight process control is more efficient than kernel-level control [ABLL92]. Special mechanisms need be provided to make RPC possible if servers are replicated, which is an another form of variation. Replicated Distributed System <ref> [Coo85] </ref>, PARPC [MBR87], Marionette [SA89] and MultiRPC [SS86] present mechanisms to call multiple instances of same remote operation in parallel on multiple servers. The caller then blocks until one or all of the requests have been completed.
Reference: [Cor91] <author> John R. Corbin. </author> <title> SUN RPC: The art of distributed applications: programming techniques for remote procedure calls. </title> <publisher> Springer-Verlag, </publisher> <year> 1991. </year>
Reference-contexts: Finally, we study various load balancing methods and compare them to our dynamic and decentralized load balancing method. 2.1 Remote Procedure Call Remote procedure call <ref> [BN84, Cor91] </ref> is a popular paradigm for distributed programming since it simplifies program construction by abstracting away from details of communication and synchronization. However, these early RPC implementations are synchronous in nature, and hence fail to exploit the inherent parallelism in distributed applications. <p> However, the remote pipe can work only for the remote operation that does not expect the return value. In fact, 11 the call streaming approach includes return value streaming as well, thus it should be su-perior in this sense. Sun RPC system <ref> [Cor91] </ref> supports three different asynchronous RPC (non-blocking, asynchronous broadcast, and callback RPC) in addition to synchronous one. For those asynchronous RPC mechanisms that support return values, the disadvantage is that the programmer is responsible for claiming the delayed return value by specifying the right location in the program. <p> To date, the decision on calling style is not the programmer's (for example, calls may be synchronous only [BN84] or they may be asynchronous only [ATK91, LS88, WFN90]), or the decision has to be made at module programming level by use of different library routines <ref> [Cor91] </ref>. If we let this decision be separate from RPC statement, the modules will remain reusable for different calling styles. <p> chapter presents the constraints that must be preserved through in-line RPC transformation process, the compiler techniques to achieve our optimization goals, and the method to finally produce appropriate client and server modules based on optimizing transformation. 1 In general, a data type is converted into a standardized type like XDT <ref> [Cor91] </ref> (encoding) before converted into a specific type (decoding).
Reference: [CP91] <author> J. R. Callahan and J. M. Purtilo. </author> <title> A packaging system for heterogeneous execution environments. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> Vol. 17(6) </volume> <pages> 626-635, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: To carry out this approach, we consider the process of writing high-performance distributed parallel programs with two cooperative levels of programming: module- and configuration-level programming. Configuration programs will be expressed by a MIL (Module Interconnection Language) that has been used in the precursor of this dissertation research, software packager <ref> [CP91] </ref>. The MIL syntax is slightly extended to include performance-related factors for our purpose (Chapter 4). Module level programming is conducted by conventional programming languages like C with a remote procedure call abstraction for non-local interactions among software modules. <p> The MIL syntax is slightly extended to include performance-related factors for our purpose (Chapter 4). Module level programming is conducted by conventional programming languages like C with a remote procedure call abstraction for non-local interactions among software modules. Conventional stub-generation based RPC implementations <ref> [BN84, Gib87, CP91] </ref> suffer low performance because parallelism is inhibited and communications may be redundant. That we want to circumvent those problems motivates us to have a parallelization phase. This is to develop source-level transformation of RPC-based distributed programs for higher performance. <p> However, such a change is not enough to improve significantly the overall performance of a program in general. Mostly, such a change is somewhat related to the porting and/or system reconfigurations rather than aimed at improving performances. Conventional interconnection methods like polygen <ref> [CP91] </ref> can be used to generate compilable source codes for this kind of various structural changes at configuration level. More significant improvement can be achieved through a server replication. Server replication is a useful strategy in many cases, but also produces new problems that need to be carefully handled. <p> Many existing performance oriented mechanisms can be achieved by using ordinary modules with proper configuration programs and source-to-source translation techniques. This frees programmers from making extensive amounts of manual adaptations for various performance configurations. This builds upon the MIL (Module Interconnection Language) approach <ref> [CP91, Pur94] </ref> for distributed programming, where the original MIL specification is intended only for structural presentation of interfaces between interacting processes. We append performance related specifications onto each interface specification in a MIL. <p> reassures the important role of configuration-level programming towards higher performance because a proper load balancing topology can be easily constructed under such a programming environment. 1.3 Summary of Contributions The major contributions of this dissertation are itemized as follows: * The configuration issues are popular in constructing large distributed softwares <ref> [DK76, CP91, Kra90] </ref>. We have extended the idea into the performance issue from the original interconnection related ones. One may perceive that any structural change in a distributed program configuration could result in a different performance. This dissertation elaborated on this perception in more details toward performance improvement. <p> RPC is a convenient paradigm for the sake of writing programs at the sacrifice of performance, if traditional stub generation based approaches <ref> [BN84, Gib87, CP91] </ref> are used. The automatic parallelization technique compensates programmers for the per formance problem. * A new decentralized load balancing scheme has been developed for workstation cluster environments [KP96a]. <p> All of these factors are related in module interactions rather than functionality; thus they will be represented at the interconnection programming level. Throughout this section, the expressions enclosed by an oval box denote our extension of the original MIL specification in polygen <ref> [CP91] </ref> for a performance configuration. Calling Style A synchronous call is a call whereby the client blocks the call until the server completes it [BN84]. An asynchronous call does not block the client, and replies can be received as they are needed. <p> Basic tools for preparing applications to run in this environment are already available within the polygen system <ref> [CP91] </ref>, although they are to be tailored to attain our source translation (rather than stub generation) principle. Therefore the principle thrust of our effort has been to add a source translator (gen trans) to the suite of polygen tools. <p> Each of the execution scenarios shows performance that is comparable to a manually coded counterparts, yet these were achieved without extensive manual intervention on the part of programmers. 39 40 Chapter 5 Source-to-Source Transformation When an RPC is implemented through traditional stub generation based methods <ref> [BN84, CP91, Gib87] </ref>, a stub takes in charge of the three functions: (1) communication - RPC arguments are transmitted to the remote callee, and the result is back to the caller, (2) synchronization the caller is suspended until the result is back, and (3) data conversion machines may have distinct data
Reference: [CS93] <author> Clemens H. Cap and Volker Strumpen. </author> <title> Efficient parallel computing in distributed workstation environments. </title> <journal> Parallel Computing, </journal> <volume> Vol. 19 </volume> <pages> 1221-1234, </pages> <year> 1993. </year>
Reference-contexts: in a distributed memory multicomputers in advance, the problem boils down to how to optimally allocate each task node in MDGs to a processor and how to schedule those limited number of available processors. 2.5 Load Balancing Schemes Load balancing concept has been widely studied from an operating system's concerns <ref> [Son94, ELZ86b, KS94, CS93, ELZ86a] </ref>. When there are multiple applications that are working simultaneously on distributed environments, some processors can be too heavily loaded while others are not. <p> Many dynamic load balancing algorithms <ref> [CS93, ELZ86b, KS94, LK87, Son94] </ref> have been devised for good load balance with less migration overhead; they are characterized by the following parameters which distinguish them. <p> if t new &lt; n n+1 t min , the execution time of (n + 1)-processors cluster is T (n+1)t new , which is longer than that of n processors! One may want to get around this problem by allocating tasks according to the known computing power of each processor <ref> [GWWECL94, CS93] </ref>. However, their methods were static, thus of limited usefulness.
Reference: [DK76] <author> F. DeRemer and H. Kron. </author> <title> Programming-in-the-large versus programming-in-the-small. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> Vol. 2(2), </volume> <month> June </month> <year> 1976. </year>
Reference-contexts: configuration level (adaptation phase). 1.1 Motivation Our approach is motivated by the success of on-line system configuration technologies [Kra90] based on the perception that module interconnection activity is to be an essentially distinct and different intellectual activity from that of implementing individual modules; that is, "programming-in-the-large" is distinct from "programming-in-the-small" <ref> [DK76] </ref>. Moreover, workstation clusters are becoming viable environments for running parallel applications. One of the characteristics of such systems is the lack of solidity in a configuration of hardware platform. At the same time, performance is a key issue in writing distributed parallel programs. <p> to those user decided performance factors with a proper performance improving techniques like parallelizing and load balancing. 6 1.2.1 Configuration for High Performance Module interconnection activity is understood to be an essentially distinct and different intellectual activity from that of implementing individual modules, that is "programming-in-the-large" is distinct from "programming-in-the-small" <ref> [DK76] </ref>. Analogously, this observation applies to performance programming as well. Decisions concerning how a configuration might be adapted in order to allow use of performance improvement mechanisms are inherently different from the task of tailoring individual program units and their interfaces to execute as dictated by the abstract decision. <p> reassures the important role of configuration-level programming towards higher performance because a proper load balancing topology can be easily constructed under such a programming environment. 1.3 Summary of Contributions The major contributions of this dissertation are itemized as follows: * The configuration issues are popular in constructing large distributed softwares <ref> [DK76, CP91, Kra90] </ref>. We have extended the idea into the performance issue from the original interconnection related ones. One may perceive that any structural change in a distributed program configuration could result in a different performance. This dissertation elaborated on this perception in more details toward performance improvement.
Reference: [DMSM94] <author> H. G. Dietz, T. Muhammad, J. B. Sponaugle, and T. Mattox. </author> <title> PAPERS:Purdue's Adapter for Parallel Execution and Rapid Synchronization. </title> <type> Technical Report TR-EE94-11, </type> <institution> Purdue University School of Electrical Engineering, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: Hardware solutions like SHRIMP (Scalable High-performance Really Inexpensive Multiprocessor) [BLA + 93] and PAPERS (Purdue's Adaptor for Parallel Execution and Rapid Synchronization) <ref> [DMSM94] </ref> projects are to develop an add-on interface unit that is connected to each workstation so that the resulting cluster can run at faster communication speeds and even allow fine-grain parallel execution.
Reference: [ELZ86a] <author> D. L. Eager, E. D. Lazowska, and J. Zahorjan. </author> <title> A comparison of receiver-initiated and sender-initiated adaptive load sharing. </title> <journal> Performance Evaluation, </journal> <volume> Vol. 6 </volume> <pages> 53-68, </pages> <year> 1986. </year>
Reference-contexts: in a distributed memory multicomputers in advance, the problem boils down to how to optimally allocate each task node in MDGs to a processor and how to schedule those limited number of available processors. 2.5 Load Balancing Schemes Load balancing concept has been widely studied from an operating system's concerns <ref> [Son94, ELZ86b, KS94, CS93, ELZ86a] </ref>. When there are multiple applications that are working simultaneously on distributed environments, some processors can be too heavily loaded while others are not.
Reference: [ELZ86b] <author> Derek L. Eager, Edward D. Lazowska, and John Zahorjan. </author> <title> Adaptive load sharing in homogeneous distributed systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> Vol. 12(5) </volume> <pages> 662-675, </pages> <month> May </month> <year> 1986. </year> <month> 76 </month>
Reference-contexts: in a distributed memory multicomputers in advance, the problem boils down to how to optimally allocate each task node in MDGs to a processor and how to schedule those limited number of available processors. 2.5 Load Balancing Schemes Load balancing concept has been widely studied from an operating system's concerns <ref> [Son94, ELZ86b, KS94, CS93, ELZ86a] </ref>. When there are multiple applications that are working simultaneously on distributed environments, some processors can be too heavily loaded while others are not. <p> Many dynamic load balancing algorithms <ref> [CS93, ELZ86b, KS94, LK87, Son94] </ref> have been devised for good load balance with less migration overhead; they are characterized by the following parameters which distinguish them. <p> the throughput of W i , which is defined by the number of unit tasks per unit time. * fl ij : the amount of load to migrate from i to j. 6.2 Load Balancing Method Two important components of dynamic load balancing schemes are transfer policy and location policy <ref> [ELZ86b, KS94] </ref>. The transfer policy determines whether a task should be processed locally or remotely by transferring it at a particular load state. The location policy determines which process initiates the migration and its source or destination. These are for global load balancing from the OS's viewpoints. <p> Multi-dimensional load vectors determine the load state of a processor. In our system, we aim to balance parallel loops in an application. A simple `demand' message is enough to initiate load migration rather than load state exchange [KS94] or random polling of candidate processors <ref> [ELZ86b] </ref> because the only load vector is the number of sub-tasks in a processor. <p> The thicker links denote higher level links; they will be used only if the load cannot be balanced through the lower links. 6.3 Analysis Of Migration Behaviors There are two important concerns in devising a load balancing scheme <ref> [ELZ86b] </ref>. First, the overhead should not negate the benefits of an improved load distribution. Next, the potential migration instability 1 , in which processors spend too much time transferring tasks, should be avoided. <p> To our knowledge, migration topology for load balancing is considered for the first time. The topology has not been considered important heretofore because sometimes it is given in a hard-wired form [LK87] or it is meaningless where distributed load patterns cannot be assumed to be known in advance <ref> [ELZ86b, KS94] </ref>. We have shown analytically that the overhead of our method is lower than that of the self-scheduling scheme when an "predictability" condition is given. We have also provided some experimental data for cases when the loop pattern is unpredictably irregular.
Reference: [Fel79] <author> J. A. Feldman. </author> <title> High level programming for distributed computing. </title> <journal> Commu--nication of the ACM, </journal> <volume> Vol. 22(6), </volume> <month> June </month> <year> 1979. </year>
Reference-contexts: Message passing primitives <ref> [Sun90, For93, BL93, Pur94, Fel79, Coo80] </ref> are expressive enough to program for efficiency; however, they are too low-level to write large distributed programs. Programmers are fully responsible for matching send/receive pairs, allocating buffers, and marshaling/unmarshaling data correctly.
Reference: [FJL + 88] <author> G. Fox, M. Johnson, G. Lyzenga, S. Otto, J. Salmon, and D. Walker. </author> <title> Solving Problems on Concurrent Processors, volume 1. </title> <publisher> Prentice Hall, </publisher> <year> 1988. </year>
Reference-contexts: Researches on function-level parallelism are relatively rare partly because many scientific problems contain data parallelism at the loop level as shown in a systematic work by Fox et el. <ref> [FJL + 88] </ref>. However, workstation clusters environments provide more necessity to exploitation of function-level or control parallelism because fine grain parallelism is not appropriate due to relatively high communication costs. <p> Initial data layout and communication optimization [vHK94, HQ91, AL93] are hot issues in this kind of computations and they are mutually related. Many practical scientific problems except fairly irregular ones are known to be efficiently computed under this structure <ref> [FJL + 88] </ref>.
Reference: [For93] <author> The MPI Forum. </author> <title> MPI: A Message Passing Interface. </title> <booktitle> In Proceedings Supercomputing '93, </booktitle> <pages> pages 878-883, </pages> <year> 1993. </year>
Reference-contexts: Message passing primitives <ref> [Sun90, For93, BL93, Pur94, Fel79, Coo80] </ref> are expressive enough to program for efficiency; however, they are too low-level to write large distributed programs. Programmers are fully responsible for matching send/receive pairs, allocating buffers, and marshaling/unmarshaling data correctly. <p> It can be analogous to the trade-off between assembler vs. compiler. Higher level message passing systems include PVM [Sun90], MPI <ref> [For93] </ref>, p4 [BL93] and Polylith [Pur94]. PVM PVM [Sun90] (Parallel Virtual Machine) is a user-level code and uses rsh commands to initiate daemons on remote machines. The user writes applications as a collection of cooperative tasks. Tasks access PVM resources through a library of standard interface routines. <p> However, the user is responsible for buffer allocation and management. Broadcast, barrier synchronization, global operations are provided. The p4 system supports both the shared-memory model through monitors and the distributed memory model through conventional message passings. MPI MPI <ref> [For93] </ref> (Message Passing Interface) is an effort by a group of vendors to consolidate the experienced gleaned from the use of various message passing packages into a standardized system. <p> Andrews [And91a, And91b] presented typical distributed and parallel applications that can be optimally expressed as one or combination of the four process types. A versatile programming environment for all of the forms of parallelism or process can be provided by message passing paradigms like PVM [Sun90] and MPI <ref> [For93] </ref>. Not only because of the resulting complexity in writing distributed programs with such a low-level abstraction, but also because of its difficulty to reconfigure, a higher level of abstraction for module interaction is called for. <p> Both of them rely on communication primitives provided by underlying MP systems <ref> [For93, Sun90] </ref> or operating systems. But the transformation based method allows us to apply various program analysis techniques for program transformation towards high performance.
Reference: [FOW87] <author> Jeanne Ferrante, Karl J. Ottenstein, and Joe D. Warren. </author> <title> The program dependence graph and its use in optimization. </title> <journal> ACM Transactions on Programming Languages and systems, </journal> <volume> 9(3) </volume> <pages> 319-349, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: An edge in E CF G is annotated by a control predicate that determines whether or not to take the edge <ref> [FOW87] </ref>. Use and Def sets Each vertex in CF G has a Def and a U se set associated with it. The U se (v) consists of all variables that are accessed during the computation associated with the vertex v. <p> In a postdominator tree (P DT ), the children of a node v are all immediately postdominated by v. Control Dependence A CF G node w is control dependent on a CF G node v if both of the following hold <ref> [FOW87] </ref>: 1. There is a non-null path p: v + ) w such that w postdominates every node after v on p. 2. The node w does not strictly postdominate the node v. <p> CF G ; E CF G ), with unique nodes Entry; Exit 2 V CF G such that there exists a path from Entry to every node in V CF G and a path from every node to Exit; Entry has no incoming edges, and Exit has no outgoing edges <ref> [FOW87] </ref>. An edge in E CF G is annotated by a control predicate that determines whether or not to take the edge. We assume T (true) on single outgoing edge (no branch), that means the edge is always taken after executing the predecessor. <p> Control dependence <ref> [FOW87] </ref> captures the essential control flow relationships in a program. Informally, for nodes v and w in CF G, w is control dependent on v if v can directly affect whether w is executed or not. <p> When v is a closer descendent to x than y in the DT , the dominator x is called closer to v than y. Node v post-dominates node w, denoted by v p w, if v appears on every path from w to Exit <ref> [FOW87] </ref>. Node v immediately post-dominates node w iff v p w and there is no node x such that v p x and x p w. In a post-dominator tree (P DT ), the children of a node v are all immediately post-dominated by v.
Reference: [FvDF + 93] <author> J. D. Foley, A. van. Dam, S. K. Feiner, J. F. Hughes, and R. L. Phillips. </author> <title> Introduction to Computer Graphics. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1993. </year>
Reference-contexts: In this dissertation, we present a new dynamic load balancing method for parallel loops of more general patterns, since many non-scientific applications such as the DNA sequence search problem [CG89] or the Mandelbrot set computation <ref> [FvDF + 93] </ref>, which are good candidate applications for workstation clusters, often do not carry conventional regular loop patterns. The unpredictable patterns can even be detrimental to those improvements [KW85, PK87, TN91], although the pure self-scheduling scheme is orthogonal to the loop patterns.
Reference: [GBSS89] <author> J. L. Gustafson, R. E. Benner, M. P. Sears, and T. D. Sullivan. </author> <title> A radar simulation program for a 1024-processor hypercube. </title> <booktitle> In Proceedings of SuperComputing 1989, </booktitle> <pages> pages 96-105, </pages> <year> 1989. </year>
Reference-contexts: This scheme contains two problems. First, the master process can generate a bottleneck <ref> [GBSS89] </ref>. For example, if there are 100 slaves and a master needs 10 2 second to prepare and send a task, the master would create a bottleneck unless the average time 32 for each slave to finish a task is greater than a second.
Reference: [Geh84] <author> N. H. Gehani. </author> <title> Broadcasting Sequential Processes (BSP). </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> Vol. 10(4), </volume> <month> July </month> <year> 1984. </year>
Reference-contexts: In other words, the nature of an application determines such an `efficient' form of interactions. Those interactions are summarized by four categories as follows, which have been presented in the previous researches on various parallel execution paradigms <ref> [YBS86, BDZ88, Geh84, Geh86, And91a, CG89, LHG86, SA89, Gen81, Geh90] </ref>. In the rest of this dissertation, we study how those fundamental parallel program structures can be properly handled during the process of automatic adaptation with optimization. 3.1 Client/Server structure a server. This is how conventional synchronous RPC [Gib87, BN84] works.
Reference: [Geh86] <author> N. H. Gehani. </author> <title> Concurrent C. </title> <journal> Journal of Software Practice and Experience, </journal> <volume> Vol. 16 </volume> <pages> 821-844, </pages> <month> September </month> <year> 1986. </year>
Reference-contexts: Writing distributed and/or parallel programs is difficult for programmers, and even more difficult when high performance is required. Many mechanisms to achieve better performance in distributed programming have been proposed <ref> [BST89, Geh86, Geh90, Gen81, LS88] </ref>; however, in practice these mechanisms are hard to utilize, and do not take into account the burden placed on programmers who already encounter difficulty in writing functionally correct programs. <p> In other words, the nature of an application determines such an `efficient' form of interactions. Those interactions are summarized by four categories as follows, which have been presented in the previous researches on various parallel execution paradigms <ref> [YBS86, BDZ88, Geh84, Geh86, And91a, CG89, LHG86, SA89, Gen81, Geh90] </ref>. In the rest of this dissertation, we study how those fundamental parallel program structures can be properly handled during the process of automatic adaptation with optimization. 3.1 Client/Server structure a server. This is how conventional synchronous RPC [Gib87, BN84] works.
Reference: [Geh90] <author> N. H. Gehani. </author> <title> Message passing in concurrent C: Synchronous versus asynchronous. </title> <journal> Journal of Software Practice and Experience, </journal> <volume> Vol. 20(6) </volume> <pages> 571-592, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Writing distributed and/or parallel programs is difficult for programmers, and even more difficult when high performance is required. Many mechanisms to achieve better performance in distributed programming have been proposed <ref> [BST89, Geh86, Geh90, Gen81, LS88] </ref>; however, in practice these mechanisms are hard to utilize, and do not take into account the burden placed on programmers who already encounter difficulty in writing functionally correct programs. <p> In other words, the nature of an application determines such an `efficient' form of interactions. Those interactions are summarized by four categories as follows, which have been presented in the previous researches on various parallel execution paradigms <ref> [YBS86, BDZ88, Geh84, Geh86, And91a, CG89, LHG86, SA89, Gen81, Geh90] </ref>. In the rest of this dissertation, we study how those fundamental parallel program structures can be properly handled during the process of automatic adaptation with optimization. 3.1 Client/Server structure a server. This is how conventional synchronous RPC [Gib87, BN84] works.
Reference: [Gen81] <author> W. M. Gentleman. </author> <title> Message passing between sequential processes: The reply primitive and the administrator concept. </title> <journal> Journal of Software Practice and Experience, </journal> <volume> Vol. 11 </volume> <pages> 435-466, </pages> <month> May </month> <year> 1981. </year>
Reference-contexts: Writing distributed and/or parallel programs is difficult for programmers, and even more difficult when high performance is required. Many mechanisms to achieve better performance in distributed programming have been proposed <ref> [BST89, Geh86, Geh90, Gen81, LS88] </ref>; however, in practice these mechanisms are hard to utilize, and do not take into account the burden placed on programmers who already encounter difficulty in writing functionally correct programs. <p> In other words, the nature of an application determines such an `efficient' form of interactions. Those interactions are summarized by four categories as follows, which have been presented in the previous researches on various parallel execution paradigms <ref> [YBS86, BDZ88, Geh84, Geh86, And91a, CG89, LHG86, SA89, Gen81, Geh90] </ref>. In the rest of this dissertation, we study how those fundamental parallel program structures can be properly handled during the process of automatic adaptation with optimization. 3.1 Client/Server structure a server. This is how conventional synchronous RPC [Gib87, BN84] works. <p> Master process generates many divided sub-tasks and allocates them to multiple slave processes and finally collect the results to get an ultimate result as shown in Many applications can be suitable to this form of computations. It is also known as administrator and workers structure <ref> [Gen81] </ref>. This category encompasses a well-known parallel loop problem in parallel processing [TY86, KW85, PK87, TN91, CLZ95]. Conventional RPC represents client/server computations. Some modifications are required to utilize the replicated servers. PARPC [MBR87] and MultiRPC [SS86] changed the basic RPC mechanism to be able to deal with multiple servers.
Reference: [GG88] <author> D. K. Gifford and N. Glasser. </author> <title> Remote pipes and procedures for efficient distributed communication. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> Vol. 6(3) </volume> <pages> 258-283, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: The RPC paradigm adopts the model of client-server computing; caller and callee correspond to client and server, respectively. Traditional researches on improving RPC program performance have focused on reducing latency and transmission time within this pairwise form <ref> [JZ93, BELL90, GG88] </ref>. When this simple topology extends to a network of client-server model computing, more advanced optimization other than just efficient pairwise hooking between client and server is called for. Figure 1.1 shows two basic topologies to form a general application. <p> Since the synchronous property of RPC results in hindrance to parallel executions that can increase the total elapse time, various asynchronous RPC mechanisms have been devised to implement RPC in a non-blocking way <ref> [LS88, ATK91, WFN90, GG88] </ref>. Call streaming [LS88] is a pioneering work in an asynchronous RPC implementation. A new data type called a promise which is created at the time of a call so that the caller can continue its execution was designed to support asynchronous calls known as call streaming. <p> However, a static alternative is more attractive because we do not need to rely on new language constructs for parallel execution. Thus far, it has not been sought as a way to improving RPC programs. Remote pipe <ref> [GG88] </ref> is used to efficiently handle communication patterns of incremental results passing and bulk rate data transfer which are major problem areas in the synchronous RPC communication model. However, the remote pipe can work only for the remote operation that does not expect the return value. <p> Even worse, contemporary RPC systems are optimized to transmit limited amounts of data (usually less than 10 3 bytes) per call. To support the incremental and bulk rate data transfer, wherein conventional RPC systems performance suffers severely, a new communication model called remote pipe <ref> [GG88] </ref> has been devised. In the framework we are motivating, these patterns may be efficiently handled with automatic communication optimization if programmers specify which communication pattern will appear.
Reference: [Gib87] <author> Philip B. Gibbons. </author> <title> A stub generator for multilanguage RPC in heterogeneous environments. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> Vol. 13(1), </volume> <month> Jan-uary </month> <year> 1987. </year>
Reference-contexts: The MIL syntax is slightly extended to include performance-related factors for our purpose (Chapter 4). Module level programming is conducted by conventional programming languages like C with a remote procedure call abstraction for non-local interactions among software modules. Conventional stub-generation based RPC implementations <ref> [BN84, Gib87, CP91] </ref> suffer low performance because parallelism is inhibited and communications may be redundant. That we want to circumvent those problems motivates us to have a parallelization phase. This is to develop source-level transformation of RPC-based distributed programs for higher performance. <p> For example, the communication between far distant layers might require a series of communications between a series of adjacent layers. The major problem of traditional stub generation based methods <ref> [BN84, Gib87] </ref> for implementing RPC paradigm is that it just adopts the natural flow of modularization as its actual flow of a program while the ideal flow of it does not conform to that way the program is written. <p> RPC is a convenient paradigm for the sake of writing programs at the sacrifice of performance, if traditional stub generation based approaches <ref> [BN84, Gib87, CP91] </ref> are used. The automatic parallelization technique compensates programmers for the per formance problem. * A new decentralized load balancing scheme has been developed for workstation cluster environments [KP96a]. <p> In the rest of this dissertation, we study how those fundamental parallel program structures can be properly handled during the process of automatic adaptation with optimization. 3.1 Client/Server structure a server. This is how conventional synchronous RPC <ref> [Gib87, BN84] </ref> works. As a typical application in this category, let's consider a disk server that repeatedly handles read and write requests from client processes. A scheduling like SST (Shortest Seek Time first), SCAN, and C-SCAN is often used to optimize the moving distance of disk head. <p> The RPC paradigm adopts a widely used and understood procedure call abstraction as the sole mechanism of remote operations; thus it simplifies distributed programming by abstracting from details of communication and synchronization. Since those conventional implementations of RPC paradigm <ref> [BN84, Gib87] </ref> only allow client and server types of processes, the RPC paradigm has a limited coverage of applications especially for high performance. Indeed, any single paradigm is not enough for high performance. <p> Each of the execution scenarios shows performance that is comparable to a manually coded counterparts, yet these were achieved without extensive manual intervention on the part of programmers. 39 40 Chapter 5 Source-to-Source Transformation When an RPC is implemented through traditional stub generation based methods <ref> [BN84, CP91, Gib87] </ref>, a stub takes in charge of the three functions: (1) communication - RPC arguments are transmitted to the remote callee, and the result is back to the caller, (2) synchronization the caller is suspended until the result is back, and (3) data conversion machines may have distinct data <p> Without having external data conversion, if L different languages and M different machines are intermixed in a distributed application, then potentially (L fi M ) 2 cases of data conversion must be used <ref> [Gib87] </ref>. 41 S 1 : x = f (x); S 3 : z = h (v); (a) Original code S 1 : x = f (x); S 3 : zz = h (v); (b) After renaming S 0 1 : Send (f (), x); S 0 2 : Send (g (),
Reference: [GP92] <author> M. Girkar and C. D. Polychronopoulos. </author> <title> Automatic extraction of functional parallelism from ordinary programs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> Vol. 3(2) </volume> <pages> 166-178, </pages> <month> March </month> <year> 1992. </year> <month> 77 </month>
Reference-contexts: However, workstation clusters environments provide more necessity to exploitation of function-level or control parallelism because fine grain parallelism is not appropriate due to relatively high communication costs. Girkar and Polychronopoulos <ref> [GP92] </ref> uses interprocedural dependency analysis techniques to exploit function-level (task-level) parallelism from ordinary programs written in a serial program model. Task-level parallelism exists across loop and procedure boundaries. Using Hierarchical Task Graph (HTG) as an intermediate parallel program representation, they try to exploit and extract task-level parallelism.
Reference: [GWWECL94] <author> A. S. Grimshaw, J. B. Weissman, E. A. West, and Jr. E. C. Loyot. Meta--systems: </author> <title> An approach combining parallel processing and heterogeneous distributed computing systems. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Vol. 21 </volume> <pages> 257-270, </pages> <year> 1994. </year>
Reference-contexts: Trapezoid self-scheduling (TSS) uses a linearly decreasing chunk function, which helps to reduce scheduling overhead while still maintaining a reasonable balance [TN91]. Grimshaw et al. <ref> [GWWECL94] </ref> presented a static load balancing method for parallel executions in heterogeneous distributed computing systems. The basic idea is to allocate sub-tasks proportionally to the known throughputs of participating workstations. <p> The task distribution ratio is the only parameter in this scheme. The approach by Grimshaw et al. <ref> [GWWECL94] </ref> belongs to this type. Since load distribution is a client side concern, an attribute loadratio is needed in the use clause. <p> if t new &lt; n n+1 t min , the execution time of (n + 1)-processors cluster is T (n+1)t new , which is longer than that of n processors! One may want to get around this problem by allocating tasks according to the known computing power of each processor <ref> [GWWECL94, CS93] </ref>. However, their methods were static, thus of limited usefulness.
Reference: [Hal85] <author> R. H. Halstead, Jr. </author> <title> Multilisp: A language for concurrent symbolic computation. </title> <journal> ACM Transactions on Programming Languages and systems, </journal> <volume> Vol. 7(4) </volume> <pages> 501-538, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: A new data type called a promise which is created at the time of a call so that the caller can continue its execution was designed to support asynchronous calls known as call streaming. It is inspired by Multilisp <ref> [Hal85] </ref> that is for parallel execution of Lisp programs by means of future data type at run-time. However, a static alternative is more attractive because we do not need to rely on new language constructs for parallel execution.
Reference: [Hal90] <author> Mary W. Hall. </author> <title> Managing Interprocedural Optimization. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> oct </month> <year> 1990. </year>
Reference-contexts: An edge without a control predicate means T (true). Since the call tree encompasses all possible call sequences in the program, the control predicates on edges are flow-sensitive information <ref> [Bar78, Cal88, Hal90] </ref>.
Reference: [HKT92] <author> S. Hiranandani, K. Kennedy, and C.-W Tseng. </author> <title> Compiling fortran D for MIMD distributed-memory machines. </title> <journal> Communication of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: If v and w conflict with each other, and if v is reachable from w or w is reachable from v in CF G, we say that v is data dependent on w or w is data dependent on v. 2.4.2 Previous Works Many parallelization compilers <ref> [KLS + 94, HKT92, CMZ92] </ref> have a main target of data parallelism at the loop level. Researches on function-level parallelism are relatively rare partly because many scientific problems contain data parallelism at the loop level as shown in a systematic work by Fox et el. [FJL + 88]. <p> Many practical scientific problems except fairly irregular ones are known to be efficiently computed under this structure [FJL + 88]. As scientific problems are main targets for this style, parallel variants of Fortran programming language | like Fortran D <ref> [HKT92] </ref>, Vienna 1 Although the previous merge-sort example uses an identical merge module in its parallel computation, this is not because of the enforcement of the structure but because of user's liberty to do so. 22 merge main (DATA) char *DATA f Split DATA stream into 4 data streams; Sort each
Reference: [HL82] <author> M. Herlihy and B. Liskov. </author> <title> A value transmission method for abstract data types. </title> <journal> ACM Transactions on Programming Languages and systems, </journal> <volume> 4(4) </volume> <pages> 527-551, </pages> <month> October </month> <year> 1982. </year>
Reference-contexts: (Eq5.5) Def (P ) = LocalDef (P ) [ Q2Called (P ) Def (Q) (Eq5.6) We can rewrite the above equations as the following concrete forms, because (1) call/return is the sole mechanism of interactions between remote processes [BN84], and (2) call-by-value semantics is useful enough in general distributed programs <ref> [HL82] </ref>.
Reference: [Hoc94] <author> R. W. Hockney. </author> <title> The communication challenge for MPP: Intel paragon and meiko CS-2. </title> <journal> Parallel Computing, </journal> <volume> 20(3) </volume> <pages> 389-398, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: However, still there is an order of magnitude difference in the speed of latency and transmission rate from a real parallel machine whether it is a distributed memory machine like Intel Paragon <ref> [Hoc94] </ref> or shared memory machine like IBM RP3 [BMW85]. Many researches have engaged in providing novel hardware or software solutions to alleviate such an obstacle for workstation clusters to be a viable environments for running parallel applications.
Reference: [HQ91] <author> P. J. Hatcher and M. J. Quinn. </author> <title> Data-parallel programming on MIMD computers. </title> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: The communication patterns among processes are expected to be highly structured and often predictable so that the entire data can be decomposed before starting computations for the good performance. Initial data layout and communication optimization <ref> [vHK94, HQ91, AL93] </ref> are hot issues in this kind of computations and they are mutually related. Many practical scientific problems except fairly irregular ones are known to be efficiently computed under this structure [FJL + 88].
Reference: [JZ93] <author> D. B. Johnson and W. Zwaenepoel. </author> <title> The Peregrine high-performance RPC system. </title> <journal> Journal of Software Practice and Experience, </journal> <volume> Vol. 23(2) </volume> <pages> 201-221, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: The RPC paradigm adopts the model of client-server computing; caller and callee correspond to client and server, respectively. Traditional researches on improving RPC program performance have focused on reducing latency and transmission time within this pairwise form <ref> [JZ93, BELL90, GG88] </ref>. When this simple topology extends to a network of client-server model computing, more advanced optimization other than just efficient pairwise hooking between client and server is called for. Figure 1.1 shows two basic topologies to form a general application. <p> Optimizing RPC performance has been limited to how to efficiently hook in a pairwise sense between client and server communications as shown in the original work by Birrel and Nelson [BN84] and Peregrine high performance RPC system <ref> [JZ93] </ref>. Since the synchronous property of RPC results in hindrance to parallel executions that can increase the total elapse time, various asynchronous RPC mechanisms have been devised to implement RPC in a non-blocking way [LS88, ATK91, WFN90, GG88]. Call streaming [LS88] is a pioneering work in an asynchronous RPC implementation.
Reference: [KLS + 94] <author> C. Koelbel, D. Loveman, R. Schreiber, Jr G. Steele, and M. Zosel. </author> <title> The High Performance Fortran Handbook. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: If v and w conflict with each other, and if v is reachable from w or w is reachable from v in CF G, we say that v is data dependent on w or w is data dependent on v. 2.4.2 Previous Works Many parallelization compilers <ref> [KLS + 94, HKT92, CMZ92] </ref> have a main target of data parallelism at the loop level. Researches on function-level parallelism are relatively rare partly because many scientific problems contain data parallelism at the loop level as shown in a systematic work by Fox et el. [FJL + 88]. <p> [j]) outstrm [k++] = strm1 [i++]; else outstm [k++] = strm2 [j++]; g else if (i &lt;= strm1 [0]) outstrm [k++] = strm1 [i++]; else outstrm [k++] = strm2 [j++]; g outstrm [0] = strm1 [0] + strm2 [0]; return (outstrm); g 23 Fortran [CMZ92] and HPF (High Performance Fortran) <ref> [KLS + 94] </ref> are popular. The primary works are concentrated on how to optimize such a Fortran program using program analysis techniques. Many research works have been done for this structure of parallel computations on distributed memory parallel machine environments. Same techniques can be applied for workstation cluster environments.
Reference: [KP95] <author> T.-H. Kim and J. M. Purtilo. </author> <title> Configuration-level optimization of RPC-based distributed programs. </title> <booktitle> In Proceedings of the 15th International Conference on Distributed Computing Systems, </booktitle> <month> May </month> <year> 1995. </year>
Reference-contexts: We showed what kind of performance factors affects the overall performance of a distributed program and how they can be represented in forms of MIL (Module Interconnection Language) based configuration programming. We studied how such an approach help to do a seamless process in developing high performance distributed programs <ref> [KP95] </ref>. * We have developed an automatic parallelization of RPC-based distributed programs [KP96b]. RPC is a convenient paradigm for the sake of writing programs at the sacrifice of performance, if traditional stub generation based approaches [BN84, Gib87, CP91] are used.
Reference: [KP96a] <author> T.-H. Kim and J. M. Purtilo. </author> <title> Load balancing for parallel loops in workstation clusters. </title> <booktitle> In Proceedings of the 25th International Conference on Parallel Processing, </booktitle> <pages> pages III:182-190, </pages> <month> August </month> <year> 1996. </year>
Reference-contexts: The automatic parallelization technique compensates programmers for the per formance problem. * A new decentralized load balancing scheme has been developed for workstation cluster environments <ref> [KP96a] </ref>. This development intensifies the importance of MIL style programming toward performance tuning because the scheme showed that the load balancing power is dependent on the topology of load migration network. Configurable load migration networks have had no application areas before the emergence of workstation clusters.
Reference: [KP96b] <author> T.-H. Kim and J. M. Purtilo. </author> <title> A source-level transformation framework for RPC-based distributed programs. </title> <booktitle> In Proceedings of the 5th IEEE International Symposium on High Performance Distributed Computing, </booktitle> <pages> pages 78-87, </pages> <month> August </month> <year> 1996. </year> <month> 78 </month>
Reference-contexts: We studied how such an approach help to do a seamless process in developing high performance distributed programs [KP95]. * We have developed an automatic parallelization of RPC-based distributed programs <ref> [KP96b] </ref>. RPC is a convenient paradigm for the sake of writing programs at the sacrifice of performance, if traditional stub generation based approaches [BN84, Gib87, CP91] are used.
Reference: [Kra90] <author> J. Kramer. </author> <title> Configuration programming | a framework for the development of distributable systems. </title> <booktitle> In Proceedings of the IEEE International Conference on Systems and Software Engineering (CompEuro 90), </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: a set of fine grained message passing primitives (optimization phase), and (2) the stubs are generated at server sites to implement the particular techniques (e.g. load balancing and/or scheduling) specified at the configuration level (adaptation phase). 1.1 Motivation Our approach is motivated by the success of on-line system configuration technologies <ref> [Kra90] </ref> based on the perception that module interconnection activity is to be an essentially distinct and different intellectual activity from that of implementing individual modules; that is, "programming-in-the-large" is distinct from "programming-in-the-small" [DK76]. Moreover, workstation clusters are becoming viable environments for running parallel applications. <p> reassures the important role of configuration-level programming towards higher performance because a proper load balancing topology can be easily constructed under such a programming environment. 1.3 Summary of Contributions The major contributions of this dissertation are itemized as follows: * The configuration issues are popular in constructing large distributed softwares <ref> [DK76, CP91, Kra90] </ref>. We have extended the idea into the performance issue from the original interconnection related ones. One may perceive that any structural change in a distributed program configuration could result in a different performance. This dissertation elaborated on this perception in more details toward performance improvement.
Reference: [KS94] <author> Philip Krueger and Niranjan G. Shivaratri. </author> <title> Adaptive location policies for global scheduling. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> Vol. 20(6) </volume> <pages> 432-444, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: in a distributed memory multicomputers in advance, the problem boils down to how to optimally allocate each task node in MDGs to a processor and how to schedule those limited number of available processors. 2.5 Load Balancing Schemes Load balancing concept has been widely studied from an operating system's concerns <ref> [Son94, ELZ86b, KS94, CS93, ELZ86a] </ref>. When there are multiple applications that are working simultaneously on distributed environments, some processors can be too heavily loaded while others are not. <p> Many dynamic load balancing algorithms <ref> [CS93, ELZ86b, KS94, LK87, Son94] </ref> have been devised for good load balance with less migration overhead; they are characterized by the following parameters which distinguish them. <p> the throughput of W i , which is defined by the number of unit tasks per unit time. * fl ij : the amount of load to migrate from i to j. 6.2 Load Balancing Method Two important components of dynamic load balancing schemes are transfer policy and location policy <ref> [ELZ86b, KS94] </ref>. The transfer policy determines whether a task should be processed locally or remotely by transferring it at a particular load state. The location policy determines which process initiates the migration and its source or destination. These are for global load balancing from the OS's viewpoints. <p> These are for global load balancing from the OS's viewpoints. Multi-dimensional load vectors determine the load state of a processor. In our system, we aim to balance parallel loops in an application. A simple `demand' message is enough to initiate load migration rather than load state exchange <ref> [KS94] </ref> or random polling of candidate processors [ELZ86b] because the only load vector is the number of sub-tasks in a processor. <p> To our knowledge, migration topology for load balancing is considered for the first time. The topology has not been considered important heretofore because sometimes it is given in a hard-wired form [LK87] or it is meaningless where distributed load patterns cannot be assumed to be known in advance <ref> [ELZ86b, KS94] </ref>. We have shown analytically that the overhead of our method is lower than that of the self-scheduling scheme when an "predictability" condition is given. We have also provided some experimental data for cases when the loop pattern is unpredictably irregular.
Reference: [KW85] <author> C. P. Kruskal and A. Weiss. </author> <title> Allocating independent subtasks on parallel processors. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> Vol. 11(10) </volume> <pages> 1001-1016, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: A loop is called a parallel loop (DOALL-loop) if there are no data dependences among all iterations. The question of how to allocate an iteration to a particular processor for minimizing the total execution time is known as a loop scheduling problem <ref> [TY86, KW85, PK87, TN91, CLZ95] </ref>. Networks of workstations are somewhat new environments for loop scheduling problems: the communication delay is longer and the granularity of a sub-task is coarser. <p> The unpredictable patterns can even be detrimental to those improvements <ref> [KW85, PK87, TN91] </ref>, although the pure self-scheduling scheme is orthogonal to the loop patterns. <p> Load balancing for multiple sub-tasks generated from a single application has been known as "parallel loop scheduling problems" <ref> [TY86, KW85, PK87, TN91, CLZ95] </ref>, which have been researched as a way of loop parallelization in a shared-memory programming model. If there are I uniform-sized iterations, and P identical processors, load can be balanced simply by assigning I=P iterations to each processor. <p> It assigns a new iteration to a processor only when the processor becomes available. However, this method requires tremendous synchronization overhead; to be practical, hardware support to fast barrier synchronization primitives is desirable. Uniform-sized chunking (CSS) reduces such synchronization overhead by sending K iterations instead of one <ref> [KW85] </ref>. In this method, the overhead is amortized to 1=K, but the possibility of load imbalance increases when K is increased. <p> It is also known as administrator and workers structure [Gen81]. This category encompasses a well-known parallel loop problem in parallel processing <ref> [TY86, KW85, PK87, TN91, CLZ95] </ref>. Conventional RPC represents client/server computations. Some modifications are required to utilize the replicated servers. PARPC [MBR87] and MultiRPC [SS86] changed the basic RPC mechanism to be able to deal with multiple servers.
Reference: [Lam78] <author> L. Lamport. </author> <title> Time, clocks, and the ordering of events in a distributed system. </title> <journal> Communication of the ACM, </journal> <volume> Vol. 21(7) </volume> <pages> 558-565, </pages> <month> July </month> <year> 1978. </year>
Reference-contexts: The relation P is an irreflexive partial ordering 2 defined as follows: (1) if S 1 is executed before S 2 , then S 1 P S 2 and (2) if S 1 P S 2 and 2 It resembles the happened-before relation on a set of distributed events <ref> [Lam78] </ref>. While the associated events in that relation are distributed, the relation P is an ordering between statements in a single program. Since a statement cannot be executed before itself, it is irreflexive. 42 S 2 P S 3 , then S 1 P S 3 .
Reference: [LHG86] <author> B. Liskov, M. Herlihy, and L. Gilbert. </author> <title> Limitations of synchronous communication with static process structure in languages for distributed computing. </title> <booktitle> In Proceedings of the 13th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 150-159, </pages> <year> 1986. </year>
Reference-contexts: In other words, the nature of an application determines such an `efficient' form of interactions. Those interactions are summarized by four categories as follows, which have been presented in the previous researches on various parallel execution paradigms <ref> [YBS86, BDZ88, Geh84, Geh86, And91a, CG89, LHG86, SA89, Gen81, Geh90] </ref>. In the rest of this dissertation, we study how those fundamental parallel program structures can be properly handled during the process of automatic adaptation with optimization. 3.1 Client/Server structure a server. This is how conventional synchronous RPC [Gib87, BN84] works.
Reference: [LK87] <author> Frank C. H. Lin and Robert M. Keller. </author> <title> The gradient model load balancing method. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> Vol. 13(1) </volume> <pages> 32-38, </pages> <month> January </month> <year> 1987. </year>
Reference-contexts: Many dynamic load balancing algorithms <ref> [CS93, ELZ86b, KS94, LK87, Son94] </ref> have been devised for good load balance with less migration overhead; they are characterized by the following parameters which distinguish them. <p> To our knowledge, migration topology for load balancing is considered for the first time. The topology has not been considered important heretofore because sometimes it is given in a hard-wired form <ref> [LK87] </ref> or it is meaningless where distributed load patterns cannot be assumed to be known in advance [ELZ86b, KS94]. We have shown analytically that the overhead of our method is lower than that of the self-scheduling scheme when an "predictability" condition is given.
Reference: [LS88] <author> B. Liskov and L. Shrira. </author> <title> Promises: Linguistic support for efficient asynchronous procedure calls in distributed systems. </title> <booktitle> In Proceedings of the ACM SIGPLAN '88 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 260-267, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Writing distributed and/or parallel programs is difficult for programmers, and even more difficult when high performance is required. Many mechanisms to achieve better performance in distributed programming have been proposed <ref> [BST89, Geh86, Geh90, Gen81, LS88] </ref>; however, in practice these mechanisms are hard to utilize, and do not take into account the burden placed on programmers who already encounter difficulty in writing functionally correct programs. <p> Since the synchronous property of RPC results in hindrance to parallel executions that can increase the total elapse time, various asynchronous RPC mechanisms have been devised to implement RPC in a non-blocking way <ref> [LS88, ATK91, WFN90, GG88] </ref>. Call streaming [LS88] is a pioneering work in an asynchronous RPC implementation. A new data type called a promise which is created at the time of a call so that the caller can continue its execution was designed to support asynchronous calls known as call streaming. <p> Since the synchronous property of RPC results in hindrance to parallel executions that can increase the total elapse time, various asynchronous RPC mechanisms have been devised to implement RPC in a non-blocking way [LS88, ATK91, WFN90, GG88]. Call streaming <ref> [LS88] </ref> is a pioneering work in an asynchronous RPC implementation. A new data type called a promise which is created at the time of a call so that the caller can continue its execution was designed to support asynchronous calls known as call streaming. <p> An asynchronous call does not block the client, and replies can be received as they are needed. To date, the decision on calling style is not the programmer's (for example, calls may be synchronous only [BN84] or they may be asynchronous only <ref> [ATK91, LS88, WFN90] </ref>), or the decision has to be made at module programming level by use of different library routines [Cor91]. If we let this decision be separate from RPC statement, the modules will remain reusable for different calling styles. <p> Executing an RPC is involved in rather longer delay. Aggregating remote messages can drastically reduce an inter-networking overhead by sharing the overhead by multiple messages. If a loop contains RPCs, the chance to reduce the overhead through aggregation is higher <ref> [LS88] </ref>, thus careful loop transformation provides good opportunity for aggregation. 51 C 1 () C n1 (Type a) C n (Type a) S (Type a) f return ( ); g (a) Call path optimization C () y=x+2; g f */ return (rv); g f rv=S n ( ); return (rv); g <p> It is called call-streaming, which was proposed to effectively support asynchronous calls with an aid of a special data type called "promises" <ref> [LS88] </ref>. Our method presents a static solution for call-streaming without relying on special programming language constructs. Moreover, an output of one remote procedure can be directly connected to an input of another one as presented in the previous section.
Reference: [MBR87] <author> Bruce Martin, Charles Bergan, and Brian Russ. PARPC: </author> <title> A system for parallel remote procedure calls. </title> <booktitle> In Proceedings of the International Conferences on Parallel Processing, </booktitle> <pages> pages 449-452, </pages> <year> 1987. </year>
Reference-contexts: Anderson et al. [ABLL92] reported that user-level light-weight process control is more efficient than kernel-level control [ABLL92]. Special mechanisms need be provided to make RPC possible if servers are replicated, which is an another form of variation. Replicated Distributed System [Coo85], PARPC <ref> [MBR87] </ref>, Marionette [SA89] and MultiRPC [SS86] present mechanisms to call multiple instances of same remote operation in parallel on multiple servers. The caller then blocks until one or all of the requests have been completed. <p> It is also known as administrator and workers structure [Gen81]. This category encompasses a well-known parallel loop problem in parallel processing [TY86, KW85, PK87, TN91, CLZ95]. Conventional RPC represents client/server computations. Some modifications are required to utilize the replicated servers. PARPC <ref> [MBR87] </ref> and MultiRPC [SS86] changed the basic RPC mechanism to be able to deal with multiple servers. They distinguish a remote procedure call to a replicated server from a normal RPC, by providing special procedures that programmers need to invoke to process multiple tasks at replicated server sites. <p> In Figure 4.1 (b), no slaves should be idle while others are busy. So far, RPC in itself does not make any association with load balancing. Previous RPC systems for multiple servers like PARPC <ref> [MBR87] </ref> and MultiRPC [SS86] do not consider load balancing. 2. Scheduling: In our example, the length of each DNA sequence varies, so does comparison time.
Reference: [New93] <author> Peter W. </author> <title> Newton. A Graphical Retargetable Parallel Programming Environment and Its Efficient Implementation. </title> <type> PhD thesis, </type> <institution> The University of Texas at Austin, </institution> <month> dec </month> <year> 1993. </year>
Reference-contexts: If hundreds of slave processes are involved, the load balancing expressions in the configuration program may exceed hundreds of lines as well. A visual approach is the alternative such as Newton's graphical environment for parallel programming <ref> [New93] </ref>. A graph editing tool that is capable of processing all necessary attributes in configuration-level programming can help to deal with a large program of many components. The tool may produce the textual equivalent MIL program as an output.
Reference: [Pat85] <author> D. Patterson. </author> <title> Reduced Instruction Set Computers. </title> <journal> Communications of ACM, </journal> <pages> pages 8-21, </pages> <month> Jan </month> <year> 1985. </year>
Reference-contexts: This is because a memory access in RISC programs appears in a form of message passing in RPC programs. More important aspects of RISC compilers are on instruction scheduling. Basically RISC programs contain many pipeline bubbles (NOP: No OPeration) to avoid pipeline interlocks <ref> [Pat85] </ref>. Instruction scheduling is how to fill up those bubbles with useful operations safely without changing program semantics. Moreover, message passing primitives also have varieties in its functional complexity.
Reference: [PK87] <author> C. D. Polychronopoulos and D. J. Kuck. </author> <title> Guided self-scheduling: A practical scheduling scheme for parallel supercomputers. </title> <journal> IEEE Transactions on Computer, </journal> <volume> Vol. C-36(12):1425-1439, </volume> <month> December </month> <year> 1987. </year>
Reference-contexts: A loop is called a parallel loop (DOALL-loop) if there are no data dependences among all iterations. The question of how to allocate an iteration to a particular processor for minimizing the total execution time is known as a loop scheduling problem <ref> [TY86, KW85, PK87, TN91, CLZ95] </ref>. Networks of workstations are somewhat new environments for loop scheduling problems: the communication delay is longer and the granularity of a sub-task is coarser. <p> The unpredictable patterns can even be detrimental to those improvements <ref> [KW85, PK87, TN91] </ref>, although the pure self-scheduling scheme is orthogonal to the loop patterns. <p> Load balancing for multiple sub-tasks generated from a single application has been known as "parallel loop scheduling problems" <ref> [TY86, KW85, PK87, TN91, CLZ95] </ref>, which have been researched as a way of loop parallelization in a shared-memory programming model. If there are I uniform-sized iterations, and P identical processors, load can be balanced simply by assigning I=P iterations to each processor. <p> self-scheduling (GSS), the fixed chunk function (K) is replaced with a non-linearly decreasing chunk function in order to reduce the overhead at the beginning of a loop by allocating larger chunks, and also to reduce the chance of load imbalancing at the end of the loop by allocating smaller chunks <ref> [PK87] </ref>. Trapezoid self-scheduling (TSS) uses a linearly decreasing chunk function, which helps to reduce scheduling overhead while still maintaining a reasonable balance [TN91]. Grimshaw et al. [GWWECL94] presented a static load balancing method for parallel executions in heterogeneous distributed computing systems. <p> It is also known as administrator and workers structure [Gen81]. This category encompasses a well-known parallel loop problem in parallel processing <ref> [TY86, KW85, PK87, TN91, CLZ95] </ref>. Conventional RPC represents client/server computations. Some modifications are required to utilize the replicated servers. PARPC [MBR87] and MultiRPC [SS86] changed the basic RPC mechanism to be able to deal with multiple servers. <p> Finally, as in Figure 6.1 (d), the workload may be quite irregular. Many non-scientific applications carry parallel loops of this type. The first three cases have been specially considered by conventional loop scheduling methods <ref> [PK87, TN91, CLZ95] </ref> in order to improve on the basic self-scheduling method. Particularly for irregular loops, we can distinguish between the two cases: predictable vs. unpredictable.
Reference: [Pol88] <author> C. D. Polychronopoulos. </author> <title> Parallel Programming and Compilers. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1988. </year> <month> 79 </month>
Reference-contexts: Figure 5.4 summarizes the algorithm for the global optimization we have discussed in this section. 5.2.5 Loop Transformation Many research works have been focused on loop transformations in various parallel compilers, for loops are hot spots in a program <ref> [Pol88] </ref>. We are interested in transforming a loop as well, especially when RPC statements are surrounded by a loop. Executing an RPC is involved in rather longer delay. Aggregating remote messages can drastically reduce an inter-networking overhead by sharing the overhead by multiple messages.
Reference: [Pur94] <author> J. M. Purtilo. </author> <title> The polylith software bus. </title> <journal> ACM Transactions on Programming Languages and systems, </journal> <volume> Vol. 16(1) </volume> <pages> 151-174, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: Many existing performance oriented mechanisms can be achieved by using ordinary modules with proper configuration programs and source-to-source translation techniques. This frees programmers from making extensive amounts of manual adaptations for various performance configurations. This builds upon the MIL (Module Interconnection Language) approach <ref> [CP91, Pur94] </ref> for distributed programming, where the original MIL specification is intended only for structural presentation of interfaces between interacting processes. We append performance related specifications onto each interface specification in a MIL. <p> Message passing primitives <ref> [Sun90, For93, BL93, Pur94, Fel79, Coo80] </ref> are expressive enough to program for efficiency; however, they are too low-level to write large distributed programs. Programmers are fully responsible for matching send/receive pairs, allocating buffers, and marshaling/unmarshaling data correctly. <p> It can be analogous to the trade-off between assembler vs. compiler. Higher level message passing systems include PVM [Sun90], MPI [For93], p4 [BL93] and Polylith <ref> [Pur94] </ref>. PVM PVM [Sun90] (Parallel Virtual Machine) is a user-level code and uses rsh commands to initiate daemons on remote machines. The user writes applications as a collection of cooperative tasks. Tasks access PVM resources through a library of standard interface routines. <p> By clearly defining the base set of standardized communication interfaces, many parallel machine vendors can optimally implement those primitive functions for distributed computing. It allows other high-level oriented software packages to use those underlying primitives in order to be portable on various systems as well as efficient. Polylith Polylith <ref> [Pur94] </ref> system integrated a collection of machine and operating system dependent ingredients for communication into a single entity called a bus. In hardware platforms, a bus system simplifies to establish a communication network among many different hardware components like main memories, disks, or I/O devices. <p> See <ref> [Pur94] </ref>. The final step is to execute the application, identify performance bottlenecks using a performance measurement tool, and repeat the process from the second step until the resulting performance is satisfactory. <p> Therefore, when there are multiple data sets that are ready to be serviced, a selection is done non-deterministically. This can be implemented by special message passing primitives that allows a non-blocking receipt. For example, in Polylith system <ref> [Pur94] </ref>, mh readselect () allows us to read the next message to arrive on any interface (it will be blocked if no message arrives), then mh readback () completes the receipt.
Reference: [PW86] <author> D. A. Padua and M. Wolfe. </author> <title> Advanced compiler optimizations for supercomputers. </title> <journal> Communication of the ACM, </journal> <volume> Vol. 29(12) </volume> <pages> 1184-1201, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: */ return (rv); g f rv=S n ( ); return (rv); g f return (rv); g (b) Return path optimization (c) Mixed path optimization Loop distribution breaks a single loop into multiple loops with the same iteration space but each enclosing a subset of the statements in the original loop <ref> [PW86] </ref>. It is used to improve instruction and data locality by shortening loop bodies and to allow parallelism that is hindered by loop-carried dependences in the original loop. The latter effect is important in applying the technique to a loop that contains RPC statements.
Reference: [RB93] <author> S. Ramaswamy and P. Banerjee. </author> <title> Processor allocation and scheduling of macro dataflow graphs on distributed memory multicomputers by the PARADIGM compiler. </title> <booktitle> In Proceedings of the 22nd International Conference on Parallel Processing, </booktitle> <year> 1993. </year>
Reference-contexts: The PARADIGM compiler project <ref> [SLR + 95, RB93] </ref> also deals with extracting function-level parallelism using MDGs (Macro Dataflow Graphs) representation, which is similar to HTG, from the perspective of processor allocation and scheduling problems.
Reference: [SA89] <author> M. Sullivan and D. Anderson. Marionette: </author> <title> a system for parallel distributed programming using a master/slave model. </title> <booktitle> In Proceedings of the 9th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 181-188, </pages> <year> 1989. </year>
Reference-contexts: Anderson et al. [ABLL92] reported that user-level light-weight process control is more efficient than kernel-level control [ABLL92]. Special mechanisms need be provided to make RPC possible if servers are replicated, which is an another form of variation. Replicated Distributed System [Coo85], PARPC [MBR87], Marionette <ref> [SA89] </ref> and MultiRPC [SS86] present mechanisms to call multiple instances of same remote operation in parallel on multiple servers. The caller then blocks until one or all of the requests have been completed. <p> In other words, the nature of an application determines such an `efficient' form of interactions. Those interactions are summarized by four categories as follows, which have been presented in the previous researches on various parallel execution paradigms <ref> [YBS86, BDZ88, Geh84, Geh86, And91a, CG89, LHG86, SA89, Gen81, Geh90] </ref>. In the rest of this dissertation, we study how those fundamental parallel program structures can be properly handled during the process of automatic adaptation with optimization. 3.1 Client/Server structure a server. This is how conventional synchronous RPC [Gib87, BN84] works.
Reference: [SB90] <author> M. Schroeber and M. Burrows. </author> <title> Performance of Firefly RPC. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 8(1) </volume> <pages> 1-17, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: Unfortunately, if users should choose those `right' routines for a proper communication style, the RPC paradigm loses its superiority over message passing style programming with explicit send and receive primitives. Another approach to cure the synchronous nature of RPC is using light-weight threads for RPC calls <ref> [BELL90, BELL89, ABLL92, SB90] </ref>. When an RPC is invoked, a new thread is created to take a waiting burden for the return value, and the calling process continues its execution. Anderson et al. [ABLL92] reported that user-level light-weight process control is more efficient than kernel-level control [ABLL92].
Reference: [SLR + 95] <author> E. Su, A. Lain, S. Ramaswamy, D. Palermo, E. Hodges IV, and R. Baner-jee. </author> <title> Advanced compilation techniques in the PARADIGM compiler for distributed-memory multicomputers. </title> <booktitle> In Proceedings of the International Conference on Supercomputing '95, </booktitle> <year> 1995. </year>
Reference-contexts: The PARADIGM compiler project <ref> [SLR + 95, RB93] </ref> also deals with extracting function-level parallelism using MDGs (Macro Dataflow Graphs) representation, which is similar to HTG, from the perspective of processor allocation and scheduling problems.
Reference: [Son94] <author> Jianjian Song. </author> <title> A partially asynchronous and iterative algorithm for distributed load balancing. </title> <journal> Parallel Computing, </journal> <volume> Vol. 20 </volume> <pages> 853-868, </pages> <year> 1994. </year>
Reference-contexts: in a distributed memory multicomputers in advance, the problem boils down to how to optimally allocate each task node in MDGs to a processor and how to schedule those limited number of available processors. 2.5 Load Balancing Schemes Load balancing concept has been widely studied from an operating system's concerns <ref> [Son94, ELZ86b, KS94, CS93, ELZ86a] </ref>. When there are multiple applications that are working simultaneously on distributed environments, some processors can be too heavily loaded while others are not. <p> Many dynamic load balancing algorithms <ref> [CS93, ELZ86b, KS94, LK87, Son94] </ref> have been devised for good load balance with less migration overhead; they are characterized by the following parameters which distinguish them.
Reference: [SS86] <author> M. Satyanarayanan and E. H. Siegel. MultiRPC: </author> <title> A parallel remote procedure call mechanism. </title> <type> Technical Report CMU-CS-86-139, </type> <institution> Carnegie-Mellon University, </institution> <year> 1986. </year>
Reference-contexts: Anderson et al. [ABLL92] reported that user-level light-weight process control is more efficient than kernel-level control [ABLL92]. Special mechanisms need be provided to make RPC possible if servers are replicated, which is an another form of variation. Replicated Distributed System [Coo85], PARPC [MBR87], Marionette [SA89] and MultiRPC <ref> [SS86] </ref> present mechanisms to call multiple instances of same remote operation in parallel on multiple servers. The caller then blocks until one or all of the requests have been completed. MultiRPC is primary intended for fault tolerance like in invoking replicated file servers, rather than for high performance through parallelism. <p> It is also known as administrator and workers structure [Gen81]. This category encompasses a well-known parallel loop problem in parallel processing [TY86, KW85, PK87, TN91, CLZ95]. Conventional RPC represents client/server computations. Some modifications are required to utilize the replicated servers. PARPC [MBR87] and MultiRPC <ref> [SS86] </ref> changed the basic RPC mechanism to be able to deal with multiple servers. They distinguish a remote procedure call to a replicated server from a normal RPC, by providing special procedures that programmers need to invoke to process multiple tasks at replicated server sites. <p> In Figure 4.1 (b), no slaves should be idle while others are busy. So far, RPC in itself does not make any association with load balancing. Previous RPC systems for multiple servers like PARPC [MBR87] and MultiRPC <ref> [SS86] </ref> do not consider load balancing. 2. Scheduling: In our example, the length of each DNA sequence varies, so does comparison time.
Reference: [SS94] <author> B. K. Schmidt and V. S. Sunderam. </author> <title> Empirical analysis of overheads in cluster environments. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> Vol. 6(1) </volume> <pages> 1-32, </pages> <month> February </month> <year> 1994. </year>
Reference-contexts: The empirical values for ff and fi under the PVM system [Sun90] at LAN-based clustered workstations are 4:527 msec, 0:0024 msec and 1:661 msec, 0:00157 msec for datagram and stream transmission cases, respectively, which imply ff fi <ref> [SS94] </ref>. In Theorems 6.1 and 6.2, we compute the total number of migrated tasks (fi's multiplier) and the frequencies of migrations (ff's multiplier) in a cluster.
Reference: [Sun90] <author> V. S. Sunderam. </author> <title> PVM: A framework for parallel distributed computing. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> Vol. 2(4) </volume> <pages> 315-339, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: Message passing primitives <ref> [Sun90, For93, BL93, Pur94, Fel79, Coo80] </ref> are expressive enough to program for efficiency; however, they are too low-level to write large distributed programs. Programmers are fully responsible for matching send/receive pairs, allocating buffers, and marshaling/unmarshaling data correctly. <p> Unix sockets are the simplest mechanisms available and they provide basic mechanism underlying systems like PVM <ref> [Sun90] </ref>. They are the lowest-level primitives provided directly by operating systems so aggressive programmers can achieve significant improvements in the efficiency of the message passing by minimizing software overhead at the expense of additional effort on the part of the programmer. <p> It can be analogous to the trade-off between assembler vs. compiler. Higher level message passing systems include PVM <ref> [Sun90] </ref>, MPI [For93], p4 [BL93] and Polylith [Pur94]. PVM PVM [Sun90] (Parallel Virtual Machine) is a user-level code and uses rsh commands to initiate daemons on remote machines. The user writes applications as a collection of cooperative tasks. Tasks access PVM resources through a library of standard interface routines. <p> It can be analogous to the trade-off between assembler vs. compiler. Higher level message passing systems include PVM <ref> [Sun90] </ref>, MPI [For93], p4 [BL93] and Polylith [Pur94]. PVM PVM [Sun90] (Parallel Virtual Machine) is a user-level code and uses rsh commands to initiate daemons on remote machines. The user writes applications as a collection of cooperative tasks. Tasks access PVM resources through a library of standard interface routines. <p> Andrews [And91a, And91b] presented typical distributed and parallel applications that can be optimally expressed as one or combination of the four process types. A versatile programming environment for all of the forms of parallelism or process can be provided by message passing paradigms like PVM <ref> [Sun90] </ref> and MPI [For93]. Not only because of the resulting complexity in writing distributed programs with such a low-level abstraction, but also because of its difficulty to reconfigure, a higher level of abstraction for module interaction is called for. <p> Both of them rely on communication primitives provided by underlying MP systems <ref> [For93, Sun90] </ref> or operating systems. But the transformation based method allows us to apply various program analysis techniques for program transformation towards high performance. <p> For example, in Polylith system [Pur94], mh readselect () allows us to read the next message to arrive on any interface (it will be blocked if no message arrives), then mh readback () completes the receipt. In PVM <ref> [Sun90] </ref>, pvm nrecv ( int msgtag ) checks to see whether a message with label msgtag has arrived. If not arrived, it immediately returns so that other message can be checked out. Non-blocking receive primitives are commonly supported by MP systems. <p> T comm = ff + fim, where ff is startup time and fi is transfer time per byte [BR89]. The empirical values for ff and fi under the PVM system <ref> [Sun90] </ref> at LAN-based clustered workstations are 4:527 msec, 0:0024 msec and 1:661 msec, 0:00157 msec for datagram and stream transmission cases, respectively, which imply ff fi [SS94].
Reference: [TN91] <author> T. H. Tzen and L. M. Ni. </author> <title> Dynamic loop scheduling for shared-memory multiprocessors. </title> <booktitle> In Proceedings of '91 International Conference on Parallel Processing, </booktitle> <pages> pages II:247-250, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: A loop is called a parallel loop (DOALL-loop) if there are no data dependences among all iterations. The question of how to allocate an iteration to a particular processor for minimizing the total execution time is known as a loop scheduling problem <ref> [TY86, KW85, PK87, TN91, CLZ95] </ref>. Networks of workstations are somewhat new environments for loop scheduling problems: the communication delay is longer and the granularity of a sub-task is coarser. <p> The unpredictable patterns can even be detrimental to those improvements <ref> [KW85, PK87, TN91] </ref>, although the pure self-scheduling scheme is orthogonal to the loop patterns. <p> Load balancing for multiple sub-tasks generated from a single application has been known as "parallel loop scheduling problems" <ref> [TY86, KW85, PK87, TN91, CLZ95] </ref>, which have been researched as a way of loop parallelization in a shared-memory programming model. If there are I uniform-sized iterations, and P identical processors, load can be balanced simply by assigning I=P iterations to each processor. <p> Trapezoid self-scheduling (TSS) uses a linearly decreasing chunk function, which helps to reduce scheduling overhead while still maintaining a reasonable balance <ref> [TN91] </ref>. Grimshaw et al. [GWWECL94] presented a static load balancing method for parallel executions in heterogeneous distributed computing systems. The basic idea is to allocate sub-tasks proportionally to the known throughputs of participating workstations. <p> It is also known as administrator and workers structure [Gen81]. This category encompasses a well-known parallel loop problem in parallel processing <ref> [TY86, KW85, PK87, TN91, CLZ95] </ref>. Conventional RPC represents client/server computations. Some modifications are required to utilize the replicated servers. PARPC [MBR87] and MultiRPC [SS86] changed the basic RPC mechanism to be able to deal with multiple servers. <p> Finally, as in Figure 6.1 (d), the workload may be quite irregular. Many non-scientific applications carry parallel loops of this type. The first three cases have been specially considered by conventional loop scheduling methods <ref> [PK87, TN91, CLZ95] </ref> in order to improve on the basic self-scheduling method. Particularly for irregular loops, we can distinguish between the two cases: predictable vs. unpredictable.
Reference: [TY86] <author> P. Tang and P. C. Yew. </author> <title> Processor self-scheduling for multiple nested parallel loops. </title> <booktitle> In Proceedings of '86 International Conference on Parallel Processing, </booktitle> <pages> pages 528-535, </pages> <month> August </month> <year> 1986. </year> <month> 80 </month>
Reference-contexts: A loop is called a parallel loop (DOALL-loop) if there are no data dependences among all iterations. The question of how to allocate an iteration to a particular processor for minimizing the total execution time is known as a loop scheduling problem <ref> [TY86, KW85, PK87, TN91, CLZ95] </ref>. Networks of workstations are somewhat new environments for loop scheduling problems: the communication delay is longer and the granularity of a sub-task is coarser. <p> Load balancing for multiple sub-tasks generated from a single application has been known as "parallel loop scheduling problems" <ref> [TY86, KW85, PK87, TN91, CLZ95] </ref>, which have been researched as a way of loop parallelization in a shared-memory programming model. If there are I uniform-sized iterations, and P identical processors, load can be balanced simply by assigning I=P iterations to each processor. <p> If there are I uniform-sized iterations, and P identical processors, load can be balanced simply by assigning I=P iterations to each processor. Since both factors may not be known in advance or may vary substantially, such a static method is often difficult or inefficient. Self-scheduling (SS) <ref> [TY86] </ref> is the simplest dynamic solution. It assigns a new iteration to a processor only when the processor becomes available. However, this method requires tremendous synchronization overhead; to be practical, hardware support to fast barrier synchronization primitives is desirable. <p> It is also known as administrator and workers structure [Gen81]. This category encompasses a well-known parallel loop problem in parallel processing <ref> [TY86, KW85, PK87, TN91, CLZ95] </ref>. Conventional RPC represents client/server computations. Some modifications are required to utilize the replicated servers. PARPC [MBR87] and MultiRPC [SS86] changed the basic RPC mechanism to be able to deal with multiple servers.
Reference: [vHK94] <author> R. von Hanxleden and K. Kennedy. </author> <title> Give-N-Take | A balanced code place-ment framework. </title> <booktitle> In Proceedings of the ACM SIGPLAN '94 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 107-120, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: The communication patterns among processes are expected to be highly structured and often predictable so that the entire data can be decomposed before starting computations for the good performance. Initial data layout and communication optimization <ref> [vHK94, HQ91, AL93] </ref> are hot issues in this kind of computations and they are mutually related. Many practical scientific problems except fairly irregular ones are known to be efficiently computed under this structure [FJL + 88].
Reference: [WFN90] <author> E. F. Walker, R. Floyd, and P. Neves. </author> <title> Asynchronous remote operation in distributed systems. </title> <booktitle> In Proceedings of the 11th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 253-259, </pages> <year> 1990. </year>
Reference-contexts: Since the synchronous property of RPC results in hindrance to parallel executions that can increase the total elapse time, various asynchronous RPC mechanisms have been devised to implement RPC in a non-blocking way <ref> [LS88, ATK91, WFN90, GG88] </ref>. Call streaming [LS88] is a pioneering work in an asynchronous RPC implementation. A new data type called a promise which is created at the time of a call so that the caller can continue its execution was designed to support asynchronous calls known as call streaming. <p> An asynchronous call does not block the client, and replies can be received as they are needed. To date, the decision on calling style is not the programmer's (for example, calls may be synchronous only [BN84] or they may be asynchronous only <ref> [ATK91, LS88, WFN90] </ref>), or the decision has to be made at module programming level by use of different library routines [Cor91]. If we let this decision be separate from RPC statement, the modules will remain reusable for different calling styles.
Reference: [Wil94] <author> Gregory V. Wilson. </author> <title> Assessing the usability of parallel programming systems: The cowichan problems. </title> <booktitle> In Proceedings of the IFIP WG 10.3 Working Conference on Programming Environments for Massively Parallel Distributed Systems, </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: As our suggested topology is of binary tree form, more interesting topologies can be studied analytically or experimentally in the future. Assessing the usability of a parallel programming system is one of important research areas from the viewpoint of software engineering. Wilson <ref> [Wil94] </ref> proposes 9 applications that can assess how well a parallel programming system can support large scale software engineering and how easily systems can be learned or how quickly code can be developed.
Reference: [YBS86] <author> Akinori Yonezawa, Jean-Pierre Briot, and Etsuya Shibayama. </author> <booktitle> Object-oriented concurrent programming in ABCL/1. In Proceedings of OOPSLA '86, </booktitle> <pages> pages 258-268, </pages> <year> 1986. </year> <month> 81 </month>
Reference-contexts: In other words, the nature of an application determines such an `efficient' form of interactions. Those interactions are summarized by four categories as follows, which have been presented in the previous researches on various parallel execution paradigms <ref> [YBS86, BDZ88, Geh84, Geh86, And91a, CG89, LHG86, SA89, Gen81, Geh90] </ref>. In the rest of this dissertation, we study how those fundamental parallel program structures can be properly handled during the process of automatic adaptation with optimization. 3.1 Client/Server structure a server. This is how conventional synchronous RPC [Gib87, BN84] works.
References-found: 88

