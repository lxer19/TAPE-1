URL: http://www.cs.rice.edu:80/~mikewu/papers/isca98.ps.gz
Refering-URL: http://www.cs.rice.edu:80/~mikewu/papers.html
Root-URL: 
Email: mikewu@rice.edu  sarita@rice.edu  willy@rice.edu  
Title: The Cache Directory  
Author: Michael Wu Sarita Adve Willy Zwaenepoel 
Date: December 3, 1997  
Affiliation: Department of Electrical and Computer Engineering Rice University  Department of Electrical and Computer Engineering Rice University  Department of Computer Science Rice University  
Abstract: The cache directory is a unified control structure that maintains the contents and the state of a variety of on-chip and off-chip cache structures, such as L2 or L3 caches, victim caches, prefetch buffers, stream buffers, and bypass buffers. Address tags with these individual cache structures are no longer needed. The cache directory provides a physical-to-physical address mapping, from the physical addresses issued by the processor (or by other devices) to a cache address space in which the aforementioned cache structures reside. The level of indirection afforded by this mapping provides a flat memory hierarchy, and avoids the need for inclusion between the different levels. It furthermore allows the storage for cache structures built out of the same technology (SRAM or DRAM) to be unified into a single area. Both of these advantages lead to a substantial reduction in complexity. Finally, the cache directory allows for more flexible placement policies within a cache structure and among cache structures. The challenge in building a cache directory is to make its lookup fast relative to the access time for the cache structures it controls, without excessive space cost and with suitable cache coverage. We propose several designs for doing so, differing in the amount of memory they are intended to address in the cache structures. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal, R. Simoni, J. Hennessy, and M. Horowitz. </author> <title> An evaluation of directory schemes for cache coherence. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 280-289, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: is requested for the same physical page, the corresponding TLB entry is flushed and its directory caching ability disabled, such that future references always reference the second level. 5 Related Work The concept of directories is commonly used in the context of large multiprocessor systems to maintain a coherent state <ref> [1, 5, 11] </ref>. A single logical cached mem ory state is held in a single logical directory that may be physically distributed across processors.
Reference: [2] <author> J. Baer and W. Wang. </author> <title> On the inclusion property for multi-level cache hierarchies. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 73-80, </pages> <year> 1988. </year>
Reference-contexts: The inclusion property is important in cache hierarchies because memory coherence operations only need to check the last level in the cache hierarchy to determine if the target of the coherence action is present in any of the caches rather than always forcing a search of each level <ref> [2] </ref>. Since the directory contains information about all the cached data, it eliminates the need for inclusion of the actual data among the various storage components. By definition, the physical addresses of all the data that resides in the cache must be present in the directory. <p> The most significant difference between this configuration and ours is that we provide many more bits than a single bit set indicator and can use that to provide create interesting cache mappings. The inclusion property has been used for many years as a practical method of enforcing cache coherence <ref> [2] </ref> and virtually all modern caches use some sort of inclusion property for this purpose. [2] explores the difficulties and requirements to properly enforce the inclusion. Cache organizations have been extensively studied [7] in various configurations. <p> The inclusion property has been used for many years as a practical method of enforcing cache coherence <ref> [2] </ref> and virtually all modern caches use some sort of inclusion property for this purpose. [2] explores the difficulties and requirements to properly enforce the inclusion. Cache organizations have been extensively studied [7] in various configurations. This large body of work supports the use of large set associative caches for directory implementations.
Reference: [3] <author> D. Burger, J. Goodman, and A. Kagi. </author> <title> The declining effectiveness of dynamic caching for general-purpose microprocessors. </title> <type> Technical Report 1261, </type> <institution> University of Wisconsin-Madison, Computer Sciences Dept., </institution> <year> 1995. </year>
Reference-contexts: Our other efforts are directed towards better memory management policies that take advantage of the flexible mapping ability of the directory. Others have shown that the usage and placement of data in caches is far from optimal <ref> [8, 3] </ref> and there is, at the very least, some potential of doing better. We are developing new management and placement policies that can operate on a single memory or as a global policy across several memories with different characteristics.
Reference: [4] <author> Michel Cekleov and Michel Dubois. </author> <title> Virtual-Address Caches Part I: Problems and Solutions in Uniprocessors. </title> <journal> IEEE Micro, </journal> <volume> 17(5) </volume> <pages> 64-71, </pages> <month> Septem-ber/October </month> <year> 1997. </year>
Reference-contexts: The distinguishing feature of this type of table is that it holds state for more memory locations than are actually present in the cache. Finally, various issues relating to virtual cache design are discussed in <ref> [4] </ref>. The issue of virtual alias resolution is critical in this case.
Reference: [5] <author> D. Chaiken, J. Kubiatowicz, and A. Agarwal. </author> <title> LimitLESS directories: A scalable cache coherence scheme. </title> <booktitle> In Proceedings of the 4th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 224-234, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: is requested for the same physical page, the corresponding TLB entry is flushed and its directory caching ability disabled, such that future references always reference the second level. 5 Related Work The concept of directories is commonly used in the context of large multiprocessor systems to maintain a coherent state <ref> [1, 5, 11] </ref>. A single logical cached mem ory state is held in a single logical directory that may be physically distributed across processors.
Reference: [6] <author> J. Heinrich. </author> <title> MIPS R10000 Microprocessor User's Manual. </title> <institution> Mountain View, California, </institution> <note> draft edition, </note> <year> 1994. </year>
Reference-contexts: Once a coherence request arrives at a processing node, the lowest level of the cache is searched to actually locate the data. External set associative caches are frequently accessed in a manner similar to our directories <ref> [6] </ref>. To locate data in the external cache RAM, the cache controller must perform one or more bus accesses to retrieve the tag to determine if data is present or not.
Reference: [7] <author> John L. Hennessy and David A. Patterson. </author> <title> Computer Architecture A Quantitative Approach. </title> <publisher> Mor-gan Kaufmann Publishers, </publisher> <address> San Mateo, California, </address> <year> 1990. </year>
Reference-contexts: The inclusion property has been used for many years as a practical method of enforcing cache coherence [2] and virtually all modern caches use some sort of inclusion property for this purpose. [2] explores the difficulties and requirements to properly enforce the inclusion. Cache organizations have been extensively studied <ref> [7] </ref> in various configurations. This large body of work supports the use of large set associative caches for directory implementations. In [9], the authors use a structure called a Mem ory Address Table to store information about memory reuse.
Reference: [8] <author> A. Huang and J. P. Shen. </author> <title> A limit study of memory requirements using value reuse profiles. </title> <booktitle> In MI-CRO95, </booktitle> <pages> pages 71-81, </pages> <month> December </month> <year> 1995. </year>
Reference-contexts: Our other efforts are directed towards better memory management policies that take advantage of the flexible mapping ability of the directory. Others have shown that the usage and placement of data in caches is far from optimal <ref> [8, 3] </ref> and there is, at the very least, some potential of doing better. We are developing new management and placement policies that can operate on a single memory or as a global policy across several memories with different characteristics.
Reference: [9] <author> T. L. Johnson and W. W. Hwu. </author> <title> Run-time adaptive cache hierarchy management via reference analysis. </title> <booktitle> In Proceedings of the 24th Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: These structures include a simple stream buffer for data streams [10], a prefetch buffer for non-stream data [12], an L2 bypassing mechanism <ref> [9] </ref>, and some on-chip DRAM [13]. A stream buffer analyzes the data references coming from the processor and attempts to prefetch streams of data with predictable sequential accesss patterns. A prefetch buffer may be used to store non-stream data prefetched through software prefetch instructions. The bypassing mechanism described in [9] consists <p> mechanism <ref> [9] </ref>, and some on-chip DRAM [13]. A stream buffer analyzes the data references coming from the processor and attempts to prefetch streams of data with predictable sequential accesss patterns. A prefetch buffer may be used to store non-stream data prefetched through software prefetch instructions. The bypassing mechanism described in [9] consists of a Memory Address Table (MAT) that retains information about how memory locations are being reused in the cache and uses that information to try to keep accesses to infrequently used data from replacing more useful data in the cache. <p> Natural storage of auxiliary information for cache management. A directory is a natural place in which to store summary information about memory usage of the type normally gathered by auxiliary structures; e.g., successful prefetch indicators or the information stored in the memory address table (MAT) <ref> [9] </ref> as discussed earlier. Such information is normally lost when the data it applies to leaves the scope of the associated memory, but since a directory (ideally) has a much larger scope, it can retain information for much longer. <p> Cache organizations have been extensively studied [7] in various configurations. This large body of work supports the use of large set associative caches for directory implementations. In <ref> [9] </ref>, the authors use a structure called a Mem ory Address Table to store information about memory reuse. The directory structure we propose is designed with storage of similar information about memory reuse in mind.
Reference: [10] <author> N. P. Jouppi. </author> <title> Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffer. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 364-373, </pages> <year> 1990. </year>
Reference-contexts: The directory lookup replaces the functions of the (set associative) lookup and tag check in traditional caches. variety of cache structures that have been proposed to reduce effective memory latencies or off-chip bandwidth. These structures include a simple stream buffer for data streams <ref> [10] </ref>, a prefetch buffer for non-stream data [12], an L2 bypassing mechanism [9], and some on-chip DRAM [13]. A stream buffer analyzes the data references coming from the processor and attempts to prefetch streams of data with predictable sequential accesss patterns.
Reference: [11] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> The directory-based cache coherence protocol for the DASH multiprocessor. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: is requested for the same physical page, the corresponding TLB entry is flushed and its directory caching ability disabled, such that future references always reference the second level. 5 Related Work The concept of directories is commonly used in the context of large multiprocessor systems to maintain a coherent state <ref> [1, 5, 11] </ref>. A single logical cached mem ory state is held in a single logical directory that may be physically distributed across processors.
Reference: [12] <author> T.C. Mowry, M.S. Lam, and A. Gupta. </author> <title> Design and evaluation of a compiler algorithm for prefetching. </title> <booktitle> In Proceedings of the 5th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 62-75, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: These structures include a simple stream buffer for data streams [10], a prefetch buffer for non-stream data <ref> [12] </ref>, an L2 bypassing mechanism [9], and some on-chip DRAM [13]. A stream buffer analyzes the data references coming from the processor and attempts to prefetch streams of data with predictable sequential accesss patterns. A prefetch buffer may be used to store non-stream data prefetched through software prefetch instructions.
Reference: [13] <author> D. Patterson, T. Anderson, N. Cardwell, R. Fromm, K. Keeton, C. Kozyrakis, R. Thomas, and K. Yelick. </author> <title> A case for intelligent ram: </title> <journal> Iram. IEEE MICRO, </journal> <volume> 17(2) </volume> <pages> 34-44, </pages> <month> April </month> <year> 1997. </year>
Reference-contexts: These structures include a simple stream buffer for data streams [10], a prefetch buffer for non-stream data [12], an L2 bypassing mechanism [9], and some on-chip DRAM <ref> [13] </ref>. A stream buffer analyzes the data references coming from the processor and attempts to prefetch streams of data with predictable sequential accesss patterns. A prefetch buffer may be used to store non-stream data prefetched through software prefetch instructions.
References-found: 13

