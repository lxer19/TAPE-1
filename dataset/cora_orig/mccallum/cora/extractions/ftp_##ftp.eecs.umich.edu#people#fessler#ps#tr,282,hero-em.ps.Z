URL: ftp://ftp.eecs.umich.edu/people/fessler/ps/tr,282,hero-em.ps.Z
Refering-URL: http://www.eecs.umich.edu/systems/TechReportList.html
Root-URL: http://www.cs.umich.edu
Title: Asymptotic Convergence Properties of EM-Type Algorithms 1  
Author: Alfred O. Hero and Jeffrey A. Fessler flfl 
Address: Ann Arbor, MI 48109  
Affiliation: Dept. of Electrical Engineering and Computer Science and flfl Division of Nuclear Medicine The University of Michigan,  
Abstract: We analyze the asymptotic convergence properties of a general class of EM-type algorithms for estimating an unknown parameter via alternating estimation and maximization. As examples, this class includes ML-EM, penalized ML-EM, Green's OSL-EM, and many other approximate EM algorithms. A theorem is given which provides conditions for monotone convergence with respect to a given norm and specifies an asymptotic rate of convergence for an algorithm in this class. By investigating different parameterizations, the condition for monotone convergence can be used to establish norms under which the distance between successive iterates and the limit point of the EM-type algorithm approaches zero monotonically. We apply these results to a modified ML-EM algorithm with stochastic complete/incomplete data mapping and establish global monotone convergence for a linear Gaussian observation model. We then establish that in the final iterations the unpenalized and quadratically penalized ML-EM algorithms for PET image reconstruction converge monotonically relative to two different norms on the logarithm of the images. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> N. Antoniadis, </author> <title> Timing delay estimation for Poisson-Gaussian processes, </title> <type> PhD thesis, </type> <institution> The University of Michigan, </institution> <address> Ann Arbor, MI 48109-2122, </address> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: This general iterative algorithm specializes to popular EM-type algorithms including: penalized and penalized ML-EM algorithms [4, 11], generalized ML-EM with stochastic complete-incomplete data mapping [6], one-step-late (OSL) penalized ML-EM [9], majorization methods [?], and approximate ML-EM algorithms such as the linear and quadratic approximations introduced in <ref> [2, 1] </ref>. Let fl be a fixed point of the EM-type algorithm which occurs on the interior of fi and assume that Q is a smooth function of both arguments. <p> Define the pfip matrices obtained by averaging r 20 Q (u; u) and r 11 Q (u; u) over the line segments u 2 ~ fl and u 2 ~ fl : A 1 (; ) = 0 A 2 (; ) = 0 4 For t k 2 <ref> [0; 1] </ref>, k = 1; : : :; p, define the p fi p matrices obtained by taking each of the p rows of r 20 Q (; ) and r 11 Q (; ) and replacing and with points (t k ) def (t k ) = t k + <p> From the mean value theorem: Z 1 @ Q ((t); i (t)) + r i @ k = r @ k @ Q ((t k ); i (t k )) i where t k is some point in <ref> [0; 1] </ref>, which in general depends on , i , and fl , and [A 1 ] kfl , [A 2 ] kfl denote the k-th rows of the matrices A 1 , A 2 defined in (7). <p> Using the facts that t b = ln b and t (t b ) = ln (t b ) for some t b 2 <ref> [0; 1] </ref>, b = 1; : : :; p, we can express the above in terms of the original parameterization to obtain: (60) 1 [B t + C t + R T 20 where D t = diag b b (t b ) R t = diag b b (t b
Reference: [2] <author> N. Antoniadis and A. O. Hero, </author> <title> "Time delay estimation for Poisson processes using the EM algorithm," </title> <booktitle> in Proc. IEEE Int. Conf. Acoust., Speech, and Sig. Proc., </booktitle> <address> San Francisco, </address> <year> 1992. </year>
Reference-contexts: This general iterative algorithm specializes to popular EM-type algorithms including: penalized and penalized ML-EM algorithms [4, 11], generalized ML-EM with stochastic complete-incomplete data mapping [6], one-step-late (OSL) penalized ML-EM [9], majorization methods [?], and approximate ML-EM algorithms such as the linear and quadratic approximations introduced in <ref> [2, 1] </ref>. Let fl be a fixed point of the EM-type algorithm which occurs on the interior of fi and assume that Q is a smooth function of both arguments. <p> This algorithm is of interest since the conventional convergence analysis of the EM algorithm is inapplicable due to the presence of additive noise in the mapping from the complete data to the incomplete data. Then we give general forms for the asymptotic convergence rates for the linearized EM algorithm <ref> [2] </ref>, the unpenalized and penalized ML-EM algorithms, the OSL penalized ML-EM algorithm [9], and the majorization method [?]. For the latter algorithms the asymptotic convergence rates are of similar form to those obtained by Green [9] for the standard case of deterministic complete/incomplete data mapping. <p> The OSL method of Green and the majorization method of DePierro, both described below, address this difficulty by modifying the Q function. III.b: Linear Approximation to ML-EM Algorithm In <ref> [2] </ref> the unpenalized ML-EM algorithm was formulated for the difficult case of intensity pa rameter estimation for continuous-time filtered Poisson-Gaussian observations.
Reference: [3] <author> I. Csiszar and G. Tusnady, </author> <title> "Information geometry and alternating minimization procedures," </title> <journal> Statistics and Decisions, </journal> <volume> vol. Supplement Issue No. 1, </volume> <pages> pp. 205-237, </pages> <year> 1984. </year>
Reference-contexts: 1 This research was supported in part by the National Science Foundation under grant BCS-9024370 and a DOE Alexander Hollaender Postdoctoral Fellowship. 1 fixed point, norms under which the convergence is monotone; and the asymptotic convergence rate of the algorithm. A number of authors <ref> [24, 16, 3] </ref> have derived global convergence results for a wide range of exact ML-EM algorithms by using an information divergence approach. These authors establish monotone convergence of ML-EM algorithms in the information divergence measure. This guarantees that successive iterates of the EM algorithm monotonically increase likelihood.
Reference: [4] <author> A. P. Dempster, N. M. Laird, and D. B. Rubin, </author> <title> "Maximum likelihood from incomplete data via the EM algorithm," </title> <journal> J. Royal Statistical Society, Ser. B, </journal> <volume> vol. 39, </volume> <pages> pp. 1-38, </pages> <year> 1977. </year>
Reference-contexts: We define an EM-type algorithm as any iterative algorithm of the form i+1 = argmax Q (; i ), i = 1; 2; : : : where 2 fi IR p . This general iterative algorithm specializes to popular EM-type algorithms including: penalized and penalized ML-EM algorithms <ref> [4, 11] </ref>, generalized ML-EM with stochastic complete-incomplete data mapping [6], one-step-late (OSL) penalized ML-EM [9], majorization methods [?], and approximate ML-EM algorithms such as the linear and quadratic approximations introduced in [2, 1]. <p> See Figure 1. Mathematically, this means that the conditional density of Y given X is functionally independent of . Observe that this definition of complete data is more general than the standard definition, e.g. as used in <ref> [4, 24] </ref>, since it permits the complete and incomplete data to be related through a stochastic mapping. Our definition of complete data reduces to the standard definition when the channel C is specialized to a noiseless channel, i.e. a deterministic many-to-one transformation h such that Y = h (X). <p> The above Q function differs from the standard penalized ML-EM Q function of <ref> [4, 9] </ref> in only one respect: the presence of the function W (). For the standard case, the mapping from complete to incomplete data is deterministic, f (yjX; ) is degenerate, and W () = 0.
Reference: [5] <author> M. Feder, A. Oppenheim, and E. Weinstein, </author> <title> "Maximum likelihood noise cancelation using the EM algorithm," </title> <journal> IEEE Trans. Acoust., Speech, and Sig. Proc., </journal> <volume> vol. 37, no. 2, </volume> <pages> pp. 204-216, </pages> <month> Feb. </month> <year> 1989. </year>
Reference: [6] <author> J. A. Fessler and A. O. Hero, </author> <title> "Complete data spaces and generalized EM algorithms," </title> <booktitle> in Proc. IEEE Int. Conf. Acoust., Speech, and Sig. Proc., </booktitle> <address> Minneapolis, MN, </address> <month> April. </month> <year> 1993. </year>
Reference-contexts: This general iterative algorithm specializes to popular EM-type algorithms including: penalized and penalized ML-EM algorithms [4, 11], generalized ML-EM with stochastic complete-incomplete data mapping <ref> [6] </ref>, one-step-late (OSL) penalized ML-EM [9], majorization methods [?], and approximate ML-EM algorithms such as the linear and quadratic approximations introduced in [2, 1]. <p> The theory presented in this paper has recently been applied to evaluating the convergence properties of a rapidly convergent class of EM-type algorithms called space-alternating generalized EM (SAGE) algorithms <ref> [6] </ref>. 25 APPENDICES A. Matrix Lemmas The following matrix results are used in the paper. Lemma 3 Let the pfip symmetric matrices A and B be positive definite and non-negative definite, respectively, and define the matrix C = A + B.
Reference: [7] <author> G. H. Golub and C. F. Van Loan, </author> <title> Matrix Computations (2nd Edition), </title> <publisher> The Johns Hopkins University Press, </publisher> <address> Baltimore, </address> <year> 1989. </year>
Reference-contexts: positivity condition (8) holds for = i then M 1 (; i ) is invertible for all and it follows from Eq. (15) that: i+1 = [M 1 ( i+1 ; i )] 1 M 2 ( i+1 ; i ) i ; (16) Furthermore, by properties of matrix norms <ref> [7] </ref>: k i+1 k fi fi fi M 1 ( i+1 ; i )] 1 M 2 ( i+1 ; i ) fi fi fi k i k 2fi fi fi fi [M 1 (; i )] 1 M 2 (; i ) fi fi fi k i k: (17) Therefore, <p> fi fi F 2 1 X fi fi fi fi fi fi k i k (45) where jjjjjj 2 is the matrix-2 norm and k k is the weighted Euclidean norm defined on vectors u 2 IR p : kuk = u T F X u: (46) Applying the Sherman-Morrisey-Woodbury <ref> [7] </ref> identity to the matrix F X F Xjy (43) we see that it is symmetric positive definite: F = F X F Xjy xx B T [Bfl xx B + fl nn ] 1 B]C = C T fl 1 xx + B T fl 1 xx C Thus F
Reference: [8] <author> F. A. Graybill, </author> <title> Matrices with Applications in Statistics, </title> <publisher> Wadsworth Publishing Co., </publisher> <address> Belmont CA, </address> <year> 1983. </year>
Reference: [9] <author> P. J. Green, </author> <title> "On the use of the EM algorithm for penalized likelihood estimation," </title> <journal> J. Royal Statistical Society, Ser. B, </journal> <volume> vol. 52, no. 2, </volume> <pages> pp. 443-452, </pages> <year> 1990. </year>
Reference-contexts: This general iterative algorithm specializes to popular EM-type algorithms including: penalized and penalized ML-EM algorithms [4, 11], generalized ML-EM with stochastic complete-incomplete data mapping [6], one-step-late (OSL) penalized ML-EM <ref> [9] </ref>, majorization methods [?], and approximate ML-EM algorithms such as the linear and quadratic approximations introduced in [2, 1]. Let fl be a fixed point of the EM-type algorithm which occurs on the interior of fi and assume that Q is a smooth function of both arguments. <p> Then we give general forms for the asymptotic convergence rates for the linearized EM algorithm [2], the unpenalized and penalized ML-EM algorithms, the OSL penalized ML-EM algorithm <ref> [9] </ref>, and the majorization method [?]. For the latter algorithms the asymptotic convergence rates are of similar form to those obtained by Green [9] for the standard case of deterministic complete/incomplete data mapping. <p> Then we give general forms for the asymptotic convergence rates for the linearized EM algorithm [2], the unpenalized and penalized ML-EM algorithms, the OSL penalized ML-EM algorithm <ref> [9] </ref>, and the majorization method [?]. For the latter algorithms the asymptotic convergence rates are of similar form to those obtained by Green [9] for the standard case of deterministic complete/incomplete data mapping. Afterwards we consider ML-EM and penalized ML-EM for two important practical 2 examples: a linear model involving jointly Gaussian statistics for the complete and incomplete data and a PET reconstruction model involving jointly Poisson statistics. <p> The above Q function differs from the standard penalized ML-EM Q function of <ref> [4, 9] </ref> in only one respect: the presence of the function W (). For the standard case, the mapping from complete to incomplete data is deterministic, f (yjX; ) is degenerate, and W () = 0. <p> In particular, with fl a fixed point, the asymptotic convergence rate to fl is [r 20 ~ Q ( fl ; fl )] 1 r 11 ~ Q ( fl ; fl ) . III.c: One-Step-Late (OSL) Penalized ML-EM Green <ref> [9, 10] </ref> proposed an approximation, which he called the one-step-late (OSL) algorithm, for the case of tomographic image reconstruction with Poisson data and a Gibbs prior for which the M step of the penalized ML-EM algorithm is intractible. <p> fl 1 + O (k i k 2 ) : ff 2 fl fl 1 + O (k i k 2 ) : (68) Thus if i is sufficently close to ^ : fl fl &lt; fl ln i fl Now it is known that the ML-EM PET algorithm converges <ref> [9] </ref> so that k i ^ k ! 0. As long as ^ is strictly positive, the relation (68) asserts that in the final iterations of the algorithm the logarithmic differences ln i ln ^ converge monotonically to zero relative to the norm (67).
Reference: [10] <author> P. J. Green, </author> <title> "Bayesian reconstructions from emission tomography using a modified EM algorithm," </title> <journal> IEEE Trans. on Medical Imaging, </journal> <volume> vol. 11, no. 1, </volume> <pages> pp. 81-90, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: In particular, with fl a fixed point, the asymptotic convergence rate to fl is [r 20 ~ Q ( fl ; fl )] 1 r 11 ~ Q ( fl ; fl ) . III.c: One-Step-Late (OSL) Penalized ML-EM Green <ref> [9, 10] </ref> proposed an approximation, which he called the one-step-late (OSL) algorithm, for the case of tomographic image reconstruction with Poisson data and a Gibbs prior for which the M step of the penalized ML-EM algorithm is intractible. <p> If the OSL algorithm converges to a fixed point fl the asymptotic con vergence rate is: [r 20 Q ( fl ; fl )] 1 [r 11 Q ( fl ; fl ) + r 2 which is identical to the result cited in <ref> [10] </ref> for the standard case of deterministic complete/incomplete data mapping and the Gibbs prior f () = expffiV ()g. IV. APPLICATIONS We consider two separate applications: the linear Gaussian model and the PET image reconstruction model. <p> random variables with intensity E fN db g = P djb b , d = 1; : : : ; m, b = 1; : : : ; p and that the conditional expectation Efln f (X; )jy; i g of the complete data log-likelihood given the incomplete data is <ref> [10] </ref>: Efln f (X; )jY; i g = m X p X " b d # where i d = b=1 i b P bjd . <p> Under these assumptions r 20 Q (; i ) = diag b b [B ( i ) + C ( i )] diag b b r 11 Q (; i ) = diag b b where, similar to the definition in <ref> [10] </ref>, B ( i ) is the positive definite p fi p matrix: B ( i ) = d=1 [ i d ] 2 P djfl P T djfl ; B ( i ) + C ( i ) is the p fi p positive definite matrix B ( i )
Reference: [11] <author> T. Hebert and R. Leahy, </author> <title> "A generalized EM algorithm for 3-D Bayesian reconstruction from Poisson data using Gibbs priors," </title> <journal> IEEE Trans. on Medical Imaging, </journal> <volume> vol. 8, no. 2, </volume> <pages> pp. 194-203, </pages> <year> 1989. </year>
Reference-contexts: We define an EM-type algorithm as any iterative algorithm of the form i+1 = argmax Q (; i ), i = 1; 2; : : : where 2 fi IR p . This general iterative algorithm specializes to popular EM-type algorithms including: penalized and penalized ML-EM algorithms <ref> [4, 11] </ref>, generalized ML-EM with stochastic complete-incomplete data mapping [6], one-step-late (OSL) penalized ML-EM [9], majorization methods [?], and approximate ML-EM algorithms such as the linear and quadratic approximations introduced in [2, 1].
Reference: [12] <author> T. Hebert and R. Leahy, </author> <title> "Statistic-based MAP image reconstruction from Poisson data using Gibbs priors," </title> <journal> IEEE Trans. on Signal Processing, </journal> <volume> vol. 40, no. 9, </volume> <pages> pp. 2290-2302, </pages> <year> 1992. </year>
Reference: [13] <author> R. A. Horn and C. R. Johnson, </author> <title> Matrix Analysis, </title> <address> Cambridge, </address> <year> 1985. </year>
Reference-contexts: For any p fi p matrix A the induced matrix norm jjjAjjj <ref> [13] </ref> of A is defined as: jjjAjjj def u2IR p kAuk kuk where u denotes a vector in IR p . <p> ( fl ; fl ) i + o (k i k): Therefore the asymptotic rate of convergence is given by the root convergence factor [r 20 Q ( fl ; fl )] 1 r 11 Q ( fl ; fl ) : For any matrix A we have (A) jjjAjjj <ref> [13, Thm. 5.6.9] </ref> so that, in view of (19), the root convergence factor is less than one. 2 The careful reader will have noticed that in the proof of Theorem 1 all that was required of i+1 was that r 10 Q ( i+1 ; i ) = 0.
Reference: [14] <author> I. A. Ibragimov and R. Z. Has'minskii, </author> <title> Statistical estimation: Asymptotic theory, </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1981. </year>
Reference-contexts: When P () is constant we obtain the standard unpenalized ML estimator: ^ = argmax 2fi Under broad conditions the penalized ML estimator enjoys many attractive properties such as consistency, asymptotic unbiasedness, and asymptotic minimum variance among unbiased estimators <ref> [14] </ref>. However, in many applications direct maximization of the functions in (24) and (23) is in-tractible. In this case the EM approach offers a simple indirect method for iteratively approximating the penalized and unpenalized ML estimates.
Reference: [15] <author> S. Kullback, </author> <title> Information Theory and Statistics, </title> <publisher> Dover, </publisher> <year> 1978. </year> <month> 28 </month>
Reference-contexts: form for the Q function in the penalized ML-EM algorithm (25) is: Q (; ) = L () D (k) P () (27) where D (k) is the Kullback-Liebler (KL) discrimination: D (k) = ln f (xjy; ) The following properties of the KL discrimination D (k) follow directly from <ref> [15, Ch. 2] </ref>: 1. D (k) 0 where "=" holds iff g (xjy; ) = g (xjy; ) a.e. in x. 2. <p> D (k) 0 where "=" holds iff g (xjy; ) = g (xjy; ) a.e. in x. 2. When differentiation of f (xjy; ) under the integral sign is justified <ref> [15, Sec. 2.6] </ref>: (a) r 10 D (k) = r 01 D (k) = 0 (c) r 11 D (k) = r 20 D (k) 3. r 20 D (k) = F Xjy (), where F Xjy () = Efr 2 is the non-negative definite conditional Fisher information.
Reference: [16] <author> K. Lange, </author> <title> "Convergence of EM image reconstruction algorithms with Gibbs smoothing," </title> <journal> IEEE Trans. on Medical Imaging, </journal> <volume> vol. 9, no. 4, </volume> <pages> pp. 439-446, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: 1 This research was supported in part by the National Science Foundation under grant BCS-9024370 and a DOE Alexander Hollaender Postdoctoral Fellowship. 1 fixed point, norms under which the convergence is monotone; and the asymptotic convergence rate of the algorithm. A number of authors <ref> [24, 16, 3] </ref> have derived global convergence results for a wide range of exact ML-EM algorithms by using an information divergence approach. These authors establish monotone convergence of ML-EM algorithms in the information divergence measure. This guarantees that successive iterates of the EM algorithm monotonically increase likelihood.
Reference: [17] <author> K. Lange and R. Carson, </author> <title> "EM reconstruction algorithms for emission and transmission tomography," </title> <journal> J. Comp. Assisted Tomography, </journal> <volume> vol. 8, no. 2, </volume> <pages> pp. 306-316, </pages> <month> April </month> <year> 1984. </year>
Reference-contexts: The standard choice of complete data X for estimation of via the EM algorithm is the set fN db g m;p d=1;b=1 , where N db denotes the number of emissions in pixel b which are detected at detector d <ref> [23, 17] </ref>. This complete data is related to the incomplete data via the deterministic many-to-one mapping: N d = P p b=1 N db , d = 1; : : :; m.
Reference: [18] <author> M. I. Miller and D. L. Snyder, </author> <title> "The role of likelihood and entropy in incomplete-data problems: applications to estimating point-process intensities and Toeplitz constrained covari-ances," </title> <journal> IEEE Proceedings, </journal> <volume> vol. 75, no. 7, </volume> <pages> pp. 892-907, </pages> <month> July </month> <year> 1987. </year>
Reference: [19] <author> M. Miller and D. Fuhrmann, </author> <title> "An EM algorithm for directions of arrival estimation for narrow-band signals in noise," </title> <booktitle> in Proceedings of the SPIE, </booktitle> <volume> volume 826, </volume> <pages> pp. 101-103, </pages> <year> 1987. </year>
Reference: [20] <author> J. M. Ortega and W. C. Rheinboldt, </author> <title> Iterative Solution of Nonlinear Equations in Several Variables, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1970. </year>
Reference-contexts: Then, since kv i+1 k jjjAjjj kv i k &lt; kv i k, the sequence fv i g converges monotonically to zero and the asymptotic rate of convergence is specified by the root convergence factor (A) which is defined as the largest magnitude eigenvalue of A <ref> [20] </ref>. Observe that (A) is identical to jjjAjjj 2 if A is real symmetric non-negative definite. Assume that the function Q (; ) is twice continuously differentiable in both arguments and over ; 2 fi.
Reference: [21] <author> E. Polak, </author> <title> Computational methods in optimization: a unified approach, </title> <publisher> Academic Press, </publisher> <address> Or-lando, </address> <year> 1971. </year>
Reference: [22] <author> M. Segal, E. Weinstein, and B. Musicus, </author> <title> "Estimate-maximize algorithms for multichannel time delay and signal estimation," </title> <journal> IEEE Trans. Acoust., Speech, and Sig. Proc., </journal> <volume> vol. 39, no. 1, </volume> <pages> pp. 1-16, </pages> <month> Jan. </month> <year> 1991. </year>
Reference: [23] <author> L. A. Shepp and Y. Vardi, </author> <title> "Maximum likelihood reconstruction for emission tomography," </title> <journal> IEEE Trans. on Medical Imaging, </journal> <volume> vol. MI-1, No. 2, </volume> <pages> pp. 113-122, </pages> <month> Oct. </month> <year> 1982. </year>
Reference-contexts: The standard choice of complete data X for estimation of via the EM algorithm is the set fN db g m;p d=1;b=1 , where N db denotes the number of emissions in pixel b which are detected at detector d <ref> [23, 17] </ref>. This complete data is related to the incomplete data via the deterministic many-to-one mapping: N d = P p b=1 N db , d = 1; : : :; m.
Reference: [24] <author> C. F. J. Wu, </author> <title> "On the convergence properties of the EM algorithm," </title> <journal> Annals of Statistics, </journal> <volume> vol. 11, </volume> <pages> pp. 95-103, </pages> <year> 1983. </year>
Reference-contexts: 1 This research was supported in part by the National Science Foundation under grant BCS-9024370 and a DOE Alexander Hollaender Postdoctoral Fellowship. 1 fixed point, norms under which the convergence is monotone; and the asymptotic convergence rate of the algorithm. A number of authors <ref> [24, 16, 3] </ref> have derived global convergence results for a wide range of exact ML-EM algorithms by using an information divergence approach. These authors establish monotone convergence of ML-EM algorithms in the information divergence measure. This guarantees that successive iterates of the EM algorithm monotonically increase likelihood. <p> See Figure 1. Mathematically, this means that the conditional density of Y given X is functionally independent of . Observe that this definition of complete data is more general than the standard definition, e.g. as used in <ref> [4, 24] </ref>, since it permits the complete and incomplete data to be related through a stochastic mapping. Our definition of complete data reduces to the standard definition when the channel C is specialized to a noiseless channel, i.e. a deterministic many-to-one transformation h such that Y = h (X).
References-found: 24

