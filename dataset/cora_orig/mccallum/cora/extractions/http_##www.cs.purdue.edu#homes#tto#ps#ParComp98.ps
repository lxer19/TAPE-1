URL: http://www.cs.purdue.edu/homes/tto/ps/ParComp98.ps
Refering-URL: http://www.cs.purdue.edu/homes/tto/mypaps.html
Root-URL: http://www.cs.purdue.edu
Email: e-mail: tto@cs.purdue.edu  e-mail: pch@imm.dtu.dk  e-mail: luzz@sun2.dmu.dk  
Title: A Coarse-Grained Parallel QR-Factorization Algorithm for Sparse Least Squares Problems  
Author: Tz. Ostromsky P. C. Hansen Z. Zlatev 
Keyword: coarse-grained parallelism, least squares problem, QR-factorization, general sparse matrix, drop-tolerance, reordering, partitioning, block algorithm.  
Address: West Lafayette, IN 47907, USA  Bldg. 305, DK-2800 Lyngby, Denmark  Frederiksborgvej 399, DK-4000 Roskilde, Denmark  
Affiliation: Purdue University, Department of Computer Science,  Institute of Mathematical Modelling, Technical University of Denmark,  National Environmental Research Institute,  
Abstract: A sparse QR-factorization algorithm SPARQR for coarse-grained parallel computations is described. The coefficient matrix, which is assumed to be general sparse, is reordered in an attempt to bring as many zero elements in the lower left corner as possible. The reordered matrix is then partitioned into block rows, and Givens plane rotations are applied in each block-row. These are independent tasks and can be done in parallel. Row and column permutations are carried out within the diagonal blocks in an attempt to preserve better the sparsity of the matrix. The algorithm can be used for solving least squares problems either directly or combined with an iterative method (preconditioned conjugate gradients are used). Small non-zero elements can optionally be dropped in the latter case. This leads to a better preservation of the sparsity and, therefore, to a faster factorization. The price which has to be paid is some loss of accuracy. The iterative method is used to regain the accuracy lost during the factorization. Numerical results from several experiments with matrices from the well-known Harwell-Boeing collection as well as with some larger sparse matrices are presented in this work. An SGI Power Challenge computer with 16 processors has been used in the experiments. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. R. Amestoy, I. S. Duff and C. Puglisi, </author> <title> Multifrontal QR-factorization in a multiprocessor environment, </title> <journal> Lin. Alg. with Appl., </journal> <volume> Vol. 3, No. 4 (1996), </volume> <pages> 275-300. </pages>
Reference-contexts: It has been implemented in many standard numerical linear algebra packages such as LAPACK [2], LINPACK [6], and NAG [24]. The Householder method has also been successfully used in some multi-frontal techniques for parallel sparse matrix factorization; mainly by applying dense matrix computations for small frontal matrices, <ref> [1, 21, 27] </ref>. Our algorithm is based on the Givens method, i.e., on a series of elementary plane rotations. A single element of the active submatrix is annihilated in each plane rotation. <p> However, the pattern will remain the same. Furthermore, the speed-ups for the most time consuming matrix, bellmedt, will practically not change (see also Table 6). Good speed-ups have also been achieved in <ref> [1] </ref> on an Alliant computer and in [23] on a Cray computer by using the multifrontal approach. 3.7 Using a large drop-tolerance Results of parallel runs of our code with a large drop-tolerance, t = 10 2 , are presented in Tables 8 and 9.
Reference: [2] <author> E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. Du Croz, A. Green-baum, S. Hammarling, A. McKenney, S. Ostrouchov and D. Sorensen, </author> <note> LAPACK: Users' guide. </note> <institution> SIAM (Society for Industrial and Applied Mathematics), Philadel-phia, </institution> <year> 1992 </year>
Reference-contexts: The Householder method [20] is commonly used for dense QR-factorization, both sequential and parallel [29], because it is stable and requires minimum floating-point operations. It has been implemented in many standard numerical linear algebra packages such as LAPACK <ref> [2] </ref>, LINPACK [6], and NAG [24]. The Householder method has also been successfully used in some multi-frontal techniques for parallel sparse matrix factorization; mainly by applying dense matrix computations for small frontal matrices, [1, 21, 27]. <p> The computations can be carried out by creating a sequence of dense blocks. The dense blocks are executed sequentially, and dense matrix kernels are called to treat each block. Parallel computations are performed by the dense kernels (LAPACK routines, <ref> [2] </ref>, are used at present, but one 4 can easily switch to other efficient dense subroutines).
Reference: [3] <author> A. Bjorck, </author> <title> Stability analysis of the method of semi-normal equations for least squares problems, </title> <journal> Lin. Alg. Appl., </journal> <volume> Vol. 88/89 1987, </volume> <pages> pp. 31-48. </pages>
Reference-contexts: The solution obtained when the iterative method converges sufficiently quickly (it will normally satisfy some accuracy requirement) will be called the iterative solution. The preconditioned conjugate gradients (PCG) method is applied to the system A T Ax = A T b of semi-normal equations <ref> [3] </ref>, where the approximate factor ~ R is used as a preconditioner, in a similar way as in [30].
Reference: [4] <author> A. Bjorck, </author> <title> Least squares methods. In Handbook of Numerical Analysis, Vol.1: Finite Difference Methods Solution of Equations in R n , ( P. </title> <editor> G. Ciarlet and J. L. Lions, eds.). </editor> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1990. </year>
Reference-contexts: Sparse linear least squares problems appear in various technical and scientific areas like tomography, photogrammetry, geodetic survey, cluster analysis, molecular structure, structural analysis, and so on. More details on these and other applications can be found, e.g., in Bjorck <ref> [4] </ref> or Rice [28].
Reference: [5] <author> T. F. Chan, </author> <title> Rank revealing QR factorizations, </title> <journal> Lin. Alg. Appl., </journal> <volume> Vol. 88/89 1987, </volume> <pages> pp. 67-82. </pages>
Reference-contexts: Golub [17] and Powell & Reid [26]. The performance on parallel computers is normally more efficient when the orthogonal factorization is carried out without pivoting for numerical stability. Column interchanges are also (explicitly or implicitly) needed if one wants to compute a rank-revealing QR-factorization; see, for example, Chan <ref> [5] </ref>, Foster [10], or Pierce and Lewis [25]. Developing a parallel rank revealing code for sparse matrices will be one of the topics for future work. Both column and row interchanges must be used in order to preserve better the sparsity of matrix A during the orthogonal decomposition.
Reference: [6] <author> J. J. Dongarra, J. R. Bunch, C. B. Moler and G. W. Stuwart, </author> <title> LINPACk: Users' guide, </title> <publisher> SIAM (Society for Industrial and Applied Mathematics), </publisher> <address> Philadelphia, </address> <year> 1979. </year>
Reference-contexts: The Householder method [20] is commonly used for dense QR-factorization, both sequential and parallel [29], because it is stable and requires minimum floating-point operations. It has been implemented in many standard numerical linear algebra packages such as LAPACK [2], LINPACK <ref> [6] </ref>, and NAG [24]. The Householder method has also been successfully used in some multi-frontal techniques for parallel sparse matrix factorization; mainly by applying dense matrix computations for small frontal matrices, [1, 21, 27]. Our algorithm is based on the Givens method, i.e., on a series of elementary plane rotations.
Reference: [7] <author> I. S. Duff, A. M. Erisman and J. K. Reid, </author> <title> Direct Methods for Sparse Matrices, </title> <publisher> Oxford University Press, </publisher> <address> Oxford-London, </address> <year> 1986. </year>
Reference-contexts: m , find x 2 R n that minimizes the Euclidean norm of the residual vector r = b Ax : kb Axk 2 = min or equivalently, solve the system of linear algebraic equations Ax = b r ^ A T r = 0 :(2) Following Duff et al. <ref> [7] </ref> and Zlatev [30] it is assumed that the matrix A is large, sparse (most of its elements are equal to zero), and has full column rank (i.e., rank (A) = n). <p> The following brief description of the main data structures used in SPARQR the row-ordered list and the column-ordered list, will be helpful for better understanding of our algorithm (see more details in <ref> [7] </ref> and [30]). The row-ordered list is the main dynamic data structure. In the beginning of the algorithm it contains the non-zero elements of the matrix A in an array AQR (the non-zero elements are ordered by rows by LORA). <p> The information given above is quite sufficient for the purposes in this paper. Detailed overviews of the particular data structure which is applied here, and other commonly used data structures for sparse matrices, can be found in <ref> [7, 30] </ref>. Some specific features of the ordered lists that are implied by the requirements to obtain parallel computations will be discussed in x2.3 and in Section 3. 2.1 The reordering stage This is the first stage of our algorithm (and of many other algorithms for sparse matrices).
Reference: [8] <author> I. S. Duff, R. G. Grimes and J. G. Lewis, </author> <title> Sparse matrix test problems, </title> <journal> ACM Trans. Math. Software, </journal> <volume> Vol. 15, </volume> <year> 1989, </year> <pages> pp. 1-14. 27 </pages>
Reference-contexts: This is a shared memory machine, in which each processor has its own cache. The results of some our experiments, performed on this machine, are given in this section. 3.1 Test matrices, used in the experiments In our experiments we have used rectangular test matrices from the Harwell-Boeing collection <ref> [8] </ref>, type rra (real, rectangular, assembled), plus two matrices amoco1 and bellmedt kindly provided to us by M. Berry. It should be mentioned here that the transposed matrix of gemat1 is used in this section.
Reference: [9] <author> A. C. N. van Duin, P. C. Hansen, Tz. Ostromsky, H. Wijshoff and Z. Zlatev, </author> <title> Improving the numerical stability and the performance of a parallel sparse solver, Computers and Mathematics with Applications, </title> <address> Vol.30, No.12, </address> <year> 1995, </year> <pages> pp. 81-96. </pages>
Reference-contexts: As soon as this is done, one can get rid of them. On the other hand, the column permutation P must be saved to be used during the back substitution and throughout the iterative Stage 5, as shown in x2.4 and x2.5, respectively. The Locally Optimized Reordering Algorithm (LORA) <ref> [9, 11] </ref> is used in SPARQR. LORA attempts to move as many zeros as possible to the lower left corner of the matrix. This has two positive effects. First, it tends to reduce both the number of non-zeros and the number of fill-ins below the main diagonal.
Reference: [10] <author> L. V. Foster, </author> <title> Rank and null space calculations using matrix decomposition without column interchanges, </title> <journal> Lin. Alg. Appl., </journal> <volume> Vol. 74 1986, </volume> <pages> pp. 47-71. </pages>
Reference-contexts: The performance on parallel computers is normally more efficient when the orthogonal factorization is carried out without pivoting for numerical stability. Column interchanges are also (explicitly or implicitly) needed if one wants to compute a rank-revealing QR-factorization; see, for example, Chan [5], Foster <ref> [10] </ref>, or Pierce and Lewis [25]. Developing a parallel rank revealing code for sparse matrices will be one of the topics for future work. Both column and row interchanges must be used in order to preserve better the sparsity of matrix A during the orthogonal decomposition.
Reference: [11] <author> K. A. Gallivan, P. C. Hansen, Tz. Ostromsky and Z. Zlatev, </author> <title> A locally optimal reordering algorithm and its application to a parallel sparse linear system solver, </title> <booktitle> Computing, </booktitle> <address> Vol.54, No.1, </address> <year> 1995, </year> <pages> pp. 39-67. </pages>
Reference-contexts: As soon as this is done, one can get rid of them. On the other hand, the column permutation P must be saved to be used during the back substitution and throughout the iterative Stage 5, as shown in x2.4 and x2.5, respectively. The Locally Optimized Reordering Algorithm (LORA) <ref> [9, 11] </ref> is used in SPARQR. LORA attempts to move as many zeros as possible to the lower left corner of the matrix. This has two positive effects. First, it tends to reduce both the number of non-zeros and the number of fill-ins below the main diagonal. <p> Second, the structure obtained by LORA (see Fig. 1) facilitates the determination of a well balanced coarse-grained block partitioning and, consequently, the application of LORA leads to an efficient block-parallel factorization. The version of LORA for square matrices is described in detail in <ref> [11] </ref>, where it was used to reorder a square matrix before its LU-factorization by Gaussian elimination. The generalization for rectangular matrices is quite straightforward; the basic properties and the complexity estimate, O (NZ log n), where NZ is the number of non-zeros, remain unchanged.
Reference: [12] <author> W. M. Gentleman, </author> <title> Least squares computations by Givens transformations without square roots, </title> <journal> J. Inst. Math. Appl., </journal> <volume> Vol. 12, </volume> <year> 1973, </year> <pages> pp. 329-336. </pages>
Reference-contexts: There are two versions of the Givens method, classical Givens [16] and Gentleman-Givens <ref> [12] </ref>. The classical Givens version was used by George and Heath in the package SPARSPAK-B [13, 14, 15]. The Gentleman-Givens version (also known as fast Givens) is more economic with regard to floating point operations, but has some overhead and may occasionally require rescaling in an attempt to prevent overflows.
Reference: [13] <author> J. A. George and M. T. Heath, </author> <title> Solution of sparse linear least squares problems using Givens rotations, </title> <journal> Lin. Alg. Appl., </journal> <volume> Vol. 34, </volume> <year> 1980, </year> <pages> pp. 69-73. </pages>
Reference-contexts: There are two versions of the Givens method, classical Givens [16] and Gentleman-Givens [12]. The classical Givens version was used by George and Heath in the package SPARSPAK-B <ref> [13, 14, 15] </ref>. The Gentleman-Givens version (also known as fast Givens) is more economic with regard to floating point operations, but has some overhead and may occasionally require rescaling in an attempt to prevent overflows. It has been applied in the code LLSS by Zlatev [30].
Reference: [14] <author> J. A. George, M. T. Heath and E. G. Y. Ng, </author> <title> A comparison of some methods for solving sparse linear least squares problems, </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> Vol. 4, </volume> <year> 1983, </year> <pages> pp. 177-187. </pages>
Reference-contexts: There are two versions of the Givens method, classical Givens [16] and Gentleman-Givens [12]. The classical Givens version was used by George and Heath in the package SPARSPAK-B <ref> [13, 14, 15] </ref>. The Gentleman-Givens version (also known as fast Givens) is more economic with regard to floating point operations, but has some overhead and may occasionally require rescaling in an attempt to prevent overflows. It has been applied in the code LLSS by Zlatev [30].
Reference: [15] <author> J. A. George and E. G. Y. Ng, SPARSPAK: </author> <title> Waterloo sparse matrix package user's guide for SPARSPAK-B, </title> <institution> Research Report CS-84-47, Department of Computer Science, University of Waterloo, </institution> <address> Ontario, </address> <year> 1984. </year>
Reference-contexts: There are two versions of the Givens method, classical Givens [16] and Gentleman-Givens [12]. The classical Givens version was used by George and Heath in the package SPARSPAK-B <ref> [13, 14, 15] </ref>. The Gentleman-Givens version (also known as fast Givens) is more economic with regard to floating point operations, but has some overhead and may occasionally require rescaling in an attempt to prevent overflows. It has been applied in the code LLSS by Zlatev [30].
Reference: [16] <author> J. W. </author> <title> Givens, Computation of plane unitary rotations transforming a general matrix to a triangular form, </title> <journal> J. Soc. Ind. Appl. Math., </journal> <volume> Vol. 6, </volume> <year> 1958, </year> <pages> pp. 26-50. </pages>
Reference-contexts: There are two versions of the Givens method, classical Givens <ref> [16] </ref> and Gentleman-Givens [12]. The classical Givens version was used by George and Heath in the package SPARSPAK-B [13, 14, 15].
Reference: [17] <author> G. H. Golub, </author> <title> Numerical methods for solving linear least squares problems, </title> <journal> Numer. Math., </journal> <volume> Vol. 7, </volume> <year> 1965, </year> <pages> pp. 206-216. </pages>
Reference-contexts: The major advantage of the orthogonal factorization is its stability. Pivoting for stability is as a rule not used in the codes based on orthogonal decomposition techniques, although column interchanges are recommended when the matrix is very ill-conditioned, cf. Golub <ref> [17] </ref> and Powell & Reid [26]. The performance on parallel computers is normally more efficient when the orthogonal factorization is carried out without pivoting for numerical stability.
Reference: [18] <author> G. H. Golub and C. F. Van Loan, </author> <title> Matrix computations, 2. </title> <editor> Ed., </editor> <publisher> John Hopkins University Press, </publisher> <address> Baltimore, Maryland, USA, </address> <year> 1989. </year>
Reference-contexts: Our algorithm is based on the Givens method, i.e., on a series of elementary plane rotations. A single element of the active submatrix is annihilated in each plane rotation. This method is as stable as the Householder method (cf. <ref> [18] </ref>), but it preserves in general much better the sparsity of the original matrix A (cf. [30]).
Reference: [19] <author> P. C. Hansen, Tz. Ostromsky, A. Sameh and Z. Zlatev, </author> <title> Solving Sparse Linear Least Squares Problems on Some Supercomputers by Using a Sequence of Large Dense Blocks, </title> <journal> BIT, </journal> <volume> Vol. 37, </volume> <year> 1997, </year> <pages> 535-558. </pages>
Reference-contexts: One can also achieved parallel computations by using another technique, which is described in detail in <ref> [19] </ref>. The computations can be carried out by creating a sequence of dense blocks. The dense blocks are executed sequentially, and dense matrix kernels are called to treat each block. <p> If the matrix is dense or if it becomes dense during the computations, then Method 1 will perform best (the high speed of computation will be a sufficient compensation for the extra operations with zero elements). A few results which confirm this statement are given in <ref> [19] </ref>. Very often the matrix is very sparse in the beginning, and then becomes denser and denser. This fact indicates that a hybrid methods, where one starts with SPARQR and switches to Method 1 when appropriate, may be a good choice. This is a good topic for a future research.
Reference: [20] <author> A. S. </author> <title> Householder, Unitary triangularization of a nonsymmetric matrix, </title> <journal> J. Assoc. Comput. Mach., </journal> <volume> Vol. 5, </volume> <year> 1958, </year> <pages> pp. 339-342. </pages>
Reference-contexts: There are several ways for computing the QR-factorization. The most popular of them, the Householder and the Givens methods, are based on representing the orthogonal matrix Q as a product of elementary transformations (Householder reflections or Givens plane rotations respectively). The Householder method <ref> [20] </ref> is commonly used for dense QR-factorization, both sequential and parallel [29], because it is stable and requires minimum floating-point operations. It has been implemented in many standard numerical linear algebra packages such as LAPACK [2], LINPACK [6], and NAG [24].
Reference: [21] <author> P. Matstoms, </author> <title> The Multifrontal Solution of Sparse Linear Least Squares Problems, </title> <institution> LIU-TEK-LIC-1991:33, Department of Mathematics, Linkoping University, Linkoping, Sweden, </institution> <year> 1991. </year>
Reference-contexts: It has been implemented in many standard numerical linear algebra packages such as LAPACK [2], LINPACK [6], and NAG [24]. The Householder method has also been successfully used in some multi-frontal techniques for parallel sparse matrix factorization; mainly by applying dense matrix computations for small frontal matrices, <ref> [1, 21, 27] </ref>. Our algorithm is based on the Givens method, i.e., on a series of elementary plane rotations. A single element of the active submatrix is annihilated in each plane rotation.
Reference: [22] <author> P. Matstoms, </author> <title> Sparse QR factorization with applications to linear least squares problems, </title> <type> Ph D Thesis, </type> <institution> Linkoping University, Linkoping, Sweden, </institution> <year> 1994. </year>
Reference-contexts: This shows that it is also necessary to show that SPARQR is at least comparable with a good sequential code. A multifrontal code, qr27 ([21], <ref> [22] </ref>, [23]), has been chosen. This code has some parallel potentials, but on SGI Power Challenge it performs as a sequential code. An experiment with 20 matrices has been performed. The average number, r, of non-zero elements per row has been varied (r = 5 (5)100).
Reference: [23] <author> P. Matstoms, </author> <title> Parallel sparse QR factorization on shared memory architectures, </title> <journal> Parallel Computing, </journal> <volume> Vol. 21 (1995), </volume> <pages> 473-486. </pages>
Reference-contexts: However, the pattern will remain the same. Furthermore, the speed-ups for the most time consuming matrix, bellmedt, will practically not change (see also Table 6). Good speed-ups have also been achieved in [1] on an Alliant computer and in <ref> [23] </ref> on a Cray computer by using the multifrontal approach. 3.7 Using a large drop-tolerance Results of parallel runs of our code with a large drop-tolerance, t = 10 2 , are presented in Tables 8 and 9. <p> This shows that it is also necessary to show that SPARQR is at least comparable with a good sequential code. A multifrontal code, qr27 ([21], [22], <ref> [23] </ref>), has been chosen. This code has some parallel potentials, but on SGI Power Challenge it performs as a sequential code. An experiment with 20 matrices has been performed. The average number, r, of non-zero elements per row has been varied (r = 5 (5)100).
Reference: [24] <editor> NAG LIBRARY FORTRAN MANUAL. </editor> <volume> Mark 7, Vol. </volume> <pages> 3-4. </pages> <booktitle> Numerical Algorithms Group, </booktitle> <address> Banbury Road 7, Oxford, England, </address> <year> 1979. </year> <month> 28 </month>
Reference-contexts: The Householder method [20] is commonly used for dense QR-factorization, both sequential and parallel [29], because it is stable and requires minimum floating-point operations. It has been implemented in many standard numerical linear algebra packages such as LAPACK [2], LINPACK [6], and NAG <ref> [24] </ref>. The Householder method has also been successfully used in some multi-frontal techniques for parallel sparse matrix factorization; mainly by applying dense matrix computations for small frontal matrices, [1, 21, 27]. Our algorithm is based on the Givens method, i.e., on a series of elementary plane rotations.
Reference: [25] <author> D. J. Pierce and J. G. Lewis, </author> <title> Sparse rank revealing QR factorizations, </title> <type> Technical Report MEA-TR-193, </type> <institution> Boeing Computer Services, </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: The performance on parallel computers is normally more efficient when the orthogonal factorization is carried out without pivoting for numerical stability. Column interchanges are also (explicitly or implicitly) needed if one wants to compute a rank-revealing QR-factorization; see, for example, Chan [5], Foster [10], or Pierce and Lewis <ref> [25] </ref>. Developing a parallel rank revealing code for sparse matrices will be one of the topics for future work. Both column and row interchanges must be used in order to preserve better the sparsity of matrix A during the orthogonal decomposition.
Reference: [26] <author> M. J. D. Powell and J. K. Reid, </author> <title> On applying Householder transformations to linear least squares problems, In: "Information Processing" (A. </title> <editor> J. H. Morrell, </editor> <publisher> ed.), </publisher> <pages> pp. 122-126. </pages> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1969. </year>
Reference-contexts: The major advantage of the orthogonal factorization is its stability. Pivoting for stability is as a rule not used in the codes based on orthogonal decomposition techniques, although column interchanges are recommended when the matrix is very ill-conditioned, cf. Golub [17] and Powell & Reid <ref> [26] </ref>. The performance on parallel computers is normally more efficient when the orthogonal factorization is carried out without pivoting for numerical stability. Column interchanges are also (explicitly or implicitly) needed if one wants to compute a rank-revealing QR-factorization; see, for example, Chan [5], Foster [10], or Pierce and Lewis [25].
Reference: [27] <author> C. Puglisi, </author> <title> QR-factorization of large sparse overdetermined and square matrices using the multifrontal method in a multiprocessor environment, </title> <type> Report TR/PA/93/33, </type> <institution> CERFACS, </institution> <year> 1993. </year>
Reference-contexts: It has been implemented in many standard numerical linear algebra packages such as LAPACK [2], LINPACK [6], and NAG [24]. The Householder method has also been successfully used in some multi-frontal techniques for parallel sparse matrix factorization; mainly by applying dense matrix computations for small frontal matrices, <ref> [1, 21, 27] </ref>. Our algorithm is based on the Givens method, i.e., on a series of elementary plane rotations. A single element of the active submatrix is annihilated in each plane rotation.
Reference: [28] <author> J. R. Rice, </author> <title> PARVEC workshop on very large least squares problems and supercomputers, </title> <type> Report CSD-TR 464, </type> <institution> Purdue University, West Lafayette, IN, </institution> <year> 1983. </year>
Reference-contexts: Sparse linear least squares problems appear in various technical and scientific areas like tomography, photogrammetry, geodetic survey, cluster analysis, molecular structure, structural analysis, and so on. More details on these and other applications can be found, e.g., in Bjorck [4] or Rice <ref> [28] </ref>.
Reference: [29] <author> G. W. Stewart, </author> <title> A parallel implementation of the QR-algorithm, </title> <journal> Parallel Computing, </journal> <volume> Vol. 5, </volume> <year> 1987, </year> <month> pp.187-196. </month>
Reference-contexts: The most popular of them, the Householder and the Givens methods, are based on representing the orthogonal matrix Q as a product of elementary transformations (Householder reflections or Givens plane rotations respectively). The Householder method [20] is commonly used for dense QR-factorization, both sequential and parallel <ref> [29] </ref>, because it is stable and requires minimum floating-point operations. It has been implemented in many standard numerical linear algebra packages such as LAPACK [2], LINPACK [6], and NAG [24].
Reference: [30] <author> Z. Zlatev, </author> <title> Computational Methods for General Sparse Matrices, </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Dordrecht-Toronto-London, </address> <year> 1991. </year> <month> 29 </month>
Reference-contexts: x 2 R n that minimizes the Euclidean norm of the residual vector r = b Ax : kb Axk 2 = min or equivalently, solve the system of linear algebraic equations Ax = b r ^ A T r = 0 :(2) Following Duff et al. [7] and Zlatev <ref> [30] </ref> it is assumed that the matrix A is large, sparse (most of its elements are equal to zero), and has full column rank (i.e., rank (A) = n). <p> A single element of the active submatrix is annihilated in each plane rotation. This method is as stable as the Householder method (cf. [18]), but it preserves in general much better the sparsity of the original matrix A (cf. <ref> [30] </ref>). <p> The Gentleman-Givens version (also known as fast Givens) is more economic with regard to floating point operations, but has some overhead and may occasionally require rescaling in an attempt to prevent overflows. It has been applied in the code LLSS by Zlatev <ref> [30] </ref>. The classical Givens version is used in our code SPARQR. The pivotal strategy is the same as Strategy 1 in Chapter 14 in [30], but it is restricted to the diagonal blocks discussed in x2.2 and in x2.3. Consider a diagonal block. <p> It has been applied in the code LLSS by Zlatev <ref> [30] </ref>. The classical Givens version is used in our code SPARQR. The pivotal strategy is the same as Strategy 1 in Chapter 14 in [30], but it is restricted to the diagonal blocks discussed in x2.2 and in x2.3. Consider a diagonal block. Assume that the computations have been carried out to some major step s 1. <p> The preconditioned conjugate gradients (PCG) method is applied to the system A T Ax = A T b of semi-normal equations [3], where the approximate factor ~ R is used as a preconditioner, in a similar way as in <ref> [30] </ref>. <p> The use of PCG methods in sparse codes for linear least squares is discussed in detail in Chapter 16 of <ref> [30] </ref>. SPARQR, which will be described in the following sections, is entirely based on sparse matrix computations (performed in one dimensional sparse arrays by trying to design pivotal strategies, restricted to the diagonal blocks, in order to keep the number of fill-ins low). <p> However, the factorization and the solution of (5) will in general be inaccurate. Therefore the iterative Stage 5 should be applied in order to obtain a sufficiently accurate iterative solution. Dropping small elements is the computer-oriented manner of exploiting the sparsity from Zlatev <ref> [30] </ref>. For unstructured matrices this device is often one of the most efficient ways to exploit the sparsity as much as possible. By varying the drop-tolerance t , which is a user-specified parameter, one can obtain a more or less accurate QR-factorization that costs more or less computing time, respectively. <p> The following brief description of the main data structures used in SPARQR the row-ordered list and the column-ordered list, will be helpful for better understanding of our algorithm (see more details in [7] and <ref> [30] </ref>). The row-ordered list is the main dynamic data structure. In the beginning of the algorithm it contains the non-zero elements of the matrix A in an array AQR (the non-zero elements are ordered by rows by LORA). <p> The information given above is quite sufficient for the purposes in this paper. Detailed overviews of the particular data structure which is applied here, and other commonly used data structures for sparse matrices, can be found in <ref> [7, 30] </ref>. Some specific features of the ordered lists that are implied by the requirements to obtain parallel computations will be discussed in x2.3 and in Section 3. 2.1 The reordering stage This is the first stage of our algorithm (and of many other algorithms for sparse matrices). <p> The PCG method is to be applied to the preconditioned system (6), in a similar way as in Zlatev <ref> [30] </ref>, in an attempt to obtain a solution with the required accuracy. The solution computed in Stage 4 is taken as an initial approximation x 0 . The PCG algorithm used in this stage is summarized in Fig. 3. The number of iterations depends strongly on the drop-tolerance. <p> be achieved if also a row ordered list of A T is prepared (the row ordered list of A T is the column ordered list of A together with the non-zero elements). 2.6 Stopping criteria and error estimation in Stage 5 A variety of stopping criteria, discussed in detail in <ref> [30] </ref>, are used to control the iterative process. The most important of them are the convergence criterion, which decides whether the method converges satisfactorily fast, and the accuracy check, which decides whether the desired accuracy is achieved. <p> If computer arithmetic is used, then the convergence itself does not necessarily mean convergence to the exact solution (such an unclear situation may arise, for example, when ff i ! 0). One should carefully check the 13 behaviour of the parameter ff i throughout the iterative process <ref> [30] </ref>. The accuracy check, described in more detail below, should be used only if the ff i do not increase, decrease, or oscillate too much. Let f i = kff i q i k. <p> Unfortunately most of them are not sufficiently large when modern high-speed computers are used. That is why we included in our experiments a much larger matrix, obtained by using one of the sparse matrix generators described in <ref> [30] </ref>. This matrix is called matrf2. Information about the size of the matrices, used in our experiments, is given in Table 1. Several other matrices are used in Table 10. <p> Table 9 contains the results for a matrix generated by the sparse matrix generator CLASSF, described in <ref> [30] </ref>. Although this matrix is much bigger than any of the Harwell-Boeing matrices, SPARQR solves this problem with large drop-tolerance, t = 10 2 , in a relatively short time.
References-found: 30

