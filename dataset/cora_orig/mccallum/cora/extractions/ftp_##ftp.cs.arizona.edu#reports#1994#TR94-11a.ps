URL: ftp://ftp.cs.arizona.edu/reports/1994/TR94-11a.ps
Refering-URL: http://www.cs.gatech.edu/computing/classes/cs6420_96_winter/sch.html
Root-URL: 
Title: Distributed Filaments: Efficient Fine-Grain Parallelism on a Cluster of Workstations  
Author: Vincent W. Freeh David K. Lowenthal Gregory R. Andrews 
Abstract-found: 0
Intro-found: 1
Reference: [ALL89] <author> T.E. Anderson, E.D. Lazowska, and H.M. Levy. </author> <title> The performance implications of thread management alternatives for shared-memory multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(12) </volume> <pages> 1631-1644, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: Nodes 1 2 4 8 DF Time (sec) 212 104 65.5 48.5 DF Speedup 1.01 2.07 3.28 4.43 Sequential program: 215 sec. optimizations such as those described in <ref> [ALL89] </ref>). The overhead of context switching is significant when threads execute a small number of instructions, as in Jacobi iteration. Hence, general-purpose threads packages are most useful for providing coarse-grain parallelism or for structuring a large concurrent system.
Reference: [AOC + 88] <author> Gregory R. Andrews, Ronald A. Olsson, Michael Coffin, Irving Elshoff, Kelvin Nilsen, Titus Pursin, and Gregg Townsend. </author> <title> An overview of the SR language and implementation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 10(1) </volume> <pages> 51-86, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: All filaments are independent; there is no guaranteed order of execution among them. Filaments are executed one at a time by server threads, which are traditional threads with stacks. Our threads package is based on the one used in the SR run-time system <ref> [AOC + 88] </ref>. It is non-preemptive, and it employs a scheduler written specifically for DF. Many Filaments programs attain good performance with little or no optimization. However, achieving good performance for applications that possess many small filaments requires three techniques: inlining, pruning, and pattern recognition.
Reference: [Bal90] <author> Henri E. Bal. </author> <title> Experience with distributed programming in Orca. </title> <booktitle> Proc. IEEE CS 1990 Int Conf on Computer Languages, </booktitle> <pages> pages 79-89, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: TreadMarks provides lazy-release consistency on a network of Unix workstations [KDCZ94]. Simulation has shown that this greatly reduces the number of messages relative to the number of messages needed in systems like Munin. Some DSMs provide different granularities of memory consistency. For example, Clouds [DJAR91] and Orca <ref> [Bal90] </ref> provide shared memory objects. This provides consistency at the granularity of user-level objects instead of operating system pages, which can reduce thrashing. Blizzard [SFL + 94] and Midway [BZS93] minimize false sharing by providing coherence at cache-line granularity.
Reference: [BS90] <author> Peter A. Buhr and R.A. Stroobosscher. </author> <title> The uSystem: providing light-weight concur-rency on shared memory multiprocessor computers running UNIX. </title> <journal> Software Practice and Experience, </journal> <pages> pages 929-964, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: In general-purpose threads packages, such as uSystem <ref> [BS90] </ref>, each thread has its own context.
Reference: [BZS93] <author> Brian N. Bershad, Matthew J. Zekauskas, and Wayne A. Sawdon. </author> <title> The Midway distributed shared memory system. </title> <booktitle> In COMPCON '93, </booktitle> <year> 1993. </year>
Reference-contexts: In particular, when a thread faults on a page, another thread can execute while the page request is still outstanding. In this way DF differs from DSMs such as IVY [LH89], Munin [CBZ91], Mirage [FP89], and Midway <ref> [BZS93] </ref>, where a page fault blocks the entire node. (VISA [HB92], a DSM specifically supporting distributed SISAL programs, does some overlapping of computation and communication, but VISA's stack-based nature limits the overlap relative to the multithreading of DF.) Overlapping communication and computation is useful on both older (e.g. <p> Some DSMs provide different granularities of memory consistency. For example, Clouds [DJAR91] and Orca [Bal90] provide shared memory objects. This provides consistency at the granularity of user-level objects instead of operating system pages, which can reduce thrashing. Blizzard [SFL + 94] and Midway <ref> [BZS93] </ref> minimize false sharing by providing coherence at cache-line granularity. Midway keeps a dirty bit per cache line and propagates changes at synchronization points. DF controls thrashing by using the Mirage window protocol and by providing the user control over the granularity of DSM pages.
Reference: [CBZ91] <author> John B. Carter, John K. Bennett, and Willy Zwaenepoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Proceedings of 13th ACM Symposium On Operating Systems, </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: In particular, when a thread faults on a page, another thread can execute while the page request is still outstanding. In this way DF differs from DSMs such as IVY [LH89], Munin <ref> [CBZ91] </ref>, Mirage [FP89], and Midway [BZS93], where a page fault blocks the entire node. (VISA [HB92], a DSM specifically supporting distributed SISAL programs, does some overlapping of computation and communication, but VISA's stack-based nature limits the overlap relative to the multithreading of DF.) Overlapping communication and computation is useful on both <p> There are any number of page consistency protocols (PCP) that could be implemented. We have found three PCPs to be sufficient to support the wide range of applications we have programmed in DF: migratory <ref> [CBZ91] </ref>, write-invalidate [LH89], and a new protocol we call implicit-invalidate. The migratory PCP keeps only one copy of each page; the page moves from node to node as needed. Write-invalidate allows replicated, read-only copies; all are invalidated explicitly when any copy is written. <p> On the other hand, Chores uses PRESTO threads as servers, so it can block a chore when necessary. However, Chores does not have a distributed implementation. Support for the recursive programming style does not generally exist on distributed-memory systems. For example, in Munin <ref> [CBZ91] </ref>, the user must program recursive applications using a shared queue (bag) of unexecuted tasks and must explicitly implement and lock the queue. DF allows recursive programs to be written naturally and efficiently by means of fork/join filaments. Several systems use overlapping to mask communication latency. <p> Mirage uses the time window coherence protocol to control thrashing. In particular, a node keeps a page for some minimum time period to guarantee that it makes some progress each time it acquires the page [FP89]. Munin uses release consistency in the write-shared protocol to handle false sharing <ref> [CBZ91] </ref>. The memory is made consistent at synchronization points so there is no thrashing. TreadMarks provides lazy-release consistency on a network of Unix workstations [KDCZ94]. Simulation has shown that this greatly reduces the number of messages relative to the number of messages needed in systems like Munin.
Reference: [CGL86] <author> Nicholas Carriero, David Gelernter, and Jerry Leichter. </author> <title> Distributed data structures in Linda. </title> <booktitle> In Thirteenth ACM Symp. on Principles of Programming Languages, </booktitle> <pages> pages 236-242, </pages> <month> January </month> <year> 1986. </year>
Reference-contexts: The programs tested evaluate a curve that causes workload imbalance. Our coarse-grain approximation divides the interval into p subintervals and assigns one to each node. However, this can lead to severe load imbalance, as reflected in Figure 6. A second program that uses a bag-of-tasks <ref> [CGL86] </ref> has better speedup, but its absolute time is much worse. The overhead of accessing the centralized bag is extremely high due to the large number of small tasks. These coarse-grain programs illustrate the need for a low-overhead, decentralized load balancing mechanism.
Reference: [CGSv93] <author> David E. Culler, Seth Copen Goldstein, Klaus Erik Schauser, and Thorsten von Eicken. </author> <title> TAM|a compiler controlled threaded abstract machine. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 18(3) </volume> <pages> 347-370, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: In Active Messages, the user has to make sure the prefetch is started soon enough, and in DF the user must correctly place filaments in pools (although we are working on making the placement automatic). The Threaded Abstract Machine, TAM <ref> [CGSv93] </ref>, uses Active Messages to achieve overlap of communication and computation in a parallel implementation of a dataflow language. CHARM is a fine-grain, explicit message-passing threads package [FRS + 91]. It provides architecture independence, overlap of communication and computation, and dynamic load balancing.
Reference: [CZ83] <author> D.R. Cheriton and W. Zwaenepoel. </author> <title> The distributed V kernel and its performance for diskless workstations. </title> <booktitle> In Proceedings of the Ninth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 128-140, </pages> <month> October </month> <year> 1983. </year>
Reference-contexts: Nodes delay at synchronization points until all outstanding page requests have been satisfied. Therefore, in a program without race conditions, a page request always returns a consistent copy of a page. Packet is similar to the VMTP protocol <ref> [CZ83] </ref>, which also does not buffer reply messages and retransmits request messages only when necessary. However, there is a fundamental difference: VMTP uses a synchronous send/receive/reply, whereas Packet uses an asynchronous send/receive/reply. Because request messages are retransmitted until acknowledged, we can implement a very efficient mutual exclusion mechanism.
Reference: [DJAR91] <author> Partha Dasgupta, Richard J. LeBlanc Jr., Mustaque Ahmad, and Umakishore Ra-machandran. </author> <title> The Clouds distributed operating system. </title> <booktitle> Computer, </booktitle> <pages> pages 34-44, </pages> <month> Novem-ber </month> <year> 1991. </year>
Reference-contexts: TreadMarks provides lazy-release consistency on a network of Unix workstations [KDCZ94]. Simulation has shown that this greatly reduces the number of messages relative to the number of messages needed in systems like Munin. Some DSMs provide different granularities of memory consistency. For example, Clouds <ref> [DJAR91] </ref> and Orca [Bal90] provide shared memory objects. This provides consistency at the granularity of user-level objects instead of operating system pages, which can reduce thrashing. Blizzard [SFL + 94] and Midway [BZS93] minimize false sharing by providing coherence at cache-line granularity.
Reference: [EAL93] <author> Dawson R. Engler, Gregory R. Andrews, and David K. Lowenthal. </author> <title> Shared Filaments: Efficient support for fine-grain parallelism on shared-memory multiprocessors. </title> <type> TR 93-13, </type> <institution> Dept. of Computer Science, University of Arizona, </institution> <month> April </month> <year> 1993. </year> <month> 16 </month>
Reference-contexts: Previous work has described the Shared Filaments (SF) package for shared-memory multiprocessors <ref> [EAL93] </ref>. We have used SF as a system-call library for a variety of applications; performance using SF is typically within 10% of that of equivalent coarse-grain programs, and it is sometimes even better (for load-imbalanced problems). <p> Hence, general-purpose threads packages are most useful for providing coarse-grain parallelism or for structuring a large concurrent system. A few threads packages support efficient fine-grain parallelism, e.g., the Uniform System [TC88], Shared Filaments <ref> [EAL93] </ref> and Chores [EZ93]. The first two restrict the generality of the threads model. In particular, the Uniform System uses task generators to provide parallelism, much in the way a parallelizing compiler works, and in Shared Filaments a thread (filament) cannot block.
Reference: [EZ93] <author> Derek L. Eager and John Zahorjan. Chores: </author> <title> Enhanced run-time support for shared memory parallel computing. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(1) </volume> <pages> 1-32, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: This not only increases the number of messages but increases the likelihood of a load-balance denial (because only two have sufficient work). 4.4 Binary Expression Trees The fork/join paradigm can also be used to compute the value of a binary expression tree, an application described in <ref> [EZ93] </ref>. The leaves are matrices and interior operators are matrix multiplication; the tree is traversed in parallel and the matrices are multiplied sequentially. Figure 7 contains the results of running the matrix expression program with 70 by 70 matrices and a balanced binary tree of height 7. <p> Hence, general-purpose threads packages are most useful for providing coarse-grain parallelism or for structuring a large concurrent system. A few threads packages support efficient fine-grain parallelism, e.g., the Uniform System [TC88], Shared Filaments [EAL93] and Chores <ref> [EZ93] </ref>. The first two restrict the generality of the threads model. In particular, the Uniform System uses task generators to provide parallelism, much in the way a parallelizing compiler works, and in Shared Filaments a thread (filament) cannot block.
Reference: [FP89] <author> Brett D. Fleisch and Gerald J. Popek. </author> <title> Mirage: a coherent distributed shared memory design. </title> <booktitle> In Proceedings of 12th ACM Symposium On Operating Systems, </booktitle> <pages> pages 211-223, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: In particular, when a thread faults on a page, another thread can execute while the page request is still outstanding. In this way DF differs from DSMs such as IVY [LH89], Munin [CBZ91], Mirage <ref> [FP89] </ref>, and Midway [BZS93], where a page fault blocks the entire node. (VISA [HB92], a DSM specifically supporting distributed SISAL programs, does some overlapping of computation and communication, but VISA's stack-based nature limits the overlap relative to the multithreading of DF.) Overlapping communication and computation is useful on both older (e.g. <p> Another concern in implementing fork/join is avoiding thrashing, which can occur when two nodes write to the same page. Fork/join in DF uses two mechanisms. The first is similar to one used in the Mirage <ref> [FP89] </ref> system. A node will keep a page for a certain length of time before giving it up; during that time all requests for the page from other nodes are dropped (they will be retransmitted later). <p> Mirage uses the time window coherence protocol to control thrashing. In particular, a node keeps a page for some minimum time period to guarantee that it makes some progress each time it acquires the page <ref> [FP89] </ref>. Munin uses release consistency in the write-shared protocol to handle false sharing [CBZ91]. The memory is made consistent at synchronization points so there is no thrashing. TreadMarks provides lazy-release consistency on a network of Unix workstations [KDCZ94].
Reference: [Fre94] <author> Vincent W. Freeh. </author> <title> A comparison of implicit and explicit parallel programming. </title> <type> TR 93-30a, </type> <institution> University of Arizona, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: We have also used SF as the back end for a modified Sisal compiler, thereby achieving efficient forall and function-call parallelism in a dataflow language <ref> [Fre94] </ref>. This paper addresses the issue of providing portable, efficient fine-grain parallelism on a cluster of workstations. Distributed Filaments (DF) extends SF and combines it with a distributed shared memory (DSM) customized for use with fine-grain threads. Figure 1 shows the components of DF and their inter-relation.
Reference: [FRS + 91] <author> W. Fenton, B. Ramkumar, V. A. Saletore, A. B. Sinha, and L. V. Kale. </author> <title> Supporting machine independent programming on diverse parallel architectures. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, volume II, Software, pages II-193-II-201, </booktitle> <address> Boca Raton, FL, August 1991. </address> <publisher> CRC Press. </publisher>
Reference-contexts: The Threaded Abstract Machine, TAM [CGSv93], uses Active Messages to achieve overlap of communication and computation in a parallel implementation of a dataflow language. CHARM is a fine-grain, explicit message-passing threads package <ref> [FRS + 91] </ref>. It provides architecture independence, overlap of communication and computation, and dynamic load balancing. CHARM has a distributed-memory programming model that can be run efficiently on both shared-and distributed-memory machines. DF provides functionality similar to CHARM using a shared-memory programming model.
Reference: [HB92] <author> Matthew Haines and Wim Bohm. </author> <title> The design of VISA: A virtual shared addressing system. </title> <type> Technical Report CS-92-120, </type> <institution> Colorado State University, </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: In particular, when a thread faults on a page, another thread can execute while the page request is still outstanding. In this way DF differs from DSMs such as IVY [LH89], Munin [CBZ91], Mirage [FP89], and Midway [BZS93], where a page fault blocks the entire node. (VISA <ref> [HB92] </ref>, a DSM specifically supporting distributed SISAL programs, does some overlapping of computation and communication, but VISA's stack-based nature limits the overlap relative to the multithreading of DF.) Overlapping communication and computation is useful on both older (e.g. Ethernet) and newer (e.g. FDDI, ATM) network technologies. <p> The Alewife and DF use similar ideas, except that DF is a software implementation requiring no specialized hardware. 14 VISA, a DSM written for the functional language Sisal, also uses overlapping <ref> [HB92] </ref>. Suspended threads are pushed on a stack, so there can be many outstanding page requests. The disadvantage of a stack-based approach is that threads are resumed in the inverse order in which they request pages.
Reference: [HFM88] <author> D. Hansgen, R. Finkel, and U. Manber. </author> <title> Two algorithms for barier synchronization. </title> <journal> Int. Journal of Parallel Programming, </journal> <volume> 17(1) </volume> <pages> 1-18, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: The overhead of barriers is a function of the number of nodes. DF uses a tournament barrier with broadcast dissemination, which has O (p) messages and a latency of O (log p) messages <ref> [HFM88] </ref>. Barrier synchronization times are shown in Figure 8. This is the cost of the barrier only; in an actual application it is likely that the nodes arrive at the barrier at different times, which increases the time a particular node is at the barrier.
Reference: [KCA91] <author> Kiyoshi Kurihara, David Chaiken, and Anant Agarwal. </author> <title> Latency tolerance through multithreading in large-scale multiprocessors. </title> <booktitle> In International Symposium on Shared Memory Multiprocessing, </booktitle> <pages> pages 91-101, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: It provides architecture independence, overlap of communication and computation, and dynamic load balancing. CHARM has a distributed-memory programming model that can be run efficiently on both shared-and distributed-memory machines. DF provides functionality similar to CHARM using a shared-memory programming model. The Alewife <ref> [KCA91] </ref>, a large-scale distributed-memory multiprocessor, provides hardware support for overlapping communication and computation. It provides the user with a shared-memory address space and enforces a context-switch to a new thread on any remote reference.
Reference: [KDCZ94] <author> Pete Keleher, Sandhya Dwarkadas, Alan Cox, and Willy Zwaenepoel. TreadMarks: </author> <title> Distributed shared memory on standard workstations and operating systems. </title> <booktitle> In Proceedings of the 1994 Winter Usenix Conference, </booktitle> <pages> pages 115-131, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: Munin uses release consistency in the write-shared protocol to handle false sharing [CBZ91]. The memory is made consistent at synchronization points so there is no thrashing. TreadMarks provides lazy-release consistency on a network of Unix workstations <ref> [KDCZ94] </ref>. Simulation has shown that this greatly reduces the number of messages relative to the number of messages needed in systems like Munin. Some DSMs provide different granularities of memory consistency. For example, Clouds [DJAR91] and Orca [Bal90] provide shared memory objects.
Reference: [LH89] <author> Kai Li and Paul Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4), </volume> <month> November </month> <year> 1989. </year>
Reference-contexts: In particular, when a thread faults on a page, another thread can execute while the page request is still outstanding. In this way DF differs from DSMs such as IVY <ref> [LH89] </ref>, Munin [CBZ91], Mirage [FP89], and Midway [BZS93], where a page fault blocks the entire node. (VISA [HB92], a DSM specifically supporting distributed SISAL programs, does some overlapping of computation and communication, but VISA's stack-based nature limits the overlap relative to the multithreading of DF.) Overlapping communication and computation is useful <p> There are any number of page consistency protocols (PCP) that could be implemented. We have found three PCPs to be sufficient to support the wide range of applications we have programmed in DF: migratory [CBZ91], write-invalidate <ref> [LH89] </ref>, and a new protocol we call implicit-invalidate. The migratory PCP keeps only one copy of each page; the page moves from node to node as needed. Write-invalidate allows replicated, read-only copies; all are invalidated explicitly when any copy is written.
Reference: [SFL + 94] <author> Ioannis Schoinas, Babak Falsafi, Alvin R. Lebeck, Steven K. Reinhardt, James R. Larus, and David A. Wood. </author> <title> Fine-grain access control for distributed shared memory. </title> <booktitle> In Sixth International Conference on Architecture Support for Programming Languages and Operating Systems (to appear), </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: Some DSMs provide different granularities of memory consistency. For example, Clouds [DJAR91] and Orca [Bal90] provide shared memory objects. This provides consistency at the granularity of user-level objects instead of operating system pages, which can reduce thrashing. Blizzard <ref> [SFL + 94] </ref> and Midway [BZS93] minimize false sharing by providing coherence at cache-line granularity. Midway keeps a dirty bit per cache line and propagates changes at synchronization points. DF controls thrashing by using the Mirage window protocol and by providing the user control over the granularity of DSM pages.
Reference: [SHG93] <author> Jaswinder Pal Singh, John L. Hennessy, and Anoop Gupta. </author> <title> Scaling parallel programs for multiprocessors: Methodology and examples. </title> <journal> Computer, </journal> <volume> 26(7) </volume> <pages> 42-50, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: A reduction is needed on every iteration. gets better than linear speedup for 2 and 4 nodes (the primary reason is the size of the working set and its effect on the cache, etc. <ref> [SHG93] </ref>), and it gets reasonable speedup on 8 nodes. The gain of overlapping communication and computation in the coarse-grain program is 5.5% and 14% on 4 and 8 nodes, respectively. The DF program uses the implicit-invalidate PCP, which eliminates invalidation messages.
Reference: [TC88] <author> Robert H. Thomas and Will Crowther. </author> <title> The Uniform system: an approach to run-time support for large scale shared memory parallel processors. </title> <booktitle> In 1988 Conference on Parallel Processing, </booktitle> <pages> pages 245-254, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: The overhead of context switching is significant when threads execute a small number of instructions, as in Jacobi iteration. Hence, general-purpose threads packages are most useful for providing coarse-grain parallelism or for structuring a large concurrent system. A few threads packages support efficient fine-grain parallelism, e.g., the Uniform System <ref> [TC88] </ref>, Shared Filaments [EAL93] and Chores [EZ93]. The first two restrict the generality of the threads model. In particular, the Uniform System uses task generators to provide parallelism, much in the way a parallelizing compiler works, and in Shared Filaments a thread (filament) cannot block.
Reference: [TL93] <author> Chanramohan A. Thekkath and Henry M. Levy. </author> <title> Limits to low-latency communication on high-speed networks. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(2) </volume> <pages> 179-203, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Ethernet) and newer (e.g. FDDI, ATM) network technologies. While FDDI and ATM provide higher bandwidth than Ethernet, there is still sufficient latency to make overlapping beneficial <ref> [TL93] </ref>. DF also uses a reliable datagram protocol built on UDP to reduce communication overhead. Our protocol buffers only short request messages, saving both time and space. This protocol provides reliable communication with the efficiency and scalability of UDP.
Reference: [vCGS92] <author> Thorsten von Eicken, David E. Culler, Seth Copen Goldstein, and Klaus Eric Schauser. </author> <title> Active Messages: a mechanism for intergrated communication and computation. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <pages> pages 256-266, </pages> <month> May </month> <year> 1992. </year> <month> 17 </month>
Reference-contexts: DF allows recursive programs to be written naturally and efficiently by means of fork/join filaments. Several systems use overlapping to mask communication latency. Active Messages <ref> [vCGS92] </ref> accomplishes overlap explicitly by placing prefetch instructions sufficiently far in advance of use that the shared data will arrive before it is used. On the other hand, DF achieves overlap implicitly through multithreading. Both require some programmer support to achieve maximal overlap of communication and computation.
References-found: 25

