URL: http://anthrax.physics.indiana.edu/~teige/minuit.ps
Refering-URL: http://anthrax.physics.indiana.edu/~teige/Manuals.html
Root-URL: http://www.cs.indiana.edu
Title: CERN Program Library Long Writeup D506 Function Minimization and Error Analysis Reference Manual  Application Software Group  
Affiliation: Computing and Networks Division CERN Geneva, Switzerland  
Date: 92.1 (March 1992)  
Note: Version  
Abstract-found: 0
Intro-found: 0
Reference: [1] <author> J. </author> <title> Kowalik and M.R. Osborne, Methods for unconstrained optimization problems (American Else-vier Publishing Co., </title> <publisher> Inc., </publisher> <address> New York, </address> <year> 1968). </year>
Reference-contexts: If the number of stages to be taken is known in advance, it is possible to improve very slightly on this technique by using a Fibonacci search, as described for example in Kowalik and Osborne <ref> [1] </ref>. Although Fibonacci can be shown to be optimal (in a sense described below), the slight improvement is probably not worth the added complication. The golden section search is optimal among algorithms where the stopping point is not decided in advance.
Reference: [2] <author> H.H. </author> <title> Rosenbrock, An automatic method for finding the greatest or least value of a function, </title> <institution> Comput. J.3, </institution> <month> 175 </month> <year> (1960). </year>
Reference-contexts: The Minuit package acts on a multiparameter Fortran function to which we give the generic name FCN, although the actual name may be chosen by the user. This function must be defined and supplied by the user (or by an intermediate program such as HBOOK <ref> [2] </ref> or PAW [3], in case Minuit is being used under the control of such an intermediate program). <p> These variations tend to be even more unstable than the basic method, since they use information from fewer points. 8.5 The success-failure method A good compromise between the stability of the grid search and the rapid convergence of quadratic interpolation is found with the success-failure technique of Rosenbrock <ref> [2] </ref>. A start point x 0 and initial 8.5. The success-failure method 47 step size d are required, and the function is evaluated at x 0 and x 0 + d. <p> It still converges, but as the valley becomes narrower, the convergence becomes arbitrarily slow. Such behaviour in many dimensions causes this method to be generally considered as unacceptably slow. Two of the more successful improvements aimed at avoiding such behaviour are due to Hooke and Jeeves [3] and Rosenbrock <ref> [2] </ref>. We discuss the latter below. 9.3 Rosenorock's method Rosenbrock's algorithm [2] starts by performing single-parameter minimizations as above. <p> Such behaviour in many dimensions causes this method to be generally considered as unacceptably slow. Two of the more successful improvements aimed at avoiding such behaviour are due to Hooke and Jeeves [3] and Rosenbrock <ref> [2] </ref>. We discuss the latter below. 9.3 Rosenorock's method Rosenbrock's algorithm [2] starts by performing single-parameter minimizations as above. Then when one full cycle of all parameters has been completed, a new set of orthogonal axes is defined with one axis taken as the vector from the start point to end point of the cycle.
Reference: [3] <author> R. Hooke and T.A. Jeeves, </author> <title> Direct search solution of numerical an statistical problems, </title> <journal> J. Assoc. Comput. Mach. </journal> <volume> 8, </volume> <month> 212 </month> <year> (1961). </year>
Reference-contexts: The Minuit package acts on a multiparameter Fortran function to which we give the generic name FCN, although the actual name may be chosen by the user. This function must be defined and supplied by the user (or by an intermediate program such as HBOOK [2] or PAW <ref> [3] </ref>, in case Minuit is being used under the control of such an intermediate program). <p> It still converges, but as the valley becomes narrower, the convergence becomes arbitrarily slow. Such behaviour in many dimensions causes this method to be generally considered as unacceptably slow. Two of the more successful improvements aimed at avoiding such behaviour are due to Hooke and Jeeves <ref> [3] </ref> and Rosenbrock [2]. We discuss the latter below. 9.3 Rosenorock's method Rosenbrock's algorithm [2] starts by performing single-parameter minimizations as above.
Reference: [4] <author> L.C.W. Dixon, </author> <title> Non-linear optimization (English Universities Press, </title> <address> London, </address> <year> 1972). </year>
Reference-contexts: Determining the Statistical Significance of Experimental Results <ref> [4] </ref>. Statistical Methods in Experimental Physics [5]. 5.5.2 The Reliability of Minuit Error Estimates. Minuit always carries around its own current estimates of the parameter errors, which it will print out on request, no matter how accurate they are at any given point in the execution. <p> All the other minimization directions are simply chosen orthogonal to the first one. Also, its terminal convergence is slow compared with the more `quadratic' methods described in Section 4. Another technique, that of Davies, Swann, and Campey <ref> [4] </ref> (unpublished, see Ref. 4) is similar to Rosenbrock's and will not be described here. 9.4 The simplex method One of the most successful stepping methods in many variables is that of Nelder and Mead [5], based on the simplex.
Reference: [5] <author> J.A. Nelder and R. Mead, </author> <title> A simplex method for function minimization, </title> <journal> Comput. J. </journal> <volume> 7, </volume> <month> 308 </month> <year> (1965). </year>
Reference-contexts: Determining the Statistical Significance of Experimental Results [4]. Statistical Methods in Experimental Physics <ref> [5] </ref>. 5.5.2 The Reliability of Minuit Error Estimates. Minuit always carries around its own current estimates of the parameter errors, which it will print out on request, no matter how accurate they are at any given point in the execution. <p> Another technique, that of Davies, Swann, and Campey [4] (unpublished, see Ref. 4) is similar to Rosenbrock's and will not be described here. 9.4 The simplex method One of the most successful stepping methods in many variables is that of Nelder and Mead <ref> [5] </ref>, based on the simplex. A simplex is an n-dimensional figure specified by giving its n + 1 vertices. It is a triangle in two dimensions, a tetrahedron in three, etc.
Reference: [6] <author> G.W. Stewart, </author> <title> A modification of Davidon's method to accept difference approximations of derivatives, </title> <journal> J. Assoc. Comput. </journal> <volume> Mach 14, </volume> <month> 72 </month> <year> (1967). </year>
Reference-contexts: A complete treatment of step sizes is beyond the scope of these lectures but can be found in a paper by Stewart <ref> [6] </ref>. The numerical evaluation of second derivatives is facilitated by the fact that they should be approximately constant over small regions, so that symmetrical steps are usually not necessary.
Reference: [7] <author> R. Fletcher and C.M. Reeves, </author> <title> Function minimization by conjugate gradients, </title> <journal> Comput. J. </journal> <volume> 7, </volume> <month> 149 </month> <year> (1964). </year>
Reference-contexts: Such vectors become interesting for minimization problems when they are conjugate with respect to the hessian (second derivative) matrix ~ G . In this case a theorem of Fletcher and Reeves <ref> [7] </ref> states that a sequence of linear minimizations in each of the n conjugate directions will minimize a general quadratic function of n variables. That this is true can be seen quite easily as follows. <p> This undesirable asymmetry is largely avoided in a variation due to Powell [8]. 10.6 Conjugate gradients When the first derivatives of the function are calculated, a somewhat more elegant method can be used, known as the method of conjugate gradients <ref> [7] </ref>. Suppose that the function and its gradient are evaluated at two points x 0 and x 1 , giving differences: x = x 1 x 0 58 Chapter 10.
Reference: [8] <author> M.J.D. Powell, </author> <title> An efficient method for finding the minimum of a function of several variables without calculating derivatives Comput. </title> <journal> J. </journal> <volume> 7, </volume> <month> 155 </month> <year> (1964). </year>
Reference-contexts: This undesirable asymmetry is largely avoided in a variation due to Powell <ref> [8] </ref>. 10.6 Conjugate gradients When the first derivatives of the function are calculated, a somewhat more elegant method can be used, known as the method of conjugate gradients [7].
Reference: [9] <author> L.D. Landau and E.M. Lifshitz, </author> <title> The classical theory of fields (Addison-Wesley Publ. </title> <publisher> Co., Inc., </publisher> <address> Reading, Mass., </address> <year> 1951). </year>
Reference-contexts: The inverse ~ V = ~ is a contravariant tensor and becomes the contravariant metric tensor. (For a discussion of covariant and contravariant tensors, see for example chapter 10 of Ref. <ref> [9] </ref>.) This immediately enables us to construct two scalar (invariant under coordinate transformations) quantities: a) ds 2 = dx T G dx is the square of the generalized distance between the point x and the point x + dx.
Reference: [10] <author> R. Fletcher and M.J.D. Powell, </author> <title> A rapidly converging descent method for minimization, </title> <journal> Comput. J. </journal> <volume> 6, </volume> <month> 163 </month> <year> (1963). </year>
Reference-contexts: the starting approximation ~ V 0 and various safeguards against `unreasonable' steps and non-positive-definiteness as for the Newton techniques. 10.8 Davidon's rank-two formula Probably the firstand perhaps still the bestvariable metric method was developed in 1959 by Davidon and later published in simplified form in 1963 by Fletcher and Powell <ref> [10] </ref>. <p> Fletcher and Powell show <ref> [10] </ref> that if the starting approximation to ~ V is positive-definite, then ~ V will remain positive-definite after all updatings, but they have to use the fact that each iteration is a linear minimization, that is g T V 0 g 0 = 0 : It can be shown that this
Reference: [11] <author> W.C. Davidon, </author> <title> Variance algorithm for minimization, </title> <journal> Comput. J. </journal> <volume> 10, </volume> <month> 406 </month> <year> (1968). </year>
Reference-contexts: The rank-one formula 61 10.9 The rank-one formula In an effort to avoid the linear minimizations required by Davidon's algorithm, several workers have independently developed an interesting updating formula of rank one. In this case Davidon in 1968 was the first to publish an algorithm <ref> [11] </ref> based on the formula, and Powell [12] has summarized the properties of this formula and of algorithms based on it The rank-one updating is: ~ V 0 + V 0 fl)(ffi ~ fl T (ffi ~ : It can be shown [12] that this is the only formula of rank <p> Different approaches are possible depending on whether it is considered important to maintain positive definiteness as in the Davidon algorithm <ref> [11] </ref>, or important not to abandon the exact rank-one formula as in Powell's method [12]. 62 Chapter 10.
Reference: [12] <author> M.J.D. Powell, </author> <title> Rank one methods for unconstrained optimization, appearing in Integer and Nonlinear Programming, </title> <editor> J. Adabie editor (North-Holland Publ. </editor> <publisher> Co., </publisher> <address> Amsterdam, </address> <year> 1970). </year>
Reference-contexts: In this case Davidon in 1968 was the first to publish an algorithm [11] based on the formula, and Powell <ref> [12] </ref> has summarized the properties of this formula and of algorithms based on it The rank-one updating is: ~ V 0 + V 0 fl)(ffi ~ fl T (ffi ~ : It can be shown [12] that this is the only formula of rank two (or less) for which not only <p> 1968 was the first to publish an algorithm [11] based on the formula, and Powell <ref> [12] </ref> has summarized the properties of this formula and of algorithms based on it The rank-one updating is: ~ V 0 + V 0 fl)(ffi ~ fl T (ffi ~ : It can be shown [12] that this is the only formula of rank two (or less) for which not only ~ V 1 fl = ffi but: V 1 fl i = ffi i ; where ffi i and fl i are the step and gradient changes at any previous iteration. <p> Different approaches are possible depending on whether it is considered important to maintain positive definiteness as in the Davidon algorithm [11], or important not to abandon the exact rank-one formula as in Powell's method <ref> [12] </ref>. 62 Chapter 10.
Reference: [13] <author> R. Fletcher, </author> <title> A new approach to variable metric algorithms, </title> <journal> Comput. J. </journal> <volume> 13, </volume> <month> 317 </month> <year> (1970). </year>
Reference-contexts: In particular, a paper by Fletcher <ref> [13] </ref> presents a unified approach to VMM, which will be given here. Recall that the rank-one equation is symmetrical (in a sense defined in Section 4.9), but as we shall now see, the rank-two formula is not. <p> Let us introduce the notation ~ T ( ~ V 0 ) f or the rank two f ormula ; and V 1 = ~ V 0 ) f or the dual f ormula ; and consider the class of updating expressions as introduced by Fletcher <ref> [13] </ref>: 10.10. <p> Probably the most important property, and the only one we will consider here, is that of monotonic convergence of ~ V toward the true covariance matrix for a quadratic function. [This is called Property 1 in Fletcher's paper <ref> [13] </ref> which should be consulted for details of the definition and for theorems concerning it.] The use of an updating formula with this property will guarantee an improvement in the approximation ~ V at each iteration (for a quadratic function). <p> The above considerations lead Fletcher to propose a new algorithm <ref> [13] </ref> which is probably the most elegant and powerful of any VMM algorithm. Basically, he uses the general updating formula ~ V , with the value of chosen according to the following scheme: If (rank-one) &lt; 0, set = 0, corresponding to the usual rank-two formula.
Reference: [14] <author> C.G. </author> <title> Broyden, Quasi-Newton methods and their application to function minimization, </title> <journal> Math. Com-put. </journal> <volume> 21, </volume> <month> 368 </month> <year> (1967). </year>
Reference-contexts: Fletcher's unified approach to VMM 63 ~ T + ( ~ where is some parameter which determines the exact formula. [Broyden <ref> [14] </ref>, using a somewhat different notation, has also considered the same class of formulas.] It then turns out that the rank-one formula is also in this class, with (rank one) = ffi fl T ~ : Having now constructed a wide class of updating formulas, which in fact includes all formulas
Reference: [15] <author> I.M. Gelfand and f.L. Tsetlin, </author> <title> The principle of non-local search in automatic optimization systems, </title> <journal> Soviet Phys. Dokl. </journal> <volume> 6, </volume> <month> 192 </month> <year> (1961). </year>
Reference-contexts: Probably the most successful of the ad hoc stepping methods is that of Gelfand <ref> [15] </ref>. It is non-local because it provides a natural way to allow for function increases as well as decreases in any one step, while tending generally to decrease the function value. The procedure is as follows.
Reference: [16] <author> A.A. Goldstein and J.F. Price, </author> <title> On descent from local minima, </title> <journal> Math. </journal> <note> Comput . 25, 569 (1971). </note>
Reference-contexts: This problem of stopping seems to be common to all non-local minimization methods. 12.3 The Goldstein-Price method Goldstein and Price <ref> [16] </ref> have proposed an elegant yet simple method for seeking other local minima after one local minimum has been found It is based on a consideration of the analytic (Taylor series) properties of the function.
Reference: [17] <author> R. Fletcher, </author> <title> Methods for the solution of optimization problems, </title> <journal> Comput. Phys. Commun. </journal> <volume> 3, </volume> <month> 159 </month> <year> (1972). </year>
Reference: [18] <author> M.J.D. Powell, </author> <title> A survey of numerical methods for unconstrained optimization, </title> <journal> SIAM Rev. </journal> <volume> 12, </volume> <month> 79 </month> <year> (1970). </year>
Reference: [19] <author> M.J.D. Powell, </author> <title> A method for minimizing a sum of squares of non-linear functions without calculating derivatives, </title> <journal> Comput. J. </journal> <volume> 7 303 (1965). 69 70 BIBLIOGRAPHY </volume>
Reference: [20] <author> J. Greenstadt, </author> <title> On the relative efficiencies of gradient methods, </title> <journal> Math. Comput. </journal> <volume> 21, </volume> <month> 360 </month> <year> (1967) </year> . 
Reference: [21] <author> R.W.H. Sargent and B.A. Murtaugh, </author> <title> Computational experience with quadratically convergent minimization methods, </title> <journal> Comput. J. </journal> <volume> 13 185 (1970). </volume>
Reference: [22] <author> A.A. Goldstein and J.F. Price, </author> <title> An effective algorithm for minimization, </title> <journal> Num. Math. </journal> <volume> 10, </volume> <month> 184 </month> <year> (1967). </year>
Reference: [23] <author> P.E. Gill, W. Murray and M.H. White, </author> <title> Practical Optimization, </title> <publisher> Academic Press, </publisher> <year> 1981. </year>
References-found: 23

