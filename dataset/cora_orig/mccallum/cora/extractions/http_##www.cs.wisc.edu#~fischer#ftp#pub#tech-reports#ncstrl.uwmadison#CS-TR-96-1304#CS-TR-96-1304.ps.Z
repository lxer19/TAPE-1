URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-96-1304/CS-TR-96-1304.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-96-1304/
Root-URL: http://www.cs.wisc.edu
Title: OPTIMIZATION AND EXECUTION TECHNIQUES FOR QUERIES WITH EXPENSIVE METHODS  
Author: by Joseph M. Hellerstein 
Degree: A dissertation submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy (Computer Sciences) at the  
Date: 1995  
Address: WISCONSIN MADISON  
Affiliation: UNIVERSITY OF  
Abstract-found: 0
Intro-found: 0
Reference: [BC81] <author> P. A. Bernstein and D. W. Chiu. </author> <title> Using Semijoins to Solve Relational Queries. </title> <journal> Journal of the ACM, </journal> <volume> 28(1) </volume> <pages> 25-40, </pages> <year> 1981. </year>
Reference-contexts: Chimenti, et al. [CGK89] note that expensive selection predicates can be viewed as joins between the input relation and an infinite logical relation of input/output pairs for the selection method. In this spirit, one can think of our last optimization as forming the semi-join <ref> [BC81] </ref> of the input relation with the (infinite) Cartesian product of the (infinite) logical relations of the methods, and then re-joining the result of the semi-join with the original relation. This is depicted in Figure 28.
Reference: [BDT83] <author> D. Bitton, D. J. DeWitt, and C. Turbyfill. </author> <title> Benchmarking database systems, a systematic approach. </title> <booktitle> In Proc. 9th International Conference on Very Large Data Bases, </booktitle> <address> Florence, Italy, </address> <month> October </month> <year> 1983. </year>
Reference-contexts: While this technique can provide some indication of the overall effectiveness of different optimization strategies, it is unlikely to provide much insight into the workings of the competing approaches. A third technique is to define an agreed-upon database and set of queries, such as the Wisconsin benchmark <ref> [BDT83] </ref>, AS 3 AP [TOB89], or TPC-D [Raa95]. Such domain-specific benchmarks [Gra91] are often based on models of real-world workloads.
Reference: [Bra84] <author> K. Bratbergsengen. </author> <title> Hashing Methods and Relational Algebra Operations. </title> <booktitle> In Proc. 10th International Conference on Very Large Data Bases, </booktitle> <pages> pages 323-333, </pages> <address> Singapore, </address> <month> August </month> <year> 1984. </year>
Reference-contexts: The next algorithm we present makes this decision unnecessary, since it extends memoization to avoid paging. 4.3 Hybrid Cache The third technique we consider is unary hybrid hashing, which has been used in the past to perform grouping for aggregation or duplicate elimination <ref> [Bra84] </ref>. Unary hybrid hashing is based on the hybrid hash join algorithm [DKO + 84]. We introduce some minor modifications to unary hybrid hashing, and since we are applying it to the problem of caching we call our variant Hybrid Cache.
Reference: [Cat94] <author> R. G. G. Cattell. </author> <title> The Object Database Standard: ODMG-93 (Release 1.1). </title> <publisher> Morgan Kauf-mann, </publisher> <address> San Mateo, CA, </address> <year> 1994. </year>
Reference-contexts: Relational DBMSs have begun to allow simple user-defined data types and functions to be utilized in ad-hoc queries [Pir94]. Simultaneously, Object-Oriented DBMSs have begun to offer ad-hoc query facilities, allowing declarative access to objects and methods that were previously only accessible through hand-coded, imperative applications <ref> [Cat94] </ref>. More recently, Object-Relational DBMSs were developed to unify these supposedly distinct approaches to data management [Ill94, Kim93]. Much of the original research in this area focused on enabling technologies, i.e. system and language designs that make it possible for a DBMS to support extensibility of data types and methods. <p> with non-trivial cost are pulled to the very top of each subplan that is enumerated during the System R algorithm; this is done before the 5 This may change in the future, since most of the OODBMS vendors plan to support the OQL query language, which includes facilities for joins <ref> [Cat94] </ref>. 44 System R algorithm chooses which subplans to keep and which to prune. The result is equivalent to removing the expensive predicates from the query, generating an optimal plan for the modified query, and then pasting the expensive predicates onto the top of that plan.
Reference: [CDKN94] <author> Michael J. Carey, David J. DeWitt, Chander Kant, and Jeffrey F. Naughton. </author> <title> A Status Report on the OO7 OODBMS Benchmarking Effort. </title> <booktitle> In Proc. Conference on Object-Oriented Programming Systems, Languages, and Applications, </booktitle> <pages> pages 414-426, </pages> <address> Portland, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: Unfortunately, commercial database systems typically have a clause in their license agreements that prohibits the release of performance numbers. It is unusual for a database 7 vendor to permit publication of any performance results at all, relative or otherwise <ref> [CDKN94] </ref>.
Reference: [CGK89] <author> D. Chimenti, R. Gamboa, and R. Krishnamurthy. </author> <title> Towards an Open Architecture for LDL. </title> <booktitle> In Proc. 15th International Conference on Very Large Data Bases, </booktitle> <pages> pages 195-203, </pages> <address> Amsterdam, </address> <month> August </month> <year> 1989. </year>
Reference-contexts: The first approach was pioneered in the LDL logic database system <ref> [CGK89] </ref>, and was proposed for extensible relational systems by Yajima, et al. [YKY + 91]. We refer to this as the LDL approach. The other approach is Predicate Migration, which was first presented in 1992 [Hel92]. Neither algorithm actually produces optimal plans in all scenarios. <p> Chimenti, et al. <ref> [CGK89] </ref> note that expensive selection predicates can be viewed as joins between the input relation and an infinite logical relation of input/output pairs for the selection method. <p> E. Smith [Smi56], and do not address the problems that arise with joins, including the issues of constraints on ordering and integrating the orderings of multiple streams. 88 The notion of expensive selections was considered in the context of the LDL logic programming system <ref> [CGK89] </ref>. Their solution was to model a selection on relation R as a join between R and a virtual relation of infinite cardinality containing the entire logical predicate of the selection. By modeling selections as joins, they were able to use a join-based query optimizer to order all predicates appropriately. <p> Is our roughness in estimation required for a polynomial-time predicate placement algorithm, or is there a way to weaken the assumptions and still develop an efficient algorithm? This question can be cast in different terms. If one combines the LDL approach for expensive methods <ref> [CGK89] </ref> with the IK-KBZ rank-based join optimizer [IK84, KBZ86], one gets a polynomial-time optimization algorithm that handles expensive selections. However as we pointed out in Section 3.2 this technique fails because IK-KBZ does not handle bushy join trees.
Reference: [CS93] <author> S. Chaudhuri and K. Shim. </author> <title> Query Optimization in the Presence of Foreign Functions. </title> <booktitle> Proc. 19th International Conference on Very Large Data Bases, </booktitle> <pages> pages 526-541, </pages> <address> Dublin, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: Neither system considered pulling up subquery predicates from their lowest eligible position [LH93]. An orthogonal issue related to predicate placement is the problem of rewriting predicates into more efficient forms <ref> [CS93, CYY + 92] </ref>. In such semantic optimization work, the focus is on rewriting expensive predicates in terms of other, cheaper predicates. Another semantic rewriting technique called "Predicate Move-Around" [LMS94] suggests rewriting SQL queries by copying predicates and then moving the copies across query blocks.
Reference: [CYY + 92] <author> H. Chen, X. Yu, K. Yamaguchi, H. Kitagawa, N. Ohbo, and Y. Fujiwara. </author> <title> Decomposition | An Approach for Optimizing Queries Including ADT Functions. </title> <journal> Information Processing Letters, </journal> <volume> 43(6) </volume> <pages> 327-333, </pages> <year> 1992. </year>
Reference-contexts: Neither system considered pulling up subquery predicates from their lowest eligible position [LH93]. An orthogonal issue related to predicate placement is the problem of rewriting predicates into more efficient forms <ref> [CS93, CYY + 92] </ref>. In such semantic optimization work, the focus is on rewriting expensive predicates in terms of other, cheaper predicates. Another semantic rewriting technique called "Predicate Move-Around" [LMS94] suggests rewriting SQL queries by copying predicates and then moving the copies across query blocks.
Reference: [Dat83] <author> C. J. Date. </author> <title> The Outer Join. </title> <booktitle> In Proceedings of the Second International Conference on Databases, </booktitle> <address> Cambridge, England, </address> <month> September </month> <year> 1983. </year>
Reference-contexts: Query processing over persistent caches is relatively simple: one performs a join of the input relation and the persistent cache by using a standard join algorithm. Similar techniques can be used for avoiding recomputation of common relational subexpressions [Sel88]. If the cache is incomplete, an outer join <ref> [Dat83] </ref> may be used | method inputs that are not found in the cache are "preserved" by the outer join, and for these inputs the method must be computed later in 90 the query.
Reference: [Day87] <author> U. Dayal. </author> <title> Of Nests and Trees: A Unified Approach to Processing Queries that Contain Nested Subqueries, Aggregates, and Quantifiers. </title> <booktitle> In Proc. 13th International Conference on Very Large Data Bases, </booktitle> <pages> pages 197-208, </pages> <address> Brighton, </address> <month> September </month> <year> 1987. </year>
Reference-contexts: The cost of a subquery predicate may be lowered by transforming it to another subquery predicate [LDH + 84], and by "early stop" techniques, which stop materializing or scanning a subquery as soon as the predicate can be resolved <ref> [Day87] </ref>.
Reference: [DKO + 84] <author> D. J. DeWitt, R. H. Katz, F. Olken, L. D. Shapiro, M. R. Stonebraker, and D. Wood. </author> <title> Implementation Techniques for Main Memory Database Systems. </title> <booktitle> In Proc. ACM-SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 1-8, </pages> <address> Boston, </address> <month> June </month> <year> 1984. </year>
Reference-contexts: Unary hybrid hashing is based on the hybrid hash join algorithm <ref> [DKO + 84] </ref>. We introduce some minor modifications to unary hybrid hashing, and since we are applying it to the problem of caching we call our variant Hybrid Cache. Our modifications to standard unary hybrid hashing are discussed further in Section 4.3.2. <p> This is a well-known constraint on both hashing and sorting, which can be overcome by recursive application of either partitioning (for hashing) or merging (for sorting) <ref> [DKO + 84, Knu73] </ref>. The number of input values v can be estimated by using stored statistics [SAC + 79] or via sampling [HOT88, HNSS95]. Unfortunately this estimation is subject to error, so we must consider how the algorithm will behave if estimates are imperfect.
Reference: [DKS92] <author> W. Du, R. Krishnamurthy, and M. Shan. </author> <title> Query Optimization in Heterogeneous DBMS. </title> <booktitle> In Proc. 18th International Conference on Very Large Data Bases, </booktitle> <pages> pages 277-291, </pages> <address> Vancouver, </address> <month> August </month> <year> 1992. </year> <month> 99 </month>
Reference-contexts: These "linear" cost models have been evaluated experimentally for nested-loop and merge joins, and were found to be relatively accurate estimates of the performance of a variety of commercial systems <ref> [DKS92] </ref>. 3.3 Predicate Placement Schemes, and The Queries They Optimize In this section we analyze four algorithms for handling expensive predicate placement, each of which can be easily integrated into a System R-style query optimizer.
Reference: [Doz92] <author> J. Dozier. </author> <title> Access to Data in NASA's Earth Observing System. </title> <booktitle> In Proc. ACM-SIGMOD International Conference on Management of Data, </booktitle> <address> San Diego, page 1, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: Another benchmark that would be helpful would be a domain-specific benchmark with expensive predicates from real-world applications such as Asset Management [Ols95] or Remote Sensing <ref> [Doz92] </ref>; of course this would be most convincing if it were agreed upon by the users in these domains.
Reference: [FK94] <author> C. Faloutsos and I. Kamel. </author> <title> Beyond Uniformity and Independence: Analysis of R-trees Using the Concept of Fractal Dimension. </title> <booktitle> In Proc. 13th ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems, </booktitle> <pages> pages 4-13, </pages> <address> Minneapolis, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: Accurate selectivity estimation is a difficult problem in query optimization, and has generated increasing interest in recent years <ref> [IP95, FK94, IC91] </ref>. In Illustra, selectivity estimation for user-defined methods can be controlled through the selfunc flag of the create function command [Ill94].
Reference: [FM85] <author> P. Flajolet and G. N. Martin. </author> <title> Probabilistic Counting Algorithms for Data Base Applications. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 31, </volume> <year> 1985. </year>
Reference-contexts: The total costs of methods in a query plan with caching can be computed by multiplying the differential cost of the method times the number of distinct values in its input. Estimating the number of values can be difficult in general, though many approaches have been proposed <ref> [SAC + 79, FM85, HNSS95] </ref>. Predicate Migration and related heuristics are typically used to place expensive predicates in a query plan. However, the estimates for Predicate Migration presented in Section 2.1 do not handle 84 the implications of caching. <p> This is done by multiplying the differential method cost times the ratio of values to tuples in a stream; this ratio can be computed quite accurately per column (or set of columns) of the base relations via system statistics <ref> [SAC + 79, FM85] </ref>. We assume that this ratio remains constant throughout the query plan, i.e. that any predicate of selectivity s reduces the number of values in each column of its input by a factor of 1 s.
Reference: [Fre95] <author> J. </author> <title> Frew. </title> <type> Personal correspondence, </type> <month> July </month> <year> 1995. </year>
Reference-contexts: veg (raster) &gt; 20; In this example, the method veg reads in 16 megabytes of raster image data (infrared and visual data from a satellite), and counts the percentage of pixels that have the characteristics of vegetation (these characteristics are computed per pixel using a standard technique in remote sensing <ref> [Fre95] </ref>.) The veg method is very time-consuming, taking many thousands of instructions and I/O operations to compute. It should be clear that the query will run faster if the selection rtime = 1 is applied before the veg selection, since doing so minimizes the number of calls to veg.
Reference: [Gra91] <author> J. Gray, </author> <title> editor. The Benchmark Handbook: For Database and Transaction Processing Systems. </title> <publisher> Morgan-Kaufmann Publishers, Inc., </publisher> <year> 1991. </year>
Reference-contexts: A third technique is to define an agreed-upon database and set of queries, such as the Wisconsin benchmark [BDT83], AS 3 AP [TOB89], or TPC-D [Raa95]. Such domain-specific benchmarks <ref> [Gra91] </ref> are often based on models of real-world workloads. In terms of optimization, such benchmarks typically expose whether or not a system implements solutions to important details exposed by the benchmark, e.g. use of indices and reordering of joins in the Wisconsin benchmark, or intelligent rewriting of sub-queries in TPC-D.
Reference: [Gra93] <author> G. Graefe. </author> <title> Query Evaluation Techniques for Large Databases. </title> <journal> ACM Computing Surveys, </journal> <volume> 25(2) </volume> <pages> 73-170, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: More effective (though somewhat complex) solutions to the problems of hybrid hash join have been proposed by Nakayama, et al. [NKT88]. 4.3.4 Sort vs. Hash Revisited In analyzing hashing and sorting, Graefe presents the interesting result that hash-based algorithms typically have "dual" sorting algorithms that perform comparably <ref> [Gra93] </ref>. However, we observe in this section that one of his dualities is based on an assumption that does not apply to the problem of caching. <p> Alternate techniques can be used for caching methods in a persistent manner to be reused across multiple queries over a period of time (for methods with per-transaction or infinite cache lives). Graefe provides an annotated bibliography of these ideas in Section 12.1 of his query processing survey <ref> [Gra93] </ref>. These persistent caches are akin to materialized views or method indices | from the point of view of a single query, they represent (partial or complete) precomputation of methods, rather than caches.
Reference: [Gra95] <author> J. Gray. </author> <type> Personal correspondence, </type> <month> July </month> <year> 1995. </year>
Reference-contexts: Many commercial implementors are wary of modifying their optimizers, because their experience is that queries that used to work well before the modification may run differently, poorly, or not at all afterwards. This can be very upsetting to customers <ref> [Loh95, Gra95] </ref>. Predicate Migration has no effect on the way that the optimizer handles queries without expensive predicates. In this sense it is entirely safe to implement Predicate Migration in systems that newly support expensive predicates; no old plans will be changed as a result of the implementation.
Reference: [Han77] <author> M. Z. Hanani. </author> <title> An Optimal Evaluation of Boolean Expressions in an Online Query System. </title> <journal> Communications of the ACM, </journal> <volume> 20(5) </volume> <pages> 344-347, </pages> <month> may </month> <year> 1977. </year>
Reference-contexts: Furthermore, their schemes are a proposal for a completely new method for query optimization, not an extension that can be applied to the plans of any query optimizer. Various papers in the Artificial Intelligence literature <ref> [Han77, Nat87, Smi89] </ref> use metrics similar to rank to determine evaluation order of AND/OR trees in logic programming systems. These papers are applications of the early Operations Research work of W. E.
Reference: [Hel92] <author> J. M. Hellerstein. </author> <title> Predicate Migration: Optimizing Queries With Expensive Predicates. </title> <type> Technical Report Sequoia 2000 92/13, </type> <institution> University of California, Berkeley, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: The first approach was pioneered in the LDL logic database system [CGK89], and was proposed for extensible relational systems by Yajima, et al. [YKY + 91]. We refer to this as the LDL approach. The other approach is Predicate Migration, which was first presented in 1992 <ref> [Hel92] </ref>. Neither algorithm actually produces optimal plans in all scenarios. <p> This can easily be shown to be the optimal ordering for a disjunction of expensive selections; the analysis is similar to the proof of Lemma 1 of Section 2.2.1. The optimization work in this dissertation has been previously published in earlier forms <ref> [Hel92, HS93a, Hel94] </ref>. 5.2 Method and Subquery Caching Chapter 4 focusses on techniques for caching method results while computing a single query language statement.
Reference: [Hel94] <author> J. M. Hellerstein. </author> <title> Practical Predicate Placement. </title> <booktitle> In Proc. ACM-SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 325-335, </pages> <address> Minneapolis, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: This can easily be shown to be the optimal ordering for a disjunction of expensive selections; the analysis is similar to the proof of Lemma 1 of Section 2.2.1. The optimization work in this dissertation has been previously published in earlier forms <ref> [Hel92, HS93a, Hel94] </ref>. 5.2 Method and Subquery Caching Chapter 4 focusses on techniques for caching method results while computing a single query language statement.
Reference: [HFLP89] <author> L. M. Haas, J. C. Freytag, G. M. Lohman, and H. Pirahesh. </author> <title> Extensible Query Processing in Starburst. </title> <booktitle> In Proc. ACM-SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 377-388, </pages> <address> Portland, </address> <month> May-June </month> <year> 1989. </year>
Reference-contexts: The hope was that the basic mechanisms of an RDBMS to support selection, projection, join and aggregation queries could remain the same. This hope was largely borne out by the early extensible research systems such as POSTGRES [SK91] and Starburst <ref> [HFLP89] </ref>.
Reference: [HNSS95] <author> P. J. Haas, J. F. Naughton, S. Seshadri, and L. </author> <title> Stokes. Sampling-Based Estimation of the Number of Distinct Values of an Attribute. </title> <booktitle> In Proc. 21st International Conference on Very Large Data Bases, </booktitle> <address> Zurich, </address> <month> September </month> <year> 1995. </year>
Reference-contexts: Given an expensive method over a large relation, a good choice between memoization and sorting depends on the number of values in the input to the method: for few values, memoization is preferable, while for many values, sorting is preferable. Since estimating the number of values can be difficult <ref> [HNSS95] </ref>, choosing between memoization and sorting can be tricky. <p> This is a well-known constraint on both hashing and sorting, which can be overcome by recursive application of either partitioning (for hashing) or merging (for sorting) [DKO + 84, Knu73]. The number of input values v can be estimated by using stored statistics [SAC + 79] or via sampling <ref> [HOT88, HNSS95] </ref>. Unfortunately this estimation is subject to error, so we must consider how the algorithm will behave if estimates are imperfect. If v is estimated too high, h will be too small | i.e., memory will be underutilized during the first two phases of Hybrid Cache. <p> The total costs of methods in a query plan with caching can be computed by multiplying the differential cost of the method times the number of distinct values in its input. Estimating the number of values can be difficult in general, though many approaches have been proposed <ref> [SAC + 79, FM85, HNSS95] </ref>. Predicate Migration and related heuristics are typically used to place expensive predicates in a query plan. However, the estimates for Predicate Migration presented in Section 2.1 do not handle 84 the implications of caching.
Reference: [HOT88] <author> W. Hou, G. Ozsoyoglu, and B. K. Taneja. </author> <title> Statistical Estimators for Relational Algebra Expressions. </title> <booktitle> In Proc. 7th ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems, </booktitle> <pages> pages 276-287, </pages> <address> Austin, </address> <month> March </month> <year> 1988. </year>
Reference-contexts: is, for any predicate p (join or selection) they estimate the value selectivity (p) = cardinality (output (p)) cardinality (input (p)) : Typically these estimations are based on default values and statistics stored by the DBMS [SAC + 79], although recent work suggests that inexpensive sampling techniques can be used <ref> [LNSS93, HOT88] </ref>. 10 flag name description percall cpu execution time per invocation, regardless of argument size perbyte cpu execution time per byte of arguments byte pct percentage of argument bytes that the method needs to access Table 1: Method expense parameters in Illustra. <p> This is a well-known constraint on both hashing and sorting, which can be overcome by recursive application of either partitioning (for hashing) or merging (for sorting) [DKO + 84, Knu73]. The number of input values v can be estimated by using stored statistics [SAC + 79] or via sampling <ref> [HOT88, HNSS95] </ref>. Unfortunately this estimation is subject to error, so we must consider how the algorithm will behave if estimates are imperfect. If v is estimated too high, h will be too small | i.e., memory will be underutilized during the first two phases of Hybrid Cache.
Reference: [HS93a] <author> J. M. Hellerstein and M. Stonebraker. </author> <title> Predicate Migration: Optimizing Queries With Expensive Predicates. </title> <booktitle> In Proc. ACM-SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 267-276, </pages> <address> Washington, D.C., </address> <month> May </month> <year> 1993. </year>
Reference-contexts: In the course of running the comparisons for this paper, a variety of subtle optimizer bugs were found, mostly in cost and selectivity estimation. The most difficult to uncover was that the original "global" cost model for Predicate Migration, presented in <ref> [HS93a] </ref>, was inaccurate in practice. Typically, bugs were exposed by running the same query under the various different optimization heuristics, and comparing the estimated costs and actual running times of the resulting plans. <p> This can easily be shown to be the optimal ordering for a disjunction of expensive selections; the analysis is similar to the proof of Lemma 1 of Section 2.2.1. The optimization work in this dissertation has been previously published in earlier forms <ref> [Hel92, HS93a, Hel94] </ref>. 5.2 Method and Subquery Caching Chapter 4 focusses on techniques for caching method results while computing a single query language statement.
Reference: [HS93b] <author> W. Hong and M. Stonebraker. </author> <title> Optimization of Parallel Query Execution Plans in XPRS. Distributed and Parallel Databases, </title> <journal> An International Journal, </journal> <volume> 1(1) </volume> <pages> 9-32, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: Typically this is done with a randomized benchmark generator, which produces both a random database and random queries (e.g., [SG88], [Swa89], [IK90], [SI92], <ref> [HS93b] </ref>, etc.) This approach is somewhat analogous to comparing multiple C++ compilers by compiling a set of randomly generated programs with each compiler, and running the resulting executables on a set of randomly generated inputs. <p> In the course of chapter, we will be using the performance of SQL queries run in Illustra to demonstrate the strengths and limitations of the algorithms. The database schema for these queries is based on the randomized benchmark of Hong and Stonebraker <ref> [HS93b] </ref>, with the cardinality distribution scaled up by a factor of 10. All tuples contain 100 bytes of user data. The tables are named with numbers in ascending order of cardinality; this will prove important in the analysis below.
Reference: [IC91] <author> Y. Ioannidis and S. Christodoulakis. </author> <title> On the Propagation of Errors in the Size of Join Results. </title> <booktitle> In Proc. ACM-SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 268-277, </pages> <address> Denver, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Accurate selectivity estimation is a difficult problem in query optimization, and has generated increasing interest in recent years <ref> [IP95, FK94, IC91] </ref>. In Illustra, selectivity estimation for user-defined methods can be controlled through the selfunc flag of the create function command [Ill94]. <p> Optimizers choose plans from an enormous search space, and within that search space plans can vary in performance by orders of magnitude. In addition, optimization decisions are based on selectivity and cost estimations that are often erroneous <ref> [IC91, IP95] </ref>.
Reference: [IK84] <author> T. Ibaraki and T. Kameda. </author> <title> Optimal Nesting for Computing N-relational Joins. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 9(3) </volume> <pages> 482-502, </pages> <month> October </month> <year> 1984. </year>
Reference-contexts: This does not integrate well with a System R-style optimization algorithm, however, since LDL increases the number of joins to order, and System R's complexity is exponential in the number of joins. Thus [KZ88] proposes using the polynomial-time IK-KBZ <ref> [IK84, KBZ86] </ref> approach for optimizing the join order. Unfortunately, both the System R and IK-KBZ optimization algorithms consider only left-deep plan trees, and no left-deep plan tree can model the optimal plan tree of Figure 5. <p> The questions posed by Stonebraker are directly addressed in this dissertation, although we vary slightly in the definition of cost metrics for expensive functions. Ibaraki and Kameda <ref> [IK84] </ref>, Krishnamurthy, Boral and Zaniolo [KBZ86], and Swami and Iyer [SI92] have developed and refined a query optimization scheme that is built on the the notion of rank. However, their scheme uses rank to reorder joins rather than selections. <p> If one combines the LDL approach for expensive methods [CGK89] with the IK-KBZ rank-based join optimizer <ref> [IK84, KBZ86] </ref>, one gets a polynomial-time optimization algorithm that handles expensive selections. However as we pointed out in Section 3.2 this technique fails because IK-KBZ does not handle bushy join trees.
Reference: [IK90] <author> Y. E. Ioannidis and Y. C. Kang. </author> <title> Randomized Algorithms for Optimizing Large Join Queries. </title> <booktitle> In Proc. ACM-SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 312-321, </pages> <address> Atlantic City, </address> <month> May </month> <year> 1990. </year> <month> 100 </month>
Reference-contexts: Typically this is done with a randomized benchmark generator, which produces both a random database and random queries (e.g., [SG88], [Swa89], <ref> [IK90] </ref>, [SI92], [HS93b], etc.) This approach is somewhat analogous to comparing multiple C++ compilers by compiling a set of randomly generated programs with each compiler, and running the resulting executables on a set of randomly generated inputs.
Reference: [Ill94] <institution> Illustra Information Technologies, Inc. </institution> <note> Illustra User's Guide, Illustra Server Release 2.1, </note> <month> June </month> <year> 1994. </year>
Reference-contexts: Simultaneously, Object-Oriented DBMSs have begun to offer ad-hoc query facilities, allowing declarative access to objects and methods that were previously only accessible through hand-coded, imperative applications [Cat94]. More recently, Object-Relational DBMSs were developed to unify these supposedly distinct approaches to data management <ref> [Ill94, Kim93] </ref>. Much of the original research in this area focused on enabling technologies, i.e. system and language designs that make it possible for a DBMS to support extensibility of data types and methods. Considerably less research explored the problem of making this new functionality efficient. <p> Accurate selectivity estimation is a difficult problem in query optimization, and has generated increasing interest in recent years [IP95, FK94, IC91]. In Illustra, selectivity estimation for user-defined methods can be controlled through the selfunc flag of the create function command <ref> [Ill94] </ref>. In this dissertation we make the standard assumptions of most query optimization algorithms, namely that estimates are accurate and predicates have independent selectivities. 2.1.2 Differential Cost of User-Defined Methods In an extensible system such as Illustra, arbitrary user-defined methods may be introduced into both selection and join predicates. <p> SQL3's create function command allows methods to be declared variant or not variant, with variant as the default. A variant function is uncacheable; a function that is not variant is assumed to have infinite cache life. SQL3 makes no syntactic provisions for identifying per-statement or per-transaction cache lives <ref> [ISO94, Ill94] </ref>. 4.2 Two Traditional Techniques This section presents two previously proposed techniques for caching the results of expensive methods. Each of these algorithms (as well as the hybrid hashing algorithm below) can be used for expensive methods that appear anywhere in a query.
Reference: [IP95] <author> Y. Ioannidis and V. Poosala. </author> <title> Balancing Histogram Optimality and Practicality for Query Result Size Estimation. </title> <booktitle> In Proc. ACM-SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 233-244, </pages> <address> San Jose, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: Accurate selectivity estimation is a difficult problem in query optimization, and has generated increasing interest in recent years <ref> [IP95, FK94, IC91] </ref>. In Illustra, selectivity estimation for user-defined methods can be controlled through the selfunc flag of the create function command [Ill94]. <p> Optimizers choose plans from an enormous search space, and within that search space plans can vary in performance by orders of magnitude. In addition, optimization decisions are based on selectivity and cost estimations that are often erroneous <ref> [IC91, IP95] </ref>.
Reference: [ISO93] <author> ISO ANSI. </author> <title> Database Language SQL ISO/IEC 9075:1992, </title> <year> 1993. </year>
Reference-contexts: The latter part of this dissertation explores a variety of techniques for efficiently avoiding redundant computation of methods. 1.4 Benefits for RDBMS: Subqueries It is important to note that expensive methods do not exist only in next-generation Object-Relational DBMSs. Current relational languages, such as the industry standard, SQL <ref> [ISO93] </ref>, have long supported expensive predicate methods in the guise of subquery predicates. A subquery predicate is one of the form expression operator query.
Reference: [ISO94] <author> ISO. </author> <title> ISO ANSI Working Draft: Database Language SQL (SQL3) TC X3H2-94-331; ISO/IEC JTC1/SC21/WG3, </title> <year> 1994. </year>
Reference-contexts: SQL3's create function command allows methods to be declared variant or not variant, with variant as the default. A variant function is uncacheable; a function that is not variant is assumed to have infinite cache life. SQL3 makes no syntactic provisions for identifying per-statement or per-transaction cache lives <ref> [ISO94, Ill94] </ref>. 4.2 Two Traditional Techniques This section presents two previously proposed techniques for caching the results of expensive methods. Each of these algorithms (as well as the hybrid hashing algorithm below) can be used for expensive methods that appear anywhere in a query.
Reference: [JK84] <author> M. Jarke and J. Koch. </author> <title> Query Optimization in Database Systems. </title> <journal> ACM Computing Surveys, </journal> <volume> 16(2) </volume> <pages> 111-152, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: The scaling of results in this dissertation does not affect the conclusions drawn from the experiments, which are based on the relative performance of various approaches. 8 Chapter 2 Optimization: Theory The field of query optimization has been explored in great detail in the relational database literature (see <ref> [JK84] </ref> for a survey.) The traditional focus of relational query optimization schemes has been on the choice of join methods and join orders. Selections have typically been handled in query optimizers by "predicate pushdown" rules, which apply selections in some arbitrary order before as many joins as possible.
Reference: [KBZ86] <author> R. Krishnamurthy, H. Boral, and C. Zaniolo. </author> <title> Optimization of Nonrecursive Queries. </title> <booktitle> In Proc. 12th International Conference on Very Large Data Bases, </booktitle> <pages> pages 128-137, </pages> <address> Kyoto, </address> <month> August </month> <year> 1986. </year>
Reference-contexts: This does not integrate well with a System R-style optimization algorithm, however, since LDL increases the number of joins to order, and System R's complexity is exponential in the number of joins. Thus [KZ88] proposes using the polynomial-time IK-KBZ <ref> [IK84, KBZ86] </ref> approach for optimizing the join order. Unfortunately, both the System R and IK-KBZ optimization algorithms consider only left-deep plan trees, and no left-deep plan tree can model the optimal plan tree of Figure 5. <p> The questions posed by Stonebraker are directly addressed in this dissertation, although we vary slightly in the definition of cost metrics for expensive functions. Ibaraki and Kameda [IK84], Krishnamurthy, Boral and Zaniolo <ref> [KBZ86] </ref>, and Swami and Iyer [SI92] have developed and refined a query optimization scheme that is built on the the notion of rank. However, their scheme uses rank to reorder joins rather than selections. <p> If one combines the LDL approach for expensive methods [CGK89] with the IK-KBZ rank-based join optimizer <ref> [IK84, KBZ86] </ref>, one gets a polynomial-time optimization algorithm that handles expensive selections. However as we pointed out in Section 3.2 this technique fails because IK-KBZ does not handle bushy join trees.
Reference: [Kim93] <author> W. Kim. </author> <title> Object-Oriented Database Systems: Promises, Reality, and Future. </title> <booktitle> In Proc. 19th International Conference on Very Large Data Bases, </booktitle> <pages> pages 676-687, </pages> <address> Dublin, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: Simultaneously, Object-Oriented DBMSs have begun to offer ad-hoc query facilities, allowing declarative access to objects and methods that were previously only accessible through hand-coded, imperative applications [Cat94]. More recently, Object-Relational DBMSs were developed to unify these supposedly distinct approaches to data management <ref> [Ill94, Kim93] </ref>. Much of the original research in this area focused on enabling technologies, i.e. system and language designs that make it possible for a DBMS to support extensibility of data types and methods. Considerably less research explored the problem of making this new functionality efficient.
Reference: [KMPS94] <author> A. Kemper, G. Moerkotte, K. Peithner, and M. Steinbrunn. </author> <title> Optimizing Disjunctive Queries with Expensive Predicates. </title> <booktitle> In Proc. ACM-SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 336-347, </pages> <address> Minneapolis, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: It is important to note 89 that these issues are indeed orthogonal to our problems of predicate placement | once queries have been rewritten into cheaper forms, they still need to have their predicates optimally placed into a query plan. Kemper, Steinbrunn, et al. <ref> [KMPS94, SPMK95] </ref> address the problem of planning disjunctive queries with expensive predicates; their work is not easily integrated with a traditional (conjunct-based) optimizer. Like most System R-based optimizers, Illustra focusses on Boolean factors (conjuncts). Within Boolean factors, the operands of OR are ordered by the metric cost/selectivity.
Reference: [Knu73] <author> D. E. Knuth. </author> <title> Sorting and Searching, </title> <booktitle> volume 3 of The Art of Computer Programming. </booktitle> <publisher> Addison-Wesley Publishing Co., </publisher> <year> 1973. </year>
Reference-contexts: This is a well-known constraint on both hashing and sorting, which can be overcome by recursive application of either partitioning (for hashing) or merging (for sorting) <ref> [DKO + 84, Knu73] </ref>. The number of input values v can be estimated by using stored statistics [SAC + 79] or via sampling [HOT88, HNSS95]. Unfortunately this estimation is subject to error, so we must consider how the algorithm will behave if estimates are imperfect. <p> Graefe's analysis of the situation leads him to a modification of sorting via replacement selection <ref> [Knu73] </ref>, which allows sort-based grouping to utilize an amount of memory closer to the cardinality of its output rather than that of its input.
Reference: [KZ88] <author> R. Krishnamurthy and C. Zaniolo. </author> <title> Optimization in a Logic Based Language for Knowledge and Data Intensive Applications. </title> <editor> In J. W. Schmidt, S. Ceri, and M. Missikoff, editors, </editor> <booktitle> Proc. International Conference on Extending Data Base Technology, Advances in Database Technology - EDBT '88. Lecture Notes in Computer Science, </booktitle> <volume> Volume 303, </volume> <pages> Venice, </pages> <address> March 1988. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: At this point, the LDL approach applies a traditional join-ordering optimizer to plan the rewritten query. This does not integrate well with a System R-style optimization algorithm, however, since LDL increases the number of joins to order, and System R's complexity is exponential in the number of joins. Thus <ref> [KZ88] </ref> proposes using the polynomial-time IK-KBZ [IK84, KBZ86] approach for optimizing the join order. Unfortunately, both the System R and IK-KBZ optimization algorithms consider only left-deep plan trees, and no left-deep plan tree can model the optimal plan tree of Figure 5.
Reference: [LDH + 84] <author> G. M. Lohman, D. Daniels, L. M. Haas, R. Kistler, and P. G. Selinger. </author> <title> Optimization of Nested Queries in a Distributed Relational Database. </title> <booktitle> In Proc. 10th International Conference on Very Large Data Bases, </booktitle> <pages> pages 403-415, </pages> <address> Singapore, </address> <month> August </month> <year> 1984. </year>
Reference-contexts: The cost estimates presented here for query language methods form a simple model and raise some issues in setting costs for subqueries. The cost of a subquery predicate may be lowered by transforming it to another subquery predicate <ref> [LDH + 84] </ref>, and by "early stop" techniques, which stop materializing or scanning a subquery as soon as the predicate can be resolved [Day87].
Reference: [LH93] <author> G M. Lohman and L. M. Haas. </author> <type> Personal correspondence, </type> <month> November </month> <year> 1993. </year>
Reference-contexts: Neither system considered pulling up subquery predicates from their lowest eligible position <ref> [LH93] </ref>. An orthogonal issue related to predicate placement is the problem of rewriting predicates into more efficient forms [CS93, CYY + 92]. In such semantic optimization work, the focus is on rewriting expensive predicates in terms of other, cheaper predicates.
Reference: [LMS94] <author> A. Y. Levy, I. S. Mumick, and Y. Sagiv. </author> <title> Query Optimization by Predicate Move-Around. </title> <booktitle> In Proc. 20th International Conference on Very Large Data Bases, </booktitle> <pages> pages 96-107, </pages> <address> Santiago, </address> <month> September </month> <year> 1994. </year>
Reference-contexts: An orthogonal issue related to predicate placement is the problem of rewriting predicates into more efficient forms [CS93, CYY + 92]. In such semantic optimization work, the focus is on rewriting expensive predicates in terms of other, cheaper predicates. Another semantic rewriting technique called "Predicate Move-Around" <ref> [LMS94] </ref> suggests rewriting SQL queries by copying predicates and then moving the copies across query blocks. The authors conjecture that this technique may be beneficial for queries with expensive predicates. <p> For example, how can Predicate Migration be applied to recursive queries and common subexpressions? When should predicates be transferred across query blocks? When should they be duplicated across query blocks, as sketched by Levy, et al. <ref> [LMS94] </ref>? These questions will require significant effort to answer, since there are no generally accepted techniques for cost-based optimization in these scenarios, even for queries without expensive predicates. As noted in Chapter 3, the problem of benchmarking optimization schemes for expensive methods remains open.
Reference: [LNSS93] <author> R. J. Lipton, J. F. Naughton, D. A. Schneider, and S. Seshadri. </author> <title> Efficient Sampling Strategies for Relational Database Operations. </title> <journal> Theoretical Computer Science, </journal> (116):195-226, 1993. 
Reference-contexts: is, for any predicate p (join or selection) they estimate the value selectivity (p) = cardinality (output (p)) cardinality (input (p)) : Typically these estimations are based on default values and statistics stored by the DBMS [SAC + 79], although recent work suggests that inexpensive sampling techniques can be used <ref> [LNSS93, HOT88] </ref>. 10 flag name description percall cpu execution time per invocation, regardless of argument size perbyte cpu execution time per byte of arguments byte pct percentage of argument bytes that the method needs to access Table 1: Method expense parameters in Illustra.
Reference: [Loh95] <author> G. M. Lohman. </author> <type> Personal correspondence, </type> <month> July </month> <year> 1995. </year>
Reference-contexts: Many commercial implementors are wary of modifying their optimizers, because their experience is that queries that used to work well before the modification may run differently, poorly, or not at all afterwards. This can be very upsetting to customers <ref> [Loh95, Gra95] </ref>. Predicate Migration has no effect on the way that the optimizer handles queries without expensive predicates. In this sense it is entirely safe to implement Predicate Migration in systems that newly support expensive predicates; no old plans will be changed as a result of the implementation.
Reference: [LS88] <author> C. Lynch and M. Stonebraker. </author> <title> Extended User-Defined Indexing with Application to Textual Databases. </title> <booktitle> In Proc. 14th International Conference on Very Large Data Bases, </booktitle> <pages> pages 306-317, </pages> <address> Los Angeles, </address> <month> August-September </month> <year> 1988. </year>
Reference-contexts: tuples is e = e p 1 t + s p 1 e p 2 t + : : : + s p 1 s p 2 s p n1 e p n t: 3 It is possible to index tables on method values as well as on table attributes <ref> [MS86, LS88] </ref>. If a scan is done on such a "method" index, then predicates over the method may be satisfied during the scan without invoking the method.
Reference: [Mic68] <author> D. Michie. </author> <title> "Memo" Functions and Machine Learning. </title> <journal> Nature, </journal> (218):19-22, April 1968. <volume> 101 </volume>
Reference-contexts: In the programming language and logic programming literature, this technique is often referred to as memoization <ref> [Mic68] </ref>. The algorithm is sketched in Figure 15. For our advertisements example, this hashtable would be keyed on a hash function of product image values (which might be object identifiers or file names), and would store (product image,thumbnail) pairs.
Reference: [ML86a] <author> L. F. Mackert and G. M. Lohman. </author> <title> R* Optimizer Validation and Performance Evaluation for Local Queries. </title> <booktitle> In Proc. ACM-SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 84-95, </pages> <address> Washington, D.C., </address> <month> May </month> <year> 1986. </year>
Reference-contexts: This is the technique used in the influential evaluation of the optimizer in the R* distributed DBMS <ref> [ML86a, ML86b] </ref>, and it is very effective for isolating inaccuracies in an optimizer's cost model. 1 In fact, the pioneering designs in query "optimization" were more accurately described by their authors as schemes for "query decomposition" [WY76] and "access path selection" [SAC + 79]. 36 A second technique is to develop
Reference: [ML86b] <author> L. F. Mackert and G. M. Lohman. </author> <title> R* Optimizer Validation and Performance Evaluation for Distributed Queries. </title> <booktitle> In Proc. 12th International Conference on Very Large Data Bases, </booktitle> <pages> pages 149-159, </pages> <address> Kyoto, </address> <month> August </month> <year> 1986. </year>
Reference-contexts: This is the technique used in the influential evaluation of the optimizer in the R* distributed DBMS <ref> [ML86a, ML86b] </ref>, and it is very effective for isolating inaccuracies in an optimizer's cost model. 1 In fact, the pioneering designs in query "optimization" were more accurately described by their authors as schemes for "query decomposition" [WY76] and "access path selection" [SAC + 79]. 36 A second technique is to develop <p> In distributed systems, semi-joins minimize communication costs. In method caching, this semi-join-like technique minimizes staging costs. In both cases, the benefits of semi-joins need to be balanced against their costs, namely scanning the input relation twice, removing duplicates, and doing an additional join <ref> [ML86b] </ref>.
Reference: [MS79] <author> C. L. Monma and J.B. Sidney. </author> <title> Sequencing with Series-Parallel Precedence Constraints. </title> <journal> Mathematics of Operations Research, </journal> <volume> 4 </volume> <pages> 215-224, </pages> <year> 1979. </year>
Reference-contexts: As a result, these predicates are considered to have zero cost, regardless of the method's expense. 15 The following lemma demonstrates that this cost can be minimized by a simple sort on the predicates. It is analogous to the Least-Cost Fault Detection problem solved by Monma and Sidney <ref> [MS79] </ref>. Lemma 1 The cost of applying expensive selection predicates to a set of tuples is minimized by applying the predicates in ascending order of the metric rank = selectivity 1 differential cost Proof. <p> In such situations, we will need to find the optimal ordering of predicates in the 20 stream subject to the precedence constraints. Monma and Sidney <ref> [MS79] </ref> have shown that finding the optimal ordering for a single stream under these kinds of precedence constraints can be done fairly simply. Their analysis is based on two key results: 1. <p> We use a version of their algorithm as a subroutine in our optimization algorithm: Predicate Migration Algorithm: To optimize a plan tree, push all predicates down as far as possible, and then repeatedly apply the Series-Parallel Algorithm Using Parallel Chains <ref> [MS79] </ref> to each stream in the tree, until no more progress can be made. Pseudo-code for the Predicate Migration Algorithm is given in Figure 2, and we provide a brief explanation of the algorithm here. <p> Note that by traversing the stream from the top down, series parallel always provides correct input to parallel chains. 5 The parallel chains routine first finds groups of nodes in the chain that are constrained to be ordered sub-optimally (i.e. by descending rank). As shown by Monma and Sidney <ref> [MS79] </ref>, there is always an optimal ordering in which such nodes are adjacent, and hence such nodes may be considered as an undivided group. The find groups routine identifies the maximal-sized groups of poorly-ordered nodes. <p> Thus parallel chains is always passed a valid job module. 23 When predicate migration terminates, it leaves a tree in which each stream has been ordered by the Series-Parallel Algorithm using Parallel Chains. The interested reader is referred to <ref> [MS79] </ref> for justification of why the Series-Parallel Algorithm using Parallel Chains optimally orders a stream. 2.3 Predicate Migration: Proofs of Optimality Upon termination, the Predicate Migration Algorithm produces a semantically correct tree in which each stream is well-ordered according to Monma and Sidney; that is each stream, taken individually, is optimally
Reference: [MS86] <author> D. Maier and J. Stein. </author> <title> Indexing in an Object-Oriented DBMS. </title> <editor> In K. R. Dittrich and U. Dayal, editors, </editor> <booktitle> Proc. Workshop on Object-Oriented Database Systems, </booktitle> <pages> pages 171-182, </pages> <address> Asilomar, </address> <month> September </month> <year> 1986. </year>
Reference-contexts: tuples is e = e p 1 t + s p 1 e p 2 t + : : : + s p 1 s p 2 s p n1 e p n t: 3 It is possible to index tables on method values as well as on table attributes <ref> [MS86, LS88] </ref>. If a scan is done on such a "method" index, then predicates over the method may be satisfied during the scan without invoking the method.
Reference: [Nat87] <author> K. S. Natarajan. </author> <title> Optimizing Backtrack Search for All Solutions to Conjunctive Problems. </title> <booktitle> In Proceedings IJCAI-87, </booktitle> <pages> pages 955-958, </pages> <address> Milan, </address> <year> 1987. </year>
Reference-contexts: Furthermore, their schemes are a proposal for a completely new method for query optimization, not an extension that can be applied to the plans of any query optimizer. Various papers in the Artificial Intelligence literature <ref> [Han77, Nat87, Smi89] </ref> use metrics similar to rank to determine evaluation order of AND/OR trees in logic programming systems. These papers are applications of the early Operations Research work of W. E.
Reference: [Nau93] <author> J. F. Naughton. </author> <booktitle> Presentation at Fifth International High Performance Transaction Workshop, </booktitle> <month> September </month> <year> 1993. </year>
Reference-contexts: The lesson to be learned here is that comparative benchmarking is absolutely crucial to thoroughly 53 debugging a query optimizer and validating its cost model. It has been noted fairly recently that a variety of commercial products still produce very poor plans even on simple queries <ref> [Nau93] </ref>. Thus benchmarks | particularly complex query benchmarks such as TPC-D [Raa95] | are critical debugging tools for DBMS developers. In our case, we were able to easily compare our Predicate Migration implementation against various heuristics, to ensure that Predicate Migration always did at least as well as the heuristics.
Reference: [NKT88] <author> M. Nakayama, M. Kitsuregawa, and M. Takagi. </author> <title> Hash-Partitioned Join Method Using Dynamic Destaging Strategy. </title> <booktitle> In Proc. 14th International Conference on Very Large Data Bases, </booktitle> <pages> pages 468-477, </pages> <address> Los Angeles, </address> <month> August-September </month> <year> 1988. </year>
Reference-contexts: in the building relation, or chooses a poor hash function, the first bucket of the building relation is likely to be too large and have to be paged, resulting in as much as a random I/O operation (seek and write) per tuple of the first bucket of the probing relation <ref> [NKT88] </ref>. By contrast, Hybrid Cache never over-utilizes memory, since its first bucket is dynamically grown to the appropriate size. Both algorithms can underutilize memory for the first bucket if estimates are incorrect, but this is less dangerous than over-utilizing memory. <p> More effective (though somewhat complex) solutions to the problems of hybrid hash join have been proposed by Nakayama, et al. <ref> [NKT88] </ref>. 4.3.4 Sort vs. Hash Revisited In analyzing hashing and sorting, Graefe presents the interesting result that hash-based algorithms typically have "dual" sorting algorithms that perform comparably [Gra93].
Reference: [Ols95] <author> M. A. Olson. </author> <title> Cover Your Assets. </title> <booktitle> In Proc. ACM-SIGMOD International Conference on Management of Data, </booktitle> <address> San Jose, page 453, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: One benchmark that could be of use is a randomized macro-benchmark, to illustrate the effectiveness of the various solutions over queries from some typical distribution. Another benchmark that would be helpful would be a domain-specific benchmark with expensive predicates from real-world applications such as Asset Management <ref> [Ols95] </ref> or Remote Sensing [Doz92]; of course this would be most convincing if it were agreed upon by the users in these domains.
Reference: [Pal74] <author> F. P. Palermo. </author> <title> A Data Base Search Problem. </title> <editor> In J. T. Tou, editor, </editor> <booktitle> Information Systems COINS IV. </booktitle> <publisher> Plenum Press, </publisher> <address> New York, </address> <year> 1974. </year>
Reference-contexts: This heuristic, often called "predicate pushdown", is considered beneficial since early selections usually lower the complexity of join processing, and are traditionally considered to be trivial to check <ref> [Pal74] </ref>. However in this example the cost of evaluating the expensive selection predicate may outweigh the benefit gained by doing selection before join. In other words, this may be a case where predicate pushdown is precisely the wrong technique.
Reference: [PHH92] <author> H. Pirahesh, J. M. Hellerstein, and W. Hasan. </author> <title> Extensible/Rule-Based Query Rewrite Optimization in Starburst. </title> <booktitle> In Proc. ACM-SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 39-48, </pages> <address> San Diego, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: While some subquery predicates can be converted into joins (thereby becoming subject to traditional join-based optimization and execution strategies) even sophisticated SQL rewrite systems, such as that of Starburst <ref> [PHH92] </ref>, cannot convert all subqueries to joins. When one is forced to compute a subquery in order to evaluate a predicate, then the predicate should be treated as an expensive method. <p> methods AS SELECT thumbnail (product image), product image, ad text FROM vals WHERE ad text similar '.*optic.*'; SELECT methods.thumbnail FROM advertisements,methods WHERE advertisements.ad text = methods.ad text AND advertisements.product image = methods.product image; Such rewriting can be done by the user, or by a system with a query rewrite engine <ref> [PHH92] </ref>. Chimenti, et al. [CGK89] note that expensive selection predicates can be viewed as joins between the input relation and an infinite logical relation of input/output pairs for the selection method. <p> The authors conjecture that this technique may be beneficial for queries with expensive predicates. All of these ideas are similar to the query rewrite facility of Starburst <ref> [PHH92] </ref>, and are heuristics rather than cost-based optimizations. It is important to note 89 that these issues are indeed orthogonal to our problems of predicate placement | once queries have been rewritten into cheaper forms, they still need to have their predicates optimally placed into a query plan.
Reference: [Pir94] <author> H. Pirahesh. </author> <title> Object-Oriented Features of DB2 Client/Server. </title> <booktitle> In Proc. ACM-SIGMOD International Conference on Management of Data, Minneapolis, </booktitle> <pages> page 483, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: Relational DBMSs have begun to allow simple user-defined data types and functions to be utilized in ad-hoc queries <ref> [Pir94] </ref>. Simultaneously, Object-Oriented DBMSs have begun to offer ad-hoc query facilities, allowing declarative access to objects and methods that were previously only accessible through hand-coded, imperative applications [Cat94]. More recently, Object-Relational DBMSs were developed to unify these supposedly distinct approaches to data management [Ill94, Kim93].
Reference: [Raa95] <author> F. Raab. </author> <title> "TPC Benchmark D Standard Specification, Revision 1.0". Transaction Processing Performance Council, </title> <month> May </month> <year> 1995. </year>
Reference-contexts: A third technique is to define an agreed-upon database and set of queries, such as the Wisconsin benchmark [BDT83], AS 3 AP [TOB89], or TPC-D <ref> [Raa95] </ref>. Such domain-specific benchmarks [Gra91] are often based on models of real-world workloads. <p> It has been noted fairly recently that a variety of commercial products still produce very poor plans even on simple queries [Nau93]. Thus benchmarks | particularly complex query benchmarks such as TPC-D <ref> [Raa95] </ref> | are critical debugging tools for DBMS developers. In our case, we were able to easily compare our Predicate Migration implementation against various heuristics, to ensure that Predicate Migration always did at least as well as the heuristics.
Reference: [SAC + 79] <author> P. G. Selinger, M. Astrahan, D. Chamberlin, R. Lorie, and T. Price. </author> <title> Access Path Selection in a Relational Database Management System. </title> <booktitle> In Proc. ACM-SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 22-34, </pages> <address> Boston, </address> <month> June </month> <year> 1979. </year>
Reference-contexts: For queries with expensive predicates, the gains in execution speed should offset the extra optimization time. We have implemented Predicate Migration in Illustra, integrating it with Illustra's standard System R-style optimizer <ref> [SAC + 79] </ref>. With modest overhead in optimization time, Predicate Migration can reduce the execution time of many practical queries by orders of magnitude. This is illustrated further below. 2.1 Background: Optimizer Estimates To develop our optimizations, we must enhance the traditional model for analyzing query plan cost. <p> A relational query in a language such as SQL may have a WHERE clause, which contains an arbitrary Boolean expression over constants and the range variables of the query. We break such clauses into a maximal set of conjuncts, or "Boolean factors" <ref> [SAC + 79] </ref>, and refer to each Boolean factor as a distinct "predicate" to be satisfied by each result tuple of the query. When we use the term "predicate" below, we refer to a Boolean factor of the query's where clause. <p> That is, for any predicate p (join or selection) they estimate the value selectivity (p) = cardinality (output (p)) cardinality (input (p)) : Typically these estimations are based on default values and statistics stored by the DBMS <ref> [SAC + 79] </ref>, although recent work suggests that inexpensive sampling techniques can be used [LNSS93, HOT88]. 10 flag name description percall cpu execution time per invocation, regardless of argument size perbyte cpu execution time per byte of arguments byte pct percentage of argument bytes that the method needs to access Table <p> We also need to characterize the selectivity of a join with respect to each of its inputs. Traditional selectivity estimation <ref> [SAC + 79] </ref> computes the selectivity s J of a join J of relations R and S as the expected number of tuples in the output of J (O J ) over the number of tuples in the Cartesian product of the input relations, i.e., s J = jO J j=jR <p> If applied to every possible join plan for a query, the Predicate Migration Algorithm is guaranteed to generate a minimum-cost plan for the query. A traditional query optimizer, however, does not enumerate all possible plans for a query; it does some pruning of the plan space while enumerating plans <ref> [SAC + 79] </ref>. Although this pruning does not affect the basic exponential nature of join plan enumeration, it can significantly lower the amounts of space and time required to optimize queries with many joins. <p> This is not implemented in Illustra, but such cardinality-independent heuristics can be used to allow pruning to happen even when all selections cannot be pulled out of a subtree during join enumeration. 6 Of course one may also choose to save particular subtrees for other reasons, such as "interesting orders" <ref> [SAC + 79] </ref>. 33 Chapter 3 Optimization: Practice In the previous chapter we demonstrated that Predicate Migration produces provably optimal plans, under the assumptions of a theoretical cost model. In this chapter we consider bringing the theory into practice, by addressing a few important questions: 1. <p> evaluation of the optimizer in the R* distributed DBMS [ML86a, ML86b], and it is very effective for isolating inaccuracies in an optimizer's cost model. 1 In fact, the pioneering designs in query "optimization" were more accurately described by their authors as schemes for "query decomposition" [WY76] and "access path selection" <ref> [SAC + 79] </ref>. 36 A second technique is to develop "macro-benchmarks", which attempt to analyze the output of an optimizer on average, for many (possibly complex) queries. <p> This will be demonstrated dramatically in Section 4.4 below. 4.2.2 Sorting: The System R Approach In Section 1.4 it was pointed out that SQL subqueries are a form of expensive method. The authors of the pioneering System R optimization paper <ref> [SAC + 79] </ref> proposed a scheme to avoid redundant computation of correlated subqueries on identical inputs; their idea is directly applicable to expensive methods in general. <p> If they are the same, the previous evaluation result can be used again. In some cases, it might even pay to sort the referenced relation on the referenced column in order to avoid re-evaluating subqueries unnecessarily <ref> [SAC + 79] </ref>. In essence, the System R solution is to sort the input relation on the input columns (if it is not already so sorted), and then maintain a cache of only the last input/output pair for the method. <p> This is a well-known constraint on both hashing and sorting, which can be overcome by recursive application of either partitioning (for hashing) or merging (for sorting) [DKO + 84, Knu73]. The number of input values v can be estimated by using stored statistics <ref> [SAC + 79] </ref> or via sampling [HOT88, HNSS95]. Unfortunately this estimation is subject to error, so we must consider how the algorithm will behave if estimates are imperfect. <p> The total costs of methods in a query plan with caching can be computed by multiplying the differential cost of the method times the number of distinct values in its input. Estimating the number of values can be difficult in general, though many approaches have been proposed <ref> [SAC + 79, FM85, HNSS95] </ref>. Predicate Migration and related heuristics are typically used to place expensive predicates in a query plan. However, the estimates for Predicate Migration presented in Section 2.1 do not handle 84 the implications of caching. <p> This is done by multiplying the differential method cost times the ratio of values to tuples in a stream; this ratio can be computed quite accurately per column (or set of columns) of the base relations via system statistics <ref> [SAC + 79, FM85] </ref>. We assume that this ratio remains constant throughout the query plan, i.e. that any predicate of selectivity s reduces the number of values in each column of its input by a factor of 1 s.
Reference: [Sel88] <author> T. Sellis. </author> <title> Multiple Query Optimization. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 13(1) </volume> <pages> 23-52, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: Query processing over persistent caches is relatively simple: one performs a join of the input relation and the persistent cache by using a standard join algorithm. Similar techniques can be used for avoiding recomputation of common relational subexpressions <ref> [Sel88] </ref>. If the cache is incomplete, an outer join [Dat83] may be used | method inputs that are not found in the cache are "preserved" by the outer join, and for these inputs the method must be computed later in 90 the query.
Reference: [SFGM93] <author> M. Stonebraker, J. Frew, K. Gardels, and J. Meredith. </author> <title> The Sequoia 2000 Storage Benchmark. </title> <booktitle> In Proc. ACM-SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 2-11, </pages> <address> Washington, D.C., </address> <month> May </month> <year> 1993. </year>
Reference-contexts: their own, potentially time-consuming methods, these systems generated new problems in query optimization and execution. 1.2 Query Optimization Issues To illustrate the issues that can arise in processing queries with expensive methods, consider the following example over the satellite image data presented in the Sequoia benchmark for Geographic Information Systems <ref> [SFGM93] </ref>. The query retrieves names of digital "raster" images taken by a satellite; particularly, it selects the names of images from the first time period of observation that show vegetation in over 20% of their pixels: Example 1.
Reference: [SG88] <author> A. Swami and A. Gupta. </author> <title> Optimization of Large Join Queries. </title> <booktitle> In Proc. ACM-SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 8-17, </pages> <address> Chicago, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: Typically this is done with a randomized benchmark generator, which produces both a random database and random queries (e.g., <ref> [SG88] </ref>, [Swa89], [IK90], [SI92], [HS93b], etc.) This approach is somewhat analogous to comparing multiple C++ compilers by compiling a set of randomly generated programs with each compiler, and running the resulting executables on a set of randomly generated inputs.
Reference: [Sha86] <author> L. D. Shapiro. </author> <title> Join Processing in Database Systems with Large Main Memories. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 11(3) </volume> <pages> 239-264, </pages> <month> September </month> <year> 1986. </year> <month> 102 </month>
Reference-contexts: Recall that we treat traditional simple predicates as being of zero cost; similarly here we ignore the CPU costs associated with joins. In terms of I/O, the costs of merge and hash joins given by Shapiro, et al. <ref> [Sha86] </ref> fit our criterion. 2 For nested-loop join with an indexed inner relation, the cost per tuple of the outer relation is the cost of probing the index (typically 3 I/Os or less), while the cost per tuple of the inner relation is essentially zero | since we never scan tuples
Reference: [SI92] <author> A. Swami and B. R. Iyer. </author> <title> A Polynomial Time Algorithm for Optimizing Join Queries. </title> <type> Research Report RJ 8812, </type> <institution> IBM Almaden Research Center, </institution> <month> June </month> <year> 1992. </year>
Reference-contexts: Typically this is done with a randomized benchmark generator, which produces both a random database and random queries (e.g., [SG88], [Swa89], [IK90], <ref> [SI92] </ref>, [HS93b], etc.) This approach is somewhat analogous to comparing multiple C++ compilers by compiling a set of randomly generated programs with each compiler, and running the resulting executables on a set of randomly generated inputs. <p> The questions posed by Stonebraker are directly addressed in this dissertation, although we vary slightly in the definition of cost metrics for expensive functions. Ibaraki and Kameda [IK84], Krishnamurthy, Boral and Zaniolo [KBZ86], and Swami and Iyer <ref> [SI92] </ref> have developed and refined a query optimization scheme that is built on the the notion of rank. However, their scheme uses rank to reorder joins rather than selections.
Reference: [SK91] <author> M. Stonebraker and G. Kemnitz. </author> <title> The POSTGRES Next-Generation Database Management System. </title> <journal> Communications of the ACM, </journal> <volume> 34(10) </volume> <pages> 78-92, </pages> <year> 1991. </year>
Reference-contexts: The hope was that the basic mechanisms of an RDBMS to support selection, projection, join and aggregation queries could remain the same. This hope was largely borne out by the early extensible research systems such as POSTGRES <ref> [SK91] </ref> and Starburst [HFLP89]. <p> In this section we discuss the implementation experience, and some issues that arose in our experiments. The Illustra "Object-Relational" DBMS is based on the publicly available POSTGRES system <ref> [SK91] </ref>. 6 In this particular query the plan chosen by PullUp took almost 100 times as long as the optimal plan, although for purposes of illustration the bar for the plan is truncated in the graph.
Reference: [Smi56] <author> W. E. Smith. </author> <title> Various Optimizers For Single-Stage Production. </title> <journal> Naval Res. Logist. Quart., </journal> <volume> 3 </volume> <pages> 59-66, </pages> <year> 1956. </year>
Reference-contexts: Lemma 1 The cost of applying expensive selection predicates to a set of tuples is minimized by applying the predicates in ascending order of the metric rank = selectivity 1 differential cost Proof. This result dates back to early work in Operations Research <ref> [Smi56] </ref>, but we review it in our context for completeness. Assume the contrary. Then in a minimum-cost ordering p 1 ; : : : ; p n , for some predicate p k there is a predicate p k+1 where rank (p k ) &gt; rank (p k+1 ). <p> Various papers in the Artificial Intelligence literature [Han77, Nat87, Smi89] use metrics similar to rank to determine evaluation order of AND/OR trees in logic programming systems. These papers are applications of the early Operations Research work of W. E. Smith <ref> [Smi56] </ref>, and do not address the problems that arise with joins, including the issues of constraints on ordering and integrating the orderings of multiple streams. 88 The notion of expensive selections was considered in the context of the LDL logic programming system [CGK89].
Reference: [Smi89] <author> D. E. Smith. </author> <title> Controlling Backward Inference. </title> <journal> Artificial Intelligence, </journal> <volume> 39 </volume> <pages> 145-208, </pages> <year> 1989. </year>
Reference-contexts: Furthermore, their schemes are a proposal for a completely new method for query optimization, not an extension that can be applied to the plans of any query optimizer. Various papers in the Artificial Intelligence literature <ref> [Han77, Nat87, Smi89] </ref> use metrics similar to rank to determine evaluation order of AND/OR trees in logic programming systems. These papers are applications of the early Operations Research work of W. E.
Reference: [SPMK95] <author> M. Steinbrunn, K. Peithner, G. Moerkotte, and A. Kemper. </author> <title> Bypassing Joins in Disjunctive Queries. </title> <booktitle> In Proc. 21st International Conference on Very Large Data Bases, </booktitle> <address> Zurich, </address> <month> September </month> <year> 1995. </year>
Reference-contexts: It is important to note 89 that these issues are indeed orthogonal to our problems of predicate placement | once queries have been rewritten into cheaper forms, they still need to have their predicates optimally placed into a query plan. Kemper, Steinbrunn, et al. <ref> [KMPS94, SPMK95] </ref> address the problem of planning disjunctive queries with expensive predicates; their work is not easily integrated with a traditional (conjunct-based) optimizer. Like most System R-based optimizers, Illustra focusses on Boolean factors (conjuncts). Within Boolean factors, the operands of OR are ordered by the metric cost/selectivity.
Reference: [Sto91] <author> M. Stonebraker. </author> <title> Managing Persistent Objects in a Multi-Level Store. </title> <booktitle> In Proc. ACM-SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 2-11, </pages> <address> Denver, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: If necessary, database users can achieve these optimizations in more basic systems by rewriting their queries or methods. 87 Chapter 5 Related Work 5.1 Optimization and Expensive Methods Stonebraker raised the issue of expensive predicate optimization in the context of the POSTGRES multi-level store <ref> [Sto91] </ref>. The questions posed by Stonebraker are directly addressed in this dissertation, although we vary slightly in the definition of cost metrics for expensive functions.
Reference: [Swa89] <author> A. Swami. </author> <title> Optimization of Large Join Queries: Combining Heuristics with Combinatorial Techniques. </title> <booktitle> In Proc. ACM-SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 367-376, </pages> <address> Portland, </address> <month> May-June </month> <year> 1989. </year>
Reference-contexts: Typically this is done with a randomized benchmark generator, which produces both a random database and random queries (e.g., [SG88], <ref> [Swa89] </ref>, [IK90], [SI92], [HS93b], etc.) This approach is somewhat analogous to comparing multiple C++ compilers by compiling a set of randomly generated programs with each compiler, and running the resulting executables on a set of randomly generated inputs.
Reference: [TOB89] <author> C. Turbyfill, C. Orji, and D. Bitton. </author> <title> AS 3 AP A Comparative Relational Database Benchmark. </title> <booktitle> In Proc. IEEE Compcon Spring '89, </booktitle> <month> February </month> <year> 1989. </year>
Reference-contexts: A third technique is to define an agreed-upon database and set of queries, such as the Wisconsin benchmark [BDT83], AS 3 AP <ref> [TOB89] </ref>, or TPC-D [Raa95]. Such domain-specific benchmarks [Gra91] are often based on models of real-world workloads. <p> The evaluation of the optimizer in these benchmarks is binary, in the sense that typically the relative performance of the good and bad strategies is not interesting; what is important is that the optimizer choose the "correct access plan" <ref> [TOB89] </ref>. 3.1.3 Analysis vs. Benchmarking for Expensive Methods An alternative to benchmarking is to run queries that expose the logic that makes one optimization strategy work where another fails.
Reference: [WY76] <author> E. Wong and K. Youssefi. </author> <title> Decomposition A Strategy for Query Processing. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 1(3) </volume> <pages> 223-241, </pages> <month> September </month> <year> 1976. </year>
Reference-contexts: technique used in the influential evaluation of the optimizer in the R* distributed DBMS [ML86a, ML86b], and it is very effective for isolating inaccuracies in an optimizer's cost model. 1 In fact, the pioneering designs in query "optimization" were more accurately described by their authors as schemes for "query decomposition" <ref> [WY76] </ref> and "access path selection" [SAC + 79]. 36 A second technique is to develop "macro-benchmarks", which attempt to analyze the output of an optimizer on average, for many (possibly complex) queries.
Reference: [YKY + 91] <author> K. Yajima, H. Kitagawa, K. Yamaguchi, N. Ohbo, and Y. Fujiwara. </author> <title> Optimization of Queries Including ADT Functions. </title> <booktitle> In Proc. 2nd International Symposium on Database Systems for Advanced Applications, </booktitle> <pages> pages 366-373, </pages> <address> Tokyo, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: The first approach was pioneered in the LDL logic database system [CGK89], and was proposed for extensible relational systems by Yajima, et al. <ref> [YKY + 91] </ref>. We refer to this as the LDL approach. The other approach is Predicate Migration, which was first presented in 1992 [Hel92]. Neither algorithm actually produces optimal plans in all scenarios. <p> A System R optimizer can be modified to explore the space of bushy trees, but this increases the complexity of the LDL approach yet further. No known modification of the IK-KBZ optimizer can handle bushy trees. Yajima et al. <ref> [YKY + 91] </ref> successfully integrate the LDL approach with an IK-KBZ optimizer, but they use an exhaustive mechanism that requires time exponential in the number of expensive selections. 3.2.2 Predicate Migration Revisited The Predicate Migration algorithm presented in Chapter 2 only works when the differential join costs are constants.
References-found: 74

