URL: http://www.cs.cmu.edu/afs/cs/project/quake/public/papers/robust-arithmetic.ps
Refering-URL: http://www.cs.cmu.edu/afs/cs/project/quake/public/www/robust.html
Root-URL: 
Title: Adaptive Precision Floating-Point Arithmetic and Fast Robust Geometric Predicates  
Author: Jonathan Richard Shewchuk 
Address: Pittsburgh, PA 15213  
Affiliation: School of Computer Science Carnegie Mellon University  
Date: May 17, 1996  
Abstract: Exact computer arithmetic has a variety of uses including, but not limited to, the robust implementation of geometric algorithms. This report has three purposes. The first is to offer fast software-level algorithms for exact addition and multiplication of arbitrary precision floating-point values. The second is to propose a technique for adaptive-precision arithmetic that can often speed these algorithms when one wishes to perform multiprecision calculations that do not always require exact arithmetic, but must satisfy some error bound. The third is to provide a practical demonstration of these techniques, in the form of implementations of several common geometric calculations whose required degree of accuracy depends on their inputs. These robust geometric predicates are adaptive; their running time depends on the degree of uncertainty of the result, and is usually small. These algorithms work on computers whose floating-point arithmetic uses radix two and exact rounding, including machines complying with the IEEE 754 standard. The inputs to the predicates may be arbitrary single or double precision floating-point numbers. C code is publicly available for the 2D and 3D orientation and incircle tests, and robust Delaunay triangulation using these tests. Timings of the implementations demonstrate their effectiveness. Supported in part by the Natural Sciences and Engineering Research Council of Canada under a 1967 Science and Engineering Scholarship and by the National Science Foundation under Grant CMS-9318163. The views and conclusions contained in this document are those of the author and should not be interpreted as representing the official policies, either express or implied, of NSERC, NSF, or the U.S. Government. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Francis Avnaim, Jean-Daniel Boissonnat, Olivier Devillers, Franco P. Preparata, and Mariette Yvinec. </author> <title> Evaluating Signs of Determinants Using Single-Precision Arithmetic. </title> <note> Manuscript available from http://www.inria.fr:/prisme/personnel/devillers/anglais/determinant, 1995. </note>
Reference-contexts: The trend is reversing for software libraries as well, and there are several proposals to use floating-point arithmetic to perform extended-precision integer calculations. Fortune and Van Wyk [10, 9], Clarkson [4], and Avnaim, Boissonnat, Devillers, Preparata, and Yvinec <ref> [1] </ref> have described algorithms of this kind, designed to attack the same computational geometry robustness problems considered later in this report. These algorithms are surveyed in Section 4.1. Another differentiating feature of multiprecision libraries is whether they use multiple exponents. <p> The significand is a p-bit binary number of the form b:bbb : : :, where each b denotes a single bit; one additional bit represents the sign. This report does not address issues of overflow and underflow, so I allow the exponent to be an integer in the range <ref> [1; 1] </ref>. (Fortunately, many applications have inputs whose exponents fall within a circumscribed range. <p> Clarkson's algorithm is naturally adaptive; its running time is small for matrices whose determinants are not near zero 6 . Recently, Avnaim, Boissonnat, Devillers, Preparata, and Yvinec <ref> [1] </ref> proposed an algorithm to evaluate signs of determinants of 2 fi 2 and 3 fi 3 matrices of p-bit integers using only p and (p + 1)-bit arithmetic, respectively. <p> A similar adaptive routine could accurately compute the numerators. It might be fruitful to explore whether the methods described by Clarkson [4] and Avnaim et al. <ref> [1] </ref> can be extended by fast multiprecision methods to handle arbitrary double precision floating-point inputs.
Reference: [2] <author> David H. Bailey. </author> <title> A Portable High Performance Multiprecision Package. </title> <type> Technical Report RNR-90 022, </type> <institution> NASA Ames Research Center, Moffett Field, California, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: Most arbitrary precision libraries store numbers in a multiple-digit format, consisting of a sequence of digits (usually of large radix, like 2 32 ) coupled with a single exponent. A freely available example of the multiple-digit approach is Bailey's MPFUN package <ref> [2] </ref>, a sophisticated portable multiprecision library that uses digits of machine-dependent radix (usually 2 24 ) stored as single precision floating-point values. An alternative is the multiple-term format, wherein a number is expressed as a sum of ordinary floating-point words, each with its own significand and exponent [21, 5, 17]. <p> Table 2 lists timings for ORIENT2D, given random inputs. Observe that the adaptive test, when it stops at the approximate result A, takes nearly twice as long as the approximate test because of the need to compute an error bound. The table includes a comparison with Bailey's MPFUN <ref> [2] </ref>, chosen because it is the fastest portable and freely available arbitrary precision package I know of. ORIENT2D coded with my (nonadaptive) algorithms is roughly thirteen times faster than ORIENT2D coded with MPFUN.
Reference: [3] <author> John Canny. </author> <title> Some Algebraic and Geometric Computations in PSPACE. </title> <booktitle> 20th Annual Symposium on the Theory of Computing (Chicago, </booktitle> <publisher> Illinois), </publisher> <pages> pages 460467. </pages> <institution> Association for Computing Machinery, </institution> <month> May </month> <year> 1988. </year>
Reference-contexts: This theory is at least NP-hard and is decidable in polynomial space <ref> [3] </ref>. Unfortunately, the full power of the theory seems to be necessary for some problems.
Reference: [4] <author> Kenneth L. Clarkson. </author> <title> Safe and Effective Determinant Evaluation. </title> <booktitle> 33rd Annual Symposium on Foundations of Computer Science (Pittsburgh, Pennsylvania), </booktitle> <pages> pages 387395. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> October </month> <year> 1992. </year>
Reference-contexts: The trend is reversing for software libraries as well, and there are several proposals to use floating-point arithmetic to perform extended-precision integer calculations. Fortune and Van Wyk [10, 9], Clarkson <ref> [4] </ref>, and Avnaim, Boissonnat, Devillers, Preparata, and Yvinec [1] have described algorithms of this kind, designed to attack the same computational geometry robustness problems considered later in this report. These algorithms are surveyed in Section 4.1. Another differentiating feature of multiprecision libraries is whether they use multiple exponents. <p> There are several exact arithmetic schemes designed specifically for computational geometry; most are methods for exactly evaluating the sign of a determinant, and hence can be used to perform the orientation and incircle tests. Clarkson <ref> [4] </ref> proposes an algorithm for using floating-point arithmetic to evaluate the sign of the determinant of a small matrix of integers. A variant of the modified Gram-Schmidt procedure is used to improve the conditioning of the matrix, so that the determinant can subsequently be evaluated safely by Gaussian elimination. <p> A similar adaptive routine could accurately compute the numerators. It might be fruitful to explore whether the methods described by Clarkson <ref> [4] </ref> and Avnaim et al. [1] can be extended by fast multiprecision methods to handle arbitrary double precision floating-point inputs.
Reference: [5] <author> T. J. Dekker. </author> <title> A Floating-Point Technique for Extending the Available Precision. </title> <journal> Numerische Mathe matik 18:224242, </journal> <year> 1971. </year>
Reference-contexts: An alternative is the multiple-term format, wherein a number is expressed as a sum of ordinary floating-point words, each with its own significand and exponent <ref> [21, 5, 17] </ref>. <p> Two such algorithms, due to Dekker and Knuth respectively, are presented. Theorem 6 (Dekker <ref> [5] </ref>) Let a and b be p-bit floating-point numbers such that jaj jbj. Then the following algorithm will produce a nonoverlapping expansion x+y such that a+b = x+y, where x is an approximation to a + b and y represents the roundoff error in the calculation of x. <p> The multiplication is performed by splitting each value into two halves with half the precision, then performing four exact multiplications on these fragments. The trick is to find a way to split a floating-point value in two. The following theorem was first proven by Dekker <ref> [5] </ref>: Theorem 17 Let a be a p-bit floating-point number, where p 3. Choose a splitting point s such that p 2 s p 1. <p> The products a hi b hi , a lo b hi , a hi b lo , and a lo b lo can each be computed exactly by the floating-point unit, producing four values. These could then be summed using the FAST-EXPANSION-SUM procedure in Section 2.4. However, Dekker <ref> [5] </ref> provides several faster ways to accomplish the computation. Dekker attributes the following method to G. W. Veltkamp.
Reference: [6] <author> Steven Fortune. </author> <title> Stable Maintenance of Point Set Triangulations in Two Dimensions. </title> <booktitle> 30th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 494499. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1989. </year> <title> 52 Jonathan Richard Shewchuk [7] . Progress in Computational Geometry. Directions in Geometric Computing (R. </title> <editor> Martin, editor), </editor> <volume> chapter 3, </volume> <pages> pages 81128. </pages> <publisher> Information Geometers Ltd., </publisher> <year> 1993. </year> <title> [8] . Numerical Stability of Algorithms for 2D Delaunay Triangulations. </title> <journal> International Journal of Computational Geometry & Applications 5(12):193213, </journal> <month> MarchJune </month> <year> 1995. </year>
Reference-contexts: A correct algorithm (that computes a purely combinatorial result) will produce a meaningful result if its test results are wrong but are consistent with each other, because there exists an input for which those test results are correct. Following Fortune <ref> [6] </ref>, an algorithm is robust if it always produces the correct output under the real RAM model, and under approximate arithmetic always produces an output that is consistent with some hypothetical input that is a perturbation of the true input; it is stable if this perturbation is small. <p> Fortune <ref> [6] </ref> explains that [a]n algorithm is parsimonious if it never performs a test whose outcome has already been determined as the formal consequence of previous tests.
Reference: [9] <author> Steven Fortune and Christopher J. Van Wyk. </author> <title> Efficient Exact Arithmetic for Computational Geometry. </title> <booktitle> Proceedings of the Ninth Annual Symposium on Computational Geometry, </booktitle> <pages> pages 163172. </pages> <institution> Association for Computing Machinery, </institution> <month> May </month> <year> 1993. </year> <title> [10] . Static Analysis Yields Efficient Exact Integer Arithmetic for Computational Geometry. </title> <note> To appear in Transactions on Mathematical Software, </note> <year> 1996. </year>
Reference-contexts: Times have changed, and modern architectures are highly optimized for floating-point performance; on many processors, floating-point arithmetic is faster than integer arithmetic. The trend is reversing for software libraries as well, and there are several proposals to use floating-point arithmetic to perform extended-precision integer calculations. Fortune and Van Wyk <ref> [10, 9] </ref>, Clarkson [4], and Avnaim, Boissonnat, Devillers, Preparata, and Yvinec [1] have described algorithms of this kind, designed to attack the same computational geometry robustness problems considered later in this report. These algorithms are surveyed in Section 4.1. <p> Although exact arithmetic banishes these difficulties, it is common to hear reports of implementations being slowed by factors of ten or more as a consequence <ref> [14, 9] </ref>. For these reasons, computational geometry is an important arena for evaluating extended precision arithmetic schemes. The orientation and incircle tests evaluate the sign of a matrix determinant. It is significant that only the sign, and not the magnitude, of the determinant is needed. <p> For these reasons, computational geometry is an important arena for evaluating extended precision arithmetic schemes. The orientation and incircle tests evaluate the sign of a matrix determinant. It is significant that only the sign, and not the magnitude, of the determinant is needed. Fortune and Van Wyk <ref> [9] </ref> take advantage of this fact by using a floating-point filter: the determinant is first evaluated approximately, and only if forward error analysis indicates that the sign of the approximate result cannot be trusted does one use an exact test. <p> One cannot generally know in advance how much precision is needed. In the context of determinant evaluation for computational geometry, Fortune and Van Wyk <ref> [9] </ref> suggest using a floating-point filter. An expression is evaluated approximately in hardware precision arithmetic first. Forward error analysis determines whether the approximate result can be trusted; if not, an exact result is computed. If the exact computation is only needed occasionally, the application is slowed only a little. <p> Fortune and Van Wyk <ref> [10, 9] </ref> propose a more general approach (not specific to determinants, or even to predicates) that represents integers using a standard multiple-digit technique with digits of radix 2 23 stored as double precision floating-point values. (53-bit double precision significands make it possible to add several products of 23-bit integers before it <p> Because exact translation is the common case, my adaptive geometric predicates test for and exploit this case. Implementation of Geometric Predicates 37 Once a determinant has been chosen for evaluation, there are several methods to evaluate it. A number of methods are surveyed by Fortune and Van Wyk <ref> [9] </ref>, and only their conclusion is repeated here. The cheapest method of evaluating the determinant of a 5 fi 5 or smaller matrix seems to be by dynamic programming applied to cofactor expansion. <p> The techniques Priest and I have developed are simple enough to be coded directly in numerical algorithms, avoiding function call overhead and conversion costs. A useful tool in coding such algorithms would be an expression compiler similar to Fortune and Van Wyk's LN <ref> [10, 9] </ref>, which converts an expression into exact arithmetic code, complete with error bound derivation and floating-point filters. Such a tool might even be able to automate the process of breaking an expression into adaptive stages as described in Section 3.
Reference: [11] <author> David Goldberg. </author> <title> What Every Computer Scientist Should Know About Floating-Point Arithmetic. </title> <journal> ACM Computing Surveys 23(1):548, </journal> <month> March </month> <year> 1991. </year>
Reference-contexts: For example, in four-bit arithmetic, binary 1101 (decimal 13) is represented as 1:101 fi 2 3 . See the survey by Goldberg <ref> [11] </ref> for a detailed explanation of floating-point storage formats, particularly the IEEE 754 standard. Exact arithmetic often produces values that require more than p bits to store. <p> This optimization would be valid if computers stored arbitrary real numbers, but is incorrect for floating-point numbers. Unfortunately, not all compiler developers are aware of the importance of maintaining correct floating-point language semantics, but as a whole, they seem to be improving. Goldberg <ref> [11, x3.2.3] </ref> presents several related examples of how carefully designed numerical algorithms can be utterly ruined by incorrect optimizations. 46 Jonathan Richard Shewchuk 3D incremental Delaunay tetrahedralization Uniform Surface Tilted Random of Sphere Grid Input sites 10,000 10,000 10,000 ORIENT3D counts Adaptive A, approximate 2,735,668 1,935,978 5,542,567 Adaptive B 602,344 Adaptive
Reference: [12] <author> Leonidas J. Guibas and Jorge Stolfi. </author> <title> Primitives for the Manipulation of General Subdivisions and the Computation of Vorono Diagrams. </title> <journal> ACM Transactions on Graphics 4(2):74123, </journal> <month> April </month> <year> 1985. </year>
Reference-contexts: The incircle test determines whether a point lies inside, outside, or on a circle or sphere, and is used for Delaunay triangulation <ref> [12] </ref>. Inexact versions of these tests are vulnerable to roundoff error, and the wrong answers they produce can cause geometric algorithms to hang, crash, or produce incorrect output. <p> Figure 19 shows an error that arose in a two-dimensional Delaunay triangulation program I wrote. The program, which employs a divide-and-conquer algorithm presented by Guibas and Stolfi <ref> [12] </ref>, failed in a subroutine that merges two triangulations into one. The geometrically nonsensical triangulation in the illustration was produced. On close inspection with a debugger, I found that the failure was caused by a single incorrect result of the incircle test. <p> Triangle [23] is a 2D Delaunay triangulator and mesh generator, publicly available from Netlib, that uses a divide-and-conquer algorithm <ref> [16, 12] </ref>. Pyramid is a 3D Delaunay tetrahedralizer that uses an incremental algorithm [25].
Reference: [13] <author> Christoph M. Hoffmann. </author> <title> The Problems of Accuracy and Robustness in Geometric Computation. </title> <booktitle> Computer 22(3):3141, </booktitle> <month> March </month> <year> 1989. </year>
Reference-contexts: The next several pages are devoted to a discussion of representative research in each class, and of the circumstances in which exact arithmetic and other techniques are or are not applicable. For more extensive surveys of geometric robustness, see Fortune [7] and Hoffmann <ref> [13] </ref>. Exact algorithms.
Reference: [14] <author> Michael Karasick, Derek Lieber, and Lee R. Nackman. </author> <title> Efficient Delaunay Triangulation Using Rational Arithmetic. </title> <journal> ACM Transactions on Graphics 10(1):7191, </journal> <month> January </month> <year> 1991. </year>
Reference-contexts: Although exact arithmetic banishes these difficulties, it is common to hear reports of implementations being slowed by factors of ten or more as a consequence <ref> [14, 9] </ref>. For these reasons, computational geometry is an important arena for evaluating extended precision arithmetic schemes. The orientation and incircle tests evaluate the sign of a matrix determinant. It is significant that only the sign, and not the magnitude, of the determinant is needed. <p> Fortune and Van Wyk report an order-of-magnitude speed improvement over the use of multiprecision libraries (for equal bit complexity). Furthermore, LN gains another speed improvement by installing floating-point filters wherever appropriate, calculating error bounds automatically. Karasick, Lieber, and Nackman <ref> [14] </ref> report their experiences optimizing a method for determinant evaluation using rational inputs. Their approach reduces the bit complexity of the inputs by performing arithmetic on intervals (with low precision bounds) rather than exact values.
Reference: [15] <author> Donald Ervin Knuth. </author> <booktitle> The Art of Computer Programming: Seminumerical Algorithms, second edition, </booktitle> <volume> volume 2. </volume> <publisher> Addison Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1981. </year>
Reference-contexts: Due to roundoff, these operators lack several desirable arithmetic properties. Associativity is an example; in four-bit arithmetic, (1000 0:011) 0:011 = 1000, but 1000 (0:011 0:011) = 1001. A list of reliable identities for floating-point arithmetic is given by Knuth <ref> [15] </ref>. Roundoff is often analyzed in terms of ulps, or units in the last place. An ulp is the effective magnitude of the low-order (pth) bit of a p-bit significand. An ulp is defined relative to a specific floating point value; I shall use ulp (a) to denote this quantity. <p> Of course, FAST-TWO-SUM remains faster if the relative sizes of the operands are known a priori, and the comparison can be avoided. Theorem 7 (Knuth <ref> [15] </ref>) Let a and b be p-bit floating-point numbers, where p 3.
Reference: [16] <author> Der-Tsai Lee and Bruce J. Schachter. </author> <title> Two Algorithms for Constructing a Delaunay Triangulation. </title> <journal> International Journal of Computer and Information Sciences 9(3):219242, </journal> <year> 1980. </year>
Reference-contexts: Triangle [23] is a 2D Delaunay triangulator and mesh generator, publicly available from Netlib, that uses a divide-and-conquer algorithm <ref> [16, 12] </ref>. Pyramid is a 3D Delaunay tetrahedralizer that uses an incremental algorithm [25].
Reference: [17] <author> Seppo Linnainmaa. </author> <title> Analysis of Some Known Methods of Improving the Accuracy of Floating-Point Sums. </title> <journal> BIT 14:167202, </journal> <year> 1974. </year>
Reference-contexts: An alternative is the multiple-term format, wherein a number is expressed as a sum of ordinary floating-point words, each with its own significand and exponent <ref> [21, 5, 17] </ref>.
Reference: [18] <author> Victor Milenkovic. </author> <title> Double Precision Geometry: A General Technique for Calculating Line and Seg ment Intersections using Rounded Arithmetic. </title> <booktitle> 30th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 500505. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1989. </year>
Reference-contexts: However, sometimes one can settle for an algorithm whose output might not be realizable. I place such algorithms in a bag labeled with the fuzzy term quasi-robust, which I apply to any algorithm whose output is somehow provably distinguishable from nonsense. Milenkovic <ref> [18] </ref> circumvents the aforementioned NP-hardness result while using approximate arithmetic by constructing pseudo-line arrangements; a pseudo-line is a curve constrained to lie very close to an actual line.
Reference: [19] <author> N. E. Mnev. </author> <title> The Universality Theorems on the Classification Problem of Configuration Varieties and Convex Polytopes Varieties. Topology and Geometry - Rohlin Seminar (O. Ya. </title> <editor> Viro, editor), </editor> <booktitle> Lecture Notes in Mathematics, </booktitle> <volume> volume 1346, </volume> <pages> pages 527543. </pages> <publisher> Springer-Verlag, </publisher> <year> 1988. </year>
Reference-contexts: An example is the line arrangement problem: given a set of lines (specified by real coordinates (a; b; c), so that ax + by = c), compute the combinatorial structure of the resulting arrangement in the plane. It follows from recent work of Mnev <ref> [19] </ref> that the problem of deciding whether a combinatorial arrangement is actually realizable with lines is as hard as the existential theory of the reals. Hence a parsimonious algorithm for the line arrangement problem : : : seems to require the solution of NP-hard problems.
Reference: [20] <author> Thomas Ottmann, Gerald Thiemt, and Christian Ullrich. </author> <title> Numerical Stability of Geometric Algorithms. </title> <booktitle> Proceedings of the Third Annual Symposium on Computational Geometry, </booktitle> <pages> pages 119125. </pages> <institution> Association for Computing Machinery, </institution> <month> June </month> <year> 1987. </year>
Reference-contexts: I am not aware of any prior literature on exact determinant evaluation that considers floating-point operands, except for one limited example: Ottmann, Thiemt, and Ullrich <ref> [20] </ref> advocate the use of an accurate scalar product operation, ideally implemented in hardware (though a software distillation algorithm may also be used), as a way to evaluate some predicates such as the 2D orientation test. Exact determinant algorithms do not satisfy the needs of all applications.
Reference: [21] <author> Douglas M. Priest. </author> <title> Algorithms for Arbitrary Precision Floating Point Arithmetic. </title> <booktitle> Tenth Symposium on Computer Arithmetic (Los Alamitos, California), </booktitle> <pages> pages 132143. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1991. </year> <title> [22] . On Properties of Floating Point Arithmetics: Numerical Stability and the Cost of Accurate Computations. </title> <type> Ph.D. thesis, </type> <institution> Department of Mathematics, University of California at Berkeley, Berke-ley, California, </institution> <month> November </month> <year> 1992. </year> <note> Available by anonymous FTP to ftp.icsi.berkeley.edu as pub/theory/priest-thesis.ps.Z. REFERENCES 53 </note>
Reference-contexts: An alternative is the multiple-term format, wherein a number is expressed as a sum of ordinary floating-point words, each with its own significand and exponent <ref> [21, 5, 17] </ref>. <p> This assumption holds on processors compliant with the IEEE 754 floating-point standard. Proofs of the correctness of all algorithms are given. The methods herein are closely related to, and occasionally taken directly from, methods developed by Priest <ref> [21, 22] </ref>, but are faster. The improvement in speed arises partly because Priest's algorithms run on a wide variety of floating-point architectures, with different radices and rounding behavior, whereas mine are limited to and optimized for radix two with exact rounding. <p> the multiple-term algorithms described herein is that the former perform exact arithmetic by keeping the bit complexity of operands small enough to avoid roundoff error, whereas the latter allow roundoff to occur, then account for 1 Note that this definition of expansion is slightly different from that used by Priest <ref> [21] </ref>; whereas Priest requires that the exponents of any two components of the expansion differ by at least p, no such requirement is made here. 2 Formally, x and y are nonoverlapping if there exist integers r and s such that x = r2 s and jyj &lt; 2 s , <p> The algorithm can also be used with round-toward-zero arithmetic, but the proof is different. I have emphasized round-to-even arithmetic here due to the IEEE 754 standard. A variant of this algorithm was presented by Priest <ref> [21] </ref>, but it is used differently here. Priest uses the algorithm to sum two nonoverlapping expansions, and proves under general conditions that the components of the resulting expansion overlap by at most one digit (i.e. one bit in binary arithmetic). <p> The COMPRESS algorithm below finds a compact form for an expansion. More importantly, COMPRESS guarantees that the largest component is a good approximation to the whole expansion. If round-to-even tiebreaking is used, COMPRESS also converts nonoverlapping expansions into nonadjacent expansions. Priest <ref> [21] </ref> presents a more complicated Renormalization procedure that compresses optimally. Its greater running time is rarely justified by the marginal reduction in expansion length, unless there is a need to put expansions in a canonical form. <p> Distillation can be performed by the divide-and-conquer algorithm of Priest <ref> [21] </ref>, which uses any expansion addition algorithm to sum the values 26 Jonathan Richard Shewchuk in a tree-like fashion as illustrated in Figure 16. Each p-bit addend is a leaf of the tree, and each interior node represents a call to an expansion addition algorithm. <p> Division cannot always, of course, be performed exactly, but it can be performed to arbitrary precision by an iterative algorithm that employs multiprecision addition and multiplication. Consult Priest <ref> [21] </ref> for one such algorithm. The easiest way to compare two expansions is to subtract one from the other, and test the sign of the result.
Reference: [23] <author> Jonathan Richard Shewchuk. </author> <title> Triangle: Engineering a 2D Quality Mesh Generator and Delaunay Triangulator. </title> <booktitle> First Workshop on Applied Computational Geometry. Association for Computing Machinery, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: For each such input, time was measured by a Unix system call before and after 1,000 iterations of the predicate. 4.5 Performance in Two Triangulation Programs To evaluate the effectiveness of the adaptive tests in applications, I tested them in two of my Delaunay triangulation codes. Triangle <ref> [23] </ref> is a 2D Delaunay triangulator and mesh generator, publicly available from Netlib, that uses a divide-and-conquer algorithm [16, 12]. Pyramid is a 3D Delaunay tetrahedralizer that uses an incremental algorithm [25].
Reference: [24] <author> Pat H. Sterbenz. </author> <title> Floating-Point Computatation. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1974. </year>
Reference-contexts: which shows that subtraction is exact for two operands within a factor of two of each other: 6 Jonathan Richard Shewchuk a = 1 1 0 1 a = 1 0 0 1 fi2 1 a b = 1 1 a b = 1 0 0 1 Lemma 5 (Sterbenz <ref> [24] </ref>) Suppose that b 2 [ a 2 ; 2a]. Then a b = a b. Proof: Without loss of generality, assume jaj jbj. (The other case is symmetric, because ab = ba.) Then b 2 [ a 2 ; a].
Reference: [25] <author> David F. Watson. </author> <title> Computing the n-dimensional Delaunay Tessellation with Application to Vorono Polytopes. </title> <journal> Computer Journal 24(2):167172, </journal> <year> 1981. </year>
Reference-contexts: Triangle [23] is a 2D Delaunay triangulator and mesh generator, publicly available from Netlib, that uses a divide-and-conquer algorithm [16, 12]. Pyramid is a 3D Delaunay tetrahedralizer that uses an incremental algorithm <ref> [25] </ref>. For both 2D and 3D, three types of inputs were tested: uniform random points, points lying (approximately) on the boundary of a circle or sphere, and a square or cubic grid of lattice points, tilted so as not to be aligned with the coordinate axes.
Reference: [26] <author> James Hardy Wilkinson. </author> <title> Rounding Errors in Algebraic Processes. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1963. </year>
Reference-contexts: Using standard terminology from forward error analysis <ref> [26] </ref>, the quantity 1 2 ulp (1) is called the machine epsilon, denoted *. Recall that exact rounding guarantees that jy i j *jx i j; the quantity * bounds the relative error err (a ~ b)=(a ~ b) of any basic floating-point operation.
References-found: 22

