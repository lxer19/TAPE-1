URL: http://www.eecs.umich.edu/~optimus/CDC97.ps
Refering-URL: http://www.eecs.umich.edu/~optimus/
Root-URL: http://www.cs.umich.edu
Email: optimus@eecs.umich.edu, pramod@eecs.umich.edu  
Title: Computational Experiments for Robust Stability Analysis  
Author: Albert Yoon and Pramod Khargonekar 
Address: Ann Arbor, MI 48109-2122, USA  
Affiliation: The University of Michigan Dept. of Electrical Engineering and Computer Science  
Abstract: In this paper, we take a "computational experiments" approach to robust stability analysis problems. Many robust control problems have been shown to be NP hard but in spite of this, it is important to develop effective techniques for solving them. A typical robust stability analysis problem is taken and formulated as an optimization problem to which several optimization algorithms are applied. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Ali, A. Torn, and S. Viitanen. </author> <type> Technical Report 98, </type> <institution> Turku Centre for Computer Science, </institution> <month> March </month> <year> 1997. </year>
Reference-contexts: Since robust control problems often lead to nondifferentiable optimization problems, direct methods will have broader applicability. The following methods are investigated here: Rosenbrock's method [3];crude and adaptive random search [8];adaptive partitioned random search [20]; controlled random search <ref> [1] </ref>; and shu*ed complex evolution [6]. These methods are described in some detail in Section 3 where we also discuss the reasons for choosing to focus on these algorithms. It is very difficult to establish theoretical properties of all but the simplest of these algorithms. <p> Variations on the pure random search such as partitioning or shrinking the input space are made to improve its efficiency. A third algorithm is the controlled random search (CRS) which has many variants. The version used in this study is from <ref> [1] </ref>. Finally, the Shu*ed Complex Evolution (SCE) approach of [6] which uses elements of the CRS and partitioned random search techniques is implemented and tested. <p> The first step is to generate a group of random points over the entire search space. In general, the strategy is to replace the worst points of the group with better points through a selected operation. The Nelder-Mead simplex method may be considered as one form of CRS. In <ref> [1] </ref>, a numerical comparison of several CRS algorithms is presented and a new CRS algorithm is introduced. In light of the positive results of the algorithm presented in [1], it is used in this study. <p> The Nelder-Mead simplex method may be considered as one form of CRS. In <ref> [1] </ref>, a numerical comparison of several CRS algorithms is presented and a new CRS algorithm is introduced. In light of the positive results of the algorithm presented in [1], it is used in this study. Controlled Random Search Inputs: N , ffi , tolerance Initialization: Uniformly generate a group, A of N points and evaluate them. <p> Only the crude random search and ARS I are guaranteed to converge in probability to the global optimum as the number of function evaluations increases. On the other hand, ARS II is not even guaranteed to converge to a local optimum. In <ref> [1] </ref>, it is stated that the CRS algorithm is totally heuristic and lacks theoretical convergence properties. According to [9], convergence theorems for the Nelder-Mead algorithm alone are practically nonexistent even though the algorithm was presented in 1965. <p> Given this, it is unlikely that any theoretical convergence results for the SCE algorithm exist. Tests for the efficiency of the algorithms have mainly been done through the use of numerical experiments. In <ref> [1] </ref>, the proposed CRS algorithm is compared with other CRS algorithms on a set of benchmark problems using numerical experiments. In [14], they are used to compare the Nelder-Mead algorithm with other direct methods and in [15] they are used to propose modifications to the algorithm to improve its efficiency.
Reference: [2] <author> B. R. Barmish and C. M. Lagoa. </author> <booktitle> In Proceedings of the 35th IEEE International Conference on Decision and Control, </booktitle> <pages> pages 3418-3423, </pages> <address> Kobe, Japan, </address> <year> 1996. </year>
Reference-contexts: However, there are no such widely accepted benchmark problems, at least as of this writing. There are some results in the literature <ref> [17, 8, 2, 21] </ref>, which indicate that randomized algorithms may provide tractable approaches to the problems of robust stability analysis and performance. Genetic algorithms for robust stability analysis have been investigated in [10, 24]. In [8], it was shown that even simple random search has potentially attractive computational complexity properties.
Reference: [3] <author> M. Bazaraa, H. Sherali, and C. M. Shetty. </author> <title> Nonlinear Programming Theory and Algorithms. </title> <publisher> Wiley & Sons, Inc., </publisher> <address> New York, </address> <note> second edition, </note> <year> 1993. </year>
Reference-contexts: A fairly wide range of random search algorithms is tested in this study. The first algorithm is a multistart version of Rosenbrock's line search method. There are several line search methods such as Powell's [16] and Hooke and Jeeve's <ref> [3] </ref> but they are not examined here. Apart from selecting the initial conditions randomly, the algorithm is purely deterministic. Another set of algorithms contains the pure random search and some variants of it. <p> All of these algorithms place an upper bound on fl opt . 3.1 Line Searches and Random Multistart There are many line search methods for solving optimization problems. The algorithm of Rosenbrock <ref> [3] </ref> is presented here as an example. It is deterministic and also differs from the random searches in that it moves only a single point; the random algorithms presented in this paper attempt to move a group of points. <p> This algorithm is performed N times with varying, random initial points (hence random multistart). Rosenbrock's algorithm is fairly well known and can be found in <ref> [3] </ref>. Rosenbrock's algorithm is attractive because, unlike say Hooke and Jeeve's algorithm [3], it changes its set of line search directions to suit the cost function. Numerical experiments have shown this algorithm to be more efficient than those that do not adapt its line search directions. <p> This algorithm is performed N times with varying, random initial points (hence random multistart). Rosenbrock's algorithm is fairly well known and can be found in <ref> [3] </ref>. Rosenbrock's algorithm is attractive because, unlike say Hooke and Jeeve's algorithm [3], it changes its set of line search directions to suit the cost function. Numerical experiments have shown this algorithm to be more efficient than those that do not adapt its line search directions. The crude line search technique also provides some robustness.
Reference: [4] <author> R. Braatz, P. Young, J. Doyle, and M. </author> <title> Morari. </title> <booktitle> In Proceedings of the American Control Conference, </booktitle> <pages> pages 1682-1683, </pages> <address> San Francisco, California, </address> <year> 1993. </year>
Reference-contexts: This appears to be even more so for the case of real parameter uncertainty. Results on computational complexity <ref> [4, 12] </ref> of robust stability analysis provide strong support for such conclusions. The fundamental underlying reason appears to be the fact that these problems typically involve nonconvex optimization having lots of local minima. It is only in very special cases that one has neat analytical solutions. <p> Experience has shown that these bounds can be arbitrarily conservative and recently, it has been proven that solving these problems is NP hard <ref> [4] </ref>. Therefore a reasonable course of action is to move our focus towards efficient means of coming to an approximate solution and reduce the conservatism of the bounds.
Reference: [5] <author> R. R. E. de Gaston and M. G. </author> <title> Safonov. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 33 </volume> <pages> 156-171, </pages> <year> 1988. </year>
Reference-contexts: (Re ((A + BC))) (2.2) where (X) is the set of eigenvalues of X, then fl opt = inf (kk 1 : (M; ) is unstable) = inf (kk 1 : (M; ) 0): Solving this problem would typically be handled by the real structured singular value (real ) theory <ref> [5, 23] </ref>. However, real analysis and synthesis problems are very difficult and one usually resorts to computing upper and lower bounds on the quantities of interest such as fl opt .
Reference: [6] <author> Q. Duan, S. Sorooshian, and V. </author> <title> Gupta. </title> <journal> Journal of Hydrology, </journal> <volume> 158 </volume> <pages> 265-284, </pages> <year> 1994. </year>
Reference-contexts: Since robust control problems often lead to nondifferentiable optimization problems, direct methods will have broader applicability. The following methods are investigated here: Rosenbrock's method [3];crude and adaptive random search [8];adaptive partitioned random search [20]; controlled random search [1]; and shu*ed complex evolution <ref> [6] </ref>. These methods are described in some detail in Section 3 where we also discuss the reasons for choosing to focus on these algorithms. It is very difficult to establish theoretical properties of all but the simplest of these algorithms. <p> A third algorithm is the controlled random search (CRS) which has many variants. The version used in this study is from [1]. Finally, the Shu*ed Complex Evolution (SCE) approach of <ref> [6] </ref> which uses elements of the CRS and partitioned random search techniques is implemented and tested. All of these algorithms place an upper bound on fl opt . 3.1 Line Searches and Random Multistart There are many line search methods for solving optimization problems. <p> Step 4 is a dedicated local search about the current best point. The number of points , N , in A is user-defined and a value of 10 (n + 1) is suggested. 3.5 Shu*ed Complex Evolution (SCE) The so-called Shu*ed Complex Evolution (SCE) approach of <ref> [6] </ref> combines ideas of APRS and CRS. The APRS part of the algorithm separates the group of points into several partitions and for a certain period of time the partitions are treated independently (until the points are shu*ed). <p> A simplex step is interpreted as an evolutionary step of a population while shu*ing is interpreted as communication and cooperation between populations. The procedure can be found in <ref> [6] </ref> and is outlined below. <p> In this way, all points in the complex have a chance to participate in evolution, but it is the better points that will participate more often. Finally, randomness can enter when an infeasible point is picked due to reflection or contraction. In <ref> [6] </ref> it is suggested that a point randomly chosen from the smallest hypercube that contains the complex replace the infeasible one. The parameters of this approach are s, p, c reflect , c contract , ff, and fi. <p> A higher fi may also increase robustness at the expense of decreased efficiency while a higher ff may decrease robustness since more worst-point replacements occur before a new simplex is chosen from the complex. The relationships between these parameters and robustness and efficiency are examined in <ref> [6] </ref> and are based on numerical experiments. 3.6 Convergence Results Almost all of the algorithms presented lack convergence results. Only the crude random search and ARS I are guaranteed to converge in probability to the global optimum as the number of function evaluations increases.
Reference: [7] <author> M. Fu and R. </author> <title> Barmish. </title> <journal> Systems & Control Letters, </journal> <volume> 11 </volume> <pages> 173-179, </pages> <year> 1988. </year>
Reference-contexts: We begin with two different formulations of the robust stability analysis problem as optimization problems. One of these formulations utilizes a result on maximal unidirectional perturbation bounds due to Fu and Barmish <ref> [7] </ref>. For global optimization, we restrict our attention to direct methods, where "direct" refers to methods that use only the value of the objective function but no derivative or any other analytical information. Since robust control problems often lead to nondifferentiable optimization problems, direct methods will have broader applicability. <p> Consider all perturbations such that kk 1 is unity and for M = (A; B; C) and ~ 2 R, define A = A + ~BC. Fu and Barmish <ref> [7] </ref> provide an analytic expression for the smallest j~j such that A 0 +~A 1 is unstable, where A 0 2 R nfin is stable. <p> Unlike the previous example, the unidirectional perturbation-based cost cannot be computed analytically using the expression in <ref> [7] </ref> due to the size of this problem. Even one attempt to evaluate the cost function results in an out of memory error.
Reference: [8] <author> P. Khargonekar and A. Tikku. </author> <booktitle> In Proceedings of the 35th IEEE International Conference on Decision and Control, </booktitle> <pages> pages 3470-3475, </pages> <address> Kobe, Japan, </address> <year> 1996. </year>
Reference-contexts: However, there are no such widely accepted benchmark problems, at least as of this writing. There are some results in the literature <ref> [17, 8, 2, 21] </ref>, which indicate that randomized algorithms may provide tractable approaches to the problems of robust stability analysis and performance. Genetic algorithms for robust stability analysis have been investigated in [10, 24]. In [8], it was shown that even simple random search has potentially attractive computational complexity properties. <p> There are some results in the literature [17, 8, 2, 21], which indicate that randomized algorithms may provide tractable approaches to the problems of robust stability analysis and performance. Genetic algorithms for robust stability analysis have been investigated in [10, 24]. In <ref> [8] </ref>, it was shown that even simple random search has potentially attractive computational complexity properties. Despite these results, it is quite unclear whether 1 This work was supported in part by the U.S. Army Research Office under Grant Nos. <p> One comes from [18] and represents a linearized model of the closed-loop system of a bank-to-turn, air-to-air missile with a linear H 1 controller. The second is the 55 state, 20 uncertain real parameters multivari-able robust stability analysis problem investigated in <ref> [8] </ref>. p. 1 To measure the computational cost, we use the number of function evaluations and flops as metrics. <p> Some simple adaptations may increase the efficiency of the random search, though possibly at the expense of robustness of the algorithm. Two random search methods, taken from <ref> [8] </ref>, and a method based on that used in [22] are grouped in this section. The first algorithm simply samples from the region of all perturbations with size less than or equal to fl a designated number of times and records the sample that provides the lowest cost. <p> Although the increase in the number of flops for the unidirectional perturbation-based cost function is about two orders of magnitude, the increase in time is only about one order higher. 4.3 Medium-sized Real Example The LTI system, M , obtained from <ref> [8] </ref> is stable and contains 55 states and 20 inputs and outputs, i.e., the number of real parameters is 20. Unlike the previous example, the unidirectional perturbation-based cost cannot be computed analytically using the expression in [7] due to the size of this problem.
Reference: [9] <author> J. Lagarias, J. Reeds, M. Wright, and P. Wright. </author> <type> Technical Report 96-4-07, </type> <institution> Lucent Technologies, </institution> <month> May </month> <year> 1997. </year>
Reference-contexts: On the other hand, ARS II is not even guaranteed to converge to a local optimum. In [1], it is stated that the CRS algorithm is totally heuristic and lacks theoretical convergence properties. According to <ref> [9] </ref>, convergence theorems for the Nelder-Mead algorithm alone are practically nonexistent even though the algorithm was presented in 1965. The authors also prove the convergence of the algorithm for strictly convex functions in one dimension and show the difficulty of finding proofs in higher dimensions.
Reference: [10] <author> C. Marrison and R. </author> <title> Stengel. </title> <booktitle> In Proceedings of American Control Conference, </booktitle> <pages> pages 1484-1489, </pages> <address> Balti-more, Maryland, </address> <year> 1994. </year>
Reference-contexts: There are some results in the literature [17, 8, 2, 21], which indicate that randomized algorithms may provide tractable approaches to the problems of robust stability analysis and performance. Genetic algorithms for robust stability analysis have been investigated in <ref> [10, 24] </ref>. In [8], it was shown that even simple random search has potentially attractive computational complexity properties. Despite these results, it is quite unclear whether 1 This work was supported in part by the U.S. Army Research Office under Grant Nos.
Reference: [11] <author> J. A. Nelder and R. </author> <title> Mead. </title> <journal> Computer Journal, </journal> <volume> 7 </volume> <pages> 308-313, </pages> <year> 1965. </year>
Reference-contexts: The APRS part of the algorithm separates the group of points into several partitions and for a certain period of time the partitions are treated independently (until the points are shu*ed). The CRS part of the algorithm uses the Nelder-Mead <ref> [11] </ref> simplex algorithm to replace the worst points in each of the partitions. A simplex is a set of points in the input space which don't all lie in a subspace of lower dimension than the input space.
Reference: [12] <author> A. </author> <title> Nemirovskii. </title> <journal> Math. of Control, Signals, and Systems, </journal> <volume> 6 </volume> <pages> 99-105, </pages> <year> 1993. </year>
Reference-contexts: This appears to be even more so for the case of real parameter uncertainty. Results on computational complexity <ref> [4, 12] </ref> of robust stability analysis provide strong support for such conclusions. The fundamental underlying reason appears to be the fact that these problems typically involve nonconvex optimization having lots of local minima. It is only in very special cases that one has neat analytical solutions.
Reference: [13] <author> M. P. Newlin and P. M. Young. </author> <booktitle> In Proceedings of the 31st IEEE International Conference on Decision and Control, </booktitle> <pages> pages 3175-3180, </pages> <address> Tucson, Arizona, </address> <year> 1992. </year>
Reference-contexts: In fact, the function must be approximated. 3. It should be noted that one drawback of the optimization methods used in this study is that they provide only upper bounds on the solution. In contrast, a branch and bound technique has been investigated in <ref> [13] </ref> using upper and lower bounds to compute . In the present paper, however, the problem of finding the smallest destabilizing perturbation is formulated differently. The advantage of our formulation over the computation is that frequency gridding is not necessary.
Reference: [14] <editor> J. M. Parkinson and D. Hutchinson. In F. A. Lootsma, editor, </editor> <booktitle> Numerical Methods for Nonlinear Optimization, </booktitle> <pages> pages 99-113. </pages> <publisher> Academic Press, </publisher> <year> 1972. </year>
Reference-contexts: Tests for the efficiency of the algorithms have mainly been done through the use of numerical experiments. In [1], the proposed CRS algorithm is compared with other CRS algorithms on a set of benchmark problems using numerical experiments. In <ref> [14] </ref>, they are used to compare the Nelder-Mead algorithm with other direct methods and in [15] they are used to propose modifications to the algorithm to improve its efficiency. Interestingly enough, several studies have provided different conclusions.
Reference: [15] <editor> J. M. Parkinson and D. Hutchinson. In F. A. Lootsma, editor, </editor> <booktitle> Numerical Methods for Nonlinear Optimization, </booktitle> <pages> pages 115-136. </pages> <publisher> Academic Press, </publisher> <year> 1972. </year>
Reference-contexts: In [1], the proposed CRS algorithm is compared with other CRS algorithms on a set of benchmark problems using numerical experiments. In [14], they are used to compare the Nelder-Mead algorithm with other direct methods and in <ref> [15] </ref> they are used to propose modifications to the algorithm to improve its efficiency. Interestingly enough, several studies have provided different conclusions.
Reference: [16] <author> M. J. D. </author> <title> Powell. </title> <journal> Computer Journal, </journal> <volume> 7 </volume> <pages> 155-162, </pages> <year> 1964. </year>
Reference-contexts: A fairly wide range of random search algorithms is tested in this study. The first algorithm is a multistart version of Rosenbrock's line search method. There are several line search methods such as Powell's <ref> [16] </ref> and Hooke and Jeeve's [3] but they are not examined here. Apart from selecting the initial conditions randomly, the algorithm is purely deterministic. Another set of algorithms contains the pure random search and some variants of it. <p> Interestingly enough, several studies have provided different conclusions. Some claim that the simplex method is inefficient and suffer when the dimension of the input p. 5 space is high while others claim that it is competitive with other methods such as that of Powell <ref> [16] </ref> especially in higher dimensions. Schwefel in [19] has performed a comprehensive numerical study involving many algorithms on a large number of different problems. His results tend to show that the simplex algorithm is very inefficient, especially as it comes close to a solution.
Reference: [17] <author> L. Ray and R. </author> <title> Stengel. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 36 </volume> <pages> 82-87, </pages> <year> 1991. </year>
Reference-contexts: However, there are no such widely accepted benchmark problems, at least as of this writing. There are some results in the literature <ref> [17, 8, 2, 21] </ref>, which indicate that randomized algorithms may provide tractable approaches to the problems of robust stability analysis and performance. Genetic algorithms for robust stability analysis have been investigated in [10, 24]. In [8], it was shown that even simple random search has potentially attractive computational complexity properties.
Reference: [18] <author> C. Schumacher and P. Khargonekar. </author> <booktitle> In Proceedings of the American Control Conference, </booktitle> <pages> pages 2759-2763, </pages> <address> Albuquerque, New Mexico, </address> <year> 1997. </year>
Reference-contexts: It is very difficult to establish theoretical properties of all but the simplest of these algorithms. Therefore, we evaluate the efficacy of the algorithms on two robust stability analysis problems with real-valued uncertainties for this investigation. One comes from <ref> [18] </ref> and represents a linearized model of the closed-loop system of a bank-to-turn, air-to-air missile with a linear H 1 controller. <p> p 0 =p CRS: ffi = 0:1, tolerance = 10 4 SCE: ff = 1, fi = 2n + 1, s = 2n + 1, c reflect = 1, c contract = 0:5 (n is number of parameters) 4.2 Real Missile Controller Example The LTI system, M , comes from <ref> [18] </ref> and represents the closed-loop linearized model of a bank-to-turn, air-to-air missile and a H 1 controller. It is stable, has 7 states and 13 inputs and outputs. Because the number of states is relatively low, the unidirectional perturbation-based cost function can be evaluated as well as the norm-based one.
Reference: [19] <author> H. Schwefel. </author> <title> Evolution and Optimum Seeking. </title> <publisher> Wi-ley & Sons, Inc., </publisher> <address> New York, </address> <year> 1995. </year>
Reference-contexts: Some claim that the simplex method is inefficient and suffer when the dimension of the input p. 5 space is high while others claim that it is competitive with other methods such as that of Powell [16] especially in higher dimensions. Schwefel in <ref> [19] </ref> has performed a comprehensive numerical study involving many algorithms on a large number of different problems. His results tend to show that the simplex algorithm is very inefficient, especially as it comes close to a solution.
Reference: [20] <author> Z. B. </author> <title> Tang. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 39(11) </volume> <pages> 2235-2244, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Since robust control problems often lead to nondifferentiable optimization problems, direct methods will have broader applicability. The following methods are investigated here: Rosenbrock's method [3];crude and adaptive random search [8];adaptive partitioned random search <ref> [20] </ref>; controlled random search [1]; and shu*ed complex evolution [6]. These methods are described in some detail in Section 3 where we also discuss the reasons for choosing to focus on these algorithms. It is very difficult to establish theoretical properties of all but the simplest of these algorithms. <p> The radius is user-specified for each iteration. 3.3 Adaptive Partitioned Random Search (APRS) An adaptive partitioned random search divides the input space into subregions and performs several smaller searches. The version of APRS presented here is from <ref> [20] </ref>. In this approach, the goal of finding the subregion with greatest expectation of containing the maximizer is abandoned and replaced with the goal of finding the partition that provides the greatest expected improvement from the current best estimate of the maximum. This expectation is called the promising index. <p> This expectation is called the promising index. Partitions with greater promising indexes are partitioned further while the remaining, inferior ones are lumped into one partition called the surrounding region. The algorithm of <ref> [20] </ref> for finding a global maximum in a rectangular input space is given below. Adaptive Partitioned Random Search (APRS) Inputs: p 0 , p, p index , best p , shrink factor, tolerance Initialize: Partition the input space into p 0 regions.
Reference: [21] <author> R. Tempo, E. W. Bai, and F. Dabbene. </author> <booktitle> In Proceedings of the 35th IEEE International Conference on Decision and Control, </booktitle> <pages> pages 3424-3428, </pages> <address> Kobe, Japan, </address> <year> 1996. </year>
Reference-contexts: However, there are no such widely accepted benchmark problems, at least as of this writing. There are some results in the literature <ref> [17, 8, 2, 21] </ref>, which indicate that randomized algorithms may provide tractable approaches to the problems of robust stability analysis and performance. Genetic algorithms for robust stability analysis have been investigated in [10, 24]. In [8], it was shown that even simple random search has potentially attractive computational complexity properties.
Reference: [22] <author> A. Yoon, P. Khargonekar, and K. Hebbale. </author> <booktitle> In Proceedings of the American Control Conference, </booktitle> <pages> pages 3359-3364, </pages> <address> Albuquerque, New Mexico, </address> <year> 1997. </year>
Reference-contexts: Direct methods are chosen since gradient information is computationally expensive to approximate. Moreover, we wish to try to solve several problems in the future which are too complex to hope to have gradient information available. For example, in <ref> [22] </ref> the cost function is nondifferentiable and each evaluation involves the solution of a system of nonlinear differential equations with table look-ups, which is done using simulation tools. A fairly wide range of random search algorithms is tested in this study. <p> Some simple adaptations may increase the efficiency of the random search, though possibly at the expense of robustness of the algorithm. Two random search methods, taken from [8], and a method based on that used in <ref> [22] </ref> are grouped in this section. The first algorithm simply samples from the region of all perturbations with size less than or equal to fl a designated number of times and records the sample that provides the lowest cost.
Reference: [23] <author> P. M. Young, M. P. Newlin, and J. C. Doyle. </author> <title> ASME, </title> <journal> Dynamic Systems and Control Division (Publication) DSC, </journal> <volume> 43 </volume> <pages> 5-12, </pages> <year> 1992. </year>
Reference-contexts: (Re ((A + BC))) (2.2) where (X) is the set of eigenvalues of X, then fl opt = inf (kk 1 : (M; ) is unstable) = inf (kk 1 : (M; ) 0): Solving this problem would typically be handled by the real structured singular value (real ) theory <ref> [5, 23] </ref>. However, real analysis and synthesis problems are very difficult and one usually resorts to computing upper and lower bounds on the quantities of interest such as fl opt .
Reference: [24] <author> X. Zhu, Y. Huang, and J. Doyle. </author> <booktitle> In Proceedings of the American Control Conference, </booktitle> <pages> pages 3756-3760, </pages> <address> Albuquerque, New Mexico, </address> <year> 1997. </year> <note> p. 8 </note>
Reference-contexts: There are some results in the literature [17, 8, 2, 21], which indicate that randomized algorithms may provide tractable approaches to the problems of robust stability analysis and performance. Genetic algorithms for robust stability analysis have been investigated in <ref> [10, 24] </ref>. In [8], it was shown that even simple random search has potentially attractive computational complexity properties. Despite these results, it is quite unclear whether 1 This work was supported in part by the U.S. Army Research Office under Grant Nos.
References-found: 24

