URL: http://www.csc.ncsu.edu/faculty/mpsingh/papers/mas/ecai-98.ps
Refering-URL: http://www.csc.ncsu.edu/faculty/mpsingh/papers/mas/
Root-URL: http://www.csc.ncsu.edu
Title: The Intentions of Teams: Team Structure, Endodeixis, and Exodeixis  
Author: Munindar P. Singh 
Abstract: Teams arise in a number of important multiagent applications. Several theories of intentions for teams have been proposed. By and large, these theories tend to model team intentions exclusively on the basis of mental concepts, and fail to acknowledge the internal structure of teams. We present a formal theory of intentions for teams that considers the structure of teams explicitly. In this context, we distinguish between exodeictic and endodeictic intentions, which are conceptualized as pointing outward or inward from a team. These concepts are formalized in a framework that models the structure of teams in terms of their members' commitments and coordination requirements. In this way, our approach combines mental and social concepts in a principled manner. We describe some postulates concerning intentions and structure, and give technical results establishing or falsifying these postulates with different definitions. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> John L. Austin, </author> <title> How to Do Things with Words, </title> <publisher> Clarendon Press, Oxford, </publisher> <year> 1962. </year>
Reference-contexts: Committed Interactions. Certain high-level interactions among team-members occur at the level of their social commitments to each other. These interactions involve the operations on commitments as described in section 2.2. Most operations on commitments are carried out through illocutionary acts between agents <ref> [1] </ref>. These operations occur in a context where the agents' prior commitments include the applicable social policies. Coordinating Interactions. Another subclass of interactions involves the establishment of various conditions in the world by some members that other members rely on.
Reference: [2] <author> Kathleen Carley and Les Gasser, </author> <note> `Computational organization research', in [21], chapter 7, </note> <year> (1998). </year>
Reference-contexts: For this reason, we introduce the social stance or level, which plays a central role in our approach. Just as the intentional stance justifies mental constructs, the social stance enables social ones, such as social commitments. For simplicity, we take the social stance as including organizational aspects <ref> [2] </ref>. We take the notion of individual agents as given, and present a recursive definition of teams, which are also treated as agents.. Mutual Beliefs. Traditional theories, such as [13, 11], involve the notion of mutual belief (essentially the same as common knowledge for our purposes).
Reference: [3] <author> Amedeo Cesta, Maria Miceli, and Paola Rizzo, </author> <title> `Help under risky conditions: Robustness of the social attitude and system performance', </title> <booktitle> in Proceedings of the International Conference on Multiagent Systems, </booktitle> <pages> pp. 1825, </pages> <year> (1996). </year>
Reference-contexts: There are a number of important directions for further work. One is the relationship with group and individual rationality. The connection between rationality and social concepts remains especially under-studied, although some conceptual and theoretical advances have been made <ref> [3, 5] </ref>. A related issue is about how agents may form goals of mutual interest [7], or how the members of a team may collectively reason about their intentions. Some nice theories of argumentation and negotiation are being developed, e.g., [16].
Reference: [4] <author> K. M. Chandy and Jayadev Misra, </author> <title> `How processes learn', </title> <journal> Distributed Computing, </journal> <volume> 1, 4052, </volume> <year> (1986). </year>
Reference-contexts: In fact, it is known that in settings with asynchronous, unreliable or unboundedly delayable communication, mutual beliefs cannot be created. They exist only if present from the start <ref> [4] </ref>. Thus, mutual beliefs are used primarily to establish impossibility results for distributed computing protocols. It is puzzling that the basis for impossibility results would form a cornerstone of theories that seek application in real environments! Exodeixis and Endodeixis.
Reference: [5] <author> Rosaria Conte and Cristiano Castelfranchi, </author> <title> Cognitive and Social Action, </title> <publisher> UCL Press, </publisher> <address> London, </address> <year> 1995. </year>
Reference-contexts: There are a number of important directions for further work. One is the relationship with group and individual rationality. The connection between rationality and social concepts remains especially under-studied, although some conceptual and theoretical advances have been made <ref> [3, 5] </ref>. A related issue is about how agents may form goals of mutual interest [7], or how the members of a team may collectively reason about their intentions. Some nice theories of argumentation and negotiation are being developed, e.g., [16].
Reference: [6] <author> Daniel C. Dennett, </author> <title> The Intentional Stance, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1987. </year>
Reference-contexts: We motivate a set of definitions of intentions of teams that combine aspects of previous work on intentions, coordination, social commitments, and structure. Social Stance. Two powerful and well-known ways of looking at agency are the intentional stance <ref> [6, 14] </ref>, and the knowledge level [15]. These approaches legitimize the ascription of intentions to complex physical systems.
Reference: [7] <author> Frank Dignum and Rosaria Conte, </author> <title> `Intentional agents and goal formation', in Intelligent Agents IV: Agent Theories, Architectures, </title> <booktitle> and Languages (ATAL-97), </booktitle> <pages> pp. 231244. </pages> <publisher> Springer-Verlag, </publisher> <year> (1998). </year>
Reference-contexts: One is the relationship with group and individual rationality. The connection between rationality and social concepts remains especially under-studied, although some conceptual and theoretical advances have been made [3, 5]. A related issue is about how agents may form goals of mutual interest <ref> [7] </ref>, or how the members of a team may collectively reason about their intentions. Some nice theories of argumentation and negotiation are being developed, e.g., [16]. A closer investigation of these forms of negotiation and the creation and maintenance of teams and team intentions remains to be made.
Reference: [8] <author> Barbara Dunin-Keplicz and Rineke Verbrugge, </author> <title> `Collective commitments', </title> <booktitle> in Proceedings of the International Conference on Multiagent Systems, </booktitle> <pages> pp. 5663, </pages> <year> (1996). </year>
Reference-contexts: For simplicity, we do not highlight the temporal operators of the language and assume they are included as needed in the propositions. We define modal accessibility relations for intentions I and commitments C. The former follows the development of [18]; the latter generalizes the construction in <ref> [8] </ref> to allow an explicit social context. I (x; t) gives the paths that are intentional-alternatives for agent x at moment t. This is assumed to be defined only for individuals.
Reference: [9] <author> Ronald Fagin, Joseph Y. Halpern, Yoram Moses, and Moshe Y. Vardi, </author> <title> Reasoning About Knowledge, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1995. </year>
Reference-contexts: Roughly, a set of agents mutually believe p iff each of them believes p, and each of them believes that each of them believes p, and so on, ad infinitum <ref> [9] </ref>. Traditional approaches require mutual beliefs among the team members essentially to achieve the effect that can be more simply attained through social commitments. In fact, it is known that in settings with asynchronous, unreliable or unboundedly delayable communication, mutual beliefs cannot be created.
Reference: [10] <author> Jacques Ferber and Jean-Pierre Muller, </author> <title> `Influences and reaction: A model of situated multiagent sytems', </title> <booktitle> in Proceedings of the International Conference on Multiagent Systems, </booktitle> <pages> pp. 7279, </pages> <year> (1996). </year>
Reference-contexts: Some nice theories of argumentation and negotiation are being developed, e.g., [16]. A closer investigation of these forms of negotiation and the creation and maintenance of teams and team intentions remains to be made. Lastly, there is a large body of work on emergent behavior in mul-tiagent systems, e.g., <ref> [10] </ref>, which gives primacy to the behavior of agents over mental concepts. We showed above how team intentions naturally depend on the coordination relationships of the teams: this provides a potentially powerful means to unite the two camps.
Reference: [11] <author> Barbara J. Grosz and Sarit Kraus, </author> <title> `Collaborative plans for complex group action', </title> <journal> Artificial Intelligence, </journal> <volume> 86(2), 269357, </volume> <month> (October </month> <year> 1996). </year>
Reference-contexts: For simplicity, we take the social stance as including organizational aspects [2]. We take the notion of individual agents as given, and present a recursive definition of teams, which are also treated as agents.. Mutual Beliefs. Traditional theories, such as <ref> [13, 11] </ref>, involve the notion of mutual belief (essentially the same as common knowledge for our purposes). Roughly, a set of agents mutually believe p iff each of them believes p, and each of them believes that each of them believes p, and so on, ad infinitum [9].
Reference: [12] <author> Nick R. Jennings, E. H. Mamdani, Jose Manuel Corera, Inaki Lares-goiti, Fabien Perriollat, Paul Skarek, and Laszlo Zsolt Varga, </author> <title> `Using Archon to develop real-world DAI applications, part 1', </title> <journal> IEEE Expert, </journal> <volume> 11(6), 6470, </volume> <month> (December </month> <year> 1996). </year>
Reference-contexts: To do so is the main objective of this paper. This exercise is of theoretical and practical importance, because not only is this issue of centrality to DAI, but implemented systems involving the intentions of teams now exist, e.g., STEAM [19] and ARCHON <ref> [12] </ref>. Because of the limitations of the present theories, existing systems were forced to invent additional representations to effectively capture the social dimension.
Reference: [13] <author> H. J. Levesque, P. R. Cohen, and J. T. Nunes, </author> <title> `On acting together', </title> <booktitle> in Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pp. 9499, </pages> <year> (1990). </year>
Reference-contexts: For simplicity, we take the social stance as including organizational aspects [2]. We take the notion of individual agents as given, and present a recursive definition of teams, which are also treated as agents.. Mutual Beliefs. Traditional theories, such as <ref> [13, 11] </ref>, involve the notion of mutual belief (essentially the same as common knowledge for our purposes). Roughly, a set of agents mutually believe p iff each of them believes p, and each of them believes that each of them believes p, and so on, ad infinitum [9].
Reference: [14] <author> John McCarthy, </author> <title> `Ascribing mental qualities to machines', </title> <booktitle> in Philosophical Perspectives in Artificial Intelligence, </booktitle> <editor> ed., Martin Ringle, </editor> <publisher> Harvester Press, </publisher> <year> (1979). </year>
Reference-contexts: We motivate a set of definitions of intentions of teams that combine aspects of previous work on intentions, coordination, social commitments, and structure. Social Stance. Two powerful and well-known ways of looking at agency are the intentional stance <ref> [6, 14] </ref>, and the knowledge level [15]. These approaches legitimize the ascription of intentions to complex physical systems.
Reference: [15] <author> Allen Newell, </author> <title> `The knowledge level', </title> <journal> Artificial Intelligence, </journal> <volume> 18(1), 87 127, </volume> <year> (1982). </year>
Reference-contexts: We motivate a set of definitions of intentions of teams that combine aspects of previous work on intentions, coordination, social commitments, and structure. Social Stance. Two powerful and well-known ways of looking at agency are the intentional stance [6, 14], and the knowledge level <ref> [15] </ref>. These approaches legitimize the ascription of intentions to complex physical systems.
Reference: [16] <author> Carles Sierra, Nick R. Jennings, Pablo Noriega, and Simon Parsons, </author> <title> `A framework for argumentation-based negotiation', in Intelligent Agents IV: Agent Theories, Architectures, </title> <booktitle> and Languages (ATAL-97), </booktitle> <pages> pp. 177 192. </pages> <publisher> Springer-Verlag, </publisher> <year> (1998). </year>
Reference-contexts: A related issue is about how agents may form goals of mutual interest [7], or how the members of a team may collectively reason about their intentions. Some nice theories of argumentation and negotiation are being developed, e.g., <ref> [16] </ref>. A closer investigation of these forms of negotiation and the creation and maintenance of teams and team intentions remains to be made. Lastly, there is a large body of work on emergent behavior in mul-tiagent systems, e.g., [10], which gives primacy to the behavior of agents over mental concepts.
Reference: [17] <author> Munindar P. Singh, </author> <title> `A customizable coordination service for autonomous agents', in Intelligent Agents IV: Agent Theories, Architectures, </title> <booktitle> and Languages (ATAL-97), </booktitle> <pages> pp. 93106. </pages> <publisher> Springer-Verlag, </publisher> <year> (1998). </year>
Reference-contexts: This is a typical pattern of coordination where an agent handles contingencies resulting from the other agents' actions. Additional details of semantics and processing are available elsewhere <ref> [17] </ref>. 2.2 Commitments Agents can commit to each other. The debtor commits to the creditor to bring about the discharge condition.
Reference: [18] <author> Munindar P. Singh, Anand S. Rao, and Michael P. Georgeff, </author> <title> `Formal methods in DAI: Logic-based representation and reasoning', </title> <note> in [21], chapter 8, (1998). www.csc.ncsu.edu/ faculty/ mpsingh/ papers/ mas/ formal-DAI.ps. </note>
Reference-contexts: We only have space to cover this informally; for a detailed exposition, please consult <ref> [18] </ref>. Briefly, the model, M , consists of moments arranged according to temporal precedence in a branching structure. Each moment represents a possible state of the world (and has associated intentions and commitments of each agent). <p> The propositional operators are standard. For simplicity, we do not highlight the temporal operators of the language and assume they are included as needed in the propositions. We define modal accessibility relations for intentions I and commitments C. The former follows the development of <ref> [18] </ref>; the latter generalizes the construction in [8] to allow an explicit social context. I (x; t) gives the paths that are intentional-alternatives for agent x at moment t. This is assumed to be defined only for individuals.
Reference: [19] <author> Milind Tambe, </author> <title> `Agent architectures for flexible, practical teamwork', </title> <booktitle> in Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pp. 2228, </pages> <year> (1997). </year>
Reference-contexts: To do so is the main objective of this paper. This exercise is of theoretical and practical importance, because not only is this issue of centrality to DAI, but implemented systems involving the intentions of teams now exist, e.g., STEAM <ref> [19] </ref> and ARCHON [12]. Because of the limitations of the present theories, existing systems were forced to invent additional representations to effectively capture the social dimension.
Reference: [20] <author> Karl E. Weick, </author> <title> The Social Psychology of Organizing, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <booktitle> 2nd edn., </booktitle> <year> 1979. </year>
Reference-contexts: These could be because of constraints of the physical environment or because the members are falling into various habits that have not yet been raised to the level of explicit social commitments <ref> [20] </ref>. Distributed AI and Multiagent Systems 304 M. P.
Reference: [21] <institution> Introduction to Distributed Artificial Intelligence, </institution> <note> ed., </note> <editor> Gerhard Wei, </editor> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1998. </year> <title> Distributed AI and Multiagent Systems 307 M. </title> <editor> P. </editor> <publisher> Singh </publisher>
References-found: 21

