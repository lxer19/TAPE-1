URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR97710-S.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Email: E-mail: meade@rice.edu  
Phone: Phone: (713) 527-8101 ext. 3590 FAX: (713) 285-5423  
Title: SEQUENTIAL FUNCTION APPROXIMATION FOR THE SOLUTION OF DIFFERENTIAL EQUATIONS  
Author: Andrew J. Meade Jr: Michael Kokkolaras, and Boris A. Zeldin 
Note: Communications in Numerical Methods in Engineering Recipient of correspondence. This work was supported under NASA grant number CRA2-35504 and ONR grant N00014-95-1-0741.  
Date: Revised.  
Address: Houston, Texas, 77251-1892, USA  
Affiliation: Department of Mechanical Engineering and Materials Science William Marsh Rice University  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> G. Cybenko, </author> <title> "Approximations by Superpositions of a Sigmoidal Function", </title> <journal> Math. Contr. Signals, Syst., </journal> <volume> 2, </volume> <pages> 303-314, </pages> <year> (1989). </year>
Reference-contexts: In this regard, function approximation in the framework of neural computations has been primarily based on the results of Cybenko <ref> [1] </ref> and Hornik et al. [2], who showed that a continuous d-dimensional function can be arbitrarily well approximated by a linear combination of one-dimensional functions i . u u a n X c i i (( ~; p i )) (1) where 2 R, ~ 2 R d , p i <p> For the one-dimensional problem the domain of interest was selected as = <ref> [0; 1] </ref> with ~ = x and a forcing function of f (x) = x 2 . In the two-dimensional case, = [1; 1] fi [1; 1], ~ = (x; y) T Piecewise linear interpolation functions (B 1 splines) were used. <p> For the one-dimensional problem the domain of interest was selected as = [0; 1] with ~ = x and a forcing function of f (x) = x 2 . In the two-dimensional case, = <ref> [1; 1] </ref> fi [1; 1], ~ = (x; y) T Piecewise linear interpolation functions (B 1 splines) were used. <p> For the one-dimensional problem the domain of interest was selected as = [0; 1] with ~ = x and a forcing function of f (x) = x 2 . In the two-dimensional case, = <ref> [1; 1] </ref> fi [1; 1], ~ = (x; y) T Piecewise linear interpolation functions (B 1 splines) were used.
Reference: [2] <author> K. Hornik, M. Stinchcombe, and H. White, </author> <title> "Multi-Layer Feedforward Networks are Universal Approximators", </title> <booktitle> Neural Networks, </booktitle> <volume> 2, </volume> <pages> 359-366, </pages> <year> (1989). </year>
Reference-contexts: In this regard, function approximation in the framework of neural computations has been primarily based on the results of Cybenko [1] and Hornik et al. <ref> [2] </ref>, who showed that a continuous d-dimensional function can be arbitrarily well approximated by a linear combination of one-dimensional functions i . u u a n X c i i (( ~; p i )) (1) where 2 R, ~ 2 R d , p i 2 R m , and
Reference: [3] <author> D. Rumelhart, G. E. Hinton, and R. J. Williams, </author> <title> "Learning Internal Representations by Error Propagation", </title> <booktitle> Parallel Distributed Processing: Exploration in the Microstruc-ture of Cognition, </booktitle> <pages> 318-362, </pages> <editor> edited by D. Rumelhart, J. L. McClelland, </editor> <booktitle> and the PDP Research Group, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: by solving a non-linear optimization problem with the objective function given by the mean square error over some domain * 2 n ) 2 d ~ : (2) In the neural network literature, the numerical minimization of equation (2) by the steepest descent method is known as the backpropagation algorithm <ref> [3] </ref>. More sophisticated opti 2 mization methods, including the conjugate gradient and the Levenberg-Marquardt meth-ods, have been also used for neural network training. However, it has been found that even these advanced optimization methods are prone to poor convergence [4].
Reference: [4] <author> S. Saarinen, R. Bramley, and G. Cybenko, </author> <title> "Ill-Conditioning in Neural Network Training Problems", </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 14, No. 3, </volume> <pages> 693-714, </pages> <year> (1993). </year>
Reference-contexts: More sophisticated opti 2 mization methods, including the conjugate gradient and the Levenberg-Marquardt meth-ods, have been also used for neural network training. However, it has been found that even these advanced optimization methods are prone to poor convergence <ref> [4] </ref>. Clearly, the training algorithms must address a multi-dimensional optimization problem with non-linear dependence on the network parameters p i .
Reference: [5] <author> L. K. Jones, </author> <title> "Constructive Approximations for Neural Networks by Sigmoidal Functions", </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 78, No. 10, </volume> <pages> 1586-1589, </pages> <year> (1990). </year>
Reference-contexts: However, it has been found that even these advanced optimization methods are prone to poor convergence [4]. Clearly, the training algorithms must address a multi-dimensional optimization problem with non-linear dependence on the network parameters p i . As an alternative, Jones <ref> [5] </ref>, [6] and Barron [7] proposed the following iterative algorithm for sequential approximation: u a n1 ( ~) + c n (( ~; p n )) (3) where p n ; c n , and ff n are optimally selected at each iteration of the algorithm.
Reference: [6] <author> L. K. Jones, </author> <title> "A Simple Lemma on Greedy Approximation in Hilbert Space and Convergence Rates For Projection Pursuit Regression and Neural Network Training", </title> <journal> The Annals of Statistics 20, </journal> <volume> No. 1, </volume> <pages> 608-613, </pages> <year> (1992). </year>
Reference-contexts: However, it has been found that even these advanced optimization methods are prone to poor convergence [4]. Clearly, the training algorithms must address a multi-dimensional optimization problem with non-linear dependence on the network parameters p i . As an alternative, Jones [5], <ref> [6] </ref> and Barron [7] proposed the following iterative algorithm for sequential approximation: u a n1 ( ~) + c n (( ~; p n )) (3) where p n ; c n , and ff n are optimally selected at each iteration of the algorithm.
Reference: [7] <author> A. R. Barron, </author> <title> "Universal Approximation Bounds for Superpositions of a Sigmoidal Function", </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 39, No. 3, </volume> <pages> 930-945, </pages> <year> (1993). </year>
Reference-contexts: However, it has been found that even these advanced optimization methods are prone to poor convergence [4]. Clearly, the training algorithms must address a multi-dimensional optimization problem with non-linear dependence on the network parameters p i . As an alternative, Jones [5], [6] and Barron <ref> [7] </ref> proposed the following iterative algorithm for sequential approximation: u a n1 ( ~) + c n (( ~; p n )) (3) where p n ; c n , and ff n are optimally selected at each iteration of the algorithm.
Reference: [8] <author> M. J. Orr, </author> <title> "Regularization in the Selection of Radial Basis Function Centres", </title> <journal> Neural Computation, </journal> <volume> 7, No. 3, </volume> <pages> 606-623, </pages> <year> (1995). </year>
Reference-contexts: A general principle of statistics was utilized to show that the upper bound of the error * is of the order C= p n, where C is a positive constant. Further, Orr <ref> [8] </ref> introduced a forward selection method of sequential network training; this is essentially a method of incremental function approximation.
Reference: [9] <author> J. Platt, </author> <title> "A Resource-Allocating Network for Function Interpolation", </title> <journal> Neural Computation, </journal> <volume> 3, </volume> <pages> 213-225, </pages> <year> (1991). </year> <month> 12 </month>
Reference-contexts: However, the forward selection training method can be inefficient in that it may require significant computational resources when the set of trial functions is large. A similar principle is utilized in Platt's resource allocating networks (RAN) <ref> [9] </ref>. Whenever an unusual pattern is presented to the network, in on- or off-line network training, a new computational "unit" is allocated. Note that these computational units respond to local regions of the input space.
Reference: [10] <author> E. R. Oliveira, </author> <booktitle> in Proc. Third Conf. Matrix Methods in Struct. Mech., AFFDL-TR-71--160, </booktitle> <pages> 423-446, </pages> <institution> Air Force Flight Dynamics Laboratory, Wright-Patterson AFB, Ohio, </institution> <year> 1971. </year>
Reference-contexts: For example, slow convergence and numerical instability, resulting from poor condition numbers of the associated matrices, have been reported in the literature for uniform grids. Therefore, optimization in computational mechanics has focused primarily in the selection of the optimal grid. Oliveira <ref> [10] </ref> showed that optimal node distribution provides minimal total potential energy of the system. In fact, it was proven that such distributed nodes lie along isoenergetic contours. Felippa [11], [12] followed an optimal node distribution approach.
Reference: [11] <author> C. A. Felippa, </author> <title> "Optimization of Finite Element Grids by Direct Energy Search," </title> <journal> Appl. Math. Modelling, </journal> <volume> 1, </volume> <pages> 93-96, </pages> <year> (1976). </year>
Reference-contexts: Therefore, optimization in computational mechanics has focused primarily in the selection of the optimal grid. Oliveira [10] showed that optimal node distribution provides minimal total potential energy of the system. In fact, it was proven that such distributed nodes lie along isoenergetic contours. Felippa <ref> [11] </ref>, [12] followed an optimal node distribution approach. The direct and com-putationally expensive grid optimization problem is solved for a given configuration of finite elements and the nodes are then relocated. This process is repeated iteratively until convergence to an optimal grid is achieved.
Reference: [12] <author> C. A. Felippa, </author> <title> "Numerical Experiments in Finite Element Grid Optimization by Direct Energy Search," </title> <journal> Appl. Math. Modelling, </journal> <volume> 1, </volume> <pages> 239-244, </pages> <year> (1977). </year>
Reference-contexts: Therefore, optimization in computational mechanics has focused primarily in the selection of the optimal grid. Oliveira [10] showed that optimal node distribution provides minimal total potential energy of the system. In fact, it was proven that such distributed nodes lie along isoenergetic contours. Felippa [11], <ref> [12] </ref> followed an optimal node distribution approach. The direct and com-putationally expensive grid optimization problem is solved for a given configuration of finite elements and the nodes are then relocated. This process is repeated iteratively until convergence to an optimal grid is achieved.
Reference: [13] <author> A. R. Diaz, N. Kikuchi, and J. E. Taylor, </author> <title> "A Method of Grid Optimization for Finite Element Methods", </title> <booktitle> Computer Methods in Applied Mechanics and Engineering, </booktitle> <volume> 41, </volume> <pages> 29-45, </pages> <year> (1983). </year>
Reference-contexts: However, this approach is limited to linear self-adjoint differential operators where energy variational principles are readily available. Results were reported for extremely coarse grids, due to the computational complexity of the associated multi-dimensional optimization problem. Diaz et al. <ref> [13] </ref> discussed an adaptive method to improve the accuracy of the finite element analysis; grid optimization was based on the minimization of an estimated upper bound on the potential energy of the finite element solution.
Reference: [14] <author> L. Demkowicz and J. T. Oden, </author> <title> "On a Mesh Optimization Method Based on a Minimization of Interpolation Error", </title> <journal> Int. J. Engng. Sci., </journal> <volume> 24, No. 1, </volume> <pages> 55-68, </pages> <year> (1986). </year>
Reference-contexts: It is noted that recently developed adaptive methods of computational mechanics substitute the principle of the system potential energy minimum with the weaker criterion of the homogeneous distribution of the approximation error <ref> [14] </ref> - [16]. The method proposed in this paper for solving computational mechanics problems closely resembles the previously discussed adaptive numerical methods.
Reference: [15] <author> A. R. Diaz, N. Kikuchi, and J. E. Taylor, </author> <title> "Optimal Design Formulations for Finite Element Grid Adaptation", </title> <booktitle> Lecture Notes in Mathematics, 1086, Sensitivity of Function-als with Applications in Engineering Science, edited by V. </booktitle> <address> Komokov, </address> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1984. </year>
Reference: [16] <author> R. A. DeVore, </author> <title> "Degree of Nonlinear Approximation", </title> <booktitle> Approximation Theory VI, </booktitle> <volume> 1, </volume> <pages> 175-201, </pages> <editor> edited by C. K. Chui, L. L. Schumaker, and J. D. Ward, </editor> <publisher> Academic Press, </publisher> <year> 1989. </year>
Reference-contexts: It is noted that recently developed adaptive methods of computational mechanics substitute the principle of the system potential energy minimum with the weaker criterion of the homogeneous distribution of the approximation error [14] - <ref> [16] </ref>. The method proposed in this paper for solving computational mechanics problems closely resembles the previously discussed adaptive numerical methods.

References-found: 16

