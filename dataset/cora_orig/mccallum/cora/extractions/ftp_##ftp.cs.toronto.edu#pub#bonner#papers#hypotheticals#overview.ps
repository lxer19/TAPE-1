URL: ftp://ftp.cs.toronto.edu/pub/bonner/papers/hypotheticals/overview.ps
Refering-URL: http://www.cs.toronto.edu/DB/people/bonner/papers.html
Root-URL: 
Email: bonner@db.toronto.edu  
Title: Hypothetical Reasoning with Intuitionistic Logic  
Author: Anthony J. Bonner 
Date: October 1994.  
Note: Appears in R. Demolombe and T. Imielinski, editors, Non-Standard Queries and Answers, Studies on Logic and Computation, Chapter 8, pages 187-219. Oxford University Press,  
Address: Toronto, Ontario M5S 1A4 Canada  
Affiliation: University of Toronto Department of Computer Science  
Abstract: This paper addresses a limitation of most deductive database systems: They cannot reason hypothetically. Although they reason effectively about the world as it is, they are poor at tasks such as planning and design, where one must explore the consequences of hypothetical actions and possibilities. To address this limitation, this paper presents a logic-programming language in which a user can create hypotheses and draw inferences from them. Two types of hypothetical operations are considered: the insertion of tuples into a database, and the creation of new constant symbols. These two operations are interesting, not only because they extend the capabilities of database systems, but also because they fit neatly into a well-established logical framework, namely intuitionistic logic. This paper presents the proof theory for the logic, outlines its intuitionistic model theory, and summarizes results on its complexity and on its ability to express database queries. Our results establish a strong link between two previously unrelated, but well-developed areas: intuitionistic logic and computational complexity. This, in turn, leads to a strong link with classical second-order logic. Moreover, unlike many expressibility results in the literature, our results do not depend on the artificial assumption that the data domain is linearly ordered. This paper is available at the following URL: ftp://db.toronto.edu/pub/bonner/papers/hypotheticals/overview.ps.gz 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Abiteboul, M.Y. Vardi, and V. Vianu. </author> <title> Fixpoint Logics, Relational Machines, and Computational Complexity. </title> <booktitle> In Proceedings of the Seventh Annual Structure in Complexity Theory Conference, </booktitle> <pages> pages 156-168, </pages> <address> Boston, MA, June 22-25 1992. </address> <publisher> IEEE Computer Society Press. </publisher> <pages> 24 </pages>
Reference-contexts: The assumption of ordered domains is a technical device that is often used to achieve expressibility results, but it is not an intrinsic feature of databases <ref> [1] </ref>. Embedded implications do not need this artificial assumption, since they can generate a linear order on the domain hypothetically. Thus, the results of this section are for arbitrary databases, ordered or not. These results characterize the queries in well-known complexity classes based on ordinary Turing machines. <p> Thus, the results of this section are for arbitrary databases, ordered or not. These results characterize the queries in well-known complexity classes based on ordinary Turing machines. In particular, our results are unrelated to the relational complexity classes introduced in <ref> [1] </ref>, which are based on so-called relational Turing machines. Although relational Turing machines capture many of the important features of database programming, they are less expressive than ordinary Turing machines.
Reference: [2] <author> S. Abiteboul and V. Vianu. </author> <title> A Transaction Language Complete for Database Update Specification. </title> <booktitle> In Proceedings of the ACM Symposium on the Principles of Database Systems (PODS), </booktitle> <pages> pages 260-268, </pages> <year> 1987. </year> <note> Published in expanded form as Rapports de Recherche no. 715, </note> <institution> INRIA, </institution> <address> 78153 Le Chesnay Cedex, France. </address>
Reference-contexts: In fact, with negation, they are expressively complete, and can therefore express any computable generic query. In database theory, the ability to express generic queries is the normal measure of expressiveness for database query languages <ref> [2, 3, 17, 18, 32, 33] </ref>. Genericity captures the idea that constant symbols are uninterpreted, i.e., that they have no innate meaning. To make this idea more precise, consider the effect on a query of renaming the constant symbols in the database. <p> The rulebase contains no constant symbols. Theorem 7.3 is particularly interesting because it establishes a strong link between an important complexity class, polynomial space, and a well-known logic, intuitionistic logic. Other database query languages have been developed that are expressively complete for PSPACE <ref> [16, 18, 2, 3] </ref>, but the language of stratified embedded implications is the only one based on a model theoretic semantics, and the only one to have an Prolog-style proof procedure, one based on resolution and unification in the logic-programming tradition [39].
Reference: [3] <author> S. Abiteboul and V. Vianu. </author> <title> Datalog extensions for database queries and updates. </title> <type> Technical Report 900, </type> <institution> Institut National de Recherche en Informatique et en Automatique (INRIA), </institution> <address> Le Chesnay Cedex, France, </address> <year> 1988. </year>
Reference-contexts: In fact, with negation, they are expressively complete, and can therefore express any computable generic query. In database theory, the ability to express generic queries is the normal measure of expressiveness for database query languages <ref> [2, 3, 17, 18, 32, 33] </ref>. Genericity captures the idea that constant symbols are uninterpreted, i.e., that they have no innate meaning. To make this idea more precise, consider the effect on a query of renaming the constant symbols in the database. <p> The rulebase contains no constant symbols. Theorem 7.3 is particularly interesting because it establishes a strong link between an important complexity class, polynomial space, and a well-known logic, intuitionistic logic. Other database query languages have been developed that are expressively complete for PSPACE <ref> [16, 18, 2, 3] </ref>, but the language of stratified embedded implications is the only one based on a model theoretic semantics, and the only one to have an Prolog-style proof procedure, one based on resolution and unification in the logic-programming tradition [39].
Reference: [4] <author> K.R. Apt, H.A. Blair, and A. Walker. </author> <title> Towards a Theory of Declarative Knowledge. </title> <editor> In Jack Minker, editor, </editor> <booktitle> Foundations of Deductive Databases and Logic Programming, chapter 2, </booktitle> <pages> pages 89-148. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: One approach has been to identify classes of Horn rulebases in which such problems do not arise. Perhaps the best known of these classes is the stratified rulebases. These rulebases are layered, and within each layer, a negated premise refers only to rules found in the layers below <ref> [4] </ref>. In this way, recursion never occurs through a negated premise, so the semantics of negation is 4 This example, taken from [14], is an improvement by McCarty on an example originally due to Bonner [10]. always well-defined. Another approach has been to allow limited recursion through negation. <p> Examples 6.1, 7.1 and 7.2 show stratified rulebases and describe their semantics informally. In the special case in which all the embedded implications are Horn, this semantics is equivalent to that of <ref> [4] </ref>. As in Horn logic, stratified rulebases act as a kind of benchmark in the semantics of negation. One reason is that most natural examples of negation are stratified. Another is that the semantics of non-stratified rulebases is likely to remain unsettled for some time.
Reference: [5] <author> K.R. Apt and M.H. Van Emden. </author> <title> Contributions to the Theory of Logic Programming. </title> <journal> Journal of the ACM, </journal> <volume> 29(3) </volume> <pages> 841-862, </pages> <year> 1982. </year>
Reference-contexts: Operationally, the expression :(B C) is interpreted as the failure to prove B C. Thus, A is inferred if B C cannot be inferred. This semantics is similar to the semantics of negation as commonly defined in Horn logic programming <ref> [5] </ref>. Example 6.1 The rules below are part of a rulebase that defines a student's eligibility for financial aid. 4 Intuitively, a student s is eligible for a stipend if he is a near-graduate but not a graduate.
Reference: [6] <author> F. Bancilhon and R. Ramakrishnan. </author> <title> An Amateur's Introduction to Recursive Query Processing Strategies. </title> <booktitle> In Proceedings of the ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 16-52, </pages> <address> Washington, D.C., </address> <month> May 28-30 </month> <year> 1986. </year>
Reference-contexts: Informally, a rule is linear iff recursion occurs through only one premise. In Horn-clause logic, "linear rules play an important role because, (i) there is a belief that most `real life' recursive rules are linear, and (ii) algorithms have been developed to handle them efficiently" <ref> [6] </ref>. A precise definition of linear recursion for embedded implications is given in [11, 9]. The following example illustrates the idea.
Reference: [7] <author> A.J. Bonner. </author> <title> Adding Negation-as-Failure to Intuitionistic Logic Programming: Part II. </title> <note> In preparation. </note>
Reference-contexts: For instance, it is possible that a rulebase may entail 8 x A (x), and yet it may not entail A (b) for any constant, b. Such issues have received little attention in the literature, and we address them in <ref> [7] </ref>. 7 Expressibility This section studies the impact of negation-as-failure on the ability of embedded implications to express database queries. We first show that with negation, embedded implications are expressively complete, i.e., that they can express any computable database query.
Reference: [8] <author> A.J. Bonner. </author> <title> A Logic for Hypothetical Reasoning. </title> <booktitle> In Proceedings of the Seventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 480-484, </pages> <address> Saint Paul, MN, </address> <month> August 21-26 </month> <year> 1988. </year> <note> Published in expanded form as Technical Report TR-DCS-230, </note> <institution> Department of Computer Science, Rutgers University, </institution> <address> New Brunswick, NJ 08903. </address>
Reference-contexts: We discuss this work in Section 6. This paper adds to the picture by summarizing some of our own results on hypothetical inference. Our work has considered a variety of hypothetical operations, including insertion, deletion and bulk updates <ref> [11, 10, 15, 12, 13, 14, 9, 8] </ref>. This paper, however, focuses on two specific operations: (i) adding formulas to a database hypothetically ("insertion"), and (ii) creating new constant symbols hypothetically ("creation"). The ability to create new constants is achieved without using function symbols. <p> Note that the premise of the first rule is a query similar to the one in Example 2.2. 2 In <ref> [8] </ref>, it is shown that the university policy in Example 2.3 cannot be expressed in Datalog or in any query language based on classical logic. Hypothetical rules thus increase the capabilities of deductive database systems. The next two examples demonstrate the use of universal quantification in queries and rule bodies. <p> Working independently, McCarty and Miller extended this work to include rules with universal quantifiers in their premises [38, 41]. The work of McCarty also includes formulas with negations in rule heads. Recently, a greatly simplified proof of soundness and completeness was developed by Bonner, first for the propositional case <ref> [8] </ref>, and then for the full predicate case [15]. The proof is not based on the fixpoint constructions of logic programming, but on the canonical-model constructions of Modal logic [21]. The rest of this section outlines intuitionistic model-theory, specializing the presentation for the special case of embedded implications.
Reference: [9] <author> A.J. Bonner. </author> <title> Hypothetical Datalog: Negation and Linear Recursion. </title> <booktitle> In Proceedings of the ACM Symposium on the Principles of Database Systems (PODS), </booktitle> <pages> pages 286-300, </pages> <address> Philadelphia, PA, </address> <month> March 29-31 </month> <year> 1989. </year>
Reference-contexts: We discuss this work in Section 6. This paper adds to the picture by summarizing some of our own results on hypothetical inference. Our work has considered a variety of hypothetical operations, including insertion, deletion and bulk updates <ref> [11, 10, 15, 12, 13, 14, 9, 8] </ref>. This paper, however, focuses on two specific operations: (i) adding formulas to a database hypothetically ("insertion"), and (ii) creating new constant symbols hypothetically ("creation"). The ability to create new constants is achieved without using function symbols. <p> That is, given and R, the system returns "yes" iff R ` can be derived. In databases and logic programming, it is common to ask queries that return a set of tuples as answers. Such queries can be defined using embedded implications with free variables <ref> [10, 15, 9, 26, 27, 39] </ref>. We address this issue in Section 7, where we discuss database queries in general. Until then, however, we shall assume that a query is a closed formula. <p> In Horn-clause logic, "linear rules play an important role because, (i) there is a belief that most `real life' recursive rules are linear, and (ii) algorithms have been developed to handle them efficiently" [6]. A precise definition of linear recursion for embedded implications is given in <ref> [11, 9] </ref>. The following example illustrates the idea. <p> Detailed proofs are given in <ref> [11, 9] </ref>. Theorem 5.3 (Linear Recursion) If universal quantifiers are not used in rule bodies, and if recursion is linear, then the data complexity of embedded implications is complete for NP. 6 Negation as Failure This section extends embedded implications by allowing negated premises.
Reference: [10] <author> A.J. Bonner. </author> <title> Hypothetical Datalog: Complexity and Expressibility. </title> <journal> Theoretical Computer Science (TCS), </journal> <volume> 76 </volume> <pages> 3-51, </pages> <year> 1990. </year> <note> Special issue on the 2 nd International Conference on Database Theory (ICDT). </note>
Reference-contexts: We discuss this work in Section 6. This paper adds to the picture by summarizing some of our own results on hypothetical inference. Our work has considered a variety of hypothetical operations, including insertion, deletion and bulk updates <ref> [11, 10, 15, 12, 13, 14, 9, 8] </ref>. This paper, however, focuses on two specific operations: (i) adding formulas to a database hypothetically ("insertion"), and (ii) creating new constant symbols hypothetically ("creation"). The ability to create new constants is achieved without using function symbols. <p> Embedded implications are first-order formulas that can hypothetically insert formulas into a database, and can hypothetically create new constant symbols. Such systems have been developed by several researchers <ref> [26, 38, 41, 10, 15, 37] </ref>. This section defines a simplified version of these systems, one that retains many of the essential properties of the more elaborate systems while admitting a clean theoretical analysis. One notable feature of this system is that it allows universal quantifiers in rule bodies. <p> That is, given and R, the system returns "yes" iff R ` can be derived. In databases and logic programming, it is common to ask queries that return a set of tuples as answers. Such queries can be defined using embedded implications with free variables <ref> [10, 15, 9, 26, 27, 39] </ref>. We address this issue in Section 7, where we discuss database queries in general. Until then, however, we shall assume that a query is a closed formula. <p> Theorem 5.2 (No New Constants) If universal quantifiers are not used in rule bodies, then the data complexity of embedded implications is complete for PSPACE. Detailed proofs of this theorem are given in [11] and <ref> [10] </ref>. Briefly, inference can be viewed as a top-down search through an and/or proof tree of polynomial depth. Because of the depth bound, the search can be carried out in polynomial space, which establishes the complexity upper-bound. <p> In this way, recursion never occurs through a negated premise, so the semantics of negation is 4 This example, taken from [14], is an improvement by McCarty on an example originally due to Bonner <ref> [10] </ref>. always well-defined. Another approach has been to allow limited recursion through negation. This approach generalizes the notion of stratification to local stratification [45]. Intuitively, a rulebase is locally stratified if its ground instantiation is stratified (with possibly infinitely many strata). <p> Perhaps the best known are the well-founded semantics [28, 52] and the stable model semantics [29]. Although, these semantics offer different interpretations of recursion through negation, they are equivalent when rulebases are stratified or locally stratified. Several researchers have extended these ideas to embedded implications with negation. For instance, <ref> [14, 10, 11] </ref> develop a semantics of stratified rulebases, including a proof theory and model theory. Examples 6.1, 7.1 and 7.2 show stratified rulebases and describe their semantics informally. In the special case in which all the embedded implications are Horn, this semantics is equivalent to that of [4]. <p> None of the expressibility results in Section 7 depends on recursion through negation. For the purpose of this paper, then, we need only consider stratified rulebases. As described above, <ref> [14, 10, 11] </ref> extend the notion of stratification from Horn programs to embedded implications. The main new idea is that in defining strata, hypothetical assumptions can be ignored. <p> As we shall see, this query is expressible in embedded implications, although it is not expressible in many other relational query languages. Before continuing, we must define what it means to express a database query using embedded implications. Complete definitions are given in <ref> [10, 11, 15] </ref>. Here, we summarize the main ideas. Formally, a database query is a mapping that takes a database as input and returns a set of ground tuples as output [17]. We can use embedded implications with free variables to define such mappings. <p> Datalog, for instance, is expressively 19 complete for PTIME if augmented with both negation-as-failure and a linear order. Em--bedded implications do not need to make this latter assumption, however, since they can generate a linear order for themselves, and insert it into the database hypothetically <ref> [11, 10] </ref>. Indeed, negation-as-failure is the only device needed to achieve expressive completeness for embedded implications. Furthermore, negation-as-failure is a natural extension to the logic, since any practical logic-programming system must have it. 7.1 Examples This section gives two examples of the expressiveness of embedded implications augmented with negation-as-failure.
Reference: [11] <author> A.J. Bonner. </author> <title> Hypothetical Reasoning in Deductive Databases. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Rutgers University, </institution> <address> New Brunswick, NJ 08903, USA, </address> <month> October </month> <year> 1991. </year> <note> Published as Rutgers Technical Report DCS-TR-283. </note>
Reference-contexts: We discuss this work in Section 6. This paper adds to the picture by summarizing some of our own results on hypothetical inference. Our work has considered a variety of hypothetical operations, including insertion, deletion and bulk updates <ref> [11, 10, 15, 12, 13, 14, 9, 8] </ref>. This paper, however, focuses on two specific operations: (i) adding formulas to a database hypothetically ("insertion"), and (ii) creating new constant symbols hypothetically ("creation"). The ability to create new constants is achieved without using function symbols. <p> Derivations thus have the form R + DB ` . Informally, the data complexity of this system is the complexity of inference when the rulebase is fixed and the database varies (acts as input). More formal definitions are given in <ref> [18, 53, 11] </ref>. We first show that inference with embedded implications is semi-decidable, and more specifically, that its data complexity is complete for re. <p> Theorem 5.2 (No New Constants) If universal quantifiers are not used in rule bodies, then the data complexity of embedded implications is complete for PSPACE. Detailed proofs of this theorem are given in <ref> [11] </ref> and [10]. Briefly, inference can be viewed as a top-down search through an and/or proof tree of polynomial depth. Because of the depth bound, the search can be carried out in polynomial space, which establishes the complexity upper-bound. <p> In Horn-clause logic, "linear rules play an important role because, (i) there is a belief that most `real life' recursive rules are linear, and (ii) algorithms have been developed to handle them efficiently" [6]. A precise definition of linear recursion for embedded implications is given in <ref> [11, 9] </ref>. The following example illustrates the idea. <p> Detailed proofs are given in <ref> [11, 9] </ref>. Theorem 5.3 (Linear Recursion) If universal quantifiers are not used in rule bodies, and if recursion is linear, then the data complexity of embedded implications is complete for NP. 6 Negation as Failure This section extends embedded implications by allowing negated premises. <p> Perhaps the best known are the well-founded semantics [28, 52] and the stable model semantics [29]. Although, these semantics offer different interpretations of recursion through negation, they are equivalent when rulebases are stratified or locally stratified. Several researchers have extended these ideas to embedded implications with negation. For instance, <ref> [14, 10, 11] </ref> develop a semantics of stratified rulebases, including a proof theory and model theory. Examples 6.1, 7.1 and 7.2 show stratified rulebases and describe their semantics informally. In the special case in which all the embedded implications are Horn, this semantics is equivalent to that of [4]. <p> None of the expressibility results in Section 7 depends on recursion through negation. For the purpose of this paper, then, we need only consider stratified rulebases. As described above, <ref> [14, 10, 11] </ref> extend the notion of stratification from Horn programs to embedded implications. The main new idea is that in defining strata, hypothetical assumptions can be ignored. <p> Indeed, the real surprize is that this 17 difference does not make itself apparent until the proof theory is augmented with negation--as-failure. In the rest of this paper, we distinguish implicitly between these two kinds of implication. Thorough developments of them can be found in <ref> [11] </ref> and in [14]. [11] develops a relatively simple semantics, while [14] develops a more complex semantics, but one that is much closer to intuitionistic model theory. These works address the paradoxes due to hypothetical insertion. Additional paradoxes arise, however, when universal quantifiers are allowed in rule bodies. <p> Indeed, the real surprize is that this 17 difference does not make itself apparent until the proof theory is augmented with negation--as-failure. In the rest of this paper, we distinguish implicitly between these two kinds of implication. Thorough developments of them can be found in <ref> [11] </ref> and in [14]. [11] develops a relatively simple semantics, while [14] develops a more complex semantics, but one that is much closer to intuitionistic model theory. These works address the paradoxes due to hypothetical insertion. Additional paradoxes arise, however, when universal quantifiers are allowed in rule bodies. <p> As we shall see, this query is expressible in embedded implications, although it is not expressible in many other relational query languages. Before continuing, we must define what it means to express a database query using embedded implications. Complete definitions are given in <ref> [10, 11, 15] </ref>. Here, we summarize the main ideas. Formally, a database query is a mapping that takes a database as input and returns a set of ground tuples as output [17]. We can use embedded implications with free variables to define such mappings. <p> Datalog, for instance, is expressively 19 complete for PTIME if augmented with both negation-as-failure and a linear order. Em--bedded implications do not need to make this latter assumption, however, since they can generate a linear order for themselves, and insert it into the database hypothetically <ref> [11, 10] </ref>. Indeed, negation-as-failure is the only device needed to achieve expressive completeness for embedded implications. Furthermore, negation-as-failure is a natural extension to the logic, since any practical logic-programming system must have it. 7.1 Examples This section gives two examples of the expressiveness of embedded implications augmented with negation-as-failure. <p> Thus, the query EVEN in Example 7.1 is generic, since its output is not affected by a renaming of the constant symbols in the database. Likewise, the query in Example 7.2 is generic. More precise definitions of genericity are given in <ref> [17, 11] </ref>. The following theorem is our first result about the ability of embedded implications to express generic queries. Theorem 7.1 Stratified embedded-implications are expressively complete; that is, they express all the generic queries in re. <p> Technically, we can obtain expressive completeness for P k by assuming that the top stratum of the rulebase is hypothetical, as in Examples 7.1 and 7.2. This result leads to a characterization of the queries in P k <ref> [11] </ref>. It also leads to an elegant characterization of the entire polynomial-time hierarchy. Theorem 7.6 Under the following restrictions, embedded implications express exactly the generic queries in PHIER: 1. The rulebase is stratified. 2. Recursion is linear. 3. Universal quantifiers do not appear in rule bodies. 4.
Reference: [12] <author> A.J. Bonner. </author> <title> The complexity of reusing and modifying rulebases. </title> <booktitle> In Proceedings of the ACM Symposium on the Principles of Database Systems (PODS), </booktitle> <pages> pages 316-330, </pages> <address> San Diego, CA, </address> <month> June 2-4 </month> <year> 1992. </year>
Reference-contexts: We discuss this work in Section 6. This paper adds to the picture by summarizing some of our own results on hypothetical inference. Our work has considered a variety of hypothetical operations, including insertion, deletion and bulk updates <ref> [11, 10, 15, 12, 13, 14, 9, 8] </ref>. This paper, however, focuses on two specific operations: (i) adding formulas to a database hypothetically ("insertion"), and (ii) creating new constant symbols hypothetically ("creation"). The ability to create new constants is achieved without using function symbols.
Reference: [13] <author> A.J. Bonner and T. Imielinski. </author> <title> The Reuse and Modification of Rulebases by Predicate Substitution. </title> <booktitle> In Proceedings of the International Conference on Extending Database Technology (EDBT), </booktitle> <pages> pages 437-451, </pages> <address> Venice, Italy, </address> <month> March 26-30 </month> <year> 1990. </year> <note> Springer-Verlag. Published as volume 416 of Lecture Notes in Computer Science. 25 </note>
Reference-contexts: We discuss this work in Section 6. This paper adds to the picture by summarizing some of our own results on hypothetical inference. Our work has considered a variety of hypothetical operations, including insertion, deletion and bulk updates <ref> [11, 10, 15, 12, 13, 14, 9, 8] </ref>. This paper, however, focuses on two specific operations: (i) adding formulas to a database hypothetically ("insertion"), and (ii) creating new constant symbols hypothetically ("creation"). The ability to create new constants is achieved without using function symbols.
Reference: [14] <author> A.J. Bonner and L.T. McCarty. </author> <title> Adding Negation-as-Failure to Intuitionistic Logic Programming. </title> <booktitle> In Proceedings of the North American Conference on Logic Programming (NACLP), </booktitle> <pages> pages 681-703, </pages> <address> Austin, Texas, Oct 29-Nov 1 1990. </address> <publisher> MIT Press. </publisher>
Reference-contexts: Earlier, Statman showed that intuitionistic logic is PSPACE-complete in the propositional case [48]. Several researchers have also investigated the the semantics of negation-as-failure for hypothetical insertions <ref> [25, 14, 31, 43, 30, 22] </ref>. We discuss this work in Section 6. This paper adds to the picture by summarizing some of our own results on hypothetical inference. <p> We discuss this work in Section 6. This paper adds to the picture by summarizing some of our own results on hypothetical inference. Our work has considered a variety of hypothetical operations, including insertion, deletion and bulk updates <ref> [11, 10, 15, 12, 13, 14, 9, 8] </ref>. This paper, however, focuses on two specific operations: (i) adding formulas to a database hypothetically ("insertion"), and (ii) creating new constant symbols hypothetically ("creation"). The ability to create new constants is achieved without using function symbols. <p> These rulebases are layered, and within each layer, a negated premise refers only to rules found in the layers below [4]. In this way, recursion never occurs through a negated premise, so the semantics of negation is 4 This example, taken from <ref> [14] </ref>, is an improvement by McCarty on an example originally due to Bonner [10]. always well-defined. Another approach has been to allow limited recursion through negation. This approach generalizes the notion of stratification to local stratification [45]. <p> Perhaps the best known are the well-founded semantics [28, 52] and the stable model semantics [29]. Although, these semantics offer different interpretations of recursion through negation, they are equivalent when rulebases are stratified or locally stratified. Several researchers have extended these ideas to embedded implications with negation. For instance, <ref> [14, 10, 11] </ref> develop a semantics of stratified rulebases, including a proof theory and model theory. Examples 6.1, 7.1 and 7.2 show stratified rulebases and describe their semantics informally. In the special case in which all the embedded implications are Horn, this semantics is equivalent to that of [4]. <p> Following [41], these proposals each define a model as a mapping from a set of logic programs to a set of values. These proposals are therefore not model theories in the traditional sense, since they only consider models of a certain syntactic form. (In contrast, <ref> [14] </ref> takes the traditional position that a model is a mapping from any set to a set of first-order semantic structures.) In [31], Harland develops a semantics in which predicates are labelled as "completely defined" or "incompletely defined". <p> Unfortunately, restrictions on the labelling preclude many stratified programs, such as those in Examples 7.1 and 7.2. They also preclude the rulebases used to achieve the expressibility results of Section 7. This point is discussed at greater length in <ref> [14] </ref>. In [43], Olivetti and Terracini present a 3-valued modal semantics for the propositional case, and in [30], Giordano and Olivetti develop a 3-valued fixpoint semantics for the predicate case. In both cases, the semantics is of finite failure in arbitrary rulebases. [30] also develops a top-down SLD-style proof procedure. <p> None of the expressibility results in Section 7 depends on recursion through negation. For the purpose of this paper, then, we need only consider stratified rulebases. As described above, <ref> [14, 10, 11] </ref> extend the notion of stratification from Horn programs to embedded implications. The main new idea is that in defining strata, hypothetical assumptions can be ignored. <p> In fact, if implicational queries are allowed, then these paradoxes arise even for Horn rules. The next example shows, for instance, that for stratified Horn rules, implication is not transitive. Other apparent paradoxes, including a failure of Modus Ponens, are described in <ref> [14] </ref>. Example 6.3 Suppose that R consists of the following two rules: A B; ~ C: Then R ` A B and R ` B C, but R 6` A C. Each of these points is straightforward to prove. <p> Thus, R + C 6` A. Hence R 6` A C. 2 These paradoxes are not hard to resolve, and as the examples in this paper suggest, they do not affect the practice of logic programming. As first pointed out in <ref> [14] </ref>, embedded implications really have two distinct kinds of implication|a distinction that one naturally makes when writing logic programs. In the rule A (B C), for instance, the outer implication and the inner implication are of a different kind. <p> Indeed, the real surprize is that this 17 difference does not make itself apparent until the proof theory is augmented with negation--as-failure. In the rest of this paper, we distinguish implicitly between these two kinds of implication. Thorough developments of them can be found in [11] and in <ref> [14] </ref>. [11] develops a relatively simple semantics, while [14] develops a more complex semantics, but one that is much closer to intuitionistic model theory. These works address the paradoxes due to hypothetical insertion. Additional paradoxes arise, however, when universal quantifiers are allowed in rule bodies. <p> In the rest of this paper, we distinguish implicitly between these two kinds of implication. Thorough developments of them can be found in [11] and in <ref> [14] </ref>. [11] develops a relatively simple semantics, while [14] develops a more complex semantics, but one that is much closer to intuitionistic model theory. These works address the paradoxes due to hypothetical insertion. Additional paradoxes arise, however, when universal quantifiers are allowed in rule bodies.
Reference: [15] <author> A.J. Bonner, L.T. McCarty, and K. Vadaparty. </author> <title> Expressing Database Queries with Intu-itionistic Logic. </title> <booktitle> In Proceedings of the North American Conference on Logic Programming (NACLP), </booktitle> <pages> pages 831-850, </pages> <address> Cleveland, Ohio, October 16-20 1989. </address> <publisher> MIT Press. </publisher>
Reference-contexts: We discuss this work in Section 6. This paper adds to the picture by summarizing some of our own results on hypothetical inference. Our work has considered a variety of hypothetical operations, including insertion, deletion and bulk updates <ref> [11, 10, 15, 12, 13, 14, 9, 8] </ref>. This paper, however, focuses on two specific operations: (i) adding formulas to a database hypothetically ("insertion"), and (ii) creating new constant symbols hypothetically ("creation"). The ability to create new constants is achieved without using function symbols. <p> Embedded implications are first-order formulas that can hypothetically insert formulas into a database, and can hypothetically create new constant symbols. Such systems have been developed by several researchers <ref> [26, 38, 41, 10, 15, 37] </ref>. This section defines a simplified version of these systems, one that retains many of the essential properties of the more elaborate systems while admitting a clean theoretical analysis. One notable feature of this system is that it allows universal quantifiers in rule bodies. <p> That is, given and R, the system returns "yes" iff R ` can be derived. In databases and logic programming, it is common to ask queries that return a set of tuples as answers. Such queries can be defined using embedded implications with free variables <ref> [10, 15, 9, 26, 27, 39] </ref>. We address this issue in Section 7, where we discuss database queries in general. Until then, however, we shall assume that a query is a closed formula. <p> The ability to create constants in this way is crucial to the inference system, and it is the basis of the Prolog-style proof procedure developed in [39]. In <ref> [15] </ref>, this ability is used to simulate the computations of arbitrary Turing machines. Without the ability to create new constant symbols, it is only possible to simulate Turing machines that use a polynomial amount of space, i.e., PSPACE-machines (see Section 5). <p> The work of McCarty also includes formulas with negations in rule heads. Recently, a greatly simplified proof of soundness and completeness was developed by Bonner, first for the propositional case [8], and then for the full predicate case <ref> [15] </ref>. The proof is not based on the fixpoint constructions of logic programming, but on the canonical-model constructions of Modal logic [21]. The rest of this section outlines intuitionistic model-theory, specializing the presentation for the special case of embedded implications. <p> We have assumed here, however, that the domain of each substate is equal to the universe of all constant symbols. In <ref> [15] </ref>, we show that for embedded implications, these are the only kind of structures that one needs to consider. We use these simplified structures since they simplify the theoretical development. Truth in an intuitionistic structure M is defined relative to its substates. <p> Definition 4.4 (Validity) A formula is valid iff M j= for all intuitionistic structures M . Definition 4.5 (Entailment) Suppose 1 and 2 are formulas. Then 1 j= 2 iff the formula 2 1 is valid. The following theorem is a central result of <ref> [15] </ref>: Theorem 4.1 (Soundness and Completeness) If is an embedded implication, and R is a set of embedded implications, then R j= iff R ` . 5 Data Complexity This section establishes the data complexity of embedded implications. <p> Since embedded implications can generate an unbounded number of new constant symbols, we can construct a counter of unbounded range, and thereby use as much time and tape as the Turing machine needs. A detailed proof is given in <ref> [15] </ref>. The computational power of embedded implications thus comes from their ability to generate new constant symbols during inference. This ability, in turn, comes from the presence of universal quantifiers in rule bodies, as in the rule A (x) 8 y B (x; y). <p> As we shall see, this query is expressible in embedded implications, although it is not expressible in many other relational query languages. Before continuing, we must define what it means to express a database query using embedded implications. Complete definitions are given in <ref> [10, 11, 15] </ref>. Here, we summarize the main ideas. Formally, a database query is a mapping that takes a database as input and returns a set of ground tuples as output [17]. We can use embedded implications with free variables to define such mappings. <p> Thus, negation-as-failure increases the data complexity of embedded implications. In fact, by using multiple strata, it provides enough power to express queries at any level in the arithmetic hierarchy. It would be interesting, however, to provide an exact, logical characterization of the generic queries in re. To this end, <ref> [15] </ref> presents simple syntactic restrictions that do precisely this. Any query satisfying these restriction is guaranteed to be in re, and all queries in re can be expressed by a rulebase satisfying these restrictions. <p> Many of the results relating to the creation of new constant symbols were developed in collaboration with Thorne McCarty and Kumar Vadaparty <ref> [15] </ref>. They also provided useful comments on the present paper. Thanks go to Eric Allender for answering all my questions about computational complexity, and to Ashok Chandra for providing valuable comments. Jan Chomicki and Ron van der Meyden also provided useful feedback during the development of the work presented herein.
Reference: [16] <author> A.K. Chandra. </author> <title> Theory of Database Queries. </title> <booktitle> In Proceedings of the ACM Symposium on the Principles of Database Systems (PODS), </booktitle> <pages> pages 1-9, </pages> <address> Austin, Texas, </address> <month> March </month> <year> 1988. </year>
Reference-contexts: The rulebase contains no constant symbols. Theorem 7.3 is particularly interesting because it establishes a strong link between an important complexity class, polynomial space, and a well-known logic, intuitionistic logic. Other database query languages have been developed that are expressively complete for PSPACE <ref> [16, 18, 2, 3] </ref>, but the language of stratified embedded implications is the only one based on a model theoretic semantics, and the only one to have an Prolog-style proof procedure, one based on resolution and unification in the logic-programming tradition [39].
Reference: [17] <author> A.K. Chandra and D. Harel. </author> <title> Computable Queries for Relational Databases. </title> <journal> Journal of Computer and System Sciences (JCSS), </journal> <volume> 21(2) </volume> <pages> 156-178, </pages> <year> 1980. </year>
Reference-contexts: Complete definitions are given in [10, 11, 15]. Here, we summarize the main ideas. Formally, a database query is a mapping that takes a database as input and returns a set of ground tuples as output <ref> [17] </ref>. We can use embedded implications with free variables to define such mappings. Some care is required though, since the inference system 18 of Definition 3.2 is defined only for embedded implications that are closed, i.e., that have no free variables. <p> In fact, with negation, they are expressively complete, and can therefore express any computable generic query. In database theory, the ability to express generic queries is the normal measure of expressiveness for database query languages <ref> [2, 3, 17, 18, 32, 33] </ref>. Genericity captures the idea that constant symbols are uninterpreted, i.e., that they have no innate meaning. To make this idea more precise, consider the effect on a query of renaming the constant symbols in the database. <p> Thus, the query EVEN in Example 7.1 is generic, since its output is not affected by a renaming of the constant symbols in the database. Likewise, the query in Example 7.2 is generic. More precise definitions of genericity are given in <ref> [17, 11] </ref>. The following theorem is our first result about the ability of embedded implications to express generic queries. Theorem 7.1 Stratified embedded-implications are expressively complete; that is, they express all the generic queries in re.
Reference: [18] <author> A.K. Chandra and D. Harel. </author> <title> Structure and Complexity of Relational Queries. </title> <journal> Journal of Computer and System Sciences (JCSS), </journal> <volume> 25(1) </volume> <pages> 99-128, </pages> <year> 1982. </year>
Reference-contexts: Derivations thus have the form R + DB ` . Informally, the data complexity of this system is the complexity of inference when the rulebase is fixed and the database varies (acts as input). More formal definitions are given in <ref> [18, 53, 11] </ref>. We first show that inference with embedded implications is semi-decidable, and more specifically, that its data complexity is complete for re. <p> Again, this is true not just of embedded implications, but of all monotonic logics. Augmenting a logic with negation-as-failure gives it the power to express non-monotonic queries. Often, however, this increased power is still limited. For instance, Datalog with negation-as-failure still cannot express EVEN <ref> [18] </ref>. In such cases, a logic is often augmented with other devices, to guarantee expressive completeness. One typical device is to assume that the data domain is linearly ordered [32, 53, 19]. Datalog, for instance, is expressively 19 complete for PTIME if augmented with both negation-as-failure and a linear order. <p> In fact, with negation, they are expressively complete, and can therefore express any computable generic query. In database theory, the ability to express generic queries is the normal measure of expressiveness for database query languages <ref> [2, 3, 17, 18, 32, 33] </ref>. Genericity captures the idea that constant symbols are uninterpreted, i.e., that they have no innate meaning. To make this idea more precise, consider the effect on a query of renaming the constant symbols in the database. <p> The rulebase contains no constant symbols. Theorem 7.3 is particularly interesting because it establishes a strong link between an important complexity class, polynomial space, and a well-known logic, intuitionistic logic. Other database query languages have been developed that are expressively complete for PSPACE <ref> [16, 18, 2, 3] </ref>, but the language of stratified embedded implications is the only one based on a model theoretic semantics, and the only one to have an Prolog-style proof procedure, one based on resolution and unification in the logic-programming tradition [39].
Reference: [19] <author> A.K. Chandra and D. Harel. </author> <title> Horn Clause Queries and Generlizations. </title> <journal> Journal of Logic Programming (JLP), </journal> <volume> 2(1) </volume> <pages> 1-15, </pages> <year> 1985. </year>
Reference-contexts: As a corollary, we conclude that stratified linear rulebases express exactly the second-order definable queries. Unlike other results in the literature <ref> [32, 53, 19] </ref>, the results in this section do not assume that the data domain is linearly ordered. The assumption of ordered domains is a technical device that is often used to achieve expressibility results, but it is not an intrinsic feature of databases [1]. <p> Often, however, this increased power is still limited. For instance, Datalog with negation-as-failure still cannot express EVEN [18]. In such cases, a logic is often augmented with other devices, to guarantee expressive completeness. One typical device is to assume that the data domain is linearly ordered <ref> [32, 53, 19] </ref>. Datalog, for instance, is expressively 19 complete for PTIME if augmented with both negation-as-failure and a linear order. Em--bedded implications do not need to make this latter assumption, however, since they can generate a linear order for themselves, and insert it into the database hypothetically [11, 10].
Reference: [20] <author> A.K. Chandra, D. Kozen, and L.J. Stockmeyer. </author> <title> Alternation. </title> <journal> Journal of the ACM, </journal> <volume> 28 </volume> <pages> 114-133, </pages> <year> 1981. </year>
Reference-contexts: In addition, because it is an and/or tree, the search can be highly complex. In fact, the search can simulate the computations of an arbitrary Alternating PTIME machine. This establishes the complexity lower-bound, since APTIME = PSPACE <ref> [20] </ref>. By imposing another syntactic restriction, called linearity, we can reduce the complexity of inference again, from PSPACE to NP. When a rulebase is linear, a proof tree has very few "and" nodes; so inference is simplified, becoming largely a process of searching an or-tree.
Reference: [21] <author> B.F. Chellas. </author> <title> Modal Logic: an Introduction. </title> <publisher> Cambridge University Press, </publisher> <year> 1980. </year>
Reference-contexts: Recently, a greatly simplified proof of soundness and completeness was developed by Bonner, first for the propositional case [8], and then for the full predicate case [15]. The proof is not based on the fixpoint constructions of logic programming, but on the canonical-model constructions of Modal logic <ref> [21] </ref>. The rest of this section outlines intuitionistic model-theory, specializing the presentation for the special case of embedded implications. Note that the syntax of the logic is first-order, but its model theory is modal, i.e., is based on a set of possible worlds.
Reference: [22] <author> P.M. Dung. </author> <title> Declarative Semantics of Hypothetical Logic Programming with Negation as Failure. </title> <booktitle> In Proceedings of the Third International Workshop on Extensions of Logic Programming, </booktitle> <pages> pages 45-58, </pages> <address> Bologna, Italy, </address> <month> February </month> <year> 1992. </year> <note> Springer-Verlag. Published as volume 660 of Lecture Notes in Artificial Intelligence, </note> <year> 1993. </year>
Reference-contexts: Earlier, Statman showed that intuitionistic logic is PSPACE-complete in the propositional case [48]. Several researchers have also investigated the the semantics of negation-as-failure for hypothetical insertions <ref> [25, 14, 31, 43, 30, 22] </ref>. We discuss this work in Section 6. This paper adds to the picture by summarizing some of our own results on hypothetical inference. <p> Another is that the semantics of non-stratified rulebases is likely to remain unsettled for some time. In addition, stratified embedded implications have important complexity-theoretic properties, as shown in Section 7. Recently, there have been several proposals for a semantics of non-stratified embedded implications <ref> [31, 43, 30, 22] </ref>. Following [41], these proposals each define a model as a mapping from a set of logic programs to a set of values. <p> Under a syntactic restriction call allowedness, this procedure is sound and avoids floundering. Unfortunately, this restriction seems to preclude many stratified programs, such as those in Examples 6.1, 6.2 and 7.2. Finally, in <ref> [22] </ref>, Dung proposes a stable model semantics that allows arbitrary rulebases, stratified or unstratified, though no proof theory is presented. None of the expressibility results in Section 7 depends on recursion through negation. For the purpose of this paper, then, we need only consider stratified rulebases.
Reference: [23] <author> M.C. </author> <title> Fitting. Intuitionistic Logic, Model Theory and Forcing. </title> <publisher> North-Holland, </publisher> <year> 1969. </year>
Reference-contexts: Theoretical work on hypothetical inference has also been carried out, largely by the logic-programming community. Most of this work focuses on the hypothetical insertion of atoms into a database. These updates, it turns out, fit neatly into a well-known logical system, intuitionistic logic <ref> [23] </ref>. Most of this work has been semantic, first showing that hypothetical insertion is indeed intuitionistic, and then casting the semantics in terms of a least fixpoint theory, in the logic-programming tradition. Gabbay first showed that hypothetical insertion is intuitionistic [25]. <p> The first example illustrates the basic idea. Example 3.1 [Propositional Inference] Suppose that R consists of the following three rules: A (B D): C D: Then R ` A. This can be proved by a straightforward, top-down argument: 3 The proof is an adaptation of a proof given in <ref> [23, Chapter 5] </ref> on the correctness of Beth Tableaus, which also create new constant symbols. 8 R ` A if R ` B D by Lemma 3.1, using the rule A (B D). if R + D ` B by inference rule 4. if R + D ` C by Lemma <p> Note that the syntax of the logic is first-order, but its model theory is modal, i.e., is based on a set of possible worlds. A complete development of intuitionistic logic can be found in <ref> [23, 35] </ref>. <p> Rather, it has an independent semantic definition. In effect, intuitionistic implication is a binary modal operator. An intuitive interpretation may be found in [35] and <ref> [23] </ref>. Definition 4.3 (Models) M j= iff s; M j= for all substates s of M . If M j= , then M is a model of . Definition 4.4 (Validity) A formula is valid iff M j= for all intuitionistic structures M .
Reference: [24] <author> The Committee for Advanced DBMS function. </author> <title> Third-Generation Database System Manifesto. </title> <booktitle> SIGMOD Record, </booktitle> <volume> 19(3) </volume> <pages> 31-44, </pages> <month> September </month> <year> 1990. </year> <note> Also published as Memorandum No. </note> <institution> UCB/ERL M90/28, Electronics Research Laboratory, College of Engineering, University of California, Berkeley. </institution>
Reference-contexts: Or he might want a table of deficit predictions for a number of hypothetical salary increases [54]. Similar problems occur in computer aided design (CAD). Here, one must evaluate the effect on the overall design of local design alternatives and of various external factors <ref> [24, 47] </ref>. For example, an engineer may need to know how much the price of an automobile would increase if supplier X raised his prices by Y percent [24]. The number of hypothetical scenarios multiplies quickly when several factors are varied simultaneously, such as prices, interest rates, tax rates, etc. <p> Here, one must evaluate the effect on the overall design of local design alternatives and of various external factors [24, 47]. For example, an engineer may need to know how much the price of an automobile would increase if supplier X raised his prices by Y percent <ref> [24] </ref>. The number of hypothetical scenarios multiplies quickly when several factors are varied simultaneously, such as prices, interest rates, tax rates, etc. One may also need to consider variations in more complex factors, such as government regulations, company policy, tax laws, etc.
Reference: [25] <author> D.M. Gabbay. N-Prolog: </author> <title> an Extension of Prolog with Hypothetical Implications. II. Logical Foundations and Negation as Failure. </title> <journal> Journal of Logic Programming (JLP), </journal> <volume> 2(4) </volume> <pages> 251-283, </pages> <year> 1985. </year>
Reference-contexts: Most of this work has been semantic, first showing that hypothetical insertion is indeed intuitionistic, and then casting the semantics in terms of a least fixpoint theory, in the logic-programming tradition. Gabbay first showed that hypothetical insertion is intuitionistic <ref> [25] </ref>. Working independently, McCarty and Miller extended this result to operations that create new constant symbols during inference, and they developed fixpoint semantics based on intuitionistic logic [38, 41]. Earlier, Statman showed that intuitionistic logic is PSPACE-complete in the propositional case [48]. <p> Earlier, Statman showed that intuitionistic logic is PSPACE-complete in the propositional case [48]. Several researchers have also investigated the the semantics of negation-as-failure for hypothetical insertions <ref> [25, 14, 31, 43, 30, 22] </ref>. We discuss this work in Section 6. This paper adds to the picture by summarizing some of our own results on hypothetical inference. <p> the inference system of Definition 3.2 is sound and complete with respect to intuitionistic model theory, and therefore forms a fragment of intuitionistic logic. (It is not full intuitionistic logic since embedded implications do not include disjunction or negation.) The intuitionistic nature of embedded implications was first established by Gabbay <ref> [25] </ref>. Working independently, McCarty and Miller extended this work to include rules with universal quantifiers in their premises [38, 41]. The work of McCarty also includes formulas with negations in rule heads. <p> R 2 A 2 (x) B (x; y); [A 2 (y) D (y)]: R 1 A 1 (x) B (x; y); [A 1 (y) D (y)]: R 0 A 0 (x) B (x; y); [A 0 (y) D (y)]: 2 As originally pointed out by Gabbay <ref> [25] </ref>, negation-as-failure displays curious paradoxes when extended from Horn rules to embedded implications. This is true even for very simple, non-recursive rulebases. In fact, if implicational queries are allowed, then these paradoxes arise even for Horn rules.
Reference: [26] <editor> D.M. Gabbay and U. Reyle. N-Prolog: </editor> <title> an Extension of Prolog with Hypothetical Implications. I. </title> <journal> Journal of Logic Programming (JLP), </journal> <volume> 1(4) </volume> <pages> 319-355, </pages> <year> 1984. </year>
Reference-contexts: Gabbay and Reyle, for instance, have reported a need to augment Prolog with hypothetical rules in order to encode the British Nationality Act, because the act contains rules such as, "You are eligible for citizenship if your father would be eligible if 2 he were still alive" <ref> [26] </ref>. And McCarty, also motivated by legal applications, has developed a wide class of hypothetical rules for computer-based consultation systems, especially systems for reasoning about corporate tax law and estate tax law [38, 40, 46]. Theoretical work on hypothetical inference has also been carried out, largely by the logic-programming community. <p> Embedded implications are first-order formulas that can hypothetically insert formulas into a database, and can hypothetically create new constant symbols. Such systems have been developed by several researchers <ref> [26, 38, 41, 10, 15, 37] </ref>. This section defines a simplified version of these systems, one that retains many of the essential properties of the more elaborate systems while admitting a clean theoretical analysis. One notable feature of this system is that it allows universal quantifiers in rule bodies. <p> That is, given and R, the system returns "yes" iff R ` can be derived. In databases and logic programming, it is common to ask queries that return a set of tuples as answers. Such queries can be defined using embedded implications with free variables <ref> [10, 15, 9, 26, 27, 39] </ref>. We address this issue in Section 7, where we discuss database queries in general. Until then, however, we shall assume that a query is a closed formula.
Reference: [27] <editor> D.M. Gabbay and U. Reyle. </editor> <title> Computation with Run Time Skolemization. </title> <journal> Journal of Applied Non-Classical Logics, </journal> <volume> 3 </volume> <pages> 93-134, </pages> <year> 1993. </year>
Reference-contexts: That is, given and R, the system returns "yes" iff R ` can be derived. In databases and logic programming, it is common to ask queries that return a set of tuples as answers. Such queries can be defined using embedded implications with free variables <ref> [10, 15, 9, 26, 27, 39] </ref>. We address this issue in Section 7, where we discuss database queries in general. Until then, however, we shall assume that a query is a closed formula. <p> This kind of run-time skolemization is central to the tableau proof procedure developed in [39], and to the interpreter presented in [42]. Run-time skolemization is treated in a more general setting by Gabbay in <ref> [27] </ref>. Because Gabbay's language includes existential quantifiers, his analogue of rule 5 uses skolem functions, not just skolem constants. The rest of this section gives examples illustrating the basic properties of the inference system above.
Reference: [28] <author> A. Van Gelder, K.A. Ross, and J.S. Schlipf. </author> <title> The Well-Founded Semantics for General Logic Programs. </title> <journal> Journal of the ACM, </journal> <volume> 38(3) </volume> <pages> 620-650, </pages> <year> 1991. </year> <month> 26 </month>
Reference-contexts: Locally stratified rulebases are perhaps the largest class of logic programs for which the semantics of negation is uncontroversial. A third approach to negation-as-failure attempts to define semantics for arbitrary Horn programs with negation. There have been numerous attempts here too. Perhaps the best known are the well-founded semantics <ref> [28, 52] </ref> and the stable model semantics [29]. Although, these semantics offer different interpretations of recursion through negation, they are equivalent when rulebases are stratified or locally stratified. Several researchers have extended these ideas to embedded implications with negation.
Reference: [29] <author> M. Gelfond and V. Lifschitz. </author> <title> The Stable Model Semantics for Logic Programming. </title> <booktitle> In Proceedings of the Fifth Logic Programming Symposium, </booktitle> <pages> pages 1070-1080. </pages> <publisher> MIT Press, </publisher> <year> 1988. </year>
Reference-contexts: A third approach to negation-as-failure attempts to define semantics for arbitrary Horn programs with negation. There have been numerous attempts here too. Perhaps the best known are the well-founded semantics [28, 52] and the stable model semantics <ref> [29] </ref>. Although, these semantics offer different interpretations of recursion through negation, they are equivalent when rulebases are stratified or locally stratified. Several researchers have extended these ideas to embedded implications with negation. For instance, [14, 10, 11] develop a semantics of stratified rulebases, including a proof theory and model theory.
Reference: [30] <author> L. Giordano and N. Olivetti. </author> <title> Negation as Failure in Intuitionistic Logic Programming. </title> <booktitle> In Proceedings of the Joint International Conference and Symposium on Logic Programming (JICSLP), </booktitle> <pages> pages 431-445, </pages> <address> Washington, D.C., 1992. </address> <publisher> MIT Press. </publisher>
Reference-contexts: Earlier, Statman showed that intuitionistic logic is PSPACE-complete in the propositional case [48]. Several researchers have also investigated the the semantics of negation-as-failure for hypothetical insertions <ref> [25, 14, 31, 43, 30, 22] </ref>. We discuss this work in Section 6. This paper adds to the picture by summarizing some of our own results on hypothetical inference. <p> Another is that the semantics of non-stratified rulebases is likely to remain unsettled for some time. In addition, stratified embedded implications have important complexity-theoretic properties, as shown in Section 7. Recently, there have been several proposals for a semantics of non-stratified embedded implications <ref> [31, 43, 30, 22] </ref>. Following [41], these proposals each define a model as a mapping from a set of logic programs to a set of values. <p> They also preclude the rulebases used to achieve the expressibility results of Section 7. This point is discussed at greater length in [14]. In [43], Olivetti and Terracini present a 3-valued modal semantics for the propositional case, and in <ref> [30] </ref>, Giordano and Olivetti develop a 3-valued fixpoint semantics for the predicate case. In both cases, the semantics is of finite failure in arbitrary rulebases. [30] also develops a top-down SLD-style proof procedure. Under a syntactic restriction call allowedness, this procedure is sound and avoids floundering. <p> In [43], Olivetti and Terracini present a 3-valued modal semantics for the propositional case, and in <ref> [30] </ref>, Giordano and Olivetti develop a 3-valued fixpoint semantics for the predicate case. In both cases, the semantics is of finite failure in arbitrary rulebases. [30] also develops a top-down SLD-style proof procedure. Under a syntactic restriction call allowedness, this procedure is sound and avoids floundering. Unfortunately, this restriction seems to preclude many stratified programs, such as those in Examples 6.1, 6.2 and 7.2.
Reference: [31] <author> J. Harland. </author> <title> A Kripke-like model for negation as failure. </title> <booktitle> In Proceedings of the North American Conference on Logic Programming (NACLP), </booktitle> <pages> pages 626-642, </pages> <address> Cleveland, Ohio, October 1989. </address> <publisher> MIT Press. </publisher>
Reference-contexts: Earlier, Statman showed that intuitionistic logic is PSPACE-complete in the propositional case [48]. Several researchers have also investigated the the semantics of negation-as-failure for hypothetical insertions <ref> [25, 14, 31, 43, 30, 22] </ref>. We discuss this work in Section 6. This paper adds to the picture by summarizing some of our own results on hypothetical inference. <p> Another is that the semantics of non-stratified rulebases is likely to remain unsettled for some time. In addition, stratified embedded implications have important complexity-theoretic properties, as shown in Section 7. Recently, there have been several proposals for a semantics of non-stratified embedded implications <ref> [31, 43, 30, 22] </ref>. Following [41], these proposals each define a model as a mapping from a set of logic programs to a set of values. <p> These proposals are therefore not model theories in the traditional sense, since they only consider models of a certain syntactic form. (In contrast, [14] takes the traditional position that a model is a mapping from any set to a set of first-order semantic structures.) In <ref> [31] </ref>, Harland develops a semantics in which predicates are labelled as "completely defined" or "incompletely defined". Unfortunately, restrictions on the labelling preclude many stratified programs, such as those in Examples 7.1 and 7.2. They also preclude the rulebases used to achieve the expressibility results of Section 7.
Reference: [32] <author> N. Immerman. </author> <title> Relational Queries Computable in Polynomial Time. </title> <booktitle> In Proceedings of the ACM Symposium on Theory of Computing (STOC), </booktitle> <pages> pages 147-152, </pages> <year> 1982. </year>
Reference-contexts: As a corollary, we conclude that stratified linear rulebases express exactly the second-order definable queries. Unlike other results in the literature <ref> [32, 53, 19] </ref>, the results in this section do not assume that the data domain is linearly ordered. The assumption of ordered domains is a technical device that is often used to achieve expressibility results, but it is not an intrinsic feature of databases [1]. <p> Often, however, this increased power is still limited. For instance, Datalog with negation-as-failure still cannot express EVEN [18]. In such cases, a logic is often augmented with other devices, to guarantee expressive completeness. One typical device is to assume that the data domain is linearly ordered <ref> [32, 53, 19] </ref>. Datalog, for instance, is expressively 19 complete for PTIME if augmented with both negation-as-failure and a linear order. Em--bedded implications do not need to make this latter assumption, however, since they can generate a linear order for themselves, and insert it into the database hypothetically [11, 10]. <p> In fact, with negation, they are expressively complete, and can therefore express any computable generic query. In database theory, the ability to express generic queries is the normal measure of expressiveness for database query languages <ref> [2, 3, 17, 18, 32, 33] </ref>. Genericity captures the idea that constant symbols are uninterpreted, i.e., that they have no innate meaning. To make this idea more precise, consider the effect on a query of renaming the constant symbols in the database.
Reference: [33] <author> N. Immerman. </author> <title> Languages that Capture Complexity Classes. </title> <journal> SIAM Journal of Computing, </journal> <volume> 16(4) </volume> <pages> 760-778, </pages> <year> 1987. </year>
Reference-contexts: In fact, with negation, they are expressively complete, and can therefore express any computable generic query. In database theory, the ability to express generic queries is the normal measure of expressiveness for database query languages <ref> [2, 3, 17, 18, 32, 33] </ref>. Genericity captures the idea that constant symbols are uninterpreted, i.e., that they have no innate meaning. To make this idea more precise, consider the effect on a query of renaming the constant symbols in the database. <p> Universal quantifiers do not appear in rule bodies. 4. The rulebase contains no constant symbols. This corollary immediately gives rise to a characterization of the second-order definable queries, since Immerman has shown that the generic queries in PHIER are precisely the queries definable in second-order logic <ref> [33] </ref>. This gives us an alternate characterization of the queries expressed by linear embedded-implications. It also gives the interesting result that the second-order queries can be characterized by a first-order logic. Corollary 7.7 The rulebases described in Theorem 7.6 express exactly the second-order definable queries.
Reference: [34] <editor> R.H. Sprague Jr. and H.J. Watson, editors. </editor> <title> Decisions Support Systems: Putting Theory into Practice. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1989. </year>
Reference-contexts: 1 Introduction Researchers from several areas have recognized the need for computer systems that reason hypothetically. Decision support systems (DSS) are a good example, especially in domains like financial planning where many "what if" scenarios must be considered <ref> [34, 44] </ref>. A typical example is an analyst who must predict a company's deficit for the upcoming year assuming that employee salaries are increased by a given percentage. Or he might want a table of deficit predictions for a number of hypothetical salary increases [54].
Reference: [35] <author> S. Kripke. </author> <title> Semantical Analysis of Intuitionistic Logic. </title> <editor> I. In J.N. Crossley and M.A.E. Dummett, editors, </editor> <booktitle> Formal Systems and Recursive Functions, </booktitle> <pages> pages 92-130. </pages> <address> North Hol-land, Amsterdam, </address> <year> 1965. </year>
Reference-contexts: Note that the syntax of the logic is first-order, but its model theory is modal, i.e., is based on a set of possible worlds. A complete development of intuitionistic logic can be found in <ref> [23, 35] </ref>. <p> Rather, it has an independent semantic definition. In effect, intuitionistic implication is a binary modal operator. An intuitive interpretation may be found in <ref> [35] </ref> and [23]. Definition 4.3 (Models) M j= iff s; M j= for all substates s of M . If M j= , then M is a model of . Definition 4.4 (Validity) A formula is valid iff M j= for all intuitionistic structures M .
Reference: [36] <author> S. Manchanda and D.S. Warren. </author> <title> A Logic-based Language for Database Updates. </title> <editor> In Jack Minker, editor, </editor> <booktitle> Foundations of Deductive Databases and Logic Programming, chapter 10, </booktitle> <pages> pages 363-394. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: Vieille, et al, for instance, have developed a deductive database along these lines [54], and Warren and Manchanda have used hypothetical rules to reason about database updates <ref> [55, 36] </ref>. In [41], Miller shows that hypothetical insertions can structure the runtime environment of logic programs, resulting in programs that are more elegant, more efficient, and easier to maintain. In [42], he develops a theory of lexical scoping based on the hypothetical creation of constant symbols during inference.
Reference: [37] <author> Sanjay Manchanda. </author> <title> A Dynamic Logic Programming Language for Relational Updates. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, State University of New York at Stony Brook, Stony Brook, </institution> <address> New York, </address> <month> December </month> <year> 1987. </year> <note> Also published as Technical Report TR 88-2, </note> <institution> Department of Computer Science, The University of Arizona, Tuscon, Arizona 85721, </institution> <month> January, </month> <year> 1988. </year>
Reference-contexts: Embedded implications are first-order formulas that can hypothetically insert formulas into a database, and can hypothetically create new constant symbols. Such systems have been developed by several researchers <ref> [26, 38, 41, 10, 15, 37] </ref>. This section defines a simplified version of these systems, one that retains many of the essential properties of the more elaborate systems while admitting a clean theoretical analysis. One notable feature of this system is that it allows universal quantifiers in rule bodies.
Reference: [38] <author> L.T. McCarty. </author> <title> Clausal Intuitionistic Logic. I. Fixed-Point Semantics. </title> <journal> Journal of Logic Programming (JLP), </journal> <volume> 5(1) </volume> <pages> 1-31, </pages> <year> 1988. </year>
Reference-contexts: And McCarty, also motivated by legal applications, has developed a wide class of hypothetical rules for computer-based consultation systems, especially systems for reasoning about corporate tax law and estate tax law <ref> [38, 40, 46] </ref>. Theoretical work on hypothetical inference has also been carried out, largely by the logic-programming community. Most of this work focuses on the hypothetical insertion of atoms into a database. These updates, it turns out, fit neatly into a well-known logical system, intuitionistic logic [23]. <p> Gabbay first showed that hypothetical insertion is intuitionistic [25]. Working independently, McCarty and Miller extended this result to operations that create new constant symbols during inference, and they developed fixpoint semantics based on intuitionistic logic <ref> [38, 41] </ref>. Earlier, Statman showed that intuitionistic logic is PSPACE-complete in the propositional case [48]. Several researchers have also investigated the the semantics of negation-as-failure for hypothetical insertions [25, 14, 31, 43, 30, 22]. We discuss this work in Section 6. <p> Numerous examples of such queries can be constructed, including sophisticated rulebases in which such queries appear in the premises of rules. Such rules are called embedded implications <ref> [38] </ref>. As we shall see, the implication sign in queries and rule bodies corresponds to hypothetical insertion, and the universal quantifier corresponds to the hypothetical creation of new constant symbols. It is in this way that the two operations of insertion and creation fit neatly into a logical, first-order framework. <p> Embedded implications are first-order formulas that can hypothetically insert formulas into a database, and can hypothetically create new constant symbols. Such systems have been developed by several researchers <ref> [26, 38, 41, 10, 15, 37] </ref>. This section defines a simplified version of these systems, one that retains many of the essential properties of the more elaborate systems while admitting a clean theoretical analysis. One notable feature of this system is that it allows universal quantifiers in rule bodies. <p> One notable feature of this system is that it allows universal quantifiers in rule bodies. In the logic programming context, such rules were first treated by McCarty. In <ref> [38] </ref>, McCarty develops an intuitionistic fixpoint semantics for embedded implications, and in [39], he describes a practical, tableau proof procedure. Such rules have also been extensively investigated by Miller. In [41], Miller develops an intuitionistic fixpoint semantics for embedded implications, and in [42], he develops an interpreter for them. <p> Working independently, McCarty and Miller extended this work to include rules with universal quantifiers in their premises <ref> [38, 41] </ref>. The work of McCarty also includes formulas with negations in rule heads. Recently, a greatly simplified proof of soundness and completeness was developed by Bonner, first for the propositional case [8], and then for the full predicate case [15].
Reference: [39] <author> L.T. McCarty. </author> <title> Clausal Intuitionistic Logic. II. Tableau Proof Procedures. </title> <journal> Journal of Logic Programming (JLP), </journal> <volume> 5(2) </volume> <pages> 93-132, </pages> <year> 1988. </year>
Reference-contexts: For instance, since they can be handled entirely within first-order intuitionistic 3 logic, they have a well-established semantics. In addition, they have an efficient Prolog--style inference procedure, one based on resolution and unification, in the logic-programming tradition <ref> [39] </ref>. Lastly, these hypothetical operations have important complexity-theoretic properties. For instance, as we show in Section 7, they characterize the database queries in many well-known complexity classes, such as NP, PSPACE, re, and the polynomial-time hierarchy. <p> One notable feature of this system is that it allows universal quantifiers in rule bodies. In the logic programming context, such rules were first treated by McCarty. In [38], McCarty develops an intuitionistic fixpoint semantics for embedded implications, and in <ref> [39] </ref>, he describes a practical, tableau proof procedure. Such rules have also been extensively investigated by Miller. In [41], Miller develops an intuitionistic fixpoint semantics for embedded implications, and in [42], he develops an interpreter for them. <p> That is, given and R, the system returns "yes" iff R ` can be derived. In databases and logic programming, it is common to ask queries that return a set of tuples as answers. Such queries can be defined using embedded implications with free variables <ref> [10, 15, 9, 26, 27, 39] </ref>. We address this issue in Section 7, where we discuss database queries in general. Until then, however, we shall assume that a query is a closed formula. <p> This is not necessary, however, and in the inference system of Definition 3.2, "skolemization" is effectively integrated into the inference process. This kind of run-time skolemization is central to the tableau proof procedure developed in <ref> [39] </ref>, and to the interpreter presented in [42]. Run-time skolemization is treated in a more general setting by Gabbay in [27]. Because Gabbay's language includes existential quantifiers, his analogue of rule 5 uses skolem functions, not just skolem constants. <p> The ability to create constants in this way is crucial to the inference system, and it is the basis of the Prolog-style proof procedure developed in <ref> [39] </ref>. In [15], this ability is used to simulate the computations of arbitrary Turing machines. Without the ability to create new constant symbols, it is only possible to simulate Turing machines that use a polynomial amount of space, i.e., PSPACE-machines (see Section 5). <p> Moreover, in practice, it is not necessary to actually check n k ground inferences, since unification-based proof procedures exist for embedded implications with free variables. One of the first such procedures was developed by McCarty <ref> [39] </ref> for the negation-free case. Having defined what it means to express queries with embedded implications, the rest of this section characterizes the exact sets of queries that can be expressed. As shown in Section 5, without negation-as-failure, embedded implications can simulate arbitrary Turing machines. <p> have been developed that are expressively complete for PSPACE [16, 18, 2, 3], but the language of stratified embedded implications is the only one based on a model theoretic semantics, and the only one to have an Prolog-style proof procedure, one based on resolution and unification in the logic-programming tradition <ref> [39] </ref>.
Reference: [40] <author> L.T. McCarty. </author> <title> A Language for Legal Discourse. I. Basic Features. </title> <booktitle> In Proceedings of the Second International Conference on Artificial Intelligence and Law, </booktitle> <pages> pages 180-189. </pages> <publisher> ACM Press, </publisher> <month> June </month> <year> 1989. </year>
Reference-contexts: And McCarty, also motivated by legal applications, has developed a wide class of hypothetical rules for computer-based consultation systems, especially systems for reasoning about corporate tax law and estate tax law <ref> [38, 40, 46] </ref>. Theoretical work on hypothetical inference has also been carried out, largely by the logic-programming community. Most of this work focuses on the hypothetical insertion of atoms into a database. These updates, it turns out, fit neatly into a well-known logical system, intuitionistic logic [23].
Reference: [41] <author> D. Miller. </author> <title> A Logical Analysis of Modules in Logic Programming. </title> <journal> Journal of Logic Programming (JLP), </journal> <volume> 6 </volume> <pages> 79-108, </pages> <year> 1989. </year>
Reference-contexts: Vieille, et al, for instance, have developed a deductive database along these lines [54], and Warren and Manchanda have used hypothetical rules to reason about database updates [55, 36]. In <ref> [41] </ref>, Miller shows that hypothetical insertions can structure the runtime environment of logic programs, resulting in programs that are more elegant, more efficient, and easier to maintain. In [42], he develops a theory of lexical scoping based on the hypothetical creation of constant symbols during inference. <p> Gabbay first showed that hypothetical insertion is intuitionistic [25]. Working independently, McCarty and Miller extended this result to operations that create new constant symbols during inference, and they developed fixpoint semantics based on intuitionistic logic <ref> [38, 41] </ref>. Earlier, Statman showed that intuitionistic logic is PSPACE-complete in the propositional case [48]. Several researchers have also investigated the the semantics of negation-as-failure for hypothetical insertions [25, 14, 31, 43, 30, 22]. We discuss this work in Section 6. <p> Embedded implications are first-order formulas that can hypothetically insert formulas into a database, and can hypothetically create new constant symbols. Such systems have been developed by several researchers <ref> [26, 38, 41, 10, 15, 37] </ref>. This section defines a simplified version of these systems, one that retains many of the essential properties of the more elaborate systems while admitting a clean theoretical analysis. One notable feature of this system is that it allows universal quantifiers in rule bodies. <p> In the logic programming context, such rules were first treated by McCarty. In [38], McCarty develops an intuitionistic fixpoint semantics for embedded implications, and in [39], he describes a practical, tableau proof procedure. Such rules have also been extensively investigated by Miller. In <ref> [41] </ref>, Miller develops an intuitionistic fixpoint semantics for embedded implications, and in [42], he develops an interpreter for them. <p> Working independently, McCarty and Miller extended this work to include rules with universal quantifiers in their premises <ref> [38, 41] </ref>. The work of McCarty also includes formulas with negations in rule heads. Recently, a greatly simplified proof of soundness and completeness was developed by Bonner, first for the propositional case [8], and then for the full predicate case [15]. <p> Another is that the semantics of non-stratified rulebases is likely to remain unsettled for some time. In addition, stratified embedded implications have important complexity-theoretic properties, as shown in Section 7. Recently, there have been several proposals for a semantics of non-stratified embedded implications [31, 43, 30, 22]. Following <ref> [41] </ref>, these proposals each define a model as a mapping from a set of logic programs to a set of values.
Reference: [42] <author> D. Miller. </author> <title> Lexical scoping as universal quantification. </title> <editor> In G. Levi and M. Martelli, editors, </editor> <booktitle> Logic Programming: Proceedings of the Sixth International Conference, </booktitle> <pages> pages 268-283, </pages> <address> Cambridge, MA, 1989. </address> <publisher> MIT Press. </publisher> <pages> 27 </pages>
Reference-contexts: In [41], Miller shows that hypothetical insertions can structure the runtime environment of logic programs, resulting in programs that are more elegant, more efficient, and easier to maintain. In <ref> [42] </ref>, he develops a theory of lexical scoping based on the hypothetical creation of constant symbols during inference. These logical systems are well-suited to solving problems in Artificial Intelligence, especially problems that involve reasoning about alternative courses of action. <p> In [38], McCarty develops an intuitionistic fixpoint semantics for embedded implications, and in [39], he describes a practical, tableau proof procedure. Such rules have also been extensively investigated by Miller. In [41], Miller develops an intuitionistic fixpoint semantics for embedded implications, and in <ref> [42] </ref>, he develops an interpreter for them. The language of our logic includes three infinite, enumerable sets: a set of variables x; y; z; :::; a set of constant symbols a; b; c; :::; and a set of predicate symbols A; B; C; :::. <p> This is not necessary, however, and in the inference system of Definition 3.2, "skolemization" is effectively integrated into the inference process. This kind of run-time skolemization is central to the tableau proof procedure developed in [39], and to the interpreter presented in <ref> [42] </ref>. Run-time skolemization is treated in a more general setting by Gabbay in [27]. Because Gabbay's language includes existential quantifiers, his analogue of rule 5 uses skolem functions, not just skolem constants. The rest of this section gives examples illustrating the basic properties of the inference system above.
Reference: [43] <author> N. Olivetti and L. Terracini. </author> <title> N-Prolog and Equivalence of Logic Programs (Part 1). </title> <journal> Journal Of Logic, Language and Information, </journal> <volume> 1(4) </volume> <pages> 253-340, </pages> <year> 1992. </year>
Reference-contexts: Earlier, Statman showed that intuitionistic logic is PSPACE-complete in the propositional case [48]. Several researchers have also investigated the the semantics of negation-as-failure for hypothetical insertions <ref> [25, 14, 31, 43, 30, 22] </ref>. We discuss this work in Section 6. This paper adds to the picture by summarizing some of our own results on hypothetical inference. <p> Another is that the semantics of non-stratified rulebases is likely to remain unsettled for some time. In addition, stratified embedded implications have important complexity-theoretic properties, as shown in Section 7. Recently, there have been several proposals for a semantics of non-stratified embedded implications <ref> [31, 43, 30, 22] </ref>. Following [41], these proposals each define a model as a mapping from a set of logic programs to a set of values. <p> Unfortunately, restrictions on the labelling preclude many stratified programs, such as those in Examples 7.1 and 7.2. They also preclude the rulebases used to achieve the expressibility results of Section 7. This point is discussed at greater length in [14]. In <ref> [43] </ref>, Olivetti and Terracini present a 3-valued modal semantics for the propositional case, and in [30], Giordano and Olivetti develop a 3-valued fixpoint semantics for the predicate case. In both cases, the semantics is of finite failure in arbitrary rulebases. [30] also develops a top-down SLD-style proof procedure.
Reference: [44] <author> R.L. Olson and R.H. Sprague Jr. </author> <title> Financial Planning in Action. </title> <editor> In R.H. Sprague Jr. and H.J. Watson, editors, </editor> <booktitle> Decisions Support Systems: Putting Theory into Practice, </booktitle> <pages> pages 373-381. </pages> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1989. </year>
Reference-contexts: 1 Introduction Researchers from several areas have recognized the need for computer systems that reason hypothetically. Decision support systems (DSS) are a good example, especially in domains like financial planning where many "what if" scenarios must be considered <ref> [34, 44] </ref>. A typical example is an analyst who must predict a company's deficit for the upcoming year assuming that employee salaries are increased by a given percentage. Or he might want a table of deficit predictions for a number of hypothetical salary increases [54].
Reference: [45] <author> T. Przymusinski. </author> <title> On the Declarative Semantics of Deductive Databases and Logic Programs. </title> <editor> In Jack Minker, editor, </editor> <booktitle> Foundations of Deductive Databases and Logic Programming, chapter 5, </booktitle> <pages> pages 193-216. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: Another approach has been to allow limited recursion through negation. This approach generalizes the notion of stratification to local stratification <ref> [45] </ref>. Intuitively, a rulebase is locally stratified if its ground instantiation is stratified (with possibly infinitely many strata). Locally stratified rulebases are perhaps the largest class of logic programs for which the semantics of negation is uncontroversial.
Reference: [46] <author> D.A. Schlobohm and L.T. McCarty. EPS-II: </author> <title> Estate planning with prototypes. </title> <booktitle> In Proceedings of the Second International Conference on Artificial Intelligence and Law, </booktitle> <pages> pages 1-10. </pages> <publisher> ACM Press, </publisher> <month> June </month> <year> 1989. </year>
Reference-contexts: And McCarty, also motivated by legal applications, has developed a wide class of hypothetical rules for computer-based consultation systems, especially systems for reasoning about corporate tax law and estate tax law <ref> [38, 40, 46] </ref>. Theoretical work on hypothetical inference has also been carried out, largely by the logic-programming community. Most of this work focuses on the hypothetical insertion of atoms into a database. These updates, it turns out, fit neatly into a well-known logical system, intuitionistic logic [23].
Reference: [47] <editor> Editor: A.P. Seth. </editor> <booktitle> Database Research at Bellcore. SIGMOD Record, </booktitle> <volume> 19(3) </volume> <pages> 45-52, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: Or he might want a table of deficit predictions for a number of hypothetical salary increases [54]. Similar problems occur in computer aided design (CAD). Here, one must evaluate the effect on the overall design of local design alternatives and of various external factors <ref> [24, 47] </ref>. For example, an engineer may need to know how much the price of an automobile would increase if supplier X raised his prices by Y percent [24]. The number of hypothetical scenarios multiplies quickly when several factors are varied simultaneously, such as prices, interest rates, tax rates, etc.
Reference: [48] <author> R. Statman. </author> <title> Intuitionistic Propositional Logic is Polynomial-Space Complete. </title> <journal> Theoretical Computer Science (TCS), </journal> <volume> 9(1) </volume> <pages> 67-72, </pages> <year> 1979. </year>
Reference-contexts: Gabbay first showed that hypothetical insertion is intuitionistic [25]. Working independently, McCarty and Miller extended this result to operations that create new constant symbols during inference, and they developed fixpoint semantics based on intuitionistic logic [38, 41]. Earlier, Statman showed that intuitionistic logic is PSPACE-complete in the propositional case <ref> [48] </ref>. Several researchers have also investigated the the semantics of negation-as-failure for hypothetical insertions [25, 14, 31, 43, 30, 22]. We discuss this work in Section 6. This paper adds to the picture by summarizing some of our own results on hypothetical inference.
Reference: [49] <author> L.J. Stockmeyer. </author> <title> The Polynomial Time Hierarchy. </title> <journal> Theoretical Computer Science (TCS), </journal> <volume> 3(1) </volume> <pages> 1-22, </pages> <year> 1976. </year>
Reference-contexts: This result eventually leads to a characterization of the second-order definable queries in terms of (first-order) embedded implications. The polynomial-time hierarchy is a sequence of complexity classes between P and PSPACE. It is based on the idea of an oracle Turing-machine and can be defined recursively as follows <ref> [49] </ref>: 0 = P * P k = those languages accepted in deterministic polynomial time by an oracle machine whose oracle is a language in P k . k+1 = NP P k = those languages accepted in non-deterministic polynomial time by an oracle machine whose oracle is a language in
Reference: [50] <author> M. Stonebraker. </author> <title> Hypothetical databases as views. </title> <booktitle> In Proceedings of the ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 224-229, </pages> <year> 1981. </year>
Reference-contexts: Hypothetical databases are derived from a real database by a series of hypothetical assumptions, or updates. Early work in this area was done by Stonebraker, who showed that hypothetical databases can be efficiently implemented by slight extensions to conventional database mechanisms <ref> [51, 50] </ref>. He pointed out that hypothetical databases are useful for debugging purposes, for generating test data, and for carrying out a variety of simulations. He also argued that "there are advantages to making hypothetical databases central to the operation of a database management system" [50]. <p> He pointed out that hypothetical databases are useful for debugging purposes, for generating test data, and for carrying out a variety of simulations. He also argued that "there are advantages to making hypothetical databases central to the operation of a database management system" <ref> [50] </ref>. The logic-programming community has taken these ideas one step further, integrating hypothetical updates not just with query processing, but with logical inference as well.
Reference: [51] <author> M. Stonebraker and K. Keller. </author> <title> Embedding expert knowledge and hypothetical databases into a data base system. </title> <booktitle> In Proceedings of the ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 58-66, </pages> <address> Santa Monica, CA, </address> <year> 1980. </year>
Reference-contexts: Hypothetical databases are derived from a real database by a series of hypothetical assumptions, or updates. Early work in this area was done by Stonebraker, who showed that hypothetical databases can be efficiently implemented by slight extensions to conventional database mechanisms <ref> [51, 50] </ref>. He pointed out that hypothetical databases are useful for debugging purposes, for generating test data, and for carrying out a variety of simulations. He also argued that "there are advantages to making hypothetical databases central to the operation of a database management system" [50].
Reference: [52] <author> A. Van Gelder, K.A. Ross, and J.S. Schlipf. </author> <title> Unfounded sets and well-founded semantics for general logic programs. </title> <booktitle> In Proceedings of the ACM Symposium on the Principles of Database Systems (PODS), </booktitle> <pages> pages 221-230, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: Locally stratified rulebases are perhaps the largest class of logic programs for which the semantics of negation is uncontroversial. A third approach to negation-as-failure attempts to define semantics for arbitrary Horn programs with negation. There have been numerous attempts here too. Perhaps the best known are the well-founded semantics <ref> [28, 52] </ref> and the stable model semantics [29]. Although, these semantics offer different interpretations of recursion through negation, they are equivalent when rulebases are stratified or locally stratified. Several researchers have extended these ideas to embedded implications with negation.
Reference: [53] <author> M. Vardi. </author> <title> The Complexity of Relational Query Languages. </title> <booktitle> In Proceedings of the ACM Symposium on Theory of Computing (STOC), </booktitle> <pages> pages 137-146, </pages> <year> 1982. </year>
Reference-contexts: Derivations thus have the form R + DB ` . Informally, the data complexity of this system is the complexity of inference when the rulebase is fixed and the database varies (acts as input). More formal definitions are given in <ref> [18, 53, 11] </ref>. We first show that inference with embedded implications is semi-decidable, and more specifically, that its data complexity is complete for re. <p> As a corollary, we conclude that stratified linear rulebases express exactly the second-order definable queries. Unlike other results in the literature <ref> [32, 53, 19] </ref>, the results in this section do not assume that the data domain is linearly ordered. The assumption of ordered domains is a technical device that is often used to achieve expressibility results, but it is not an intrinsic feature of databases [1]. <p> Often, however, this increased power is still limited. For instance, Datalog with negation-as-failure still cannot express EVEN [18]. In such cases, a logic is often augmented with other devices, to guarantee expressive completeness. One typical device is to assume that the data domain is linearly ordered <ref> [32, 53, 19] </ref>. Datalog, for instance, is expressively 19 complete for PTIME if augmented with both negation-as-failure and a linear order. Em--bedded implications do not need to make this latter assumption, however, since they can generate a linear order for themselves, and insert it into the database hypothetically [11, 10].
Reference: [54] <author> L. Vieille, P. Bayer, V. Kuchenhoff, and A. Lefebvre. EKS-V1, </author> <title> A Short Overview. </title> <booktitle> Presented at the AAAI-90 Workshop on Knowledge Base Management Systems, </booktitle> <address> July 1990, Boston, USA. </address>
Reference-contexts: A typical example is an analyst who must predict a company's deficit for the upcoming year assuming that employee salaries are increased by a given percentage. Or he might want a table of deficit predictions for a number of hypothetical salary increases <ref> [54] </ref>. Similar problems occur in computer aided design (CAD). Here, one must evaluate the effect on the overall design of local design alternatives and of various external factors [24, 47]. <p> Since the premise of a logical rule is just a query, several researchers have developed hypothetical rules, in which the premise can query not only a real database, but hypothetical databases as well. Vieille, et al, for instance, have developed a deductive database along these lines <ref> [54] </ref>, and Warren and Manchanda have used hypothetical rules to reason about database updates [55, 36]. In [41], Miller shows that hypothetical insertions can structure the runtime environment of logic programs, resulting in programs that are more elegant, more efficient, and easier to maintain.
Reference: [55] <author> D.S. Warren. </author> <title> Database Updates in Pure Prolog. </title> <booktitle> In Proceedings of the International Conference on Fifth Generation Computer Systems, </booktitle> <pages> pages 244-253, </pages> <year> 1984. </year> <month> 28 </month>
Reference-contexts: Vieille, et al, for instance, have developed a deductive database along these lines [54], and Warren and Manchanda have used hypothetical rules to reason about database updates <ref> [55, 36] </ref>. In [41], Miller shows that hypothetical insertions can structure the runtime environment of logic programs, resulting in programs that are more elegant, more efficient, and easier to maintain. In [42], he develops a theory of lexical scoping based on the hypothetical creation of constant symbols during inference.
References-found: 55

