URL: http://www.cs.utexas.edu/users/dagh/Papers/sc97.ps
Refering-URL: http://www.cs.utexas.edu/users/dagh/papers.html
Root-URL: 
Email: parashar@cs.utexas.edu  browne@cs.utexas.edu  carter@ticam.utexas.edu  
Phone: Tel: (512) 471-3312; Fax: (512) 471-8694  Tel: (512) 471-9579; Fax: (512) 471-8885  Tel: (512) 471-3312; Fax: (512) 471-8694  
Title: A Common Data Management Infrastructure for Adaptive Algorithms for PDE Solutions  
Author: Manish Parashar James C. Browne Carter Edwards Kenneth Klimkowski 
Web: http://www.ticam.utexas.edu/~parashar/public_html/  http://www.cs.utexas.edu/users/browne/  http://www.ticam.utexas.edu/~carter/  
Address: 2.400 Taylor Hall Austin, TX 78712  2.400 Taylor Hall Austin, TX 78712  2.400 Taylor Hall Austin, TX 78712  
Affiliation: Department of Computer Sciences and Texas Institute for Computational and Applied Mathematics University of Texas at Austin  Department of Computer Sciences and Texas Institute for Computational and Applied Mathematics University of Texas at Austin  Texas Institute for Computational and Applied Mathematics University of Texas at Austin  Texas Institute for Computational and Applied Mathematics University of Texas at Austin  
Abstract-found: 0
Intro-found: 1
Reference: 1. <author> M. Parashar and J. C. Browne, </author> <title> System Engineering for High Performance Computing Software: The HDDA/DAGH Infrastructure for Implementation of Parallel Structured Adaptive Mesh Refinement, to be published in Structured Adaptive Mesh Refinement Grid Methods, IMA Volumes in Mathematics and its Applications, </title> <publisher> Springer-Verlag, </publisher> <year> 1997. </year>
Reference-contexts: This layer implements data abstractions such as grids, meshes and trees which underlie different solution methods. The design of the PSE is based on a separation of concerns and the definition of hierarchical abstractions based on the separation. Such a clean separation of concerns <ref> [1] </ref> is critical to the success of an infrastructure that can provide a foundation for several different solution methods. <p> This is achieved through the application of the principle of separation of concerns <ref> [1] </ref> to the DDA design. An overview of this design is shown in Figure 3.1. Distributed dynamic arrays are defined in the following subsection. 3.1 Distributed Dynamic Array Abstraction The SDDA and HDDA are implementations of a distributed dynamic array.
Reference: 2. <institution> Harold Carter Edwards,A Parallel Infrastructure for Scalable Adaptive Finite Element Methods and its Application to Least Squares C-infinity Collocation, </institution> <type> PhD Thesis, </type> <institution> The University of Texas at Austin, </institution> <month> May </month> <year> 1997. </year>
Reference-contexts: An overview of this design is shown in Figure 3.1. Distributed dynamic arrays are defined in the following subsection. 3.1 Distributed Dynamic Array Abstraction The SDDA and HDDA are implementations of a distributed dynamic array. The distributed dynamic array abstraction, presented in detail in <ref> [2] </ref>, is summarized as follows. In general, an array is defined by a data set D, an index space I, and an injective function F : D -&gt; I An array is dynamic if elements may be dynamically inserted into or removed from its data set D.
Reference: 3. <author> Hans Sagan, </author> <title> Space Filling Curves, </title> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: Recall that an index space requires a well-defined linear ordering relation. Thus the "natural" N dimensional hierarchical index space must effectively be mapped to a linear, or one-dimensional, index space. An efficient family of such maps are defined by space-filling curves (SFC) <ref> [3] </ref>. One such mapping is defined by the Hilbert space-filling curve, illustrated in Figure 3.2. In this mapping a bounded domain is hierarchically partitioned into regions where the regions are given a particular ordering. <p> When used in a parallel/distributed environment, the grid hierarchy is partitioned and distributed across the processors and serves as a template for all application variables or grid functions. The locality preserving composite distribution [9] based on recursive Space-filling Curves <ref> [3] </ref> is used to partition the dynamic grid hierarchy. Operations defined on the grid hierarchy include indexing of individual component grid in the hierarchy, refinement, coarsening, recomposition of the hierarchy after regriding, and querying of the structure of the hierarchy at any instant.
Reference: 4. <author> H.F. Korth, A. Silberschatz, </author> <title> Database System Concepts,. </title> <publisher> McGraw Hill. </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: A DDA's storage structure and converse function consists of two components: (1) local object storage and access and (2) object distribution. The HDDA and SDDA implementations of a DDA use extendible hashing <ref> [4] </ref> & [5] and red-black balanced binary trees [6] respectively for local object storage and access. An application instructs the DDA as to how to distribute data by defining a partitioning of index space I among processors.
Reference: 5. <author> W. Litwin. </author> <title> Linear Hashing: a New Tool for File and Table Addressing, </title> <booktitle> Proceedings of the 6th Conference on VLDB, </booktitle> <address> Montreal, Canada, </address> <year> 1980. </year>
Reference-contexts: A DDA's storage structure and converse function consists of two components: (1) local object storage and access and (2) object distribution. The HDDA and SDDA implementations of a DDA use extendible hashing [4] & <ref> [5] </ref> and red-black balanced binary trees [6] respectively for local object storage and access. An application instructs the DDA as to how to distribute data by defining a partitioning of index space I among processors.
Reference: 6. <author> Robert Sedgewick. </author> <title> Algorithms, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1983. </year>
Reference-contexts: A DDA's storage structure and converse function consists of two components: (1) local object storage and access and (2) object distribution. The HDDA and SDDA implementations of a DDA use extendible hashing [4] & [5] and red-black balanced binary trees <ref> [6] </ref> respectively for local object storage and access. An application instructs the DDA as to how to distribute data by defining a partitioning of index space I among processors. Each index in the index space is uniquely assigned to a particular processor i -&gt; P.
Reference: 7. <author> Marsha J. Berger, Joseph Oliger, </author> <title> Adaptive Mesh-Refinement for Hyperbolic Partial Differential Equations, </title> <journal> Journal of Computational Physics, </journal> <pages> pp. 484-512, </pages> <year> 1984. </year>
Reference-contexts: Refinement proceeds recursively so that regions on the finer grid requiring more resolution are similarly tagged and even finer grids are overlayed on these regions. The resulting grid structure is a dynamic adaptive grid hierarchy. The adaptive grid hierarchy corresponding to the AMR formulation by Berger & Oliger <ref> [7] </ref> is shown in Figure 4.1.
Reference: 8. <author> Manish Parashar and James C. Browne, </author> <title> Distributed Dynamic Data-Structures for Parallel Adaptive Mesh-Refinement, </title> <booktitle> Proceedings of the International Conference for High Performance Computing, </booktitle> <pages> pp. 22-27, </pages> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: A detailed description of the design of these data-structures can be found in <ref> [8] </ref>. SDDG Representation: A multi-dimensional SDDG is represented as a one dimensional ordered list of SDDG blocks. The list is obtained by first blocking the SDDG to achieve the required granularity, and then ordering the SDDG blocks based on the selected space-filling curve.
Reference: 9. <author> Manish Parashar and James C. Browne, </author> <title> On Partitioning Dynamic Adaptive Grid Hierarchies, </title> <booktitle> Proceedings of the 29th Annual Hawaii International Conference on System Sciences, </booktitle> <volume> 1 </volume> <pages> 604-613, </pages> <month> Jan. </month> <year> 1996. </year>
Reference-contexts: When used in a parallel/distributed environment, the grid hierarchy is partitioned and distributed across the processors and serves as a template for all application variables or grid functions. The locality preserving composite distribution <ref> [9] </ref> based on recursive Space-filling Curves [3] is used to partition the dynamic grid hierarchy. Operations defined on the grid hierarchy include indexing of individual component grid in the hierarchy, refinement, coarsening, recomposition of the hierarchy after regriding, and querying of the structure of the hierarchy at any instant.
Reference: 10. <author> J. Masso and C. Bona, </author> <title> Hyperbolic System for Numerical Relativity, </title> <journal> Physics Review Letters, </journal> <volume> 68(1097), </volume> <year> 1992. </year>
Reference-contexts: National Center for Supercomputing Applications (NCSA), University of Illinois at Urbana, has H3expresso (developed at National Center for Supercomputing Applications (NCSA), University of Illinois at Urbana) is a ``concentrated'' version of the full H version 3.3 code that solves the general relativistic Einstein's Equations in a variety of physical scenarios <ref> [10] </ref>. The original H3expresso code is non-adaptive and is implemented in FORTRAN 90. Representation Overheads The overheads of the proposed DAGH/SDDG representation are evaluated by comparing the performance of a hand-coded, unigrid, Fortran 90+MPI implementation of the H3expresso application with a version built using the data-management infrastructure.
Reference: 11. <author> Robert van de Geijn, </author> <title> Using PLAPACK: Parallel Linear Algebra Package, </title> <publisher> The MIT Press, </publisher> <year> 1997. </year>
Reference-contexts: The finite element application uses a second parallel infrastructure which supports distributed vectors and matrices, as denoted in the lower right corner of Figure 5.6. The current release of this infrastructure is documented in <ref> [11] </ref>. Both DDA and linear algebra infrastructures are based upon the common abstraction of an index space.
Reference: 12. <author> Leslie Greengard, </author> <title> The rapid evaluation of potential fields in particle systems, </title> <year> 1987. </year>
Reference-contexts: However, once a threshold in the number of particles is surpassed, approximating the interaction of particles with interactions between sufficiently separated particle clusters allows the computational effort to be substantially reduced. The best known of these fast summation approaches is the fast multipole method <ref> [12] </ref>, which, under certain assumptions, gives a method of O (N) The fast multipole method is a typical divide and conquer algorithm. A cubic computational domain is recursively subdivided into octants.
Reference: 13. <author> Jrgen K. Singer, </author> <title> The Parallel Fast Multipole Method in Molecular Dynamics, </title> <type> PhD thesis, </type> <institution> The University of Houston, </institution> <month> August </month> <year> 1995. </year>

References-found: 13

