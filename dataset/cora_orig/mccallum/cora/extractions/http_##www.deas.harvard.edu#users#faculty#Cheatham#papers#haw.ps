URL: http://www.deas.harvard.edu/users/faculty/Cheatham/papers/haw.ps
Refering-URL: http://www.deas.harvard.edu/users/faculty/Cheatham/cheatham.html
Root-URL: 
Email: valiant@das.harvard.edu  
Title: Bulk Synchronous Parallel Computing A Paradigm for Transportable Software  
Author: Thomas Cheatham Amr Fahmy Dan C. Stefanescu Leslie G. Valiant Email:cheatham, amr, dan, 
Note: Research supported in part by ARPA Contract Nr. F19628-92-C-0113 and a grant from the National Science Foundation, NSF-CDA-9308833 Research supported in part by a grant from the National Science Foundation, NSF-CCR-9200884.  
Address: 33 Oxford St, Cambridge MA 02138  
Affiliation: Aiken Computation Laboratory Harvard University  
Abstract: A necessary condition for the establishment, on a substantial basis, of a parallel software industry would appear to be the availability of technology for generating transportable software, i.e. architecture independent software which delivers scalable performance for a wide variety of applications on a wide range of multiprocessor computers. This paper describes H-BSP - a general purpose parallel computing environment for developing transportable algorithms. H-BSP is based on the Bulk Synchronous Parallel Model (BSP), in which a computation involves a number of supersteps, each having several parallel computational threads that synchronize at the end of the superstep. The BSP Model deals explicitly with the notion of communication among computational threads and introduces parameters g and L that quantify the ratio of communication throughput to computation throughput, and the synchronization period, respectively. These two parameters, together with the number of processors and the problem size, are used to quantify the performance and, therefore, the transportability of given classes of algorithms across machines having different values for these parameters. This paper describes the role of unbundled compiler technology in facilitating the development of such a parallel computer environment. fl Research supported in part by ARPA Contract Nr. F19628-92-C-0113.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A.Aggarwal, </author> <title> A.Chandra and M.Snir Communication Complexity of PRAMs, </title> <journal> Theoretical Computer Science, </journal> <volume> 71(1990), </volume> <pages> pp 3-28. </pages>
Reference-contexts: For example, in Figure 2 the code: Forall i in 1 to d k in 1 to d do d,tsize,TA [i,j],TB [j,k]; C [i, k+(j-1)*d]) generates d 3 processes which are indexed in the itera-tor set by the triple &lt; i; j; k &gt;, i; j; k 2 <ref> [1; d] </ref>. As such the processes are organized as a cube of size d = p 1=3 in each dimension so that we can talk about the process P [i,j,k]. Each of the p processes executes the supersteps associated with the mat thread call that corresponds to it. <p> int) /*superstep 2: multiplication of tiles and redistribution of smaller tiles */ For s in 1 to tsize do For q in 1 to tsize do For r in 1 to tsize do C0 [s, r] &lt;- C0 [s, r] + A [s, q] * B [q, r] put (&lt;index <ref> [1] </ref>, r, index [3]&gt;, TC0, &lt;1..1, r..r&gt;, _) For q in 1 to d get (_, C, &lt;1..tsize, 1..tsize1, q&gt;, _) synch /* superstep 3: final summations */ For q in 1 to tsize For r in 1 to tsize1 For s in 1 to d do END_THREAD /* Main program:
Reference: [2] <author> S. P. Amarasinghe and M. </author> <title> Lam Communication Optimization and Code Generation for Distributed Memory Machines Proceedings of the ACM SIGPLAN'93, </title> <booktitle> Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1993 </year>
Reference-contexts: For example, [14] discusses message aggregation, message pipelining as well as various optimizations of communications. In [13] there is an algorithm for labeling statements with sync ranks which are used in producing optimized programs with less barrier synchronization. Reference <ref> [2] </ref> gives an algorithm for communication optimization by solving a set of inequalities. As described in [24, 17] a BSP program is a sequence of sequential supersteps separated by barrier synchronizations.
Reference: [3] <author> R.H. Bisseling and W. F. </author> <title> McColl Scientific Computing on Bulk Synchronous Parallel Architectures Preprint 836, </title> <institution> Dept. of Mathematics, Utrecht University, </institution> <month> December </month> <year> 1993 </year>
Reference: [4] <author> T. Cheatham, H. Gao, and D. </author> <title> Stefanescu A Suite of Analysis Tools Based on a General Purpose Abstract Interpreter, </title> <booktitle> Proceedings of the International Conference on Compiler Construction, </booktitle> <address> Ed-inburgh, </address> <month> April </month> <year> 1994 </year>
Reference: [5] <author> T. Cheatham, A. Fahmy, and D. </author> <title> Stefanescu Supporting Multiple Evolving Compilers, </title> <address> SEKE'94, Riga, </address> <month> June </month> <year> 1994 </year>
Reference-contexts: This work is described in detail in [9] and is the basis for compiling BSP-L as well as other parallel programming languages. Furthermore, the unbundled nature of the compiler raises issues of configuration management whose solution is described in <ref> [5] </ref>. The rest of the paper illustrates our approach by using as a running example the familiar, yet important problem of matrix multiplication. Section 2 describes an example of a BSP algorithm that is efficient over the full spectrum of parameters of the cost model.
Reference: [6] <author> T. Cheatham, </author> <title> Models, Languages, and Compiler Technology for High Performance Computers, </title> <booktitle> Workshop on Mathematical Foundations of Computer Science, Kosice, Slovakia, Lecture Notes on Computer Science, </booktitle> <publisher> Springer Verlag, </publisher> <month> August </month> <year> 1994. </year>
Reference: [7] <author> T. Cheatham, A. Fahmy, and D. </author> <title> Stefanescu H-BSP A General Purpose Parallel Computing Environment, </title> <booktitle> Proceedings of IFIP World Congress, </booktitle> <volume> Vol. 1, </volume> <pages> pp 515-520, </pages> <address> Hamburg, </address> <month> August </month> <year> 1994 </year>
Reference-contexts: For example, if L=g = 200, as reported in [20] for one measurement, the compiler will choose code implementing straight message broadcast for the case that d = p 1=3 = 8 and N 112. Other BSP-style optimizations are described in <ref> [7] </ref>.
Reference: [8] <author> T. Cheatham, A. Fahmy, and D. </author> <title> Stefanescu A Compiler for BSP-L, A Programming Language for the Bulk Synchronous Processing Model, </title> <booktitle> Proceedings of IEEE TENCON'94, </booktitle> <address> Singapore, </address> <month> August </month> <year> 1994 </year>
Reference-contexts: This is an elementary example of intentionally allowing the use of model of computation parameters to permeate to the language level (see also [18]). In general the parameters L and g may be used in programs in a similar way. The BSP-L language <ref> [8] </ref> is a classically sequential language to which we add several constructs to support parallel processing. For example, the implementation in Figure 2 features sequential constructs like declarations and initializations of variables (e.g. tsize, tsize1 which denote tile sizes) and arrays (e.g.
Reference: [9] <author> T.Cheatham The Unbundled Compiler, </author> <type> Technical Report, </type> <institution> Harvard University, </institution> <year> 1993 </year>
Reference-contexts: Adding or modifying language constructs, primitives, or target architectures is accomplished by modifying one or more of the C j . This work is described in detail in <ref> [9] </ref> and is the basis for compiling BSP-L as well as other parallel programming languages. Furthermore, the unbundled nature of the compiler raises issues of configuration management whose solution is described in [5].
Reference: [10] <author> D. E. Culler, et al. </author> <title> Introduction to Split-C EECS, </title> <institution> UC Berkeley, Berkeley, </institution> <address> CA 94720, </address> <month> April </month> <year> 1993 </year>
Reference-contexts: This is a crucial aspect of what the BSP approach offers when compared with alternative proposals (e.g. <ref> [10, 11, 14] </ref>). Recent work ([19]) reports favorable experience with the Oxford BSP Library which provides six basic BSP primitives to be called from standard sequential languages. The goal of H-BSP is to provide a higher level programming environment, in the same vein as the GPL project ([18]).
Reference: [11] <author> A. Geist, et al. </author> <title> PVM3 Users Guide and Reference Manual ORNL/TM-12187, </title> <institution> Oak Ridge National Laboratory, Tennessee, </institution> <month> May </month> <year> 1993 </year>
Reference-contexts: This is a crucial aspect of what the BSP approach offers when compared with alternative proposals (e.g. <ref> [10, 11, 14] </ref>). Recent work ([19]) reports favorable experience with the Oxford BSP Library which provides six basic BSP primitives to be called from standard sequential languages. The goal of H-BSP is to provide a higher level programming environment, in the same vein as the GPL project ([18]).
Reference: [12] <editor> A.V.Gerbessiotis and L.G.Valiant Direct bulk-synchronous parallel algorithms, </editor> <booktitle> Third Scandina-vian Workshop on Algorithm Theory, </booktitle> <volume> vol. 621, </volume> <pages> pages 1-18, </pages> <publisher> Springer Verlag, </publisher> <year> 1992. </year> <note> Extended version in Journal of Parallel and Distributed Computing, </note> <month> 22, </month> <pages> pp. 251-267, </pages> <year> 1994 </year>
Reference: [13] <author> E. Heinz, M. </author> <title> Phillipson Synchronization Barrier Elimination in Synchronous FORALLs TR13/93 University of Karlsruhe, </title> <month> April </month> <year> 1993 </year>
Reference-contexts: For example, [14] discusses message aggregation, message pipelining as well as various optimizations of communications. In <ref> [13] </ref> there is an algorithm for labeling statements with sync ranks which are used in producing optimized programs with less barrier synchronization. Reference [2] gives an algorithm for communication optimization by solving a set of inequalities.
Reference: [14] <author> S. Hiranandani, K.Kennedy, </author> <title> C.Tseng Compiling Fortran D for MIMD Distributed-Memory Machines Communications of the ACM, </title> <month> August </month> <year> 1992 </year>
Reference-contexts: This is a crucial aspect of what the BSP approach offers when compared with alternative proposals (e.g. <ref> [10, 11, 14] </ref>). Recent work ([19]) reports favorable experience with the Oxford BSP Library which provides six basic BSP primitives to be called from standard sequential languages. The goal of H-BSP is to provide a higher level programming environment, in the same vein as the GPL project ([18]). <p> For example, &lt; 1::n; 1::n; q &gt; describes the set of tuples &lt; i; j; k &gt; where 1 i; j n and k = q. 4 Optimizations There has been considerable work done in developing optimizations for parallel target architectures. For example, <ref> [14] </ref> discusses message aggregation, message pipelining as well as various optimizations of communications. In [13] there is an algorithm for labeling statements with sync ranks which are used in producing optimized programs with less barrier synchronization. Reference [2] gives an algorithm for communication optimization by solving a set of inequalities.
Reference: [15] <editor> J.W.Hong and H.T.Kung I/O Complexity: </editor> <booktitle> The Red-Blue Pebble Game Proceedings of the 13-th ACM Symposium on Theory of Computing, </booktitle> <pages> pp 326-333, </pages> <year> 1981 </year>
Reference: [16] <author> V.Kathail and D. </author> <title> Stefanescu A Data Mapping Parallel Language TR-21-89, </title> <institution> Center for Research in Computing Technology, Harvard University, </institution> <month> December </month> <year> 1989 </year>
Reference-contexts: The size of the dimensions of the new array Q are given by the built-in function dim (Q,k) which returns the size of the k-th dimension of array Q. Some other constructs for defining different views on arrays are particularly applicable to programs solving PDEs and are described in <ref> [16] </ref>. 3.2 Constructs for Specifying Parallelism A process can start new processes using the Forall construct.
Reference: [17] <author> W. F. </author> <title> McColl General Purpose Parallel Computing, </title> <editor> In A.M. Gibbons and P.Spirakis, editors, </editor> <booktitle> Lectures on Parallel Computation, Proc. 1991 ALCOM Spring School on Parallel Computation, vol 4 of Cambridge International Series on Parallel Computation, </booktitle> <publisher> Cambridge University Press, </publisher> <year> 1993 </year>
Reference-contexts: In [13] there is an algorithm for labeling statements with sync ranks which are used in producing optimized programs with less barrier synchronization. Reference [2] gives an algorithm for communication optimization by solving a set of inequalities. As described in <ref> [24, 17] </ref> a BSP program is a sequence of sequential supersteps separated by barrier synchronizations. This organization induces a natural dichotomy on the performance evaluation of a BSP program, and, as a result, on the optimization opportunities for BSP programs. <p> For example, consider the matrix multiplication implementation and suppose that arrays A and B reside on the same processor. Than the compiler may generate the following code for the first stage of the execution of thread mat: 4 This definition is a slight variant of the ones used in <ref> [24, 17, 20] </ref>. /* distribute data from master to workers */ For i in 1 to d do For k in 1 to d do put (P [i,j,k], TB, &lt;j..j, k..k&gt;, 200) In the innermost loop the master broadcasts the TA [i, j] tile to d processes P [ , ,
Reference: [18] <author> W. F. </author> <booktitle> McColl BSP Programming In DIMACS Series of Discrete Mathematics and Theoretical Computer Science, </booktitle> <year> 1994. </year> <note> To appear. </note>
Reference-contexts: It is important to notice that this implementation depends upon p, the number of processors. This is an elementary example of intentionally allowing the use of model of computation parameters to permeate to the language level (see also <ref> [18] </ref>). In general the parameters L and g may be used in programs in a similar way. The BSP-L language [8] is a classically sequential language to which we add several constructs to support parallel processing.
Reference: [19] <author> W. F. </author> <booktitle> McColl Scalable Parallel Computing: A Grand Unified Theory and its Practical Development Proceedings of IFIP World Congress, </booktitle> <volume> Vol. 1, </volume> <pages> pp 539-546, </pages> <address> Hamburg, </address> <month> August </month> <year> 1994 </year>
Reference: [20] <author> R. Miller and J. </author> <title> Reed The Oxford BSP Library. Users Guide. Version 1.0, </title> <publisher> Oxford University, </publisher> <year> 1994 </year>
Reference-contexts: For example, consider the matrix multiplication implementation and suppose that arrays A and B reside on the same processor. Than the compiler may generate the following code for the first stage of the execution of thread mat: 4 This definition is a slight variant of the ones used in <ref> [24, 17, 20] </ref>. /* distribute data from master to workers */ For i in 1 to d do For k in 1 to d do put (P [i,j,k], TB, &lt;j..j, k..k&gt;, 200) In the innermost loop the master broadcasts the TA [i, j] tile to d processes P [ , , <p> The compiler can choose which of these codes to generate by comparing C 1 and C 2 . For example, if L=g = 200, as reported in <ref> [20] </ref> for one measurement, the compiler will choose code implementing straight message broadcast for the case that d = p 1=3 = 8 and N 112. Other BSP-style optimizations are described in [7].
Reference: [21] <author> M. Paterson. </author> <type> Manuscript. </type> <year> 1994 </year>
Reference: [22] <author> J. P. Singh, E. Rothberg, and A. </author> <title> Gupta Modeling Communication in Parallel Algorithms: </title> <booktitle> A Fruitful Interaction Between Theory and Systems Proceedings of the ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <year> 1994. </year>
Reference: [23] <author> D. Stefanescu and Y. </author> <title> Zhou An Equational Framework for the Abstract Analysis of Functional Programs, </title> <booktitle> Proceedings of ACM Conference on Lisp and Functional Programming, </booktitle> <address> Orlando, </address> <year> 1994. </year>
Reference: [24] <author> L. G. </author> <title> Valiant A Bridging Model for Parallel Computation Communications of the ACM, </title> <booktitle> 33(8) </booktitle> <pages> 103-111, </pages> <year> 1990 </year>
Reference-contexts: In [13] there is an algorithm for labeling statements with sync ranks which are used in producing optimized programs with less barrier synchronization. Reference [2] gives an algorithm for communication optimization by solving a set of inequalities. As described in <ref> [24, 17] </ref> a BSP program is a sequence of sequential supersteps separated by barrier synchronizations. This organization induces a natural dichotomy on the performance evaluation of a BSP program, and, as a result, on the optimization opportunities for BSP programs. <p> For example, consider the matrix multiplication implementation and suppose that arrays A and B reside on the same processor. Than the compiler may generate the following code for the first stage of the execution of thread mat: 4 This definition is a slight variant of the ones used in <ref> [24, 17, 20] </ref>. /* distribute data from master to workers */ For i in 1 to d do For k in 1 to d do put (P [i,j,k], TB, &lt;j..j, k..k&gt;, 200) In the innermost loop the master broadcasts the TA [i, j] tile to d processes P [ , ,
Reference: [25] <author> L. G. </author> <title> Valiant A combining mechanism for parallel computers. In Parallel Architectures and Their Efficient Use, </title> <booktitle> Proceedings of First Heinz Nix-dorf Symposium, </booktitle> <address> Paderborn,Germany, </address> <month> November </month> <year> 1992. </year> <booktitle> Lecture Notes in Computer Science Vol678, </booktitle> <publisher> Springer-Verlag, </publisher> <pages> 1-10. </pages>
Reference: [26] <author> L. G. </author> <booktitle> Valiant Why BSP Computers? Proceedings of the 7-th International Parallel Processing Symposium, </booktitle> <pages> pp 2-5, </pages> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1993 </year>
Reference: [27] <editor> M.E.Wolf and M.Lam A Data Locality Optimizing Algorithm, </editor> <booktitle> Conference on Programming Languages Design and Implementation'91, </booktitle> <year> 1991 </year>
References-found: 27

