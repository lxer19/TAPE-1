URL: http://www.cogs.susx.ac.uk/users/christ/papers/complexity.ps
Refering-URL: http://www.cogs.susx.ac.uk/users/christ/index-noframes.html
Root-URL: 
Email: Email: Chris.Thornton@cogs.susx.ac.uk  
Phone: Tel: (44)273 606755 3239  
Title: Measuring the Difficulty of Specific Learning Problems  
Author: Chris Thornton 
Date: October 21, 1994  
Address: Brighton BN1 9QN  
Affiliation: Cognitive and Computing Sciences University of Sussex  
Abstract: Existing complexity measures from contemporary learning theory cannot be conveniently applied to specific learning problems (e.g., training sets). Moreover, they are typically non-generic, i.e., they necessitate making assumptions about the way in which the learner will operate. The lack of a satisfactory, generic complexity measure for learning problems poses difficulties for researchers in various areas; the present paper puts forward an idea which may help to alleviate these. It shows that supervised learning problems fall into two, generic, complexity classes only one of which is associated with computational tractability. By determining which class a particular problem belongs to, we can thus effectively evaluate its degree of generic difficulty. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Rumelhart, D., Hinton, G. and Williams, R. </author> <year> (1986). </year> <title> Learning representations by back-propagating errors. </title> <booktitle> Nature, </booktitle> <pages> 323 (pp. 533-6). </pages>
Reference-contexts: 1 Introduction Recent progress in computational learning has produced a wide variety of learning models and methods. Some of these are associated with the field of connectionism, e.g. backpropagation <ref> [1] </ref>; others are associated with work on evolutionary and genetic methods [2]; still others are associated with work in the field of machine learning, e.g. C4.5 [3], Foil [4] and Golem [5]. <p> Various other learning algorithms exist which provide effective methods for accessing type-1 justifications. Such methods are often biased towards first-order justifications (cf. the Least-Mean-Squares [19] and Perceptron [20] methods) and are thus subject to the same reservations | and recommendations | as the ID3 algorithm. Methods related to backpropagation <ref> [1] </ref>, which do not have such an obvious first-order bias, can potentially be used to find higher-order type-1 solutions (se further discussion below). 4 2 Of course, there is no commitment in this to the idea that the learning method actually carries out a search. 3 See also the latest variant <p> If we leave aside those mechanisms which are based on domain-specific search [21, 22, 23, 24], we are left with a fairly small field, the main contenders in which appear to be the well-known connectionist learning algorithms such as backpropagation <ref> [1] </ref> and cascade-correlation [25]. However, the jury is still very much out on the degree to which such algorithms are capable of discovering and exploiting type-2 (i.e., relational) effects [26]. The empirical evidence, then, is somewhat contradictory. <p> But the picture is clouded by the fact that backpropagation reliably fails to solve parity problems when they are presented as learning problems, i.e., generalization problems presented using non-exhaustive training sets. The graph shown in Figure 1 was produced from an empirical study that involved running standard backpropagation <ref> [1] </ref> on 4-bit parity generalization problems (with four cases used as unseens) using a wide range of internal architectures. All the curves in the upper half of the graph are error profiles 7 for the testing set of four cases.
Reference: [2] <author> Goldberg, D. </author> <year> (1989). </year> <title> Genetic Algorithms in Search, Optimization, and Machine Learning. </title> <publisher> Addison-Wesley. </publisher>
Reference-contexts: 1 Introduction Recent progress in computational learning has produced a wide variety of learning models and methods. Some of these are associated with the field of connectionism, e.g. backpropagation [1]; others are associated with work on evolutionary and genetic methods <ref> [2] </ref>; still others are associated with work in the field of machine learning, e.g. C4.5 [3], Foil [4] and Golem [5]. Given the wide range of methods on offer, it is important for researchers to adopt a principled approach to the task of matching up methods with learning problems.
Reference: [3] <author> Quinlan, J. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Mateo, California: </address> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: Some of these are associated with the field of connectionism, e.g. backpropagation [1]; others are associated with work on evolutionary and genetic methods [2]; still others are associated with work in the field of machine learning, e.g. C4.5 <ref> [3] </ref>, Foil [4] and Golem [5]. Given the wide range of methods on offer, it is important for researchers to adopt a principled approach to the task of matching up methods with learning problems. Unfortunately, there are still major obstacles to be overcome. <p> such an obvious first-order bias, can potentially be used to find higher-order type-1 solutions (se further discussion below). 4 2 Of course, there is no commitment in this to the idea that the learning method actually carries out a search. 3 See also the latest variant of ID3 called C4.5 <ref> [3] </ref>. 4 The Perceptron learning algorithm is, of course, a method which can only be successfully applied to linearly separable problems, i.e., discrimination problems which can be solved by identifying a linear hyperplane separating the relevant classes.
Reference: [4] <author> Quinlan, J. </author> <year> (1990). </year> <title> Learning logical definitions from relations. </title> <booktitle> Machine Learning, </booktitle> <pages> 5 (pp. 239-266). </pages>
Reference-contexts: Some of these are associated with the field of connectionism, e.g. backpropagation [1]; others are associated with work on evolutionary and genetic methods [2]; still others are associated with work in the field of machine learning, e.g. C4.5 [3], Foil <ref> [4] </ref> and Golem [5]. Given the wide range of methods on offer, it is important for researchers to adopt a principled approach to the task of matching up methods with learning problems. Unfortunately, there are still major obstacles to be overcome.
Reference: [5] <author> Feng, C. and Muggleton, S. </author> <year> (1992). </year> <title> Towards inductive generalization in higher-order logic. </title> <editor> In D. Sleeman and P. Edwards (Eds.), </editor> <booktitle> Proceedings of the Ninth International Workshop on Machine Learning (ML92) (pp. </booktitle> <pages> 154-162). </pages> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Some of these are associated with the field of connectionism, e.g. backpropagation [1]; others are associated with work on evolutionary and genetic methods [2]; still others are associated with work in the field of machine learning, e.g. C4.5 [3], Foil [4] and Golem <ref> [5] </ref>. Given the wide range of methods on offer, it is important for researchers to adopt a principled approach to the task of matching up methods with learning problems. Unfortunately, there are still major obstacles to be overcome.
Reference: [6] <author> Kearns, M. </author> <year> (1990). </year> <title> The Computational Complexity of Machine Learning. </title> <publisher> The MIT Press. </publisher>
Reference-contexts: Recent results in computational learning theory <ref> [6] </ref> extend earlier work by Valiant on PAC learning [7, 8] and rely in particular on the concept of VC dimension [9]. Work in this area has tended to concentrate on distribution-free (i.e., problem-independent) analysis although some recent work has given attention to distribution-specific analysis [10].
Reference: [7] <author> Valiant, L. </author> <year> (1984). </year> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <pages> 27 (pp. 1134-42). </pages>
Reference-contexts: Recent results in computational learning theory [6] extend earlier work by Valiant on PAC learning <ref> [7, 8] </ref> and rely in particular on the concept of VC dimension [9]. Work in this area has tended to concentrate on distribution-free (i.e., problem-independent) analysis although some recent work has given attention to distribution-specific analysis [10].
Reference: [8] <author> Valiant, L. </author> <year> (1985). </year> <title> Learning disjunctions of conjunctions. </title> <booktitle> Proceedings of the Ninth International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 560-566). </pages> <address> Los Altos: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Recent results in computational learning theory [6] extend earlier work by Valiant on PAC learning <ref> [7, 8] </ref> and rely in particular on the concept of VC dimension [9]. Work in this area has tended to concentrate on distribution-free (i.e., problem-independent) analysis although some recent work has given attention to distribution-specific analysis [10].
Reference: [9] <author> Vapnik, V. and Chervonenkis, A. </author> <year> (1971). </year> <title> On the uniform convergence of relative frequencies of events to their probabilities. </title> <journal> Theor. Probab. Appl., </journal> <volume> 16, No. </volume> <pages> 2 (pp. 264-280). </pages>
Reference-contexts: Recent results in computational learning theory [6] extend earlier work by Valiant on PAC learning [7, 8] and rely in particular on the concept of VC dimension <ref> [9] </ref>. Work in this area has tended to concentrate on distribution-free (i.e., problem-independent) analysis although some recent work has given attention to distribution-specific analysis [10]. However, even results in distribution-specific analysis do not address the issue of the complexity of a specified learning problem.
Reference: [10] <author> Linial, N., Mansour, Y. and Nisan, N. </author> <year> (1988). </year> <title> Constant depth circuits, fourier transform and learnability. </title> <booktitle> Proceedings of the 30th I.E.E.E. Symposium on Computational Learning Theory (pp. </booktitle> <pages> 56-68). </pages> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: Work in this area has tended to concentrate on distribution-free (i.e., problem-independent) analysis although some recent work has given attention to distribution-specific analysis <ref> [10] </ref>. However, even results in distribution-specific analysis do not address the issue of the complexity of a specified learning problem. As things stand, then, there is no general method to evaluate the difficulty of a specific learning problem without making assumptions about which learning algorithm will be used.
Reference: [11] <author> Wittgenstein, L. </author> <year> (1958). </year> <title> Philosophical Investigations. </title> <publisher> Oxford: Basil Blackwell. </publisher> <pages> 11 </pages>
Reference-contexts: The conclusion seems to be reinforced by Wittgenstein's well-known claim to the effect that any mathematical sequence has an arbitrary number of generative rules. <ref> [11] </ref> It is also mutually-reinforcing with the observation that solving a learning problem is very much the same task as finding the `best' (i.e. smallest) computer program which reproduces a particular dataset. This task is known to be NP-hard [12, 13,14,15].
Reference: [12] <author> Chaitin, G. </author> <year> (1987). </year> <title> Algorithmic Information Theory. </title> <publisher> Cambridge: Cambridge University Press. </publisher>
Reference-contexts: This task is known to be NP-hard <ref> [12, 13,14,15] </ref>. So, once again, the implication seems to be that ideal learning is impossible and that generic measurement of learning difficulty is a non-starter.
Reference: [13] <author> Chaitin, G. </author> <year> (1966). </year> <title> On the length of programs for computing finite binary sequences. </title> <journal> Journal of The Association of Computing Machinery, </journal> <pages> 13 (pp. 547-569). </pages>
Reference: [14] <author> Kolmogorov, A. </author> <year> (1965). </year> <title> Three approaches to the quantitative definition of information. </title> <journal> Prob. Inf. Trans., </journal> <pages> 1 (pp. 1-7). </pages>
Reference: [15] <author> Solomonoff, R. </author> <year> (1964). </year> <title> A formal theory of inductive inference i and II. </title> <journal> Information and Control, </journal> <pages> 7 (pp. 1-22 and 224-254). </pages>
Reference: [16] <author> Dietterich, T., London, B., Clarkson, K. and Dromey, G. </author> <year> (1982). </year> <title> Learning and inductive inference. </title> <editor> In P. Cohen and E. Feigenbaum (Eds.), </editor> <booktitle> The Handbook of Artificial Intelligence: Vol III. </booktitle> <address> Los Altos: </address> <publisher> Kaufmann. </publisher>
Reference-contexts: A long-standing complexity-estimation heuristic is simply to determine whether or not the learning problem in question involves the capturing of relations. As has been known for some time <ref> [16] </ref> problems which involve relational effects are, in general, harder to solve than problems which do not. Another, reliable heuristic approach involves determining the order of statistical effect which underpins the solution to the problem [17]. The idea here is that higher-order effects are harder to exploit.
Reference: [17] <author> Hinton, G. and Sejnowski, T. </author> <year> (1986). </year> <title> Learning and relearning in boltzmann machines. </title> <editor> In D. Rumelhart, J. McClelland and the PDP Research Group (Eds.), </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructures of Cognition. Vols I and II (pp. </booktitle> <pages> 282-317). </pages> <address> Cambridge, Mass.: </address> <publisher> MIT Press. </publisher>
Reference-contexts: As has been known for some time [16] problems which involve relational effects are, in general, harder to solve than problems which do not. Another, reliable heuristic approach involves determining the order of statistical effect which underpins the solution to the problem <ref> [17] </ref>. The idea here is that higher-order effects are harder to exploit. Are these heuristics unfounded? If so, we need to ask why researchers find them relatively reliable. If they are not unfounded then we need to determine precisely what their true foundation is.
Reference: [18] <author> Quinlan, J. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <booktitle> Machine Learning, </booktitle> <pages> 1 (pp. 81-106). </pages>
Reference-contexts: But, in fact, by using learning algorithms which perform a close approximation of the type-1 discovery task, we can effectively measure type-1 complexity much more efficiently than is done via exhaustive search. The well-known ID3 algorithm <ref> [18] </ref> is perhaps the most obvious candidate for our purposes here. 3 This supervised algorithm builds a solution by recursively splitting the training cases into subsets until the point is reached where each subset is associated with a unique output.
Reference: [19] <author> Hinton, G. </author> <year> (1989). </year> <title> Connectionist learning procedures. </title> <booktitle> Artificial Intelligence, </booktitle> <pages> 40 (pp. 185-234). </pages>
Reference-contexts: If this property is obtained, then higher-order justifications will not exist and any type-1 solution will necessarily be the first-order solution identified by ID3. Various other learning algorithms exist which provide effective methods for accessing type-1 justifications. Such methods are often biased towards first-order justifications (cf. the Least-Mean-Squares <ref> [19] </ref> and Perceptron [20] methods) and are thus subject to the same reservations | and recommendations | as the ID3 algorithm.
Reference: [20] <author> Minsky, M. and Papert, S. </author> <year> (1988). </year> <title> Perceptrons: An Introduction to Computational Geometry (expanded edn). </title> <address> Cambridge, Mass.: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Various other learning algorithms exist which provide effective methods for accessing type-1 justifications. Such methods are often biased towards first-order justifications (cf. the Least-Mean-Squares [19] and Perceptron <ref> [20] </ref> methods) and are thus subject to the same reservations | and recommendations | as the ID3 algorithm. <p> The empirical evidence, then, is somewhat contradictory. For example, it is well known that backpropagation can solve parity problems such as exclusive-or, the notoriously `difficult' problem that could not be solved by perceptron learning <ref> [20] </ref>. As I showed earlier, exhaustive training sets for parity problems are completely lacking in type-1 effects.
Reference: [21] <author> Langley, P., Simon, H., Bradshaw, G. and Zytkow, J. </author> <year> (1987). </year> <title> Scientific Discovery: Computational Explorations of the Creative Processes. </title> <address> Cambridge, Mass.: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Unfortunately, the choice of candidate mechanisms is much smaller in the type-2 case than it is in the type-1 case. If we leave aside those mechanisms which are based on domain-specific search <ref> [21, 22, 23, 24] </ref>, we are left with a fairly small field, the main contenders in which appear to be the well-known connectionist learning algorithms such as backpropagation [1] and cascade-correlation [25].
Reference: [22] <author> Lenat, D. </author> <year> (1983). </year> <title> Theory formation by heuristic search; the nature of heuristics II: background and examples. </title> <booktitle> Artificial Intelligence, </booktitle> <pages> 21 (pp. 31-59). </pages>
Reference-contexts: Unfortunately, the choice of candidate mechanisms is much smaller in the type-2 case than it is in the type-1 case. If we leave aside those mechanisms which are based on domain-specific search <ref> [21, 22, 23, 24] </ref>, we are left with a fairly small field, the main contenders in which appear to be the well-known connectionist learning algorithms such as backpropagation [1] and cascade-correlation [25].
Reference: [23] <author> Lenat, D. </author> <year> (1983). </year> <title> EURISKO: a program that learns new heuristics and domain concepts: the nature of heuristics III: program design and results. </title> <booktitle> Artificial Intelligence, </booktitle> <pages> 21 (pp. 61-98). </pages>
Reference-contexts: Unfortunately, the choice of candidate mechanisms is much smaller in the type-2 case than it is in the type-1 case. If we leave aside those mechanisms which are based on domain-specific search <ref> [21, 22, 23, 24] </ref>, we are left with a fairly small field, the main contenders in which appear to be the well-known connectionist learning algorithms such as backpropagation [1] and cascade-correlation [25].
Reference: [24] <author> Hofstadter, D. </author> <year> (1984). </year> <title> The copycat project. A.I. </title> <type> Memo 755, </type> <institution> Masachusetts Institute of Technology. </institution>
Reference-contexts: Unfortunately, the choice of candidate mechanisms is much smaller in the type-2 case than it is in the type-1 case. If we leave aside those mechanisms which are based on domain-specific search <ref> [21, 22, 23, 24] </ref>, we are left with a fairly small field, the main contenders in which appear to be the well-known connectionist learning algorithms such as backpropagation [1] and cascade-correlation [25].
Reference: [25] <author> Fahlman, S. and Lebiere, C. </author> <year> (1990). </year> <title> The Cascade-Correlation Learning Architecture. </title> <institution> CMU-CS-90-100, School of Computer Science, Carnegie-Mellon University, </institution> <address> Pittsburgh, PA 15213. </address>
Reference-contexts: If we leave aside those mechanisms which are based on domain-specific search [21, 22, 23, 24], we are left with a fairly small field, the main contenders in which appear to be the well-known connectionist learning algorithms such as backpropagation [1] and cascade-correlation <ref> [25] </ref>. However, the jury is still very much out on the degree to which such algorithms are capable of discovering and exploiting type-2 (i.e., relational) effects [26]. The empirical evidence, then, is somewhat contradictory.
Reference: [26] <author> Clark, A. and Thornton, C. </author> <year> (1993). </year> <title> Trading spaces: computation, representation and the limits of learning. </title> <booktitle> Cognitive Science Research Paper 291, </booktitle> <address> Brighton BN1 9QH: </address> <institution> University of Sussex (Price:1.50). </institution> <month> 12 </month>
Reference-contexts: However, the jury is still very much out on the degree to which such algorithms are capable of discovering and exploiting type-2 (i.e., relational) effects <ref> [26] </ref>. The empirical evidence, then, is somewhat contradictory. For example, it is well known that backpropagation can solve parity problems such as exclusive-or, the notoriously `difficult' problem that could not be solved by perceptron learning [20].
References-found: 26

