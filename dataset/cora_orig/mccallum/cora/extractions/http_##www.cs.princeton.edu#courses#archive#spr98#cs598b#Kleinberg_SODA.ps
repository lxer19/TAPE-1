URL: http://www.cs.princeton.edu/courses/archive/spr98/cs598b/Kleinberg_SODA.ps
Refering-URL: http://www.cs.princeton.edu/courses/archive/spr98/cs598b/papers.html
Root-URL: http://www.cs.princeton.edu
Title: Authoritative Sources in a Hyperlinked Environment  
Author: Jon M. Kleinberg 
Abstract: The link structure of a hypermedia environment can be a rich source of information about the content of the environment, provided we have effective means for understanding it. Versions of this principle have been studied in the hypertext research community and (in a context predating hypermedia) through journal citation analysis in the field of bibliometrics. But for the problem of searching in hyperlinked environments such as the World Wide Web, it is clear from the prevalent techniques that the information inherent in the links has yet to be fully exploited. In this work we develop a new method for automatically extracting certain types of information about a hypermedia environment from its link structure, and we report on experiments that demonstrate its effectiveness for a variety of search problems on the www. The central problem we consider is that of determining the relative "authority" of pages in such environments. This issue is central to a number of basic hypertext search tasks; for example, if the result of a query-based search consists of a large set of relevant pages, one may wish to select a small subset of the most "definitive" or "authoritative" pages to present to a user. At the same time, it is clearly difficult to formulate a definition of authority precise enough to be used in such contexts. We propose and test an algorithmic formulation of the notion of authority, based on a method for locating dense bipartite communities in the link structure. Our formulation has an interesting interpretation in terms of the eigenvectors of certain matrices associated with the link graph; this motivates additional heuristics for clustering and for computing a type of link-based similarity among hyperlinked documents. fl Preliminary versions of this paper appear in the Proceedings of the ACM-SIAM Symposium on Discrete Algorithms, 1998, and as IBM Research Report RJ 10076, May 1997. y Department of Computer Science, Cornell University, Ithaca NY 14853. Email: kleinber@cs.cornell.edu. This work was performed in large part while on leave at the IBM Almaden Research Center, San Jose CA 95120. The author is currently supported by an Alfred P. Sloan Research Fellowship and by NSF Faculty Early Career Development Award CCR-9701399. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G.O. Arocena, A.O. Mendelzon, G.A. Mihaila, </author> <title> "Applications of a Web query language," </title> <booktitle> Proc. 6th International World Wide Web Conference, </booktitle> <year> 1997. </year>
Reference-contexts: More recently, Pirolli, Pitkow, and Rao [24] have used a combination of link topology and textual similarity to group together and categorize pages on the www. Arocena, Mendelzon, and Mihaila <ref> [1] </ref> and Spertus [33] have described frameworks for constructing www queries from a combination of term-matching and link-based predicates.
Reference: [2] <author> A.E. Bayer, J.C. Smart, G.W. McLaughlin, </author> <title> "Mapping intellectual structure of scientific subfields through author co-citations," </title> <journal> J. American Soc. Info. Sci., </journal> <volume> 41(1990), </volume> <pages> pp. 444-452. </pages>
Reference: [3] <author> R. Botafogo, E. Rivlin, B. Shneiderman, </author> <title> "Structural analysis of hypertext: Identifying hierarchies and useful metrics," </title> <journal> ACM Trans. Inf. Sys., </journal> <volume> 10(1992), </volume> <pages> pp. 142-180. </pages>
Reference-contexts: 1 Introduction The link structure of a hypermedia environment can be a rich source of information about the content of the environment, provided we have effective means for understanding it. Versions of this principle have been studied in the hypertext research community <ref> [3, 13, 26, 36] </ref> and (in a context predating hypermedia) through journal citation analysis in the field of bibliometrics [37]. <p> There has been some work on using hyperlinks for clustering and searching in the hypertext research community. Rivlin, Botafogo, and Schneiderman <ref> [3, 26] </ref> use basic graph-theoretic notions such as connectivity, as well as "compactness" measures based on node-to-node distances, to identify clusters in the graph of a hypertext environment.
Reference: [4] <author> J. Carriere, R. Kazman, "WebQuery: </author> <title> Searching and visualizing the Web through connectivity," </title> <booktitle> Proc. 6th International World Wide Web Conference, </booktitle> <year> 1997. </year>
Reference-contexts: Finally, Carriere and Kazman <ref> [4] </ref> propose a link-based method for visualizing and ranking the results of queries returned by www search engines. <p> Although the notion of augmenting search engine results to a "one-step neighborhood" is a basic step in our method, the algorithmic component of our work differs significantly from that of <ref> [4] </ref>.
Reference: [5] <author> C. Chekuri, M. Goldwasser, P. Raghavan and E. </author> <title> Upfal "Web search using automated classification," </title> <note> submitted for publication. </note>
Reference-contexts: This may arise for several reasons: there could "on-topic" versus "off-topic" communities (e.g. a community of pages on java together with a smaller community on Caribbean vacations); the query term could have multiple meanings or uses in different settings (e.g. "jaguar" <ref> [5] </ref>); or the query term could refer to a polarized issue involving groups that will not link to one another (e.g. "abortion"). <p> Alternately, it is possible that there will be several communities, all of them relevant, but well-separated from one another in the graph on T for a variety of possible reasons. For example, (1) The string may have several different meanings. E.g. (jaguar) (an example drawn from <ref> [5] </ref>). (2) The string may arise as a term in the context of multiple technical communities. E.g. ("randomized algorithms"). (3) The string may refer to a highly polarized issue, involving groups that are not likely to link to one another. E.g. (abortion).
Reference: [6] <institution> Digital Equipment Corporation, </institution> <note> AltaVista search engine, http://altavista.digital.com/. </note>
Reference-contexts: other hand, one could easily expect to find many thousand relevant pages in an environment such as the www; such a set of pages might be generated by variants of term-matching (e.g. one enters a string such as "web browsers," "Gates," or "censorship" into a search engine such as AltaVista <ref> [6] </ref>), or by more sophisticated means. Thus, there is not an issue of scarcity here. Instead, the fundamental difficulty lies in what could be called the Abundance Problem: The number of pages that could reasonably be returned as "relevant" is far too large for a human user to digest. <p> Multiple Search Engines One straightforward way to produce different root sets for the same query string is to issue the query to several different term-based search engines, such as AltaVista <ref> [6] </ref>, Infoseek [18], and Excite [10]. Typically, issuing the query to several search engines will have the effect of producing root sets that have very little intersection with one another.
Reference: [7] <author> W.E. Donath, A.J. Hoffman, </author> <title> "Algorithms for partitioning of graphs and computer logic based on eigenvectors of connections matrices," </title> <journal> IBM Technical Disclosure Bulletin, </journal> <volume> 15(1972), </volume> <pages> pp. 938-944. </pages>
Reference-contexts: The heuristic intuition behind this approach is analogous to the spectral partitioning of undirected graphs (e.g. <ref> [7, 11, 34] </ref>); however, it is important to note that what we are doing here is not simply a spectral partitioning of the Web graph. <p> Another interesting feature of the communities derived from non-principal eigenvectors is the following. Drawing on the heuristic intuition underlying spectral graph partitioning <ref> [7, 11, 34] </ref>, one expects pairs of communities (X + i ; Y + i ; Y i ) associated with the same eigenvector to be very sparsely connected in the underlying graph. In some cases, this sparse linkage can have meaning in the context of the query topic.
Reference: [8] <author> B. Duffy, J. Yacovissi, </author> <title> "Seven self-contradicting reasons why the World Wide Web is such a big deal," Multimedia Monitor, </title> <month> August </month> <year> 1996. </year> <note> Also at http://www.strcom.com/7reasons.htm. </note>
Reference-contexts: And look beyond the grass-roots Webheads too, if you want. You may be surprised at how often similar though usually more carefully constituted - jump-lists turn up at the Web sites of corporations as well <ref> [8] </ref>. Overview In Section 2, we describe the basic method and show examples of its behavior for finding authoritative pages.
Reference: [9] <author> S. Deerwester, S. Dumais, T. Landauer, G. Furnas, R. Harshman, </author> <title> "Indexing by latent semantic analysis," </title> <journal> J. American Soc. Info. Sci., </journal> <volume> 41(1990), </volume> <pages> pp. 391-407. </pages>
Reference-contexts: Unfortunately, none of these pages contain the respective query term. While much work has gone into text-based methods for circumventing these relevance-related difficulties (e.g. latent semantic indexing <ref> [9] </ref> and a range of other clustering techniques in information retrieval), our goal here is to understand how much can be accomplished by focusing on link structures, for finding pages that are simultaneously authoritative and relevant. <p> In the context of this scoring framework, the best function to use is most likely something that is not based purely on term-matching; for example, one could use a scoring function derived from a technique such as latent semantic indexing <ref> [9] </ref>, although we have not tested this here. Despite the crudeness of our scoring function, it has proved to be surprisingly effective in many of our tests. The clustering performed by our algorithm may provide one reason why pure term-counting is more effective in this setting than in others.
Reference: [10] <author> Excite Inc. </author> <title> Excite navigation service, </title> <address> http://www.excite.com. </address>
Reference-contexts: Multiple Search Engines One straightforward way to produce different root sets for the same query string is to issue the query to several different term-based search engines, such as AltaVista [6], Infoseek [18], and Excite <ref> [10] </ref>. Typically, issuing the query to several search engines will have the effect of producing root sets that have very little intersection with one another.
Reference: [11] <author> M. Fielder, </author> <title> "Algebraic connectivity of graphs," Czech. </title> <journal> Math. J., </journal> <volume> 23(1973), </volume> <pages> pp. 298-305. </pages>
Reference-contexts: The heuristic intuition behind this approach is analogous to the spectral partitioning of undirected graphs (e.g. <ref> [7, 11, 34] </ref>); however, it is important to note that what we are doing here is not simply a spectral partitioning of the Web graph. <p> Another interesting feature of the communities derived from non-principal eigenvectors is the following. Drawing on the heuristic intuition underlying spectral graph partitioning <ref> [7, 11, 34] </ref>, one expects pairs of communities (X + i ; Y + i ; Y i ) associated with the same eigenvector to be very sparsely connected in the underlying graph. In some cases, this sparse linkage can have meaning in the context of the query topic.
Reference: [12] <author> FindLaw, FindLaw - LawCrawler, </author> <note> http://www.lawcrawler.com. </note>
Reference: [13] <author> M.E. Frisse, </author> <title> "Searching for information in a hypertext medical handbook," </title> <journal> Communications of the ACM, </journal> <volume> 31(7), </volume> <pages> pp. 880-886. </pages>
Reference-contexts: 1 Introduction The link structure of a hypermedia environment can be a rich source of information about the content of the environment, provided we have effective means for understanding it. Versions of this principle have been studied in the hypertext research community <ref> [3, 13, 26, 36] </ref> and (in a context predating hypermedia) through journal citation analysis in the field of bibliometrics [37]. <p> Larson [22] performs a co-citation analysis of a set of pages relevant to a sample query and generates clusters by dimension-reduction techniques. Finally, Frisse <ref> [13] </ref> describes a method that is applicable in a tree-structured environment: the relevance of a page with respect to a query is also based on the relevance of its descendants in the tree.
Reference: [14] <author> TradeWave Corporation, </author> <note> Galaxy, http://doradus.einet.net/galaxy.html. </note>
Reference-contexts: We will be presenting a number of such examples in the following sections. Second, there is a sense in which we can compare the technique to existing human-constructed benchmarks: there are a number of searchable hierarchies on the www, such as yahoo [38], Galaxy <ref> [14] </ref>, Zia [39], and the distributed www Virtual Library [35]. These hierarchies provide lists of authoritative pages, compiled by human moderators, for many standard search topics. In this way, they represent high-quality hub pages, and can be used for comparison with our automated method. <p> We mentioned examples of such searchable hierarchies in the introduction (e.g. yahoo [38], Galaxy <ref> [14] </ref>, Zia [39], and the www Virtual Library [35]); they are lists of authoritative pages, compiled by humans, on a variety of broad search topics.
Reference: [15] <author> E. </author> <title> Garfield, "Citation analysis as a tool in journal evaluation," </title> <journal> Science, </journal> <volume> 178(1972), </volume> <pages> pp. 471-479. </pages>
Reference-contexts: There has also been work in bibliometrics on using citation counts to assess the "impact" of scientific journals; this is more closely related to the issue we are considering here. The classic work in this area is that of Garfield <ref> [15] </ref>; see also e.g. [16, 27].
Reference: [16] <author> E. </author> <title> Garfield, "The impact factor," Current Contents, </title> <address> June 20, </address> <year> 1994. </year>
Reference-contexts: There has also been work in bibliometrics on using citation counts to assess the "impact" of scientific journals; this is more closely related to the issue we are considering here. The classic work in this area is that of Garfield [15]; see also e.g. <ref> [16, 27] </ref>. <p> At the same time, it has been observed in bibliometric studies that review journals often constitute exceptions to certain basic principles <ref> [16, 27] </ref>; it would be interesting to see whether the distinction between hubs and authorities would be useful in this regard. There has been some work on using hyperlinks for clustering and searching in the hypertext research community.
Reference: [17] <author> G. Golub, C.F. Van Loan, </author> <title> Matrix Computations, </title> <publisher> Johns Hopkins University Press, </publisher> <year> 1989. </year>
Reference-contexts: Thus x n is the unit vector in the direction of (A T A) n1 A T z, and y n is the unit vector in the direction of (AA T ) n z. Now, a standard result of linear algebra (see e.g. <ref> [17] </ref>) states that if M is a symmetric n fi n matrix, and v is a vector not orthogonal to the principal eigenvector ! 1 (M ), then the unit vector in the direction of M n v converges to ! 1 (M ) as n increases without bound.
Reference: [18] <author> Infoseek Corporation, </author> <title> Infoseek search engine, </title> <note> http://www.infoseek.com. 30 </note>
Reference-contexts: Multiple Search Engines One straightforward way to produce different root sets for the same query string is to issue the query to several different term-based search engines, such as AltaVista [6], Infoseek <ref> [18] </ref>, and Excite [10]. Typically, issuing the query to several search engines will have the effect of producing root sets that have very little intersection with one another.
Reference: [19] <institution> International Business Machines, IBM patent server, </institution> <note> http://patent.womplex.ibm.com. </note>
Reference-contexts: This is an issue that is largely separate from the computational requirements of our algorithm. For example, in preliminary experiments on a local corpus of two million U.S. patents <ref> [19] </ref>, these problems associated with processing the documents did not arise.) Related Work on Link Structures Methodologically, our work has connections to the area of bibliometrics [37] | the study of written documents and their citation structure. Some related work has also been done in the hypertext research community. <p> At the most basic level, one can investigate its application to the cross-referencing structure of collections of scientific papers or patents; we have begun experimenting with this method on a large corpus of U.S. patents <ref> [19] </ref>. But more generally, there are a number of naturally arising directed graph structures in which one can find clear interpretations for the notions of "hubs" and "authorities," and the dense communities which they comprise.
Reference: [20] <author> M.M. Kessler, </author> <title> "Bibliographic coupling between scientific papers," </title> <journal> American Documentation, </journal> <volume> 14(1963), </volume> <pages> pp. 10-25. </pages>
Reference-contexts: This work has focused predominantly on the use of citations and/or explicit hyperlinks as a means of clustering and enhancing relevance judgments. Two basic measures of document similarity to emerge from the study of bibliometrics are bibliographic coupling <ref> [20] </ref> and co-citation [32]. For two documents p and q, the former quantity is equal to the number of documents cited by both p and q, and the latter quantity is the number of documents that cite both p and q.
Reference: [21] <author> T.R. Kochtanek, </author> <title> "Document clustering using macro retrieval techniques," </title> <journal> J. American Soc. Info. Sci., </journal> <volume> 34(1983), </volume> <pages> pp. 356-359. </pages>
Reference: [22] <author> R. Larson, </author> <title> "Bibliometrics of the World Wide Web: An exploratory analysis of the intellectual structure of cyberspace," </title> <journal> Ann. Meeting of the American Soc. Info. Sci., </journal> <year> 1996. </year>
Reference-contexts: Weiss et al. [36] define similarity measures among 8 pages in a hypertext environment based on the link structure; these measures are generalizations of co-citation and bibliographic coupling to allow for arbitrarily long chains of references. Larson <ref> [22] </ref> performs a co-citation analysis of a set of pages relevant to a sample query and generates clusters by dimension-reduction techniques.
Reference: [23] <author> L. Page, "PageRank: </author> <title> Bringing order to the Web," Stanford Digital Libraries working paper 1997-0072. </title>
Reference-contexts: Arocena, Mendelzon, and Mihaila [1] and Spertus [33] have described frameworks for constructing www queries from a combination of term-matching and link-based predicates. Page <ref> [23] </ref> has developed a method for assigning a universal "rank" to each page on the www, so that subsequent user searches can be focused on highly ranked pages; the rank of a page is based on a weight-propagation algorithm that corresponds roughly to simulating a short random walk on the directed
Reference: [24] <author> P. Pirolli, J. Pitkow, R. Rao, </author> <title> "Silk from a sow's ear: Extracting usable structures from the Web," </title> <booktitle> Proceedings of ACM SIGCHI Conference on Human Factors in Computing, </booktitle> <year> 1996. </year>
Reference-contexts: Finally, Frisse [13] describes a method that is applicable in a tree-structured environment: the relevance of a page with respect to a query is also based on the relevance of its descendants in the tree. More recently, Pirolli, Pitkow, and Rao <ref> [24] </ref> have used a combination of link topology and textual similarity to group together and categorize pages on the www. Arocena, Mendelzon, and Mihaila [1] and Spertus [33] have described frameworks for constructing www queries from a combination of term-matching and link-based predicates.
Reference: [25] <author> C.J. van Rijsbergen, </author> <note> Information Retrieval, Butterworths, 1979. Also at http://dcs.glasgow.ac.uk/Keith/Preface.html. </note>
Reference-contexts: Documents constitute the nodes of an undirected graph whose edge weights are measures of similarity; edges are deleted in order of increasing weight, producing a hierarchical clustering. (See e.g. <ref> [25] </ref>.) Schwanke and Platoff [29] discuss an interesting application of these bibliometric measures for clustering purposes in a completely different realm | that of analyzing the relationships among modules in a large software system.
Reference: [26] <author> E. Rivlin, R. Botafogo, B. Shneiderman, </author> <title> "Navigating in hyperspace: designing a structure-based toolbox," </title> <journal> Communications of the ACM, </journal> <volume> 37(2), </volume> <year> 1994, </year> <pages> pp. 87-96. </pages>
Reference-contexts: 1 Introduction The link structure of a hypermedia environment can be a rich source of information about the content of the environment, provided we have effective means for understanding it. Versions of this principle have been studied in the hypertext research community <ref> [3, 13, 26, 36] </ref> and (in a context predating hypermedia) through journal citation analysis in the field of bibliometrics [37]. <p> There has been some work on using hyperlinks for clustering and searching in the hypertext research community. Rivlin, Botafogo, and Schneiderman <ref> [3, 26] </ref> use basic graph-theoretic notions such as connectivity, as well as "compactness" measures based on node-to-node distances, to identify clusters in the graph of a hypertext environment.
Reference: [27] <author> R. Rousseau, G. Van Hooydonk, </author> <title> "Journal production and journal impact factors," </title> <journal> J. American Soc. Info. Sci., </journal> <volume> 47(1996), </volume> <pages> pp. 775-780. </pages>
Reference-contexts: There has also been work in bibliometrics on using citation counts to assess the "impact" of scientific journals; this is more closely related to the issue we are considering here. The classic work in this area is that of Garfield [15]; see also e.g. <ref> [16, 27] </ref>. <p> At the same time, it has been observed in bibliometric studies that review journals often constitute exceptions to certain basic principles <ref> [16, 27] </ref>; it would be interesting to see whether the distinction between hubs and authorities would be useful in this regard. There has been some work on using hyperlinks for clustering and searching in the hypertext research community.
Reference: [28] <author> G. Salton. </author> <title> Automatic Text Processing. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1989. </year>
Reference: [29] <author> R.W. Schwanke, M.A. Platoff, </author> <title> "Cross references are features," in Machine Learning: From Theory to Applications, S.J. </title> <editor> Hanson, W. Remmele, R.L. Rivest, eds., </editor> <publisher> Springer, </publisher> <year> 1993. </year>
Reference-contexts: Documents constitute the nodes of an undirected graph whose edge weights are measures of similarity; edges are deleted in order of increasing weight, producing a hierarchical clustering. (See e.g. [25].) Schwanke and Platoff <ref> [29] </ref> discuss an interesting application of these bibliometric measures for clustering purposes in a completely different realm | that of analyzing the relationships among modules in a large software system. <p> But more generally, there are a number of naturally arising directed graph structures in which one can find clear interpretations for the notions of "hubs" and "authorities," and the dense communities which they comprise. Consider, for example, the implicit analogy drawn in <ref> [29] </ref> between the relationships among modules in a large software system and the basic measures used in bibliomet-rics.
Reference: [30] <author> W.M. Shaw, </author> <title> "Subject and Citation Indexing. Part I: The clustering structure of composite representations in the cystic fibrosis document collection," </title> <journal> J. American Soc. Info. Sci., </journal> <volume> 42(1991), </volume> <pages> pp. 669-675. </pages>
Reference-contexts: We will see that these two quantities arise inside the analysis of our method. Shaw <ref> [30, 31] </ref> uses a combination of these measures, together with some textual measures, as part of a graph-based clustering algorithm.
Reference: [31] <author> W.M. Shaw, </author> <title> "Subject and Citation Indexing. Part II: The optimal, cluster-based retrieval performance of composite representations," </title> <journal> J. American Soc. Info. Sci., </journal> <volume> 42(1991), </volume> <pages> pp. 676-684. </pages>
Reference-contexts: We will see that these two quantities arise inside the analysis of our method. Shaw <ref> [30, 31] </ref> uses a combination of these measures, together with some textual measures, as part of a graph-based clustering algorithm.
Reference: [32] <author> H. </author> <title> Small, "Co-citation in the scientific literature: A new measure of the relationship between two documents," </title> <journal> J. American Soc. Info. Sci., </journal> <volume> 24(1973), </volume> <pages> pp. 265-269. </pages>
Reference-contexts: This work has focused predominantly on the use of citations and/or explicit hyperlinks as a means of clustering and enhancing relevance judgments. Two basic measures of document similarity to emerge from the study of bibliometrics are bibliographic coupling [20] and co-citation <ref> [32] </ref>. For two documents p and q, the former quantity is equal to the number of documents cited by both p and q, and the latter quantity is the number of documents that cite both p and q.
Reference: [33] <author> E. Spertus, "ParaSite: </author> <title> Mining structural information on the Web," </title> <booktitle> Proc. 6th International World Wide Web Conference, </booktitle> <year> 1997. </year>
Reference-contexts: More recently, Pirolli, Pitkow, and Rao [24] have used a combination of link topology and textual similarity to group together and categorize pages on the www. Arocena, Mendelzon, and Mihaila [1] and Spertus <ref> [33] </ref> have described frameworks for constructing www queries from a combination of term-matching and link-based predicates.
Reference: [34] <author> D. Spielman, S. Teng, </author> <title> "Spectral partitioning works: Planar graphs and finite-element meshes," </title> <booktitle> Processedings of the 37th IEEE Symposium on Foundations of Computer Science, </booktitle> <year> 1996. </year>
Reference-contexts: The heuristic intuition behind this approach is analogous to the spectral partitioning of undirected graphs (e.g. <ref> [7, 11, 34] </ref>); however, it is important to note that what we are doing here is not simply a spectral partitioning of the Web graph. <p> Another interesting feature of the communities derived from non-principal eigenvectors is the following. Drawing on the heuristic intuition underlying spectral graph partitioning <ref> [7, 11, 34] </ref>, one expects pairs of communities (X + i ; Y + i ; Y i ) associated with the same eigenvector to be very sparsely connected in the underlying graph. In some cases, this sparse linkage can have meaning in the context of the query topic.
Reference: [35] <author> World Wide Web Consortium, </author> <title> World Wide Web Virtual Library, </title> <note> http://www.w3.org/vl/. 31 </note>
Reference-contexts: Second, there is a sense in which we can compare the technique to existing human-constructed benchmarks: there are a number of searchable hierarchies on the www, such as yahoo [38], Galaxy [14], Zia [39], and the distributed www Virtual Library <ref> [35] </ref>. These hierarchies provide lists of authoritative pages, compiled by human moderators, for many standard search topics. In this way, they represent high-quality hub pages, and can be used for comparison with our automated method. <p> We mentioned examples of such searchable hierarchies in the introduction (e.g. yahoo [38], Galaxy [14], Zia [39], and the www Virtual Library <ref> [35] </ref>); they are lists of authoritative pages, compiled by humans, on a variety of broad search topics. Such hierarchies provide us both with externally generated lists of query topics, and with lists of authoritative pages on these topics against which to compare our results.
Reference: [36] <author> R. Weiss, B. Velez, M. Sheldon, C. Nemprempre, P. Szilagyi, D.K. Gifford, "HyPursuit: </author> <title> A Hierarchical Network Search Engine that Exploits Content-Link Hypertext Clustering," </title> <booktitle> Proceedings of the Seventh ACM Conference on Hypertext, </booktitle> <year> 1996. </year>
Reference-contexts: 1 Introduction The link structure of a hypermedia environment can be a rich source of information about the content of the environment, provided we have effective means for understanding it. Versions of this principle have been studied in the hypertext research community <ref> [3, 13, 26, 36] </ref> and (in a context predating hypermedia) through journal citation analysis in the field of bibliometrics [37]. <p> Rivlin, Botafogo, and Schneiderman [3, 26] use basic graph-theoretic notions such as connectivity, as well as "compactness" measures based on node-to-node distances, to identify clusters in the graph of a hypertext environment. Weiss et al. <ref> [36] </ref> define similarity measures among 8 pages in a hypertext environment based on the link structure; these measures are generalizations of co-citation and bibliographic coupling to allow for arbitrarily long chains of references.
Reference: [37] <author> H.D. White, K.W. McCain, "Bibliometrics," </author> <title> in Ann. </title> <type> Rev. </type> <institution> Info. Sci. and Technology, </institution> <address> Elsevier, </address> <year> 1989, </year> <pages> pp. 119-186. </pages>
Reference-contexts: Versions of this principle have been studied in the hypertext research community [3, 13, 26, 36] and (in a context predating hypermedia) through journal citation analysis in the field of bibliometrics <ref> [37] </ref>. But for the problem of searching in hyperlinked environments such as the World Wide Web, it is clear from the prevalent techniques that the information inherent in the links has yet to be fully exploited. <p> For example, in preliminary experiments on a local corpus of two million U.S. patents [19], these problems associated with processing the documents did not arise.) Related Work on Link Structures Methodologically, our work has connections to the area of bibliometrics <ref> [37] </ref> | the study of written documents and their citation structure. Some related work has also been done in the hypertext research community. This work has focused predominantly on the use of citations and/or explicit hyperlinks as a means of clustering and enhancing relevance judgments.
Reference: [38] <author> Yahoo! Corporation, Yahoo!, </author> <note> http://www.yahoo.com. </note>
Reference-contexts: We will be presenting a number of such examples in the following sections. Second, there is a sense in which we can compare the technique to existing human-constructed benchmarks: there are a number of searchable hierarchies on the www, such as yahoo <ref> [38] </ref>, Galaxy [14], Zia [39], and the distributed www Virtual Library [35]. These hierarchies provide lists of authoritative pages, compiled by human moderators, for many standard search topics. In this way, they represent high-quality hub pages, and can be used for comparison with our automated method. <p> We mentioned examples of such searchable hierarchies in the introduction (e.g. yahoo <ref> [38] </ref>, Galaxy [14], Zia [39], and the www Virtual Library [35]); they are lists of authoritative pages, compiled by humans, on a variety of broad search topics.
Reference: [39] <author> Zia, </author> <note> http://www.zia.com. </note>
Reference-contexts: We will be presenting a number of such examples in the following sections. Second, there is a sense in which we can compare the technique to existing human-constructed benchmarks: there are a number of searchable hierarchies on the www, such as yahoo [38], Galaxy [14], Zia <ref> [39] </ref>, and the distributed www Virtual Library [35]. These hierarchies provide lists of authoritative pages, compiled by human moderators, for many standard search topics. In this way, they represent high-quality hub pages, and can be used for comparison with our automated method. <p> We mentioned examples of such searchable hierarchies in the introduction (e.g. yahoo [38], Galaxy [14], Zia <ref> [39] </ref>, and the www Virtual Library [35]); they are lists of authoritative pages, compiled by humans, on a variety of broad search topics. Such hierarchies provide us both with externally generated lists of query topics, and with lists of authoritative pages on these topics against which to compare our results.
References-found: 39

