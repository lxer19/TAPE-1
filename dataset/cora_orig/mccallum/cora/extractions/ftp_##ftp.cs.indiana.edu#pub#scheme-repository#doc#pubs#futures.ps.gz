URL: ftp://ftp.cs.indiana.edu/pub/scheme-repository/doc/pubs/futures.ps.gz
Refering-URL: http://www.cs.indiana.edu/scheme-repository/doc.publications.html
Root-URL: http://www.cs.indiana.edu
Title: An Efficient and General Implementation of Futures on Large Scale Shared-Memory Multiprocessors  
Author: James S. Miller, advisor Marc Feeley 
Degree: A Dissertation Presented to The Faculty of the Graduate School of Arts and Sciences  In Partial Fulfillment of the Requirements of the Degree of Doctor of Philosophy by  
Date: April 1993  
Affiliation: Brandeis University Department of Computer Science  
Abstract-found: 0
Intro-found: 1
Reference: [ Adams and Rees, 1988 ] <author> N. Adams and J. Rees. </author> <title> Object-oriented programming in Scheme. </title> <booktitle> In Conference Record of the 1988 ACM Conference on Lisp and Functional Programming, </booktitle> <pages> pages 277-288, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: In Scheme, procedures are viewed as first-class values and thus have the same basic properties as the other data types. With first-class procedures many programming techniques are easily implemented. Higher order functions, lazy evaluation, streams and object-oriented programming can all be done using first-class procedures (for example see <ref> [ Adams and Rees, 1988, Friedman et al., 1992 ] </ref> ). Procedures created by lambda-expressions are usually called closures to distinguish them from predefined procedures. The static scoping rules require all closures to carry, at least conceptually, the set of variables to which they might refer (the closed variables).
Reference: [ Agarwal, 1991 ] <author> A. Agarwal. </author> <title> Performance tradeoffs in multithreaded processors. </title> <type> Technical Report MIT/LCS/TR-501, </type> <institution> Massachusetts Institute of Technology, </institution> <address> Cambridge, MA, </address> <month> April </month> <year> 1991. </year>
Reference: [ Appel, 1989 ] <author> A. W. Appel. </author> <title> Allocation without locking. </title> <journal> Software Practice and Experience, </journal> <volume> 19(7) </volume> <pages> 703-705, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: The method used by Gambit is to detect interrupts via polling and never check for interrupts inside the popping sequence (efficient polling is explained in Chapter 4). There are other methods that have no direct overhead. For example, in the instruction interpretation method <ref> [ Appel, 1989 ] </ref> the hardware interrupt handler checks to see if the interrupted instruction is in an "uninterruptible" section (i.e. a popping sequence). If it is, the rest of the section is interpreted by the interrupt handler before the interrupt is serviced.
Reference: [ Arvind and Nikhil, 1990 ] <author> Arvind and R. S. Nikhil. </author> <title> Executing a program on the MIT tagged-token dataflow architecture. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 39(3) </volume> <pages> 300-318, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: A dynamic partitioning strategy must find some compromise between the benefit of added concurrency and the drawback of added overhead. Some have avoided this problem to some extent by relying on specialized hardware to reduce the cost of managing tasks. Dataflow machines <ref> [ Srini, 1986, Arvind and Nikhil, 1990 ] </ref> and multithreaded architectures [ Halstead and Fujita, 1988, Nikhil et al., 1991, Agarwal, 1991 ] fall in this category. However, software methods are attractive because they offer portability and low hardware cost.
Reference: [ Baker and Hewitt, 1978 ] <author> H. Baker and C. Hewitt. </author> <title> The incremental garbage collection of processes. </title> <type> Technical Report AI Memo 454, </type> <institution> Mass. Inst. of Technology, Artificial Intelligence Laboratory, </institution> <month> March </month> <year> 1978. </year>
Reference-contexts: It was designed by Halstead [ Halstead, 1984 ] as an extension of Scheme with a few additional constructs to deal with parallelism. The most important of these is the future special form whose origin can be traced back to <ref> [ Baker and Hewitt, 1978 ] </ref> . From its inception, the purpose of Multilisp has been to provide a testbed for experimentation in the design and implementation of parallel symbolic processing systems. Through the years it has evolved along several distinct paths to accommodate novel uses of the language.
Reference: [ BBN, 1989 ] <institution> BBN Advanced Computers Inc., </institution> <address> Cambridge, MA. Inside the GP1000, </address> <year> 1989. </year>
Reference-contexts: There is a single local memory per node that is partitioned into shared and private sections by system calls to the operating system. Other system calls allow the selection of the caching policy for each memory block allocated. The GP1000 computer <ref> [ BBN, 1989 ] </ref> , also by BBN, has a very similar architecture but uses older technology (the TC2000 uses M88000 processors rated at 20 MIPS whereas the GP1000 uses M68020 processors rated at roughly 3 MIPS).
Reference: [ BBN, 1990 ] <institution> BBN Advanced Computers Inc., </institution> <address> Cambridge, MA. </address> <booktitle> Inside the TC2000 Computer, </booktitle> <year> 1990. </year>
Reference-contexts: Each node in the architecture corresponds roughly to a modern uniprocessor computer. The only extra hardware needed to build a complete machine is that for the interconnect and its interface to the processing nodes. The TC2000 computer <ref> [ BBN, 1990 ] </ref> , manufactured by BBN Computers and introduced in 1989, matches this structure very closely. A scalable multistage butterfly network is used for the interconnection network. <p> The timings correspond to the latency for referencing a single word for each level of the hierarchy 4 . Note that the 3 However, each processor has a small instruction cache. 4 These costs were measured with benchmarks specially designed to test the memory. As reported in <ref> [ BBN, 1990 ] </ref> , the timing depends on many parameters such as the caching policy in use, the type of access (read or write), the size of machine and the contention on the interconnection network.
Reference: [ Bilardi and Nicolau, 1989 ] <author> G. Bilardi and A. Nicolau. </author> <title> Adaptive bitonic sorting: An optimal parallel algorithm for shared-memory machines. </title> <journal> SIAM Journal of Computing, </journal> <volume> 12(2) </volume> <pages> 216-228, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: Appendix B contains execution profiles for the benchmarks. These indicate the activity of the processors as a function of time, thus allowing a better visualization of the program's behavior. 2.9.1 abisort This program sorts n = 16384 integers using the adaptive bitonic sort algorithm <ref> [ Bilardi and Nicolau, 1989 ] </ref> . This algorithm is optimal in the sense that, on the PRAM-EREW 13 theoretical model, it runs in O ( n log n p ) time, where p is the number of processors and 1 p n 2 blog log nc . <p> A brief description is included with each program. 172 APPENDIX A. SOURCE CODE FOR PARALLEL BENCHMARKS A.1 abisort This program sorts 16384 integers using the adaptive bitonic sort algorithm described in <ref> [ Bilardi and Nicolau, 1989 ] </ref> . (##define-macro (make-node) `(make-vector 3 #f)) (##define-macro (node-left x) `(vector-ref ,x 0)) (##define-macro (node-value x) `(vector-ref ,x 1)) (##define-macro (node-right x) `(vector-ref ,x 2)) (##define-macro (node-left-set! x v) `(vector-set! ,x 0 ,v)) (##define-macro (node-value-set! x v) `(vector-set! ,x 1 ,v)) (##define-macro (node-right-set! x v) `(vector-set!
Reference: [ Callahan and Smith, 1989 ] <author> D. Callahan and B. Smith. </author> <title> A future-based parallel language for a general-purpose highly-parallel computer. </title> <booktitle> In Papers from the Second Workshop 207 208 BIBLIOGRAPHY on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 95-113. </pages> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1989. </year>
Reference-contexts: The future construct is actually quite general and it has been used in more conventional languages such as C <ref> [ Callahan and Smith, 1989 ] </ref> . 1.3 Fundamental Issues Assuming that speed of computation is the main objective, the job of a Multilisp implementor can be seen as an optimization problem constrained by three factors 1. The semantics of the language. 2. The characteristics of the target machine. 3.
Reference: [ Censier and Feautrier, 1978 ] <author> L. M. Censier and P. Feautrier. </author> <title> A new solution to coherence problems in multicache systems. </title> <journal> IEEE Transactions on Computers, </journal> <pages> pages 1112-1118, </pages> <month> December </month> <year> 1978. </year>
Reference-contexts: ARCHITECTURE 11 of the bus. For example the bus in the Encore Multimax can support up to 20 fairly low-power processors 2 . Maintaining consistency on scalable architectures is much harder. Currently, most scalable cache designs are based on directories <ref> [ Censier and Feautrier, 1978 ] </ref> . With each datum is kept a list of the caches that are holding a copy of the datum and that must be notified of any mutation.
Reference: [ Chaiken et al., 1991 ] <author> D. Chaiken, J. Kubiatowicz, and A. Agarwal. </author> <title> LimitLESS directories: A scalable cache coherence scheme. </title> <booktitle> In ASPLOS IV: Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 224-234, </pages> <year> 1991. </year>
Reference-contexts: Fortunately, it seems that in typical applications most of the shared data is shared by a very small number of processors [ Lenoski et al., 1992, O'Krafka and Newton, 1990 ] . Limited directory caching methods, such as <ref> [ Chaiken et al., 1991 ] </ref> , take advantage of this fact to reduce the space for the directory by only allowing a small number of copies of a datum to exist at any given point in time.
Reference: [ Clinger et al., 1988 ] <author> W. Clinger, A. Hartheimer, and E. </author> <title> Ost. Implementation strategies for continuations. </title> <booktitle> In Conference Record of the 1988 ACM Conference on Lisp and Functional Programming, </booktitle> <pages> pages 124-131, </pages> <address> Snowbird, UT., </address> <month> July </month> <year> 1988. </year>
Reference-contexts: The oldest frame's parent is the root continuation which is special in that it has no parent. The root continuation symbolizes the end of the program. Several strategies for implementing continuations have been described and compared by <ref> [ Clinger et al., 1988 ] </ref> . Their results suggest that the incremental stack /heap strategy is more efficient than the other strategies in most cases and not noticeably slower than the other strategies in extreme cases.
Reference: [ Clinger, 1984 ] <author> W. Clinger. </author> <title> The Scheme 311 compiler: an exercise in denotational semantics. </title> <booktitle> In Conference Record of the 1984 ACM Symposium on Lisp and Functional Programming, </booktitle> <pages> pages 356-364, </pages> <year> 1984. </year>
Reference-contexts: FIRST-CLASS CONTINUATIONS 25 2.2 First-Class Continuations Perhaps Scheme's most unusual features is the availability of first-class continuation objects. Continuations have been used in the past to express the denotational semantics of programming languages such as Algol60 and Scheme itself <ref> [ R3RS, 1986, Clinger, 1984 ] </ref> . Most programming languages use continuations but they are usually hidden whereas in Scheme they can be manipulated explicitly. First-class continuations are useful to implement advanced control structures that would be hard to express otherwise.
Reference: [ Dijkstra, 1968 ] <author> E. W. Dijkstra. </author> <title> Cooperating sequential processes. </title> <booktitle> In Programming Languages, </booktitle> <pages> pages 43-112. </pages> <publisher> Academic Press, </publisher> <year> 1968. </year>
Reference-contexts: if still there */ secondary_ret_point: SP--; /* pop ret adr from continuation frame */ . . 3.5.1 Avoiding Hardware Locks Hardware locks can be avoided in the task popping operation by implementing the popping locks with any of several "software lock" algorithms based on shared variables (such as Dekker's algorithm <ref> [ Dijkstra, 1968 ] </ref> and Peterson's algorithm [ Peterson, 1981 ] ). The same basic principles used by these algorithms can be adapted to design a special purpose synchronization mechanism for LTC as described next.
Reference: [ Dubois and Scheurich, 1990 ] <author> M. Dubois and C. Scheurich. </author> <title> Memory access dependencies in shared-memory multiprocessors. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 16(6) </volume> <pages> 660-673, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Machines implementing processor consistency usually have a "write barrier" instruction which waits until the memory has processed all of that processor's writes. The weak consistency and release consistency models <ref> [ Dubois and Scheurich, 1990 ] </ref> are still weaker and more efficient. They guarantee consistency only at synchronization points in the program. In other words, lock and unlock operations (or similar synchronization operations) are barriers which wait until the memory has processed all pending transactions.
Reference: [ Feeley and Miller, 1990 ] <author> M. Feeley and J. S. Miller. </author> <title> A parallel virtual machine for efficient Scheme compilation. </title> <booktitle> In Proceedings of the 1990 ACM Conference on Lisp and Functional Programming, </booktitle> <address> Nice, France, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: WHY MULTILISP? 3 Absolute performance is a major concern in this thesis. For this reason, the Multilisp implementation techniques proposed here are evaluated in the context of a "production quality" implementation. To perform experiments, a highly efficient Scheme compiler called Gambit <ref> [ Feeley and Miller, 1990 ] </ref> is used as a platform into which the implementation techniques are integrated and tested. This is to ensure that the setting is realistic and that performance-critical issues are not overlooked. <p> This includes both academic research systems such as QLisp [ Gabriel and McCarthy, 1984, Goldman and Gabriel, 1988 ] , MultiScheme [ Miller, 1987, Miller, 1988 ] , Mul-T [ Kranz et 4 CHAPTER 1. INTRODUCTION al., 1989 ] , Gambit <ref> [ Feeley and Miller, 1990 ] </ref> , PaiLisp [ Ito and Matsui, 1990 ] , Spur Lisp [ Zorn et al., 1988 ] , Butterfly portable standard lisp [ Swanson et al., 1988 ] and Concurrent Scheme [ Kessler and Swanson, 1990, Kessler et al., 1992 ] as well as commercially <p> As a first step of this work, a highly optimizing compiler for Scheme was developed to provide a realistic setting for exploring new implementation strategies for Multilisp and evaluating their performance. This effort resulted in Gambit <ref> [ Feeley and Miller, 1990 ] </ref> , currently the best Scheme compiler in terms of performance of the code generated. The system was ported to the GP1000 and TC2000 multiprocessors, and support for Multilisp's parallelism constructs added to the compiler.
Reference: [ Feeley, 1993 ] <author> M. Feeley. </author> <title> Polling efficiently on stock hardware. </title> <booktitle> In Proceedings of the 1993 ACM Conference on Functional Programming Languages and Computer Architecture, </booktitle> <year> 1993. </year>
Reference-contexts: If it is, the rest of the section is interpreted by the interrupt handler before the interrupt is serviced. Other zero cost techniques are described in <ref> [ Feeley, 1993 ] </ref> . Thirdly, the operation swap_child_ret_adr_with_underflow (p) can be implemented according to its original specification (i.e. an actual mutation of the child's return address), thus avoiding the push of the body's return address to the stack and the explicit check for underflow at the future's return point.
Reference: [ Fra, 1990 ] <author> Franz Inc., </author> <title> Berkeley, CA. Allegro CL User Manual, </title> <year> 1990. </year>
Reference-contexts: [ Zorn et al., 1988 ] , Butterfly portable standard lisp [ Swanson et al., 1988 ] and Concurrent Scheme [ Kessler and Swanson, 1990, Kessler et al., 1992 ] as well as commercially available systems such as BBN Lisp [ Steinberg et al., 1986 ] , Allegro Common Lisp <ref> [ Fra, 1990 ] </ref> , and Top Level Common Lisp [ Murray, 1990 ] .
Reference: [ Friedman and Haynes, 1985 ] <author> D. P. Friedman and C. T. Haynes. </author> <title> Constraining control. </title> <booktitle> In Proceedings of the Twelfth Annual Symposium on Principles of Programming Languages, </booktitle> <pages> pages 245-254, </pages> <address> New Orleans, LA., </address> <month> January </month> <year> 1985. </year> <note> ACM. </note>
Reference-contexts: Because the programmer's intuition often fails when dealing directly with continuations it is sometimes helpful to build abstraction barriers that offer restricted versions of call/cc (for example see <ref> [ Friedman and Haynes, 1985 ] </ref> ). First-class continuations also cause an implementation problem. If procedures have dynamic extent, continuations can easily be represented by a single stack of control frames (i.e. return addresses).
Reference: [ Friedman et al., 1992 ] <author> D. P. Friedman, M. Wand, and C. T. Haynes. </author> <title> Essentials of Programming Languages. </title> <publisher> MIT Press and McGraw-Hill, </publisher> <year> 1992. </year> <note> BIBLIOGRAPHY 209 </note>
Reference-contexts: In Scheme, procedures are viewed as first-class values and thus have the same basic properties as the other data types. With first-class procedures many programming techniques are easily implemented. Higher order functions, lazy evaluation, streams and object-oriented programming can all be done using first-class procedures (for example see <ref> [ Adams and Rees, 1988, Friedman et al., 1992 ] </ref> ). Procedures created by lambda-expressions are usually called closures to distinguish them from predefined procedures. The static scoping rules require all closures to carry, at least conceptually, the set of variables to which they might refer (the closed variables).
Reference: [ Gabriel and McCarthy, 1984 ] <author> R. P. Gabriel and J. McCarthy. </author> <booktitle> Queue-based multiprocessing Lisp. In Conference Record of the 1984 ACM Symposium on Lisp and Functional Programming, </booktitle> <pages> pages 25-44, </pages> <address> Austin, TX., </address> <month> August </month> <year> 1984. </year>
Reference-contexts: Multilisp's model of parallel computation has become increasingly popular and some of its features have now been adopted by other parallel Lisp systems. This includes both academic research systems such as QLisp <ref> [ Gabriel and McCarthy, 1984, Goldman and Gabriel, 1988 ] </ref> , MultiScheme [ Miller, 1987, Miller, 1988 ] , Mul-T [ Kranz et 4 CHAPTER 1.
Reference: [ Gabriel, 1985 ] <author> R. P. Gabriel. </author> <title> Performance and Evaluation of Lisp Systems. </title> <booktitle> Research Reports and Notes, Computer Systems Series. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1985. </year>
Reference-contexts: The main advantage of small programs is that they usually stress a well defined part of the system, so the measurement can be interpreted more readily. Both sequential and parallel benchmarks were used. The sequential benchmarks are mostly taken from the Gabriel suite <ref> [ Gabriel, 1985 ] </ref> which has traditionally been used 60 CHAPTER 2. BACKGROUND to evaluate implementations of Lisp. To these benchmarks were added four sequential benchmarks: compiler (the Gambit compiler), conform (a type checker), earley (a parser) and peval (a partial evaluator).
Reference: [ Gharachorloo et al., 1991 ] <author> K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> Performance evaluation of memory consistency models for shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 245-257. </pages> <publisher> ACM, </publisher> <month> April </month> <year> 1991. </year>
Reference-contexts: The first is to put the responsibility of consistency on the programmer or compiler by providing a less rigid consistency model. At appropriate points in the program special operations must be added to flush or invalidate some of the entries in the caches. In the terminology of <ref> [ Gharachorloo et al., 1991 ] </ref> , the strictest consistency model is sequential consistency . In this model, memory behaves as though only one access is serviced at a time (i.e. accesses are sequential). Thus any read request returns the last value written.
Reference: [ Goldman and Gabriel, 1988 ] <author> R. Goldman and R. P. Gabriel. </author> <title> Preliminary results with the initial implementation of Qlisp. </title> <booktitle> In Conference Record of the 1988 ACM Conference on Lisp and Functional Programming, </booktitle> <pages> pages 143-152, </pages> <address> Snowbird, UT., </address> <month> July </month> <year> 1988. </year>
Reference-contexts: Multilisp's model of parallel computation has become increasingly popular and some of its features have now been adopted by other parallel Lisp systems. This includes both academic research systems such as QLisp <ref> [ Gabriel and McCarthy, 1984, Goldman and Gabriel, 1988 ] </ref> , MultiScheme [ Miller, 1987, Miller, 1988 ] , Mul-T [ Kranz et 4 CHAPTER 1. <p> Other compiler based systems require even more instructions. Portable Standard Lisp on the GP1000 [ Swanson et al., 1988 ] takes 480 secs (about 1440 instructions given that each processor gives out 3 MIPS) and QLisp on an Alliant FX/8 <ref> [ Goldman and Gabriel, 1988 ] </ref> takes 1400 instructions. With this lower bound on T future min it is possible to get a lower bound on O expose from the value of G.
Reference: [ Goodman, 1983 ] <author> J. R. Goodman. </author> <title> Using cache memory to reduce processor-memory traffic. </title> <booktitle> Proceedings of the 10th International Symposium on Computer Architecture, </booktitle> <pages> pages 124-131, </pages> <month> June </month> <year> 1983. </year>
Reference-contexts: This is relatively easy to perform on bus-based architectures because all caches and memory are immediately aware of all transactions (they are directly connected to the shared bus). So called snoopy-caches <ref> [ Goodman, 1983 ] </ref> are based on this principle. Unfortunately, bus-based architectures do not scale well because the bus has a limited bandwidth. Typically, bus-based machines are designed with just enough processors to match the bandwidth 1.4. ARCHITECTURE 11 of the bus.
Reference: [ Gray, 1986 ] <author> S. L. Gray. </author> <title> Using futures to exploit parallelism in Lisp. </title> <type> Master's thesis, </type> <institution> Mass. Inst. of Technology, </institution> <year> 1986. </year>
Reference-contexts: It is also appropriate when Multilisp is considered as the "object code" of a compiler for a higher level parallel language. Such a compiler could be aware of where parallelism is both possible and desirable and emit code with appropriately placed futures ( <ref> [ Gray, 1986 ] </ref> is a good example of this application). 2.3.1 FUTURE and TOUCH Futures are expressed as (FUTURE expr) where expr is called the future's body . The future construct behaves like the identity function in the sense that its value is the value of its body.
Reference: [ Halstead and Fujita, 1988 ] <author> R. Halstead and T. Fujita. MASA: </author> <title> A multithreaded processor architecture for parallel symbolic computing. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 443-451, </pages> <year> 1988. </year>
Reference: [ Halstead et al., 1986 ] <author> R. Halstead, T. Anderson, R. Osborne, and T. Sterling. </author> <title> Concert: Design of a multiprocessor development system. </title> <booktitle> In Int'l. Symp. on Computer Architecture, </booktitle> <volume> volume 13, </volume> <pages> pages 40-48, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: Through the years it has evolved along several distinct paths to accommodate novel uses of the language. The first implementation of Multilisp was "Concert Multi-lisp" which ran on a custom designed multiprocessor <ref> [ Halstead, 1987, Halstead et al., 1986 ] </ref> . Multilisp's model of parallel computation has become increasingly popular and some of its features have now been adopted by other parallel Lisp systems.
Reference: [ Halstead, 1984 ] <author> R. Halstead. </author> <title> Implementation of Multilisp: Lisp on a multiprocessor. </title> <booktitle> In Conference Record of the 1984 ACM Symposium on Lisp and Functional Programming, </booktitle> <pages> pages 9-17, </pages> <address> Austin, TX., </address> <month> August </month> <year> 1984. </year>
Reference-contexts: The growing need for high-performance parallel symbolic processing systems is the initial motivation for this work. Multilisp suggests itself naturally since it is a member of the Lisp family of symbolic processing languages. It was designed by Halstead <ref> [ Halstead, 1984 ] </ref> as an extension of Scheme with a few additional constructs to deal with parallelism. The most important of these is the future special form whose origin can be traced back to [ Baker and Hewitt, 1978 ] . <p> However, programs with a less restrictive task termination order exhibit a measurable but small overhead (no more than 10% on 16 processors). Chapter 6 Conclusion The initial goal of this work was the implementation of a high-performance Multilisp system. Earlier implementations of Multilisp, such as Concert Multilisp <ref> [ Halstead, 1984 ] </ref> and MultiScheme [ Miller, 1987 ] , gave interesting self relative speedups but because they were based on interpreters it was not clear that the same speedups would apply to a "production quality" system.
Reference: [ Halstead, 1985 ] <author> R. Halstead. </author> <title> Multilisp: A language for concurrent symbolic computation. </title> <journal> In ACM Trans. on Prog. Languages and Systems, </journal> <pages> pages 501-538, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: BACKGROUND The original Multilisp semantics <ref> [ Halstead, 1985 ] </ref> had a scheduling policy that was fair as long as all tasks were of finite duration. The only guarantee made by the scheduler was that a runnable task would run if there were no other runnable tasks. <p> In other words, what should be done with the value returned by the body? This is an important question because the approach chosen will specify the behavior of first-class continuations in the presence of futures. 2.8.1 Original Semantics Several approaches have been proposed. In the original Multilisp definition <ref> [ Halstead, 1985 ] </ref> the body's value was used to determine the placeholder created for the future and the task was simply terminated. This is the semantics implemented by the code in 2.8.2 MultiScheme Semantics MultiScheme adopted a subtly different model for continuations.
Reference: [ Halstead, 1987 ] <author> R. Halstead. </author> <title> Overview of concert Multilisp: A multiprocessor symbolic computing system. </title> <journal> ACM Computer Architecture News, </journal> <volume> 15(1) </volume> <pages> 5-14, </pages> <month> March </month> <year> 1987. </year> <note> 210 BIBLIOGRAPHY </note>
Reference-contexts: Through the years it has evolved along several distinct paths to accommodate novel uses of the language. The first implementation of Multilisp was "Concert Multi-lisp" which ran on a custom designed multiprocessor <ref> [ Halstead, 1987, Halstead et al., 1986 ] </ref> . Multilisp's model of parallel computation has become increasingly popular and some of its features have now been adopted by other parallel Lisp systems.
Reference: [ Haynes et al., 1984 ] <author> C. T. Haynes, D. P. Friedman, and M. Wand. </author> <title> Continuations and coroutines. </title> <booktitle> In Conference Record of the 1984 ACM Symposium on Lisp and Functional Programming, </booktitle> <pages> pages 293-298, </pages> <address> Austin, TX., </address> <year> 1984. </year>
Reference-contexts: Thus it is possible to directly transfer control between two different branches of the call tree. This characteristic can be exploited to implement specialized control structures such as backtracking [ Haynes, 1986 ] , coroutines <ref> [ Haynes et al., 1984 ] </ref> and multitasking [ Wand, 1980 ] . A less frequent, but possible, use of continuations is to reenter a computation that has already completed (see [ Rozas, 1987 ] for an application).
Reference: [ Haynes, 1986 ] <author> Christopher T. Haynes. </author> <title> Logic continuations. </title> <booktitle> In Proceedings of the Third International Conference on Logic Programming, </booktitle> <pages> pages 671-685. </pages> <publisher> Springer-Verlag, </publisher> <month> July </month> <year> 1986. </year>
Reference-contexts: Thus it is possible to directly transfer control between two different branches of the call tree. This characteristic can be exploited to implement specialized control structures such as backtracking <ref> [ Haynes, 1986 ] </ref> , coroutines [ Haynes et al., 1984 ] and multitasking [ Wand, 1980 ] . A less frequent, but possible, use of continuations is to reenter a computation that has already completed (see [ Rozas, 1987 ] for an application).
Reference: [ Hieb et al., 1990 ] <author> Robert Hieb, R. Kent Dybvig, and Carl Bruggeman. </author> <title> Representing control in the presence of first-class continuations. </title> <booktitle> In ACM SIGPLAN '89 Conf. on Programming Language Design and Implementation, </booktitle> <pages> pages 66-77, </pages> <address> White Plains, New York, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: One way to achieve this is to associate the description of a frame's layout (length and return address location) with the return address of the subproblem call that created the frame. The frame descriptor can for example be stored just before the return point, as is done in <ref> [ Hieb et al., 1990 ] </ref> . RET can then be used to get the size of the topmost stack frame and the location of its return address. The return address in this frame in turn gives the size of the next frame and so on.
Reference: [ Hockney and Jesshope, 1988 ] <author> R. W. Hockney and C. R. Jesshope. </author> <title> Parallel Computers 2. </title> <institution> Adam Hilger, Bristol and Philadelphia, </institution> <year> 1988. </year>
Reference-contexts: The computation proceeds in two sequential phases: the reduction of the system by the method of cyclic reduction <ref> [ Hockney and Jesshope, 1988 ] </ref> followed by backsubstitution. Cyclic reduction takes a tridiagonal system of order n = 2 k 1 (i.e. n equations over the variables x 0 to x n1 ) and produces a reduced tridiagonal system of order (n + 1)=2 1.
Reference: [ IEEE Std 1178-1990, 1991 ] <editor> IEEE Std 1178-1990. </editor> <title> IEEE Standard for the Scheme Programming Language. </title> <publisher> Institute of Electrical and Electronic Engineers, Inc., </publisher> <address> New York, NY, </address> <year> 1991. </year>
Reference-contexts: The ETC implementation of this semantics is then presented. The chapter ends with a description of some Multilisp programs later used to evaluate and compare various implementation strategies. 2.1 Scheme's Legacy Multilisp inherits its sequential programming features from the Scheme dialect of Lisp <ref> [ IEEE Std 1178-1990, 1991 ] </ref> . Scheme was designed to be a relatively small and simple 21 22 CHAPTER 2. BACKGROUND language with exceptional expressive power. <p> There are six basic types of expressions in Scheme: constant, variable reference, assignment, conditional, procedure abstraction (lambda-expression) and procedure call. All the other types of expressions can be derived from the basic types and this is in fact how they are defined in the standard <ref> [ IEEE Std 1178-1990, 1991, R4RS, 1991 ] </ref> . Being able to reduce a program to the basic expressions is helpful both as an implementation technique and as a means to understand programs and prove some of their properties.
Reference: [ Ito and Matsui, 1990 ] <author> T. Ito and M. Matsui. </author> <title> A parallel Lisp language PaiLisp and its kernel specification. </title> <booktitle> In Parallel Lisp: Languages and Systems, </booktitle> <pages> pages 58-100. </pages> <publisher> Springer-Verlag, </publisher> <year> 1990. </year>
Reference-contexts: This includes both academic research systems such as QLisp [ Gabriel and McCarthy, 1984, Goldman and Gabriel, 1988 ] , MultiScheme [ Miller, 1987, Miller, 1988 ] , Mul-T [ Kranz et 4 CHAPTER 1. INTRODUCTION al., 1989 ] , Gambit [ Feeley and Miller, 1990 ] , PaiLisp <ref> [ Ito and Matsui, 1990 ] </ref> , Spur Lisp [ Zorn et al., 1988 ] , Butterfly portable standard lisp [ Swanson et al., 1988 ] and Concurrent Scheme [ Kessler and Swanson, 1990, Kessler et al., 1992 ] as well as commercially available systems such as BBN Lisp [ Steinberg
Reference: [ Katz and Weise, 1990 ] <author> M. Katz and D. Weise. </author> <title> Continuing into the future: on the interaction of futures and first-class continuations. </title> <booktitle> In Proceedings of the 1990 ACM Conference on Lisp and Functional Programming, </booktitle> <address> Nice, France, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: This is clearly an error because a placeholder cannot represent more than one value and deadlock would occur (since all tasks would have terminated). An interesting implementation of futures that solves this problem was proposed by Katz and Weise <ref> [ Katz and Weise, 1990 ] </ref> . The idea is to preserve the link between the future body's continuation and the future's continuation. On the first return to the body's continuation, the placeholder gets determined and the task is terminated (as in the original semantics). <p> One of these tasks has the right result (i.e. the same result as a sequential version of the program) but which task? Choosing the first task to arrive at the program's root continuation is not a valid technique because of the race condition involved. The solution proposed in <ref> [ Katz and Weise, 1990 ] </ref> introduces the concept of legitimacy . A particular sequence of evaluation steps (a thread ) is legitimate if and only if it is executed by the sequential version of the program.
Reference: [ Kessler and Swanson, 1990 ] <author> R. Kessler and M. Swanson. </author> <title> Concurrent Scheme. </title> <booktitle> In Parallel Lisp: Languages and Systems, </booktitle> <pages> pages 200-234. </pages> <publisher> Springer-Verlag, </publisher> <year> 1990. </year>
Reference-contexts: INTRODUCTION al., 1989 ] , Gambit [ Feeley and Miller, 1990 ] , PaiLisp [ Ito and Matsui, 1990 ] , Spur Lisp [ Zorn et al., 1988 ] , Butterfly portable standard lisp [ Swanson et al., 1988 ] and Concurrent Scheme <ref> [ Kessler and Swanson, 1990, Kessler et al., 1992 ] </ref> as well as commercially available systems such as BBN Lisp [ Steinberg et al., 1986 ] , Allegro Common Lisp [ Fra, 1990 ] , and Top Level Common Lisp [ Murray, 1990 ] .
Reference: [ Kessler et al., 1992 ] <author> R. Kessler, H. Carr, L. Stroller, and M. Swanson. </author> <title> Implementing concurrent Scheme for the Mayfly distributed parallel processing system. Lisp and Symbolic Computation: </title> <journal> An International Journal, </journal> 5(1/2):73-93, 1992. 
Reference-contexts: INTRODUCTION al., 1989 ] , Gambit [ Feeley and Miller, 1990 ] , PaiLisp [ Ito and Matsui, 1990 ] , Spur Lisp [ Zorn et al., 1988 ] , Butterfly portable standard lisp [ Swanson et al., 1988 ] and Concurrent Scheme <ref> [ Kessler and Swanson, 1990, Kessler et al., 1992 ] </ref> as well as commercially available systems such as BBN Lisp [ Steinberg et al., 1986 ] , Allegro Common Lisp [ Fra, 1990 ] , and Top Level Common Lisp [ Murray, 1990 ] .
Reference: [ Kranz et al., 1989 ] <author> D. Kranz, R. Halstead, and E. Mohr. Mul-T: </author> <title> A high-performance parallel Lisp. </title> <booktitle> In ACM SIGPLAN '89 Conf. on Programming Language Design and Implementation, </booktitle> <pages> pages 81-90, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: The performance of previous implementations of ETC seem to confirm this lower bound. The Mul-T system was carefully designed to minimize the cost of ETC <ref> [ Kranz et al., 1989 ] </ref> . When run on an Encore Multimax, Mul-T requires roughly 130 machine instructions to implement the sequence (the actual cost depends on the number of closed variables, their location, etc.). Other compiler based systems require even more instructions. <p> Work on the lazy task creation (LTC) mechanism was triggered by a comment on "lazy futures" in <ref> [ Kranz et al., 1989 ] </ref> . LTC postpones the creation of a task until it needs to be transferred to another processor, the "thief". Consequently, the overhead of task creation is mostly dependent on the work distribution needs of the program and not so much the program's granularity.
Reference: [ LeBlanc and Markatos, 1992 ] <author> T. J. LeBlanc and E. P. Markatos. </author> <title> Shared memory vs. message passing in shared-memory multiprocessors. </title> <type> Technical report, </type> <institution> University of Rochester, </institution> <month> April </month> <year> 1992. </year> <note> BIBLIOGRAPHY 211 </note>
Reference-contexts: Secondly, the existence of a shared memory does not imply that the programs make an important use of it. Message-passing paradigms can easily and efficiently be implemented on top of a shared memory (for example, see <ref> [ LeBlanc and Markatos, 1992 ] </ref> ). However, implementing shared memory on conventional message-passing machines is impractical because shared-memory operations are usually fine grained whereas message-passing operations are typically optimized to manipulate large chunks of data.
Reference: [ Lenoski et al., 1992 ] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W.-D. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. S. Lam. </author> <title> The Stanford Dash multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Fortunately, it seems that in typical applications most of the shared data is shared by a very small number of processors <ref> [ Lenoski et al., 1992, O'Krafka and Newton, 1990 ] </ref> .
Reference: [ Miller, 1987 ] <author> J. S. Miller. MultiScheme: </author> <title> A Parallel Processing System Based on MIT Scheme. </title> <type> PhD thesis, </type> <institution> Mass. Inst. of Technology, </institution> <month> August </month> <year> 1987. </year> <note> Available as MIT LCS/TR/402. </note>
Reference-contexts: Multilisp's model of parallel computation has become increasingly popular and some of its features have now been adopted by other parallel Lisp systems. This includes both academic research systems such as QLisp [ Gabriel and McCarthy, 1984, Goldman and Gabriel, 1988 ] , MultiScheme <ref> [ Miller, 1987, Miller, 1988 ] </ref> , Mul-T [ Kranz et 4 CHAPTER 1. <p> BACKGROUND 2.3.2 Placeholders A more traditional description of futures consists of introducing a new type of object, the placeholder , that is used to synchronize the computation of a future's body with the touching of its value <ref> [ Miller, 1987 ] </ref> . When a future is evaluated it returns a placeholder as a representative of the value of the body. A placeholder can be in one of two states. It is undetermined initially and for as long as the evaluation of the future's body has not completed. <p> It will serve both as a reference implementation and as a basis on which lazy task creation is built. A few implementation details have been omitted for the sake of clarity. A more elaborate description can be found in <ref> [ Miller, 1987 ] </ref> . As might be expected, the implementation of a Multilisp system is in many ways similar to that of a multitasking operating system. At the heart of both are utilities to support the management of various processing resources. <p> cache to hold the value of recently accessed variables (for example see [ Rozas and Miller, 1991 ] ). 10 Multilisp was not designed to support first-class continuations so it isn't surprising that the original semantics does not interact well with them. 11 The term "motivated task" was used in <ref> [ Miller, 1987 ] </ref> . 50 CHAPTER 2. <p> Chapter 6 Conclusion The initial goal of this work was the implementation of a high-performance Multilisp system. Earlier implementations of Multilisp, such as Concert Multilisp [ Halstead, 1984 ] and MultiScheme <ref> [ Miller, 1987 ] </ref> , gave interesting self relative speedups but because they were based on interpreters it was not clear that the same speedups would apply to a "production quality" system.
Reference: [ Miller, 1988 ] <author> J. S. Miller. </author> <title> Implementing a Scheme-based parallel processing system. </title> <journal> International Journal of Parallel Processing, </journal> <volume> 17(5), </volume> <month> October </month> <year> 1988. </year>
Reference-contexts: Multilisp's model of parallel computation has become increasingly popular and some of its features have now been adopted by other parallel Lisp systems. This includes both academic research systems such as QLisp [ Gabriel and McCarthy, 1984, Goldman and Gabriel, 1988 ] , MultiScheme <ref> [ Miller, 1987, Miller, 1988 ] </ref> , Mul-T [ Kranz et 4 CHAPTER 1.
Reference: [ Mohr, 1991 ] <author> E. Mohr. </author> <title> Dynamic Partitioning of Parallel Lisp Programs. </title> <type> PhD thesis, </type> <institution> Yale University Department of Computer Science, </institution> <month> October </month> <year> 1991. </year>
Reference-contexts: It is only in this case that a high cost is paid to create a heavyweight task and transfer it between processors. Shared-Memory Protocol But how exactly does this interaction take place? The protocol adopted in <ref> [ Mohr, 1991 ] </ref> uses a shared-memory paradigm. The stack and LTQ of all processors are directly accessible to all processors (i.e. they are shared data). When processor A needs to get work from processor B, it directly manipulates B's LTQ and stack to extract a task. <p> Note that for some measurements it was not possible to run compiler due to lack of memory. There are twelve parallel benchmarks. Half of these were originally written in Mul-T by Eric Mohr as part of his PhD thesis work <ref> [ Mohr, 1991 ] </ref> . To these were added a few classical parallel programs (matrix multiplication, parallel prefix and parallel reduction) and programs based on pipeline parallelism (polynomial multiplication and quicksort). A general description of the parallel benchmarks is given next. <p> As the granularity decreases, the overhead increases and almost reaches a factor of 5 for fine grain programs. This overhead is a conservative estimate. Mul-T's implementation of ETC gives a measured value of O expose = 8:9 for fib <ref> [ Mohr, 1991 ] </ref> . Whether this is an acceptable overhead or not for "typical" programs is of course a subjective matter. However, it is clear that a high overhead for fine grain programs will have an impact on the style of programming adopted by users. <p> This parallelism, which is no more than a degree of 2, enables tasks to be created and started faster. In addition, better caching of the task stack top is possible because it is single writer shared data (as opposed to multiple writer shared data for top stealing). Mohr <ref> [ Mohr, 1991 ] </ref> has analyzed the task stealing behavior of bottom stealing for tree-like DAC parallel programs. He has derived an upper bound of p 2 h task steals for programs with binary spawning trees of height h running on a machine with p processors. <p> The same terminology as <ref> [ Mohr, 1991 ] </ref> has been used when possible for consistency. The term lazy task refers to a task in the lightweight representation (i.e. a task contained in the task stack). These three data structures are really double ended queues which are mostly used as stacks. <p> The topmost lazy task could simply be recreated and resumed (i.e. popped from the task stack) after adding the current task on the placeholder's waiting queue. Mohr's system <ref> [ Mohr, 1991 ] </ref> uses this approach (which he calls tail-biting ) even though he concedes that "... it goes against our preference for oldest-first scheduling, since we have effectively 96 CHAPTER 3. LAZY TASK CREATION created a task at the newest potential fork point. <p> The same basic principles used by these algorithms can be adapted to design a special purpose synchronization mechanism for LTC as described next. With the exception of the previously mentioned method to make TAIL private, this algorithm is similar to the one described in <ref> [ Mohr, 1991 ] </ref> . The only atomic operations in these algorithms are the memory references and lock operations (increments and decrements do not have to be atomic). <p> Eric Mohr independently explored the LTC mechanism with the Mul-T system on the Encore Multimax multiprocessor (a UMA computer with up to 20 processors) and 165 166 CHAPTER 6. CONCLUSION ended up using a version of the shared-memory (SM) protocol very similar to the one used here <ref> [ Mohr, 1991 ] </ref> . In the SM protocol, thief processors directly access the stack of other processors to "steal" tasks. <p> A general description of these programs is given in section 2.9. Half of the programs were originally written in Mul-T by Eric Mohr as part of his PhD thesis work <ref> [ Mohr, 1991 ] </ref> . These programs were translated to Scheme with superficial changes to suit Gambit's particular features.
Reference: [ Mou, 1990 ] <author> Z. G. Mou. </author> <title> A formal model of divide-and-conquer and its parallel realization. </title> <institution> Computer science research report #795 (PhD dissertation), Yale University, </institution> <year> 1990. </year>
Reference-contexts: BACKGROUND a fundamental technique for constructing parallel algorithms <ref> [ Mou, 1990 ] </ref> , it also blends naturally with the recursive algorithms and data structures commonly found in Lisp and symbolic processing.
Reference: [ Murray, 1990 ] <author> K. Murray. </author> <title> The future of Common Lisp: Higher performance through parallelism. </title> <booktitle> In The first European Conference on the Practical Application of Lisp, </booktitle> <address> Cambridge, UK, </address> <month> March </month> <year> 1990. </year>
Reference-contexts: lisp [ Swanson et al., 1988 ] and Concurrent Scheme [ Kessler and Swanson, 1990, Kessler et al., 1992 ] as well as commercially available systems such as BBN Lisp [ Steinberg et al., 1986 ] , Allegro Common Lisp [ Fra, 1990 ] , and Top Level Common Lisp <ref> [ Murray, 1990 ] </ref> .
Reference: [ Nikhil et al., 1991 ] <author> R. S. Nikhil, G. M. Papadopoulos, and Arvind. </author> <title> *T: A multithreaded massively parallel architecture. </title> <type> Technical Report Computations Structures Group Memo 325-1, </type> <institution> Mass. Inst. of Technology, Laboratory for Computer Science, </institution> <address> Cam-bridge, MA, </address> <month> November </month> <year> 1991. </year>
Reference: [ O'Krafka and Newton, 1990 ] <author> B. W. O'Krafka and A. R. </author> <title> Newton. An empirical evaluation of two memory-efficient directory methods. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 138-147. </pages> <publisher> ACM, </publisher> <month> May </month> <year> 1990. </year>
Reference-contexts: Fortunately, it seems that in typical applications most of the shared data is shared by a very small number of processors <ref> [ Lenoski et al., 1992, O'Krafka and Newton, 1990 ] </ref> .
Reference: [ Osborne, 1989 ] <author> R. Osborne. </author> <title> Speculative Computation in Multilisp. </title> <type> PhD thesis, </type> <institution> Mass. Inst. of Technology, </institution> <year> 1989. </year> <note> Available as MIT LCS/TR/464. </note>
Reference-contexts: Speculative computation arises naturally in search problems where multiple solutions may exist but only one is needed. Several search paths can be explored in parallel and as soon as a solution is found the search can be stopped. This form of computation, which Osborne <ref> [ Osborne, 1989 ] </ref> calls multiple approach speculative computation, is known in parallel logic programming as OR-parallel . If the likelihood of finding a solution in any given path is fairly similar, then it is reasonable to spend an equal effort searching each path.
Reference: [ Peterson, 1981 ] <author> G. L. Peterson. </author> <title> Myths about the mutual exclusion problem. </title> <journal> Information Processing Letters, </journal> <volume> 12(3) </volume> <pages> 115-116, </pages> <year> 1981. </year>
Reference-contexts: pop ret adr from continuation frame */ . . 3.5.1 Avoiding Hardware Locks Hardware locks can be avoided in the task popping operation by implementing the popping locks with any of several "software lock" algorithms based on shared variables (such as Dekker's algorithm [ Dijkstra, 1968 ] and Peterson's algorithm <ref> [ Peterson, 1981 ] </ref> ). The same basic principles used by these algorithms can be adapted to design a special purpose synchronization mechanism for LTC as described next.
Reference: [ Pfister et al., 1985 ] <author> G. F. Pfister, W. C. Brantley, D. A. George, S. L. Harvey, W. J. Kleinfelder, K. P. McAuliffe, E. A. Melton, V. A. Norton, and J. Weiss. </author> <title> The IBM Research Parallel Processor Prototype (RP3): Introduction and architecture. </title> <booktitle> International Conference on Parallel Processing, </booktitle> <pages> pages 764-771, </pages> <year> 1985. </year>
Reference-contexts: The same problem occurs when unrelated data values are referenced simultaneously and they happen to have been allocated in the same memory bank. Certain shared-memory machines, such as the BBN Monarch [ Rettberg et al., 1990 ] and IBM RP3 <ref> [ Pfister et al., 1985 ] </ref> , avoid some contention problems by using "combining" networks which combine similar requests to the same memory location (e.g. read, clear, add a constant). However, combining networks are ineffective for contention to unrelated data.
Reference: [ R3RS, 1986 ] <institution> Revised 3 report on the algorithmic language Scheme. ACM Sigplan Notices, </institution> <month> 21(12), December </month> <year> 1986. </year> <note> 212 BIBLIOGRAPHY </note>
Reference-contexts: FIRST-CLASS CONTINUATIONS 25 2.2 First-Class Continuations Perhaps Scheme's most unusual features is the availability of first-class continuation objects. Continuations have been used in the past to express the denotational semantics of programming languages such as Algol60 and Scheme itself <ref> [ R3RS, 1986, Clinger, 1984 ] </ref> . Most programming languages use continuations but they are usually hidden whereas in Scheme they can be manipulated explicitly. First-class continuations are useful to implement advanced control structures that would be hard to express otherwise.
Reference: [ R4RS, 1991 ] <institution> Revised 4 report on the algorithmic language Scheme. Technical Report MIT AI Memo 848b, Mass. Inst. of Technology, </institution> <address> Cambridge, Mass., </address> <month> November </month> <year> 1991. </year>
Reference-contexts: There are six basic types of expressions in Scheme: constant, variable reference, assignment, conditional, procedure abstraction (lambda-expression) and procedure call. All the other types of expressions can be derived from the basic types and this is in fact how they are defined in the standard <ref> [ IEEE Std 1178-1990, 1991, R4RS, 1991 ] </ref> . Being able to reduce a program to the basic expressions is helpful both as an implementation technique and as a means to understand programs and prove some of their properties. <p> In practice, it seems that Scheme favors a "mostly" functional style of programming where side-effects are used with discretion. This style of programming lends itself well to parallelism because sub-problems are often independent and are thus possible targets for concurrent evaluation. 1 Delay only exists in R 4 RS <ref> [ R4RS, 1991 ] </ref> . 2.2. FIRST-CLASS CONTINUATIONS 25 2.2 First-Class Continuations Perhaps Scheme's most unusual features is the availability of first-class continuation objects.
Reference: [ Rettberg et al., 1990 ] <author> R. D. Rettberg, W. R. Crowther, P. P. Carvey, and R. S. Tomlin-son. </author> <title> The Monarch parallel processor hardware design. </title> <journal> IEEE Computer, </journal> <volume> 23(4) </volume> <pages> 18-30, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: The same problem occurs when unrelated data values are referenced simultaneously and they happen to have been allocated in the same memory bank. Certain shared-memory machines, such as the BBN Monarch <ref> [ Rettberg et al., 1990 ] </ref> and IBM RP3 [ Pfister et al., 1985 ] , avoid some contention problems by using "combining" networks which combine similar requests to the same memory location (e.g. read, clear, add a constant). However, combining networks are ineffective for contention to unrelated data.
Reference: [ Rozas and Miller, 1991 ] <author> G. Rozas and J. S. Miller. </author> <title> Free variables and first-class environments. Lisp and Symbolic Computation: </title> <journal> An International Journal, </journal> <volume> 3(4) </volume> <pages> 107-141, </pages> <year> 1991. </year>
Reference-contexts: The placeholder is called the goal of the task and the task is the placeholder's owner 11 . This linkage was introduced 9 Efficiency can be improved somewhat by adding a cache to hold the value of recently accessed variables (for example see <ref> [ Rozas and Miller, 1991 ] </ref> ). 10 Multilisp was not designed to support first-class continuations so it isn't surprising that the original semantics does not interact well with them. 11 The term "motivated task" was used in [ Miller, 1987 ] . 50 CHAPTER 2.
Reference: [ Rozas, 1987 ] <author> G. Rozas. </author> <title> A computational model for observation in quantum mechanics. </title> <type> Master's thesis, </type> <institution> Mass. Inst. of Technology, </institution> <year> 1987. </year> <note> Available as MIT AI/TR/925. </note>
Reference-contexts: A less frequent, but possible, use of continuations is to reenter a computation that has already completed (see <ref> [ Rozas, 1987 ] </ref> for an application). The generality of first-class continuations comes at a price: a more complex programming model. In many languages, including Lisp, procedure calls have dynamic extent. This means that every entry of a procedure is balanced by a corresponding exit (normal or not).
Reference: [ Shivers, 1988 ] <author> O. Shivers. </author> <title> Control flow analysis in Scheme. </title> <booktitle> In ACM SIGPLAN '88 Conf. on Programming Language Design and Implementation, </booktitle> <pages> pages 164-174, </pages> <address> At-lanta, Georgia, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: A better solution would be for the compiler to do a data-flow analysis of the program to identify all the strict operations that might be passed a placeholder. Control-flow and data-flow analysis techniques such as <ref> [ Shivers, 1988, Shivers, 1991 ] </ref> would be a good starting point. 168 CHAPTER 6. CONCLUSION Appendix A Source Code for Parallel Benchmarks This appendix contains the source code for the parallel benchmark programs used in chapters 2, chapters 3, chapters 4, and 5.
Reference: [ Shivers, 1991 ] <author> O. Shivers. </author> <title> Data-flow analysis and type recovery in Scheme. </title> <editor> In Peter Lee, editor, </editor> <booktitle> Topics in Advanced Language Implementation. </booktitle> <publisher> The MIT Press, </publisher> <address> Cam-bridge, Mass., </address> <year> 1991. </year>
Reference-contexts: A better solution would be for the compiler to do a data-flow analysis of the program to identify all the strict operations that might be passed a placeholder. Control-flow and data-flow analysis techniques such as <ref> [ Shivers, 1988, Shivers, 1991 ] </ref> would be a good starting point. 168 CHAPTER 6. CONCLUSION Appendix A Source Code for Parallel Benchmarks This appendix contains the source code for the parallel benchmark programs used in chapters 2, chapters 3, chapters 4, and 5.
Reference: [ Srini, 1986 ] <author> V. P. Srini. </author> <title> An architectural comparison of dataflow systems. </title> <journal> IEEE Computer, </journal> <volume> 19(3) </volume> <pages> 68-88, </pages> <month> March </month> <year> 1986. </year>
Reference-contexts: A dynamic partitioning strategy must find some compromise between the benefit of added concurrency and the drawback of added overhead. Some have avoided this problem to some extent by relying on specialized hardware to reduce the cost of managing tasks. Dataflow machines <ref> [ Srini, 1986, Arvind and Nikhil, 1990 ] </ref> and multithreaded architectures [ Halstead and Fujita, 1988, Nikhil et al., 1991, Agarwal, 1991 ] fall in this category. However, software methods are attractive because they offer portability and low hardware cost.
Reference: [ Steele, 1978 ] <author> G. L. Steele. Rabbit: </author> <title> a compiler for Scheme. MIT AI Memo 474, </title> <institution> Mas-sachusetts Institute of Technology, </institution> <address> Cambridge, Mass., </address> <month> May </month> <year> 1978. </year>
Reference-contexts: Thus, one of the parameters of this internal procedure is a continuation which takes a single argument: the value of the expression. This model of evaluation gives rise to a programming style called continuation passing style , or CPS. CPS was originally used as a compilation technique for Scheme <ref> [ Steele, 1978 ] </ref> but CPS is equally useful to explain how continuations work. The interest of CPS is that programs written in this style are expressed in a restricted variant of Scheme yet all Scheme programs can be converted to CPS.
Reference: [ Steinberg et al., 1986 ] <author> S. Steinberg, D. Allen, L. Bagnall, and C. Scott. </author> <title> The Butterfly Lisp system. </title> <booktitle> In Proc. 1986 AAAI, volume 2, </booktitle> <address> Philadelphia, PA, </address> <month> August </month> <year> 1986. </year>
Reference-contexts: PaiLisp [ Ito and Matsui, 1990 ] , Spur Lisp [ Zorn et al., 1988 ] , Butterfly portable standard lisp [ Swanson et al., 1988 ] and Concurrent Scheme [ Kessler and Swanson, 1990, Kessler et al., 1992 ] as well as commercially available systems such as BBN Lisp <ref> [ Steinberg et al., 1986 ] </ref> , Allegro Common Lisp [ Fra, 1990 ] , and Top Level Common Lisp [ Murray, 1990 ] .
Reference: [ Swanson et al., 1988 ] <author> M. Swanson, R. Kessler, and G. Lindstrom. </author> <title> An implementation of portable standard Lisp on the BBN Butterfly. </title> <booktitle> In Conference Record of the 1988 ACM Conference on Lisp and Functional Programming, </booktitle> <pages> pages 132-141, </pages> <address> Snowbird, UT., </address> <month> July </month> <year> 1988. </year>
Reference-contexts: INTRODUCTION al., 1989 ] , Gambit [ Feeley and Miller, 1990 ] , PaiLisp [ Ito and Matsui, 1990 ] , Spur Lisp [ Zorn et al., 1988 ] , Butterfly portable standard lisp <ref> [ Swanson et al., 1988 ] </ref> and Concurrent Scheme [ Kessler and Swanson, 1990, Kessler et al., 1992 ] as well as commercially available systems such as BBN Lisp [ Steinberg et al., 1986 ] , Allegro Common Lisp [ Fra, 1990 ] , and Top Level Common Lisp [ Murray, <p> When run on an Encore Multimax, Mul-T requires roughly 130 machine instructions to implement the sequence (the actual cost depends on the number of closed variables, their location, etc.). Other compiler based systems require even more instructions. Portable Standard Lisp on the GP1000 <ref> [ Swanson et al., 1988 ] </ref> takes 480 secs (about 1440 instructions given that each processor gives out 3 MIPS) and QLisp on an Alliant FX/8 [ Goldman and Gabriel, 1988 ] takes 1400 instructions.
Reference: [ Wand, 1980 ] <author> M. Wand. </author> <title> Continuation-based program transformation strategies. </title> <journal> Journal of the ACM, </journal> <volume> 27(1) </volume> <pages> 164-180, </pages> <year> 1980. </year>
Reference-contexts: Thus it is possible to directly transfer control between two different branches of the call tree. This characteristic can be exploited to implement specialized control structures such as backtracking [ Haynes, 1986 ] , coroutines [ Haynes et al., 1984 ] and multitasking <ref> [ Wand, 1980 ] </ref> . A less frequent, but possible, use of continuations is to reenter a computation that has already completed (see [ Rozas, 1987 ] for an application). The generality of first-class continuations comes at a price: a more complex programming model.
Reference: [ Weening, 1989 ] <author> J. S. Weening. </author> <title> Parallel Execution of Lisp Programs. </title> <type> PhD thesis, </type> <institution> Stan-ford University, Department of Computer Science, </institution> <year> 1989. </year> <note> Available as STAN-CS-89-1265. BIBLIOGRAPHY 213 </note>
Reference-contexts: An overhead cost must also be expected if task grouping is managed dynamically by user code (as is the case for the depth and height cutoff methods proposed for tree-like computations by Weening <ref> [ Weening, 1989 ] </ref> ). The transformation is also error prone. Logical bugs as well as performance problems can be introduced by the user. For example, the recursion of fib can be unrolled once as shown in Figure 2.16 to double the task granularity.
Reference: [ Zorn et al., 1988 ] <author> B. Zorn, P. Hilfinger, K. Ho, J. Larus, and L. Semenzato. </author> <title> Features for multiprocessing in SPUR Lisp. </title> <type> Technical Report Report UCB/CSD 88/406, </type> <institution> University of California, Computer Science Division (EECS), </institution> <month> March </month> <year> 1988. </year>
Reference-contexts: INTRODUCTION al., 1989 ] , Gambit [ Feeley and Miller, 1990 ] , PaiLisp [ Ito and Matsui, 1990 ] , Spur Lisp <ref> [ Zorn et al., 1988 ] </ref> , Butterfly portable standard lisp [ Swanson et al., 1988 ] and Concurrent Scheme [ Kessler and Swanson, 1990, Kessler et al., 1992 ] as well as commercially available systems such as BBN Lisp [ Steinberg et al., 1986 ] , Allegro Common Lisp [
References-found: 67

