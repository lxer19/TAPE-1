URL: ftp://ftp.cs.rochester.edu/pub/papers/systems/96.Sampling_for_data_mining_of_association_rules.ps.gz
Refering-URL: http://www.cs.rochester.edu/u/srini/papers.html
Root-URL: 
Title: Evaluation of Sampling for Data Mining of Association Rules  
Author: Mohammed Javeed Zaki, Srinivasan Parthasarathy Wei Li, Mitsunori Ogihara 
Keyword: Data Mining, Association Rules, Random Sampling, Chernoff bounds.  
Note: supported this work. This work was supported in part by an NSF Research Initiation Award (CCR-9409120) and ARPA contract F19628-94-C-0057.  
Address: Rochester, New York 14627  
Affiliation: The University of Rochester Computer Science Department  The University of Rochester Computer Science Department  
Pubnum: Technical Report 617  
Email: fzaki,srini,wei,ogiharag@cs.rochester.edu  
Date: May 1996  
Abstract: Data mining is an emerging research area, whose goal is to extract significant patterns or interesting rules from large databases. High-level inference from large volumes of routine business data can provide valuable information to businesses, such as customer buying patterns, shelving criterion in supermarkets and stock trends. However, many algorithms proposed for data mining of association rules make repeated passes over the database to determine the commonly occurring itemsets (or set of items). For large databases, the I/O overhead in scanning the database can be extremely high. In this paper we show that random sampling of transactions in the database is an effective method for finding association rules. Sampling can speed up the mining process by more than an order of magnitude by reducing I/O costs and drastically shrinking the number of transaction to be considered. We may also be able to make the sampled database resident in main-memory. Furthermore, we show that sampling can accurately represent the data patterns in the database with high confidence. We experimentally evaluate the effectiveness of sampling on three databases. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Agrawal, T. Imielinski, and A. Swami. </author> <title> Mining association rules between sets of items in large databases. </title> <booktitle> In Proc. ACM SIGMOD Intl. Conf. Management of Data, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: In this paper we will concentrate on data mining for association rules. The problem of mining association rules over basket data was introduced in <ref> [1] </ref>. Basket data usually consists of a record per customer with a transaction date, along with items bought by the customer. An example of an association rule over such a database could be that 80% of the customers that bought bread and milk, also bought eggs. <p> The effectiveness of sampling is experimentally analyzed in section 4. Section 5 presents our conclusions. 2 Data Mining for Association Rules We now present the formal statement of the problem of mining association rules over basket data. The discussion below closely follows that in <ref> [1, 2] </ref>. Let I = fi 1 ; i 2 ; ; i m g be a set of m distinct attributes, also called items. Each transaction T in the database D of transactions, has a unique identifier T ID, and contains a set of items, such that T I. <p> We refer the reader to [2] for more detail on rule generation. Henceforth, we will deal only with the first step. Many algorithms for finding large itemsets have been proposed in the literature since the introduction of this problem in <ref> [1] </ref> (AIS algorithm). In [7] a pass minimization approach was presented, which uses the idea that if an itemset belongs to the set of large (k + e)-itemsets, then it must contain k k-itemsets.
Reference: [2] <author> R. Agrawal and R. Srikant. </author> <title> Fast algorithms for mining association rules. </title> <booktitle> In Proc. 20th VLDB Conf., </booktitle> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: The effectiveness of sampling is experimentally analyzed in section 4. Section 5 presents our conclusions. 2 Data Mining for Association Rules We now present the formal statement of the problem of mining association rules over basket data. The discussion below closely follows that in <ref> [1, 2] </ref>. Let I = fi 1 ; i 2 ; ; i m g be a set of m distinct attributes, also called items. Each transaction T in the database D of transactions, has a unique identifier T ID, and contains a set of items, such that T I. <p> Generate rules from these large itemsets. Given that X is a large k-itemset, for every non-empty subset A X, a rule of the form A ) B is generated, where B = X A, and provided that this rule has the required confidence. We refer the reader to <ref> [2] </ref> for more detail on rule generation. Henceforth, we will deal only with the first step. Many algorithms for finding large itemsets have been proposed in the literature since the introduction of this problem in [1] (AIS algorithm). <p> In [7] a pass minimization approach was presented, which uses the idea that if an itemset belongs to the set of large (k + e)-itemsets, then it must contain k k-itemsets. The Apriori algorithm <ref> [2] </ref> also uses the property that any subset of a large itemset must itself be large. These algorithms had performance superior to AIS. Newer algorithms with better performance than Apriori were presented in [9, 10]. <p> Let L k denote the set of Large k-itemsets and C k the set of candidate k-itemsets. The general structure of the algorithm is given in figure 1. We refer the reader to <ref> [2] </ref> for more detail on Apriori, and its performance characteristics. <p> All experiments were conducted on a DEC alpha processor. We used three different databases to evaluate the effectiveness of sampling. These are: * SYNTH: This is a database of synthetic transactions, based on the one used in <ref> [2] </ref>. This database mimics the transactions in a retailing environment. Each transaction has a unique ID followed by a list of items bought in that transaction. The data-mining provides information about the set of items generally bought together. <p> We refer the reader to <ref> [2] </ref> for more detail on the database generation. * ENROLL: This is a database of student enrollments for a particular graduating class. Each transaction consists of a student ID followed by information on the college, major, department, semester, and a list of courses taken during that semester.
Reference: [3] <author> W. G. Cochran. </author> <title> Sampling Techniques. </title> <publisher> John Wiley & Sons, </publisher> <year> 1977. </year>
Reference-contexts: The random variable X giving the number of transactions in the sample containing the itemset I, has a binomial distribution of n trials, with the probability of success t (note: the correct distribution for finite populations is the Hypergeometric distribution, although the Binomial distribution is a satisfactory approximation <ref> [3] </ref>).
Reference: [4] <author> T. Hagerup and C. Rub. </author> <title> A guided tour of chernoff bounds. </title> <booktitle> In Information Processing Letters, </booktitle> <pages> pages 305-308. </pages> <publisher> North-Holland, 1989/90. </publisher>
Reference-contexts: For any positive constant, 0 * 1, the Chernoff bounds <ref> [4] </ref> state that P (X (1 *)nt ) e * 2 nt=2 (1) Chernoff bounds provide information on how close is the actual occurrence of an itemset in the sample, as compared to the expected count in the sample.
Reference: [5] <author> M. Holsheimer, M. Kersten, H. Mannila, and H. Toivonen. </author> <title> A perspective on databases and data mining. </title> <booktitle> In 1st Intl. Conf. Knowledge Discovery and Data Mining, </booktitle> <month> Aug. </month> <year> 1995. </year>
Reference-contexts: The above algorithms are all specialized black-box techniques which do not use any database operations. Algorithms using only general-purpose DBMS systems and relational algebra operations have also been proposed <ref> [5, 6] </ref>. In this paper we will use the Apriori algorithm to evaluate the effectiveness of sampling for data mining. We would like to point out that our results are about sampling, and as such independent of, and equally applicable to any algorithm for mining of association rules.
Reference: [6] <author> M. Houtsma and A. Swami. </author> <title> Set-oriented mining of association rules. In RJ 9567. </title> <institution> IBM Almaden, </institution> <month> Oct. </month> <year> 1993. </year>
Reference-contexts: The above algorithms are all specialized black-box techniques which do not use any database operations. Algorithms using only general-purpose DBMS systems and relational algebra operations have also been proposed <ref> [5, 6] </ref>. In this paper we will use the Apriori algorithm to evaluate the effectiveness of sampling for data mining. We would like to point out that our results are about sampling, and as such independent of, and equally applicable to any algorithm for mining of association rules.
Reference: [7] <author> H. Mannila, H. Toivonen, and I. Verkamo. </author> <title> Efficient algorithms for discovering association rules. </title> <note> In AAAI Wkshp. Knowledge Discovery in Databases, </note> <month> July </month> <year> 1994. </year>
Reference-contexts: We refer the reader to [2] for more detail on rule generation. Henceforth, we will deal only with the first step. Many algorithms for finding large itemsets have been proposed in the literature since the introduction of this problem in [1] (AIS algorithm). In <ref> [7] </ref> a pass minimization approach was presented, which uses the idea that if an itemset belongs to the set of large (k + e)-itemsets, then it must contain k k-itemsets. The Apriori algorithm [2] also uses the property that any subset of a large itemset must itself be large. <p> Such information can be used for statistical analyses of databases, where approximate answers would suffice. It may also be used to estimate selectivities or intermediate result sizes for query optimization [8]. The application of sampling for mining association rules was suggested in <ref> [7] </ref>. In this paper we extend their suggestion by evaluating the effective ness of sampling in practice. 3.1 Binomial Distribution A Bernoulli trial is an experiment with only two outcomes. Namely, success, which occurs with probability p, and failure which occurs with probability q = 1 p.
Reference: [8] <author> F. Olken and D. Rotem. </author> <title> Random sampling from databases a survey. In Draft. </title> <institution> ICS Div, Lawrence Berkeley Lab., </institution> <month> Mar. </month> <year> 1994. </year>
Reference-contexts: This may assist the user by providing approximate answers. Such information can be used for statistical analyses of databases, where approximate answers would suffice. It may also be used to estimate selectivities or intermediate result sizes for query optimization <ref> [8] </ref>. The application of sampling for mining association rules was suggested in [7]. In this paper we extend their suggestion by evaluating the effective ness of sampling in practice. 3.1 Binomial Distribution A Bernoulli trial is an experiment with only two outcomes.
Reference: [9] <author> J. S. Park, M. Chen, and P. S. Yu. </author> <title> An effective hash based algorithm for mining association rules. </title> <booktitle> In Proc. ACM SIGMOD Intl. Conf. Management of Data, </booktitle> <month> May </month> <year> 1995. </year>
Reference-contexts: The Apriori algorithm [2] also uses the property that any subset of a large itemset must itself be large. These algorithms had performance superior to AIS. Newer algorithms with better performance than Apriori were presented in <ref> [9, 10] </ref>. The DHP algorithm [9] 2 uses a hash table in pass k to do efficient pruning of (k + 1)-itemsets. The Parti--tion algorithm [10] minimizes I/O by scanning the database only twice. <p> The Apriori algorithm [2] also uses the property that any subset of a large itemset must itself be large. These algorithms had performance superior to AIS. Newer algorithms with better performance than Apriori were presented in [9, 10]. The DHP algorithm <ref> [9] </ref> 2 uses a hash table in pass k to do efficient pruning of (k + 1)-itemsets. The Parti--tion algorithm [10] minimizes I/O by scanning the database only twice.
Reference: [10] <author> A. Savasere, E. Omiecinski, and S. Navathe. </author> <title> An efficient algorithm for mining association rules in large databases. </title> <booktitle> In Proc. 21st VLDB Conf., </booktitle> <year> 1995. </year>
Reference-contexts: The Apriori algorithm [2] also uses the property that any subset of a large itemset must itself be large. These algorithms had performance superior to AIS. Newer algorithms with better performance than Apriori were presented in <ref> [9, 10] </ref>. The DHP algorithm [9] 2 uses a hash table in pass k to do efficient pruning of (k + 1)-itemsets. The Parti--tion algorithm [10] minimizes I/O by scanning the database only twice. <p> These algorithms had performance superior to AIS. Newer algorithms with better performance than Apriori were presented in [9, 10]. The DHP algorithm [9] 2 uses a hash table in pass k to do efficient pruning of (k + 1)-itemsets. The Parti--tion algorithm <ref> [10] </ref> minimizes I/O by scanning the database only twice. In the first pass it generates the set of all potentially large itemsets, and in the second pass the support for all these is measured. The above algorithms are all specialized black-box techniques which do not use any database operations.
Reference: [11] <author> J. S. Vitter. </author> <title> An efficient algorithm for sequential random sampling. </title> <type> Technical Report 624, </type> <institution> INRIA, </institution> <month> Feb. </month> <year> 1987. </year> <month> 13 </month>
Reference-contexts: On the other hand, if at any drawing, each unit has an equal chance of being selected, no matter how many times it has been drawn, it leads to random sampling with replacement. An efficient algorithm for sequential random sampling without replacement is presented in <ref> [11] </ref>. Random sampling from databases has been successfully used in query size estimation. This may assist the user by providing approximate answers. Such information can be used for statistical analyses of databases, where approximate answers would suffice.
References-found: 11

