URL: ftp://ftp.icsi.berkeley.edu/pub/techreports/1992/tr-92-081.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/techreports/1992.html
Root-URL: http://www.icsi.berkeley.edu
Title: Connectionist Probability Estimation in HMM Speech Recognition  
Phone: 1-510-642-4274 FAX 1-510-643-7684  
Author: Steve Renals and Nelson Morgan 
Date: December 1992  
Address: I 1947 Center Street Suite 600 Berkeley, California 94704  
Affiliation: INTERNATIONAL COMPUTER SCIENCE INSTITUTE  
Pubnum: TR-92-081  
Abstract: This report is concerned with integrating connectionist networks into a hidden Markov model (HMM) speech recognition system, This is achieved through a statistical understanding of connectionist networks as probability estimators, first elucidated by Herv e Bourlard. We review the basis of HMM speech recognition, and point out the possible benefits of incorporating connectionist networks. We discuss some issues necessary to the construction of a connectionist HMM recognition system, and describe the performance of such a system, including evaluations on the DARPA database, in collaboration with Mike Co-hen and Horacio Franco of SRI International. In conclusion, we show that a connectionist component improves a state of the art HMM system. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aizerman, M. A., Braverman, E. M., & Rozonoer, L. I. </author> <year> (1964). </year> <title> Theoretical foundations of the potential function method in pattern recognition learning. </title> <journal> Automation and Remote Control, </journal> <volume> 25, </volume> <pages> 821-837. </pages>
Reference-contexts: But there is no sense of discrimination. 5 There are many discriminative pattern recognition methods in the literature including the method of Potential Functions <ref> (Aizerman et al., 1964) </ref>, learning vector quantisation (LVQ) (Kohonen et al., 1988) and the multi-layer perceptron (MLP) (see e.g. Hertz et al. (1991)). Unlike the other methods, the MLP may be used directly to compute class-conditional posterior probabilities (see section III).
Reference: <author> Austin, S., Zavaliagkos, G., Makhoul, J., & Schwartz, R. </author> <year> (1992). </year> <title> Speech recognition using sege-mental neural nets. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pp. </pages> <address> 625-628 San Francisco. </address>
Reference: <author> Bahl, L. R., Brown, P. F., de Souza, P. V., & Mercer, R. L. </author> <year> (1986). </year> <title> Maximum mutual information estimation of hidden Markov model parameters for speech recognition. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pp. </pages> <address> 49-52 Tokyo. </address>
Reference: <author> Bahl, L. R., Jelinek, F., & Mercer, R. L. </author> <year> (1983). </year> <title> A maximum likelihood approach to continuous speech recognition. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-5, </volume> <pages> 179-190. </pages>
Reference: <author> Baum, L. E., Petrie, T., Soules, G., & Weiss, N. </author> <year> (1970). </year> <title> A maximization technique occuring in the statistical analysis of probabilistic functions of Markov chains. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 41, </volume> <pages> 164-171. </pages>
Reference: <author> Bengio, Y., de Mori, R., Flammia, G., & Kompe, R. </author> <year> (1992). </year> <title> Global optimization of a neural network-hidden Markov model hybrid. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 3, </volume> <pages> 252-259. </pages>
Reference: <author> Bourlard, H., & Morgan, N. </author> <year> (1990). </year> <title> A continuous speech recognition system embedding MLP into HMM. </title> <editor> In Touretzky, D. S. (Ed.), </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> Vol. 2, </volume> <pages> pp. 413-416. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo CA. </address>
Reference-contexts: A further benefit of using MLPs comes from the regularity of the resulting recognition time computations, enabling an efficient implementation using parallel hardware. 7. This substitution is not forced upon us; there is no technical reason why a Viterbi search cannot be carried out using posterior probabilities <ref> (Bourlard & Wellekens, 1990) </ref>. 9 PRIORS AND BIASES If we use a softmax output transfer function then: P (q i |x) = P j w ij hid j + bias i ) P P j w kj hid j + bias k ) (7) X w ij hid j + bias <p> A second approach to global optimisation of discriminative HMMs is analagous to segmental k-means training and has been referred to as embedded training <ref> (Bourlard & Morgan, 1990) </ref> or connectionist Viterbi training (Franzini, Lee, & Waibel, 1990). In this method a frame level optimization is interleaved with a Viterbi re-alignment. It should be noted that the transition probabilities are still optimised by a maximum likelihood criterion (or the Viterbi approximation to it). <p> It should be noted that the transition probabilities are still optimised by a maximum likelihood criterion (or the Viterbi approximation to it). It may be proved that performing a Viterbi segmentation using posterior local probabilities will also result in a global optimisation <ref> (Bourlard & Wellekens, 1990) </ref>: however, there is a mismatch between model and acoustic data priors, as discussed earlier. 12 Part V A CONNECTIONIST-HMM RECOGNITION SYSTEM The previously described components may be put together to form a hybrid connectionist-HMM continuous speech recognition system (figure 3).
Reference: <author> Bourlard, H., & Wellekens, C. J. </author> <year> (1989). </year> <title> Links between Markov models and multi-layer perceptrons. </title>
Reference: <editor> In Touretzky, D. S. (Ed.), </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> Vol. 1, </volume> <pages> pp. 502-510. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo CA. </address>
Reference: <author> Bourlard, H., & Wellekens, C. J. </author> <year> (1990). </year> <title> Links between Markov models and multilayer perceptrons. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-12, </volume> <pages> 1167-1178. </pages>
Reference-contexts: A further benefit of using MLPs comes from the regularity of the resulting recognition time computations, enabling an efficient implementation using parallel hardware. 7. This substitution is not forced upon us; there is no technical reason why a Viterbi search cannot be carried out using posterior probabilities <ref> (Bourlard & Wellekens, 1990) </ref>. 9 PRIORS AND BIASES If we use a softmax output transfer function then: P (q i |x) = P j w ij hid j + bias i ) P P j w kj hid j + bias k ) (7) X w ij hid j + bias <p> A second approach to global optimisation of discriminative HMMs is analagous to segmental k-means training and has been referred to as embedded training <ref> (Bourlard & Morgan, 1990) </ref> or connectionist Viterbi training (Franzini, Lee, & Waibel, 1990). In this method a frame level optimization is interleaved with a Viterbi re-alignment. It should be noted that the transition probabilities are still optimised by a maximum likelihood criterion (or the Viterbi approximation to it). <p> It should be noted that the transition probabilities are still optimised by a maximum likelihood criterion (or the Viterbi approximation to it). It may be proved that performing a Viterbi segmentation using posterior local probabilities will also result in a global optimisation <ref> (Bourlard & Wellekens, 1990) </ref>: however, there is a mismatch between model and acoustic data priors, as discussed earlier. 12 Part V A CONNECTIONIST-HMM RECOGNITION SYSTEM The previously described components may be put together to form a hybrid connectionist-HMM continuous speech recognition system (figure 3).
Reference: <author> Bridle, J. S. </author> <year> (1990a). </year> <title> Alpha-nets: a recurrent neural network architecture with a hidden Markov model interpretation. </title> <journal> Speech Communication, </journal> <volume> 9, </volume> <pages> 83-92. </pages>
Reference-contexts: Bahl et al. (1986) presented a training scheme for continuous HMMs in which the mutual information between the acoustic evidence and the word sequence was maximised using gradient descent. This is a discriminative objective function, maximised by gradient descent. More recently, Bridle introduced the alphanet representation <ref> (Bridle, 1990a) </ref> of HMMs, in which the computation of the HMM forward probabilities a jt = P (X T 1 , q (t) = j) is performed by the forward dynamics of a recurrent network. Alphanets may be discriminatively trained by minimising a relative entropy objective function.
Reference: <author> Bridle, J. S. </author> <year> (1990b). </year> <title> Training stochastic model recognition algorithms as networks can lead to maximum mutual information estimation of parameters. </title> <editor> In Touretzky, D. S. (Ed.), </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> Vol. 2, </volume> <pages> pp. 211-217. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo CA. </address>
Reference-contexts: However in some applications (e.g. when combining or comparing the estimates from different networks) it is desirable to enforce a `sum-to-1' constraint. One way of achieving this is by adopting a normalising output transfer function such as the normalised exponential or `softmax' <ref> (Bridle, 1990b) </ref>. In estimating posteriors using a MLP, we are using a discriminative training criterion and not performing density estimation.
Reference: <author> Bridle, J. S., & Dodd, L. </author> <year> (1991). </year> <title> An Alphanet approach to optimising input transformations for continuous speech recognition. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pp. 277-280 Toronto. </pages>
Reference: <author> Brown, P. F. </author> <year> (1987). </year> <title> The Acoustic-Modelling Problem in Automatic Speech Recognition. </title> <type> Ph.D. thesis, </type> <institution> School of Computer Science, Carnegie Mellon University. </institution>
Reference-contexts: This is a result of the observation independence assumption, causing an under-estimate of the joint observation density <ref> (Brown, 1987) </ref>. 15 Part VI EXPERIMENTS METHODS Most of our experiments have been performed using the DARPA Resource Management (RM) speaker independent continuous speech database, in collaboration with Mike Cohen and Horacio Franco of SRI International. This is a very well studied database with a vocabulary of 991 words.
Reference: <author> Cohen, M., Murveit, H., Bernstein, J., Price, P., & Weintraub, M. </author> <year> (1990). </year> <title> The DECIPHER speech recognition system. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pp. </pages> <address> 77-80 Albuquerque. </address>
Reference-contexts: These two methods together improved the speed of training by a factor of 2 (training time of 10 passes 12 through the training set reduced to 5) and also improved generalisation. We integrated these probability estimates into SRI's system, DECIPHER <ref> (Cohen et al., 1990) </ref>. DECIPHER is a much richer system than the previous baseline systems we have used. It includes both multiple probabilistic word pronunciations, cross-word phonological models, multiple densities per phone, and context dependent phone modelling. 13 The basic DECIPHER system uses tied Gaussian mixtures to estimate pdfs.
Reference: <author> Duda, R. O., & Hart, P. E. </author> <year> (1973). </year> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley Interscience, </publisher> <address> New York. </address>
Reference: <author> Fanty, M., Cole, R. A., & Roginski, K. </author> <year> (1992). </year> <title> English alphabet recognition with telephone speech. </title>
Reference: <editor> In Moody, J. E., Hanson, S. J., & Lippmann, R. P. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> Vol. 2, </volume> <pages> pp. 199-206. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo CA. </address> <note> 22 Franzini, </note> <author> M. A., Lee, K. F., & Waibel, A. </author> <year> (1990). </year> <title> Connectionist Viterbi training: a new hybrid method for continuous speech recognition. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pp. </pages> <address> 425-428 Albuquerque. </address>
Reference-contexts: A second approach to global optimisation of discriminative HMMs is analagous to segmental k-means training and has been referred to as embedded training (Bourlard & Morgan, 1990) or connectionist Viterbi training <ref> (Franzini, Lee, & Waibel, 1990) </ref>. In this method a frame level optimization is interleaved with a Viterbi re-alignment. It should be noted that the transition probabilities are still optimised by a maximum likelihood criterion (or the Viterbi approximation to it).
Reference: <author> Gish, H. </author> <year> (1990). </year> <title> A probabilistic approach to the understanding and training of neural network classifiers. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pp. </pages> <address> 1361-1364 Albuquerque. </address>
Reference: <author> Hertz, J., Krogh, A., & Palmer, R. </author> <year> (1991). </year> <title> Introduction to the theory of neural computation. </title> <publisher> Addison-Wesley, </publisher> <address> Redwood City, CA. </address>
Reference: <author> Huang, W., Lippmann, R., & Gold, B. </author> <year> (1988). </year> <title> A neural net approach to speech recognition. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pp. </pages> <address> 99-102 New York. </address>
Reference: <author> Kohonen, T., Brna, G., & Chrisley, R. </author> <year> (1988). </year> <title> Statistical pattern recognition with neural networks: benchmarking studies. </title> <booktitle> In Proceedings Second IEEE International Conference on Neural Networks, </booktitle> <volume> Vol. 1, </volume> <pages> pp. </pages> <address> 61-68 San Diego. </address>
Reference-contexts: But there is no sense of discrimination. 5 There are many discriminative pattern recognition methods in the literature including the method of Potential Functions (Aizerman et al., 1964), learning vector quantisation (LVQ) <ref> (Kohonen et al., 1988) </ref> and the multi-layer perceptron (MLP) (see e.g. Hertz et al. (1991)). Unlike the other methods, the MLP may be used directly to compute class-conditional posterior probabilities (see section III).
Reference: <author> Lee, K. F. </author> <year> (1988). </year> <title> Large Vocabulary Speaker-Independent Continuous Speech Recognition: The SPHINX System. </title> <type> Ph.D. thesis, </type> <institution> School of Computer Science, Carnegie Mellon University. </institution>
Reference: <author> Liporace, L. A. </author> <year> (1982). </year> <title> Maximum likelihood estimation for multivariate observations of Markov sources. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-28, </volume> <pages> 729-734. </pages>
Reference: <author> Makino, S., Kawabata, T., & Kido, K. </author> <year> (1983). </year> <title> Recognition of consonants based on the perceptron model. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pp. </pages> <address> 738-741 Boston MA. </address>
Reference: <author> Ney, H. </author> <year> (1984). </year> <title> The use of a one-stage dynamic programming algorithm for connected word recognition. </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> 32, </volume> <pages> 263-271. </pages>
Reference-contexts: An efficient algorithm for computing this state sequence is a dynamic programming algorithm known as Viterbi decoding. The Viterbi algorithm essentially traces the minimum cost (or maximum probability) path through a time-state lattice <ref> (Ney, 1984) </ref> subject to the constraints imposed by the acoustic and language models. The Viterbi algorithm may also be used in training. In this case a Viterbi alignment is performed for a known word model sequence to obtain the optimal state segmentation.
Reference: <author> Niles, L. T. </author> <year> (1991). </year> <title> Timit phoneme recognition using an HMM-derived recurrent neural network. </title> <booktitle> In Proceedings European Conference on Speech Communication and Technology, </booktitle> <pages> pp. 559-562 Genova, </pages> <address> Italy. </address>
Reference: <author> Parzen, E. </author> <year> (1962). </year> <title> On estimation of a probability density function and mode. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 33, </volume> <pages> 1065-1076. </pages>
Reference-contexts: In non-parametric density estimation, the family of pdfs under consideration changes as more data is seen. An example is Parzen window estimation <ref> (Parzen, 1962) </ref>, a kernel-based method. In this technique, as new data points occur, new kernels corresponding to those data points are added to the estimator. This has been used in HMM speech recognition by Soudoplatoff (1986).
Reference: <author> Rabiner, L. R., Wilpon, J. G., & Soong, F. K. </author> <year> (1989). </year> <title> High performance connected digit recognition using hidden Markov models. </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> 37, </volume> <pages> 1214-1225. </pages>
Reference-contexts: Given this optimal segmentation the output pdf parameters (e.g. means and variances of Gaussians, weights of a MLP, etc.) may be re-estimated. In the case of Gaussian pdfs, this process is known as the segmental k-means algorithm <ref> (Rabiner et al., 1989) </ref>. PRIOR PROBABILITIES The combination of phone models to form word models is constrained by a phone-structured lexicon that details the allowed pronunciations for each word.
Reference: <author> Renals, S., Morgan, N., Cohen, M., & Franco, H. </author> <year> (1992). </year> <title> Connectionist probability estimation in the DECIPHER speech recognition system. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pp. </pages> <address> 601-604 San Francisco. </address>
Reference: <author> Richard, M. D., & Lippmann, R. P. </author> <year> (1991). </year> <title> Neural network classifiers estimate Bayesian a posteriori probabilities. </title> <journal> Neural Computation, </journal> <volume> 3, </volume> <pages> 461-483. </pages>
Reference: <author> Robinson, T. </author> <year> (1992). </year> <title> Recurrent nets for phone probability estimation. </title> <booktitle> In Proceedings DARPA Workshop on Continuous Speech Recognition, </booktitle> <address> p. </address> <publisher> In press. </publisher>
Reference: <author> Soudoplatoff, S. </author> <year> (1986). </year> <title> Markov modelling of continuous parameters in speech recognition. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pp. </pages> <address> 45-48 Tokyo. </address>
Reference: <author> Waibel, A., Hanazawa, T., Hinton, G., Shikano, K., & Lang, K. </author> <year> (1989). </year> <title> Phoneme recognition using time-delay neural networks. </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> 37, </volume> <pages> 328-339. </pages>
References-found: 34

