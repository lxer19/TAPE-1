URL: http://www.cs.cmu.edu/~jab/pubs/schneider.sked.ps
Refering-URL: http://www.cs.cmu.edu/afs/cs/user/jab/web/cv/cv100.html
Root-URL: 
Email: fschneide,jab,awmg@cs.cmu.edu  
Title: Value Function Based Production Scheduling  
Author: Jeff G. Schneider Justin A. Boyan Andrew W. Moore 
Address: Pittsburgh, PA 15213  
Affiliation: The Robotics Institute and Computer Science Department Carnegie Mellon University  
Abstract: Production scheduling, the problem of sequentially configuring a factory to meet forecasted demands, is a critical problem throughout the manufacturing industry. The requirement of maintaining product inventories in the face of unpredictable demand and stochastic factory output makes standard scheduling models, such as job-shop, inadequate. Currently applied algorithms, such as simulated annealing and constraint propagation, must employ ad-hoc methods such as frequent replanning to cope with uncertainty. In this paper, we describe a Markov Decision Process (MDP) formulation of production scheduling which captures stochasticity in both production and demands. The solution to this MDP is a value function which can be used to generate optimal scheduling decisions online. A simple example illustrates the theoretical superiority of this approach over replanning-based methods. We then describe an industrial application and two reinforcement learning methods for generating an approximate value function on this domain. Our results demonstrate that in both deterministic and noisy scenarios, value function approximation is an effective technique. 
Abstract-found: 1
Intro-found: 1
Reference: [ Atkeson et al., 1995 ] <author> C. Atkeson, S. Schaal, and A. Moore. </author> <title> Locally weighted learning. </title> <journal> AI Review, </journal> <year> 1995. </year>
Reference: [ Barto et al., 1995 ] <author> A. G. Barto, S. J. Bradtke, and S. P. Singh. </author> <title> Real-time learning and control using asynchronous dynamic programming. </title> <journal> Artificial Intelligence, </journal> <year> 1995. </year>
Reference-contexts: The two methods we tested are Memory-based RTDP and ROUT. 3.3.1 Memory-Based RTDP Memory-based RTDP is a reinforcement learning approach that is closely related to RTDP (Real-Time Dynamic Programming) <ref> [ Barto et al., 1995 ] </ref> and to Tesauro's application of TD (0) to the game of backgammon [ Sutton, 1988, Tesauro, 1992 ] . It is also similar to the instance-based approach to representing value functions used in [ Peng, 1993 ] . <p> It is also similar to the instance-based approach to representing value functions used in [ Peng, 1993 ] . Trajectories through the MDP model are generated repeatedly, using the current approximation of the value function to guide standard Boltzmann-style exploration <ref> [ Barto et al., 1995 ] </ref> . At each step of each trajectory, a one-step backup operation (Eq. 1) is performed and the function approximator is updated.
Reference: [ Bellman, 1957 ] <author> R. Bellman. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <year> 1957. </year>
Reference-contexts: Dynamic programming methods tabulate this optimal cumulative reward in the optimal value function V fl (x), which is the unique solution to the Bellman equations <ref> [ Bellman, 1957 ] </ref> : V fl (x) = max X P (x 0 jx; a)V fl (x 0 ) (1) Once V fl is computed, the optimal policy fl is immediately obtained by choosing any action which instantiates the max in Eq. 1.
Reference: [ Boyan and Moore, 1996 ] <author> J. A. Boyan and A. W. Moore. </author> <title> Learning evaluation functions for large acyclic domains. </title> <editor> In L. Saitta, editor, </editor> <booktitle> Machine Learning: Proceedings of the Thirteenth International Conference. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1996. </year>
Reference-contexts: fl , we did not find it necessary to include an explicit "forgetting" mechanism in the learning; the bad points are quickly overwhelmed by later, more accurate samples. 3.3.2 ROUT ROUT is an active learning algorithm for value function approximation that is specifically designed for the subclass of acyclic MDPs <ref> [ Boyan and Moore, 1996 ] </ref> . Note that the scheduling MDP is certainly acyclic, since its state representation includes the time counter t. <p> Unlike Memory-based RTDP and most other reinforcement learning methods, ROUT explicitly tries to prevent the function approximator from seeing any inaccurate samples of V fl . Details of how ROUT identifies such states automat ically are given in <ref> [ Boyan and Moore, 1996 ] </ref> . One by one, these useful states are accumulated into a training set of accurate samples of V fl (x). The training set grows backwards from the terminal states.
Reference: [ Cleveland and Delvin, 1988 ] <author> W. Cleveland and S. Delvin. </author> <title> Locally weighted regression: An approach to regression analysis by local fitting. </title> <journal> Journal of the American Statistical Association, </journal> <pages> pages 596-610, </pages> <month> September </month> <year> 1988. </year>
Reference: [ Mahadevan and Theocharous, 1998 ] <author> S. Mahadevan and G. Theocharous. </author> <title> Optimizing production manufacturing using reinforcement learning. </title> <booktitle> In Eleventh International FLAIRS Conference, </booktitle> <year> 1998. </year>
Reference-contexts: To our knowledge, this work represents the first application of reinforcement learning to production scheduling with multiple products made on multiple machines. The scheduling of machine maintenance is discussed in [ Mahadevan et al., 1997 ] , and transfer line production scheduling is discussed in <ref> [ Mahadevan and Theocharous, 1998 ] </ref> . In their task, each product or sub-product is produced on a single machine and each machine makes a local decision on whether to produce one of its products or go down for maintenance.
Reference: [ Mahadevan et al., 1997 ] <author> S. Mahadevan, N. Marchal leck, T. Das, and A. Gosavi. </author> <title> Self-Improving Factory Simulation using Continuous-Time Average-Reward Reinforcement Learning. </title> <booktitle> In Proceedings of the 14th International Conference on Machine Learning (IMLC '97), </booktitle> <address> Nashville, TN. </address> <publisher> Morgan Kauf-mann, </publisher> <month> July </month> <year> 1997. </year>
Reference-contexts: To our knowledge, this work represents the first application of reinforcement learning to production scheduling with multiple products made on multiple machines. The scheduling of machine maintenance is discussed in <ref> [ Mahadevan et al., 1997 ] </ref> , and transfer line production scheduling is discussed in [ Mahadevan and Theocharous, 1998 ] .
Reference: [ Moore et al., 1995 ] <author> A. Moore, C. Atkeson, and S. Schaal. </author> <title> Locally weighted learning for control. </title> <journal> AI Review, </journal> <year> 1995. </year>
Reference: [ Moore et al., 1997 ] <author> A. Moore, J. Schneider, and K. Deng. </author> <title> Efficient locally weighted polynomial regression predictions. </title> <booktitle> In International Conference on Machine Learning, </booktitle> <year> 1997. </year>
Reference-contexts: We applied ROUT to this instance, trying three different function approximators: 1-nearest neighbor, locally-weighted linear regression, and global quadratic regression. KD-trees were used to keep the computation efficient <ref> [ Moore et al., 1997 ] </ref> . For the locally weighted regression, a kernel width of 2 3 of the range of each input dimension in the training data was used. ROUT's exploration and tolerance parameters were tuned manually. Table 1 summarizes the results. <p> The simulated annealing runs made use of the successful "modified Lam" adaptive annealing schedule [ Ochotta, 1994 ] . Memory-based RTDP used kernel regression with a kernel width of 2 5 of the range of each state variable, and used KD-trees for efficiency <ref> [ Moore et al., 1997 ] </ref> . Boltzmann exploration (without cooling) was used for the deter ministic case, but proved unnecessary in the stochastic case because the noise alone caused sufficient exploration.
Reference: [ Ochotta, 1994 ] <author> E. Ochotta. </author> <title> Synthesis of High Performance Analog Cells in ASTRX/OBLX. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University Department of Electrical and Computer Engineering, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: It usually incurs no heuristic penalties in its schedules, so that figure is a profit in real dollars. The simulated-annealing, greedy-exploration, and Memory-based RTDP algorithms are run as described in the previous sections. The simulated annealing runs made use of the successful "modified Lam" adaptive annealing schedule <ref> [ Ochotta, 1994 ] </ref> . Memory-based RTDP used kernel regression with a kernel width of 2 5 of the range of each state variable, and used KD-trees for efficiency [ Moore et al., 1997 ] .
Reference: [ Peng, 1993 ] <author> J. Peng. </author> <title> Efficient Dynamic Programming based Learning for Control. </title> <type> PhD. Thesis, </type> <institution> Northeastern University, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: It is also similar to the instance-based approach to representing value functions used in <ref> [ Peng, 1993 ] </ref> . Trajectories through the MDP model are generated repeatedly, using the current approximation of the value function to guide standard Boltzmann-style exploration [ Barto et al., 1995 ] .
Reference: [ Sutton, 1988 ] <author> R. S. Sutton. </author> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <year> 1988. </year>
Reference-contexts: The two methods we tested are Memory-based RTDP and ROUT. 3.3.1 Memory-Based RTDP Memory-based RTDP is a reinforcement learning approach that is closely related to RTDP (Real-Time Dynamic Programming) [ Barto et al., 1995 ] and to Tesauro's application of TD (0) to the game of backgammon <ref> [ Sutton, 1988, Tesauro, 1992 ] </ref> . It is also similar to the instance-based approach to representing value functions used in [ Peng, 1993 ] .
Reference: [ Tesauro and Galperin, 1997 ] <author> G. Tesauro and G. R. Galperin. </author> <title> On-line policy improvement using Monte-Carlo search. </title> <editor> In M. C. Mozer, M. I. Jordan, and T. Petsche, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 9. </volume> <publisher> MIT Press, </publisher> <year> 1997. </year>
Reference-contexts: Further empirical work is required to answer that question. As the size of the scheduling problem increases, it becomes increasingly expensive to compute the value function accurately. However, even an inexact value function can be useful as the basis for a quasi-greedy search or "rollout" search performed online <ref> [ Tesauro and Galperin, 1997 ] </ref> . We intend to test such methods in future work on larger scheduling problems. Acknowledgements The second author acknowledges the support of a NASA GSRP fellowship. The third author acknowledges the support of an NSF Career Award.
Reference: [ Tesauro, 1992 ] <author> G. Tesauro. </author> <title> Practical issues in tem poral difference learning. </title> <journal> Machine Learning, </journal> <volume> 8(3/4), </volume> <month> May </month> <year> 1992. </year>
Reference-contexts: The two methods we tested are Memory-based RTDP and ROUT. 3.3.1 Memory-Based RTDP Memory-based RTDP is a reinforcement learning approach that is closely related to RTDP (Real-Time Dynamic Programming) [ Barto et al., 1995 ] and to Tesauro's application of TD (0) to the game of backgammon <ref> [ Sutton, 1988, Tesauro, 1992 ] </ref> . It is also similar to the instance-based approach to representing value functions used in [ Peng, 1993 ] .
Reference: [ Zhang and Dietterich, 1995 ] <author> W. Zhang and T. G. Di etterich. </author> <title> A reinforcement learning approach to job-shop scheduling. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI), </booktitle> <pages> pages 1114-1120, </pages> <year> 1995. </year>
Reference: [ Zweben and Fox, 1994 ] <author> M. Zweben and M. Fox. </author> <title> In telligent Scheduling. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: This model cannot readily be adapted to handle production rate interdependencies among machines, the desire to keep inventory levels above zero at all times (rather than just completing jobs by their deadlines), and stochasticity of demand forecasts and production. Constraint propagation methods (e.g. <ref> [ Zweben and Fox, 1994 ] </ref> ) are commonly used to solve industrial problems. They operate by efficiently managing constraints on production deadlines and machine capabilities. Solution methods tend to search by iteratively fixing violated constraints, applying heuristics to guide the fixes.
References-found: 16

