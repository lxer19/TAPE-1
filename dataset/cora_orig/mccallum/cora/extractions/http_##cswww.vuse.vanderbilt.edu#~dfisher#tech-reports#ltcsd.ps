URL: http://cswww.vuse.vanderbilt.edu/~dfisher/tech-reports/ltcsd.ps
Refering-URL: http://cswww.vuse.vanderbilt.edu/~dfisher/courses/cs362/ch5.html
Root-URL: 
Title: Learning to Classify Sensor Data  inductive bias, supervised Bayesian learning, minimum description length.  
Author: Stefanos Manganaris 
Keyword: electrical domain, classification, time series sensor data, serial correlation  
Note: as  This research has been supported in part by a grant from NASA Ames (NAG 2-834).  
Address: Box 1679, Station B Nashville, TN 37235, U.S.A.  
Affiliation: Dept. of Computer Science Vanderbilt University  
Email: E-mail: stefanos@vuse.vanderbilt.edu.  
Phone: Tel. (615)343-4111, (615)343-8006 (fax).  
Abstract:  
Abstract-found: 1
Intro-found: 1
Reference: [Bel61] <author> R. E. Bellman. </author> <title> Adaptive Control Processes. </title> <publisher> Princeton Univ. Press, </publisher> <year> 1961. </year>
Reference-contexts: The dimensionality of the space of concept instances in temporal domains is proportional to the length of the time interval. This usually leads to high-dimensional spaces, which have a great potential for complex concepts, and which can only be sparsely represented by training sets of reasonable size <ref> [Bel61] </ref>. Concept learning in high-dimensional spaces requires strong induction biases; serial correlation is a particularly appropriate one for temporal domains. In the absence of noise, correlation prior knowledge does not offer a critical advantage.
Reference: [BFOS84] <author> L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth, </publisher> <address> Belmont, CA, </address> <year> 1984. </year>
Reference-contexts: When the posterior probabilities can be computed precisely, this classifier is optimal, in terms of classification accuracy <ref> [BFOS84] </ref>. For the problem at hand, the probabilities can only be estimated with some confidence from the data and prior knowledge available.
Reference: [Che90] <author> Peter Cheeseman. </author> <title> On finding the most probable model. </title> <editor> In J. Shrager and P. Langley, editors, </editor> <title> Computational Models of Discovery and Theory Formation, </title> <booktitle> chapter 3, </booktitle> <pages> pages 73-95. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: Additional model complexity is only justified when there is enough unaccounted structure in the data to benefit from it and the benefit exceeds the cost of the added complexity. To assign priors, I use the minimum message length principle <ref> [Che90, Ris87, WF87] </ref>. Intuitively, consider the transmission of information from a sender to a receiver.
Reference: [Eli75] <author> Peter Elias. </author> <title> Universal codeword sets and representations of the integers. </title> <journal> IEEE Trans. on Information Theory, </journal> <volume> IT-21(2):194-203, </volume> <month> March </month> <year> 1975. </year>
Reference-contexts: A positive integer i is coded with log 2 (m + 1) bits, when m is a known upper bound for i; this is equivalent to a uniform prior. When there is no bound in our state of prior knowledge, I use the universal prior <ref> [Ris83, Eli75] </ref>: log fl 2 (i + 1) + c, where c 2:865 and where log fl 2 x log 2 x + log 2 log 2 x + : : : up to, but not including, the first negative term.
Reference: [HW73] <author> M. Hollander and D. Wolfe. </author> <title> Nonparametric Statistical Methods. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: they were computed by the Wilcoxon rank sum test. "NS" denotes differences not statistically significant. the differences in accuracy between Calchas and C4.5 with feature extraction was evaluated by a Wilcoxon rank sum test (a nonparametric test for comparing the medians of two independent samples of cardinal or ordinal data <ref> [HW73] </ref>); p-values are shown at each point. Calchas' performance gains showed under moderate noise levels; these gains were all found to be statistically significant.
Reference: [Ped89] <author> Edwin P.D. Pednault. </author> <title> Some experiments in applying inductive inference principles to surface reconstruction. </title> <booktitle> In Proc. of the Eleventh International Joint Conf. on Artificial Intelligence, </booktitle> <pages> pages 1603-1609, </pages> <year> 1989. </year>
Reference-contexts: As for k-models, the "goodness" of a piecewise polynomial is judged by its posterior probability. This part of the algorithm is adopted from an algorithm presented by Pednault in <ref> [Ped89] </ref>, originally used for surface reconstruction in computer vision. Although the general strategy is the same, the probability measures are computed differently. The number of possible partitions for an interval grows exponentially with the length of the time series.
Reference: [PTVF92] <author> William H. Press, Saul A. Teukolsky, William T. Vetterling, and Brian P. Flannery. </author> <title> Numerical Recipes in C: </title> <booktitle> The Art of Scientific Computing. </booktitle> <publisher> Cam-bridge University Press, </publisher> <address> 2nd edition, </address> <year> 1992. </year>
Reference-contexts: The optimal precision trades message length for likelihood, and vice-versa, to maximize the posterior probability. To estimate the optimal precision I use a standard golden-section search algorithm <ref> [PTVF92] </ref>. To compute the length of the optimal encoding of a k-model, I first encode the number of disjuncts.
Reference: [Qui93] <author> J. Ross Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kauf-mann, </publisher> <year> 1993. </year>
Reference-contexts: It has been implemented as part of an experimental system called Calchas. I empirically evaluate the algorithm in a NASA telemetry monitoring problem, by comparing its performance, in terms of attained classification accuracy, with C4.5, a popular algorithm not designed specifically for temporal data <ref> [Qui93] </ref>. Two comparisons are performed: one in which C4.5 is applied directly to the original data, and another in which features that capture serial correlations are extracted. C4.5 coped much better with noise when features were extracted.
Reference: [Ris83] <author> J. Rissanen. </author> <title> A universal prior for integers and estimation by minimum description length. </title> <journal> The Annals of Statistics, </journal> <volume> 11(2) </volume> <pages> 416-431, </pages> <year> 1983. </year>
Reference-contexts: A positive integer i is coded with log 2 (m + 1) bits, when m is a known upper bound for i; this is equivalent to a uniform prior. When there is no bound in our state of prior knowledge, I use the universal prior <ref> [Ris83, Eli75] </ref>: log fl 2 (i + 1) + c, where c 2:865 and where log fl 2 x log 2 x + log 2 log 2 x + : : : up to, but not including, the first negative term.
Reference: [Ris87] <author> Jorma Rissanen. </author> <title> Stochastic complexity. </title> <journal> Journal of the Royal Statistical Society, Series B, </journal> <volume> 49(3) </volume> <pages> 223-239, </pages> <year> 1987. </year>
Reference-contexts: Additional model complexity is only justified when there is enough unaccounted structure in the data to benefit from it and the benefit exceeds the cost of the added complexity. To assign priors, I use the minimum message length principle <ref> [Che90, Ris87, WF87] </ref>. Intuitively, consider the transmission of information from a sender to a receiver.
Reference: [SM92] <author> Padhraic Smyth and Jeff Mellstrom. </author> <title> Detecting novel classes with applications to fault diagnosis. </title> <editor> In Derek Sleeman and Peter Edwards, editors, </editor> <booktitle> Proc. of the Ninth Intl. Conf. on Machine Learning, </booktitle> <pages> pages 416-425, </pages> <year> 1992. </year>
Reference-contexts: To use one of the well established learning algorithms with such temporal data, one can simply treat each property at each time point as an attribute. Alternatively, one can preprocess each object to extract other informative, time-invariant, properties; for instance, Smyth and Mellstrom in <ref> [SM92] </ref> extract autoregressive coefficients to classify stationary, stochastic, time series. In this study, I investigate the appropriateness of using general purpose induction algorithms with temporal data, in the context of deterministic time series. I propose a learning algorithm that works directly with temporal data.
Reference: [WF87] <author> C. S. Wallace and P. R. Freeman. </author> <title> Estimation and inference by compact coding. </title> <journal> Journal of the Royal Statistical Society, Series B, </journal> <volume> 49(3) </volume> <pages> 252-265, </pages> <year> 1987. </year>
Reference-contexts: Additional model complexity is only justified when there is enough unaccounted structure in the data to benefit from it and the benefit exceeds the cost of the added complexity. To assign priors, I use the minimum message length principle <ref> [Che90, Ris87, WF87] </ref>. Intuitively, consider the transmission of information from a sender to a receiver.
References-found: 12

