URL: http://www.cri.ensmp.fr/doc/A-266.ps.Z
Refering-URL: http://www.cri.ensmp.fr/rapports.html
Root-URL: 
Title: Minimal Data Dependence Abstractions for Loop Transformations (Extended version)  
Author: Yi-Qing Yang, Corinne Ancourt, Fran~cois Irigoin 
Keyword: parallelizing compiler, program transformation, data dependence abstraction.  
Address: Paris  
Affiliation: Centre de Recherche en Informatique Ecole Nationale Superieure des Mines de  
Abstract: Many abstractions of program dependences have already been proposed, such as the Dependence Distance, the Dependence Direction Vector, the Dependence Level or the Dependence Cone. These different abstractions have different precisions. The minimal abstraction associated to a transformation is the abstraction that contains the minimal amount of information necessary to decide when such a transformation is legal. Minimal abstractions for loop reordering and unimodular transformations are presented. As an example, the dependence cone, which approximates dependences by a convex cone of the dependence distance vectors, is the minimal abstraction for unimodular transformations. It also contains enough information for legally applying all loop reordering transformations and finding the same set of valid mono- and multidimensional linear schedules as the dependence distance set. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> R. Allen, K. Kennedy, </author> <title> "Automatic Translation of FORTRAN Programs to Vector Form", </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <month> Oct. </month> <year> 1987. </year>
Reference-contexts: Dependence relations are represented by finite dependence abstractions that contain the data flow dependences of the statement set. Many dependence abstractions have already been proposed, such as the Dependence Distance [23], the Dependence Direction Vector [30], the Dependence Level <ref> [1] </ref> or the Dependence Cone [14]. These different abstractions have different precisions. They are presented in Sections 2 and 3. <p> However, transformations such as loop interchange or permutation, that only modify the sign or the order of the dependence distance vector, do not need the actual distance information. So, abstractions like DDV (L) [30] and DL (L) <ref> [1] </ref> relating only the sign or the level of dependence vectors have been suggested. Each Dependence Direction Vector element is one of f&lt; ; =; &gt;g. Other elements like ; ; fl may be used to summarize two or three DDV's. <p> It is represented in Figure 6. r r r r r r r r - 1 di Fig. 6. DDV for Program 1 2.6 The Dependence Level The dependence level DL has been introduced by Allen & Kennedy <ref> [1] </ref> for the vectorization and parallelization of programs. It gives the nest level of the outermost loop l that carries dependences, when the dependence vector component is positive. To maintain the program semantics, Loop l must be kept sequential. <p> The new loop nest L 0 is defined by L 0 = (l p <ref> [1] </ref> ; l p [2] ; :::; l p [n] ) where fp [1]; p [2]; ::; p [n]g is a permutation of f1; 2; ::ng. <p> The new loop nest L 0 is defined by L 0 = (l p <ref> [1] </ref> ; l p [2] ; :::; l p [n] ) where fp [1]; p [2]; ::; p [n]g is a permutation of f1; 2; ::ng. The effect of the transformation on the dependence distance vector ~ d = (d 1 ; ::; d k ; ::; d n ) is: ~ d 0 = Perm P ( ~ d) = (d p [1] <p> <ref> [1] </ref>; p [2]; ::; p [n]g is a permutation of f1; 2; ::ng. The effect of the transformation on the dependence distance vector ~ d = (d 1 ; ::; d k ; ::; d n ) is: ~ d 0 = Perm P ( ~ d) = (d p [1] ; :::; d p [k] ; :::; d p [n] ). So, the lexico-positivity of the dependences is maintained after transformation if the following condition is verified: legal (Perm P ; L) () 8 ~ d 2 D (L); (d p [1] ; :::; d p [k] ; :::; d <p> Perm P ( ~ d) = (d p <ref> [1] </ref> ; :::; d p [k] ; :::; d p [n] ). So, the lexico-positivity of the dependences is maintained after transformation if the following condition is verified: legal (Perm P ; L) () 8 ~ d 2 D (L); (d p [1] ; :::; d p [k] ; :::; d p [n] ) 0 Theorem 4 For loop permutations, the minimal abstraction is the Dependence Direction Vector DDV. DO I = 2, n DO K = 2,n,-1 ENDDO ENDDO ENDDO Fig. 9. <p> Like these examples, the dependence direction vector was defined by M. Wolfe for loop interchanging [30] and the dependence level by Allen & Kennedy for the vectorization and the parallelization of programs <ref> [1] </ref>. The dependence direction vector has been the most popular dependence abstraction because it has been successfully used for some important transformations such as loop interchanging and loop permutation. Moreover, its computation is easy and its representation in the dependence graph handy.
Reference: 2. <author> U. Banerjee, </author> <title> "A Theory of Loop Permutations", </title> <booktitle> 2nd Workshop on Languages and compilers for parallel computing, </booktitle> <year> 1989 </year>
Reference-contexts: All abstractions DI, D, DC, DP, DDV and DL are valid for a loop reversal transformation. The test of legality associated with the minimal abstraction DL is: legal (Inv k ; L) ()62 DL (L) () projection (DL (L); k) = ;: 5.2 Loop Permutation A loop permutation transformation <ref> [2] </ref> Perm P (L) performs permutation P on the n-dimensional iteration set of the loop nest L = (l 1 ; l 2 ; ::; l n ). The new loop nest L 0 is defined by L 0 = (l p [1] ; l p [2] ; :::; l p <p> A loop permutation transformation <ref> [2] </ref> Perm P (L) performs permutation P on the n-dimensional iteration set of the loop nest L = (l 1 ; l 2 ; ::; l n ). The new loop nest L 0 is defined by L 0 = (l p [1] ; l p [2] ; :::; l p [n] ) where fp [1]; p [2]; ::; p [n]g is a permutation of f1; 2; ::ng. <p> The new loop nest L 0 is defined by L 0 = (l p [1] ; l p <ref> [2] </ref> ; :::; l p [n] ) where fp [1]; p [2]; ::; p [n]g is a permutation of f1; 2; ::ng.
Reference: 3. <author> U. Banerjee, </author> <title> "Unimodular Transformation of Double Loops", </title> <booktitle> 3rd Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <address> Irvine, </address> <year> 1990 </year>
Reference-contexts: As for restructuring transformations, when an abstraction A different from DI is used, a definition using dependences in function of f DI A is derived from Definition 4. Unimodular Transformation A unimodular transformation <ref> [3, 29, 20] </ref> is a one-to-one mapping between two iteration sets I n and I 0 n having the same dimension. It corresponds to a unimodular change of basis M with ~ I 0 = M fi ~ I.
Reference: 4. <author> U. Banerjee, </author> <title> "Loop transformations for restructuring compilers: the foundations", </title> <publisher> Kluwer Academic Editor, </publisher> <year> 1993 </year>
Reference-contexts: This cannot be decided in general and Definitions 2, 3, 4, 5 are based on the fact that the new execution of statement instances must maintain all the loop nest lexico-positive dependences <ref> [4] </ref>. In 3 Proof is given in [33] contrived cases, these definitions are too pessimistic and conclude that a trans-formation cannot be applied although it is semantically legal.
Reference: 5. <author> A. J. Bernstein, </author> <title> "Analysis of Programs for Parallel Processing", </title> <journal> IEEE Transactions on Electronic Computers, </journal> <volume> Vol. EC-15, No. 5, </volume> <month> Oct. </month> <year> 1966. </year>
Reference: 6. <author> E. D'hollander, </author> <title> "Partitioning and Labeling of Index Sets in Do Loops with Constant Dependence Vectors", </title> <booktitle> International Conference on Parallel Processing, </booktitle> <year> 1989 </year>
Reference-contexts: The dependence abstraction is used for computing a valid linear schedule. Most of the algorithms suggested in the literature <ref> [6, 27, 7, 8] </ref> require that the dependences be uniform or are approximated by dependence distance vectors. This condition is too restrictive for real programs. <p> In this section, we show that the use of the dependence cone abstraction enables the obtaining of the same set of valid linear schedules as the dependence distance vector. However, when the computation of the linear schedule depends on particular distance vector characteristics (uniform) <ref> [6] </ref>, the dependence cone that approximates the distance vectors cannot be used for defining the linear schedule but only for testing its validity. 6.1 One-dimensional Linear Schedule The one-dimensional linear schedule is also called the hyperplane method.
Reference: 7. <author> A. Darte, Y. Robert, </author> <title> "Scheduling Uniform Loop Nests", </title> <institution> Rapport de Laboratoire de l'informatique du Paralle'lisme, Ecole Normale Sup'erieure de Lyon', No. </institution> <address> 92-10, Fev. </address> <year> 1992. </year>
Reference-contexts: The dependence abstraction is used for computing a valid linear schedule. Most of the algorithms suggested in the literature <ref> [6, 27, 7, 8] </ref> require that the dependences be uniform or are approximated by dependence distance vectors. This condition is too restrictive for real programs.
Reference: 8. <author> A. Darte, T. Risset, Y. Robert, </author> <title> "Loop Nest Scheduling and Transformations", </title> <booktitle> Conference on Environement and Tools for Parallel Scientific Computing, </booktitle> <address> CNRS-SNF, Saint-Hilaire du Touvier, France, </address> <month> Sep. </month> <year> 1992. </year>
Reference-contexts: The dependence abstraction is used for computing a valid linear schedule. Most of the algorithms suggested in the literature <ref> [6, 27, 7, 8] </ref> require that the dependences be uniform or are approximated by dependence distance vectors. This condition is too restrictive for real programs.
Reference: 9. <author> P. Feautrier, </author> <title> "Dataflow Analysis of Scalar and Array References", </title> <journal> Int. Journal of Parallel Programming, </journal> <volume> Vol. 20, No 1,February,1991, </volume> <pages> pp 23-53. </pages>
Reference-contexts: Our study deals with abstractions that are derived from the dependence distance vectors. But, abstraction such as the information given by array data flow analysis <ref> [9, 21] </ref> provides more precise information about dependencies than the dependence distance vectors. At that moment, these precise analyses are limited to static control programs.
Reference: 10. <author> P. Feautrier, </author> <title> "Some Efficient Solutions to the Affine Scheduling Problem, Part I, One-Dimensional Time", </title> <journal> Int. Journal of Parallel Programming, </journal> <volume> Vol 21, </volume> <year> 1992. </year>
Reference-contexts: At that moment, these precise analyses are limited to static control programs. However, when computed, they give information sufficient to decide the legality of transformations such as the privatization [21] or linear scheduling computation <ref> [10, 11] </ref> while other abstractions might not be able to reach the necessary accuracy. In this study we restricted our attention to abstractions that can be computed on any program. Abstractions providing more precise information than the dependence distance vectors have to be introduced to deal with those transformations. <p> In this study we restricted our attention to abstractions that can be computed on any program. Abstractions providing more precise information than the dependence distance vectors have to be introduced to deal with those transformations. Linear scheduling computation techniques <ref> [10, 11, 17] </ref> exploit the additional information provided by array data flow graphs (especially, the sources of the dependent iterations) to restructure the program as they compute the optimal scheduling. <p> <ref> [10, 11, 17] </ref> exploit the additional information provided by array data flow graphs (especially, the sources of the dependent iterations) to restructure the program as they compute the optimal scheduling. The linear schedules that can be computed from DC are limited to loop reordering transformations while affine scheduling obtained by [10, 11, 17] is more general and may be combination of loop skewing, loop peeling and statement reordering.
Reference: 11. <author> P. Feautrier, </author> <title> "Some Efficient Solutions to the Affine Scheduling Problem, Part II, Multi-Dimensional Time", </title> <journal> Int. Journal of Parallel Programming, </journal> <volume> Vol 21, </volume> <year> 1992. </year>
Reference-contexts: At that moment, these precise analyses are limited to static control programs. However, when computed, they give information sufficient to decide the legality of transformations such as the privatization [21] or linear scheduling computation <ref> [10, 11] </ref> while other abstractions might not be able to reach the necessary accuracy. In this study we restricted our attention to abstractions that can be computed on any program. Abstractions providing more precise information than the dependence distance vectors have to be introduced to deal with those transformations. <p> In this study we restricted our attention to abstractions that can be computed on any program. Abstractions providing more precise information than the dependence distance vectors have to be introduced to deal with those transformations. Linear scheduling computation techniques <ref> [10, 11, 17] </ref> exploit the additional information provided by array data flow graphs (especially, the sources of the dependent iterations) to restructure the program as they compute the optimal scheduling. <p> <ref> [10, 11, 17] </ref> exploit the additional information provided by array data flow graphs (especially, the sources of the dependent iterations) to restructure the program as they compute the optimal scheduling. The linear schedules that can be computed from DC are limited to loop reordering transformations while affine scheduling obtained by [10, 11, 17] is more general and may be combination of loop skewing, loop peeling and statement reordering.
Reference: 12. <author> F. Irigoin, P. Jouvelot, R. Triolet, </author> <title> "Semantical Interprocedural Parallelization: an Overview of the PIPS Project", </title> <booktitle> In ACM International Conference on Supercomputing, ICS'91, </booktitle> <address> Cologne, Allemagne, </address> <month> 16-21 Juin </month> <year> 1991. </year>
Reference-contexts: Associated to other simple tests, these experiments conclude that expensive tests in a dependence testing context are not often executed on real applications. Experiments carried out with "expensive" dependence tests and dependence representations showed that their relative impact on the compiler speed is small <ref> [12] </ref>. 5 Minimal Abstraction and Transformation Dependence abstractions and loop transformations are linked. The dependences are used to test the legality of a transformation when it changes the program dependences.
Reference: 13. <author> F. Irigoin, </author> <title> "Loop Reordering With Dependence Direction Vectors", In Journees Firtech Systemes et Telematique Architectures Futures: Programmation parallele et integration VLSI, </title> <address> Paris, </address> <month> Nov. </month> <year> 1988. </year>
Reference-contexts: Since a DDV is also a polyhedron, it can be rewritten by using a generating system <ref> [13] </ref>. HDV, the set of valid linear schedules corresponding to DDV, can be computed similarly as HC. DO I = 1, N S1: V (I) = W (I+J) ENDDO ENDDO Fig. 18.
Reference: 14. <author> F. Irigoin, R. Triolet, </author> <title> "Computing Dependence Direction Vectors and Dependence Cones with Linear Systems", </title> <institution> Rap.Int. CAI87E94 Ecole des Mines de Paris, </institution>
Reference-contexts: Dependence relations are represented by finite dependence abstractions that contain the data flow dependences of the statement set. Many dependence abstractions have already been proposed, such as the Dependence Distance [23], the Dependence Direction Vector [30], the Dependence Level [1] or the Dependence Cone <ref> [14] </ref>. These different abstractions have different precisions. They are presented in Sections 2 and 3. Depending on their precisions, the abstractions contain either not enough or sufficient or too much information to decide if a transformation T can be legally ? E-mail: fyang,ancourt,irigoing@cri.ensmp.fr applied or not. <p> DP and DC are easier to use than D (L) for computing loop partitioning [15]. Abstractions DP and DC can be automatically computed and accurately so when array subscript expressions are affine. Algorithms for computing DP (L) and DC (L) are described in <ref> [14, 33] </ref>. They use techniques as the simplex or the Fourier-Motzkin algorithm that have an exponential time complexity. <p> The definitions of union for the different abstractions are given in <ref> [14, 32, 33] </ref>.
Reference: 15. <author> F. Irigoin, R. Triolet, </author> <title> "Supernode Partitioning", </title> <booktitle> In Conference Record of Fifteenth ACM Symposium on Principles of Programming Languages, </booktitle> <year> 1988. </year>
Reference-contexts: The memory space gain is notable especially when the dependence set is not finite or is large with non-uniform dependences. DP and DC are easier to use than D (L) for computing loop partitioning <ref> [15] </ref>. Abstractions DP and DC can be automatically computed and accurately so when array subscript expressions are affine. Algorithms for computing DP (L) and DC (L) are described in [14, 33]. They use techniques as the simplex or the Fourier-Motzkin algorithm that have an exponential time complexity. <p> The test of legality associated with the minimal abstraction is: legal (TU M ; L) () ^ 1ik (M fi DC i 0) with DC (L) = ([ 1ik DC i ). 5.4 Partitioning A partitioning transformation <ref> [18, 19, 15] </ref> Part H applied on an n-dimensional loop nest splits the iteration space into blocks of some regular shape by using p families of parallel hyperplanes. <p> This transforms the iteration space I n into a new one I p+n . Two iterations i 1 and i 2 of the initial iteration space belong to the same partitioned block if <ref> [15] </ref>: (b ~ h 1 fi ~ i 1 c; b ~ h 2 fi ~ i 1 c; :::; b ~ h n fi ~ i 1 c) = (b ~ h 1 fi ~ i 2 c; b ~ h 2 fi ~ i 2 c; :::; b ~ <p> 2 fi ~ i 1 c; :::; b ~ h n fi ~ i 1 c) = (b ~ h 1 fi ~ i 2 c; b ~ h 2 fi ~ i 2 c; :::; b ~ h n fi ~ i 2 c) According to the results of <ref> [15] </ref>, performing a partitioning H is legal if the following condition is fulfilled: (1) 8 ~ d 2 D (L) H ~ d ~ 0 For an abstraction A different from D, the polyhedron characterizing the depen-dences represented in A is noted P A . <p> The test of legality associated with the minimal abstraction is: (^ 1ik (H DC i ~ 0)) _ (^ 1ik (H DC i ~ 0)) =) legal (Part H ; L) In using DC, Irigoin and Triolet detail in <ref> [15] </ref> the algorithm for computing hyper--plane partitioning, which is a generalization of strip-mining with no restriction on the stripping direction. 5.5 Parallelization Performing a parallelization Paral on a loop nest L along the parallelizing vector ! pv transforms each loop i with ! pv (i) = 0 in a parallel loop.
Reference: 16. <author> F. Irigoin, R. Triolet, </author> <title> "Dependence Approximation and Global Parallel Code Gen--eration for Nested Loops", </title> <booktitle> In International Workshop Parallel and Distributed Algorithms, </booktitle> <address> Bonas, France, </address> <month> Oct. </month> <year> 1988. </year>
Reference-contexts: Since P k ~ h dc (min i ( ~ h ~ d i )) 1. =) ~ h 2 HC. Thus we have HC = H.fl The set of valid linear schedules H (or HC) can be represented by a polyhedron <ref> [16] </ref>. Definition 9 The set of valid linear schedules H is a polyhedron.
Reference: 17. <author> W. Kelly, W. Pugh, </author> <title> "Finding Legal Reordering transformations using Mappings", </title> <booktitle> In Seventh Annual Workshop Languages and Compilers for Parallel Computing, </booktitle> <address> Ithaca, New York, </address> <month> August </month> <year> 8-10,1994. </year>
Reference-contexts: In this study we restricted our attention to abstractions that can be computed on any program. Abstractions providing more precise information than the dependence distance vectors have to be introduced to deal with those transformations. Linear scheduling computation techniques <ref> [10, 11, 17] </ref> exploit the additional information provided by array data flow graphs (especially, the sources of the dependent iterations) to restructure the program as they compute the optimal scheduling. <p> <ref> [10, 11, 17] </ref> exploit the additional information provided by array data flow graphs (especially, the sources of the dependent iterations) to restructure the program as they compute the optimal scheduling. The linear schedules that can be computed from DC are limited to loop reordering transformations while affine scheduling obtained by [10, 11, 17] is more general and may be combination of loop skewing, loop peeling and statement reordering.
Reference: 18. <author> R. Karp, R. Miller and S. Winograd, </author> <title> "The Organization of Computations for Uniform Recurrence Equations", </title> <journal> Journal of the ACM, v. </journal> <volume> 14, </volume> <editor> n. </editor> <volume> 3, </volume> <pages> pp. 563-590, </pages> <year> 1967 </year>
Reference-contexts: The test of legality associated with the minimal abstraction is: legal (TU M ; L) () ^ 1ik (M fi DC i 0) with DC (L) = ([ 1ik DC i ). 5.4 Partitioning A partitioning transformation <ref> [18, 19, 15] </ref> Part H applied on an n-dimensional loop nest splits the iteration space into blocks of some regular shape by using p families of parallel hyperplanes.
Reference: 19. <author> L. Lamport, </author> <title> "The Parallel Execution of DO Loops", </title> <journal> Communications of the ACM 17(2), </journal> <pages> pp. 83-93, </pages> <year> 1974 </year>
Reference-contexts: The test of legality associated with the minimal abstraction is: legal (TU M ; L) () ^ 1ik (M fi DC i 0) with DC (L) = ([ 1ik DC i ). 5.4 Partitioning A partitioning transformation <ref> [18, 19, 15] </ref> Part H applied on an n-dimensional loop nest splits the iteration space into blocks of some regular shape by using p families of parallel hyperplanes.
Reference: 20. <author> W. Li, K. Pingali, </author> <title> "A singular loop transformation framework based on non-singular matrices", </title> <booktitle> In Languages and Compilers for Parallel Computing, </booktitle> <institution> Yale University, </institution> <month> August </month> <year> 1992. </year>
Reference-contexts: As for restructuring transformations, when an abstraction A different from DI is used, a definition using dependences in function of f DI A is derived from Definition 4. Unimodular Transformation A unimodular transformation <ref> [3, 29, 20] </ref> is a one-to-one mapping between two iteration sets I n and I 0 n having the same dimension. It corresponds to a unimodular change of basis M with ~ I 0 = M fi ~ I.
Reference: 21. <author> D. Maydan, S. Amarasinghe, M.Lam, </author> <title> "Array Data Flow Analysis and its use in Array Privatization", </title> <type> Stanford Report, </type> <year> 1993 </year>
Reference-contexts: Our study deals with abstractions that are derived from the dependence distance vectors. But, abstraction such as the information given by array data flow analysis <ref> [9, 21] </ref> provides more precise information about dependencies than the dependence distance vectors. At that moment, these precise analyses are limited to static control programs. <p> At that moment, these precise analyses are limited to static control programs. However, when computed, they give information sufficient to decide the legality of transformations such as the privatization <ref> [21] </ref> or linear scheduling computation [10, 11] while other abstractions might not be able to reach the necessary accuracy. In this study we restricted our attention to abstractions that can be computed on any program.
Reference: 22. <author> Dror E. Maydan, John L. Hennessy, Monica S. Lam, </author> " <title> Efficient and Exact Dependence Analysis ", ACM Sigplan PLDI'91 , Toronto, </title> <address> Ontario, Canada, </address> <month> June </month> <year> 1991 </year>
Reference-contexts: In order to obtain good accuracy, many satisfiability tests combine several simple tests. Tests having polynomial complexity and quickly solving particular cases are applied first. Accurate tests dealing with integer linear system satisfiability are applied last <ref> [22, 24, 33] </ref>. The individual tests are combined in order to find as soon as possible whether the dependence system does not have a solution.
Reference: 23. <author> Y. Muraoka, </author> <title> "Parallelism Exposure and Exploitation in Programs", </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, University of Illinois at Urbana-champaign, </institution> <month> Feb. </month> <year> 1971. </year>
Reference-contexts: Many transformations require more than simple dependent/independent information to be legally applied to a set of statements. Dependence relations are represented by finite dependence abstractions that contain the data flow dependences of the statement set. Many dependence abstractions have already been proposed, such as the Dependence Distance <ref> [23] </ref>, the Dependence Direction Vector [30], the Dependence Level [1] or the Dependence Cone [14]. These different abstractions have different precisions. They are presented in Sections 2 and 3.
Reference: 24. <author> William Pugh, </author> <title> "A Practical Algorithm for Exact Array Dependence Analysis", </title> <journal> Communications of the ACM , August 1992, pp.102-114 </journal>
Reference-contexts: Abstractions DP and DC can be automatically computed and accurately so when array subscript expressions are affine. Algorithms for computing DP (L) and DC (L) are described in [14, 33]. They use techniques as the simplex or the Fourier-Motzkin algorithm that have an exponential time complexity. However, as remark <ref> [24, 33] </ref>, these techniques have shown a polynomial behavior in practice in this context. 2.5 The Dependence Direction Vector Abstractions DP (L) and DC (L) represent approximated sets of D (L). These three abstractions contain the information useful for legally applying transformations that reorder iteration sets. <p> In order to obtain good accuracy, many satisfiability tests combine several simple tests. Tests having polynomial complexity and quickly solving particular cases are applied first. Accurate tests dealing with integer linear system satisfiability are applied last <ref> [22, 24, 33] </ref>. The individual tests are combined in order to find as soon as possible whether the dependence system does not have a solution.
Reference: 25. <author> V. Sarkar, R. Thekkath, </author> <title> "A General Framework for Iteration-Reordering Loop Transformations", </title> <booktitle> In Programming Language Design and Implementation, </booktitle> <address> San Francisco, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: A table gives the valid dependence abstraction supporting each considered transformation. However, some important abstractions, such as the dependence cone, and some advanced transformations, such as unimodular transformations and loop partitioning, were not considered. V. Sarkar & R. Thekkath <ref> [25] </ref> developed a general framework for applying reordering transformations. They used the dependence vector abstraction whose element is either a distance value, in case of constant dependence, or a direction value, in the other cases.
Reference: 26. <author> A. Schrijver, </author> <title> Theory of Linear and Integer Programming, </title> <publisher> John Wiley & Sons 1986. </publisher>
Reference-contexts: It constrains the set of integer points of the D (L)'s convex hull. DP (L) = f~v = 1 k X i = 1g The polyhedron DP (L) can be concisely described by its generating system <ref> [26] </ref>, which is a triplet made of three sets of vertices, rays and lines (f~v i g; f ~r j g; f ~ l k g). Even if DP (L) approximates D (L), it keeps all the information useful to legally apply reordering transformations as abstraction D. <p> DC for Program 1 The main advantage of Abstractions DP and DC is that, in many cases, only one structure (a polyhedron that is either a set of linear constraints or a generating system <ref> [26] </ref>) is necessary to the representation of information contained in D (L). The memory space gain is notable especially when the dependence set is not finite or is large with non-uniform dependences. DP and DC are easier to use than D (L) for computing loop partitioning [15].
Reference: 27. <author> W. Shang, J. A. B. Fortes, </author> <title> "Time Optimal Linear Schedules for Algorithms with Uniform Dependencies", </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. 40, No. 6, </volume> <month> June </month> <year> 1991. </year>
Reference-contexts: The dependence abstraction is used for computing a valid linear schedule. Most of the algorithms suggested in the literature <ref> [6, 27, 7, 8] </ref> require that the dependences be uniform or are approximated by dependence distance vectors. This condition is too restrictive for real programs.
Reference: 28. <author> M.E. Wolf, </author> <title> M.S. Lam, "Maximizing Parallelism via Loop Transformations", </title> <booktitle> In Programming Languages and Compilers for Parallel Computing, </booktitle> <month> Aug. </month> <pages> 1-3, </pages> <year> 1990. </year>
Reference-contexts: These mapping rules were developed from the dependence vector abstraction. However, the dependence vector does not contain sufficient information to legally apply advanced transformations, such as unimodular transformations and loop tiling, without some risks of losing precision. M. E. Wolf & M. S. Lam <ref> [28] </ref> introduced a new type of dependence vector for applying unimodular transformations and loop tiling. In their definition, each component d i of the dependence vector can be an infinite range of integers, represented by [d min i ; d max i ].
Reference: 29. <author> M.E. Wolf, </author> <title> M.S. Lam, "A Loop Transformation Theory and an Algorithm to Maximize Parallelism", </title> <journal> Transactions on Parallel and Distributed Systems, </journal> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: As for restructuring transformations, when an abstraction A different from DI is used, a definition using dependences in function of f DI A is derived from Definition 4. Unimodular Transformation A unimodular transformation <ref> [3, 29, 20] </ref> is a one-to-one mapping between two iteration sets I n and I 0 n having the same dimension. It corresponds to a unimodular change of basis M with ~ I 0 = M fi ~ I.
Reference: 30. <author> M. Wolfe, </author> <title> "Optimizing Supercompilers for Supercomputers", </title> <type> PhD Thesis, </type> <institution> Dept. of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> October </month> <year> 1982. </year>
Reference-contexts: Dependence relations are represented by finite dependence abstractions that contain the data flow dependences of the statement set. Many dependence abstractions have already been proposed, such as the Dependence Distance [23], the Dependence Direction Vector <ref> [30] </ref>, the Dependence Level [1] or the Dependence Cone [14]. These different abstractions have different precisions. They are presented in Sections 2 and 3. <p> These three abstractions contain the information useful for legally applying transformations that reorder iteration sets. However, transformations such as loop interchange or permutation, that only modify the sign or the order of the dependence distance vector, do not need the actual distance information. So, abstractions like DDV (L) <ref> [30] </ref> and DL (L) [1] relating only the sign or the level of dependence vectors have been suggested. Each Dependence Direction Vector element is one of f&lt; ; =; &gt;g. Other elements like ; ; fl may be used to summarize two or three DDV's. <p> Proof: This proof is similar to the previous one. fl 7 Related Work Most dependence abstractions have originally been developed for particular transformations. Like these examples, the dependence direction vector was defined by M. Wolfe for loop interchanging <ref> [30] </ref> and the dependence level by Allen & Kennedy for the vectorization and the parallelization of programs [1]. The dependence direction vector has been the most popular dependence abstraction because it has been successfully used for some important transformations such as loop interchanging and loop permutation.
Reference: 31. <author> M. Wolfe, </author> <title> "Experiences with Data Dependence and Loop Restructuring in the Tiny Research Tool", </title> <type> Technical Report, No. CS/E 90-016, </type> <month> Sep. </month> <year> 1990. </year>
Reference-contexts: Most compiler systems have implemented distance and direction vector abstractions. A lot of work has been done on developing more advanced transformations but little effort has been spent on studying the valid and minimal dependence abstraction for a loop transformation. In <ref> [31] </ref>, M. Wolfe has posed a similar question, "What information is needed to decide when a transformation is legal?". A review of the related work on loop transformations and dependence abstractions, including some particular dependence information such as crossing threshold and cross-direction, is presented.
Reference: 32. <author> M. Wolfe, </author> <title> "Experiences with Data Dependence Abstractions", </title> <booktitle> In Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <address> Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: The definitions of union for the different abstractions are given in <ref> [14, 32, 33] </ref>.
Reference: 33. <author> Y.Q. Yang, </author> <title> "Tests de Dependance et Transformations de programme", PhD of University Pierre et Marie Curie , November 93. This article was processed using the L a T E X macro package with LLNCS style </title>
Reference-contexts: DP and DC are easier to use than D (L) for computing loop partitioning [15]. Abstractions DP and DC can be automatically computed and accurately so when array subscript expressions are affine. Algorithms for computing DP (L) and DC (L) are described in <ref> [14, 33] </ref>. They use techniques as the simplex or the Fourier-Motzkin algorithm that have an exponential time complexity. <p> Abstractions DP and DC can be automatically computed and accurately so when array subscript expressions are affine. Algorithms for computing DP (L) and DC (L) are described in [14, 33]. They use techniques as the simplex or the Fourier-Motzkin algorithm that have an exponential time complexity. However, as remark <ref> [24, 33] </ref>, these techniques have shown a polynomial behavior in practice in this context. 2.5 The Dependence Direction Vector Abstractions DP (L) and DC (L) represent approximated sets of D (L). These three abstractions contain the information useful for legally applying transformations that reorder iteration sets. <p> This cannot be decided in general and Definitions 2, 3, 4, 5 are based on the fact that the new execution of statement instances must maintain all the loop nest lexico-positive dependences [4]. In 3 Proof is given in <ref> [33] </ref> contrived cases, these definitions are too pessimistic and conclude that a trans-formation cannot be applied although it is semantically legal. For instance, the transposition of a symmetric matrix induces dependence relations that do not change the program semantics, even though they are not maintained by the transformation. <p> In order to obtain good accuracy, many satisfiability tests combine several simple tests. Tests having polynomial complexity and quickly solving particular cases are applied first. Accurate tests dealing with integer linear system satisfiability are applied last <ref> [22, 24, 33] </ref>. The individual tests are combined in order to find as soon as possible whether the dependence system does not have a solution. <p> Accurate tests dealing with integer linear system satisfiability are applied last [22, 24, 33]. The individual tests are combined in order to find as soon as possible whether the dependence system does not have a solution. Experiments made on the Perfect Club programs <ref> [33] </ref> show that the simple test checking the equality of (numerical and symbolic) constant terms in the equations of dependence systems has a very low cost. It still detects that 76% of the dependence systems have no solution. <p> The theoretical proofs of minimality of abstractions associated with these transformations are given in <ref> [33] </ref>. Each proof proceeds in two steps: (1) proves that the abstraction is valid for the transformation and (2) according to Definition 7, gives a counter example which shows that an abstraction less precise than the minimal abstraction does not contain enough information to decide the transformation legality. <p> The first step of the proof of minimality for the loop reversal transformation is presented in the next section. The other loop transformation proofs follow the same scheme and are given in <ref> [33] </ref>. A counter example illustrating the second step is presented for each transformation. 5.1 Loop Reversal A loop reversal transformation Inv k (l 1 ; l 2 ; :::; l n ) applied on a n-dimensional loop nest reverses the execution order of loop l k . <p> The definitions of union for the different abstractions are given in <ref> [14, 32, 33] </ref>.
References-found: 33

