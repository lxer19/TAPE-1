URL: http://wwwsyseng.anu.edu.au/~jon/papers/acnn98.ps.gz
Refering-URL: http://www.ai.univie.ac.at/~juffi/lig/lig-bib.html
Root-URL: 
Email: fJon.Baxter,Andrew.Tridgell,Lex.Weaverg@anu.edu.au  
Title: TDLeaf(): Combining Temporal Difference learning with game-tree search.  
Author: Jonathan Baxter Andrew Tridgell Lex Weaver 
Address: Canberra 0200, Australia Canberra 0200, Australia  
Affiliation: Department of Systems Engineering Department of Computer Science Australian National University Australian National University  
Abstract: In this paper we present TDLeaf(), a variation on the TD() algorithm that enables it to be used in conjunction with minimax search. We present some experiments in both chess and backgammon which demonstrate its utility and provide comparisons with TD() and another less radical variant, TD-directed(). In particular, our chess program, KnightCap, used TDLeaf() to learn its evaluation function while playing on the Free Internet Chess Server (FICS, fics.onenet.net). It improved from a 1650 rating to a 2100 rating in just 308 games. We discuss some of the reasons for this success and the relationship between our results and Tesauro's results in backgammon. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D P Bertsekas and J N Tsitsiklis. </author> <title> Neuro-Dynamic Programming. </title> <publisher> Athena Scientific, </publisher> <year> 1996. </year>
Reference-contexts: Unfortunately, there is no such guarantee if ~ J (; w) is non-linear [8], or if a (x t ) depends on w <ref> [1] </ref>. 3. Minimax Search and TD () For games, any action a taken in state x will lead to a predetermined state which we will denote by x 0 a .
Reference: [2] <author> Jordan Pollack, Alan Blair, and Mark Land. </author> <title> Coevolution of a Backgammon Player. </title> <booktitle> In Proceedings of the Fifth Artificial Life Conference, </booktitle> <address> Nara, Japan, </address> <year> 1996. </year>
Reference-contexts: Perhaps the most remarkable success of TD () is Tesauro's TD-Gammon, a neural network backgammon player that has proven itself to be competitive with the best human backgammon players [6]. Many authors have discussed the peculiarities of backgammon that make it particularly suitable for Temporal Difference learning with self-play <ref> [5, 3, 2] </ref>.
Reference: [3] <author> Nicol Schraudolph, Peter Dayan, and Terrence Se-jnowski. </author> <title> Temporal Difference Learning of Position Evaluation in the Game of Go. </title> <editor> In Jack Cowan, Gerry Tesauro, and Josh Alspector, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <address> San Fransisco, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Perhaps the most remarkable success of TD () is Tesauro's TD-Gammon, a neural network backgammon player that has proven itself to be competitive with the best human backgammon players [6]. Many authors have discussed the peculiarities of backgammon that make it particularly suitable for Temporal Difference learning with self-play <ref> [5, 3, 2] </ref>. <p> In contrast, finding a representation for chess, othello or Go which allows a small neural network to order moves at one-ply with near human performance is a far more difficult task <ref> [7, 9, 3] </ref>. For these games, reliable tactical evaluation is difficult to achieve without deep search. This requires an exponential increase in the number of positions evaluated as the search depth increases.
Reference: [4] <author> Richard Sutton. </author> <title> Learning to Predict by the Method of Temporal Differences. </title> <journal> Machine Learning, </journal> <volume> 3:9 44, </volume> <year> 1988. </year>
Reference-contexts: 1. Introduction Temporal Difference learning or TD (), first introduced by Sutton <ref> [4] </ref>, is an elegant algorithm for approximating the expected long term future cost (or cost-to-go) of a stochastic dynamical system as a function of the current state. The mapping from states to future cost is implemented by a parameterised function approximator such as a neural network.
Reference: [5] <author> Gerald Tesauro. </author> <title> Practical Issues in Temporal Difference Learning. </title> <booktitle> Machine Learning, </booktitle> <address> 8:257278, </address> <year> 1992. </year>
Reference-contexts: Perhaps the most remarkable success of TD () is Tesauro's TD-Gammon, a neural network backgammon player that has proven itself to be competitive with the best human backgammon players [6]. Many authors have discussed the peculiarities of backgammon that make it particularly suitable for Temporal Difference learning with self-play <ref> [5, 3, 2] </ref>. <p> Experiments with Backgammon For our backgammon experiments we were fortunate to have Mark Land (University of California, San Diego) provide us with the source code for his LGammon program which has been implemented along the lines of Tesauro's TD-Gammon <ref> [5, 6] </ref>. Along with the code for LGammon, Land also provided a set of weights for the neural network.
Reference: [6] <author> Gerald Tesauro. </author> <title> TD-Gammon, a self-teaching backgammon program, achieves master-level play. </title> <booktitle> Neural Computation, </booktitle> <address> 6:215219, </address> <year> 1994. </year>
Reference-contexts: Perhaps the most remarkable success of TD () is Tesauro's TD-Gammon, a neural network backgammon player that has proven itself to be competitive with the best human backgammon players <ref> [6] </ref>. Many authors have discussed the peculiarities of backgammon that make it particularly suitable for Temporal Difference learning with self-play [5, 3, 2]. <p> Additionally, this recognises that the single move look-ahead evaluation functions trained using TD () or TD-directed () are often used in conjunction with deeper minimax searches for post-training real world play <ref> [6] </ref>. To test the effectiveness of our variants, we incorporated them into our own chess program Knight-Cap 1 , which plays on the Free Internet Chess Server 1 Source code available at http://syseng.anu.edu.au/lsg/knightcap.html (FICS, fics.onenet.net) and the Internet Chess Club (ICC, chessclub.com). <p> Experiments with Backgammon For our backgammon experiments we were fortunate to have Mark Land (University of California, San Diego) provide us with the source code for his LGammon program which has been implemented along the lines of Tesauro's TD-Gammon <ref> [5, 6] </ref>. Along with the code for LGammon, Land also provided a set of weights for the neural network.
Reference: [7] <author> Sebastian Thrun. </author> <title> Learning to Play the Game of Chess. </title> <editor> In G Tesauro, D Touretzky, and T Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <address> San Fransisco, 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In contrast, finding a representation for chess, othello or Go which allows a small neural network to order moves at one-ply with near human performance is a far more difficult task <ref> [7, 9, 3] </ref>. For these games, reliable tactical evaluation is difficult to achieve without deep search. This requires an exponential increase in the number of positions evaluated as the search depth increases.
Reference: [8] <author> John N Tsitsikilis and Benjamin Van Roy. </author> <title> An Analysis of Temporal Difference Learning with Function Approximation. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 42(5):674690, </volume> <year> 1997. </year>
Reference-contexts: Provided the actions a (x t ) are independent of the parameter vector w, it can be shown that for linear ~ J (; w), the TD () algorithm converges to a near-optimal parameter vector <ref> [8] </ref>. Unfortunately, there is no such guarantee if ~ J (; w) is non-linear [8], or if a (x t ) depends on w [1]. 3. <p> Provided the actions a (x t ) are independent of the parameter vector w, it can be shown that for linear ~ J (; w), the TD () algorithm converges to a near-optimal parameter vector <ref> [8] </ref>. Unfortunately, there is no such guarantee if ~ J (; w) is non-linear [8], or if a (x t ) depends on w [1]. 3. Minimax Search and TD () For games, any action a taken in state x will lead to a predetermined state which we will denote by x 0 a . <p> No such improvement was observed for backgammon which suggests that the optimal network to use in 1-ply search is close to the optimal network for 2-ply search. On the theoretical side, it has recently been shown that TD () converges for linear evaluation functions <ref> [8] </ref>. An interesting avenue for further investigation would be to determine whether TDLeaf () has similar convergence properties.
Reference: [9] <author> Steven Walker, Raymond Lister, and Tom Downs. </author> <title> On Self-Learning Patterns in the Othello Board Game by the Method of Temporal Differences. </title> <editor> In C Rowles, H liu, and N Foo, editors, </editor> <booktitle> Proceedings of the 6th Australian Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 328333, </pages> <address> Melbourne, 1993. </address> <publisher> World Scientific. </publisher>
Reference-contexts: In contrast, finding a representation for chess, othello or Go which allows a small neural network to order moves at one-ply with near human performance is a far more difficult task <ref> [7, 9, 3] </ref>. For these games, reliable tactical evaluation is difficult to achieve without deep search. This requires an exponential increase in the number of positions evaluated as the search depth increases.
References-found: 9

