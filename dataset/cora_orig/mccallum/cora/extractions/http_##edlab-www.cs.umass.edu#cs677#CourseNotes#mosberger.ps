URL: http://edlab-www.cs.umass.edu/cs677/CourseNotes/mosberger.ps
Refering-URL: http://edlab-www.cs.umass.edu/cs677/useful.html
Root-URL: 
Title: Memory Consistency Models  
Author: David Mosberger 
Address: Tucson, AZ 85721  
Affiliation: Department of Computer Science The University of Arizona  
Date: TR 93/11  
Abstract: This paper discusses memory consistency models and their influence on software in the context of parallel machines. In the first part we review previous work on memory consistency models. The second part discusses the issues that arise due to weakening memory consistency. We are especially interested in the influence that weakened consistency models have on language, compiler, and runtime system design. We conclude that tighter interaction between those parts and the memory system might improve performance considerably. 
Abstract-found: 1
Intro-found: 1
Reference: [ABJ + 92] <author> Mustaque Ahamad, Rida Bazzi, Ranjit John, Prince Kohli, and Gil Neiger. </author> <title> The power of processor consistency. </title> <type> Technical Report GIT-CC-92/34, </type> <institution> Georgia Institute of Technology, </institution> <address> Atlanta, GA 30332-0280, USA, </address> <year> 1992. </year>
Reference-contexts: We do not give formal definitions for the presented models as they do not help much to understand a model's implications on the programming model. More formal descriptions can be found for example in Ahamad et al. <ref> [ABJ + 92] </ref> and Gharachorloo et al. [GLL + 90]. We first discuss uniform models and then hybrid models. Figure 2 gives an overview of the relationships among the uniform models. An arrow from model A to B indicates that A is more strict than B. <p> In essence, coherence removes the ordering constraints that program order imposes on accesses to different memory locations. 3.6 Processor Consistency (PC) Goodman proposed processor consistency in [Goo89]. Unfortunately, his definition is informal and caused a controversy as to what exactly PC refers to. Ahamad et al. <ref> [ABJ + 92] </ref> give a formal definition of PC which removes all ambiguity and appears to be a faithful translation of Goodman's definition.
Reference: [AH90] <author> Sarita Adve and Mark Hill. </author> <title> Weak ordering: A new definition. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-14, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Consistency models that distinguish access categories employ different ordering constraints depending on the access category. We therefore call such models hybrid. In contrast, models that do not distinguish access categories are called uniform. The motivation for hybrid models is engendered in Adve and Hill's definition for weak ordering <ref> [AH90] </ref>: Hardware is weakly ordered with respect to a synchronization model if and only if it ap pears sequentially consistent to all software that obey the synchronization model. That is, as long as the synchronization model is respected, the memory system appears to be sequentially consistent. <p> The synchronization model corresponding to these access order constraints is relatively simple. A program executing on a weakly consistent system appears sequentially consistent if the following two constraints are observed <ref> [AH90, ZB92] </ref>: 1. there are no data races (i.e., no competing accesses) 2. synchronization is visible to the memory system Note that WC does not allow for chaotic accesses as found in chaotic relaxation algorithms.
Reference: [AHJ91] <author> M. Ahamad, P. W. Hutto, and R. John. </author> <title> Implementing and programming causal distributed shared memory. </title> <booktitle> In Proceedings of the 11th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 274-281, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Clearly, no implementation will ever exhibit an execution with such a history. In general, it is often simpler to implement a slightly stricter model than its definition would require. This is especially true for hardware realizations of shared memories <ref> [AHJ91, GLL + 90] </ref>. For each consistency model there are a number of implementation issues.
Reference: [And91] <author> Gregory R. Andrews. </author> <title> Concurrent Programming: </title> <booktitle> Principles and Practice. </booktitle> <address> Ben-jamin/Cummings, Menlo Park, </address> <year> 1991. </year>
Reference-contexts: He also says that many existing multiprocessors (e.g., VAX 8800) satisfy PC, but not sequential consistency [Goo89]. Ahamad et al. prove that the Tie-Breaker algorithm executes correctly under PC while the Bakery algorithm does not (see <ref> [And91] </ref> for a description of those algorithms). Bershad and Zekauskas [BZ91] mention that processor consistent machines are easier to build than sequentially consistent systems. 3.7 Slow Memory Slow memory is a location relative weakening of PRAM [HA90]. <p> Another, more subtle, change is that special accesses are executed under PCD only (not under SC, as in WC). To make the model more concrete, we give an example of how a critical section and a coordinator barrier could be programmed under RC (see <ref> [And91] </ref>, for example). Below we show how a critical section could be implemented under this model: do test and set (locked) ! [rd :acquire;wr :nsync] skip od : : :critical section: : : locked := false [release] Note the labeling of the read-modify-write operation test and set ().
Reference: [BH90] <author> Lothar Borrmann and Martin Herdiecker-hoff. </author> <title> A coherency model for virtually shared memory. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <volume> volume II, </volume> <pages> pages 252-257, </pages> <year> 1990. </year>
Reference-contexts: The notion of eager versus lazy maintenance of memory consistency appears to have been invented independently by Borrmann/Herdieckerhoff <ref> [BH90] </ref> and Ber-shad/Zekauskas [BZ91]. This notion is based on the observation that the consistency protocol can either be invoked each time an inconsistency arises or only when an inconsistency could be detected. Eager implementations do the former, lazy the latter.
Reference: [BHG87] <author> Philip A. Bernstein, Vassos Hadzilacos, and Nathan Goodman. </author> <title> Concurrency Control and Recovery in Database Systems. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1987. </year>
Reference-contexts: This is equivalent to the one-copy serializability concept found in work on concurrency control for database systems <ref> [BHG87] </ref>. In a sequentially consistent system, all processors must agree on the order of observed effects.
Reference: [BZ91] <author> Brian N. Bershad and Matthew J. Zekauskas. Midway: </author> <title> Shared memory parallel programming with entry consistency for distributed memory multiprocessors. </title> <type> Technical Report CMU-CS-91-170, </type> <institution> Carnegie-Mellon University, </institution> <year> 1991. </year>
Reference-contexts: In the latter case it is usually called Distributed Shared Memory (DSM). At both levels work has been done to reap the benefits of weaker models. We conjecture that in the near future most parallel machines will be based on consistency models significantly weaker than SC <ref> [LLG + 92, Sit92, BZ91, CBZ91, KCZ92] </ref>. The rest of this paper is organized as follows. In section 2 we discuss issues characteristic to memory consistency models. In the following section we present several consistency models and their implications on the programming model. <p> He also says that many existing multiprocessors (e.g., VAX 8800) satisfy PC, but not sequential consistency [Goo89]. Ahamad et al. prove that the Tie-Breaker algorithm executes correctly under PC while the Bakery algorithm does not (see [And91] for a description of those algorithms). Bershad and Zekauskas <ref> [BZ91] </ref> mention that processor consistent machines are easier to build than sequentially consistent systems. 3.7 Slow Memory Slow memory is a location relative weakening of PRAM [HA90]. It requires that all processors agree on the order of observed writes to each location by a single processor. <p> Also, it is always safe to label a program conservatively. For example, if a compiler has incomplete information available, it could always revert to label reads with acquire and writes with release. 3.10 Entry Consistency (EC) The entry consistency model is even weaker than RC <ref> [BZ91] </ref>. However, it imposes more restrictions on the programming model. EC is like RC except that every shared variable needs to be associated with a synchronization variable. A synchronizing variable is either a lock or a barrier. <p> The notion of eager versus lazy maintenance of memory consistency appears to have been invented independently by Borrmann/Herdieckerhoff [BH90] and Ber-shad/Zekauskas <ref> [BZ91] </ref>. This notion is based on the observation that the consistency protocol can either be invoked each time an inconsistency arises or only when an inconsistency could be detected. Eager implementations do the former, lazy the latter. <p> The expected benefit of lazy implementations is that if a process has a cached copy of a shared variable but doesn't access it anymore, then this process does not have to participate in maintaining consistency for this variable. Lazy release consistency [KCZ92] and Midway <ref> [BZ91] </ref> are two examples of lazy implementations. No performance data is yet available. 5 Influence of Consistency Model on Software As mentioned earlier, choosing a memory consistency model is a tradeoff between increasing concurrency by decreasing ordering constraints and implementation and programming model complexity.
Reference: [CBZ91] <author> John B. Carter, John K. Bennett, and Willy Zwaenepoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Symposium on Operating System Principles, </booktitle> <pages> pages 152-164, </pages> <year> 1991. </year>
Reference-contexts: In the latter case it is usually called Distributed Shared Memory (DSM). At both levels work has been done to reap the benefits of weaker models. We conjecture that in the near future most parallel machines will be based on consistency models significantly weaker than SC <ref> [LLG + 92, Sit92, BZ91, CBZ91, KCZ92] </ref>. The rest of this paper is organized as follows. In section 2 we discuss issues characteristic to memory consistency models. In the following section we present several consistency models and their implications on the programming model. <p> However, it is possible and beneficial to go beyond that point. If the software can provide information on the expected access pattern to a shared variable, optimizations for each particular access pattern could be enabled resulting in substantially improved performance. Munin <ref> [CBZ91] </ref> does this by providing a fixed set of sharing annotations. Each annotation corresponds to a consistency protocol optimized for a particular access pattern. <p> Unfortunately, so far no performance study of the advantage of such guided memory systems has been reported. Carter <ref> [CBZ91] </ref> indicates that Munin performs well for matrix multiplication and SOR when compared to a hand-coded message passing algorithm, but no comparison with a single-protocol DSM or a strict DSM was reported.
Reference: [CKM92] <author> Shigeru Chiba, Kazuhiko Kato, and Takashi Masuda. </author> <title> Exploiting a weak consistency to implement distributed tuple space. </title> <booktitle> In Proceedings of the 12th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 416-423, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: Munin [CBZ91] does this by providing a fixed set of sharing annotations. Each annotation corresponds to a consistency protocol optimized for a particular access pattern. A similar approach was taken by Chiba et al. <ref> [CKM92] </ref> where they annotate Linda programs in order to select an optimized protocol 2 By explicitly concurrent language we mean a language in which it is possible to program synchronization operations. for in operations if they are used with certain restrictions.
Reference: [DSB86] <author> M. Dubois, C. Scheurich, and F. A. Briggs. </author> <title> Memory access buffering in multiprocessors. </title> <booktitle> In Proceedings of the Thirteenth Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 434-442, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: Slow memory does not appear to be of any practical significance. 3.8 Weak Consistency (WC) Weak consistency is the first and most strict hybrid model we discuss. The model was originally proposed by Dubois et al. <ref> [DSB86] </ref>.
Reference: [GGH91] <author> Kourosh Gharachorloo, Anoop Gupta, and John Hennessy. </author> <title> Performance evaluation of memory consistency models for shared memory multiprocessors. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 26(4) </volume> <pages> 245-257, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: None of these optimizations is possible with the strict SC model. Simulations have shown that weaker models allowing such optimizations could improve performance on the order of 10 to 40 percent over a strictly sequential model <ref> [GGH91, ZB92] </ref>. However, weakening the memory consistency model goes hand in hand with a change in the programming model. In general, the programming model becomes more restricted (and complicated) as the consistency model becomes weaker. <p> It is only natural to attempt to make this implicit weakening explicit in order to let the memory system take advantage too. In fact, it is anticipated that software could gain from a weak model to a much higher degree than hardware <ref> [GGH91] </ref> by enabling optimizations such as code scheduling or delaying updates that are not legal under SC. In short, weaker memory consistency models can have a positive effect on the performance of parallel shared memory machines. The benefit increases as memory latency increases.
Reference: [GLL + 90] <author> K. Gharachorloo, D. Lenoski, J. Laudon, Phillip Gibbons, Anoop Gupta, and John Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <journal> Computer Architecture News, </journal> <volume> 18(2) </volume> <pages> 15-26, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: The access category is a static property of accesses. A useful (but by no means the only possible) categorization is shown in Figure 1. It is an extension of the categorization used in <ref> [GLL + 90] </ref>. A memory access is either shared or private. Private accesses are easy to deal with, so we don't discuss them further. Shared accesses can be divided into competing and non-competing accesses. <p> We do not give formal definitions for the presented models as they do not help much to understand a model's implications on the programming model. More formal descriptions can be found for example in Ahamad et al. [ABJ + 92] and Gharachorloo et al. <ref> [GLL + 90] </ref>. We first discuss uniform models and then hybrid models. Figure 2 gives an overview of the relationships among the uniform models. An arrow from model A to B indicates that A is more strict than B. Each model is labeled with the subsection it is described in. <p> Thus, this would not be a legal history for CC. 3.5 Cache Consistency (Coherence) Cache consistency [Goo89] and coherence <ref> [GLL + 90] </ref> are synonymous and to avoid confusion with causal consistency, we will use the term coherence in this paper. Coherence is a location-relative weakening of SC. Recall that under SC, all processors have to agree on some sequential order of execution for all accesses. <p> Ahamad et al. [ABJ + 92] give a formal definition of PC which removes all ambiguity and appears to be a faithful translation of Goodman's definition. They also show that PC as defined by the DASH group in <ref> [GLL + 90] </ref> is not comparable to Goodman's definition (i.e., it is neither weaker nor stronger). <p> Such algorithms would either have to be changed to avoid data races or it would be necessary to mask chaotic accesses as synchronizing accesses. The latter would be overly restrictive. 3.9 Release Consistency (RC) Release consistency as defined by Gharachorloo et al. <ref> [GLL + 90] </ref> is a refinement of WC in the sense that competing accesses are divided into acquire, release, and non-synchronizing accesses. Competing accesses are also called special to distinguish them from non-competing, ordinary accesses. Non-synchronizing accesses are competing accesses that do not serve a synchronization purpose. <p> Clearly, no implementation will ever exhibit an execution with such a history. In general, it is often simpler to implement a slightly stricter model than its definition would require. This is especially true for hardware realizations of shared memories <ref> [AHJ91, GLL + 90] </ref>. For each consistency model there are a number of implementation issues.
Reference: [Goo89] <author> James R. Goodman. </author> <title> Cache consistency and sequential consistency. </title> <type> Technical Report 61, </type> <institution> SCI Committee, </institution> <month> March </month> <year> 1989. </year>
Reference-contexts: Thus, this would not be a legal history for CC. 3.5 Cache Consistency (Coherence) Cache consistency <ref> [Goo89] </ref> and coherence [GLL + 90] are synonymous and to avoid confusion with causal consistency, we will use the term coherence in this paper. Coherence is a location-relative weakening of SC. Recall that under SC, all processors have to agree on some sequential order of execution for all accesses. <p> The history is therefore coherent, but not SC. In essence, coherence removes the ordering constraints that program order imposes on accesses to different memory locations. 3.6 Processor Consistency (PC) Goodman proposed processor consistency in <ref> [Goo89] </ref>. Unfortunately, his definition is informal and caused a controversy as to what exactly PC refers to. Ahamad et al. [ABJ + 92] give a formal definition of PC which removes all ambiguity and appears to be a faithful translation of Goodman's definition. <p> The example given for coherence is also PC so we give here a history that fails to be PC (this and the previous example are from <ref> [Goo89] </ref>): 4 P 1 : W (x)1 W (c)1 R (y)0 Notice that P 1 observes accesses in the order: W (x)1; W (c)1; R (y)0; W (y)1; W (c)2; while P 2 observes accesses in the order: W (y)1; W (c)2; R (x)0; W (x)1; W (c)1: That is, P <p> The differences between PC and SC are subtle enough that Goodman claims most applications give the same results under these two models. He also says that many existing multiprocessors (e.g., VAX 8800) satisfy PC, but not sequential consistency <ref> [Goo89] </ref>. Ahamad et al. prove that the Tie-Breaker algorithm executes correctly under PC while the Bakery algorithm does not (see [And91] for a description of those algorithms).
Reference: [HA90] <author> P. W. Hutto and M. Ahamad. </author> <title> Slow memory: Weakening consistency to enhance concur-rency in distributed shared memories. </title> <booktitle> In Proceedings of the 10th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 302-311, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: One solution is to define read operations to take effect at read-begin time and write operations to take effect at write-end time. This is called static atomic consistency <ref> [HA90] </ref>. With dynamic AC, operations can take effect at any point in the operation interval, as long as the resulting history is equivalent to some serial execution. <p> Sequential consistency has been the canonical memory consistency model for a long time. However, many multi-processor machines actually implement a slightly weaker model called processor consistency (see below). 3.3 Causal Consistency Hutto and Ahamad <ref> [HA90] </ref> introduced causal consistency. Lamport [Lam78] defined the notion of potential causality to capture the flow of information in a distributed system. This notion can be applied to a memory system by interpreting a write as a message-send event and a read as a message-read event. <p> Bershad and Zekauskas [BZ91] mention that processor consistent machines are easier to build than sequentially consistent systems. 3.7 Slow Memory Slow memory is a location relative weakening of PRAM <ref> [HA90] </ref>. It requires that all processors agree on the order of observed writes to each location by a single processor. Furthermore, local writes must be visible immediately (as in the PRAM model). The name for this model was chosen because writes propagate slowly through the system. <p> The name for this model was chosen because writes propagate slowly through the system. Slow memory is probably one of the weakest uniform consistency models that can still be used for interprocess communication. Hutto and Ahamad present a mutual exclusion algorithm in <ref> [HA90] </ref>. However, this algorithm guarantees physical exclusion only. There is no guarantee of logical exclusion.
Reference: [KCZ92] <author> P. Keleher, A. L. Cox, and W. Zwaenepoel. </author> <title> Lazy release consistency for software distributed shared memory. </title> <journal> SIGARCH Computer Architecture News, </journal> <volume> 20(2), </volume> <month> May </month> <year> 1992. </year>
Reference-contexts: In the latter case it is usually called Distributed Shared Memory (DSM). At both levels work has been done to reap the benefits of weaker models. We conjecture that in the near future most parallel machines will be based on consistency models significantly weaker than SC <ref> [LLG + 92, Sit92, BZ91, CBZ91, KCZ92] </ref>. The rest of this paper is organized as follows. In section 2 we discuss issues characteristic to memory consistency models. In the following section we present several consistency models and their implications on the programming model. <p> Eager implementations do the former, lazy the latter. The expected benefit of lazy implementations is that if a process has a cached copy of a shared variable but doesn't access it anymore, then this process does not have to participate in maintaining consistency for this variable. Lazy release consistency <ref> [KCZ92] </ref> and Midway [BZ91] are two examples of lazy implementations. No performance data is yet available. 5 Influence of Consistency Model on Software As mentioned earlier, choosing a memory consistency model is a tradeoff between increasing concurrency by decreasing ordering constraints and implementation and programming model complexity.
Reference: [Lam78] <author> Leslie Lamport. </author> <title> Time, clocks, and the ordering of events in a distributed system. </title> <journal> Communications of the ACM, </journal> <volume> 21(7) </volume> <pages> 558-565, </pages> <year> 1978. </year>
Reference-contexts: distinguish are listed below: * location of access * direction of access (read, write, or both) * value transmitted in access * causality of access 1 * category of access The causality attribute is a relation that tells if two accesses a 1 and a 2 are (potentially) causally related <ref> [Lam78] </ref> and if so, whether a 1 occurred before a 2 or vice versa. The access category is a static property of accesses. A useful (but by no means the only possible) categorization is shown in Figure 1. It is an extension of the categorization used in [GLL + 90]. <p> Sequential consistency has been the canonical memory consistency model for a long time. However, many multi-processor machines actually implement a slightly weaker model called processor consistency (see below). 3.3 Causal Consistency Hutto and Ahamad [HA90] introduced causal consistency. Lamport <ref> [Lam78] </ref> defined the notion of potential causality to capture the flow of information in a distributed system. This notion can be applied to a memory system by interpreting a write as a message-send event and a read as a message-read event.
Reference: [Lam79] <author> Leslie Lamport. </author> <title> How to make a multiprocessor computer that correctly executes multiprocess programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9):690-691, </volume> <month> Septem-ber </month> <year> 1979. </year>
Reference-contexts: Atomic consistency is often used as a base model when evaluating the performance of an MCM. 3.2 Sequential Consistency (SC) Sequential consistency was first defined by Lamport in 1979 <ref> [Lam79] </ref>.
Reference: [LLG + 92] <author> D. Lenoski, J. Laudon, K. Gharachor-loo, W.-D. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. S. Lam. </author> <title> The Stan-ford Dash multiprocessor. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: In the latter case it is usually called Distributed Shared Memory (DSM). At both levels work has been done to reap the benefits of weaker models. We conjecture that in the near future most parallel machines will be based on consistency models significantly weaker than SC <ref> [LLG + 92, Sit92, BZ91, CBZ91, KCZ92] </ref>. The rest of this paper is organized as follows. In section 2 we discuss issues characteristic to memory consistency models. In the following section we present several consistency models and their implications on the programming model.
Reference: [LS88] <author> R. J. Lipton and J. S. Sandberg. </author> <title> PRAM: A scalable shared memory. </title> <type> Technical Report CS-TR-180-88, </type> <institution> Princeton University, </institution> <month> September </month> <year> 1988. </year> <month> 9 </month>
Reference-contexts: However, this does not imply that a CC implementation necessarily performs worse than an implementation of one of the simpler uniform models. 3.4 Pipelined RAM (PRAM) Lipton and Sandberg <ref> [LS88] </ref> defined the Pipelined RAM (PRAM) consistency model. The reader should be aware that the acronym PRAM is often used as a shorthand for Parallel Random Access Machine which has nothing in common with the Pipelined RAM consistency model.
Reference: [Mos93] <author> David Mosberger. </author> <title> Memory consistency models. </title> <journal> Operating Systems Review, </journal> <volume> 17(1) </volume> <pages> 18-26, </pages> <month> January </month> <year> 1993. </year>
Reference: [Rin92] <author> Martin Rinard, </author> <month> September </month> <year> 1992. </year> <type> Personal communication. </type>
Reference-contexts: That is, if accesses to variable x are unsynchronized, then reading x must not return any previously written value but a recent one. For example, the LocusRoute application of the SPLASH benchmark does not perform well if non-synchronizing competing accesses return very old values <ref> [Rin92, SWG91] </ref>. RC maintains such accesses under PCD (which is safe but conservative in many cases).
Reference: [RSL92] <author> Martin C. Rinard, Daniel J. Scales, and Monica S. Lam. </author> <title> Jade: A high-level, machine-independent language for parallel programming. </title> <month> September </month> <year> 1992. </year>
Reference-contexts: As mentioned above, it is still advantageous to integrate the consistency model with the compiler and runtime system more tightly. As the compiler already has information on synchronization and the concurrency structure of the program, it might as well make this information available to the memory system. Jade <ref> [RSL92] </ref> is a step in this direction. Its runtime system has for each process precise information on the accessed locations and whether a location is only read or also modified. The language also allows one to express that some data will not be accessed anymore in the future.
Reference: [Sit92] <author> Richard L. </author> <title> Sites, editor. Alpha Architecture Reference Manual. </title> <publisher> Digital Press, </publisher> <address> Burling-ton, MA, </address> <year> 1992. </year>
Reference-contexts: In the latter case it is usually called Distributed Shared Memory (DSM). At both levels work has been done to reap the benefits of weaker models. We conjecture that in the near future most parallel machines will be based on consistency models significantly weaker than SC <ref> [LLG + 92, Sit92, BZ91, CBZ91, KCZ92] </ref>. The rest of this paper is organized as follows. In section 2 we discuss issues characteristic to memory consistency models. In the following section we present several consistency models and their implications on the programming model.
Reference: [SWG91] <author> Jaswinder Pal Singh, Wolf-Dietrich Weber, and Anoop Gupta. </author> <title> Splash: Stanford parallel applications for shared-memory. </title> <type> Technical Report CSL-TR-91-469, </type> <institution> Stanford University, </institution> <year> 1991. </year>
Reference-contexts: That is, if accesses to variable x are unsynchronized, then reading x must not return any previously written value but a recent one. For example, the LocusRoute application of the SPLASH benchmark does not perform well if non-synchronizing competing accesses return very old values <ref> [Rin92, SWG91] </ref>. RC maintains such accesses under PCD (which is safe but conservative in many cases).
Reference: [ZB92] <author> R. N. Zucker and J-L. Baer. </author> <title> A performance study of memory consistency models. </title> <journal> SIGARCH Computer Architecture News, </journal> <volume> 20(2), </volume> <month> May </month> <year> 1992. </year> <month> 10 </month>
Reference-contexts: None of these optimizations is possible with the strict SC model. Simulations have shown that weaker models allowing such optimizations could improve performance on the order of 10 to 40 percent over a strictly sequential model <ref> [GGH91, ZB92] </ref>. However, weakening the memory consistency model goes hand in hand with a change in the programming model. In general, the programming model becomes more restricted (and complicated) as the consistency model becomes weaker. <p> The synchronization model corresponding to these access order constraints is relatively simple. A program executing on a weakly consistent system appears sequentially consistent if the following two constraints are observed <ref> [AH90, ZB92] </ref>: 1. there are no data races (i.e., no competing accesses) 2. synchronization is visible to the memory system Note that WC does not allow for chaotic accesses as found in chaotic relaxation algorithms.
References-found: 25

