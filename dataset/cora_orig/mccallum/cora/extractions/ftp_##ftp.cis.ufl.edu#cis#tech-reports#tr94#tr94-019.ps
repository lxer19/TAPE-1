URL: ftp://ftp.cis.ufl.edu/cis/tech-reports/tr94/tr94-019.ps
Refering-URL: http://www.cis.ufl.edu/tech-reports/tech-reports/tr94-abstracts.html
Root-URL: http://www.cis.ufl.edu
Title: ON THE LU FACTORIZATION OF SEQUENCES OF IDENTICALLY STRUCTURED SPARSE MATRICES WITHIN A DISTRIBUTED MEMORY ENVIRONMENT  
Author: BY STEVEN MICHAEL HADFIELD 
Degree: A DISSERTATION PRESENTED TO THE GRADUATE SCHOOL OF THE UNIVERSITY OF FLORIDA IN PARTIAL FULFILLMENT OF THE REQUIREMENTS FOR THE DEGREE OF DOCTOR OF PHILOSOPHY  
Date: 1994  
Affiliation: UNIVERSITY OF FLORIDA  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> S. G. Akl. </author> <title> The Design and Analysis of Parallel Algorithms. </title> <publisher> Prentice-Hall, Inc., </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1989. </year>
Reference-contexts: This concept of independent instruction execution, when combined with the distribution of different data to each processor, is defined by another model that governs this implementation, the Multiple Instruction, Multiple Data (MIMD) model <ref> [1] </ref>. With this conceptual framework established, the rest of this section will describe the implementation of the primary functions of the parallel refactorization code.
Reference: [2] <author> S. Al-Bassam and H. El-Rewini. </author> <title> Processor allocation for hypercubes. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 16 </volume> <pages> 394-401, </pages> <year> 1992. </year>
Reference-contexts: This provides a great deal of flexibility in subcube allocations and extensions to the binary buddy system have been proposed to take advantage of this property <ref> [20, 2] </ref>. However, the added flexibility comes at a significant cost in terms of complexity of the allocation mechanisms. Furthermore, splitting the hypercube along multiple alternative dimensions can create complicated fragmentation scenarios that can limit availability of larger subcubes.
Reference: [3] <author> G. Alaghband. </author> <title> Parallel pivoting combined with parallel reduction and fill-in control. </title> <journal> Parallel Computing, </journal> <volume> 11 </volume> <pages> 201-221, </pages> <year> 1989. </year>
Reference: [4] <author> P. R. Amestoy. </author> <title> Factorization of large unsymmetric sparse matrices based on a multifrontal approach in a multiprocessor environment. </title> <type> PhD thesis, </type> <institution> L'Institut National Polytechnique de Toulouse, Toulouse, France, </institution> <month> November </month> <year> 1990. </year>
Reference-contexts: The classical multifrontal method avoids changes in the edge set of the assembly DAG because the assumption of symmetry results in an assembly DAG that is a tree in which each frontal matrix can be recovered by its single direct LU parent <ref> [7, 4] </ref>. Thus the size and required computations for frontal matrices can change (and hence the structure of the LU factors), but the edge set of the assembly DAG (tree) remains constant. Within the context of an unsymmetric multifrontal approach, lost pivot recovery becomes more difficult.
Reference: [5] <author> P. R. Amestoy, M. J. Dayde, and I. S. Duff. </author> <title> Use of Level-3 BLAS kernels in the solution of full and sparse linear equations. </title> <editor> In J.-L. Delhaye and E. Gelenbe, editors, </editor> <booktitle> High Performance Computing, </booktitle> <pages> pages 19-31. </pages> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1989. </year>
Reference-contexts: One of the major advantages of a multifrontal method is that the regularity found in the dense matrix operations can be used to take advantage of advanced architectural features such as vector processors, hierarchical memories, multiprocessors with shared memories or distributed memories connected by regular pattern communication networks <ref> [5, 7] </ref>. Furthermore, when frontal matrices do not overlap in their pivot rows and columns, the factorization steps associated with those pivots can be done concurrently [46]. This provides an additional degree of parallelism that can be 20 exploited by multiprocessors.
Reference: [6] <author> P. R. Amestoy and I. S. Duff. </author> <title> Efficient and portable implementation of a multi-frontal method on a range of MIMD computers. </title> <type> Technical report, </type> <institution> CERFACS, </institution> <year> 1989. </year>
Reference: [7] <author> P. R. Amestoy and I. S. Duff. </author> <title> Vectorization of a multiprocessor multifrontal code. </title> <journal> International Journal of Supercomputing Applications, </journal> <volume> 3 </volume> <pages> 41-59, </pages> <year> 1989. </year>
Reference-contexts: One of the major advantages of a multifrontal method is that the regularity found in the dense matrix operations can be used to take advantage of advanced architectural features such as vector processors, hierarchical memories, multiprocessors with shared memories or distributed memories connected by regular pattern communication networks <ref> [5, 7] </ref>. Furthermore, when frontal matrices do not overlap in their pivot rows and columns, the factorization steps associated with those pivots can be done concurrently [46]. This provides an additional degree of parallelism that can be 20 exploited by multiprocessors. <p> The classical multifrontal method avoids changes in the edge set of the assembly DAG because the assumption of symmetry results in an assembly DAG that is a tree in which each frontal matrix can be recovered by its single direct LU parent <ref> [7, 4] </ref>. Thus the size and required computations for frontal matrices can change (and hence the structure of the LU factors), but the edge set of the assembly DAG (tree) remains constant. Within the context of an unsymmetric multifrontal approach, lost pivot recovery becomes more difficult.
Reference: [8] <author> C. P. Arnold, M. I. Parr, and M. B. Dewe. </author> <title> An efficient parallel algorithm for the solution of large sparse linear matrix equations. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-32(3):265-272, </volume> <year> 1983. </year>
Reference: [9] <author> C. Ashcraft, S. Eisenstat, and J. Liu. </author> <title> A fan-in algorithm for distributed sparse numerical factorization. </title> <journal> SIAM Journal of Scientific and Statistical Computing, </journal> <volume> 11 </volume> <pages> 593-599, </pages> <year> 1990. </year>
Reference-contexts: This is the fan-out approach that has been used in initial Cholesky factorization routines. The excessive communication overheads of these methods limited their performance. Another class of methods (used for sparse Cholesky factorization) are the fan-in methods <ref> [9, 117, 88] </ref>. These methods accumulate contributions to the updating of the active submatrix and send fewer but larger messages [9]. An increasingly popular approach to sparse matrix factorization are the symmetric-pattern, multifrontal methods [105, 76, 51, 121, 122, 77]. <p> The excessive communication overheads of these methods limited their performance. Another class of methods (used for sparse Cholesky factorization) are the fan-in methods [9, 117, 88]. These methods accumulate contributions to the updating of the active submatrix and send fewer but larger messages <ref> [9] </ref>. An increasingly popular approach to sparse matrix factorization are the symmetric-pattern, multifrontal methods [105, 76, 51, 121, 122, 77]. Large grain parallelism is available in these methods via independent subtrees in the assembly trees.
Reference: [10] <author> C. Ashcraft, S. Eisenstat, J. Liu, B. Peyton, and A. Sherman. </author> <title> A compute-based implementation of the fan-in sparse distributed factorization scheme. </title> <type> Technical Report ORNL/TM-11496, </type> <institution> Oak Ridge National Laboratory, Oak Ridge, TN, </institution> <year> 1990. </year> <month> 209 </month>
Reference: [11] <author> C. Ashcraft, S. Eisenstat, J. Liu, and A. Sherman. </author> <title> A comparison of three column-based distributed sparse factorization schemes. </title> <type> Technical Report YALEU/DCS/RR-810, </type> <institution> Department of Computer Science, Yale University, </institution> <address> New Haven, CT, </address> <year> 1990. </year>
Reference: [12] <author> C. Ashcraft and R. Grimes. </author> <title> The influence of relaxed supernode partitions on the multifrontal method. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 15(4) </volume> <pages> 291-309, </pages> <year> 1989. </year>
Reference: [13] <author> Kendall E. Atkinson. </author> <title> An Introduction to Numerical Analysis. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, NY, </address> <note> second edition, </note> <year> 1989. </year>
Reference-contexts: Systems of Nonlinear Algebraic Equations: Newton's method is a very common technique in numerical analysis that is used to find the zeros of a function by an approximation of the function that is based on a first order Taylor's series expansion <ref> [13] </ref>.
Reference: [14] <author> U. Banerjee, D. Gajski, and D. J. Kuck. </author> <title> Accessing sparse arrays in parallel memories. </title> <journal> Journal of VLSI and Computer Systems, </journal> <volume> 1(1) </volume> <pages> 69-100, </pages> <year> 1983. </year>
Reference: [15] <author> R. Benner, G. Montry, and G. Weigand. </author> <title> Concurrent multifrontal methods: Shared memory, cache, and frontwidth issues. </title> <journal> International Journal of Supercomputing Applications, </journal> <volume> 1 </volume> <pages> 26-44, </pages> <year> 1987. </year>
Reference: [16] <author> Marsha J. Berger and S. H. Bokhari. </author> <title> A partitioning strategy for non-uniform problems on multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 35(5) </volume> <pages> 570-580, </pages> <year> 1987. </year>
Reference: [17] <author> R. D. Berry. </author> <title> An optimal ordering of electronic circuit equations for a sparse matrix solution. </title> <journal> IEEE Transactions on Circuit Theory, </journal> <volume> CT-19(1):40-50, </volume> <month> Jan </month> <year> 1971. </year>
Reference: [18] <author> A Bjork, R. J. Plemmons, and H. Schneider, </author> <title> editors. Large Scale Matrix Problems. </title> <address> New York: </address> <publisher> North-Holland, </publisher> <year> 1981. </year>
Reference: [19] <author> A. Bojanczyk. </author> <title> Complexity of solving linear systems in different models of computation. </title> <journal> SIAM Journal of Numerical Analysis, </journal> <volume> 21(3) </volume> <pages> 591-603, </pages> <month> June </month> <year> 1984. </year>
Reference: [20] <author> Ming-Syan Chen and Kang G. Shin. </author> <title> Processor allocation in an n-cube multiprocessor using gray codes. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(12):1396-1407, </volume> <month> December </month> <year> 1987. </year>
Reference-contexts: This provides a great deal of flexibility in subcube allocations and extensions to the binary buddy system have been proposed to take advantage of this property <ref> [20, 2] </ref>. However, the added flexibility comes at a significant cost in terms of complexity of the allocation mechanisms. Furthermore, splitting the hypercube along multiple alternative dimensions can create complicated fragmentation scenarios that can limit availability of larger subcubes.
Reference: [21] <author> J. Choi, J. Dongarra, R. Pozo, and D. Walker. </author> <title> ScaLAPACK: A scalable linear algebra library for distributed memory concurrent computers. </title> <booktitle> In 4th Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <address> McLean, VA, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: Choi, Dongarra, Pozo, and Walker explore the various data allocation schemes and associated algorithms and develop a generalized block scattered approach with data blocked and blocks scattered (wrapped) to processors <ref> [21] </ref>. By varying the parameters of this method both pure blocked and pure scattered (together with inbe-tween formats) can be achieved. A block-cyclic strategy has recently been proposed by Lichtenstein and Johnsson for dense linear algebra on distributed memory multiprocessors [100].
Reference: [22] <author> E. Chu and A. George. </author> <title> Gaussian elimination with partial pivoting and load balancing on a multiprocessor. </title> <journal> Parallel Computing, </journal> <volume> 5 </volume> <pages> 65-74, </pages> <year> 1987. </year>
Reference-contexts: This section summarizes some previous efforts that explored distributed memory implementations of both dense and sparse matrix factorizations. Distributed Memory Factorization: In distributed memory environments, algorithms are heavily dependent upon the storage allocation scheme for the matrix. The two most common schemes are row oriented <ref> [22] </ref> and column oriented [64, 40, 27, 76]. Blocking schemes of rows or columns are frequently used and Dongarra and Os-trouchov [40] discuss such methods together with the need for adaptive blocking mechanisms. <p> Most implementations exploit only large and medium grains of parallelism. Within a distributed memory environment, it is important to exploit locality of the data. Using data local to the processor as much as possible avoids the communication and synchronization overheads imposed by message passing between processors. Chu and George <ref> [22] </ref> discuss a row oriented dense LU factorization routine for distributed memory environments that could be extended in concept to sparse matrices.
Reference: [23] <author> E. G. Coffman Jr. and R. L. Graham. </author> <title> Optimal scheduling for two-processor systems. </title> <journal> Acta Informatica, </journal> <volume> 1 </volume> <pages> 200-213, </pages> <year> 1972. </year> <month> 210 </month>
Reference-contexts: The most common and effective priority schemes are based on a critical path analysis of the precedence DAG with the priority of a task defined as the heaviest weighted path from that task's node to an exit node (node with no successors) in the DAG <ref> [23] </ref>. Variations within these schemes primarily deal with tie breaking with a Most Immediate Successors First criteria both popular and effective [94]. Within the context of a distributed memory environment, communication delays (represented by edge weights in the task precedence graph) can become quite significant.
Reference: [24] <author> S. D. Conte and Carl de Boor. </author> <title> Elementary Numerical Analysis. </title> <publisher> McGraw-Hill Book Company, </publisher> <address> New York, NY, </address> <year> 1980. </year>
Reference: [25] <author> A. R. Curtis and J. K. Reid. </author> <title> On the automatic scaling of matrices for Gaussian elimination. </title> <journal> Journal of the Institute of Mathematics and its Applications, </journal> <volume> 10 </volume> <pages> 118-124, </pages> <year> 1972. </year>
Reference: [26] <author> E. Cuthill and J. McKee. </author> <title> Reducing the bandwidth of sparse symmetric matrices. </title> <booktitle> In Proceedings 24th National Conference of the Association for Computing Machinery, </booktitle> <pages> pages 157-172, </pages> <address> New Jersey, 1969. </address> <publisher> Brandon Press. </publisher>
Reference: [27] <author> G. Davis. </author> <title> Column LU factorization with pivoting on a message passing multiprocessor. </title> <journal> SIAM Journal of Algebra and Discrete Methods, </journal> <volume> 7(4) </volume> <pages> 538-550, </pages> <year> 1986. </year>
Reference-contexts: This section summarizes some previous efforts that explored distributed memory implementations of both dense and sparse matrix factorizations. Distributed Memory Factorization: In distributed memory environments, algorithms are heavily dependent upon the storage allocation scheme for the matrix. The two most common schemes are row oriented [22] and column oriented <ref> [64, 40, 27, 76] </ref>. Blocking schemes of rows or columns are frequently used and Dongarra and Os-trouchov [40] discuss such methods together with the need for adaptive blocking mechanisms.
Reference: [28] <author> T. A. Davis. Psolve: </author> <title> A concurrent algorithm for solving sparse systems of linear equations. </title> <type> Technical Report CSRD-612, </type> <institution> Center for Supercomputing Research and Development, Univ. of Illinois, Urbana-Champaign, </institution> <year> 1986. </year>
Reference: [29] <author> T. A. Davis. </author> <title> A parallel algorithm for sparse unsymmetric LU factorization. </title> <type> Technical report, </type> <institution> Center for Supercomputing Research and Development, University of Illinois, Urbana-Champaign, </institution> <year> 1989. </year>
Reference: [30] <author> T. A. Davis. </author> <title> Parallel algorithms for the direct solution of sparse linear systems. </title> <editor> In I. St. Doltsinis, editor, </editor> <booktitle> Proc. Second World Congress on Computational Mechanics, </booktitle> <pages> pages 960-963, </pages> <address> Stuttgart, Germany, </address> <month> Aug. </month> <year> 1990. </year> <note> Int. Assoc. of Computational Mechanics. </note>
Reference: [31] <author> T. A. Davis. </author> <title> Performance of an unsymmetric-pattern multifrontal method for sparse LU factorization. </title> <type> Technical Report TR-92-014, </type> <institution> Comp. and Info. Sci. Dept., Univ. of Florida, </institution> <address> Gainesville, FL, </address> <month> May </month> <year> 1992. </year>
Reference: [32] <author> T. A. Davis. </author> <title> Users' guide for the unsymmetric-pattern multifrontal package (UMFPACK). </title> <type> Technical Report TR-93-020, </type> <institution> Computer and Information Sciences Department, University of Florida, </institution> <address> Gainesville, FL, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: Specifically, the parallel algorithms developed will perform the numerical factorization of sequences of identically structured sparse matrices using a directed acyclic graph (DAG) structure produced by a sequential implementation of Davis and Duff's Unsymmetric Multifrontal Package (UMFPACK) <ref> [34, 35, 32] </ref>. This DAG structure, known as the assembly DAG, defines the necessary computations in terms of the partial factorizations of small, dense submatrices, which are known as frontal matrices and represented by nodes in the assembly DAG. <p> The discussion in this chapter is divided into three sections. In the first section, the focus is on the preprocessing that takes place within the host processor. This includes the process of acquiring and refining the specification of the assembly DAG from the UMFPACK software <ref> [32] </ref>, the two developed scheduling methods, the sub-cube allocation process (which is integrated into the scheduling), and the various techniques developed for the assignment problem. The second section describes the parallel processing routines that accept the statically produced schedule and frontal matrix descriptions and then perform the parallel factorization. <p> the assembly DAG input format, the two scheduling/subcube allocation methods developed, details on the various data 101 allocation options, and the message passing required to launch an initial parallel refactorization. 6.1.1 Assembly DAG Acquisition The description of the assembly DAG produced by UMFPACK is held in an integer array format <ref> [32] </ref>. For convenience, UMFPACK dumps this array to a file with one entry per line and the SCHEDULE REFACTOR routine simply reads this file to obtain the assembly DAG, which also includes other pertinent data such as the permutation matrices and descriptions of the LU factors. <p> First among these is that the assembly DAG produced by UMFPACK and used by the parallel refactor-ization (PRF) code does not explicitly contain all of the "true" dependencies. This is a result of the aggressive edge reduction done by UMFPACK <ref> [34, 35, 32] </ref>. Whether or not this reduced edge set is sufficient to reflect the triangular solve dependencies and the implications of using this edge set both require further investigation. <p> This initial implementation was restricted to maintaining the pivot order established by the prerequisite analysis of the UMFPACK software <ref> [32, 34, 35] </ref>. A major emphasis of this initial implementation was to investigate the use of various scheduling, allocation, and assignment mechanisms. Also mechanisms were implemented as preprocessing to be performed on a sequential host processor. <p> Later results will establish the relationship between E T and the edge set of the assembly DAG provided by the UMFPACK software <ref> [32, 34, 35] </ref>. 163 7.1.4 Impacts on Assembly DAG One of the most significant features of the lost pivot recovery mechanisms developed in this chapter is the minimal impact on the relationships between frontal matrices as defined by the edge set E T . <p> An alternative would be to allow the factorization of rank deficient matrices in a manner similar to that done by Davis <ref> [32] </ref>. In this alternative, the permutation vectors are flagged, via negation, to indicate the position of zeroed pivots.
Reference: [33] <author> T. A. Davis and E. S. Davidson. </author> <title> Pairwise reduction for the direct, parallel solution of sparse unsymmetric sets of linear equations. </title> <journal> IEEE Trans. Comput., </journal> <volume> 37(12) </volume> <pages> 1648-1654, </pages> <year> 1988. </year>
Reference: [34] <author> T. A. Davis and I. S. Duff. </author> <title> Unsymmetric-pattern multifrontal methods for parallel sparse LU factorization. </title> <type> Technical Report TR-91-023, </type> <institution> Computer and Information Sciences Department, University of Florida, </institution> <address> Gainesville, FL, </address> <year> 1991. </year>
Reference-contexts: Triangular solves (forward and backward substitutions) can then be used to solve the system. One approach to LU factorization that holds significant potential for parallel implementation is the unsymmetric-pattern multifrontal method of Davis and Duff <ref> [34, 35] </ref>. Like most sparse matrix factorization algorithms, the unsymmetric-pattern mul-tifrontal method has two principal operations, analyze and factorize. The analyze operation selects matrix entries to act as pivots for the numerical factorization with objectives of reducing computations and maintaining numerical stability. <p> Specifically, the parallel algorithms developed will perform the numerical factorization of sequences of identically structured sparse matrices using a directed acyclic graph (DAG) structure produced by a sequential implementation of Davis and Duff's Unsymmetric Multifrontal Package (UMFPACK) <ref> [34, 35, 32] </ref>. This DAG structure, known as the assembly DAG, defines the necessary computations in terms of the partial factorizations of small, dense submatrices, which are known as frontal matrices and represented by nodes in the assembly DAG. <p> Furthermore, concepts and techniques for sparse matrices must be defined and described as they will be frequently referenced. In addition to this general background, the specifics of the unsymmetric-pattern multifrontal method of Davis and Duff <ref> [34, 35] </ref> are necessary as this entire effort is based on that method. Mathematical techniques that give rise to sequences of identially structured sparse matrices together with specific applications justify the need to be able to factorize such matrix sequences. <p> However, the multifrontal approach has been used most extensively for the Cholesky factorization of symmetric, positive-definite matrices [70, 65, 67]. Most recently, Davis and Duff have generalized the method to take advantage of unsymmetric-pattern matrices <ref> [34, 35] </ref>. <p> The assembly graphs associated with symmetric matrices always form tree structures. Such a structure simplifies both the task mapping and scheduling problems when employing a multiprocessors on the method. Recent efforts by Davis and Duff have focused on extension of the method to unsymmetric-pattern matrices <ref> [34, 35] </ref>. With such matrices, the frontal matrices are no longer guaranteed to be square and are typically rectangular. Furthermore, the assembly graph is no longer a tree. Instead, it becomes a directed acyclic graph (DAG). The rest of this section describes the multifrontal method applied to unsymmetric-pattern matrices. <p> Other entries may affect the anticipated pivot's value during previous steps of elimination, as was the case in the example provided. Davis and Duff suggest two possible approaches to dealing with the loss of anticipated pivots <ref> [34, 35] </ref>. They are: * Force the amalgamation of the lost pivot with subsequent frontal matrices during the numerical factorization. This creates a larger pivot block from which permutations can be used to select alternative pivots. <p> This characterization will also be used in Chapters 5 and 6 when a parallel version of the unsymmetric-pattern multifrontal method is designed and implemented on the nCUBE 2. CHAPTER 4 EVALUATION OF POTENTIAL PARALLELISM In this chapter, the parallelism available in the unsymmetric-pattern multifrontal method <ref> [34, 35] </ref> is investigated. The assembly DAG is the principle data structure of this method and describes the data dependencies that exist between the various dense submatrices, called frontal matrices. This assembly DAG is also significant as it exposes parallelism between the frontal matrices. <p> The analysis of the unbounded parallelism models is based on the representations of assembly DAGs built from traces produced by a sequential version of the unsymmetric-pattern multifrontal method on matrices from real applications <ref> [34, 35] </ref>. 45 46 These assembly DAGs are analyzed and then used as input for the five unbounded parallelism models. <p> This 53 can be appropriately modeled via a trace-driven simulation run against the representation of the assembly DAGs obtained from the traces produced by the sequential version of the unsymmetric-pattern multifrontal method <ref> [34, 35] </ref>. <p> The algorithm is deterministic. 5.4 Single Pivot Version Frequently in the current sequential implementation of the unsymmetric-pattern multifrontal method <ref> [34, 35] </ref>, frontal matrices are encountered that require only a single pivot step. In such frontal matrices the pivot block is a single entry and no row or column permutations can be made. Furthermore, the looping overhead for all the pivots can be eliminated as can other related overhead. <p> contributions is completed, we are assured that all prerequisite data dependencies have been satisfied and may commence factorization of the frontal matrix. 123 While contribution edges initially result from contribution entries that reside within the pivot rows or columns of the frontal matrix, the aggressive edge reduction done by UMFPACK <ref> [34, 35] </ref> causes contributions to be possible for any entry within the frontal matrix. Thus, contributions can be spread across all frontal matrix columns with beneficial effects on parallelism. However, the distribution of these contributions is highly problem specific, so no hard and fast parallelism results can be predicted. <p> First among these is that the assembly DAG produced by UMFPACK and used by the parallel refactor-ization (PRF) code does not explicitly contain all of the "true" dependencies. This is a result of the aggressive edge reduction done by UMFPACK <ref> [34, 35, 32] </ref>. Whether or not this reduced edge set is sufficient to reflect the triangular solve dependencies and the implications of using this edge set both require further investigation. <p> This advantage of blocking over scattering is likely the result of the manner in which frontal matrices are defined in the UMFPACK software <ref> [34, 35] </ref>. A specific illustration of these results in seen Table 6-3, which is an excerpt of the results for the RDIST1 matrix. This data comes from the S3 method with all edges considered and indicate the percentage of worst case required communications that have been eliminated. Table 6-3. <p> This initial implementation was restricted to maintaining the pivot order established by the prerequisite analysis of the UMFPACK software <ref> [32, 34, 35] </ref>. A major emphasis of this initial implementation was to investigate the use of various scheduling, allocation, and assignment mechanisms. Also mechanisms were implemented as preprocessing to be performed on a sequential host processor. <p> Within a particular frontal matrix, the specific components (submatrices) will frequently need to be referenced. This will be done using the same notation found in Davis and Duff's original work <ref> [34, 35] </ref>. Specifically, L A refers to the set of row indices that define the rows from the overall matrix that make up the frontal matrix A 2 F . <p> Later results will establish the relationship between E T and the edge set of the assembly DAG provided by the UMFPACK software <ref> [32, 34, 35] </ref>. 163 7.1.4 Impacts on Assembly DAG One of the most significant features of the lost pivot recovery mechanisms developed in this chapter is the minimal impact on the relationships between frontal matrices as defined by the edge set E T . <p> When the matrices are not symmetric and positive-definite, the most common solution technique is LU factorization. One method that can be used to perform the LU factorizations is the unsymmetric-pattern multifrontal approach <ref> [34, 35] </ref>. With this approach, the assembly DAG (and implied pivot ordering) produced by a single analyze operation can be repeatedly used to factorize the subsequent matrices in the sequence and reduce the overall required computation time.
Reference: [35] <author> T. A. Davis and I. S. Duff. </author> <title> An unsymmetric-pattern multifrontal method for parallel sparse LU factorization. </title> <type> Technical Report TR-93-018, </type> <institution> Computer and Information Sciences Department, University of Florida, </institution> <address> Gainesville, FL, </address> <year> 1993. </year> <month> 211 </month>
Reference-contexts: Triangular solves (forward and backward substitutions) can then be used to solve the system. One approach to LU factorization that holds significant potential for parallel implementation is the unsymmetric-pattern multifrontal method of Davis and Duff <ref> [34, 35] </ref>. Like most sparse matrix factorization algorithms, the unsymmetric-pattern mul-tifrontal method has two principal operations, analyze and factorize. The analyze operation selects matrix entries to act as pivots for the numerical factorization with objectives of reducing computations and maintaining numerical stability. <p> Specifically, the parallel algorithms developed will perform the numerical factorization of sequences of identically structured sparse matrices using a directed acyclic graph (DAG) structure produced by a sequential implementation of Davis and Duff's Unsymmetric Multifrontal Package (UMFPACK) <ref> [34, 35, 32] </ref>. This DAG structure, known as the assembly DAG, defines the necessary computations in terms of the partial factorizations of small, dense submatrices, which are known as frontal matrices and represented by nodes in the assembly DAG. <p> Furthermore, concepts and techniques for sparse matrices must be defined and described as they will be frequently referenced. In addition to this general background, the specifics of the unsymmetric-pattern multifrontal method of Davis and Duff <ref> [34, 35] </ref> are necessary as this entire effort is based on that method. Mathematical techniques that give rise to sequences of identially structured sparse matrices together with specific applications justify the need to be able to factorize such matrix sequences. <p> However, the multifrontal approach has been used most extensively for the Cholesky factorization of symmetric, positive-definite matrices [70, 65, 67]. Most recently, Davis and Duff have generalized the method to take advantage of unsymmetric-pattern matrices <ref> [34, 35] </ref>. <p> The assembly graphs associated with symmetric matrices always form tree structures. Such a structure simplifies both the task mapping and scheduling problems when employing a multiprocessors on the method. Recent efforts by Davis and Duff have focused on extension of the method to unsymmetric-pattern matrices <ref> [34, 35] </ref>. With such matrices, the frontal matrices are no longer guaranteed to be square and are typically rectangular. Furthermore, the assembly graph is no longer a tree. Instead, it becomes a directed acyclic graph (DAG). The rest of this section describes the multifrontal method applied to unsymmetric-pattern matrices. <p> Other entries may affect the anticipated pivot's value during previous steps of elimination, as was the case in the example provided. Davis and Duff suggest two possible approaches to dealing with the loss of anticipated pivots <ref> [34, 35] </ref>. They are: * Force the amalgamation of the lost pivot with subsequent frontal matrices during the numerical factorization. This creates a larger pivot block from which permutations can be used to select alternative pivots. <p> This characterization will also be used in Chapters 5 and 6 when a parallel version of the unsymmetric-pattern multifrontal method is designed and implemented on the nCUBE 2. CHAPTER 4 EVALUATION OF POTENTIAL PARALLELISM In this chapter, the parallelism available in the unsymmetric-pattern multifrontal method <ref> [34, 35] </ref> is investigated. The assembly DAG is the principle data structure of this method and describes the data dependencies that exist between the various dense submatrices, called frontal matrices. This assembly DAG is also significant as it exposes parallelism between the frontal matrices. <p> The analysis of the unbounded parallelism models is based on the representations of assembly DAGs built from traces produced by a sequential version of the unsymmetric-pattern multifrontal method on matrices from real applications <ref> [34, 35] </ref>. 45 46 These assembly DAGs are analyzed and then used as input for the five unbounded parallelism models. <p> This 53 can be appropriately modeled via a trace-driven simulation run against the representation of the assembly DAGs obtained from the traces produced by the sequential version of the unsymmetric-pattern multifrontal method <ref> [34, 35] </ref>. <p> Test Matrices With the updates just described made to the simulation program, traces of assembly DAGs from the latest version of the unsymmetric-pattern multifrontal method were used as input <ref> [35] </ref>. This latest version of the method, called afstack, produced assembly DAGS with significantly better parallelism than seen in the earlier DAGs used in the unbounded and bounded parallelism PRAM models. <p> The algorithm is deterministic. 5.4 Single Pivot Version Frequently in the current sequential implementation of the unsymmetric-pattern multifrontal method <ref> [34, 35] </ref>, frontal matrices are encountered that require only a single pivot step. In such frontal matrices the pivot block is a single entry and no row or column permutations can be made. Furthermore, the looping overhead for all the pivots can be eliminated as can other related overhead. <p> contributions is completed, we are assured that all prerequisite data dependencies have been satisfied and may commence factorization of the frontal matrix. 123 While contribution edges initially result from contribution entries that reside within the pivot rows or columns of the frontal matrix, the aggressive edge reduction done by UMFPACK <ref> [34, 35] </ref> causes contributions to be possible for any entry within the frontal matrix. Thus, contributions can be spread across all frontal matrix columns with beneficial effects on parallelism. However, the distribution of these contributions is highly problem specific, so no hard and fast parallelism results can be predicted. <p> First among these is that the assembly DAG produced by UMFPACK and used by the parallel refactor-ization (PRF) code does not explicitly contain all of the "true" dependencies. This is a result of the aggressive edge reduction done by UMFPACK <ref> [34, 35, 32] </ref>. Whether or not this reduced edge set is sufficient to reflect the triangular solve dependencies and the implications of using this edge set both require further investigation. <p> This advantage of blocking over scattering is likely the result of the manner in which frontal matrices are defined in the UMFPACK software <ref> [34, 35] </ref>. A specific illustration of these results in seen Table 6-3, which is an excerpt of the results for the RDIST1 matrix. This data comes from the S3 method with all edges considered and indicate the percentage of worst case required communications that have been eliminated. Table 6-3. <p> This initial implementation was restricted to maintaining the pivot order established by the prerequisite analysis of the UMFPACK software <ref> [32, 34, 35] </ref>. A major emphasis of this initial implementation was to investigate the use of various scheduling, allocation, and assignment mechanisms. Also mechanisms were implemented as preprocessing to be performed on a sequential host processor. <p> Within a particular frontal matrix, the specific components (submatrices) will frequently need to be referenced. This will be done using the same notation found in Davis and Duff's original work <ref> [34, 35] </ref>. Specifically, L A refers to the set of row indices that define the rows from the overall matrix that make up the frontal matrix A 2 F . <p> Later results will establish the relationship between E T and the edge set of the assembly DAG provided by the UMFPACK software <ref> [32, 34, 35] </ref>. 163 7.1.4 Impacts on Assembly DAG One of the most significant features of the lost pivot recovery mechanisms developed in this chapter is the minimal impact on the relationships between frontal matrices as defined by the edge set E T . <p> When the matrices are not symmetric and positive-definite, the most common solution technique is LU factorization. One method that can be used to perform the LU factorizations is the unsymmetric-pattern multifrontal approach <ref> [34, 35] </ref>. With this approach, the assembly DAG (and implied pivot ordering) produced by a single analyze operation can be repeatedly used to factorize the subsequent matrices in the sequence and reduce the overall required computation time.
Reference: [36] <author> T. A. Davis and P. Yew. </author> <title> A nondeterministic parallel algorithm for general unsymmetric sparse LU factorization. </title> <journal> SIAM Journal of Matrix Applications, </journal> <volume> 11 </volume> <pages> 383-402, </pages> <year> 1990. </year>
Reference: [37] <author> Benjamin Dembart and Albert M. Erisman. </author> <title> Hybrid sparse-matrix methods. </title> <journal> IEEE Transactions on Circuit Theory, </journal> <volume> CT-20(6):641-649, </volume> <month> November </month> <year> 1973. </year>
Reference: [38] <author> R. H. Dodds, Jr. and L. A. Lopez. </author> <title> Substructuring in linear and non-linear analysis. </title> <journal> International Journal of Numerical Method for Engineering, </journal> <volume> 15 </volume> <pages> 583-597, </pages> <year> 1980. </year>
Reference: [39] <author> J. Dongarra, J. Du Croz, S. Hammarling, and I. Duff. </author> <title> A set of level 3 basic linear algebra subprograms. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 16(1) </volume> <pages> 1-17, </pages> <year> 1990. </year>
Reference: [40] <author> J. Dongarra and S. Ostrouchov. </author> <title> LAPACK block factorization algorithms on the Intel iPSC/860. </title> <note> Technical Report LAPACK Working Note 24, </note> <institution> University of Tennessee, Knoxville, TN, </institution> <year> 1990. </year>
Reference-contexts: This section summarizes some previous efforts that explored distributed memory implementations of both dense and sparse matrix factorizations. Distributed Memory Factorization: In distributed memory environments, algorithms are heavily dependent upon the storage allocation scheme for the matrix. The two most common schemes are row oriented [22] and column oriented <ref> [64, 40, 27, 76] </ref>. Blocking schemes of rows or columns are frequently used and Dongarra and Os-trouchov [40] discuss such methods together with the need for adaptive blocking mechanisms. <p> Distributed Memory Factorization: In distributed memory environments, algorithms are heavily dependent upon the storage allocation scheme for the matrix. The two most common schemes are row oriented [22] and column oriented [64, 40, 27, 76]. Blocking schemes of rows or columns are frequently used and Dongarra and Os-trouchov <ref> [40] </ref> discuss such methods together with the need for adaptive blocking mechanisms. Choi, Dongarra, Pozo, and Walker explore the various data allocation schemes and associated algorithms and develop a generalized block scattered approach with data blocked and blocks scattered (wrapped) to processors [21].
Reference: [41] <author> J. Dongarra, R. van de Geijn, and D. Walker. </author> <title> A look at scalable dense linear algebra libraries. </title> <type> Technical Report ORNL/TM-12126, </type> <institution> Oak Ridge National Laboratory, Oak Ridge, TN, </institution> <year> 1992. </year>
Reference: [42] <author> I. S. Duff. </author> <title> On algorithms for obtaining a maximum transversal. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 7 </volume> <pages> 315-330, </pages> <year> 1981. </year>
Reference: [43] <author> I. S. Duff. </author> <title> Sparse Matrices and their Uses. </title> <publisher> Academic Press, </publisher> <address> New York and London, </address> <year> 1981. </year>
Reference: [44] <author> I. S. Duff. </author> <title> The solution of nearly symmetric sparse linear systems. </title> <editor> In R. Glowin-ski and J.-L. Lions, editors, </editor> <booktitle> Computing Methods in Applied Sciences and Engineering. </booktitle> <publisher> North-Holland, </publisher> <address> Amsterdam, New York, and London, </address> <year> 1984. </year>
Reference: [45] <author> I. S. Duff. </author> <title> Data structures, algorithms and software for sparse matrices. </title> <editor> In D. J. Evans, editor, </editor> <booktitle> Sparsity and Its Applications, </booktitle> <pages> pages 1-29. </pages> <address> Cambridge, United Kingdom: </address> <publisher> Cambridge University Press, </publisher> <year> 1985. </year>
Reference: [46] <author> I. S. Duff. </author> <title> Parallel implementation of multifrontal schemes. </title> <journal> Parallel Computing, </journal> <volume> 3 </volume> <pages> 193-204, </pages> <year> 1986. </year>
Reference-contexts: Furthermore, when frontal matrices do not overlap in their pivot rows and columns, the factorization steps associated with those pivots can be done concurrently <ref> [46] </ref>. This provides an additional degree of parallelism that can be 20 exploited by multiprocessors. The combination of these two sources of parallelism, within dense frontal matrix operations and between frontal matrices, has shown to provide a significant amount of potential speed-up in several studies [83, 51, 121, 122].
Reference: [47] <author> I. S. Duff. </author> <title> Multiprocessing a sparse matrix code on the Alliant FX/8. </title> <journal> Journal of Computational and Applied Mathematics, </journal> <volume> 27 </volume> <pages> 229-239, </pages> <year> 1989. </year>
Reference: [48] <author> I. S. Duff, A. M. Erisman, and J. K. Reid. </author> <title> Direct Methods for Sparse Matrices. </title> <publisher> Oxford Science Publications, </publisher> <address> New York, NY, </address> <year> 1989. </year>
Reference-contexts: establish its significance. 2.1 LU Factorization The most common method for solving general systems of linear equations in the form Ax = b (where A is an nfin matrix of full rank and x and b are vectors of length n) is LU factorization which is based on Gauss Elimination <ref> [48] </ref>. In this method, the matrix A is factorized into A = LU where L is unit lower triangular and U is upper triangular. The problem Ax = b then becomes LU x = b and can be solved with two subsequent triangular solves. <p> n do endfor for i := k+1 to n do for j := k+1 to n do endfor endfor endfor When row and column permutations are introduced into the algorithm, the fac torization becomes P AQ = LU where P provides the row permutations and Q provides the column permutations <ref> [48] </ref>. <p> Threshold pivoting provides an alternative that allows greater flexibility in pivot selection to aid in the preservation of sparsity <ref> [48] </ref>. Pivot selection in threshold pivoting requires the selected pivot to meet ja kk j uja ik j; i &gt; k; 0 u 1: Here u is a parameter that is typically set between 0:001 and 0:1. <p> The condition number can be defined in terms of matrix norms as p (A) = kAk p kA 1 k p where p designates the specific norm in use <ref> [48] </ref>. An alternative, but equivalent formulation based on singular values is 2 (A) = 1 = n 10 where 1 is the largest singular value and n the smallest [78]. <p> Relationship to Graph Theory: Critical to the exploitation of sparsity in sparse matrices is the understanding and representation of the inherent matrix structure. Graph theory provides an excellent vehicle for this purpose <ref> [48, 70] </ref>. By representing the structure of a matrix with a graph construct, exploitable patterns can be more readily recognized. <p> While both static and dynamic storage techniques for sparse matrices will be addressed, the storage of sparse vectors is discussed first as a necessary and illustrative 15 prerequisite. Much of this summary will follow that provided in Duff, Reid, and Erisman's text on sparse matrices <ref> [48] </ref>. A full storage scheme for a sparse vector requires O (n) memory but has the advantage that individual vector components can be directly indexed. <p> In particular, only the B ii blocks need to be factorized. A back substitution process can then be used with a set of simple matrix-vector multiplications to evaluate the contributions of the off diagonal blocks <ref> [48] </ref>. <p> most LU factorizations is to solve a system of linear equations in the form Ax = b: This is done by factorizing the matrix A such that P AQ = LU with L and U lower and upper triangular matrices, respectively, and P and Q row and column permutations, respectively <ref> [48] </ref>. The row and column permutations are typically done to improve numerical stability and retain sparsity, but may also be used to put the matrix into a block upper triangular form [48]. <p> with L and U lower and upper triangular matrices, respectively, and P and Q row and column permutations, respectively <ref> [48] </ref>. The row and column permutations are typically done to improve numerical stability and retain sparsity, but may also be used to put the matrix into a block upper triangular form [48]. <p> Lost pivot avoidance is a useful strategy but will not completely solve the problem. Changing the pivot threshold will also allow for greater growth in the error bounds as the bound on any particular entry's growth () when using pivot thresholding <ref> [48] </ref> is i;j Another recourse available is to find a new pivot within the pivot block of the current frontal matrix (assuming the frontal matrix has more than one potential pivot). <p> This implies that the original matrix was not in block upper triangular form as this form has been proven to be unique for nonsingular matrices up to permutations strictly within the blocks and reorderings of the blocks on the diagonal <ref> [48] </ref>. Furthermore, the transpose of the argument that follows establishes the same result if L 00 C = ;.
Reference: [49] <author> I. S. Duff, R. G. Grimes, and J. G. Lewis. </author> <title> Sparse matrix problems. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 14 </volume> <pages> 1-14, </pages> <year> 1989. </year> <month> 212 </month>
Reference-contexts: They represent a cross section of the type of assembly DAGs that are generated. The matrices are from the Harwell Boeing set and each matrix comes from a real application <ref> [49] </ref>. Table 4-1 briefly describes these matrices: 4.1.4 Assembly DAG Analysis Results The first objective of this part of the effort was to characterize the assembly DAGs of the test matrices. Table 4-2 below provides these characterizations that were produced with the analysis program written for the effort.
Reference: [50] <author> I. S. Duff, R. G. Grimes, and J. G. Lewis. </author> <title> User's guide for the Harwell-Boeing sparse matrix collection (release I). </title> <type> Technical Report TR/PA/92/86, </type> <institution> Computer Science and Systems Division, Harwell Laboratory, Oxon, U.K., </institution> <month> October </month> <year> 1992. </year>
Reference-contexts: Parallelism is evaluated by comparing the execution time of the parallel refactorization code running on a single processor to its execution on larger dimension hypercubes. Competitiveness is addressed by running the Harwell MA28B general sparse matrix refactorization code <ref> [50] </ref> on a single nCUBE processor and comparing its execution time to that of the parallel refactorization code on both a single processing node and a 6 dimensional hypercube. Dynamic memory utilization is traced for the runs on various hypercube sizes. <p> To this end, three test matrices were chosen. RDIST1 and EXTR1 come from chemical engineering applications [144, 143] and GEMAT11 from a power system application <ref> [50] </ref>. RDIST2 and RDIST3A are also from chemical engineering applications [144, 143] and will only be used as additional test matrices for the parallelism. All of these matrices are the lead matrices in sequences of identically structured, unsymmetric pattern sparse matrices.
Reference: [51] <author> I. S. Duff and L. S. Johnsson. </author> <title> Node orderings and concurrency in structurally-symmetric sparse problems. </title> <editor> In Graham F. Carey, editor, </editor> <booktitle> Parallel Supercomputing: Methods, Algorithms, and Applications, </booktitle> <pages> pages 177-189. </pages> <publisher> John Wiley and Sons Ltd., </publisher> <address> New York, NY, </address> <year> 1989. </year>
Reference-contexts: This provides an additional degree of parallelism that can be 20 exploited by multiprocessors. The combination of these two sources of parallelism, within dense frontal matrix operations and between frontal matrices, has shown to provide a significant amount of potential speed-up in several studies <ref> [83, 51, 121, 122] </ref>. <p> Another class of methods (used for sparse Cholesky factorization) are the fan-in methods [9, 117, 88]. These methods accumulate contributions to the updating of the active submatrix and send fewer but larger messages [9]. An increasingly popular approach to sparse matrix factorization are the symmetric-pattern, multifrontal methods <ref> [105, 76, 51, 121, 122, 77] </ref>. Large grain parallelism is available in these methods via independent subtrees in the assembly trees. However, as subtrees combine and parallelism at that level decreases, a switch is made to exploit parallelism at a finer grain within the factorization of a particular frontal matrix. <p> The exploitation of parallelism both in the elimination structure and within the distinct pivot steps has been found to be critical to the success of all of these methods <ref> [76, 51, 121] </ref> Recently, fine grain parallelism has been investigated by Gilbert and Schreiber for a variety of sparse Cholesky methods on a SIMD distributed memory environment [77]. The multifrontal approach has demonstrated the greatest potential for exploiting this level of parallelism. <p> Finally, the fourth term defines the updating of the active submatrix, which is essentially the carrying out of the necessary row operations needed to reduce the matrix. These models are very similar to an earlier effort by Duff and Johnsson <ref> [51] </ref> that focused on the assembly trees produced by symmetric-pattern, multifrontal methods. 47 Each of the models use the following terms: * A j number of matrix entries assembled into the frontal matrix represented by node j . * S j number of children assembling entries into node j . * <p> Model 2 (Full Concurrency Within Frontal Matrices Only): Concurrency within a frontal matrix is extended by Model 2. In this model, very fine grain parallelism is exploited. Assembly is done for each element in parallel. However, in order to maintain consistency with the earlier work of Johnsson and Duff <ref> [51] </ref>, a CREW memory 48 is used by this model and the assembly of contributions to an entry from multiple children is done via a parallel prefix computation using the associative operator of addition. <p> However, the speed-ups obtained are actually much better than this expectation. This is consistent with the results of the earlier study by Duff and Johnsson <ref> [51] </ref>. 51 Table 4-3.
Reference: [52] <author> I. S. Duff and U. Nowak. </author> <title> On sparse solvers in a stiff integrator of extrapolation type. </title> <journal> IMA Journal of Numerical Analysis, </journal> <volume> 7 </volume> <pages> 391-405, </pages> <year> 1987. </year>
Reference: [53] <author> I. S. Duff and J. K. Reid. </author> <title> Some design features of a sparse matrix code. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 5(18) </volume> <pages> 18-35, </pages> <year> 1979. </year>
Reference-contexts: Thus, no one method is best for all cases. None the less, Harwell's current MA28 package, including the MA28B refactorization routine, is a very commonly accepted standard for general, unsymmetric pattern sparse matrices <ref> [53] </ref>. This package is currently the most accepted standard routine for this function. In order to compare the parallel refactorization code to MA28B, an MA28A/C (initial factor/solve) driver was modified to also call and time the MA28B routine. <p> The RDIST1 matrix had a predicted speed-up of 23.2 and actually achieved 20.2 on 64 processors. Furthermore, the parallel refactoriza-tion routine was seen to be very competitive as compared to the commonly accepted sequential standard of Harwell's MA28B <ref> [53] </ref>. For example, refactorization of the GEMAT11 matrix on a single processor took 3.103 seconds with MA28B and only 2.521 seconds with the parallel refactorization routine implemented. The scalability of the parallel refactorization was assessed empirically. The memory requirements scale well, diminishing almost linearly with increased numbers of processors.
Reference: [54] <author> I. S. Duff and J. K. Reid. </author> <title> The multifrontal solution of indefinite sparse symmetric linear systems. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 9 </volume> <pages> 302-325, </pages> <year> 1983. </year>
Reference-contexts: Factorization of these smaller matrices (called elements or frontal matrices) can be done using dense matrix operations that do not require the indirect addressing required of conventional sparse matrix operations. The multifrontal approach was first developed by Duff and Reid for symmetric, indefinite matrices <ref> [54] </ref> and then extended to unsymmetric matrices [55]. However, the multifrontal approach has been used most extensively for the Cholesky factorization of symmetric, positive-definite matrices [70, 65, 67]. Most recently, Davis and Duff have generalized the method to take advantage of unsymmetric-pattern matrices [34, 35].
Reference: [55] <author> I. S. Duff and J. K. Reid. </author> <title> The multifrontal solution of unsymmetric set of linear equations. </title> <journal> SIAM Journal of Scientific and Statistical Computing, </journal> <volume> 5(3) </volume> <pages> 633-641, </pages> <year> 1984. </year>
Reference-contexts: The multifrontal approach was first developed by Duff and Reid for symmetric, indefinite matrices [54] and then extended to unsymmetric matrices <ref> [55] </ref>. However, the multifrontal approach has been used most extensively for the Cholesky factorization of symmetric, positive-definite matrices [70, 65, 67]. Most recently, Davis and Duff have generalized the method to take advantage of unsymmetric-pattern matrices [34, 35].
Reference: [56] <author> S. Eisenstat, M. Heath, C. Henkel, and C. Romine. </author> <title> Modified cyclic algorithms for solving triangular systems on distributed-memory multiprocessors. </title> <journal> SIAM Journal of Scientific and Statistical Computing, </journal> <volume> 9(3) </volume> <pages> 589-600, </pages> <year> 1988. </year>
Reference-contexts: However, the many broadcast and parallel prefix operations together with the sequential orientation of the actual forward and back substitutions causes parallel execution time to be unacceptable for real applications. Furthermore, there are significant earlier efforts to implement parallel triangular solve algorithms that have been achieved better performance <ref> [56, 99, 85] </ref>. Therefore, I conclude the discussion of the distributed triangular solve algorithm with some thoughts on improving parallel execution time. I believe the most significant performance gains to be achieved by exploiting the natural parallelism revealed by the multifrontal method.
Reference: [57] <author> H. El-Rewini and T. G. Lewis. </author> <title> Scheduling parallel program tasks onto arbitrary target machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 9 </volume> <pages> 138-153, </pages> <year> 1990. </year>
Reference-contexts: In 70% of the cases SSCT was better and in 28% of the cases was SSCT worse. MH Mapping Heuristic: The Mapping Heuristic (MH) introduced by El-Rewi-ni and Lewis takes both communication and contention into account when addressing the TCS problem <ref> [57] </ref>. The MH algorithm accepts a task graph description of the 35 parallel program and a description of the target machine topology and produces a Gantt chart that shows the allocation of tasks to processor elements and their execution time frames.
Reference: [58] <author> K. Gallivan, W. Jalby, and U. Meier. </author> <title> The use of BLAS3 in linear algebra on a parallel processor with hierarchical memory. </title> <journal> SIAM Journal of Scientific and Statistical Computing, </journal> <volume> 8(6), </volume> <year> 1987. </year>
Reference: [59] <author> K. A. Gallivan, R. J. Plemmons, and A. H. Sameh. </author> <title> Parallel algorithms for dense linear algebra computations. </title> <editor> In R. J. Plemmons, editor, </editor> <booktitle> Parallel Algorithms for Matrix Computations, </booktitle> <pages> pages 1-82. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1990. </year>
Reference-contexts: For efficiency of pivot determination, row pivoting is preferred with column storage and column pivoting with row storage. A simple illustrative example of a basic (non-pipelined) algorithm (taken from Gallivan, Plemmons, and Sameh <ref> [59] </ref>) is shown in Figure 2-16. It uses column oriented storage with a row pivoting scheme. <p> In order to see this consider the data dependency chart for a lower triangular solve seen in Figure 6-7 <ref> [59] </ref>. At each diagonal entry, a component of the solution is determined and can then be multiplied by the rest of that column with the resulting set of updates subtracted from the right hand side in a column-oriented approach.
Reference: [60] <author> Dennis B. Gannon and John van Rosendale. </author> <title> On the impact of communication complexity on the design of parallel numerical algorithms. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-33(12), </volume> <year> 1984. </year>
Reference: [61] <author> G. A. Geist. </author> <title> Solving finite element problems with parallel multifrontal schemes. </title> <editor> In M. T. Heath, editor, </editor> <booktitle> Hypercube Multiprocessors, </booktitle> <pages> pages 656-661. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1987. </year> <month> 213 </month>
Reference: [62] <author> G. A. Geist and M. Heath. </author> <title> Matrix factorization on a hypercube. </title> <editor> In M. Heath, editor, </editor> <booktitle> Hypercube Multiprocessors 1986, </booktitle> <pages> pages 161-180. </pages> <institution> Society for Industrial and Applied Mathematics, </institution> <address> Philadelphia, PA, </address> <year> 1986. </year>
Reference-contexts: The block-cyclic strategy is applied to both rows and columns and is effectively applied to LU factorization and the subsequent triangular solves. Furthermore, pivoting, if required, can be done in a row or column fashion. When partial pivoting is used, Geist and Heath <ref> [62] </ref> recommend inclusion of pipelining to offset the cost of pivoting. For efficiency of pivot determination, row pivoting is preferred with column storage and column pivoting with row storage. A simple illustrative example of a basic (non-pipelined) algorithm (taken from Gallivan, Plemmons, and Sameh [59]) is shown in Figure 2-16. <p> This algorithm was originally suggested by Geist and Heath <ref> [62] </ref>. In this pipelined version, once the processor that owns the next pivot receives the multipliers for the current pivot, it updates only the next pivot column and then computes and broadcasts the multipliers for that pivot.
Reference: [63] <author> G. A. Geist and E. Ng. </author> <title> Task scheduling for parallel sparse Cholesky factorization. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 18(4) </volume> <pages> 291-314, </pages> <year> 1989. </year>
Reference-contexts: Hence, a static allocation is frequently used and tightly coupled to the data allocation scheme. For Cholesky factorization, early column oriented algorithms allocated data and column tasks to processors in a wrapped fashion proceeding in a bottom up manner from the elimination tree <ref> [63, 71] </ref>. This did well to address the load balancing issue but caused excessive communication. Later schemes allocated subcubes to subtrees in the elimination tree and effectively reduced communication [63, 71]. <p> allocated data and column tasks to processors in a wrapped fashion proceeding in a bottom up manner from the elimination tree <ref> [63, 71] </ref>. This did well to address the load balancing issue but caused excessive communication. Later schemes allocated subcubes to subtrees in the elimination tree and effectively reduced communication [63, 71]. This worked well for the well balanced trees produced from nested dissection orderings but not for the potentially unbalanced trees of minimum degree orderings. <p> A further refined allocation scheme was developed by Geist and Ng to deal with unbalanced trees in a way that addresses both communication elimination and load balancing <ref> [63] </ref>. Additional advantages are typically found when using contemporary architectures by organizing tasks to use fewer, but larger, messages. Hulbert and Zmijewski propose such a method that combine update contributions into a single message [88].
Reference: [64] <author> G. A. Geist and C. H. Romine. </author> <title> LU factorization algorithms on distributed-memory multiprocessor architectures. </title> <journal> SIAM Journal of Scientific and Statistical Computing, </journal> <volume> 9(4) </volume> <pages> 639-649, </pages> <year> 1988. </year>
Reference-contexts: This section summarizes some previous efforts that explored distributed memory implementations of both dense and sparse matrix factorizations. Distributed Memory Factorization: In distributed memory environments, algorithms are heavily dependent upon the storage allocation scheme for the matrix. The two most common schemes are row oriented [22] and column oriented <ref> [64, 40, 27, 76] </ref>. Blocking schemes of rows or columns are frequently used and Dongarra and Os-trouchov [40] discuss such methods together with the need for adaptive blocking mechanisms.
Reference: [65] <author> A. George. </author> <title> Computer Implementation of the Finite-Element Method. </title> <type> PhD thesis, Report STAN CS-71-208, </type> <institution> Department of Computer Science, Stanford University, Stanford, </institution> <address> CA, </address> <year> 1971. </year>
Reference-contexts: The multifrontal approach was first developed by Duff and Reid for symmetric, indefinite matrices [54] and then extended to unsymmetric matrices [55]. However, the multifrontal approach has been used most extensively for the Cholesky factorization of symmetric, positive-definite matrices <ref> [70, 65, 67] </ref>. Most recently, Davis and Duff have generalized the method to take advantage of unsymmetric-pattern matrices [34, 35].
Reference: [66] <author> A. George. </author> <title> Nested dissection of regular finite-element mesh. </title> <journal> SIAM Journal of Numerical Analysis, </journal> <volume> 10 </volume> <pages> 345-363, </pages> <year> 1973. </year>
Reference: [67] <author> A. George. </author> <title> Solution of linear systems of equations: Direct methods for finite-element problems. </title> <editor> In V. A. Barker, editor, </editor> <title> Sparse Matrix Techniques. </title> <booktitle> Lecture Notes in Mathematics, </booktitle> <pages> page 572. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, Heidelberg, New York, and Tokyo, </address> <year> 1977. </year>
Reference-contexts: The multifrontal approach was first developed by Duff and Reid for symmetric, indefinite matrices [54] and then extended to unsymmetric matrices [55]. However, the multifrontal approach has been used most extensively for the Cholesky factorization of symmetric, positive-definite matrices <ref> [70, 65, 67] </ref>. Most recently, Davis and Duff have generalized the method to take advantage of unsymmetric-pattern matrices [34, 35].
Reference: [68] <author> A. George. </author> <title> An automatic one-way dissection algorithm for irregular finite-element problems. </title> <journal> SIAM Journal of Numerical Analysis, </journal> <volume> 17 </volume> <pages> 740-751, </pages> <year> 1980. </year>
Reference: [69] <author> A. George, M. Heath, J. W.-H. Liu, and E. G.-Y. Ng. </author> <title> Sparse Cholesky factorization on a local-memory multiprocessor. </title> <journal> SIAM Journal of Scientific and Statistical Computing, </journal> <volume> 9 </volume> <pages> 327-340, </pages> <year> 1988. </year>
Reference: [70] <author> A. George and J. W.-H. Liu. </author> <title> Computer Solution of Large Sparse Positive-Definite Systems. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1981. </year>
Reference-contexts: Relationship to Graph Theory: Critical to the exploitation of sparsity in sparse matrices is the understanding and representation of the inherent matrix structure. Graph theory provides an excellent vehicle for this purpose <ref> [48, 70] </ref>. By representing the structure of a matrix with a graph construct, exploitable patterns can be more readily recognized. <p> The multifrontal approach was first developed by Duff and Reid for symmetric, indefinite matrices [54] and then extended to unsymmetric matrices [55]. However, the multifrontal approach has been used most extensively for the Cholesky factorization of symmetric, positive-definite matrices <ref> [70, 65, 67] </ref>. Most recently, Davis and Duff have generalized the method to take advantage of unsymmetric-pattern matrices [34, 35].
Reference: [71] <author> A. George, J. W.-H. Liu, and E. G.-Y. Ng. </author> <title> Communications results for parallel sparse Cholesky factorization on a hypercube. </title> <journal> Parallel Computing, </journal> <volume> 10 </volume> <pages> 287-298, </pages> <year> 1989. </year>
Reference-contexts: Hence, a static allocation is frequently used and tightly coupled to the data allocation scheme. For Cholesky factorization, early column oriented algorithms allocated data and column tasks to processors in a wrapped fashion proceeding in a bottom up manner from the elimination tree <ref> [63, 71] </ref>. This did well to address the load balancing issue but caused excessive communication. Later schemes allocated subcubes to subtrees in the elimination tree and effectively reduced communication [63, 71]. <p> allocated data and column tasks to processors in a wrapped fashion proceeding in a bottom up manner from the elimination tree <ref> [63, 71] </ref>. This did well to address the load balancing issue but caused excessive communication. Later schemes allocated subcubes to subtrees in the elimination tree and effectively reduced communication [63, 71]. This worked well for the well balanced trees produced from nested dissection orderings but not for the potentially unbalanced trees of minimum degree orderings.
Reference: [72] <author> A. George and E. G.-Y. Ng. </author> <title> Parallel sparse Gaussian elimination with partial pivoting. </title> <journal> Annals of Operations Research, </journal> <volume> 22 </volume> <pages> 219-240, </pages> <year> 1990. </year>
Reference: [73] <author> N. E. Gibbs, W. G. Poole, Jr., and P. K. Stockmeyer. </author> <title> An algorithm for reducing the bandwidth and profile of a sparse matrix. </title> <journal> SIAM Journal of Numerical Analysis, </journal> <volume> 13 </volume> <pages> 236-250, </pages> <year> 1976. </year>
Reference: [74] <author> J. Gilbert and H. Hafsteinsson. </author> <title> Parallel symbolic factorization of sparse linear systems. </title> <journal> Parallel Computing, </journal> <volume> 14 </volume> <pages> 151-162, </pages> <year> 1990. </year> <month> 214 </month>
Reference: [75] <author> J. Gilbert and J. Liu. </author> <title> Elimination structures for unsymmetric sparse LU factors. </title> <type> Technical Report CS-90-11, </type> <institution> Department of Computer Science, York University, </institution> <address> Ontario, Canada, </address> <year> 1991. </year>
Reference-contexts: This is similar to a formulation of elimination DAGs done by Gilbert and Liu <ref> [75] </ref>, which is used to predict the structure of the LU factors. The edge set used to define the assembly DAG (which is provided as input) is called the assembly edge set and denoted by E A .
Reference: [76] <author> J. R. Gilbert. </author> <title> An efficient parallel sparse partial pivoting algorithm. </title> <type> Technical Report CMI No. 88/45052-1, </type> <institution> Christian Michelsen Institute, </institution> <year> 1988. </year>
Reference-contexts: This section summarizes some previous efforts that explored distributed memory implementations of both dense and sparse matrix factorizations. Distributed Memory Factorization: In distributed memory environments, algorithms are heavily dependent upon the storage allocation scheme for the matrix. The two most common schemes are row oriented [22] and column oriented <ref> [64, 40, 27, 76] </ref>. Blocking schemes of rows or columns are frequently used and Dongarra and Os-trouchov [40] discuss such methods together with the need for adaptive blocking mechanisms. <p> Another class of methods (used for sparse Cholesky factorization) are the fan-in methods [9, 117, 88]. These methods accumulate contributions to the updating of the active submatrix and send fewer but larger messages [9]. An increasingly popular approach to sparse matrix factorization are the symmetric-pattern, multifrontal methods <ref> [105, 76, 51, 121, 122, 77] </ref>. Large grain parallelism is available in these methods via independent subtrees in the assembly trees. However, as subtrees combine and parallelism at that level decreases, a switch is made to exploit parallelism at a finer grain within the factorization of a particular frontal matrix. <p> Assembly involves sending of contributions to appropriate follow-on processors where they are added to the appropriate entries. While these distributed multifrontal approaches typically require more communication than a fan-in approach, their performance has been quite promising for the Cholesky factorization of symmetric, positive definite matrices <ref> [76, 119, 120] </ref>. Specifically, speed-ups of 5.9 to 21.4 were achieved on a 32 processor Intel iPSC/2 using Cholesky factorization on matrices varying in order from 1,824 to 16,129 [119, 120]. Lucas has implemented a distributed memory mul-tifrontal LU factorization routine that assumes numerically acceptable pivots and a symmetric-pattern. <p> The exploitation of parallelism both in the elimination structure and within the distinct pivot steps has been found to be critical to the success of all of these methods <ref> [76, 51, 121] </ref> Recently, fine grain parallelism has been investigated by Gilbert and Schreiber for a variety of sparse Cholesky methods on a SIMD distributed memory environment [77]. The multifrontal approach has demonstrated the greatest potential for exploiting this level of parallelism. <p> Choice of Algorithm The algorithm chosen for dense matrix factorization is a column-oriented, fan-out method. This model was chosen because it is easy to analyze and implement and also because this was the approach taken in successful distributed memory implementations of multifrontal Cholesky factorization <ref> [105, 76] </ref>. Specifically, columns of the matrix are stored in a scattered fashion across the processors. For added simplicity, diagonal dominance is assumed, so no numerical pivoting will take place. An outer indexed loop provides one iteration for each pivot with which the matrix will be partially factorized.
Reference: [77] <author> J. R. Gilbert and R. Schreiber. </author> <title> Highly parallel sparse cholesky factorization. </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 13(5) </volume> <pages> 1151-1172, </pages> <year> 1992. </year>
Reference-contexts: Another class of methods (used for sparse Cholesky factorization) are the fan-in methods [9, 117, 88]. These methods accumulate contributions to the updating of the active submatrix and send fewer but larger messages [9]. An increasingly popular approach to sparse matrix factorization are the symmetric-pattern, multifrontal methods <ref> [105, 76, 51, 121, 122, 77] </ref>. Large grain parallelism is available in these methods via independent subtrees in the assembly trees. However, as subtrees combine and parallelism at that level decreases, a switch is made to exploit parallelism at a finer grain within the factorization of a particular frontal matrix. <p> the elimination structure and within the distinct pivot steps has been found to be critical to the success of all of these methods [76, 51, 121] Recently, fine grain parallelism has been investigated by Gilbert and Schreiber for a variety of sparse Cholesky methods on a SIMD distributed memory environment <ref> [77] </ref>. The multifrontal approach has demonstrated the greatest potential for exploiting this level of parallelism. A critical subissue in any parallel sparse matrix factorization routine is how to efficiently schedule the component tasks.
Reference: [78] <author> Gene H. Golub and Charles F. van Loan. </author> <title> Matrix Computations. </title> <publisher> The Johns Hopkins University Press, </publisher> <address> Baltimore, MD and London, UK, </address> <note> second edition, </note> <year> 1990. </year>
Reference-contexts: The version of Gauss Elimination presented in Figure 2.1 is based on the most common ordering of the nested loops (i.e., the kij ordering). However, alternative orderings are also possible that correspond to all six of the possible permutations of the indices k, i, and j <ref> [78] </ref>. for j := k to n do endfor for i := k+1 to n do for j := k+1 to n do endfor endfor endfor When row and column permutations are introduced into the algorithm, the fac torization becomes P AQ = LU where P provides the row permutations and <p> Consider the real value represented by a. The corresponding floating point value would be f l (a) = a (1 + *); * where * is the relative error introduced by the finite bit representation, which is bounded by the machine precision, , of the particular processor <ref> [78] </ref>. In a similar fashion, finite arithmetic computations introduce additional errors as results of the computations are subject to the same finite storage restrictions. Particularly vulnerable to this effect is addition (subtraction) as order of magnitude differences (similarities) in the values of the two operands cause increasing losses in accuracy. <p> (a + b) = (f l (a) + f l (b))(1 + * 1 ) = ((a (1 + * 2 )) + (b (1 + * 3 )))(1 + * 1 ) where * 1 , * 2 , and * 3 are each bounded by the machine precision <ref> [78] </ref>. Algorithm Stability: Algorithm stability (or instability) is a measure of the effects of computing round off errors on the computation sequence dictated by the algorithm. <p> When threshold pivoting is employed, the growth bounding function becomes (1 + u 1 ) n1 max ja ij j: When LU factorization is applied to a matrix A and no zero pivots are encountered <ref> [78] </ref>, the bound on H in ~ L ~ U = A + H is jHj 3 (n 1)[jAj + j ~ Ljj ~ U j] + O ( 2 ): 9 Here jM j refers to the largest entry in the matrix M . <p> An alternative, but equivalent formulation based on singular values is 2 (A) = 1 = n 10 where 1 is the largest singular value and n the smallest <ref> [78] </ref>. This later formulation illustrates that ill conditioning can be viewed as a measure of singularity as n will be zero for a singular n fi n matrix, and a relatively small value of n (as compared to 1 ) will produce a large condition number. <p> Specifically, the relative distance between the true (x) and computed (~x) solutions is k~x xk 1 1 r kAk 1 where r = kEkkA 1 k &lt; 1 <ref> [78] </ref>. Prevention of ill conditioning may be addressed by analyzing if the process has the same sensitivities as the model. If not, identify the problem sources in the model and alter the model. <p> Typical mathematical operations on vectors include dot products and the saxpy operation. The saxpy operation is defined as z = ffx + y where x, y, and z are n-vectors and ff is a scalar. The name saxpy comes from the software mnemonic for "scalar alpha x plus y" <ref> [78] </ref>. As the packed forms of sparse vectors are typically unordered (done for efficiency), operations on these vectors will frequently follow the outline provided below: * Scatter the nonzero entries of one of the vectors into a full length vector known to be initially zeroed. * For each operation: 1. <p> Specifically, each processing node can compute the updates for its rows of the block in parallel. Forward Substitutions The forward substitution proceeds down the columns of L i for the ith diagonal block and is based on a standard column-oriented forward substitution <ref> [78] </ref>. Each processing node checks if it contains the next column of L i . If it does, it computes the next component of the substitution's solution and then uses a SAXPY operation with this component and the rest of this column to update the right hand side.
Reference: [79] <author> J. Gustafson, G. Montry, and R. Benner. </author> <title> Development of parallel methods for a 1024-processor hypercube. </title> <journal> SIAM Journal of Scientific and Statistical Computing, </journal> <volume> 9 </volume> <pages> 609-638, </pages> <year> 1988. </year>
Reference: [80] <author> F. G. Gustavson. </author> <title> Finding the block lower-triangular form of a sparse matrix. </title> <editor> In J. R. Bunch and D. J. Rose, editors, </editor> <booktitle> Sparse Matrix Computations, </booktitle> <pages> pages 275-289. </pages> <publisher> Academic Press, </publisher> <address> New York and London, </address> <year> 1976. </year>
Reference: [81] <author> Gary D. Hachtel. </author> <title> Vector and matrix variability type in sparse matrix algorithms. </title> <editor> In D. J. Rose and R. A. Willoughby, editors, </editor> <booktitle> Sparse Matrices and Their Applications, </booktitle> <pages> pages 53-64. </pages> <publisher> Plenum Press, </publisher> <address> New York, NY, </address> <year> 1972. </year>
Reference: [82] <author> Gary D. Hachtel, Robert K. Brayton, and Fred G. Gustavson. </author> <title> The sparse tableau approach to network analysis and design. </title> <journal> IEEE Transactions on Circuit Theory, </journal> <volume> CT-18(1):101-113, </volume> <month> January </month> <year> 1971. </year>
Reference-contexts: Electronic circuit simulations are another common source of large systems of nonlinear, first order ODEs as evidenced in the modified nodal analysis approach used by Saleh and Kundert [131, 97] and the sparse tableau formulation of Hachtel <ref> [82] </ref>. 2.6 Parallel Matrix Computations A major emphasis of this research effort is to perform the numerical factorization of sequences of sparse matrices in parallel. Furthermore, the unsymmetric-pattern multifrontal approach provides dense submatrices which may also be factorized in parallel.
Reference: [83] <author> S. M. Hadfield and T. A. Davis. </author> <title> Analysis of potential parallel implementation of the unsymmetric-pattern multifrontal method for sparse LU factorization. </title> <type> Technical Report TR-92-017, </type> <institution> Department of Computer and Information Systems, University of Florida, </institution> <address> Gainesville, FL, </address> <year> 1992. </year>
Reference-contexts: This provides an additional degree of parallelism that can be 20 exploited by multiprocessors. The combination of these two sources of parallelism, within dense frontal matrix operations and between frontal matrices, has shown to provide a significant amount of potential speed-up in several studies <ref> [83, 51, 121, 122] </ref>. <p> This section is a summary of the bounded parallelism models. For a full description of the models and their revisions, see the Hadfield and Davis' technical report <ref> [83] </ref>. 4.2.1 Initial Models The initial bounded parallelism models follow directly from the unbounded parallelism models. The critical difference is that a limited processor set is assumed so tasks that are ready for execution may have to wait for available processors. <p> Very little difference was found using these scheduling methods and they will not be discussed further in this document (for a detailed description of these results, see <ref> [83] </ref>). 4.2.5 Simulation Results The results of the simulations are summarized for each model.
Reference: [84] <author> William W. Hager. </author> <title> Applied Numerical Linear Algebra. </title> <publisher> Prentice-Hall, </publisher> <address> Engle-wood Cliffs, NJ, </address> <year> 1988. </year>
Reference-contexts: Thus, there is a need to measure the stability of an algorithm. The relative residual, as defined below, provides such a measure <ref> [84] </ref>. relative residual = kA~x bk In this equation, ~x represents the computed solution. To understand how the relative residual measures stability, a simple derivation is useful.
Reference: [85] <author> M. Heath and C. Romine. </author> <title> Parallel solution of triangular systems on distributed-memory multiprocessors. </title> <journal> SIAM Journal of Scientific and Statistical Computing, </journal> <volume> 9(3) </volume> <pages> 558-588, </pages> <year> 1988. </year>
Reference-contexts: However, the many broadcast and parallel prefix operations together with the sequential orientation of the actual forward and back substitutions causes parallel execution time to be unacceptable for real applications. Furthermore, there are significant earlier efforts to implement parallel triangular solve algorithms that have been achieved better performance <ref> [56, 99, 85] </ref>. Therefore, I conclude the discussion of the distributed triangular solve algorithm with some thoughts on improving parallel execution time. I believe the most significant performance gains to be achieved by exploiting the natural parallelism revealed by the multifrontal method.
Reference: [86] <author> Michael T. Heath, Esmond Ng, and Barry W. Peyton. </author> <title> Parallel algorithms for sparse linear systems. </title> <editor> In R. J. Plemmons, editor, </editor> <booktitle> Parallel Algorithms for Matrix Computations, </booktitle> <pages> pages 83-124. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1990. </year>
Reference: [87] <author> T. C. Hu. </author> <title> Parallel sequencing and assembly line problems. </title> <journal> Operations Research, </journal> <volume> 9 </volume> <pages> 841-848, </pages> <year> 1961. </year> <month> 215 </month>
Reference: [88] <author> L. Hulbert and E. Zmijewski. </author> <title> Limiting communications in parallel sparse Cholesky factorization. </title> <journal> SIAM Journal of Scientific and Statistical Computing, </journal> <volume> 12(5) </volume> <pages> 1184-1197, </pages> <year> 1991. </year>
Reference-contexts: Additional advantages are typically found when using contemporary architectures by organizing tasks to use fewer, but larger, messages. Hulbert and Zmijewski propose such a method that combine update contributions into a single message <ref> [88] </ref>. While some granularity is lost, there are fewer message setup delays which tend to dominate much of the communication cost in contemporary architectures. <p> This is the fan-out approach that has been used in initial Cholesky factorization routines. The excessive communication overheads of these methods limited their performance. Another class of methods (used for sparse Cholesky factorization) are the fan-in methods <ref> [9, 117, 88] </ref>. These methods accumulate contributions to the updating of the active submatrix and send fewer but larger messages [9]. An increasingly popular approach to sparse matrix factorization are the symmetric-pattern, multifrontal methods [105, 76, 51, 121, 122, 77].
Reference: [89] <author> J. J. Hwang, Y. C. Chow, F. D. Anger, and C. Y. Lee. </author> <title> Scheduling precedence graphs in systems with interprocessor communication times. </title> <journal> SIAM Journal of Computing, </journal> <volume> 18(2) </volume> <pages> 244-257, </pages> <year> 1989. </year>
Reference-contexts: Three methods for dealing with such Task with Communications Scheduling (TCS) problems are presented in this section. ETF Earliest Task First: The Earliest Task First (ETF) heuristic <ref> [89] </ref> is based on an extension of the Rayward-Smith computing model [125]. This extended model uses message counts for edge weights in the task graph together with a transmission cost function that defines the communication costs between any two processors.
Reference: [90] <author> K. R. Jackson and W. L. Seward. </author> <title> Adaptive linear equation solvers in codes for large stiff systems of ODEs. </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 14(4) </volume> <pages> 800-823, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: Specific methods vary in their value for p and the weights given by the ff i 's and fi i 's. Such methods include Adams-Bashford (explicit), Adams-Moulton (implicit), Nystrom, Newton-Cates, and the Backward Differentiation Formula <ref> [107, 124, 90] </ref>. For reasons of stability, implicit methods are preferred for "stiff" (that is, ill-conditioned) problems. In particular, explicit linear multi-step methods cannot meet A-stable criteria and can only meet A (ff)-stable criteria if p 3 [107]. <p> Thus Newton's method can be used to solve for the zero of the modified function: G (x) = F (x) b = 0: This has proven useful in practice due to the quadratic convergence of Newton's method <ref> [107, 90] </ref>. The same approach is used when f is a nonlinear function (representing a system of nonlinear ODEs). When Newton's method is applied within each time step as above, each Newton iteration requires solving a linearized system (as discussed in the previous subsection on systems of nonlinear algebraic equations).
Reference: [91] <author> J. A. G. Jess and H. G. M. Kees. </author> <title> A data structure for parallel L/U decomposition. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 31 </volume> <pages> 231-239, </pages> <year> 1982. </year>
Reference-contexts: With this model (which restricts the factorization to a given pivot ordering), the maximal parallelism available can be realized. A large grain model was developed by Jess and Kees <ref> [91] </ref> that defines a task as the factorizing and complete corresponding update of a single pivot. Liu [102] establishes a third, medium grain model with the various column operations associated with a single pivot defined as tasks with one task per column.
Reference: [92] <author> S. Lenhart Johnsson. </author> <title> Communication efficient basic linear algebra computations on hypercube architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 4 </volume> <pages> 133-172, </pages> <year> 1987. </year>
Reference: [93] <author> L. </author> <title> Kantorovich. Functional Analysis in Normed Spaces. </title> <publisher> Pergamon Press, Oxford, </publisher> <address> U.K., </address> <year> 1964. </year>
Reference-contexts: formula by replacing x 1 with x k+1 and x 0 with x k : x k+1 = x k f (x k )=f 0 (x k ): The method can be generalized from strictly real (or complex) functions to any mapping between Banach spaces by using the Frechet derivative <ref> [93] </ref>. A Banach space is a normed vector space that is complete (that is, all Cauchy sequences will converge to an element in the space).
Reference: [94] <author> H. Kasahara and S. Narita. </author> <title> Practical multiprocessor scheduling algorithms for efficient parallel processing. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-33(11), </volume> <year> 1984. </year>
Reference-contexts: Variations within these schemes primarily deal with tie breaking with a Most Immediate Successors First criteria both popular and effective <ref> [94] </ref>. Within the context of a distributed memory environment, communication delays (represented by edge weights in the task precedence graph) can become quite significant. Three methods for dealing with such Task with Communications Scheduling (TCS) problems are presented in this section.
Reference: [95] <author> K. C. Knowlton. </author> <title> A fast storage allocator. </title> <journal> Communications of the ACM, </journal> <volume> 8 </volume> <pages> 623-625, </pages> <month> October </month> <year> 1965. </year>
Reference-contexts: Care is taken to avoid unnecessary fragmentation of the hypercube. The key to the S3 method is the efficient management of subcube allocations and the ability to easily coalesce adjacent available subcubes into larger subcubes. An excellent method for doing this is the binary buddy system <ref> [95] </ref>. One of the interesting (and frequently useful) properties of the hypercube topology is that it can be split along any dimension into two subcubes of the next smaller dimension. <p> In response to this concern, the S3 method was developed to make maximal use of available processors/subcubes. Subcubes are scheduled in strict critical path priority with subcube allocations managed by an augmented binary buddy management system <ref> [95] </ref>.
Reference: [96] <author> C. P. Kruskal, L. Rudolph, and M. Snir. </author> <title> Techniques for parallel manipulation of sparse matrices. </title> <journal> Theoretical Computer Science, </journal> <volume> 64 </volume> <pages> 135-157, </pages> <year> 1989. </year>
Reference: [97] <author> K. S. Kundert. </author> <title> Sparse matrix techniques and their applications to circuit simulation. </title> <editor> In A. E. Ruehli, editor, </editor> <title> Circuit Analysis, Simulation and Design. </title> <address> New York: </address> <publisher> North-Holland, </publisher> <year> 1986. </year>
Reference-contexts: Electronic circuit simulations are another common source of large systems of nonlinear, first order ODEs as evidenced in the modified nodal analysis approach used by Saleh and Kundert <ref> [131, 97] </ref> and the sparse tableau formulation of Hachtel [82]. 2.6 Parallel Matrix Computations A major emphasis of this research effort is to perform the numerical factorization of sequences of sparse matrices in parallel. Furthermore, the unsymmetric-pattern multifrontal approach provides dense submatrices which may also be factorized in parallel.
Reference: [98] <author> Th. Lengauer and Ch. Wieners. </author> <title> Efficient solutions of hierarchical systems of linear equations. </title> <journal> Computing, </journal> <volume> 39 </volume> <pages> 111-132, </pages> <year> 1987. </year>
Reference: [99] <author> G. Li and T. Coleman. </author> <title> A parallel triangular solver for a distributed-memory multiprocessor. </title> <journal> SIAM Journal of Scientific and Statistical Computing, </journal> <volume> 9(3) </volume> <pages> 485-502, </pages> <year> 1988. </year>
Reference-contexts: However, the many broadcast and parallel prefix operations together with the sequential orientation of the actual forward and back substitutions causes parallel execution time to be unacceptable for real applications. Furthermore, there are significant earlier efforts to implement parallel triangular solve algorithms that have been achieved better performance <ref> [56, 99, 85] </ref>. Therefore, I conclude the discussion of the distributed triangular solve algorithm with some thoughts on improving parallel execution time. I believe the most significant performance gains to be achieved by exploiting the natural parallelism revealed by the multifrontal method.
Reference: [100] <author> W. Lichtenstein and S. L. Johnsson. </author> <title> Block-cyclic dense linear algebra. </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 14(6) </volume> <pages> 1259-1288, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: By varying the parameters of this method both pure blocked and pure scattered (together with inbe-tween formats) can be achieved. A block-cyclic strategy has recently been proposed by Lichtenstein and Johnsson for dense linear algebra on distributed memory multiprocessors <ref> [100] </ref>. The block-cyclic strategy is applied to both rows and columns and is effectively applied to LU factorization and the subsequent triangular solves. Furthermore, pivoting, if required, can be done in a row or column fashion.
Reference: [101] <author> J. W. H. Liu. </author> <title> A compact row storage scheme for Cholesky factors using elimination trees. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 12 </volume> <pages> 127-148, </pages> <year> 1986. </year>
Reference: [102] <author> J. W. H. Liu. </author> <title> Computational models and task scheduling for parallel sparse Cholesky factorization. </title> <journal> Parallel Computing, </journal> <volume> 3 </volume> <pages> 327-342, </pages> <year> 1986. </year> <month> 216 </month>
Reference-contexts: With this model (which restricts the factorization to a given pivot ordering), the maximal parallelism available can be realized. A large grain model was developed by Jess and Kees [91] that defines a task as the factorizing and complete corresponding update of a single pivot. Liu <ref> [102] </ref> establishes a third, medium grain model with the various column operations associated with a single pivot defined as tasks with one task per column.
Reference: [103] <author> J. W. H. Liu. </author> <title> On the storage requirement in the out-of-core multifrontal method for sparse factorization. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 12 </volume> <pages> 249-264, </pages> <year> 1986. </year>
Reference: [104] <author> J. W. H. Liu. </author> <title> The multifrontal method for sparse matrix solution: </title> <journal> Theory and practice. SIAM Review, </journal> <volume> 34(1) </volume> <pages> 82-109, </pages> <year> 1992. </year>
Reference: [105] <author> R. Lucas, T. Blank, and J. Tiemann. </author> <title> A parallel solution method for large sparse systems of equations. </title> <journal> IEEE Transactions on Computer-Aided Design, </journal> <volume> CAD-6(6):981-991, </volume> <month> November </month> <year> 1987. </year>
Reference-contexts: Another class of methods (used for sparse Cholesky factorization) are the fan-in methods [9, 117, 88]. These methods accumulate contributions to the updating of the active submatrix and send fewer but larger messages [9]. An increasingly popular approach to sparse matrix factorization are the symmetric-pattern, multifrontal methods <ref> [105, 76, 51, 121, 122, 77] </ref>. Large grain parallelism is available in these methods via independent subtrees in the assembly trees. However, as subtrees combine and parallelism at that level decreases, a switch is made to exploit parallelism at a finer grain within the factorization of a particular frontal matrix. <p> Choice of Algorithm The algorithm chosen for dense matrix factorization is a column-oriented, fan-out method. This model was chosen because it is easy to analyze and implement and also because this was the approach taken in successful distributed memory implementations of multifrontal Cholesky factorization <ref> [105, 76] </ref>. Specifically, columns of the matrix are stored in a scattered fashion across the processors. For added simplicity, diagonal dominance is assumed, so no numerical pivoting will take place. An outer indexed loop provides one iteration for each pivot with which the matrix will be partially factorized.
Reference: [106] <author> H. M. Markowitz. </author> <title> The elimination form of the inverse and its application to linear programming. </title> <journal> Management Science, </journal> <volume> 3 </volume> <pages> 255-269, </pages> <month> April </month> <year> 1957. </year>
Reference-contexts: Markowitz Criterion: When selecting the kth pivot for the factorizing of the A (k) active submatrix, the Markowitz Criterion chooses the a (k) ij entry that is numerically acceptable and minimizes the product of the corresponding row and column degrees <ref> [106] </ref>. The row degree for an entry is simply the number of nonzeroes currently in the entry's row of the active submatrix. Likewise, the column degree is the number of nonzeroes in the entry's column of the active submatrix.
Reference: [107] <author> Willard L. Miranker. </author> <title> Numerical Methods for Stiff Equations. </title> <address> D. </address> <publisher> Reidel Publishing Company, </publisher> <address> Dordrecht, Holland; Boston, USA; London, England, </address> <year> 1981. </year>
Reference-contexts: The predominant type of numerical methods for such tasks when initial values are provided are linear single- and multi-step techniques due mainly to their ease of implementation and analysis <ref> [107] </ref>. These techniques divide the independent axis into steps or mesh points across which the differential equations are integrated via a discrete summation method. <p> Specific methods vary in their value for p and the weights given by the ff i 's and fi i 's. Such methods include Adams-Bashford (explicit), Adams-Moulton (implicit), Nystrom, Newton-Cates, and the Backward Differentiation Formula <ref> [107, 124, 90] </ref>. For reasons of stability, implicit methods are preferred for "stiff" (that is, ill-conditioned) problems. In particular, explicit linear multi-step methods cannot meet A-stable criteria and can only meet A (ff)-stable criteria if p 3 [107]. <p> For reasons of stability, implicit methods are preferred for "stiff" (that is, ill-conditioned) problems. In particular, explicit linear multi-step methods cannot meet A-stable criteria and can only meet A (ff)-stable criteria if p 3 <ref> [107] </ref>. These stability conditions relate the effectiveness of a method to the conditioning of the problem as evidenced by the eigenvalues of the corresponding matrix. <p> Thus Newton's method can be used to solve for the zero of the modified function: G (x) = F (x) b = 0: This has proven useful in practice due to the quadratic convergence of Newton's method <ref> [107, 90] </ref>. The same approach is used when f is a nonlinear function (representing a system of nonlinear ODEs). When Newton's method is applied within each time step as above, each Newton iteration requires solving a linearized system (as discussed in the previous subsection on systems of nonlinear algebraic equations). <p> Other methods also exist and are particularly useful for solving stiff systems. These include nonlinear methods such as Certaine's method, Jain's method and the class of Runge-Kutta techniques <ref> [107] </ref>. Certaine's method is also an example of another class called predictor/corrector methods where an explicit technique is used to predict a first approximation to x n+1 and then an implicit method is used to correct (refine) the approximation. <p> More specifically, Miranker provides examples of systems of nonlinear ODEs with the circuit simulation of tunnel diodes commonly used in high speed circuits, thermal decomposition of ozone, and the behavior of a catalytic fluidized bed <ref> [107] </ref>.
Reference: [108] <author> M. Nakhla, K. Singhal, and J. Vlach. </author> <title> An optimal pivoting order for the solution of sparse systems of equations. </title> <journal> IEEE Transactions on Circuits and Systems, </journal> <volume> CAS-21(2):222-225, </volume> <month> Mar </month> <year> 1974. </year> <title> [109] nCUBE. nCUBE 2 Processor Manual. </title> <address> Foster City, CA, </address> <year> 1992. </year> <title> [110] nCUBE. nCUBE 2 Programmer's Guide. </title> <address> Foster City, CA, </address> <year> 1992. </year> <title> [111] nCUBE. nCUBE 2 Programmers Reference Manual. </title> <address> Foster City, CA, </address> <year> 1992. </year> <title> [112] nCUBE. nCUBE 2 Systems: Technical Overview. </title> <address> Foster City, CA, </address> <year> 1992. </year> <title> [113] nCUBE. ndoc on-line documentation facility, </title> <year> 1992. </year>
Reference: [114] <author> R. S. Norin and C. Pottle. </author> <title> Effective ordering of sparse matrices arising from nonlinear electrical networks. </title> <journal> IEEE Transactions on Circuit Theory, </journal> <volume> CT-18:139-145, </volume> <month> Jan </month> <year> 1971. </year>
Reference: [115] <author> G. J. Nutt. </author> <title> Centralized and Distributed Operating Systems. </title> <publisher> Prentice-Hall, Inc., </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1992. </year>
Reference-contexts: The actual calling sequence is similar to that employed for remote procedure calls <ref> [115] </ref>. A host-based user processor calls one of the four host resident routines that provide the interface to the parallel factorization code. Once these routines send the appropriate messages to the parallel processors, they wait for a response and return the received results via the standard function return value.
Reference: [116] <author> Dianne P. O'Leary and G. W. Stewart. </author> <title> Assignment and scheduling in parallel matrix factorization. </title> <booktitle> Linear Algebra and its Applications, </booktitle> <pages> pages 275-299, </pages> <year> 1986. </year>
Reference: [117] <author> L. S. Ostrouchov, M. T. Heath, and C. H. Romine. </author> <title> Modeling speedup in parallel sparse matrix factorization. </title> <type> Technical Report ORNL/TM-11786, </type> <institution> Oak Ridge National Laboratory, Oak Ridge, TN, </institution> <year> 1990. </year>
Reference-contexts: Liu [102] establishes a third, medium grain model with the various column operations associated with a single pivot defined as tasks with one task per column. Ostrouchov, Heath, and Romine <ref> [117] </ref> investigate whether the rather disappointing results seen so far with Cholesky factorization are due to an inherent lack of parallelism in the problems or to limitations on contemporary hardware. They conclude that parallelism is available, but the relatively high cost of communication (relative to computation) has limited performance. <p> This is the fan-out approach that has been used in initial Cholesky factorization routines. The excessive communication overheads of these methods limited their performance. Another class of methods (used for sparse Cholesky factorization) are the fan-in methods <ref> [9, 117, 88] </ref>. These methods accumulate contributions to the updating of the active submatrix and send fewer but larger messages [9]. An increasingly popular approach to sparse matrix factorization are the symmetric-pattern, multifrontal methods [105, 76, 51, 121, 122, 77].
Reference: [118] <author> S. V. Parter. </author> <title> The use of linear graphs in Gaussian elimination. </title> <journal> SIAM Review, </journal> <volume> 3 </volume> <pages> 119-130, </pages> <year> 1961. </year> <month> 217 </month>
Reference-contexts: When Gauss Elimination is performed on a matrix to zero a column below the diagonal, the structure of the matrix changes and corresponding alterations occur within the graph representation <ref> [118] </ref>. In particular, assume the column below the 13 0 @ X X 1 A diagonal that corresponds to the node y in the graph is eliminated.
Reference: [119] <author> A. Pothen and C. Sun. </author> <title> A distributed multifrontal algorithm using clique trees. </title> <type> Technical Report CS-91-24, </type> <institution> Department of Computer Science, Pennsylvania State University, University Park, </institution> <address> PA, </address> <year> 1991. </year>
Reference-contexts: Assembly involves sending of contributions to appropriate follow-on processors where they are added to the appropriate entries. While these distributed multifrontal approaches typically require more communication than a fan-in approach, their performance has been quite promising for the Cholesky factorization of symmetric, positive definite matrices <ref> [76, 119, 120] </ref>. Specifically, speed-ups of 5.9 to 21.4 were achieved on a 32 processor Intel iPSC/2 using Cholesky factorization on matrices varying in order from 1,824 to 16,129 [119, 120]. Lucas has implemented a distributed memory mul-tifrontal LU factorization routine that assumes numerically acceptable pivots and a symmetric-pattern. <p> Specifically, speed-ups of 5.9 to 21.4 were achieved on a 32 processor Intel iPSC/2 using Cholesky factorization on matrices varying in order from 1,824 to 16,129 <ref> [119, 120] </ref>. Lucas has implemented a distributed memory mul-tifrontal LU factorization routine that assumes numerically acceptable pivots and a symmetric-pattern. Speed-ups of up to 10.2 were obtained on 16 processors of an iPSC/2 using electronic device simulation matrices of order 7225.
Reference: [120] <author> A. Pothen and C. Sun. </author> <title> A mapping algorithm for parallel sparse cholesky factorization. </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 14(5) </volume> <pages> 1253-1257, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: Assembly involves sending of contributions to appropriate follow-on processors where they are added to the appropriate entries. While these distributed multifrontal approaches typically require more communication than a fan-in approach, their performance has been quite promising for the Cholesky factorization of symmetric, positive definite matrices <ref> [76, 119, 120] </ref>. Specifically, speed-ups of 5.9 to 21.4 were achieved on a 32 processor Intel iPSC/2 using Cholesky factorization on matrices varying in order from 1,824 to 16,129 [119, 120]. Lucas has implemented a distributed memory mul-tifrontal LU factorization routine that assumes numerically acceptable pivots and a symmetric-pattern. <p> Specifically, speed-ups of 5.9 to 21.4 were achieved on a 32 processor Intel iPSC/2 using Cholesky factorization on matrices varying in order from 1,824 to 16,129 <ref> [119, 120] </ref>. Lucas has implemented a distributed memory mul-tifrontal LU factorization routine that assumes numerically acceptable pivots and a symmetric-pattern. Speed-ups of up to 10.2 were obtained on 16 processors of an iPSC/2 using electronic device simulation matrices of order 7225.
Reference: [121] <author> Roldan Pozo. </author> <title> Performance modeling of sparse matrix methods for distributed memory architectures. </title> <booktitle> In CONPAR '92, </booktitle> <address> VAPP-V, Philadelphia, PA, Septem-ber 1992. </address> <publisher> SIAM. </publisher>
Reference-contexts: This provides an additional degree of parallelism that can be 20 exploited by multiprocessors. The combination of these two sources of parallelism, within dense frontal matrix operations and between frontal matrices, has shown to provide a significant amount of potential speed-up in several studies <ref> [83, 51, 121, 122] </ref>. <p> Another class of methods (used for sparse Cholesky factorization) are the fan-in methods [9, 117, 88]. These methods accumulate contributions to the updating of the active submatrix and send fewer but larger messages [9]. An increasingly popular approach to sparse matrix factorization are the symmetric-pattern, multifrontal methods <ref> [105, 76, 51, 121, 122, 77] </ref>. Large grain parallelism is available in these methods via independent subtrees in the assembly trees. However, as subtrees combine and parallelism at that level decreases, a switch is made to exploit parallelism at a finer grain within the factorization of a particular frontal matrix. <p> Speed-ups of up to 10.2 were obtained on 16 processors of an iPSC/2 using electronic device simulation matrices of order 7225. Recent simulation studies of similar methods for the LU factorization of (assumed-)symmetric matrices <ref> [121, 122] </ref> have shown a reasonable parallel potential in distributed memory environments. Here speed-ups of up to 30-40 were predicted for 128 node iPSC/2 and iPSC/860 hypercube topology multiprocessors. <p> The exploitation of parallelism both in the elimination structure and within the distinct pivot steps has been found to be critical to the success of all of these methods <ref> [76, 51, 121] </ref> Recently, fine grain parallelism has been investigated by Gilbert and Schreiber for a variety of sparse Cholesky methods on a SIMD distributed memory environment [77]. The multifrontal approach has demonstrated the greatest potential for exploiting this level of parallelism. <p> This model indicates a relatively low communication to computation ratio compared to other machines in the class <ref> [121] </ref> and is characteristic of a well balanced design. In more detail, the nCUBE 2 uses an e-code default routing that progresses from least to most significant bit in the destination address.
Reference: [122] <author> Roldan Pozo and Sharon Smith. </author> <title> Performance evaluation of the parallel multi-frontal method in a distributed memory environment. </title> <booktitle> In SIAM 6th Conference of Parallel Processing for Scientific Computing. </booktitle> <publisher> SIAM, </publisher> <month> March </month> <year> 1993. </year>
Reference-contexts: This provides an additional degree of parallelism that can be 20 exploited by multiprocessors. The combination of these two sources of parallelism, within dense frontal matrix operations and between frontal matrices, has shown to provide a significant amount of potential speed-up in several studies <ref> [83, 51, 121, 122] </ref>. <p> Another class of methods (used for sparse Cholesky factorization) are the fan-in methods [9, 117, 88]. These methods accumulate contributions to the updating of the active submatrix and send fewer but larger messages [9]. An increasingly popular approach to sparse matrix factorization are the symmetric-pattern, multifrontal methods <ref> [105, 76, 51, 121, 122, 77] </ref>. Large grain parallelism is available in these methods via independent subtrees in the assembly trees. However, as subtrees combine and parallelism at that level decreases, a switch is made to exploit parallelism at a finer grain within the factorization of a particular frontal matrix. <p> Speed-ups of up to 10.2 were obtained on 16 processors of an iPSC/2 using electronic device simulation matrices of order 7225. Recent simulation studies of similar methods for the LU factorization of (assumed-)symmetric matrices <ref> [121, 122] </ref> have shown a reasonable parallel potential in distributed memory environments. Here speed-ups of up to 30-40 were predicted for 128 node iPSC/2 and iPSC/860 hypercube topology multiprocessors. <p> The results of this revision to the model are shown in the Tables 4-14 and 4-15. 4.4 Conclusions While the results may not be outwardly spectacular, they are actually very promising when compared to a similar simulation study based on a distributed memory version of the classical multifrontal method <ref> [122] </ref>. In this study by Pozo, speed-ups for the BCSSTK24 matrix were 29.27 based on an iPSC/2 hypercube and 23.35 based Table 4-10.
Reference: [123] <author> M. Prastein. </author> <title> Precedence-constrained scheduling with minimum time and communications. </title> <type> Master's thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1987. </year>
Reference: [124] <author> Douglas Quinney. </author> <title> An Introduction to the Numerical Solution of Differential Equations. </title> <publisher> Research Studies Press, Ltd, </publisher> <address> Letchworth, Hertfordshire, England, </address> <note> revised edition, </note> <year> 1987. </year>
Reference-contexts: Specific methods vary in their value for p and the weights given by the ff i 's and fi i 's. Such methods include Adams-Bashford (explicit), Adams-Moulton (implicit), Nystrom, Newton-Cates, and the Backward Differentiation Formula <ref> [107, 124, 90] </ref>. For reasons of stability, implicit methods are preferred for "stiff" (that is, ill-conditioned) problems. In particular, explicit linear multi-step methods cannot meet A-stable criteria and can only meet A (ff)-stable criteria if p 3 [107].
Reference: [125] <author> V. J. Rayward-Smith. </author> <title> UET scheduling with interprocessor communication delays. </title> <type> Technical Report SYS-C86-06, </type> <institution> University of East Anglia, Norwich, U.K., </institution> <year> 1986. </year>
Reference-contexts: Three methods for dealing with such Task with Communications Scheduling (TCS) problems are presented in this section. ETF Earliest Task First: The Earliest Task First (ETF) heuristic [89] is based on an extension of the Rayward-Smith computing model <ref> [125] </ref>. This extended model uses message counts for edge weights in the task graph together with a transmission cost function that defines the communication costs between any two processors. Different network topologies can be represented by varying this transmission cost function. Task computation times are represented as node weights. <p> Different network topologies can be represented by varying this transmission cost function. Task computation times are represented as node weights. Rayward-Smith proposed a list scheduling (LS) approach based on a greedy strategy for his original unit execution time/unit communication time (UET/UCT) model <ref> [125] </ref>. The idea is that no processor should be left idle if there is an available task to execute.
Reference: [126] <author> J. K. Reid. TREESOLVE. </author> <title> A Fortran package for solving linear sets of linear finite-element equations. </title> <type> Technical Report Report CSS 155, </type> <institution> Computer Science and Systems Division, Harwell Laboratory, Oxon, U.K., </institution> <year> 1984. </year>
Reference: [127] <author> D. J. Rose and R. E. Tarjan. </author> <title> Algorithmic aspects of vertex elimination. </title> <booktitle> In Proc. 7th Annual Symposium on the Theory of Computing, </booktitle> <pages> pages 245-254, </pages> <year> 1975. </year>
Reference-contexts: zero and that U (F i ) U (F 1 i ): The breaking of F i into two independent diagonal blocks implies that the matrix was not in block upper triangular form. 2 A more elegant proof of Theorem 7.32 is possible using the results of Rose and Tarjan <ref> [128, 127] </ref> but a different graph formulation of the matrix would be required. The results of Theorem 7.32 are very powerful in that they provide for a number of simplifications for lost pivot recovery, which are defined in the following theorems.
Reference: [128] <author> D. J. Rose and R. E. Tarjan. </author> <title> Algorithmic aspects of vertex elimination on directed graphs. </title> <journal> SIAM J. Appl. Math., </journal> <volume> 34(1) </volume> <pages> 176-197, </pages> <year> 1978. </year>
Reference-contexts: zero and that U (F i ) U (F 1 i ): The breaking of F i into two independent diagonal blocks implies that the matrix was not in block upper triangular form. 2 A more elegant proof of Theorem 7.32 is possible using the results of Rose and Tarjan <ref> [128, 127] </ref> but a different graph formulation of the matrix would be required. The results of Theorem 7.32 are very powerful in that they provide for a number of simplifications for lost pivot recovery, which are defined in the following theorems.
Reference: [129] <author> Youcef Saad. </author> <title> Communication complexity of Gaussian elimination algorithm on multiprocessors. </title> <journal> Linear Algebra and its Applications, </journal> <volume> 77 </volume> <pages> 315-340, </pages> <year> 1986. </year>
Reference: [130] <author> Youcef Saad and Martin H. Schultz. </author> <title> Topological properties of hypercubes. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 37(7) </volume> <pages> 867-872, </pages> <month> July </month> <year> 1988. </year>
Reference: [131] <author> R. Saleh, K. Gallivan, M. Chang, I. Hajj, D. Smart, and T. </author> <title> Trick. Parallel circuit simulation on supercomputers. </title> <journal> Proc. IEEE, </journal> <volume> 77(12) </volume> <pages> 1915-1931, </pages> <year> 1989. </year>
Reference-contexts: Electronic circuit simulations are another common source of large systems of nonlinear, first order ODEs as evidenced in the modified nodal analysis approach used by Saleh and Kundert <ref> [131, 97] </ref> and the sparse tableau formulation of Hachtel [82]. 2.6 Parallel Matrix Computations A major emphasis of this research effort is to perform the numerical factorization of sequences of sparse matrices in parallel. Furthermore, the unsymmetric-pattern multifrontal approach provides dense submatrices which may also be factorized in parallel.
Reference: [132] <author> N. Sato and W. F. Tinney. </author> <title> Techniques for exploiting the sparsity of the network admittance matrix. </title> <journal> IEEE Transactions on Power, </journal> <volume> PAS-82:944-949, </volume> <year> 1963. </year> <month> 218 </month>
Reference: [133] <author> B. Shirazi, M. Wang, and G. Pathak. </author> <title> Analysis and evaluation of heuristic methods for static task scheduling. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 10 </volume> <pages> 222-232, </pages> <year> 1990. </year>
Reference-contexts: Within shared memory environments, a list scheduling scheme is typically used. Under such schemes, tasks are assigned priorities and when made ready (predecessors in the partial order have all completed) are put into a single priority queue for scheduling by next available processor <ref> [133] </ref>. The most common and effective priority schemes are based on a critical path analysis of the precedence DAG with the priority of a task defined as the heaviest weighted path from that task's node to an exit node (node with no successors) in the DAG [23].
Reference: [134] <author> Mandayam A. Srinivas. </author> <title> Optimal parallel scheduling of Gaussian elimination. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-32(12):1109-1117, </volume> <month> December </month> <year> 1983. </year>
Reference: [135] <author> H. S. Stone. </author> <title> High-Performance Computer Architecture. </title> <publisher> Addison-Wesley Publishing, Co., </publisher> <address> Reading, MA, </address> <note> second edition, </note> <year> 1990. </year>
Reference-contexts: As the assembly coded BLAS effectively reduce the cost of computations, they lower the ratio of required computation time over communication time (frequently called the R/C ratio) <ref> [135] </ref>. The result is a higher relative cost for communication and reduced speed-ups.
Reference: [136] <author> W. F. Tinney and J. W. Walker. </author> <title> Direct solutions of sparse network equations by optimally ordered triangular factorization. </title> <booktitle> Proceedings of IEEE, </booktitle> <volume> 55 </volume> <pages> 1801-1809, </pages> <year> 1967. </year>
Reference: [137] <author> J. D. Ullman. </author> <title> NP-Complete scheduling problems. </title> <journal> Journal of Computer System Science, </journal> <volume> 10 </volume> <pages> 384-393, </pages> <year> 1975. </year>
Reference-contexts: A critical subissue in any parallel sparse matrix factorization routine is how to efficiently schedule the component tasks. This issue will be discussed in the next section. 2.7 Multiprocessor Scheduling The precedence constrained multiprocessor scheduling problem has been shown to be NP-Complete in both shared and distributed memory environments <ref> [137] </ref>. Thus, 32 heuristic techniques have been the emphasis of much of the research in this area. Within shared memory environments, a list scheduling scheme is typically used.
Reference: [138] <author> J. H. Wilkinson. </author> <title> Error analysis of direct methods of matrix inversion. </title> <journal> Journal of the ACM, </journal> <volume> 8 </volume> <pages> 281-330, </pages> <year> 1961. </year>
Reference-contexts: Complete pivoting chooses the kth pivot as the largest entry in the entire active submatrix A (k) . In doing so the growth is limited to = f (n) max ja ij j where f (n) is a nearly linear function <ref> [138] </ref>. A problem with both partial and complete pivoting is that the rigid pivot selection rules can result in extensive fill-in (fill-in is the changing of zero entries into nonzero entries) when dealing with sparse matrices.
Reference: [139] <author> J. H. Wilkinson. </author> <title> The Algebraic Eigenvalue Problem. </title> <publisher> Oxford Press, </publisher> <address> London, </address> <year> 1965. </year>
Reference-contexts: This difference is accounted for in the matrix 8 term H where ~ L ~ U = A + H: Wilkinson <ref> [139] </ref> showed that the entrywise deviations accounted for by H in the absence of pivoting are bounded by jh ij j 5:01n; where = max k (k) for each h ij 2 H where a (k) ij refers to the value of a ij after the kth step of factorization. <p> kth column of the active submatrix such that ja kk j ja ik j; i &gt; k: With this approach the growth represented by is limited to = 2 n1 max ja ij j: While this is not a particularly good bound, in practice the results are consistently much better <ref> [139] </ref>. A stronger alternative to partial pivoting is complete pivoting. Complete pivoting chooses the kth pivot as the largest entry in the entire active submatrix A (k) .
Reference: [140] <author> Omar Wing and John W. Huang. </author> <title> A computation model of parallel solution of linear equations. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-29(7):632-638, </volume> <year> 1980. </year>
Reference-contexts: Wing and Huang developed a very fine grain model with each divide/update operation on a single entry considered a task <ref> [140] </ref>. With this model (which restricts the factorization to a given pivot ordering), the maximal parallelism available can be realized. A large grain model was developed by Jess and Kees [91] that defines a task as the factorizing and complete corresponding update of a single pivot.
Reference: [141] <author> M. Yannakakis. </author> <title> Computing the minimum fill-in is NP-Complete. </title> <journal> SIAM Journal of Algebra and Discrete Methods, </journal> <volume> 2 </volume> <pages> 77-79, </pages> <year> 1981. </year>
Reference-contexts: This in itself is a very difficult problem. In fact, a minimum fill-in pivot selection is NP-complete in the restricted case where the diagonal consists of all nonzero entries and all pivots are chosen from the diagonal <ref> [141] </ref>. However, there are also other 19 considerations. They include the selection of pivots to preserve numerical stability, as well as, parallel and vector processing considerations. Furthermore, the selection of a minimum fill-in pivot sequence may be so costly as to overshadow any potential savings in the factorization.
Reference: [142] <author> Z. Yin, C. Chui, R. Shu, and K. Huang. </author> <title> Two precedence-related task-scheduling algorithms. </title> <journal> International Journal of High Speed Computing, </journal> <volume> 3 </volume> <pages> 223-240, </pages> <year> 1991. </year>
Reference-contexts: The Earliest Task First (ETF) seems to be a very natural and effective extension to the shared memory environment list scheduling methods. SCST Scheduling with Communication Successors' Priority Tracking: Another algorithm that addresses the scheduling of task graphs with nonzero communication costs is due to Yin <ref> [142] </ref>. This method defines the combinatorial level of a particular task node as the heaviest node and edge weighted path from that node to any accessible leaf node (exit node). The node weights represent task execution times and the edge weights reflect communication costs.
Reference: [143] <author> S. E. Zitney. </author> <title> A frontal code for ASPEN PLUS on advanced architecture computers. </title> <booktitle> In Proc. AIChE Annual Meeting, Symposium on Parallel Computing, </booktitle> <address> Chicago, IL, </address> <year> 1990. </year> <journal> American Institute of Chemical Engineers. </journal>
Reference-contexts: One application area of such nonlinear algebraic equations is chemical engineering. Specifically, Zitney discusses the solution of such systems for distillation flowsheet-ing problems where the Jacobians are of an unsymmetric-pattern and fail to have favorable numerical properities such as diagonal dominance <ref> [144, 143] </ref>. Systems of Ordinary Differential Equations: Another major type of problems where the factorization of sequences of identically structured matrices is required is the solving of systems of ordinary differential equations using an implicit method. <p> To this end, three test matrices were chosen. RDIST1 and EXTR1 come from chemical engineering applications <ref> [144, 143] </ref> and GEMAT11 from a power system application [50]. RDIST2 and RDIST3A are also from chemical engineering applications [144, 143] and will only be used as additional test matrices for the parallelism. All of these matrices are the lead matrices in sequences of identically structured, unsymmetric pattern sparse matrices. <p> To this end, three test matrices were chosen. RDIST1 and EXTR1 come from chemical engineering applications <ref> [144, 143] </ref> and GEMAT11 from a power system application [50]. RDIST2 and RDIST3A are also from chemical engineering applications [144, 143] and will only be used as additional test matrices for the parallelism. All of these matrices are the lead matrices in sequences of identically structured, unsymmetric pattern sparse matrices. Some of the characteristics of these matrices are provided in Table 6-1. <p> However, sequences were available for only the RDIST1, RDIST2, and RDIST3A matrices. These sequences consist of 41, 40, and 37 distinct matrices, respectively. These sequences are from three different chemical engineering distillation problems that were originally solved using a unifrontal method <ref> [144, 143] </ref>. This method maintains fully assembled rows, which allow partial pivoting. The various matrices in the sequence correspond to the iterations of a Newton's method so the values of the next matrix in the sequence will be dependent upon the solution of the current matrix.
Reference: [144] <author> S. E. Zitney and M. A. Stadtherr. </author> <title> A frontal algorithm for equation-based chemical process flowsheeting on vector and parallel computers. </title> <booktitle> In Proc. AIChE Annual Meeting, </booktitle> <address> Washington, DC, </address> <year> 1988. </year> <journal> American Institute of Chemical Engineers. </journal>
Reference-contexts: One application area of such nonlinear algebraic equations is chemical engineering. Specifically, Zitney discusses the solution of such systems for distillation flowsheet-ing problems where the Jacobians are of an unsymmetric-pattern and fail to have favorable numerical properities such as diagonal dominance <ref> [144, 143] </ref>. Systems of Ordinary Differential Equations: Another major type of problems where the factorization of sequences of identically structured matrices is required is the solving of systems of ordinary differential equations using an implicit method. <p> To this end, three test matrices were chosen. RDIST1 and EXTR1 come from chemical engineering applications <ref> [144, 143] </ref> and GEMAT11 from a power system application [50]. RDIST2 and RDIST3A are also from chemical engineering applications [144, 143] and will only be used as additional test matrices for the parallelism. All of these matrices are the lead matrices in sequences of identically structured, unsymmetric pattern sparse matrices. <p> To this end, three test matrices were chosen. RDIST1 and EXTR1 come from chemical engineering applications <ref> [144, 143] </ref> and GEMAT11 from a power system application [50]. RDIST2 and RDIST3A are also from chemical engineering applications [144, 143] and will only be used as additional test matrices for the parallelism. All of these matrices are the lead matrices in sequences of identically structured, unsymmetric pattern sparse matrices. Some of the characteristics of these matrices are provided in Table 6-1. <p> However, sequences were available for only the RDIST1, RDIST2, and RDIST3A matrices. These sequences consist of 41, 40, and 37 distinct matrices, respectively. These sequences are from three different chemical engineering distillation problems that were originally solved using a unifrontal method <ref> [144, 143] </ref>. This method maintains fully assembled rows, which allow partial pivoting. The various matrices in the sequence correspond to the iterations of a Newton's method so the values of the next matrix in the sequence will be dependent upon the solution of the current matrix.
Reference: [145] <author> Z. Zlatev. </author> <title> On some pivotal strategies in Gaussian elimination by sparse technique. </title> <journal> SIAM Journal of Numerical Analysis, </journal> <volume> 17 </volume> <pages> 18-30, </pages> <year> 1980. </year>
Reference: [146] <author> Z. Zlatev. </author> <title> Sparse matrix techniques for general matrices with real elements: Pivotal strategies, decompositions and applications in ODE software. </title> <editor> In D. J. Evans, editor, </editor> <booktitle> Sparsity and Its Applications, </booktitle> <pages> pages 185-228. </pages> <address> Cambridge, United Kingdom: </address> <publisher> Cambridge University Press, </publisher> <year> 1985. </year> <month> 219 </month>
Reference: [147] <author> Zahari Zlatev. </author> <title> Computational Methods for General Sparse Matrices. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Dordrecht, Boston, London, </address> <year> 1991. </year>
Reference-contexts: Furthermore, these systems are frequently encountered in applications that include nuclear magnetic resonance spectroscopy, computational 28 chemistry, and computational biology <ref> [147] </ref>. More specifically, Miranker provides examples of systems of nonlinear ODEs with the circuit simulation of tunnel diodes commonly used in high speed circuits, thermal decomposition of ozone, and the behavior of a catalytic fluidized bed [107].

References-found: 142

