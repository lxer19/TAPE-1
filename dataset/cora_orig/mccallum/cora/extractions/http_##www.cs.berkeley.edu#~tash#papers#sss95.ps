URL: http://www.cs.berkeley.edu/~tash/papers/sss95.ps
Refering-URL: http://www.cs.berkeley.edu/~tash/papers/sss95.html
Root-URL: 
Email: tash@cs.berkeley.edu  
Title: Abstract Actions for Stochastic Planning  
Author: Jonathan King Tash 
Address: Berkeley, CA 94720  
Affiliation: Group in Logic and the Methodology of Science University of California  
Abstract: This paper presents a method for abstracting action representation, reducing the computational burden of stochastic planning. Previous methods of forming abstractions in probabilistic environments have relied on summarizing clusters of states or actions with worst-case or average feature values. In contrast to these, the current proposal treats abstract actions as plans to plan. An underspecified action sequence is used in abstract plans like the expected consequences of its realization. This more accurately reflects our natural use of abstract actions, and improves their utility for planning. An exemplification of this idea is presented for maze route-finding modeled as a Markov decision process. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Boutilier, C. and Dearden, R. </author> <year> (1994). </year> <title> Using abstractions for decision-theoretic planning with time constraints. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence. </booktitle> <publisher> AAAI Press/ MIT Press. </publisher>
Reference: <author> Dean, T., Kaelbling, L., Kirman, J., and Nicholson, A. </author> <year> (1993). </year> <title> Deliberation scheduling for time-critical sequential decision making. </title> <booktitle> In Proceedings of the Ninth Conference on Uncertainty in Artificial Intelligence. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: It is reasonable to hope that the decision-theoretic methods of (Tash and Russell, 1994) can be extended to this problem. Those methods were used to choose an appropriate neighborhood of the current state on which to update the policy. That form of localized computation (similar to that discussed in <ref> (Dean et. al., 1993) </ref>) can be considered an abstraction of the current type where all states bordering the chosen neighborhood are assigned an abstract action leading to the goal with an expected cost of their estimated remaining distance.
Reference: <author> Haddaway, P. and Doan, A. </author> <year> (1994). </year> <title> Abstracting probabilistic actions. </title> <booktitle> In Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Howard, R. </author> <year> (1960). </year> <title> Dynamic Programming and Markov Processes. </title> <publisher> MIT Press. </publisher>
Reference-contexts: A plan will be a generalization of a concrete policy, which assigns a physical action to each state. The optimal concrete policy for a given Markov decision process can be found using various algorithms such as policy iteration <ref> (Howard, 1960) </ref>, but these are too computationally demanding for use with large state spaces. A planner must be able to generate a useful approximation to such a policy with considerably lower computational overhead.
Reference: <author> Simon, H. </author> <year> (1955). </year> <title> A behavioral model of rational choice. </title> <journal> Quarterly Journal of Economics, </journal> <volume> 69, </volume> <pages> 99-118. </pages>
Reference-contexts: This characterization of abstract planning has several interesting parallels with historic AI planning approaches. The freedom to realize a plan with any stronger action substituted in is reminiscent of satisficing <ref> (Simon, 1955) </ref>; the planner on recursive calls needs only construct an expansion of the action of adequate strength, rather than optimize, in order to fulfill the role of the action in producing the abstract approximation to the global optimum.
Reference: <author> Tash, J. </author> <year> (1993). </year> <title> A framework for planning under uncertainty. In Spring Symposium on Foundations of Automatic Planning: The Classical Approach and Beyond, </title> <type> AAAI Tech. Report. </type>
Reference-contexts: Such a representation will expand the state space enormously its self-referentiality can even cause an infinite blowup so careful clustering abstractions would be needed to manage the larger space. Such a definition for planning is cursorily treated in <ref> (Tash, 1993) </ref>. Even for the presented planning architecture, there are many ways in which the traditional clustering abstraction methods could increase its efficacy. The function estimating realizability can be seen as an example of such a method of generalization, as it is a summary of previous cases of action abstraction.
Reference: <author> Tash, J. </author> <title> (1994a) Issues in constructing and learning abstract decision models. </title> <note> In Spring Symposium on DecisionTheoretic Planning, AAAI Tech. Report. </note>
Reference-contexts: This method faces various This work was supported by NASA JPL through the Graduate Student Researchers Program. difficulties: worst-case analyses tend to degenerate into uninformativeness, and choice of an appropriate distribution for averaging can be as hard as solving the original, unabstracted problem <ref> (Tash, 1994a) </ref>. It has, however, been used quite effectively for simplifying both state and action descriptions (Boutilier and Dearden, 1994; Haddaway and Doan, 1994).
Reference: <author> Tash, J. </author> <title> (1994b) Formal rationality and limited agents. </title> <booktitle> In Proceedings of the Sixteenth Annual Conference of the Cognitive Science Society. </booktitle> <publisher> Lawrence Erlbaum Assoc. </publisher>
Reference-contexts: It can be used as an element in other planning activities so long as some information about its expected results is available. Such information cannot in general be fully computed, as that would involve doing the work which the abstraction is supposed to postpone <ref> (Tash, 1994b) </ref>. However, generalizing from similar previous refinement efforts (i.e. applying clustering abstraction to the space of abstract actions) may allow initial estimates useful for planning to be formed.
Reference: <author> Tash, J. and Russell, S. </author> <title> (1994) Control strategies for a stochastic planner. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence. </booktitle> <publisher> AAAI Press/ MIT Press. </publisher>
Reference-contexts: Especially when actions can have indeterminate effects, the search space of possible plans is too large to be explored efficiently. The results of a decision-theoretic optimal plan must be approximated, ideally in a manner making effective use of the available computational resources. A previous paper <ref> (Tash and Russell, 1994) </ref> has discussed the use of decision theory to control allocation of computational resources so as to direct computations towards maximal plan improvement, assuming a model of computational effectiveness generalized from previous efforts using a presumed learning bias. <p> This is a special case of the general Markov decision process, to which the following considerations are applicable. As discussed in <ref> (Tash and Russell, 1994) </ref>, the agent needs to make tradeoffs between calculating the optimal move for all possible states it could find itself in and reducing its uncertainty by actually moving to see where it really has to plan from. <p> D i s c u s s i o n This model for abstraction is preliminary in several respects. The choice of a good abstraction has not yet been addressed. It is reasonable to hope that the decision-theoretic methods of <ref> (Tash and Russell, 1994) </ref> can be extended to this problem. Those methods were used to choose an appropriate neighborhood of the current state on which to update the policy.
References-found: 9

