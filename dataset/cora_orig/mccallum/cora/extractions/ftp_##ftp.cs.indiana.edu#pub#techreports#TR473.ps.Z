URL: ftp://ftp.cs.indiana.edu/pub/techreports/TR473.ps.Z
Refering-URL: http://www.cs.indiana.edu/trindex.html
Root-URL: 
Email: ajcbik@cs.indiana.edu  
Phone: 215,  
Title: Automatically Exploiting Implicit Parallelism in Multi-way Recursive Methods in Java  
Author: Aart J.C. Bik and Dennis B. Gannon 
Affiliation: Computer Science Dept., Indiana University  
Address: Lindley Hall  Bloomington, Indiana 47405-4101, USA  
Abstract: In this paper we show how implicit parallelism in multi-way recursive methods, typically used to implement tree traversal or divide-and-conquer algorithms, can be made explicit by a restructuring compiler using the multi-threading mechanism of Java. Expressing parallelism in Java itself clearly has the advantage that the transformed program remains portable. After compilation of the transformed Java program into byte-code, speedup can be obtained on any platform on which the Java byte-code interpreter support the true parallel execution of threads, whereas only a slight overhead is induced on uni-processors.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Ken Arnold and James Gosling. </author> <title> The Java Programming Language. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1996. </year>
Reference-contexts: In previous work [2] we have shown how a restructuring compiler can exploit implicit loop parallelism in Java programs. In this paper we discuss a method that can be used by a restructuring compiler to make implicit parallelism in multi-way recursive methods explicit by means of multi-threading (see e.g. <ref> [5, 1, 3, 4, 8, 10, 11, 12, 16] </ref> for a detailed presentation of multi-threading in Java). Since threads are lightweight processes that share an address-space, true parallel execution of threads is usually only supported on shared-address space architectures [7].
Reference: [2] <author> Aart J.C. Bik and Dennis B. Gannon. </author> <title> A strategy for exploiting implicit loop parallelism in java programs. </title> <type> Technical Report TR-465, </type> <institution> Computer Science Department, Indiana University, </institution> <year> 1996. </year>
Reference-contexts: With this approach, performance close to the performance of compiled languages can be obtained. However, because the demand for more computing power is likely to remain, other means to speedup Java programs have to be found. In previous work <ref> [2] </ref> we have shown how a restructuring compiler can exploit implicit loop parallelism in Java programs.
Reference: [3] <author> H.M. Deitel and P.J. Deitel. </author> <title> Java, How to Program. </title> <publisher> Prentice-Hall, </publisher> <year> 1997. </year>
Reference-contexts: In previous work [2] we have shown how a restructuring compiler can exploit implicit loop parallelism in Java programs. In this paper we discuss a method that can be used by a restructuring compiler to make implicit parallelism in multi-way recursive methods explicit by means of multi-threading (see e.g. <ref> [5, 1, 3, 4, 8, 10, 11, 12, 16] </ref> for a detailed presentation of multi-threading in Java). Since threads are lightweight processes that share an address-space, true parallel execution of threads is usually only supported on shared-address space architectures [7]. <p> `-noasyncgc' and with both just-in-time compilation and the parallel execution of threads enabled. 3.1 Tree Traversals In this section, we illustrate the parallelization of multi-way recursive methods in full detail with two very simple tree traversal methods for trees containing integer data items that are implemented as follows (see e.g. <ref> [3] </ref> for discussion of how some typical data structures can be implemented in Java): class Tree - int val; Tree left, right ... <p> invocation trees are always well-balanced (independent of the actual values of the elements), the overhead of data movement has a clear impact on the performance of this sorting method. 18 3.4.2 Merge Sorting of Linked Lists Consider a linked-list of integers that are implemented using the following class List (cf. <ref> [3] </ref>): class List - int val; List next; List (int val, next) - this.val = val; this.next = next; - - Under the assumption that each list is terminated with a special node SENTINEL that points to itself and with a data item larger than all elements in the list, merge-sorting
Reference: [4] <author> David Flanagan. </author> <title> Java in a Nutshell. </title> <publisher> O'Reilly & Associates, </publisher> <address> Sebastopol, CA, </address> <year> 1996. </year>
Reference-contexts: In previous work [2] we have shown how a restructuring compiler can exploit implicit loop parallelism in Java programs. In this paper we discuss a method that can be used by a restructuring compiler to make implicit parallelism in multi-way recursive methods explicit by means of multi-threading (see e.g. <ref> [5, 1, 3, 4, 8, 10, 11, 12, 16] </ref> for a detailed presentation of multi-threading in Java). Since threads are lightweight processes that share an address-space, true parallel execution of threads is usually only supported on shared-address space architectures [7].
Reference: [5] <author> James Gosling, Bill Joy, and Guy Steele. </author> <title> Java Programming Language. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1996. </year>
Reference-contexts: In previous work [2] we have shown how a restructuring compiler can exploit implicit loop parallelism in Java programs. In this paper we discuss a method that can be used by a restructuring compiler to make implicit parallelism in multi-way recursive methods explicit by means of multi-threading (see e.g. <ref> [5, 1, 3, 4, 8, 10, 11, 12, 16] </ref> for a detailed presentation of multi-threading in Java). Since threads are lightweight processes that share an address-space, true parallel execution of threads is usually only supported on shared-address space architectures [7]. <p> for a void-method: if (wi_x != null) - try - wi_x.join (); - catch (Expection e) -- ri = wi_x.result_x; - If ri is a local variable, it may be necessary to add a dummy assignment to the declaration of this variable to preserve the definite assignment property of Java <ref> [5] </ref>, because the original assignment has been moved into two conditional statements. 4 After these transformations have been applied to an n-way recursive method, n-way forks will be performed in the top levels 0 : : : c of the method invocation tree in case CUT DEPTH = c.
Reference: [6] <author> C.A.R. </author> <title> Hoare. </title> <journal> Quick-sorting. Communications of the ACM. </journal>
Reference-contexts: In the next section we will see that starting some additional threads can also be useful to overcome less trivial load im-balancing. 3.2 Quick Sorting As an example of a typical divide-and-conquer algorithm, consider the following implementation of quick-sorting <ref> [6] </ref> in which, as advocated in [15] small sub-arrays are sorted by insertion sorting to prevent further recursive method invocations for small sub-arrays: 1 1 Alternatively, we could ignore small sub-arrays during the recursion and apply a single insertion sort to the whole array afterwards.
Reference: [7] <author> Vipin Kumar, Ananth Grama, Anshul Gupta, and George Karypis. </author> <title> Introduction to Parallel Programming. </title> <publisher> The Benjamin/Cummings Publishing Company, </publisher> <address> Redwood City, CA, </address> <year> 1994. </year>
Reference-contexts: Since threads are lightweight processes that share an address-space, true parallel execution of threads is usually only supported on shared-address space architectures <ref> [7] </ref>. Hence, the focus of this paper is on obtaining speedup on such architectures. The rest of this paper is organized as follows. <p> The most straightforward way to exploit implicit parallelism in a parallel n-way recursive method is to let a running thread assign all but one of the recursive method invocations to other threads <ref> [7, 9, 13, 14] </ref>. Although our method is based on this simple approach, the analysis shown below reveals the limitation on the corresponding speedup, and better ways of parallelizing an algorithm may exist.
Reference: [8] <author> Doug Lea. </author> <title> Concurrent Programming in Java. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1997. </year>
Reference-contexts: In previous work [2] we have shown how a restructuring compiler can exploit implicit loop parallelism in Java programs. In this paper we discuss a method that can be used by a restructuring compiler to make implicit parallelism in multi-way recursive methods explicit by means of multi-threading (see e.g. <ref> [5, 1, 3, 4, 8, 10, 11, 12, 16] </ref> for a detailed presentation of multi-threading in Java). Since threads are lightweight processes that share an address-space, true parallel execution of threads is usually only supported on shared-address space architectures [7].
Reference: [9] <author> Ted G. Lewis. </author> <title> Foundations of Parallel Programming. </title> <publisher> IEEE Computer Society Press, </publisher> <address> Wash-ington, </address> <year> 1994. </year>
Reference-contexts: The most straightforward way to exploit implicit parallelism in a parallel n-way recursive method is to let a running thread assign all but one of the recursive method invocations to other threads <ref> [7, 9, 13, 14] </ref>. Although our method is based on this simple approach, the analysis shown below reveals the limitation on the corresponding speedup, and better ways of parallelizing an algorithm may exist.
Reference: [10] <author> Michael Morrison. </author> <title> Java Unleashed. </title> <address> Samsnet, Indianapolis, Indiana, </address> <year> 1996. </year>
Reference-contexts: In previous work [2] we have shown how a restructuring compiler can exploit implicit loop parallelism in Java programs. In this paper we discuss a method that can be used by a restructuring compiler to make implicit parallelism in multi-way recursive methods explicit by means of multi-threading (see e.g. <ref> [5, 1, 3, 4, 8, 10, 11, 12, 16] </ref> for a detailed presentation of multi-threading in Java). Since threads are lightweight processes that share an address-space, true parallel execution of threads is usually only supported on shared-address space architectures [7].
Reference: [11] <author> Patrick Naughton. </author> <title> The Java Handbook. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1996. </year>
Reference-contexts: In previous work [2] we have shown how a restructuring compiler can exploit implicit loop parallelism in Java programs. In this paper we discuss a method that can be used by a restructuring compiler to make implicit parallelism in multi-way recursive methods explicit by means of multi-threading (see e.g. <ref> [5, 1, 3, 4, 8, 10, 11, 12, 16] </ref> for a detailed presentation of multi-threading in Java). Since threads are lightweight processes that share an address-space, true parallel execution of threads is usually only supported on shared-address space architectures [7].
Reference: [12] <author> Patrick Niemeyer and Joshua Peck. </author> <title> Exploring Java. </title> <publisher> O'Reilly & Associates, </publisher> <address> Sebastopol, CA, </address> <year> 1996. </year>
Reference-contexts: In previous work [2] we have shown how a restructuring compiler can exploit implicit loop parallelism in Java programs. In this paper we discuss a method that can be used by a restructuring compiler to make implicit parallelism in multi-way recursive methods explicit by means of multi-threading (see e.g. <ref> [5, 1, 3, 4, 8, 10, 11, 12, 16] </ref> for a detailed presentation of multi-threading in Java). Since threads are lightweight processes that share an address-space, true parallel execution of threads is usually only supported on shared-address space architectures [7].
Reference: [13] <author> Michael J. Quinn. </author> <title> Designing Efficient Algorithms for Parallel Computers. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: The most straightforward way to exploit implicit parallelism in a parallel n-way recursive method is to let a running thread assign all but one of the recursive method invocations to other threads <ref> [7, 9, 13, 14] </ref>. Although our method is based on this simple approach, the analysis shown below reveals the limitation on the corresponding speedup, and better ways of parallelizing an algorithm may exist.
Reference: [14] <author> Michael J. Quinn. </author> <title> Parallel Computing: Theory and Practice. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: The most straightforward way to exploit implicit parallelism in a parallel n-way recursive method is to let a running thread assign all but one of the recursive method invocations to other threads <ref> [7, 9, 13, 14] </ref>. Although our method is based on this simple approach, the analysis shown below reveals the limitation on the corresponding speedup, and better ways of parallelizing an algorithm may exist.
Reference: [15] <author> Robert Sedgewick. </author> <title> Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1988. </year> <note> Second Edition. </note>
Reference-contexts: In the next section we will see that starting some additional threads can also be useful to overcome less trivial load im-balancing. 3.2 Quick Sorting As an example of a typical divide-and-conquer algorithm, consider the following implementation of quick-sorting [6] in which, as advocated in <ref> [15] </ref> small sub-arrays are sorted by insertion sorting to prevent further recursive method invocations for small sub-arrays: 1 1 Alternatively, we could ignore small sub-arrays during the recursion and apply a single insertion sort to the whole array afterwards. <p> accessing a shared-tree control, the effects of the corresponding improved load-balancing become only clear for sorting arrays with a length that exceeds 1; 000; 000. 15 3.3 Radix-Exchange Sorting An alternative divide-and-conquer sorting algorithm that can better adapt to integers with truly random bits is so-called radix-exchange sorting (see e.g. <ref> [15] </ref>). <p> sorting is more expensive than serial quick/insertion sorting, the parallel versions perform better for these random integer arrays, because the method invocation trees are kept more balanced. 3.4 Merge Sorting In this section, we discuss the results of the parallelization of an array and linked-lists version of merge-sorting (see e.g. <ref> [15] </ref>). 3.4.1 Merge Sorting of Arrays An implementation of merge-sorting that uses insertion sorting for small sub-arrays is shown below.
Reference: [16] <author> Glenn L. Vanderburg et al. </author> <title> Tricks of the Java Programming Gurus. </title> <address> Samsnet, Indianapolis, Indiana, </address> <year> 1996. </year> <month> 24 </month>
Reference-contexts: In previous work [2] we have shown how a restructuring compiler can exploit implicit loop parallelism in Java programs. In this paper we discuss a method that can be used by a restructuring compiler to make implicit parallelism in multi-way recursive methods explicit by means of multi-threading (see e.g. <ref> [5, 1, 3, 4, 8, 10, 11, 12, 16] </ref> for a detailed presentation of multi-threading in Java). Since threads are lightweight processes that share an address-space, true parallel execution of threads is usually only supported on shared-address space architectures [7].
References-found: 16

