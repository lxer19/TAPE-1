URL: ftp://ftp.cs.wisc.edu/computer-vision/theses/thesis-lai.ps
Refering-URL: http://www.cs.wisc.edu/computer-vision/pubs.html
Root-URL: 
Title: DEFORMABLE CONTOURS: MODELING, EXTRACTION, DETECTION AND CLASSIFICATION  
Author: by KOK FUNG LAI 
Degree: A thesis submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy (Electrical Engineering) at the  
Date: 1994  
Affiliation: UNIVERSITY OF WISCONSIN-MADISON  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> G. P. Ashkar & J. W. Modestino, </author> <title> "The Contour Extraction Problem with Biomedical Applications," </title> <journal> Computer Graphics Image Processing, </journal> <volume> vol. 7, </volume> <year> 1978, </year> <pages> pp. 331-355. </pages>
Reference-contexts: The snake then turns boundary extraction into an energy minimization problem. Under this framework, one seeks to obtain V fl where V fl = arg min n X i E int (v i ) + (1 i )E ext (v i ) (2.1) i 2 <ref> [0; 1] </ref> are the regularization parameters that significantly influence the solution to (2.1). Setting (1 ) emphasizes regularization, yielding strongly model-driven solutions which are robust to noise. In contrast, small values of enable the snakes to effectively capture boundary discontinuities, but they also make the solutions very noise sensitive. <p> Thus, for a fixed V, the maximum value of e (V; ) occurs at either = 0 or = 1 and this maximum value is max (E int (V); E ext (V)). For each 2 <ref> [0; 1] </ref>, let V be the optimum snake that minimizes equation (2.2) corresponding to the chosen , i.e., V = arg min e (V; ) (2.4) and let U () = e (V ; ). <p> Consequently only an optimum snake can be the minimax solution for this figure. Moreover, by examination of Figure 2.2 (b), we see that the minimax solution is an optimum snake V fl for a parameter fl that maximizes U () over 2 <ref> [0; 1] </ref>: min max e (V; ) = max min e (V; ) (2.6) Since fl maximizes the minimum snake energy U (), a minimax snake is the optimum snake that minimizes the worst case energy. <p> Proposition 2 Suppose that fl is a solution to U ( fl ) = max U () where 2 <ref> [0; 1] </ref>. Suppose further that either fl = 0 or fl = 1, or E int (V fl ) = E ext (V fl ). Then V fl is a snake that satisfies the minimax criterion. <p> Let 1 = <ref> [1; 1; :::; 1] </ref> T . <p> Solving for MAP estimation and ignoring constants, we have fU map ; g map g = arg min n X ( 2 + 2 ) U;g i=1 i E int (u i ) + E ext (u i ; g) (3.23) 44 where i = + 2 2 <ref> [0; 1] </ref> (3.24) are the local regularization parameters which control the amount of local template deformation. The formulation in (3.23) is analogous to the active contour models (snakes) [21]. However, the original E int only constrains the solution to the class of controlled continuity splines. <p> The experiment uses synthesized images containing deformed rectangles as shown in Figure 3.9. We deform the boundary on each of the 4 sides independently using a randomly generated cosine function. Let b (s) represents an undeformed boundary indexed by s 2 <ref> [0; 1] </ref>. The deformed boundary d (s) is given by: d (s) = b (s) + cos (s + ) (3.43) where - : N (0; 2 - ) is a normally distributed boundary deformation process and is a random phase shift uniformly distributed in [0; 2). <p> For rectangles, we deform the boundary on each of the 4 sides independently using a randomly generated cosine function. Let b (s) represents an undeformed boundary indexed by s 2 <ref> [0; 1] </ref>. The deformed boundary d (s) is given by: d (s) = b (s) + cos (s + ) (4.37) where - : N (0; 2 - ) is a normally distributed boundary deformation process and is a random phase shift uniformly distributed in [0; 2).
Reference: [2] <author> A. A. Amini, T.E. Weymouth & R. C. Jain, </author> <title> "Using Dynamic Programming for Solving Variational Problems in Vision," </title> <journal> IEEE Trans. Pat. Anal. Mach. Intell., </journal> <volume> vol. PAMI-12, no. 9, </volume> <year> 1990, </year> <pages> pp. 855-867. </pages>
Reference-contexts: In contrast, small values of enable the snakes to effectively capture boundary discontinuities, but they also make the solutions very noise sensitive. Figure 2.1 illustrates these conflicting goals. Despite its significance, regularization selection were either ignored [6, 21, 29, 49], or based on empirical observations <ref> [2, 9] </ref> and heuristics [46]. Another factor which directly affects the solutions is energy formulation. The original E int formulation [21] causes a snake to shrink around strong edge points. <p> Another factor which directly affects the solutions is energy formulation. The original E int formulation [21] causes a snake to shrink around strong edge points. Ballooning force [9] and hard constraint on snaxel proximity <ref> [2] </ref> have been suggested, but these require prior information in setting the weighing factor or constraint. Moreover, existing curvature measure on E int is not rotational and scale invariant, causing erroneous imposition of the smoothness constraint. Snaxel initialization is the next factor with significant impact. <p> Moreover, existing curvature measure on E int is not rotational and scale invariant, causing erroneous imposition of the smoothness constraint. Snaxel initialization is the next factor with significant impact. For satisfactory results, most existing methods <ref> [2, 21, 46] </ref> require an operator to judiciously place the initial snaxels near desired boundaries. Alternatively, short snakes are initialized at locations of high intensity gradient and they either overlap [49] or extend in length [6] to cover a longer boundary. <p> Note from (2.14) that this criterion completely bypasses the explicit selection of the nuisance parameters fl fl . It can therefore be incorporated into discrete energy minimization algorithms <ref> [2, 46] </ref> with no added computational cost. <p> Firstly, E int vanishes when v i = v for all i, often causing a snake to shrink around strong edge points. To counter the shrinkage tendency, ballooning force [9] and hard constraint on snaxel proximity <ref> [2] </ref> have been suggested, but these require prior information in setting the weighing factor or constraint. Secondly, E int in (2.17) is not invariant to scale change and rotation, thus often causing error in constraining smoothness on the snake. <p> Fortunately, we can exploit the local characteristics of the Markov random field to yield an algorithm with complexity O (nm 3 ). The basic idea is to decompose the minimization process into n independent stages, where each stage considers only 3 neighboring points. This idea is first proposed in <ref> [2] </ref> under the framework of dynamic programming.
Reference: [3] <author> G. P. Ashkar & J. W. Modestino, </author> <title> "The Contour Extraction Problem with Biomedical Applications," </title> <journal> Computer Graphics Image Processing, </journal> <volume> vol. 7, </volume> <year> 1978, </year> <pages> pp. 331-355. </pages>
Reference-contexts: While enormous amount of literature exists in each of the fields, these works have typically been developed in isolation. The following is a representative list: contour modeling [13, 33, 35, 48], edge detection [8, 11, 17, 30, 43], edge linking <ref> [3, 32, 34] </ref>, contour detection and classification [18, 20, 36, 45]. This strict dichotomy of visual tasks has been widely adopted because it decomposes the problem of image analysis into independent and managable components. <p> The enormous amount of research in edge detection can be broadly classified into one of several categories: optimal filtering [8, 11], scale space [30], surface fitting [17] and cost minimization [43]. A great deal of literature also exists on edge linking <ref> [3, 32, 34] </ref>. Despite this tremendous effort, there is little consensus that the problem of boundary extraction has been solved. A problem with this approach is that errors made in edge detection propagate to edge linking without opportunity for correction. <p> Thus the starting point of modeling usually consists of segmented (binary) images, or an ordered set of points forming the contour. On the other hand, the two essential tasks in extraction, edge detection [8, 11, 30] and edge linking <ref> [3] </ref>, are usually attempted without reference to a contour model. Such strict dichotomy of visual tasks is charateristic of Marr's paradigm [31], widely adopted because it decomposes the problem of image analysis into independent and managable components.
Reference: [4] <author> D. H. Ballard, </author> <title> "Generalizing the Hough Transform to Detect Arbitrary Shapes," </title> <journal> Pattern Recognition, </journal> <volume> vol. 13, </volume> <year> 1981, </year> <pages> pp. 111-122. </pages>
Reference-contexts: Moreover, due to image noise and the image projection process, the lower level visual tasks are generally ill-posed [37], i.e., they are underconstrained and so do not have unique solutions. The classical and generalized Hough transforms <ref> [12, 4] </ref> are popular methods that combine modeling and extraction for rigid contours. Because these methods work by accumulating large number of votes, they are relatively insensitive to small vote fluctuation caused by noise and occlusion. In fact, if the superimposed noise is i.i.d. <p> In existing literature dealing with contour modeling, specific knowledge is sometimes used to yield satisfactory extraction. See [33] for an example. However, tailored to specific applications, such techniques are seldom transferrable. The Hough transform <ref> [4, 12] </ref> is a well-known method that combines contour modeling 33 and extraction for rigid objects. Choosing a model a priori, one accumulates local evidence of its presence by incrementing cells in the appropriate parameter space. <p> In the same token, contours that may be deterministically deformed have the following ML estimates: f g; Tg = arg max max n X h T where the estimated contour U = TU. The correlation can be efficiently implemented 42 using generalized Hough transforms <ref> [4] </ref>. Note that the computation cost required in (3.19) depends greatly on the range and resolution of T. For example, if one expects the contours to be scaled and rotated, then the range of T should span the two operations. <p> Under these assumptions, it can be shown [40] that matched template is optimum in terms of probability of error. Moreover, since it essentially involves correlating a particular template with the image, the matched template can be efficiently implemented using the classical or generalized Hough transform <ref> [4, 12] </ref>. In practice, however, the assumption of rigidity is often violated. The contours of interest often exhibit deformation which arises from diversity and irregularity of shape. A good example is handwritten character recognition, where considerable deformation is produced by variation in writing style, as evidenced in Figure 4.1.
Reference: [5] <author> A. Blake, R. Curwen & A. Zisserman, </author> <title> "Affine-invariant Contour Tracking with Automatic Control of Spatiotemporal Scale", </title> <booktitle> Proc Int. Conf. on Comp. Vis., </booktitle> <year> 1993, </year> <pages> pp. 66-75. </pages>
Reference-contexts: Recently, numerous works in deformable templates have been reported, using weak models which deform in conformation to salient image features. Examples include the active contour model (snake) [21], parameterized templates [47], elliptic Fourier decompositions [41], implicit polynomials [42], handprinted character recognition [19] and affine invariant contour tracking <ref> [5] </ref>. In addition, several methods that classify deformable contours have also been reported [18, 45], but these ignore the problem of extraction. <p> (snake) which uses smoothness constraints to restrict its solutions to controlled continuity splines [21]; parameterized templates for facial feature extraction [47]; elliptic Fourier decomposition for objects with shape irregularities [41]; implicit polynomials for curve and surface modeling [42]; movable control points for hand-printed character recognition [19]; and affine-invariant contour tracking <ref> [5] </ref>. With the exception of [5], these methods typically consider only global [41, 42, 47] or local [19, 21] deformations. <p> to restrict its solutions to controlled continuity splines [21]; parameterized templates for facial feature extraction [47]; elliptic Fourier decomposition for objects with shape irregularities [41]; implicit polynomials for curve and surface modeling [42]; movable control points for hand-printed character recognition [19]; and affine-invariant contour tracking <ref> [5] </ref>. With the exception of [5], these methods typically consider only global [41, 42, 47] or local [19, 21] deformations. While global templates involve large structural interactions and contain fewer parameters to be optimized, these global parameters cannot exercise local control along the contour and their physical meaning is sometimes obscure. <p> Combined with the local characteristics of the Markov random field to model local deformations, this yields a prior distribution that exerts influence over a global model while allowing for deformations. Our approach differs from <ref> [5] </ref> which achieves affine invariance by creating a subspace containing all allowable transformations of a memorized template. Besides possessing the added advantage of being unique, affine invariance of shape matrix is implicit and does not require exhaustive comparison into a created subspace. <p> Firstly, the shape matrix is invariant under affine transformation, which is crucial in detecting or classifying objects undergoing rigid motions in the 3-D space. Furthermore, affine invariance of the shape matrix is implicit, unlike Fourier descriptors [41] or memorized templates <ref> [19, 20, 5] </ref> which require exhaustive comparison into a created subspace. Secondly, the shape matrix is unique. Hence two contours that cannot be related by an affine transformation cannot have similar shape matrix. Recognition is impossible without this characteristics.
Reference: [6] <author> M. Berger & R. Mohr, </author> <title> "Towards Autonomy in Active Contour Models," </title> <booktitle> IEEE 10th International Conference on Pattern Recognition, </booktitle> <year> 1990, </year> <month> pp.847-851. </month>
Reference-contexts: In contrast, small values of enable the snakes to effectively capture boundary discontinuities, but they also make the solutions very noise sensitive. Figure 2.1 illustrates these conflicting goals. Despite its significance, regularization selection were either ignored <ref> [6, 21, 29, 49] </ref>, or based on empirical observations [2, 9] and heuristics [46]. Another factor which directly affects the solutions is energy formulation. The original E int formulation [21] causes a snake to shrink around strong edge points. <p> For satisfactory results, most existing methods [2, 21, 46] require an operator to judiciously place the initial snaxels near desired boundaries. Alternatively, short snakes are initialized at locations of high intensity gradient and they either overlap [49] or extend in length <ref> [6] </ref> to cover a longer boundary. However, the latter strategies cause the snake to lose the globalities that enable it to interpolate subjective contours across boundary gaps and 6 (a) = 0:8 (b) = 0:2 (c) minimax noise; (b) Snake capture corners but noise sensitive; (c) good compromise. noisy segments. <p> Unlike methods <ref> [6] </ref> which use gradient direction to discard spurious edge points, the proposed E ext continuously weigh the gradient magnitude against any orientation inconsistency. Consequently, besides a non edge point, an edge point whose orientation disagrees with that of the overlaying snake may also yield large external energy.
Reference: [7] <author> P. J. Burt & E. H. </author> <title> Adelson,"The Laplacian Pyramid as a Compact Image Code" IEEE Trans. </title> <journal> on Commun., </journal> <volume> vol. COM-31, no. 4, </volume> <year> 1983, </year> <pages> pp. 532-540. </pages>
Reference: [8] <author> J. Canny, </author> <title> "A Computational Approach to Edge Detection," </title> <journal> IEEE Trans. Pat. Anal. Mach. Intell., </journal> <volume> vol PAMI-8, </volume> <year> 1986, </year> <pages> pp. 679-698. </pages>
Reference-contexts: While enormous amount of literature exists in each of the fields, these works have typically been developed in isolation. The following is a representative list: contour modeling [13, 33, 35, 48], edge detection <ref> [8, 11, 17, 30, 43] </ref>, edge linking [3, 32, 34], contour detection and classification [18, 20, 36, 45]. This strict dichotomy of visual tasks has been widely adopted because it decomposes the problem of image analysis into independent and managable components. <p> An edge linking algorithm then yields an ordering of successive edge points based on some predefined criterion functions such as continuity and connectivity. The enormous amount of research in edge detection can be broadly classified into one of several categories: optimal filtering <ref> [8, 11] </ref>, scale space [30], surface fitting [17] and cost minimization [43]. A great deal of literature also exists on edge linking [3, 32, 34]. Despite this tremendous effort, there is little consensus that the problem of boundary extraction has been solved. <p> Thus the starting point of modeling usually consists of segmented (binary) images, or an ordered set of points forming the contour. On the other hand, the two essential tasks in extraction, edge detection <ref> [8, 11, 30] </ref> and edge linking [3], are usually attempted without reference to a contour model. Such strict dichotomy of visual tasks is charateristic of Marr's paradigm [31], widely adopted because it decomposes the problem of image analysis into independent and managable components.
Reference: [9] <author> L. D. Cohen, </author> <title> "On Active Contour Models and Balloons," </title> <booktitle> CVGIP-Image Understanding, </booktitle> <volume> vol. 53, no. 2, </volume> <year> 1991, </year> <pages> pp. 211-218. </pages>
Reference-contexts: In contrast, small values of enable the snakes to effectively capture boundary discontinuities, but they also make the solutions very noise sensitive. Figure 2.1 illustrates these conflicting goals. Despite its significance, regularization selection were either ignored [6, 21, 29, 49], or based on empirical observations <ref> [2, 9] </ref> and heuristics [46]. Another factor which directly affects the solutions is energy formulation. The original E int formulation [21] causes a snake to shrink around strong edge points. <p> Despite its significance, regularization selection were either ignored [6, 21, 29, 49], or based on empirical observations [2, 9] and heuristics [46]. Another factor which directly affects the solutions is energy formulation. The original E int formulation [21] causes a snake to shrink around strong edge points. Ballooning force <ref> [9] </ref> and hard constraint on snaxel proximity [2] have been suggested, but these require prior information in setting the weighing factor or constraint. Moreover, existing curvature measure on E int is not rotational and scale invariant, causing erroneous imposition of the smoothness constraint. <p> Firstly, E int vanishes when v i = v for all i, often causing a snake to shrink around strong edge points. To counter the shrinkage tendency, ballooning force <ref> [9] </ref> and hard constraint on snaxel proximity [2] have been suggested, but these require prior information in setting the weighing factor or constraint. Secondly, E int in (2.17) is not invariant to scale change and rotation, thus often causing error in constraining smoothness on the snake.
Reference: [10] <author> L. D. Cohen & I. Cohen, </author> <title> "Finite-Element Methods for Active Contour Models and Balloons for 2-D and 3-D Images," </title> <journal> IEEE Trans. Pat. Anal. Mach. Intell., </journal> <volume> vol. PAMI-15, </volume> <year> 1993, </year> <pages> pp. 1131-1147. 89 </pages>
Reference-contexts: The objective can be achieved 47 square is the initial contour, and the 8 normal vectors are the initial search directions. by searching in the normal directions of u i . Note that this differs from <ref> [10] </ref> that we do not introduce additional inflation or deflation force; different parts of U can be inflated or deflated simultaneously.
Reference: [11] <author> F. M. Dickey & K. S. Shanmugam, </author> <title> "Optimum Edge Detection Filter," </title> <journal> Appl. Optics, </journal> <volume> vol. 16, no. 1, </volume> <year> 1977, </year> <pages> pp. 145-148. </pages>
Reference-contexts: While enormous amount of literature exists in each of the fields, these works have typically been developed in isolation. The following is a representative list: contour modeling [13, 33, 35, 48], edge detection <ref> [8, 11, 17, 30, 43] </ref>, edge linking [3, 32, 34], contour detection and classification [18, 20, 36, 45]. This strict dichotomy of visual tasks has been widely adopted because it decomposes the problem of image analysis into independent and managable components. <p> An edge linking algorithm then yields an ordering of successive edge points based on some predefined criterion functions such as continuity and connectivity. The enormous amount of research in edge detection can be broadly classified into one of several categories: optimal filtering <ref> [8, 11] </ref>, scale space [30], surface fitting [17] and cost minimization [43]. A great deal of literature also exists on edge linking [3, 32, 34]. Despite this tremendous effort, there is little consensus that the problem of boundary extraction has been solved. <p> Thus the starting point of modeling usually consists of segmented (binary) images, or an ordered set of points forming the contour. On the other hand, the two essential tasks in extraction, edge detection <ref> [8, 11, 30] </ref> and edge linking [3], are usually attempted without reference to a contour model. Such strict dichotomy of visual tasks is charateristic of Marr's paradigm [31], widely adopted because it decomposes the problem of image analysis into independent and managable components.
Reference: [12] <author> R. O. Duda & P. E. Hart, </author> <title> "Use of the Hough Transformation to Detect Lines and Curves in Pictures," </title> <journal> Communs. Ass. Comput. Mach., </journal> <volume> vol. 15, </volume> <year> 1972, </year> <pages> pp. </pages> <month> 11-15. </month> <title> [13] h&gt; Freeman, "Computer Processing of Line Drawing Images," Computer Survey 6, </title> <booktitle> 1974, </booktitle> <pages> pp. 57-98. </pages>
Reference-contexts: Moreover, due to image noise and the image projection process, the lower level visual tasks are generally ill-posed [37], i.e., they are underconstrained and so do not have unique solutions. The classical and generalized Hough transforms <ref> [12, 4] </ref> are popular methods that combine modeling and extraction for rigid contours. Because these methods work by accumulating large number of votes, they are relatively insensitive to small vote fluctuation caused by noise and occlusion. In fact, if the superimposed noise is i.i.d. <p> We also generalize E ext to incorporate the gradient direction. This ensures consistency between a snake's orientation to the underlying intensity profile. * Automatic initialization. Using the reformulated energy functionals, we show that Hough transform <ref> [12] </ref> is a special case of a snake with large regularization parame 7 ters. <p> This suggests a counting procedure that can be implemented using Hough transform <ref> [12] </ref>. Since e (V; ) is continuous in , the optimum snake V fl 1 must be located near V fl 2 if the difference j 1 2 j is small. <p> In existing literature dealing with contour modeling, specific knowledge is sometimes used to yield satisfactory extraction. See [33] for an example. However, tailored to specific applications, such techniques are seldom transferrable. The Hough transform <ref> [4, 12] </ref> is a well-known method that combines contour modeling 33 and extraction for rigid objects. Choosing a model a priori, one accumulates local evidence of its presence by incrementing cells in the appropriate parameter space. <p> Under these assumptions, it can be shown [40] that matched template is optimum in terms of probability of error. Moreover, since it essentially involves correlating a particular template with the image, the matched template can be efficiently implemented using the classical or generalized Hough transform <ref> [4, 12] </ref>. In practice, however, the assumption of rigidity is often violated. The contours of interest often exhibit deformation which arises from diversity and irregularity of shape. A good example is handwritten character recognition, where considerable deformation is produced by variation in writing style, as evidenced in Figure 4.1.
Reference: [14] <author> M. A. Gennert & A. L. </author> <title> Yuille,"Determining the Optimal Weights in Multiple Objective Function Optimization," </title> <booktitle> Proceedings of Second International Conference on Computer Vision, </booktitle> <year> 1988, </year> <pages> pp. 87-89. </pages>
Reference-contexts: This chapter tackles the fundamental issues on regularization, formulation and initialization of the active contour models. We made a number of contributions which are summarized below: * Automatic and implicit selection of local regularization parameters. We present a parameter selection strategy based on the minimax criterion <ref> [14] </ref>. The resulting local minimax criterion automatically determines the optimal tradeoff at every location along the boundary of interest without any user input or heuristics. Moreover, this criterion can be implicitly incorporated into the energy computation process with no increase in computational cost. * Formulation of energy functionals. <p> Since values chosen a priori are unlikely to be satisfactory for all cases, it becomes necessary to define a criterion to determine . A viable criterion is to seek a V that minimizes, over all , the maximum of e (V; ). This yields the minimax <ref> [14] </ref> criterion : e (V fl ; fl ) = min max E int (V) + (1 )E ext (V) (2.3) where V fl and fl are, respectively, the minimax snake and regularization parameter. <p> In some applications these parameters may be learned from training samples. In cases where learning is impossible or unreliable, the minimax principle <ref> [14] </ref> can be used. The local minimax criterion [23] determines the optimal regularization by minimizing the worst case energy.
Reference: [15] <author> W. L. Grimson & D. P. Huttenlocher, </author> <title> "On the sensitivity of the Hough Transform for Object Recognition," </title> <journal> IEEE Trans. Pat. Anal. Mach. Intell., </journal> <volume> vol. PAMI-12, </volume> <year> 1990, </year> <pages> pp. 255-274. </pages>
Reference-contexts: These methods, however, do not scale very well to recognition in complex scenes where considerable outliers are present. In these cases, the probabilities of false positives can be very high <ref> [15, 16] </ref>. 2 Furthermore, rigid templates used in Hough transforms cannot account for deformations which frequently arise from diversity and irregularity of shape. Since the degree of deformation is unknown in advance, a rigid template chosen a priori cannot produce satisfactory results for all cases. <p> Because matched template fails to model this effect, we have shown in Chapter 3 that its averaged correlation degrades rapidly with deformation. Moreover, it has high probability of generating false peaks for complex scenes <ref> [15, 16] </ref>. Although there exist numerous methods [18, 45] that detect and classify deformable contours, these typically assume that the contour has been previously extracted. How 63 ever, without a contour model, contour extraction has been shown to be an ill-posed problem [37].
Reference: [16] <author> W. L. Grimson, D. P. Huttenlocher & T. D. </author> <title> Alter, "Recognizing 3D Objects from 2D Images: An Error Analysis," </title> <booktitle> Proc CVPR 1992, </booktitle> <pages> pp. 316-321. </pages>
Reference-contexts: These methods, however, do not scale very well to recognition in complex scenes where considerable outliers are present. In these cases, the probabilities of false positives can be very high <ref> [15, 16] </ref>. 2 Furthermore, rigid templates used in Hough transforms cannot account for deformations which frequently arise from diversity and irregularity of shape. Since the degree of deformation is unknown in advance, a rigid template chosen a priori cannot produce satisfactory results for all cases. <p> Because matched template fails to model this effect, we have shown in Chapter 3 that its averaged correlation degrades rapidly with deformation. Moreover, it has high probability of generating false peaks for complex scenes <ref> [15, 16] </ref>. Although there exist numerous methods [18, 45] that detect and classify deformable contours, these typically assume that the contour has been previously extracted. How 63 ever, without a contour model, contour extraction has been shown to be an ill-posed problem [37].
Reference: [17] <author> R. M. Haralick, </author> <title> "Digital Step Edges from Zero Crossing of Second Directional Derivatives," </title> <journal> IEEE Trans. Pat. Anal. Mach. Intell., </journal> <volume> vol. PAMI-6, no. 1, </volume> <year> 1984, </year> <pages> pp. 56-68. </pages>
Reference-contexts: While enormous amount of literature exists in each of the fields, these works have typically been developed in isolation. The following is a representative list: contour modeling [13, 33, 35, 48], edge detection <ref> [8, 11, 17, 30, 43] </ref>, edge linking [3, 32, 34], contour detection and classification [18, 20, 36, 45]. This strict dichotomy of visual tasks has been widely adopted because it decomposes the problem of image analysis into independent and managable components. <p> An edge linking algorithm then yields an ordering of successive edge points based on some predefined criterion functions such as continuity and connectivity. The enormous amount of research in edge detection can be broadly classified into one of several categories: optimal filtering [8, 11], scale space [30], surface fitting <ref> [17] </ref> and cost minimization [43]. A great deal of literature also exists on edge linking [3, 32, 34]. Despite this tremendous effort, there is little consensus that the problem of boundary extraction has been solved.
Reference: [18] <author> Y. He & A. Kundu, </author> <title> "2-D Shape Classification using Hidden Markov Model," </title> <journal> IEEE Trans. Pat. Anal. Mach. Intell., </journal> <volume> vol. PAMI-13, </volume> <year> 1991, </year> <pages> pp. 1172-1184. </pages>
Reference-contexts: While enormous amount of literature exists in each of the fields, these works have typically been developed in isolation. The following is a representative list: contour modeling [13, 33, 35, 48], edge detection [8, 11, 17, 30, 43], edge linking [3, 32, 34], contour detection and classification <ref> [18, 20, 36, 45] </ref>. This strict dichotomy of visual tasks has been widely adopted because it decomposes the problem of image analysis into independent and managable components. Unfor-turnately, the Marr's paradigm also results in unidirectional and irreversible flow of error from one component to the next. <p> Examples include the active contour model (snake) [21], parameterized templates [47], elliptic Fourier decompositions [41], implicit polynomials [42], handprinted character recognition [19] and affine invariant contour tracking [5]. In addition, several methods that classify deformable contours have also been reported <ref> [18, 45] </ref>, but these ignore the problem of extraction. There have been relatively few methods that consider detection and classification of deformable contours directly from noisy images [19, 42]. 1.2 Contributions This thesis presents an integrated approach in modeling, extracting, detecting and classifying deformable contours directly from noisy images. <p> Because matched template fails to model this effect, we have shown in Chapter 3 that its averaged correlation degrades rapidly with deformation. Moreover, it has high probability of generating false peaks for complex scenes [15, 16]. Although there exist numerous methods <ref> [18, 45] </ref> that detect and classify deformable contours, these typically assume that the contour has been previously extracted. How 63 ever, without a contour model, contour extraction has been shown to be an ill-posed problem [37]. Consequently, the practicality of these methods is limited for applications involving noisy images.
Reference: [19] <author> G. E. Hinton, C. K. I. Williams & M. D. Revow, </author> <title> "Adaptive Elastic Models for Hand-Printed Character Recognition," </title> <booktitle> Neural Information Processing Systems, </booktitle> <volume> vol 4, </volume> <editor> ed. J. E. Moody et. al., </editor> <year> 1992, </year> <pages> pp. 512-519. </pages>
Reference-contexts: Recently, numerous works in deformable templates have been reported, using weak models which deform in conformation to salient image features. Examples include the active contour model (snake) [21], parameterized templates [47], elliptic Fourier decompositions [41], implicit polynomials [42], handprinted character recognition <ref> [19] </ref> and affine invariant contour tracking [5]. In addition, several methods that classify deformable contours have also been reported [18, 45], but these ignore the problem of extraction. <p> In addition, several methods that classify deformable contours have also been reported [18, 45], but these ignore the problem of extraction. There have been relatively few methods that consider detection and classification of deformable contours directly from noisy images <ref> [19, 42] </ref>. 1.2 Contributions This thesis presents an integrated approach in modeling, extracting, detecting and classifying deformable contours directly from noisy images. We begin by conducting a close examinination on the active contour models of [21]. <p> include the active contour model (snake) which uses smoothness constraints to restrict its solutions to controlled continuity splines [21]; parameterized templates for facial feature extraction [47]; elliptic Fourier decomposition for objects with shape irregularities [41]; implicit polynomials for curve and surface modeling [42]; movable control points for hand-printed character recognition <ref> [19] </ref>; and affine-invariant contour tracking [5]. With the exception of [5], these methods typically consider only global [41, 42, 47] or local [19, 21] deformations. <p> With the exception of [5], these methods typically consider only global [41, 42, 47] or local <ref> [19, 21] </ref> deformations. While global templates involve large structural interactions and contain fewer parameters to be optimized, these global parameters cannot exercise local control along the contour and their physical meaning is sometimes obscure. <p> Consequently, the practicality of these methods is limited for applications involving noisy images. Existing methods dealing with detecting and classifying deformable contours directly from noisy image are mostly based on intuitive ideas or heuristics. In handwritten character recognition, <ref> [19] </ref> uses a weighted combination of energy functions which measure deviation from an ideal model and closeness of match. However, there is no formal mechanism to determine the weighting factor which significantly affects the performance. In shape matching, [20] uses the Hausdorff distance which is tolerant of small position errors. <p> Firstly, the shape matrix is invariant under affine transformation, which is crucial in detecting or classifying objects undergoing rigid motions in the 3-D space. Furthermore, affine invariance of the shape matrix is implicit, unlike Fourier descriptors [41] or memorized templates <ref> [19, 20, 5] </ref> which require exhaustive comparison into a created subspace. Secondly, the shape matrix is unique. Hence two contours that cannot be related by an affine transformation cannot have similar shape matrix. Recognition is impossible without this characteristics.
Reference: [20] <author> D. P. Huttenlocher, G. A. Klanderman & W. J. Rucklidge, </author> <title> "Comparing Images Using the Hausdorff Distance," </title> <journal> IEEE Trans. Pat. Anal. Mach. Intell., </journal> <volume> vol. PAMI-15, </volume> <year> 1993, </year> <pages> pp. 850-863. 90 </pages>
Reference-contexts: While enormous amount of literature exists in each of the fields, these works have typically been developed in isolation. The following is a representative list: contour modeling [13, 33, 35, 48], edge detection [8, 11, 17, 30, 43], edge linking [3, 32, 34], contour detection and classification <ref> [18, 20, 36, 45] </ref>. This strict dichotomy of visual tasks has been widely adopted because it decomposes the problem of image analysis into independent and managable components. Unfor-turnately, the Marr's paradigm also results in unidirectional and irreversible flow of error from one component to the next. <p> In handwritten character recognition, [19] uses a weighted combination of energy functions which measure deviation from an ideal model and closeness of match. However, there is no formal mechanism to determine the weighting factor which significantly affects the performance. In shape matching, <ref> [20] </ref> uses the Hausdorff distance which is tolerant of small position errors. However, the method is likely to break down when outliers or considerable deformations are present. <p> Firstly, the shape matrix is invariant under affine transformation, which is crucial in detecting or classifying objects undergoing rigid motions in the 3-D space. Furthermore, affine invariance of the shape matrix is implicit, unlike Fourier descriptors [41] or memorized templates <ref> [19, 20, 5] </ref> which require exhaustive comparison into a created subspace. Secondly, the shape matrix is unique. Hence two contours that cannot be related by an affine transformation cannot have similar shape matrix. Recognition is impossible without this characteristics.
Reference: [21] <author> M. Kass, A. Witkin & D. Terzopoulos, "Snakes: </author> <title> Active Contour Models," </title> <booktitle> Proceedings of First International Conference on Computer Vision, </booktitle> <year> 1987, </year> <pages> pp. 259-269. </pages>
Reference-contexts: In fact, we will show in Chapter 3 of this thesis that its performance degrades with deformation. Recently, numerous works in deformable templates have been reported, using weak models which deform in conformation to salient image features. Examples include the active contour model (snake) <ref> [21] </ref>, parameterized templates [47], elliptic Fourier decompositions [41], implicit polynomials [42], handprinted character recognition [19] and affine invariant contour tracking [5]. In addition, several methods that classify deformable contours have also been reported [18, 45], but these ignore the problem of extraction. <p> We begin by conducting a close examinination on the active contour models of <ref> [21] </ref>. This yields enhanced understanding on the various aspects of deformable models which forms the corner stone of our subsequent works. We then proceed to formally develop the necessary theories that enable us to integrate the various visual tasks. <p> Moreover, the lower level visual tasks are generally ill-posed [37], i.e., they are underconstrained and so do not have unique solutions. The active contour models, or snakes, were originally proposed by <ref> [21] </ref> as a regularization approach [44] to the ill-posed edge detection problem. <p> In contrast, small values of enable the snakes to effectively capture boundary discontinuities, but they also make the solutions very noise sensitive. Figure 2.1 illustrates these conflicting goals. Despite its significance, regularization selection were either ignored <ref> [6, 21, 29, 49] </ref>, or based on empirical observations [2, 9] and heuristics [46]. Another factor which directly affects the solutions is energy formulation. The original E int formulation [21] causes a snake to shrink around strong edge points. <p> Figure 2.1 illustrates these conflicting goals. Despite its significance, regularization selection were either ignored [6, 21, 29, 49], or based on empirical observations [2, 9] and heuristics [46]. Another factor which directly affects the solutions is energy formulation. The original E int formulation <ref> [21] </ref> causes a snake to shrink around strong edge points. Ballooning force [9] and hard constraint on snaxel proximity [2] have been suggested, but these require prior information in setting the weighing factor or constraint. <p> Moreover, existing curvature measure on E int is not rotational and scale invariant, causing erroneous imposition of the smoothness constraint. Snaxel initialization is the next factor with significant impact. For satisfactory results, most existing methods <ref> [2, 21, 46] </ref> require an operator to judiciously place the initial snaxels near desired boundaries. Alternatively, short snakes are initialized at locations of high intensity gradient and they either overlap [49] or extend in length [6] to cover a longer boundary. <p> The next section presents normalized energy formulations which have also undergone some enhancements. 2.3 Energy Formulation In the original formulation of <ref> [21] </ref>, the internal energy E int was defined as the first and second derivatives of along the boundary, giving the snake rubber-sheet and thin-plate 13 like behavior respectively. <p> Contour extraction is then formulated as an optimization problem to maximize or minimize the overall objective function. 34 Examples of deformable template include the active contour model (snake) which uses smoothness constraints to restrict its solutions to controlled continuity splines <ref> [21] </ref>; parameterized templates for facial feature extraction [47]; elliptic Fourier decomposition for objects with shape irregularities [41]; implicit polynomials for curve and surface modeling [42]; movable control points for hand-printed character recognition [19]; and affine-invariant contour tracking [5]. <p> With the exception of [5], these methods typically consider only global [41, 42, 47] or local <ref> [19, 21] </ref> deformations. While global templates involve large structural interactions and contain fewer parameters to be optimized, these global parameters cannot exercise local control along the contour and their physical meaning is sometimes obscure. <p> Under a Bayesian framework, the problem of extracting contours with unknown deformation from noisy images turns into maximum a posteriori 35 estimation. This is equivalent to minimizing the energy of a generalized active contour model (g-snake), which is a generalization of the snake of <ref> [21] </ref>. We discuss pertinent issues in g-snakes, including shape training, energy minimization, line search strategies, minimax regularization and initialization by generalized Hough transform. <p> The formulation in (3.23) is analogous to the active contour models (snakes) <ref> [21] </ref>. However, the original E int only constrains the solution to the class of controlled continuity splines. Our formulation generalizes E int , allowing for incorporation of prior models to create bias towards a particular type of contour. <p> Consequently, unless the initial U is placed very close to the global minimum, a gradient-based <ref> [21] </ref> or point-wise [46] algorithm will perform poorly as many local minima are present. Therefore an exhaustive search algorithm should be used. Suppose that for each point u i , we perform exhaustive search in its neighborhood of size m. <p> The g-snake accounts for local deformation to produce contours that match the underlying images, see Figure 3.6 (a), (b) and (c) (bottom). 3.5.4 Comparison to Snake Figuer 3.6 (d) shows the results of applying the snake of <ref> [21] </ref>. As its internal energy restricts the solution to controlled continuity spline, the initialized state is a circle. Failing to utilize the appropriate contour model (i.e., rectangle), it yields suboptimal solution with smoothed corners.
Reference: [22] <author> R. Kindermann & J. L. Snell, </author> <title> Markov Random Fields and their Applications, </title> <publisher> American Mathematical Society, </publisher> <year> 1980. </year>
Reference-contexts: Now, we may assign probabilities to U as follows : p (U) = Z where Z = P U2 exp (E int (U)) is a normalizing constant. A probability measure of the form in (3.10) is called a Gibbs measure <ref> [22] </ref>. <p> A probability measure of the form (4.8) is called a Gibbs measure. By equivalence, it also defines a Markov random field <ref> [22] </ref>, i.e., p (u i ju 1 ; u 2 ; : : : ; u n ) = p (u i ju i ff ; u i fi ) (4.9) where the conditional probability of u i given the entire chain U is completely specified by the conditional probability of
Reference: [23] <author> K. F. Lai & R. T. Chin, </author> <title> "On Regularization, Formulation and Initialization of the Active Contour Models (Snakes)," </title> <booktitle> Asian Conference on Computer Vision, </booktitle> <year> 1993, </year> <pages> pp. 542-545. </pages>
Reference-contexts: In some applications these parameters may be learned from training samples. In cases where learning is impossible or unreliable, the minimax principle [14] can be used. The local minimax criterion <ref> [23] </ref> determines the optimal regularization by minimizing the worst case energy.
Reference: [24] <author> K. F. Lai & R. T. Chin, </author> <title> "On Regularization, Formulation and Initialization of the Active Contour Models (Snakes)," </title> <note> submitted to IEEE Trans. Pat. Anal. Mach. Intell., </note> <year> 1993. </year>
Reference: [25] <author> K. F. Lai & R. T. Chin, </author> <title> "Deformable Contours: Modeling and Extraction," </title> <booktitle> Proc Comp. Vis. Patt. </booktitle> <address> Recog., </address> <year> 1994. </year>
Reference-contexts: In the presence of noise, these coefficients quickly become unstable and their joint probabilities are difficult to compute. This chapter presents a formal strategy to detect and classify deformable contours directly from noisy images. Our formulation is based on the generalized active contour models (g-snake) <ref> [25] </ref>, which incorporates both a deformation model and an image noise model. Unlike Fourier descriptors [41] or implicit polynomials [42], the shape representation in g-snake is stable i.e., small deviations in shape will yield small perturbations in shape representation and vice versa. <p> For each pair of and n , we generate training samples and use them to estimate the regularization parameters for deformable templates. To marginalize the distribution in (4.19) and (4.22), we first obtain the posterior estimate using energy minimization and search strategies presented in <ref> [25] </ref>. As the summation is peaked, we can approximate the results by performing summation in 3 fi 3 regions centered around the posterior estimate. Using the summation algorithm presented in the previous section, the computation complexity is O (16 fi 9 3 ).
Reference: [26] <author> K. F. Lai & R. T. Chin, </author> <title> "Deformable Contours: Modeling and Extraction," </title> <note> to appear in IEEE Trans. Pat. Anal. Mach. Intell., </note> <year> 1994. </year>
Reference: [27] <author> K. F. Lai & R. T. Chin, </author> <title> "On Classifying Deformable Contours Using the Generalized Active Contour Model," </title> <booktitle> submitted to Int. Conf. on Automation, Robotics & Comp. Vis., </booktitle> <year> 1994. </year>
Reference: [28] <author> K. F. Lai & R. T. Chin, </author> <title> "Deformable Contours: Detection and Classification," </title> <note> in preparation. </note>
Reference: [29] <author> F. Leymarie & M. D. Levine, </author> <title> "Simulating the Grassfire Transform Using an Active Contour Model," </title> <journal> IEEE Trans. Pat. Anal. Mach. Intell., </journal> <volume> vol. PAMI-14, no. 1, </volume> <year> 1992, </year> <pages> pp. 56-75. </pages>
Reference-contexts: In contrast, small values of enable the snakes to effectively capture boundary discontinuities, but they also make the solutions very noise sensitive. Figure 2.1 illustrates these conflicting goals. Despite its significance, regularization selection were either ignored <ref> [6, 21, 29, 49] </ref>, or based on empirical observations [2, 9] and heuristics [46]. Another factor which directly affects the solutions is energy formulation. The original E int formulation [21] causes a snake to shrink around strong edge points.
Reference: [30] <author> D. Marr & E. Hildreth, </author> <title> "Theory of Edge Detection," </title> <journal> Proceedings of Royal Society of London, </journal> <volume> B207, </volume> <year> 1980, </year> <pages> pp. 187-217. 91 </pages>
Reference-contexts: While enormous amount of literature exists in each of the fields, these works have typically been developed in isolation. The following is a representative list: contour modeling [13, 33, 35, 48], edge detection <ref> [8, 11, 17, 30, 43] </ref>, edge linking [3, 32, 34], contour detection and classification [18, 20, 36, 45]. This strict dichotomy of visual tasks has been widely adopted because it decomposes the problem of image analysis into independent and managable components. <p> An edge linking algorithm then yields an ordering of successive edge points based on some predefined criterion functions such as continuity and connectivity. The enormous amount of research in edge detection can be broadly classified into one of several categories: optimal filtering [8, 11], scale space <ref> [30] </ref>, surface fitting [17] and cost minimization [43]. A great deal of literature also exists on edge linking [3, 32, 34]. Despite this tremendous effort, there is little consensus that the problem of boundary extraction has been solved. <p> Thus the starting point of modeling usually consists of segmented (binary) images, or an ordered set of points forming the contour. On the other hand, the two essential tasks in extraction, edge detection <ref> [8, 11, 30] </ref> and edge linking [3], are usually attempted without reference to a contour model. Such strict dichotomy of visual tasks is charateristic of Marr's paradigm [31], widely adopted because it decomposes the problem of image analysis into independent and managable components.
Reference: [31] <author> D. Marr & H. K. Nishihara, </author> <title> "Visual Information Processing: </title> <journal> Artificial Intelligence and the Sensorium of Sight," Technology Review, </journal> <volume> V81(1), </volume> <year> 1978, </year> <pages> pp. 2-23. </pages>
Reference-contexts: Introduction 1.1 Motivation Under the Marr's paradigm <ref> [31] </ref>, the problems of contour modeling, extraction, detection and classification have conventionally been treated as separate issues in computer vision. While enormous amount of literature exists in each of the fields, these works have typically been developed in isolation. <p> Finally, we perform extensive experimentations and report significant improvement over matched template in handwritten numeral recognition. Finally, Chapter 5 summarizes the work and suggests future research directions. 4 Chapter 2 On Regularization, Formulation and Initialization of the Active Contour Models (Snakes) 2.1 Introduction Under the Marr's paradigm <ref> [31] </ref>, boundary extraction has conventionally been treated as two independent problems: edge detection and edge linking. At the lowest level, an edge detector operates directly on the image intensities to produce an edge map marking the location, stength and direction of edge points. <p> On the other hand, the two essential tasks in extraction, edge detection [8, 11, 30] and edge linking [3], are usually attempted without reference to a contour model. Such strict dichotomy of visual tasks is charateristic of Marr's paradigm <ref> [31] </ref>, widely adopted because it decomposes the problem of image analysis into independent and managable components. Unfortunately, this paradigm also results in unidirectional and irreversible flow of error from one component to the next.
Reference: [32] <author> A. Martelli, </author> <title> "Edge Detection Using Heuristic Search Methods," </title> <journal> Computer Graphics Image Processing, </journal> <volume> vol. 1, </volume> <year> 1972, </year> <pages> pp. 169-182. </pages>
Reference-contexts: While enormous amount of literature exists in each of the fields, these works have typically been developed in isolation. The following is a representative list: contour modeling [13, 33, 35, 48], edge detection [8, 11, 17, 30, 43], edge linking <ref> [3, 32, 34] </ref>, contour detection and classification [18, 20, 36, 45]. This strict dichotomy of visual tasks has been widely adopted because it decomposes the problem of image analysis into independent and managable components. <p> The enormous amount of research in edge detection can be broadly classified into one of several categories: optimal filtering [8, 11], scale space [30], surface fitting [17] and cost minimization [43]. A great deal of literature also exists on edge linking <ref> [3, 32, 34] </ref>. Despite this tremendous effort, there is little consensus that the problem of boundary extraction has been solved. A problem with this approach is that errors made in edge detection propagate to edge linking without opportunity for correction.
Reference: [33] <author> F. Mokhtarian & A. K. Mackworth, </author> <title> "A Theory of Multiscale, Curvature-Based Shape Representation for Planar Curves," </title> <journal> IEEE Trans. Pat. Anal. Mach. Intell., </journal> <volume> vol. PAMI-14, No. 8, </volume> <year> 1992, </year> <pages> pp. 789-805. </pages>
Reference-contexts: While enormous amount of literature exists in each of the fields, these works have typically been developed in isolation. The following is a representative list: contour modeling <ref> [13, 33, 35, 48] </ref>, edge detection [8, 11, 17, 30, 43], edge linking [3, 32, 34], contour detection and classification [18, 20, 36, 45]. This strict dichotomy of visual tasks has been widely adopted because it decomposes the problem of image analysis into independent and managable components. <p> On one hand, the works in contour modeling <ref> [13, 33, 35, 45, 48] </ref> typically ignore the problem of extraction. Thus the starting point of modeling usually consists of segmented (binary) images, or an ordered set of points forming the contour. <p> To model a contour, one must first extract it from an image; but without prior knowledge of the contour of interest, extraction is an ill-posed problem. In existing literature dealing with contour modeling, specific knowledge is sometimes used to yield satisfactory extraction. See <ref> [33] </ref> for an example. However, tailored to specific applications, such techniques are seldom transferrable. The Hough transform [4, 12] is a well-known method that combines contour modeling 33 and extraction for rigid objects.
Reference: [34] <author> U. Montanari, </author> <title> "On the Optimal Detection of Curves in Noisy Pictures," </title> <journal> Commun. ACM, </journal> <volume> vol. 14, </volume> <year> 1971, </year> <pages> pp. 335-345. </pages>
Reference-contexts: While enormous amount of literature exists in each of the fields, these works have typically been developed in isolation. The following is a representative list: contour modeling [13, 33, 35, 48], edge detection [8, 11, 17, 30, 43], edge linking <ref> [3, 32, 34] </ref>, contour detection and classification [18, 20, 36, 45]. This strict dichotomy of visual tasks has been widely adopted because it decomposes the problem of image analysis into independent and managable components. <p> The enormous amount of research in edge detection can be broadly classified into one of several categories: optimal filtering [8, 11], scale space [30], surface fitting [17] and cost minimization [43]. A great deal of literature also exists on edge linking <ref> [3, 32, 34] </ref>. Despite this tremendous effort, there is little consensus that the problem of boundary extraction has been solved. A problem with this approach is that errors made in edge detection propagate to edge linking without opportunity for correction.
Reference: [35] <author> D. W. Paglierioni & A. K. Jain, </author> <title> "A Control Point Theory for Boundary Representation and Matching," </title> <booktitle> Proc ICASSP, </booktitle> <volume> vol. 4, </volume> <year> 1985, </year> <pages> pp. 1851-1854. </pages>
Reference-contexts: While enormous amount of literature exists in each of the fields, these works have typically been developed in isolation. The following is a representative list: contour modeling <ref> [13, 33, 35, 48] </ref>, edge detection [8, 11, 17, 30, 43], edge linking [3, 32, 34], contour detection and classification [18, 20, 36, 45]. This strict dichotomy of visual tasks has been widely adopted because it decomposes the problem of image analysis into independent and managable components. <p> On one hand, the works in contour modeling <ref> [13, 33, 35, 45, 48] </ref> typically ignore the problem of extraction. Thus the starting point of modeling usually consists of segmented (binary) images, or an ordered set of points forming the contour.
Reference: [36] <author> M. J. Paulik & N. Mohankrishnan, </author> <title> "Shape Recognition Using a Nonstationary Autoregressive Hidden Markov Model," </title> <booktitle> Proc ICASSP, </booktitle> <year> 1991, </year> <pages> pp. 2377-2380. </pages>
Reference-contexts: While enormous amount of literature exists in each of the fields, these works have typically been developed in isolation. The following is a representative list: contour modeling [13, 33, 35, 48], edge detection [8, 11, 17, 30, 43], edge linking [3, 32, 34], contour detection and classification <ref> [18, 20, 36, 45] </ref>. This strict dichotomy of visual tasks has been widely adopted because it decomposes the problem of image analysis into independent and managable components. Unfor-turnately, the Marr's paradigm also results in unidirectional and irreversible flow of error from one component to the next.
Reference: [37] <author> T. Poggio & V. Torre, </author> <title> "Ill-posed Problems and Regularization Analysis in Early Vision, </title> " <booktitle> Proceedings of AARPA Image Understanding Workshop, </booktitle> <year> 1984, </year> <pages> pp. 257-263. </pages>
Reference-contexts: Unfor-turnately, the Marr's paradigm also results in unidirectional and irreversible flow of error from one component to the next. Moreover, due to image noise and the image projection process, the lower level visual tasks are generally ill-posed <ref> [37] </ref>, i.e., they are underconstrained and so do not have unique solutions. The classical and generalized Hough transforms [12, 4] are popular methods that combine modeling and extraction for rigid contours. <p> Despite this tremendous effort, there is little consensus that the problem of boundary extraction has been solved. A problem with this approach is that errors made in edge detection propagate to edge linking without opportunity for correction. Moreover, the lower level visual tasks are generally ill-posed <ref> [37] </ref>, i.e., they are underconstrained and so do not have unique solutions. The active contour models, or snakes, were originally proposed by [21] as a regularization approach [44] to the ill-posed edge detection problem. <p> Unfortunately, this paradigm also results in unidirectional and irreversible flow of error from one component to the next. Moreover, due to image noise and the image projection process, edge detection and linking are ill-posed <ref> [37] </ref> problems, i.e., they are underconstrained and so do not have unique solutions. We submit that it is fundamentally questionable to treat contour modeling and extraction as two separate issues. <p> Although there exist numerous methods [18, 45] that detect and classify deformable contours, these typically assume that the contour has been previously extracted. How 63 ever, without a contour model, contour extraction has been shown to be an ill-posed problem <ref> [37] </ref>. Consequently, the practicality of these methods is limited for applications involving noisy images. Existing methods dealing with detecting and classifying deformable contours directly from noisy image are mostly based on intuitive ideas or heuristics.
Reference: [38] <author> H. V. </author> <title> Poor, An Introduction to Signal Detection and Estimation, </title> <publisher> Springer-Verlag, </publisher> <year> 1988. </year>
Reference-contexts: Consequently, the above proposition covers all cases, except when U () has an interior maximum at which it is not differentiable. Such case is rare in most applications; a complete proof can be found from page 22 to 27 in <ref> [38] </ref>. The results obtained thus far illustrate the intuitively pleasing characteristics of minimax regularization fl which minimizes the worst case energy in a global sense.
Reference: [39] <author> A. Rattarangsi & R. T. Chin, </author> <title> "Scale-based Detection of Corners of Planar Curves," </title> <journal> IEEE Trans. Pat. Anal. Mach. Intell., </journal> <volume> vol. PAMI-14, </volume> <year> 1992, </year> <pages> pp. 430-449. </pages>
Reference-contexts: We also compare the matching performance of g-snakes and rigid templates. 3.5 Applications and Experiments 3.5.1 Training planes in 2 fi 2 windows using the method of least squares. include locations of high curvature which can be selected either manually or automatically through dominant point detection <ref> [39] </ref>. An initial estimate of the shape matrix are computed from U. Using this shape matrix and minimax regularization, the total energy of the g-snake is minimized to yield ^ U 1 as in Figure 3.4 (b). The shape matrix is then updated to ^ A 1 .
Reference: [40] <author> L. L. Scharf, </author> <title> Statistical Signal Processing: Detection, Estimation and Time Series Analysis, </title> <publisher> Addison-Wesley, </publisher> <year> 1991. </year>
Reference-contexts: A popular approach is to assume that the images contain rigid contours in additive Gaussian white noise. Under these assumptions, it can be shown <ref> [40] </ref> that matched template is optimum in terms of probability of error. Moreover, since it essentially involves correlating a particular template with the image, the matched template can be efficiently implemented using the classical or generalized Hough transform [4, 12]. In practice, however, the assumption of rigidity is often violated.
Reference: [41] <author> L. H. Staib & J. Duncan, </author> <title> "Boundary Finding with Parametrically Deformable Models," </title> <journal> IEEE Trans. Pat. Anal. Mach. Intell., </journal> <volume> vol. PAMI-14, </volume> <year> 1992, </year> <pages> pp. 1061-1075. 92 </pages>
Reference-contexts: Recently, numerous works in deformable templates have been reported, using weak models which deform in conformation to salient image features. Examples include the active contour model (snake) [21], parameterized templates [47], elliptic Fourier decompositions <ref> [41] </ref>, implicit polynomials [42], handprinted character recognition [19] and affine invariant contour tracking [5]. In addition, several methods that classify deformable contours have also been reported [18, 45], but these ignore the problem of extraction. <p> an optimization problem to maximize or minimize the overall objective function. 34 Examples of deformable template include the active contour model (snake) which uses smoothness constraints to restrict its solutions to controlled continuity splines [21]; parameterized templates for facial feature extraction [47]; elliptic Fourier decomposition for objects with shape irregularities <ref> [41] </ref>; implicit polynomials for curve and surface modeling [42]; movable control points for hand-printed character recognition [19]; and affine-invariant contour tracking [5]. With the exception of [5], these methods typically consider only global [41, 42, 47] or local [19, 21] deformations. <p> With the exception of [5], these methods typically consider only global <ref> [41, 42, 47] </ref> or local [19, 21] deformations. While global templates involve large structural interactions and contain fewer parameters to be optimized, these global parameters cannot exercise local control along the contour and their physical meaning is sometimes obscure. <p> This chapter presents a formal strategy to detect and classify deformable contours directly from noisy images. Our formulation is based on the generalized active contour models (g-snake) [25], which incorporates both a deformation model and an image noise model. Unlike Fourier descriptors <ref> [41] </ref> or implicit polynomials [42], the shape representation in g-snake is stable i.e., small deviations in shape will yield small perturbations in shape representation and vice versa. Moreover, this shape representation is unique, implicitly invariant to affine transformations, and possesses metric properties. <p> Firstly, the shape matrix is invariant under affine transformation, which is crucial in detecting or classifying objects undergoing rigid motions in the 3-D space. Furthermore, affine invariance of the shape matrix is implicit, unlike Fourier descriptors <ref> [41] </ref> or memorized templates [19, 20, 5] which require exhaustive comparison into a created subspace. Secondly, the shape matrix is unique. Hence two contours that cannot be related by an affine transformation cannot have similar shape matrix. Recognition is impossible without this characteristics.
Reference: [42] <author> J. Subrahmonia, J. D. Karen & D. B. Cooper, </author> <title> "Bayesian Method for the use of Implicit Polynomials and Algebraic Invariants in Practical Computer Vision," Curves and Surfaces in Computer Vision & Graphics 3, </title> <booktitle> 1992, </booktitle> <pages> pp. 104-117. </pages>
Reference-contexts: Recently, numerous works in deformable templates have been reported, using weak models which deform in conformation to salient image features. Examples include the active contour model (snake) [21], parameterized templates [47], elliptic Fourier decompositions [41], implicit polynomials <ref> [42] </ref>, handprinted character recognition [19] and affine invariant contour tracking [5]. In addition, several methods that classify deformable contours have also been reported [18, 45], but these ignore the problem of extraction. <p> In addition, several methods that classify deformable contours have also been reported [18, 45], but these ignore the problem of extraction. There have been relatively few methods that consider detection and classification of deformable contours directly from noisy images <ref> [19, 42] </ref>. 1.2 Contributions This thesis presents an integrated approach in modeling, extracting, detecting and classifying deformable contours directly from noisy images. We begin by conducting a close examinination on the active contour models of [21]. <p> overall objective function. 34 Examples of deformable template include the active contour model (snake) which uses smoothness constraints to restrict its solutions to controlled continuity splines [21]; parameterized templates for facial feature extraction [47]; elliptic Fourier decomposition for objects with shape irregularities [41]; implicit polynomials for curve and surface modeling <ref> [42] </ref>; movable control points for hand-printed character recognition [19]; and affine-invariant contour tracking [5]. With the exception of [5], these methods typically consider only global [41, 42, 47] or local [19, 21] deformations. <p> With the exception of [5], these methods typically consider only global <ref> [41, 42, 47] </ref> or local [19, 21] deformations. While global templates involve large structural interactions and contain fewer parameters to be optimized, these global parameters cannot exercise local control along the contour and their physical meaning is sometimes obscure. <p> However, the method is likely to break down when outliers or considerable deformations are present. Moreover, since no image noise model was considered, the algorithm must heuristically determine the fraction of edge points to be compared with the model. In curve recognition, <ref> [42] </ref> uses the implicit polynomials and formally invokes the Bayes decision rules. Nonetheless, to achieve adequate representation power, polynomials of rather high degree are typically required. In the presence of noise, these coefficients quickly become unstable and their joint probabilities are difficult to compute. <p> This chapter presents a formal strategy to detect and classify deformable contours directly from noisy images. Our formulation is based on the generalized active contour models (g-snake) [25], which incorporates both a deformation model and an image noise model. Unlike Fourier descriptors [41] or implicit polynomials <ref> [42] </ref>, the shape representation in g-snake is stable i.e., small deviations in shape will yield small perturbations in shape representation and vice versa. Moreover, this shape representation is unique, implicitly invariant to affine transformations, and possesses metric properties. These characteristics make g-snakes suitable for detection and classification tasks.
Reference: [43] <author> H. L. Tan, S. B. Gelfand & E. J. Delp, </author> <title> "A Cost Minimization Approach to Edge Detection Using Simulated Annealing," </title> <journal> IEEE Trans. Pat. Anal. Mach. Intell., </journal> <volume> vol. PAMI-14, no. 1, </volume> <year> 1992, </year> <pages> pp. 3-18. </pages>
Reference-contexts: While enormous amount of literature exists in each of the fields, these works have typically been developed in isolation. The following is a representative list: contour modeling [13, 33, 35, 48], edge detection <ref> [8, 11, 17, 30, 43] </ref>, edge linking [3, 32, 34], contour detection and classification [18, 20, 36, 45]. This strict dichotomy of visual tasks has been widely adopted because it decomposes the problem of image analysis into independent and managable components. <p> The enormous amount of research in edge detection can be broadly classified into one of several categories: optimal filtering [8, 11], scale space [30], surface fitting [17] and cost minimization <ref> [43] </ref>. A great deal of literature also exists on edge linking [3, 32, 34]. Despite this tremendous effort, there is little consensus that the problem of boundary extraction has been solved.
Reference: [44] <author> D. Terzopoulos, </author> <title> "Regularization of Inverse Visual Problems Involving Disconti-nuities, </title> " <journal> IEEE Trans. Pat. Anal. Mach. Intell., </journal> <volume> vol. PAMI-8, no. 4, </volume> <year> 1986, </year> <pages> pp. 413-424, </pages> <year> 1989. </year>
Reference-contexts: Moreover, the lower level visual tasks are generally ill-posed [37], i.e., they are underconstrained and so do not have unique solutions. The active contour models, or snakes, were originally proposed by [21] as a regularization approach <ref> [44] </ref> to the ill-posed edge detection problem.
Reference: [45] <author> S. Umeyama, </author> <title> "Parameterized Point Pattern Matching and Its Application to Recognition of Object Families," </title> <journal> IEEE Trans. Pat. Anal. Mach. Intell., </journal> <volume> vol. PAMI-15, </volume> <year> 1993, </year> <pages> pp. 136-144. </pages>
Reference-contexts: While enormous amount of literature exists in each of the fields, these works have typically been developed in isolation. The following is a representative list: contour modeling [13, 33, 35, 48], edge detection [8, 11, 17, 30, 43], edge linking [3, 32, 34], contour detection and classification <ref> [18, 20, 36, 45] </ref>. This strict dichotomy of visual tasks has been widely adopted because it decomposes the problem of image analysis into independent and managable components. Unfor-turnately, the Marr's paradigm also results in unidirectional and irreversible flow of error from one component to the next. <p> Examples include the active contour model (snake) [21], parameterized templates [47], elliptic Fourier decompositions [41], implicit polynomials [42], handprinted character recognition [19] and affine invariant contour tracking [5]. In addition, several methods that classify deformable contours have also been reported <ref> [18, 45] </ref>, but these ignore the problem of extraction. There have been relatively few methods that consider detection and classification of deformable contours directly from noisy images [19, 42]. 1.2 Contributions This thesis presents an integrated approach in modeling, extracting, detecting and classifying deformable contours directly from noisy images. <p> On one hand, the works in contour modeling <ref> [13, 33, 35, 45, 48] </ref> typically ignore the problem of extraction. Thus the starting point of modeling usually consists of segmented (binary) images, or an ordered set of points forming the contour. <p> Because matched template fails to model this effect, we have shown in Chapter 3 that its averaged correlation degrades rapidly with deformation. Moreover, it has high probability of generating false peaks for complex scenes [15, 16]. Although there exist numerous methods <ref> [18, 45] </ref> that detect and classify deformable contours, these typically assume that the contour has been previously extracted. How 63 ever, without a contour model, contour extraction has been shown to be an ill-posed problem [37]. Consequently, the practicality of these methods is limited for applications involving noisy images.
Reference: [46] <author> D. J. Williams & M. Shah, </author> <title> "A Fast Algorithm for Active Contours and Curvature Estimation," Computer Vision, Graphics, </title> <booktitle> Image Processing, </booktitle> <volume> vol. 55, </volume> <year> 1992, </year> <pages> pp. 14-26. </pages>
Reference-contexts: In contrast, small values of enable the snakes to effectively capture boundary discontinuities, but they also make the solutions very noise sensitive. Figure 2.1 illustrates these conflicting goals. Despite its significance, regularization selection were either ignored [6, 21, 29, 49], or based on empirical observations [2, 9] and heuristics <ref> [46] </ref>. Another factor which directly affects the solutions is energy formulation. The original E int formulation [21] causes a snake to shrink around strong edge points. <p> Moreover, existing curvature measure on E int is not rotational and scale invariant, causing erroneous imposition of the smoothness constraint. Snaxel initialization is the next factor with significant impact. For satisfactory results, most existing methods <ref> [2, 21, 46] </ref> require an operator to judiciously place the initial snaxels near desired boundaries. Alternatively, short snakes are initialized at locations of high intensity gradient and they either overlap [49] or extend in length [6] to cover a longer boundary. <p> Note from (2.14) that this criterion completely bypasses the explicit selection of the nuisance parameters fl fl . It can therefore be incorporated into discrete energy minimization algorithms <ref> [2, 46] </ref> with no added computational cost. <p> We employ a point-wise minimization algorithm for energy minimization which ensures convergence to local minima. This algorithm is a modification of the greedy algorithm <ref> [46] </ref>: it moves a snaxel v i to a location that minimizes the sum of energy at v i1 , v i and v i+1 , rather than just the energy in v i . 2.5.1 Real Images of body cells, an aerial photograph, and an image of soil particles. <p> Consequently, unless the initial U is placed very close to the global minimum, a gradient-based [21] or point-wise <ref> [46] </ref> algorithm will perform poorly as many local minima are present. Therefore an exhaustive search algorithm should be used. Suppose that for each point u i , we perform exhaustive search in its neighborhood of size m.
Reference: [47] <author> A. L. Yuille, D. S. Cohen & P. W. Hallinan, </author> <title> "Feature Extraction from Faces using Deformable Templates," </title> <booktitle> Proc. CVPR 1989, </booktitle> <pages> pp. 104-109. </pages>
Reference-contexts: In fact, we will show in Chapter 3 of this thesis that its performance degrades with deformation. Recently, numerous works in deformable templates have been reported, using weak models which deform in conformation to salient image features. Examples include the active contour model (snake) [21], parameterized templates <ref> [47] </ref>, elliptic Fourier decompositions [41], implicit polynomials [42], handprinted character recognition [19] and affine invariant contour tracking [5]. In addition, several methods that classify deformable contours have also been reported [18, 45], but these ignore the problem of extraction. <p> Contour extraction is then formulated as an optimization problem to maximize or minimize the overall objective function. 34 Examples of deformable template include the active contour model (snake) which uses smoothness constraints to restrict its solutions to controlled continuity splines [21]; parameterized templates for facial feature extraction <ref> [47] </ref>; elliptic Fourier decomposition for objects with shape irregularities [41]; implicit polynomials for curve and surface modeling [42]; movable control points for hand-printed character recognition [19]; and affine-invariant contour tracking [5]. With the exception of [5], these methods typically consider only global [41, 42, 47] or local [19, 21] deformations. <p> With the exception of [5], these methods typically consider only global <ref> [41, 42, 47] </ref> or local [19, 21] deformations. While global templates involve large structural interactions and contain fewer parameters to be optimized, these global parameters cannot exercise local control along the contour and their physical meaning is sometimes obscure.
Reference: [48] <author> C. T. Zahn & R. S. Roskies, </author> <title> "Fourier Descriptors for Plane Closed Curves," </title> <journal> IEEE Trans. Computers, </journal> <volume> vol. C-21, </volume> <year> 1972, </year> <pages> pp. 269-281. </pages>
Reference-contexts: While enormous amount of literature exists in each of the fields, these works have typically been developed in isolation. The following is a representative list: contour modeling <ref> [13, 33, 35, 48] </ref>, edge detection [8, 11, 17, 30, 43], edge linking [3, 32, 34], contour detection and classification [18, 20, 36, 45]. This strict dichotomy of visual tasks has been widely adopted because it decomposes the problem of image analysis into independent and managable components. <p> On one hand, the works in contour modeling <ref> [13, 33, 35, 45, 48] </ref> typically ignore the problem of extraction. Thus the starting point of modeling usually consists of segmented (binary) images, or an ordered set of points forming the contour.
Reference: [49] <author> S. Zucker, C. David, A. Dobbins & L. Iverson, </author> <title> "The Organization of Curve Detection: Coarse Tangent Fields and Fine Spline Coverings, </title> " <booktitle> Proceedings of Second International Conference on Computer Vision, </booktitle> <year> 1988, </year> <pages> pp. 568-577. </pages>
Reference-contexts: In contrast, small values of enable the snakes to effectively capture boundary discontinuities, but they also make the solutions very noise sensitive. Figure 2.1 illustrates these conflicting goals. Despite its significance, regularization selection were either ignored <ref> [6, 21, 29, 49] </ref>, or based on empirical observations [2, 9] and heuristics [46]. Another factor which directly affects the solutions is energy formulation. The original E int formulation [21] causes a snake to shrink around strong edge points. <p> Snaxel initialization is the next factor with significant impact. For satisfactory results, most existing methods [2, 21, 46] require an operator to judiciously place the initial snaxels near desired boundaries. Alternatively, short snakes are initialized at locations of high intensity gradient and they either overlap <ref> [49] </ref> or extend in length [6] to cover a longer boundary.
References-found: 48

