URL: http://www.cs.helsinki.fi/research/cosco/Projects/NONE/mdata.ps.gz
Refering-URL: http://www.cs.Helsinki.FI/research/cosco/Projects/NONE/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Phone: 26,  26,  
Title: Stochastic Complexity Based Estimation of Missing Elements in Questionnaire Data  
Author: Henry Tirri Tomi Silander 
Address: P.O.Box  FIN-00014 University of Helsinki, Finland  P.O.Box  FIN-00014 University of Helsinki, Finland  
Affiliation: Complex Systems Computation Group (CoSCo)  Department of Computer Science  Complex Systems Computation Group (CoSCo)  Department of Computer Science  
Abstract: In this paper we study a new information-theoretically justified approach to missing data estimation for multivariate categorical data. The approach discussed is a model-based imputation procedure relative to a model class (i.e., a functional form for the probability distribution of the complete data matrix), which in our case is the set of multinomial models with some independence assumptions. Based on the given model class assumption an information-theoretic criterion can be derived to select between the different complete data matrices. Intuitively this general criterion, called stochastic complexity, represents the shortest code length needed for coding the complete data matrix relative to the model class chosen. Using this information-theoretic criteria, the missing data problem is reduced to a search problem, i.e., finding the data completion with minimal stochastic complexity. In the experimental part of the paper we present empirical results of the approach using two real data sets, and compare these results to those achived by commonly used techniques such as case deletion and imputating sample averages. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aarts, E., & Korst, J. </author> <year> (1989). </year> <title> Simulated annealing and Boltzmann machines: A stochastic approach to combinatorial optimization and neural computing. </title> <address> Chichester: </address> <publisher> John Wiley & Sons. </publisher>
Reference-contexts: However, locally optimal solutions can be found by using stochastic search methods such as EM and simulated annealing <ref> (Aarts & Korst, 1989) </ref>. In this paper for the experiments we use a simple easy-to-implement variant called stochastic greedy, which has a comparable performance to the more complex search methods (Kontkanen, Myllymaki, Silander, & Tirri, 1997a). <p> The SA algorithm converges as the temperature T approaches zero. It can be shown that if the initial temperature is high enough, and the decrement of the parameter is done slowly 6 TIRRI AND SILANDER enough, the process converges to the global optimum almost surely <ref> (Aarts & Korst, 1989) </ref>. Finally, by the stochastic greedy (SG) algorithm we mean a simple procedure where new solution candidates are generated as in simulated annealing, but the candidates are accepted only if the marginal likelihood is increased.
Reference: <author> Baxter, R., & Oliver, J. </author> <year> (1994). </year> <title> MDL and MML: Similarities and differences (Tech. </title> <type> Rep. No. 207). </type> <institution> Department of Computer Science, Monash University. </institution>
Reference: <author> Berger, J. </author> <year> (1985). </year> <title> Statistical decision theory and bayesian analysis. </title> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Bernardo, J., & Smith, A. </author> <year> (1994). </year> <title> Bayesian theory. </title> <publisher> John Wiley. </publisher>
Reference-contexts: A good candidate for such a distribution is the Bayesian marginal likelihood P (DjM ) <ref> (Bernardo & Smith, 1994) </ref>, also sometimes known as the evidence, which can be computed by integrating over all possible models (parameter settings) Q, P (DjM ) = P (D obs ; D mis jM ) Z As discussed in (Rissanen, 1996) for many model classes (4) approximates SC (DjM ) extremely
Reference: <author> Cooper, G., & Herskovits, E. </author> <year> (1992). </year> <title> A Bayesian method for the induction of probabilistic networks from data. </title> <journal> Machine Learning, </journal> <volume> 9, </volume> <pages> 309347. </pages>
Reference: <author> Cover, T., & Thomas, J. </author> <year> (1991). </year> <title> Elements of information theory. </title> <address> New York, NY: </address> <publisher> John Wiley & Sons. </publisher>
Reference-contexts: However, we are interested in the problem of finding a single optimal completion of the incomplete data set instead of a set of completions typical to multiple imputation procedures. Moreover, as we will see in the next section, the augmentation criterion has its foundations in information and coding theory <ref> (Cover & Thomas, 1991) </ref> rather than Bayesian statistics. Intuitively the approach can be described as follows. <p> In general, we denote the length (in bits) of the encoding of D when the encoding is done using a code C by L C (D). All codes considered in MDL are prefix codes (Rissanen, 1989). From the Kraft inequality (see for example (Rissanen, 1989) or <ref> (Cover & Thomas, 1991) </ref>) it follows that for every prefix code C, there exists a corresponding probability distribution P such that for all data sets D of given length N (i.e., with N data instantiations), we have log P (D) = L C (D) 1 .
Reference: <author> DeGroot, M. </author> <year> (1970). </year> <title> Optimal statistical decisions. </title> <publisher> McGraw-Hill. </publisher>
Reference-contexts: Since the family of Dirichlet densities is conjugate (see e.g., <ref> (DeGroot, 1970) </ref>) to the family of multinomials, it is convenient to assume that the prior distributions of the parameters are from this family (see, e.g., (Heckerman, Geiger, & Chickering, 1995)).
Reference: <author> Dempster, A., Laird, N., & Rubin, D. </author> <year> (1977). </year> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, Series B, </journal> <volume> 39(1), </volume> <pages> 138. </pages>
Reference-contexts: In general we have several alternative approaches for searching the data matrix completion with the minimal stochastic complexity. One possibility is to use a variant of the Expectation-Maximization (EM) algorithm <ref> (Dempster et al., 1977) </ref>, which consists of two abstract plexity minimization. Algorithm 2 Simulated Annealing (SA) 1. Generate an initial random guess D mis of the missing data. Set the temperature parameter T to its initial value. 2.
Reference: <author> Gelman, A., Carlin, J., Stern, H., & Rubin, D. </author> <year> (1995). </year> <title> Bayesian data analysis. </title> <publisher> Chapman & Hall. </publisher>
Reference: <author> Gilks, W. R., Richardson, S., & J., S. D. </author> <year> (1996). </year> <title> Markov chain monte carlo in practice. </title> <address> London, GB: </address> <publisher> Chapman & Hall. </publisher>
Reference-contexts: The last quarter of a century has seen many developments in this area. The EM algorithm together with its extensions (Dempster, Laird, & Rubin, 1977; McLachlan & Thriyambakam, 1997), multiple imputation (Rubin, 1987, 1996; Schafer, 1995) and Markov Chain Monte Carlo <ref> (Gilks, Richardson, & J., 1996) </ref> all provide tools for inference in large classes of missing data problems. In practice, however, these developments have not had large impact on the way most data analysts handle missing values on a routine basis.
Reference: <editor> Greaney, V., & Kelleghan, T. (Eds.). </editor> <year> (1984). </year> <title> Equality of opportunity in irish schools. </title> <publisher> Dublin: Educational Company. </publisher>
Reference-contexts: The questionnaire also asked for gender, grade level, and other demographic information. The study involved a classification task of correctly identifying the one of the six schools using the other variables as predictors. The second data set was Irish educational transitions data (IRISH) <ref> (Greaney & Kelleghan, 1984) </ref> reanalyzed by Raftery and Hout (1993). Subjects of this data set were 500 Irish schoolchildren aged 11 in 1967. The data were also used, in a simplified form, as an example to illustrate Bayesian model selection methods by Kass and Raftery (1994).
Reference: <author> Heckerman, D., Geiger, D., & Chickering, D. </author> <year> (1995). </year> <title> Learning Bayesian networks: The combination of knowledge and statistical data. </title> <journal> Machine Learning, </journal> <volume> 20(3), </volume> <year> 197243. </year>
Reference-contexts: Since the family of Dirichlet densities is conjugate (see e.g., (DeGroot, 1970)) to the family of multinomials, it is convenient to assume that the prior distributions of the parameters are from this family (see, e.g., <ref> (Heckerman, Geiger, & Chickering, 1995) </ref>).
Reference: <author> Jensen, F. </author> <year> (1996). </year> <title> An introduction to bayesian networks. </title> <publisher> London: UCL Press. </publisher>
Reference: <author> Kass, R., & Raftery, A. </author> <year> (1994). </year> <type> Bayes factors (Tech. Rep. No. 254). </type> <institution> Department of Statistics, University of Washington. </institution>
Reference: <author> Kirkpatrick, S., Gelatt, D., & Vecchi, M. </author> <year> (1983). </year> <title> Optimization by simulated annealing. </title> <journal> Science, </journal> <volume> 220(4598), </volume> <pages> 671680. </pages>
Reference: <author> Kontkanen, P., Myllymaki, P., Silander, T., & Tirri, H. </author> <year> (1997a). </year> <title> Comparing stochastic complexity minimization algorithms in estimating missing data. </title> <booktitle> In Proceedings of wupes'97, the 4th workshop on uncertainty processing (pp. 8190). </booktitle> <address> Prague, Czech Republic. </address>
Reference-contexts: However, locally optimal solutions can be found by using stochastic search methods such as EM and simulated annealing (Aarts & Korst, 1989). In this paper for the experiments we use a simple easy-to-implement variant called stochastic greedy, which has a comparable performance to the more complex search methods <ref> (Kontkanen, Myllymaki, Silander, & Tirri, 1997a) </ref>. In the experimental part of the paper we present empirical results of the approach using two real data sets, and compare these results to those achieved by commonly used techniques such as case deletion and imputating sample averages.
Reference: <author> Kontkanen, P., Myllymaki, P., Silander, T., & Tirri, H. </author> <year> (1997b). </year> <title> On the accuracy of stochastic complexity approximations. In Proceedings of the causal models and statistical learning seminar (pp. </title> <type> 103117). </type> <address> London, UK. </address> <note> 10 TIRRI AND SILANDER Kontkanen, </note> <author> P., Myllymaki, P., Silander, T., Tirri, H., & Grunwald, P. </author> <year> (1997). </year> <title> Comparing predictive inference methods for discrete domains. </title> <booktitle> In Proceedings of the sixth international workshop on artificial intelligence and statistics (pp. 311318). </booktitle> <address> Ft. Laud-erdale, Florida. </address>
Reference: <author> Kontkanen, P., Myllymaki, P., Silander, T., Tirri, H., & Grunwald, P. </author> <year> (1998). </year> <title> Bayesian and information-theoretic priors for Bayesian network parameters. </title> <booktitle> In Proceedings of the 10th eu-ropean conference on machine learning (ECML-98). </booktitle> <address> Chemnitz, Germany. </address> <note> ((To appear).) </note> <author> Little, R., & Rubin, D. </author> <year> (1987). </year> <title> Statistical analysis with missing data. </title> <publisher> Wiley. </publisher>
Reference-contexts: Set t = t + 1. Goto 2 if not converged. simplicity, we will use here the uniform prior for the parameters, so all k and s kil are set to 1. A more detailed discussion on the priors can be found in <ref> (Kontkanen, Myllymaki, Silander, Tirri, & Grunwald, 1998, 1997) </ref>.
Reference: <editor> McLachlan, G., & Thriyambakam, K. (Eds.). </editor> <year> (1997). </year> <title> The EM algorithm and extensions. </title> <address> New York: </address> <publisher> John Wiley & Sons. </publisher>
Reference: <author> Metropolis, N., Rosenbluth, A., Rosenbluth, M., Teller, M., & Teller, E. </author> <year> (1953). </year> <title> Equations of state calculations by fast computing machines. </title> <journal> Journal of Chem. Phys., </journal> <volume> 21, </volume> <pages> 10871092. </pages>
Reference: <author> O'Hagan, A. </author> <year> (1994). </year> <title> Kendall's advanced theory of statistics. volume 2b: Bayesian inference. </title> <publisher> Cambridge: Edward Arnold. </publisher>
Reference: <author> Pearl, J. </author> <year> (1988). </year> <title> Probabilistic reasoning in intelligent systems: Networks of plausible inference. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Raftery, A., & Hout, M. </author> <year> (1993). </year> <title> Maximally maintained inequality: Expansion, reform and opportunity in irish schools. </title> <journal> Sociology of Education, </journal> <volume> 66, </volume> <pages> 4162. </pages>
Reference: <author> Rissanen, J. </author> <year> (1984). </year> <title> Universal coding, information, prediction, and estimation. </title> <journal> IEEE Trans. on Inf. Theory, IT-30(4), </journal> <volume> 629636. </volume>
Reference-contexts: The purpose of this paper is to study a new information-theoretically justified approach to missing data estimation. The method discussed is deeply related to Bayesian inference, but originates from the research on universal coding <ref> (Rissanen, 1984) </ref>, which aims at finding good (short) encodings of data. We do not make an attempt to provide a survey of the aforementioned developments in missing data estimationan interested reader can consult the excellent books by Little and Rubin (1987) and Schafer (1997).
Reference: <author> Rissanen, J. </author> <year> (1987). </year> <title> Stochastic complexity. </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> 49(3), 223239 and 252265. </volume>
Reference-contexts: Based on the given model class assumption an information-theoretic criteria can be derived to select between the different complete data matrices for the more likely one (in abstract sense). Intuitively this general criteria, called stochastic complexity <ref> (Rissanen, 1987, 1989, 1996) </ref> represents the shortest code length needed for coding the complete data matrix relative to the model class chosen. <p> Ris-sanen (1987) defines the stochastic complexity informally as follows: The stochastic complexity of the data set D with respect to the model class M is the shortest code length of D obtainable when the encoding is done with the help of class M <ref> (Rissanen, 1987, 1996) </ref>. Here with the help of has a clear intuitive meaning: if there exists a model in M which captures the regularities in D well, or equivalently gives a good fit to D, then the code length of D should be short. <p> Howeverand this is the crucial observation which makes MDL very different from maximum likelihood principle MDL says that we must code our data using some fixed code, which compresses all data sets for which there is a good-fitting model in M <ref> (Rissanen, 1987) </ref>. But the code corresponding to Q (D), i.e., the code that encodes any D 0 using L (D 0 j Q (D)) = log P (D 0 j Q (D)) bits, only gives optimal compression for some data sets (including D).
Reference: <author> Rissanen, J. </author> <year> (1989). </year> <title> Stochastic complexity in statistical inquiry. </title> <address> New Jersey: </address> <publisher> World Scientific Publishing Company. </publisher>
Reference-contexts: Data compression involves the use of a description method or code, which is a oneone mapping from datasets to their descriptions. Without loss of generality, these descriptions may be taken to be binary strings <ref> (Rissanen, 1989) </ref>. Intuitively, the shorter the description or codelength of a set of D, the more regular or simpler the set D is. <p> In general, we denote the length (in bits) of the encoding of D when the encoding is done using a code C by L C (D). All codes considered in MDL are prefix codes <ref> (Rissanen, 1989) </ref>. From the Kraft inequality (see for example (Rissanen, 1989) or (Cover & Thomas, 1991)) it follows that for every prefix code C, there exists a corresponding probability distribution P such that for all data sets D of given length N (i.e., with N data instantiations), we have log P <p> In general, we denote the length (in bits) of the encoding of D when the encoding is done using a code C by L C (D). All codes considered in MDL are prefix codes <ref> (Rissanen, 1989) </ref>. From the Kraft inequality (see for example (Rissanen, 1989) or (Cover & Thomas, 1991)) it follows that for every prefix code C, there exists a corresponding probability distribution P such that for all data sets D of given length N (i.e., with N data instantiations), we have log P (D) = L C (D) 1 . <p> Fortunately there exist several good approximations to SC <ref> (Rissanen, 1989, 1996) </ref>. One good approximation is based on the equivalence between codes and probability distributions discussed earlier. As argued before, we can map code C fl to a probability distribution P fl such that for all D, log P fl (D) = SC.
Reference: <author> Rissanen, J. </author> <year> (1996). </year> <title> Fisher information and stochastic complexity. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 42(1), </volume> <pages> 4047. </pages>
Reference-contexts: However, it turns out to be very hard to define with the help of in a formal manner. Indeed, a completely satisfactory formal definition has only been found very recently <ref> (Rissanen, 1996) </ref>. <p> such a distribution is the Bayesian marginal likelihood P (DjM ) (Bernardo & Smith, 1994), also sometimes known as the evidence, which can be computed by integrating over all possible models (parameter settings) Q, P (DjM ) = P (D obs ; D mis jM ) Z As discussed in <ref> (Rissanen, 1996) </ref> for many model classes (4) approximates SC (DjM ) extremely well, and thus in the sequel we will use this approximation as the pragmatic definition of stochastic complexity.
Reference: <author> Rubin, D. </author> <year> (1987). </year> <title> Multiple imputation for nonresponse in surveys. </title> <address> New York: </address> <publisher> John Wiley & Sons. </publisher>
Reference-contexts: In imputation-based procedures the missing data values are filled with plausible values which forces the incomplete data set into a complete data format. The methods in this group vary from simple sample average imputation approaches <ref> (Little & Rubin, 1987) </ref> to complex multiple imputation procedures (Schafer, 1997). The latter share the same underlying philosophy as EM and data augmentation: an incomplete-data problem is solved by repeatedly solving the complete-data version.
Reference: <author> Rubin, D. </author> <year> (1996). </year> <title> Multiple inputation after 18 years. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 91, </volume> <pages> 473478. </pages>
Reference: <author> Schafer, J. </author> <year> (1995). </year> <title> Model-based imputation of census short-form items. </title> <booktitle> In Proceedings of the annual research conference (pp. 267299). </booktitle> <address> Washington, DC: </address> <institution> Bureau of the Census. </institution>
Reference: <author> Schafer, J. </author> <year> (1997). </year> <title> Analysis of incomplete multivariate data. </title> <publisher> Lon-don: Chapman & Hall. </publisher>
Reference-contexts: In imputation-based procedures the missing data values are filled with plausible values which forces the incomplete data set into a complete data format. The methods in this group vary from simple sample average imputation approaches (Little & Rubin, 1987) to complex multiple imputation procedures <ref> (Schafer, 1997) </ref>. The latter share the same underlying philosophy as EM and data augmentation: an incomplete-data problem is solved by repeatedly solving the complete-data version. <p> The resulting 2 TIRRI AND SILANDER variability is then taken to reflect the uncertainty caused by the missing data. The approach discussed here can be characterized as a model-based imputation procedure. Of the existing approaches, the method described here is somewhat related to Bayesianly proper multiple imputation <ref> (Schafer, 1997) </ref>, which uses independent realizations of the posterior predictive distribution of the missing data under some complete-data model and prior. <p> In each data sample we deleted (completely at random) 5%, 10%, 20%, 35% and 50% of the elements thus creating artificial missing data problems satisfying the missing completely at random data matrix, AVG=imputing averages, RND=imputing by random, SC=imputing by minimizing the stochastic complexity). (MCAR) assumption <ref> (Schafer, 1997) </ref>. We then created complete data matrices by imputing the missing values using two alternative techniques. The first technique (AVG) was simply to impute the averages in the observed data.
Reference: <author> Schwarz, G. </author> <year> (1978). </year> <title> Estimating the dimension of a model. </title> <journal> Annals of Statistics, </journal> <volume> 6, </volume> <pages> 461464. </pages>
Reference: <author> Titterington, D., Smith, A., & Makov, U. </author> <year> (1985). </year> <title> Statistical analysis of finite mixture distributions. </title> <address> New York: </address> <publisher> John Wiley & Sons. </publisher>
Reference-contexts: In addition to selecting the multinomial model class some independence assumptions for the variables have to be made to make the stochastic complexity approach feasible in practice. This leads us to consider a special subclass of finite mixture models <ref> (Titterington et al., 1985) </ref> known as Naive Bayes models. For Naive Bayes model class, the categorical variables X i ; i 6= s are assumed to be independent, given the values of a specific observed variable X s often called the class variable.
Reference: <author> Wallace, C., & Freeman, P. </author> <year> (1987). </year> <title> Estimation and inference by compact coding. </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> 49(3), </volume> <pages> 240265. </pages>
References-found: 34

