URL: http://www.mrc-bbc.ox.ac.uk/~schultz/rstab.ps.gz
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00380.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Stability of the replica symmetric solution for the information conveyed by a neural network  
Author: Simon Schultzy and Alessandro Trevesz 
Date: (November 7, 1997)  
Note: Programme in Neuroscience, International  84.35.+i,89.70.+c,87.10.+e Typeset using REVT  
Address: Oxford, Oxford OX1 3UD, U.K.  via Beirut 2-4, 34013 Trieste, Italy  
Affiliation: Department of Experimental Psychology, South Parks Rd., University of  School for Advanced Studies,  E X  
Abstract: The information that a pattern of firing in the output layer of a feedforward network of threshold-linear neurons conveys about the network's inputs is considered. A replica-symmetric solution is found to be stable for all but small amounts of noise. The region of instability depends on the contribution of the threshold and the sparseness: for distributed pattern distributions, the unstable region extends to higher noise variances than for very sparse distributions, for which it is almost nonexistant. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Amit, H. Gutfreund, and H. Sompolinsky, Ann. </author> <title> Phys. </title> <address> (N.Y.) 173, </address> <month> 30 </month> <year> (1987). </year>
Reference-contexts: However, intuitively we expect, from the analogy of the noise with the `temperature' parameter in other models of neural networks <ref> [1] </ref> and physical systems [14] that if replica-symmetry breaking is to set in, it will do so at low noise variances. This is confirmed by the eminently sensible behaviour of the mutual information curves of Fig. 1 at medium to high noise, but nonphysical behaviour at very low noise values.
Reference: [2] <author> E. Gardner, J. </author> <title> Phys. A: </title> <journal> Math. Gen. </journal> <volume> 21, </volume> <month> 257 </month> <year> (1988). </year>
Reference-contexts: 1 It can also be shown (we have done so for the case of a Gaussian distribution), that as r ! 0, the Gaussian channel bound is also reached. 7 For the solution for another quantity, the Gardner volume, this was addressed in <ref> [2] </ref> for Ising (1) neurons. In contrast, here we are determining the stability of the solution for mutual information in a network comprised of threshold-linear neurons, although the technique proceeds very similarly.
Reference: [3] <author> W. Bialek and A. </author> <title> Zee, </title> <journal> Phys. Rev. Lett. </journal> <volume> 61, </volume> <month> 1512 </month> <year> (1988). </year>
Reference: [4] <author> A. Treves, J. </author> <title> Phys. A: </title> <journal> Math. Gen. </journal> <volume> 23, </volume> <month> 2631 </month> <year> (1990). </year>
Reference: [5] <author> J.-P. Nadal and N. Parga, </author> <title> Network 4, </title> <month> 295 </month> <year> (1993). </year>
Reference: [6] <author> A. Treves, C. A. Barnes, and E. T. </author> <title> Rolls, in Perception, Memory and Emotion: Frontier in Neuroscience, edited by T. </title> <editor> Ono et al. </editor> <publisher> (Elsevier, </publisher> <address> Amsterdam, </address> <year> 1996), </year> <journal> Chap. </journal> <volume> 37, </volume> <pages> pp. 567-579. </pages>
Reference: [7] <author> M. Mezard, G. Parisi, and M. Virasoro, </author> <title> Spin glass theory and beyond (World Scientific, </title> <address> Singapore, </address> <year> 1987). </year>
Reference: [8] <author> A. </author> <title> Treves, </title> <journal> J. Comput. Neurosci. </journal> <volume> 2, </volume> <month> 259 </month> <year> (1995). </year>
Reference: [9] <author> S. Schultz, S. Panzeri, E. T. Rolls, and A. Treves, </author> <title> in Information Theory and the Brain, edited by R. </title> <editor> Baddeley, P. Foldiak, and P. </editor> <publisher> Hancock (Cambridge University Press, </publisher> <address> Cambridge, U.K., </address> <year> 1997). </year>
Reference: [10] <author> J.-P. Nadal and N. </author> <title> Parga, </title> <booktitle> Neural Computation 6, </booktitle> <month> 491 </month> <year> (1994). </year>
Reference: [11] <author> J. R. L. de Almeida and D. J. Thouless, J. Phy. </author> <title> A: </title> <journal> Math. Gen. </journal> <volume> 11, </volume> <month> 983 </month> <year> (1978). </year>
Reference-contexts: We have to solve the eigenvalue equation A = : (42) The eigenvectors have the column-vector form = fffiz fffi g We now proceed as described in <ref> [11] </ref>. There are three classes of eigenvectors (and corresponding eigenvalues) those invariant under interchange of all indices, those invariant under interchange of all but one index, and those invariant under interchange of all but two indices. <p> As with <ref> [11] </ref>, we have for A (fffi);(flffi) an eigenvalue A = P 2Q + R (46) with in this case 1 2 (n + 1)(n 2)-fold degeneracy, and P , Q and R as described above.
Reference: [12] <author> A. Treves, J. </author> <title> Phys. A: </title> <journal> Math. Gen. </journal> <volume> 24, </volume> <month> 2645 </month> <year> (1991). </year>
Reference: [13] <author> E. Gardner and B. Derrida, J. </author> <title> Phys. A: </title> <journal> Meth. Gen. </journal> <volume> 21, </volume> <month> 271 </month> <year> (1988). </year>
Reference: [14] <author> D. Sherrington and S. </author> <title> Kirkpatrick, </title> <journal> Phys. Rev. Lett. </journal> <volume> 35, </volume> <month> 1792 </month> <year> (1975). </year>
Reference-contexts: However, intuitively we expect, from the analogy of the noise with the `temperature' parameter in other models of neural networks [1] and physical systems <ref> [14] </ref> that if replica-symmetry breaking is to set in, it will do so at low noise variances. This is confirmed by the eminently sensible behaviour of the mutual information curves of Fig. 1 at medium to high noise, but nonphysical behaviour at very low noise values.

References-found: 14

