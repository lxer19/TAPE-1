URL: ftp://ftp.cs.colorado.edu/users/mozer/papers/dolce.ps
Refering-URL: http://www.cs.colorado.edu/~mozer/papers/dolce.html
Root-URL: http://www.cs.colorado.edu
Title: Dynamic on-line clustering and state extraction: An approach to symbolic learning  
Author: Sreerupa Das Michael Mozer 
Date: August 20, 1997  
Address: 11900 North Pecos Street, Room 31K52 Denver, CO 80234  Boulder, CO 80309-0430  
Affiliation: Lucent Technologies, Bell Labs Innovations  Department of Computer Science University of Colorado  
Abstract-found: 0
Intro-found: 1
Reference: <author> Ball, G. and Hall, D. </author> <year> (1966). </year> <title> ISODATA: a novel method of data analysis and pattern classification. </title> <type> Technical report, </type> <institution> Stanford Research Institute, Menlo Park, California. </institution>
Reference: <author> Das, S. and Das, R. </author> <year> (1991). </year> <title> Induction of discrete state-machine by stabilizing a continuous recurrent network using clustering. </title> <journal> Computer Science and Informatics, </journal> <volume> 21(2) </volume> <pages> 35-40. </pages> <note> Special Issue on Neural Computing. </note>
Reference: <author> Das, S. and Mozer, M. </author> <year> (1994). </year> <title> A hybrid clustering/gradient descent architecture for finite state machine induction. </title> <editor> In Cowan, J., Tesauro, G., and Alspector, J., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> pages 19-26. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Elman, J. </author> <year> (1990). </year> <title> Finding structure in time. </title> <journal> Cognitive Science, </journal> <volume> 14(2). </volume>
Reference: <author> Forgy, E. </author> <year> (1965). </year> <title> Cluster analysis of multivariate data: efficiency versus interpretability of classifications. Biometrics, </title> <publisher> 21:768. </publisher>
Reference: <author> Giles, C., Miller, C. B., Chen, H., Sun, G., and Lee, Y. </author> <year> (1992). </year> <title> Learning and extracting finite state automata with second-order recurrent neural network. </title> <journal> Neural Computation, </journal> <volume> 4(3) </volume> <pages> 393-405. </pages>
Reference-contexts: However, instead of having standard connectivity between the input (including the context layer) and the hidden layer, the connections into the hidden layer in DOLCE u are of the second order <ref> (inspired by Giles et al., 1992) </ref>, meaning that the net input to a hidden unit is described by the equation: h i (t) = j k where h i (t) represents the activity of the i-th hidden unit at time t, x k represents the activity of the k-th input unit,
Reference: <author> McMillan, C., Mozer, M., and Smolensky, P. </author> <year> (1992). </year> <title> Rule induction through integrated symbolic and subsymbolic processing. </title> <editor> In Moody, J., Hanson, S., and Lippmann, R., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 969-976. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Mozer, M. and Bachrach, J. </author> <year> (1990). </year> <title> Discovering the structure of a reactive environment by exploration. </title> <journal> Neural Computation, </journal> <volume> 2(4) </volume> <pages> 447-457. </pages>
Reference: <author> Mozer, M. and Das, S. </author> <year> (1993). </year> <title> A connectionist symbol manipulator that discovers the structure of context-free languages. </title> <editor> In Giles, C., Hanson, S., and Cowan, J., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 863-870. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Nowlan, S. and Hinton, G. </author> <year> (1992). </year> <title> Simplifying neural networks by soft weight-sharing. Neural computation, </title> <publisher> 4(4):473. </publisher>
Reference: <author> Pollack, J. </author> <year> (1991). </year> <title> The induction of dynamical recognizers. </title> <journal> Machine Learning, </journal> 7(2/3):123-148. 
Reference: <author> Schutze, H. </author> <year> (1993). </year> <title> Word space. </title> <editor> In Giles, C., Hanson, S., and Cowan, J., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 895-902. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher> <month> August 20, </month> <note> 1997 26 Servan-Schreiber, </note> <author> D., Cleeremans, A., and Mcclelland, J. </author> <year> (1991). </year> <title> Graded state machines: the representation of temporal contingencies in simple recurrent network. </title> <journal> Machine Learning, </journal> 7(2/3):161-193. 
Reference: <author> Towell, G. and Shavlik, J. </author> <year> (1992). </year> <title> Interpretion of artificial neural networks: mapping knowledge-based neural networks into rules. </title> <editor> In Touretzky, D., editor, </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <pages> pages 977-984. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Watrous, R. and Kuhn, G. </author> <year> (1992). </year> <title> Induction of finite state languages using second-order recurrent networks. </title> <editor> In Moody, J., Hanson, S., and Lippmann, R., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 309-316, </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference: <author> Zeng, Z., Goodman, R., and Smythe, P. </author> <year> (1993). </year> <title> Learning finite state machines with self-clustering recurrent networks. </title> <journal> Neural Computation, </journal> <volume> 5(6) </volume> <pages> 976-990. </pages>
Reference-contexts: Giles et al. (1992) evenly segmented the activation space, and each segment with non-zero membership was considered a state. In a second approach, quantization is enforced during training by mapping the hidden state at each time step to the nearest corner of a [0; 1] n hypercube <ref> (Zeng, Goodman, & Smyth, 1993) </ref>. During the testing phase, a hidden activity pattern is discretized by mapping it to the center of the region to which it belongs. Each of these approaches has its limitations.
References-found: 15

