URL: http://www.cse.ogi.edu/~hyang/paper/ngdIEEEnn97.ps.Z
Refering-URL: http://www.cse.ogi.edu/~hyang/paper.html
Root-URL: http://www.cse.ogi.edu
Email: E-mails: hhy@koala.riken.go.jp, amari@zoo.riken.go.jp  
Phone: FAX: +81 48462 4633  
Title: Natural Gradient Descent for Training Multi-Layer Perceptrons  
Author: Howard Hua Yang and Shun-ichi Amari 
Date: May 16, 1996  
Note: Submitted to IEEE Tr. on Neural Networks  
Address: Hirosawa 2-1, Wako-shi, Saitama 351-01, JAPAN  
Affiliation: Lab. for Information Representation, FRP, RIKEN  
Abstract: The main difficulty in implementing the natural gradient learning rule is to compute the inverse of the Fisher information matrix when the input dimension is large. We have found a new scheme to represent the Fisher information matrix. Based on this scheme, we have designed an algorithm to compute the inverse of the Fisher information matrix. When the input dimension n is much larger than the number of hidden neurons, the complexity of this algorithm is of order O(n 2 ) while the complexity of conventional algorithms for the same purpose is of order O(n 3 ). The simulation has confirmed the efficience and robustness of the natural gradient learning rule.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Amari. </author> <title> Differential-Geometrical Methods in Statistics, </title> <booktitle> Lecture Notes in Statistics vol.28. </booktitle> <publisher> Springer, </publisher> <year> 1985. </year>
Reference: [2] <author> S. Amari. </author> <title> Natural gradient works efficiently in learning. Accepted by Neural Computation, </title> <year> 1997. </year>
Reference-contexts: 1 Introduction Inversion of the Fisher information matrix is required to obtain the Cramer-Rao lower bound which is fundamental for analyzing the performance of an unbiased estimator. It is also needed in the natural gradient learning framework <ref> [2, 3] </ref> to design statistically efficient learning algorithms for parameter estimation in general and for training neural networks in particular. In this paper, we assume a stochastic model for multi-layer perceptrons. <p> Here, the Fisher information matrix G () plays the role of the Riemannian metric tensor. See (Amari, 1985)[1] and (Murray and Rice, 1993)[9] for information geometry. The natural gradient descent method proposed by Amari <ref> [3, 2] </ref> makes use of the Riemannian metric given by the Fisher information matrix to optimize the learning dynamics such that the Cramer-Rao lower bound is achieved asymptotically. The idea is to convert the covariant gradient @l @ into contravariant form G 1 @l @ . <p> The idea is to convert the covariant gradient @l @ into contravariant form G 1 @l @ . It is shown by Amari <ref> [2] </ref> that the steepest descent direction of a function C () in the Riemannian space fi is e rC () = G 1 ()rC (): The on-line learning algorithms corresponding to @l @ @ are, respectively, the ordinary gradient descent algorithm: t+1 = t t @ and the natural gradient descent <p> It is proved in <ref> [2] </ref> that the natural gradient learning is Fisher efficient. <p> It is shown in <ref> [2] </ref> that the gradient descent learning rule (6) is Fisher efficient.
Reference: [3] <author> S. Amari. </author> <title> Neural learning in structured parameter spaces natural Riemannian gradient. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <volume> 9, </volume> <publisher> MIT Press: </publisher> <address> Cambridge, MA. </address> <note> (to appear), </note> <year> 1997. </year>
Reference-contexts: 1 Introduction Inversion of the Fisher information matrix is required to obtain the Cramer-Rao lower bound which is fundamental for analyzing the performance of an unbiased estimator. It is also needed in the natural gradient learning framework <ref> [2, 3] </ref> to design statistically efficient learning algorithms for parameter estimation in general and for training neural networks in particular. In this paper, we assume a stochastic model for multi-layer perceptrons. <p> Here, the Fisher information matrix G () plays the role of the Riemannian metric tensor. See (Amari, 1985)[1] and (Murray and Rice, 1993)[9] for information geometry. The natural gradient descent method proposed by Amari <ref> [3, 2] </ref> makes use of the Riemannian metric given by the Fisher information matrix to optimize the learning dynamics such that the Cramer-Rao lower bound is achieved asymptotically. The idea is to convert the covariant gradient @l @ into contravariant form G 1 @l @ . <p> [d 1 (w; b)I + fd 2 (w; b) d 1 (w; b)gu i u T and its inverse formula G 1 (w) = 2 [ d 1 (w; b) 1 d 1 (w; b) i ]: (28) When b = 0, the above formula is found by Amari in <ref> [3] </ref>. 7 To obtain the explicit forms of the other blocks A ij in G, we need to introduce two bases in &lt; n which possess certain properties.
Reference: [4] <author> S. Amari and N. Murata. </author> <title> Statistical theory of learning curves under entropic loss criterion. </title> <journal> Neural Computation, </journal> <volume> 5 </volume> <pages> 140-153, </pages> <year> 1993. </year>
Reference: [5] <author> S. Amari, N. Murata, K.-R. Muller, M. Finke, and H. H. Yang. </author> <title> Asymptotic statistical theory of overtraining and cross-validation. </title> <journal> IEEE Trans. on Neural Networks, </journal> <note> page to appear, </note> <year> 1997. </year>
Reference-contexts: Minimizing the training error is not equivalent to minimizing the generalization error in general (see, for example, Amari and Murata, 1993)[4]. Some auxiliary methods such as early stopping and regularization are necessary to avoid overtraining or overfitting (see, for example, Ripley, 1996 [10]; Amari et al, 1997 <ref> [5] </ref>). The overtraining problem is important but it will not be discussed here.
Reference: [6] <author> J.-F. Cardoso and B. Laheld. </author> <title> Equivariant adaptive source separation. </title> <journal> IEEE Trans. on Signal Processing, </journal> <volume> 44(12) </volume> <pages> 3017-3030, </pages> <month> December </month> <year> 1996. </year>
Reference-contexts: We can also apply the following on-line whitening algorithm in <ref> [6] </ref>: B t+1 = (1 + )B t u t u T 10 where u t = B t x t and &gt; 0 is a learning rate. If the input is not Gaussian, we need a non-linear function to transform the input x t to a Gaussian process.
Reference: [7] <author> C. Darken and J. Moody. </author> <title> Towards faster stochastic gradient search. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <volume> 4, </volume> <editor> eds. Moody, Hanson, and Lippmann, </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, </address> <pages> pages 1009-1016, </pages> <year> 1992. </year>
Reference-contexts: The learning rate function i (t) is a special form of the following search-then-converge schedules proposed in <ref> [7] </ref>: (t) = t t + t 2 : (60) t &lt; t is a "search phase" and t &gt; t is a "converge phase". The learning rate functions i (t) do not have the search phase but they start learning with a weaker converge phase when i are small.
Reference: [8] <author> R. A. Jacobs. </author> <title> Increased rates of convergence through learning rate adaptation. </title> <booktitle> Neural Networks, </booktitle> <volume> 1 </volume> <pages> 295-307, </pages> <year> 1988. </year>
Reference: [9] <author> M. K. Murray and J. W. Rice. </author> <title> Differential Geometry and Statistics . New York: </title> <publisher> Chapman & Hall, </publisher> <year> 1993. </year>
Reference: [10] <author> B. D. Ripley. </author> <title> Pattern Recognition and Neural Networks. </title> <publisher> Cambridge University Press, </publisher> <year> 1996. </year>
Reference-contexts: Minimizing the training error is not equivalent to minimizing the generalization error in general (see, for example, Amari and Murata, 1993)[4]. Some auxiliary methods such as early stopping and regularization are necessary to avoid overtraining or overfitting (see, for example, Ripley, 1996 <ref> [10] </ref>; Amari et al, 1997 [5]). The overtraining problem is important but it will not be discussed here.
Reference: [11] <author> G. W. Stewart. </author> <title> Introduction to Matrix Computations. </title> <address> New York: </address> <publisher> Academic Press, </publisher> <year> 1973. </year>
Reference-contexts: The size of the working memory needed is still of order O (n 2 ) since m t n. But, when m t n, the time complexity of computing G 1 11 is significantly reduced from O (n 3 ) by conventional methods such as the one in <ref> [11] </ref> to O (n 2 ) by our method.
Reference: [12] <author> A. Stuart and J. K. </author> <title> Ord. Kendall's Advanced Theory of Statistics. </title> <editor> Edward Arnold, </editor> <year> 1994. </year> <month> 25 </month>
Reference-contexts: If the probability density function (pdf) of the input can be approximated by some expansions such as the Gram-Charlier expansion and the Edgeworth expansion <ref> [12] </ref>, F x () can be approximated online by using an adaptive algorithm to compute the moments or cumulants in these expansions.
References-found: 12

