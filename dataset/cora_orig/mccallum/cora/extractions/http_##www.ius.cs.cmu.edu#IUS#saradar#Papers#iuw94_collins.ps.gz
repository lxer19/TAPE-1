URL: http://www.ius.cs.cmu.edu/IUS/saradar/Papers/iuw94_collins.ps.gz
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/rcollins/www/Pub/iuw94.html
Root-URL: 
Title: Site Model Acquisition under the UMass RADIUS Project  
Author: Robert T. Collins, Allen R. Hanson, Edward M. Riseman 
Address: Box 34610,  Amherst, MA. 01003-4610  
Affiliation: Department of Computer Science Lederle Graduate Research Center  University of Massachusetts  
Abstract: A set of image understanding (IU) modules is being developed for performing several geometric site modeling tasks, including initial model acquisition, model extension, model-to-image registration and site model refinement. This paper describes how the UMass system would acquire an initial site model. IU algorithms have been developed to hypothesize potential building roofs in an image, automatically locate supporting geometric evidence in other images, and determine the precise shape and position of the new buildings via multi-image triangulation. This process is demonstrated on a subset of images from the RADIUS Model Board 1 data set. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Beveridge and E. Riseman, </author> <title> "Hybrid Weak-Perspective and Full-Perspective Matching," </title> <booktitle> Proceedings IEEE Computer Vision and Pattern Recognition, </booktitle> <address> Champaign, IL, </address> <year> 1992, </year> <pages> pp. 432-438. </pages>
Reference-contexts: The UMass design philosophy emphasizes model-directed processing, rigorous 3D perspective camera equations, and fusion of information across multiple images for increased accuracy and reliability. Acquired site models will be used for automated model-to-image registration and resection of new images <ref> [1] </ref>. Proper registration between an incoming image and a stored geometric site model determines the position and appearance of model features in the image. The model can then be overlaid on the image to aid visual change detection and verification of expected scene features.
Reference: [2] <author> M. Boldt, R. Weiss and E. Riseman, </author> <title> "Token Based Extraction of Straight Lines," </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> Vol. 19, No. 6, </volume> <year> 1989, </year> <pages> pp. 1581-1594. </pages>
Reference-contexts: To help bridge the huge representational gap between pixels and site models, feature extraction routines are applied to produce symbolic, geometric representations of potentially important image features. Many algorithms for acquiring building models rely on extracted straight line segments. Algorithm. We use the Boldt algorithm for extracting line segments <ref> [2] </ref>. At the heart of the Boldt algorithm is a hierarchical grouping system inspired by the Gestalt laws of perceptual organization. Zero-crossing points of the Laplacian of the intensity image provide an initial set of local intensity edges.
Reference: [3] <author> R. Collins, A. Hanson, E. Riseman and Y. Cheng, </author> <title> "Model Matching and Extension for Automated 3D Site Modeling," </title> <booktitle> Proceedings Arpa Image Understanding Workshop, </booktitle> <address> Wash-ington, DC, </address> <month> April </month> <year> 1993, </year> <pages> pp. 197-203. </pages>
Reference-contexts: Projective mapping of intensity information from the images onto these polyhedral models results in a compelling site model display that can be interactively explored on the SGI using fly-through graphics. The algorithms described here are part of a larger system being developed at UMass for site modeling applications <ref> [3] </ref>. The UMass design philosophy emphasizes model-directed processing, rigorous 3D perspective camera equations, and fusion of information across multiple images for increased accuracy and reliability. Acquired site models will be used for automated model-to-image registration and resection of new images [1].
Reference: [4] <author> C. Jaynes, F. Stolle and R. Collins, </author> <title> "Task Driven Perceptual Organization for Extraction of Rooftop Polygons," </title> <booktitle> Proc. Arpa Image Understanding Workshop, 1994 (these proceedings). </booktitle>
Reference-contexts: Algorithm. The building detection algorithm is based on finding image polygons corresponding to the boundaries of flat, rectilinear rooftops in the scene. The algorithm is described in detail elsewhere in these proceedings <ref> [4] </ref>. Briefly, possible roof corners are identified by convolution with a set of oriented corner templates that respond to perspective projections of flat, orthogonal rooftop corners in the scene. Perceptually compatible corner pairs initiate a search for supporting line segment data. <p> Results. The building detector was run on image J3. This happens to be a near-nadir view, but nothing in the code precludes using one of the oblique views instead (see <ref> [4] </ref>). Roof detection is computa-tionally expensive due to low-level feature extraction and the rapid growth of the feature-relation graph with image size. For this experiment the image was partitioned into nine separate chunks, loosely representing different "functional areas".
Reference: [5] <author> R. Kumar and A. Hanson, </author> <title> "Application of Pose Determination Techniques to Model Extension and Refinement," </title> <booktitle> Proceedings Darpa Image Understanding Workshop, </booktitle> <address> San Diego, CA, </address> <month> January </month> <year> 1992, </year> <pages> pp. 727-744. </pages>
Reference-contexts: Two other important site modeling tasks are model extension updating the geometric site model by adding or removing new buildings based on the results of change detection and model refinement iteratively refining the shape, placement and surface structure of building models as more views become available <ref> [5] </ref>. Model extension and refinement are expected to be ongoing processes that are repeated whenever new images become available, each up dated model becoming the current site model for the next iteration.
Reference: [6] <author> Martin Marietta and SRI International, </author> <title> RCDE User's Guide, Martin Marietta, </title> <booktitle> Management and Data Systems, </booktitle> <address> Philadelphia, PA, </address> <year> 1993. </year>
Reference-contexts: The model board images were not supplied with an accurate set of camera parameters, however. We originally formed DLT matrices for images J1-J8 using the resected camera parameters provided with version 1.0 of the RCDE (RADIUS Common Development Environment) software package <ref> [6] </ref>. The RCDE camera parameters worked fine for building detection and epipolar matching, but the building triangulation results were not very accurate when compared with corresponding 3D ground truth measurements.
Reference: [7] <author> H. Schultz, </author> <title> "Terrain Reconstruction from Oblique Views," </title> <booktitle> Proc. Arpa Image Understanding Workshop, 1994 (these proceedings). </booktitle>
Reference-contexts: For the Model Board 1 site we represented the ground as a horizontal plane with Z-coordinate value determined from the ground truth measurements. More generally, we will soon be combining our symbolic building extraction routines with the digital terrain maps produced by the UMass Terrain Reconstruction System <ref> [7] </ref>. Results. Outlines of the final set of triangulated rooftops are shown in Figure 4. The rightmost polygon in the image is noticeably incorrect. This polygon actually corresponds to a split-level building containing two roofs at different heights in the scene.
References-found: 7

