URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR96721.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: Key Concepts For Parallel Out-Of-Core LU Factorization  
Author: Jack J. Dongarra zfl Sven Hammarling David W. Walker 
Date: March 28, 1996  
Abstract: This paper considers key ideas in the design of out-of-core dense LU factorization routines. A left-looking variant of the LU factorization algorithm is shown to require less I/O to disk than the right- looking variant, and is used to develop a parallel, out-of-core implementation. This implementation makes use of a small library of parallel I/O routines, together with ScaLAPACK and PBLAS routines. Results for runs on an Intel Paragon are presented and interpreted using a simple performance model.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Anderson, Z. Bai, C. H. Bischof, J. Demmel, J. J. Dongarra, J. Du Croz, A. Greenbaum, S. Hammarling, A. McKenney, S. Ostrouchov, and D. C. Sorensen. </author> <title> LAPACK Users' Guide. </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA, USA, 2nd edition, </address> <year> 1995. </year>
Reference-contexts: After all the block columns to the left of the block have been processed in this way using the Level 3 BLAS routines TRSM and GEMM [6], the matrix E is then factored using the LAPACK routine GETRF <ref> [1] </ref>. If the matrix is stored on disk without applying the pivots to it, then whenever a block column is read in the pivots found up to that point must be applied to it using LASWP, an LAPACK auxiliary routine.
Reference: [2] <author> J. Choi, J. Demmel, I. Dhillon, J. J. Dongarra, S. Ostrouchov, A. P. Petitet, K. Stanley, D. W. Walker, and R. C. Whaley. </author> <title> ScaLAPACK: a portable linear algebra library for distributed memory computers - design issues and performance. </title> <note> LAPACK Working Note No.95. Technical Report CS-95-283, </note> <institution> Department of Computer Science, University of Tennessee, 107 Ayres Hall, </institution> <address> Knoxville, TN 37996-1301, USA, </address> <year> 1995. </year>
Reference-contexts: Section 5 outlines the main components of a library of routines for perform <p>- ing I/O on dense matrices. A complete parallel, out-of-core LU factorization routine is described in section 6. This algorithm is implemented in terms of the BLACS [9], PBLAS [3], and ScaLAPACK <ref> [2] </ref> routines. Section 7 presents some preliminary performance results on the Intel Paragon. A summary and conclusions are presented in section 8. 2 Sequential Out-Of-Core LU Factorization Let us consider the decomposition of the matrix A into its LU factorization with the matrix partitioned in the following way. <p> necessary to position the file pointer (at the start of the file) only once in each pass through the outer loop. 4 Approaches To Parallel I/O Our discussion of parallel I/O for dense matrices assumes that in-core matrices are distributed over processes using a block-cyclic data distribution as in ScaLAPACK <ref> [4, 2] </ref>. Processes are viewed as being laid out with a two- dimensional logical topology, forming a P fi Q process mesh. Our approach 12 to parallel I/O for dense matrices hinges on the number of file pointers, and on which processes have access to the file pointers.
Reference: [3] <author> J. Choi, J. J. Dongarra, S. Ostrouchov, A. P. Petitet, D. W. Walker, and R. C. Whaley. </author> <title> A proposal for a set of Parallel Basic Linear Algebra Subprograms. </title> <note> LAPACK Working Note No.100. Technical Report CS95-292, </note> <institution> Department of Computer Science, University of Tennessee, 107 Ayres Hall, </institution> <address> Knoxville, TN 37996-1301, USA, </address> <year> 1995. </year>
Reference-contexts: Section 5 outlines the main components of a library of routines for perform <p>- ing I/O on dense matrices. A complete parallel, out-of-core LU factorization routine is described in section 6. This algorithm is implemented in terms of the BLACS [9], PBLAS <ref> [3] </ref>, and ScaLAPACK [2] routines. Section 7 presents some preliminary performance results on the Intel Paragon.
Reference: [4] <author> J. Choi, J. J. Dongarra, R. Pozo, and D. W. Walker. </author> <title> ScaLAPACK: A scalable linear algebra library for distributed memory concurrent com-puters. </title> <booktitle> In Proceedings of the Fourth Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 120-127. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1992. </year> <month> 31 </month>
Reference-contexts: necessary to position the file pointer (at the start of the file) only once in each pass through the outer loop. 4 Approaches To Parallel I/O Our discussion of parallel I/O for dense matrices assumes that in-core matrices are distributed over processes using a block-cyclic data distribution as in ScaLAPACK <ref> [4, 2] </ref>. Processes are viewed as being laid out with a two- dimensional logical topology, forming a P fi Q process mesh. Our approach 12 to parallel I/O for dense matrices hinges on the number of file pointers, and on which processes have access to the file pointers.
Reference: [5] <author> Peter Corbett, Dror Feitelson, Sam Fineberg, Yarsun Hsu, Bill Nitzberg, Jean-Pierre Prost, Marc Snir, Bernard Traversat, and Parkson Wong. </author> <title> Overview of the MPI-IO parallel I/O interface. </title> <booktitle> In IPPS '95 Workshop on Input/Output in Parallel and Distributed Systems, </booktitle> <pages> pages 1-15, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: Parallel I/O is an area of much active research (see, for example, [12] and the parallel I/O archive at 17 http://www.cs.dartmouth.edu/pario.html for more information.) Although there is currently no generally accepted parallel I/O standard, MPI-IO, the proposed extensions to MPI [14] for performing parallel I/O, is a strong contender <ref> [5] </ref>. We shall, therefore, briefly consider how the BLAPIOS might be implemented on top of MPI-IO. MPI-IO contains routines for collective and independent I/O operations. All the I/O operations in the BLAPIOS are independent. MPI-IO partitions a file using filetypes, which are an extension of MPI datatypes.
Reference: [6] <author> J. J. Dongarra, J. Du Croz, I. S. Duff, and S. Hammarling. </author> <title> A set of Level 3 Basic Linear Algebra Subprograms. </title> <journal> ACM Trans. Math. Software, </journal> <volume> 16 </volume> <pages> 1-28, </pages> <year> 1990. </year> <title> (Algorithm 679). </title>
Reference-contexts: After all the block columns to the left of the block have been processed in this way using the Level 3 BLAS routines TRSM and GEMM <ref> [6] </ref>, the matrix E is then factored using the LAPACK routine GETRF [1].
Reference: [7] <author> J. J. Dongarra, J. Du Croz, S. Hammarling, and R. J. Hanson. </author> <title> An extended set of FORTRAN Basic Linear Algebra Subprograms. </title> <journal> ACM Trans. Math. Software, </journal> <volume> 14 </volume> <pages> 1-32, </pages> <year> 1988. </year> <title> (Algorithm 656). </title>
Reference-contexts: Matrix-matrix operations generally perform more efficiently than matrix-vector operations on high performance computers. However, if the block size is equal to 1, then a matrix-vector operation is used to perform an outer product | generally the least efficient of the Level 2 BLAS <ref> [7] </ref> since it updates the whole submatrix. Note that the original array A may be used to store the factorization, since the L is unit lower triangular and U is upper triangular.
Reference: [8] <author> J. J. Dongarra, F. G. Gustavson, and A. Karp. </author> <title> Implementing linear al-gebra algorithms for dense matrices on a vector pipeline machine. </title> <journal> SIAM Review, </journal> <volume> 26 </volume> <pages> 91-112, </pages> <year> 1984. </year>
Reference-contexts: These variants have been called the i,j,k variants owing to the arrangement of loops in the algorithm. For a more complete discussion of the different variants, see <ref> [8, 13] </ref>. We now develop these block variants of LU factorization with partial pivoting. 2.1 Right-Looking Algorithm Suppose that a partial factorization of A has been obtained so that the first k columns of L and the first k rows of U have been evaluated.
Reference: [9] <author> J. J. Dongarra and R. C. Whaley. </author> <title> A users' guide to the BLACS v1.0. </title> <note> LAPACK Working Note No.94. Technical Report CS-95-281, </note> <institution> Department of Computer Science, University of Tennessee, 107 Ayres Hall, </institution> <address> Knoxville, TN 37996-1301, USA, </address> <year> 1995. </year>
Reference-contexts: Section 5 outlines the main components of a library of routines for perform <p>- ing I/O on dense matrices. A complete parallel, out-of-core LU factorization routine is described in section 6. This algorithm is implemented in terms of the BLACS <ref> [9] </ref>, PBLAS [3], and ScaLAPACK [2] routines. Section 7 presents some preliminary performance results on the Intel Paragon.
Reference: [10] <author> A. Edelman. </author> <title> Large dense numerical linear algebra in 1993: The parallel computing influence. </title> <journal> International Journal Supercomputer Applications, </journal> <volume> 7 </volume> <pages> 113-128, </pages> <year> 1993. </year>
Reference-contexts: These types of large linear system arise, for example, in three-dimensional electromagnetic scattering problems and in fluid flow past complex objects <ref> [10, 11] </ref>. This paper presents a prototype for the design of a parallel software li <p>- brary for the out-of-core solution of dense linear systems.
Reference: [11] <author> A. Edelman. </author> <title> Large dense numerical linear algebra in 1994: The contin-uing influence of parallel computing. </title> <booktitle> In Proceedings of the 1994 Scalable High Performance Computing Conference, </booktitle> <pages> pages 781-787. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1994. </year>
Reference-contexts: These types of large linear system arise, for example, in three-dimensional electromagnetic scattering problems and in fluid flow past complex objects <ref> [10, 11] </ref>. This paper presents a prototype for the design of a parallel software li <p>- brary for the out-of-core solution of dense linear systems.
Reference: [12] <institution> Proceedings of the Third Annual Workshop on I/O in Parallel and Distributed Systems. </institution> <note> Held in conjunction with IPPS`95, </note> <institution> Santa Barbara, </institution> <month> April </month> <year> 1995. </year>
Reference-contexts: Parallel I/O is an area of much active research (see, for example, <ref> [12] </ref> and the parallel I/O archive at 17 http://www.cs.dartmouth.edu/pario.html for more information.) Although there is currently no generally accepted parallel I/O standard, MPI-IO, the proposed extensions to MPI [14] for performing parallel I/O, is a strong contender [5].
Reference: [13] <author> J. Ortega and C. Romine. </author> <title> The ijk forms of factorization II. Parallel systems. </title> <journal> Parallel Computing, </journal> <volume> 7(2) </volume> <pages> 149-162, </pages> <year> 1988. </year> <month> 32 </month>
Reference-contexts: These variants have been called the i,j,k variants owing to the arrangement of loops in the algorithm. For a more complete discussion of the different variants, see <ref> [8, 13] </ref>. We now develop these block variants of LU factorization with partial pivoting. 2.1 Right-Looking Algorithm Suppose that a partial factorization of A has been obtained so that the first k columns of L and the first k rows of U have been evaluated.
Reference: [14] <author> M. Snir, S. W. Otto, S. Huss-Lederman, D. W. Walker, and J. J. Don-garra. </author> <title> MPI: The Complete Reference. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, USA, </address> <year> 1996. </year>
Reference-contexts: Parallel I/O is an area of much active research (see, for example, [12] and the parallel I/O archive at 17 http://www.cs.dartmouth.edu/pario.html for more information.) Although there is currently no generally accepted parallel I/O standard, MPI-IO, the proposed extensions to MPI <ref> [14] </ref> for performing parallel I/O, is a strong contender [5]. We shall, therefore, briefly consider how the BLAPIOS might be implemented on top of MPI-IO. MPI-IO contains routines for collective and independent I/O operations. All the I/O operations in the BLAPIOS are independent.
Reference: [15] <author> S. Toledo and F. Gustavson. </author> <title> The design and implementation of SOLAR, a portable library for scalable out-of-core linear algebra computations. </title> <booktitle> In Fourth Annual Workshop on I/O in Parallel and Distributed Systems. </booktitle> <publisher> ACM Press, </publisher> <month> May </month> <year> 1996. </year> <month> 33 </month>
Reference-contexts: Thus, we describe only the high-level functionality of the BLAPIOS, and defer specifying the detailed semantics and syntax. A similar approach has been taken by Toledo and Gustavson in the Matrix Input-Output Subroutines (MIOS) which forms part of the SOLAR library for out-of-core dense matrix computations <ref> [15] </ref>. Before describing the BLAPIOS we shall consider the fundamental I/O operation supported by the BLAPIOS in which a rectangular array of data is read from (written to) the out-of-core file into (from) a given in-core array.
References-found: 15

