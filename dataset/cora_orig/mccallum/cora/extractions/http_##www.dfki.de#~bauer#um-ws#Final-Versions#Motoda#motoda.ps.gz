URL: http://www.dfki.de/~bauer/um-ws/Final-Versions/Motoda/motoda.ps.gz
Refering-URL: http://www.dfki.de/~bauer/um-ws/
Root-URL: 
Title: Extracting Behavioral Patterns from Relational History Data  
Author: Hiroshi Motoda Takashi Washio Toshihiro Kayama Kenichi Yoshida 
Address: Mihogaoka, Ibaraki, Osaka 567, Japan  350, Japan  
Affiliation: The Inst. of Scientific and Industrial Research, Osaka University  Advanced Research Laboratory, Hitachi, Ltd. Hatoyama, Saitama  
Abstract: Identifying user-dependent information that can be automatically collected helps build a user model by which to predict what the user wants to do next. Such information is often relational and is not suited to the traditional attribute-value representation. Graph-based induction (GBI), a machine learning technique to find typical patterns in a directed graph, efficiently extracts rules to predict next command from such data. The heart of GBI is pairwise chunking. The paper shows how this simple mechanism applies to the top down induction of decision trees for nested attribute representation. The algorithm is implemented and tested against both artificial and real data. The results clearly shows that the dependency analysis of computational processes activated by the user commands which is made possible by GBI is indeed useful to build a behavior model and increase prediction accuracy.
Abstract-found: 1
Intro-found: 1
Reference: [ Breiman et al., 1984 ] <author> L. J. Breiman, H. Friedman, R. A. Olshen, and C. J. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth & Brooks/Cole Advanced Books & Software, </publisher> <year> 1984. </year>
Reference-contexts: Proper termination condition must be used in accordance with the selection criterion (e.g., iteration number, chunk size, change rate of selection index, etc.). Examples of such indexes are information gain [ Quinlan, 1986 ] , information gain ratio [ Quinlan, 1993 ] and gini index <ref> [ Breiman et al., 1984 ] </ref> . We use information gain as an index here, but the other indexes can be used in the same way. Unlike decision tree building where the index is used for selecting an attribute, here we have to select linked pair nodes. <p> LD is a linear discrimination method [ James, 1984 ] , which gave the same answer as the default and did not improve the accuracy. The best result by the conventional method was achieved by the decision-tree learning method CART <ref> [ Breiman et al., 1984 ] </ref> . LD and CART use only sequential information because these methods cannot deal with information having a graph structure.
Reference: [ Cypher, 1991 ] <author> A. Cypher. Eager: </author> <title> Programming Repetitive Tasks by Example. </title> <booktitle> In Proc. of CHI'91, </booktitle> <pages> pages 33-39, </pages> <year> 1991. </year>
Reference-contexts: What is common to many of them is that they observe repetition or regularity in the user's behavior and use them for automation, prediction and customization in one way or another. EAGER <ref> [ Cypher, 1991 ] </ref> is an example of program by demonstration (PBD), which is a HyperText system that keeps watching a user's actions, detects an iteration and offers to run the iterative procedure to completion by generalizing the repetitions and making macros.
Reference: [ Dent et al., 1992 ] <author> L. Dent, J. Boticario, J. McDermott, T. Mitchell, and D. Zabowski. </author> <title> A Personal Learning Apprentice. </title> <booktitle> In Proc. of AAAI'92, </booktitle> <pages> pages 96-101, </pages> <year> 1992. </year>
Reference-contexts: They used memory-based learning (k-nearest neighbor). Situations in the user are described in terms of a set of attributes which are hand-coded. The tasks that they applied are a calendar management agent and an electronic mail clerk. The personal learning apprentice CAP <ref> [ Dent et al., 1992 ] </ref> is similar to the above. It is an interactive assistance that learns continually from the user to predict default values. Their application is a calendar management apprentice. Two competing leaning methods are used: decision tree learning and backpropagation neural net.
Reference: [ Etzioni94 and Weld, 1994 ] <author> O. Etzioni94 and D. </author> <note> Weld. </note>
References-found: 4

