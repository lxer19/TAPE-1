URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/thrun/public_html/papers/thrun.ras.ps.gz
Refering-URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/thrun/public_html/papers/thrun.ras.html
Root-URL: 
Title: An Approach to Learning Mobile Robot Navigation  
Author: Sebastian Thrun 
Keyword: explanation-based learning, mobile robots, machine learning, navigation, neural networks, perception  
Note: To appear in: Robotics And Autonomous Systems special issue on Robot Learning,  
Address: Romerstr. 164, 53117 Bonn, Germany  
Affiliation: Universitat Bonn Institut fur Informatik III  
Email: E-mail: thrun@carbon.informatik.uni-bonn.de, thrun@cs.cmu.edu  
Date: 1995.  
Abstract: This paper describes an approach to learning a simple indoor robot navigation task through trial-and-error. A mobile robot, equipped with visual, ultrasonic and laser sensors, learns to servo to a designated target object. In less than ten minutes of operation time, the robot is able to navigate to a marked target object in an office environment. The central learning mechanism is the explanation-based neural network learning algorithm (EBNN). EBNN initially learns function purely inductively using neural network representations. With increasing experience, EBNN employs domain knowledge to explain and to analyze training data in order to generalize in a more knowledgeable way. Here EBNN is applied in the context of reinforcement learning, which allows the robot to learn control using dynamic programming.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Andrew G. Barto, Steven J. Bradtke, and Satinder P. Singh. </author> <title> Real-time learning and control using asynchronous dynamic programming. </title> <type> Technical Report COINS 91-57, </type> <institution> Department of Computer Science, University of Massachusetts, </institution> <address> MA, </address> <month> August </month> <year> 1991. </year> <title> S. Thrun An Approach to Learning Mobile Robot Navigation 26 </title>
Reference-contexts: Recently, Q-Learning and other related reinforcement learning and dynamic programming algorithms [2, 34] have been applied successfully to robot manipulation [8], robot control [16] and games [35]. In essence, Q-Learning learns control policies for partially controllable Markov chains with delayed pay-off (penalty/reward) <ref> [1, 2] </ref>. It does this by constructing a value function Q (s; a), which maps perceptual sensations, denoted by s, and actions, denoted by a, to task-specific utility values.
Reference: [2] <author> Andrew G. Barto, Steven J. Bradtke, and Satinder P. Singh. </author> <title> Learning to act using real-time dynamic programming. </title> <journal> Artificial Intelligence, </journal> <note> to appear. </note>
Reference-contexts: S. Thrun An Approach to Learning Mobile Robot Navigation 11 3 Reinforcement Learning In order to learn sequences of actions, as required for controlling a robot, we applied EBNN in the context of Q-Learning [41, 42]. Recently, Q-Learning and other related reinforcement learning and dynamic programming algorithms <ref> [2, 34] </ref> have been applied successfully to robot manipulation [8], robot control [16] and games [35]. In essence, Q-Learning learns control policies for partially controllable Markov chains with delayed pay-off (penalty/reward) [1, 2]. <p> Recently, Q-Learning and other related reinforcement learning and dynamic programming algorithms [2, 34] have been applied successfully to robot manipulation [8], robot control [16] and games [35]. In essence, Q-Learning learns control policies for partially controllable Markov chains with delayed pay-off (penalty/reward) <ref> [1, 2] </ref>. It does this by constructing a value function Q (s; a), which maps perceptual sensations, denoted by s, and actions, denoted by a, to task-specific utility values.
Reference: [3] <author> Francesco Bergadano and Aiello Giordana. </author> <title> Guiding Induction with Domain Theories, </title> <address> pages 474-492. </address> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: Therefore, EBL methods have often been applied to speed-up learning [18]. Recent work has produced EBL methods can learn from domain knowledge that is only partially correct <ref> [3, 24, 25, 28] </ref>. 2.3 The Explanation-Based Neural Network Learning Algo rithm Most approaches to EBL require that the domain theory be represented by symbolic rules, and, moreover, be correct and complete.
Reference: [4] <author> Joachim Buhmann, Wolfram Burgard, Armin B. Cremers, Dieter Fox, Thomas Hofmann, Frank Schneider, Jiannis Strikos, and Sebastian Thrun. </author> <title> The mobile robot Rhino. </title> <journal> AI Magazine, </journal> <note> 16(1), to appear. </note>
Reference-contexts: One of the latter experiments is summarized in Fig. 10. In this figure, we visualized several learning episodes seen from a bird eye's view, using a sonar-based technique for building occupancy maps described in <ref> [4, 38] </ref>. In all cases Xavier learned to navigate to a static target location in less than 19 episodes (with action models) and 24 episodes (without action models). Each episode lasted for between two and eleven actions.
Reference: [5] <author> Gerald DeJong and Raymond Mooney. </author> <title> Explanation-based learning: An alternative view. </title> <journal> Machine Learning, </journal> <volume> 1(2) </volume> <pages> 145-176, </pages> <year> 1986. </year>
Reference-contexts: Inductive learning techniques, like decision tree learning [27], spline interpolation [7] or artificial neural network learning [29], generalize sets of training examples via a built-in, domain-independent inductive bias. They typically can learn functions from scratch, based purely on observation. Analytical approaches to learning, like explanation-based learning <ref> [5, 18, 20] </ref>, generalize training examples based on domain-specific knowledge. They employ a built-in theory of the domain of the target function for analyzing and generalizing individual training examples. Both families of approaches are characterized by opposite strengths and weaknesses. <p> The generalization accuracy of the trained neural network depends on the number of training examples, and typically a large set of training examples is required to fit a target function accurately. S. Thrun An Approach to Learning Mobile Robot Navigation 5 2.2 Explanation-Based Learning To illustrate explanation-based learning (EBL) <ref> [5, 20] </ref>, which is the most widely studied analytical approach to machine learning, imagine one were given a theory of the domain.
Reference: [6] <author> Thomas G. Dietterich. </author> <title> Learning at the knowledge level. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 287-316, </pages> <year> 1986. </year>
Reference-contexts: In its most pure form, EBL can acquire only rules that follow from the initial domain theory it does not learn at the knowledge level <ref> [6] </ref>. Therefore, EBL methods have often been applied to speed-up learning [18].
Reference: [7] <author> Jerome H. Friedman. </author> <title> Multivariate adaptive regression splines. </title> <journal> Annals of Statistics, </journal> <volume> 19(1) </volume> <pages> 1-141, </pages> <month> March </month> <year> 1991. </year>
Reference-contexts: Generally speaking, approaches to machine learning can be divided into two major categories: inductive learning and analytical learning. Inductive learning techniques, like decision tree learning [27], spline interpolation <ref> [7] </ref> or artificial neural network learning [29], generalize sets of training examples via a built-in, domain-independent inductive bias. They typically can learn functions from scratch, based purely on observation. Analytical approaches to learning, like explanation-based learning [5, 18, 20], generalize training examples based on domain-specific knowledge.
Reference: [8] <author> Vijaykumar Gullapalli. </author> <title> Reinforcement Learning and its Application to Control. </title> <type> PhD thesis, </type> <institution> Department of Computer and Information Science, University of Mas-sachusetts, </institution> <year> 1992. </year>
Reference-contexts: Recently, Q-Learning and other related reinforcement learning and dynamic programming algorithms [2, 34] have been applied successfully to robot manipulation <ref> [8] </ref>, robot control [16] and games [35]. In essence, Q-Learning learns control policies for partially controllable Markov chains with delayed pay-off (penalty/reward) [1, 2].
Reference: [9] <author> Vijaykumar Gullapalli, Judy A. Franklin, and Hamid Benbrahim. </author> <title> Acquiring robot skills via reinforcement learning. </title> <journal> IEEE Control Systems, </journal> <volume> 272(1708) </volume> <pages> 13-24, </pages> <month> Febru-ary </month> <year> 1994. </year>
Reference-contexts: If a robot is placed in an unknown environment, or faced with a novel task for which no a priori solution is available, a robot that learns shall collect new experiences, acquire new skills, and eventually perform new tasks all by itself. For example, in <ref> [9] </ref> a robot manipulator is described which learns to insert a peg into a hole without prior knowledge regarding the manipulator or the hole. Maes and Brooks [15] have successfully applied learning techniques to coordinating leg motion for an insect-like robot.
Reference: [10] <author> John Hertz, Anders Krogh, and Richard G. Palmer. </author> <title> Introduction to the theory of neural computation. </title> <publisher> Addison-Wesley Pub. Co., </publisher> <address> Redwood City, California, </address> <year> 1991. </year>
Reference-contexts: Before explaining EBNN, let us briefly consider its components: (inductive) neural network learning and (analytical) explanation-based learning. 2.1 Neural Network Backpropagation Artificial neural networks (see <ref> [10, 30, 40] </ref> for an introduction) consist of a set of simple, densely interconnected processing units. These units transform signals in a non-linear S.
Reference: [11] <author> Tommi Jaakkola, Michael I. Jordan, and Satinder P. Singh. </author> <title> On the convergence of stochastic iterative dynamic programming algorithms. </title> <type> Technical Report 9307, </type> <institution> Department of Brain and Cognitive Sciences, Massachusetts Institut of Technology, </institution> <month> July </month> <year> 1993. </year>
Reference-contexts: a 2 ); Q (s 3 ; a 3 ); : : : in the episode. concerning the exploration scheme, the environment, the learning rate and the representation of Q|to converge to the optimal value function ^ Q, and hence to produce optimal policies that maximize the future, discounted pay-off <ref> [11, 41] </ref>. Q-Learning propagates knowledge in the value function from the end of an episode gradually to the beginning. This is because at the end of an episode, Q is learned directly based on the final pay-off. In between, Q is updated based on later Q-values.
Reference: [12] <author> J. Laird, E. Yager, C. Tuck, and M. Hucka. </author> <title> Learning in tele-autonomous systems using Soar. </title> <booktitle> In Proceedings of the 1989 NASA Conference of Space Telerobotics, </booktitle> <year> 1989. </year>
Reference-contexts: Therefore, analytical learning techniques seem to have a clear advantage. Their strong requirement for accurate domain knowledge, however, has found to be a severe obstacle in applying analytical learning to realistic robotics domains <ref> [12, 19] </ref>. In this paper we present the explanation-based neural network (EBNN) learning algorithm [22, 37] which integrates both analytical and inductive learning. It smoothly blends both learning principles depending on the quality of the available domain knowledge.
Reference: [13] <author> Jean-Claude Latombe. </author> <title> Robot Motion Planning. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, MA, </address> <year> 1991. </year>
Reference-contexts: Even if the robot, its environment and its goals can be modeled in sufficient detail, generating control for a general-purpose device has often been found to be of enormous computational complexity (see for example <ref> [13, 31] </ref>). Moreover, the computational complexity often increases drastically with the complexity of the mechanical device. Machine learning aims to overcome these limitations, by enabling a robot to collect its knowledge on-the-fly, through real-world experimentation.
Reference: [14] <author> Long-Ji Lin. </author> <title> Self-supervised Learning by Reinforcement and Artificial Neural Networks. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, School of Computer Science, </institution> <address> Pitts-burgh, PA, </address> <year> 1992. </year> <title> S. Thrun An Approach to Learning Mobile Robot Navigation 27 </title>
Reference-contexts: In all our experiments the action models were trained first, prior to learning Q, and frozen during learning control. When training the Q networks (46 input, eight hidden and one output unit), we explicitly memorized all training data, and used a replay technique similar to "experience replay" described in <ref> [14] </ref>. This procedure memorizes all past experiences explicitly. After each learning episode, it re-estimates the target values of the Q function by recursively replaying past episodes, as if they had just S.
Reference: [15] <author> Pattie Maes and Rodney A. Brooks. </author> <title> Learning to coordinate behaviors. </title> <booktitle> In Proceedings Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pages 796-802, </pages> <address> Cam-bridge, MA, 1990. </address> <publisher> AAAI, The MIT Press. </publisher>
Reference-contexts: For example, in [9] a robot manipulator is described which learns to insert a peg into a hole without prior knowledge regarding the manipulator or the hole. Maes and Brooks <ref> [15] </ref> have successfully applied learning techniques to coordinating leg motion for an insect-like robot. Their approach, too, operates in the absence of a model of the dynamics of the system. Learning techniques have frequently come to bear in situations where the physical world is extremely hard to model by hand.
Reference: [16] <author> Sridhar Mahadevan and Jonathan Connell. </author> <title> Automatic programming of behavior-based robots using reinforcement learning. </title> <month> December </month> <year> 1990. </year>
Reference-contexts: Recently, Q-Learning and other related reinforcement learning and dynamic programming algorithms [2, 34] have been applied successfully to robot manipulation [8], robot control <ref> [16] </ref> and games [35]. In essence, Q-Learning learns control policies for partially controllable Markov chains with delayed pay-off (penalty/reward) [1, 2]. It does this by constructing a value function Q (s; a), which maps perceptual sensations, denoted by s, and actions, denoted by a, to task-specific utility values.
Reference: [17] <author> Ryusuke Masuoka. </author> <title> Noise robustness of EBNN learning. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks, </booktitle> <month> October </month> <year> 1993. </year>
Reference-contexts: Gradient descent in weight space is employed to iteratively minimize E. Notice that in our implementation we used a modified version of the Tangent-Prog algorithm [32] to refine the weights and biases of the target network <ref> [17] </ref>. 2.4 Accommodating Imperfect Domain Theories Thus far, we have not addressed the potential damage arising from poor domain theories. Because the domain theory itself is learned from training data and thus might be erroneous, the analytically extracted slopes will only be approximately correct.
Reference: [18] <author> Steven Minton. </author> <title> Learning Search Control Knowledge: An Explanation-Based Approach. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1988. </year>
Reference-contexts: Inductive learning techniques, like decision tree learning [27], spline interpolation [7] or artificial neural network learning [29], generalize sets of training examples via a built-in, domain-independent inductive bias. They typically can learn functions from scratch, based purely on observation. Analytical approaches to learning, like explanation-based learning <ref> [5, 18, 20] </ref>, generalize training examples based on domain-specific knowledge. They employ a built-in theory of the domain of the target function for analyzing and generalizing individual training examples. Both families of approaches are characterized by opposite strengths and weaknesses. <p> In its most pure form, EBL can acquire only rules that follow from the initial domain theory it does not learn at the knowledge level [6]. Therefore, EBL methods have often been applied to speed-up learning <ref> [18] </ref>. Recent work has produced EBL methods can learn from domain knowledge that is only partially correct [3, 24, 25, 28]. 2.3 The Explanation-Based Neural Network Learning Algo rithm Most approaches to EBL require that the domain theory be represented by symbolic rules, and, moreover, be correct and complete.
Reference: [19] <author> Tom M. Mitchell. </author> <title> Becoming increasingly reactive. </title> <booktitle> In Proceedings of 1990 AAAI Conference, </booktitle> <address> Menlo Park, CA, </address> <month> August </month> <year> 1990. </year> <booktitle> AAAI, </booktitle> <publisher> AAAI Press / The MIT Press. </publisher>
Reference-contexts: Learning techniques have also successfully been applied to speedup robot control, by observing the statistical regularities of "typical" situations (like typical robot and environment configurations), and compiling more compact controllers S. Thrun An Approach to Learning Mobile Robot Navigation 3 for the frequently encountered. For example, Mitchell <ref> [19] </ref> describes an approach in which a mobile robot becomes increasingly reactive, by using observations to compile fast rules out of a database of domain knowledge. Generally speaking, approaches to machine learning can be divided into two major categories: inductive learning and analytical learning. <p> Therefore, analytical learning techniques seem to have a clear advantage. Their strong requirement for accurate domain knowledge, however, has found to be a severe obstacle in applying analytical learning to realistic robotics domains <ref> [12, 19] </ref>. In this paper we present the explanation-based neural network (EBNN) learning algorithm [22, 37] which integrates both analytical and inductive learning. It smoothly blends both learning principles depending on the quality of the available domain knowledge.
Reference: [20] <author> Tom M. Mitchell, Rich Keller, and Smadar Kedar-Cabelli. </author> <title> Explanation-based generalization: A unifying view. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 47-80, </pages> <year> 1986. </year>
Reference-contexts: Inductive learning techniques, like decision tree learning [27], spline interpolation [7] or artificial neural network learning [29], generalize sets of training examples via a built-in, domain-independent inductive bias. They typically can learn functions from scratch, based purely on observation. Analytical approaches to learning, like explanation-based learning <ref> [5, 18, 20] </ref>, generalize training examples based on domain-specific knowledge. They employ a built-in theory of the domain of the target function for analyzing and generalizing individual training examples. Both families of approaches are characterized by opposite strengths and weaknesses. <p> The generalization accuracy of the trained neural network depends on the number of training examples, and typically a large set of training examples is required to fit a target function accurately. S. Thrun An Approach to Learning Mobile Robot Navigation 5 2.2 Explanation-Based Learning To illustrate explanation-based learning (EBL) <ref> [5, 20] </ref>, which is the most widely studied analytical approach to machine learning, imagine one were given a theory of the domain.
Reference: [21] <author> Tom M. Mitchell, Joseph O'Sullivan, and Sebastian Thrun. </author> <title> Explanation-based learning for mobile robot perception. In Workshop on Robot Learning, </title> <booktitle> Eleventh Conference on Machine Learning, </booktitle> <year> 1994. </year>
Reference-contexts: EBNN, thus, allows to collect domain knowledge on-the-fly, and to employ it for biasing generalization in future control learning tasks. The experimental results reported here indicate that an appropriate domain theory can reduce training time significantly|an effect which has also been demonstrated in different studies in computer perception <ref> [21, 23] </ref> and vision [39] domains, and chess [36]. Acknowledgment The author thanks Tom Mitchell, Ryusuke Masuoka, Joseph O'Sullivan, Reid Simmons, and the CMU mobile robot group for their invaluable advice and support. This work was supported in part by grant IRI-9313367 from the US National Science Foundation to Tom Mitchell.
Reference: [22] <author> Tom M. Mitchell and Sebastian Thrun. </author> <title> Explanation-based neural network learning for robot control. </title> <editor> In S. J. Hanson, J. Cowan, and C. L. Giles, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 287-294, </pages> <address> San Mateo, CA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Therefore, analytical learning techniques seem to have a clear advantage. Their strong requirement for accurate domain knowledge, however, has found to be a severe obstacle in applying analytical learning to realistic robotics domains [12, 19]. In this paper we present the explanation-based neural network (EBNN) learning algorithm <ref> [22, 37] </ref> which integrates both analytical and inductive learning. It smoothly blends both learning principles depending on the quality of the available domain knowledge. <p> This weighting scheme ensures that accurate slopes will have a large weight in training, whereas inaccurate slopes will be gradually ignored. We consistently found in our experiments that the arbitration scheme, called LOB* <ref> [22] </ref>, is crucial for successful learning in cases where the domain theory is poor and misleading, since it diminishes the damaging effect arising from misleadingly wrong domain theories [22, 23]. This completes the description of the EBNN learning mechanism. <p> We consistently found in our experiments that the arbitration scheme, called LOB* [22], is crucial for successful learning in cases where the domain theory is poor and misleading, since it diminishes the damaging effect arising from misleadingly wrong domain theories <ref> [22, 23] </ref>. This completes the description of the EBNN learning mechanism. To summarize, EBNN refines the target network using a combined inductive-analytical mechanism. Inductive training information is obtained through observation, and analytical training information is obtained by explaining and analyzing these observations in terms of the learner's prior knowledge. <p> Before learning a value function, we trained 5 neural network action models, one for each individual action. Subsequently, 5 Q-functions were trained to evaluate the values of each action. In this implementation we used a real-valued approximation scheme using nearest-neighbor generalization for the Q-functions <ref> [22] </ref>. This scheme was empirically found to outperform Backpropagation. Parameters were otherwise identical to the navigation task described above. Experiment 1: The role of analysis. In a first experiment, we were interested in the overall merit of the analytical learning component of EBNN.
Reference: [23] <author> Joseph O'Sullivan, Tom M. Mitchell, and Sebastian Thrun. </author> <title> Explanation-based neural network learning from mobile robot perception. </title> <editor> In Katsushi Ikeuchi and Manuela Veloso, editors, </editor> <title> Symbolic Visual Learning. </title> <institution> Oxford University Press, </institution> <note> to appear. </note>
Reference-contexts: We consistently found in our experiments that the arbitration scheme, called LOB* [22], is crucial for successful learning in cases where the domain theory is poor and misleading, since it diminishes the damaging effect arising from misleadingly wrong domain theories <ref> [22, 23] </ref>. This completes the description of the EBNN learning mechanism. To summarize, EBNN refines the target network using a combined inductive-analytical mechanism. Inductive training information is obtained through observation, and analytical training information is obtained by explaining and analyzing these observations in terms of the learner's prior knowledge. <p> This finding illustrates that EBNN can operate S. Thrun An Approach to Learning Mobile Robot Navigation 22 robustly over a broad range of domain theories, from strong to random. The reader may notice that this effect was also observed in other experiments involving mobile robot perception, described in <ref> [23] </ref>. An interesting scaling benchmark of EBNN is the reduction in the number of training examples required to achieve a certain level of performance. Fig. 16 displays this average speed-up factor for EBNN relative to plain, inductive Backpropagation. <p> EBNN recovers from poor domain theories because of its inductive component, which enables it to overturn misleading bias extracted from inaccurate prior knowledge. These findings match our experimental results reported in <ref> [23] </ref>. 5 Conclusion In this paper we have reported results obtained for applying the explanation-based neural network learning algorithm to problems of indoor robot navigation. Despite the high-dimensional sensor spaces faced by the Xavier robot, it consistently managed S. <p> EBNN, thus, allows to collect domain knowledge on-the-fly, and to employ it for biasing generalization in future control learning tasks. The experimental results reported here indicate that an appropriate domain theory can reduce training time significantly|an effect which has also been demonstrated in different studies in computer perception <ref> [21, 23] </ref> and vision [39] domains, and chess [36]. Acknowledgment The author thanks Tom Mitchell, Ryusuke Masuoka, Joseph O'Sullivan, Reid Simmons, and the CMU mobile robot group for their invaluable advice and support. This work was supported in part by grant IRI-9313367 from the US National Science Foundation to Tom Mitchell.
Reference: [24] <author> Dirk Ourston and Raymond J. Mooney. </author> <title> Theory refinement with noisy data. </title> <type> Technical Report AI 91-153, </type> <institution> Artificial Intelligence Lab, University of Texas at Austin, </institution> <month> March </month> <year> 1991. </year>
Reference-contexts: Therefore, EBL methods have often been applied to speed-up learning [18]. Recent work has produced EBL methods can learn from domain knowledge that is only partially correct <ref> [3, 24, 25, 28] </ref>. 2.3 The Explanation-Based Neural Network Learning Algo rithm Most approaches to EBL require that the domain theory be represented by symbolic rules, and, moreover, be correct and complete.
Reference: [25] <author> Michael J. Pazzani, Clifford A. Brunk, and Glenn Silverstein. </author> <title> A knowledge-intensive approach to learning relational concepts. </title> <booktitle> In Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <pages> pages 432-436, </pages> <address> Evanston, IL, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Therefore, EBL methods have often been applied to speed-up learning [18]. Recent work has produced EBL methods can learn from domain knowledge that is only partially correct <ref> [3, 24, 25, 28] </ref>. 2.3 The Explanation-Based Neural Network Learning Algo rithm Most approaches to EBL require that the domain theory be represented by symbolic rules, and, moreover, be correct and complete.
Reference: [26] <author> Dean A. Pomerleau. ALVINN: </author> <title> an autonomous land vehicle in a neural network. </title> <type> Technical Report CMU-CS-89-107, </type> <institution> Computer Science Dept. Carnegie Mellon University, </institution> <address> Pittsburgh PA, </address> <year> 1989. </year> <title> S. Thrun An Approach to Learning Mobile Robot Navigation 28 </title>
Reference-contexts: Learning techniques have frequently come to bear in situations where the physical world is extremely hard to model by hand. For example, Pomerleau describes a computer system that learns to steer a vehicle driving at 55mph on public highways, based on sensor data from a video camera <ref> [26] </ref>. Learning techniques have also successfully been applied to speedup robot control, by observing the statistical regularities of "typical" situations (like typical robot and environment configurations), and compiling more compact controllers S. Thrun An Approach to Learning Mobile Robot Navigation 3 for the frequently encountered.
Reference: [27] <author> J. Ross Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: Generally speaking, approaches to machine learning can be divided into two major categories: inductive learning and analytical learning. Inductive learning techniques, like decision tree learning <ref> [27] </ref>, spline interpolation [7] or artificial neural network learning [29], generalize sets of training examples via a built-in, domain-independent inductive bias. They typically can learn functions from scratch, based purely on observation. Analytical approaches to learning, like explanation-based learning [5, 18, 20], generalize training examples based on domain-specific knowledge.
Reference: [28] <author> Paul S. Rosenbloom and Jans Aasman. </author> <title> Knowledge level and inductive uses of chunking (EBL). </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pages 821-827, </pages> <address> Boston, 1990. </address> <publisher> AAAI, MIT Press. </publisher>
Reference-contexts: Therefore, EBL methods have often been applied to speed-up learning [18]. Recent work has produced EBL methods can learn from domain knowledge that is only partially correct <ref> [3, 24, 25, 28] </ref>. 2.3 The Explanation-Based Neural Network Learning Algo rithm Most approaches to EBL require that the domain theory be represented by symbolic rules, and, moreover, be correct and complete.
Reference: [29] <author> David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In D. E. Rumelhart and J. L. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing. </booktitle> <volume> Vol. I + II. </volume> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: Generally speaking, approaches to machine learning can be divided into two major categories: inductive learning and analytical learning. Inductive learning techniques, like decision tree learning [27], spline interpolation [7] or artificial neural network learning <ref> [29] </ref>, generalize sets of training examples via a built-in, domain-independent inductive bias. They typically can learn functions from scratch, based purely on observation. Analytical approaches to learning, like explanation-based learning [5, 18, 20], generalize training examples based on domain-specific knowledge. <p> Neural networks are nonparametric estimators which can fit smooth functions based on input-output examples. The internal parameters of a neural network, which are adapted in the process of function fitting (learning), are called weights and biases. The Backpropagation algorithm <ref> [29] </ref>, which is the currently most widely used supervised neural network learning algorithm, learns purely inductively, by observing statistical regularities in the training patterns. Consider the example depicted in Fig. 1. Suppose we are facing the problem of learning to classify cups.
Reference: [30] <author> David E. Rumelhart, Bernard Widrow, and Michael A. Lehr. </author> <title> The basic ideas in neural networks. </title> <journal> Communications of the ACM, </journal> <volume> 37(3) </volume> <pages> 87-92, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Before explaining EBNN, let us briefly consider its components: (inductive) neural network learning and (analytical) explanation-based learning. 2.1 Neural Network Backpropagation Artificial neural networks (see <ref> [10, 30, 40] </ref> for an introduction) consist of a set of simple, densely interconnected processing units. These units transform signals in a non-linear S.
Reference: [31] <author> Jacob T. Schwartz, Micha Scharir, and John Hopcroft. </author> <title> Planning, Geometry and Complexity of Robot Motion. </title> <publisher> Ablex Publishing Corporation, </publisher> <address> Norwood, NJ, </address> <year> 1987. </year>
Reference-contexts: Even if the robot, its environment and its goals can be modeled in sufficient detail, generating control for a general-purpose device has often been found to be of enormous computational complexity (see for example <ref> [13, 31] </ref>). Moreover, the computational complexity often increases drastically with the complexity of the mechanical device. Machine learning aims to overcome these limitations, by enabling a robot to collect its knowledge on-the-fly, through real-world experimentation.
Reference: [32] <author> Patrice Simard, Bernard Victorri, Yann LeCun, and John Denker. </author> <title> Tangent prop a formalism for specifying selected invariances in an adaptive network. </title> <editor> In J. E. Moody, S. J. Hanson, and R. P. Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 895-903, </pages> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: Thus, weight updates in EBNN seek to fit both the inductively derived target values, as well as the analytically derived target slopes. Gradient descent in weight space is employed to iteratively minimize E. Notice that in our implementation we used a modified version of the Tangent-Prog algorithm <ref> [32] </ref> to refine the weights and biases of the target network [17]. 2.4 Accommodating Imperfect Domain Theories Thus far, we have not addressed the potential damage arising from poor domain theories.
Reference: [33] <author> Richard S. Sutton. </author> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <year> 1988. </year>
Reference-contexts: In between, Q is updated based on later Q-values. This process of backing up values can be slow, since they are propagated in the reverse order of the robot's operation. For this and other reasons, Q-Learning has often been combined with Sutton's temporal difference learning <ref> [33] </ref>, which basically mixes multiple value estimates.
Reference: [34] <author> Richard S. Sutton. </author> <title> Integrated modeling and control based on reinforcement learning and dynamic programming. </title> <editor> In R. P. Lippmann, J. E. Moody, and D. S. Touretzky, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 471-478, </pages> <address> San Mateo, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: S. Thrun An Approach to Learning Mobile Robot Navigation 11 3 Reinforcement Learning In order to learn sequences of actions, as required for controlling a robot, we applied EBNN in the context of Q-Learning [41, 42]. Recently, Q-Learning and other related reinforcement learning and dynamic programming algorithms <ref> [2, 34] </ref> have been applied successfully to robot manipulation [8], robot control [16] and games [35]. In essence, Q-Learning learns control policies for partially controllable Markov chains with delayed pay-off (penalty/reward) [1, 2].
Reference: [35] <author> Gerald J. Tesauro. </author> <title> Practical issues in temporal difference learning. </title> <editor> In J. E. Moody, S. J. Hanson, and R. P. Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 259-266, </pages> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Recently, Q-Learning and other related reinforcement learning and dynamic programming algorithms [2, 34] have been applied successfully to robot manipulation [8], robot control [16] and games <ref> [35] </ref>. In essence, Q-Learning learns control policies for partially controllable Markov chains with delayed pay-off (penalty/reward) [1, 2]. It does this by constructing a value function Q (s; a), which maps perceptual sensations, denoted by s, and actions, denoted by a, to task-specific utility values.
Reference: [36] <author> Sebastian Thrun. </author> <title> Learning to play the game of chess. </title> <editor> In G. Tesauro, D. Touretzky, and T. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <address> San Mateo, CA, </address> <year> 1995. </year> <note> Morgan Kaufmann. (to appear). </note>
Reference-contexts: The experimental results reported here indicate that an appropriate domain theory can reduce training time significantly|an effect which has also been demonstrated in different studies in computer perception [21, 23] and vision [39] domains, and chess <ref> [36] </ref>. Acknowledgment The author thanks Tom Mitchell, Ryusuke Masuoka, Joseph O'Sullivan, Reid Simmons, and the CMU mobile robot group for their invaluable advice and support. This work was supported in part by grant IRI-9313367 from the US National Science Foundation to Tom Mitchell.
Reference: [37] <author> Sebastian Thrun and Tom M. Mitchell. </author> <title> Integrating inductive neural network learning and explanation-based learning. </title> <booktitle> In Proceedings of IJCAI-93, </booktitle> <address> Chamberry, France, </address> <month> July </month> <year> 1993. </year> <title> IJCAI, </title> <publisher> Inc. </publisher>
Reference-contexts: Therefore, analytical learning techniques seem to have a clear advantage. Their strong requirement for accurate domain knowledge, however, has found to be a severe obstacle in applying analytical learning to realistic robotics domains [12, 19]. In this paper we present the explanation-based neural network (EBNN) learning algorithm <ref> [22, 37] </ref> which integrates both analytical and inductive learning. It smoothly blends both learning principles depending on the quality of the available domain knowledge.
Reference: [38] <author> Sebastian Thrun and Tom M. Mitchell. </author> <title> Lifelong robot learning. </title> <booktitle> Robotics and Autonomous Systems, </booktitle> <year> 1993. </year> <note> (to appear). Also appeared as Technical Report IAI-TR-93-7, </note> <institution> University of Bonn, Dept. </institution> <note> of Computer Science III. </note> <author> S. </author> <title> Thrun An Approach to Learning Mobile Robot Navigation 29 </title>
Reference-contexts: One of the latter experiments is summarized in Fig. 10. In this figure, we visualized several learning episodes seen from a bird eye's view, using a sonar-based technique for building occupancy maps described in <ref> [4, 38] </ref>. In all cases Xavier learned to navigate to a static target location in less than 19 episodes (with action models) and 24 episodes (without action models). Each episode lasted for between two and eleven actions.
Reference: [39] <author> Sebastian Thrun and Tom M. Mitchell. </author> <title> Learning one more thing. </title> <type> Technical Report CMU-CS-94-184, </type> <institution> Carnegie Mellon University, </institution> <address> Pittsburgh, PA 15213, </address> <month> September </month> <year> 1994. </year>
Reference-contexts: The experimental results reported here indicate that an appropriate domain theory can reduce training time significantly|an effect which has also been demonstrated in different studies in computer perception [21, 23] and vision <ref> [39] </ref> domains, and chess [36]. Acknowledgment The author thanks Tom Mitchell, Ryusuke Masuoka, Joseph O'Sullivan, Reid Simmons, and the CMU mobile robot group for their invaluable advice and support. This work was supported in part by grant IRI-9313367 from the US National Science Foundation to Tom Mitchell.
Reference: [40] <author> Philip D. Wasserman. </author> <title> Neural computing: theory and practice. </title> <publisher> Von Nostrand Reinhold, </publisher> <address> New York, </address> <year> 1989. </year>
Reference-contexts: Before explaining EBNN, let us briefly consider its components: (inductive) neural network learning and (analytical) explanation-based learning. 2.1 Neural Network Backpropagation Artificial neural networks (see <ref> [10, 30, 40] </ref> for an introduction) consist of a set of simple, densely interconnected processing units. These units transform signals in a non-linear S.
Reference: [41] <author> Christopher J. C. H. Watkins. </author> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> King's College, </institution> <address> Cambridge, England, </address> <year> 1989. </year>
Reference-contexts: S. Thrun An Approach to Learning Mobile Robot Navigation 11 3 Reinforcement Learning In order to learn sequences of actions, as required for controlling a robot, we applied EBNN in the context of Q-Learning <ref> [41, 42] </ref>. Recently, Q-Learning and other related reinforcement learning and dynamic programming algorithms [2, 34] have been applied successfully to robot manipulation [8], robot control [16] and games [35]. In essence, Q-Learning learns control policies for partially controllable Markov chains with delayed pay-off (penalty/reward) [1, 2]. <p> a 2 ); Q (s 3 ; a 3 ); : : : in the episode. concerning the exploration scheme, the environment, the learning rate and the representation of Q|to converge to the optimal value function ^ Q, and hence to produce optimal policies that maximize the future, discounted pay-off <ref> [11, 41] </ref>. Q-Learning propagates knowledge in the value function from the end of an episode gradually to the beginning. This is because at the end of an episode, Q is learned directly based on the final pay-off. In between, Q is updated based on later Q-values.
Reference: [42] <author> Christopher J. C. H. Watkins and Peter Dayan. </author> <title> Q-learning. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 279-292, </pages> <year> 1992. </year>
Reference-contexts: S. Thrun An Approach to Learning Mobile Robot Navigation 11 3 Reinforcement Learning In order to learn sequences of actions, as required for controlling a robot, we applied EBNN in the context of Q-Learning <ref> [41, 42] </ref>. Recently, Q-Learning and other related reinforcement learning and dynamic programming algorithms [2, 34] have been applied successfully to robot manipulation [8], robot control [16] and games [35]. In essence, Q-Learning learns control policies for partially controllable Markov chains with delayed pay-off (penalty/reward) [1, 2].
References-found: 42

