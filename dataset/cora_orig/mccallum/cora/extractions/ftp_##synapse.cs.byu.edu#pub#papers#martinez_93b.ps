URL: ftp://synapse.cs.byu.edu/pub/papers/martinez_93b.ps
Refering-URL: ftp://synapse.cs.byu.edu/pub/papers/details.html
Root-URL: 
Title: Towards a General Distributed Platform for Learning and Generalization and Word Perfect Corp. 1 Introduction
Author: Tony R. Martinez Brent W. Hughes 
Date: [6,7,8].  
Address: Provo, Utah 84602  
Affiliation: Computer Science Department, Brigham Young University,  
Note: In ANNES'93 Conference on Artificial Neural Networks and Expert Systems, pp. 216-219, 1993.  This research supported in part by grants from Novell, Inc.  ASOCS  has been developed [1].  have been proposed to date  2 Generalization  These include  
Abstract: Different learning models employ different styles of generalization on novel inputs. This paper proposes the need for multiple styles of generalization to support a broad application base. The Priority ASOCS model (Priority Adaptive Self-Organizing Concurrent System) is overviewed and presented as a potential platform which can support multiple generalization styles. PASOCS is an adaptive network composed of many simple computing elements operating asynchronously and in parallel. The PASOCS can operate in either a data processing mode or a learning mode. During data processing mode, the system acts as a parallel hardware circuit. During learning mode, the PASOCS incorporates rules, with attached priorities, which represent the application being learned. Learning is accomplished in a distributed fashion in time logarithmic in the number of rules. The new model has significant learning time and space complexity improvements over previous models. Generalization in a learning system is at best always a guess. The proper style of generalization is application dependent. Thus, one style of generalization may not be sufficient to allow a learning system to support a broad spectrum of applications [14]. Current connectionist models use one specific style of generalization which is implicit in the learning algorithm. We suggest that the type of generalization used be a self-organizing parameter of the learning system which can be discovered as learning takes place. This requires a) a model which allows flexible generalization styles, and b) mechanisms to guide the system into the best style of generalization for the problem being learned. This paper overviews a learning model which seeks to efficiently support requirement a) above. The model is called Priority ASOCS (PASOCS) [9], which is a member of a class of models called ASOCS (Adaptive Self-Organizing Concurrent Systems) [5]. Section 2 of this paper gives an example of how different generalization techniques can approach a problem. Section 3 presents an overview of PASOCS. Section 4 illustrates how flexible generalization can be supported. Section 5 concludes the paper. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Chang, J. and J. J. Vidal, </author> <title> Inferencing in Hardware, </title> <booktitle> Proceedings of the MCC-University Research Symposium, </booktitle> <address> Austin, TX, </address> <month> (July </month> <year> 1987). </year>
Reference-contexts: Current ASOCS models are based on digital nodes. ASOCS also supports use of symbolic and heuristic learning mechanisms, thus combining the parallelism and distributed nature of connectionist computing with the potential power of AI symbolic learning. A proof of concept ASOCS chip has been developed <ref> [1] </ref>. ASOCS models have two significant advantages over other learning models: a) they are guaranteed to learn any arbitrary set of legal rules; b) learning time is both fast and bounded (linear with the depth of the network, and logarithmic with the number of nodes).
Reference: 2. <author> Hammond, K., CHEF, </author> <title> In Inside Case-Based Reasoning, </title> <editor> Eds C.K. Riesbeck & R.C. Shank, </editor> <address> Hillsdale, NJ, </address> <publisher> Erlbaum, </publisher> <year> 1989. </year>
Reference-contexts: Hamming, or similarity based generalization schemes, set the output of a novel instance according to how closelyit matches a stored prototype or stable state in a system. This type of generalization is employed in such approaches as Hopfield networks [4], Boltzmann machines [3], competitive learning [12], and case-based reasoning <ref> [2] </ref>. Other approaches seek to discover general critical features which can direct generalization of novel inputs. These include Backpropagation [13], ID3 [11], the A q algorithms [10], and ASOCS.
Reference: 3. <author> Hinton, G. E., Sejnowski, T.J., and Ackley, D.H., </author> <title> Boltzmann Machines: Constraint Satisfaction Networks that Learn, </title> <type> Tech Rep. </type> <institution> No. CMU-CS-84 119, Carnegie-Mellon Univ. </institution>
Reference-contexts: Hamming, or similarity based generalization schemes, set the output of a novel instance according to how closelyit matches a stored prototype or stable state in a system. This type of generalization is employed in such approaches as Hopfield networks [4], Boltzmann machines <ref> [3] </ref>, competitive learning [12], and case-based reasoning [2]. Other approaches seek to discover general critical features which can direct generalization of novel inputs. These include Backpropagation [13], ID3 [11], the A q algorithms [10], and ASOCS.
Reference: 4. <author> Hopfield, J. J., </author> <title> Neural Networks and Physical Systems with Emergent Collective Computational Abilities, </title> <journal> Proc. Natl. Acad. Sci. USA, </journal> <volume> vol 79, </volume> <pages> 2554-2558, </pages> <month> April </month> <year> 1982. </year>
Reference-contexts: Hamming, or similarity based generalization schemes, set the output of a novel instance according to how closelyit matches a stored prototype or stable state in a system. This type of generalization is employed in such approaches as Hopfield networks <ref> [4] </ref>, Boltzmann machines [3], competitive learning [12], and case-based reasoning [2]. Other approaches seek to discover general critical features which can direct generalization of novel inputs. These include Backpropagation [13], ID3 [11], the A q algorithms [10], and ASOCS.
Reference: 5. <author> Martinez, T. R., </author> <title> Adaptive Self-Organizing Concurrent Systems, Progress in Neural N e t w o r k s , vol. 1, </title> <journal> Ch. </journal> <volume> 5, </volume> <pages> 105-126, </pages> <publisher> Ablex Publishing, </publisher> <year> 1990. </year>
Reference-contexts: This paper overviews a learning model which seeks to efficiently support requirement a) above. The model is called Priority ASOCS (PASOCS) [9], which is a member of a class of models called ASOCS (Adaptive Self-Organizing Concurrent Systems) <ref> [5] </ref>. ASOCS models support efficient computation through self-organized learning and parallel execution. Learning is done through the incremental presentation of rules and/or examples. ASOCS models learn by modifying their topology. Data types include Boolean and multi-state variables; recent models support analog variables.
Reference: 6. <author> Martinez, T. R. and Vidal, J.J., </author> <title> Adaptive Parallel Logic Networks, </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> vol. 5, no. 1, </volume> <pages> 26-58, </pages> <year> 1988. </year>
Reference-contexts: Flexible Generalization The PASOCS model presented above generalizes in the sense of simplifying rules. However, no mechanism is defined for what should be output when the input environment matches no learned rule. Other ASOCS models <ref> [6] </ref> use a distributed mechanism to accomplish automatic generalization for novel inputs. In PASOCS, the style of generalization can be both arbitrary and potentially dynamic. Current research is testing different styles of generalization in order to understand: Which generalization schemes support which styles of application.
Reference: 7. <author> Martinez, T. R. and Campbell, D. M., </author> <title> A Self Adjusting Dynamic Logic Module, </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> vol. 11, no. 4, </volume> <pages> 303-313, </pages> <year> 1991. </year>
Reference: 8. <author> Martinez, T. R. and Campbell, D. M., </author> <title> A Self Organizing Binary Decision Tree for Incrementally Defined Rule Based Systems, </title> <journal> IEEE Transactions on System, Man, and Cybernetics, </journal> <volume> vol. 21, no. 5, </volume> <pages> 1231-1238, </pages> <year> 1991. </year>
Reference: 9. <author> Martinez, T. R., Hughes, B., and Campbell, D. M., </author> <title> Priority ASOCS, </title> <note> To appear in Journal of Artificial Neural Networks, </note> <year> 1993. </year>
Reference-contexts: This requires a) a model which allows flexible generalization styles, and b) mechanisms to guide the system into the best style of generalization for the problem being learned. This paper overviews a learning model which seeks to efficiently support requirement a) above. The model is called Priority ASOCS (PASOCS) <ref> [9] </ref>, which is a member of a class of models called ASOCS (Adaptive Self-Organizing Concurrent Systems) [5]. ASOCS models support efficient computation through self-organized learning and parallel execution. Learning is done through the incremental presentation of rules and/or examples. ASOCS models learn by modifying their topology. <p> Which method (if either of the two) is correct is application dependent. 3 . Overview of Priority ASOCS To give the reader the overall flavor of PASOCS processing, we informally describe a Prioritized ASOCS architecture. For a detailed presentation of the PASOCS model see <ref> [9] </ref>. For simplicity, PASOCS is here described with only Boolean variables. PASOCS naturally supports nominal variables and supports continuous variables through use of a preprocessing module. A Priority ASOCS is an ASOCS model which has the following attributes.
Reference: 10. <author> Michalski, </author> <title> R.S., A Theory and Methodology of Inductive Learning, </title> <journal> Artificial Intelligence, </journal> <volume> 20, </volume> <pages> pp. 111-116, </pages> <year> 1983. </year>
Reference-contexts: This type of generalization is employed in such approaches as Hopfield networks [4], Boltzmann machines [3], competitive learning [12], and case-based reasoning [2]. Other approaches seek to discover general critical features which can direct generalization of novel inputs. These include Backpropagation [13], ID3 [11], the A q algorithms <ref> [10] </ref>, and ASOCS. One of the apparent powers of natural nervous systems is the ability to discriminate the current critical inputs from a massive barrage of total inputs.
Reference: 11. <author> Quinlan. J. R., </author> <title> Induction of Decision Trees, </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: This type of generalization is employed in such approaches as Hopfield networks [4], Boltzmann machines [3], competitive learning [12], and case-based reasoning [2]. Other approaches seek to discover general critical features which can direct generalization of novel inputs. These include Backpropagation [13], ID3 <ref> [11] </ref>, the A q algorithms [10], and ASOCS. One of the apparent powers of natural nervous systems is the ability to discriminate the current critical inputs from a massive barrage of total inputs.
Reference: 12. <author> Rumelhart, D. and McClelland J., </author> <title> P a r a l l e l Distributed Processing: Explorations in the Microstructure of Cognition. </title> <publisher> 1 , MIT Press, </publisher> <address> Cambridge, MA 1986, Ch. </address> <month> 5. </month>
Reference-contexts: Hamming, or similarity based generalization schemes, set the output of a novel instance according to how closelyit matches a stored prototype or stable state in a system. This type of generalization is employed in such approaches as Hopfield networks [4], Boltzmann machines [3], competitive learning <ref> [12] </ref>, and case-based reasoning [2]. Other approaches seek to discover general critical features which can direct generalization of novel inputs. These include Backpropagation [13], ID3 [11], the A q algorithms [10], and ASOCS.
Reference: 13. <author> Rumelhart, D. and McClelland J., </author> <title> P a r a l l e l Distributed Processing: Explorations in the Microstructure of Cognition. </title> <publisher> 1 , MIT Press, </publisher> <address> Cambridge, MA 1986, Ch. </address> <month> 8. </month>
Reference-contexts: This type of generalization is employed in such approaches as Hopfield networks [4], Boltzmann machines [3], competitive learning [12], and case-based reasoning [2]. Other approaches seek to discover general critical features which can direct generalization of novel inputs. These include Backpropagation <ref> [13] </ref>, ID3 [11], the A q algorithms [10], and ASOCS. One of the apparent powers of natural nervous systems is the ability to discriminate the current critical inputs from a massive barrage of total inputs.
Reference: 14. <author> Wilson, D. R, and Martinez, T. R., </author> <title> The Importance of Using Multiple Styles of Generalization on Training Set Data, </title> <note> submitted. </note>
Reference-contexts: 1 . Introduction Generalization in a learning system is at best always a guess. The proper style of generalization is application dependent. Thus, one style of generalization may not be sufficient to allow a learning system to support a broad spectrum of applications <ref> [14] </ref>. Current connectionist models use one specific style of generalization which is implicit in the learning algorithm. We suggest that the type of generalization used be a self-organizing parameter of the learning system which can be discovered as learning takes place.
References-found: 14

