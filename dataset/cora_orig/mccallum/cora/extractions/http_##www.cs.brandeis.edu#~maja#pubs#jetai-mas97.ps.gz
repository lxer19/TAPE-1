URL: http://www.cs.brandeis.edu/~maja/pubs/jetai-mas97.ps.gz
Refering-URL: http://www.cs.brandeis.edu/~maja/publications.html
Root-URL: http://www.cs.brandeis.edu
Email: mataric@cs.usc.edu  
Title: Using Communication to Reduce Locality in Distributed Multi-Agent Learning  
Author: Maja J Mataric 
Address: 941 West 37th Place, SAL 228 MC 0781 Los Angeles, CA 90089-0781  
Affiliation: Computer Science Department University of Southern California  
Abstract: This paper attempts to bridge the fields of machine learning, robotics, and distributed AI. It discusses the use of communication in reducing the undesirable effects of locality in fully distributed multi-agent systems with multiple agents/robots learning in parallel while interacting with each other. Two key problems, hidden state and credit assignment, are addressed by applying local undirected broadcast communication in a dual role: as sensing and as reinforcement. The methodology is demonstrated on two multi-robot learning experiments. The first describes learning a tightly-coupled coordination task with two robots, the second a loosely-coupled task with four robots learning social rules. Communication is used to 1) share sensory data to overcome hidden state and 2) share reinforcement to overcome the credit assignment problem between the agents and bridge the gap between local/individual and global/group payoff. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Altenburg, K. & Pavicic, M. </author> <year> (1993), </year> <title> Initial Results in the Use of Inter-Robot Communication for a Multiple, Mobile Robotic System, </title> <booktitle> in `Proceedings, 17 IJCAI-93 Workshop on Dynamically Interacting Robots', </booktitle> <address> Chambery, France, </address> <pages> pp. 96-100. </pages>
Reference: <author> Asada, M., Uchibe, E. & Hosoda, K. </author> <year> (1995), </year> <title> Agents That Learn from Other Competitive Agents, </title> <booktitle> in `Proceedings, Machine Learning Conference Workshop on Agents That Learn From Other Agents'. </booktitle>
Reference: <author> Axelrod, R. </author> <year> (1984), </year> <title> The Evolution of Cooperation, </title> <publisher> Basic Books, </publisher> <address> NY. </address>
Reference-contexts: Yet, from the multi-agent perspective, the ability to sense and correctly distinguish members of one's group from others, from obstacles, and from various features in the the environment is crucial for most tasks. While the inability to make such distinctions does not preclude symbiotic relationships <ref> (Axelrod 1984) </ref>, the lack of sophisticated perceptual discrimination is a critical limitation in multi-robot work. Non-visual sensors such as infra-red, contact sensors, and sonars are all of limited use in the social recognition task. <p> Thus, it is difficult for individual agents to learn such social rules without having them centrally imposed or pre-programmed, as advocated in some game-theoretic work <ref> (Axelrod 1984) </ref>. Our experiments demonstrated that through the use of simple communication between spatially local agents, the described social rules can be learned. The communication channel was used to provide payoff, i.e., to generate reinforcement, for observing the behavior of the near-by agent and mimicking it.
Reference: <author> Donald, B. R., Jennings, J. & Rus, D. </author> <year> (1993), </year> <title> Information Invariants for Cooperating Autonomous Mobile Robots, </title> <booktitle> in `Proc. International Symposium on Robotics Research', </booktitle> <address> Hidden Valley, PA. </address>
Reference-contexts: Fortunately, most task do not require this level of communication as they are 8 decomposable into concurrent subtasks. This two-robot experiment demon-strates two tightly-coupled sub-tasks that require communication to overcome sensory deficiencies. Alternative solutions could employ robots with more powerful sensors <ref> (Donald, Jennings & Rus 1993) </ref>, or a larger group of robots with looser communication and information-sharing requirements, as demonstrated in our second robot experiment, in the next section. Another challenge of this task is finding the joint robot actions that result in the appropriate coordinated state-action mapping for the task.
Reference: <author> Dudek, G., Jenkin, M., Milios, E. & Wilkes, D. </author> <year> (1993), </year> <title> On the utility of multi-agent autonomous robot systems, </title> <booktitle> in `Proceedings, IJCAI-93 Workshop on Dynamically Interacting Robots', </booktitle> <address> Chambery, France, </address> <pages> pp. 101-108. </pages>
Reference: <author> Horswill, I. D. </author> <year> (1993), </year> <title> Specialization of Perceptual Processes, </title> <type> PhD thesis, </type> <institution> MIT. </institution>
Reference: <author> Kaelbling, L. P. </author> <year> (1990), </year> <title> Learning in Embedded Systems, </title> <type> PhD thesis, </type> <institution> Stan-ford University. </institution>
Reference-contexts: The experiment was set up within a reinforcement learning (RL) framework, so that each of the robots was learning a reactive mapping between its sensory perceptions and its repertoire of pre-programmed fixed-duration behaviors based on reinforcement received over time <ref> (Kaelbling 1990) </ref>. The sensory data was based on the inputs from the whiskers and the pyroelectric (light) sensors. Each of the two whiskers returned a one-bit contact/no-contact value whenever probed.
Reference: <author> Littman, M. L. </author> <year> (1994), </year> <title> Markov games as a framework for multi-agent reinforcement learning, </title> <editor> in W. W. Cohen & H. Hirsh, eds, </editor> <booktitle> `Proceedings of the Eleventh International Conference on Machine Learning (ML-94)', </booktitle> <publisher> Morgan Kauffman Publishers, Inc., </publisher> <address> New Brunswick, NJ, </address> <pages> pp. 157-163. </pages>
Reference: <author> Mahadevan, S. & Connell, J. </author> <year> (1991), </year> <title> Automatic Programming of Behavior-Based Robots Using Reinforcement Learning, </title> <booktitle> in `Proceedings, AAAI-91', </booktitle> <address> Pittsburgh, PA, </address> <pages> pp. 8-14. </pages>
Reference: <author> Mahadevan, S. & Kaelbling, L. P. </author> <year> (1996), </year> <booktitle> `The National Science Foundation Workshop on Reinforcement Learning', AI Magazine 17(4), </booktitle> <pages> 89-97. </pages>
Reference-contexts: The work we have described utilizes some simple built-in communication strategies to compensate for sensory limitations as well as exploit message-passing as a broader form of sensing and reinforcement. Robot learning is known to be a very difficult problem <ref> (Mahadevan & Kaelbling 1996) </ref>, and multi-robot learning an even more challenging instance of it that combines issues from learning, robots, and multi-agent systems. We applied some simplifying strategies to make the learning problem manageable, largely at the individual robot level. <p> In DAI learning work, several related contributions have been made in the area of studying multiple simulated reinforcement learners (see collection by Weiss & Sen (1996)), but have not yet been applied to robotics, where multi-robot learning was identified as an area to be addressed <ref> (Mahadevan & Kaelbling 1996) </ref>.
Reference: <author> Mataric, M. J. </author> <year> (1994), </year> <title> Reward Functions for Accelerated Learning, </title> <editor> in W. </editor> <publisher> W. </publisher>
Reference-contexts: In contrast, our work uses communication to compensate for the robots' sensory limitations and to facilitate learning. Our previous work <ref> (Mataric 1994) </ref> treated learning in a multi-robot system, and introduced the use of behaviors and their associated conditions as a means for state space reduction which was applied in the work described here.
Reference: <editor> Cohen & H. Hirsh, eds, </editor> <booktitle> `Proceedings of the Eleventh International Conference on Machine Learning (ML-94)', </booktitle> <publisher> Morgan Kauffman Publishers, Inc., </publisher> <address> New Brunswick, NJ, </address> <pages> pp. 181-189. </pages> <note> 18 Mataric, </note> <author> M. J. </author> <year> (1995), </year> <title> `Issues and Approaches In the Design of Collective Autonomous Agents', </title> <booktitle> Robotics and Autonomous Systems 16(2-4), </booktitle> <pages> 321-331. </pages>
Reference: <author> Mataric, M. J. </author> <year> (1996), </year> <title> `Reinforcement Learning in the Multi-Robot Domain', </title> <booktitle> Autonomous Robots 4(1), </booktitle> <pages> 73-83. </pages>
Reference-contexts: In robotics in particular, sensors have been targeted as one of the limiting factors in the way of progress toward more complex autonomous behavior. Most of the commonly used sensors provide noisy data and are difficult to accurately characterize and model, presenting a major challenge for real-time robot learning <ref> (Mataric 1996) </ref>. Yet, from the multi-agent perspective, the ability to sense and correctly distinguish members of one's group from others, from obstacles, and from various features in the the environment is crucial for most tasks. <p> Not all behaviors were used in the learning; the following subset enabling the social rules was a part of the learning system: stop, send-msg, proceed and follow-msg. The perceptual conditions for the rest of the behaviors were fixed, based on previous learning experiments <ref> (Mataric 1996) </ref>. Consequently, the robots did not need to learn how to collect pucks, but they did need to learn how to do so in a more social and efficient fashion.
Reference: <author> McCallum, A. R. </author> <year> (1996), </year> <title> Learning to use selective attention and short-term memory in sequential tasks, </title> <editor> in P. Maes, M. Mataric, J.-A. Meyer, J. Pol-lack & S. Wilson, eds, </editor> <booktitle> `From Animals to Animats 4: Proceedings of the Fourth International Conference on Simulation of Adaptive Behavior', </booktitle> <publisher> The MIT Press, </publisher> <pages> pp. 315-324. </pages>
Reference-contexts: The problems of hidden state and credit assignment have been dealt with extensively in the ML literature <ref> (McCallum 1996, Sutton 1992) </ref>. Whitehead (1991) analyzed cooperative RL mechanisms and Littman (1994) used the Markov games framework for simulated RL soccer-playing agents.
Reference: <author> Murciano, A. & del R. Millan, J. </author> <year> (1997), </year> <title> `Learning Signaling Behaviors and Specialization in Cooperative Agents', </title> <booktitle> Adaptive Behavior 5(1), </booktitle> <pages> 5-28. </pages>
Reference: <author> Parker, L. E. </author> <year> (1993), </year> <title> Learning in Cooperative Robot Teams, </title> <booktitle> in `Proceedings, IJCAI-93 Workshop on Dynamically Interacting Robots', </booktitle> <address> Chambery, France, </address> <pages> pp. 12-23. </pages>
Reference: <author> Seeley, T. D. </author> <year> (1989), </year> <title> `The Honey Bee Colony as a Superorganism', </title> <journal> American Scientist 77, </journal> <pages> 546-553. </pages>
Reference-contexts: For example, bees use signals, such as the waggle dance, with the sole purpose of transmitting information and recruiting. In contrast, they also use cues, such as the direction of their flight, which transmit hive information as a by-product of their other behaviors <ref> (Seeley 1989) </ref>. Cooperation is a form of interaction, usually based on communication. Certain types of cooperative behavior depend on directed communication. Specifically, any cooperative behaviors that require negotiation between agents depend on directed communication in order to assign particular tasks to the participants.
Reference: <author> Simsarian, K. T. & Mataric, M. J. </author> <year> (1995), </year> <title> Learning to Cooperate Using Two Six-Legged Mobile Robots, </title> <booktitle> in `Proceedings, Third European Workshop of Learning Robots', </booktitle> <address> Heraklion, Crete, Greece. </address>
Reference: <author> Sutton, R. S. </author> <year> (1992), </year> <title> Machine Learning, Special Issue on Reinforcement Learning, </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston. </address>
Reference: <author> Tan, M. </author> <year> (1993), </year> <title> Multi-Agent Reinforcement Learning: Independent vs. </title> <booktitle> Cooperative Agents, in `Proceedings, Tenth International Conference on Machine Learning', </booktitle> <address> Amherst, MA, </address> <pages> pp. 330-337. </pages>
Reference: <author> Uchibe, E., Asada, M. & Hosoda, K. </author> <year> (1996), </year> <title> Strategy Classification in the Multi-Agent Enviornment Applying Reinforcement Learing to Soccer Agents, in `Proceedings, ICMASS Workshop on RoboCup: Soccer as a Problem for Mulit-Agent Systems'. </title> <type> 19 Wehner, </type> <institution> R. </institution> <year> (1987), </year> <title> `Matched Filters Neural Models of the External World', </title> <journal> Journal of Computational Physiology A(161), </journal> <pages> 511-531. </pages>
Reference: <author> Weiss, G. & Sen, S. </author> <year> (1996), </year> <title> Adaptation and Learning in Multi-Agent Systems, </title> <booktitle> Lecture Notes in Artificial Intelligence, </booktitle> <volume> Vol. 1042, </volume> <publisher> Springer-Verlag. </publisher>
Reference: <author> Whitehead, S. D. </author> <year> (1991), </year> <title> A complexity analysis of cooperative mechanisms in reinforcement learning, </title> <booktitle> in `Proceedings, AAAI-91', </booktitle> <address> Pittsburgh, PA. </address>
Reference: <author> Whitehead, S. D. & Ballard, D. H. </author> <year> (1990), </year> <title> Active Perception and Reinforcement Learning, </title> <booktitle> in `Proceedings, Seventh International Conference on Machine Learning', </booktitle> <address> Austin, Texas. </address>
Reference-contexts: Some effective low-overhead approaches to robot vision have been imple 4 mented on single robots (e.g., Horswill (1993)) but have not yet been scaled up to groups. The inability to obtain sufficient sensory information to properly discriminate results in perceptual aliasing or the hidden state problem <ref> (Whitehead & Ballard 1990) </ref>. Due to sensory limitations, multiple world states are perceived as the same input state, inducing serious problems for learning in any domain.
Reference: <author> Yanco, H. & Stein, L. A. </author> <year> (1993), </year> <title> An Adaptive Communication Protocol for Cooperating Mobile Robots, </title> <editor> in J.-A. Meyer, H. Roitblat & S. Wilson, eds, </editor> <booktitle> `From Animals to Animats: International Conference on Simulation of Adaptive Behavior', </booktitle> <publisher> The MIT Press, </publisher> <pages> pp. 478-485. 20 </pages>
References-found: 25

