URL: http://www.cs.berkeley.edu/~demmel/cs267-1995/zege.ps.gz
Refering-URL: http://www.cs.berkeley.edu/~demmel/cs267-1995/
Root-URL: http://www.cs.berkeley.edu
Title: Model of LPARX multigrid performance on CM-5  
Author: A.Zege 
Date: May 23, 1995  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> S.Baden and S.Kohn,"LPARX", </author> <note> http: //www-cse.ucsd.edu /users/skohn/lparx.html. 9 </note>
Reference-contexts: 1 Introduction The goal of this work was to compare behaviour of a parallel code, in particular its communication pattern, with a simple analytical model and understand how good are predictions of the model. The particular code used for analysis and measurement was a parallel multigrid code from LPARX library <ref> [1] </ref> written by S.Kohn and S.Baden. LPARX was configured and compiled on CM-5 at Berkeley. The timing and account of communication of the code were implemented with low-level CMMD function calls. <p> Number of floating point operations in different subroutines of LPARX multigrid code is specified in the table 3, assuming the cost of addition equal to the cost of multiplication. Calculating the total number of flops, I took into account that LPARX code executes two presmoothing and two postsmoothing relaxations <ref> [1] </ref>, so the number of the relaxation flops should be multiplied by four. 4.2 Communication To figure out communication, let's look on the pseudocode of the parallel functions refine, coarsen, and relax which are the building blocks of the multigrid V-cycle.
Reference: [2] <author> J.Demmel, </author> <note> "Sping 95 CS267 lectures", http: //www.icsi.berkeley.edu/ cs267. </note>
Reference-contexts: LPARX was configured and compiled on CM-5 at Berkeley. The timing and account of communication of the code were implemented with low-level CMMD function calls. Results of the measurements are described and a detailed comparison with a simple model of parallel multigrid, proposed by J.Demmel <ref> [2] </ref> is given. It was found that the model gives a very good prediction of the scaling of all measured parameters with the grid dimension. Crude analytical estimation of coefficients in scaling laws is worse | it gives an error of the order of 15-20%. <p> For a basic V-cycle refining stops where the coarsen ing started. The Fig. 1 shows the basic multigrid cycle along with its recursive pseu docode. To describe the model of parallel computation, I used parametrization by <ref> [2] </ref> which defines the time for a processor to send n messages and do m floating point operations as T = T calc + T comm ; T calc = mf; T comm = ff + n fl fi; where f is time for a floating point operation, ff is latency, <p> These values are given both for N proc = 4 and N proc = 16 in the table 6 and compared with the correspond ing values for matrix multiplication <ref> [2] </ref>.
Reference: [3] <author> W.Briggs, </author> <title> "A Multigrid tutorial", </title> <publisher> SIAM, </publisher> <year> 1987. </year> <title> 10 11 12 along direction indicated with a thin arrow are negligible. </title> <type> 13 14 15 </type>
Reference-contexts: Additinally, hardware parameters best fitting the data were calculated and found in a reasonable accord with the known data. 1 2 Multigrid performance model Multigrid is an algorithm for solving Poisson equation <ref> [3] </ref>; multigrid's prin cipal part is the so-called V-cycle that works in the following stages: 1. coarsening: relaxation is performed on the sequence of grids, where each consequtive grid has twice as less grid points in every dimension than the current grid.
References-found: 3

