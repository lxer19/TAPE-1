URL: ftp://ftp.cs.umd.edu/pub/papers/papers/2972/2972.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Title: Determining Rank in the Presence of Error  
Author: G. W. Stewart 
Date: October, 1992  
Affiliation: University of Maryland College Park Institute for Advanced Computer Studies TR-92-108 Department of Computer Science  
Pubnum: TR-2972  
Abstract: The problem of determining rank in the presence of error occurs in a number of applications. The usual approach is to compute a rank-revealing decomposition and make a decision about the rank by examining the small elements of the decomposition. In this paper we look at three commonly use decompositions: the singular value decomposition, the pivoted QR decomposition, and the URV decomposition. fl This report is available by anonymous ftp from thales.cs.umd.edu in the directory pub/reports. The report will appear in the proceedings of the NATO Workshop on Large Scale Linear Algebra, Leuven, Belgium, 1992. y Department of Computer Science and Institute for Advanced Computer Studies, University of Mary land, College Park, MD 20742. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. Adams, M. F. Griffin, and G. W. Stewart. </author> <title> Direction-of-arrival estimation using the rank-revealing URV decomposition. </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, </booktitle> <address> Washington, DC, 1991. </address> <publisher> IEEE. </publisher>
Reference-contexts: Notes and References The problem of rank determination in the presence of error arises in a number of applications: e.g., variable selection in statistics and engineering [7, 47, 61], direction of arrival estimation in signal processing <ref> [1, 58, 59] </ref>, and the projection of ill-conditioned problems onto manifolds where they become well conditioned [51, 21, 22]. In many instances, the original matrix X is not exactly of rank k as we have described it in the introduction.
Reference: [2] <author> T. W. Anderson. </author> <title> Estimation of linear functional relationships: Approximate distributions and connections with simultaneous equations in econometrics. </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> 38 </volume> <pages> 1-31, </pages> <year> 1976. </year>
Reference-contexts: The fact that something must be known about the errors in order to make statements about rank is a commonplace in areas like signal processing, where the errors are relatively large, or statistics, where there is a vast literature under the heading "errors in the variables" <ref> [2, 3, 5, 4, 8, 20, 34, 48, 65] </ref>, or numerical analysis, where there is a growing literature under the heading "total least squares" [31, 41, 63, 72, 74].
Reference: [3] <author> A. E. Beaton, D. B. Rubin, and J. L. Barone. </author> <title> The acceptability of regression solutions: Another look at computational accuracy. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 71 </volume> <pages> 158-168, </pages> <year> 1976. </year>
Reference-contexts: The fact that something must be known about the errors in order to make statements about rank is a commonplace in areas like signal processing, where the errors are relatively large, or statistics, where there is a vast literature under the heading "errors in the variables" <ref> [2, 3, 5, 4, 8, 20, 34, 48, 65] </ref>, or numerical analysis, where there is a growing literature under the heading "total least squares" [31, 41, 63, 72, 74].
Reference: [4] <author> D. A. Belsley. </author> <title> Assessing the presence of harmful collinearity and other forms of weak data through a test for signal-to-noise. </title> <journal> Journal of Econometrics, </journal> <volume> 20 </volume> <pages> 211-253, </pages> <year> 1982. </year>
Reference-contexts: The fact that something must be known about the errors in order to make statements about rank is a commonplace in areas like signal processing, where the errors are relatively large, or statistics, where there is a vast literature under the heading "errors in the variables" <ref> [2, 3, 5, 4, 8, 20, 34, 48, 65] </ref>, or numerical analysis, where there is a growing literature under the heading "total least squares" [31, 41, 63, 72, 74].
Reference: [5] <author> D. A. Belsley, A. E. Kuh, and R. E. Welsch. </author> <title> Regression Diagnostics: Identifying Influential Data and Sources of Collinearity. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1980. </year>
Reference-contexts: The fact that something must be known about the errors in order to make statements about rank is a commonplace in areas like signal processing, where the errors are relatively large, or statistics, where there is a vast literature under the heading "errors in the variables" <ref> [2, 3, 5, 4, 8, 20, 34, 48, 65] </ref>, or numerical analysis, where there is a growing literature under the heading "total least squares" [31, 41, 63, 72, 74].
Reference: [6] <editor> E. Beltrami. Sulle funzioni bilineari. Giornale di Matematiche ad Uso degli Studenti Delle Universita, </editor> <volume> 11 </volume> <pages> 98-106, </pages> <note> 1873. An English translation by D. Boley is available as University of Minnesota, </note> <institution> Department of Computer Science, </institution> <type> Technical Report 90-37, </type> <year> 1990. </year>
Reference-contexts: Recently new algorithms for reducing the bidiagonal matrix have been proposed [25, 30]. The idea of first computing the QR decomposition has been exploited by Chan [12, 11]. Beltrami <ref> [6] </ref> first established the existence of the singular value decomposition in 1873 by computing the eigendecomposition of the cross-product matrix, and this is still a popular way of doing things in some disciplines. In fact, sometimes the singular value decomposition completely disappears.
Reference: [7] <author> K. N. </author> <title> Berk. Comparing subset regression procedures. </title> <journal> Technometrics, </journal> <volume> 20 </volume> <pages> 1-6, </pages> <year> 1978. </year> <title> 14 Rank Determination </title>
Reference-contexts: Notes and References The problem of rank determination in the presence of error arises in a number of applications: e.g., variable selection in statistics and engineering <ref> [7, 47, 61] </ref>, direction of arrival estimation in signal processing [1, 58, 59], and the projection of ill-conditioned problems onto manifolds where they become well conditioned [51, 21, 22]. In many instances, the original matrix X is not exactly of rank k as we have described it in the introduction.
Reference: [8] <author> J. Berkson. </author> <title> Are there two regressions. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 45 </volume> <pages> 164-180, </pages> <year> 1950. </year>
Reference-contexts: The fact that something must be known about the errors in order to make statements about rank is a commonplace in areas like signal processing, where the errors are relatively large, or statistics, where there is a vast literature under the heading "errors in the variables" <ref> [2, 3, 5, 4, 8, 20, 34, 48, 65] </ref>, or numerical analysis, where there is a growing literature under the heading "total least squares" [31, 41, 63, 72, 74].
Reference: [9] <author> D. Bogert and W. R. Burris. </author> <title> Comparison of least squares algorithms. </title> <type> Report ORNL-3499, </type> <institution> Neutron Physics Division, Oak Ridge National Laboratory, </institution> <year> 1963. </year> <note> Vol. 1, Sec. 5.5. </note>
Reference-contexts: Gram [44] and Schmidt [60] orthogonalized series of functions: Gram by determinantal expressions (hence the Gramian matrix) and Schmidt by the now classic algorithm. The use of orthogonal transformations to compute the decomposition is due to Householder [50], Bogert and Burris <ref> [9] </ref>, and Golub [36]. The last mentioned work also contains the notion of column pivoting and the first updating algorithm for the QR decomposition. The name QR decomposition is from Francis's QR algorithm [33], which uses the decomposition.
Reference: [10] <author> J. R. Bunch and C. P. Nielsen. </author> <title> Updating the singular value decomposition. </title> <journal> Nu-merische Mathematik, </journal> <volume> 31 </volume> <pages> 111-129, </pages> <year> 1978. </year>
Reference-contexts: In fact, sometimes the singular value decomposition completely disappears. Algorithms for updating the singular value decomposition have been given in <ref> [10, 18] </ref>; however, they require O (p 3 ) operations, the same as required to compute the decomposition from scratch. Iterative algorithms that maintain an approximate factorization may be found in [54, 55, 56].
Reference: [11] <author> T. F. Chan. </author> <title> Algorithm 581: An improved algorithm for computing the singular value decomposition. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 8 </volume> <pages> 84-88, </pages> <year> 1982. </year>
Reference-contexts: Recently new algorithms for reducing the bidiagonal matrix have been proposed [25, 30]. The idea of first computing the QR decomposition has been exploited by Chan <ref> [12, 11] </ref>. Beltrami [6] first established the existence of the singular value decomposition in 1873 by computing the eigendecomposition of the cross-product matrix, and this is still a popular way of doing things in some disciplines. In fact, sometimes the singular value decomposition completely disappears.
Reference: [12] <author> T. F. Chan. </author> <title> An improved algorithm for computing the singular value decomposition. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 8 </volume> <pages> 72-83, </pages> <year> 1982. </year>
Reference-contexts: Recently new algorithms for reducing the bidiagonal matrix have been proposed [25, 30]. The idea of first computing the QR decomposition has been exploited by Chan <ref> [12, 11] </ref>. Beltrami [6] first established the existence of the singular value decomposition in 1873 by computing the eigendecomposition of the cross-product matrix, and this is still a popular way of doing things in some disciplines. In fact, sometimes the singular value decomposition completely disappears.
Reference: [13] <author> T. F. Chan. </author> <title> Rank revealing QR factorizations. </title> <journal> Linear Algebra and Its Applications, </journal> 88/89:67-82, 1987. 
Reference-contexts: The name QR decomposition is from Francis's QR algorithm [33], which uses the decomposition. Although pivoting for column size while computing the QR decomposition has long been regarded as a reliable way of determining rank (e.g., see [39, 62]), Chan <ref> [13] </ref> was the first to give bounds for a rank-revealing decomposition (the descriptive phrase "rank revealing" was coined by him). Unfortunately, the bounds were exponential in the defect p k in the rank.
Reference: [14] <author> S. Chandrasekaran and I. Ipsen. </author> <title> Perturbation theory for the solution of systems of linear equations. </title> <institution> Research Report YALEU/DCS/RR-866, Department of Computer Science, Yale University, </institution> <year> 1991. </year>
Reference-contexts: Unfortunately, the bounds were exponential in the defect p k in the rank. In fact, only recently have Hong and Pan [49] established the existence of a rank revealing QR decomposition. Although their approach is not constructive, Chandrasekaran and Ipsen <ref> [14] </ref> have given an algorithm, which unfortunately has combinatorial complexity (this paper is an excellent source for other pivoting strategies that have appeared in the literature). In a personal communication and Pan and Tang have described and algorithm that requires less work.
Reference: [15] <author> S. Chandrasekaran and I. Ipsen. </author> <title> Analysis of a QR algorithm for computing singular values. </title> <institution> Research Report YALEU/DCS/RR-917, Department of Computer Science, Yale University, </institution> <year> 1992. </year>
Reference-contexts: For a survey with references see [46]. URV and ULV decompositions [68, 66] had their genesis in the author's unsuccessful attempt to update a rank-revealing QR factorization. A refinement step, which tends to decrease the size of the off-diagonal elements has been analyzed in [53] (see also <ref> [56, 15, 28] </ref>). A parallel implementation of the updating algorithm is described in [69]. The methods treated here are not the only ones for revealing rank. For example, methods based on the Lanczos algorithm have been proposed for the case where the rank is small [17, 77, 78].
Reference: [16] <author> R. Choudary, Hanumara, and W. A. Thompson Jr. </author> <title> Percentage points of the extreme roots of a Wishart matrix. </title> <journal> Biometrika, </journal> <volume> 55 </volume> <pages> 505-512, </pages> <year> 1968. </year>
Reference-contexts: More generally the singular values 2 k+1 ; : : : 2 p are approximately the eigenvalues of a Wishart matrix, whose distributions are known (e.g., see <ref> [26, 16] </ref>). The problem of poorly scaled errors is closely related to the problem of artificial ill-conditioning, which is discussed in [64]. The equal error scaling advocated there is the equivalent of noise whitening.
Reference: [17] <author> P. Comon and G. H. Golub. </author> <title> Tracking a few extreme singular values and vectors in signal processing. </title> <journal> Proc. IEEE, </journal> <volume> 78 </volume> <pages> 1327-1343, </pages> <year> 1990. </year>
Reference-contexts: A parallel implementation of the updating algorithm is described in [69]. The methods treated here are not the only ones for revealing rank. For example, methods based on the Lanczos algorithm have been proposed for the case where the rank is small <ref> [17, 77, 78] </ref>. The perturbation of singular values, including Fischer's theorem, is surveyed in [70]. The relation (8) is a consequence of theorems in [53]. The approach to rank determination followed here is rather crude, suitable for the crude models and data one Rank Determination 13 can expect in practice.
Reference: [18] <author> J. J. M. Cuppen. </author> <title> The singular value decomposition in product form. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 4 </volume> <pages> 216-222, </pages> <year> 1983. </year> <note> Cited in [42]. </note>
Reference-contexts: In fact, sometimes the singular value decomposition completely disappears. Algorithms for updating the singular value decomposition have been given in <ref> [10, 18] </ref>; however, they require O (p 3 ) operations, the same as required to compute the decomposition from scratch. Iterative algorithms that maintain an approximate factorization may be found in [54, 55, 56].
Reference: [19] <author> G. B. Dantzig. </author> <title> Linear Programming and Extensions. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, New Jersey, </address> <year> 1963. </year>
Reference-contexts: Although the updating of least squares solutions goes back to Gauss [35], the updating of decompositions seems to have arisen in linear program, where the inverse basis matrix must be updated <ref> [19] </ref>. A closely related problem is that of downdating | the removal of rows from X | a process that is also called windowing. The literature on updating and downdating is too voluminous to survey here.
Reference: [20] <author> R. B. Davies and B. Hutton. </author> <title> The effects of errors in the independent variables in linear regression. </title> <journal> Biometrika, </journal> <volume> 62 </volume> <pages> 383-391, </pages> <year> 1975. </year>
Reference-contexts: The fact that something must be known about the errors in order to make statements about rank is a commonplace in areas like signal processing, where the errors are relatively large, or statistics, where there is a vast literature under the heading "errors in the variables" <ref> [2, 3, 5, 4, 8, 20, 34, 48, 65] </ref>, or numerical analysis, where there is a growing literature under the heading "total least squares" [31, 41, 63, 72, 74].
Reference: [21] <author> J. Demmel. </author> <title> A Numerical Analyst's Jordan Canonical Form. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of California at Berleley, </institution> <year> 1983. </year> <note> Center for Pure and Applied Mathematics Technical Report PAM-156. Rank Determination 15 </note>
Reference-contexts: References The problem of rank determination in the presence of error arises in a number of applications: e.g., variable selection in statistics and engineering [7, 47, 61], direction of arrival estimation in signal processing [1, 58, 59], and the projection of ill-conditioned problems onto manifolds where they become well conditioned <ref> [51, 21, 22] </ref>. In many instances, the original matrix X is not exactly of rank k as we have described it in the introduction.
Reference: [22] <author> J. Demmel. </author> <title> On condition numbers and the distance to the nearest ill-posed problem. </title> <journal> Numerische Mathematik, </journal> <volume> 51 </volume> <pages> 251-290, </pages> <year> 1987. </year>
Reference-contexts: References The problem of rank determination in the presence of error arises in a number of applications: e.g., variable selection in statistics and engineering [7, 47, 61], direction of arrival estimation in signal processing [1, 58, 59], and the projection of ill-conditioned problems onto manifolds where they become well conditioned <ref> [51, 21, 22] </ref>. In many instances, the original matrix X is not exactly of rank k as we have described it in the introduction.
Reference: [23] <author> J. Demmel. </author> <title> The smallest perturbation of a submatrix which lowers the rank and constrained total least squares problems. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 24 </volume> <pages> 199-206, </pages> <year> 1987. </year>
Reference-contexts: The equal error scaling advocated there is the equivalent of noise whitening. One solution to the problem of constrained errors is to project the problem onto a submanifold where the errors can be whitened <ref> [23, 37, 73] </ref>. The results on testing QRP decompositions appear to be new. The consequence of (11) and (12) are that kG 22 k 2 F will tend to be larger than ~ 2 k .
Reference: [24] <author> J. Demmel and B. K-agstrom. </author> <title> Accurate solutions of ill-posed problems in control theory. </title> <journal> SIAM Journal on Matrix Analysis and Applications, </journal> <volume> 9 </volume> <pages> 126-145, </pages> <year> 1988. </year>
Reference-contexts: Closely related, but of a different flavor, is the problem of regularizing the ill-posed problems which arise from discretizations of compact or unbounded operators <ref> [24, 32, 45, 57, 71, 75] </ref>.
Reference: [25] <author> J. Demmel and W. Kahan. </author> <title> Accurate singular values of bidiagonal matrices. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 11 </volume> <pages> 873-912, </pages> <year> 1989. </year>
Reference-contexts: Until recently reduction to bidiagonal form followed by a variant of the QR algorithm, due to Golub [40], has been the standard way to compute the decomposition. Recently new algorithms for reducing the bidiagonal matrix have been proposed <ref> [25, 30] </ref>. The idea of first computing the QR decomposition has been exploited by Chan [12, 11].
Reference: [26] <author> A. P. Dempster. </author> <title> Elements of Continuous Multivariate Analysis. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1969. </year>
Reference-contexts: More generally the singular values 2 k+1 ; : : : 2 p are approximately the eigenvalues of a Wishart matrix, whose distributions are known (e.g., see <ref> [26, 16] </ref>). The problem of poorly scaled errors is closely related to the problem of artificial ill-conditioning, which is discussed in [64]. The equal error scaling advocated there is the equivalent of noise whitening.
Reference: [27] <author> J. J. Dongarra, J. R. Bunch, C. B. Moler, and G. W. Stewart. </author> <title> LINPACK User's Guide. </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1979. </year>
Reference-contexts: Although the first such algorithm is found in [43], it was LINPACK <ref> [27] </ref> that popularized the idea. For a survey with references see [46]. URV and ULV decompositions [68, 66] had their genesis in the author's unsuccessful attempt to update a rank-revealing QR factorization.
Reference: [28] <author> E. M. Dowling, L. P. Ammann, and R. D. DeGroat. </author> <title> A tqr-iteration based adaptive svd for real time angle and frequency tracking. </title> <institution> Erik Johnson School of Engineering and Computer Science, The University of Texas at Dallas. </institution> <note> Manuscript submitted to IEEE Transactions on Signal Processing, </note> <year> 1992. </year>
Reference-contexts: For a survey with references see [46]. URV and ULV decompositions [68, 66] had their genesis in the author's unsuccessful attempt to update a rank-revealing QR factorization. A refinement step, which tends to decrease the size of the off-diagonal elements has been analyzed in [53] (see also <ref> [56, 15, 28] </ref>). A parallel implementation of the updating algorithm is described in [69]. The methods treated here are not the only ones for revealing rank. For example, methods based on the Lanczos algorithm have been proposed for the case where the rank is small [17, 77, 78].
Reference: [29] <author> C. Eckart and G. Young. </author> <title> The approximation of one matrix by another of lower rank. </title> <journal> Psychometrika, </journal> <volume> 1 </volume> <pages> 211-218, </pages> <year> 1936. </year>
Reference-contexts: The literature on updating and downdating is too voluminous to survey here. The singular value decomposition dates to the last half of the nineteenth century (for a history see [67]). The theorem cited here as Schmidt's theorem [60], is often attributed to Eckart and Young <ref> [29] </ref>, who rediscovered it thirty years later. The popularity of the singular value decomposition in numerical analysis is due to Golub and Kahan [38].
Reference: [30] <author> K. V. Fernando and B. Parlett. </author> <title> Accurate singular values and differential qd algorithms. </title> <type> Technical Report PAM-554, </type> <institution> Department of Mathematics, University of California, Berkeley, </institution> <year> 1992. </year>
Reference-contexts: Until recently reduction to bidiagonal form followed by a variant of the QR algorithm, due to Golub [40], has been the standard way to compute the decomposition. Recently new algorithms for reducing the bidiagonal matrix have been proposed <ref> [25, 30] </ref>. The idea of first computing the QR decomposition has been exploited by Chan [12, 11].
Reference: [31] <author> R. D. Fierro and J. R. Bunch. </author> <title> Multicollinearity and total least squares. </title> <type> Preprint Series 977, </type> <institution> Institute for Mathematics and Its Applications, </institution> <year> 1992. </year>
Reference-contexts: in areas like signal processing, where the errors are relatively large, or statistics, where there is a vast literature under the heading "errors in the variables" [2, 3, 5, 4, 8, 20, 34, 48, 65], or numerical analysis, where there is a growing literature under the heading "total least squares" <ref> [31, 41, 63, 72, 74] </ref>. Although the updating of least squares solutions goes back to Gauss [35], the updating of decompositions seems to have arisen in linear program, where the inverse basis matrix must be updated [19].
Reference: [32] <author> H. E. Fleming. </author> <title> Equivalence of regularization and truncated iteration in the solution of ill-posed image reconstruction problems. </title> <journal> Linear Algebra and Its Applicaations, </journal> <volume> 130 </volume> <pages> 133-150, </pages> <year> 1990. </year>
Reference-contexts: Closely related, but of a different flavor, is the problem of regularizing the ill-posed problems which arise from discretizations of compact or unbounded operators <ref> [24, 32, 45, 57, 71, 75] </ref>.
Reference: [33] <author> J. G. F. Francis. </author> <title> The QR transformation, parts I and II. </title> <journal> Computer Journal, </journal> <volume> 4 </volume> <pages> 265-271, 332-345, </pages> <year> 1961, 1962. </year>
Reference-contexts: The last mentioned work also contains the notion of column pivoting and the first updating algorithm for the QR decomposition. The name QR decomposition is from Francis's QR algorithm <ref> [33] </ref>, which uses the decomposition.
Reference: [34] <author> W. A. Fuller. </author> <title> Measurement Error Models. </title> <publisher> John Wiley, </publisher> <address> New York, </address> <year> 1987. </year> <title> 16 Rank Determination </title>
Reference-contexts: The fact that something must be known about the errors in order to make statements about rank is a commonplace in areas like signal processing, where the errors are relatively large, or statistics, where there is a vast literature under the heading "errors in the variables" <ref> [2, 3, 5, 4, 8, 20, 34, 48, 65] </ref>, or numerical analysis, where there is a growing literature under the heading "total least squares" [31, 41, 63, 72, 74].
Reference: [35] <editor> C. F. Gauss. Theoria combinationis observationum erroribus minimis obnoxiae, pars posterior. In Werke, </editor> <booktitle> IV, </booktitle> <pages> pages 27-53. </pages> <institution> Koniglichen Gesellshaft der Wis-senschaften zu Gottingin (1880), </institution> <month> 1823. </month>
Reference-contexts: Although the updating of least squares solutions goes back to Gauss <ref> [35] </ref>, the updating of decompositions seems to have arisen in linear program, where the inverse basis matrix must be updated [19]. A closely related problem is that of downdating | the removal of rows from X | a process that is also called windowing.
Reference: [36] <author> G. H. Golub. </author> <title> Numerical methods for solving least squares problems. </title> <journal> Numerische Mathematik, </journal> <volume> 7 </volume> <pages> 206-216, </pages> <year> 1965. </year>
Reference-contexts: Gram [44] and Schmidt [60] orthogonalized series of functions: Gram by determinantal expressions (hence the Gramian matrix) and Schmidt by the now classic algorithm. The use of orthogonal transformations to compute the decomposition is due to Householder [50], Bogert and Burris [9], and Golub <ref> [36] </ref>. The last mentioned work also contains the notion of column pivoting and the first updating algorithm for the QR decomposition. The name QR decomposition is from Francis's QR algorithm [33], which uses the decomposition.
Reference: [37] <author> G. H. Golub, A. Hoffman, and G. W. Stewart. </author> <title> A generalization of the Eckart-Young matrix approximation theorem. </title> <journal> Linear Algebra and Its Applications, </journal> 88/89:317-327, 1987. 
Reference-contexts: The equal error scaling advocated there is the equivalent of noise whitening. One solution to the problem of constrained errors is to project the problem onto a submanifold where the errors can be whitened <ref> [23, 37, 73] </ref>. The results on testing QRP decompositions appear to be new. The consequence of (11) and (12) are that kG 22 k 2 F will tend to be larger than ~ 2 k .
Reference: [38] <author> G. H. Golub and W. Kahan. </author> <title> Calculating the singular values and pseudo-inverse of a matrix. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 2 </volume> <pages> 205-224, </pages> <year> 1965. </year>
Reference-contexts: The theorem cited here as Schmidt's theorem [60], is often attributed to Eckart and Young [29], who rediscovered it thirty years later. The popularity of the singular value decomposition in numerical analysis is due to Golub and Kahan <ref> [38] </ref>. Until recently reduction to bidiagonal form followed by a variant of the QR algorithm, due to Golub [40], has been the standard way to compute the decomposition. Recently new algorithms for reducing the bidiagonal matrix have been proposed [25, 30].
Reference: [39] <author> G. H. Golub, V. Klema, and G. W. Stewart. </author> <title> Rank degeneracy and least squares problems. </title> <type> Technical Report TR-456, </type> <institution> Department of Computer Science, University of Maryland, </institution> <year> 1976. </year>
Reference-contexts: The name QR decomposition is from Francis's QR algorithm [33], which uses the decomposition. Although pivoting for column size while computing the QR decomposition has long been regarded as a reliable way of determining rank (e.g., see <ref> [39, 62] </ref>), Chan [13] was the first to give bounds for a rank-revealing decomposition (the descriptive phrase "rank revealing" was coined by him). Unfortunately, the bounds were exponential in the defect p k in the rank.
Reference: [40] <author> G. H. Golub and C. Reinsch. </author> <title> Singular value decomposition and least squares solution. </title> <journal> Numerische Mathematik, </journal> <volume> 14 </volume> <pages> 403-420, </pages> <year> 1970. </year> <note> Also in [76, pp.134-151]. </note>
Reference-contexts: The popularity of the singular value decomposition in numerical analysis is due to Golub and Kahan [38]. Until recently reduction to bidiagonal form followed by a variant of the QR algorithm, due to Golub <ref> [40] </ref>, has been the standard way to compute the decomposition. Recently new algorithms for reducing the bidiagonal matrix have been proposed [25, 30]. The idea of first computing the QR decomposition has been exploited by Chan [12, 11].
Reference: [41] <author> G. H. Golub and C. F. Van Loan. </author> <title> An analysis of the total least squares problem. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 17 </volume> <pages> 883-893, </pages> <year> 1980. </year>
Reference-contexts: in areas like signal processing, where the errors are relatively large, or statistics, where there is a vast literature under the heading "errors in the variables" [2, 3, 5, 4, 8, 20, 34, 48, 65], or numerical analysis, where there is a growing literature under the heading "total least squares" <ref> [31, 41, 63, 72, 74] </ref>. Although the updating of least squares solutions goes back to Gauss [35], the updating of decompositions seems to have arisen in linear program, where the inverse basis matrix must be updated [19].
Reference: [42] <author> G. H. Golub and C. F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> Johns Hopkins University Press, </publisher> <address> Baltimore, Maryland, 2nd edition, </address> <year> 1989. </year>
Reference: [43] <author> W. B. Gragg and G. W. Stewart. </author> <title> A stable variant of the secant method for solving nonlinear equations. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 13 </volume> <pages> 880-903, </pages> <year> 1976. </year>
Reference-contexts: Although the first such algorithm is found in <ref> [43] </ref>, it was LINPACK [27] that popularized the idea. For a survey with references see [46]. URV and ULV decompositions [68, 66] had their genesis in the author's unsuccessful attempt to update a rank-revealing QR factorization.
Reference: [44] <author> J. P. </author> <title> Gram. Uber die Entwicklung reeler Functionen in Reihen mittelst der Methode der kleinsten Quadrate. </title> <journal> Journal fur die reine und angewandte Mathematik, </journal> <volume> 94 </volume> <pages> 41-73, 1883. </pages>
Reference-contexts: Gram <ref> [44] </ref> and Schmidt [60] orthogonalized series of functions: Gram by determinantal expressions (hence the Gramian matrix) and Schmidt by the now classic algorithm. The use of orthogonal transformations to compute the decomposition is due to Householder [50], Bogert and Burris [9], and Golub [36].
Reference: [45] <author> P. C. Hansen. </author> <title> Truncated singular value decomposition solutions to discrete ill-posed problems with ill-determined numerical rank. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 11 </volume> <pages> 503-518, </pages> <year> 1990. </year>
Reference-contexts: Closely related, but of a different flavor, is the problem of regularizing the ill-posed problems which arise from discretizations of compact or unbounded operators <ref> [24, 32, 45, 57, 71, 75] </ref>.
Reference: [46] <author> N. J. Higham. </author> <title> A survey of condition number estimation for triangular matrices. </title> <journal> SIAM Review, </journal> <volume> 29 </volume> <pages> 575-596, </pages> <year> 1987. </year>
Reference-contexts: Although the first such algorithm is found in [43], it was LINPACK [27] that popularized the idea. For a survey with references see <ref> [46] </ref>. URV and ULV decompositions [68, 66] had their genesis in the author's unsuccessful attempt to update a rank-revealing QR factorization. A refinement step, which tends to decrease the size of the off-diagonal elements has been analyzed in [53] (see also [56, 15, 28]).
Reference: [47] <author> R. R. Hocking. </author> <title> The analysis and selection of variables in linear regression. </title> <journal> Bio-metrics, </journal> <volume> 32 </volume> <pages> 1-49, </pages> <year> 1976. </year> <title> Rank Determination 17 </title>
Reference-contexts: Notes and References The problem of rank determination in the presence of error arises in a number of applications: e.g., variable selection in statistics and engineering <ref> [7, 47, 61] </ref>, direction of arrival estimation in signal processing [1, 58, 59], and the projection of ill-conditioned problems onto manifolds where they become well conditioned [51, 21, 22]. In many instances, the original matrix X is not exactly of rank k as we have described it in the introduction.
Reference: [48] <author> S. D. Hodges and P. G. Moore. </author> <title> Data uncertainties and least squares regression. </title> <journal> Applied Statistics, </journal> <volume> 21 </volume> <pages> 185-195, </pages> <year> 1972. </year>
Reference-contexts: The fact that something must be known about the errors in order to make statements about rank is a commonplace in areas like signal processing, where the errors are relatively large, or statistics, where there is a vast literature under the heading "errors in the variables" <ref> [2, 3, 5, 4, 8, 20, 34, 48, 65] </ref>, or numerical analysis, where there is a growing literature under the heading "total least squares" [31, 41, 63, 72, 74].
Reference: [49] <author> Y. P. Hong and C.-T. Pan. </author> <title> Rank-revealing QR factorizations and the singular value decomposition. </title> <journal> Mathematics of Computation, </journal> <volume> 58 </volume> <pages> 213-232, </pages> <year> 1992. </year>
Reference-contexts: Unfortunately, the bounds were exponential in the defect p k in the rank. In fact, only recently have Hong and Pan <ref> [49] </ref> established the existence of a rank revealing QR decomposition. Although their approach is not constructive, Chandrasekaran and Ipsen [14] have given an algorithm, which unfortunately has combinatorial complexity (this paper is an excellent source for other pivoting strategies that have appeared in the literature).
Reference: [50] <author> A. S. </author> <title> Householder. Unitary triangularization of a nonsymmetric matrix. </title> <journal> Journal of the ACM, </journal> <volume> 5 </volume> <pages> 339-342, </pages> <year> 1958. </year>
Reference-contexts: Gram [44] and Schmidt [60] orthogonalized series of functions: Gram by determinantal expressions (hence the Gramian matrix) and Schmidt by the now classic algorithm. The use of orthogonal transformations to compute the decomposition is due to Householder <ref> [50] </ref>, Bogert and Burris [9], and Golub [36]. The last mentioned work also contains the notion of column pivoting and the first updating algorithm for the QR decomposition. The name QR decomposition is from Francis's QR algorithm [33], which uses the decomposition.
Reference: [51] <author> W. Kahan. </author> <title> Conserving confluence curbs ill-conditioning. </title> <type> Technical Report 6, </type> <institution> Computer Science Department, University of California, Berkeley, </institution> <year> 1972. </year>
Reference-contexts: References The problem of rank determination in the presence of error arises in a number of applications: e.g., variable selection in statistics and engineering [7, 47, 61], direction of arrival estimation in signal processing [1, 58, 59], and the projection of ill-conditioned problems onto manifolds where they become well conditioned <ref> [51, 21, 22] </ref>. In many instances, the original matrix X is not exactly of rank k as we have described it in the introduction.
Reference: [52] <author> P. S. </author> <title> Laplace. Theoria analytique des probabilities (3rd ed.) premier supplement: Sur l'application du calcul des probabilites a la philosophie naturelle. Oeuvres, </title> <journal> v.7. </journal> <note> Gauthier-Villars, 1820. Supplement published before 1820. </note>
Reference-contexts: Iterative algorithms that maintain an approximate factorization may be found in [54, 55, 56]. Formulas for the discrete version of the Gram-Schmidt algorithm can be found in the 12 Rank Determination first supplement to Laplace's Theoria Analytique des Probabilities <ref> [52] </ref>; however, Laplace was after an expression for the variance of a regression parameter and did not regard his formulas as a computational device. Gram [44] and Schmidt [60] orthogonalized series of functions: Gram by determinantal expressions (hence the Gramian matrix) and Schmidt by the now classic algorithm.
Reference: [53] <author> R. Mathias and G. W. Stewart. </author> <title> A block qr algorithm and the singular value decomposition. </title> <type> Technical Report CS-TR 2626, </type> <institution> Department of Computer Science, University of Maryland, College Park, </institution> <year> 1992. </year> <note> To appear in Linear Algebra adn Its Applications. </note>
Reference-contexts: For a survey with references see [46]. URV and ULV decompositions [68, 66] had their genesis in the author's unsuccessful attempt to update a rank-revealing QR factorization. A refinement step, which tends to decrease the size of the off-diagonal elements has been analyzed in <ref> [53] </ref> (see also [56, 15, 28]). A parallel implementation of the updating algorithm is described in [69]. The methods treated here are not the only ones for revealing rank. <p> For example, methods based on the Lanczos algorithm have been proposed for the case where the rank is small [17, 77, 78]. The perturbation of singular values, including Fischer's theorem, is surveyed in [70]. The relation (8) is a consequence of theorems in <ref> [53] </ref>. The approach to rank determination followed here is rather crude, suitable for the crude models and data one Rank Determination 13 can expect in practice.
Reference: [54] <author> M. Moonen. </author> <title> Jacobi-Type Updating Algorithms for Signal Processing, Systems Identification and Control. </title> <type> PhD thesis, </type> <institution> Katholieke Universiteit Leuven, </institution> <year> 1990. </year>
Reference-contexts: Algorithms for updating the singular value decomposition have been given in [10, 18]; however, they require O (p 3 ) operations, the same as required to compute the decomposition from scratch. Iterative algorithms that maintain an approximate factorization may be found in <ref> [54, 55, 56] </ref>.
Reference: [55] <author> M. Moonen, P. Van Dooren, and J. Vandewalle. </author> <title> Combined Jacobi-type algorithms in signal processing. </title> <editor> In R. J. Vaccaro, editor, </editor> <booktitle> SVD and Signal Processing, II, </booktitle> <pages> pages 177-188, </pages> <address> Amsterdam, 1991. </address> <publisher> Elsevier Science Publishers. </publisher>
Reference-contexts: Algorithms for updating the singular value decomposition have been given in [10, 18]; however, they require O (p 3 ) operations, the same as required to compute the decomposition from scratch. Iterative algorithms that maintain an approximate factorization may be found in <ref> [54, 55, 56] </ref>.
Reference: [56] <author> M. Moonen, P. Van Dooren, and F. Vanpoucke. </author> <title> On the QR algorithm and updating the SVD and URV decomposition in parallel. </title> <institution> ESAT Katholieke Universiteit Leuven, </institution> <year> 1992. </year>
Reference-contexts: Algorithms for updating the singular value decomposition have been given in [10, 18]; however, they require O (p 3 ) operations, the same as required to compute the decomposition from scratch. Iterative algorithms that maintain an approximate factorization may be found in <ref> [54, 55, 56] </ref>. <p> For a survey with references see [46]. URV and ULV decompositions [68, 66] had their genesis in the author's unsuccessful attempt to update a rank-revealing QR factorization. A refinement step, which tends to decrease the size of the off-diagonal elements has been analyzed in [53] (see also <ref> [56, 15, 28] </ref>). A parallel implementation of the updating algorithm is described in [69]. The methods treated here are not the only ones for revealing rank. For example, methods based on the Lanczos algorithm have been proposed for the case where the rank is small [17, 77, 78].
Reference: [57] <author> D. P. O'Leary and J. A. Simmons. </author> <title> A bidiagonalization-regularization procedure for large scale discretizations of ill-posed problems. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 2 </volume> <pages> 474-489, </pages> <year> 1981. </year>
Reference-contexts: Closely related, but of a different flavor, is the problem of regularizing the ill-posed problems which arise from discretizations of compact or unbounded operators <ref> [24, 32, 45, 57, 71, 75] </ref>.
Reference: [58] <author> S. Prasac and B. Chandna. </author> <title> Direction-of-arrival estimation using rank revealing QR factorization. </title> <journal> IEEE Transactions on Signal Processing, </journal> <volume> 39 </volume> <pages> 1224-1229, </pages> <year> 1991. </year> <note> Citation communicated by Per Christian Hansen. </note>
Reference-contexts: Notes and References The problem of rank determination in the presence of error arises in a number of applications: e.g., variable selection in statistics and engineering [7, 47, 61], direction of arrival estimation in signal processing <ref> [1, 58, 59] </ref>, and the projection of ill-conditioned problems onto manifolds where they become well conditioned [51, 21, 22]. In many instances, the original matrix X is not exactly of rank k as we have described it in the introduction.
Reference: [59] <author> R. Roy and T. Kailath. </author> <title> ESPRIT-estimation of signal parameters via rotational invariance techniques. </title> <editor> In F. A. Grunbaum, J. W. Helton, and P. Khargonekar, editors, </editor> <booktitle> Signal Processing Part II: Control Theory and Applications, </booktitle> <pages> pages 369-411, </pages> <address> New York, </address> <year> 1990. </year> <title> Springer. 18 Rank Determination </title>
Reference-contexts: Notes and References The problem of rank determination in the presence of error arises in a number of applications: e.g., variable selection in statistics and engineering [7, 47, 61], direction of arrival estimation in signal processing <ref> [1, 58, 59] </ref>, and the projection of ill-conditioned problems onto manifolds where they become well conditioned [51, 21, 22]. In many instances, the original matrix X is not exactly of rank k as we have described it in the introduction.
Reference: [60] <author> E. Schmidt. </author> <title> Zur Theorie der linearen und nichtlinearen Integralgleichungen. I Teil. Entwicklung willkurlichen Funktionen nach System vorgeschriebener. </title> <journal> Mathematis-che Annalen, </journal> <volume> 63 </volume> <pages> 433-476, </pages> <year> 1907. </year>
Reference-contexts: The literature on updating and downdating is too voluminous to survey here. The singular value decomposition dates to the last half of the nineteenth century (for a history see [67]). The theorem cited here as Schmidt's theorem <ref> [60] </ref>, is often attributed to Eckart and Young [29], who rediscovered it thirty years later. The popularity of the singular value decomposition in numerical analysis is due to Golub and Kahan [38]. <p> Gram [44] and Schmidt <ref> [60] </ref> orthogonalized series of functions: Gram by determinantal expressions (hence the Gramian matrix) and Schmidt by the now classic algorithm. The use of orthogonal transformations to compute the decomposition is due to Householder [50], Bogert and Burris [9], and Golub [36].
Reference: [61] <author> G. N. Stenbakken, T. M. Souders, and G. W. Stewart. </author> <title> Ambiguity groups and testability. </title> <journal> IEEE Transactions on Instrumentation, </journal> <volume> 38 </volume> <pages> 941-947, </pages> <year> 1989. </year>
Reference-contexts: Notes and References The problem of rank determination in the presence of error arises in a number of applications: e.g., variable selection in statistics and engineering <ref> [7, 47, 61] </ref>, direction of arrival estimation in signal processing [1, 58, 59], and the projection of ill-conditioned problems onto manifolds where they become well conditioned [51, 21, 22]. In many instances, the original matrix X is not exactly of rank k as we have described it in the introduction.
Reference: [62] <author> G. W. Stewart. </author> <title> The efficient generation of random orthogonal matrices with an application to condition estimators. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 17 </volume> <pages> 403-404, </pages> <year> 1980. </year>
Reference-contexts: The name QR decomposition is from Francis's QR algorithm [33], which uses the decomposition. Although pivoting for column size while computing the QR decomposition has long been regarded as a reliable way of determining rank (e.g., see <ref> [39, 62] </ref>), Chan [13] was the first to give bounds for a rank-revealing decomposition (the descriptive phrase "rank revealing" was coined by him). Unfortunately, the bounds were exponential in the defect p k in the rank.
Reference: [63] <author> G. W. Stewart. </author> <title> On the invariance of perturbed null vectors under column scaling. </title> <journal> Numerische Mathematik, </journal> <volume> 44 </volume> <pages> 61-65, </pages> <year> 1984. </year>
Reference-contexts: in areas like signal processing, where the errors are relatively large, or statistics, where there is a vast literature under the heading "errors in the variables" [2, 3, 5, 4, 8, 20, 34, 48, 65], or numerical analysis, where there is a growing literature under the heading "total least squares" <ref> [31, 41, 63, 72, 74] </ref>. Although the updating of least squares solutions goes back to Gauss [35], the updating of decompositions seems to have arisen in linear program, where the inverse basis matrix must be updated [19].
Reference: [64] <author> G. W. Stewart. </author> <title> Rank degeneracy. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 5 </volume> <pages> 403-413, </pages> <year> 1984. </year>
Reference-contexts: More generally the singular values 2 k+1 ; : : : 2 p are approximately the eigenvalues of a Wishart matrix, whose distributions are known (e.g., see [26, 16]). The problem of poorly scaled errors is closely related to the problem of artificial ill-conditioning, which is discussed in <ref> [64] </ref>. The equal error scaling advocated there is the equivalent of noise whitening. One solution to the problem of constrained errors is to project the problem onto a submanifold where the errors can be whitened [23, 37, 73]. The results on testing QRP decompositions appear to be new.
Reference: [65] <author> G. W. Stewart. </author> <title> Collinearity and least squares regression. </title> <journal> Statistical Science, </journal> <volume> 2 </volume> <pages> 68-100, </pages> <year> 1987. </year>
Reference-contexts: The fact that something must be known about the errors in order to make statements about rank is a commonplace in areas like signal processing, where the errors are relatively large, or statistics, where there is a vast literature under the heading "errors in the variables" <ref> [2, 3, 5, 4, 8, 20, 34, 48, 65] </ref>, or numerical analysis, where there is a growing literature under the heading "total least squares" [31, 41, 63, 72, 74].
Reference: [66] <author> G. W. Stewart. </author> <title> Updating a rank-revealing ULV decomposition. </title> <type> Technical Report CS-TR 2627, </type> <institution> Department of Computer Science, University of Maryland, </institution> <year> 1991. </year>
Reference-contexts: Although the first such algorithm is found in [43], it was LINPACK [27] that popularized the idea. For a survey with references see [46]. URV and ULV decompositions <ref> [68, 66] </ref> had their genesis in the author's unsuccessful attempt to update a rank-revealing QR factorization. A refinement step, which tends to decrease the size of the off-diagonal elements has been analyzed in [53] (see also [56, 15, 28]). A parallel implementation of the updating algorithm is described in [69].
Reference: [67] <author> G. W. Stewart. </author> <title> On the early history of the singular value decomposition. </title> <type> Technical Report CS-TR-2848, </type> <institution> Department of Computer Science, University of Maryland, College Park, </institution> <year> 1992. </year>
Reference-contexts: The literature on updating and downdating is too voluminous to survey here. The singular value decomposition dates to the last half of the nineteenth century (for a history see <ref> [67] </ref>). The theorem cited here as Schmidt's theorem [60], is often attributed to Eckart and Young [29], who rediscovered it thirty years later. The popularity of the singular value decomposition in numerical analysis is due to Golub and Kahan [38].
Reference: [68] <author> G. W. Stewart. </author> <title> An updating algorithm for subspace tracking. </title> <journal> IEEE Transactions on Signal Processing, </journal> <volume> 40 </volume> <pages> 1535-1541, </pages> <year> 1992. </year>
Reference-contexts: Although the first such algorithm is found in [43], it was LINPACK [27] that popularized the idea. For a survey with references see [46]. URV and ULV decompositions <ref> [68, 66] </ref> had their genesis in the author's unsuccessful attempt to update a rank-revealing QR factorization. A refinement step, which tends to decrease the size of the off-diagonal elements has been analyzed in [53] (see also [56, 15, 28]). A parallel implementation of the updating algorithm is described in [69].
Reference: [69] <author> G. W. Stewart. </author> <title> Updating URV decompositions in parallel. </title> <type> Technical Report CS-TR-2880, </type> <institution> Department of Computer Science, University of Maryland, College Park, </institution> <year> 1992. </year>
Reference-contexts: A refinement step, which tends to decrease the size of the off-diagonal elements has been analyzed in [53] (see also [56, 15, 28]). A parallel implementation of the updating algorithm is described in <ref> [69] </ref>. The methods treated here are not the only ones for revealing rank. For example, methods based on the Lanczos algorithm have been proposed for the case where the rank is small [17, 77, 78]. The perturbation of singular values, including Fischer's theorem, is surveyed in [70].
Reference: [70] <author> G. W. Stewart and J.-G. Sun. </author> <title> Matrix Perturbation Theory. </title> <publisher> Academic Press, </publisher> <address> Boston, </address> <year> 1990. </year>
Reference-contexts: The methods treated here are not the only ones for revealing rank. For example, methods based on the Lanczos algorithm have been proposed for the case where the rank is small [17, 77, 78]. The perturbation of singular values, including Fischer's theorem, is surveyed in <ref> [70] </ref>. The relation (8) is a consequence of theorems in [53]. The approach to rank determination followed here is rather crude, suitable for the crude models and data one Rank Determination 13 can expect in practice.
Reference: [71] <author> A. N. Tihonov. </author> <title> Regularization of incorrectly posed problems. </title> <journal> Soviet Mathematics, </journal> <volume> 4 </volume> <pages> 1624-1627, </pages> <year> 1963. </year>
Reference-contexts: Closely related, but of a different flavor, is the problem of regularizing the ill-posed problems which arise from discretizations of compact or unbounded operators <ref> [24, 32, 45, 57, 71, 75] </ref>.
Reference: [72] <author> S. Van Huffel. </author> <title> Analysis of the Total Least Squares Problem and Its Use in Parameter Estimation. </title> <type> PhD thesis, </type> <institution> Katholeike Universiteit Leuven, </institution> <year> 1987. </year> <title> Rank Determination 19 </title>
Reference-contexts: in areas like signal processing, where the errors are relatively large, or statistics, where there is a vast literature under the heading "errors in the variables" [2, 3, 5, 4, 8, 20, 34, 48, 65], or numerical analysis, where there is a growing literature under the heading "total least squares" <ref> [31, 41, 63, 72, 74] </ref>. Although the updating of least squares solutions goes back to Gauss [35], the updating of decompositions seems to have arisen in linear program, where the inverse basis matrix must be updated [19].
Reference: [73] <author> S. Van Huffel and J. Vandervalle. </author> <title> Analysis and properties of the generalized total least squares problem AX B when some or all columns in A are subject to error. </title> <journal> SIAM Journal on Matrix Analysis and Applications, </journal> <volume> 10 </volume> <pages> 294-315, </pages> <year> 1989. </year>
Reference-contexts: The equal error scaling advocated there is the equivalent of noise whitening. One solution to the problem of constrained errors is to project the problem onto a submanifold where the errors can be whitened <ref> [23, 37, 73] </ref>. The results on testing QRP decompositions appear to be new. The consequence of (11) and (12) are that kG 22 k 2 F will tend to be larger than ~ 2 k .
Reference: [74] <author> S. Van Huffel and J. Vandewalle. </author> <title> The Total Least Squares Problem: Computational Aspects and Analysis. </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1991. </year>
Reference-contexts: in areas like signal processing, where the errors are relatively large, or statistics, where there is a vast literature under the heading "errors in the variables" [2, 3, 5, 4, 8, 20, 34, 48, 65], or numerical analysis, where there is a growing literature under the heading "total least squares" <ref> [31, 41, 63, 72, 74] </ref>. Although the updating of least squares solutions goes back to Gauss [35], the updating of decompositions seems to have arisen in linear program, where the inverse basis matrix must be updated [19].
Reference: [75] <author> J. M. Varah. </author> <title> A practical examination of some numerical methods for linear discrete ill-posed problems. </title> <journal> SIAM Review, </journal> <volume> 21 </volume> <pages> 100-111, </pages> <year> 1979. </year>
Reference-contexts: Closely related, but of a different flavor, is the problem of regularizing the ill-posed problems which arise from discretizations of compact or unbounded operators <ref> [24, 32, 45, 57, 71, 75] </ref>.
Reference: [76] <author> J. H. Wilkinson and C. Reinsch. </author> <title> Handbook for Automatic Computation. Vol. II Linear Algebra. </title> <publisher> Springer, </publisher> <address> New York, </address> <year> 1971. </year>
Reference: [77] <author> G. Xu and T. Kailath. </author> <title> Fast signal-subspace decomposition | Part I: Ideal covari-ance matrices. </title> <note> Manuscript submitted to ASSP. </note> <institution> Information Systems Laboratory, Stanford University, </institution> <year> 1990. </year>
Reference-contexts: A parallel implementation of the updating algorithm is described in [69]. The methods treated here are not the only ones for revealing rank. For example, methods based on the Lanczos algorithm have been proposed for the case where the rank is small <ref> [17, 77, 78] </ref>. The perturbation of singular values, including Fischer's theorem, is surveyed in [70]. The relation (8) is a consequence of theorems in [53]. The approach to rank determination followed here is rather crude, suitable for the crude models and data one Rank Determination 13 can expect in practice.
Reference: [78] <author> G. Xu and T. Kailath. </author> <title> Fast signal-subspace decomposition | Part II: Sample co-variance matrices. </title> <note> Manuscript submitted to ASSP. </note> <institution> Information Systems Laboratory, Stanford University, </institution> <year> 1990. </year>
Reference-contexts: A parallel implementation of the updating algorithm is described in [69]. The methods treated here are not the only ones for revealing rank. For example, methods based on the Lanczos algorithm have been proposed for the case where the rank is small <ref> [17, 77, 78] </ref>. The perturbation of singular values, including Fischer's theorem, is surveyed in [70]. The relation (8) is a consequence of theorems in [53]. The approach to rank determination followed here is rather crude, suitable for the crude models and data one Rank Determination 13 can expect in practice.
References-found: 78

