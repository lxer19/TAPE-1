URL: http://www.cs.utah.edu/~cs680-2/proc_coupling.ps
Refering-URL: http://www.cs.utah.edu/~cs680-2/index.html
Root-URL: 
Title: Processor Coupling: Integrating Compile Time and Runtime Scheduling for Parallelism  
Author: Stephen W. Keckler and William J. Dally 
Address: Cambridge, Massachusetts 02139  
Affiliation: Artificial Intelligence Laboratory and Laboratory for Computer Science Massachusetts Institute of Technology  
Abstract: The technology to implement a single-chip node composed of 4 high-performance floating-point ALUs will be available by 1995. This paper presents processor coupling, a mechanism for controlling multiple ALUs to exploit both instruction-level and inter-thread parallelism, by using compile time and runtime scheduling. The compiler statically schedules individual threads to discover available intra-thread instruction-level parallelism. The runtime scheduling mechanism interleaves threads, exploiting inter-thread parallelism to maintain high ALU utilization. ALUs are assigned to threads on a cycle by cycle basis, and several threads can be active concurrently. We provide simulation results demonstrating that, on four simple numerical benchmarks, processor coupling achieves better performance than purely statically scheduled or multi-processor machine organizations. We examine how performance is affected by restricted communication between ALUs and by long memory latencies. We also present an implementation and feasibility study of a processor coupled node. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> ALVERSON, R., CALLAHAN, D., CUMMINGS, D., KOBLENZ, B., PORTERFIELD, A., AND SMITH, B. </author> <title> The Tera computer system. </title> <booktitle> In Proceedings of the International Conference on Supercomputing(June 1990), </booktitle> <pages> pp. 1-6. </pages>
Reference-contexts: Like the registers, each memory location has a valid bit. Different flavors of loads and stores are used to access memory locations. The capabilities of memory reference operations are similar to those in the Tera machine description <ref> [1] </ref> and are summarized in Table 1. These mechanisms can be used to build producer-consumer relationships, atomic updates, semaphores, and other types of synchronization schemes. 3 On-chip memory is used as a cache and is interleaved into banks to allow concurrent access to multiple memory locations.
Reference: [2] <author> ARVIND, AND CULLER, D. E. </author> <title> Dataflow architectures. </title> <booktitle> Annual Reviews in Computer Science 1 (February 1986), </booktitle> <pages> 225-53. </pages>
Reference-contexts: Using multiple threads to hide memory latencies and pipeline delays has been explored in [7, 8, 13, 17]. Dataflow and hybrid dataflow approaches such as <ref> [2, 9] </ref> have decomposed programs into large numbers of threads consisting of one or a few instructions which are dynamically scheduled. Fisher and Rau summarize many instruction-level parallelism techniques in [6].
Reference: [3] <author> COLWELL, R. P., HALL, W. E., JOSHI, C. S., PAPWORTH, D. B., ROD-MAN, P. K., AND TORNES, J. E. </author> <title> Architecture and implementation of a VLIW supercomputer. </title> <booktitle> In Proceedings of Supercomputing '90 (November 1990), </booktitle> <publisher> IEEE Computer Society Press, </publisher> <pages> pp. 910-919. </pages>
Reference-contexts: Additional information and further analysis of processor coupling can be found in [12]. Related Work Processor coupling incorporates ideas from research in compile time scheduling, multiple instruction issue architectures, multi-threaded machines, and runtime scheduling. Very long instruction word (VLIW) processors such as the Multiflow Trace series <ref> [3] </ref> use only compile time scheduling to manage instruction-level parallelism and resource use. Superscalar processors execute multiple instructions simultaneously by relying upon runtime scheduling mechanisms to determine data dependencies [18, 10].
Reference: [4] <author> COLWELL, R. P., NIX, R. P., O'DONNELL, J. J., PAPWORTH, D. B., AND RODMAN, P. K. </author> <title> A VLIW architecture for a trace scheduling compiler. </title> <journal> IEEE Transactions on Computers 37, </journal> <month> 8 (August </month> <year> 1988), </year> <pages> 967-979. </pages>
Reference-contexts: Several threads may be active simultaneously sharing use of the function unit pipelines. Instruction level parallelism within a single thread is exploited using static scheduling techniques similar to those demonstrated in the Multiflow Trace system <ref> [4] </ref>. At runtime, the hardware scheduling mechanism interleaves several threads exploiting inter-thread parallelism to maintain high utilization of function units. function units. The operations from threads A, B, and C are scheduled independently at compile time as shown in the top of the figure.
Reference: [5] <author> ELLIS, J. R. Bulldog: </author> <title> A Compiler for VLIW Architectures. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: A compiler can be used to extract the maximum amount of statically available instruction-level parallelism from a program fragment. However, compile time scheduling is limited by unpredictable memory latencies and by some dependencies, such as data dependent array references, which cannot be statically determined. Furthermore, although trace scheduling <ref> [5] </ref> and software pipelining techniques [14] can be used, branch boundaries tend to limit the number of operations that can be scheduled simultaneously. By interleaving multiple threads, the hardware runtime scheduling mechanisms of processor coupling address the limits of static scheduling due to dynamic program behavior.
Reference: [6] <author> FISHER, J. A., AND RAU, B. R. </author> <title> Instruction-level parallel processing. </title> <booktitle> Science 253 (September 1991), </booktitle> <pages> 1233-1241. </pages>
Reference-contexts: Dataflow and hybrid dataflow approaches such as [2, 9] have decomposed programs into large numbers of threads consisting of one or a few instructions which are dynamically scheduled. Fisher and Rau summarize many instruction-level parallelism techniques in <ref> [6] </ref>. Context of Processor Coupling Processor coupling is useful in machines ranging from workstations based upon a single multi-ALU node to massively parallel machines such as the MIT M-Machine, which is currently being designed.
Reference: [7] <author> GUPTA, A., AND WEBER, W.-D. </author> <title> Exploring the benefits of multiple hardware contexts in a multiprocessor architecture: preliminary results. </title> <booktitle> In Proceedings of the 16th Annual Symposium on Computer Architecture (May 1989), IEEE, </booktitle> <pages> pp. 273-280. </pages>
Reference-contexts: The proposed XIMD [19] architecture employs compile time techniques to statically schedule instructions as well as threads, but does not dynamically interleave multiple thread execution. Using multiple threads to hide memory latencies and pipeline delays has been explored in <ref> [7, 8, 13, 17] </ref>. Dataflow and hybrid dataflow approaches such as [2, 9] have decomposed programs into large numbers of threads consisting of one or a few instructions which are dynamically scheduled. Fisher and Rau summarize many instruction-level parallelism techniques in [6].
Reference: [8] <author> HALSTEAD, R. H., AND FUJITA, T. MASA: </author> <title> A multithreaded processor architecture for parallel symbolic computing. </title> <booktitle> In Proceedings of the 15th Annual Symposium on Computer Architecture (1988), IEEE, </booktitle> <pages> pp. 443-451. </pages>
Reference-contexts: The proposed XIMD [19] architecture employs compile time techniques to statically schedule instructions as well as threads, but does not dynamically interleave multiple thread execution. Using multiple threads to hide memory latencies and pipeline delays has been explored in <ref> [7, 8, 13, 17] </ref>. Dataflow and hybrid dataflow approaches such as [2, 9] have decomposed programs into large numbers of threads consisting of one or a few instructions which are dynamically scheduled. Fisher and Rau summarize many instruction-level parallelism techniques in [6].
Reference: [9] <author> IANUCCI, R. A. </author> <title> Toward a dataflow/Von Neumann hybrid architecture. </title> <booktitle> In Proceedings of the 15th Annual Symposium on Computer Architecture (1988), IEEE, </booktitle> <pages> pp. 131-140. </pages>
Reference-contexts: Using multiple threads to hide memory latencies and pipeline delays has been explored in [7, 8, 13, 17]. Dataflow and hybrid dataflow approaches such as <ref> [2, 9] </ref> have decomposed programs into large numbers of threads consisting of one or a few instructions which are dynamically scheduled. Fisher and Rau summarize many instruction-level parallelism techniques in [6].
Reference: [10] <author> JOHNSON, W. M. </author> <title> Superscalar Microprocessor Design. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1991. </year>
Reference-contexts: Very long instruction word (VLIW) processors such as the Multiflow Trace series [3] use only compile time scheduling to manage instruction-level parallelism and resource use. Superscalar processors execute multiple instructions simultaneously by relying upon runtime scheduling mechanisms to determine data dependencies <ref> [18, 10] </ref>. The proposed XIMD [19] architecture employs compile time techniques to statically schedule instructions as well as threads, but does not dynamically interleave multiple thread execution. Using multiple threads to hide memory latencies and pipeline delays has been explored in [7, 8, 13, 17].
Reference: [11] <author> JOUPPI, N. P., AND WALL, D. W. </author> <title> Available instruction level parallelism for superscalar and superpipelined machines. </title> <booktitle> In Proceedings of the Third International Conference on Architectural Support for Programming Languages and Operating Systems (April 1989), </booktitle> <publisher> ACM Press, </publisher> <pages> pp. 272-282. </pages>
Reference-contexts: One possible use of these multiple arithmetic units is to organize them in a single processor to exploit instruction-level parallelism. Controlling concurrent function units presents a challenge. Applications exhibit an uneven amount of instruction-level parallelism during their execution <ref> [11] </ref>.
Reference: [12] <author> KECKLER, S. W. </author> <title> A coupled multi-ALU processing node for a highly parallel computer. </title> <type> Tech. Rep. 1355, </type> <institution> Massachusetts Institute of Technology Artificial Intelligence Laboratory, </institution> <address> Cambridge, MA 02139, </address> <year> 1992. </year>
Reference-contexts: By interleaving multiple threads, the hardware runtime scheduling mechanisms of processor coupling address the limits of static scheduling due to dynamic program behavior. Additional information and further analysis of processor coupling can be found in <ref> [12] </ref>. Related Work Processor coupling incorporates ideas from research in compile time scheduling, multiple instruction issue architectures, multi-threaded machines, and runtime scheduling. Very long instruction word (VLIW) processors such as the Multiflow Trace series [3] use only compile time scheduling to manage instruction-level parallelism and resource use.
Reference: [13] <author> KUEHN, J. T., AND SMITH, B. J. </author> <title> The Horizon supercomputing system: </title> <booktitle> Architecture and software. In Proceedings of Supercomputing '88 (Orlando, </booktitle> <address> Florida, </address> <month> November </month> <year> 1988), </year> <pages> pp. 28-34. </pages>
Reference-contexts: The proposed XIMD [19] architecture employs compile time techniques to statically schedule instructions as well as threads, but does not dynamically interleave multiple thread execution. Using multiple threads to hide memory latencies and pipeline delays has been explored in <ref> [7, 8, 13, 17] </ref>. Dataflow and hybrid dataflow approaches such as [2, 9] have decomposed programs into large numbers of threads consisting of one or a few instructions which are dynamically scheduled. Fisher and Rau summarize many instruction-level parallelism techniques in [6].
Reference: [14] <author> LAM, M. </author> <title> Software pipelining: An effective scheduling technique for VLIW machines. </title> <booktitle> In ACM Sigplan '88 Conference on Programming Language Design and Implementation (1988), </booktitle> <pages> pp. 318-328. </pages>
Reference-contexts: However, compile time scheduling is limited by unpredictable memory latencies and by some dependencies, such as data dependent array references, which cannot be statically determined. Furthermore, although trace scheduling [5] and software pipelining techniques <ref> [14] </ref> can be used, branch boundaries tend to limit the number of operations that can be scheduled simultaneously. By interleaving multiple threads, the hardware runtime scheduling mechanisms of processor coupling address the limits of static scheduling due to dynamic program behavior.
Reference: [15] <author> NUTH, P. R., AND DALLY, W. J. </author> <title> A mechanism for efficient context switching. </title> <booktitle> In Proceedings of the International Conference on Computer Design (October 1991), IEEE, </booktitle> <pages> pp. 301-304. </pages>
Reference-contexts: A thread's register set is distributed over all of the clusters that it uses. The combined register set in each cluster can be implemented as separate register files or as a collection of virtually mapped registers <ref> [15] </ref>. Communication between threads takes place through the shared memory on the node; synchronization between threads is on the presence or absence of data in a memory location. Each function unit determines independently, through examination of dynamic data dependencies, the next operation to issue.
Reference: [16] <author> SADAYAPPAN, P., AND VISVANATHAN, V. </author> <title> Circuit simulation on shared memory multiprocessors. </title> <journal> IEEE Transactions on Computers 37, </journal> <month> 12 (December </month> <year> 1988), </year> <pages> 1634-1642. </pages>
Reference-contexts: Like LUD, no loops are unrolled. Although they are small and well contained problems, these benchmark programs can be used as building blocks for larger numerical applications. For example, the compute intensive portions of a circuit simulator such as SPICE include a model evaluator and sparse matrix solver <ref> [16] </ref>. Using these benchmarks, we first compare the performance of the different types of simulated machines. Further experiments explore issues concerning restricted connectivity between function units, variable memory latencies, and different mixes of function units. Baseline Comparisons The baseline machine consists of four arithmetic clusters and two branch clusters.
Reference: [17] <author> SMITH, B. J. </author> <title> Architecture and applications of the HEP multiprocessor computer system. </title> <booktitle> SPIE 298 (1981), </booktitle> <pages> 241-248. </pages>
Reference-contexts: The proposed XIMD [19] architecture employs compile time techniques to statically schedule instructions as well as threads, but does not dynamically interleave multiple thread execution. Using multiple threads to hide memory latencies and pipeline delays has been explored in <ref> [7, 8, 13, 17] </ref>. Dataflow and hybrid dataflow approaches such as [2, 9] have decomposed programs into large numbers of threads consisting of one or a few instructions which are dynamically scheduled. Fisher and Rau summarize many instruction-level parallelism techniques in [6].
Reference: [18] <author> TOMASULO, R. </author> <title> An efficient algorithm for exploiting multiple arithmetic units. </title> <journal> IBM Journal 11 (January 1967), </journal> <pages> 25-33. </pages>
Reference-contexts: Very long instruction word (VLIW) processors such as the Multiflow Trace series [3] use only compile time scheduling to manage instruction-level parallelism and resource use. Superscalar processors execute multiple instructions simultaneously by relying upon runtime scheduling mechanisms to determine data dependencies <ref> [18, 10] </ref>. The proposed XIMD [19] architecture employs compile time techniques to statically schedule instructions as well as threads, but does not dynamically interleave multiple thread execution. Using multiple threads to hide memory latencies and pipeline delays has been explored in [7, 8, 13, 17]. <p> The operation buffer can hold one pending operation from each active thread. The operation buffer maintains the status for each pending operation to determine when that operation is ready to issue. Its function is similar to that of a reservation station <ref> [18] </ref>. An operation is enabled to issue when all of its source registers are ready and when all operations from previous instructions in that thread have issued. A pending operation's status is updated as source registers become available and as operations from previous instructions complete.
Reference: [19] <author> WOLFE, A., AND SHEN, J. P. </author> <title> A variable instruction stream extension to the VLIW architecture. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (April 1991), </booktitle> <publisher> ACM Press, </publisher> <pages> pp. 2-14. 12 </pages>
Reference-contexts: Very long instruction word (VLIW) processors such as the Multiflow Trace series [3] use only compile time scheduling to manage instruction-level parallelism and resource use. Superscalar processors execute multiple instructions simultaneously by relying upon runtime scheduling mechanisms to determine data dependencies [18, 10]. The proposed XIMD <ref> [19] </ref> architecture employs compile time techniques to statically schedule instructions as well as threads, but does not dynamically interleave multiple thread execution. Using multiple threads to hide memory latencies and pipeline delays has been explored in [7, 8, 13, 17].
References-found: 19

