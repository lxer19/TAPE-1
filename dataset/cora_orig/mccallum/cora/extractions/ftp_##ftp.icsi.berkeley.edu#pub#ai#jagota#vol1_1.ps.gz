URL: ftp://ftp.icsi.berkeley.edu/pub/ai/jagota/vol1_1.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/~jagota/NCS/vol1.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Probabilistic Analysis of Learning in Artificial Neural Networks: The PAC Model and its Variants  
Author: Martin Anthony 
Note: Contents  
Address: Street, London WC2A 2AE, UK  
Affiliation: Department of Mathematics, The London School of Economics and Political Science, Houghton  
Abstract: There are a number of mathematical approaches to the study of learning and generalization in artificial neural networks. Here we survey the `probably approximately correct' (PAC) model of learning and some of its variants. These models provide a probabilistic framework for the discussion of generalization and learning. This survey concentrates on the sample complexity questions in these models; that is, the emphasis is on how many examples should be used for training. Computational complexity considerations are briefly discussed for the basic PAC model. Throughout, the importance of the Vapnik-Chervonenkis dimension is highlighted. Particular attention is devoted to describing how the probabilistic models apply in the context of neural network learning, both for networks with binary-valued output and for networks with real-valued output. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> N. Alon, S. Ben-David, N. Cesa-Bianchi, and D. Haussler. </author> <title> Scale-sensitive dimensions, uniform convergence, and learnability. </title> <booktitle> In Proceedings of the Symposium on Foundations of Computer Science. </booktitle> <publisher> IEEE Press, </publisher> <year> 1993. </year> <note> To appear, Journal of the ACM. </note>
Reference-contexts: The Heaviside function H is just one example of an activation function and in practice it is often replaced by a sigmoid function, f . This is some `smooth' function from R to <ref> [0; 1] </ref>, with f (x) ! 0 as x ! 1 and f (x) ! 1 as x ! 1. The best-known example is the standard sigmoid function, given by (x) = 1=(1+ e x ). <p> In this case, one obtains bounds on the sample length m 0 (ffi; *) for the UCE property, and the sample complexity of ^ L, which depend on pdim (H). Recently, Alon et al. <ref> [1] </ref> have determined a necessary and sufficient condition for H to have the UCE property (for loss function l). This condition is in many cases weaker than finite pseudo-dimension of l H . Their result will be described later. <p> Much attention has been on `learning a good model of probability' of a p-concept and it is this problem we shall discuss here. A p-concept (or probabilistic concept) is a function t from X to the interval <ref> [0; 1] </ref>. The value t (x) is meant to represent a probability. A motivating example given by Kearns and Schapire is that in which the example space x encodes a set of meteorological measurements and t (x) is the probability, given measurements x, that rain will follow. <p> is possible to give a more general definition of p-concept learning, in which the aim is not to find a good model of probability but to find a hypothesis which almost minimizes the linear or quadratic loss with respect to a target distribution P on X fi f0; 1g; see <ref> [1] </ref>, for instance. In their paper, Kearns and Schapire gave examples of efficient algorithms for learning particular classes of p-concepts. One example they gave is the class ND of all non-decreasing functions from [0; 1] to [0; 1]. <p> In their paper, Kearns and Schapire gave examples of efficient algorithms for learning particular classes of p-concepts. One example they gave is the class ND of all non-decreasing functions from <ref> [0; 1] </ref> to [0; 1]. They show that if H has finite pseudo-dimension then H is learnable (by an algorithm which near-minimizes observed quadratic loss, as in Theorem 12.3). <p> In their paper, Kearns and Schapire gave examples of efficient algorithms for learning particular classes of p-concepts. One example they gave is the class ND of all non-decreasing functions from <ref> [0; 1] </ref> to [0; 1]. They show that if H has finite pseudo-dimension then H is learnable (by an algorithm which near-minimizes observed quadratic loss, as in Theorem 12.3). <p> Kearns and Schapire also obtain a lower bound on the sample complexity of a p-concept learning algorithm, involving a `scale-sensitive' version of the pseudo-dimension (to be described below). Alon et al. <ref> [1] </ref> proved a very general result which shows that the parameter involved in the lower bound of Kearns and Schapire is the crucial one for the analysis of p-concept learnability. This parameter, the fl-dimension, is formally defined as follows. <p> Definition 14.2 For fl &gt; 0, we say that a sample (x 1 ; : : : ; x d ) is fl-shattered by H if the following holds: there are r 1 ; r 2 ; : : : ; r d in <ref> [0; 1] </ref> such that for each b 2 f0; 1g d there is h b 2 H with h b (x i ) &gt; r i + fl if b i = 1; h b (x i ) &lt; r i fl if b i = 0: The fl-dimension of H, <p> It is possible, as in the case of ND, for dim fl (H) to be finite for all fl but for pdim (H) to be infinite. This scale-sensitive dimension has been bounded for classes of neural networks by Gurvits and Koiran [54] and Bartlett [23]. Alon et al. <ref> [1] </ref> proved the following result. (The necessity was proved earlier by Kearns and Schapire [70].) Theorem 14.3 Let H be a class of p-concepts. Then H is learnable (in the p-concept model) if and only if dim fl (H) is finite for all fl &gt; 0. <p> Alon et al. <ref> [1] </ref> prove some very general results. A corollary of their main result is the following charac terisation of spaces having the UCE property with respect to a loss function l. Theorem 14.4 Let H be a set of functions from X to R. <p> The result follows. ut Definition 14.6 is very similar to the definition of p-concept learning. If H is a set of functions from X to <ref> [0; 1] </ref>, it is clear that the p-concept learning of H (by H) is more difficult than the learning of H (by H) with a good model: in learning with a good model, if x is an example in the training sample, then the learning algorithm receives the value t (x),
Reference: [2] <author> D. Angluin. </author> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 319-342, </pages> <month> Apr. </month> <year> 1988. </year>
Reference-contexts: In the next few sections, we present a theory which shows that, in many such cases, it is possible. However, for the moment, consider the following informal arguments. 3 This is not exactly the definition given by Valiant. 4 The terminology `probably approximately correct' is due to Angluin <ref> [2] </ref>. <p> We refer the reader to [94, 57] for details. Another very important variant is that in which the learning algorithm can ask questions concerning, for example, the classification of a chosen example; see, for example <ref> [2, 28, 53] </ref>. Such `query learning' is an active area of research, and the paper by Angluin [3] provides a good survey.
Reference: [3] <author> D. Angluin. </author> <title> Computational learning theory: survey and selected bibliography. </title> <booktitle> In Proc. 24th Annu. ACM Sympos. Theory Comput., </booktitle> <pages> pages 351-369. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1992. </year>
Reference-contexts: Another very important variant is that in which the learning algorithm can ask questions concerning, for example, the classification of a chosen example; see, for example [2, 28, 53]. Such `query learning' is an active area of research, and the paper by Angluin <ref> [3] </ref> provides a good survey.
Reference: [4] <author> D. Angluin and P. Laird. </author> <title> Learning from noisy examples. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 343-370, </pages> <year> 1988. </year>
Reference-contexts: As already indicated, it can be useful when the target `concept' is not a function. It can also be useful when there is `classification noise' (see <ref> [4] </ref>), that is, where there is an underlying target function, but the randomly chosen examples have their labels `flipped' occasionally. This corrupts the training data and results in a stochastic concept.
Reference: [5] <author> M. Anthony. </author> <title> Uniform Convergence and Learnability. </title> <type> PhD thesis, </type> <institution> University of London (London School of Economics and Political Science), </institution> <month> Feb. </month> <year> 1991. </year> <note> A revised version appears as London School of Economics Mathematics Preprint LSE-MPS-11, </note> <month> May </month> <year> 1991. </year>
Reference-contexts: Their results were subsequently improved in [12, 105]. The precise result presented here is a slight improvement of a special case of a result of Vapnik, and the proof is based on one from <ref> [5] </ref>. Suppose that H is a set of functions from X to f0; 1g, and let S be the cartesian product X fi f0; 1g. <p> to construct a probability measure 7 P on S such that for any (measurable) subset A of X, P (f (x; t (x)) : x 2 Ag) = (A) and P (f (x; y) : x 2 A; y 6= t (x)g) = 0: The details may be found in <ref> [5, 19] </ref>. We shall see later that this more general framework of a probability distribution on X fi f0; 1g is useful. The following result is a consequence of Theorem 5.1, together with Sauer's Lemma. <p> The following result is a consequence of Theorem 5.1, together with Sauer's Lemma. A better sample size bound, with smaller constants, improving a previous bound from [37], has been obtained in <ref> [5, 19] </ref>. However, the following result is adequate for our purposes. Theorem 5.2 Let H be a hypothesis space of f0; 1g-valued functions defined on an input space X. <p> The details, which are omitted here, may be found in <ref> [5, 19] </ref>. ut 6 PAC Learning and the VC-Dimension In this section we show that basic PAC learning is characterised by the VC-dimension. More precisely, suppose that L is a (C; H)-learning algorithm and that C H. <p> H with respect to and has polynomial sample complexity. (Note that this is a stronger conclusion than described above, since it says that all consistent learning algorithms learn and have sample complexity polynomial in the relevant parameters, rather than simply that there is some efficient learning algorithm.) Anthony and Shawe-Taylor <ref> [5, 17] </ref> found a further sufficient condition for polynomial sample complexity. Let us say that a sequence fS k g 1 k=1 of subsets of X is non-decreasing if for each k, S k S k+1 . <p> For S X, let HjS denote the set of functions in H restricted to domain S. Suppose that X = [ 1 k=1 S k , where fS k g is non-decreasing and VCdim (HjS k ) k. Anthony and Shawe-Taylor <ref> [5, 17] </ref> proved that if the probability distribution satisfies 1 (S k ) = O (k fi ) for some fi &gt; 0, then any consistent learning algorithm for H learns with respect to and has polynomial sample complexity. <p> This result follows easily from the more general form of Vapnik's result when one takes the events to be the error sets. Details may be found in <ref> [5, 19, 37] </ref>. Neural Computing Surveys 1, 1-47, 1997, http://www.icsi.berkeley.edu/~jagota/NCS 30 they show that in this case, the Natarajan dimension is finite if and only if the graph dimension is finite, so that Natarajan's necessary and sufficient conditions are matching.
Reference: [6] <author> M. Anthony. </author> <title> Classification by polynomial surfaces. Mathematics Preprint Series LSE-MPS-39, </title> <institution> London School of Economics, </institution> <month> Oct. </month> <year> 1992. </year> <note> Revised version appears Discrete Applied Mathematics, </note> <month> 61 </month> <year> (1995): </year> <pages> 91-103. </pages>
Reference-contexts: The following result is due to Dudley [42]; we present here a proof from <ref> [6] </ref>. Theorem 4.2 Let V be a real vector space of real-valued functions defined on a set X. Suppose that V has linear dimension d. <p> We have the following result <ref> [6, 16, 15] </ref>, which is proved using Dudley's result, Theorem 4.2. Theorem 7.3 Suppose that N is a polynomial discriminator of degree at most k, on n inputs.
Reference: [7] <author> M. Anthony and P. Bartlett. </author> <title> Function learning from interpolation. Extended abstract in Proceedings EuroCOLT'95, </title> <publisher> Springer-Verlag, </publisher> <year> 1995: </year> <pages> 211-221. </pages> <note> Full version submitted. </note>
Reference-contexts: In [9], it is also shown that finite pseudo-dimension of H is a necessary condition for the conclusion of Theorem 14.8 to hold. Thus the condition described in this theorem is stronger than learnability. However, Anthony and Bartlett <ref> [7] </ref> have shown that finite fl-dimension for all fl is sufficient for a weaker conclusion, as the following theorem shows. Neural Computing Surveys 1, 1-47, 1997, http://www.icsi.berkeley.edu/~jagota/NCS 40 Theorem 14.9 Let H be a set of functions from X to [0; M ], for some M . <p> He has shown that, to within a logarithmic factor, this bound is optimal for a number of spaces H. Anthony and Bartlett <ref> [7] </ref> have shown that finite fl-dimension for all fl is necessary as well as sufficient for the conclusion of Theorem 14.9 to hold. 15 Conclusions and further reading There are many aspects of learning theory not discussed in this work.
Reference: [8] <author> M. Anthony and P. Bartlett. </author> <title> Theory of Learning in Neural Networks (working title). </title> <note> In preparation. To be published by Cambridge University Press. </note>
Reference-contexts: In this part of the article, we discuss some of these. A fuller treatment of these and other variants will be found in the forthcoming book <ref> [8] </ref>. 9 Stochastic Concepts The results presented so far have nothing to say if there is some form of `noise' present during the learning procedure.
Reference: [9] <author> M. Anthony, P. Bartlett, Y. Ishai, and J. Shawe-Taylor. </author> <title> Valid generalisation from approximate interpolation. Combinatorics, </title> <journal> Probability and Computing, </journal> <volume> Volume 5 191-214 (1996). </volume>
Reference-contexts: In <ref> [9, 20, 18] </ref>, the following result is given. This shows that if the hypothesis space has finite pseudo-dimension and a learning algorithm interpolates well enough on the training sample then it is a probably approximately correct algorithm. <p> pdim (H) ln * + ln 1 such that if m m L , the following holds: for any probability distribution on X and any t 2 C, with probability at least 1 ffi, (fx 2 X : jh L (x) t (x)j g) &lt; *: The result presented in <ref> [9] </ref> is more general than this. In that paper, it is shown that to obtain accurate bounds on the sample complexity function m L (and, indeed, bounds depending on ), it is more appropriate to use a scale-sensitive dimension termed the band-dimension, also used in [91]. In [9], it is also <p> result presented in <ref> [9] </ref> is more general than this. In that paper, it is shown that to obtain accurate bounds on the sample complexity function m L (and, indeed, bounds depending on ), it is more appropriate to use a scale-sensitive dimension termed the band-dimension, also used in [91]. In [9], it is also shown that finite pseudo-dimension of H is a necessary condition for the conclusion of Theorem 14.8 to hold. Thus the condition described in this theorem is stronger than learnability.
Reference: [10] <author> M. Anthony and N. Biggs. </author> <title> Computational Learning Theory: An Introduction. </title> <booktitle> Cambridge Tracts in Theoretical Computer Science (30). </booktitle> <publisher> Cambridge University Press, </publisher> <address> Cambridge, UK, </address> <year> 1992. </year> <note> (Reprinted, in paperback, with corrections, </note> <year> 1997.) </year>
Reference-contexts: I have placed emphasis on those topics I find to be of most interest and, consequently, there is more discussion of sample complexity than of computational complexity. There are now a number of books dealing with probabilistic models of learning: the interested reader might consult <ref> [115, 114, 95, 73, 90, 40, 10] </ref> for further information. The first part of this work concerns the basic PAC model, applicable for classification problems. The second part concerns extensions of the basic PAC model, such as those relevant to neural networks with real-valued outputs. <p> Suppose that H is some hypothesis space, and that L is any PAC (C; H)-learning algorithm. Then the sample complexity of L satisfies m L (ffi; *) &gt; max VCdim (C) 1 32* 1 ln 1 for all * 1=8 and ffi 1=100. Proof This proof is from <ref> [10] </ref>. <p> The fact that computational complexity-theoretic hardness results hold for neural networks was first shown by Judd [66]. In this section we shall prove a simple hardness result from <ref> [10, 11] </ref> along the lines of one due to Blum and Rivest [36]. Neural Computing Surveys 1, 1-47, 1997, http://www.icsi.berkeley.edu/~jagota/NCS 23 The network has n inputs and k + 1 computation units (k 1). <p> We have barely touched on the computational complexity of PAC learning and its variants. Further discussion of this may be found, for example, in <ref> [82, 21, 10, 73, 93, 69, 70, 101, 63, 79] </ref>. There are also many more variants of the PAC model which we have not mentioned here. Throughout the discussion here, it has been assumed that there is some fixed hypothesis space.
Reference: [11] <author> M. Anthony and N. Biggs. </author> <title> Computational learning theory for artificial neural networks. </title> <editor> In J. Taylor, editor, </editor> <title> Mathematical Approaches to Neural Networks, </title> <publisher> North Holland Mathematical Library (51), </publisher> <pages> pages 25-62. </pages> <publisher> Elsevier Science Publishers B. V., </publisher> <address> Amsterdam, </address> <year> 1993. </year>
Reference-contexts: The fact that computational complexity-theoretic hardness results hold for neural networks was first shown by Judd [66]. In this section we shall prove a simple hardness result from <ref> [10, 11] </ref> along the lines of one due to Blum and Rivest [36]. Neural Computing Surveys 1, 1-47, 1997, http://www.icsi.berkeley.edu/~jagota/NCS 23 The network has n inputs and k + 1 computation units (k 1).
Reference: [12] <author> M. Anthony, N. Biggs, and J. Shawe-Taylor. </author> <title> The learnability of formal concepts. </title> <booktitle> In Proc. 3rd Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 246-257. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: Results of a similar form, but specifically for learning theory applications, were given by Blumer et al. [37], who were the first to highlight the importance of this area of probability for the theory of PAC learning. Their results were subsequently improved in <ref> [12, 105] </ref>. The precise result presented here is a slight improvement of a special case of a result of Vapnik, and the proof is based on one from [5]. <p> The only other observation needed is that, in this case, P (E h ) is precisely er (h; t). ut The constants in the sample complexity bound of the above theorem can be improved; see <ref> [12, 105] </ref>. We now present a lower bound result, part of which is due to Ehrenfeucht et al. [43] and part of which is due to Blumer et al. [37].
Reference: [13] <author> M. Anthony, G. Brightwell, D. Cohen, and J. Shawe-Taylor. </author> <title> On exact specification by examples. </title> <booktitle> In Proc. 5th Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 311-318. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1992. </year>
Reference-contexts: becomes when there is a `helpful teacher' providing cleverly-chosen examples as training sample. (This is very different from the PAC model in that the training examples are no longer randomly chosen.) Such models of teaching have been studied in the case where the goal is to learn the target exactly <ref> [49, 13, 65, 108, 14] </ref> and in which the goal is to learn an approximation to the target in a probabilistic sense [97, 96].
Reference: [14] <author> M. Anthony, G. Brightwell, and J. Shawe-Taylor. </author> <title> On specifying Boolean functions by labelled examples. </title> <journal> Discrete Applied Mathematics, </journal> <volume> 61 (1995): </volume> <pages> 1-25. </pages>
Reference-contexts: becomes when there is a `helpful teacher' providing cleverly-chosen examples as training sample. (This is very different from the PAC model in that the training examples are no longer randomly chosen.) Such models of teaching have been studied in the case where the goal is to learn the target exactly <ref> [49, 13, 65, 108, 14] </ref> and in which the goal is to learn an approximation to the target in a probabilistic sense [97, 96].
Reference: [15] <author> M. Anthony and S. B. Holden. </author> <title> Quantifying generalisation in linearly weighted neural networks. </title> <journal> Complex Systems 8, </journal> <year> (1994), </year> <pages> 91-114. </pages> <booktitle> Neural Computing Surveys 1, </booktitle> <pages> 1-47, </pages> <year> 1997, </year> <note> http://www.icsi.berkeley.edu/~jagota/NCS 42 </note>
Reference-contexts: We have the following result <ref> [6, 16, 15] </ref>, which is proved using Dudley's result, Theorem 4.2. Theorem 7.3 Suppose that N is a polynomial discriminator of degree at most k, on n inputs. <p> We have the following result, which is obtained using interpolation results of Micchelli [87]. Details of the proof may be found in <ref> [16, 15, 64] </ref>, where further results along the same lines are presented.
Reference: [16] <author> M. Anthony and S. B. Holden. </author> <title> On the power of polynomial discriminators and radial basis function networks. </title> <booktitle> In Proc. 6th Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 158-164. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1993. </year>
Reference-contexts: We have the following result <ref> [6, 16, 15] </ref>, which is proved using Dudley's result, Theorem 4.2. Theorem 7.3 Suppose that N is a polynomial discriminator of degree at most k, on n inputs. <p> We have the following result, which is obtained using interpolation results of Micchelli [87]. Details of the proof may be found in <ref> [16, 15, 64] </ref>, where further results along the same lines are presented.
Reference: [17] <author> M. Anthony and J. Shawe-Taylor. </author> <title> A sufficient condition for polynomial distribution-dependent learn-ability. </title> <note> To appear, Discrete Applied Mathematics.. </note>
Reference-contexts: H with respect to and has polynomial sample complexity. (Note that this is a stronger conclusion than described above, since it says that all consistent learning algorithms learn and have sample complexity polynomial in the relevant parameters, rather than simply that there is some efficient learning algorithm.) Anthony and Shawe-Taylor <ref> [5, 17] </ref> found a further sufficient condition for polynomial sample complexity. Let us say that a sequence fS k g 1 k=1 of subsets of X is non-decreasing if for each k, S k S k+1 . <p> For S X, let HjS denote the set of functions in H restricted to domain S. Suppose that X = [ 1 k=1 S k , where fS k g is non-decreasing and VCdim (HjS k ) k. Anthony and Shawe-Taylor <ref> [5, 17] </ref> proved that if the probability distribution satisfies 1 (S k ) = O (k fi ) for some fi &gt; 0, then any consistent learning algorithm for H learns with respect to and has polynomial sample complexity.
Reference: [18] <author> M. Anthony and J. Shawe-Taylor. </author> <title> Generalising from approximate interpolation. Mathematics Preprint Series LSE-MPS-47, </title> <institution> London School of Economics, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: In <ref> [9, 20, 18] </ref>, the following result is given. This shows that if the hypothesis space has finite pseudo-dimension and a learning algorithm interpolates well enough on the training sample then it is a probably approximately correct algorithm.
Reference: [19] <author> M. Anthony and J. Shawe-Taylor. </author> <title> A result of Vapnik with applications. </title> <journal> Discrete Applied Mathematics, </journal> <volume> 47 </volume> <pages> 207-217, </pages> <year> 1994. </year> <note> Also, technical report CSD-TR-628, </note> <institution> Royal Holloway and Bedford New College, University of London, </institution> <year> 1990. </year>
Reference-contexts: to construct a probability measure 7 P on S such that for any (measurable) subset A of X, P (f (x; t (x)) : x 2 Ag) = (A) and P (f (x; y) : x 2 A; y 6= t (x)g) = 0: The details may be found in <ref> [5, 19] </ref>. We shall see later that this more general framework of a probability distribution on X fi f0; 1g is useful. The following result is a consequence of Theorem 5.1, together with Sauer's Lemma. <p> The following result is a consequence of Theorem 5.1, together with Sauer's Lemma. A better sample size bound, with smaller constants, improving a previous bound from [37], has been obtained in <ref> [5, 19] </ref>. However, the following result is adequate for our purposes. Theorem 5.2 Let H be a hypothesis space of f0; 1g-valued functions defined on an input space X. <p> The details, which are omitted here, may be found in <ref> [5, 19] </ref>. ut 6 PAC Learning and the VC-Dimension In this section we show that basic PAC learning is characterised by the VC-dimension. More precisely, suppose that L is a (C; H)-learning algorithm and that C H. <p> Such techniques have been discussed in [102], for example. Shawe-Taylor [103] has bounded the sample complexity of such learning algorithms by bounding the growth function in a manner similar to that in [27]. His results, following <ref> [19, 104] </ref>, also apply more generally to networks having more than one output node, but we shall not discuss this aspect here. Let N be a feedforward linear threshold network. <p> This result follows easily from the more general form of Vapnik's result when one takes the events to be the error sets. Details may be found in <ref> [5, 19, 37] </ref>. Neural Computing Surveys 1, 1-47, 1997, http://www.icsi.berkeley.edu/~jagota/NCS 30 they show that in this case, the Natarajan dimension is finite if and only if the graph dimension is finite, so that Natarajan's necessary and sufficient conditions are matching. <p> We do not explicitly present a bound on the sample complexity of a consistent learning algorithm as a PAC algorithm, but such a bound could be derived either from the graph dimension bound or (better) from the bound on the growth function (see <ref> [19, 104, 103] </ref>). We first note that there is another way of describing the notion of graph dimension. <p> It follows that if one can bound the quantity H (m), then a bound on the growth function of the graph space, and hence on the graph dimension, can be obtained. This is the technique used in obtaining the following result. (See <ref> [104, 19, 103] </ref> for improvements on this.) Theorem 11.3 Suppose that N is a feedforward linear threshold network having W variable parameters (weights and thresholds) and any number of output units. Let H be the set H N of functions computable by N .
Reference: [20] <author> M. Anthony and J. Shawe-Taylor. </author> <title> Valid generalisation of functions from close approximations on a sample. In Computational Learning Theory: </title> <editor> Euro-COLT'93, (ed. J. Shawe-Taylor and M. Anthony), </editor> <publisher> Oxford University Press, </publisher> <year> 1994. </year>
Reference-contexts: In <ref> [9, 20, 18] </ref>, the following result is given. This shows that if the hypothesis space has finite pseudo-dimension and a learning algorithm interpolates well enough on the training sample then it is a probably approximately correct algorithm.
Reference: [21] <author> P. Auer, P. Long, W. Maass, and G. Woeginger. </author> <title> On the complexity of function learning. </title> <booktitle> In Proc. 6th Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 392-401. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1993. </year>
Reference-contexts: We have barely touched on the computational complexity of PAC learning and its variants. Further discussion of this may be found, for example, in <ref> [82, 21, 10, 73, 93, 69, 70, 101, 63, 79] </ref>. There are also many more variants of the PAC model which we have not mentioned here. Throughout the discussion here, it has been assumed that there is some fixed hypothesis space.
Reference: [22] <author> P. Bartlett. </author> <title> Vapnik-Chervonenkis dimension bounds for two- and three-layer networks. </title> <booktitle> Neural Computation 5 (3): </booktitle> <pages> 371-373, </pages> <year> 1993. </year>
Reference-contexts: In their paper, Baum and Haussler [27] proved that there are threshold networks with one hidden layer (that is, of depth two) and VC-dimension (W ), where W is the number of variable weights. Bartlett <ref> [22] </ref> showed that this holds for all such networks and he showed also, by means of results such as the following, that, for large classes of depth-three networks, the VC-dimension is (W ).
Reference: [23] <author> P. Bartlett. </author> <title> The sample complexity of pattern classification with neural networks: the size of the weights is more important than the size of the network. </title> <type> Technical report, </type> <institution> Department of Systems Engineering, Australian National University, </institution> <month> May </month> <year> 1996. </year>
Reference-contexts: It is possible, as in the case of ND, for dim fl (H) to be finite for all fl but for pdim (H) to be infinite. This scale-sensitive dimension has been bounded for classes of neural networks by Gurvits and Koiran [54] and Bartlett <ref> [23] </ref>. Alon et al. [1] proved the following result. (The necessity was proved earlier by Kearns and Schapire [70].) Theorem 14.3 Let H be a class of p-concepts. Then H is learnable (in the p-concept model) if and only if dim fl (H) is finite for all fl &gt; 0. <p> Something which has not been discussed in any detail here is the use of real-output neural networks for classification. Recent work <ref> [106, 23] </ref> has shown that, here, the scale-sensitive fl-dimension is very useful. Neural Computing Surveys 1, 1-47, 1997, http://www.icsi.berkeley.edu/~jagota/NCS 41 Acknowledgements I thank Peter Bartlett, Nicolo Cesa-Bianchi and Mark Jerrum for comments and suggestions on an earlier version of this article.
Reference: [24] <author> P. Bartlett and R. C. Williamson. </author> <title> The Vapnik-Chervonenkis dimension and pseudodimension of two-layer neural networks with discrete inputs. </title> <booktitle> Neural Computation 8: </booktitle> <pages> 653-656, </pages> <year> 1996. </year>
Reference-contexts: W . (This observation was communicated to me by Wolfgang Maass.) Neural Computing Surveys 1, 1-47, 1997, http://www.icsi.berkeley.edu/~jagota/NCS 19 In practice, computers work to finite accuracy and therefore in any computer simulation of a neural network, the inputs (and weights) are discrete and the following theorem of Bartlett and Williamson <ref> [24] </ref> is applicable. Theorem 7.13 Let D be a positive integer and X D = fD; D + 1; : : : ; D 1; Dg n : Suppose N is a depth-two binary-output neural network having standard sigmoid activation functions on the hidden units. <p> Macintyre and Sontag [84] proved the finiteness of the pseudo-dimension of feedforward neural networks having as activation the standard sigmoid function. However, no explicit bounds were given. Bartlett and Williamson <ref> [24] </ref> obtained the following result, relevant for discrete inputs.
Reference: [25] <author> P. L. Bartlett. </author> <title> Learning with a slowly changing distribution. </title> <booktitle> In Proc. 5th Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 243-252. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1992. </year>
Reference-contexts: Other useful variants not discussed here are those in which the distribution and target are permitted to change a little between observations, as in <ref> [25, 58, 59] </ref>, models of weak learning in which the learner only has to do slightly better than random guessing [51, 100, 60], and variants in which the learning algorithm has access to the predictions of `experts' [38].
Reference: [26] <author> P. L. Bartlett, P. M. Long, and R. C. Williamson. </author> <title> Fat-shattering and the learnability of real-valued functions. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 52(3): </volume> <pages> 434-452, </pages> <year> 1996. </year> <note> Extended abstract in Proceedings of COLT'94. </note>
Reference-contexts: Further, it shows that if there is some noise in the classification of the training sample, then learning `up to the level of the noise' is possible in some cases. (See <ref> [91, 26] </ref> for other results on function learning in the presence of noise.) Theorem 14.8 Let H be a set of functions from X to [0; M ], for some M . Suppose H has finite pseudo dimension and that C H. <p> (H) ln 2 dim ff=8 (H) such that if m m L , the following holds: for any probability distribution on X and any t 2 C, with probability at least 1 ffi, (fx 2 X : jh L (x) t (x)j + ffg) &lt; *: Bartlett, Long and Williamson <ref> [26] </ref> have recently shown that even if there is some reasonable level of noise in the labeling of the training sample, then, provided dim fl (H) is finite for all fl, learning with a good model is possible. <p> It is not possible to obtain non-trivial general lower bounds on the sample complexity of (noiseless) function learning in terms of dim fl (H). A simple `coding' argument shows this. (See <ref> [26] </ref>.) However, Bartlett et al. have obtained lower bounds on the sample complexity in the presence of noise.
Reference: [27] <author> E. Baum and D. Haussler. </author> <title> What size net gives valid generalization? Neural Computation, </title> <booktitle> 1(1) </booktitle> <pages> 151-160, </pages> <year> 1989. </year>
Reference-contexts: Since, for any finite space H, VCdim (H) log jHj, we have VCdim (N ; f0; 1g n ) W N log N . ut This result is limited in that it concerns only networks with binary-valued inputs. The following result of Baum and Haussler <ref> [27] </ref> applies to feedforward linear threshold networks. These are linear threshold networks with no feedback; in other words, networks in which the underlying directed graph is acyclic. This result applies for real inputs, as well as for binary inputs. <p> If we do this, we obtain a sample complexity bound of order * 1 W (log (W=*) + log (1=ffi)), as in <ref> [27] </ref>. The VC-dimension bound presented by Baum and Haussler, which is obtained in the same way, but by using Sauer's Lemma a little more carefully, is 2W log (eN ), where N is the number of computation units and e is the base of the natural logarithm. <p> This leads us to a discussion of results providing lower bounds on the VC-dimension of linear threshold networks. In their paper, Baum and Haussler <ref> [27] </ref> proved that there are threshold networks with one hidden layer (that is, of depth two) and VC-dimension (W ), where W is the number of variable weights. <p> First, we have a result like one due to Baum and Haussler <ref> [27] </ref>, which concerns the VC-dimension of a feedforward linear threshold network in which only a certain number of weights can be non-zero. These relevant connections are not specified in advance of training. <p> Using the bound on the growth function obtained in the proof, as in <ref> [27] </ref>, one obtains a bound on the sample complexity of any consistent learning algorithm for N which produces states with at most W 0 non-zero weights and thresholds. (Additionally, one obtains a sample length bound for the property described in Theorem 7.1.) As earlier, the bounds obtained using the explicit bound <p> Such techniques have been discussed in [102], for example. Shawe-Taylor [103] has bounded the sample complexity of such learning algorithms by bounding the growth function in a manner similar to that in <ref> [27] </ref>. His results, following [19, 104], also apply more generally to networks having more than one output node, but we shall not discuss this aspect here. Let N be a feedforward linear threshold network.
Reference: [28] <author> E. B. Baum. </author> <title> Polynomial time algorithms for learning neural nets. </title> <booktitle> In Proc. 3rd Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 258-272. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: We refer the reader to [94, 57] for details. Another very important variant is that in which the learning algorithm can ask questions concerning, for example, the classification of a chosen example; see, for example <ref> [2, 28, 53] </ref>. Such `query learning' is an active area of research, and the paper by Angluin [3] provides a good survey.
Reference: [29] <author> S. Ben-David, G. M. Benedek, and Y. Mansour. </author> <title> A parametrization scheme for classifying models of learnability. </title> <booktitle> In Proc. 2nd Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 285-302. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1989. </year> <title> Journal version appears as A parameterization scheme for classifying models of PAC learnability, </title> <booktitle> Information and Computation 120 (1): </booktitle> <pages> 11-21, </pages> <year> 1995. </year>
Reference-contexts: For further discussion of distribution-dependent learning, we refer the reader to the papers of Benedek and Itai [34], Ben-David, Benedek and Mansour <ref> [29] </ref>, Bertoni et al. [35], Kharitonov [74], Li and Vitanyi [77], Linial, Mansour and Nisan [78]. For discussion specific to neural networks, see [52, 86]. 11 Graph Dimension and Multiple-Output Nets The basic PAC model concerns learning f0; 1g-valued functions only; that is, it is concerned only with classification problems.
Reference: [30] <author> S. Ben-David, N. Cesa-Bianchi, D. Haussler, and P. M. </author> <title> Long. Characterizations of learnability for classes of f0; : : :; ng-valued functions. </title> <journal> J. of Comp. and Sys. Sci. </journal> <volume> 50(1): </volume> <pages> 74-86, </pages> <year> 1995. </year>
Reference-contexts: Natarajan finds a weaker necessary condition for learnability, showing that a certain measure, now known as the Natarajan dimension, must be finite for H to be PAC learnable. More recently, Ben-David, Cesa-Bianchi and Long <ref> [31, 30] </ref> have shown that when Y is finite, the finiteness of the graph dimension is a necessary and sufficient condition for H to be PAC learnable.
Reference: [31] <author> S. Ben-David, N. Cesa-Bianchi, and P. M. </author> <title> Long. Characterizations of learnability for classes of f0; :::; ng-valued functions. </title> <booktitle> In Proc. 5th Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 333-340. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1992. </year> <booktitle> Neural Computing Surveys 1, </booktitle> <pages> 1-47, </pages> <year> 1997, </year> <note> http://www.icsi.berkeley.edu/~jagota/NCS 43 </note>
Reference-contexts: Natarajan finds a weaker necessary condition for learnability, showing that a certain measure, now known as the Natarajan dimension, must be finite for H to be PAC learnable. More recently, Ben-David, Cesa-Bianchi and Long <ref> [31, 30] </ref> have shown that when Y is finite, the finiteness of the graph dimension is a necessary and sufficient condition for H to be PAC learnable.
Reference: [32] <author> G. Benedek and A. Itai. </author> <title> Learnability with respect to fixed distributions. </title> <journal> Theoret. Comput. Sci., </journal> <volume> 86(2) </volume> <pages> 377-389, </pages> <year> 1991. </year>
Reference-contexts: Learnability with respect to a particular distribution has also been studied by Benedek and Itai <ref> [33, 32] </ref>. They show that if is discrete|that is, if the support of is a countable subset of X|then any hypothesis space H is learnable with respect to . For general distributions, they develop a theory involving `*-covers'.
Reference: [33] <author> G. M. Benedek and A. Itai. </author> <title> Learnability by fixed distributions. </title> <booktitle> In Proc. 1st Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 80-90. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1988. </year>
Reference-contexts: Learnability with respect to a particular distribution has also been studied by Benedek and Itai <ref> [33, 32] </ref>. They show that if is discrete|that is, if the support of is a countable subset of X|then any hypothesis space H is learnable with respect to . For general distributions, they develop a theory involving `*-covers'.
Reference: [34] <author> G. M. Benedek and A. Itai. </author> <title> Dominating distributions and learnability. </title> <booktitle> In Proc. 5th Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 253-264. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1992. </year>
Reference-contexts: For further discussion of distribution-dependent learning, we refer the reader to the papers of Benedek and Itai <ref> [34] </ref>, Ben-David, Benedek and Mansour [29], Bertoni et al. [35], Kharitonov [74], Li and Vitanyi [77], Linial, Mansour and Nisan [78].
Reference: [35] <author> A. Bertoni, P. Campadelli, A. Morpurgo, and S. Panizza. </author> <title> Polynomial uniform convergence and polynomial-sample learnability. </title> <booktitle> In Proc. 5th Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 265-271. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1992. </year>
Reference-contexts: It follows immediately from the bounds of Benedek and Itai that a necessary and sufficient condition is that N (*) &lt; 2 p (1=*) for some polynomial p, but this is not a very manageable condition. Bertoni et al. <ref> [35] </ref> took a different approach, following on from the work of Vapnik Neural Computing Surveys 1, 1-47, 1997, http://www.icsi.berkeley.edu/~jagota/NCS 28 and Chervonenkis. <p> For further discussion of distribution-dependent learning, we refer the reader to the papers of Benedek and Itai [34], Ben-David, Benedek and Mansour [29], Bertoni et al. <ref> [35] </ref>, Kharitonov [74], Li and Vitanyi [77], Linial, Mansour and Nisan [78]. For discussion specific to neural networks, see [52, 86]. 11 Graph Dimension and Multiple-Output Nets The basic PAC model concerns learning f0; 1g-valued functions only; that is, it is concerned only with classification problems.
Reference: [36] <author> A. Blum and R. L. Rivest. </author> <title> Training a 3-node neural net is NP-Complete. </title> <editor> In D. S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems I, </booktitle> <pages> pages 494-501. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1989. </year>
Reference-contexts: The fact that computational complexity-theoretic hardness results hold for neural networks was first shown by Judd [66]. In this section we shall prove a simple hardness result from [10, 11] along the lines of one due to Blum and Rivest <ref> [36] </ref>. Neural Computing Surveys 1, 1-47, 1997, http://www.icsi.berkeley.edu/~jagota/NCS 23 The network has n inputs and k + 1 computation units (k 1). The first k computation units are `in parallel' and each of them is connected to all the inputs. <p> But GRAPH k-COLORING is NP-complete [47], and hence it follows that the N k CONSISTENCY problem is NP-hard if k 3. (In fact, the same is true if k = 2. This follows from work of Blum and Rivest <ref> [36] </ref>.) Thus, fixing k, we have a very simple family of feedforward linear threshold networks, each consisting of k + 1 computation units (one of which is `hard-wired' and acts simply as an AND gate) for which the Neural Computing Surveys 1, 1-47, 1997, http://www.icsi.berkeley.edu/~jagota/NCS 24 problem of `loading' a training
Reference: [37] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. </author> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> J. ACM, </journal> <volume> 36(4) </volume> <pages> 929-965, </pages> <year> 1989. </year>
Reference-contexts: Explicitly, we have the following theorem [99, 107, 116], usually known as Sauer's Lemma. The second inequality is elementary|a proof was given by Blumer et al. <ref> [37] </ref>. Theorem 3.3 (Sauer's Lemma) Let d 0 and m 1 be given integers and let F be a set of f0; 1g-valued functions with VCdim (F ) = d 1. <p> This result and related ones have been central to the mathematical development of PAC learning theory. The paper of Vapnik and Chervonenkis [116] gave the first such results. Results of a similar form, but specifically for learning theory applications, were given by Blumer et al. <ref> [37] </ref>, who were the first to highlight the importance of this area of probability for the theory of PAC learning. Their results were subsequently improved in [12, 105]. <p> Because the action of fl is measure preserving (with respect to the product measure P 2m ), it can easily be shown (see, for example, <ref> [37, 116] </ref>) that P 2m (R) jflj : Fix z = ((x 1 ; b 1 ); (x 2 ; b 2 ); : : : ; (x 2m ; b 2m )) 2 S 2m and let x = (x 1 ; x 2 ; : : : ; x <p> We shall see later that this more general framework of a probability distribution on X fi f0; 1g is useful. The following result is a consequence of Theorem 5.1, together with Sauer's Lemma. A better sample size bound, with smaller constants, improving a previous bound from <ref> [37] </ref>, has been obtained in [5, 19]. However, the following result is adequate for our purposes. Theorem 5.2 Let H be a hypothesis space of f0; 1g-valued functions defined on an input space X. <p> We now present a lower bound result, part of which is due to Ehrenfeucht et al. [43] and part of which is due to Blumer et al. <ref> [37] </ref>. <p> on the sample complexity of any PAC (C; H)-learning algorithm when C has finite VC-dimension and is `non-trivial'. (A result of this strength is not needed simply to show that finite VC-dimension of the concept space is necessary for PAC learning: a simpler result proving this may be found in <ref> [37] </ref>. But we wish also to demonstrate that there are lower bounds and upper bounds on the sample complexity of consistent PAC algorithms which do not differ too greatly.) Theorem 6.2 Let C be a concept space of VC-dimension at least 1 and consisting of at least three distinct concepts. <p> Alternatively, the problem may not lie with the teacher, but with the `concept' itself. This may be ill-formed and may not be a function at all. To deal with these situations, we have the notion of a stochastic concept <ref> [70, 119, 37] </ref>. A stochastic concept on X is simply a probability distribution P on X fi f0; 1g. Informally, for finite or countable X, one interprets P ((x; b)) to be the probability that x will be given classification b. <p> This result follows easily from the more general form of Vapnik's result when one takes the events to be the error sets. Details may be found in <ref> [5, 19, 37] </ref>. Neural Computing Surveys 1, 1-47, 1997, http://www.icsi.berkeley.edu/~jagota/NCS 30 they show that in this case, the Natarajan dimension is finite if and only if the graph dimension is finite, so that Natarajan's necessary and sufficient conditions are matching.
Reference: [38] <author> N. Cesa-Bianchi, Y. Freund, D. P. Helmbold, D. Haussler, R. E. Schapire, and M. K. Warmuth. </author> <title> How to use expert advice. </title> <booktitle> In Proc. 25th Annu. ACM Sympos. Theory Comput., </booktitle> <pages> pages 382-391. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1993. </year>
Reference-contexts: distribution and target are permitted to change a little between observations, as in [25, 58, 59], models of weak learning in which the learner only has to do slightly better than random guessing [51, 100, 60], and variants in which the learning algorithm has access to the predictions of `experts' <ref> [38] </ref>. Something which has not been discussed in any detail here is the use of real-output neural networks for classification. Recent work [106, 23] has shown that, here, the scale-sensitive fl-dimension is very useful.
Reference: [39] <author> T. Cormen, C. Leiserson, and R. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA., </address> <year> 1990. </year>
Reference-contexts: It turns out that efficient PAC learning does imply efficient consistent-hypothesis-finding, provided we are prepared to accept a randomised algorithm. For our purposes, a randomised algorithm A has access to a random number generator and is allowed to use these random numbers as part of its input. (See <ref> [39] </ref>.) The computation carried out by the algorithm is determined by its input, so that it depends on the particular sequence produced by the random number generator. It follows that we can speak of the probability that A has a given outcome.
Reference: [40] <author> L. Devroye, L. Gyorfi and G. Lugosi. </author> <title> A Probabilistic Theory of Pattern Recognition. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1996. </year>
Reference-contexts: I have placed emphasis on those topics I find to be of most interest and, consequently, there is more discussion of sample complexity than of computational complexity. There are now a number of books dealing with probabilistic models of learning: the interested reader might consult <ref> [115, 114, 95, 73, 90, 40, 10] </ref> for further information. The first part of this work concerns the basic PAC model, applicable for classification problems. The second part concerns extensions of the basic PAC model, such as those relevant to neural networks with real-valued outputs.
Reference: [41] <author> R. Dudley, S. Kulkarni, T. Richardson, and O. Zeitouni. </author> <title> A metric entropy bound is not sufficient for learnability. </title> <journal> IEEE Transactions on Information Theory 40(3): </journal> <pages> 883-885, </pages> <year> 1994. </year>
Reference-contexts: Benedek and Itai conjectured that this condition is also sufficient for learnability with respect to P. However, by means of a counterexample, Dudley et al. <ref> [41] </ref> proved that this is not so. We discussed the efficiency of (distribution-independent) learning earlier, in which we required that a learning algorithm for a graded space run in time polynomial in n, the size of the examples, and in * 1 .
Reference: [42] <author> R. M. Dudley. </author> <title> Central limit theorems for empirical measures. </title> <journal> Annals of Probability, </journal> <volume> 6(6) </volume> <pages> 899-929, </pages> <year> 1978. </year>
Reference-contexts: The following result is due to Dudley <ref> [42] </ref>; we present here a proof from [6]. Theorem 4.2 Let V be a real vector space of real-valued functions defined on a set X. Suppose that V has linear dimension d.
Reference: [43] <author> A. Ehrenfeucht, D. Haussler, M. Kearns, and L. Valiant. </author> <title> A general lower bound on the number of examples needed for learning. </title> <journal> Information and Computation, </journal> <volume> 82 </volume> <pages> 247-261, </pages> <year> 1989. </year>
Reference-contexts: We now present a lower bound result, part of which is due to Ehrenfeucht et al. <ref> [43] </ref> and part of which is due to Blumer et al. [37].
Reference: [44] <author> U. Faigle and W. Kern. </author> <title> On learnability of monotone DNF functions under uniform distribution. </title> <type> Manuscript, </type> <institution> Department of Mathematics, University of Twente, Netherlands, </institution> <year> 1990. </year>
Reference-contexts: Furthermore, learning with respect to a particular distribution or class of distributions may be computationally much easier than PAC learning; see <ref> [45, 117, 44, 68, 92, 50, 78, 46] </ref>. Neural Computing Surveys 1, 1-47, 1997, http://www.icsi.berkeley.edu/~jagota/NCS 27 The original statistical work of Vapnik and Chervonenkis [116] has something to say about learning with respect to particular distributions.
Reference: [45] <author> M. Flammini, A. Marchetti-Spaccamela, and L. K. Cera. </author> <title> Learning DNF formulae under classes of probability distributions. </title> <booktitle> In Proc. 5th Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 85-92. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1992. </year>
Reference-contexts: Furthermore, learning with respect to a particular distribution or class of distributions may be computationally much easier than PAC learning; see <ref> [45, 117, 44, 68, 92, 50, 78, 46] </ref>. Neural Computing Surveys 1, 1-47, 1997, http://www.icsi.berkeley.edu/~jagota/NCS 27 The original statistical work of Vapnik and Chervonenkis [116] has something to say about learning with respect to particular distributions.
Reference: [46] <author> M. L. Furst, J. C. Jackson, and S. W. Smith. </author> <title> Improved learning of AC 0 functions. </title> <booktitle> In Proc. 4th Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 317-325. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: Furthermore, learning with respect to a particular distribution or class of distributions may be computationally much easier than PAC learning; see <ref> [45, 117, 44, 68, 92, 50, 78, 46] </ref>. Neural Computing Surveys 1, 1-47, 1997, http://www.icsi.berkeley.edu/~jagota/NCS 27 The original statistical work of Vapnik and Chervonenkis [116] has something to say about learning with respect to particular distributions.
Reference: [47] <author> M. Garey and D. Johnson. </author> <title> Computers and Intractibility: A Guide to the Theory of NP-Completeness. </title> <address> Freemans, San Francisco, </address> <year> 1979. </year>
Reference-contexts: We have shown that if there is a polynomial time algorithm for N k CONSISTENCY, then there is one for GRAPH k-COLORING. But GRAPH k-COLORING is NP-complete <ref> [47] </ref>, and hence it follows that the N k CONSISTENCY problem is NP-hard if k 3. (In fact, the same is true if k = 2.
Reference: [48] <author> P. Goldberg and M. Jerrum. </author> <title> Bounding the Vapnik-Chervonenkis dimension of concept classes parameterized by real numbers. </title> <booktitle> Machine Learning, </booktitle> <pages> 18(2-3): 131-148, </pages> <year> 1995. </year> <booktitle> (Extended abstract appeared in Proceedings of 6th Annual ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 361-369. </pages> <publisher> ACM Press, </publisher> <year> 1993.) </year> <booktitle> Neural Computing Surveys 1, </booktitle> <pages> 1-47, </pages> <year> 1997, </year> <note> http://www.icsi.berkeley.edu/~jagota/NCS 44 </note>
Reference-contexts: In view of this, it is not possible to obtain a general VC-dimension bound for sigmoid neural networks and one must consider particular sigmoids or particular types of sigmoid separately. First, we present the following, quite general, result of Goldberg and Jerrum <ref> [48] </ref>. Neural Computing Surveys 1, 1-47, 1997, http://www.icsi.berkeley.edu/~jagota/NCS 18 Theorem 7.10 Suppose that fN W;n : W; n 2 Ng is a family of binary-output neural networks, where N W;n has n inputs and W variable real parameters (weights and thresholds). <p> If this is so, then the theorem implies that the VC-dimension of the network N W;n is of order W t where t is a bound on the time taken by the network to calculate its output. As a very simple example from <ref> [48] </ref>, suppose that each N W;n is a feedforward linear threshold network. <p> Another application of Theorem 7.10 is to networks in which each activation function f j is a piecewise polynomial function. Maass [80] proved that the VC-dimension of such networks can be polynomially bounded provided the networks have constant depth. Applying Theorem 7.10 gives the following result <ref> [48, 83] </ref>, in which no bound on the depth is necessary. Theorem 7.11 Let fN W g be a family of feedforward neural networks with binary outputs, having piecewise polynomial functions of bounded degree and bounded number of pieces as activation functions on the hidden units. <p> Then pdim (l W ) = O (W 2 ) as W ! 1. This theorem is proved by generalizing the techniques in <ref> [48] </ref>. The lower bound results of Theorem 7.8 and Theorem 7.9 show that a lower bound (W log W ) holds in general.
Reference: [49] <author> S. A. Goldman and M. J. Kearns. </author> <title> On the complexity of teaching. </title> <booktitle> In Proc. 4th Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 303-314. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: becomes when there is a `helpful teacher' providing cleverly-chosen examples as training sample. (This is very different from the PAC model in that the training examples are no longer randomly chosen.) Such models of teaching have been studied in the case where the goal is to learn the target exactly <ref> [49, 13, 65, 108, 14] </ref> and in which the goal is to learn an approximation to the target in a probabilistic sense [97, 96].
Reference: [50] <author> S. A. Goldman, M. J. Kearns, and R. E. Schapire. </author> <title> Exact identification of circuits using fixed points of amplification functions. </title> <booktitle> In Proc. of the 31st Symposium on the Foundations of Comp. Sci., </booktitle> <pages> pages 193-202. </pages> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1990. </year>
Reference-contexts: Furthermore, learning with respect to a particular distribution or class of distributions may be computationally much easier than PAC learning; see <ref> [45, 117, 44, 68, 92, 50, 78, 46] </ref>. Neural Computing Surveys 1, 1-47, 1997, http://www.icsi.berkeley.edu/~jagota/NCS 27 The original statistical work of Vapnik and Chervonenkis [116] has something to say about learning with respect to particular distributions.
Reference: [51] <author> S. A. Goldman, M. J. Kearns, and R. E. Schapire. </author> <title> On the sample complexity of weak learning. </title> <booktitle> In Proc. 3rd Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 217-231. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: Other useful variants not discussed here are those in which the distribution and target are permitted to change a little between observations, as in [25, 58, 59], models of weak learning in which the learner only has to do slightly better than random guessing <ref> [51, 100, 60] </ref>, and variants in which the learning algorithm has access to the predictions of `experts' [38]. Something which has not been discussed in any detail here is the use of real-output neural networks for classification.
Reference: [52] <author> M. Golea, M. Marchand and T. Hancock. </author> <title> On learning -perceptron networks on the uniform distribution. </title> <booktitle> Neural Networks 9: </booktitle> <pages> 67-82, </pages> <year> 1996. </year>
Reference-contexts: For further discussion of distribution-dependent learning, we refer the reader to the papers of Benedek and Itai [34], Ben-David, Benedek and Mansour [29], Bertoni et al. [35], Kharitonov [74], Li and Vitanyi [77], Linial, Mansour and Nisan [78]. For discussion specific to neural networks, see <ref> [52, 86] </ref>. 11 Graph Dimension and Multiple-Output Nets The basic PAC model concerns learning f0; 1g-valued functions only; that is, it is concerned only with classification problems.
Reference: [53] <author> T. Hancock, M. Golea, and M. Marchand. </author> <title> Learning nonoverlapping perceptron networks from examples and membership queries. </title> <booktitle> Machine Learning 16(3): </booktitle> <pages> 161-183, </pages> <year> 1994. </year>
Reference-contexts: We refer the reader to [94, 57] for details. Another very important variant is that in which the learning algorithm can ask questions concerning, for example, the classification of a chosen example; see, for example <ref> [2, 28, 53] </ref>. Such `query learning' is an active area of research, and the paper by Angluin [3] provides a good survey.
Reference: [54] <author> L. Gurvits, L. and P. Koiran. </author> <title> Approximation and learning of convex superpositions. </title> <booktitle> In Proceedings of Eurocolt'95, Springer-Verlag Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 222-236. </pages>
Reference-contexts: It is possible, as in the case of ND, for dim fl (H) to be finite for all fl but for pdim (H) to be infinite. This scale-sensitive dimension has been bounded for classes of neural networks by Gurvits and Koiran <ref> [54] </ref> and Bartlett [23]. Alon et al. [1] proved the following result. (The necessity was proved earlier by Kearns and Schapire [70].) Theorem 14.3 Let H be a class of p-concepts.
Reference: [55] <author> D. Haussler. </author> <title> Decision theoretic generalizations of the PAC model for neural net and other learning applications. </title> <journal> Inform. Comput., </journal> <volume> 100(1) </volume> <pages> 78-150, </pages> <month> Sept. </month> <year> 1992. </year>
Reference-contexts: Hence fi (z) P t i=1 fi i (z) where, for each i, fi i (z) is the number of permutations in fl taking z into R i . We now bound fi i (z). Following <ref> [55] </ref>, for each 1 j 2m, let X j = 1 if z j 2 E h i (that is, if h i (x j ) 6= b j ) and X j = 0 otherwise and, for 1 j m, let Y j be the random variable which equals X <p> Further, let GH = fGh : h 2 Hg, the graph space of H. We have the following definition [89]. 10 In what follows, certain technical measure-theoretic conditions have to be satisfied; we shall not discuss these, but the reader may find the details in the paper of Haussler <ref> [55] </ref> or the book by Pollard [95]. <p> Furthermore, when H is a vector space of real functions, the pseudo-dimension of H is precisely the vector-space dimension of H; see <ref> [55] </ref>. (This generalizes Dudley's result, Theorem 4.2.) 12.2 Learning real-valued functions When considering a space H of functions from X to R k , as is appropriate for applications to neural networks with k real outputs, it seems rather over-restrictive, and inappropriate, to say that a hypothesis h is erroneous with <p> Up to now, this is the definition of error we have used. There are other ways of measuring error, if one is prepared to ask not is the output correct? but is the output close? in some sense. Haussler <ref> [55] </ref> has developed a `decision-theoretic' framework encompassing many ways of measuring error by means of loss functions. We shall describe this framework in a way which also subsumes the discussion on stochastic concepts. First, we need some definitions. <p> The reader is referred to the paper of Haussler <ref> [55] </ref> for a far more detailed discussion of the general decision-theoretic approach and its applications. As in our discussion of stochastic concepts, we consider probability Neural Computing Surveys 1, 1-47, 1997, http://www.icsi.berkeley.edu/~jagota/NCS 32 distributions P on X fi Y . <p> If this is the case, then a learning algorithm which outputs a hypothesis with near-minimal observed error will be a probably approximately optimal learning algorithm. More precisely, we have the following result <ref> [55, 114] </ref>. Neural Computing Surveys 1, 1-47, 1997, http://www.icsi.berkeley.edu/~jagota/NCS 33 Theorem 12.3 Suppose that H is a hypothesis space of functions from X to Y and that l is a loss function defined on Y fi Y . <p> A bound on the sample complexity Neural Computing Surveys 1, 1-47, 1997, http://www.icsi.berkeley.edu/~jagota/NCS 34 of this algorithm follows from the theorem. If H maps into R, then the pseudo-dimension of l H can be related to that of H itself if l has a certain `monotonicity' property (see <ref> [55] </ref>). In this case, one obtains bounds on the sample length m 0 (ffi; *) for the UCE property, and the sample complexity of ^ L, which depend on pdim (H). <p> Another way, described in <ref> [55] </ref>, is to use the notion of the capacity of a function space. <p> If there is no finite *-cover for some , or if the supremum does not exist, we say that the *-capacity is infinite. 12 Results of Haussler <ref> [55] </ref> and Pollard [95] provide the following uniform bound on the rate of convergence of observed errors to actual errors. <p> Neural Computing Surveys 1, 1-47, 1997, http://www.icsi.berkeley.edu/~jagota/NCS 35 When k = 1 and H maps into [0; M ], the capacity can be related to the pseudo-dimension of H. Haus sler <ref> [55] </ref> (see also Pollard [95]) showed that if H has finite pseudo-dimension then C H (*) &lt; 2 2eM ln * pdim (H) : This, combined with the above result, shows that, in this case, H has the UCE property and that a sufficient sample length m 0 (ffi; *) is <p> Then pdim (H D ) &lt; 8W log (11W D). We now describe an approach taken by Haussler, bounding directly the capacity of more general types of sigmoid network. In his paper, Haussler <ref> [55] </ref> shows how the general framework and results can, in addition, be applied to radial basis function networks and networks composed of product units. Suppose that each activation function f i is a `smooth' bounded monotone function, which need not be the standard sigmoid. <p> We state the following special case of a result from <ref> [55] </ref>. Theorem 13.3 Suppose that N is a feedforward sigmoid network of depth d, with any number of output nodes and W variable parameters (weights and thresholds). Let be the maximum in-degree of a computation node. Suppose that each activation function maps into the interval [ff; fi].
Reference: [56] <author> D. Haussler, M. Kearns, N. Littlestone, and M. K. Warmuth. </author> <title> Equivalence of models for polynomial learnability. </title> <journal> Inform. Comput., </journal> <volume> 95(2) </volume> <pages> 129-161, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: The probability that the algorithm fails in k attempts is at most (1=2) k , which approaches zero very rapidly with increasing k. The following result is from [93]. (See also <ref> [89, 56] </ref>.) Theorem 8.1 Let H = S H n be a hypothesis space and suppose that there is a PAC learning algorithm for H which is efficient with respect to accuracy and example size. <p> This is not really so different from the basic PAC learning framework: in the original PAC model as introduced by Valiant, a learning algorithm was given the accuracy and confidence parameters and had access to an `oracle' generating random labeled examples. Subsequently, Haussler et al. <ref> [56] </ref> showed that this model is equivalent to the `functional' model we described in earlier sections, in which the learning algorithm is given only a training sample as input.
Reference: [57] <author> D. Haussler, N. Littlestone, and M. K. Warmuth. </author> <title> Predicting f0,1g functions on randomly drawn points. </title> <booktitle> Information and Computation 108(2): </booktitle> <pages> 212-261, </pages> <year> 1994. </year> <booktitle> (Extended abstract in Proceedings of the 29th Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 100-109. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1988.) </year>
Reference-contexts: However, there is a variant of the PAC model known as the prediction model, in which there is no fixed hypothesis space; in a sense, the output of a prediction algorithm is some program which classifies further examples. We refer the reader to <ref> [94, 57] </ref> for details. Another very important variant is that in which the learning algorithm can ask questions concerning, for example, the classification of a chosen example; see, for example [2, 28, 53].
Reference: [58] <author> D. P. Helmbold and P. M. </author> <title> Long. Tracking drifting concepts by minimizing disagreements. </title> <booktitle> Machine Learning 14 (1): </booktitle> <pages> 27-45, </pages> <year> 1994. </year>
Reference-contexts: Other useful variants not discussed here are those in which the distribution and target are permitted to change a little between observations, as in <ref> [25, 58, 59] </ref>, models of weak learning in which the learner only has to do slightly better than random guessing [51, 100, 60], and variants in which the learning algorithm has access to the predictions of `experts' [38].
Reference: [59] <author> D. P. Helmbold and P. M. </author> <title> Long. Tracking drifting concepts using random examples. </title> <booktitle> In Proc. 4th Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 13-23. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: Other useful variants not discussed here are those in which the distribution and target are permitted to change a little between observations, as in <ref> [25, 58, 59] </ref>, models of weak learning in which the learner only has to do slightly better than random guessing [51, 100, 60], and variants in which the learning algorithm has access to the predictions of `experts' [38].
Reference: [60] <author> D. P. Helmbold and M. K. Warmuth. </author> <title> Some weak learning results. </title> <booktitle> In Proc. 5th Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 399-412. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1992. </year>
Reference-contexts: Other useful variants not discussed here are those in which the distribution and target are permitted to change a little between observations, as in [25, 58, 59], models of weak learning in which the learner only has to do slightly better than random guessing <ref> [51, 100, 60] </ref>, and variants in which the learning algorithm has access to the predictions of `experts' [38]. Something which has not been discussed in any detail here is the use of real-output neural networks for classification.
Reference: [61] <author> J. Hertz, A. Krogh, and R. Palmer. </author> <title> Introduction to the Theory of Neural Computation. </title> <publisher> Addison-Wesley, </publisher> <address> Redwood City, </address> <year> 1991. </year>
Reference-contexts: In both the above cases, restricting the weights reduces the VC dimension. A common technique in neural network learning is to keep the magnitude of the weights as small as possible; see <ref> [61] </ref>, for example. It is natural to ask whether such a restriction significantly decreases the VC-dimension. Lee, Bartlett and Williamson [76] have investigated this problem for depth two feedforward networks with certain activation functions on the hidden units and a linear threshold unit as the output unit.
Reference: [62] <author> W. Hoeffding. </author> <title> Probability inequalities for sums of bounded random variables. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 58(301) </volume> <pages> 13-30, </pages> <month> Mar. </month> <year> 1963. </year>
Reference-contexts: i (z) jflj = U &gt; &lt; t 2 fl : j=1 X t 1 (m+j) X t 1 (j) &gt; @ m 2m X X j A 9 &gt; ; = Prob 8 &gt; : j=1 0 m 2m X X j A 9 &gt; ; By Hoeffding's inequality <ref> [62] </ref>, this probability is bounded by exp P 2m 4 j=1 (X j X m+j ) 2 exp 4 This holds for each 1 i t.
Reference: [63] <author> K. Hoffgen and H. Simon. </author> <title> Robust trainability of single neurons. </title> <booktitle> In Proc. 5th Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 428-439. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1992. </year>
Reference-contexts: We have barely touched on the computational complexity of PAC learning and its variants. Further discussion of this may be found, for example, in <ref> [82, 21, 10, 73, 93, 69, 70, 101, 63, 79] </ref>. There are also many more variants of the PAC model which we have not mentioned here. Throughout the discussion here, it has been assumed that there is some fixed hypothesis space.
Reference: [64] <author> S. B. Holden. </author> <title> On the theory of generalization and self-structuring in linearly weighted connectionist networks. </title> <type> PhD thesis, </type> <institution> University of Cambridge, </institution> <month> Sept. </month> <year> 1993. </year> <note> Also appears as Cambridge University Engineering Department technical report CUED/F-INFENG/TR.161, </note> <month> January </month> <year> 1994. </year>
Reference-contexts: We have the following result, which is obtained using interpolation results of Micchelli [87]. Details of the proof may be found in <ref> [16, 15, 64] </ref>, where further results along the same lines are presented.
Reference: [65] <author> J. Jackson and A. Tomkins. </author> <title> A computational model of teaching. </title> <booktitle> In Proc. 5th Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 319-326. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1992. </year> <booktitle> Neural Computing Surveys 1, </booktitle> <pages> 1-47, </pages> <year> 1997, </year> <note> http://www.icsi.berkeley.edu/~jagota/NCS 45 </note>
Reference-contexts: becomes when there is a `helpful teacher' providing cleverly-chosen examples as training sample. (This is very different from the PAC model in that the training examples are no longer randomly chosen.) Such models of teaching have been studied in the case where the goal is to learn the target exactly <ref> [49, 13, 65, 108, 14] </ref> and in which the goal is to learn an approximation to the target in a probabilistic sense [97, 96].
Reference: [66] <author> S. Judd. </author> <title> Learning in neural networks. </title> <booktitle> In Proc. 1st Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 2-8. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1988. </year>
Reference-contexts: Then, unless RP equals NP, there is no PAC learning algorithm for H which runs in time polynomial in * 1 and n. The fact that computational complexity-theoretic hardness results hold for neural networks was first shown by Judd <ref> [66] </ref>. In this section we shall prove a simple hardness result from [10, 11] along the lines of one due to Blum and Rivest [36]. Neural Computing Surveys 1, 1-47, 1997, http://www.icsi.berkeley.edu/~jagota/NCS 23 The network has n inputs and k + 1 computation units (k 1).
Reference: [67] <author> M. Karpinski and A. Macintyre. </author> <title> Polynomial bounds for VC Dimension of Sigmoidal Neural Networks. </title> <booktitle> In Proceedings of the 27th annual ACM Symposium on the Theory of Computing, </booktitle> <pages> 200-208, </pages> <year> 1995. </year>
Reference-contexts: Then VCdim (N ; X D ) 8W log (11W D); where W is the number of weights and thresholds. Karpinski and Macintyre <ref> [67] </ref>, using some fairly sophisticated mathematical machinery, obtained a polynomial upper bound on the VC-dimension of standard sigmoid networks. Specifically, they proved the following. Theorem 7.14 Let N be a feedforward network with binary output and the standard sigmoid function as activation function on the hidden units. <p> Koiran and Sontag [75] showed that the VC-dimension of (unbounded depth) standard sigmoid nets is (W 2 ), so there is a strict separation between the VC-dimension of threshold nets and sigmoid nets. We have presented a number of results on the VC-dimensions of neural networks. Similar analyses <ref> [80, 67] </ref> can be carried out if the activation functions do not take as input the linear weighted sum of the outputs, o i , feeding in, but rather take as their arguments some polynomial function of these. Such units are often known as sigma-pi units.
Reference: [68] <author> M. Kearns, M. Li, L. Pitt, and L. Valiant. </author> <title> On the learnability of Boolean formulae. </title> <booktitle> In Proc. 19th Annu. ACM Sympos. Theory Comput., </booktitle> <pages> pages 285-294. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1987. </year>
Reference-contexts: Furthermore, learning with respect to a particular distribution or class of distributions may be computationally much easier than PAC learning; see <ref> [45, 117, 44, 68, 92, 50, 78, 46] </ref>. Neural Computing Surveys 1, 1-47, 1997, http://www.icsi.berkeley.edu/~jagota/NCS 27 The original statistical work of Vapnik and Chervonenkis [116] has something to say about learning with respect to particular distributions.
Reference: [69] <author> M. J. Kearns. </author> <title> The Computational Complexity of Machine Learning. </title> <booktitle> ACM Distinguished Dissertation Series. </booktitle> <publisher> The MIT Press, </publisher> <address> Cambridge, MA., </address> <year> 1989. </year>
Reference-contexts: We have barely touched on the computational complexity of PAC learning and its variants. Further discussion of this may be found, for example, in <ref> [82, 21, 10, 73, 93, 69, 70, 101, 63, 79] </ref>. There are also many more variants of the PAC model which we have not mentioned here. Throughout the discussion here, it has been assumed that there is some fixed hypothesis space.
Reference: [70] <author> M. J. Kearns and R. E. Schapire. </author> <title> Efficient distribution-free learning of probabilistic concepts. </title> <journal> Journal of Computer and System Sciences 48: </journal> <pages> 464-497, </pages> <year> 1994. </year> <title> (Extended abstract in Proc. </title> <booktitle> of the 31st Symposium on the Foundations of Comp. Sci., </booktitle> <pages> pages 382-391. </pages> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1990.) </year>
Reference-contexts: Alternatively, the problem may not lie with the teacher, but with the `concept' itself. This may be ill-formed and may not be a function at all. To deal with these situations, we have the notion of a stochastic concept <ref> [70, 119, 37] </ref>. A stochastic concept on X is simply a probability distribution P on X fi f0; 1g. Informally, for finite or countable X, one interprets P ((x; b)) to be the probability that x will be given classification b. <p> the set of functions computed by a standard sigmoid network with unrestricted weights (and on unrestricted real inputs) has finite pseudo-dimension. 14 Scale-Sensitive Dimensions 14.1 Learnability of p-concepts An interesting variant of the PAC model which has received attention recently is that of `p-concept learning', introduced by Kearns and Schapire <ref> [70] </ref>. Much attention has been on `learning a good model of probability' of a p-concept and it is this problem we shall discuss here. A p-concept (or probabilistic concept) is a function t from X to the interval [0; 1]. The value t (x) is meant to represent a probability. <p> This scale-sensitive dimension has been bounded for classes of neural networks by Gurvits and Koiran [54] and Bartlett [23]. Alon et al. [1] proved the following result. (The necessity was proved earlier by Kearns and Schapire <ref> [70] </ref>.) Theorem 14.3 Let H be a class of p-concepts. Then H is learnable (in the p-concept model) if and only if dim fl (H) is finite for all fl &gt; 0. <p> We have barely touched on the computational complexity of PAC learning and its variants. Further discussion of this may be found, for example, in <ref> [82, 21, 10, 73, 93, 69, 70, 101, 63, 79] </ref>. There are also many more variants of the PAC model which we have not mentioned here. Throughout the discussion here, it has been assumed that there is some fixed hypothesis space.
Reference: [71] <author> M. J. Kearns, R. E. Schapire, and L. M. Sellie. </author> <title> Toward efficient agnostic learning. </title> <booktitle> In Proc. 5th Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 341-352. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1992. </year>
Reference-contexts: This model of learning, in which the aim is to produce, from a class H 0 with H 0 H, an output hypothesis only slightly worse than the best approximation in H, was introduced by Kearns, Schapire and Sellie <ref> [71] </ref>, who called it agnostic learning. 10 Distribution-Specific Learning Perhaps the main attraction of the definition of PAC learning is the `distribution-free' criterion: the sample complexity is independent of the probability distribution.
Reference: [72] <author> M.J. Kearns and L.G. Valiant. </author> <title> Cryptographic limitations on learning Boolean formulae and finite automata. </title> <journal> Journal of the ACM 41(1): </journal> <pages> 67-95, </pages> <year> 1994. </year>
Reference-contexts: Hardness results of a `representation-independent' nature, in which the only specification on the hypothesis space is the reasonable one that it be `polynomially evaluatable', have been obtained using assumptions about the intractability of certain problems arising in cryptography; see, for example <ref> [72] </ref>. PART 2: VARIANTS AND EXTENSIONS There are a number of limitations to the applicability of the basic PAC model and many variants have been studied in recent years in an attempt to generalize and extend the theory. In this part of the article, we discuss some of these.
Reference: [73] <author> M.J. Kearns and U. </author> <title> Vazirani (1995). Introduction to Computational Learning Theory, </title> <publisher> MIT Press 1995. </publisher>
Reference-contexts: I have placed emphasis on those topics I find to be of most interest and, consequently, there is more discussion of sample complexity than of computational complexity. There are now a number of books dealing with probabilistic models of learning: the interested reader might consult <ref> [115, 114, 95, 73, 90, 40, 10] </ref> for further information. The first part of this work concerns the basic PAC model, applicable for classification problems. The second part concerns extensions of the basic PAC model, such as those relevant to neural networks with real-valued outputs. <p> We have barely touched on the computational complexity of PAC learning and its variants. Further discussion of this may be found, for example, in <ref> [82, 21, 10, 73, 93, 69, 70, 101, 63, 79] </ref>. There are also many more variants of the PAC model which we have not mentioned here. Throughout the discussion here, it has been assumed that there is some fixed hypothesis space.
Reference: [74] <author> M. Kharitonov. </author> <title> Cryptographic hardness of distribution-specific learning. </title> <booktitle> In Proc. 25th Annu. ACM Sympos. Theory Comput., </booktitle> <pages> pages 372-381. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1993. </year>
Reference-contexts: For further discussion of distribution-dependent learning, we refer the reader to the papers of Benedek and Itai [34], Ben-David, Benedek and Mansour [29], Bertoni et al. [35], Kharitonov <ref> [74] </ref>, Li and Vitanyi [77], Linial, Mansour and Nisan [78]. For discussion specific to neural networks, see [52, 86]. 11 Graph Dimension and Multiple-Output Nets The basic PAC model concerns learning f0; 1g-valued functions only; that is, it is concerned only with classification problems.
Reference: [75] <author> P. Koiran and E.D. Sontag, </author> <title> Neural Networks with Quadratic VC Dimension. </title> <journal> Journal of Computer and System Sciences 54(1): </journal> <pages> 190-198, </pages> <year> 1997 </year>
Reference-contexts: Suppose that the total number of adjustable weights and thresholds is W and that there are k computational units. Then N has finite VC-dimension at most W k (W k 1) + lower order terms; which is O (W 4 ). Koiran and Sontag <ref> [75] </ref> showed that the VC-dimension of (unbounded depth) standard sigmoid nets is (W 2 ), so there is a strict separation between the VC-dimension of threshold nets and sigmoid nets. We have presented a number of results on the VC-dimensions of neural networks.
Reference: [76] <author> W. S. Lee, P. L. Bartlett, and R. C. Williamson. </author> <title> Lower bounds on the VC-dimension of smoothly parametrized function classes. </title> <booktitle> Neural Computation 7: </booktitle> <pages> 990-1002, </pages> <year> 1995. </year>
Reference-contexts: A common technique in neural network learning is to keep the magnitude of the weights as small as possible; see [61], for example. It is natural to ask whether such a restriction significantly decreases the VC-dimension. Lee, Bartlett and Williamson <ref> [76] </ref> have investigated this problem for depth two feedforward networks with certain activation functions on the hidden units and a linear threshold unit as the output unit. In particular, they have the following result.
Reference: [77] <author> M. Li and P. M. B. Vitanyi. </author> <title> Learning simple concepts under simple distributions. </title> <journal> SIAM J. Computing 20(5): </journal> <pages> 911-935, </pages> <year> 1991. </year>
Reference-contexts: For further discussion of distribution-dependent learning, we refer the reader to the papers of Benedek and Itai [34], Ben-David, Benedek and Mansour [29], Bertoni et al. [35], Kharitonov [74], Li and Vitanyi <ref> [77] </ref>, Linial, Mansour and Nisan [78]. For discussion specific to neural networks, see [52, 86]. 11 Graph Dimension and Multiple-Output Nets The basic PAC model concerns learning f0; 1g-valued functions only; that is, it is concerned only with classification problems.
Reference: [78] <author> N. Linial, Y. Mansour, and N. Nisan. </author> <title> Constant depth circuits, Fourier transform, and learnability. </title> <booktitle> In Proc. of the 31st Symposium on the Foundations of Comp. Sci., </booktitle> <pages> pages 574-579. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1989. </year>
Reference-contexts: Furthermore, learning with respect to a particular distribution or class of distributions may be computationally much easier than PAC learning; see <ref> [45, 117, 44, 68, 92, 50, 78, 46] </ref>. Neural Computing Surveys 1, 1-47, 1997, http://www.icsi.berkeley.edu/~jagota/NCS 27 The original statistical work of Vapnik and Chervonenkis [116] has something to say about learning with respect to particular distributions. <p> For further discussion of distribution-dependent learning, we refer the reader to the papers of Benedek and Itai [34], Ben-David, Benedek and Mansour [29], Bertoni et al. [35], Kharitonov [74], Li and Vitanyi [77], Linial, Mansour and Nisan <ref> [78] </ref>. For discussion specific to neural networks, see [52, 86]. 11 Graph Dimension and Multiple-Output Nets The basic PAC model concerns learning f0; 1g-valued functions only; that is, it is concerned only with classification problems.
Reference: [79] <author> W. Maass. </author> <title> Agnostic PAC-learning of functions on analog neural nets (extended abstract) In Advances in Neural Information Processing Systems, 6. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year> <note> Full version: Neural Computation 7(5): 1054-1078, </note> <year> 1995. </year>
Reference-contexts: Suppose the network N W has W variable parameters. Then VCdim (N W ; R n ) = O (W 2 ). Elaborations on this result have been obtained by Maass <ref> [79] </ref>. Whether this bound can be improved to O (W log W ) is an open problem. Returning to sigmoid functions, the following result was obtained by Macintyre and Sontag [84], using deep results from logic. <p> Their result will be described later. For the moment, we present a result on the pseudo-dimension of a particular loss space relevant in the study of neural networks. The following result is a special case of one due to Maass <ref> [79] </ref>. <p> We have barely touched on the computational complexity of PAC learning and its variants. Further discussion of this may be found, for example, in <ref> [82, 21, 10, 73, 93, 69, 70, 101, 63, 79] </ref>. There are also many more variants of the PAC model which we have not mentioned here. Throughout the discussion here, it has been assumed that there is some fixed hypothesis space.
Reference: [80] <author> W. Maass. </author> <title> Bounds on the computational power and learning complexity of analog neural nets (extended abstract). </title> <booktitle> In Proceedings of 25th Annual ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 335-344. </pages> <publisher> ACM Press, </publisher> <year> 1993. </year>
Reference-contexts: We still, therefore, have the following question: can it be true that some feedforward linear threshold networks have VC-dimension significantly larger than W , the number of variable parameters? The answer is `yes', and this was first shown by Maass <ref> [80] </ref>. The following result may be found in [81]. <p> Theorem 7.10 implies that the VC-dimension of such a network is polynomial in the number of weights. Another application of Theorem 7.10 is to networks in which each activation function f j is a piecewise polynomial function. Maass <ref> [80] </ref> proved that the VC-dimension of such networks can be polynomially bounded provided the networks have constant depth. Applying Theorem 7.10 gives the following result [48, 83], in which no bound on the depth is necessary. <p> Koiran and Sontag [75] showed that the VC-dimension of (unbounded depth) standard sigmoid nets is (W 2 ), so there is a strict separation between the VC-dimension of threshold nets and sigmoid nets. We have presented a number of results on the VC-dimensions of neural networks. Similar analyses <ref> [80, 67] </ref> can be carried out if the activation functions do not take as input the linear weighted sum of the outputs, o i , feeding in, but rather take as their arguments some polynomial function of these. Such units are often known as sigma-pi units.
Reference: [81] <author> W. Maass. </author> <title> Neural nets with superlinear VC-dimension. </title> <journal> Neural Computation, </journal> <volume> 6(5): </volume> <pages> 877-884, </pages> <year> 1994. </year>
Reference-contexts: We still, therefore, have the following question: can it be true that some feedforward linear threshold networks have VC-dimension significantly larger than W , the number of variable parameters? The answer is `yes', and this was first shown by Maass [80]. The following result may be found in <ref> [81] </ref>.
Reference: [82] <author> W. Maass. </author> <title> On the complexity of learning on feedforward neural nets. </title> <type> Manuscript, </type> <institution> Institute for Theoretical Computer Science, Technische Universitaet Graz., </institution> <year> 1993. </year>
Reference-contexts: types of threshold network which are not feedforward networks, namely those which compute a function of their inputs in a finite time, with no infinite `cycling'.) The result presented here is a slightly weaker version of Baum and Haussler's original result, with a simpler proof that makes use, as in <ref> [82] </ref>, of a form of Sauer's inequality. Theorem 7.6 Suppose that N is a feedforward linear threshold network having a total of W variable weights and thresholds, and n inputs. <p> opt H (P ) = inf h2H er P;l (h). (As earlier, there is an `agnostic learning' variant of this, in which the aim is to output a hypothesis from a larger hypothesis space which approximates to the target almost as well as the best approximation in H does; see <ref> [82] </ref>.) As in the standard PAC model and the stochastic PAC model described earlier, this can be guaranteed provided we have a `uniform convergence of errors' property. <p> We have barely touched on the computational complexity of PAC learning and its variants. Further discussion of this may be found, for example, in <ref> [82, 21, 10, 73, 93, 69, 70, 101, 63, 79] </ref>. There are also many more variants of the PAC model which we have not mentioned here. Throughout the discussion here, it has been assumed that there is some fixed hypothesis space.
Reference: [83] <author> W. Maass. </author> <title> Vapnik-Chervonenkis dimension of neural nets. In The Handbook of Brain Theory and Neural Networks (ed. </title> <address> M.A. </address> <publisher> Arbib), Bradford Books/MIT Press, </publisher> <year> 1995, </year> <pages> pp. 1000-1003. </pages> <booktitle> Neural Computing Surveys 1, </booktitle> <pages> 1-47, </pages> <year> 1997, </year> <note> http://www.icsi.berkeley.edu/~jagota/NCS 46 </note>
Reference-contexts: We now consider networks with one linear threshold output unit and sigmoid activation functions at all hidden units. (The linear threshold output unit ensures that the network computes f0; 1g-valued functions, so that it is legitimate to discuss the VC dimension.) Sontag has shown (see <ref> [111, 83] </ref>) that there is a neural network N of infinite VC-dimension, having two real inputs, two hidden units with sigmoid activation function f (y) = and a linear threshold output unit. <p> Another application of Theorem 7.10 is to networks in which each activation function f j is a piecewise polynomial function. Maass [80] proved that the VC-dimension of such networks can be polynomially bounded provided the networks have constant depth. Applying Theorem 7.10 gives the following result <ref> [48, 83] </ref>, in which no bound on the depth is necessary. Theorem 7.11 Let fN W g be a family of feedforward neural networks with binary outputs, having piecewise polynomial functions of bounded degree and bounded number of pieces as activation functions on the hidden units.
Reference: [84] <author> A. Macintyre and E. D. Sontag. </author> <title> Finiteness results for sigmoidal "neural" networks. </title> <booktitle> In Proceedings of 25th Annual ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 325-334. </pages> <publisher> ACM Press, </publisher> <year> 1993. </year>
Reference-contexts: Elaborations on this result have been obtained by Maass [79]. Whether this bound can be improved to O (W log W ) is an open problem. Returning to sigmoid functions, the following result was obtained by Macintyre and Sontag <ref> [84] </ref>, using deep results from logic. Theorem 7.12 Let N be a feedforward network with binary output and the standard sigmoid function as activation function on the hidden units. Then N has finite VC-dimension. <p> Furthermore, in such cases, the bound depends on the pseudo-dimension of the set of functions computed by the network, rather than on the pseudo-dimension of the associated loss space. Macintyre and Sontag <ref> [84] </ref> proved the finiteness of the pseudo-dimension of feedforward neural networks having as activation the standard sigmoid function. However, no explicit bounds were given. Bartlett and Williamson [24] obtained the following result, relevant for discrete inputs. <p> Nonetheless, the sample length bounds are similar to those obtained for linear threshold networks. Although in this theorem there is assumed to be some uniform upper bound V on the maximum magnitude of the weights, the result of Macintyre and Sontag <ref> [84] </ref> mentioned earlier shows that if every activation function is the standard sigmoid function and if there is one output node, then such a bound is not necessary.
Reference: [85] <author> C. McDiarmid. </author> <title> On the method of bounded differences. </title> <editor> In J. Siemons, editor, </editor> <booktitle> Surveys in Combinatorics, 1989, London Mathematical Society Lecture Note Series (141). </booktitle> <publisher> Cambridge University Press, </publisher> <address> Cambridge, UK, </address> <year> 1989. </year>
Reference-contexts: If m r=32*, the probability that a sample of length m has at least d=2 entries from F is therefore, by a standard Chernoff bound (see <ref> [85] </ref>, for example), at most 93=100. Therefore if m r=(32*) = (d 1)=(32*), we have m fs 2 S (m; t) : er (L (s)) *g 7 100 1 ; as required. Now we prove the second bound.
Reference: [86] <author> M. Marchand and S. Hadjifaradji. </author> <title> Strong unimodality and exact learning of constant depth - perceptron networks. </title> <editor> in D.S. Touretzky, M.C. Moxer and M.E. Hasselmo, eds., </editor> <booktitle> Advances in Neural Information Processing Systems 8, </booktitle> <pages> 288-294, </pages> <publisher> MIT Press, </publisher> <address> Cambridge MA, </address> <year> 1996. </year>
Reference-contexts: For further discussion of distribution-dependent learning, we refer the reader to the papers of Benedek and Itai [34], Ben-David, Benedek and Mansour [29], Bertoni et al. [35], Kharitonov [74], Li and Vitanyi [77], Linial, Mansour and Nisan [78]. For discussion specific to neural networks, see <ref> [52, 86] </ref>. 11 Graph Dimension and Multiple-Output Nets The basic PAC model concerns learning f0; 1g-valued functions only; that is, it is concerned only with classification problems.
Reference: [87] <author> C. Micchelli. </author> <title> Interpolation of scattered data: Distance matrices and conditionally positive definite functions. Constructive Approximation, </title> <booktitle> 2 </booktitle> <pages> 11-22, </pages> <year> 1986. </year>
Reference-contexts: We have the following result, which is obtained using interpolation results of Micchelli <ref> [87] </ref>. Details of the proof may be found in [16, 15, 64], where further results along the same lines are presented.
Reference: [88] <author> B. Natarajan. </author> <title> Probably approximate learning over classes of distributions. </title> <journal> SIAM J. Computing, </journal> <volume> 21(3) </volume> <pages> 438-449, </pages> <year> 1992. </year>
Reference-contexts: Let us consider the case P = fg, in which P consists of just one distribution. The results of Vapnik and Chervonenkis (see also <ref> [88] </ref>) imply that H is learnable with respect to , by any consistent learning algorithm, if E m (log (x)) ! 0 as m ! 1; where E m (:) denotes expected value with respect to the distribution m .
Reference: [89] <author> B. K. Natarajan. </author> <title> On learning sets and functions. </title> <journal> Machine Learning, </journal> <volume> 4(1), </volume> <year> 1989. </year>
Reference-contexts: It forms the weighted sum of its inputs and it outputs 1 if this sum exceeds its threshold and 0 otherwise. The following result, a bound on the VC-dimension of such networks on binary inputs, is due to Natara-jan <ref> [89] </ref>. Theorem 7.5 Let N be a linear threshold network with N neurons (including the input nodes), n input nodes and a total of W variable weights and thresholds. <p> The probability that the algorithm fails in k attempts is at most (1=2) k , which approaches zero very rapidly with increasing k. The following result is from [93]. (See also <ref> [89, 56] </ref>.) Theorem 8.1 Let H = S H n be a hypothesis space and suppose that there is a PAC learning algorithm for H which is efficient with respect to accuracy and example size. <p> Further, let GH = fGh : h 2 Hg, the graph space of H. We have the following definition <ref> [89] </ref>. 10 In what follows, certain technical measure-theoretic conditions have to be satisfied; we shall not discuss these, but the reader may find the details in the paper of Haussler [55] or the book by Pollard [95].
Reference: [90] <author> B. K. Natarajan. </author> <title> Machine Learning: A Theoretical Approach. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, Cali-fornia, </address> <year> 1991. </year>
Reference-contexts: I have placed emphasis on those topics I find to be of most interest and, consequently, there is more discussion of sample complexity than of computational complexity. There are now a number of books dealing with probabilistic models of learning: the interested reader might consult <ref> [115, 114, 95, 73, 90, 40, 10] </ref> for further information. The first part of this work concerns the basic PAC model, applicable for classification problems. The second part concerns extensions of the basic PAC model, such as those relevant to neural networks with real-valued outputs. <p> Then VCdim (N ; f0; 1g n ) W N log N: Proof The proof uses a result due to Hong (see <ref> [90] </ref> for details), which states that a linear threshold network with N neurons and f0; 1g-inputs need only use integer weights which can be encoded using N log N binary bits. It follows that the number of distinct functions computable by the network is at most . <p> It is natural to ask whether finite graph dimension is a necessary condition for learnability in this generalized model. Natarajan showed that it is not: there are PAC learnable function spaces with infinite graph dimension (see Natarajan <ref> [90] </ref>). Natarajan finds a weaker necessary condition for learnability, showing that a certain measure, now known as the Natarajan dimension, must be finite for H to be PAC learnable.
Reference: [91] <author> B. K. Natarajan. </author> <title> Occam's razor for functions. </title> <booktitle> In Proc. 6th Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 370-376. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1993. </year>
Reference-contexts: Further, it shows that if there is some noise in the classification of the training sample, then learning `up to the level of the noise' is possible in some cases. (See <ref> [91, 26] </ref> for other results on function learning in the presence of noise.) Theorem 14.8 Let H be a set of functions from X to [0; M ], for some M . Suppose H has finite pseudo dimension and that C H. <p> In that paper, it is shown that to obtain accurate bounds on the sample complexity function m L (and, indeed, bounds depending on ), it is more appropriate to use a scale-sensitive dimension termed the band-dimension, also used in <ref> [91] </ref>. In [9], it is also shown that finite pseudo-dimension of H is a necessary condition for the conclusion of Theorem 14.8 to hold. Thus the condition described in this theorem is stronger than learnability.
Reference: [92] <author> G. Pagallo and D. Haussler. </author> <title> A greedy method for learning -DNF functions under the uniforn distribution. </title> <type> Technical report, </type> <institution> University of California at Santa Cruz, UCSC-CRL-89-12, </institution> <year> 1989. </year>
Reference-contexts: Furthermore, learning with respect to a particular distribution or class of distributions may be computationally much easier than PAC learning; see <ref> [45, 117, 44, 68, 92, 50, 78, 46] </ref>. Neural Computing Surveys 1, 1-47, 1997, http://www.icsi.berkeley.edu/~jagota/NCS 27 The original statistical work of Vapnik and Chervonenkis [116] has something to say about learning with respect to particular distributions.
Reference: [93] <author> L. Pitt and L. Valiant. </author> <title> Computational limitations on learning from examples. </title> <journal> J. ACM, </journal> <volume> 35 </volume> <pages> 965-984, </pages> <year> 1988. </year>
Reference-contexts: The probability that the algorithm fails in k attempts is at most (1=2) k , which approaches zero very rapidly with increasing k. The following result is from <ref> [93] </ref>. (See also [89, 56].) Theorem 8.1 Let H = S H n be a hypothesis space and suppose that there is a PAC learning algorithm for H which is efficient with respect to accuracy and example size. <p> In this sense, the hardness result just given is `representation-dependent' <ref> [93] </ref>. Hardness results of a `representation-independent' nature, in which the only specification on the hypothesis space is the reasonable one that it be `polynomially evaluatable', have been obtained using assumptions about the intractability of certain problems arising in cryptography; see, for example [72]. <p> We have barely touched on the computational complexity of PAC learning and its variants. Further discussion of this may be found, for example, in <ref> [82, 21, 10, 73, 93, 69, 70, 101, 63, 79] </ref>. There are also many more variants of the PAC model which we have not mentioned here. Throughout the discussion here, it has been assumed that there is some fixed hypothesis space.
Reference: [94] <author> L. Pitt and M. K. Warmuth. </author> <title> Prediction preserving reducibility. </title> <journal> J. of Comput. Syst. Sci., </journal> <volume> 41(3) </volume> <pages> 430-467, </pages> <month> December </month> <year> 1990. </year> <booktitle> Special issue on the Third Annual Conference of Structure in Complexity Theory (Washington, </booktitle> <address> DC., </address> <month> June 88). </month>
Reference-contexts: However, there is a variant of the PAC model known as the prediction model, in which there is no fixed hypothesis space; in a sense, the output of a prediction algorithm is some program which classifies further examples. We refer the reader to <ref> [94, 57] </ref> for details. Another very important variant is that in which the learning algorithm can ask questions concerning, for example, the classification of a chosen example; see, for example [2, 28, 53].
Reference: [95] <author> D. Pollard. </author> <title> Convergence of Stochastic Processes. </title> <publisher> Springer-Verlag, </publisher> <year> 1984. </year>
Reference-contexts: I have placed emphasis on those topics I find to be of most interest and, consequently, there is more discussion of sample complexity than of computational complexity. There are now a number of books dealing with probabilistic models of learning: the interested reader might consult <ref> [115, 114, 95, 73, 90, 40, 10] </ref> for further information. The first part of this work concerns the basic PAC model, applicable for classification problems. The second part concerns extensions of the basic PAC model, such as those relevant to neural networks with real-valued outputs. <p> This technique, relating the required probability to the probability of a `sample-based' event, is known as `symmetrization' <ref> [95] </ref>. The next part of the proof is to bound the probability of this latter event, using a technique known as `combinatorial bounding'. Let fl be the `swapping' subgroup of the symmetric group of degree 2m. <p> The probability measure P is defined on the product -algebra fi 2 f0;1g . To ensure the measurability of the sets occuring in the result and its proof, one assumes that H satisfies certain conditions. These conditions, which may be found in <ref> [95] </ref>, will hold in all cases considered here. <p> We have the following definition [89]. 10 In what follows, certain technical measure-theoretic conditions have to be satisfied; we shall not discuss these, but the reader may find the details in the paper of Haussler [55] or the book by Pollard <ref> [95] </ref>. Neural Computing Surveys 1, 1-47, 1997, http://www.icsi.berkeley.edu/~jagota/NCS 29 Definition 11.1 For a set H of functions from X to Y , define the graph dimension of H, denoted gdim (H), is the VC-dimension of the set GH of f0; 1g-valued functions on X fi Y . <p> But there are other such measures. In passing, we have already mentioned the Natarajan dimension. We now introduce a very useful dimension, known as the pseudo-dimension. This was introduced by Pollard <ref> [95] </ref> and is defined whenever the set of functions maps into Y R. (More generally, it may be defined when Y Neural Computing Surveys 1, 1-47, 1997, http://www.icsi.berkeley.edu/~jagota/NCS 31 is any totally ordered set, but this shall not concern us here.) Let H be a set of functions from X to <p> If there is no finite *-cover for some , or if the supremum does not exist, we say that the *-capacity is infinite. 12 Results of Haussler [55] and Pollard <ref> [95] </ref> provide the following uniform bound on the rate of convergence of observed errors to actual errors. <p> Neural Computing Surveys 1, 1-47, 1997, http://www.icsi.berkeley.edu/~jagota/NCS 35 When k = 1 and H maps into [0; M ], the capacity can be related to the pseudo-dimension of H. Haus sler [55] (see also Pollard <ref> [95] </ref>) showed that if H has finite pseudo-dimension then C H (*) &lt; 2 2eM ln * pdim (H) : This, combined with the above result, shows that, in this case, H has the UCE property and that a sufficient sample length m 0 (ffi; *) is m 0 (ffi; *)
Reference: [96] <author> K. Romanik. </author> <title> Testing as a dual to learning. </title> <type> Technical Report UMIACS-TR-91.93, </type> <institution> CS-TR-2704, Dept. of Computer Science, University of Maryland, </institution> <month> June </month> <year> 1991. </year>
Reference-contexts: that the training examples are no longer randomly chosen.) Such models of teaching have been studied in the case where the goal is to learn the target exactly [49, 13, 65, 108, 14] and in which the goal is to learn an approximation to the target in a probabilistic sense <ref> [97, 96] </ref>.
Reference: [97] <author> K. Romanik. </author> <title> Approximate testing and learnability. </title> <booktitle> In Proc. 5th Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 327-332. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1992. </year>
Reference-contexts: that the training examples are no longer randomly chosen.) Such models of teaching have been studied in the case where the goal is to learn the target exactly [49, 13, 65, 108, 14] and in which the goal is to learn an approximation to the target in a probabilistic sense <ref> [97, 96] </ref>.
Reference: [98] <author> A. Sakurai. </author> <title> Tighter bounds of the VC-dimension of three-layer networks. </title> <booktitle> In Proceedings of World Congress on Neural Networks, </booktitle> <pages> pages 540-543, </pages> <year> 1993. </year>
Reference-contexts: This result has subsequently been improved by Sakurai who, in <ref> [98] </ref>, announces an upper bound of W (log (N 1) + o (log N )), as N ! 1. The constants in the bound are not of any great theoretical significance. <p> The result shows that no upper bound better than order W log W can be given: to within a constant, the bound of Theorem 7.6 is tight. The networks of Theorem 7.8 have depth at least three. The following result of Sakurai <ref> [98] </ref> shows that there are feedforward linear threshold networks of depth two (with just one hidden layer) having superlinear VC-dimension on real inputs. These networks are smaller than those of Theorem 7.8, but the result concerns real inputs, not binary inputs and hence is not immediately comparable with Theorem 7.8.
Reference: [99] <author> N. Sauer. </author> <title> On the density of families of sets. </title> <journal> Journal of Combinatorial Theory (A), </journal> <volume> 13 </volume> <pages> 145-147, </pages> <year> 1972. </year>
Reference-contexts: But there is another, less obvious, relationship: the growth function F (m) can be bounded by a polynomial function of m, and the degree of the polynomial is the VC-dimension d of F . Explicitly, we have the following theorem <ref> [99, 107, 116] </ref>, usually known as Sauer's Lemma. The second inequality is elementary|a proof was given by Blumer et al. [37].
Reference: [100] <author> R. E. Schapire. </author> <title> The strength of weak learnability. </title> <journal> Machine Learning, </journal> <volume> 5(2) </volume> <pages> 197-227, </pages> <year> 1990. </year>
Reference-contexts: Other useful variants not discussed here are those in which the distribution and target are permitted to change a little between observations, as in [25, 58, 59], models of weak learning in which the learner only has to do slightly better than random guessing <ref> [51, 100, 60] </ref>, and variants in which the learning algorithm has access to the predictions of `experts' [38]. Something which has not been discussed in any detail here is the use of real-output neural networks for classification.
Reference: [101] <author> R. E. Schapire. </author> <title> The Design and Analysis of Efficient Learning Algorithms. ACM Distinguished Dissertation Awards Series. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA., </address> <year> 1991. </year>
Reference-contexts: We have barely touched on the computational complexity of PAC learning and its variants. Further discussion of this may be found, for example, in <ref> [82, 21, 10, 73, 93, 69, 70, 101, 63, 79] </ref>. There are also many more variants of the PAC model which we have not mentioned here. Throughout the discussion here, it has been assumed that there is some fixed hypothesis space.
Reference: [102] <author> J. Shawe-Taylor. </author> <title> Building symmetries into feedforward network architectures. </title> <booktitle> In Proceedings of First IEE Conference on Artificial Neural Networks, </booktitle> <pages> pages 158-162, </pages> <year> 1989. </year>
Reference-contexts: Suppose that there are known symmetries inherent in the class of concepts being learned. These symmetries may be reflected in learning algorithms for the neural network, by constraining groups of weights to have the same value. Such techniques have been discussed in <ref> [102] </ref>, for example. Shawe-Taylor [103] has bounded the sample complexity of such learning algorithms by bounding the growth function in a manner similar to that in [27].
Reference: [103] <author> J. Shawe-Taylor. </author> <title> Sample sizes for threshold networks with equivalences. </title> <journal> Information and Computation, </journal> <volume> 118(1): </volume> <pages> 65-72, </pages> <year> 1995. </year> <booktitle> Neural Computing Surveys 1, </booktitle> <pages> 1-47, </pages> <year> 1997, </year> <note> http://www.icsi.berkeley.edu/~jagota/NCS 47 </note>
Reference-contexts: Suppose that there are known symmetries inherent in the class of concepts being learned. These symmetries may be reflected in learning algorithms for the neural network, by constraining groups of weights to have the same value. Such techniques have been discussed in [102], for example. Shawe-Taylor <ref> [103] </ref> has bounded the sample complexity of such learning algorithms by bounding the growth function in a manner similar to that in [27]. His results, following [19, 104], also apply more generally to networks having more than one output node, but we shall not discuss this aspect here. <p> We do not explicitly present a bound on the sample complexity of a consistent learning algorithm as a PAC algorithm, but such a bound could be derived either from the graph dimension bound or (better) from the bound on the growth function (see <ref> [19, 104, 103] </ref>). We first note that there is another way of describing the notion of graph dimension. <p> It follows that if one can bound the quantity H (m), then a bound on the growth function of the graph space, and hence on the graph dimension, can be obtained. This is the technique used in obtaining the following result. (See <ref> [104, 19, 103] </ref> for improvements on this.) Theorem 11.3 Suppose that N is a feedforward linear threshold network having W variable parameters (weights and thresholds) and any number of output units. Let H be the set H N of functions computable by N .
Reference: [104] <author> J. Shawe-Taylor and M. Anthony. </author> <title> Sample sizes for multiple output threshold networks. Network: </title> <booktitle> Computation in Neural Systems, </booktitle> <volume> 2 </volume> <pages> 107-117, </pages> <year> 1991. </year>
Reference-contexts: Such techniques have been discussed in [102], for example. Shawe-Taylor [103] has bounded the sample complexity of such learning algorithms by bounding the growth function in a manner similar to that in [27]. His results, following <ref> [19, 104] </ref>, also apply more generally to networks having more than one output node, but we shall not discuss this aspect here. Let N be a feedforward linear threshold network. <p> We do not explicitly present a bound on the sample complexity of a consistent learning algorithm as a PAC algorithm, but such a bound could be derived either from the graph dimension bound or (better) from the bound on the growth function (see <ref> [19, 104, 103] </ref>). We first note that there is another way of describing the notion of graph dimension. <p> It follows that if one can bound the quantity H (m), then a bound on the growth function of the graph space, and hence on the graph dimension, can be obtained. This is the technique used in obtaining the following result. (See <ref> [104, 19, 103] </ref> for improvements on this.) Theorem 11.3 Suppose that N is a feedforward linear threshold network having W variable parameters (weights and thresholds) and any number of output units. Let H be the set H N of functions computable by N .
Reference: [105] <author> J. Shawe-Taylor, M. Anthony, and N. Biggs. </author> <title> Bounding sample-size with the Vapnik-Chervonenkis dimension. </title> <journal> Discrete Applied Mathematics, </journal> <volume> 42 </volume> <pages> 65-73, </pages> <year> 1993. </year>
Reference-contexts: Results of a similar form, but specifically for learning theory applications, were given by Blumer et al. [37], who were the first to highlight the importance of this area of probability for the theory of PAC learning. Their results were subsequently improved in <ref> [12, 105] </ref>. The precise result presented here is a slight improvement of a special case of a result of Vapnik, and the proof is based on one from [5]. <p> The only other observation needed is that, in this case, P (E h ) is precisely er (h; t). ut The constants in the sample complexity bound of the above theorem can be improved; see <ref> [12, 105] </ref>. We now present a lower bound result, part of which is due to Ehrenfeucht et al. [43] and part of which is due to Blumer et al. [37].
Reference: [106] <author> J. Shawe-Taylor, P. Bartlett, R. Williamson, M. Anthony. </author> <title> Structural risk minimisation over data-dependent hierarchies. </title> <note> submitted. </note>
Reference-contexts: Something which has not been discussed in any detail here is the use of real-output neural networks for classification. Recent work <ref> [106, 23] </ref> has shown that, here, the scale-sensitive fl-dimension is very useful. Neural Computing Surveys 1, 1-47, 1997, http://www.icsi.berkeley.edu/~jagota/NCS 41 Acknowledgements I thank Peter Bartlett, Nicolo Cesa-Bianchi and Mark Jerrum for comments and suggestions on an earlier version of this article.
Reference: [107] <author> S. Shelah. </author> <title> A combinatorial problem: Stability and order for models and theories in infinitary languages. </title> <journal> Pacific Journal of Mathematics, </journal> <volume> 41 </volume> <pages> 247-261, </pages> <year> 1972. </year>
Reference-contexts: But there is another, less obvious, relationship: the growth function F (m) can be bounded by a polynomial function of m, and the degree of the polynomial is the VC-dimension d of F . Explicitly, we have the following theorem <ref> [99, 107, 116] </ref>, usually known as Sauer's Lemma. The second inequality is elementary|a proof was given by Blumer et al. [37].
Reference: [108] <author> A. Shinohara and S. Miyano. </author> <title> Teachability in computational learning. </title> <journal> New Generation Computing, </journal> <volume> 8 </volume> <pages> 337-347, </pages> <year> 1991. </year>
Reference-contexts: becomes when there is a `helpful teacher' providing cleverly-chosen examples as training sample. (This is very different from the PAC model in that the training examples are no longer randomly chosen.) Such models of teaching have been studied in the case where the goal is to learn the target exactly <ref> [49, 13, 65, 108, 14] </ref> and in which the goal is to learn an approximation to the target in a probabilistic sense [97, 96].
Reference: [109] <author> H. U. Simon. </author> <title> General bounds on the number of examples needed for learning probabilistic concepts. </title> <booktitle> In Proceedings of the Sixth Annual ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 402-411. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1993. </year> <title> Full version: </title> <journal> J. of Comp. and Sys. Sci. </journal> <volume> 52(2): </volume> <pages> 239-254, </pages> <year> 1996. </year>
Reference-contexts: The bound given here follows by an argument analogous to that given in the proof of Theorem 14.7 below.) Note that the condition in this theorem|finite fl-dimension for all fl|is a weaker condition than that of having finite pseudo-dimension. In related work, Simon <ref> [109] </ref> has obtained general lower bounds on the sample complexity of p-concept learning algorithms in terms of a different scale-sensitive dimension.
Reference: [110] <author> H. U. Simon. </author> <title> Bounds on the number of examples needed for learning functions. In Computational Learning Theory: </title> <publisher> Eurocolt'93. Oxford University Press, </publisher> <year> 1994. </year>
Reference-contexts: Using a different scale-sensitive dimension|which may be thought of as a scale-sensitive version of the Natarajan dimension|Simon <ref> [110] </ref> has obtained general lower bounds on the sample complexity of noiseless learning. He has shown that, to within a logarithmic factor, this bound is optimal for a number of spaces H.
Reference: [111] <author> E. Sontag. </author> <title> Feedforward nets for interpolation and classification. </title> <journal> J. Comp. Syst. Sci, </journal> <volume> 45 </volume> <pages> 20-48, </pages> <year> 1992. </year>
Reference-contexts: We now consider networks with one linear threshold output unit and sigmoid activation functions at all hidden units. (The linear threshold output unit ensures that the network computes f0; 1g-valued functions, so that it is legitimate to discuss the VC dimension.) Sontag has shown (see <ref> [111, 83] </ref>) that there is a neural network N of infinite VC-dimension, having two real inputs, two hidden units with sigmoid activation function f (y) = and a linear threshold output unit.
Reference: [112] <author> L.G. Valiant. </author> <title> Deductive learning. </title> <journal> Phil. Trans. Roy. Soc. Lond. A, </journal> <volume> 312 </volume> <pages> 441-446, </pages> <year> 1984. </year>
Reference-contexts: Neural Computing Surveys 1, 1-47, 1997, http://www.icsi.berkeley.edu/~jagota/NCS 3 PART 1: THE BASIC PAC MODEL 2 The Basic PAC Model of Learning In this section, we describe the basic `probably approximately correct' (PAC) model of learning, which arises from the work of Valiant <ref> [113, 112] </ref> and Vapnik and Chervonenkis [116, 114]. This model is applicable to neural networks with one output unit which outputs either the value 0 or 1. Thus, it applies to neural network classification problems.
Reference: [113] <author> L. G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Commun. ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <month> Nov. </month> <year> 1984. </year>
Reference-contexts: Neural Computing Surveys 1, 1-47, 1997, http://www.icsi.berkeley.edu/~jagota/NCS 3 PART 1: THE BASIC PAC MODEL 2 The Basic PAC Model of Learning In this section, we describe the basic `probably approximately correct' (PAC) model of learning, which arises from the work of Valiant <ref> [113, 112] </ref> and Vapnik and Chervonenkis [116, 114]. This model is applicable to neural networks with one output unit which outputs either the value 0 or 1. Thus, it applies to neural network classification problems. <p> This leads us to the following formal definition of PAC learning, first given by Valiant in 1984 <ref> [113] </ref> 3 : Definition 2.1 (PAC learning algorithm) Let L be a (C; H)-learning algorithm.
Reference: [114] <author> V. N. Vapnik. </author> <title> Estimation of Dependences Based on Empirical Data. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1982. </year>
Reference-contexts: I have placed emphasis on those topics I find to be of most interest and, consequently, there is more discussion of sample complexity than of computational complexity. There are now a number of books dealing with probabilistic models of learning: the interested reader might consult <ref> [115, 114, 95, 73, 90, 40, 10] </ref> for further information. The first part of this work concerns the basic PAC model, applicable for classification problems. The second part concerns extensions of the basic PAC model, such as those relevant to neural networks with real-valued outputs. <p> Neural Computing Surveys 1, 1-47, 1997, http://www.icsi.berkeley.edu/~jagota/NCS 3 PART 1: THE BASIC PAC MODEL 2 The Basic PAC Model of Learning In this section, we describe the basic `probably approximately correct' (PAC) model of learning, which arises from the work of Valiant [113, 112] and Vapnik and Chervonenkis <ref> [116, 114] </ref>. This model is applicable to neural networks with one output unit which outputs either the value 0 or 1. Thus, it applies to neural network classification problems. <p> Each of these has basis ff 0 ; f 1 ; : : : ; f n g, where f 0 is the identically-1 function and f i (x) = x i . ut 5 A Useful Probability Theorem In this section, we prove a result of Vapnik <ref> [114] </ref>. This result and related ones have been central to the mathematical development of PAC learning theory. The paper of Vapnik and Chervonenkis [116] gave the first such results. <p> If this is the case, then a learning algorithm which outputs a hypothesis with near-minimal observed error will be a probably approximately optimal learning algorithm. More precisely, we have the following result <ref> [55, 114] </ref>. Neural Computing Surveys 1, 1-47, 1997, http://www.icsi.berkeley.edu/~jagota/NCS 33 Theorem 12.3 Suppose that H is a hypothesis space of functions from X to Y and that l is a loss function defined on Y fi Y .
Reference: [115] <author> V. N. Vapnik. </author> <title> The Nature of Statistical Learning Theory. </title> <publisher> Springer, </publisher> <address> New York, </address> <year> 1995. </year>
Reference-contexts: I have placed emphasis on those topics I find to be of most interest and, consequently, there is more discussion of sample complexity than of computational complexity. There are now a number of books dealing with probabilistic models of learning: the interested reader might consult <ref> [115, 114, 95, 73, 90, 40, 10] </ref> for further information. The first part of this work concerns the basic PAC model, applicable for classification problems. The second part concerns extensions of the basic PAC model, such as those relevant to neural networks with real-valued outputs.
Reference: [116] <author> V. N. Vapnik and A. Y. Chervonenkis. </author> <title> On the uniform convergence of relative frequencies of events to their probabilities. </title> <journal> Theory of Probab. and its Applications, </journal> <volume> 16(2) </volume> <pages> 264-280, </pages> <year> 1971. </year>
Reference-contexts: Neural Computing Surveys 1, 1-47, 1997, http://www.icsi.berkeley.edu/~jagota/NCS 3 PART 1: THE BASIC PAC MODEL 2 The Basic PAC Model of Learning In this section, we describe the basic `probably approximately correct' (PAC) model of learning, which arises from the work of Valiant [113, 112] and Vapnik and Chervonenkis <ref> [116, 114] </ref>. This model is applicable to neural networks with one output unit which outputs either the value 0 or 1. Thus, it applies to neural network classification problems. <p> But there is another, less obvious, relationship: the growth function F (m) can be bounded by a polynomial function of m, and the degree of the polynomial is the VC-dimension d of F . Explicitly, we have the following theorem <ref> [99, 107, 116] </ref>, usually known as Sauer's Lemma. The second inequality is elementary|a proof was given by Blumer et al. [37]. <p> This result and related ones have been central to the mathematical development of PAC learning theory. The paper of Vapnik and Chervonenkis <ref> [116] </ref> gave the first such results. Results of a similar form, but specifically for learning theory applications, were given by Blumer et al. [37], who were the first to highlight the importance of this area of probability for the theory of PAC learning. <p> Because the action of fl is measure preserving (with respect to the product measure P 2m ), it can easily be shown (see, for example, <ref> [37, 116] </ref>) that P 2m (R) jflj : Fix z = ((x 1 ; b 1 ); (x 2 ; b 2 ); : : : ; (x 2m ; b 2m )) 2 S 2m and let x = (x 1 ; x 2 ; : : : ; x <p> Results of Vapnik and Chervonenkis <ref> [116] </ref> on the uniform convergence of relative frequencies to probabilities show that H has the UCE property if and only if H has finite VC-dimension. (The fact that the VC-dimension is sufficient follows from Theorem 5.1.) It is easy to see that if H has the UCE property then there is <p> Furthermore, learning with respect to a particular distribution or class of distributions may be computationally much easier than PAC learning; see [45, 117, 44, 68, 92, 50, 78, 46]. Neural Computing Surveys 1, 1-47, 1997, http://www.icsi.berkeley.edu/~jagota/NCS 27 The original statistical work of Vapnik and Chervonenkis <ref> [116] </ref> has something to say about learning with respect to particular distributions. Let us consider the case P = fg, in which P consists of just one distribution.
Reference: [117] <author> K. Verbeurgt. </author> <title> Learning DNF under the uniform distribution in quasi-polynomial time. </title> <booktitle> In Proc. 3rd Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 314-326. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: Furthermore, learning with respect to a particular distribution or class of distributions may be computationally much easier than PAC learning; see <ref> [45, 117, 44, 68, 92, 50, 78, 46] </ref>. Neural Computing Surveys 1, 1-47, 1997, http://www.icsi.berkeley.edu/~jagota/NCS 27 The original statistical work of Vapnik and Chervonenkis [116] has something to say about learning with respect to particular distributions.
Reference: [118] <author> D. </author> <title> Wolpert (editor). The Mathematics of Generalization: </title> <booktitle> the Proceedings of the SFI/CNLS Workshop on Formal Approaches to Supervised Learning, </booktitle> <address> Santa Fe, 1992. </address> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1995. </year>
Reference-contexts: This is the probabilistic framework which has become known as the `probably approximately correct', or PAC, model of learning. Of course, there are many other mathematical frameworks in which to discuss learning and generalisation (see, for instance <ref> [118] </ref>), and I make no claim that the framework discussed here is superior to others discussed elsewhere. I do not survey here the whole area of the PAC model and its important variants.
Reference: [119] <author> K. Yamanishi. </author> <title> A learning criterion for stochastic rules. </title> <booktitle> Machine Learning 9: </booktitle> <pages> 165-203, </pages> <year> 1992. </year>
Reference-contexts: Alternatively, the problem may not lie with the teacher, but with the `concept' itself. This may be ill-formed and may not be a function at all. To deal with these situations, we have the notion of a stochastic concept <ref> [70, 119, 37] </ref>. A stochastic concept on X is simply a probability distribution P on X fi f0; 1g. Informally, for finite or countable X, one interprets P ((x; b)) to be the probability that x will be given classification b.
References-found: 119

