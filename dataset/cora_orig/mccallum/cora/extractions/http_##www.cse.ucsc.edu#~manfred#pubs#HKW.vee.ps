URL: http://www.cse.ucsc.edu/~manfred/pubs/HKW.vee.ps
Refering-URL: http://www.cse.ucsc.edu/~manfred/pubs.html
Root-URL: http://www.cse.ucsc.edu
Email: e-mail haussler@cse.ucsc.edu.  e-mail jkivinen@cs.helsinki.fi  e-mail manfred@cse.ucsc.edu.  
Title: Sequential prediction of individual sequences under general loss functions N depending on the loss function,
Author: David Haussler Jyrki Kivinen Manfred K. Warmuth 
Note: log  Supported by NSF grant IRI-9123692 and DOE grant DE-FG03-95ER62112;  Supported by Emil Aaltonen Foundation, the Academy of Finland, and ESPRIT Project NeuroCOLT;  Supported by NSF grants IRI-9123692 and CCR 9700201;  
Address: Santa Cruz, CA 95064 USA  P.O. Box 26 (Teollisuuskatu 23)  Finland  Santa Cruz, CA 95064 USA  
Affiliation: Computer Science University of California, Santa Cruz  Department of Computer Science  University of Helsinki  Computer Science University of California, Santa Cruz  
Pubnum: FIN-00014  
Abstract: We consider adaptive sequential prediction of arbitrary binary sequences when the performance is evaluated using a general loss function. The goal is to predict on each individual sequence nearly as well as the best prediction strategy in a given comparison class of (possibly adaptive) prediction strategies, called experts. By using a general loss function, we generalize previous work on universal prediction, forecasting, and data compression. However, here we restrict ourselves to the case when the comparison class is finite. For a given sequence, we define the regret as the total loss on the entire sequence suffered by the adaptive sequential predictor, minus the total loss suffered by the predictor in the comparison class that performs best on that particular sequence. We show that for a large class of loss functions, the minimax regret is either fi(log N) or ( fl Preliminary results have appeared in Computational Learning Theory: EuroCOLT '93, pp. 109-120, Clarendon Press, Oxford, 1994, and in Computational Learning Theory: Second European Conference, Euro-COLT '95, pp. 69-83, Springer, 1995, and in Technical Report UCSC-CRL-94-36, University of California, Santa Cruz, 1994. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. Schapire, </author> <title> "Gambling in a rigged casino: the adversarial multi-armed bandit problem," </title> <booktitle> in Proc. 36th Annual Symposium on Foundations of Computer Science, </booktitle> <address> Los Alamitos, CA: </address> <publisher> IEEE Computer Society Press, </publisher> <year> 1995, </year> <pages> pp. 322-331. </pages>
Reference-contexts: At each time step or trial t, after seeing the outcomes y 1 ; : : : ; y t1 , you must predict the next outcome y t by producing a number ^y t 2 <ref> [0; 1] </ref>. When the actual next outcome y t is revealed, you then suffer a loss L (y t ; ^y t ), where L is a fixed loss function. One example of this scenario is in producing sequential probability assignments for individual binary sequences [29, 32, 33, 39]. <p> Closely related work has also been done in the area of mathematical finance, where one seeks a stock portfolio re-balancing strategy that performs almost as well as the best strategy in a given comparison class on any market [12, 22]. More general decision-theoretic scenarios are considered in <ref> [10, 37, 1] </ref>. Also related is the work on on-line competitive algorithms in computer science (see e.g. [34]). In this paper we focus on the case in which the comparison class E is finite. Our goal is to develop the most general results possible for this finite case. <p> Finally, in Subsection 4.1 we show that for certain loss functions, such as the square and logarithmic loss, Vovk's algorithm achieves the same worst-case regret even if the outcomes are allowed to be arbitrary real numbers in the interval <ref> [0; 1] </ref>. <p> outcomes by using a slightly more complicated algorithm, as we show in Subsection 4.2. 2 On-line prediction and loss bounds We consider the predictive performance of an on-line learning algorithm A on a sequence of outcomes y 1 ; : : : ; y ` , where y t 2 <ref> [0; 1] </ref> for each 1 t `. The algorithm's performance is compared to that of the best expert in a given set E = f E 1 ; : : : ; E N g of experts, each of which is an arbitrary on-line prediction strategy. <p> The prediction of the expert E i for the outcome y t is denoted by x t;i and is a real number in the interval <ref> [0; 1] </ref>. This prediction can depend on the previous (and current) predictions of the other experts as well as the previous outcomes. <p> The vector of predictions by all the experts for trial t, called the prediction vector, is defined by x t = (x t;1 ; : : : ; x t;N ). When the algorithm makes its prediction ^y t 2 <ref> [0; 1] </ref> for the outcome y t , we assume that it has access to all previous outcomes, as well as all previous predictions of the experts, including the predictions x t for the current trial t. <p> We consider separately the cases of binary outcomes, with the outcomes 5 y t either 0 or 1, and continuous-valued outcomes, with y t any real number from the interval <ref> [0; 1] </ref>. The performance of the learner at trial t is measured by L (y t ; ^y t ), where L is a loss function with the range [0; 1), or sometimes [0; 1]. <p> The performance of the learner at trial t is measured by L (y t ; ^y t ), where L is a loss function with the range <ref> [0; 1), or sometimes [0; 1] </ref>. For binary outcomes y t 2 f 0; 1 g it suffices to consider the functions L 0 and L 1 defined by L 0 (^y) = L (0; ^y) and L 1 (^y) = L (1; ^y). <p> t either 0 or 1, and continuous-valued outcomes, with y t any real number from the interval <ref> [0; 1] </ref>. The performance of the learner at trial t is measured by L (y t ; ^y t ), where L is a loss function with the range [0; 1), or sometimes [0; 1]. For binary outcomes y t 2 f 0; 1 g it suffices to consider the functions L 0 and L 1 defined by L 0 (^y) = L (0; ^y) and L 1 (^y) = L (1; ^y). <p> In each case, the function L 0 is increasing and L 1 decreasing in <ref> [0; 1] </ref>, so the loss L (y; ^y) increases as the prediction ^y moves away from the outcome y. The functions L 0 and L 1 are differentiable, and by the previous remark, L 0 0 (z) 0 and L 0 z. <p> We let V L;A (N; `) = sup V L;A (((x 1 ; y 1 ); : : : ; (x ` ; y ` ))) j x t 2 <ref> [0; 1] </ref> N ; y t 2 f 0; 1 g be the worst case regret for A, when the outcomes in an N -expert trial of length ` are restricted to be binary. <p> This is the minimax value for this more challenging game. The goal of this paper is to study V L (N; `) for general loss functions L, and to generalize the results for continuous-valued outcomes y t 2 <ref> [0; 1] </ref>. 6 Some general mathematical notation we will need is as follows. We use E [X] and Var [X] to denote the expected value and variance of a random variable X. <p> Our results include both upper and lower bounds for the minimax regret. In Section 4 we show how at least for the usual loss functions, the upper bounds can be generalized to allow for continuous-valued outcomes y t 2 <ref> [0; 1] </ref>. The main results are summarized in Subsection 3.1. Subsection 3.2 gives the the algorithm that obtains the upper bounds, and the proof that it does so. <p> To conclude this section, we clarify the intuitive meaning of the function S by connecting it to Bayes-optimal predictions in a simple probabilistic prediction game. Let Q be a probability measure on f 0; 1 g, with Pr y2Q [y = 1] = q. For a prediction z 2 <ref> [0; 1] </ref>, the expected loss for probability measure Q, or for bias q, is E y2Q [L (y; z)] = (1 q)L 0 (z) + qL 1 (z). Here we define 0 1 = 0. <p> To obtain explicit bounds for ^y t from these conditions, we need to have some notion of an inverse for L 0 and L 1 . Assume that L 0 is continuous and strictly increasing and L 1 is continuous and strictly decreasing in <ref> [0; 1] </ref>, which is implied by the assumptions of Theorem 3.1. <p> Thus, the equivalence between (3.12) and (3.15) will be maintained for all nonnegative 0 if the inverse L 1 0 is extended in such a way that the condition ^y t L 1 0 ( 0 ) holds for all ^y t 2 <ref> [0; 1] </ref> when 0 &gt; L 0 (1). Hence, we say that L 1 0 is a generalized inverse of L 0 if L 1 0 ( 0 ) 1 whenever 0 L 0 (1). <p> Similarly, L 1 1 is a generalized inverse of L 1 if L 1 1 (L 1 (^y)) = ^y for all ^y 2 <ref> [0; 1] </ref> and L 1 1 ( 1 ) 0 whenever 1 L 1 (0). <p> Lemma 3.9 Assume that L is a loss function such that L 0 (0) = L 1 (1) = 0, L 0 is continuous and strictly increasing in <ref> [0; 1] </ref>, and L 1 is continuous and strictly decreasing in [0; 1]. For any generalized inverses L 1 0 and L 1 1 , the condition (3.15) is equivalent to (3.12) for y 2 f 0; 1 g. <p> Lemma 3.9 Assume that L is a loss function such that L 0 (0) = L 1 (1) = 0, L 0 is continuous and strictly increasing in <ref> [0; 1] </ref>, and L 1 is continuous and strictly decreasing in [0; 1]. For any generalized inverses L 1 0 and L 1 1 , the condition (3.15) is equivalent to (3.12) for y 2 f 0; 1 g. <p> Proof If 0 62 [0; L 0 (1)], then both L 0 (^y t ) 0 and ^y t L 1 0 ( 0 ) hold for all ^y t 2 <ref> [0; 1] </ref>. If 1 62 [0; L 1 (0)], then both L 1 (^y t ) 1 and L 1 1 ( 1 ) ^y t hold for all ^y t 2 [0; 1]. <p> t ) 0 and ^y t L 1 0 ( 0 ) hold for all ^y t 2 <ref> [0; 1] </ref>. If 1 62 [0; L 1 (0)], then both L 1 (^y t ) 1 and L 1 1 ( 1 ) ^y t hold for all ^y t 2 [0; 1]. Hence, we may assume that 0 is in the range of L 0 and 1 is in the range of L 1 . <p> We therefore let &gt; 0 be arbitrary, and see for which values c the absolute loss is (c; )-realizable. By using the bound e x 1 (1 e )x that holds for all x 2 <ref> [0; 1] </ref>, we obtain L 1 1 ( 1 ) N X v t;i e x t;i 1 + c ln i=1 ! c ln i=1 N X v t;i (1 (1 e )(1 x t;i )) 1 where p t = P i v t;i x t;i . <p> are independent, identically distributed random variables with some distribution Q (over f 0; 1 g) and the experts' predictions x t;i , t = 1; : : : ; `, i = 1; : : : ; N , are independent, identically distributed random variables with some distribution P (over <ref> [0; 1] </ref>). We then derive for an arbitrary algorithm A a lower bound for the expected regret E S V L;A (S) when the trial sequence S is drawn from this distribution. <p> Define H (z) = (1 q)L 0 (z) + qL 1 (z) to denote the expected loss of a prediction z with this bias. Recall from Section 3.1 that any z 2 <ref> [0; 1] </ref> for which H (z) is minimized is called a Bayes-optimal prediction for the bias q. Assume that b is a Bayes-optimal prediction. <p> First we provide a bound that holds for arbitrary distributions P and Q. Theorem 3.15 Let P be a probability measure on <ref> [0; 1] </ref> and Q a probability measure on f 0; 1 g. Assume that for y = 0 and y = 1, the condition Pr x2P [L (y; x) &gt; K] = 0 holds for some constant K. Let b be a Bayes-optimal prediction for Q. <p> Then for all " &gt; 0 there is an ` " such that for all ` ` " we have V L (N; `) `E y2Q [L (y; b)] `t + (a N ") ` ln N ; (3.23) where lim N!1 a N = p Proof Given x 2 <ref> [0; 1] </ref> Nfi` and y 2 f 0; 1 g ` , we define an N -expert trial sequence of length ` by hx; yi = ((x 1 ; y 1 ); : : : ; (x ` ; y ` )). <p> For x 2 <ref> [0; 1] </ref> Nfi1 and y 2 f 0; 1 g , let T ij (x) = L (y j ; x j;i ) be the loss of expert i at trial j, if x is the sequence of experts' predictions and y the sequence of outcomes. <p> We consider T y a random variable on the domain <ref> [0; 1] </ref> Nfi1 . We now define for i = 1; : : : ; N and ` = 1; 2; : : : the random variable S i` in the domain [0; 1] Nfi1 fi f 0; 1 g 1 P ` to denote the loss of expert i in <p> We consider T y a random variable on the domain <ref> [0; 1] </ref> Nfi1 . We now define for i = 1; : : : ; N and ` = 1; 2; : : : the random variable S i` in the domain [0; 1] Nfi1 fi f 0; 1 g 1 P ` to denote the loss of expert i in the first ` trials. We also define for a given sequence y 2 f 0; 1 g the random variable S y y P ` y ij (x). <p> Proof Let Q be the probability measure on f 0; 1 g for which Pr y2Q [y = 1] = q. Let A be an arbitrary on-line prediction algorithm. For any probability measure P on <ref> [0; 1] </ref> and for any " &gt; 0, we have by Theorem 3.15 for sufficiently large ` the bound V L;A (N; `) `(E y2Q [L (y; b)] t ) + (a N ") ` ln N ; (3.27) where lim N!1 a N = p 2. <p> Lemma 3.18 If a prediction z 2 (0; 1) is not Bayes-optimal for any bias q 2 <ref> [0; 1] </ref>, then there are two predictions b 1 and b 2 with b 1 &lt; z &lt; b 2 such that for some bias q both b 1 and b 2 are Bayes-optimal. Proof Consider a prediction z 2 (0; 1) that is not Bayes-optimal for any bias. <p> If we can show R 1 " R 2 6= ;, we are done. Since z is never Bayes-optimal, we have R 1 <ref> [ R 2 = [0; 1] </ref>. Hence, if both R 1 and R 2 are closed, their intersection cannot be empty. Suppose that R 1 is not closed. <p> If we can show R 1 " R 2 6= ;, we are done. Since z is never Bayes-optimal, we have R 1 [ R 2 = <ref> [0; 1] </ref>. Hence, if both R 1 and R 2 are closed, their intersection cannot be empty. Suppose that R 1 is not closed. <p> show lim `!1 V L (2; `) c L ln 2, and our conjecture is that this is true for the square loss. 4 Continuous-valued outcomes 4.1 Applying the Generic Algorithm We now show that under certain assumptions, The Generic Algorithm 3.7 also works for continuous-valued outcomes y t 2 <ref> [0; 1] </ref>. These assumptions hold for the square and relative entropy loss, but not for the absolute loss, which will be considered in Subsection 4.2. We also consider the more general situation where the values x t;i and y t are not in the range [0; 1]. <p> continuous-valued outcomes y t 2 <ref> [0; 1] </ref>. These assumptions hold for the square and relative entropy loss, but not for the absolute loss, which will be considered in Subsection 4.2. We also consider the more general situation where the values x t;i and y t are not in the range [0; 1]. Lemma 4.1 Assume that for all y; a; b 2 [0; 1], the function g defined by g (y; a; b) = L (y; a)=c L (y; b) satisfies @ 2 g (y; a; b) @y 0 : (4.1) If (3.12) holds for binary values y 2 f 0; <p> We also consider the more general situation where the values x t;i and y t are not in the range <ref> [0; 1] </ref>. Lemma 4.1 Assume that for all y; a; b 2 [0; 1], the function g defined by g (y; a; b) = L (y; a)=c L (y; b) satisfies @ 2 g (y; a; b) @y 0 : (4.1) If (3.12) holds for binary values y 2 f 0; 1 g, then it holds for all values y 2 [0; 1]. <p> 2 <ref> [0; 1] </ref>, the function g defined by g (y; a; b) = L (y; a)=c L (y; b) satisfies @ 2 g (y; a; b) @y 0 : (4.1) If (3.12) holds for binary values y 2 f 0; 1 g, then it holds for all values y 2 [0; 1]. Proof We write (3.12) as (L (y; ^y t ) (y))=c 0. By exponentiating both sides and applying (3.11), we get e L (y;^y t )=c i=1 Let us denote the left-hand side of (4.2) by f (y). <p> get @ 2 f (y) N X v t;i @ @y 2 + @g (y; ^y t ; x t;i ) ! 2 A e g (y;^y t ;x t;i ) : As our assumption implies this to be nonnegative, the maximum value of f for y in the interval <ref> [0; 1] </ref> occurs for y = 0 or y = 1. <p> Let S = ((x 1 ; y 1 ); : : : ; (x ` ; y ` )) be a trial sequence for which x t 2 <ref> [0; 1] </ref> N and y t 2 [0; 1] hold for all t. Then the algorithm does not fail during the trial sequence, and its regret satisfies V L;A (S) c L ln N : Proof First note that by Lemma 3.10, the algorithm A does not fail. <p> Let S = ((x 1 ; y 1 ); : : : ; (x ` ; y ` )) be a trial sequence for which x t 2 <ref> [0; 1] </ref> N and y t 2 [0; 1] hold for all t. Then the algorithm does not fail during the trial sequence, and its regret satisfies V L;A (S) c L ln N : Proof First note that by Lemma 3.10, the algorithm A does not fail. <p> Recall that c L = 1 for the relative entropy loss. Hence, by Theorem 4.2, if A is the Generic Algorithm 3.7 with c = = 1, we have V L;A (S) ln N for any N -expert trial sequence S even if the outcomes y t 2 <ref> [0; 1] </ref> are continuous-valued. 2 Example 4.4 Let L be the square loss L sq . As the second derivative @ 2 L (y; z)=@y 2 is constant, the second derivative of the function g of Lemma 4.1 is 0 whenever c = 1=, and hence (4.1) trivially holds. <p> We see in Lemma 4.6 that there always is a prediction ^y t that satisfies (4.4) and that (4.4) implies jy ^y t j (y) for all y 2 <ref> [0; 1] </ref> and not merely for y 2 f 0; 1 g, which was the requirement in the Generic Algorithm. Hence, we now get for continuous-valued outcomes y t 2 [0; 1] the bound (3.20) that was previously obtained for binary outcomes y t 2 f 0; 1 g. <p> a prediction ^y t that satisfies (4.4) and that (4.4) implies jy ^y t j (y) for all y 2 <ref> [0; 1] </ref> and not merely for y 2 f 0; 1 g, which was the requirement in the Generic Algorithm. Hence, we now get for continuous-valued outcomes y t 2 [0; 1] the bound (3.20) that was previously obtained for binary outcomes y t 2 f 0; 1 g. Note that if (3.20) holds for y t 2 [0; 1], it actually holds for all y t , provided we still have x t;i 2 [0; 1]. <p> Hence, we now get for continuous-valued outcomes y t 2 <ref> [0; 1] </ref> the bound (3.20) that was previously obtained for binary outcomes y t 2 f 0; 1 g. Note that if (3.20) holds for y t 2 [0; 1], it actually holds for all y t , provided we still have x t;i 2 [0; 1]. <p> continuous-valued outcomes y t 2 <ref> [0; 1] </ref> the bound (3.20) that was previously obtained for binary outcomes y t 2 f 0; 1 g. Note that if (3.20) holds for y t 2 [0; 1], it actually holds for all y t , provided we still have x t;i 2 [0; 1]. <p> Again, the parameter can be tuned as mentioned in Example 3.14, and the scaling method of Example 4.4 can be used if the values x t;i are not in the range <ref> [0; 1] </ref>. 30 For the absolute loss, (3.12) has a simple geometric interpretation. Figure 1 gives an example of the graphs of the left-hand side jy ^yj and the right-hand side (y) as functions of y, fixing ^y = 0:58 and x = (0:33; 0:83; 0:97; 0:52). <p> The graph of has a non-differentiable tip at each value y = x i . The condition (3.12) states that the vee-curve must be below the graph of at y. For continuous-valued outcomes we wish (3.12) to hold for y 2 <ref> [0; 1] </ref> and hence the vee-curve to be below the graph of everywhere. If we were to move the tip of the vee to the left of 0:51, the right arm of the vee would intersect the -curve at the value y = 0:97. <p> We now show that a prediction that satisfies (4.4) always exists and satisfies the conditions of Theorem 3.8. Lemma 4.6 Let v t 2 <ref> [0; 1] </ref> N with P i v t;i = 1 and x t 2 [0; 1] N , and let &gt; 0. Then a prediction ^y t that satisfies (4.4) exists. Further, (4.4) implies jy ^y t j (y) for all y 2 [0; 1]. <p> We now show that a prediction that satisfies (4.4) always exists and satisfies the conditions of Theorem 3.8. Lemma 4.6 Let v t 2 <ref> [0; 1] </ref> N with P i v t;i = 1 and x t 2 [0; 1] N , and let &gt; 0. Then a prediction ^y t that satisfies (4.4) exists. Further, (4.4) implies jy ^y t j (y) for all y 2 [0; 1]. <p> Lemma 4.6 Let v t 2 <ref> [0; 1] </ref> N with P i v t;i = 1 and x t 2 [0; 1] N , and let &gt; 0. Then a prediction ^y t that satisfies (4.4) exists. Further, (4.4) implies jy ^y t j (y) for all y 2 [0; 1]. Proof We prove the existence of ^y t by showing that y (y) z + (z) (4.5) holds for all y, z, v t , and x t . <p> This holds for all 0 p 1 because the function ln is concave. A similar argument based on second derivatives shows that for y 2 <ref> [0; 1] </ref>, the value y (y) obtains its maximum and the value y+(y) its minimum when y 2 f 0; 1; x t;1 ; : : : ; x t;N g. 2 Lemma 4.6 immediately implies the following result. <p> Theorem 4.7 Let S = ((x 1 ; y 1 ); : : : ; (x ` ; y ` )) be a trial sequence with x t 2 <ref> [0; 1] </ref> N and y t 2 [0; 1] for all t. Let L be the absolute loss and A be the Vee Algorithm 4.5. <p> Theorem 4.7 Let S = ((x 1 ; y 1 ); : : : ; (x ` ; y ` )) be a trial sequence with x t 2 <ref> [0; 1] </ref> N and y t 2 [0; 1] for all t. Let L be the absolute loss and A be the Vee Algorithm 4.5.
Reference: [2] <author> A. R. Barron and Q. Xie, </author> <title> "Asymptotic minimax loss for data compression, gambling, and prediction," </title> <booktitle> in Proc. 9th Annual Conference on Computational Learning Theory, </booktitle> <address> New York, NY: </address> <publisher> ACM Press, </publisher> <year> 1996. </year>
Reference: [3] <author> Patrick Billingsley, </author> <title> Probability and Measure (Second Edition). </title> <address> New York: </address> <publisher> Wiley, </publisher> <year> 1986. </year>
Reference-contexts: x2P jU fl` (x)j i h y 1+p = N jU 1` j dP Z 1` j 1 y 1+p ! h y i As the sequence U y y fl2 ; : : : converges in distribution to F fl , the bound (A.1) with p = 1 guarantees <ref> [3, Corollary, p. 292] </ref> lim `!1 E x2P [U y fl` (x)] = E [F fl ] for all y and, therefore, lim `!1 r ` (y)E x2P [U fl` (x)] = E [F fl ] with probability 1 for y drawn from Q. <p> The bound (A.1) with p = 0 implies jr ` (y)E x2P [U y fl` (x)]j 2BN , and the bounded convergence theorem <ref> [3, Thm. 16.5, p. 180] </ref> `!1 y as claimed. 2
Reference: [4] <author> D. Blackwell, </author> <title> "Controlled random walks," </title> <booktitle> in Proc. Internat. Congress Mathematicians, vol. III, </booktitle> <address> Amsterdam, </address> <year> 1954, </year> <pages> pp. 336-338. </pages>
Reference: [5] <author> D. Blackwell, </author> <title> "An analog of the minimax theorem for vector payoffs," </title> <journal> Pacific Journal of Mathematics, </journal> <volume> vol. 6, </volume> <pages> pp. 1-8, </pages> <year> 1956. </year>
Reference: [6] <author> N. Cesa-Bianchi, Y. Freund, D. Haussler, D. P. Helmbold, R. E. Schapire, and M. K. Warmuth, </author> <title> "How to use expert advice," </title> <journal> Journal of the ACM, </journal> <volume> vol. 44, </volume> <pages> pp. 427-485, </pages> <year> 1997. </year>
Reference-contexts: ^y t ) = (y t ^y t ) 2 , often used in the literature on sequential forecasting [16, 13, 36, 9], and the absolute loss L abs (y t ; ^y t ) = jy t ^y t j, often used in pattern recognition and computational learning theory <ref> [25, 6, 7, 8] </ref>, where the term on-line is used to describe a sequential procedure of this type. <p> We call this the minimax regret. The exact minimax regret depends strongly on the comparison class, and even for simple comparison classes it does not usually have a nice closed form formula for each loss function L and length of play ` <ref> [6, 10] </ref>. So here we focus instead on obtaining good upper and lower bounds for the minimax regret. Our upper bounds on the minimax regret are quite general, in that they depend only on the number N of experts in the comparison class. <p> So they in fact hold for a more challenging game in which Nature chooses the (predictions of the) experts in the comparison class in an 3 adversarial manner, in addition to choosing the outcomes. A game of this type was defined and analyzed by Cesa-Bianchi et al. <ref> [6] </ref> for the absolute loss; here we extend this analysis to more general loss functions. The upper bounds we obtain on the minimax regret of this more challenging game provide upper bounds on the minimax regret of any standard universal prediction game. <p> On the other hand, for the absolute loss L abs , Cesa-Bianchi et al. <ref> [6] </ref> have shown that the best general bounds on the minimax regret that can be obtained are fi p , and that the best possible constant in this bound approaches 1= p 2 for a large number N of experts, when the natural logarithm is used. <p> Thus, in a sense we see that in our particular setting, the average case is almost as difficult as the worst case. The proof technique with a randomized adversary was used previously by Cesa-Bianchi et al. <ref> [6] </ref> in the special case of the absolute loss. Finally, in Subsection 4.1 we show that for certain loss functions, such as the square and logarithmic loss, Vovk's algorithm achieves the same worst-case regret even if the outcomes are allowed to be arbitrary real numbers in the interval [0; 1]. <p> For the absolute loss, the worst-case regret bounds proven for binary outcomes <ref> [35, 6] </ref> can be achieved with continuous-valued outcomes by using a slightly more complicated algorithm, as we show in Subsection 4.2. 2 On-line prediction and loss bounds We consider the predictive performance of an on-line learning algorithm A on a sequence of outcomes y 1 ; : : : ; y <p> This is always true if the algorithm has access to the previous outcomes and can simulate the predictive mechanisms of the experts. However, our upper bound results also hold in more general cases in which the algorithm cannot simulate the experts; see <ref> [6] </ref> for further discussion. Also, most algorithms considered in this paper make their predictions ^y t independently of the length ` of the whole trial sequence, but in some situations we also consider how the algorithms can be fine-tuned if ` is known in advance. <p> 0 for some 0 &lt; z &lt; 1, or there are values a &lt; b such that S (z) = 0 for all a z b, we have V L (N; `) = ` log N : (3.7) The special case of absolute loss was considered by Cesa-Bianchi et al. <ref> [6] </ref>. They show that for the optimal algorithm A we have V L;A (N; `) = fi p . For the absolute loss, the denominator S (z) is 0 for all z. Thus, our lower bound (3.7) generalizes their lower bound for more general loss functions. <p> This latter quantity can be interpreted as the likelihood of y t under a probability model used by the ith expert. Hence the update (3.13) can be interpreted as a Bayesian update of posterior probabilities v t;i over the set of experts <ref> [6] </ref>. The additivity of the logarithmic loss, and its associated statistical interpretation and chain rule, makes the analysis of this special loss more convenient, as pointed out in, e.g., [21]. <p> By Jensen's inequality, this is positive for c (2 ln 2 1+e ) 1 , and the prediction condition (3.12) for y 2 f 0; 1 g becomes 1 + P N 2 ln 2 ^y t P N 2 ln 2 : (3.19) Cesa-Bianchi et al. <ref> [6] </ref> have noted that (3.19) always holds if we choose ^y t = ln (1 p t + p t e ) + ln ((1 p t )e + p t ) but does not in general hold for ^y t = p t . <p> We would like to choose the learning rate in such a way that the loss bound on the right-hand side of (3.20) is minimized. This tuning of the learning rate is discussed in detail by Cesa-Bianchi et al. <ref> [6, 7] </ref>. Here we just cite some of the basic results. <p> Similar results can be obtained by basing the choice of on an upper bound for the loss min i Loss L (E i ; S) of the best expert instead of on `. Finally, we consider the variations of the Generic Algorithm given by Cesa-Bianchi et al. <ref> [6] </ref> for the special case of the absolute loss. <p> and gives the same worst-case loss bound for any choice e jy t x t;i j ff t;i 1 (1 e ) jy t x t;i j : (3.21) Interestingly enough, the weights obtained using ff t;i = 1 (1e ) jy t x t;i j have a Bayesian interpretation <ref> [6] </ref>. 2 16 3.3 Lower bounds This subsection contains proofs of the lower bounds for V L (N; `) stated in Theorems 3.1 and 3.3 in Subsection 3.1. The lower bounds hold even for algorithms that receive ` as input before the first trial. <p> For the absolute loss, we can apply Lemma 3.17 with q = 1=2, b 1 = 0, and b 1 = 1. This gives = 1=2, and hence V L (N; `) (1 o (1)) q (` ln N )=2, which is the result obtained by Cesa-Bianchi et al. <ref> [6] </ref>. Recall that in Lemma 3.16 we had a lower bound in terms of R (b) assuming that b is the unique Bayes-optimal prediction for some bias. <p> For binary outcomes, the loss bound (3.20) was previously shown for a whole family of algorithms defined by a number of different prediction and update factors ff t;i <ref> [6] </ref>, as was briefly explained in Example 3.14. In the continuous case we have less freedom. Suppose we were to use ff t;i = 1 (1 e ) jy t x t;i j, and let N = 1, x = (0), and = 1.
Reference: [7] <author> N. Cesa-Bianchi, Y. Freund, D. P. Helmbold, and M. K. Warmuth, </author> <title> "On-line prediction and conversion strategies," </title> <journal> Machine Learning, </journal> <volume> vol. 25, </volume> <pages> pp. 71-110, </pages> <year> 1996. </year>
Reference-contexts: ^y t ) = (y t ^y t ) 2 , often used in the literature on sequential forecasting [16, 13, 36, 9], and the absolute loss L abs (y t ; ^y t ) = jy t ^y t j, often used in pattern recognition and computational learning theory <ref> [25, 6, 7, 8] </ref>, where the term on-line is used to describe a sequential procedure of this type. <p> We would like to choose the learning rate in such a way that the loss bound on the right-hand side of (3.20) is minimized. This tuning of the learning rate is discussed in detail by Cesa-Bianchi et al. <ref> [6, 7] </ref>. Here we just cite some of the basic results.
Reference: [8] <author> N. Cesa-Bianchi, D. P. Helmbold, and S. Panizza, </author> <title> "On Bayes methods for on-line boolean prediction," </title> <booktitle> in Proc. 9th Annual Conference on Computational Learning Theory, </booktitle> <address> New York: </address> <publisher> ACM Press, </publisher> <year> 1996, </year> <pages> pp. 314-324. 34 </pages>
Reference-contexts: ^y t ) = (y t ^y t ) 2 , often used in the literature on sequential forecasting [16, 13, 36, 9], and the absolute loss L abs (y t ; ^y t ) = jy t ^y t j, often used in pattern recognition and computational learning theory <ref> [25, 6, 7, 8] </ref>, where the term on-line is used to describe a sequential procedure of this type.
Reference: [9] <author> N. Cesa-Bianchi, P. Long, and M.K. Warmuth, </author> <title> "Worst-case quadratic loss bounds for on-line prediction of linear functions by gradient descent," </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 7, </volume> <pages> pp. 604-619, </pages> <year> 1996. </year>
Reference-contexts: Other possible loss functions are the square loss L sq (y t ; ^y t ) = (y t ^y t ) 2 , often used in the literature on sequential forecasting <ref> [16, 13, 36, 9] </ref>, and the absolute loss L abs (y t ; ^y t ) = jy t ^y t j, often used in pattern recognition and computational learning theory [25, 6, 7, 8], where the term on-line is used to describe a sequential procedure of this type. <p> A more challenging problem is to bound the regret of the algorithms over the best linear combination of experts <ref> [24, 9, 23] </ref>. The only worst-case loss bounds for the latter case that have been obtained were for the square loss function. Hopefully, some of the results of the present paper can be generalized to the linear combination case.
Reference: [10] <author> T. H. Chung, </author> <title> "Approximate methods for sequential decision making using expert advice," </title> <booktitle> in Proc. 7th Annual ACM Workshop on Computational Learning Theory, </booktitle> <address> New York: </address> <publisher> ACM Press, </publisher> <year> 1994, </year> <pages> pp. 183-189. </pages>
Reference-contexts: Closely related work has also been done in the area of mathematical finance, where one seeks a stock portfolio re-balancing strategy that performs almost as well as the best strategy in a given comparison class on any market [12, 22]. More general decision-theoretic scenarios are considered in <ref> [10, 37, 1] </ref>. Also related is the work on on-line competitive algorithms in computer science (see e.g. [34]). In this paper we focus on the case in which the comparison class E is finite. Our goal is to develop the most general results possible for this finite case. <p> In this paper we focus on the case in which the comparison class E is finite. Our goal is to develop the most general results possible for this finite case. Whereas most previous papers (with the exception of <ref> [35, 37, 10] </ref>) have each focused on a single loss function, which has often been different in different disciplines, we give a unified treatment of a very general class of loss functions, including the usual ones. <p> We call this the minimax regret. The exact minimax regret depends strongly on the comparison class, and even for simple comparison classes it does not usually have a nice closed form formula for each loss function L and length of play ` <ref> [6, 10] </ref>. So here we focus instead on obtaining good upper and lower bounds for the minimax regret. Our upper bounds on the minimax regret are quite general, in that they depend only on the number N of experts in the comparison class. <p> The next challenge is to extend the results for continuous-valued outcomes to more general loss functions. Another direction worth exploring is to let outcomes be discrete valued with more than two choices. The recent results of Chung <ref> [10] </ref> address some of these problems. In this paper we restricted the predictions of the experts to lie between zero and one, except in specific examples where we have indicated how scaling tricks can be used.
Reference: [11] <author> T. </author> <title> Cover, "Behavior of sequential predictors of binary sequences," </title> <booktitle> in Proc. 4th Prague Conference on Information Theory, Statistical Decision Functions and Random Processes, </booktitle> <publisher> Publishing House of the Czechoslovak Academy of Sciences, </publisher> <year> 1965, </year> <pages> pp. 263-272. </pages>
Reference: [12] <author> T. M. Cover and E. Ordentlich, </author> <title> "Universal portfolios with side information," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> vol. 42, </volume> <pages> pp. 348-363, </pages> <year> 1996. </year>
Reference-contexts: Closely related work has also been done in the area of mathematical finance, where one seeks a stock portfolio re-balancing strategy that performs almost as well as the best strategy in a given comparison class on any market <ref> [12, 22] </ref>. More general decision-theoretic scenarios are considered in [10, 37, 1]. Also related is the work on on-line competitive algorithms in computer science (see e.g. [34]). In this paper we focus on the case in which the comparison class E is finite.
Reference: [13] <author> A. P. Dawid, </author> <title> "Prequential analysis, stochastic complexity and Bayesian inference," in Bayesian Statistics 4, </title> <publisher> Oxford: Clarendon Press, </publisher> <year> 1992 </year>
Reference-contexts: Other possible loss functions are the square loss L sq (y t ; ^y t ) = (y t ^y t ) 2 , often used in the literature on sequential forecasting <ref> [16, 13, 36, 9] </ref>, and the absolute loss L abs (y t ; ^y t ) = jy t ^y t j, often used in pattern recognition and computational learning theory [25, 6, 7, 8], where the term on-line is used to describe a sequential procedure of this type.
Reference: [14] <author> A. DeSantis, G. Markowsky, and M. N. Wegman, </author> <title> "Learning probabilistic prediction functions," </title> <booktitle> in Proc. 29th Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <address> Los Alamitos, CA: </address> <publisher> IEEE Computer Society Press, </publisher> <year> 1988, </year> <pages> pp. 110-119. </pages>
Reference-contexts: For instance, for the square loss Vovk's algorithm achieves this bound with c L = 1=2 [35], and for logarithmic loss with c L = 1, when the natural logarithm is used to define the loss function L <ref> [14, 35] </ref>. <p> The loss bound we obtain was previously shown by De Santis et al. <ref> [14] </ref> and Vovk [35]. 2 Example 3.13 Let L be the square loss. Vovk [35] has shown that the square loss is (1=2; 2)- realizable. Here the result follows from Lemma 3.10 and Example 3.2.
Reference: [15] <author> M. Feder, N. Merhav, and M. Gutman, </author> <title> "Universal prediction of individual sequences," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> vol. 38, </volume> <pages> pp. 1258-1270, </pages> <year> 1992. </year>
Reference-contexts: Extensions of this include the case where E consists of all Markov predictors of a given order, or all finite state predictors of a given number of states <ref> [38, 26, 15] </ref>. Of course, since the learning algorithm must make its predictions on-line, it cannot know ahead of time which expert in E will perform best on the sequence of outcomes produced by Nature. <p> In particular, it has been shown that the Lempel-Ziv algorithm universally achieves quite a small regret when compared to the best finite state predictor <ref> [15] </ref>. A more general analysis of universal prediction is given in [31] for a comparison class that is a smooth parametric family, and in more general cases in [28].
Reference: [16] <author> D. P. Foster, </author> <title> "Prediction in the worst case," </title> <journal> Annals of Statistics, </journal> <volume> vol. 19, </volume> <pages> pp. 1084-1090, </pages> <year> 1991. </year>
Reference-contexts: Other possible loss functions are the square loss L sq (y t ; ^y t ) = (y t ^y t ) 2 , often used in the literature on sequential forecasting <ref> [16, 13, 36, 9] </ref>, and the absolute loss L abs (y t ; ^y t ) = jy t ^y t j, often used in pattern recognition and computational learning theory [25, 6, 7, 8], where the term on-line is used to describe a sequential procedure of this type.
Reference: [17] <author> Y. Freund, </author> <title> "Predicting bits almost as well as the optimal biased coin," </title> <booktitle> in Proc. 9th Annual Conference on Computational Learning Theory, </booktitle> <address> New York: </address> <publisher> ACM Press, </publisher> <year> 1996, </year> <pages> pp. 89-98. </pages>
Reference: [18] <author> J. Galambos, </author> <title> The Asymptotic Theory of Extreme Order Statistics (Second Edition). </title> <note> Malabar, </note> <author> FL: R. E. Krieger, </author> <year> 1987. </year>
Reference-contexts: Let F 1 ; F 2 ; : : : ; F N be N independent standard normal random variables. It is well known <ref> [18] </ref> that E min F i = a N ln N ; where lim N!1 a N = p 2.
Reference: [19] <author> J. Hannan, </author> <title> "The dynamic statistical decision problem when the component problem involves a finite number, m, of distributions," </title> <journal> Annals of Mathematical Statistics, </journal> <volume> vol. 27, </volume> <editor> p. </editor> <volume> 212, </volume> <year> 1956. </year>
Reference: [20] <author> J. Hannan, </author> <title> "Approximations to Bayes risk in repeated play," </title> <journal> Ann. Math. Stud., </journal> <volume> vol. 39, </volume> <pages> pp. 97-139, </pages> <address> 1957. </address> <publisher> Princeton University Press. </publisher>
Reference: [21] <author> D. Haussler and A. Barron, </author> <title> "How well do Bayes methods work for on-line prediction of f+1; 1g values?," </title> <booktitle> in Proc. 3rd NEC Symposium on Computation and Cognition, </booktitle> <publisher> SIAM, </publisher> <year> 1992, </year> <pages> pp. 74-100. </pages>
Reference-contexts: The additivity of the logarithmic loss, and its associated statistical interpretation and chain rule, makes the analysis of this special loss more convenient, as pointed out in, e.g., <ref> [21] </ref>. In that paper, bounds for the logarithm loss are obtained first, and then these are used, along with certain inequalities, to derive bounds for other losses.
Reference: [22] <author> D. Helmbold, R. E. Schapire, Y. Singer, and M. K. Warmuth, </author> <title> "On-line portfolio selection using multiplicative updates," </title> <booktitle> in Proc. 13th International Conference on Machine Learning, </booktitle> <address> San Francisco: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1996, </year> <pages> pp. 243-251. 35 </pages>
Reference-contexts: Closely related work has also been done in the area of mathematical finance, where one seeks a stock portfolio re-balancing strategy that performs almost as well as the best strategy in a given comparison class on any market <ref> [12, 22] </ref>. More general decision-theoretic scenarios are considered in [10, 37, 1]. Also related is the work on on-line competitive algorithms in computer science (see e.g. [34]). In this paper we focus on the case in which the comparison class E is finite.
Reference: [23] <author> J. Kivinen and M. K. Warmuth, </author> <title> "Additive versus exponentiated gradient updates for linear prediction," </title> <journal> Information and Computation, </journal> <volume> vol. 132, </volume> <pages> pp. 1-64, </pages> <year> 1997. </year>
Reference-contexts: A more challenging problem is to bound the regret of the algorithms over the best linear combination of experts <ref> [24, 9, 23] </ref>. The only worst-case loss bounds for the latter case that have been obtained were for the square loss function. Hopefully, some of the results of the present paper can be generalized to the linear combination case.
Reference: [24] <author> N. Littlestone, P. M. Long, and M. K. Warmuth, </author> <title> "On-line learning of linear functions," </title> <journal> Journal of Computational Complexity, </journal> <volume> vol. 5, </volume> <pages> pp. 1-23, </pages> <year> 1995. </year>
Reference-contexts: A more challenging problem is to bound the regret of the algorithms over the best linear combination of experts <ref> [24, 9, 23] </ref>. The only worst-case loss bounds for the latter case that have been obtained were for the square loss function. Hopefully, some of the results of the present paper can be generalized to the linear combination case.
Reference: [25] <author> N. Littlestone and M. K. Warmuth, </author> <title> "The weighted majority algorithm," </title> <journal> Information and Computation, </journal> <volume> vol. 108, </volume> <pages> pp. 212-261, </pages> <year> 1994. </year>
Reference-contexts: ^y t ) = (y t ^y t ) 2 , often used in the literature on sequential forecasting [16, 13, 36, 9], and the absolute loss L abs (y t ; ^y t ) = jy t ^y t j, often used in pattern recognition and computational learning theory <ref> [25, 6, 7, 8] </ref>, where the term on-line is used to describe a sequential procedure of this type. <p> Here there is a strong dependence on the number ` of trials. Slightly weaker results for the absolute loss were obtained earlier by Littlestone and Warmuth <ref> [25] </ref>. It is instructive to compare these general results obtained for a finite comparison class of size N and logarithmic loss to the universal prediction results of Rissanen and others for smooth parametric families of models, which form infinite comparison classes. <p> Then (0) = 0, so to satisfy jy ^yj (y) for y = 0 we must choose ^y = 0. However, as (0:2) 0:178, we cannot then have jy ^yj (y) for y = 0:2. The Algorithm WMC <ref> [25] </ref> does work for the continuous case, and is allowed to use any update that satisfies (3.21). However, its worst case bound has 1 e in the denominator instead of 2 ln (2=(1 + e )), and hence it is slightly worse than the bounds given here.
Reference: [26] <author> N. Merhav and M. Feder, </author> <title> "Universal sequential learning and decisions from individual data sequences," </title> <booktitle> in Proc. 5th Annual Workshop on Computational Learning Theory, </booktitle> <address> New York: </address> <publisher> ACM Press, </publisher> <year> 1992, </year> <pages> pp. 413-427. </pages>
Reference-contexts: Extensions of this include the case where E consists of all Markov predictors of a given order, or all finite state predictors of a given number of states <ref> [38, 26, 15] </ref>. Of course, since the learning algorithm must make its predictions on-line, it cannot know ahead of time which expert in E will perform best on the sequence of outcomes produced by Nature.
Reference: [27] <author> J. Mycielski, </author> <title> "A learning algorithm for linear operators," </title> <journal> Proceedings of the American Mathematical Society, </journal> <volume> vol. 103, </volume> <pages> pp. 547-550, </pages> <year> 1988. </year>
Reference: [28] <author> M. Opper and D. Haussler, </author> <title> "Worst case prediction over sequences under log loss" in The Mathematics of Information Coding, Extraction and Distribution, </title> <publisher> Springer Verlag, </publisher> <year> 1997. </year>
Reference-contexts: A more general analysis of universal prediction is given in [31] for a comparison class that is a smooth parametric family, and in more general cases in <ref> [28] </ref>. Closely related work has also been done in the area of mathematical finance, where one seeks a stock portfolio re-balancing strategy that performs almost as well as the best strategy in a given comparison class on any market [12, 22].
Reference: [29] <author> J. Rissanen, </author> <title> "Universal coding, information, prediction and estimation," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> vol. 30, </volume> <pages> pp. 629-636, </pages> <year> 1984. </year>
Reference-contexts: When the actual next outcome y t is revealed, you then suffer a loss L (y t ; ^y t ), where L is a fixed loss function. One example of this scenario is in producing sequential probability assignments for individual binary sequences <ref> [29, 32, 33, 39] </ref>. Here ^y t is an estimate of the probability that y t = 1, given the previous outcomes y 1 ; : : : ; y t1 .
Reference: [30] <author> J. Rissanen, </author> <title> Stochastic Complexity in Statistical Inquiry. </title> <booktitle> Volume 15 of Series in Computer Science, World Scientific, </booktitle> <year> 1989. </year>
Reference: [31] <author> J. Rissanen, </author> <title> "Fisher Information and Stochastic Complexity," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> vol. 42, </volume> <pages> pp. 40-47, </pages> <year> 1996. </year>
Reference-contexts: In particular, it has been shown that the Lempel-Ziv algorithm universally achieves quite a small regret when compared to the best finite state predictor [15]. A more general analysis of universal prediction is given in <ref> [31] </ref> for a comparison class that is a smooth parametric family, and in more general cases in [28]. <p> under logarithmic loss, essentially without loss of performance, under suitable smoothness conditions one can replace a continuous k-dimensional comparison class of models with a finite approximation to this class in which the parameters are given to precision roughly 1= p `, where ` is the number of data points (trials) <ref> [31] </ref> (see Equation (27) and the preceding equation). This gives a finite comparison class of size N = O (` k=2 ).
Reference: [32] <author> J. Shtarkov, </author> <title> "Coding of discrete sources with unknown statistics," </title> <booktitle> in Topics in Information Theory, </booktitle> <address> Amsterdam: </address> <publisher> North Holland, </publisher> <year> 1975, </year> <pages> pp. 559-574. </pages>
Reference-contexts: When the actual next outcome y t is revealed, you then suffer a loss L (y t ; ^y t ), where L is a fixed loss function. One example of this scenario is in producing sequential probability assignments for individual binary sequences <ref> [29, 32, 33, 39] </ref>. Here ^y t is an estimate of the probability that y t = 1, given the previous outcomes y 1 ; : : : ; y t1 .
Reference: [33] <author> Y. M. Shtarkov, </author> <title> "Universal sequential coding of single messages," </title> <journal> Prob. Pered. Inf., </journal> <volume> vol. 23, </volume> <pages> pp. 175-186, </pages> <year> 1987. </year>
Reference-contexts: When the actual next outcome y t is revealed, you then suffer a loss L (y t ; ^y t ), where L is a fixed loss function. One example of this scenario is in producing sequential probability assignments for individual binary sequences <ref> [29, 32, 33, 39] </ref>. Here ^y t is an estimate of the probability that y t = 1, given the previous outcomes y 1 ; : : : ; y t1 .
Reference: [34] <author> J. S. Vitter and P. Krishnan, </author> <title> "Optimal prefetching via data compression," </title> <booktitle> in Proc. 32nd IEEE Symposium on Foundations of Computer Science, </booktitle> <address> Los Alamitos, CA: </address> <publisher> IEEE Computer Society Press, </publisher> <year> 1991, </year> <pages> pp. 121-130. </pages>
Reference-contexts: More general decision-theoretic scenarios are considered in [10, 37, 1]. Also related is the work on on-line competitive algorithms in computer science (see e.g. <ref> [34] </ref>). In this paper we focus on the case in which the comparison class E is finite. Our goal is to develop the most general results possible for this finite case.
Reference: [35] <author> V. Vovk, </author> <title> "Aggregating strategies," </title> <booktitle> in Proc. 3rd Annual Workshop on Computational Learning Theory, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1990, </year> <pages> pp. 371-383. </pages>
Reference-contexts: In this paper we focus on the case in which the comparison class E is finite. Our goal is to develop the most general results possible for this finite case. Whereas most previous papers (with the exception of <ref> [35, 37, 10] </ref>) have each focused on a single loss function, which has often been different in different disciplines, we give a unified treatment of a very general class of loss functions, including the usual ones. <p> Vovk <ref> [35] </ref> introduced an on-line prediction algorithm that is applicable to all loss functions when the outcomes are binary. This algorithm can be used to obtain good general upper bounds on the minimax regret. <p> For instance, for the square loss Vovk's algorithm achieves this bound with c L = 1=2 <ref> [35] </ref>, and for logarithmic loss with c L = 1, when the natural logarithm is used to define the loss function L [14, 35]. <p> For instance, for the square loss Vovk's algorithm achieves this bound with c L = 1=2 [35], and for logarithmic loss with c L = 1, when the natural logarithm is used to define the loss function L <ref> [14, 35] </ref>. <p> For the absolute loss, the worst-case regret bounds proven for binary outcomes <ref> [35, 6] </ref> can be achieved with continuous-valued outcomes by using a slightly more complicated algorithm, as we show in Subsection 4.2. 2 On-line prediction and loss bounds We consider the predictive performance of an on-line learning algorithm A on a sequence of outcomes y 1 ; : : : ; y <p> The main results are summarized in Subsection 3.1. Subsection 3.2 gives the the algorithm that obtains the upper bounds, and the proof that it does so. Both the algorithm and analysis are originally by Vovk <ref> [35] </ref>; here we are able to simplify them by considering only continuous loss functions. Subsection 3.3 contains the main lower bound proofs. <p> The algorithm A that obtains the bound (3.4), as well as the proof of the bound, are already given by Vovk <ref> [35] </ref>. The algorithm makes its predictions independently of the length ` of the trial sequence. We give the algorithm and a simplified proof in Subsection 3.2. Note that the length ` of the sequence does not appear on the right-hand side of (3.4). <p> However, for the bias q = 1=2 any prediction is Bayes-optimal. 2 3.2 The algorithm and the upper bound We consider an algorithm first introduced by Vovk <ref> [35] </ref>. We give the general algorithm and its analysis, applied to our situation in which the loss function is continuous. We then work out as examples the details for several interesting loss functions. The algorithm has two positive real valued parameters c and . <p> The following upper bound was already given by Vovk <ref> [35] </ref>. Theorem 3.8 Let L be any loss function. Let S = ((x 1 ; y 1 ); : : : ; (x ` ; y ` )) be an N -expert trial sequence in which the outcomes y t 2 f 0; 1 g are binary. <p> The result then follows from Theorem 3.8 by setting w 1;i = 1 for all i. The rest of this subsection gives our formulation of Vovk's <ref> [35] </ref> proof for these results. We first develop an equivalent version of condition (3.12). <p> The loss bound we obtain was previously shown by De Santis et al. [14] and Vovk <ref> [35] </ref>. 2 Example 3.13 Let L be the square loss. Vovk [35] has shown that the square loss is (1=2; 2)- realizable. Here the result follows from Lemma 3.10 and Example 3.2. <p> The loss bound we obtain was previously shown by De Santis et al. [14] and Vovk <ref> [35] </ref>. 2 Example 3.13 Let L be the square loss. Vovk [35] has shown that the square loss is (1=2; 2)- realizable. Here the result follows from Lemma 3.10 and Example 3.2. The note after the proof of Lemma 3.10 further implies that the square loss is not (c; 1=c)-realizable for any c &lt; 1=2. <p> The bound obtained by applying Theorem 3.8 for the absolute loss with the choice c = (2 ln (2=(1 + e ))) 1 , namely Loss L (A; S) ln W 1 + Loss L (E i ; S) 2 ln 2 ; (3.20) was first proven by Vovk <ref> [35] </ref>. We would like to choose the learning rate in such a way that the loss bound on the right-hand side of (3.20) is minimized. This tuning of the learning rate is discussed in detail by Cesa-Bianchi et al. [6, 7]. Here we just cite some of the basic results.
Reference: [36] <author> V. Vovk, </author> <title> "Universal forecasting algorithms," </title> <journal> Information and Computation, </journal> <volume> vol. 96, </volume> <pages> pp. 245-277, </pages> <year> 1992. </year>
Reference-contexts: Other possible loss functions are the square loss L sq (y t ; ^y t ) = (y t ^y t ) 2 , often used in the literature on sequential forecasting <ref> [16, 13, 36, 9] </ref>, and the absolute loss L abs (y t ; ^y t ) = jy t ^y t j, often used in pattern recognition and computational learning theory [25, 6, 7, 8], where the term on-line is used to describe a sequential procedure of this type.
Reference: [37] <author> V. G. Vovk, </author> <title> "A game of prediction with expert advice," </title> <booktitle> in Proc. 8th Annual Conference on Computational Learning Theory, </booktitle> <address> New York: </address> <publisher> ACM Press, </publisher> <year> 1995, </year> <pages> pp. 51-60. 36 </pages>
Reference-contexts: Closely related work has also been done in the area of mathematical finance, where one seeks a stock portfolio re-balancing strategy that performs almost as well as the best strategy in a given comparison class on any market [12, 22]. More general decision-theoretic scenarios are considered in <ref> [10, 37, 1] </ref>. Also related is the work on on-line competitive algorithms in computer science (see e.g. [34]). In this paper we focus on the case in which the comparison class E is finite. Our goal is to develop the most general results possible for this finite case. <p> In this paper we focus on the case in which the comparison class E is finite. Our goal is to develop the most general results possible for this finite case. Whereas most previous papers (with the exception of <ref> [35, 37, 10] </ref>) have each focused on a single loss function, which has often been different in different disciplines, we give a unified treatment of a very general class of loss functions, including the usual ones.
Reference: [38] <author> M. J. Weinberger, A. Lempel, and J. Ziv, </author> <title> "A sequential algorithm for the universal coding of finite memory sources," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> vol. 38, </volume> <pages> pp. 1002-1014, </pages> <year> 1992. </year>
Reference-contexts: Extensions of this include the case where E consists of all Markov predictors of a given order, or all finite state predictors of a given number of states <ref> [38, 26, 15] </ref>. Of course, since the learning algorithm must make its predictions on-line, it cannot know ahead of time which expert in E will perform best on the sequence of outcomes produced by Nature.
Reference: [39] <author> M. J. Weinberger, N. Merhav, and M. Feder, </author> <title> "Optimal sequential probability assignment for individual sequences," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> vol. 40, </volume> <pages> pp. 384-396, </pages> <year> 1994. </year>
Reference-contexts: When the actual next outcome y t is revealed, you then suffer a loss L (y t ; ^y t ), where L is a fixed loss function. One example of this scenario is in producing sequential probability assignments for individual binary sequences <ref> [29, 32, 33, 39] </ref>. Here ^y t is an estimate of the probability that y t = 1, given the previous outcomes y 1 ; : : : ; y t1 .
References-found: 39

