URL: http://www.cs.utah.edu/~retrac/papers/asplos98.ps.Z
Refering-URL: http://www.cs.utah.edu/~retrac/papers.html
Root-URL: 
Title: Impulse An Adaptable Memory System  
Author: John Carter, Wilson Hsieh, Leigh Stoller, Mark Swanson, Lixin Zhang, Erik Brunvand, Al Davis, Chen-Chi Kuo, Ravindra Kuramkote, Michael Parker, Lambert Schaelicke, and Terry Tateyama 
Abstract: This paper presents the Impulse adaptable memory system, which allows applications to make efficient use of cache space and bus bandwidth. Impulse has a configurable memory controller that allows applications to remap data in the memory system. As a result, applications can control how their data is accessed, organized, and cached. We describe the current design of the Impulse architecture, describe the software transformations that will be necessary to make full use of Impulse, and examine in detail the optimizations that Impulse enables: scatter/gather access to sparse data, no-copy page coloring, and no-copy tile remapping. Results of a simulation study show that Impulse can improve the performance of conjugate gradient by 30%.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Alverson, D. Callahan, D. Cummings, B. Koblenz, A. Porterfield, and B. Smith. </author> <title> The Tera computer system. </title> <booktitle> In Proceedings of the International Conference on Supercomputing, </booktitle> <pages> pages 272-277, </pages> <address> Amsterdam, The Netherlands, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Because Impulse remaps tiles without copying, we expect that tile remapping using Impulse will not be sensitive to cross-interference between tiles. 6 Related Work A number of projects have proposed modifications to conventional CPU or DRAM designs to attack the memory wall: supporting massive multithreading <ref> [1] </ref>, moving processing power on to DRAM chips [8], building programmable stream buffers and memory controllers [10], or developing configurable architectures [17]. While these projects show promise, it is now almost impossible to prototype non-traditional CPU or cache designs that can perform as well as commodity processors.
Reference: [2] <author> D. Bailey et al. </author> <title> The NAS parallel benchmarks. </title> <type> Technical Report RNR-94-007, </type> <institution> NASA Ames Research Center, </institution> <month> Mar. </month> <year> 1994. </year>
Reference-contexts: For example, most of the time in a conjugate gradient computation <ref> [2] </ref> is spent in performing a sparse matrix-vector multiply. Similarly, the Spark98 [11] kernels are all sparse matrix-vector multiplication codes. The matrix A is encoded using three dense arrays: DATA, ROWS, and COLUMN. The contents of A are in DATA.
Reference: [3] <author> B. Bershad, T. Anderson, E. Lazowska, and H. Levy. </author> <title> Lightweight Remote Procedure Call. </title> <booktitle> In Proceedings of the 12th Symposium on Operating Systems Principles, </booktitle> <pages> pages 102-113, </pages> <address> Litchfield Park, AZ, </address> <month> Dec. </month> <year> 1989. </year>
Reference-contexts: Impulse's sup-port for scatter/gather removes the high overhead of gathering data from software, thereby significantly reducing IPC overhead. Its support for building superpages means that network interfaces need not perform complex and expensive address translation. Fast local IPC mechanisms, such as LPRC <ref> [3] </ref>, use shared memory to map buffers into sender and receiver address spaces, and Impulse could be used to support fast, no-copy scatter/gather into these shared shadow memory spaces. We plan to improve our simulation environment and infrastructure in several ways.
Reference: [4] <author> B. Bershad, D. Lee, T. Romer, and J. Chen. </author> <title> Avoiding conflict misses dynamically in large direct-mapped caches. </title> <booktitle> In Proceedings of the 6th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 158-170, </pages> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: For example, in the CG-A benchmark, x is over 100K: it would not fit in most processors' L1 caches, but would fit in many L2 caches. 5 On a conventional machine, physical page coloring is hard to exploit. (Virtual page coloring has been explored by other authors <ref> [4] </ref>.) Coloring means that the operating system does not map data with poor locality to physical pages that have the same color (i.e., map to the same portion of the cache) as pages that contain data with good locality.
Reference: [5] <author> D. Burger, J. Goodman, and A. Kagi. </author> <title> Memory bandwidth limitations of future microprocessors. </title> <booktitle> In Proceedingsof the 23rd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 78-89, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: In contrast, DRAM latencies have improved by only 7% per year, and DRAM bandwidths by only 15-20% per year. In addition, as the issue rates of processors increase, the demand for memory bandwidth increases proportionately (and possibly even super-linearly) <ref> [5, 6] </ref>. The result of these trends is that it is increasingly hard to make effective use of the tremendous processing power of modern microprocessors. Fundamentally, the problem is that modern caches and memory systems are optimized for applications that have high degrees of spatial and temporal locality.
Reference: [6] <author> A. Huang and J. Shen. </author> <title> The intrinsic bandwidth requirements of ordinary programs. </title> <booktitle> In Proceedingsof the 7th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 105-114, </pages> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: In contrast, DRAM latencies have improved by only 7% per year, and DRAM bandwidths by only 15-20% per year. In addition, as the issue rates of processors increase, the demand for memory bandwidth increases proportionately (and possibly even super-linearly) <ref> [5, 6] </ref>. The result of these trends is that it is increasingly hard to make effective use of the tremendous processing power of modern microprocessors. Fundamentally, the problem is that modern caches and memory systems are optimized for applications that have high degrees of spatial and temporal locality.
Reference: [7] <author> N. Jouppi. </author> <title> Improving direct-mapped cache performance by the addition of a small fully associative cache and prefetch buffers. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 364-373, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Several researchers have proposed different forms of hardware to improve the performance of applications that access memory using regular strides (vector applications, for example). Jouppi first proposed the notion of a stream buffer <ref> [7] </ref>, which is a device that detects strided accesses, and prefetches along those strides. McKee and others proposed a programmable variant of the stream buffer [10] that allows applications to explicitly specify when they make vector accesses.
Reference: [8] <editor> C. E. Kozyrakis et al. </editor> <booktitle> Scalable processors in the billion-transistor era: IRAM. IEEE Computer, </booktitle> <pages> pages 75-78, </pages> <month> Sept. </month> <year> 1997. </year>
Reference-contexts: tiles without copying, we expect that tile remapping using Impulse will not be sensitive to cross-interference between tiles. 6 Related Work A number of projects have proposed modifications to conventional CPU or DRAM designs to attack the memory wall: supporting massive multithreading [1], moving processing power on to DRAM chips <ref> [8] </ref>, building programmable stream buffers and memory controllers [10], or developing configurable architectures [17]. While these projects show promise, it is now almost impossible to prototype non-traditional CPU or cache designs that can perform as well as commodity processors. <p> The primary difference between Impulse and Morph is that Impulse is a simpler design that current architectures can take advantage of. The RADram project at UC Davis is building a memory system that lets the memory perform computation [12]. RADram is a PIM (processor-in-memory) project similar to IRAM <ref> [8] </ref>, where the goal is to put processors close to memory. In contrast, Impulse does not seek to put a full-blown processor in memory, since DRAM processes are substantially slower than logic processes.
Reference: [9] <author> M. S. Lam, E. E. Rothberg, and M. E. Wolf. </author> <title> The cache performance and optimizations of blocked algorithms. </title> <booktitle> In Proceedings of the 4th ASPLOS, </booktitle> <pages> pages 63-74, </pages> <address> Santa Clara, CA, </address> <month> Apr. </month> <year> 1991. </year>
Reference-contexts: Such algorithms are tiled (or blocked) in order to increase their efficiency. That is, the iterations of tiled algorithms are reordered so as to improve their memory performance. The difficulty with using tiled algorithms lies in choosing a tile size <ref> [9] </ref>. Because tiles are non-contiguous in the virtual address space, it is difficult to keep them from conflicting with each other (or with themselves) in the caches. As a result, tile sizes must typically be chosen to be smaller than one might otherwise choose.
Reference: [10] <author> S. McKee et al. </author> <title> Design and evaluation of dynamic access ordering hardware. </title> <booktitle> In Proceedings of the 10th ACM International Conference on Supercomputing, </booktitle> <address> Philadelphia, PA, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: In effect, we can use a small part of the L2 cache, as a stream buffer for DATA, ROWS, and COLUMNS <ref> [10] </ref>. 4.3 Tile Remapping Impulse can be used to improve the performance of a tiled matrix multiply. <p> using Impulse will not be sensitive to cross-interference between tiles. 6 Related Work A number of projects have proposed modifications to conventional CPU or DRAM designs to attack the memory wall: supporting massive multithreading [1], moving processing power on to DRAM chips [8], building programmable stream buffers and memory controllers <ref> [10] </ref>, or developing configurable architectures [17]. While these projects show promise, it is now almost impossible to prototype non-traditional CPU or cache designs that can perform as well as commodity processors. <p> Jouppi first proposed the notion of a stream buffer [7], which is a device that detects strided accesses, and prefetches along those strides. McKee and others proposed a programmable variant of the stream buffer <ref> [10] </ref> that allows applications to explicitly specify when they make vector accesses. Both forms of stream buffer allow applications to improve their performance on regular applications, but they do not support irregular applications. Yamada [16] proposed instruction set changes to support combined relocation and prefetching into the L1 cache.
Reference: [11] <author> D. R. O'Hallaron. Spark98: </author> <title> Sparse matrix kernels for shared memory and message passing systems. </title> <type> Technical Report CMU-CS-97-178, </type> <institution> Carnegie Mellon University School of Computer Science, </institution> <month> Oct. </month> <year> 1997. </year>
Reference-contexts: For example, most of the time in a conjugate gradient computation [2] is spent in performing a sparse matrix-vector multiply. Similarly, the Spark98 <ref> [11] </ref> kernels are all sparse matrix-vector multiplication codes. The matrix A is encoded using three dense arrays: DATA, ROWS, and COLUMN. The contents of A are in DATA. ROWS [i] indicates where the i th row begins in DATA.
Reference: [12] <author> M. Oskin, F. T. Chong, and T. Sherwood. </author> <title> Active pages: A model of computation for intelligent memory. </title> <booktitle> In Proceedings of the 25th International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1998. </year> <note> To appear. </note>
Reference-contexts: The primary difference between Impulse and Morph is that Impulse is a simpler design that current architectures can take advantage of. The RADram project at UC Davis is building a memory system that lets the memory perform computation <ref> [12] </ref>. RADram is a PIM (processor-in-memory) project similar to IRAM [8], where the goal is to put processors close to memory. In contrast, Impulse does not seek to put a full-blown processor in memory, since DRAM processes are substantially slower than logic processes.
Reference: [13] <author> S. E. Perl and R. </author> <title> Sites. Studies of Windows NT performance using dynamic execution traces. </title> <booktitle> In Proceedings of the Second Symposium on Operating System Design and Implementation, </booktitle> <pages> pages 169-184, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: For scientific applications, the size of the datasets often dominate the relatively small amounts of code executed. For commercial applications such as databases, however, the amount of bandwidth required by the instruction stream can be significant <ref> [13] </ref>. Therefore, we will extend our simulation environment to explore the page coloring of instruction streams. In addition, an Impulse memory system can be used to support efficient interprocess communication (IPC). A major chore of remote IPC is collecting message data from 9 multiple user buffers and protocol headers.
Reference: [14] <author> M. Swanson, L. Stoller, and J. Carter. </author> <title> Increasing TLB reach using superpages backed by shadow memory. </title> <booktitle> In Proceedings of the 25th Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1998. </year>
Reference-contexts: By remapping physical pages in this manner, applications can color pages without copying (described in Section 4.2) and form superpages from non-contiguous physical pages <ref> [14] </ref>. * Strided physical memory: Impulse allows applications to map the addresses in a shadow page to strided physical memory. That is, a shadow address at offset soffset on the shadow page is mapped to a physical address paddr + stride fl soffset. <p> An Impulse memory controller can also be used to dynamically build superpages, which can save processor TLB entries and reduce the frequency of TLB faults <ref> [14] </ref>. Because the physical memory associated with a superpage must be contiguous and correctly aligned, it is difficult to map user data using superpages on conventional memory systems.
Reference: [15] <author> O. Temam, E. D. Granston, and W. Jalby. </author> <title> To copy or not to copy: A compile-time technique for assessing when data copying should be used to eliminate cache conflicts. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 410-419, </pages> <address> Portland, OR, </address> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: To analyze this effect, we will perform a sensitivity analysis on the results with respect to matrix size, tile size, and cache size; we have been limited by simulation time. Other authors have found that the performance of copying can vary greatly with those parameters <ref> [15] </ref>.
Reference: [16] <author> Y. Yamada. </author> <title> Data Relocation and Prefetching in Programs with Large Data Sets. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, Urbana, IL, </institution> <year> 1995. </year>
Reference-contexts: McKee and others proposed a programmable variant of the stream buffer [10] that allows applications to explicitly specify when they make vector accesses. Both forms of stream buffer allow applications to improve their performance on regular applications, but they do not support irregular applications. Yamada <ref> [16] </ref> proposed instruction set changes to support combined relocation and prefetching into the L1 cache. Because relocation is done at the processor in his system, no bus bandwidth is saved. In addition, because relocation is done on virtual addresses, the utilization of the L2 cache cannot be improved.
Reference: [17] <author> X. Zhang, A. Dasdan, M. Schulz, R. K. Gupta, and A. A. Chien. </author> <title> Architectural adaptation for application-specific locality optimizations. </title> <booktitle> In Proceedings of the 1997 IEEE International Conference on Computer Design, </booktitle> <year> 1997. </year> <month> 10 </month>
Reference-contexts: sensitive to cross-interference between tiles. 6 Related Work A number of projects have proposed modifications to conventional CPU or DRAM designs to attack the memory wall: supporting massive multithreading [1], moving processing power on to DRAM chips [8], building programmable stream buffers and memory controllers [10], or developing configurable architectures <ref> [17] </ref>. While these projects show promise, it is now almost impossible to prototype non-traditional CPU or cache designs that can perform as well as commodity processors. In addition, the performance of processor-in-memory approaches are handicapped by the optimization of DRAM processes for capacity (to increase bit density) rather than speed. <p> In addition, the performance of processor-in-memory approaches are handicapped by the optimization of DRAM processes for capacity (to increase bit density) rather than speed. We briefly describe the most closely related research projects. The Morph architecture <ref> [17] </ref> is almost entirely configurable: programmable logic is embedded in virtually every datapath in the system. As a result, optimizations similar to those that we have described are possible using Morph.
References-found: 17

