URL: http://www.cse.psu.edu/~barlow/fundamental.ps
Refering-URL: http://www.cse.psu.edu/~barlow/papers.html
Root-URL: http://www.cse.psu.edu
Title: COMPUTING THE FUNDAMENTAL MATRIX OF A MARKOV CHAIN  
Author: JESSE L. BARLOW 
Abstract: The short term behavior of a Markov chain can be inferred from its fundamental matrix F . One method of computing the parts of F that are needed is to compute F y for a given vector y. All forward stable algorithms that solve a particular least squares problem lead to forward stable algorithms for computing F y. This leads to a class of algorithms that compute F y accurately whenever the underlying problem is well-conditioned. One algorithm from this class is based upon the Grassman-Taksar-Heyman variant of Gaussian elimination. Other such algorithms include one based upon orthogonal factorization and one based upon the CGLS (Conjugate Gradient Least Squares ) algorithm. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J.L. Barlow. </author> <title> Error bounds and condition estimates for the computation of null vectors with applications to Markov chains. </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 14 </volume> <pages> 797-812, </pages> <year> 1993. </year>
Reference-contexts: The steps (1.12)- (1.13) are just the normalization p T x = p T y. Since A is irreducible, it is known that rank (B k ) = n 1 <ref> [1] </ref>, thus (1.10) has a unique solution. Moreover, B k ^x k = ^y is consistent, thus (1.10) always has a zero residual. <p> The use of the group inverse, A # , given in (1.14). See Meyer [8]. 2. The use of the sep function, see Meyer and Stewart [9] or Stewart and Sun [12, pp.230-246]. 3. The use of the Moore-Penrose inverse, A y . See, for instance, Barlow <ref> [1] </ref>. We will show that the first two approaches can be bounded using A y , thereby justifying the use of the third approach. <p> A y , is given by A y = ^ V 1 ^ U T :(2.4) Thus kA y k 2 = k 1 k 2 = oe 1 Below we show an important relation between the Moore-Penrose inverse and other characterizations of the Markov chain, generalizing a result in <ref> [1] </ref>. For this problem, the important separation for the sep function is between the matrix C = ^ U T A ^ U (2.5) and the zero eigenvalue of A. <p> Results on standard Gaussian elimination (see Barlow <ref> [1] </ref>) are sufficient to obtain the bound in Proposition 3.1. In [6], much is made of the fact that R may be ill-conditioned when A is well-conditioned. Note, however, that the above analysis by-passes any ill-conditioning in R, that ill-conditioning has no effect on the quality of the results.
Reference: [2] <author> A. Bjorck. </author> <title> Numerical Methods for Least Squares Problems. </title> <publisher> SIAM Publications, </publisher> <address> Philadelphia, PA, </address> <year> 1996. </year>
Reference-contexts: Proof. This corollary is simply the result of a classic theorem on perturbation, see for instance, Bjorck <ref> [2, p.30, Theorem 1.4.6] </ref>. Using the fact that the residual is zero, we have that kz k ^x k k 2 OE 0 (n)" M k k 2 (kyk 2 + kB y M ): The use of Lemma 2.2 and i = 1 j yields the result. 3.
Reference: [3] <author> A. Bjorck, T. Elfving, and Z. Strakos. </author> <title> Stability of conjugate gradient-type methods for solving linear least squares problems. </title> <type> Technical LiTH-MAT-R-1995-26, </type> <institution> Department of Mathematics, Linkoping University, Linkoping, Sweden, </institution> <month> November </month> <year> 1995. </year>
Reference-contexts: Other algorithms that could be used include: 10 * The CGLS (Conjugate Gradient Least Squares ) algorithm applied to (1.10). An analysis by Bjorck, Elfving, and Strakos <ref> [3] </ref> suggests that the conjugate gradient based CGLS algorithm could be used to solve (1.10) with forward stability. * Orthogonal factorization applied to B k .
Reference: [4] <author> A. Bjorck. </author> <title> Stability analysis of the method of semi-normal equations for linear least squares problems. </title> <journal> Lin. Alg. Appl., </journal> 88/89:31-48, 1987. 
Reference-contexts: Well-known backward error bounds from [14, p.236] on orthogonal factorization would be sufficient. * Corrected semi-normal equation applied to B k with the upper triangular factor from orthgonal factorization <ref> [4] </ref>. The choice of algorithm for computing (1.7) depends upon the choice of algorithm to compute p.
Reference: [5] <author> C. Davis and W.M. Kahan. </author> <title> The rotation of eigenvectors by a perturbation III. </title> <journal> SIAM J. Num. Anal., </journal> <volume> 7 </volume> <pages> 1-46, </pages> <year> 1970. </year>
Reference-contexts: Note that W is an orthogonal matrix. It can be partitioned W = n 1 1 1 W 21 W 22 = ^ V T ^ U ^ V T ^p It is an immediate corollary of the C-S decomposition <ref> [5, 11] </ref> that W 11 and W 22 are singular or nonsingular together and that if they are nonsingular then kW 1 22 k 2 : We have that W 22 = ^c T ^p = c T p=kck 2 kpk 2 : The vector c is just an n-vector of
Reference: [6] <author> D.P. </author> <title> Heyman and D.P. O'Leary. Overcoming instability in computing the fundamental matrix for a Markov chain. </title> <type> Technical Report CS-TR-6322, </type> <institution> Department of Computer Science and Institute for Advanced Computer Studies, University of Maryland, College Park, MD, </institution> <month> April </month> <year> 1996. </year>
Reference-contexts: In x2, we show that they are all closely related. For convenience, we use the one associated with the Moore-Penrose inverse of A. We also show our main result, that forward stable methods for solving (1.10) lead to forward stable methods for solving (1.7) and (1.15). Heyman and O'Leary <ref> [6] </ref> applied a variant of the Grassman-Taksar-Heyman (GTH) algorithm to solve (1.7). In x3, we show that this algorithm implicitly produces a backward stable solution of (1.10) and thus always obtains as accurate a solution of (1.7) as can be expected. <p> Why the GTH algorithm and Other Algorithms Yield Accurate Solutions. Heyman and O'Leary <ref> [6] </ref> suggest an algorithm that uses the GTH (Grassman-Taksar-Heyman) algorithm to solve (1.10) (although they do not state it as such). <p> The diagonal elements of L are computed so as to satisfy this constraint. This modification leads to the componentwise accuracy in computing p in (1.4)-(1.5) [10]. Heyman and O'Leary <ref> [6] </ref> then use this factorization to solve (1.10) with the con straint e T The constraint arises naturally out of the factorization. The first row of L must be zero, thus if we solve Rv = y;(3.3) then e T 1 v = 0 because of consistency. <p> Results on standard Gaussian elimination (see Barlow [1]) are sufficient to obtain the bound in Proposition 3.1. In <ref> [6] </ref>, much is made of the fact that R may be ill-conditioned when A is well-conditioned. Note, however, that the above analysis by-passes any ill-conditioning in R, that ill-conditioning has no effect on the quality of the results.
Reference: [7] <author> N.J. Higham. </author> <title> Accuracy and Stability of Numerical Algorithms. </title> <publisher> SIAM Publications, </publisher> <address> Philadel-phia, </address> <year> 1996. </year>
Reference-contexts: In our analysis, we assume that p is exact solution of (1.4)-(1.5). That is, all rounding errors come after p is computed. Two terms used throughout the paper are backward stable and forward stable. Their definitions correspond to those given by Higham <ref> [7, pp.8-10] </ref>. Three definitions of the conditioning of A have been used frequently in the literature. In x2, we show that they are all closely related. For convenience, we use the one associated with the Moore-Penrose inverse of A.
Reference: [8] <author> C.D. Meyer, Jr. </author> <title> The role of the group inverse in the theory of finite Markov chains. </title> <journal> SIAM Review, </journal> <volume> 17 </volume> <pages> 443-464, </pages> <year> 1975. </year>
Reference-contexts: At least three approaches have been used to characterize the condition of solving (1.4)-(1.5). It is reasonable to consider these approaches for the condition of (1.7) or (1.15). The three approaches are: 1. The use of the group inverse, A # , given in (1.14). See Meyer <ref> [8] </ref>. 2. The use of the sep function, see Meyer and Stewart [9] or Stewart and Sun [12, pp.230-246]. 3. The use of the Moore-Penrose inverse, A y . See, for instance, Barlow [1].
Reference: [9] <author> C.D. Meyer, Jr. and G.W. Stewart. </author> <title> Derivatives and perturbations of eigenvectors. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 25 </volume> <pages> 679-691, </pages> <year> 1988. </year>
Reference-contexts: It is reasonable to consider these approaches for the condition of (1.7) or (1.15). The three approaches are: 1. The use of the group inverse, A # , given in (1.14). See Meyer [8]. 2. The use of the sep function, see Meyer and Stewart <ref> [9] </ref> or Stewart and Sun [12, pp.230-246]. 3. The use of the Moore-Penrose inverse, A y . See, for instance, Barlow [1]. We will show that the first two approaches can be bounded using A y , thereby justifying the use of the third approach. <p> For this problem, the important separation for the sep function is between the matrix C = ^ U T A ^ U (2.5) and the zero eigenvalue of A. Meyer and Stewart <ref> [9] </ref> show that sep (C; 0) 1 = kC 1 k 2 = kA # ^ U k 2 kA # k 2 :(2.6) For the class of matrices given in (1.1), kC 1 k 2 is never far from kA y k 2 . Proposition 2.1.
Reference: [10] <author> C.A. O'Cinneide. </author> <title> Entrywise perturbation theory and error analysis for Markov chains. </title> <journal> Numer. Math., </journal> <volume> 65 </volume> <pages> 109-120, </pages> <year> 1993. </year>
Reference-contexts: The diagonal elements of L are computed so as to satisfy this constraint. This modification leads to the componentwise accuracy in computing p in (1.4)-(1.5) <ref> [10] </ref>. Heyman and O'Leary [6] then use this factorization to solve (1.10) with the con straint e T The constraint arises naturally out of the factorization.
Reference: [11] <author> G.W. Stewart. </author> <title> On the perturbation of psuedo-inverses, projections, and linear least squares problems. </title> <journal> SIAM Review, </journal> <volume> 19 </volume> <pages> 634-662, </pages> <year> 1977. </year>
Reference-contexts: Note that W is an orthogonal matrix. It can be partitioned W = n 1 1 1 W 21 W 22 = ^ V T ^ U ^ V T ^p It is an immediate corollary of the C-S decomposition <ref> [5, 11] </ref> that W 11 and W 22 are singular or nonsingular together and that if they are nonsingular then kW 1 22 k 2 : We have that W 22 = ^c T ^p = c T p=kck 2 kpk 2 : The vector c is just an n-vector of
Reference: [12] <author> G.W. Stewart and J.-G. Sun. </author> <title> Matrix Perturbation Theory. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: The three approaches are: 1. The use of the group inverse, A # , given in (1.14). See Meyer [8]. 2. The use of the sep function, see Meyer and Stewart [9] or Stewart and Sun <ref> [12, pp.230-246] </ref>. 3. The use of the Moore-Penrose inverse, A y . See, for instance, Barlow [1]. We will show that the first two approaches can be bounded using A y , thereby justifying the use of the third approach.
Reference: [13] <author> J.H. Wilkinson. </author> <title> Error analysis of direct methods of matrix inversion. </title> <journal> J. Assoc. Comput. Mach., </journal> <volume> 8 </volume> <pages> 281-330, </pages> <year> 1961. </year>
Reference: [14] <author> J.H. Wilkinson. </author> <title> The Algebraic Eigenvalue Problem. </title> <publisher> Oxford University Press, </publisher> <address> London, </address> <year> 1965. </year>
Reference-contexts: An analysis by Bjorck, Elfving, and Strakos [3] suggests that the conjugate gradient based CGLS algorithm could be used to solve (1.10) with forward stability. * Orthogonal factorization applied to B k . Well-known backward error bounds from <ref> [14, p.236] </ref> on orthogonal factorization would be sufficient. * Corrected semi-normal equation applied to B k with the upper triangular factor from orthgonal factorization [4]. The choice of algorithm for computing (1.7) depends upon the choice of algorithm to compute p.
References-found: 14

