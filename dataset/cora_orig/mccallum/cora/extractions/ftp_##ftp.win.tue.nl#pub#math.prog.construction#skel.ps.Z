URL: ftp://ftp.win.tue.nl/pub/math.prog.construction/skel.ps.Z
Refering-URL: http://www.win.tue.nl/cs/wp/papers/papers.html
Root-URL: http://www.win.tue.nl
Title: Transformational derivation of (parallel) programs using skeletons  
Author: Eerke A. Boiten A. Max Geerling Helmut A. Partsch 
Affiliation: Eindhoven University of Technology  University of Nijmegen  University of Ulm  
Abstract: We describe a framework for the derivation of programs for arbitrary (in particular, parallel) architectures, motivated by a generalization of the derivation process for sequential algorithms. The central concept in this approach is that of a skeleton: on the one hand, a higher-order function for targeting transformational derivations at, on the other hand representing an elementary computation on the architecture aimed at. Skeletons thus form a basis for intermediate languages, that can be implemented once and for all, as a process separate from individual program developments. The available knowledge on the derivation of (higher-order) functional programs can be used for deriving parallel ones. This paper presents an overview of the method, illustrated with an example (trapezoidal rule on SIMD processor array), and ideas for future research. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Backus. </author> <title> Can programming be liberated from the Von Neumann style? A functional style and its algebra of programs. </title> <journal> Communications of the ACM, </journal> <volume> 21 </volume> <pages> 613-641, </pages> <year> 1978. </year>
Reference-contexts: Of course, some skeletons may be valid for several architectures | it does not matter that their implementations differ. The map skeleton, for instance, is valid for all SIMD architectures. Furthermore, transformational programming has moved to argument-free, higher-order reasoning as it is calculi like BMF [4, 15] and FP <ref> [1] </ref> can already be viewed as calculi for inter-skeleton transformations. 4 Skeletons for linear arrays We consider as an example a processor array machine of the SIMD class, based on the treatment in [17, 12, 13]. <p> j [index translation; distributivity P over +] h P n j [index translation; 0 neutral for +] h P n i=0 (if i = 0 then 0 else f (x + (i 1)h) f i + if i = 0 then 0 else f (x + ih) f i) j <ref> [i = 0 j i &lt; 1] </ref> 2 i=0 (if i &lt; 1 then 0 else f (x + (i 1)h) f i + if i = 0 then 0 else f (x + ih) f i) Now, we can restrict the domain of Trap by changing the type of parameter
Reference: [2] <author> F.L. Bauer, R. Berghammer, M. Broy, W. Dosch, F. Geiselbrechtinger, R. Gnatz, E. Hangel, W. Hesse, B. Krieg-Bruckner, A. Laut, T. Matzner, B. Moller, F. Nickl, H. Partsch, P. Pepper, K. Samelson, M. Wirsing, and H. Wossner. </author> <title> The Munich Project CIP. Volume I: The Wide Spectrum Language CIP-L, </title> <booktitle> Lecture Notes in Computer Science 183. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin/Heidelberg/New York, </address> <year> 1985. </year>
Reference-contexts: A notable example of such a language is the one developed in the CIP project, called CIP-L <ref> [2] </ref> we will make use of a variant of CIP-L in this paper, basically a (usually) explicitly typed, in most places functional notation.
Reference: [3] <author> R.M. Burstall and J. Darlington. </author> <title> A transformation system for developing recursive programs. </title> <journal> Journal of the ACM, </journal> <volume> 24(1) </volume> <pages> 44-67, </pages> <year> 1977. </year>
Reference-contexts: Central to a program development methodology is the formulation of development strategies (tactics, heuristics [11]). The available case studies of skeleton developments show large numbers of small steps, like in traditional unfold/fold developments <ref> [3] </ref>, with little strategic information. Most of the steps are motivated by aspects of the envisaged solutions. Eventually, from these concrete motivations more abstract underlying principles of parallel-algorithm design should be extracted. The case studies on SIMD architectures suggest a few candidates for strategic activities.
Reference: [4] <author> R.S. Bird. </author> <title> An introduction to the theory of lists. </title> <editor> In M. Broy, editor, </editor> <booktitle> Logic of Programming and Calculi of Discrete Design. NATO ASI Series Vol. F36, </booktitle> <pages> pages 5-42. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1987. </year>
Reference-contexts: The advantage is obvious: the transformations step a hard one only has to made once for such a class of programs. So, program skeletons are used to increase expressiveness. Examples of such skeletons are divide&conquer and parallel prefix. Operators like map and reduce (cf. <ref> [4] </ref>, see section 4) are also useful for specifying programs in, even though their implementations may not always be obvious (cf. reduce below) this is also extensively argued by Skillicorn [21]. <p> Of course, some skeletons may be valid for several architectures | it does not matter that their implementations differ. The map skeleton, for instance, is valid for all SIMD architectures. Furthermore, transformational programming has moved to argument-free, higher-order reasoning as it is calculi like BMF <ref> [4, 15] </ref> and FP [1] can already be viewed as calculi for inter-skeleton transformations. 4 Skeletons for linear arrays We consider as an example a processor array machine of the SIMD class, based on the treatment in [17, 12, 13].
Reference: [5] <author> G.E. Blelloch. </author> <title> Scans as primitive parallel operations. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(11) </volume> <pages> 1526-1538, </pages> <year> 1989. </year>
Reference-contexts: reduce : (((m fi m) ! m) fi (inat ! m)) ! m reduce (; a) = n j=0 a (j) This operation is closely related to the "parallel prefix" operator, the implementation of which on various machines seems to be a main interest in the parallel programming community (cf. <ref> [5] </ref>). prefix : (((m fi m) ! m) fi (inat ! m)) ! (inat ! m) prefix (; a) = (inat i)m : i j=0 a (j) An implementation of this program skeleton in terms of the skeletons of section 4 is derived in [13].
Reference: [6] <author> E.A. Boiten, H.A. Partsch, D. Tuijnman, and N. Volker. </author> <title> How to produce correct software - an introduction to formal specification and program development by transformations. </title> <journal> The Computer Journal, </journal> <volume> 35(6) </volume> <pages> 547-554, </pages> <year> 1992. </year>
Reference-contexts: The final section draws a few conclusions and mentions a number of interesting research subjects. 2 A new viewpoint on the derivation of sequential programs A lot of experience has been gained with the transformational derivation of efficient sequential programs from functional or even non-operational formal specifications <ref> [6] </ref>. Comprehensive textbooks, based on the Munich CIP project, are [7, 16]. Feather [11] gives an extensive summary of approaches to program transformation and the most important techniques employed in those. Various sequential algorithms known from the literature have been explained, reinvented, or even corrected by transformational derivation.
Reference: [7] <author> F.L. Bauer and H. Wossner. </author> <title> Algorithmic Language and Program Development. </title> <publisher> Springer--Verlag, </publisher> <address> Berlin, </address> <year> 1982. </year>
Reference-contexts: Comprehensive textbooks, based on the Munich CIP project, are <ref> [7, 16] </ref>. Feather [11] gives an extensive summary of approaches to program transformation and the most important techniques employed in those. Various sequential algorithms known from the literature have been explained, reinvented, or even corrected by transformational derivation.
Reference: [8] <author> M. Chen and Y. Choo. </author> <title> Domain morphisms: A new construct for parallel programming and formalizing program optimization. </title> <type> Technical Report YALEU/DCS/TR-817, </type> <institution> Department of Computer Science, Yale University, </institution> <year> 1990. </year>
Reference-contexts: In both cases covers can be profitably exploited. They provide the means to express problem data structures and architecture data structures in a unifying way. A related idea is that of domain morphisms <ref> [8] </ref>. Future research should also focus on the expansions of the model in two directions. First, the smallest piece of information should be larger than an inat, e.g. a segment, such that the number of PEs available is less a restriction.
Reference: [9] <author> M.I. Cole. </author> <title> Algorithmic Skeletons: Structured Management of Parallel Computation. </title> <publisher> Pitman, </publisher> <address> London, </address> <year> 1989. </year>
Reference-contexts: This paper presents a modest approach to deriving parallel programs. Techniques already known from the area of transformational (or: calculational) program development [11, 16] are used for and extended to the derivation of parallel programs. The method of using so-called skeletons <ref> [9] </ref> that is described here has been introduced by Darlington and others [10] and used in several case studies [17, 14, 12, 13]. Recent work by Pepper [19] emphasizes the problem of data distribution in parallel-program development in this approach. <p> The idea of distinguishing separate design and transformation phases suggests the concept of a program skeleton, as opposed to the architecture skeleton of definition 1. This definition of a skeleton is closer to the one used in <ref> [9, 10, 21] </ref>: Definition 2 A program skeleton is a higher-order function that can be expressed in terms of the available skeletons for some architecture. A program skeleton represents a whole class of algorithms with similar program structures, for which efficient implementations can be derived for various architectures.
Reference: [10] <author> J. Darlington, A.J. Field, P.G. Harrison, D. Harper, G.K. Jouret, P.J. Kelly, K.M. Sephton, and D.W. Sharp. </author> <title> Structured parallel functional programming. </title> <editor> In H. Glaser and P. Hartel, editors, </editor> <booktitle> Proc. Third International Workshop on the Implementation of Functional Languages on Parallel Architectures, </booktitle> <pages> pages 31-51, </pages> <year> 1991. </year>
Reference-contexts: Techniques already known from the area of transformational (or: calculational) program development [11, 16] are used for and extended to the derivation of parallel programs. The method of using so-called skeletons [9] that is described here has been introduced by Darlington and others <ref> [10] </ref> and used in several case studies [17, 14, 12, 13]. Recent work by Pepper [19] emphasizes the problem of data distribution in parallel-program development in this approach. <p> We propose skeletons for the architectures of Figure 2 that are categorized as being data- or process-parallel. The T R-skeleton for sequential architectures given above is an example of a control-oriented (i.e., process-"parallel") skeleton. Another example of this kind is a skeleton for pipelines <ref> [10] </ref>: pipe : [stream ( ) ! stream ( )] fi stream (m) ! stream (n) pipe (fs; xs) = apply (reduce (ffi; fs); xs) where the reduce operation accumulates all values in a list with a binary operation, ffi is function composition, and [m] is the type of lists of <p> The idea of distinguishing separate design and transformation phases suggests the concept of a program skeleton, as opposed to the architecture skeleton of definition 1. This definition of a skeleton is closer to the one used in <ref> [9, 10, 21] </ref>: Definition 2 A program skeleton is a higher-order function that can be expressed in terms of the available skeletons for some architecture. A program skeleton represents a whole class of algorithms with similar program structures, for which efficient implementations can be derived for various architectures. <p> Apart from the ones mentioned above, case studies have been done by Harrison [14] on pipeline architectures, and by Pepper [18] on processor networks. Darlington et al. <ref> [10] </ref> give examples of skeletons for some more architectures. It is clear that more and larger case studies, both in describing more architectures and in deriving algorithms, should be done. Central to a program development methodology is the formulation of development strategies (tactics, heuristics [11]). <p> An example of the former can be found in [13], an example of the latter in <ref> [10] </ref>. In both cases covers can be profitably exploited. They provide the means to express problem data structures and architecture data structures in a unifying way. A related idea is that of domain morphisms [8]. Future research should also focus on the expansions of the model in two directions.
Reference: [11] <author> M.S. Feather. </author> <title> A survey and classification of some program transformation approaches and techniques. </title> <editor> In L.G.L.T. Meertens, editor, </editor> <booktitle> Proc. IFIP TC2/WG2.1 Working Conference on Program Specification and Transformation, </booktitle> <pages> pages 165-196. </pages> <publisher> North-Holland Publishing Company, </publisher> <address> Amsterdam, </address> <year> 1987. </year>
Reference-contexts: We seek to solve these problems by applying formal methods. This paper presents a modest approach to deriving parallel programs. Techniques already known from the area of transformational (or: calculational) program development <ref> [11, 16] </ref> are used for and extended to the derivation of parallel programs. The method of using so-called skeletons [9] that is described here has been introduced by Darlington and others [10] and used in several case studies [17, 14, 12, 13]. <p> Comprehensive textbooks, based on the Munich CIP project, are [7, 16]. Feather <ref> [11] </ref> gives an extensive summary of approaches to program transformation and the most important techniques employed in those. Various sequential algorithms known from the literature have been explained, reinvented, or even corrected by transformational derivation. <p> Darlington et al. [10] give examples of skeletons for some more architectures. It is clear that more and larger case studies, both in describing more architectures and in deriving algorithms, should be done. Central to a program development methodology is the formulation of development strategies (tactics, heuristics <ref> [11] </ref>). The available case studies of skeleton developments show large numbers of small steps, like in traditional unfold/fold developments [3], with little strategic information. Most of the steps are motivated by aspects of the envisaged solutions.
Reference: [12] <author> P.J.M. Frederiks. </author> <title> From applicative specifications to parallel programs. </title> <type> Master's thesis, </type> <institution> Katholieke Universiteit Nijmegen, </institution> <year> 1992. </year>
Reference-contexts: The method of using so-called skeletons [9] that is described here has been introduced by Darlington and others [10] and used in several case studies <ref> [17, 14, 12, 13] </ref>. Recent work by Pepper [19] emphasizes the problem of data distribution in parallel-program development in this approach. The formalization is done by means of so-called covers [23], which partition a data structure in such a way that subobjects may overlap. <p> programming has moved to argument-free, higher-order reasoning as it is calculi like BMF [4, 15] and FP [1] can already be viewed as calculi for inter-skeleton transformations. 4 Skeletons for linear arrays We consider as an example a processor array machine of the SIMD class, based on the treatment in <ref> [17, 12, 13] </ref>. The data processors (DP i ) can send information to their left and right neighbors, and they all have a (small) local memory DM i . In the example below, we assume that all processors know their indices in the array. <p> where prefix 0 (a 0 ; i; b) = if i = n then b else prefix 0 (shiftr (a 0 ; 1; 1 ); i + 1; zipw (; b; a 0 )) f i 5 Example: trapezoidal rule This example was first treated in Paul Frederiks' master's thesis <ref> [12] </ref>, and suggested by Norbert Volker. A well-known simple approximation of the definite integral D = R b a f (x)dx is given by the so-called trapezoidal rule. <p> The example derivations here and in the previous case studies <ref> [17, 12] </ref> show that deriving programs for a processor array is "just as easy" as deriving sequential programs, due to the single flow of instruction [13]. From these case studies, it may be concluded that the "skeleton method" is a promising and, indeed, powerful one.
Reference: [13] <author> A.M. Geerling. </author> <title> Two examples of parallel-program derivation: Parallel-prefix and matrix multiplication. </title> <type> Research Report DoC 92/33, </type> <institution> Imperial College London, </institution> <year> 1992. </year>
Reference-contexts: The method of using so-called skeletons [9] that is described here has been introduced by Darlington and others [10] and used in several case studies <ref> [17, 14, 12, 13] </ref>. Recent work by Pepper [19] emphasizes the problem of data distribution in parallel-program development in this approach. The formalization is done by means of so-called covers [23], which partition a data structure in such a way that subobjects may overlap. <p> programming has moved to argument-free, higher-order reasoning as it is calculi like BMF [4, 15] and FP [1] can already be viewed as calculi for inter-skeleton transformations. 4 Skeletons for linear arrays We consider as an example a processor array machine of the SIMD class, based on the treatment in <ref> [17, 12, 13] </ref>. The data processors (DP i ) can send information to their left and right neighbors, and they all have a (small) local memory DM i . In the example below, we assume that all processors know their indices in the array. <p> in the parallel programming community (cf. [5]). prefix : (((m fi m) ! m) fi (inat ! m)) ! (inat ! m) prefix (; a) = (inat i)m : i j=0 a (j) An implementation of this program skeleton in terms of the skeletons of section 4 is derived in <ref> [13] </ref>. <p> The example derivations here and in the previous case studies [17, 12] show that deriving programs for a processor array is "just as easy" as deriving sequential programs, due to the single flow of instruction <ref> [13] </ref>. From these case studies, it may be concluded that the "skeleton method" is a promising and, indeed, powerful one. One important reason for this is the separation of concerns between deriving programs and implementing elementary computations that is induced by employing an intermediate language. <p> Also, if the distribution of the domain is according to one of the several parameters of the function, a disentanglement of the parameters is required. An important issue related to the question of strategies also seems to be the role of data type transformations <ref> [13] </ref>. Their application is twofold: on the one hand they provide a mechanism for inter-skeleton transformations and on the other hand they can map the problem data structure to the "data structure" of the architecture at hand. An example of the former can be found in [13], an example of the <p> of data type transformations <ref> [13] </ref>. Their application is twofold: on the one hand they provide a mechanism for inter-skeleton transformations and on the other hand they can map the problem data structure to the "data structure" of the architecture at hand. An example of the former can be found in [13], an example of the latter in [10]. In both cases covers can be profitably exploited. They provide the means to express problem data structures and architecture data structures in a unifying way. A related idea is that of domain morphisms [8].
Reference: [14] <author> P.G. Harrison. </author> <title> Towards the synthesis of static parallel algorithms: a categorical approach. </title> <editor> In B. Moller, editor, </editor> <booktitle> Proc. IFIP TC2 Working Conference on Constructing Programs from Specifications, </booktitle> <pages> pages 49-69. </pages> <publisher> North-Holland Publishing Company, </publisher> <address> Amsterdam, </address> <year> 1991. </year>
Reference-contexts: The method of using so-called skeletons [9] that is described here has been introduced by Darlington and others [10] and used in several case studies <ref> [17, 14, 12, 13] </ref>. Recent work by Pepper [19] emphasizes the problem of data distribution in parallel-program development in this approach. The formalization is done by means of so-called covers [23], which partition a data structure in such a way that subobjects may overlap. <p> Essential is also that the large amount of expertise that is already available on the derivation of "sequential" algorithms can be used to a large extent. Apart from the ones mentioned above, case studies have been done by Harrison <ref> [14] </ref> on pipeline architectures, and by Pepper [18] on processor networks. Darlington et al. [10] give examples of skeletons for some more architectures. It is clear that more and larger case studies, both in describing more architectures and in deriving algorithms, should be done.
Reference: [15] <author> L.G.L.T. Meertens. </author> <title> Algorithmics | towards programming as a mathematical activity. </title> <editor> In J.W. de Bakker, M. Hazewinkel, and J.K. Lenstra, editors, </editor> <booktitle> Proc. CWI Symposium on Mathematics and Computer Science, CWI Monographs 1, </booktitle> <pages> pages 289-334, </pages> <year> 1986. </year>
Reference-contexts: Of course, some skeletons may be valid for several architectures | it does not matter that their implementations differ. The map skeleton, for instance, is valid for all SIMD architectures. Furthermore, transformational programming has moved to argument-free, higher-order reasoning as it is calculi like BMF <ref> [4, 15] </ref> and FP [1] can already be viewed as calculi for inter-skeleton transformations. 4 Skeletons for linear arrays We consider as an example a processor array machine of the SIMD class, based on the treatment in [17, 12, 13].
Reference: [16] <author> H. Partsch. </author> <title> Specification and Transformation of Programs a Formal Approach to Software Development. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1990. </year>
Reference-contexts: We seek to solve these problems by applying formal methods. This paper presents a modest approach to deriving parallel programs. Techniques already known from the area of transformational (or: calculational) program development <ref> [11, 16] </ref> are used for and extended to the derivation of parallel programs. The method of using so-called skeletons [9] that is described here has been introduced by Darlington and others [10] and used in several case studies [17, 14, 12, 13]. <p> Comprehensive textbooks, based on the Munich CIP project, are <ref> [7, 16] </ref>. Feather [11] gives an extensive summary of approaches to program transformation and the most important techniques employed in those. Various sequential algorithms known from the literature have been explained, reinvented, or even corrected by transformational derivation. <p> The transformation rules used in this derivation are mostly of a simple, "calculational" kind, e.g. distributivity and commutativity of arithmetic operators. For a survey of more transformation rules and rules of a greater complexity, cf. <ref> [16] </ref>. According to the transformational tradition, we use the term "fold" for application of a definition in one direction, viz. introducing the defined notion.
Reference: [17] <author> H. Partsch. </author> <title> Some experiments in transforming towards parallel executability. </title> <booktitle> In [20]. </booktitle>
Reference-contexts: The method of using so-called skeletons [9] that is described here has been introduced by Darlington and others [10] and used in several case studies <ref> [17, 14, 12, 13] </ref>. Recent work by Pepper [19] emphasizes the problem of data distribution in parallel-program development in this approach. The formalization is done by means of so-called covers [23], which partition a data structure in such a way that subobjects may overlap. <p> programming has moved to argument-free, higher-order reasoning as it is calculi like BMF [4, 15] and FP [1] can already be viewed as calculi for inter-skeleton transformations. 4 Skeletons for linear arrays We consider as an example a processor array machine of the SIMD class, based on the treatment in <ref> [17, 12, 13] </ref>. The data processors (DP i ) can send information to their left and right neighbors, and they all have a (small) local memory DM i . In the example below, we assume that all processors know their indices in the array. <p> The example derivations here and in the previous case studies <ref> [17, 12] </ref> show that deriving programs for a processor array is "just as easy" as deriving sequential programs, due to the single flow of instruction [13]. From these case studies, it may be concluded that the "skeleton method" is a promising and, indeed, powerful one.
Reference: [18] <author> P. Pepper. </author> <title> Deductive derivation of parallel programs. </title> <booktitle> In [20]. </booktitle>
Reference-contexts: Essential is also that the large amount of expertise that is already available on the derivation of "sequential" algorithms can be used to a large extent. Apart from the ones mentioned above, case studies have been done by Harrison [14] on pipeline architectures, and by Pepper <ref> [18] </ref> on processor networks. Darlington et al. [10] give examples of skeletons for some more architectures. It is clear that more and larger case studies, both in describing more architectures and in deriving algorithms, should be done.
Reference: [19] <author> P. Pepper, J. Exner, and M. Sudholt. </author> <title> Functional development of massively parallel programs. In Formal Methods in Programming and Their Applications, </title> <note> Lecture Notes in Computer Science, to appear 1993. </note>
Reference-contexts: The method of using so-called skeletons [9] that is described here has been introduced by Darlington and others [10] and used in several case studies [17, 14, 12, 13]. Recent work by Pepper <ref> [19] </ref> emphasizes the problem of data distribution in parallel-program development in this approach. The formalization is done by means of so-called covers [23], which partition a data structure in such a way that subobjects may overlap. The overlapping parts in a cover entail communication patterns.
Reference: [20] <author> R. Paige, J. Reif, and R. Wachter, </author> <title> editors. Parallel Algorithm Derivation and Program Transformation. </title> <publisher> Kluwer Academic Publishers, </publisher> <month> to appear </month> <year> 1993. </year>
Reference: [21] <author> D.B. Skillicorn. </author> <title> Architecture-independent parallel computation. </title> <journal> IEEE Computer, </journal> <volume> 23(12) </volume> <pages> 38-50, </pages> <year> 1990. </year>
Reference-contexts: The idea of distinguishing separate design and transformation phases suggests the concept of a program skeleton, as opposed to the architecture skeleton of definition 1. This definition of a skeleton is closer to the one used in <ref> [9, 10, 21] </ref>: Definition 2 A program skeleton is a higher-order function that can be expressed in terms of the available skeletons for some architecture. A program skeleton represents a whole class of algorithms with similar program structures, for which efficient implementations can be derived for various architectures. <p> Examples of such skeletons are divide&conquer and parallel prefix. Operators like map and reduce (cf. [4], see section 4) are also useful for specifying programs in, even though their implementations may not always be obvious (cf. reduce below) this is also extensively argued by Skillicorn <ref> [21] </ref>. The notion of program skeleton is relevant for solving another problem we identified for parallel programming, namely the diversity of available architectures, that results in the situation that software has to be developed separately for each machine. <p> : : b n zipw : (((m fi n) ! p) fi (inat ! m) fi (inat ! n)) ! (inat ! p) zipw (f; a; b) = (inat i)p : f (a (i); b (i)) ( = map (f; zip (a; b))) An important operation for specifying algorithms (cf. <ref> [21] </ref>) is the reduce operation actually a program skeleton that accumulates all values in an array with a binary associative operator.
Reference: [22] <author> D.B. Skillicorn. </author> <title> Deriving parallel programs from specifications using cost information. </title> <booktitle> Science of Computer Programming, </booktitle> <volume> 20 </volume> <pages> 205-221, </pages> <year> 1993. </year>
Reference-contexts: Second, the type of architectures should be broadened from SIMD to other data-parallel models, e.g. message-passing. A last observation is that skeletons, exactly because they represent basic computations, may be assigned computation costs, based on the costs of their components <ref> [22] </ref>. Based on this, formal complexity considerations may even be usable as a practical guideline in stepwise derivation of parallel programs. Acknowledgements For the main example in this paper we would like to thank Paul Frederiks.
Reference: [23] <author> Y.V. Srinivas. </author> <title> A sheaf-theoretic approach to pattern matching and related problems. </title> <journal> Theoretical Computer Science, </journal> <volume> 112 </volume> <pages> 53-97, </pages> <year> 1993. </year>
Reference-contexts: Recent work by Pepper [19] emphasizes the problem of data distribution in parallel-program development in this approach. The formalization is done by means of so-called covers <ref> [23] </ref>, which partition a data structure in such a way that subobjects may overlap. The overlapping parts in a cover entail communication patterns.
References-found: 23

