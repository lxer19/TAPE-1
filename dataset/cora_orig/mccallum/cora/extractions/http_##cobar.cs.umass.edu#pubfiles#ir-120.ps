URL: http://cobar.cs.umass.edu/pubfiles/ir-120.ps
Refering-URL: http://cobar.cs.umass.edu/pubfiles/
Root-URL: 
Email: fponte, croftg@cs.umass.edu  
Title: A Language Modeling Approach to Information Retrieval  
Author: Jay M. Ponte and W. Bruce Croft 
Address: Amherst  
Affiliation: Computer Science Department University of Massachusetts,  
Abstract: Models of document indexing and document retrieval have been extensively studied. The integration of these two classes of models has been the goal of several researchers but it is a very difficult problem. We argue that much of the reason for this is the lack of an adequate indexing model. This suggests that perhaps a better indexing model would help solve the problem. However, we feel that making unwarranted parametric assumptions will not lead to better retrieval performance. Furthermore, making prior assumptions about the similarity of documents is not warranted either. Instead, we propose an approach to retrieval based on probabilistic language modeling. We estimate models for each document individually. Our approach to modeling is non-parametric and integrates document indexing and document retrieval into a single model. One advantage of our approach is that collection statistics which are used heuristically in many other retrieval models are an integral part of our model. We have implemented our model and tested it empirically. Our approach significantly outperforms standard tf.idf weighting on two different collections and query sets. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Bookstein, A. and D. Swanson. </author> <title> "Probabilistic models for automatic indexing" Journal for the American Society for Information Science. </title> <journal> v.25 no.5 pp. </journal> <pages> 312-318, </pages> <year> 1976. </year>
Reference-contexts: Now we take a brief look at some existing models of document indexing. We begin our discussion of indexing models with the 2-Poisson model, due to Bookstein and Swanson <ref> [1] </ref> and Permission to make digital/hard copy of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage, the copyright notice, the title of the publication and its date appear, and notice is
Reference: [2] <author> Byrd, D. </author> <type> Personal Communication. </type> <year> 1998. </year>
Reference-contexts: We can imagine a variety of both textual and graphical tools to help users get a better sense of the distribution of terms. This especially important to win over expert users who often prefer boolean retrieval because they like the sense of control over the search <ref> [2] </ref>. Regarding the results of our study, performance on two different query sets was better than that obtained by tf.idf weighting. However, the improvement in performance is not the main point. More significant is that a different approach to retrieval has been shown to be effective.
Reference: [3] <author> Croft, W. B. and D. J. Harper. </author> <title> "Using probabilistic models of document retrieval without relevance information." </title> <journal> Journal of Documentation, </journal> <volume> 35, </volume> <pages> 1979 (pp. 285-295). </pages>
Reference-contexts: Two well known probabilistic approaches to retrieval are the Robertson and Sparck Jones model [14] and the Croft and Harper model <ref> [3] </ref>. Both of these models estimate the probability of relevance of each document to the query. Our approach differs in that we do not focus on relevance except to the extent that the process of query production is correlated with it. An additional probabilistic model is that of Fuhr [4].
Reference: [4] <author> Fuhr, N. </author> <title> "Models for Retrieval with Probabilistic Indexing" Information Processing and Management. </title> <editor> v. </editor> <volume> 25, no. </volume> <month> 1 </month> <year> 1989. </year>
Reference-contexts: Both of these models estimate the probability of relevance of each document to the query. Our approach differs in that we do not focus on relevance except to the extent that the process of query production is correlated with it. An additional probabilistic model is that of Fuhr <ref> [4] </ref>. A notable feature of the Fuhr model is the integration of indexing and retrieval models. The main difference between this approach and ours is that in the Fuhr model the collection statistics are used in a heuristic fashion in order to estimate the probabilities of assigning concepts to documents.
Reference: [5] <author> Ghosh, M. J., T. Hwang and K. W. Tsui. </author> <title> "Construction of Improved Estimators in Multiparameter Estimation for Discrete Exponential Families." </title> <journal> Annals of Statistics, v. </journal> <volume> 11, </volume> <pages> 351-367. </pages>
Reference-contexts: The intuition behind this formula is that as the tf gets further away from the normalized mean, the mean probability becomes riskier to use as an estimate. For a somewhat related use of the geometric distribution see <ref> [5] </ref>.
Reference: [6] <author> Greenwood, M. and G. U. Yule. </author> <title> "An Inquiry into the Nature of Frequency Distribution Representative of Multiple Happenings with Particular Reference to the Occurrence of Multiple Attacks of Disease or of Repeated Accidents." </title> <journal> Journal of the Royal Statistical Society. v. </journal> <volume> 83, </volume> <pages> pp. 255-279, </pages> <year> 1920. </year>
Reference: [7] <author> Harter, S. P. </author> <title> "A Probabilistic Approach to Automatic Keyword Indexing" Journal of the American Society for Information Science, </title> <address> July-August, </address> <year> 1975. </year>
Reference-contexts: To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. SIGIR'98, Melbourne, Australia c fl 1998 ACM 1-58113-015-5 8/98 $5.00. also to Harter <ref> [7] </ref>. By analogy to manual indexing, the task was to assign a subset of words contained in a document (the `specialty words') as indexing terms. <p> Instead, we rely on non-parametric methods. Regarding the second assumption, the 2-Poisson model was originally based on the idea of `eliteness' <ref> [7] </ref>. It was assumed that a document elite for a given term would satisfy a user if the user posed that single term as a query. Since that time, the prevailing view has come to be that multiple term queries are more realistic. <p> One of the assumptions of the model was that a subset of terms occurring in a document would be useful for indexing. According to Harter <ref> [7] </ref>, such words can be identified by their distribution and thereby assigned as indexing terms. Documents were assumed to be of approximately equal length, a reasonable assumption for the data used in the initial studies [7]. <p> According to Harter <ref> [7] </ref>, such words can be identified by their distribution and thereby assigned as indexing terms. Documents were assumed to be of approximately equal length, a reasonable assumption for the data used in the initial studies [7]. This model is somewhat similar to ours if one views the probability of term assignment as analogous to the term generation probability.
Reference: [8] <author> Kalt, T. </author> <title> "A New Probabilistic Model of Text Classification and Retrieval", </title> <type> CIIR Tech. Report No. 78, </type> <year> 1996. </year>
Reference-contexts: In our approach we have been able to avoid this extra complexity and perform retrieval according to a single probabilistic model. The most similar approach to the one we have taken is that of Kalt <ref> [8] </ref>. In this model, documents are assumed to be generated by a stochastic process; a multinomial model. The task Kalt investigated was text classification. Each document was treated as a sample from a language model representing the class of that document.
Reference: [9] <author> Kwok, K. L. </author> <title> "A New Method of Weighting Query Terms for Ad-Hoc Retrieval", </title> <booktitle> In proceedings of ACM SIGIR 1996, </booktitle> <pages> pp 187-195. </pages>
Reference-contexts: We will discuss this point further in section 5. In section 3 we will discuss our probability estimation procedure. One statistic that we will be using is the average probability of term occurrence. A similar statistic was used by Kwok <ref> [9] </ref> for a different purpose. Kwok used the unnormalized average tf to estimate the importance of a term with respect to the query. In our approach we use the average of tf normalized by document length in the estimation of the generation probability.
Reference: [10] <author> Margulis, E. L. </author> <title> "Modeling documents with multiple Poisson distributions." </title> <booktitle> Information Processing and Management v. </booktitle> <volume> 29 no. </volume> <pages> 2 pp. 215-227, </pages> <year> 1993. </year>
Reference-contexts: Other researchers have proposed a mixture model of more than two Poisson distributions in order to better fit the observed data. Margulis proposed the n-Poisson model and tested the idea empirically <ref> [10] </ref>. The conclusion of this study was that a mixture of n-Poisson distributions provides a very close fit to the data. In a certain sense, this is not surprising. <p> However, what is somewhat surprising is the closeness of fit for relatively small values of n reported by Margulis <ref> [10] </ref>. Nevertheless, the n-Poisson model has not brought about increased retrieval effectiveness in spite of the close fit to the data. In any event, the semantics of the underlying distributions are less obvious in the n-Poisson case as compared to the 2-Poisson case where they model the concept of eliteness.
Reference: [11] <author> Parzen, E. </author> <title> "On estimation of a probability density function and mode." </title> <journal> Annals of Mathematical Statistics, </journal> <volume> vol. 33, </volume> <year> 1962. </year>
Reference: [12] <author> Robertson, S. E. and S. Walker. </author> <title> Some simple effec-tive approximations to the 2-Poisson model for probabilistic weighted retrieval. </title> <booktitle> In proceedings of ACM SIGIR 1994. </booktitle> <pages> pp. 232-241. </pages>
Reference-contexts: The success of the 2-Poisson model has been somewhat limited but it should be noted that Robertson's tf, which has been quite successful, was intended to behave similarly to the 2-Poisson model <ref> [12] </ref>. Other researchers have proposed a mixture model of more than two Poisson distributions in order to better fit the observed data. Margulis proposed the n-Poisson model and tested the idea empirically [10].
Reference: [13] <author> Ponte, J. M. and W. B. Croft. </author> <title> "Text Segmentation by Topic," </title> <booktitle> in Proceedings of the First European Conference on Research and Advanced Technology for Digital Libraries, </booktitle> <year> 1997. </year>
Reference-contexts: This engine was originally implemented as a high throughput retrieval system in the context of our previous work on topic segmentation <ref> [13] </ref>. For these experiments, the system does tokenization, stopping and stemming in the usual way. We have implemented both standard tf.idf weighting as well our language modeling approach. 4.3 Recall/Precision Experiments Table 1 shows the results for TREC topics 202-250 on TREC disks 2 and 3.
Reference: [14] <author> Robertson, S. E. and K. Sparck Jones. </author> <title> "Relevance Weighting Of Search Terms," </title> <journal> Journal of the American Society for Information Science, </journal> <volume> vol. 27, </volume> <year> 1977. </year>
Reference-contexts: The two main differences are that we do not make distributional assumptions and we do not not distinguish a subset of specialty words or assume a preexisting classification of documents into elite and non-elite sets. Two well known probabilistic approaches to retrieval are the Robertson and Sparck Jones model <ref> [14] </ref> and the Croft and Harper model [3]. Both of these models estimate the probability of relevance of each document to the query. Our approach differs in that we do not focus on relevance except to the extent that the process of query production is correlated with it.
Reference: [15] <author> Salton, G. </author> <title> Automatic Text Processing. </title> <publisher> Addison Wesley, </publisher> <year> 1989. </year>
Reference-contexts: The first sense denotes an abstraction of the retrieval task itself. The best example of this is the vector space model which allows one to talk about the task of retrieval apart from implementation details such as storage media, and data structures <ref> [15] </ref>. A second sense of the word 'model' is the probabilistic sense where it refers to an explanatory model of the data. This was intention behind the 2-Poisson model. We add a third sense of the word when we refer to language modeling.
Reference: [16] <author> Silverman, B. W. </author> <title> Density Estimation for Statistics and Data Analysis Chapman and Hall, </title> <year> 1986. </year>
Reference-contexts: In our approach we relax these two assumptions. Rather than making parametric assumptions, as is done in the 2-Poisson model it is assumed that terms follow a mixture of two Poisson distributions, as Silverman said, "the data will be allowed to speak for themselves <ref> [16] </ref>." We feel that it is unnecessary to construct a parametric model of the data when we have the actual data. Instead, we rely on non-parametric methods. Regarding the second assumption, the 2-Poisson model was originally based on the idea of `eliteness' [7].
Reference: [17] <author> Terrell, G. R. and D. W. </author> <title> Scott. </title> <journal> "Oversmoothed Nonparametric Density Estimators" Journal of the American Statistical Association. </journal> <volume> Vol. 3, Number 389, </volume> <year> 1985. </year>
Reference: [18] <author> Titterington, D. M., U. E. Makov and A. F. M Smith. </author> <title> Statistical Analysis of Finite Mixture Distributions John Wiley and Sons, </title> <year> 1985. </year>
Reference-contexts: In a certain sense, this is not surprising. For large values of n one can fit a very complex distribution arbitrarily closely by a mixture of n parametric models if one has enough data to estimate the parameters <ref> [18] </ref>. However, what is somewhat surprising is the closeness of fit for relatively small values of n reported by Margulis [10]. Nevertheless, the n-Poisson model has not brought about increased retrieval effectiveness in spite of the close fit to the data.
Reference: [19] <author> Turtle H. and W. B. Croft. </author> <title> "Efficient Probabilistic Inference for Text Retrieval," </title> <booktitle> Proceedings of RIAO 3, </booktitle> <year> 1991. </year>
Reference-contexts: In our approach, we are able to avoid using heuristic methods since we are not inferring concepts from terms. Another recent probabilistic approach is the INQUERY inference network model by Turtle and Croft <ref> [19] </ref>. Similar to the Fuhr model, Turtle and Croft integrate indexing and retrieval by making inferences of concepts from features. Features include words, phrases and more complex structured features.
Reference: [20] <author> Wong, S. K. M. and Y. Y. Yao. </author> <title> "A Probability Distribution Model for Information Retrieval" Information Processing and Management, </title> <editor> v. </editor> <volume> 25 no. </volume> <pages> 1 pp. 39-53, </pages> <year> 1989. </year>
Reference-contexts: Kwok used the unnormalized average tf to estimate the importance of a term with respect to the query. In our approach we use the average of tf normalized by document length in the estimation of the generation probability. Wong and Yao <ref> [20] </ref> proposed a model in which they represented documents according to a probability distribution. They then developed two separate approaches to retrieval, one based on utility theory and the other based on information theory. Regarding the probability distribution, Wong and Yao use a maximum likelihood estimator for term probabilities.
Reference: [21] <author> Yamron, J. </author> <title> "Topic Detection and Tracking Segmentation Task" In proceedings of The Topic Detection and Tracking Workshop, </title> <address> Oct. </address> <year> 1997. </year>
Reference-contexts: This was intention behind the 2-Poisson model. We add a third sense of the word when we refer to language modeling. The phrase `language model' is used by the speech recognition community to refer to a probability distribution that captures the statistical regularities of the generation of language <ref> [21] </ref>. In the context of the retrieval task, we can treat the generation of queries as a random process. Generally speaking, language models for speech attempt to predict the probability of the next word in an ordered sequence.
Reference: [22] <author> Zelen, M. and N. Severo. </author> <title> "Probability Functions" Handbook of Mathematical Functions. </title> <editor> M. Abramowitz and I. A. Stegun ed. </editor> <booktitle> National Bureau of Standards Applied Mathematics Series No. </booktitle> <volume> 55. </volume> <year> 1964. </year>
Reference-contexts: In order to benefit from the robustness of this estimator and to minimize the risk we will model the risk for a term t in a document d using the geometric distribution <ref> [22] </ref> as follows: ^ R t;d = 1:0 (1:0 + f t ) where f t is the the mean term frequency of term t in documents where t occurs, i.e., p avg (t) fi dl d .
References-found: 22

