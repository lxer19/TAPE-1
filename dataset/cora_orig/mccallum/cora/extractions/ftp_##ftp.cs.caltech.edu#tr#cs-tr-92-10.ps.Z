URL: ftp://ftp.cs.caltech.edu/tr/cs-tr-92-10.ps.Z
Refering-URL: ftp://ftp.cs.caltech.edu/tr/INDEX.html
Root-URL: http://www.cs.caltech.edu
Title: Runtime Systems for Fine-Grain Multicomputers  
Author: Nanette Jackson Boden 
Degree: Thesis by  In Partial Fulfillment of the Requirements for the Degree of Doctor of Philosophy  
Note: Caltech-CS-TR-92-10  
Date: 1993 (Submitted January 20, 1993)  
Address: Pasadena, California USA  
Affiliation: California Institute of Technology  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Gul A. Agha. </author> <title> ACTORS: A Model of Concurrent Computation in Distributed Systems. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1987. </year>
Reference-contexts: Based on the state of the process and the contents of the message, this code may create new processes, send messages, and modify its own state. These semantics were heavily influenced by the Actor model of computation <ref> [1] </ref>. When receiving a message, the process must accept the message at the front of the queue (ie, the message that has been in the queue for the longest time). No hidden facilities for buffering unwanted messages are provided; each message must be processed in the order received. <p> Other researchers implemented unbounded data structures similar to the queue, but they often modified the problem specification to solve a restricted case. Notably, in Agha's Actor implementation of a stack <ref> [1] </ref>, values are pushed and popped based on the arrival order of PUSH and POP requests from other processes. However, message order is not preserved in Actor systems, so messages sent between pairs of communicating actors may be received in any order. <p> Nondeterminacy, the absence of a rigid execution order, is fundamental to the concurrent execution of operations [10, page 4]. However, excessive nondeterminacy can significantly increase the total amount of work to be performed. Agha describes an actor that could be used to remove unwanted nondeterminacy, namely, a buffer actor <ref> [1] </ref> that creates new actors to buffer messages. Restoring the original message order between pairs of actors would require some explicit representation of the original message order that buffer actors could use to identify and forward desired messages, while buffering "out-of-order" messages. <p> These observations were influenced by the work of Agha <ref> [1] </ref>. Consequently, later programming models focused on computations composed of reactive processes. A reactive process is a process that is normally at rest. It executes for a bounded time in response to receiving a message and then either prepares to receive another message or exits. <p> The base language Smalltalk complicated the implementation of parallel runtime support, so CST was supported only on sequential computers. CST now follows its own evolutionary track [12]. Cantor Between 1984 and 1987, Athas developed Cantor, an experimental Actor language <ref> [1] </ref> that our research group used to study the programming limits of reactive semantics. The Cantor Experiment [4, 5, 8] also explored the balance between programming expressivity and efficiency of implementation.
Reference: [2] <author> R. Arlauskas, P. </author> <title> Close, and S.F. Nugent. Assorted iPSC papers. </title> <booktitle> In Proceedings of the Third Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 38-60 (also 843-846), </pages> <address> New York, 1988. </address> <publisher> ACM Press. </publisher>
Reference-contexts: Scaling Tracks The design choices concerning how to apply additional technology resources have defined two scaling tracks of multicomputer architecture. Table 1.1 details the characteristics of some implementations of the multicomputer architecture that have been constructed during the past decade <ref> [29, 26, 2, 32, 23] </ref>. The increases in processor speed and the network channel throughput are consequences of organizational improvements in addition to advances in technology [31, page 157].
Reference: [3] <author> W.C. Athas and C.L. Seitz. </author> <title> Multicomputers: Message-Passing Concurrent Computers. </title> <journal> IEEE Computer, </journal> <volume> 21(8) </volume> <pages> 9-24, </pages> <year> 1988. </year>
Reference: [4] <author> William C. Athas. </author> <title> Fine Grain Concurrent Computations. </title> <type> Ph.D. thesis, </type> <institution> California Institute of Technology, </institution> <year> 1987. </year>
Reference-contexts: No hidden facilities for buffering unwanted messages are provided; each message must be processed in the order received. This reactive handling of messages <ref> [40, 4] </ref> is a significant departure from traditional protocols used for process interaction. Selective-receive capabilities, the ability to receive messages based on their contents and/or the state of the process, must be implemented using purely reactive semantics. <p> A better solution to the unbounded queue problem would use distributed processes to implement the queue of values. 1.2.2 A Solution Using Distributed Processes The question of how to use distributed processes to implement an unbounded queue has been studied for roughly a decade <ref> [22, 4, 8] </ref>. Most of the problem "solutions" were either restricted to simplified forms of the unbounded queue problem or were quite wasteful of space and/or work. The development of an acceptable solution within our computational framework was a key starting point for the research in this thesis. <p> Recall that this type of buffering is expressly not provided in our model. An obvious solution to this problem is to constrain the queue's environment so that the problem does not arise. In <ref> [4] </ref>, Athas defines the environment as a single-consumer process that does not issue a get message until the reply from the previous get message has arrived. In this case, the queue manager is guaranteed to be reconnected to the queue before the next get message arrives. <p> More importantly, the queue manager does not send a new get message to the request queue until the reply from the previous get message is received. Thus, the request queue is an instance of Athas's constrained queue solution mentioned above, and described in <ref> [4] </ref>. Since the multiple-producer, multiple-consumer queue implementation relies only on a known solution to the simpler problem, there is no recursive requirement. The solution to the general unbounded queue problem that has been presented is an improvement over earlier solutions and approaches (see next section). <p> The Reactive Kernel system [34] provides message discretion via library functions that are layered on a kernel that handles messages reactively. In [8], we proposed that a remote function construct be added to the reactive programming language Cantor <ref> [4] </ref> (section 1.3.2) to provide the programmer with a selective receive. This construct, in turn, relied on a system-level mechanism called a custom function to buffer unwanted messages. <p> A very expressive, high-level, programming notation usually leads to a commensurately inefficient implementation. Conversely, a very efficient implementation typically relies on limiting the expressive power of the notation <ref> [4, pages 155-159] </ref>. In this section, the programming models and notations that have been developed specifically for multicomputers will be briefly discussed. In the next section, the runtime systems for executing programs on multicomputers will be described. <p> Seitz, who was involved in the development of most of these programming systems. 18 Year Notation Target RTS Multicomputers Implementation 1982 Simula [22] fine/medium not implemented 1985 Cosmic Cube C [41] medium Cosmic Kernel [41] 1985 CST [13] fine/medium not implemented 1986 Cantor <ref> [4] </ref> fine Cantor [4] 1986 Cosmic C [33] medium Reactive Kernel [34] 1989 Reactive C [40] fine/medium Reactive Kernel [34] 1992 Affinity [37] medium Affinity Kernel [37] 1993 C + [35] medium C + [35] fine MADRE Table 1.2: Evolution of Multicomputer Software Systems. processes to nodes, concurrency can be exploited. <p> Seitz, who was involved in the development of most of these programming systems. 18 Year Notation Target RTS Multicomputers Implementation 1982 Simula [22] fine/medium not implemented 1985 Cosmic Cube C [41] medium Cosmic Kernel [41] 1985 CST [13] fine/medium not implemented 1986 Cantor <ref> [4] </ref> fine Cantor [4] 1986 Cosmic C [33] medium Reactive Kernel [34] 1989 Reactive C [40] fine/medium Reactive Kernel [34] 1992 Affinity [37] medium Affinity Kernel [37] 1993 C + [35] medium C + [35] fine MADRE Table 1.2: Evolution of Multicomputer Software Systems. processes to nodes, concurrency can be exploited. <p> CST now follows its own evolutionary track [12]. Cantor Between 1984 and 1987, Athas developed Cantor, an experimental Actor language [1] that our research group used to study the programming limits of reactive semantics. The Cantor Experiment <ref> [4, 5, 8] </ref> also explored the balance between programming expressivity and efficiency of implementation. In its early form, Cantor was a deliberately restrictive programming language: programmers could not express pointers, internal iteration, or internal data structures. <p> The runtime system executes the message queue, invoking the process that is the destination of the message at the front of the message queue and then removing that message from the queue. The Cantor runtime system <ref> [4, 5] </ref> is also based on reactive scheduling. Cantor aggressively exploits compiler technology to minimize runtime support. In addition, this system automatically manages system resources, including placing processes. <p> The Cantor runtime system [4, 5] is also based on reactive scheduling. Cantor aggressively exploits compiler technology to minimize runtime support. In addition, this system automatically manages system resources, including placing processes. A series of simulations <ref> [4] </ref>, conducted using a simplified machine model, demonstrate that a runtime system can do a reasonable job of automatic resource management. The Affinity Kernel (1992) [37] is a node runtime system that supports the execution of reactive processes that share data structures. <p> In general, fine-grain programmers should express the maximal concurrency in their application using fine-grain, reactive processes. The Cantor project, as a experiment in how to write fine-grain programs, was the major vehicle for the development of these methods <ref> [4, 8] </ref>. 3.2.1 Maximal Concurrency Since a fine-grain multicomputer has a large number of nodes, each of which can support multiple resident processes, an application program may employ an enormous number of concurrent processes. The more processes an application can legitimately use, the more opportunity there is for concurrency. <p> Since reactive processes consume messages in the order in which they are received, communication deadlock is avoided and the small local memory of the node does not overflow. 46 In the Cantor experiment <ref> [4] </ref>, all programs were originally written with purely reactive semantics. This programming constraint was sometimes cumbersome, particularly if the algorithm being implemented included use of "conventional" data structures, such as stacks and queues (section 1.2). <p> No locality between the parent and child processes is preserved using this algorithm. Placing processes purely at random provides a useful base case for the class of randomized process-placement algorithms. The practicality of this algorithm has been demonstrated by Athas in earlier work <ref> [4] </ref>. walk One of the four neighbors of the node where the parent process resides is selected at random. If the placement attempts repeatedly fail, this algorithm in effect executes a random walk of the machine.
Reference: [5] <author> William C. Athas and C.L. Seitz. </author> <type> Cantor User Report Version 2.0. </type> <institution> Computer Science Department 5232:TR:86, California Institute of Technology, </institution> <year> 1986. </year>
Reference-contexts: CST now follows its own evolutionary track [12]. Cantor Between 1984 and 1987, Athas developed Cantor, an experimental Actor language [1] that our research group used to study the programming limits of reactive semantics. The Cantor Experiment <ref> [4, 5, 8] </ref> also explored the balance between programming expressivity and efficiency of implementation. In its early form, Cantor was a deliberately restrictive programming language: programmers could not express pointers, internal iteration, or internal data structures. <p> The runtime system executes the message queue, invoking the process that is the destination of the message at the front of the message queue and then removing that message from the queue. The Cantor runtime system <ref> [4, 5] </ref> is also based on reactive scheduling. Cantor aggressively exploits compiler technology to minimize runtime support. In addition, this system automatically manages system resources, including placing processes.
Reference: [6] <author> Sandeep Bhatt and Jin-Yi Cai. </author> <title> Take a walk, grow a tree. </title> <booktitle> In Proceedings of the 29th Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <year> 1988. </year>
Reference-contexts: The combination of multiprogramming on each node and the large number of small processes that comprise a computation simplifies the task of distributing the processes across the nodes of the ensemble. In <ref> [6] </ref>, Bhatt presents a load-balancing algorithm that uses a simple randomized algorithm to dynamically embed binary trees into hypercubes. This algorithm requires only local information and has provable bounds on the distance between the parent and the child processes and on the number of processes placed on each node.
Reference: [7] <author> Peter Bickel and Kjell Doksum. </author> <title> Mathematical Statistics: Basic Ideas and Selected Topics. </title> <publisher> Holden-Day, </publisher> <address> San Francisco, CA, </address> <year> 1977. </year>
Reference-contexts: Test programs have been selected for inclusion in the suite because they satisfy two criteria: their behaviors are simple enough to be immediately understood, yet they illustrate the performance differences between runtime-system algorithm alternatives. Each program is executed for a number of Monte Carlo trials <ref> [7] </ref>. The random-number generator of the runtime system is seeded differently for each run. Executing the program repeatedly mitigates the effects of initial conditions on runtime-system performance and provides a more accurate estimate of output distributions.
Reference: [8] <author> Nanette J. Boden. </author> <title> A Study of Fine-Grain Programming Using Cantor. </title> <institution> Computer Science Department Caltech-CS-TR-88-11, California Institute of Technology, </institution> <year> 1988. </year> <type> Master's thesis. </type>
Reference-contexts: A better solution to the unbounded queue problem would use distributed processes to implement the queue of values. 1.2.2 A Solution Using Distributed Processes The question of how to use distributed processes to implement an unbounded queue has been studied for roughly a decade <ref> [22, 4, 8] </ref>. Most of the problem "solutions" were either restricted to simplified forms of the unbounded queue problem or were quite wasteful of space and/or work. The development of an acceptable solution within our computational framework was a key starting point for the research in this thesis. <p> In the original Cosmic Kernel system [29], programmers could embed type information in outgoing messages and specify the desired type of messages to be received. The Reactive Kernel system [34] provides message discretion via library functions that are layered on a kernel that handles messages reactively. In <ref> [8] </ref>, we proposed that a remote function construct be added to the reactive programming language Cantor [4] (section 1.3.2) to provide the programmer with a selective receive. This construct, in turn, relied on a system-level mechanism called a custom function to buffer unwanted messages. <p> CST now follows its own evolutionary track [12]. Cantor Between 1984 and 1987, Athas developed Cantor, an experimental Actor language [1] that our research group used to study the programming limits of reactive semantics. The Cantor Experiment <ref> [4, 5, 8] </ref> also explored the balance between programming expressivity and efficiency of implementation. In its early form, Cantor was a deliberately restrictive programming language: programmers could not express pointers, internal iteration, or internal data structures. <p> Applications written to be executed on medium-grain machines may consist of processes that are simply too large even to be loaded on a fine-grain machine. In contrast, fine-grain computations are typically collections of hundreds, thousands, or tens of thousands of small cooperating processes <ref> [8] </ref>. Messages are frequently sent between processes; these messages may be only a few words in length. New processes are created liberally; virtually any opportunity for concurrency is exploited. <p> In general, fine-grain programmers should express the maximal concurrency in their application using fine-grain, reactive processes. The Cantor project, as a experiment in how to write fine-grain programs, was the major vehicle for the development of these methods <ref> [4, 8] </ref>. 3.2.1 Maximal Concurrency Since a fine-grain multicomputer has a large number of nodes, each of which can support multiple resident processes, an application program may employ an enormous number of concurrent processes. The more processes an application can legitimately use, the more opportunity there is for concurrency. <p> Such process data structures are useful for a variety of grid-based application programs (eg, fluid flow). The algorithm for the mesh program is completely described in <ref> [8, pp. 24-29] </ref>. The size of the mesh, size 1 , is input to the main program. The (0; 0) element process is created by 1 For simplicity, the mesh is assumed to be square. 125 Random-Random Placement. <p> Since each process must in general possess references to each of its four neighbors, the references to the north and south processes are obtained using a "cross-stitching" pattern of message passing as described in <ref> [8] </ref>. Histograms of the distance between mesh-neighbor processes, on the 16fi16 Mosaic ensemble, when a square mesh of size 50 is placed using each of the three 2 major process-placement strategies are shown in Figure 6.22, Figure 6.23, and Figure 6.24.
Reference: [9] <author> Nanette J. Boden, Charles L. Seitz, Jakov Seizovic, and Wen-king Su. </author> <title> The design of the Caltech Mosaic C multicomputer. In Gaetano Boriello, editor, Advanced Research in VLSI. </title> <note> UW Press, to be published in 1993. </note>
Reference-contexts: The Mosaic is also the experimental platform for the series of runtime experiments presented in this thesis. (For a description of the design and implementation of the Mosaic, see <ref> [9] </ref>). As depicted in Figure 1.4, the multicomputer architecture can be decomposed into its constituent parts, the computing node and the message-passing network.
Reference: [10] <author> K. Mani Chandy and Jayadev Misra. </author> <title> Parallel Program Design: A Foundation. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, MA, </address> <year> 1988. </year>
Reference-contexts: This solution is only LIFO from the point of reference of the stack itself, not from the point of reference of any user processes. Nondeterminacy, the absence of a rigid execution order, is fundamental to the concurrent execution of operations <ref> [10, page 4] </ref>. However, excessive nondeterminacy can significantly increase the total amount of work to be performed. Agha describes an actor that could be used to remove unwanted nondeterminacy, namely, a buffer actor [1] that creates new actors to buffer messages.
Reference: [11] <author> K. Mani Chandy and Stephen Taylor. </author> <title> An Introduction to Parallel Programming. </title> <publisher> Jones and Bartlett Publishers, </publisher> <address> Boston, MA, </address> <year> 1992. </year>
Reference-contexts: The goal of this project is not only to develop a high-performance machine, but also to develop the experimental apparatus for investigations into other issues for fine-grain machines. For example, the Mosaic is a target for the Program Composition Notation (PCN) of Chandy and Taylor <ref> [11] </ref>, a high-level architecture-independent concurrent language. The Mosaic is also the experimental platform for the series of runtime experiments presented in this thesis. (For a description of the design and implementation of the Mosaic, see [9]).
Reference: [12] <author> Andrew Chien and William J. Dally. </author> <title> CST: An Object-Oriented Concurrent Language. </title> <journal> SIG-PLAN Notices, </journal> <month> April </month> <year> 1989. </year>
Reference-contexts: At the time of its development, CST was primarily a reference language. The base language Smalltalk complicated the implementation of parallel runtime support, so CST was supported only on sequential computers. CST now follows its own evolutionary track <ref> [12] </ref>. Cantor Between 1984 and 1987, Athas developed Cantor, an experimental Actor language [1] that our research group used to study the programming limits of reactive semantics. The Cantor Experiment [4, 5, 8] also explored the balance between programming expressivity and efficiency of implementation.
Reference: [13] <author> William J. Dally. </author> <title> A VLSI architecture for Concurrent Data Structures. </title> <publisher> Kluwer Academic Publishers, Norwell, </publisher> <address> MA, </address> <year> 1987. </year>
Reference-contexts: Seitz, who was involved in the development of most of these programming systems. 18 Year Notation Target RTS Multicomputers Implementation 1982 Simula [22] fine/medium not implemented 1985 Cosmic Cube C [41] medium Cosmic Kernel [41] 1985 CST <ref> [13] </ref> fine/medium not implemented 1986 Cantor [4] fine Cantor [4] 1986 Cosmic C [33] medium Reactive Kernel [34] 1989 Reactive C [40] fine/medium Reactive Kernel [34] 1992 Affinity [37] medium Affinity Kernel [37] 1993 C + [35] medium C + [35] fine MADRE Table 1.2: Evolution of Multicomputer Software Systems. processes <p> In the earlier process model, a process could specify which message to receive next. A purely reactive process cannot receive incoming messages selectively; it must consume the message at the front of its message queue. CST The programming notation CST (Concurrent Smalltalk) by Dally (1985) <ref> [13] </ref> was an early reactive-process programming language. At the time of its development, CST was primarily a reference language. The base language Smalltalk complicated the implementation of parallel runtime support, so CST was supported only on sequential computers. CST now follows its own evolutionary track [12]. <p> This situation is analogous to the problem of deadlock-free routing on cyclic message networks as studied by Dally and Seitz [14]. Dally and Seitz show that communication deadlock can occur if and only if there are cycles in the channel dependency graph. In that work and later in <ref> [13] </ref>, the authors resolve cyclic channel dependencies by introducing virtual channels between nodes. Following the example of Dally and Seitz, we replace each communication channel with high and low virtual channels (Figure 5.11). <p> In that case, the message-exportation message can be redirected immediately to the DISK, or an error condition reported. Implementing Virtual Channels Now it remains to "implement" the virtual channels along which user and system messages travel. In the Torus Routing Chip <ref> [13] </ref>, Dally implements two virtual channels by multiplexing on a single physical channel. The arbitration between the channels is strongly fair for each byte of the outgoing messages.
Reference: [14] <author> William J. Dally and Charles L. Seitz. </author> <title> Deadlock-Free Message Routing in Multiprocessor Interconnection Networks. </title> <institution> Computer Science Department 5231:TR:86, California Institute of Technology, </institution> <year> 1986. </year> <month> 134 </month>
Reference-contexts: To receive the incoming user message, each node must export a message by newing an exported-message remote process. Communication deadlock can occur if and only if there are cycles in the channel dependency graph. (proof in <ref> [14] </ref>). The crux of this solution is the "put" mechanism that allows processes to be appended to the distributed queue of processes. <p> If each of the nodes must export some message in order to continue to receive incoming messages, deadlock can occur. This situation is analogous to the problem of deadlock-free routing on cyclic message networks as studied by Dally and Seitz <ref> [14] </ref>. Dally and Seitz show that communication deadlock can occur if and only if there are cycles in the channel dependency graph. In that work and later in [13], the authors resolve cyclic channel dependencies by introducing virtual channels between nodes. <p> A sending node does not free message space or send a new user message until the ack to the previous message has been received. If a nack is received, the sender retransmits the current outgoing user message. The requirement that each virtual channel have its own outgoing queue <ref> [14, page 13] </ref> is achieved in the MADRE system by allocating memory for the user-message send queue in the SNDQ partition and for the outgoing system message in the SYS partition. This approach was chosen for implementation primarily to investigate the feasibility of software-level experimentation with message-passing protocols.
Reference: [15] <author> E.W. Dijkstra and C.S. Scholten. </author> <title> Termination detection for diffusing computations. </title> <type> Technical Report EWD687, </type> <institution> Phillips Research Laboratories, </institution> <address> 1978. Eindhoven, The Netherlands. </address>
Reference-contexts: In the MADRE system, this handler is used to detect acquiescence both of the loading phase and, later, of the user computation. Program 5.11 outlines the definition of this handler. The distributed algorithm executed by the termination handlers on each node is an extension of Dijkstra's algorithm <ref> [15] </ref> for detecting termination. In [39], Taylor presents the distributed algorithm and a proof of its correctness. A similar termination detection algorithm is used in the Cantor runtime system. Each node maintains a "color" variable depending on whether it is busy (BLACK) or idle (WHITE).
Reference: [16] <author> D.J. Kuck et al. </author> <title> Dependence graphs and compiler optimizations. </title> <booktitle> In Proceedings of the 8th ACM Symposium on the Principles of Programming Languages, </booktitle> <month> Jan. </month> <year> 1981. </year>
Reference-contexts: If the application is to be run on a computer with different characteristics, such as a medium-grain machine, the concurrent elements can be aggregated to "build up the granularity." Contrast aggregation of concurrency with the difficulty of extracting parallelism from programs <ref> [16] </ref>.
Reference: [17] <author> C.M. Flaig. </author> <title> VLSI Mesh Routing Systems. </title> <institution> Computer Science Department 5241:TR:87, Cali-fornia Institute of Technology, </institution> <year> 1988. </year> <type> Master's thesis. </type>
Reference-contexts: In future implementations, established runtime algorithms could be moved to the ROM. 2.1.4 Router The unconventional components of the runtime system are those concerned with message passing, the router and the packet interface. The router design in the Mosaic is an oblivious, dimension-order router as described in <ref> [17] </ref>. The router on each node has eight channel connections to its neighbors (bidirectional in each compass direction).
Reference: [18] <author> A. Goscinski. </author> <title> Distributed Operating Systems: The Logical Design. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, MA, </address> <year> 1991. </year>
Reference-contexts: A fine-grain runtime system thus is composed of components running on each node that communicate and cooperate to manage their collective resources (Figure 4.1). This definition of a fine-grain runtime system classifies it as a distributed runtime system. Traditionally, a distributed operating system <ref> [18] </ref> is an operating system in which global-resource management is performed, using only local state information, in a distributed network of computers. A distributed operating system provides the abstraction of a virtual uniprocessor to the user.
Reference: [19] <institution> Proceedings of the Third Conference on Hypercube Concurrent Computers and Applications, </institution> <address> New York, 1988. </address> <publisher> ACM Press. </publisher>
Reference-contexts: In compiled code, intermediate values are often stored in registers because register allocation and access is significantly less expensive than a main-memory access. The existing literature concerning medium-grain multicomputer programming (eg, <ref> [19] </ref>) contains many programmer observations about the techniques used to achieve high performance. For example, since memory is plentiful on a medium-grain node and message passing is relatively expensive, programmers often build and maintain copies of data structures such as look-up tables in every process requiring access.
Reference: [20] <author> Arjun Khanna. </author> <title> On managing classes in a distributed object-oriented operating system. </title> <booktitle> In Proceedings of the Fifth Distributed Memory Computing Conference, </booktitle> <year> 1990. </year>
Reference-contexts: The LRU code-management strategy is essentially conventional code caching. The concept of having some nodes maintain permanent copies code pieces while other nodes request temporary copies is reminiscent of the librarian entities in <ref> [20] </ref>. This approach has been used on medium-grain machines, and will be evaluated for fine-grain machines in Chapter 6. Remote Code Execution The confluence of message, process, and code is required for process execution. The LRU code-management strategy brings the code to the message and the process.
Reference: [21] <author> Donald E. Knuth. </author> <title> Fundamental Algorithms, </title> <booktitle> volume 1 of The Art of Computer Programming. </booktitle> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, MA, </address> <note> second edition, </note> <year> 1973. </year>
Reference-contexts: The algorithm used to manage the memory in the SNDQ, RCVQ and PROC partitions is the buddy-system algorithm described in <ref> [21] </ref>. The available node memory in each partition is divided into blocks according to a sequence of Fibonacci numbers (Figure 5.5).
Reference: [22] <author> Charles R. Lang, Jr. </author> <title> The Extension of Object-Oriented Languages to a Homogeneous, Concurrent Architecture. </title> <type> Ph.D. thesis, </type> <institution> California Institute of Technology, </institution> <year> 1982. </year>
Reference-contexts: A better solution to the unbounded queue problem would use distributed processes to implement the queue of values. 1.2.2 A Solution Using Distributed Processes The question of how to use distributed processes to implement an unbounded queue has been studied for roughly a decade <ref> [22, 4, 8] </ref>. Most of the problem "solutions" were either restricted to simplified forms of the unbounded queue problem or were quite wasteful of space and/or work. The development of an acceptable solution within our computational framework was a key starting point for the research in this thesis. <p> However, because a selective receive is such a convenient programming mechanism, our software systems included it as a primitive operation. In <ref> [22] </ref>, Lang added a SELECT construct to Simula so that the programmer could control the order of messages to be processed. In the original Cosmic Kernel system [29], programmers could embed type information in outgoing messages and specify the desired type of messages to be received. <p> By mapping 2 Some parts of the description of the evolution of multicomputer programming notations and their implementations were based on discussions with my advisor, C.L. Seitz, who was involved in the development of most of these programming systems. 18 Year Notation Target RTS Multicomputers Implementation 1982 Simula <ref> [22] </ref> fine/medium not implemented 1985 Cosmic Cube C [41] medium Cosmic Kernel [41] 1985 CST [13] fine/medium not implemented 1986 Cantor [4] fine Cantor [4] 1986 Cosmic C [33] medium Reactive Kernel [34] 1989 Reactive C [40] fine/medium Reactive Kernel [34] 1992 Affinity [37] medium Affinity Kernel [37] 1993 C + <p> If the nodes support multiprogramming (ie, multiple processes per node), a degree of machine independence can be achieved. Simula An extension of the early object-oriented language Simula (1982) <ref> [22] </ref> was one of the first multicomputer programming notations. By extending Simula to include representations for message passing and concurrent processes, Lang demonstrated that programmers can expose at the language level the available concurrency in an application. However, the complexity of the base language, Simula, precluded efficient parallel implementation.
Reference: [23] <author> S.L. Lillevik. </author> <title> DELTA:a 30 Gigaflop Parallel Supercomputer for Touchstone. </title> <booktitle> In NORTHCON Conference Proceedings, </booktitle> <pages> pages 294-304, </pages> <year> 1990. </year>
Reference-contexts: Scaling Tracks The design choices concerning how to apply additional technology resources have defined two scaling tracks of multicomputer architecture. Table 1.1 details the characteristics of some implementations of the multicomputer architecture that have been constructed during the past decade <ref> [29, 26, 2, 32, 23] </ref>. The increases in processor speed and the network channel throughput are consequences of organizational improvements in addition to advances in technology [31, page 157]. <p> The emphasis on message-passing performance can be observed by comparing the network capabilities of the Mosaic and the Intel Delta <ref> [23] </ref>. The communication channels for both machines are essentially identical except for their throughput rates: Mosaic channels run at 60 MB/s, Delta channels at 80 MB/s.
Reference: [24] <author> Johan J. Lukkien and Jan L.A. van de Snepscheut. </author> <title> A Tutorial Introduction to Mosaic Pascal. </title> <institution> Computer Science Department Caltech-CS-TR-91-02, California Institute of Technology, </institution> <year> 1991. </year>
Reference-contexts: Each of these compilers was developed by porting the corresponding Gnu compiler to the Mosaic. A complete Pascal programming system, including compiler and runtime system, has been developed by van de Snepscheut and his students <ref> [24] </ref>. 2.5.2 Debugger An assembly-level debugger is available for use with Mosaic Development Boards.
Reference: [25] <author> John Y. Ngai. </author> <title> A Framework for Adaptive Routing in Multicomputer Networks. </title> <institution> Computer science department, California Institute of Technology, </institution> <year> 1989. </year>
Reference-contexts: When a node can no longer receive messages, additional messages remain queued in the message network, blocking communication channels and potentially causing deadlock in the network. Reactive programs consume messages in the order received hence there are no messages needing to be buffered indefinitely. This "consumption assumption" <ref> [25] </ref> ensures that, even though queued messages may temporarily block communication channels in the network, all messages will eventually be received and consumed by the node, thus avoiding communication deadlock. <p> Thus, a fine-grain runtime system is both a distributed runtime system in that it manages distributed resources using local state information, and it is a distributed fine-grain program. 4.1.2 Robust Operation As described in <ref> [25] </ref>, the assumption that incoming messages will eventually be consumed by the node is fundamental to ensuring that the message network will not deadlock.
Reference: [26] <author> Douglas M. Pase and Allan R. Larrabee. </author> <title> Intel iPSC Concurrent Computer. </title> <editor> In Robert G. Babb II, editor, </editor> <booktitle> Programming Parallel Processors. </booktitle> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, MA, </address> <year> 1988. </year>
Reference-contexts: Scaling Tracks The design choices concerning how to apply additional technology resources have defined two scaling tracks of multicomputer architecture. Table 1.1 details the characteristics of some implementations of the multicomputer architecture that have been constructed during the past decade <ref> [29, 26, 2, 32, 23] </ref>. The increases in processor speed and the network channel throughput are consequences of organizational improvements in addition to advances in technology [31, page 157].
Reference: [27] <author> P. Pierce. </author> <title> The NX/2 Operating System. </title> <booktitle> In Proceedings of the Third Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 384-390, </pages> <address> New York, 1988. </address> <publisher> ACM Press. </publisher>
Reference-contexts: As the entire state of the process must be saved, context switches are quite expensive. The runtime system also uses timers to ensure fairness in process execution, swapping out long-running processes. Intel's NX node operating system (1987) <ref> [27] </ref> was (and is) based on this runtime model. The next generation of runtime systems adopted the reactive property as its primary scheduling mechanism and provided support for reactive processes.
Reference: [28] <author> Charles L. Seitz. </author> <title> Mosaic C: An experimental fine-grain multicomputer. </title> <booktitle> In Proceedings of the Twenty-Fifth Conference INRIA Conference, </booktitle> <address> New York, 1992. </address> <publisher> Springer-Verlag Press. </publisher>
Reference-contexts: The host-interface configuration currently used for Mosaic ensembles includes 2 memoryless-Mosaic nodes mounted on a Sun SPARCstation Sbus board. Each node includes 64Kfi16 external memory. The Sbus board is connected to the Mosaic ensemble using cables and custom "slack" chips that execute a zero-slack (non-interference) protocol <ref> [28] </ref>. 2.5 Programming Toolkit 2.5.1 Compilers The low-level programming toolkit for the Mosaic C includes a C compiler and a C ++ compiler. Each of these compilers was developed by porting the corresponding Gnu compiler to the Mosaic.
Reference: [29] <author> C.L. Seitz. </author> <title> The Cosmic Cube. </title> <journal> Communications of the ACM, </journal> <volume> 28(1) </volume> <pages> 22-33, </pages> <year> 1985. </year>
Reference-contexts: However, because a selective receive is such a convenient programming mechanism, our software systems included it as a primitive operation. In [22], Lang added a SELECT construct to Simula so that the programmer could control the order of messages to be processed. In the original Cosmic Kernel system <ref> [29] </ref>, programmers could embed type information in outgoing messages and specify the desired type of messages to be received. The Reactive Kernel system [34] provides message discretion via library functions that are layered on a kernel that handles messages reactively. <p> Scaling Tracks The design choices concerning how to apply additional technology resources have defined two scaling tracks of multicomputer architecture. Table 1.1 details the characteristics of some implementations of the multicomputer architecture that have been constructed during the past decade <ref> [29, 26, 2, 32, 23] </ref>. The increases in processor speed and the network channel throughput are consequences of organizational improvements in addition to advances in technology [31, page 157]. <p> Runtime support on parallel machines for the early programming systems such as Cosmic Cube C [41] was essentially conventional. The Cosmic Kernel <ref> [29] </ref> was widely distributed as the operating system running on individual nodes of the first-generation multicomputers (Cosmic Cube, iPSC/1, NCUBE). In this system, a process is represented by a code segment and a data segment. The runtime system performs address translation to support a private address space for each process. <p> The VLSI layout of the Mosaic node is shown in Figure 2.1; Figure 2.2 identifies the major components in the layout. This layout was developed entirely within our research group [35]. Implementing nodes as single-chip computers has been a goal throughout the multi-computer evolution. In <ref> [29] </ref> (1985), Seitz says "The Cosmic Cube nodes were designed as a hardware simulation of what we expect to be able to integrate onto one or two chips in about five years".
Reference: [30] <author> C.L. Seitz. </author> <title> Concurrent architectures. In VLSI and Parallel Computation. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, California, </address> <year> 1990. </year>
Reference-contexts: Balancing the processing load across the nodes of the ensemble is also facilitated by splitting the computation into many pieces that execute for a short period of time <ref> [30, page 20] </ref>. 3.2.3 Reactive Behavior As discussed in section 1.3.3, the reactive behavior of processes is a characteristic that is ideally suited to fine-grain machines.
Reference: [31] <author> C.L. Seitz. </author> <title> Multicomputers. In C.A.R. Hoare, editor, Developments in Concurrency and Communication. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, MA, </address> <year> 1990. </year> <month> 135 </month>
Reference-contexts: These groundrules outline a model of computation that has developed along with the evolution of the multicomputer architecture. The factors that motivate this model are discussed in the literature <ref> [31, 40] </ref> and in subsequent sections of this chapter. 1.2 The Unbounded Queue Problem 1.2.1 A Single-Process Implementation 2 class element f int value; elementfl next; public: element (int V) f value = V; next = NIL;g void set next (element flN) f next = N;g int get value () f <p> Each of the nodes executes its own program and is composed of a processor and private memory. full analysis of this argument, see <ref> [31, pages 142-147] </ref>.) As technology advances, a fixed silicon budget purchases more machine resources. The peak performance of a concurrent machine is the product of N , the number of nodes, and the operations per second on each node. <p> Table 1.1 details the characteristics of some implementations of the multicomputer architecture that have been constructed during the past decade [29, 26, 2, 32, 23]. The increases in processor speed and the network channel throughput are consequences of organizational improvements in addition to advances in technology <ref> [31, page 157] </ref>. However, the increase in the amount of memory per node for the so-called medium-grain multicomputers has consumed much of the additional technology resources made available as multicomputers evolved. <p> The increases in processor speed and the network channel throughput are consequences of organizational improvements in addition to advances in technology <ref> [31, page 157] </ref>. The increase in the amount of memory per node has consumed much of the additional technology resources made available as multicomputers evolved. The Mosaic C clearly follows a different scaling track from the medium-grain multicomputer architecture implementations. can be constructed within a fixed budget. <p> Fine-Grain Multicomputers The other scaling track in Figure 1.6 reflects architectures in which new hardware resources are devoted to obtaining additional concurrency rather than additional node performance or memory <ref> [31, page 146] </ref>. From an initial multicomputer configuration such as the Cosmic Cube in Figure 1.6, if the number of nodes is increased while the amount of memory per node is held constant, the resulting configuration is a fine-grain multicomputer. <p> In addition, since a reactive process executes (by definition) for a bounded time, no timing mechanisms are needed to ensure fairness in process execution. Runtime support for fine-grain computations can be simplified so that process scheduling is analogous to invoking a function. <ref> [31, page 160] </ref>. However, the physical realities of a fine-grain node do have significant repercussions for process placement and other runtime resource-management issues. In general, a mul-ticomputer node maintains an internal queue for incoming messages so that messages can be removed from the network as quickly as possible. <p> Thus, the latency of interprocess communication (ie, message passing) can be at least partially hidden using multiprogramming <ref> [31, page 195] </ref>. In addition, since message passing typically transfers blocks of data at once, the cost per data item is small. However, the latency of intra-process communication (ie, processor-to-memory communication) cannot be hidden or amortized. <p> In addition, the message load on a fine-grain message network can be heavier, simply because more nodes can send messages concurrently. A fine-grain computation is more sensitive to message latency since, due to the increased concurrency of message consumption, messages stay enqueued at a node for less time <ref> [31, page 195] </ref>. Using efficient network hardware and low software overhead for message passing (section 2.1.5), the message system of the Mosaic exhibits latencies more than one order of magnitude less than medium-grain systems. <p> Implementation Embedded Processes The questions of process dispatch and data communication have been addressed previously in multicomputer runtime support systems. In <ref> [31, page 161] </ref>, Seitz describes a dispatch mechanism, used at the runtime-system level, that assumes user processes are implemented as embedded processes (Program 4.1). <p> In this case, a process invokes an embedded process by dereferencing the function pointer. The arguments passed to the embedded process are the pointer to its data and a pointer to the message to be consumed. In the systems described in <ref> [31] </ref>, there is however no unified mechanism for providing specifying which data and services should be provided to individual processes. <p> An embedded process is invoked by dereferencing the function pointer; the arguments passed are the pointer to its data and a pointer to the message to be consumed. (Adapted from <ref> [31] </ref>) C + The development of C + provides an opportunity to support process layering directly in user programs. The inclusion of explicit process layering was motivated by the implementation of MADRE, the Mosaic runtime system, in C + .
Reference: [32] <author> C.L. Seitz, W.C. Athas, C.M. Flaig, A.J. Martin, J. Seizovic, C.S. Steele, and W.-K. Su. </author> <booktitle> The Architecture and Programming of the Ametek Series 2010 Multicomputer. In Proceedings of the Third Conference on Hypercube Concurrent Computers and Applications, </booktitle> <address> New York, 1988. </address> <publisher> ACM Press. </publisher>
Reference-contexts: Scaling Tracks The design choices concerning how to apply additional technology resources have defined two scaling tracks of multicomputer architecture. Table 1.1 details the characteristics of some implementations of the multicomputer architecture that have been constructed during the past decade <ref> [29, 26, 2, 32, 23] </ref>. The increases in processor speed and the network channel throughput are consequences of organizational improvements in addition to advances in technology [31, page 157]. <p> Use of this table decreases the time required for each message send, at the cost of dedicating enough memory to represent the entire ensemble. For a Symult S2010 ensemble <ref> [34, 32] </ref>, this table requires a few hundred words. For a Mosaic ensemble, such a table could require 16K words, or half the memory on the node. In addition to avoiding large system data structures, the resident runtime system on each node must be quite small, ideally a few kilobytes.
Reference: [33] <author> C.L. Seitz, J. Seizovic, and W.-K. Su. </author> <title> The C Programmer's Abbreviated Guide to Multicom-puter Programming. </title> <institution> Computer Science Department Caltech-CS-TR-88-1, California Institute of Technology, </institution> <year> 1988. </year> <note> (revised 1989). </note>
Reference-contexts: Seitz, who was involved in the development of most of these programming systems. 18 Year Notation Target RTS Multicomputers Implementation 1982 Simula [22] fine/medium not implemented 1985 Cosmic Cube C [41] medium Cosmic Kernel [41] 1985 CST [13] fine/medium not implemented 1986 Cantor [4] fine Cantor [4] 1986 Cosmic C <ref> [33] </ref> medium Reactive Kernel [34] 1989 Reactive C [40] fine/medium Reactive Kernel [34] 1992 Affinity [37] medium Affinity Kernel [37] 1993 C + [35] medium C + [35] fine MADRE Table 1.2: Evolution of Multicomputer Software Systems. processes to nodes, concurrency can be exploited. <p> Since the definitions are compiled together, an analog of interprocedural analysis can be used to perform significant error checking on the messages being passed, and on the processes being created, thus decreasing the required runtime support. Cosmic C The programming notation Cosmic C (1986) <ref> [33] </ref>, the language C extended with reactive primitives for message-passing and process creation, is semantically equivalent to Cantor with respect to reactive processes and messages. However, like the earlier C-based notation, process programs are compiled separately, and machine resources are named directly. <p> The host program running on the workstation interacts with the sending and receiving interface nodes using a set of message-passing primitives patterned after the x primitives used in earlier programming systems such as Cosmic C <ref> [33] </ref>. The Mosaic primitives are called m primitives. * void *mmalloc (int size) This function returns a message buffer of size size. <p> In earlier multicomputer programming systems such as Cosmic C <ref> [33] </ref>, sending these data structures in messages to other processes required that the programmer explicitly "flatten" the structure at runtime. When the message arrives to its destination, that process interprets the "flattened" structure in the message.
Reference: [34] <author> Jakov Seizovic. </author> <title> The Reactive Kernel. </title> <institution> Computer Science Department Caltech-CS-TR-88-10, California Institute of Technology, </institution> <year> 1988. </year> <type> Master's thesis. </type>
Reference-contexts: In the original Cosmic Kernel system [29], programmers could embed type information in outgoing messages and specify the desired type of messages to be received. The Reactive Kernel system <ref> [34] </ref> provides message discretion via library functions that are layered on a kernel that handles messages reactively. In [8], we proposed that a remote function construct be added to the reactive programming language Cantor [4] (section 1.3.2) to provide the programmer with a selective receive. <p> in the development of most of these programming systems. 18 Year Notation Target RTS Multicomputers Implementation 1982 Simula [22] fine/medium not implemented 1985 Cosmic Cube C [41] medium Cosmic Kernel [41] 1985 CST [13] fine/medium not implemented 1986 Cantor [4] fine Cantor [4] 1986 Cosmic C [33] medium Reactive Kernel <ref> [34] </ref> 1989 Reactive C [40] fine/medium Reactive Kernel [34] 1992 Affinity [37] medium Affinity Kernel [37] 1993 C + [35] medium C + [35] fine MADRE Table 1.2: Evolution of Multicomputer Software Systems. processes to nodes, concurrency can be exploited. <p> systems. 18 Year Notation Target RTS Multicomputers Implementation 1982 Simula [22] fine/medium not implemented 1985 Cosmic Cube C [41] medium Cosmic Kernel [41] 1985 CST [13] fine/medium not implemented 1986 Cantor [4] fine Cantor [4] 1986 Cosmic C [33] medium Reactive Kernel <ref> [34] </ref> 1989 Reactive C [40] fine/medium Reactive Kernel [34] 1992 Affinity [37] medium Affinity Kernel [37] 1993 C + [35] medium C + [35] fine MADRE Table 1.2: Evolution of Multicomputer Software Systems. processes to nodes, concurrency can be exploited. If the nodes support multiprogramming (ie, multiple processes per node), a degree of machine independence can be achieved. <p> Intel's NX node operating system (1987) [27] was (and is) based on this runtime model. The next generation of runtime systems adopted the reactive property as its primary scheduling mechanism and provided support for reactive processes. The Reactive Kernel (1988) <ref> [34] </ref> is a widely distributed node runtime system that is based on reactive processes. (The Cosmic Environment [40] is the companion host runtime system.) Processes are represented by a code segment and a data segment, but process scheduling is dictated by the queue of messages entering the node. <p> A medium-grain operating system is typically composed of component operating systems that run on each node (eg, <ref> [34] </ref>). These component operating systems manage the local resources of the node | performing memory allocation, process scheduling, etc. Global management of the processors and memory, via process placement and load-balancing techniques, is left to the programmer. <p> Space Efficiency The efficiency of the runtime system must also be evaluated on a "space" level. Some medium-grain operating systems maintain data structures that contain state information about other nodes in the ensemble. For example, in the Reactive Kernel <ref> [34] </ref>, each node keeps a table containing the relative coordinates of each node in ensemble. Use of this table decreases the time required for each message send, at the cost of dedicating enough memory to represent the entire ensemble. <p> Use of this table decreases the time required for each message send, at the cost of dedicating enough memory to represent the entire ensemble. For a Symult S2010 ensemble <ref> [34, 32] </ref>, this table requires a few hundred words. For a Mosaic ensemble, such a table could require 16K words, or half the memory on the node. In addition to avoiding large system data structures, the resident runtime system on each node must be quite small, ideally a few kilobytes. <p> This message passing can be eliminated by having the kernel processes directly invoke the atomic functions of other kernel processes. Third, using C + , the set of kernel processes is instantiated when the component runtime-system program begins executing. In previous operating systems, such as the Reactive Kernel <ref> [34] </ref>, substantial effort was required to create a process (the spawn handler, in that case) that would be used to create additional processes. Kernel processes may be layered, so that kernel processes can inherit access to certain data and functions. <p> The CPM process consumes this queue by decoding the messages and dispatching to C + user processes. 65 interrupt by decoding a section of the message and then dispatching to one of the statically instantiated kernel processes. This low-level organization is similar to the organization of the Reactive Kernel <ref> [34] </ref>. The processes to which the Root process dispatches can be viewed as handlers. Handlers may in turn dispatch to other MADRE processes. User messages percolate up through layers of the system, and are eventually consumed by user processes. <p> In previous operating systems for multicomputers, the requirement that the code be resident with the process has been satisfied in one of two ways. First, the code for a process is made resident on a node when the process is created, eg, Reactive Kernel <ref> [34] </ref>. If a copy of the code does not already exist on the destination node, the code is copied from the host. This approach is not practical for fine-grain machines, since hundreds of nodes could simultaneously request code from the host, creating a significant communication bottleneck.
Reference: [35] <author> Jakov Seizovic. </author> <title> Mosaic and the C + Programming Notation. </title> <type> Ph.D. thesis, </type> <institution> California Institute of Technology, </institution> <year> 1993. </year>
Reference-contexts: fine/medium not implemented 1985 Cosmic Cube C [41] medium Cosmic Kernel [41] 1985 CST [13] fine/medium not implemented 1986 Cantor [4] fine Cantor [4] 1986 Cosmic C [33] medium Reactive Kernel [34] 1989 Reactive C [40] fine/medium Reactive Kernel [34] 1992 Affinity [37] medium Affinity Kernel [37] 1993 C + <ref> [35] </ref> medium C + [35] fine MADRE Table 1.2: Evolution of Multicomputer Software Systems. processes to nodes, concurrency can be exploited. If the nodes support multiprogramming (ie, multiple processes per node), a degree of machine independence can be achieved. <p> Cosmic Cube C [41] medium Cosmic Kernel [41] 1985 CST [13] fine/medium not implemented 1986 Cantor [4] fine Cantor [4] 1986 Cosmic C [33] medium Reactive Kernel [34] 1989 Reactive C [40] fine/medium Reactive Kernel [34] 1992 Affinity [37] medium Affinity Kernel [37] 1993 C + <ref> [35] </ref> medium C + [35] fine MADRE Table 1.2: Evolution of Multicomputer Software Systems. processes to nodes, concurrency can be exploited. If the nodes support multiprogramming (ie, multiple processes per node), a degree of machine independence can be achieved. <p> However, since the complexity of the implementation of the Cantor system does not facilitate experimentation, Seizovic has developed a programming notation derived from C ++ called C + (1992) <ref> [35] </ref>. Basing a notation on an established language provides a suite of programming tools that can be tailored for experimentation. With its constructs for member functions and classes, C + provides much of the abstraction necessary to express reactive semantics while also providing significant error checking at compile-time. <p> Each of these components is discussed further in following subsections. The VLSI layout of the Mosaic node is shown in Figure 2.1; Figure 2.2 identifies the major components in the layout. This layout was developed entirely within our research group <ref> [35] </ref>. Implementing nodes as single-chip computers has been a goal throughout the multi-computer evolution. In [29] (1985), Seitz says "The Cosmic Cube nodes were designed as a hardware simulation of what we expect to be able to integrate onto one or two chips in about five years". <p> We then present a general description of the fine-grain programming methods that have evolved in conjunction with the multicomputer architecture. 3.1 C + C + is a C ++ -based programming notation developed by Jakov Seizovic that will be described in detail in his upcoming Ph.D. thesis <ref> [35] </ref>. C + is a translation-based system | programs are written in C + and then translated into C ++ . The translated program is then compiled using conventional C ++ software tools. <p> By using the member-protection capabilities of C ++ and C + , dynamic process inheritance can provide controlled access to data and services in other process layers. Currently, work is proceeding on integrating mechanisms for process inheritance, process dispatch and for associating the structure of messages with process layers <ref> [35] </ref>. The structure of messages should reflect the structure of process derivation. A message sent to a derived process type includes segments that are interpreted by base processes. In C + , each process type must include, or inherit from a base process, a HEAD and a TAIL function. <p> Future versions may generate individual code pieces for each non-atomic functions. (For a full description of this code splitting, see <ref> [35] </ref>.) Partitioning the code for a process type by atomic function decreases the granularity of elements that the runtime system must manage, leading to more flexibility in runtime-system algorithms. Since the entire C + program is linked together, each code piece can be assigned a unique index.
Reference: [36] <author> Don Speck. </author> <title> Mosaic RAM Design. </title> <editor> In William J. Dally, editor, </editor> <booktitle> Advanced Research in VLSI. </booktitle> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: In addition to being fast, the Mosaic memory must also be dense, given that all the memory is resident on the chip. The Mosaic RAM cells were designed to achieve both goals <ref> [36] </ref>. The RAM is composed of 1-transistor cells, with a total of 64 KB of memory. The access time is 2B per cycle (every 33 ns running at 30 MHz).
Reference: [37] <author> Craig S. Steele. </author> <title> Affinity, A Concurrent Programming System for Multicomputers. </title> <type> Ph.D. thesis, </type> <institution> California Institute of Technology, </institution> <year> 1992. </year>
Reference-contexts: Notation Target RTS Multicomputers Implementation 1982 Simula [22] fine/medium not implemented 1985 Cosmic Cube C [41] medium Cosmic Kernel [41] 1985 CST [13] fine/medium not implemented 1986 Cantor [4] fine Cantor [4] 1986 Cosmic C [33] medium Reactive Kernel [34] 1989 Reactive C [40] fine/medium Reactive Kernel [34] 1992 Affinity <ref> [37] </ref> medium Affinity Kernel [37] 1993 C + [35] medium C + [35] fine MADRE Table 1.2: Evolution of Multicomputer Software Systems. processes to nodes, concurrency can be exploited. If the nodes support multiprogramming (ie, multiple processes per node), a degree of machine independence can be achieved. <p> Implementation 1982 Simula [22] fine/medium not implemented 1985 Cosmic Cube C [41] medium Cosmic Kernel [41] 1985 CST [13] fine/medium not implemented 1986 Cantor [4] fine Cantor [4] 1986 Cosmic C [33] medium Reactive Kernel [34] 1989 Reactive C [40] fine/medium Reactive Kernel [34] 1992 Affinity <ref> [37] </ref> medium Affinity Kernel [37] 1993 C + [35] medium C + [35] fine MADRE Table 1.2: Evolution of Multicomputer Software Systems. processes to nodes, concurrency can be exploited. If the nodes support multiprogramming (ie, multiple processes per node), a degree of machine independence can be achieved. <p> The C + notation is briefly described in Chapter 3 and several example application programs are presented and discussed in Chapter 6. Affinity The Affinity programming notation and runtime system were recently developed by Steele (1992) <ref> [37] </ref>. Using Affinity, programmers partition the computation into pieces of code and data. Light-weight, reactive processes called actions are executed to provide atomicity and maintain the consistency of data modifications. <p> Cantor aggressively exploits compiler technology to minimize runtime support. In addition, this system automatically manages system resources, including placing processes. A series of simulations [4], conducted using a simplified machine model, demonstrate that a runtime system can do a reasonable job of automatic resource management. The Affinity Kernel (1992) <ref> [37] </ref> is a node runtime system that supports the execution of reactive processes that share data structures. The MADRE (MosAic Distributed RuntimE) system has been developed as the runtime system for the Mosaic C.
Reference: [38] <author> Bjarne Stroustrup. </author> <title> The C++ Programming Language. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, MA, </address> <note> second edition, </note> <year> 1991. </year>
Reference-contexts: As most of the programs in this thesis are written in C + , the reader should already be familiar with C ++ (or study a good C ++ programming reference such as <ref> [38] </ref>). The primary differences between C + and C ++ are listed below. * C + does not permit global variables. * C + adds a process abstraction (indicated by the keyword processdef) that is an extension of the C ++ class abstraction. <p> These data types can include member variables, member functions and constructors and destructors <ref> [38] </ref>. Unlike C ++ C + is based on a concurrent model of execution. A mechanism for process abstraction is fundamental for expressing concurrent computations. In C + , process types can be defined using the keyword processdef.
Reference: [39] <author> Stephen Taylor. </author> <title> Parallel Logic Programming Techniques. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1989. </year>
Reference-contexts: Program 5.11 outlines the definition of this handler. The distributed algorithm executed by the termination handlers on each node is an extension of Dijkstra's algorithm [15] for detecting termination. In <ref> [39] </ref>, Taylor presents the distributed algorithm and a proof of its correctness. A similar termination detection algorithm is used in the Cantor runtime system. Each node maintains a "color" variable depending on whether it is busy (BLACK) or idle (WHITE).
Reference: [40] <author> Wen-king Su. </author> <title> Reactive-Process Programming and Distributed Discrete-Event Simulation. </title> <type> Ph.D. thesis, </type> <institution> California Institute of Technology, </institution> <year> 1989. </year>
Reference-contexts: No hidden facilities for buffering unwanted messages are provided; each message must be processed in the order received. This reactive handling of messages <ref> [40, 4] </ref> is a significant departure from traditional protocols used for process interaction. Selective-receive capabilities, the ability to receive messages based on their contents and/or the state of the process, must be implemented using purely reactive semantics. <p> These groundrules outline a model of computation that has developed along with the evolution of the multicomputer architecture. The factors that motivate this model are discussed in the literature <ref> [31, 40] </ref> and in subsequent sections of this chapter. 1.2 The Unbounded Queue Problem 1.2.1 A Single-Process Implementation 2 class element f int value; elementfl next; public: element (int V) f value = V; next = NIL;g void set next (element flN) f next = N;g int get value () f <p> most of these programming systems. 18 Year Notation Target RTS Multicomputers Implementation 1982 Simula [22] fine/medium not implemented 1985 Cosmic Cube C [41] medium Cosmic Kernel [41] 1985 CST [13] fine/medium not implemented 1986 Cantor [4] fine Cantor [4] 1986 Cosmic C [33] medium Reactive Kernel [34] 1989 Reactive C <ref> [40] </ref> fine/medium Reactive Kernel [34] 1992 Affinity [37] medium Affinity Kernel [37] 1993 C + [35] medium C + [35] fine MADRE Table 1.2: Evolution of Multicomputer Software Systems. processes to nodes, concurrency can be exploited. <p> However, like the earlier C-based notation, process programs are compiled separately, and machine resources are named directly. Cosmic C is currently used as an efficient, reactive-process programming language for most of the multicomputers in Table 1.1. Reactive C Another C-based language, Reactive C (1989) <ref> [40] </ref>, was developed by Wen-king Su to experiment with processes executing within a single address space. Since the runtime system does not have to support multiple address spaces and address translation, Reactive C is more efficient than the earlier C-based notations. <p> The next generation of runtime systems adopted the reactive property as its primary scheduling mechanism and provided support for reactive processes. The Reactive Kernel (1988) [34] is a widely distributed node runtime system that is based on reactive processes. (The Cosmic Environment <ref> [40] </ref> is the companion host runtime system.) Processes are represented by a code segment and a data segment, but process scheduling is dictated by the queue of messages entering the node. <p> C + is a translation-based system | programs are written in C + and then translated into C ++ . The translated program is then compiled using conventional C ++ software tools. C + is currently supported on computer systems that run the Cosmic Environment <ref> [40] </ref>, which includes multicomputers and networks of workstations. C + programs can also be executed on the Mosaic C. Section 3.1.4 briefly presents the interface between C + and the Mosaic runtime system. Chapter 5 details the Mosaic runtime system, which itself is written in C + . <p> The prototype Mosaic runtime system includes a C + program that executes on the host to provide services such as error reporting and I/O. 42 In systems such as the Cosmic Environment <ref> [40] </ref>, programmers are required to include code that invokes a special function that translates the contents of the message each time that message travels to or from the host. C + provides an abstraction mechanism that allows the programmer to declare the the characteristics of the target ensemble at compile-time.

References-found: 40

