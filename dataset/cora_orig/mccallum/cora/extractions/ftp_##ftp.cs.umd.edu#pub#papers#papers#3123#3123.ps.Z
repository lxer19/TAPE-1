URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3123/3123.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Title: Scalable Data Parallel Algorithms for Texture Synthesis and Compression using Gibbs Random Fields  
Author: David A. Bader Joseph JaJa Rama Chellappa 
Keyword: Gibbs Sampler, Gaussian Markov Random Fields, Image Processing, Texture Synthesis, Texture Compression, Scalable Parallel Processing, Data Parallel Algorithms.  
Note: Permission to publish this abstract separately is granted.  The support by NASA Graduate Student Researcher Fellowship No. NGT-50951 is gratefully acknowledged. Supported in part by NSF Engineering Research Center Program NSFD CDR 8803012 and NSF grant No. CCR-9103135. Also, affiliated with the Institute for Systems Research. Supported in part by Air Force grant No. F49620-92-J0130.  
Address: College Park, MD 20742  
Affiliation: Department of Electrical Engineering, and Institute for Advanced Computer Studies, University of Maryland,  
Email: dbader@eng.umd.edu  joseph@src.umd.edu  chella@eng.umd.edu  
Date: October 4, 1993  
Abstract: This paper introduces scalable data parallel algorithms for image processing. Focusing on Gibbs and Markov Random Field model representation for textures, we present parallel algorithms for texture synthesis, compression, and maximum likelihood parameter estimation, currently implemented on Thinking Machines CM-2 and CM-5. Use of fine-grained, data parallel processing techniques yields real-time algorithms for texture synthesis and compression that are substantially faster than the previously known sequential implementations. Although current implementations are on Connection Machines, the methodology presented here enables machine independent scalable algorithms for a number of problems in image processing and analysis. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. G. Akl. </author> <title> The Design and Analysis of Parallel Algorithms. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1989. </year>
Reference-contexts: The FFT is well-suited for parallel applications because it is efficient and inherently parallel ([20], <ref> [1] </ref>, [22], [23], [42]). With an image size of n elements, O (n log n) operations are needed for an FFT. On a parallel machine with p processors, O p log n computational steps are required. <p> We begin with , a Gaussian zero mean noise vector with identity covariance matrix. We generate its the Fourier series, via the Fast Fourier Transform from Subsection 2.4, using f , the Fourier vector defined below: f = Col <ref> [1; - ; 2 - t | ] </ref>, is an M 2 vector, (15) t | = Col [1; | ; 2 | ], is an M -vector, and (16) - = exp 1 and finally apply (12). <p> We generate its the Fourier series, via the Fast Fourier Transform from Subsection 2.4, using f , the Fourier vector defined below: f = Col [1; - ; 2 - t | ], is an M 2 vector, (15) t | = Col <ref> [1; | ; 2 | ] </ref>, is an M -vector, and (16) - = exp 1 and finally apply (12).
Reference: [2] <author> D. A. Bader, J. JaJa, and R. Chellappa. </author> <title> Scalable Data Parallel Algorithms for Texture Synthesis and Compression Using Gibbs Random Fields. </title> <institution> Technical Report CS-TR-3123 and UMIACS-TR-93-80, UMIACS and Electrical Engineering, University of Maryland, College Park, MD, </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: Similarly, CM-2 timings for these estimates can be found in <ref> [2] </ref>. Tables 8 - 15 are given in Appendix C. 21 5 Texture Compression We implement an algorithm for compressing an image of a GMRF texture to approximately 1 bit/pixel from the original 8 bits/pixel image.
Reference: [3] <author> J. E. Besag and P. A. P. Moran. </author> <title> On the Estimation and Testing of Spacial Interaction in Gaussian Lattice Processes. </title> <journal> Biometrika, </journal> <volume> 62 </volume> <pages> 555-562, </pages> <year> 1975. </year>
Reference-contexts: As described in <ref> [3] </ref> and [6], the log-likelihood function can be maximized: (Note that F (fi; -) = log p (yjfi; -)).
Reference: [4] <author> G. E. Blelloch. </author> <title> C ? data layout for the CM-2. </title> <type> Personal Communications, </type> <month> August 17, </month> <year> 1993. </year>
Reference-contexts: In this analysis, we view each Sprint node as a single 32-bit processing node. The CM-2 programming model of C ? uses a canonical data layout similar to that on the CM-5 described in Section 2.2.1 ([39], [38], [44], <ref> [4] </ref>). The only difference on the CM-2 and CM-5 is that the Sprint node, or major grid, portion of each data element address is in 8 reflected binary gray code, insuring that nearest neighbor communications are at most one hop away in the hypercube interconnection network.
Reference: [5] <author> G. E. Blelloch, C. E. Leiserson, B. M. Maggs, C. G. Plaxton, S. J. Smith, and M. </author> <note> Zagha. </note>
Reference-contexts: A scan operation on a sequential machine obviously takes O (n) operations. An efficient parallel algorithm uses a binary tree to compute the scan in O (log n) time with O (n) operations [20]. On the CM-2, the complexity for a scan is given by: <ref> [5] </ref> 8 : T comp [scan] (n; p) = O p ; T comm [scan] (n; p) = O ( n p fl): The CM-5 efficiently supports scans in hardware [29] and has complexity: 8 : T comp [scan] (n; p) = O p ; T comm [scan] (n; p) =
References-found: 5

