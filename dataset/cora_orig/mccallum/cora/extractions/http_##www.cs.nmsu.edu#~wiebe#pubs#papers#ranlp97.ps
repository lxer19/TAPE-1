URL: http://www.cs.nmsu.edu/~wiebe/pubs/papers/ranlp97.ps
Refering-URL: http://www.cs.nmsu.edu/~wiebe/pubs/index.html
Root-URL: http://www.cs.nmsu.edu
Email: wiebe,lduan@cs.nmsu.edu, rbruce@seas.smu.edu  
Title: Probabilistic Event Categorization  
Author: Janyce Wiebey and Rebecca Brucez and Lei Duany 
Date: 163-170.  
Note: Recent Advances in Natural Language Processing (RANLP-97), European Commission, DG XIII, Tzigov Chark, Bulgaria, September 1997, pp.  
Address: Las Cruces, NM 88003  Dallas, TX 75275-0112  
Affiliation: yDept. of Computer Science and the Computing Research Laboratory New Mexico State University  zDept. of Computer Science and Engineering Southern Methodist University  
Abstract: This paper describes the automation of a new text categorization task. The categories assigned in this task are more syntactically, semantically, and contextually complex than those typically assigned by fully automatic systems that process unseen test data. Our system for assigning these categories uses a probabilistic classifier, developed with a recent method for formulating a probabilistic model from a predefined set of potential features (Bruce 1995, Bruce and Wiebe 1994, Pedersen et al. 1996). This paper focuses on feature selection. It presents various types of properties experimented with in this work. We identify and evaluate various approaches to organizing the collocational properties into features. With the more complex features we define, there is an organization that yields the best results; but the same organization with less complex features yields inferior results. The results suggest a way to take advantage of properties that are low frequency but strongly indicative of a class. The problems of recognizing and organizing the various kinds of contextual information required to perform a linguistically complex categorization task has rarely been systematically investigated in NLP. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Barnden, J.A. </author> <year> (1992). </year> <title> Belief in metaphor: taking com-monsense psychology seriously. </title> <booktitle> Computational Intelligence 8 (3): </booktitle> <pages> 520-552. </pages>
Reference-contexts: The speech-event category is divided into subcategories based on how the event is presented syntactically and how much of what was said is presented in the sentence. The language used to describe private states and speech events is rich and varied, including idiomatic and metaphorical expressions <ref> (Barnden 1992) </ref>. There is a large amount of syntactic and part-of-speech variation, and the categorization is context dependent. Although the categories are complex, it has been demonstrated in an inter-coder reliability study (Wiebe and Bruce 1997) that these classifications can be performed with high reliability by human judges.
Reference: <author> Berger, A., Della Pietra, A., & Della Pietra, V. </author> <year> (1996). </year> <title> A maximum entropy approach to natural language processing. </title> <booktitle> Computational Linguistics 22 (1): </booktitle> <pages> 39-72. </pages>
Reference-contexts: The method permits the use of many features of different kinds, including n-gram properties as well as those types of features typically included in Maximum Entropy models <ref> (Berger et al. 1996) </ref> and Decision Trees (Breiman et al. 1994). In addition, as in Decision Tree induction (Breiman et al. 1994), feature selection can be performed as part of the process of model formulation. We experimented with many different kinds of properties to perform the classification task.
Reference: <author> Bishop, Y. M., Fienberg, S., & Holland, P. </author> <year> (1975). </year> <title> Discrete Multivariate Analysis: Theory and Practice. </title> <publisher> Cambridge: The MIT Press. </publisher>
Reference-contexts: The model selection process also performs feature selection. If a model is selected where there is no edge connecting a feature variable to the classification variable, then that feature has been, in essence, dropped from the classifier. The Log-likelihood ratio statistic G 2 <ref> (Bishop et al. 1975) </ref> is used as the model evaluation criterion in all of the experiments. 3 The Experiments and Results This section presents the results of the comparative experiments performed in this paper.
Reference: <author> Breiman, L., Friedman, J., Olshen, R., & Stone, C. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <address> Mon terey, CA: </address> <publisher> Wadsworth & Brooks/Cole Advanced Books & Software. </publisher>
Reference: <author> Brill, E. </author> <year> (1992). </year> <title> A simple rule-based part of speech tagger. </title> <booktitle> In Proc. of the Third Conference on Applied Natural Language Processing, </booktitle> <pages> pp. 152-155. </pages>
Reference-contexts: In identifying and applying the collocational properties listed below, the morphological analyzer described in Karp et al. (1992) is used to match the root forms of words, and Brill's tagger <ref> (Brill 1992) </ref> is used to assign parts of speech. We begin with the non-collocational properties, listing first those from the best experiment we found. Listed second are properties that were chosen in some experiment for inclusion in the most accurate model.
Reference: <author> Bruce, R. </author> <year> (1995). </year> <title> A Statistical Method for Word-Sense Disambiguation. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, New Mexico State University, </institution> <year> 1995. </year>
Reference-contexts: The method we use to automate this task is probabilistic classification. We perform an explicit model search to find a model that provides a good characterization of the relationships among the targeted classification and properties in the data <ref> (Bruce 1995, Bruce and Wiebe 1994, Pedersen et al. 1997) </ref>. Doing so is in contrast to one common practice in NLP of assuming a certain model form, such as n-gram and Naive Bayesian models, without testing how well those models fit the data. <p> We adopt a statistical approach whereby a probabilistic model is selected that describes the interactions among the feature variables. This approach is described fully elsewhere <ref> (Bruce 1995, Bruce and Wiebe 1994, Pedersen et al. 1996) </ref>. Such a model can form the basis of a probabilistic classifier since it specifies the probability of observing any and all combinations of the values of the feature variables.
Reference: <author> Bruce, R., & Wiebe, J. </author> <year> (1994). </year> <title> Word-sense disambiguation using decomposable models. </title> <booktitle> In Proc. of the 32nd Annual Meeting of the Association for Computational Linguistics (ACL-94), </booktitle> <pages> pp 139-146. </pages>
Reference: <author> Bruder, G. & Wiebe, J. </author> <year> (1990). </year> <title> A Psychological Test of an Algorithm for Recognizing Subjectivity in Narrative Text. </title> <booktitle> In Proc. of the 12th Annual Conference of the Cognitive Science Society, </booktitle> <pages> pp 947-952. </pages>
Reference: <author> Gale, W., Church, K., & Yarowsky, D. </author> <year> (1992a). </year>
References-found: 9

