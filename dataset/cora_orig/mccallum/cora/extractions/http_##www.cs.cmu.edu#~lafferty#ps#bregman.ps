URL: http://www.cs.cmu.edu/~lafferty/ps/bregman.ps
Refering-URL: http://www.cs.cmu.edu/~lafferty/publications.html
Root-URL: 
Title: Statistical Learning Algorithms Based on Bregman Distances  
Author: John D. Lafferty Stephen Della Pietra Vincent Della Pietra 
Affiliation: Carnegie Mellon University  Renaissance Technologies  Renaissance Technologies  
Abstract: We present a class of statistical learning algorithms formulated in terms of minimizing Breg man distances, a family of generalized entropy mea sures associated with convex functions. The inductive learning scheme is akin to growing a decision tree, with the Bregman distance filling the role of the im purity function in tree-based classifiers. Our approach is based on two components. In the feature selection step, each linear constraint in a pool of candidate fea tures is evaluated by the reduction in Bregman dis tance that would result from adding it to the model. In the constraint satisfaction step, all of the parame ters are adjusted to minimize the Bregman distance subject to the chosen constraints. We introduce a new iterative estimation algorithm for carrying out both the feature selection and constraint satisfaction steps, and outline a proof of the convergence of these algorithms. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L.M. Bregman, </author> <title> "The relaxation method to find the com mon point of convex sets and its applications to the solu tion of problems in convex programming," </title> <journal> USSR Compu tational Mathematics and Mathematical Physics, </journal> <volume> 7, 200 217, </volume> <year> 1967. </year>
Reference-contexts: The distances B F were introduced in <ref> [1] </ref> along with an interative algo rithm for minimizing B F subject to linear constraints. Another such algorithm is given in [4].
Reference: [2] <author> L. Breiman, J. Friedman, R. Olshen, and C. Stone, </author> <title> Classi fication and Regression Trees, </title> <publisher> Wadsworth, </publisher> <address> Belmont, </address> <year> 1984. </year>
Reference-contexts: 1 Introduction In this paper we present a class of statistical learning algorithms formulated in terms of minimizing Bregman distances, a class of generalized entropy measures as sociated with convex functions. The closest relatives of these algorithms are statistical methods for grow ing decision trees <ref> [2] </ref>. <p> Popular impu rity functions are the Shannon entropy f (p 1 ; : : : ; p C ) = j p j log p j and the Gini criterion f (p 1 ; : : : ; p C ) = P P j . We refer to <ref> [2] </ref> for an in-depth presentation of the art and science of growing decision trees. In the algorithms for Bregman distances that we outline here, questions are formulated as linear constraints on the fl School of Computer Science, Carnegie Mellon University, Pitts burgh, PA 15213, USA. E-mail: lafferty@cs.cmu.edu.
Reference: [3] <author> Y. Censor, </author> <title> "Optimization of `log-x'-entropy over linear equality constraints," </title> <journal> SIAM J. Control Optim. </journal> <volume> 25, 921 933, </volume> <year> 1987. </year>
Reference: [4] <author> Y. Censor and A. Lent, </author> <title> "An iterative row-action methd for interval convex programming," </title> <journal> J. Optim. Theory Appl. </journal> <volume> 34, </volume> <pages> 321-353, </pages> <year> 1981. </year>
Reference-contexts: The distances B F were introduced in [1] along with an interative algo rithm for minimizing B F subject to linear constraints. Another such algorithm is given in <ref> [4] </ref>.
Reference: [5] <author> I. Csiszar, "Maxent, </author> <title> mathematics, and information the ory," In Maximum Entropy and Bayesian Methods, </title> <editor> K. Hanson and R. Silver, eds., </editor> <publisher> Kluwer Academic Publishers, </publisher> <year> 1996. </year>
Reference: [6] <author> I. Csiszar, </author> <title> "Why least squares and maximum entropy? An axiomatic approach to inference for linear inverse prob lems," </title> <journal> Ann. Statist., </journal> <volume> 19(4), </volume> <pages> 2032-2066, </pages> <year> 1991. </year>
Reference-contexts: These distances are naturally placed within continuous family of convex functions f ff (x) = &gt; &lt; x ff + ffx ff + 1 for 0 &lt; ff &lt; 1 x log x x + 1 if ff = 1 The significance of this family lies in Csiszar's results <ref> [6] </ref> showing that scale-invariant inference for linear inverse problems can be characterized in terms of minimizing a Bregman distance of the form B f ff . A sample of these curves is shown graphically in Figure 2.
Reference: [7] <author> I. Csiszar, </author> <title> "Generalized projections for non-negative func tions," </title> <journal> Acta Math. </journal> <volume> Hungar., </volume> <pages> 68(1-2), 161-185, </pages> <year> 1995. </year>
Reference: [8] <author> J. Darroch and D. Ratcliff, </author> <title> "Generalized iterative scaling for log-linear models," </title> <journal> Ann. Math. Statist. </journal> <volume> 43, </volume> <pages> 1470-1480, </pages> <year> 1972. </year>
Reference: [9] <author> S. Della Pietra, V. Della Pietra, and J. Lafferty, </author> <title> "Inducing features of random fields," </title> <journal> IEEE Trans. Pattern Analysis and Machine Intell., </journal> <note> 19(4), April 1997 (in press). </note>
Reference-contexts: The details of this proof in the special case of the Kullback-Leibler divergence appear in <ref> [9] </ref>. The basic idea behind the algorithm is to make use of an auxiliary func tion which bounds the change in divergence from below after each iteration.
Reference: [10] <author> A.P. Dempster, N.M. Laird, and D.B. Rubin, </author> <title> "Maximum likelihood from incomplete data via the EM algorithm," </title> <journal> Journal of the Royal Statistical Society 39, </journal> <volume> No. B, </volume> <pages> 1-38, </pages> <year> 1977. </year>
Reference-contexts: The basic idea behind the algorithm is to make use of an auxiliary func tion which bounds the change in divergence from below after each iteration. This use of an auxiliary function is the standard means of justifying and implementing the EM algorithm <ref> [10] </ref>, but the application of this technique to generalized maximum entropy problems appears to be new. 2 Bregman Distance If f is a strictly convex real-valued function, the f -entropy of a discrete measure p (x) 0 is defined by H f (p) = x and the Bregman distance B f
Reference: [11] <author> P.S. Neelakanta, S. Abusalah, D. De Groff, R. Sudhakar, and J.C. Park, </author> <title> "Csiszar's generalized error measures for gradient-descent-based optimizations in neural networks using the backpropagation algorithm," </title> <journal> Connection Sci ence, </journal> <volume> 8, No. 1, </volume> <pages> 79-114, </pages> <year> 1996. </year>
Reference-contexts: Other closely related "distances" associated with convex functions are Csiszar's f-divergences D f (p k q) = x q (x) We do not consider algorithms for these divergences in this paper, but note that they are investigated as error measures in backpropagation training of neural networks in <ref> [11] </ref>. man distances: f 2 (t) = (t 1) 2 , f 0 = t log (t) 1, f 1 = t log (t) t + 1, and f 1 = t 1=2 + t=2 + 1=2, corresponding to the mean-squared distance, Itakura-Saito distortion, and Kullback-Leibler divergence, and the intermediate distance
References-found: 11

