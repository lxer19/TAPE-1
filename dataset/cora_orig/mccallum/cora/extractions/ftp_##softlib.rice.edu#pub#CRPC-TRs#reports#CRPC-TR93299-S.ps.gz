URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR93299-S.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: Automatic Data Layout for Distributed-Memory Machines  
Author: Ulrich Kremer 
Address: P.O. Box 1892 Houston, TX 77251-1892  
Affiliation: Rice University  
Note: Center for Research on Parallel Computation  
Date: February, 1993  
Pubnum: CRPC-TR93299-S  
Abstract-found: 0
Intro-found: 1
Reference: [AKLS88] <author> E. Albert, K. Knobe, J. Lukas, and G. Steele, Jr. </author> <title> Compiling Fortran 8x array features for the Connection Machine computer system. </title> <booktitle> In Proceedings of the ACM SIGPLAN Symposium on Parallel Programming: Experience with Applications, Languages, and Systems (PPEALS), </booktitle> <address> New Haven, CT, </address> <month> July </month> <year> 1988. </year>
Reference-contexts: gradient program. 3.2 SIMD Machines 3.2.1 Albert, Knobe, Lukas, Natarajan, Steele, and Weiss at Compass and Thinking Machines Albert, Knobe, Lukas, Natarajan, Steele, and Weiss discuss automatic data layout as part of the design and implementation at Compass of SIMD compilers for Fortran 77 extended by Fortran 8x array features <ref> [AKLS88, KLS88, KLS90, KN90, Wei91] </ref>. The target machines are the Connection Machine CM-2 and the MasPar MP-1. Automatic data layout is an integral part of these compilers. Arrays are aligned by mapping them onto virtual processors based on their usage as opposed to a their declared shape.
Reference: [ASU86] <author> A. V. Aho, R. Sethi, and J. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <note> second edition, </note> <year> 1986. </year>
Reference-contexts: The compiler module employs a compiler-level training set written in the source language that consists of common computation patterns and program kernels such as stencil computations and matrix multiplication. Note that in contrast to pattern matching algorithms as they are used in compilers for optimization or code generation <ref> [ASU86, LC90b, PP91] </ref>, our pattern matcher does not need to preserve the semantics of the program. Program segments are considered `equivalent' if their corresponding compiler generated code exhibit a similar performance behavior. <p> The merging problem can be formulated as a single-source shortest paths problem over the phase control flow graph. The phase control flow graph is similar to the control flow graph <ref> [ASU86] </ref> where all nodes associated with a phase are substituted by nodes representing the set of reasonable data decomposition schemes for the phase. The static performance estimator is used to predict the costs for these reasonable decomposition schemes.
Reference: [BFKK90] <author> V. Balasundaram, G. Fox, K. Kennedy, and U. Kremer. </author> <title> An interactive environment for data partitioning and distribution. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: a tool has been recognized in the final report of the findings of the Pasadena Workshop on System Software and Tools for High-Performance Computing Environments [SMC + 92]. 4.1.4 Implementation The proposed automatic techniques will be implemented as part of the ParaScope parallel programming environment adapted to distributed memory multiprocessors <ref> [BKK + 89, KMT91, BFKK90, HKK + 91] </ref>. An overview of the system is shown in Figure 2. The system is part of the D Environment currently under development at Rice University [CCH + 92].
Reference: [BFKK91] <author> V. Balasundaram, G. Fox, K. Kennedy, and U. Kremer. </author> <title> A static performance estimator to guide data partitioning decisions. </title> <booktitle> In Proceedings of the Third ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Williamsburg, VA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: An overview of the system is shown in Figure 2. The system is part of the D Environment currently under development at Rice University [CCH + 92]. A prototype of the machine module of the static performance estimator is available <ref> [BFKK91] </ref>. 14 4.2 Program Phases The analysis performed by the automatic data partitioner divides the program into separate computation phases. A phase is a syntactic entity. Phases try to identify program segments that perform operations on entire data objects. Dynamic realignment and redistribution is allowed only between phases. <p> The prototype performance estimator has proved quite precise, especially in predicting the relative performances of different data decompositions <ref> [BFKK91] </ref>. Our experience with applying the prototype estimator to a point-wise red-black relaxation routine is given in Appendix C.
Reference: [BGMZ92] <author> P. Brezany, M. Gerndt, P. Mehrotra, and H. Zima. </author> <title> Concurrent file operations in a high performance FORTRAN. </title> <booktitle> In Proceedings of Supercomputing '92, </booktitle> <address> Minneapolis, MN, </address> <month> November </month> <year> 1992. </year>
Reference: [BKK + 89] <author> V. Balasundaram, K. Kennedy, U. Kremer, K. S. M c Kinley, and J. Subhlok. </author> <title> The ParaScope Editor: An interactive parallel programming tool. </title> <booktitle> In Proceedings of Supercomputing '89, </booktitle> <address> Reno, NV, </address> <month> November </month> <year> 1989. </year>
Reference-contexts: a tool has been recognized in the final report of the findings of the Pasadena Workshop on System Software and Tools for High-Performance Computing Environments [SMC + 92]. 4.1.4 Implementation The proposed automatic techniques will be implemented as part of the ParaScope parallel programming environment adapted to distributed memory multiprocessors <ref> [BKK + 89, KMT91, BFKK90, HKK + 91] </ref>. An overview of the system is shown in Figure 2. The system is part of the D Environment currently under development at Rice University [CCH + 92].
Reference: [Bri92] <author> P. Briggs. </author> <title> Register Allocation via Graph Coloring. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> April </month> <year> 1992. </year>
Reference-contexts: Node compilers may perform optimizations to exploit the memory hierarchy and instruction-level parallelism available on the target node processor <ref> [Car92, Wol92, Bri92] </ref>. Our tool for automatic data decomposition is based on a specific Fortran D compilation system representing state-of-the-art compiler technology. Note that the optimizations performed by the Fortran D compiler are not restricted to the ones implemented in the current prototype of the Fortran D compiler [Tse93].
Reference: [Car92] <author> S. Carr. </author> <title> Memory-Hierarchy Management. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> September </month> <year> 1992. </year>
Reference-contexts: Node compilers may perform optimizations to exploit the memory hierarchy and instruction-level parallelism available on the target node processor <ref> [Car92, Wol92, Bri92] </ref>. Our tool for automatic data decomposition is based on a specific Fortran D compilation system representing state-of-the-art compiler technology. Note that the optimizations performed by the Fortran D compiler are not restricted to the ones implemented in the current prototype of the Fortran D compiler [Tse93].
Reference: [CCH + 92] <author> A. Carle, K. Cooper, M. Hall, K. Kennedy, , C. Koelbel, J. Mellor-Crummey, L. Torczon, and S. Warren. </author> <title> A software platform for parallel scientific programming. </title> <type> Internal Report, </type> <institution> Rice University, </institution> <year> 1992. </year>
Reference-contexts: An overview of the system is shown in Figure 2. The system is part of the D Environment currently under development at Rice University <ref> [CCH + 92] </ref>. A prototype of the machine module of the static performance estimator is available [BFKK91]. 14 4.2 Program Phases The analysis performed by the automatic data partitioner divides the program into separate computation phases. A phase is a syntactic entity.
Reference: [CCL88] <author> M. Chen, Y. Choo, and J. Li. </author> <title> Compiling parallel programs by optimizing performance. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 1(2) </volume> <pages> 171-207, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: A number of researchers have proposed annotating a language based on a global name space with directives specifying how the data should be mapped onto the distributed memory machine <ref> [CK88, CCL88, 1 KMV90, PSvG91, RA90, RP89, RSW88, ZBG88, KZBG88, Ger89] </ref>. This approach was inspired by the observation that the most demanding intellectual step in writing a program for a distributed memory machine is the appropriate data layout the rest is straightforward but tedious and error prone work.
Reference: [CCL89] <author> M. Chen, Y. Choo, and J. Li. </author> <title> Theory and pragmatics of compiling efficient parallel code. </title> <type> Technical Report YALEU/DCS/TR-760, </type> <institution> Dept. of Computer Science, Yale University, </institution> <address> New Haven, CT, </address> <month> December </month> <year> 1989. </year>
Reference-contexts: of this section we will only present related work that discusses automatic data decomposition for entire subroutines or whole programs. 3.1.1 Li, Chen, and Choo at Yale University Li, Chen, and Choo investigated techniques for automatic data layout as part of the Crystal compiler and language project at Yale University <ref> [CCL89, LC90a, LC91b, LC91a, LC90b] </ref>. Crystal is a high-level, purely functional language. It does not contain statements that specify the data layout. The goal of the Crystal compiler is to generate efficient SPMD node programs with explicit communications or synchronization for a variety of massively parallel machines.
Reference: [CGST92] <author> S. Chatterjee, J.R. Gilbert, R. Schreiber, and S-H. Teng. </author> <title> Optimal evaluation of array expres 23 sions on massively parallel machines. </title> <booktitle> In Proceedings of the Second Workshop on Languages, Compilers, and Runtime Environments for Distributed Memory Multiprocessors, </booktitle> <address> Bolder, CO, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: Performance figures of actual runs are not reported. 3.2.2 Chatterjee, Gilbert, Schreiber, and Teng at RIACS, Xerox PARC, and MIT Chatterjee, Gilbert, Schreiber, and Teng discuss a framework for automatic alignment in an array-based, data-parallel language such as Fortran90 <ref> [CGST93, CGST92, GS91] </ref>. They provide algorithms for automatic alignment of arrays in a single basic block. Each intermediate result of a computations in a basic block is assigned to a temporary array. This allows intermediate results to be mapped explicitly.
Reference: [CGST93] <author> S. Chatterjee, J.R. Gilbert, R. Schreiber, and S-H. Teng. </author> <title> Automatic array alignment in data-parallel programs. </title> <booktitle> In Proceedings of the Twentieth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Albuquerque, NM, </address> <month> January </month> <year> 1993. </year>
Reference-contexts: Performance figures of actual runs are not reported. 3.2.2 Chatterjee, Gilbert, Schreiber, and Teng at RIACS, Xerox PARC, and MIT Chatterjee, Gilbert, Schreiber, and Teng discuss a framework for automatic alignment in an array-based, data-parallel language such as Fortran90 <ref> [CGST93, CGST92, GS91] </ref>. They provide algorithms for automatic alignment of arrays in a single basic block. Each intermediate result of a computations in a basic block is assigned to a temporary array. This allows intermediate results to be mapped explicitly. <p> This is often referred to as "relaxing the owner-computes rule". In the SIMD model of execution, array temporaries must be introduced by the compiler for intermediate values <ref> [CGST93] </ref>. The possibility of using these temporaries to relax the owner computes rule comes therefore `for free'. This is not true for the compilation for MIMD machines with scalar node processors. The node compiler will generate the necessary temporaries. <p> Interprocedural techniques will be developed that identify good alignment schemes. In a source-to-source transformation step, temporary arrays are introduced to facilitate the relaxation of the owner computes rule by naming intermediate results of computations explicitly. This process is selective and not as general as described in <ref> [CGST93] </ref> where all intermediate results 16 REAL a (N), b (N), c (N) DECOMPOSITION d1 (N) ALIGN a, b, c WITH d1 DISTRIBUTE d1 (BLOCK) . . . a (i) = F ( b (i+1), c (i+1) ) ENDDO REAL a (N), b (N), c (N), temp (N) DECOMPOSITION d1 (N) <p> In case (C), arrays b and c have to be transposed. In contrast, only the transpose of temp is necessary in case (D). We intend to build on the inter-dimensional and intra-dimensional alignment techniques of Li and Chen [LC90a], Knobe et al. [KLS90], and Chatterjee, Gilbert, Schreiber, and Teng <ref> [CGST93] </ref>. The alignment problems can be formulated as an optimization problem on an undirected, weighted 17 graph. Some instances of the problem of alignment have been shown to be NP-complete [LC90a, CGST93]. <p> The alignment problems can be formulated as an optimization problem on an undirected, weighted 17 graph. Some instances of the problem of alignment have been shown to be NP-complete <ref> [LC90a, CGST93] </ref>. One major challenge in our proposed work will be to define the appropriate weights of the graph and to develop an interprocedural algorithm that solves the alignment problem in a way that is suitable for loosely synchronous problems.
Reference: [CH91] <author> B.M. Chapman and H.M. Herbeck. </author> <title> Knowledge-based parallelization for distributed memory systems. </title> <booktitle> In First International Conference of the Austrian Center for Parallel Computation, </booktitle> <address> Salzburg, Austria, </address> <month> September </month> <year> 1991. </year>
Reference-contexts: Data redistribution or replication is not considered. 3.1.5 Chapman, Fahringer, Blasko, Herbeck, Zima at the University of Vienna Chapman, Fahringer, Blasko, Herbeck, and Zima at the University of Vienna propose automatic data decomposition as part of the interactive parallelization system SUPERB-2 <ref> [CHZ91, CH91, FBZ92] </ref>. SUPERB-2 takes Fortran 77 programs as input and generates SPMD node programs with explicit communication. Their approach is based on Gupta's and Banerjee's work at Illinois (see Section 3.1.2).
Reference: [CHK92] <author> K. Cooper, M. W. Hall, and K. Kennedy. </author> <title> Procedure cloning. </title> <booktitle> In Proceedings of the 1992 IEEE International Conference on Computer Language, </booktitle> <address> Oakland, CA, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: Inter-procedural analysis is used to allow the merging of computation phases across procedure boundaries. Inter-procedural phase merging is compiler dependent. In particular, the automatic data partitioner has to know whether and when the compiler performs inter-procedural optimizations such as procedure cloning <ref> [CHK92, HHKT91] </ref> or procedure inlining [Hal91]. In the following we will assume that the compiler performs procedure cloning for every distinct pattern of entry and exit decomposition schemes. To simplify our discussion we will assume that programs have only acyclic call graphs.
Reference: [CHZ91] <author> B. Chapman, H. Herbeck, and H. Zima. </author> <title> Automatic support for data distribution. </title> <booktitle> In Proceedings of the 6th Distributed Memory Computing Conference, </booktitle> <address> Portland, OR, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Data redistribution or replication is not considered. 3.1.5 Chapman, Fahringer, Blasko, Herbeck, Zima at the University of Vienna Chapman, Fahringer, Blasko, Herbeck, and Zima at the University of Vienna propose automatic data decomposition as part of the interactive parallelization system SUPERB-2 <ref> [CHZ91, CH91, FBZ92] </ref>. SUPERB-2 takes Fortran 77 programs as input and generates SPMD node programs with explicit communication. Their approach is based on Gupta's and Banerjee's work at Illinois (see Section 3.1.2).
Reference: [CK88] <author> D. Callahan and K. Kennedy. </author> <title> Compiling programs for distributed-memory multiprocessors. </title> <journal> Journal of Supercomputing, </journal> <volume> 2 </volume> <pages> 151-169, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: A number of researchers have proposed annotating a language based on a global name space with directives specifying how the data should be mapped onto the distributed memory machine <ref> [CK88, CCL88, 1 KMV90, PSvG91, RA90, RP89, RSW88, ZBG88, KZBG88, Ger89] </ref>. This approach was inspired by the observation that the most demanding intellectual step in writing a program for a distributed memory machine is the appropriate data layout the rest is straightforward but tedious and error prone work.
Reference: [CKK89] <author> D. Callahan, K. Kennedy, and U. Kremer. </author> <title> A dynamic study of vectorization in PFC. </title> <type> Technical Report TR89-97, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> July </month> <year> 1989. </year>
Reference-contexts: A program is written in a data-parallel programming style if it allows advanced compilation systems to generate efficient code for most distributed-memory machines. In the context of vectorization, the existence of a usable programming style has been partially responsible for the success of automatic vectorization <ref> [CKK89, Wol89] </ref>. I do not believe that such fully automatic techniques will be successful for parallelizing `dusty deck' programs. To support my thesis I will build a prototype tool that takes a Fortran 77 program as input and generates a Fortran D program.
Reference: [CLR90] <author> T. H. Cormen, C. E. Leiserson, and R. L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> The MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: The availability of fast collective communication routines will be crucial for the profitability of realignment and redistribution. The merging problem for a linear phase control flow graph can be solved as a single-source shortest paths problem in a directed acyclic graph <ref> [CLR90] </ref>. For example, Figure 4 shows a three phase problem with four reasonable decompositions for each phase. Each decomposition scheme is represented by a node. The node is labeled with the predicted cost of the decomposition scheme for the phase.
Reference: [Clu89] <author> The Perfect Club. </author> <title> The Perfect Club benchmarks: efficient performance evaluation of supercomputers. </title> <journal> Int. J. Supercomp. Appl., </journal> <volume> 3(3) </volume> <pages> 5-40, </pages> <year> 1989. </year>
Reference-contexts: The automatic techniques have been implemented as part of Parafrase-2. They have been applied to five Fortran programs, namely one routine from the Linpack library (dgefa), one Eispack routine (tred2), and three programs from the Perfect Club Benchmark Suite (trfd, mdg, flo52) <ref> [Clu89] </ref>. In the study, all the steps of the described automatic data layout techniques are simulated by hand. A distributed memory compiler is not part of Parafrase-2. Actual performance figures for the generated data layout schemes are only given for tred2 on an iPSC/2 hypercube system.
Reference: [D'H89] <author> E. D'Hollander. </author> <title> Partitioning and labeling of index sets in do loops with constant dependence. </title> <booktitle> In Proceedings of the 1989 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1989. </year>
Reference-contexts: The iteration space is first partitioned into sets of iterations that can be executed independently. The data mapping is then determined by the iterations that are assigned to the different processors <ref> [RS89, Ram90, D'H89, KKBP91] </ref>. Other techniques for single loop nests are based on recognizing specific computation patterns in the loop, called stencils [SS90, HA90]. This more abstract representation is used to find a good data mapping.
Reference: [EXP89] <institution> Parasoft Corporation. </institution> <note> Express User's Manual, </note> <year> 1989. </year>
Reference-contexts: Techniques to estimate the performance of programs where communication and computation overlap, for instance pipelined computations, have to be developed as part of this thesis. 15 The prototype predicts the performance of a node program using Express communication routines for different numbers of processors and data sizes <ref> [EXP89] </ref>. The prototype performance estimator has proved quite precise, especially in predicting the relative performances of different data decompositions [BFKK91]. Our experience with applying the prototype estimator to a point-wise red-black relaxation routine is given in Appendix C.
Reference: [Fah92] <author> T. Fahringer. </author> <title> Private communication. </title> <year> 1992. </year>
Reference-contexts: The tool performs inter-procedural analysis and determines the profitability of redistribution. The proposed tool is currently being implemented at the University of Vienna. A prototype static performance estimator is in its testing phase <ref> [FBZ92, Fah92] </ref>. No experimental results have been published. 8 3.1.6 ASPAR and P 3 C ASPAR is a compiler for the C language developed by the ParaSoft corporation [IFKF90]. P 3 C is a research Pascal compiler designed and implemented at the Tel-Aviv University by Gabber, Averbuch, and Yehudai [GAY91].
Reference: [FBZ92] <author> T. Fahringer, R. Blasko, and H.P. Zima. </author> <title> Automatic performance prediction to support paral-lelization of Fortran programs for massively parallel systems. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <address> Washington, DC, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Data redistribution or replication is not considered. 3.1.5 Chapman, Fahringer, Blasko, Herbeck, Zima at the University of Vienna Chapman, Fahringer, Blasko, Herbeck, and Zima at the University of Vienna propose automatic data decomposition as part of the interactive parallelization system SUPERB-2 <ref> [CHZ91, CH91, FBZ92] </ref>. SUPERB-2 takes Fortran 77 programs as input and generates SPMD node programs with explicit communication. Their approach is based on Gupta's and Banerjee's work at Illinois (see Section 3.1.2). <p> The tool performs inter-procedural analysis and determines the profitability of redistribution. The proposed tool is currently being implemented at the University of Vienna. A prototype static performance estimator is in its testing phase <ref> [FBZ92, Fah92] </ref>. No experimental results have been published. 8 3.1.6 ASPAR and P 3 C ASPAR is a compiler for the C language developed by the ParaSoft corporation [IFKF90]. P 3 C is a research Pascal compiler designed and implemented at the Tel-Aviv University by Gabber, Averbuch, and Yehudai [GAY91].
Reference: [FHK + 90] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, C. Tseng, and M. Wu. </author> <title> Fortran D language specification. </title> <type> Technical Report TR90-141, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: This approach was inspired by the observation that the most demanding intellectual step in writing a program for a distributed memory machine is the appropriate data layout the rest is straightforward but tedious and error prone work. The Fortran D language and its compiler <ref> [FHK + 90, HKT92b] </ref> support this programming style. Given a Fortran D program, the compiler uses data layout directives to automatically generates a single-program, multiple data (SPMD) node program for a given distributed-memory target machine. Selecting a good data layout is important for a program to achieve high performance. <p> In addition, we believe that the two-phase strategy for specifying data decomposition is natural and conducive to writing modular and portable code. Fortran D bears similarities to both CM Fortran [TMC89] and Kali [KM91]. The complete language is described in detail elsewhere <ref> [FHK + 90] </ref>. 2.2 The Compilation System A Fortran D compilation system translates a Fortran D program into a Fortran 77 SPMD node program that contains calls to library primitives for interprocessor communication.
Reference: [FJL + 88] <author> G. Fox, M. Johnson, G. Lyzenga, S. Otto, J. Salmon, and D. Walker. </author> <title> Solving Problems on Concurrent Processors, volume 1. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1988. </year>
Reference-contexts: Loosely synchronous problems represent a large class of scientific computations <ref> [FJL + 88] </ref>.
Reference: [GAY91] <author> E. Gabber, A. Averbuch, and A. Yehudai. </author> <title> Experience with a portable parallelizing Pascal compiler. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1991. </year> <month> 24 </month>
Reference-contexts: No experimental results have been published. 8 3.1.6 ASPAR and P 3 C ASPAR is a compiler for the C language developed by the ParaSoft corporation [IFKF90]. P 3 C is a research Pascal compiler designed and implemented at the Tel-Aviv University by Gabber, Averbuch, and Yehudai <ref> [GAY91] </ref>. Both systems generate SPMD node programs that contain calls to communication library routines. The compilers perform only a simple form of program analysis to generate the correct communications. The set of possible data decomposition schemes is small. Inter-procedural analysis is performed.
Reference: [GB90] <author> M. Gupta and P. Banerjee. </author> <title> Automatic data partitioning on distributed memory multiproces-sors. </title> <type> Technical Report CRHC-90-14, </type> <institution> Center for Reliable and High-Performance Computing, Coordinated Science Laboratory, University of Illinois at Urbana-Champaign, </institution> <month> October </month> <year> 1990. </year>
Reference-contexts: The distribution strategy is read in at runtime [Li92]. 3.1.2 Banerjee and Gupta at the University of Illinois Gupta and Banerjee at the University of Illinois at Urbana-Champaign developed techniques for automatic data layout as part of a compiler based on the Parafrase-2 program restructurer <ref> [GB90, GB91, GB92] </ref>. The compiler takes Fortran 77 as input and generates SPMD node programs with explicit communication. The compiler performs alignment and distribution analysis based on constraints for each single statement in the program. Constraints represent properties of the data 6 layout and are associated with a quality measure.
Reference: [GB91] <author> M. Gupta and P. Banerjee. </author> <title> Automatic data partitioning on distributed memory multiprocessors. </title> <booktitle> In Proceedings of the 6th Distributed Memory Computing Conference, </booktitle> <address> Portland, OR, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: The distribution strategy is read in at runtime [Li92]. 3.1.2 Banerjee and Gupta at the University of Illinois Gupta and Banerjee at the University of Illinois at Urbana-Champaign developed techniques for automatic data layout as part of a compiler based on the Parafrase-2 program restructurer <ref> [GB90, GB91, GB92] </ref>. The compiler takes Fortran 77 as input and generates SPMD node programs with explicit communication. The compiler performs alignment and distribution analysis based on constraints for each single statement in the program. Constraints represent properties of the data 6 layout and are associated with a quality measure.
Reference: [GB92] <author> M. Gupta and P. Banerjee. </author> <title> Demonstration of automatic data partitioning techniques for paral-lelizing compilers on multicomputers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <month> April </month> <year> 1992. </year>
Reference-contexts: The distribution strategy is read in at runtime [Li92]. 3.1.2 Banerjee and Gupta at the University of Illinois Gupta and Banerjee at the University of Illinois at Urbana-Champaign developed techniques for automatic data layout as part of a compiler based on the Parafrase-2 program restructurer <ref> [GB90, GB91, GB92] </ref>. The compiler takes Fortran 77 as input and generates SPMD node programs with explicit communication. The compiler performs alignment and distribution analysis based on constraints for each single statement in the program. Constraints represent properties of the data 6 layout and are associated with a quality measure.
Reference: [Ger89] <author> M. Gerndt. </author> <title> Automatic Parallelization for Distributed-Memory Multiprocessing Systems. </title> <type> PhD thesis, </type> <institution> University of Bonn, </institution> <month> December </month> <year> 1989. </year>
Reference-contexts: A number of researchers have proposed annotating a language based on a global name space with directives specifying how the data should be mapped onto the distributed memory machine <ref> [CK88, CCL88, 1 KMV90, PSvG91, RA90, RP89, RSW88, ZBG88, KZBG88, Ger89] </ref>. This approach was inspired by the observation that the most demanding intellectual step in writing a program for a distributed memory machine is the appropriate data layout the rest is straightforward but tedious and error prone work.
Reference: [GS91] <author> J.R. Gilbert and R. Schreiber. </author> <title> Optimal expression evaluation for data parallel architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13(1) </volume> <pages> 58-64, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: Performance figures of actual runs are not reported. 3.2.2 Chatterjee, Gilbert, Schreiber, and Teng at RIACS, Xerox PARC, and MIT Chatterjee, Gilbert, Schreiber, and Teng discuss a framework for automatic alignment in an array-based, data-parallel language such as Fortran90 <ref> [CGST93, CGST92, GS91] </ref>. They provide algorithms for automatic alignment of arrays in a single basic block. Each intermediate result of a computations in a basic block is assigned to a temporary array. This allows intermediate results to be mapped explicitly.
Reference: [HA90] <author> D. Hudak and S. Abraham. </author> <title> Compiler techniques for data partitioning of sequentially iterated parallel loops. </title> <booktitle> In Proceedings of the 1990 ACM International Conference on Supercomputing, </booktitle> <address> Amsterdam, The Netherlands, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: The data mapping is then determined by the iterations that are assigned to the different processors [RS89, Ram90, D'H89, KKBP91]. Other techniques for single loop nests are based on recognizing specific computation patterns in the loop, called stencils <ref> [SS90, HA90] </ref>. This more abstract representation is used to find a good data mapping. While these approaches will minimize the necessary communication in single loop nests, it is not clear whether a compiler can generate efficient node programs for the loop nests or between adjacent loop nests.
Reference: [Hal91] <author> M. W. Hall. </author> <title> Managing Interprocedural Optimization. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: Inter-procedural analysis is used to allow the merging of computation phases across procedure boundaries. Inter-procedural phase merging is compiler dependent. In particular, the automatic data partitioner has to know whether and when the compiler performs inter-procedural optimizations such as procedure cloning [CHK92, HHKT91] or procedure inlining <ref> [Hal91] </ref>. In the following we will assume that the compiler performs procedure cloning for every distinct pattern of entry and exit decomposition schemes. To simplify our discussion we will assume that programs have only acyclic call graphs. The augmented call graph is used to identify phases across procedure boundaries [HKM91].
Reference: [HHKT91] <author> M. W. Hall, S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Interprocedural compilation of Fortran D for MIMD distributed-memory machines. </title> <type> Technical Report TR91-169, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> November </month> <year> 1991. </year>
Reference-contexts: Procedure cloning or inlining may be applied under certain conditions to improve context for optimization. A Fortran D compiler may relax the owner computes rule for reductions and parallel prefix operations, and for scalars or arrays that are recognized to be temporaries <ref> [HKT91, HKT92a, HHKT91, Tse93] </ref>. Node compilers may perform optimizations to exploit the memory hierarchy and instruction-level parallelism available on the target node processor [Car92, Wol92, Bri92]. Our tool for automatic data decomposition is based on a specific Fortran D compilation system representing state-of-the-art compiler technology. <p> Inter-procedural analysis is used to allow the merging of computation phases across procedure boundaries. Inter-procedural phase merging is compiler dependent. In particular, the automatic data partitioner has to know whether and when the compiler performs inter-procedural optimizations such as procedure cloning <ref> [CHK92, HHKT91] </ref> or procedure inlining [Hal91]. In the following we will assume that the compiler performs procedure cloning for every distinct pattern of entry and exit decomposition schemes. To simplify our discussion we will assume that programs have only acyclic call graphs.
Reference: [HKK + 91] <author> S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, and C. Tseng. </author> <title> An overview of the Fortran D programming system. </title> <booktitle> In Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Santa Clara, CA, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: a tool has been recognized in the final report of the findings of the Pasadena Workshop on System Software and Tools for High-Performance Computing Environments [SMC + 92]. 4.1.4 Implementation The proposed automatic techniques will be implemented as part of the ParaScope parallel programming environment adapted to distributed memory multiprocessors <ref> [BKK + 89, KMT91, BFKK90, HKK + 91] </ref>. An overview of the system is shown in Figure 2. The system is part of the D Environment currently under development at Rice University [CCH + 92].
Reference: [HKM91] <author> M. W. Hall, K. Kennedy, and K. S. M c Kinley. </author> <title> Interprocedural transformations for parallel code generation. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <address> Albuquerque, NM, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: In the following we will assume that the compiler performs procedure cloning for every distinct pattern of entry and exit decomposition schemes. To simplify our discussion we will assume that programs have only acyclic call graphs. The augmented call graph is used to identify phases across procedure boundaries <ref> [HKM91] </ref>. Subsequently, the call graph is traversed in reverse topological order. For each procedure P the single-source shortest paths problem is solved on its phase control flow graph using the hierarchical approach of algorithm DECOMP in Figure 6.
Reference: [HKT91] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiler optimizations for Fortran D on MIMD distributed-memory machines. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <address> Albuquerque, NM, </address> <month> Novem-ber </month> <year> 1991. </year>
Reference-contexts: Procedure cloning or inlining may be applied under certain conditions to improve context for optimization. A Fortran D compiler may relax the owner computes rule for reductions and parallel prefix operations, and for scalars or arrays that are recognized to be temporaries <ref> [HKT91, HKT92a, HHKT91, Tse93] </ref>. Node compilers may perform optimizations to exploit the memory hierarchy and instruction-level parallelism available on the target node processor [Car92, Wol92, Bri92]. Our tool for automatic data decomposition is based on a specific Fortran D compilation system representing state-of-the-art compiler technology. <p> Distribution analysis is compiler, machine, and problem dependent. For instance, the compiler may not be able to generate efficient wavefront computations [Lam74] for a subset of distributions. Transformations such as loop interchange and strip-mining can substantially improve the degree of parallelism induced by the wavefront <ref> [HKT91] </ref>. A consideration in our pruning heuristic are the sizes of the dimensions of the decomposition. If the size of a dimension is smaller than a machine dependent threshold, the dimension will always be localized. This eliminates all distributions that map small dimensions to distinct processors.
Reference: [HKT92a] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Evaluation of compiler optimizations for Fortran D on MIMD distributed-memory machines. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <address> Washington, DC, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Procedure cloning or inlining may be applied under certain conditions to improve context for optimization. A Fortran D compiler may relax the owner computes rule for reductions and parallel prefix operations, and for scalars or arrays that are recognized to be temporaries <ref> [HKT91, HKT92a, HHKT91, Tse93] </ref>. Node compilers may perform optimizations to exploit the memory hierarchy and instruction-level parallelism available on the target node processor [Car92, Wol92, Bri92]. Our tool for automatic data decomposition is based on a specific Fortran D compilation system representing state-of-the-art compiler technology.
Reference: [HKT92b] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiler support for machine-independent parallel programming in Fortran D. </title> <editor> In J. Saltz and P. Mehrotra, editors, </editor> <title> Compilers and Runtime Software for Scalable Multiprocessors. </title> <publisher> Elsevier, </publisher> <address> Amsterdam, The Netherlands, </address> <note> to appear 1992. </note>
Reference-contexts: This approach was inspired by the observation that the most demanding intellectual step in writing a program for a distributed memory machine is the appropriate data layout the rest is straightforward but tedious and error prone work. The Fortran D language and its compiler <ref> [FHK + 90, HKT92b] </ref> support this programming style. Given a Fortran D program, the compiler uses data layout directives to automatically generates a single-program, multiple data (SPMD) node program for a given distributed-memory target machine. Selecting a good data layout is important for a program to achieve high performance.
Reference: [IFKF90] <author> K. Ikudome, G. Fox, A. Kolawa, and J. Flower. </author> <title> An automatic and symbolic parallelization system for distributed memory parallel computers. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: A prototype static performance estimator is in its testing phase [FBZ92, Fah92]. No experimental results have been published. 8 3.1.6 ASPAR and P 3 C ASPAR is a compiler for the C language developed by the ParaSoft corporation <ref> [IFKF90] </ref>. P 3 C is a research Pascal compiler designed and implemented at the Tel-Aviv University by Gabber, Averbuch, and Yehudai [GAY91]. Both systems generate SPMD node programs that contain calls to communication library routines. The compilers perform only a simple form of program analysis to generate the correct communications.
Reference: [KKBP91] <author> D. Kulkarni, K. Kumar, A. Basu, and A. Paulraj. </author> <title> Loop partitioning for distributed memory multiprocessors as unimodular transformations. </title> <booktitle> In Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year> <month> 25 </month>
Reference-contexts: The iteration space is first partitioned into sets of iterations that can be executed independently. The data mapping is then determined by the iterations that are assigned to the different processors <ref> [RS89, Ram90, D'H89, KKBP91] </ref>. Other techniques for single loop nests are based on recognizing specific computation patterns in the loop, called stencils [SS90, HA90]. This more abstract representation is used to find a good data mapping.
Reference: [KLD92] <author> K. Knobe, J.D. Lukas, and W.J. Dally. </author> <title> Dynamic alignment on distributed memory systems. </title> <booktitle> In Proceedings of the Third Workshop on Compilers for Parallel Computers, </booktitle> <address> Vienna, Austria, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: More recently, Knobe, Lukas, and Dally introduced the concept of a control preference. A control preference exists between the corresponding dimensions of an array in a conditional expression and an array occurrence in an operation that is control dependent on this expression <ref> [KLD92] </ref>. The preferences of the program are represented by the undirected preference graph where the arcs correspond to the preferences and the nodes are dimensions of textual occurrences of arrays and array sections.
Reference: [KLS88] <author> K. Knobe, J. Lukas, and G. Steele, Jr. </author> <title> Massively parallel data optimization. </title> <booktitle> In Frontiers88: The 2nd Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <address> Fairfax, VA, </address> <month> October </month> <year> 1988. </year>
Reference-contexts: gradient program. 3.2 SIMD Machines 3.2.1 Albert, Knobe, Lukas, Natarajan, Steele, and Weiss at Compass and Thinking Machines Albert, Knobe, Lukas, Natarajan, Steele, and Weiss discuss automatic data layout as part of the design and implementation at Compass of SIMD compilers for Fortran 77 extended by Fortran 8x array features <ref> [AKLS88, KLS88, KLS90, KN90, Wei91] </ref>. The target machines are the Connection Machine CM-2 and the MasPar MP-1. Automatic data layout is an integral part of these compilers. Arrays are aligned by mapping them onto virtual processors based on their usage as opposed to a their declared shape. <p> The alignment algorithm is based on the usage patterns of arrays and Fortran 8x array sections in the source program. Each pattern generates allocation requests, called preferences, that indicate the optimal layout of the arrays relative to each other <ref> [KLS88, KLS90] </ref>. An identity preference exists between corresponding dimensions of a definition and a use of the same array. It describes a preference to allocate identical elements of the array on the same processors for the two textual occurrences.
Reference: [KLS90] <author> K. Knobe, J. Lukas, and G. Steele, Jr. </author> <title> Data optimization: Allocation of arrays to reduce communication on SIMD machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8(2) </volume> <pages> 102-118, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: Dynamic realignment or redistribution is not considered, but inter-procedural performance analysis is performed. Alignment analysis is done at compile time based on the approach by Knobe, Lukas, and Steele <ref> [KLS90] </ref> (see Section 3.2.1). Distribution analysis is performed at run-time. Each primitive operation is associated with a cost function that computes the execution time of the operation under a given distribution, problem size, machine size, and machine topology. <p> gradient program. 3.2 SIMD Machines 3.2.1 Albert, Knobe, Lukas, Natarajan, Steele, and Weiss at Compass and Thinking Machines Albert, Knobe, Lukas, Natarajan, Steele, and Weiss discuss automatic data layout as part of the design and implementation at Compass of SIMD compilers for Fortran 77 extended by Fortran 8x array features <ref> [AKLS88, KLS88, KLS90, KN90, Wei91] </ref>. The target machines are the Connection Machine CM-2 and the MasPar MP-1. Automatic data layout is an integral part of these compilers. Arrays are aligned by mapping them onto virtual processors based on their usage as opposed to a their declared shape. <p> The alignment algorithm is based on the usage patterns of arrays and Fortran 8x array sections in the source program. Each pattern generates allocation requests, called preferences, that indicate the optimal layout of the arrays relative to each other <ref> [KLS88, KLS90] </ref>. An identity preference exists between corresponding dimensions of a definition and a use of the same array. It describes a preference to allocate identical elements of the array on the same processors for the two textual occurrences. <p> To locate the cycles, a spanning tree is constructed, using a greedy algorithm that chooses the next arc to add by finding the highest cost arc that is not already processed. If a cycle-creating arc induces a conflict, the corresponding preference will not be honored <ref> [KLS90] </ref>. Knobe and Natara-jan have extended this algorithm to optimize the communication resulting from unhonored identity and conformance preferences [KN90]. For the MasPar machine, the data allocation functions generated by the data optimization component have to be transformed from mappings based on virtual processors to mappings based on physical processors. <p> In case (C), arrays b and c have to be transposed. In contrast, only the transpose of temp is necessary in case (D). We intend to build on the inter-dimensional and intra-dimensional alignment techniques of Li and Chen [LC90a], Knobe et al. <ref> [KLS90] </ref>, and Chatterjee, Gilbert, Schreiber, and Teng [CGST93]. The alignment problems can be formulated as an optimization problem on an undirected, weighted 17 graph. Some instances of the problem of alignment have been shown to be NP-complete [LC90a, CGST93].
Reference: [KM91] <author> C. Koelbel and P. Mehrotra. </author> <title> Compiling global name-space parallel loops for distributed execution. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 440-451, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: As a result, it should be easy to use by computational scientists. In addition, we believe that the two-phase strategy for specifying data decomposition is natural and conducive to writing modular and portable code. Fortran D bears similarities to both CM Fortran [TMC89] and Kali <ref> [KM91] </ref>. The complete language is described in detail elsewhere [FHK + 90]. 2.2 The Compilation System A Fortran D compilation system translates a Fortran D program into a Fortran 77 SPMD node program that contains calls to library primitives for interprocessor communication.
Reference: [KMM91] <author> K. Kennedy, N. M c Intosh, and K. S. M c Kinley. </author> <title> Static performance estimation. </title> <type> Technical Report TR91-174, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> December </month> <year> 1991. </year>
Reference-contexts: Our experience with applying the prototype estimator to a point-wise red-black relaxation routine is given in Appendix C. In the context of shared-memory programming, Kennedy, McIntosh, and McKinley have used our training set approach to estimate the performance of entire programs with do-loop parallelism <ref> [KMM91] </ref>. 4.3.2 Compiler Module The compiler module forms the second part of the static performance estimator. It predicts the performance of a program at the source level for a set of data decomposition schemes.
Reference: [KMT91] <author> K. Kennedy, K. S. M c Kinley, and C. Tseng. </author> <title> Interactive parallel programming using the ParaS-cope Editor. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 329-341, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: a tool has been recognized in the final report of the findings of the Pasadena Workshop on System Software and Tools for High-Performance Computing Environments [SMC + 92]. 4.1.4 Implementation The proposed automatic techniques will be implemented as part of the ParaScope parallel programming environment adapted to distributed memory multiprocessors <ref> [BKK + 89, KMT91, BFKK90, HKK + 91] </ref>. An overview of the system is shown in Figure 2. The system is part of the D Environment currently under development at Rice University [CCH + 92].
Reference: [KMV90] <author> C. Koelbel, P. Mehrotra, and J. Van Rosendale. </author> <title> Supporting shared data structures on distributed memory machines. </title> <booktitle> In Proceedings of the Second ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Seattle, WA, </address> <month> March </month> <year> 1990. </year>
Reference: [KN90] <author> K. Knobe and V. Natarajan. </author> <title> Data optimization: Minimizing residual interprocessor data motion on SIMD machines. </title> <booktitle> In Frontiers90: The 3rd Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <address> College Park, MD, </address> <month> October </month> <year> 1990. </year>
Reference-contexts: gradient program. 3.2 SIMD Machines 3.2.1 Albert, Knobe, Lukas, Natarajan, Steele, and Weiss at Compass and Thinking Machines Albert, Knobe, Lukas, Natarajan, Steele, and Weiss discuss automatic data layout as part of the design and implementation at Compass of SIMD compilers for Fortran 77 extended by Fortran 8x array features <ref> [AKLS88, KLS88, KLS90, KN90, Wei91] </ref>. The target machines are the Connection Machine CM-2 and the MasPar MP-1. Automatic data layout is an integral part of these compilers. Arrays are aligned by mapping them onto virtual processors based on their usage as opposed to a their declared shape. <p> Each virtual processor holds at most a single element of each array. The alignment algorithm performs intra-dimensional alignment and inter-dimensional alignment using similar techniques as Li and Chen (see Section 3.1.1). However, inter-dimensional permutations are not supported. Arrays may be mapped differently in different sections of the program <ref> [KN90] </ref>. The described techniques work only on single procedures. However, they handle complex control flow. Since the CM-2 supports the concept of virtual processors through its programming environment, data alignment is sufficient to specify the data layout. In contrast, the MasPar machine does not support virtual processors. <p> If a cycle-creating arc induces a conflict, the corresponding preference will not be honored [KLS90]. Knobe and Natara-jan have extended this algorithm to optimize the communication resulting from unhonored identity and conformance preferences <ref> [KN90] </ref>. For the MasPar machine, the data allocation functions generated by the data optimization component have to be transformed from mappings based on virtual processors to mappings based on physical processors. Weiss discusses three distribution schemes, namely cyclic (horizontal), block (vertical), and block-cyclic distributions [Wei91].
Reference: [KZBG88] <author> U. Kremer, H. Zima, H.-J. Bast, and M. Gerndt. </author> <title> Advanced tools and techniques for automatic parallelization. </title> <journal> Parallel Computing, </journal> <volume> 7 </volume> <pages> 387-393, </pages> <year> 1988. </year>
Reference-contexts: A number of researchers have proposed annotating a language based on a global name space with directives specifying how the data should be mapped onto the distributed memory machine <ref> [CK88, CCL88, 1 KMV90, PSvG91, RA90, RP89, RSW88, ZBG88, KZBG88, Ger89] </ref>. This approach was inspired by the observation that the most demanding intellectual step in writing a program for a distributed memory machine is the appropriate data layout the rest is straightforward but tedious and error prone work.
Reference: [Lam74] <author> L. Lamport. </author> <title> The parallel execution of DO loops. </title> <journal> Communications of the ACM, </journal> <volume> 17(2) </volume> <pages> 83-93, </pages> <month> February </month> <year> 1974. </year>
Reference-contexts: The following subsection contains a brief description of our overall research plan, the remainder discusses each research problem in more detail. 12 4.1 Overview The proposed tool will focus on regular problems which are loosely synchronous or result in wavefront style computations <ref> [Lam74] </ref>. Loosely synchronous problems represent a large class of scientific computations [FJL + 88]. <p> Intra-Phase distribution analysis follows alignment analysis. It applies heuristics to prune unprofitable choices in the search space of possible distributions for each single phase. Distribution analysis is compiler, machine, and problem dependent. For instance, the compiler may not be able to generate efficient wavefront computations <ref> [Lam74] </ref> for a subset of distributions. Transformations such as loop interchange and strip-mining can substantially improve the degree of parallelism induced by the wavefront [HKT91]. A consideration in our pruning heuristic are the sizes of the dimensions of the decomposition.
Reference: [LC90a] <author> J. Li and M. Chen. </author> <title> Index domain alignment: Minimizing cost of cross-referencing between distributed arrays. </title> <booktitle> In Frontiers90: The 3rd Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <address> College Park, MD, </address> <month> October </month> <year> 1990. </year>
Reference-contexts: of this section we will only present related work that discusses automatic data decomposition for entire subroutines or whole programs. 3.1.1 Li, Chen, and Choo at Yale University Li, Chen, and Choo investigated techniques for automatic data layout as part of the Crystal compiler and language project at Yale University <ref> [CCL89, LC90a, LC91b, LC91a, LC90b] </ref>. Crystal is a high-level, purely functional language. It does not contain statements that specify the data layout. The goal of the Crystal compiler is to generate efficient SPMD node programs with explicit communications or synchronization for a variety of massively parallel machines. <p> The goal is to find a partitioning that minimizes the overall sum of weights of edges between nodes in distinct partitions. Note that edges between partitions are alignment requests that cannot be satisfied. The solution of this alignment problem is shown to be NP-complete <ref> [LC90a] </ref>. The intra-dimensional alignment algorithm is based on the affine reference patterns and is straight forward. In the next compiler step the functional program is transformed into an imperative program that allows multiple assignments into the same memory location in order to ensure efficient reuse of memory. <p> Once a distribution strategy is chosen, redundant communication is eliminated. A prototype of the compiler has been implemented as part of Li's Ph.D. thesis at Yale University. Experimental results are reported for a heuristic algorithm that performs inter-dimensional alignment on a set of randomly generated component affinity graphs <ref> [LC90a] </ref>. Distribution analysis has not been implemented. <p> The compiler does not perform inter-procedural analysis. A single, static decomposition scheme is derived for the entire program, i.e. dynamic realignment or redistribution are not supported. The compiler performs alignment analysis based on Li's and Chen' s approach <ref> [LC90a] </ref> (see Section 3.1.1). The communication cost of each statement with an array reference is expressed as a function of the machine size, number of processors in each dimension, and the method of partitioning, namely block or cyclic. <p> In case (C), arrays b and c have to be transposed. In contrast, only the transpose of temp is necessary in case (D). We intend to build on the inter-dimensional and intra-dimensional alignment techniques of Li and Chen <ref> [LC90a] </ref>, Knobe et al. [KLS90], and Chatterjee, Gilbert, Schreiber, and Teng [CGST93]. The alignment problems can be formulated as an optimization problem on an undirected, weighted 17 graph. Some instances of the problem of alignment have been shown to be NP-complete [LC90a, CGST93]. <p> The alignment problems can be formulated as an optimization problem on an undirected, weighted 17 graph. Some instances of the problem of alignment have been shown to be NP-complete <ref> [LC90a, CGST93] </ref>. One major challenge in our proposed work will be to define the appropriate weights of the graph and to develop an interprocedural algorithm that solves the alignment problem in a way that is suitable for loosely synchronous problems.
Reference: [LC90b] <author> J. Li and M. Chen. </author> <title> Synthesis of explicit communication from shared-memory program references. </title> <type> Technical Report YALEU/DCS/TR-755, </type> <institution> Dept. of Computer Science, Yale University, </institution> <address> New Haven, CT, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: of this section we will only present related work that discusses automatic data decomposition for entire subroutines or whole programs. 3.1.1 Li, Chen, and Choo at Yale University Li, Chen, and Choo investigated techniques for automatic data layout as part of the Crystal compiler and language project at Yale University <ref> [CCL89, LC90a, LC91b, LC91a, LC90b] </ref>. Crystal is a high-level, purely functional language. It does not contain statements that specify the data layout. The goal of the Crystal compiler is to generate efficient SPMD node programs with explicit communications or synchronization for a variety of massively parallel machines. <p> The compiler module employs a compiler-level training set written in the source language that consists of common computation patterns and program kernels such as stencil computations and matrix multiplication. Note that in contrast to pattern matching algorithms as they are used in compilers for optimization or code generation <ref> [ASU86, LC90b, PP91] </ref>, our pattern matcher does not need to preserve the semantics of the program. Program segments are considered `equivalent' if their corresponding compiler generated code exhibit a similar performance behavior.
Reference: [LC91a] <author> J. Li and M. Chen. </author> <title> Compiling communication-efficient programs for massively parallel machines. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 361-376, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: of this section we will only present related work that discusses automatic data decomposition for entire subroutines or whole programs. 3.1.1 Li, Chen, and Choo at Yale University Li, Chen, and Choo investigated techniques for automatic data layout as part of the Crystal compiler and language project at Yale University <ref> [CCL89, LC90a, LC91b, LC91a, LC90b] </ref>. Crystal is a high-level, purely functional language. It does not contain statements that specify the data layout. The goal of the Crystal compiler is to generate efficient SPMD node programs with explicit communications or synchronization for a variety of massively parallel machines.
Reference: [LC91b] <author> J. Li and M. Chen. </author> <title> The data alignment phase in compiling programs for distributed-memory machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13(4) </volume> <pages> 213-221, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: of this section we will only present related work that discusses automatic data decomposition for entire subroutines or whole programs. 3.1.1 Li, Chen, and Choo at Yale University Li, Chen, and Choo investigated techniques for automatic data layout as part of the Crystal compiler and language project at Yale University <ref> [CCL89, LC90a, LC91b, LC91a, LC90b] </ref>. Crystal is a high-level, purely functional language. It does not contain statements that specify the data layout. The goal of the Crystal compiler is to generate efficient SPMD node programs with explicit communications or synchronization for a variety of massively parallel machines.
Reference: [Li92] <author> J. Li. </author> <title> Private communication. </title> <year> 1992. </year>
Reference-contexts: Experimental results are reported for a heuristic algorithm that performs inter-dimensional alignment on a set of randomly generated component affinity graphs [LC90a]. Distribution analysis has not been implemented. The distribution strategy is read in at runtime <ref> [Li92] </ref>. 3.1.2 Banerjee and Gupta at the University of Illinois Gupta and Banerjee at the University of Illinois at Urbana-Champaign developed techniques for automatic data layout as part of a compiler based on the Parafrase-2 program restructurer [GB90, GB91, GB92].
Reference: [MFvL + 92] <author> A. Mohamed, G. Fox, G. von Laszewski, M. Parashar, T. Haupt, K. Mills, Y-H. Lu, N-T. Lin, and N-K. Yeh. </author> <title> Applications benchmark set for Fortran D and High Performance Fortran. </title> <type> 26 Technical Report SCCS-327, </type> <institution> NPAC, Syracuse University, </institution> <month> October </month> <year> 1992. </year>
Reference-contexts: I will validate my thesis by applying my tool to a test suite of programs and program kernels written in a data-parallel programming style. I will use a benchmark suite developed by Geoffrey Fox at Syracuse <ref> [MFvL + 92] </ref> and a set of real application programs. If automatic techniques will fail to generate data decomposition schemes that are close to optimal, I want to answer the question how user interaction can help to overcome the deficiencies of automatic techniques. <p> We will use a benchmark suite being developed by Geoffrey Fox at Syracuse that consists of a collection of Fortran programs and program kernels written in a data-parallel programming style 13 Sets Training Estimator Performance Static Partitioner Data Automatic EnvironmentFortranUser Fortran D Fortran D Compiler Message Passing Fortran <ref> [MFvL + 92] </ref>. In addition, we will examine parts of the version of UTCOMP that is written in a data-parallel programming style. <p> Fox's program suite consists of a collection of Fortran programs and program kernels and is described in detail in <ref> [MFvL + 92] </ref>.
Reference: [PP91] <author> S. Pinter and R. Pinter. </author> <title> Program optimization and parallelization using idioms. </title> <booktitle> In Proceedings of the Eighteenth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Orlando, FL, </address> <month> January </month> <year> 1991. </year>
Reference-contexts: The compiler module employs a compiler-level training set written in the source language that consists of common computation patterns and program kernels such as stencil computations and matrix multiplication. Note that in contrast to pattern matching algorithms as they are used in compilers for optimization or code generation <ref> [ASU86, LC90b, PP91] </ref>, our pattern matcher does not need to preserve the semantics of the program. Program segments are considered `equivalent' if their corresponding compiler generated code exhibit a similar performance behavior.
Reference: [PSvG91] <author> E. Paalvast, H. Sips, and A. van Gemund. </author> <title> Automatic parallel program generation and optimization from data decompositions. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: A number of researchers have proposed annotating a language based on a global name space with directives specifying how the data should be mapped onto the distributed memory machine <ref> [CK88, CCL88, 1 KMV90, PSvG91, RA90, RP89, RSW88, ZBG88, KZBG88, Ger89] </ref>. This approach was inspired by the observation that the most demanding intellectual step in writing a program for a distributed memory machine is the appropriate data layout the rest is straightforward but tedious and error prone work.
Reference: [RA90] <author> R. Ruhl and M. Annaratone. </author> <title> Parallelization of fortran code on distributed-memory parallel processors. </title> <booktitle> In Proceedings of the 1990 ACM International Conference on Supercomputing, </booktitle> <address> Amsterdam, The Netherlands, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: A number of researchers have proposed annotating a language based on a global name space with directives specifying how the data should be mapped onto the distributed memory machine <ref> [CK88, CCL88, 1 KMV90, PSvG91, RA90, RP89, RSW88, ZBG88, KZBG88, Ger89] </ref>. This approach was inspired by the observation that the most demanding intellectual step in writing a program for a distributed memory machine is the appropriate data layout the rest is straightforward but tedious and error prone work.
Reference: [Ram90] <author> J. Ramanujam. </author> <title> Compile-time Techniques for Parallel Execution of Loops on Distributed Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> Department of Computer and Information Science, Ohio State University, Columbus, OH, </institution> <year> 1990. </year>
Reference-contexts: The iteration space is first partitioned into sets of iterations that can be executed independently. The data mapping is then determined by the iterations that are assigned to the different processors <ref> [RS89, Ram90, D'H89, KKBP91] </ref>. Other techniques for single loop nests are based on recognizing specific computation patterns in the loop, called stencils [SS90, HA90]. This more abstract representation is used to find a good data mapping.
Reference: [RP89] <author> A. Rogers and K. Pingali. </author> <title> Process decomposition through locality of reference. </title> <booktitle> In Proceedings of the SIGPLAN '89 Conference on Program Language Design and Implementation, </booktitle> <address> Portland, OR, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: A number of researchers have proposed annotating a language based on a global name space with directives specifying how the data should be mapped onto the distributed memory machine <ref> [CK88, CCL88, 1 KMV90, PSvG91, RA90, RP89, RSW88, ZBG88, KZBG88, Ger89] </ref>. This approach was inspired by the observation that the most demanding intellectual step in writing a program for a distributed memory machine is the appropriate data layout the rest is straightforward but tedious and error prone work.
Reference: [RS89] <author> J. Ramanujam and P. Sadayappan. </author> <title> A methodology for parallelizing programs for multicom-puters and complex memory multiprocessors. </title> <booktitle> In Proceedings of Supercomputing '89, </booktitle> <address> Reno, NV, </address> <month> November </month> <year> 1989. </year>
Reference-contexts: The iteration space is first partitioned into sets of iterations that can be executed independently. The data mapping is then determined by the iterations that are assigned to the different processors <ref> [RS89, Ram90, D'H89, KKBP91] </ref>. Other techniques for single loop nests are based on recognizing specific computation patterns in the loop, called stencils [SS90, HA90]. This more abstract representation is used to find a good data mapping.
Reference: [RSW88] <author> M. Rosing, R. Schnabel, and R. Weaver. Dino: </author> <title> Summary and examples. </title> <type> Technical Report CU-CS-386-88, </type> <institution> Dept. of Computer Science, University of Colorado, </institution> <month> March </month> <year> 1988. </year>
Reference-contexts: A number of researchers have proposed annotating a language based on a global name space with directives specifying how the data should be mapped onto the distributed memory machine <ref> [CK88, CCL88, 1 KMV90, PSvG91, RA90, RP89, RSW88, ZBG88, KZBG88, Ger89] </ref>. This approach was inspired by the observation that the most demanding intellectual step in writing a program for a distributed memory machine is the appropriate data layout the rest is straightforward but tedious and error prone work.
Reference: [SMC + 92] <author> T. Sterling, P. Messina, M. Chen, F. Darema, G. Fox, M. Heath, K. Kennedy, R. Knighten, R. Moore, S. Ranka, J. Saltz, L. Tucker, and P. Woodward. </author> <title> Workshop on system software and tools for high performance computing environments. </title> <booktitle> In Final Report on the Findings of the Workshop, </booktitle> <address> Pasadena, CA, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: The usefulness of such a tool has been recognized in the final report of the findings of the Pasadena Workshop on System Software and Tools for High-Performance Computing Environments <ref> [SMC + 92] </ref>. 4.1.4 Implementation The proposed automatic techniques will be implemented as part of the ParaScope parallel programming environment adapted to distributed memory multiprocessors [BKK + 89, KMT91, BFKK90, HKK + 91]. An overview of the system is shown in Figure 2.
Reference: [SS90] <author> L. Snyder and D. Socha. </author> <title> An algorithm producing balanced partitionings of data arrays. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: The data mapping is then determined by the iterations that are assigned to the different processors [RS89, Ram90, D'H89, KKBP91]. Other techniques for single loop nests are based on recognizing specific computation patterns in the loop, called stencils <ref> [SS90, HA90] </ref>. This more abstract representation is used to find a good data mapping. While these approaches will minimize the necessary communication in single loop nests, it is not clear whether a compiler can generate efficient node programs for the loop nests or between adjacent loop nests.
Reference: [Sus91] <author> A. Sussman. </author> <title> Model-Driven Mapping onto Distributed Memory Parallel Computers. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: performance of different data layouts is demonstrated by comparing the estimated costs with the actual execution times of a simplex program on a CM-2. 3.1.4 Sussman at Carnegie Mellon University Sussman discusses static performance estimation to guide the mapping of data and computation onto distributed-memory machines in an automatic compiler <ref> [Sus92, Sus91] </ref>. His work focuses on determining efficient data and computation mappings for programs consisting of single loop nests. A program is classified as either a sequential loop, a sequentially iterated parallel loop, or a parallel loop.
Reference: [Sus92] <author> A. Sussman. </author> <title> Model-driven mapping onto distributed memory parallel computers. </title> <booktitle> In Proceedings of Supercomputing '92, </booktitle> <address> Minneapolis, MN, </address> <month> November </month> <year> 1992. </year>
Reference-contexts: performance of different data layouts is demonstrated by comparing the estimated costs with the actual execution times of a simplex program on a CM-2. 3.1.4 Sussman at Carnegie Mellon University Sussman discusses static performance estimation to guide the mapping of data and computation onto distributed-memory machines in an automatic compiler <ref> [Sus92, Sus91] </ref>. His work focuses on determining efficient data and computation mappings for programs consisting of single loop nests. A program is classified as either a sequential loop, a sequentially iterated parallel loop, or a parallel loop.
Reference: [Tar74] <author> R. E. Tarjan. </author> <title> Testing flow graph reducibility. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 9 </volume> <pages> 355-365, </pages> <year> 1974. </year>
Reference-contexts: The merging of phases in a strongly connected component of the phase control flow graph should be done before merging any of its phases with a phase outside of the strongly connected component. This suggests a hierarchical algorithm for merging phases based on, for example, Tarjan intervals <ref> [Tar74] </ref>. Assuming that the innermost loop bodies can be represented by a linear phase control flow subgraph, the merging problem is solved by adding a shadow copy of the first phase after the last phase in the linear subgraph keeping the subgraph acyclic.
Reference: [TMC89] <institution> Thinking Machines Corporation, </institution> <address> Cambridge, MA. </address> <note> CM Fortran Reference Manual, version 5.2-0.6 edition, </note> <month> September </month> <year> 1989. </year>
Reference-contexts: As a result, it should be easy to use by computational scientists. In addition, we believe that the two-phase strategy for specifying data decomposition is natural and conducive to writing modular and portable code. Fortran D bears similarities to both CM Fortran <ref> [TMC89] </ref> and Kali [KM91]. The complete language is described in detail elsewhere [FHK + 90]. 2.2 The Compilation System A Fortran D compilation system translates a Fortran D program into a Fortran 77 SPMD node program that contains calls to library primitives for interprocessor communication.
Reference: [Tse93] <author> C. Tseng. </author> <title> An Optimizing Fortran D Compiler for MIMD Distributed-Memory Machines. </title> <type> PhD thesis, </type> <institution> Rice University, Houston, TX, </institution> <month> January </month> <year> 1993. </year> <institution> Rice COMP TR93-199. </institution>
Reference-contexts: Procedure cloning or inlining may be applied under certain conditions to improve context for optimization. A Fortran D compiler may relax the owner computes rule for reductions and parallel prefix operations, and for scalars or arrays that are recognized to be temporaries <ref> [HKT91, HKT92a, HHKT91, Tse93] </ref>. Node compilers may perform optimizations to exploit the memory hierarchy and instruction-level parallelism available on the target node processor [Car92, Wol92, Bri92]. Our tool for automatic data decomposition is based on a specific Fortran D compilation system representing state-of-the-art compiler technology. <p> Our tool for automatic data decomposition is based on a specific Fortran D compilation system representing state-of-the-art compiler technology. Note that the optimizations performed by the Fortran D compiler are not restricted to the ones implemented in the current prototype of the Fortran D compiler <ref> [Tse93] </ref>. Our compilation system will target a variety of distributed-memory multiprocessors such as Intel's iPSC/860 and Paragon, Ncube's Ncube-1 and Ncube-2, and Thinking Machine Corporation's CM-5. 2.3 The Programming Style Our tool takes sequential Fortran 77 programs that solve regular problems as input.
Reference: [Wei91] <author> M. Weiss. </author> <title> Strip mining on SIMD architectures. </title> <booktitle> In Proceedings of the 1991 ACM International 27 Conference on Supercomputing, </booktitle> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: gradient program. 3.2 SIMD Machines 3.2.1 Albert, Knobe, Lukas, Natarajan, Steele, and Weiss at Compass and Thinking Machines Albert, Knobe, Lukas, Natarajan, Steele, and Weiss discuss automatic data layout as part of the design and implementation at Compass of SIMD compilers for Fortran 77 extended by Fortran 8x array features <ref> [AKLS88, KLS88, KLS90, KN90, Wei91] </ref>. The target machines are the Connection Machine CM-2 and the MasPar MP-1. Automatic data layout is an integral part of these compilers. Arrays are aligned by mapping them onto virtual processors based on their usage as opposed to a their declared shape. <p> Since the CM-2 supports the concept of virtual processors through its programming environment, data alignment is sufficient to specify the data layout. In contrast, the MasPar machine does not support virtual processors. The virtual processors have to be mapped onto the physical processors explicitly <ref> [Wei91] </ref>. The alignment algorithm is based on the usage patterns of arrays and Fortran 8x array sections in the source program. Each pattern generates allocation requests, called preferences, that indicate the optimal layout of the arrays relative to each other [KLS88, KLS90]. <p> For the MasPar machine, the data allocation functions generated by the data optimization component have to be transformed from mappings based on virtual processors to mappings based on physical processors. Weiss discusses three distribution schemes, namely cyclic (horizontal), block (vertical), and block-cyclic distributions <ref> [Wei91] </ref>. Most of the described work has been implemented as part of the CM-2 and MP-1 Fortran compilers developed at Compass. The authors report significant performance improvements of up to a factor of 60 due to using the compiler generated data mapping instead of the naive, canonical mapping.
Reference: [Who91] <author> S. Wholey. </author> <title> Automatic Data Mapping for Distributed-Memory Parallel Computers. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: Actual performance figures for the generated data layout schemes are only given for tred2 on an iPSC/2 hypercube system. The automatic layout performs well compared to three other data mappings. 3.1.3 Wholey at Carnegie Mellon University Wholey at Carnegie Mellon University <ref> [Who92a, Who91] </ref> developed a compiler for the high-level, block structured, non-recursive language ALEXI. Communication and parallelism is expressed explicitly by primitive operations that are similar to Fortran90 array constructs and intrinsic communication functions.
Reference: [Who92a] <author> S. Wholey. </author> <title> Automatic data mapping for distributed-memory parallel computers. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <address> Washington, DC, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Actual performance figures for the generated data layout schemes are only given for tred2 on an iPSC/2 hypercube system. The automatic layout performs well compared to three other data mappings. 3.1.3 Wholey at Carnegie Mellon University Wholey at Carnegie Mellon University <ref> [Who92a, Who91] </ref> developed a compiler for the high-level, block structured, non-recursive language ALEXI. Communication and parallelism is expressed explicitly by primitive operations that are similar to Fortran90 array constructs and intrinsic communication functions.
Reference: [Who92b] <author> S. Wholey. </author> <title> Private communication. </title> <year> 1992. </year>
Reference-contexts: The performance of automatically determined data layouts has not been compared with the optimal possible <ref> [Who92b] </ref>.
Reference: [Wol89] <author> M. J. Wolfe. </author> <title> Semi-automatic domain decomposition. </title> <booktitle> In Proceedings of the 4th Conference on Hypercube Concurrent Computers and Applications, </booktitle> <address> Monterey, CA, </address> <month> March </month> <year> 1989. </year>
Reference-contexts: A program is written in a data-parallel programming style if it allows advanced compilation systems to generate efficient code for most distributed-memory machines. In the context of vectorization, the existence of a usable programming style has been partially responsible for the success of automatic vectorization <ref> [CKK89, Wol89] </ref>. I do not believe that such fully automatic techniques will be successful for parallelizing `dusty deck' programs. To support my thesis I will build a prototype tool that takes a Fortran 77 program as input and generates a Fortran D program.
Reference: [Wol92] <author> M.E. Wolf. </author> <title> Improving Locality and Parallelism in Nested Loops. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> August </month> <year> 1992. </year>
Reference-contexts: Node compilers may perform optimizations to exploit the memory hierarchy and instruction-level parallelism available on the target node processor <ref> [Car92, Wol92, Bri92] </ref>. Our tool for automatic data decomposition is based on a specific Fortran D compilation system representing state-of-the-art compiler technology. Note that the optimizations performed by the Fortran D compiler are not restricted to the ones implemented in the current prototype of the Fortran D compiler [Tse93].
Reference: [ZBG88] <author> H. Zima, H.-J. Bast, and M. Gerndt. </author> <title> SUPERB: A tool for semi-automatic MIMD/SIMD par-allelization. </title> <journal> Parallel Computing, </journal> <volume> 6 </volume> <pages> 1-18, </pages> <year> 1988. </year> <month> 28 </month>
Reference-contexts: A number of researchers have proposed annotating a language based on a global name space with directives specifying how the data should be mapped onto the distributed memory machine <ref> [CK88, CCL88, 1 KMV90, PSvG91, RA90, RP89, RSW88, ZBG88, KZBG88, Ger89] </ref>. This approach was inspired by the observation that the most demanding intellectual step in writing a program for a distributed memory machine is the appropriate data layout the rest is straightforward but tedious and error prone work.
References-found: 79

