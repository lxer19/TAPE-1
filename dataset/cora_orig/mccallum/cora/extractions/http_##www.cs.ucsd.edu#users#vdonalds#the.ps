URL: http://www.cs.ucsd.edu/users/vdonalds/the.ps
Refering-URL: http://www.cs.ucsd.edu/users/vdonalds/
Root-URL: http://www.cs.ucsd.edu
Title: Asynchronous Pipeline Analysis and Scheduling  
Author: Professor Francine D. Berman Professor Samuel R. Buss Professor Larry Carter Professor William G. Griswold Professor Tao Yang 
Degree: A dissertation submitted in partial satisfaction of the requirements for the degree Doctor of Philosophy in Computer Science by Val Donaldson Committee in charge: Professor Jeanne Ferrante, Chair  
Date: 1997  
Affiliation: UNIVERSITY OF CALIFORNIA, SAN DIEGO  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Alexander Aiken and Alexandru Nicolau. </author> <title> Optimal loop parallelization. </title> <booktitle> Proc. SIGPLAN '88 Conference on Programming Language Design and Implementation, </booktitle> <address> Atlanta, GA, </address> <month> June </month> <year> 1988, </year> <pages> pp. 308-317. </pages>
Reference-contexts: 1 :moment increment; G 0 2 :moment increment ; : : : ; G 0 ` :moment increment (b) Integer values G 0 1 :stage increment; G 0 2 :stage increment; : : : ; G 0 ` :stage increment subject to the constraint that for any i; j 2 <ref> [1; `] </ref>, i 6= j, if there is a path in G 0 from G 0 j , then G 0 i :moment increment G 0 j :moment increment and G 0 i :stage increment G 0 j :stage increment 6. <p> G 0 :min scc mcr , the smallest maximum cycle ratio of any SCC in G 0 (after self-cycles have been deleted, but before modifying cycle edges), and if G 0 :min scc mcr &lt; G 0 :mcr , use scaling factors for all SCC's evenly distributed over the interval <ref> [1; G 0 :mcr =G 0 :min scc mcr ] </ref>, rounding scaling factors down to G 0 :mcr =G 0 i :mcr when the current scaling bound is greater than this value for a particular SCC G 0 i . <p> For any "moment expansion bound" mexp bound &gt; 1 we can perform multiple task sequencing trials by multiplying all task moments by values uniformly distributed over the interval <ref> [1; mexp bound ] </ref>. For example, to perform four trials with mexp bound = 1:3, the expansion factors for the trials would be 1, 1.1, 1.2, and 1.3. Using an expansion factor of 1 corresponds to choosing unmodified moments, so the schedule generated for this trial will not deadlock. <p> Table 9.4 shows the results for a multiple-pass kernel scheduling algorithm using a DDG increment of .1 and the first k task weight sets from Table 9.1, for k 2 <ref> [1; 10] </ref>, plus k = 24. In contrast to task sequencing, it appears to be more effective to use multiple DDG increments than to use multiple task weights for this range of scheduling trials. <p> 8 9 10 24 100 (b=b lb 1) 47.82 46.62 45.87 45.49 45.24 44.91 44.40 44.33 44.22 44.14 43.43 Table 9.4: Statistics for a multiple-pass kernel scheduling algorithm using SCC scheduling, DAG edge optimization, DDG increment .1, and the first k task weight sets from Table 9.1, for k 2 <ref> [1; 10] </ref>, plus k = 24. moment increments might be used to advantage. Relaxing deadlock constraints in a multiple-pass version of the algorithm might be useful. Even the techniques that we have explicitly introduced might be improved. <p> Question: Is there a partition V = V 1 [ V 2 [ [ V p of V into p disjoint sets such that for all i 2 <ref> [1; p] </ref>, X2V i X:time B? The best fit decreasing (BFD) algorithm [14, 22], is one of a family of bin packing algorithms that are guaranteed to use no more than 22% more processors than optimal. <p> Cycle edges that are critical or "almost critical" will be given positive critical ranks in later algorithm steps. Step 2 computes a "maximum critical edge gap" ce max gap in terms of a "critical edge multiplier" ce multiplier 2 <ref> [0; 1] </ref> and G 0 0 :mcr , the maximum cycle ratio of the DDG obtained from G 0 by zeroing the data communication times of all edges. G 0 0 :mcr is a lower bound on the iteration interval of any schedule. <p> In this case the algorithm in Section 10.4 below generates schedules that average within 52.58% of optimal. Table 10.1 shows 100 (b=b lb 1) summary values for the scheduling algorithm in Section 10.4, using selected values of ce class count &gt; 1 and ce multiplier 2 <ref> [0; 1] </ref>. The lowest values in each column are marked with asterisks. <p> For some value alb multiplier 2 <ref> [0; 1] </ref>, the Step 3.5 code first determines a lower bound B lb on the bin size parameter B.
Reference: [2] <author> Sati Banerjee, Takeo Hamada, Paul M. Chau, and Ronald D. Fellman. </author> <title> Macro pipelining based scheduling on high performance heterogeneous multiprocessor systems. </title> <journal> IEEE Transactions on Signal Processing 43:8 (June 1995), </journal> <pages> pp. 1468-1484. </pages>
Reference-contexts: 43.59 Table 9.3: Statistics for a multiple-pass kernel scheduling algorithm using SCC scheduling, DAG edge optimization, the balanced weight set (w est = w lct = 1=2 and w ect = w lst = 0), and k DDG increments uniformly distributed over the interval [0; (k 1)=k], for k 2 <ref> [2; 10] </ref>. # Trials 1 2 3 4 5 6 7 8 9 10 24 100 (b=b lb 1) 47.82 46.62 45.87 45.49 45.24 44.91 44.40 44.33 44.22 44.14 43.43 Table 9.4: Statistics for a multiple-pass kernel scheduling algorithm using SCC scheduling, DAG edge optimization, DDG increment .1, and the first <p> The third of these schedules is shown in Figure 8.1. The scheduling algorithm of Banerjee et al. <ref> [2] </ref> requires that the input DDG be a task graph. Figure 17 in [2] shows a pipeline schedule for a task graph with 16 tasks and nonzero data communication times on 4 processors, where the schedule executes in G:time n = 24 + 22n time units, which is 22% above the <p> The third of these schedules is shown in Figure 8.1. The scheduling algorithm of Banerjee et al. <ref> [2] </ref> requires that the input DDG be a task graph. Figure 17 in [2] shows a pipeline schedule for a task graph with 16 tasks and nonzero data communication times on 4 processors, where the schedule executes in G:time n = 24 + 22n time units, which is 22% above the optimal iteration interval of b = 18. <p> In Chapter 10 we also compared PA/TS scheduling algorithm performance to other scheduling algorithms, using available examples from the literature. It would nevertheless be useful to evaluate our conclusions in the context of DDG's derived from industry benchmarks or other programs. The discussions of pipelining in <ref> [2, 19] </ref> suggest that digital signal processing applications may be one class of programs that are often well suited to execution using asynchronous pipeline parallelism. The generation at runtime of pipeline schedules for sparse matrix computations as considered in [40] is another possibility. 18. <p> Heterogeneous processing | We have assumed that the processors available for implementing a pipeline schedule were identical, homogeneous processors. It is also possible to use heterogeneous processors to improve the quality of a schedule <ref> [2, 11] </ref>. Since processor or other forms of heterogeneity are primarily relevant in the processor assignment component of a schedule, the PA/TS scheduling algorithm appears to provide a good foundation for exploiting heterogeneous processing capabilities.
Reference: [3] <author> Adam Beguelin, Jack J. Dongarra, G. A. Geist, Robert Manchek, and V. S. Sunderam. </author> <title> Graphical development tools for network-based concurrent supercomputing. </title> <booktitle> Proc. Supercomputing '91, </booktitle> <address> Albuquerque, NM, </address> <month> November </month> <year> 1991, </year> <pages> pp. 435-444. </pages>
Reference: [4] <author> Steven M. Burns. </author> <title> Performance Analysis and Optimization of Asynchronous Circuits. </title> <type> Ph.D. Thesis, </type> <institution> California Institute of Technology, Pasadena, California, </institution> <year> 1991. </year>
Reference-contexts: For each test DDG, tasks are assigned to processors using Graham's O (v log v) time LPT multiprocessor scheduling algorithm [17]. Data communication times between tasks assigned to the same processor are zeroed. We use our variant of Burns' primal-dual algorithm <ref> [4] </ref> from Section 7.4 for computing the maximum cycle ratio of a DDG, modified to use the solution of a longest path problem to provide the initial feasible solution when the DDG has negative dependence distances, or when there is an upper bound on the maximum cycle ratio from earlier trials. <p> Maximum cycle ratio algorithms | We developed a new monotonic search algorithm, which appears to have several advantages over variants of the binary search algorithm in [28]. We proved several useful facts about the behavior of Burns' primal-dual algorithm <ref> [4] </ref>, which potentially increase its applicability.
Reference: [5] <author> Pierre-Yves Calland, Alain Darte, and Yves Robert. </author> <title> A new guaranteed heuristic for the software pipelining problem. </title> <booktitle> Proc. International Conference on Supercomputing, </booktitle> <address> Philadelphia, PA, </address> <month> May </month> <year> 1996, </year> <pages> pp. 261-269. </pages>
Reference-contexts: Both of these algorithms could potentially benefit from the retiming modifications described by Calland et al. <ref> [5] </ref>, which are described in the context of Gasperoni and Schwiegelshohn's algorithm, although the retiming modifications might be more effective when data communication times are zero. <p> We will informally justify each of the changes we suggest, but in contrast to our discussion of task sequencing, we will not formally prove their correctness. Note also that while the modifications we consider are different than those suggested by Calland et al. <ref> [5] </ref>, there is likely to be at least some overlap in the efficacy of the two sets of improvement techniques. The first such modification we introduce is post-optimization of DAG edges as in task sequencing. <p> The PA/TS algorithm generates an asymptotically optimal schedule that executes in G:time n = 31:58 + 41:80n time units. The running example in Calland et al. <ref> [5] </ref> is a cyclic DDG with 6 tasks, where data communication times are zero. Calland et al. note that the original algorithm of Gasperoni and Schwiegelshohn generates a schedule on 2 processors for this example that executes in G:time n = 5 + 25n time units. <p> The primary, O (ve log v + t mcr (G)) time version of the pipeline scheduling algorithm in <ref> [5] </ref> generates a schedule that executes in G:time n = 4 + 20 time units. <p> We introduced the notion of negative data dependence distances in an unnested loop, which may be useful in other contexts. Our use of retiming in this context may have additional implications as a loop transformation technique (see also <ref> [5, 32] </ref>). Scheduled DDG's provide the foundation for our generalization of previous work on determining whether or not an arbitrary schedule will deadlock. A schedule will deadlock if and only if the associated scheduled DDG violates the positive cycle constraint. <p> scheduling algorithm, analogous to the generic task sequencing algorithm, which can be used to tune and improve kernel scheduling algorithm performance, by allowing modifications such as DAG edge optimization, DDG increments, and task weights? Are there other useful modifications to the kernel scheduling algorithm? The kernel scheduling techniques considered in <ref> [5, 15, 39, 40] </ref>, as well as our discussion in Chapter 9 suggest that kernel scheduling is a reasonable approach to scheduling, but there is room for improvement to the theory underlying kernel scheduling, as well as opportunities for heuristic improvements. 14. <p> The generation at runtime of pipeline schedules for sparse matrix computations as considered in [40] is another possibility. 18. Loop unrolling | It is well-known that unrolling a loop before scheduling may improve the quality of the generated schedule <ref> [5, 32, 40] </ref>. This is particularly true in software pipelining, where unrolling can sometimes be used to reduce or eliminate the adverse effect of a nonintegral maximum cycle ratio in the input DDG on the quality of a pipeline schedule for a loop.
Reference: [6] <author> E. G. Coffman, Jr., M. R. Garey, and D. S. Johnson. </author> <title> An application of bin-packing to multiprocessor scheduling. </title> <journal> SIAM Journal on Computing 7:1 (February 1978), </journal> <pages> pp. 1-17. </pages>
Reference-contexts: Graham's "largest processing time first" (LPT) multiprocessor scheduling algorithm [17] has a performance guarantee of 33%; Coffman, Garey, and Johnson's MULTIFIT algorithm 99 <ref> [6] </ref> has a 20% guarantee; and Friesen and Langston's improved MULTIFIT algorithm [12] has an 18% guarantee. All of these algorithms execute in O (v log v) time, although the constants involved increase as the guarantee improves. These algorithms are described in more detail in Section 10.1. <p> We conjecture that this is always the case, and further conjecture that startup times for scheduling in terms of tasks will typically be smaller than when SCC's are scheduled directly. The MULTIFIT multiprocessor scheduling algorithm from Coffman et al. <ref> [6] </ref> has a 20% performance guarantee, and is simpler to implement than the improved version of the algorithm in Friesen and Langston [12], which has an 18% guarantee (see Section 10.1 for a description of these algorithms). <p> The BFD bin packing algorithm may also be used as a subroutine in an algorithm for solving the multiprocessor scheduling problem. Figure 10.3 is the code for the MULTIFIT multiprocessor scheduling algorithm of Coffman et al. <ref> [6] </ref>. Step 1 of the MULTIFIT algorithm initializes lower and upper bounds on the bin size bound used in calls to the BFD subalgorithm.
Reference: [7] <author> F. Commoner, A. W. Holt, S. Even, and A. Pnueli. </author> <title> Marked directed graphs. </title> <journal> Journal of Computer and System Sciences 5:5 (October 1971), </journal> <pages> pp. 511-523. </pages>
Reference: [8] <author> Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: For any SCC G 0 i , if we temporarily undirect the directed edges in G 0 i we can then partition the edges in G 0 i into biconnected components <ref> [8] </ref>, which are subcomponents of G 0 i that have at most one task in common with each of the other subcomponents. <p> We may be able to speed up algorithm execution slightly if we preprocess the first p tasks by assigning each of them to a distinct processor. If v p, assigning one task per processor is an optimal solution. If we maintain a priority queue <ref> [8] </ref> for processors, using the current sum of task times on a processor as the priority key, the loop in Step 3 requires O (v log p) time. The entire algorithm therefore executes in O (v log v) time, due to the sort in Step 1. <p> The algorithm exits with a false return value if no processor has at least X:time capacity remaining. If the algorithm reaches Step 4, the assignment of tasks to processors was successful. If we use a balanced binary tree such as a red-black tree <ref> [8] </ref> to maintain remaining capacities for processors, Step 3 can be performed in O (v log p) time. The entire algorithm therefore executes in O (v log v) time due to the sort in Step 1.
Reference: [9] <author> G.B. Dantzig, W.O. Blattner, and M.R. Rao. </author> <title> Finding a cycle in a graph with minimum cost to time ratio with application to a ship routing problem. </title> <editor> In P. Rosenstiehl (Ed.), </editor> <booktitle> Theory of Graphs, </booktitle> <address> Dunod, Paris, </address> <publisher> and Gordon and Breach, </publisher> <address> New York, NY, </address> <year> 1967, </year> <pages> pp. 77-83. </pages>
Reference: [10] <author> Goran L. Djordjevic and Milorad B. Tosic. </author> <title> A heuristic for scheduling task graphs with communication delays onto multiprocessors. </title> <booktitle> Parallel Computing 22:9 (November 1996), </booktitle> <pages> pp. 1197-1214. </pages>
Reference-contexts: We implemented the kernel scheduling algorithm using our variant of Burns' primal-dual algorithm from Section 7.4 as the component maximum cycle ratio algorithm, and Djordjevic and Tosic's task selection first (TSF) algorithm <ref> [10] </ref> as the noniterative DAG scheduling algorithm. The TSF algorithm is a recent O (v (v + e)) time algorithm that shares some features with the DSC algorithm, such as a preference for reducing the length of the "dominant sequence" of a partially scheduled DAG. <p> Table 9.4 shows the results for a multiple-pass kernel scheduling algorithm using a DDG increment of .1 and the first k task weight sets from Table 9.1, for k 2 <ref> [1; 10] </ref>, plus k = 24. In contrast to task sequencing, it appears to be more effective to use multiple DDG increments than to use multiple task weights for this range of scheduling trials. <p> 43.59 Table 9.3: Statistics for a multiple-pass kernel scheduling algorithm using SCC scheduling, DAG edge optimization, the balanced weight set (w est = w lct = 1=2 and w ect = w lst = 0), and k DDG increments uniformly distributed over the interval [0; (k 1)=k], for k 2 <ref> [2; 10] </ref>. # Trials 1 2 3 4 5 6 7 8 9 10 24 100 (b=b lb 1) 47.82 46.62 45.87 45.49 45.24 44.91 44.40 44.33 44.22 44.14 43.43 Table 9.4: Statistics for a multiple-pass kernel scheduling algorithm using SCC scheduling, DAG edge optimization, DDG increment .1, and the first <p> 8 9 10 24 100 (b=b lb 1) 47.82 46.62 45.87 45.49 45.24 44.91 44.40 44.33 44.22 44.14 43.43 Table 9.4: Statistics for a multiple-pass kernel scheduling algorithm using SCC scheduling, DAG edge optimization, DDG increment .1, and the first k task weight sets from Table 9.1, for k 2 <ref> [1; 10] </ref>, plus k = 24. moment increments might be used to advantage. Relaxing deadlock constraints in a multiple-pass version of the algorithm might be useful. Even the techniques that we have explicitly introduced might be improved. <p> Noniterative DAG scheduling in the kernel scheduling algorithm [Chapter 9] | How does the kernel scheduling algorithm using Djordjevic and Tosic's TSF noniterative DAG scheduling algorithm <ref> [10] </ref> compare to the kernel scheduling algorithm using Yang and Gerasoulis' DSC algorithm [41], in terms of the quality of the generated schedules, as well as kernel scheduling algorithm execution time? How do these two algorithms compare to kernel scheduling using other noniterative DAG scheduling algorithms? 16.
Reference: [11] <author> Val Donaldson, Francine Berman, and Ramamohan Paturi. </author> <title> Program speedup in a heterogeneous computing network. </title> <journal> Journal of Parallel and Distributed Computing 21:3 (June 1994), </journal> <pages> pp. 316-322. </pages>
Reference-contexts: Heterogeneous processing | We have assumed that the processors available for implementing a pipeline schedule were identical, homogeneous processors. It is also possible to use heterogeneous processors to improve the quality of a schedule <ref> [2, 11] </ref>. Since processor or other forms of heterogeneity are primarily relevant in the processor assignment component of a schedule, the PA/TS scheduling algorithm appears to provide a good foundation for exploiting heterogeneous processing capabilities.
Reference: [12] <author> D. K. Friesen and M. A. Langston. </author> <title> Evaluation of a MULTIFIT-based scheduling algorithm. </title> <journal> Journal of Algorithms 7:1 (March, </journal> <year> 1986), </year> <pages> pp. 35-59. </pages>
Reference-contexts: Graham's "largest processing time first" (LPT) multiprocessor scheduling algorithm [17] has a performance guarantee of 33%; Coffman, Garey, and Johnson's MULTIFIT algorithm 99 [6] has a 20% guarantee; and Friesen and Langston's improved MULTIFIT algorithm <ref> [12] </ref> has an 18% guarantee. All of these algorithms execute in O (v log v) time, although the constants involved increase as the guarantee improves. These algorithms are described in more detail in Section 10.1. <p> The MULTIFIT multiprocessor scheduling algorithm from Coffman et al. [6] has a 20% performance guarantee, and is simpler to implement than the improved version of the algorithm in Friesen and Langston <ref> [12] </ref>, which has an 18% guarantee (see Section 10.1 for a description of these algorithms). <p> For any fixed number of iterations of the binary search loop in Step 2, the algorithm executes in O (v log v) time. Assignments generated by the MULTIFIT algorithm are guaranteed to be within (20 + 1=2 k )% of optimal <ref> [12] </ref>; we use k = 8 iterations. Friesen and Langston [12] describe an O (1) time modification to the MULTIFIT algorithm where the failing branch of the if statement in the search loop is modified to make another attempt to generate a successful assignment before admitting failure. <p> Assignments generated by the MULTIFIT algorithm are guaranteed to be within (20 + 1=2 k )% of optimal <ref> [12] </ref>; we use k = 8 iterations. Friesen and Langston [12] describe an O (1) time modification to the MULTIFIT algorithm where the failing branch of the if statement in the search loop is modified to make another attempt to generate a successful assignment before admitting failure. <p> When a DDG is self-cyclic, tasks are equivalent to SCC's. Using the MULTIFIT algorithm as the processor assignment algorithm in this case, the generated schedule is guaranteed to be within 20% of optimal (18% of optimal using the improved MULTIFIT algorithm in <ref> [12] </ref>). For cyclic DDG's, tasks may also be assigned to processors using the BFD/LPT algorithm.
Reference: [13] <author> Harold N. Gabow and Robert E. Tarjan. </author> <title> Faster scaling algorithms for network problems. </title> <journal> SIAM Journal on Computing 18:5 (October 1989), </journal> <pages> pp. 1013-1036. 159 160 </pages>
Reference-contexts: If we prefer to use approximation algorithms to reduce the asymptotic execution time of the scheduling algorithm, we can use Gabow and Tarjan's O ( p ve log G:vemax * ) time approximation algorithm <ref> [13] </ref> for the longest path subalgorithm of the task sequencing algorithm. Similar comments apply to the algorithm modifications considered in the remainder of the chapter.
Reference: [14] <author> Michael R. Garey and David S. Johnson. </author> <title> Computers and Intractability: A Guide to the Theory of NP-Completeness. </title> <publisher> Freeman, </publisher> <address> New York, </address> <year> 1979. </year>
Reference-contexts: 10.1 Multiprocessor Scheduling and Bin Packing In Chapter 6 we formally defined the multiprocessor scheduling problem as the problem of assigning a set of independent tasks (tasks with no intertask dependences) to a set of processors, with the goal of minimizing the maximum sum of task times on any processor <ref> [14] </ref>. Figure 10.1 shows Graham's largest processing time first (LPT) multiprocessor scheduling algorithm [17], which generates processor assignments that are guaranteed to be within 33% of optimal. <p> The bin packing problem <ref> [14, 22] </ref> may be viewed as a dual of the multiprocessor scheduling problem. <p> Question: Is there a partition V = V 1 [ V 2 [ [ V p of V into p disjoint sets such that for all i 2 [1; p], X2V i X:time B? The best fit decreasing (BFD) algorithm <ref> [14, 22] </ref>, is one of a family of bin packing algorithms that are guaranteed to use no more than 22% more processors than optimal.
Reference: [15] <author> Franco Gasperoni and Uwe Schwiegelshohn. </author> <title> Scheduling loops on parallel processors: a simple algorithm with close to optimum performance. </title> <booktitle> Second Joint International Conference on Vector and Parallel Processing (Parallel Processing: CONPAR 92-VAPP V), </booktitle> <address> Lyon, France, </address> <month> September </month> <year> 1992, </year> <pages> pp. 625-636. </pages>
Reference-contexts: The offset of a task's moment from the lower bound of its stage is used to determine the task's position in its task order. Figure 8.1 illustrates this process for the example loop from <ref> [15] </ref>, which was first introduced in Figure 2.1. Figure 8.1a and the upper half of Figure 8.1b are the DDG G from Figure 2.1b and the task boundary times for DDG tasks from Figure 2.1c, using the value b fl = G:mcr = 1:5. <p> In Chapter 10, after developing a new scheduling algorithm, we will investigate the effect of executing both algorithms and choosing the best generated schedule. 9.1 Original Kernel Scheduling Algorithm The algorithm of Yang et al. in [40] is based on the algorithm of Gasperoni and Schwiegels-hohn <ref> [15] </ref>, but modifies the earlier algorithm to permit nonzero communication times, and to allow a schedule to be implemented without reference to a global clock, among other extensions. <p> When communication times are zero, the kernel scheduling algorithm generates schedules that are guaranteed to be within 100% of optimal, i.e., a factor of 2 of optimal <ref> [15, 40] </ref>, although this guarantee is with respect to noniterative DAG scheduling for a completely unrolled loop, rather than any of the optimality measures defined in Chapter 6. <p> We can compare PA/TS algorithm execution to other scheduling algorithms using examples from the literature. Gasperoni and Schwiegelshohn <ref> [15] </ref> generate the schedule fhA : 0; B : 0i; hD : 1; C : 0i; hE : 1ig for the example loop in Figure 2.1 on 3 processors, which executes in G:time n = 1+3n time units, which is 50% above the optimal iteration interval of b = 2. <p> scheduling algorithm, analogous to the generic task sequencing algorithm, which can be used to tune and improve kernel scheduling algorithm performance, by allowing modifications such as DAG edge optimization, DDG increments, and task weights? Are there other useful modifications to the kernel scheduling algorithm? The kernel scheduling techniques considered in <ref> [5, 15, 39, 40] </ref>, as well as our discussion in Chapter 9 suggest that kernel scheduling is a reasonable approach to scheduling, but there is room for improvement to the theory underlying kernel scheduling, as well as opportunities for heuristic improvements. 14.
Reference: [16] <author> Donald Goldfarb, Jianxiu Hao, and Sheng-Roan Kai. </author> <title> Shortest path algorithms using dynamic breadth-first search. </title> <booktitle> Networks 21:1 (January 1991), </booktitle> <pages> pp. 29-50. </pages>
Reference: [17] <author> R. L. Graham. </author> <title> Bounds on multiprocessing timing anomalies. </title> <journal> SIAM Journal on Applied Mathematics 17:2 (March, </journal> <year> 1969), </year> <pages> pp. 416-429. </pages>
Reference-contexts: Graham's "largest processing time first" (LPT) multiprocessor scheduling algorithm <ref> [17] </ref> has a performance guarantee of 33%; Coffman, Garey, and Johnson's MULTIFIT algorithm 99 [6] has a 20% guarantee; and Friesen and Langston's improved MULTIFIT algorithm [12] has an 18% guarantee. <p> We use the set of 10,800 test DDG's described in Section 2.4 to empirically evaluate the performance of the base algorithm and other task sequencing algorithm variants. For each test DDG, tasks are assigned to processors using Graham's O (v log v) time LPT multiprocessor scheduling algorithm <ref> [17] </ref>. Data communication times between tasks assigned to the same processor are zeroed. <p> Figure 10.1 shows Graham's largest processing time first (LPT) multiprocessor scheduling algorithm <ref> [17] </ref>, which generates processor assignments that are guaranteed to be within 33% of optimal. Step 1 of the LPT algorithm sorts the tasks in decreasing (nonincreasing) order of execution times, and Step 2 initializes the sum of task execution times on each processor to 0.
Reference: [18] <author> Mark Hartmann and James B. Orlin. </author> <title> Finding minimum cost to time ratio cycles with small integral transit times. </title> <booktitle> Networks 23:6 (September 1993), </booktitle> <pages> pp. 567-74. </pages>
Reference: [19] <author> Phu D. Hoang and Jan M. Rabaey. </author> <title> Scheduling of DSP programs onto multiprocessors for maximum throughput. </title> <journal> IEEE Transactions on Signal Processing 41:6 (June 1993), </journal> <pages> pp. 2225-2235. </pages>
Reference-contexts: The DDG for the program is a self-cyclic task graph with 12 tasks, where data communication times are zero. The derived pipeline schedule for 6 processors using the algorithm in <ref> [19] </ref> executes in G:time n = 29:00 + 44:38n time units, which is 6% above the optimal iteration interval of b = 41:80. The PA/TS algorithm generates an asymptotically optimal schedule that executes in G:time n = 31:58 + 41:80n time units. <p> In Chapter 10 we also compared PA/TS scheduling algorithm performance to other scheduling algorithms, using available examples from the literature. It would nevertheless be useful to evaluate our conclusions in the context of DDG's derived from industry benchmarks or other programs. The discussions of pipelining in <ref> [2, 19] </ref> suggest that digital signal processing applications may be one class of programs that are often well suited to execution using asynchronous pipeline parallelism. The generation at runtime of pipeline schedules for sparse matrix computations as considered in [40] is another possibility. 18. <p> Hierarchical scheduling | The definition of a DDG in Definition 2.1 permitted tasks to be large or small, consisting of simple operations or arbitrarily complex compound statements, but assumed that they were atomic, indivisible units of computation. The pipeline scheduling algorithm in <ref> [19] </ref> incorporates a "hierarchical scheduling" capability, where large "bottleneck" tasks may be decomposed into smaller tasks in some cases. A called routine might be expanded by inlining, or the iterations of a nested loop might be split into two or more smaller loops. <p> A practical pipeline scheduling implementation must take memory requirements into account <ref> [19] </ref>. In software pipelining, storage typically takes the form of registers, but an asynchronous pipelining implementation may require explicitly allocated storage in a shared memory system, or implicitly allocated message buffers in a distributed memory system.
Reference: [20] <author> J. A. Hoogeveen, S. L. van de Velde, and B. Veltman. </author> <title> Complexity of scheduling multiprocessor tasks with prespecified processor allocations. </title> <booktitle> Discrete Applied Mathematics 55:3 (December 1994), </booktitle> <pages> pp. 259-272. </pages>
Reference: [21] <author> Jing-Jang Hwang, Yuan-Chieh Chow, Frank D. Anger, and Chung-Yee Lee. </author> <title> Scheduling precedence graphs in systems with interprocessor communication times. </title> <journal> SIAM Journal on Computing 18:2 (April 1989), </journal> <pages> pp. 244-257. </pages>
Reference: [22] <author> David S. Johnson. </author> <title> Fast algorithms for bin packing. </title> <journal> Journal of Computer and System Sciences 8:3 (June, </journal> <year> 1974), </year> <pages> pp. 272-314. </pages>
Reference-contexts: The bin packing problem <ref> [14, 22] </ref> may be viewed as a dual of the multiprocessor scheduling problem. <p> Question: Is there a partition V = V 1 [ V 2 [ [ V p of V into p disjoint sets such that for all i 2 [1; p], X2V i X:time B? The best fit decreasing (BFD) algorithm <ref> [14, 22] </ref>, is one of a family of bin packing algorithms that are guaranteed to use no more than 22% more processors than optimal.
Reference: [23] <author> Donald B. Johnson. </author> <title> Finding all the elementary circuits of a directed graph. </title> <journal> SIAM Journal on Computing 4:1 (March 1975), </journal> <pages> pp. 77-84. </pages>
Reference: [24] <author> S. J. Kim and J. C. Browne. </author> <title> A general approach to mapping of parallel computation upon multiprocessor architectures. </title> <booktitle> Proc. 17th International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1988, </year> <booktitle> Vol III, </booktitle> <pages> pp. 1-8. </pages>
Reference-contexts: Our approach to clustering has some similarities to the clustering technique used in Kim and Browne's noniterative 133 DAG scheduling algorithm <ref> [24] </ref>. The BFD/LPT processor assignment algorithm is shown in Figure 10.5. In addition to the input DDG G and a processor count p, input to the algorithm consists of a nonnegative real bin size B as in the BFD algorithm, and a nonnegative real "cluster bound" CB.
Reference: [25] <author> Peter M. Kogge. </author> <title> The Architecture of Pipelined Computers, </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1981. </year>
Reference: [26] <author> S. Y. Kung, P. S. Lewis, and S. C. Lo. </author> <title> Performance analysis and optimization of VLSI dataflow arrays. </title> <journal> Journal of Parallel and Distributed Computing 4:6 (December 1987), </journal> <pages> pp. 592-618. </pages>
Reference: [27] <author> Monica Lam. </author> <title> Software pipelining: an effective scheduling technique for VLIW machines. </title> <booktitle> Proc. SIGPLAN '88 Conference on Programming Language Design and Implementation, </booktitle> <address> Atlanta, GA, </address> <month> June </month> <year> 1988, </year> <pages> pp. 318-328. </pages>
Reference: [28] <author> Eugene L. Lawler. </author> <title> Combinatorial Optimization: Networks and Matroids. </title> <publisher> Holt, Rinehart, and Winston, </publisher> <address> New York, </address> <year> 1976. </year>
Reference-contexts: We also considered multiple-pass scheduling algorithms, including algorithm variants that combined PA/TS scheduling passes with kernel scheduling passes. Maximum cycle ratio algorithms | We developed a new monotonic search algorithm, which appears to have several advantages over variants of the binary search algorithm in <ref> [28] </ref>. We proved several useful facts about the behavior of Burns' primal-dual algorithm [4], which potentially increase its applicability.
Reference: [29] <author> F. Thomson Leighton. </author> <title> Introduction to Parallel Algorithms and Architectures: Arrays, Trees, Hypercubes. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1992. </year>
Reference: [30] <author> Tsuneo Nakanishi, Kazuki Joe, Constantine D. Polychronopoulos, Akira Fukuda, and Keijiro Araki. </author> <title> Estimating parallel execution time of loops with loop-carried dependences. </title> <booktitle> Proc. 25th International Conference on Parallel Processing, </booktitle> <address> Ithaca, NY, </address> <month> August </month> <year> 1996, </year> <booktitle> Vol III, </booktitle> <pages> pp. 61-69. 161 </pages>
Reference-contexts: Again it is not known if this can be done in polynomial time, independent of n. The integer linear program in <ref> [30] </ref> for the case where processors are not shared suggests that this problem is probably in NP. 7.
Reference: [31] <author> David A. Padua and Michael J. Wolfe. </author> <title> Advanced compiler optimizations for supercomputers. </title> <journal> Communications of the ACM 29:12 (December 1986), </journal> <pages> pp. 1184-1201. </pages>
Reference: [32] <author> Keshab K. Parhi and David G. Messerschmitt. </author> <title> Static rate-optimal scheduling of iterative data-flow programs via optimum unfolding. </title> <journal> IEEE Transactions on Computers 40:2 (February 1991), </journal> <pages> pp. 178-195. </pages>
Reference-contexts: We introduced the notion of negative data dependence distances in an unnested loop, which may be useful in other contexts. Our use of retiming in this context may have additional implications as a loop transformation technique (see also <ref> [5, 32] </ref>). Scheduled DDG's provide the foundation for our generalization of previous work on determining whether or not an arbitrary schedule will deadlock. A schedule will deadlock if and only if the associated scheduled DDG violates the positive cycle constraint. <p> The generation at runtime of pipeline schedules for sparse matrix computations as considered in [40] is another possibility. 18. Loop unrolling | It is well-known that unrolling a loop before scheduling may improve the quality of the generated schedule <ref> [5, 32, 40] </ref>. This is particularly true in software pipelining, where unrolling can sometimes be used to reduce or eliminate the adverse effect of a nonintegral maximum cycle ratio in the input DDG on the quality of a pipeline schedule for a loop. <p> Unrolling also increases the options for balancing the task load across processors in all forms of pipelining, including asynchronous pipeline scheduling. Unfortunately, techniques such as those in <ref> [32] </ref> may require that a loop be unrolled an excessively large number of times (the least common multiple of the sum of dependence distances in a loop, across all loops).
Reference: [33] <author> C. V. Ramamoorthy and Gary S. Ho. </author> <title> Performance evaluation of asynchronous concurrent systems using Petri nets. </title> <journal> IEEE Transactions on Software Engineering SE-6:5 (September 1980), </journal> <pages> pp. 440-449. </pages>
Reference: [34] <author> Raymond Reiter. </author> <title> Scheduling parallel computations. </title> <journal> Journal of the ACM 15:4 (October 1968), </journal> <pages> pp. 590-599. </pages>
Reference-contexts: Scheduled DDG's are also the basis for our generalization of previous work on determining the execution time of a deadlock-free schedule, most notably our generalization of Reiter's iteration interval formula <ref> [34] </ref>. Our generalization of this formula equates the iteration interval of a deadlock-free schedule to the maximum cycle ratio of the associated scheduled DDG. We defined the startup time of a schedule, and discussed techniques for bounding its value.
Reference: [35] <author> Vivek Sarkar. </author> <title> Partitioning and Scheduling Parallel Programs for Multiprocessors. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1989. </year>
Reference-contexts: However, unlike pure multiprocessor scheduling and bin packing algorithms, our hybrid algorithm also takes intertask dependences into account when making assignment decisions, as discussed next. 10.2 Ranking Edges Sarkar's noniterative DAG scheduling algorithm <ref> [35] </ref> sorts edges in order of decreasing data communication times, and then visits edges in this order while making scheduling decisions. We do not use communication times as our sorting criterion, nor do we evaluate scheduling decisions for each individual edge as in [35], but we do borrow the notion of <p> Ranking Edges Sarkar's noniterative DAG scheduling algorithm <ref> [35] </ref> sorts edges in order of decreasing data communication times, and then visits edges in this order while making scheduling decisions. We do not use communication times as our sorting criterion, nor do we evaluate scheduling decisions for each individual edge as in [35], but we do borrow the notion of ranking edges. Scheduling decisions in the DSC algorithm, Yang and Gerasoulis' noniterative DAG scheduling algorithm [41], are made with the goal of decreasing (or not increasing) the "dominant sequence" (critical path) length of a partially scheduled DAG. <p> Our approach to ranking edges and making scheduling decisions is an indirect attempt to decrease the maximum cycle ratio of a scheduled DDG. It is worth noting that although we have not chosen to directly emulate the scheduling techniques in <ref> [35, 41] </ref> or any other noniterative DAG scheduling algorithm, in part because of computational complexity considerations, this could be done. <p> Based on our formulation of the asynchronous pipeline scheduling problem as a generalization of the noniterative DAG scheduling problem on one hand, and on the existence of our task sequencing algorithm on the other hand, there are fairly direct pipeline scheduling analogs for the algorithms in <ref> [35, 41] </ref> and other widely used noniterative DAG scheduling algorithms. From Section 2.2, an edge (X; Y ) in a boundary graph G b fl is a critical edge if X:est b fl + X:weight b fl = Y:est b fl . <p> In reality, interprocessor communication costs consist not only of delays between the time a message is sent and the time that it is received, but also typically require overhead "write time" on the sending processor, and "read time" on the receiving processor <ref> [35] </ref>. In some circumstances these administrative costs may be small enough to be negligible, but must be accounted for in others to obtain reasonable schedules. 24. Heterogeneous processing | We have assumed that the processors available for implementing a pipeline schedule were identical, homogeneous processors. <p> It might also be possible to borrow ideas from the PA/TS scheduling algorithm to improve existing noniterative DAG scheduling algorithms, or to suggest new techniques for noniterative DAG scheduling. As an example, noniterative DAG scheduling algorithms such as the second phase of Sarkar's algorithm <ref> [35] </ref> have a step where two clusters of tasks from different "virtual" processors are assigned to the same physical processor. The execution order of tasks on the processor is typically chosen at least in part using earliest starting times for tasks.
Reference: [36] <author> Gilbert C. Sih and Edward A. Lee. </author> <title> Scheduling to account for interprocessor communication within interconnection-constrained processor networks. </title> <booktitle> Proc. 1990 International Conference on Parallel Processing, </booktitle> <address> University Park, PA, </address> <month> August </month> <year> 1990, </year> <pages> pp. 9-16. </pages>
Reference: [37] <author> James C. Tiernan. </author> <title> An efficient search algorithm to find the elementary circuits of a graph. </title> <journal> Communications of the ACM 13:12 (December 1970), </journal> <pages> pp. 722-726. </pages>
Reference: [38] <author> Rob F. Van der Wijngaart, Sekhar R. Sarukkai, and Pankaj Mehra. </author> <title> Analysis and optimization of software pipeline performance on MIMD parallel computers. </title> <journal> Journal of Parallel and Distributed Computing 38:1 (October 1996), </journal> <pages> pp. 37-50. </pages>
Reference-contexts: constrained to wait for execution of any other task, and since A:time = 4 &lt; G:vmax = 7, task A may execute faster than other tasks as shown in Figure 5.2b, overwhelming the data storage or message buffering resources of successor tasks, as well as creating contention for communication resources <ref> [38] </ref>. Although we have not explicitly addressed this issue, this is the primary reason for allowing task execution times to be zero in Definition 2.1.
Reference: [39] <author> Tao Yang and Cong Fu. </author> <title> Heuristic algorithms for scheduling iterative task graphs on distributed memory machines. </title> <journal> IEEE Transactions on Parallel and Distributed Systems 8:6 (June 1997), </journal> <pages> pp. 608-622. </pages>
Reference-contexts: The algorithm of Yang et al. in [40] assumes that the loop being scheduled contains cycles. Yang and Fu <ref> [39] </ref> also describe a distinct scheduling algorithm that applies only to acyclic loops, 113 114 Kernel Scheduling Algorithm Input | A cyclic DDG G that satisfies the positive cycle constraint Output | An asynchronous pipeline schedule s for G Method 1. /* Find a lower bound b fl on the iteration <p> scheduling algorithm, analogous to the generic task sequencing algorithm, which can be used to tune and improve kernel scheduling algorithm performance, by allowing modifications such as DAG edge optimization, DDG increments, and task weights? Are there other useful modifications to the kernel scheduling algorithm? The kernel scheduling techniques considered in <ref> [5, 15, 39, 40] </ref>, as well as our discussion in Chapter 9 suggest that kernel scheduling is a reasonable approach to scheduling, but there is room for improvement to the theory underlying kernel scheduling, as well as opportunities for heuristic improvements. 14.
Reference: [40] <author> Tao Yang, Cong Fu, Apostolos Gerasoulis, and Vivek Sarkar. </author> <title> Mapping iterative task graphs on distributed memory machines. </title> <booktitle> Proc. 24th International Conference on Parallel Processing, </booktitle> <address> Oconomowoc, WI, </address> <month> August </month> <year> 1995, </year> <booktitle> Vol II, </booktitle> <pages> pp. 151-158. </pages>
Reference-contexts: In this case, generated task sequences average significantly closer to optimal. Chapter 9 Kernel Scheduling The kernel scheduling algorithm of Yang et al. <ref> [40] </ref> is the only existing asynchronous pipeline scheduling algorithm capable of generating schedules with nontrivial stage assignments, which generally results in schedules with smaller iteration intervals than more restricted algorithms. In this chapter we empirically evaluate an implementation of the algorithm. <p> In Chapter 10, after developing a new scheduling algorithm, we will investigate the effect of executing both algorithms and choosing the best generated schedule. 9.1 Original Kernel Scheduling Algorithm The algorithm of Yang et al. in <ref> [40] </ref> is based on the algorithm of Gasperoni and Schwiegels-hohn [15], but modifies the earlier algorithm to permit nonzero communication times, and to allow a schedule to be implemented without reference to a global clock, among other extensions. <p> We refer to these algorithms collectively as "kernel scheduling" algorithms, since a key step of these algorithms is to choose an acyclic subgraph of the input DDG, called a "kernel graph" in <ref> [40] </ref>, which is then scheduled using a noniterative DAG scheduling algorithm. The algorithm of Yang et al. is the only one of these algorithms that meets the requirements for asynchronous execution, so we also use the term "kernel scheduling algorithm" to specifically denote this algorithm. <p> The algorithm of Yang et al. is the only one of these algorithms that meets the requirements for asynchronous execution, so we also use the term "kernel scheduling algorithm" to specifically denote this algorithm. The algorithm of Yang et al. in <ref> [40] </ref> assumes that the loop being scheduled contains cycles. <p> We therefore focus our discussion in this chapter on the algorithm for cyclic DDG's in <ref> [40] </ref>. The kernel scheduling algorithm is shown in Figure 9.1. The algorithm assumes that all tasks have positive execution times. As in task sequencing, we can modify any task with a zero execution time to have a small execution time * &gt; 0 if necessary. <p> Borrowing our terminology from Chapter 8, Step 2 sets the moment of each task X to X:est b fl (Definition 2.5), and then decomposes this value into stage and offset values. Yang et al. actually specify moments (ff values in <ref> [40] </ref>) in terms of an arbitrary solution to a linear programming problem, but in practice they use earliest starting times as indicated. Although these first two steps are fairly similar to comparable steps in our task sequencing algorithm in Chapter 8, Steps 3 and 4 are unique to kernel scheduling. <p> When communication times are zero, the kernel scheduling algorithm generates schedules that are guaranteed to be within 100% of optimal, i.e., a factor of 2 of optimal <ref> [15, 40] </ref>, although this guarantee is with respect to noniterative DAG scheduling for a completely unrolled loop, rather than any of the optimality measures defined in Chapter 6. <p> Most or perhaps all of the O (v log v + t mcr (G)) time multiple-pass PA/TS algorithm variants considered in Section 10.5 below will generate schedules with an iteration interval b = 19. times, consisting of 2 SCC's. The algorithm in <ref> [40] </ref> computes an unroll depth of 3 for the DDG. Yang et al. arbitrarily reduce this unroll depth to 2 and then generate a kernel schedule on 3 processors for the resulting unrolled DDG, which they estimate to execute with an iteration interval of b = 90. <p> Any loop may be sequenced in O (v log v + t mcr (G)) time. We used our experience with task sequencing to suggest a number of modifications to the kernel scheduling algorithm of Yang et al. <ref> [40] </ref>, which significantly improve the quality of generated schedules. The technique of generating an "SCC schedule" for a loop to provide an upper bound on the iteration interval of any generated schedule effectively generalizes the technique of using the sequential loop iteration interval as this upper bound. <p> Variable loop component times [Chapter 4] | With the exception of Section 4.7, we assumed that task execution and data communication times were known, fixed values that were invariant from iteration to iteration. Section 4.7 briefly discussed the effect of allowing these times to vary (see also <ref> [40] </ref>). It would be useful to more fully characterize these effects. 3. <p> scheduling algorithm, analogous to the generic task sequencing algorithm, which can be used to tune and improve kernel scheduling algorithm performance, by allowing modifications such as DAG edge optimization, DDG increments, and task weights? Are there other useful modifications to the kernel scheduling algorithm? The kernel scheduling techniques considered in <ref> [5, 15, 39, 40] </ref>, as well as our discussion in Chapter 9 suggest that kernel scheduling is a reasonable approach to scheduling, but there is room for improvement to the theory underlying kernel scheduling, as well as opportunities for heuristic improvements. 14. <p> The discussions of pipelining in [2, 19] suggest that digital signal processing applications may be one class of programs that are often well suited to execution using asynchronous pipeline parallelism. The generation at runtime of pipeline schedules for sparse matrix computations as considered in <ref> [40] </ref> is another possibility. 18. Loop unrolling | It is well-known that unrolling a loop before scheduling may improve the quality of the generated schedule [5, 32, 40]. <p> The generation at runtime of pipeline schedules for sparse matrix computations as considered in [40] is another possibility. 18. Loop unrolling | It is well-known that unrolling a loop before scheduling may improve the quality of the generated schedule <ref> [5, 32, 40] </ref>. This is particularly true in software pipelining, where unrolling can sometimes be used to reduce or eliminate the adverse effect of a nonintegral maximum cycle ratio in the input DDG on the quality of a pipeline schedule for a loop. <p> Unfortunately, techniques such as those in [32] may require that a loop be unrolled an excessively large number of times (the least common multiple of the sum of dependence distances in a loop, across all loops). The technique for determining an a priori unroll depth for a loop in <ref> [40] </ref> appears to generate rather arbitrary unroll factors that are not correlated to the actual benefit of unrolling for a particular loop, and the precalculated unroll factors may also be prohibitively large.
Reference: [41] <author> Tao Yang and Apostolos Gerasoulis. </author> <title> DSC: scheduling parallel tasks on an unbounded number of processors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems 5:9 (September 1994), </journal> <pages> pp. 951-967. </pages>
Reference-contexts: Yang et al. use several variants of the O ((v + e) log v) time dominant sequence clustering (DSC) algorithm <ref> [41] </ref> as the primary component of the noniterative DAG scheduling process, so the reported complexity of the kernel scheduling algorithm is O ( ve log 2 G:vtime * ) time. <p> We do not use communication times as our sorting criterion, nor do we evaluate scheduling decisions for each individual edge as in [35], but we do borrow the notion of ranking edges. Scheduling decisions in the DSC algorithm, Yang and Gerasoulis' noniterative DAG scheduling algorithm <ref> [41] </ref>, are made with the goal of decreasing (or not increasing) the "dominant sequence" (critical path) length of a partially scheduled DAG. The analog of a dominant sequence in pipeline scheduling is a critical cycle in a partially scheduled DDG. <p> Our approach to ranking edges and making scheduling decisions is an indirect attempt to decrease the maximum cycle ratio of a scheduled DDG. It is worth noting that although we have not chosen to directly emulate the scheduling techniques in <ref> [35, 41] </ref> or any other noniterative DAG scheduling algorithm, in part because of computational complexity considerations, this could be done. <p> Based on our formulation of the asynchronous pipeline scheduling problem as a generalization of the noniterative DAG scheduling problem on one hand, and on the existence of our task sequencing algorithm on the other hand, there are fairly direct pipeline scheduling analogs for the algorithms in <ref> [35, 41] </ref> and other widely used noniterative DAG scheduling algorithms. From Section 2.2, an edge (X; Y ) in a boundary graph G b fl is a critical edge if X:est b fl + X:weight b fl = Y:est b fl . <p> Noniterative DAG scheduling in the kernel scheduling algorithm [Chapter 9] | How does the kernel scheduling algorithm using Djordjevic and Tosic's TSF noniterative DAG scheduling algorithm [10] compare to the kernel scheduling algorithm using Yang and Gerasoulis' DSC algorithm <ref> [41] </ref>, in terms of the quality of the generated schedules, as well as kernel scheduling algorithm execution time? How do these two algorithms compare to kernel scheduling using other noniterative DAG scheduling algorithms? 16.
Reference: [42] <author> Jin Y. Yen. </author> <title> An algorithm for finding shortest routes from all source nodes to a given destination in general networks. </title> <journal> Quarterly of Applied Mathematics 27:4 (January 1970), </journal> <pages> pp. 526-530. </pages>
References-found: 42

