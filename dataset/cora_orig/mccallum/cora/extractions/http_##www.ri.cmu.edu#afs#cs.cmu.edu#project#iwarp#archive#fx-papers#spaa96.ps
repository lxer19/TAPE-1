URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/project/iwarp/archive/fx-papers/spaa96.ps
Refering-URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/jass/www/project-auto.html
Root-URL: 
Email: jass@cs.cmu.edu  vondran@hpl.hp.com  
Title: Optimal Latency-Throughput Tradeoffs for Data Parallel Pipelines  
Author: Jaspal Subhlok Gary Vondran 
Address: Pittsburgh PA 15213  Cambridge, MA 02142  
Affiliation: School of Computer Science Carnegie Mellon University  Hewlett Packard Laboratories Hewlett Packard Company  
Abstract: This paper addresses optimal mapping of parallel programs composed of a chain of data parallel tasks onto the processors of a parallel system. The input to this class of programs is a stream of data sets, each of which is processed in order by the chain of tasks. This computation structure, also referred to as a data parallel pipeline, is common in several application domains including digital signal processing, image processing, and computer vision. The parameters of the performance of stream processing are latency (the time to process an individual data set) and throughput (the aggregate rate at which the data sets are processed). These two criterion are distinct since multiple data sets can be pipelined or processed in parallel. We present a new algorithm to determine a processor mapping of a chain of tasks that optimizes the latency in the presence of throughput constraints, and discuss optimization of the throughput with latency constraints. The problem formulation uses a general and realistic model of inter-task communication, and addresses the entire problem of mapping, which includes clustering tasks into modules, assignment of processors to modules, and possible replication of modules. The main algorithms are based on dynamic programming and their execution time complexity is polynomial in the number of processors and tasks. The entire framework is implemented as an automatic mapping tool in the Fx parallelizing compiler for a dialect of High Performance Fortran. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Bokhari, S. </author> <title> Assignment Problems in Parallel and Distributed Computing. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1987. </year>
Reference-contexts: This approach assumes that the execution behavior of a program is not strongly dependent on the properties of the input data sets. There is a large large volume of literature on mapping and scheduling parallel programs, and some excellent references are <ref> [1, 12, 19] </ref>. We focus only on mapping coarse grain task parallelism, and assume that a data parallel compiler is available to exploit fine grain parallelism. We compare our approach to two other research efforts that address assignment of processors to coarse grain tasks.
Reference: [2] <author> Chakrabarti, S., Demmel, J., and Yelick, K. </author> <title> Modeling the benefits of mixed data and task parallelism. </title> <booktitle> In Seventh Annual ACM Symposium on Parallel Algorithms and Architectures (Santa Barbara, </booktitle> <address> CA, </address> <month> July </month> <year> 1995). </year>
Reference-contexts: Compiler and runtime support for task and data parallel computing is an active area of research, and several solutions have been proposed [3, 4, 8, 9, 14]. Recent research has also examined the benefits of mixed task and data parallel programming <ref> [2, 6, 13] </ref>. This paper specifically addresses the mapping of applications composed of a linear chain of data parallel tasks that act on a stream of input data sets. Each task repeatedly receives input from its predecessor task, performs its computation, and sends the output to its successor task.
Reference: [3] <author> Chandy, M., Foster, I., Kennedy, K., Koelbel, C., and Tseng, C. </author> <title> Integrated support for task and data parallelism. </title> <journal> International Journal of Supercomputer Applications 8, </journal> <volume> 2 (1994), </volume> <pages> 80-98. </pages>
Reference-contexts: Most modern parallel machines support MIMD execution, and therefore combined data and task (or function) parallel computing. Compiler and runtime support for task and data parallel computing is an active area of research, and several solutions have been proposed <ref> [3, 4, 8, 9, 14] </ref>. Recent research has also examined the benefits of mixed task and data parallel programming [2, 6, 13]. This paper specifically addresses the mapping of applications composed of a linear chain of data parallel tasks that act on a stream of input data sets.
Reference: [4] <author> Chapman, B., Mehrotra, P., Van Rosendale, J., and Zima, H. </author> <title> A software architecture for multidisciplinary applications: Integrating task and data parallelism. </title> <type> Tech. Rep. 94-18, </type> <institution> ICASE, NASA Langley Research Center, Hampton, VA, </institution> <month> Mar. </month> <year> 1994. </year>
Reference-contexts: Most modern parallel machines support MIMD execution, and therefore combined data and task (or function) parallel computing. Compiler and runtime support for task and data parallel computing is an active area of research, and several solutions have been proposed <ref> [3, 4, 8, 9, 14] </ref>. Recent research has also examined the benefits of mixed task and data parallel programming [2, 6, 13]. This paper specifically addresses the mapping of applications composed of a linear chain of data parallel tasks that act on a stream of input data sets.
Reference: [5] <author> Choudhary, A., Narahari, B., Nicol, D., and Simha, R. </author> <title> Optimal processor assignment for a class of pipelined computations. </title> <journal> IEEE Transactions on Parallel and Distributed Systems 5, </journal> <volume> 4 (April 94), </volume> <pages> 439-445. </pages>
Reference-contexts: We focus only on mapping coarse grain task parallelism, and assume that a data parallel compiler is available to exploit fine grain parallelism. We compare our approach to two other research efforts that address assignment of processors to coarse grain tasks. Choudhary et. al. <ref> [5] </ref> address the problem of optimal processor assignment to a pipeline of coarse grain tasks, assuming that the communication cost can be folded into the computation cost. In our experience, a realistic model of communication is very important for a practical automatic mapping system.
Reference: [6] <author> Crowl, L., Crovella, M., LeBlanc, T., and Scott, M. </author> <title> The advantages of multiple parallelizations in combinatorial search. </title> <journal> Journal of Parallel and Distributed Computing 21 (1994), </journal> <pages> 110-123. </pages>
Reference-contexts: Compiler and runtime support for task and data parallel computing is an active area of research, and several solutions have been proposed [3, 4, 8, 9, 14]. Recent research has also examined the benefits of mixed task and data parallel programming <ref> [2, 6, 13] </ref>. This paper specifically addresses the mapping of applications composed of a linear chain of data parallel tasks that act on a stream of input data sets. Each task repeatedly receives input from its predecessor task, performs its computation, and sends the output to its successor task.
Reference: [7] <author> Dinda, P., Gross, T., O'Hallaron, D., Segall, E., Stichnoth, J., Subhlok, J., Webb, J., and Yang, B. </author> <title> The CMU task parallel program suite. </title> <type> Tech. Rep. </type> <institution> CMU-CS-94-131, School of Computer Science, Carnegie Mellon University, </institution> <month> Mar. </month> <year> 1994. </year>
Reference-contexts: Figure 1 shows a chain of three tasks. A chain of tasks is a relatively simple model of task and data parallel computing, yet it is fairly common in several application areas, including computer vision, image processing, and signal processing <ref> [7] </ref>. These applications are generally driven by physical sensors like cameras and antennas. They act on streams of relatively small data sets but often have strict throughput and latency requirements. <p> The methods developed in this paper are implemented as an automatic mapping tool for the Fx compiler, and have been used to map several applications, including multibase-line stereo and narrowband tracking radar <ref> [7] </ref>. Fx is a subset High Performance Fortran [10] compiler with support for task parallelism [14, 18]. The targets for Fx are the Intel Paragon, the Intel iWarp,the IBM SP2, the Cray T3D, and networks of workstations running PVM. This paper is organized as follows. <p> In Table 1 we present results from FFT-Hist and two other applications; narrowband tracking radar and multibaseline stereo <ref> [7] </ref>. The relevant properties of the latter two programs are discussed in [13]. Table 1 shows the optimal latency obtained with and without a throughput constraint. We observe that adding a throughput constraint can lead to a mapping with a significantly higher latency.
Reference: [8] <author> Foster, I., Avalani, B., Choudhary, A., and Xu, M. </author> <title> A compilation system that integrates High Performance Fortran and Fortran M. </title> <booktitle> In Proceeding of 1994 Scalable High Performance Computing Conference (Knoxville, </booktitle> <address> TN, </address> <month> October </month> <year> 1994), </year> <pages> pp. 293-300. </pages>
Reference-contexts: Most modern parallel machines support MIMD execution, and therefore combined data and task (or function) parallel computing. Compiler and runtime support for task and data parallel computing is an active area of research, and several solutions have been proposed <ref> [3, 4, 8, 9, 14] </ref>. Recent research has also examined the benefits of mixed task and data parallel programming [2, 6, 13]. This paper specifically addresses the mapping of applications composed of a linear chain of data parallel tasks that act on a stream of input data sets.
Reference: [9] <author> Gross, T., O'Hallaron, D., and Subhlok, J. </author> <title> Task parallelism in a High Performance Fortran framework. </title> <journal> IEEE Parallel & Distributed Technology, </journal> <volume> 3 (1994), </volume> <pages> 16-26. </pages>
Reference-contexts: Most modern parallel machines support MIMD execution, and therefore combined data and task (or function) parallel computing. Compiler and runtime support for task and data parallel computing is an active area of research, and several solutions have been proposed <ref> [3, 4, 8, 9, 14] </ref>. Recent research has also examined the benefits of mixed task and data parallel programming [2, 6, 13]. This paper specifically addresses the mapping of applications composed of a linear chain of data parallel tasks that act on a stream of input data sets. <p> processors, and hence there are only 3 terms. f icom (p) = C icom 1 + C icom 2 =p + C icom 3 fl p 6 Results The execution model and the mapping algorithms presented in this paper are implemented as an automatic mapping tool in the Fx compiler <ref> [9] </ref>. We present a set of results using a 64 processor Intel iWarp, and 64 processors of an Intel Paragon.
Reference: [10] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Language Specification, </title> <note> Version 1.0, </note> <month> May </month> <year> 1993. </year>
Reference-contexts: The methods developed in this paper are implemented as an automatic mapping tool for the Fx compiler, and have been used to map several applications, including multibase-line stereo and narrowband tracking radar [7]. Fx is a subset High Performance Fortran <ref> [10] </ref> compiler with support for task parallelism [14, 18]. The targets for Fx are the Intel Paragon, the Intel iWarp,the IBM SP2, the Cray T3D, and networks of workstations running PVM. This paper is organized as follows. Section 2 introduces the algorithmic problem to be solved.
Reference: [11] <author> Ramaswamy, S., Sapatnekar, S., and Banerjee, P. </author> <title> A convex programming approach for exploiting data and functional parallelism. </title> <booktitle> In Proceedings of the 1994 International Conference on Parallel Processing (St Charles, </booktitle> <address> IL, </address> <month> August </month> <year> 1994), </year> <journal> vol. </journal> <volume> 2, </volume> <pages> pp. 116-125. </pages>
Reference-contexts: In our experience, a realistic model of communication is very important for a practical automatic mapping system. Ramaswamy et. al. <ref> [11] </ref> use convex programming to address the problem of minimizing the completion time for one data set (latency), but do not address the throughput constraints. Also, our algorithms are not tied to any particular way of modeling execution , unlike mathematical programming techniques like convex programming.
Reference: [12] <author> Sarkar, V. </author> <title> Partitioning and Scheduling Parallel Programs for Multiprocessors. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1989. </year>
Reference-contexts: This approach assumes that the execution behavior of a program is not strongly dependent on the properties of the input data sets. There is a large large volume of literature on mapping and scheduling parallel programs, and some excellent references are <ref> [1, 12, 19] </ref>. We focus only on mapping coarse grain task parallelism, and assume that a data parallel compiler is available to exploit fine grain parallelism. We compare our approach to two other research efforts that address assignment of processors to coarse grain tasks.
Reference: [13] <author> Subhlok, J., O'Hallaron, D., Gross, T., Dinda, P., and Webb, J. </author> <title> Communication and memory requirements as the basis for mapping task and data parallel programs. </title> <booktitle> In Supercomputing '94 (Washington, </booktitle> <address> DC, </address> <month> November </month> <year> 1994), </year> <pages> pp. 330-339. </pages>
Reference-contexts: Compiler and runtime support for task and data parallel computing is an active area of research, and several solutions have been proposed [3, 4, 8, 9, 14]. Recent research has also examined the benefits of mixed task and data parallel programming <ref> [2, 6, 13] </ref>. This paper specifically addresses the mapping of applications composed of a linear chain of data parallel tasks that act on a stream of input data sets. Each task repeatedly receives input from its predecessor task, performs its computation, and sends the output to its successor task. <p> In Table 1 we present results from FFT-Hist and two other applications; narrowband tracking radar and multibaseline stereo [7]. The relevant properties of the latter two programs are discussed in <ref> [13] </ref>. Table 1 shows the optimal latency obtained with and without a throughput constraint. We observe that adding a throughput constraint can lead to a mapping with a significantly higher latency. The sole exception is the radar program, for which the latency remains unchanged.
Reference: [14] <author> Subhlok, J., Stichnoth, J., O'Hallaron, D., and Gross, T. </author> <title> Exploiting task and data parallelism on a multicomputer. </title> <booktitle> In ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (San Diego, </booktitle> <address> CA, </address> <month> May </month> <year> 1993), </year> <pages> pp. 13-22. </pages>
Reference-contexts: Most modern parallel machines support MIMD execution, and therefore combined data and task (or function) parallel computing. Compiler and runtime support for task and data parallel computing is an active area of research, and several solutions have been proposed <ref> [3, 4, 8, 9, 14] </ref>. Recent research has also examined the benefits of mixed task and data parallel programming [2, 6, 13]. This paper specifically addresses the mapping of applications composed of a linear chain of data parallel tasks that act on a stream of input data sets. <p> The methods developed in this paper are implemented as an automatic mapping tool for the Fx compiler, and have been used to map several applications, including multibase-line stereo and narrowband tracking radar [7]. Fx is a subset High Performance Fortran [10] compiler with support for task parallelism <ref> [14, 18] </ref>. The targets for Fx are the Intel Paragon, the Intel iWarp,the IBM SP2, the Cray T3D, and networks of workstations running PVM. This paper is organized as follows. Section 2 introduces the algorithmic problem to be solved.
Reference: [15] <author> Subhlok, J., and Vondran, G. </author> <title> Optimal mapping of sequences of data parallel tasks. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (Santa Barbara, </booktitle> <address> CA, </address> <month> July </month> <year> 1995), </year> <pages> pp. 134-143. </pages>
Reference-contexts: Also, our algorithms are not tied to any particular way of modeling execution , unlike mathematical programming techniques like convex programming. Finally, both these approaches address the processor assignment problem, but do not address clustering of tasks into modules, or replication of tasks. Our earlier work <ref> [15] </ref> addressed unconstrained throughput optimization, while this paper addresses tradeoffs between latency and throughput optimization. The methods developed in this paper are implemented as an automatic mapping tool for the Fx compiler, and have been used to map several applications, including multibase-line stereo and narrowband tracking radar [7]. <p> We believe that this is a practically useful way to use these algorithms for large numbers of processors. 4 Throughput optimization The general throughput optimization problem is to find a mapping that corresponds to the maximum throughput with a maximum latency constraint. In previous work <ref> [15] </ref>, we developed an algorithm for optimizing the throughput, but without a latency constraint. It appears that there is no polynomial solution to the problem of optimizing throughput with a latency constraint. <p> An initial range for the throughput can be determined by solving the unconstrained throughput problem, which has the same complexity as the general latency problem <ref> [15] </ref>. Therefore, the general throughput problem can be solved in O (log (H 0 =ffi)P 4 k 2 ) time by this procedure, where H 0 is the range of possible throughput values, and ffi is the maximum allowed difference between the approximate and the exact optimal throughput.
Reference: [16] <author> Vondran, G. </author> <title> Optimization of latency, throughput and processors for pipelines of data parallel tasks. </title> <type> Master's thesis, </type> <institution> Dept. of Electrical and Computer Engineering, Carnegie Mellon University, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: We outline the changes to the dynamic programming algorithm to take clustering into account. Because of lack of space, we will not present the modified equation system and, and instead refer to <ref> [16] </ref>.
Reference: [17] <author> Webb, J. </author> <title> Latency and bandwidth consideration in parallel robotics image processing. </title> <booktitle> In Supercomputing '93 (Portland, </booktitle> <address> OR, </address> <month> Nov. </month> <year> 1993), </year> <pages> pp. 230-239. </pages>
Reference-contexts: These applications are generally driven by physical sensors like cameras and antennas. They act on streams of relatively small data sets but often have strict throughput and latency requirements. An example application is multibaseline stereo <ref> [17] </ref>, where the first task captures three (or more) images from the cameras, the second task computes a difference image for each of 16 disparity levels, the third task computes an error image for each difference image, and the final task performs a minimum reduction across error images and computes the
Reference: [18] <author> Yang, B., Webb, J., Stichnoth, J., O'Hallaron, D., and Gross, T. Do&merge: </author> <title> Integrating parallel loops and reductions. </title> <booktitle> In Sixth Annual Workshop on Languages and Compilers for Parallel Computing (Portland, </booktitle> <address> Oregon, </address> <month> Aug </month> <year> 1993). </year>
Reference-contexts: The methods developed in this paper are implemented as an automatic mapping tool for the Fx compiler, and have been used to map several applications, including multibase-line stereo and narrowband tracking radar [7]. Fx is a subset High Performance Fortran [10] compiler with support for task parallelism <ref> [14, 18] </ref>. The targets for Fx are the Intel Paragon, the Intel iWarp,the IBM SP2, the Cray T3D, and networks of workstations running PVM. This paper is organized as follows. Section 2 introduces the algorithmic problem to be solved.
Reference: [19] <author> Yang, T. </author> <title> Scheduling and Code Generation for Parallel Architectures. </title> <type> PhD thesis, </type> <institution> Rutgers University, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: This approach assumes that the execution behavior of a program is not strongly dependent on the properties of the input data sets. There is a large large volume of literature on mapping and scheduling parallel programs, and some excellent references are <ref> [1, 12, 19] </ref>. We focus only on mapping coarse grain task parallelism, and assume that a data parallel compiler is available to exploit fine grain parallelism. We compare our approach to two other research efforts that address assignment of processors to coarse grain tasks.
References-found: 19

