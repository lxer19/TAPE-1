URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-260.ps.Z
Refering-URL: http://www-white.media.mit.edu/cgi-bin/tr_pagemaker/
Root-URL: http://www.media.mit.edu
Email: steve@media.mit.edu  
Phone: Tel. (617) 253-9610; Fax. (617) 253-8874  
Author: Steve Mann, NNLF 
Address: Building E15-389, 20 Ames Street, Cambridge, MA02139  
Affiliation: MIT Media Lab,  
Web: reality'  
Note: `Mediated  
Abstract: M.I.T Media Laboratory Perceptual Computing Section Technical Report No. 260 Submitted to PRESENCE Special Issue on Augmented Reality Abstract The general spirit and intent of Augmented Reality (AR) is to add virtual objects to the real world. A typical AR apparatus consists of a video display with partially transparent visor, upon which computer-generated information is overlayed. The general spirit of what is proposed, like typical AR, includes adding virtual objects, but also includes the desire to take away, alter, or more generally to visually `mediate' real objects, using a body-worn apparatus where both the real and virtual objects are placed on an equal footing, in the sense that both are presented together via a synthetic medium. Successful implementations have been realized by viewing the real world using a head-mounted display (HMD) fitted with video camera(s), body-worn processing, and/or bidirectional wireless communications. This portability enabled various forms of the apparatus to be tested extensively in everyday circumstances, such as while riding the bus, or shopping. The proposed approach shows promise in applications where it is desired to have the ability to reconfigure reality. For example, color may be deliberately diminished or completely removed from the real world at certain times when it is desired to highlight parts of a virtual world with graphic objects having unique colors. The fact that vision may be completely reconfigured also suggests utility to the visually handicapped.
Abstract-found: 1
Intro-found: 1
Reference: [jhu, 1995] <institution> (1995). Lions vision research and rehabilitation center. </institution> <address> http://www.wilmer.jhu.edu/low vis/low vis.htm. </address>
Reference-contexts: Researchers at Johns Hopkins University have been experimenting with the use of cameras and head-mounted displays for helping the visually handicapped. Their approach has been to use the optics of the cameras for magnification, together with the contrast adjustments of the video display to increase apparent scene contrast <ref> [jhu, 1995] </ref>. <p> One of the most exciting developments in the field of low vision is the Low Vision Enhancement System (LVES). This is an electronic vision enhancement system that provides contrast enhancement... Future enhancements to the device include text manipulation, autofocus and image remapping. (quote from their WWW page <ref> [jhu, 1995] </ref>, emphasis added). This research effort suggests the utility of the real-time visual mappings (Fig 6) successfully implemented using the apparatus of Fig 3.
Reference: [Anstis, 1992] <author> Anstis, S. </author> <year> (1992). </year> <title> Visual adaptation to a negative, brightness-reversed world: some preliminary observations. </title> <editor> In Carpenter, G. and Grossberg, S., editors, </editor> <booktitle> Neural Networks for Vision and Image Processing, </booktitle> <pages> pages 1-15. </pages> <publisher> MIT Press. </publisher>
Reference-contexts: This simple example points out the fact that a `mediated reality' system need not function as just a `reality enhancer', but rather, it may enhance, alter, or deliberately degrade reality. Stuart Anstis <ref> [Anstis, 1992] </ref>, using a camcorder that had a "negation" switch on the viewfinder, experimented with living in a "negated" world. He walked around holding the camcorder up to one eye, looking through it, and observed that he was unable to learn to recognize faces in a negated world.
Reference: [Arfken, 1985] <author> Arfken, G. </author> <year> (1985). </year> <title> Mathematical Methods for Physicists. </title> <publisher> Academic Press, </publisher> <address> Orlando, Florida, third edition. </address>
Reference-contexts: good description of linear time-invariant systems may be found in a communications or electrical engineering textbook such as [Haykin, 1983].) The optical transformation to greyscale, described earlier, could also be realized by a `visual filter' (Fig 1 (a)) that is a linear time-invariant system, in particular, a linear integral operator <ref> [Arfken, 1985] </ref> (page 669) that, for each ray of light, collapses all wavelengths into a single quantity giving rise to a ray of light, having a flat spectrum, emerging from the other side.
Reference: [Azuma, 1994] <author> Azuma, R. </author> <year> (1994). </year> <title> Registration Errors in Augmented Reality: </title> <institution> NSF/ARPA Science and Technology Center for Computer Graphics and Scientific Visualization . http://www.cs.unc.edu/~azuma/azuma AR.html. </institution>
Reference-contexts: In addition to this `chromatic mediation', other forms of `mediated reality' are often useful. 1.2.1 Registration between real and virtual worlds Alignment of the real and virtual worlds is very important, as indicated in the following quote <ref> [Azuma, 1994] </ref>: Unfortunately, registration is a difficult problem, for a number of reasons. First, the human visual system is very good at detecting even small misregistrations, because of the resolution of the fovea and the sensitivity of the human visual system to differences.
Reference: [Bush, 1945] <author> Bush, V. </author> <year> (1945). </year> <title> As we may think. </title> <journal> Atlantic Monthly. </journal> <note> http://www2.theatlantic.com/atlantic/atlweb/flashbks/ computer/bushf.htm. </note>
Reference-contexts: A TINY CAMERA FITTED WITH UNIVERSAL-FOCUS LENS. THE SMALL SQUARE IN THE EYEGLASS AT THE LEFT SIGHTS THE OBJECT" 7 A new cinematographic reality In 1945, Vannevar Bush described a wearable camera (Fig 14) that would record whatever the wearer was looking at onto microfilm <ref> [Bush, 1945] </ref>.
Reference: [Clynes, 1960] <author> Clynes, M. </author> <month> (September </month> <year> 1960). </year> <title> Cyborgs and space. </title> <booktitle> Astronautics, </booktitle> <pages> pages 26,27,and 74-75, pages 26,27,and 74-75. </pages>
Reference-contexts: In some sense I subsume the visual reconfiguration induced by the apparatus into my brain, so that, in a sense, the apparatus and I act as a single unit. Manfred Clynes uses the example of a person riding a bicycle to describe this sort of synergism <ref> [Clynes, 1960] </ref> where, after sufficient adaptation time, conscious effort is no longer needed in order to use the machine. He refers to this state as cyborgian. <p> With my biosensors, I hope it will be possible to have a visual rembrance agent that has an awareness of my affective state [Picard, 1995] and operates without conscious thought or effort <ref> [Clynes, 1960] </ref>. 6 Wearable Interactive Video Environment (WIVE) 6.1 Drawing in the air Video environments like Myron Krueger's and the ALIVE are useful because they recognizes the user's gestures. Similarly, the RM can be used to allow my body-worn computer 2 to recognize my own jestures.
Reference: [Dolezal, 1982] <author> Dolezal, H. </author> <year> (1982). </year> <title> Living in a world transformed. Academic press series in cognition and perception. </title> <publisher> Academic press, Chicago, Illinois. </publisher>
Reference-contexts: Stratton, upon first wearing the glasses, reported seeing the world upside-down, but, after an adaptation period of several days, was able to function completely normally with the glasses on. Dolezal <ref> [Dolezal, 1982] </ref> (page 19) describes "various types of optical transformations", such as the inversion explored by Stratton, as well as displacement, reversal, tilt, magnification, and scrambling. Kohler [Kohler, 1964] also discusses "transformation of the perceptual world". <p> In some sense both the regular eyeglasses that people commonly wear, as well as the special glasses researchers have used in prism adaptation experiments [Kohler, 1964] <ref> [Dolezal, 1982] </ref> are reality mediators, but it appears that Anstis was the first to explore, in detail, an electronically mediated world. 2.4 The Reconfigured Eyes Using my `reality mediator', I repeated the classic experiments like those of Stratton and Anstis (e.g. living in an upside-down or negated world), as well as <p> This research effort suggests the utility of the real-time visual mappings (Fig 6) successfully implemented using the apparatus of Fig 3. The idea of living in a coordinate transformed world has been explored extensively by other authors [Kohler, 1964] <ref> [Dolezal, 1982] </ref>, using optical methods (such as prisms and the like). Much could be written about my experiences in various electronically coordinate transformed worlds, but a detailed account of all of the various experiences is beyond the scope of this paper. <p> Also thanks to Chuck Oman for pointing out references <ref> [Dolezal, 1982] </ref> and [Kohler, 1964].
Reference: [Drascic, 1993] <author> Drascic, D. </author> <year> (1993). </year> <note> David drascic's papers and presentations. http://vered.rose.utoronto.ca/people/david dir/Bibliography.html. 15 </note>
Reference-contexts: Other research groups [Fuchs et al., ] also contributed to this development. Some research in AR arises from work in telepresence <ref> [Drascic, 1993] </ref>. AR, although lesser known than VR, is currently used in some specific applications. <p> Thus two views, one for each eye, suffice to create a reasonable `illusion of transparency'. Others [Fuchs et al., ] <ref> [Drascic, 1993] </ref> [Nagao, 1995] have also explored video-based `illusory transparency', augmenting it with virtual overlays. Nagao, in the context of his hand-held TV set with single camera [Nagao, 1995] calls the it "video see-through". <p> Nagao, in the context of his hand-held TV set with single camera [Nagao, 1995] calls the it "video see-through". It is worth noting that whenever `illusory transparency' is used, as in the work of [Fuchs et al., ] <ref> [Drascic, 1993] </ref> [Nagao, 1995] reality will be `mediated', whether or not that mediation was intended. At the very least this mediation takes on the form of limited dynamic range and color gamut, as well as some kind of distortion, which may be modeled as a 2D coordinate transformation.
Reference: [Earnshaw et al., 1993] <author> Earnshaw, R. A., Gigante, M. A., and Jones, H. </author> <year> (1993). </year> <title> Virtual reality systems. </title> <publisher> Academic press. </publisher>
Reference-contexts: 1 Introduction Ivan Sutherland, a pioneer in the field of computer graphics, described a head-mounted display with half-silvered mirrors so that the wearer could see a virtual world superimposed on reality <ref> [Earnshaw et al., 1993] </ref> [Sutherland, 1968], giving rise to "Augmented Reality (AR)". Others have adopted Sutherland's concept of a Head-Mounted Display (HMD) but generally without the see-through capability. An artificial environment in which the user cannot see through the display is generally referred as a Virtual Reality (VR) environment. <p> the signal path may now be conveniently interrupted so that a `visual filter' may be installed, to some degree, behaving as though it were positioned between the eye and the brain. 3 Partially mediated reality Artificial Reality is a term defined by Myron Krueger to describe video-based, computer-mediated interactive media <ref> [Earnshaw et al., 1993] </ref>. His apparatus consisted of a video display (screen) with a camera above it that projected a 2D outline of the user together with sprite-like objects.
Reference: [Edgerton, 1979] <author> Edgerton, H. E. </author> <year> (1979). </year> <title> Electronic flash, </title> <publisher> strobe. MIT Press, </publisher> <address> Cambridge, Massachusetts. </address>
Reference-contexts: Looking at airplanes, I could see the number of blades on the spinning propellers, and, depending on the sampling rate of my RM, the blades would appear to rotate slowly backwards or forwards, in much the same way as objects do under the stroboscopic lights of Harold Edgerton <ref> [Edgerton, 1979] </ref>. By manually adjusting the processing parameters of my RM, I could see many things that escape normal vision. 2.4.4 Virtual `smart strobe' By applying machine vision (some rudimentary intelligence) to the incoming video, the RM should be able to decide what sampling rate to apply.
Reference: [Feiner et al., 1993a] <author> Feiner, MacIntyre, </author> <title> and Seligmann (1993a). Karma (knowledge-based augmented reality for maintenance assistance). </title> <address> http://www.cs.columbia.edu/graphics/projects/karma/ karma.html. </address>
Reference-contexts: In particular, the `visual filter' may itself embody the ability to create computer-generated objects and therefore subsume the "virtual" channel. worlds could be registered [Feiner et al., 1993b] <ref> [Feiner et al., 1993a] </ref>. Other research groups [Fuchs et al., ] also contributed to this development. Some research in AR arises from work in telepresence [Drascic, 1993]. AR, although lesser known than VR, is currently used in some specific applications. <p> The approach presented here might also be useful within the context of work done by other researchers, such as Knowledge-based Augmented Reality for Maintenance Assistance (KARMA) [Feiner et al., 1993b] <ref> [Feiner et al., 1993a] </ref>, seeing architectural anatomy of buildings [Feiner et al., 1995], and combining ultrasound with virtual reality in obstetrics [Fuchs et al., ]. 13 (a) (b) (c) antenna (inbound transmit channel, operating at microwave frequencies), while the outbound receive signal (UHF frequencies) arrives via the longer antenna and is
Reference: [Feiner et al., 1993b] <author> Feiner, MacIntyre, </author> <month> and Seligmann (Jul </month> <year> 1993b). </year> <title> Knowledge-based augmented reality. </title> <journal> Communications of the ACM, </journal> <volume> 36(7). </volume>
Reference-contexts: In particular, the `visual filter' may itself embody the ability to create computer-generated objects and therefore subsume the "virtual" channel. worlds could be registered <ref> [Feiner et al., 1993b] </ref> [Feiner et al., 1993a]. Other research groups [Fuchs et al., ] also contributed to this development. Some research in AR arises from work in telepresence [Drascic, 1993]. AR, although lesser known than VR, is currently used in some specific applications. <p> The approach presented here might also be useful within the context of work done by other researchers, such as Knowledge-based Augmented Reality for Maintenance Assistance (KARMA) <ref> [Feiner et al., 1993b] </ref> [Feiner et al., 1993a], seeing architectural anatomy of buildings [Feiner et al., 1995], and combining ultrasound with virtual reality in obstetrics [Fuchs et al., ]. 13 (a) (b) (c) antenna (inbound transmit channel, operating at microwave frequencies), while the outbound receive signal (UHF frequencies) arrives via the
Reference: [Feiner et al., 1995] <author> Feiner, Webster, Krueger, MacIntyre, </author> <title> and Keller (1995). Architectural anatomy. </title> <journal> Presence, </journal> <volume> 4(3), </volume> <pages> 318-325. </pages>
Reference-contexts: The approach presented here might also be useful within the context of work done by other researchers, such as Knowledge-based Augmented Reality for Maintenance Assistance (KARMA) [Feiner et al., 1993b] [Feiner et al., 1993a], seeing architectural anatomy of buildings <ref> [Feiner et al., 1995] </ref>, and combining ultrasound with virtual reality in obstetrics [Fuchs et al., ]. 13 (a) (b) (c) antenna (inbound transmit channel, operating at microwave frequencies), while the outbound receive signal (UHF frequencies) arrives via the longer antenna and is fed to my right-eyed display (modified Virtual Vision system
Reference: [Fuchs et al., ] <author> Fuchs, H., Bajura, M., and Ohbuchi, R. </author> <title> Teaming ultrasound data with virtual reality in obstetrics. </title> <address> http://www.ncsa.uiuc.edu/Pubs/MetaCenter/SciHi93/ 1c.Highlights-BiologyC.html. </address>
Reference-contexts: In particular, the `visual filter' may itself embody the ability to create computer-generated objects and therefore subsume the "virtual" channel. worlds could be registered [Feiner et al., 1993b] [Feiner et al., 1993a]. Other research groups <ref> [Fuchs et al., ] </ref> also contributed to this development. Some research in AR arises from work in telepresence [Drascic, 1993]. AR, although lesser known than VR, is currently used in some specific applications. <p> Thus two views, one for each eye, suffice to create a reasonable `illusion of transparency'. Others <ref> [Fuchs et al., ] </ref> [Drascic, 1993] [Nagao, 1995] have also explored video-based `illusory transparency', augmenting it with virtual overlays. Nagao, in the context of his hand-held TV set with single camera [Nagao, 1995] calls the it "video see-through". <p> Nagao, in the context of his hand-held TV set with single camera [Nagao, 1995] calls the it "video see-through". It is worth noting that whenever `illusory transparency' is used, as in the work of <ref> [Fuchs et al., ] </ref> [Drascic, 1993] [Nagao, 1995] reality will be `mediated', whether or not that mediation was intended. <p> approach presented here might also be useful within the context of work done by other researchers, such as Knowledge-based Augmented Reality for Maintenance Assistance (KARMA) [Feiner et al., 1993b] [Feiner et al., 1993a], seeing architectural anatomy of buildings [Feiner et al., 1995], and combining ultrasound with virtual reality in obstetrics <ref> [Fuchs et al., ] </ref>. 13 (a) (b) (c) antenna (inbound transmit channel, operating at microwave frequencies), while the outbound receive signal (UHF frequencies) arrives via the longer antenna and is fed to my right-eyed display (modified Virtual Vision system using a high-resolution CRT instead of the original LCD). (b) View of
Reference: [Hancock et al., 1995] <author> Hancock, L., Kalb, C., and Under-hill, W. </author> <month> (July 17, </month> <year> 1995). </year> <title> You don't have to smile. </title> <publisher> Newsweek. </publisher>
Reference: [Haykin, 1983] <author> Haykin, S. </author> <year> (1983). </year> <title> Communication Systems. </title> <publisher> Wiley, second edition. </publisher>
Reference-contexts: Each of these "optical transformations" could be realized by selecting a particular linear time-invariant system as the visual filter in Fig 1. (A good description of linear time-invariant systems may be found in a communications or electrical engineering textbook such as <ref> [Haykin, 1983] </ref>.) The optical transformation to greyscale, described earlier, could also be realized by a `visual filter' (Fig 1 (a)) that is a linear time-invariant system, in particular, a linear integral operator [Arfken, 1985] (page 669) that, for each ray of light, collapses all wavelengths into a single quantity giving rise
Reference: [Hilaire et al., 1990] <author> Hilaire, P. S., Benton, S., and Lu-cente, M. </author> <year> (1990). </year> <title> Electronic display system for computational holography. </title> <booktitle> In SPIE Proceedings #1212 "Practical holography IV", </booktitle> <pages> pages 174-182. </pages>
Reference-contexts: This hypothetical `lightspace glass', would be essentially an ideal holographic video camera and ideal holographic video display in one. Holographic video cameras and holovideo displays have actually been built <ref> [Hilaire et al., 1990] </ref> [Lucente et al., 1992], and the two have been connected, though the apparatus occupied some large optical benches and a room full of equipment. It would not even fit in a small room, let alone be small enough to wear in a pair of eye glasses.
Reference: [HIROSE, 1994] <author> HIROSE, M. </author> <year> (1994). </year> <note> Welcome to our hirose laboratory !! http://ghidorah.t.u-tokyo.ac.jp/index.html. </note>
Reference-contexts: The image mosaicing principle (e.g. figures like Fig 11) has been previously explored [Hirose et al., 1994] [Mann and Picard, 1994b] [Szeliski and Coughlan, 1994]. Of particular interest is the work using image mosaic-ing in a virtual reality environment [Hirose et al., 1994] <ref> [HIROSE, 1994] </ref>. The system depicted in Fig 3 allows me to explore the video mosaics of Hirose as I walk around outdoors, in my day to day activities.
Reference: [Hirose et al., 1994] <author> Hirose, M., Takahashi, K., Koshizuka, T., and Watanabe, Y. </author> <year> (1994). </year> <title> A study on image editing technology for synthetic sensation. </title> <booktitle> ICAT '94 Proceedings, </booktitle> <address> pp.63-70. </address>
Reference-contexts: All of the images may be brought into a single coordinate system [Mann and Picard, 1994c] (Fig 10). Once the images are stabilized, they can be assembled together to make an image mosaic (Fig 11). The image mosaicing principle (e.g. figures like Fig 11) has been previously explored <ref> [Hirose et al., 1994] </ref> [Mann and Picard, 1994b] [Szeliski and Coughlan, 1994]. Of particular interest is the work using image mosaic-ing in a virtual reality environment [Hirose et al., 1994] [HIROSE, 1994]. <p> The image mosaicing principle (e.g. figures like Fig 11) has been previously explored <ref> [Hirose et al., 1994] </ref> [Mann and Picard, 1994b] [Szeliski and Coughlan, 1994]. Of particular interest is the work using image mosaic-ing in a virtual reality environment [Hirose et al., 1994] [HIROSE, 1994]. The system depicted in Fig 3 allows me to explore the video mosaics of Hirose as I walk around outdoors, in my day to day activities.
Reference: [Kohler, 1964] <author> Kohler, I. </author> <year> (1964). </year> <title> The formation and transformation of the perceptual world, </title> <booktitle> volume 3 of Pshcho-logical issues. </booktitle> <publisher> International university press, </publisher> <address> 227 West 13 Street. monograph 12. </address>
Reference-contexts: Dolezal [Dolezal, 1982] (page 19) describes "various types of optical transformations", such as the inversion explored by Stratton, as well as displacement, reversal, tilt, magnification, and scrambling. Kohler <ref> [Kohler, 1964] </ref> also discusses "transformation of the perceptual world". <p> In some sense both the regular eyeglasses that people commonly wear, as well as the special glasses researchers have used in prism adaptation experiments <ref> [Kohler, 1964] </ref> [Dolezal, 1982] are reality mediators, but it appears that Anstis was the first to explore, in detail, an electronically mediated world. 2.4 The Reconfigured Eyes Using my `reality mediator', I repeated the classic experiments like those of Stratton and Anstis (e.g. living in an upside-down or negated world), as <p> This research effort suggests the utility of the real-time visual mappings (Fig 6) successfully implemented using the apparatus of Fig 3. The idea of living in a coordinate transformed world has been explored extensively by other authors <ref> [Kohler, 1964] </ref> [Dolezal, 1982], using optical methods (such as prisms and the like). Much could be written about my experiences in various electronically coordinate transformed worlds, but a detailed account of all of the various experiences is beyond the scope of this paper. <p> Also thanks to Chuck Oman for pointing out references [Dolezal, 1982] and <ref> [Kohler, 1964] </ref>.
Reference: [Lucente et al., 1992] <author> Lucente, M., Hilaire, P. S., and Ben-ton, S. </author> <year> (1992). </year> <title> A new approach to holographic video. </title> <booktitle> SPIE Proceedings #1732 "Holography '92". </booktitle>
Reference-contexts: This hypothetical `lightspace glass', would be essentially an ideal holographic video camera and ideal holographic video display in one. Holographic video cameras and holovideo displays have actually been built [Hilaire et al., 1990] <ref> [Lucente et al., 1992] </ref>, and the two have been connected, though the apparatus occupied some large optical benches and a room full of equipment. It would not even fit in a small room, let alone be small enough to wear in a pair of eye glasses.
Reference: [Maes et al., ] <author> Maes, Darrell, Blumberg, and Pentland. </author> <title> The alive system: Full-body interaction with animated autonomous agents. </title> <type> TR 257, </type> <institution> M.I.T. Media Lab Perceptual Computing Section, </institution> <address> Cambridge, Ma. </address>
Reference-contexts: The Artificial Life Interactive Video Environment (ALIVE) <ref> [Maes et al., ] </ref> is similar to Myron Krueger's environment. In the ALIVE, a user sees him/her self in a "magic mirror" created by displaying a left-right reversed video image from a camera above the screen.
Reference: [Mann, 1994a] <author> Mann, S. </author> <year> (1994a). </year> <title> Lightspace: Recording varying shadows and highlights that reproduce with same varying illumination during reconstruction. </title> <type> Technical Report 348, </type> <institution> M.I.T. Media Lab Perceptual Computing Section, Boston, Massachusetts. </institution>
Reference-contexts: In either case (idealized or practical), the entire apparatus will be referred to as a `Reality Mediator' (RM). 1.1 `lightspace glass' In what follows, a simplified model <ref> [Mann, 1994a] </ref> rays of light will be used. This simplified model neglects both wave-like properties (such as diffraction) and particle-like properties (such as quantum effects) of light. (The model also assumes the existence of the abstract notion of instantaneous frequency). <p> Nevertheless, perhaps in years to come, these technologies could become feasible. The use of a hypothetical `lightspace glass' to measure the way a scene responds to light has been previously proposed <ref> [Mann, 1994a] </ref>, together with a more practical realization over a very limited domain (`lightspace' of a static monochromatic scene), though this apparatus has not been realized for moving scenes, and certainly not in a small enough package to be body-worn.
Reference: [Mann, 1994b] <author> Mann, S. </author> <year> (1994b). </year> <title> Personal WWW page: `put yourself in my shoes and see the world from my perspective'. </title> <address> http://wearcam.org. </address>
Reference-contexts: a different time zone, say somewhere in the world where it is morning, so the virtual escort has fresh alert eyes). 4.2 On the "safety versus privacy" argument The ease with which wearable wireless video cameras allow one to roam about and share viewpoints with others raises many privacy issues <ref> [Mann, 1994b] </ref>, and it is important to look at these issues within the broader context of video privacy in general. When I first joined the Media Lab, I expressed concern regarding the possible development of surveillance technologies, such as ubiquitous use of video cameras, face recognition and the like. <p> This need arises from my limited peripheral vision. After wearing the apparatus for some time, I learn how to compensate for defficiencies such as limited peripheral vision and limited dynamic range. The resulting video <ref> [Mann, 1994b] </ref> is much more like having extracted a signal from my eye than the signal arising from the traditional point-of-view methods, because I live with the RM over an extended period of time and, after time, it begins to behave as though it truly were an extension of my own
Reference: [Mann and Picard, 1994a] <author> Mann, S. and Picard, R. </author> <year> (1994a). </year> <title> Being `undigital' with digital cameras: Extending dynamic range by combining differently exposed pictures. </title> <type> Technical Report 323, </type> <institution> M.I.T. Media Lab Perceptual Computing Section, Boston, Massachusetts. </institution> <note> Also appears, IS&T's 46th annual conference, </note> <month> May </month> <year> 1995. </year>
Reference-contexts: The resulting extended-response video may be displayed on a conventional HMD by using Stockham's homomorphic filter [T. G. Stockham, Jr., 1972] as the `visual filter'. The principle of extending dynamic range by combining differently exposed pictures is known as the Wyckoff principle <ref> [Mann and Picard, 1994a] </ref>, in honor of Charles Wyckoff. Using a Wyckoff composite, I could be outside on bright sunny days and see shadow detail when I looked into open doorways to dark interiours, as well as see detail in bright objects like the sun.
Reference: [Mann and Picard, 1994b] <author> Mann, S. and Picard, R. W. </author> <year> (1994b). </year> <title> `virtual bellows': Assembling video into high quality still images. </title> <type> TR 259, </type> <institution> M.I.T. Media Lab Perceptual Computing Section, </institution> <address> Cambridge, Ma. </address>
Reference-contexts: Once the images are stabilized, they can be assembled together to make an image mosaic (Fig 11). The image mosaicing principle (e.g. figures like Fig 11) has been previously explored [Hirose et al., 1994] <ref> [Mann and Picard, 1994b] </ref> [Szeliski and Coughlan, 1994]. Of particular interest is the work using image mosaic-ing in a virtual reality environment [Hirose et al., 1994] [HIROSE, 1994]. <p> Head rotation is tracked using the featureless video orbits method <ref> [Mann and Picard, 1994b] </ref>. This interactive video environment allows me to explore the world in new and interesting ways, and to see how well different views of, say, a building, will fit together into an image mosaic.
Reference: [Mann and Picard, 1994c] <author> Mann, S. and Picard, R. W. </author> <year> (1994c). </year> <title> Virtual bellows: constructing high-quality images from video. </title> <booktitle> In Proceedings of the IEEE first international conference on image processing, </booktitle> <address> Austin, Texas. </address>
Reference-contexts: All of the images may be brought into a single coordinate system <ref> [Mann and Picard, 1994c] </ref> (Fig 10). Once the images are stabilized, they can be assembled together to make an image mosaic (Fig 11). The image mosaicing principle (e.g. figures like Fig 11) has been previously explored [Hirose et al., 1994] [Mann and Picard, 1994b] [Szeliski and Coughlan, 1994].
Reference: [McGreevy, 1992] <author> McGreevy, M. W. </author> <year> (1992). </year> <title> The presence of field geologists in mars-like terrain. </title> <journal> PRESENCE, </journal> <volume> 1(4) </volume> <pages> 375-403. </pages> <publisher> MIT Press. </publisher>
Reference-contexts: A reality mediator may also be useful to those who are really only interested in designing a traditional beam-splitter-based AR system because RM could be used as a development tool, and could also be used to explore new conceptual frameworks. 2.2 Mediated presence McGreevy <ref> [McGreevy, 1992] </ref> explored the use of a "head-mounted camera/display/recorder system" that deprived the user of both color and stereo vision. Even though it had 2 cameras: "The lack of stereo vision provided in the head-mounted display prompted both subjects to use alternative cues to make spatial judgements [McGreevy, 1992]" (His subjects <p> Mediated presence McGreevy <ref> [McGreevy, 1992] </ref> explored the use of a "head-mounted camera/display/recorder system" that deprived the user of both color and stereo vision. Even though it had 2 cameras: "The lack of stereo vision provided in the head-mounted display prompted both subjects to use alternative cues to make spatial judgements [McGreevy, 1992]" (His subjects would move their heads back and forth to and perceive depth from the induced motion parralax.) He referred to the experience as a "mediated presence". His head-mounted camera/display system was a very simple form of reality mediator, where the mediation was fixed (e.g. not computer controllable).
Reference: [Mitchell, 1992] <author> Mitchell, W. J. </author> <year> (1992). </year> <title> The Reconfigured Eye. </title> <publisher> The MIT Press. </publisher>
Reference-contexts: Falsification of video surveillance recordings is a point addressed in the movie Rising Sun, and in William Mitchell's book, The Reconfigured Eye <ref> [Mitchell, 1992] </ref>. However, if there is a chance that individuals might have their own account of what happened, organizations using surveillance would not even consider falsifying surveillance data. Even though it is easy to falsify images [Mitchell, 1992], when accounts of what happened differ, further investigation would be called for. <p> addressed in the movie Rising Sun, and in William Mitchell's book, The Reconfigured Eye <ref> [Mitchell, 1992] </ref>. However, if there is a chance that individuals might have their own account of what happened, organizations using surveillance would not even consider falsifying surveillance data. Even though it is easy to falsify images [Mitchell, 1992], when accounts of what happened differ, further investigation would be called for.
Reference: [Nagao, 1995] <author> Nagao, K. </author> <year> (1995). </year> <title> Ubiquitous talker: Spoken language interaction with real world objects. </title> <address> http://www.csl.sony.co.jp/person/nagao.html. </address>
Reference-contexts: Thus two views, one for each eye, suffice to create a reasonable `illusion of transparency'. Others [Fuchs et al., ] [Drascic, 1993] <ref> [Nagao, 1995] </ref> have also explored video-based `illusory transparency', augmenting it with virtual overlays. Nagao, in the context of his hand-held TV set with single camera [Nagao, 1995] calls the it "video see-through". <p> Thus two views, one for each eye, suffice to create a reasonable `illusion of transparency'. Others [Fuchs et al., ] [Drascic, 1993] <ref> [Nagao, 1995] </ref> have also explored video-based `illusory transparency', augmenting it with virtual overlays. Nagao, in the context of his hand-held TV set with single camera [Nagao, 1995] calls the it "video see-through". It is worth noting that whenever `illusory transparency' is used, as in the work of [Fuchs et al., ] [Drascic, 1993] [Nagao, 1995] reality will be `mediated', whether or not that mediation was intended. <p> Nagao, in the context of his hand-held TV set with single camera <ref> [Nagao, 1995] </ref> calls the it "video see-through". It is worth noting that whenever `illusory transparency' is used, as in the work of [Fuchs et al., ] [Drascic, 1993] [Nagao, 1995] reality will be `mediated', whether or not that mediation was intended. At the very least this mediation takes on the form of limited dynamic range and color gamut, as well as some kind of distortion, which may be modeled as a 2D coordinate transformation.
Reference: [Patton, 1995] <author> Patton, P. </author> <year> (1995). </year> <note> Caught. WIRED. </note>
Reference-contexts: However, if we look along a different dimension, characterized by symmetry, the small town and the Orwellian future are exact opposites. In a small town, the sheriff knows what everyone's 10 up to, but everyone also knows what the sheriff is up to. Phil Patton <ref> [Patton, 1995] </ref> discusses the surveillance dilemma, making reference to the ubiquitous "ceiling domes of wine-dark opacity", making mention that "many department stores use hidden cameras behind one-way mirrors in fitting rooms", and in general, that there is much more video surveillance than we might at first think.
Reference: [Picard, 1995] <author> Picard, R. W. </author> <year> (1995). </year> <title> Affective computing. </title> <type> TR 321, </type> <institution> MIT Media Lab. </institution>
Reference-contexts: In addition to video input on my wearable apparatus, I have a variety of other devices, such as biosensors. With my biosensors, I hope it will be possible to have a visual rembrance agent that has an awareness of my affective state <ref> [Picard, 1995] </ref> and operates without conscious thought or effort [Clynes, 1960]. 6 Wearable Interactive Video Environment (WIVE) 6.1 Drawing in the air Video environments like Myron Krueger's and the ALIVE are useful because they recognizes the user's gestures.
Reference: [Shaw, 1966] <author> Shaw, B. </author> <year> (1966). </year> <title> Light of Other Days. Analog. </title>
Reference-contexts: out across the river, I had the illusion that the skyscrapers on the other side were within my arm's reach in both distance and height. 2.4.2 `Slowglasses' Suppose that we had a hypothetical glass of very high refractive index. (Science fiction writer Bob Shaw refers to such glass as slowglass <ref> [Shaw, 1966] </ref>.
Reference: [Starner, 1993] <author> Starner, T. </author> <year> (1993). </year> <title> The remembrance agent. Class project for intelligent software agents class of Patti Maes. </title>
Reference-contexts: Video frames shown here are `unrotated' back, hence the tall (1 : 0:75) aspect ratio. (a) (b) (c) (d) (e) except for the region over which they are defined. 12 back to the RM. brance agent <ref> [Starner, 1993] </ref>. This program continually runs in the background and helps him remember text that he has previously typed. It is not hard to imagine a visual remembrance agent.
Reference: [Starner, 1995] <author> Starner, T. </author> <year> (1995). </year> <title> The cyborgs are coming or the real personal computers. </title> <note> http://www-white.media.mit.edu/vismod/publications/tech reports/ abstracts/TR-318-ABSTRACT.html. </note>
Reference-contexts: I use my apparatus as an artist's sketch pad of sorts, useful for taking down visual "notes", and helping me overcome my memory disability. Thad Starner <ref> [Starner, 1995] </ref> uses his wearable computer with a program he calls a rem 1 a small amount is also due to parallax since the camera is mounted out away from the exact point of rotation of my neck 11 (a) (b) (c) (d) (e) adapted, for several days, to the `rot90'
Reference: [Stratton, 1896] <author> Stratton, G. M. </author> <title> (1896). Some preliminary experiements on vision. </title> <journal> Psychological Review. </journal>
Reference-contexts: Although the vast majority of RM users of the future will no doubt have no desire to live in an upside-down, left-right-reversed, or sideways rotated world, these visual worlds serve as illustrative examples of extreme reality mediation. In his 1896 paper <ref> [Stratton, 1896] </ref>, George Stratton reported on experiments in which he wore eyeglasses that inverted his visual field of view. Stratton argued that since the image upon the retina was inverted, it seemed reasonable to examine the effect of presenting the retina with an "upright image".
Reference: [Sutherland, 1968] <author> Sutherland, I. </author> <year> (1968). </year> <title> A head-mounted three dimensional display. </title> <booktitle> In Proc. Fall Joint Computer Conference, </booktitle> <pages> pages 757-764. </pages>
Reference-contexts: 1 Introduction Ivan Sutherland, a pioneer in the field of computer graphics, described a head-mounted display with half-silvered mirrors so that the wearer could see a virtual world superimposed on reality [Earnshaw et al., 1993] <ref> [Sutherland, 1968] </ref>, giving rise to "Augmented Reality (AR)". Others have adopted Sutherland's concept of a Head-Mounted Display (HMD) but generally without the see-through capability. An artificial environment in which the user cannot see through the display is generally referred as a Virtual Reality (VR) environment.
Reference: [Szeliski and Coughlan, 1994] <author> Szeliski, R. and Coughlan, J. </author> <year> (1994). </year> <title> Hierarchical spline-based image registration. </title> <publisher> CVPR. </publisher>
Reference-contexts: Once the images are stabilized, they can be assembled together to make an image mosaic (Fig 11). The image mosaicing principle (e.g. figures like Fig 11) has been previously explored [Hirose et al., 1994] [Mann and Picard, 1994b] <ref> [Szeliski and Coughlan, 1994] </ref>. Of particular interest is the work using image mosaic-ing in a virtual reality environment [Hirose et al., 1994] [HIROSE, 1994]. The system depicted in Fig 3 allows me to explore the video mosaics of Hirose as I walk around outdoors, in my day to day activities.
Reference: [T. G. Stockham, Jr., 1972] <author> T. G. Stockham, Jr. </author> <year> (1972). </year> <title> Image processing in the context of a visual model. </title> <journal> Proc. IEEE, </journal> <volume> 60(7) </volume> <pages> 828-842. </pages>
Reference-contexts: The shadow detail may then be derived from the overexposed stream, the highlight detail from the underexposed stream, and the midtones from a combination of the two streams. The resulting extended-response video may be displayed on a conventional HMD by using Stockham's homomorphic filter <ref> [T. G. Stockham, Jr., 1972] </ref> as the `visual filter'. The principle of extending dynamic range by combining differently exposed pictures is known as the Wyckoff principle [Mann and Picard, 1994a], in honor of Charles Wyckoff.
Reference: [V. M. Bove and Watlington, 1995] <author> V. M. Bove, J. and Watlington, J. A. </author> <year> (1995). </year> <title> Cheops: A reconfigurable data-flow system for video processing. </title> <journal> IEEE Transactions on Circuits and Systems for Video Processing, </journal> <volume> 5 </volume> <pages> 140-149. </pages>
Reference-contexts: The compute-power required to perform general-purpose manipulation of color video streams is too unwieldy to be worn in a backpack (although I've constructed body-worn computers and other hardware to facilitate very limited forms of reality mediation). In particular, a system with good video-processing capability, such as Cheops <ref> [V. M. Bove and Watlington, 1995] </ref> or one or more SGI Reality Engines, may be used remotely by establishing a full-duplex video communications channel between the RM and the host computer (s).
References-found: 40

