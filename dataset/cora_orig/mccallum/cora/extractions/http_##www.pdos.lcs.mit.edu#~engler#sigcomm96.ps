URL: http://www.pdos.lcs.mit.edu/~engler/sigcomm96.ps
Refering-URL: http://www.pdos.lcs.mit.edu/~engler/
Root-URL: 
Email: fkerr, engler, kaashoekg@lcs.mit.edu  
Title: ASHs: Application-Specific Handlers for High-Performance Messaging  
Author: Deborah A. Wallach, Dawson R. Engler, and M. Frans Kaashoek 
Address: Cambridge, MA 02139, U.S.A.  
Affiliation: M.I.T. Laboratory for Computer Science  
Date: August 1996.  
Note: To appear in SIGCOMM '96,  
Abstract: Application-specific safe message handlers (ASHs) are designed to provide applications with hardware-level network performance. ASHs are user-written code fragments that safely and efficiently execute in the kernel in response to message arrival. ASHs can direct message transfers (thereby eliminating copies) and send messages (thereby reducing send-response latency). In addition, the ASH system provides support for dynamic integrated layer processing (thereby eliminating duplicate message traversals) and dynamic protocol composition (thereby supporting modularity). ASHs provide this high degree of flexibility while still providing network performance as good as, or (if they exploit application-specific knowledge) even better than, hard-wired in-kernel implementations. A combination of user-level microbenchmarks and end-to-end system measurements using TCP demonstrate the benefits of the ASH system. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M.B. Abbott and L.L. Peterson. </author> <title> Increasing network throughput by integrating protocol layers. </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> 1(5) </volume> <pages> 600-610, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: ILP can be dynamically provided through the use of pipes, which were first proposed by Abbott and Peterson <ref> [1] </ref> for use in static composition. A pipe is a computation written to act on streaming data, taking several bytes of data as input and producing several bytes of output while performing only a tiny computation (such as a byte swap, or an accumulation for a checksum). <p> To address this bottleneck, applications must be able to direct message placement, and to exploit ILP during copying. We examine each below. 5.1.1 Avoiding message copies Message copies cripple networking performance <ref> [1, 10, 39] </ref>. However, most network systems make little provision for application-directed data transfer. This results in needless data copies as incoming messages are copied from network buffers to intermediate buffers (e.g., BSD's mbufs [27]) and then copied to their eventual destination. <p> There is also quite a bit of work on protocol composition [5, 23, 24, 41, 42]. The first system to provide an automatic modular mechanism for ILP is Abbott and Peterson <ref> [1] </ref>. They describe an ILP system that composes macros into integrated loops at compile time, eliminating multiple data traversals. Each macro is written with initialization and finalization code and a main body that takes in word-sized input and emits word-sized output.
Reference: [2] <author> T.E. Anderson, M.D. Dahlin, J.M. Neefe, D.S. Roselli D.A. Patterson, and R.Y. Wang. </author> <title> Serverless network file systems. </title> <booktitle> In Proceedings of the 15th Symposium on Operating Systems Principles, </booktitle> <pages> pages 109-126, </pages> <address> Copper Mountain Resort, CO, USA, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: 1 Introduction Applications' complexity and ambition scale with increases in processing power and network performance. For example, the last few years have seen a proliferation of distributed shared memory systems [25, 26, 28], real-time video and voice applications [45], parallel applications [11, 34], and tightly-coupled distributed systems <ref> [2, 36, 39] </ref>. Unfortunately, although raw CPU and networking hardware speeds have increased, this increase is not reaching applications: networking software and memory subsystem performance already limit applications and will only do so more in the future [10, 13, 39].
Reference: [3] <author> M.L. Bailey, B. Gopal, M.A. Pagels, L.L. Peterson, and P. Sarkar. PATHFINDER: </author> <title> A pattern-based packet classifier. </title> <booktitle> In Proceedings of the First Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 115-123, </pages> <address> Monterey, CA, USA, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: DPF is an order of magnitude faster than the highest performance packet filter engines (MPF [47] and PATHFINDER <ref> [3] </ref>) in the literature. Similarly to other systems [15, 34, 36], the AN2 device is securely exported by using the ATM connection identifier to demulti-plex packets. Processes bind to a virtual circuit identifier, providing a section of their memory for messages to be DMA'ed to.
Reference: [4] <author> B.N. Bershad, S. Savage, P. Pardyak, E.G. Sirer, M. Fiuczyn-ski, D. Becker, S. Eggers, and C. Chambers. </author> <title> Extensibility, safety and performance in the SPIN operating system. </title> <booktitle> In Proceedings of the Fifteenth ACM Symposium on Operating Systems Principles, </booktitle> <address> Copper Mountain Resort, CO, USA, </address> <month> De-cember </month> <year> 1995. </year>
Reference-contexts: In some sense this work can be viewed as a natural extension of the same philosophical foundation that inspired the packet filter: we have provided a framework that allows applications outside of the operating system to install new functionality without kernel modifications. The SPIN project <ref> [4] </ref> is concurrently investigating the use of downloading code into the kernel. SPIN's Plexus network system runs user code fragments in the interrupt handler [21] or as a kernel thread. Plexus guarantees safety by requiring that these code fragments are written in a type-safe language, Modula-3.
Reference: [5] <author> N.T. Bhatti and R.D. Schlichting. </author> <title> A system for constructing configurable high-level protocols. </title> <booktitle> In ACM SIGCOMM '95, </booktitle> <pages> pages 138-150, </pages> <address> Cambridge, MA, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: Furthermore, many systems compose protocols at runtime <ref> [5, 23, 24, 41, 42] </ref>, making static ILP infeasible. <p> The tradeoffs discussed there are applicable here. ILP and protocol composition There have been many instances of ad hoc ILP, for example, in many networking kernels [9]. There is also quite a bit of work on protocol composition <ref> [5, 23, 24, 41, 42] </ref>. The first system to provide an automatic modular mechanism for ILP is Abbott and Peterson [1]. They describe an ILP system that composes macros into integrated loops at compile time, eliminating multiple data traversals.
Reference: [6] <editor> R. Braden, D. Borman, and C. Partridge. </editor> <title> Computing the Internet checksum. </title> <type> RFC 1071. </type>
Reference-contexts: The VCODE interface is that of an extended RISC machine: instructions are low-level register-to-register operations. A sample pipe to compute the Internet checksum <ref> [6] </ref> is provided in Figure 2. Each pipe is allocated in the context of a pipe list (pl in the figure) and given a pipe identifier that is used to name it.
Reference: [7] <author> T. Braun and C. Diot. </author> <title> Protocol implemetation using integrated layer processing. </title> <booktitle> In ACM SIGCOMM '95, </booktitle> <pages> pages 151-161, </pages> <address> Cambridge, MA, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: The end-to-end measurements are taken on the system described in the previous section. Because TCP is important, well-documented, and widely-used, we try to illustrate the benefits of ASHs using TCP. Also, as pointed out by Braun et al. <ref> [7] </ref>, it is important to evaluate ILP in a complete protocol environment. For most of our experiments we used dynamic ILP but not protocol composition in order to separate out the cost of dynamically composing ASHs.
Reference: [8] <author> D.D. Clark. </author> <title> The structuring of systems using upcalls. </title> <booktitle> In Proceedings of the 10th Symposium on Operating Systems Principles, </booktitle> <pages> pages 171-180, </pages> <address> Orcas Island, WA, USA, </address> <month> December </month> <year> 1985. </year>
Reference-contexts: This paper addresses the important problem of delivering hardware-level network performance to applications by introducing application-specific safe message handlers (ASHs), which are user-written upcalls <ref> [8] </ref> that are safely and efficiently executed in the kernel in response to a message arrival. <p> This data shows that even with an untuned implementation of ASHs, protocol composition can be performed at runtime without a performance penalty. 6 Related work Upcalls ASHs can been viewed as a restricted form of Clark's upcalls <ref> [8] </ref>. ASHs are intended primarily for simple, small-latency operations; the time they run in can be bounded, because the operating system can reason about their behavior (as well as check for safety). ASHs must be limited in expressiveness to allow the operating system to do this reasoning.
Reference: [9] <author> D.D. Clark, V. Jacobson, J. Romkey, and H. Salwen. </author> <title> An analysis of TCP processing overhead. </title> <journal> IEEE Communications Magazine, </journal> <volume> 27(6) </volume> <pages> 23-29, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: Issues about schedulability and when and how a message handler should abort have been recently explored in Optimistic Active Messages [46]. The tradeoffs discussed there are applicable here. ILP and protocol composition There have been many instances of ad hoc ILP, for example, in many networking kernels <ref> [9] </ref>. There is also quite a bit of work on protocol composition [5, 23, 24, 41, 42]. The first system to provide an automatic modular mechanism for ILP is Abbott and Peterson [1].
Reference: [10] <author> D.D. Clark and D.L. Tennenhouse. </author> <title> Architectural considerations for a new generation of protocols. </title> <booktitle> In ACM Communication Architectures, Protocols, and Applications (SIGCOMM '90), </booktitle> <address> Philadelphia, PA, USA, </address> <month> September </month> <year> 1990. </year>
Reference-contexts: Unfortunately, although raw CPU and networking hardware speeds have increased, this increase is not reaching applications: networking software and memory subsystem performance already limit applications and will only do so more in the future <ref> [10, 13, 39] </ref>. This paper addresses the important problem of delivering hardware-level network performance to applications by introducing application-specific safe message handlers (ASHs), which are user-written upcalls [8] that are safely and efficiently executed in the kernel in response to a message arrival. <p> Therefore, the negotiation of protocol layers can require multiple costly memory traversals, 1 stressing a weak link in high-performance networking: the memory subsystems of the endpoint nodes. As argued by Clark and Ten-nenhouse <ref> [10] </ref>, an integrated approach, where these operations are combined into a single memory traversal, can greatly improve the latency and throughput of a system. ASHs integrate data manipulations such as checksumming or conversions into the data transfer engine itself. ASHs automatically and dynamically perform integrated layering processing (ILP). <p> In addition to simple data copying, many systems perform multiple traversals of message data as every layer of the networking software performs its operations (e.g., checksumming, encryption, conversion). At an operational level, these multiple data manipulations are as bad as multiple copies. To remove this overhead, Clark and Tennenhouse <ref> [10] </ref> propose integrated layer processing (ILP), where the manipulations of each layer are compressed into a single operation. To the best of our knowledge, all systems based on ILP are static, in that all integration must be hard coded into the networking system. <p> Many of our experiments are influenced by Clark and Tennenhouse <ref> [10] </ref>. We use a combination of user-level microbenchmarks and end-to-end system measurements. The microbenchmarks gauge the individual effects of, for example, avoiding copies, while the system measurements give insight into the end-to-end performance effects. <p> Unfortunately, while network throughput and CPU performance have improved significantly in the last decade, workstation memory subsystems have not. As a result, the crucial bottleneck in bulk data transfer occurs during the movement of data from the network buffer to its final destination in application space <ref> [10, 13] </ref>. To address this bottleneck, applications must be able to direct message placement, and to exploit ILP during copying. We examine each below. 5.1.1 Avoiding message copies Message copies cripple networking performance [1, 10, 39]. However, most network systems make little provision for application-directed data transfer. <p> To address this bottleneck, applications must be able to direct message placement, and to exploit ILP during copying. We examine each below. 5.1.1 Avoiding message copies Message copies cripple networking performance <ref> [1, 10, 39] </ref>. However, most network systems make little provision for application-directed data transfer. This results in needless data copies as incoming messages are copied from network buffers to intermediate buffers (e.g., BSD's mbufs [27]) and then copied to their eventual destination.
Reference: [11] <author> D.E. Culler, A. Dusseau, S.C. Goldstein, A. Krishnamurthy, S. Lumetta, T. von Eicken, and K. Yelick. </author> <title> Parallel programming in Split-C. </title> <booktitle> In Supercomputing, </booktitle> <pages> pages 262-273, </pages> <address> Portland, OR, USA, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Applications' complexity and ambition scale with increases in processing power and network performance. For example, the last few years have seen a proliferation of distributed shared memory systems [25, 26, 28], real-time video and voice applications [45], parallel applications <ref> [11, 34] </ref>, and tightly-coupled distributed systems [2, 36, 39]. Unfortunately, although raw CPU and networking hardware speeds have increased, this increase is not reaching applications: networking software and memory subsystem performance already limit applications and will only do so more in the future [10, 13, 39].
Reference: [12] <author> P. Deutsch and C.A. Grant. </author> <title> A flexible measurement tool for software systems. </title> <booktitle> Information Processing 71, </booktitle> <year> 1971. </year>
Reference-contexts: We describe these software techniques here in detail. The ASH system for the MIPS bounds execution time using a framework inspired by Deutsch <ref> [12] </ref>. Exceptions are prevented using runtime and static checks (as is done in existing packet-filters [31, 47]). Wild memory references are prevented using a combination of address-space fire-walls and sandboxing [44]. Wild jumps are prevented using language support. We examine each technique in further detail below. <p> We believe that both of the models can be useful in sys-tems. Those that can tolerate more latency can use the flexibility of the upcall; those that cannot will be confined to ASHs. Code importation There are a number of clear antecedents to our work: Deutsch's seminal paper <ref> [12] </ref> and Wahbe et al.'s modern revisitation of safe code importation [44] influenced our ideas strongly, as did Mogul's original packet filter paper [31].
Reference: [13] <author> P. Druschel, M.B. Abbott, M.A. Pagels, and L.L. Peterson. </author> <title> Network subsystem design. </title> <journal> IEEE Network, </journal> <volume> 7(4) </volume> <pages> 8-17, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: Unfortunately, although raw CPU and networking hardware speeds have increased, this increase is not reaching applications: networking software and memory subsystem performance already limit applications and will only do so more in the future <ref> [10, 13, 39] </ref>. This paper addresses the important problem of delivering hardware-level network performance to applications by introducing application-specific safe message handlers (ASHs), which are user-written upcalls [8] that are safely and efficiently executed in the kernel in response to a message arrival. <p> Unfortunately, while network throughput and CPU performance have improved significantly in the last decade, workstation memory subsystems have not. As a result, the crucial bottleneck in bulk data transfer occurs during the movement of data from the network buffer to its final destination in application space <ref> [10, 13] </ref>. To address this bottleneck, applications must be able to direct message placement, and to exploit ILP during copying. We examine each below. 5.1.1 Avoiding message copies Message copies cripple networking performance [1, 10, 39]. However, most network systems make little provision for application-directed data transfer.
Reference: [14] <author> P. Druschel and L.L. Peterson. Fbufs: </author> <title> A high-bandwidth cross-domain transfer facility. </title> <booktitle> In Proceedings of the 14th Symposium on Operating Systems Principles, </booktitle> <pages> pages 175-189, </pages> <address> Ashville, NC, USA, </address> <month> December </month> <year> 1993. </year>
Reference-contexts: Small and Seltzer compare a number of approaches to safely executing untrusted code [35]. Message vectoring Message vectoring has been a popular focus of the networking community <ref> [14, 15, 16, 36] </ref>. The main difference between our work and previous work is that ASHs can perform application-specific computation at message arrival. By using application-state and domain knowledge these handlers can perform operations difficult in the context of static protocol specifications.
Reference: [15] <author> P. Druschel, L.L. Peterson, and B.S. Davie. </author> <title> Experiences with a high-speed network adaptor: A software perspective. </title> <booktitle> In ACM Communication Architectures, Protocols, and Applications (SIGCOMM '94), </booktitle> <pages> pages 2-13, </pages> <address> London, UK, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: The next section reports on the benefits of using ASHs. Like other systems <ref> [15, 16, 29, 36, 40] </ref>, all the protocols are implemented in user space. The main point to take from the results in this section is that our implementation without ASHs performs well and is competitive with the best systems reported in the literature. <p> DPF is an order of magnitude faster than the highest performance packet filter engines (MPF [47] and PATHFINDER [3]) in the literature. Similarly to other systems <ref> [15, 34, 36] </ref>, the AN2 device is securely exported by using the ATM connection identifier to demulti-plex packets. Processes bind to a virtual circuit identifier, providing a section of their memory for messages to be DMA'ed to. <p> Direct comparisons with other high-performance systems such as Osiris <ref> [15] </ref> and Afterburner [16] are difficult since they run on different networks and have special purpose network cards, but our implementation appears to be competitive. 4.3 Internet protocols On top of the raw interface we have implemented several network protocols (ARP/RARP, IP, UDP, TCP, HTTP, and NFS) as user-level libraries, which <p> In summary, the base performance of our system for UDP and TCP is in the same ballpark or is better than most high-performance user-level and in-kernel implementations <ref> [15, 16, 21, 29, 40] </ref>. 5 Using application-specific handlers In this section, we examine how application-specific safe handlers can be exploited to achieve good throughput, data transfer latency and control transfer latency. Many of our experiments are influenced by Clark and Tennenhouse [10]. <p> Small and Seltzer compare a number of approaches to safely executing untrusted code [35]. Message vectoring Message vectoring has been a popular focus of the networking community <ref> [14, 15, 16, 36] </ref>. The main difference between our work and previous work is that ASHs can perform application-specific computation at message arrival. By using application-state and domain knowledge these handlers can perform operations difficult in the context of static protocol specifications.
Reference: [16] <author> A. Edwards, G. Watson, J. Lumley, D. Banks, C. Clamvokis, and C. Dalton. </author> <title> User-space protocols deliver high performance to applications on a low-cost Gb/s LAN. </title> <booktitle> In ACM Communication Architectures, Protocols, and Applications (SIGCOMM '94), </booktitle> <pages> pages 14-24, </pages> <address> London, UK, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: The next section reports on the benefits of using ASHs. Like other systems <ref> [15, 16, 29, 36, 40] </ref>, all the protocols are implemented in user space. The main point to take from the results in this section is that our implementation without ASHs performs well and is competitive with the best systems reported in the literature. <p> Direct comparisons with other high-performance systems such as Osiris [15] and Afterburner <ref> [16] </ref> are difficult since they run on different networks and have special purpose network cards, but our implementation appears to be competitive. 4.3 Internet protocols On top of the raw interface we have implemented several network protocols (ARP/RARP, IP, UDP, TCP, HTTP, and NFS) as user-level libraries, which are then linked <p> The general structure is similar to other implementations of user-level protocols <ref> [16, 36] </ref>. The UDP implementation is a straightforward implementation of the UDP protocol as specified in RFC768. Similarly, the TCP implementation is a library-based implementation of RFC793. <p> In summary, the base performance of our system for UDP and TCP is in the same ballpark or is better than most high-performance user-level and in-kernel implementations <ref> [15, 16, 21, 29, 40] </ref>. 5 Using application-specific handlers In this section, we examine how application-specific safe handlers can be exploited to achieve good throughput, data transfer latency and control transfer latency. Many of our experiments are influenced by Clark and Tennenhouse [10]. <p> Small and Seltzer compare a number of approaches to safely executing untrusted code [35]. Message vectoring Message vectoring has been a popular focus of the networking community <ref> [14, 15, 16, 36] </ref>. The main difference between our work and previous work is that ASHs can perform application-specific computation at message arrival. By using application-state and domain knowledge these handlers can perform operations difficult in the context of static protocol specifications. <p> By using application-state and domain knowledge these handlers can perform operations difficult in the context of static protocol specifications. The most similar work to the ASH system is Edwards et al. <ref> [16] </ref>, who import simple scripts using the Unix ioctl system call to copy messages to their destination. The main differences are the expressiveness of the two implementations. Their system supplies only rudimentary operations (e.g., copy and allocate), limiting the flexibility with which applications can manipulate data transfer.
Reference: [17] <author> D.R. Engler. </author> <title> VCODE: a retargetable, extensible, very fast dynamic code generation system. </title> <booktitle> In Proceedings of the SIG-PLAN '96 Conference on Programming Language Design and Implementation, </booktitle> <address> Philadelphia, PA, USA, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: Second, it is modular: the ASH system converts between gauge sizes and prevents name conflicts by binding the context inside the pipe itself, which can be handled as an inviolable object. The pipes for ASHs are written in VCODE <ref> [17] </ref>, which is a low-level extension language designed to be simple to implement and efficient both in terms of the cost of code generation and in terms of the computational performance of its generated code. The VCODE interface is that of an extended RISC machine: instructions are low-level register-to-register operations.
Reference: [18] <author> D.R. Engler and M.F. Kaashoek. DPF: </author> <title> fast, flexible message demultiplexing using dynamic code generation. </title> <booktitle> In ACM Communication Architectures, Protocols, and Applications (SIG-COMM '96), </booktitle> <address> Stanford, CA, USA, </address> <month> August </month> <year> 1996. </year>
Reference-contexts: The Ethernet device is securely exported by a packet filter engine [31]. The Aegis implementation of the packet filter engine, DPF <ref> [18] </ref>, uses dynamic code generation. DPF exploits dynamic code generation in two ways: by using it to eliminate interpretation overhead by compiling packet filters to executable code when they are installed into the kernel, and by using filter constants to aggressively optimize this executable code.
Reference: [19] <author> D.R. Engler, M.F. Kaashoek, and J. O'Toole Jr. Exokernel: </author> <title> an operating system architecture for application-specific resource management. </title> <booktitle> In Proceedings of the Fifteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 251-266, </pages> <address> Copper Mountain Resort, CO, USA, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: The ASH system provides a simple interface to dynamically compose protocols. Using on-the-fly code generation, these composed protocols are integrated into an efficient data path. The ASH system has been implemented in an exokernel operating system <ref> [19] </ref>. Aegis, an exokernel for MIPS-based DECsta-tions, securely exports two network devices: a 10 Mbit/s Ethernet and a 155 Mbit/s AN2 (Digital's ATM network). <p> In general, we would expect even better performance improvements for using ASHs in other kernels than exokernels, since Aegis has been highly optimized for fast kernel crossings <ref> [19] </ref>, reducing the benefit of downloading code into the kernel. The remainder of the paper is structured as follows. Section 2 discusses design issues and Section 3 implementation issues for ASHs. Section 4 describes the experimental environment for evaluating the benefits of ASHs. <p> We will discuss in turn the testbed, the raw performance of the network system, and the performance of our user-level implementations of UDP and TCP. 4.1 Testbed We have implemented a system for ASHs in an exokernel operating system <ref> [19] </ref>. Although our implementation is for an exokernel, ASHs are largely independent of the specific operating system and operating system architecture. They should apply equally well to monolithic and microkernel systems. <p> experiments greatly underestimate the benefits of running ASHs in any other kernel, because kernel crossings in Aegis have been highly optimized: Aegis kernel's crossings are five times better than the best reported numbers in the literature and are an order of magnitude better than a run-of-the-mill UNIX system like Ultrix <ref> [19] </ref>.
Reference: [20] <author> D.R. Engler, D.A. Wallach, and M.F. Kaashoek. </author> <title> Design and implementation of a modular, flexible, and fast system for dynamic protocol composition. </title> <type> Technical Memorandum TM-552, </type> <institution> Massachusetts Institute of Technology Laboratory for Computer Science, </institution> <month> May </month> <year> 1996. </year>
Reference-contexts: We exploit dynamic code generation techniques to efficiently implement dynamic ILP and dynamic protocol composition. The details of the implementation are more fully described in <ref> [20] </ref>. 3.5 Implementation caveats The current system as measured in the rest of the paper has three limitations.
Reference: [21] <author> M.E. Fiuczynski and B.N. Bershad. </author> <title> An extensible protocol architecture for application-specific networking. </title> <booktitle> In Proceedings of USENIX, </booktitle> <pages> pages 55-64, </pages> <address> San Diego, CA, USA, </address> <month> January </month> <year> 1996. </year>
Reference-contexts: In summary, the base performance of our system for UDP and TCP is in the same ballpark or is better than most high-performance user-level and in-kernel implementations <ref> [15, 16, 21, 29, 40] </ref>. 5 Using application-specific handlers In this section, we examine how application-specific safe handlers can be exploited to achieve good throughput, data transfer latency and control transfer latency. Many of our experiments are influenced by Clark and Tennenhouse [10]. <p> The SPIN project [4] is concurrently investigating the use of downloading code into the kernel. SPIN's Plexus network system runs user code fragments in the interrupt handler <ref> [21] </ref> or as a kernel thread. Plexus guarantees safety by requiring that these code fragments are written in a type-safe language, Modula-3. Plexus simplifies protocol composition, but unlike ASHs, does not provide direct support for dynamic ILP.
Reference: [22] <author> J. Gosling. </author> <title> Java intermediate bytecodes. </title> <booktitle> In ACM SIGPLAN Workshop on Intermediate Representations (IR'95), </booktitle> <pages> pages 111-118, </pages> <address> San Francisco, CA, USA, </address> <month> March </month> <year> 1995. </year>
Reference-contexts: Preliminary Plexus numbers for in-kernel UDP on Ethernet and ATM look promising but are slower than our user-level implementation of UDP. No numbers are reported yet for TCP. With the advent of HotJava and Java <ref> [22] </ref>, code importation in the form of mobile code has received a lot of press. Recently Tennenhouse and Wetherall have proposed to use mobile code to build Active Networks [37]; in an active network, protocols are replaced by programs, which are safely executed in the operating system on message arrival.
Reference: [23] <author> R. Harper and P. Lee. </author> <title> Advanced languages for systems software: The Fox project in 1994. </title> <type> Technical Report CMU-SC-94-104, </type> <institution> Carnegie Mellon University, </institution> <address> Pittsburgh, PA 15213, </address> <month> January </month> <year> 1994. </year>
Reference-contexts: Furthermore, many systems compose protocols at runtime <ref> [5, 23, 24, 41, 42] </ref>, making static ILP infeasible. <p> The tradeoffs discussed there are applicable here. ILP and protocol composition There have been many instances of ad hoc ILP, for example, in many networking kernels [9]. There is also quite a bit of work on protocol composition <ref> [5, 23, 24, 41, 42] </ref>. The first system to provide an automatic modular mechanism for ILP is Abbott and Peterson [1]. They describe an ILP system that composes macros into integrated loops at compile time, eliminating multiple data traversals.
Reference: [24] <author> N.C. Hutchinson and L.L. Peterson. </author> <title> The x-kernel: an architecture for implementing network protocols. </title> <journal> IEEE Trans. on Soft. Eng., </journal> <volume> 17(1), </volume> <month> January </month> <year> 1991. </year>
Reference-contexts: Furthermore, many systems compose protocols at runtime <ref> [5, 23, 24, 41, 42] </ref>, making static ILP infeasible. <p> The tradeoffs discussed there are applicable here. ILP and protocol composition There have been many instances of ad hoc ILP, for example, in many networking kernels [9]. There is also quite a bit of work on protocol composition <ref> [5, 23, 24, 41, 42] </ref>. The first system to provide an automatic modular mechanism for ILP is Abbott and Peterson [1]. They describe an ILP system that composes macros into integrated loops at compile time, eliminating multiple data traversals.
Reference: [25] <author> K.L. Johnson, M.F. Kaashoek, and D.A. Wallach. </author> <title> CRL: High-performance all-software distributed shared memory. </title> <booktitle> In Proceedings of the 15th Symposium on Operating Systems Principles, </booktitle> <pages> pages 213-228, </pages> <address> Copper Mountain Resort, CO, USA, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: 1 Introduction Applications' complexity and ambition scale with increases in processing power and network performance. For example, the last few years have seen a proliferation of distributed shared memory systems <ref> [25, 26, 28] </ref>, real-time video and voice applications [45], parallel applications [11, 34], and tightly-coupled distributed systems [2, 36, 39].
Reference: [26] <author> P. Keleher, S. Dwarkadas, A.L. Cox, and W. Zwaenepoel. Treadmarks: </author> <title> Distributed Shared Memory on Standard Workstations and Operating Systems. </title> <booktitle> In Proc. of the Winter 1994 USENIX Conference, </booktitle> <pages> pages 115-132, </pages> <address> San Francisco, CA, USA, </address> <month> January </month> <year> 1994. </year> <month> 12 </month>
Reference-contexts: 1 Introduction Applications' complexity and ambition scale with increases in processing power and network performance. For example, the last few years have seen a proliferation of distributed shared memory systems <ref> [25, 26, 28] </ref>, real-time video and voice applications [45], parallel applications [11, 34], and tightly-coupled distributed systems [2, 36, 39].
Reference: [27] <author> S.J. Leffler, M.K. McKusick, M.J. Karels, and J.S. Quarter--man. </author> <title> The design and implementation of the 4.3BSD UNIX operating system. </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: We examine each below. 5.1.1 Avoiding message copies Message copies cripple networking performance [1, 10, 39]. However, most network systems make little provision for application-directed data transfer. This results in needless data copies as incoming messages are copied from network buffers to intermediate buffers (e.g., BSD's mbufs <ref> [27] </ref>) and then copied to their eventual destination. To solve this problem, we allow an ASH to control where messages are placed in memory, eliminating all intermediate copies. Our general computational model provides two additional benefits.
Reference: [28] <author> K. Li. IVY: </author> <title> A shared virtual memory system for parallel computing. </title> <booktitle> In International Conference on Parallel Computing, </booktitle> <pages> pages 94-101, </pages> <address> University Park, PA, USA, </address> <month> August </month> <year> 1988. </year>
Reference-contexts: 1 Introduction Applications' complexity and ambition scale with increases in processing power and network performance. For example, the last few years have seen a proliferation of distributed shared memory systems <ref> [25, 26, 28] </ref>, real-time video and voice applications [45], parallel applications [11, 34], and tightly-coupled distributed systems [2, 36, 39].
Reference: [29] <author> C. Maeda and B.N. Bershad. </author> <title> Protocol service decomposition for high-performance networking. </title> <booktitle> In Proceedings of the Fourteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 244-255, </pages> <address> Ashville, NC, USA, </address> <year> 1993. </year>
Reference-contexts: The next section reports on the benefits of using ASHs. Like other systems <ref> [15, 16, 29, 36, 40] </ref>, all the protocols are implemented in user space. The main point to take from the results in this section is that our implementation without ASHs performs well and is competitive with the best systems reported in the literature. <p> In summary, the base performance of our system for UDP and TCP is in the same ballpark or is better than most high-performance user-level and in-kernel implementations <ref> [15, 16, 21, 29, 40] </ref>. 5 Using application-specific handlers In this section, we examine how application-specific safe handlers can be exploited to achieve good throughput, data transfer latency and control transfer latency. Many of our experiments are influenced by Clark and Tennenhouse [10].
Reference: [30] <author> R.P. Martin. HPAM: </author> <title> An Active Message layer for a network of HP workstations. </title> <booktitle> In Proceedings of Hot Interconnects II, </booktitle> <month> August </month> <year> 1994. </year>
Reference-contexts: Active messages on parallel machines do not worry about issues of software protection. Several user-level AM implementations for networks of workstations have recently become available <ref> [30, 36] </ref>. U-Net designed for ATM networks, does provide protection, but only at a cost of higher latency: messages are not executed until the corresponding process happens to be scheduled by the kernel [36]. HPAM is designed for HP workstations connected via an FDDI layer.
Reference: [31] <author> J.C. Mogul, R.F. Rashid, and M.J. Accetta. </author> <title> The packet filter: An efficient mechanism for user-level network code. </title> <booktitle> In Proceedings of the Eleventh ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 39-51, </pages> <address> Austin, TX, USA, </address> <month> Novem-ber </month> <year> 1987. </year>
Reference-contexts: We describe these software techniques here in detail. The ASH system for the MIPS bounds execution time using a framework inspired by Deutsch [12]. Exceptions are prevented using runtime and static checks (as is done in existing packet-filters <ref> [31, 47] </ref>). Wild memory references are prevented using a combination of address-space fire-walls and sandboxing [44]. Wild jumps are prevented using language support. We examine each technique in further detail below. <p> The Ethernet device is securely exported by a packet filter engine <ref> [31] </ref>. The Aegis implementation of the packet filter engine, DPF [18], uses dynamic code generation. <p> Code importation There are a number of clear antecedents to our work: Deutsch's seminal paper [12] and Wahbe et al.'s modern revisitation of safe code importation [44] influenced our ideas strongly, as did Mogul's original packet filter paper <ref> [31] </ref>. In some sense this work can be viewed as a natural extension of the same philosophical foundation that inspired the packet filter: we have provided a framework that allows applications outside of the operating system to install new functionality without kernel modifications.
Reference: [32] <author> D. Mosberger, L.L. Peterson, P.G. Bridges, and S. O'Malley. </author> <title> Analysis of techniques to improve protocol processing latency. </title> <type> Technical Report TR96-93, </type> <institution> University of Arizona, </institution> <year> 1996. </year>
Reference-contexts: Memory and I/O devices are accessed over a 25-Mhz TURBOchannel bus. The two DECstation 240s are connected with an AN2 switch. While collecting the numbers reported in this paper, we had a fair number of problems with cache conflicts (similar to problems reported by others <ref> [32] </ref>), because the DECstations have direct-mapped caches.
Reference: [33] <author> T.A. Proebsting and S.A. Watterson. </author> <title> Filter fusion. </title> <booktitle> In Proceedings of the 23th Annual Symposium on Principles of Programming Languages, </booktitle> <pages> pages 119-130, </pages> <address> St. Petersburg Beach, FL, USA, </address> <month> January </month> <year> 1996. </year>
Reference-contexts: Given the richness of possible data manipulations, however, disallowing application-specific operations can carry a significant cost. For example, even a single re-traversal of the data can halve available bandwidth. Proebsting and Watterson describe a new algorithm for static ILP using filter fusion <ref> [33] </ref>. Static composition requires that all desired compositions be known and performed at compilation time. There are two main drawbacks to such an approach. The first is the exponential code growth inherent in it.
Reference: [34] <author> D.J. Scales, M. Burrows, and C.A. Thekkath. </author> <title> Experience with parallel computing on the AN2 network. </title> <booktitle> In International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1996. </year>
Reference-contexts: 1 Introduction Applications' complexity and ambition scale with increases in processing power and network performance. For example, the last few years have seen a proliferation of distributed shared memory systems [25, 26, 28], real-time video and voice applications [45], parallel applications <ref> [11, 34] </ref>, and tightly-coupled distributed systems [2, 36, 39]. Unfortunately, although raw CPU and networking hardware speeds have increased, this increase is not reaching applications: networking software and memory subsystem performance already limit applications and will only do so more in the future [10, 13, 39]. <p> DPF is an order of magnitude faster than the highest performance packet filter engines (MPF [47] and PATHFINDER [3]) in the literature. Similarly to other systems <ref> [15, 34, 36] </ref>, the AN2 device is securely exported by using the ATM connection identifier to demulti-plex packets. Processes bind to a virtual circuit identifier, providing a section of their memory for messages to be DMA'ed to. <p> For the AN2 interface, the table also compares the user-level version to the best in-kernel version we were able to write. Since the 6 hardware overhead for a round trip is approximately 96 microseconds <ref> [34] </ref>, the kernel software is adding only 16 microseconds of overhead. <p> The maximum achievable per-link bandwidth is about 16.8 MBytes/second (134 Mbits/second) <ref> [34] </ref>. At a 4-Kbyte packet size, we reach 16.11 MBytes/second. These raw numbers are competitive with other high-performance implementations that also export the network to user space. Scales et al. [34] measure about twice as much software overhead (7,600 cycles or 34 microseconds) for a null packet send using their pvm <p> The maximum achievable per-link bandwidth is about 16.8 MBytes/second (134 Mbits/second) <ref> [34] </ref>. At a 4-Kbyte packet size, we reach 16.11 MBytes/second. These raw numbers are competitive with other high-performance implementations that also export the network to user space. Scales et al. [34] measure about twice as much software overhead (7,600 cycles or 34 microseconds) for a null packet send using their pvm send and pvm receive interface using the same ATM board, with a substantially faster machine (a 225-MHz DEC 3000 Model 700 AlphaStation rated at 157 SPECint92).
Reference: [35] <author> C. Small and M. Seltzer. </author> <title> A comparison of OS extension technologies. </title> <booktitle> In Proceedings of USENIX, </booktitle> <pages> pages 41-54, </pages> <address> San Diego, CA, USA, </address> <month> January </month> <year> 1996. </year>
Reference-contexts: Recently Tennenhouse and Wetherall have proposed to use mobile code to build Active Networks [37]; in an active network, protocols are replaced by programs, which are safely executed in the operating system on message arrival. Small and Seltzer compare a number of approaches to safely executing untrusted code <ref> [35] </ref>. Message vectoring Message vectoring has been a popular focus of the networking community [14, 15, 16, 36]. The main difference between our work and previous work is that ASHs can perform application-specific computation at message arrival.
Reference: [36] <author> V. Buch T. von Eicken, A. Basu and W. Vogels. U-Net: </author> <title> A user-level network interface for parallel and distributed computing. </title> <booktitle> In Proceedings of the Fifteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 40-53, </pages> <address> Copper Mountain Resort, CO, USA, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: 1 Introduction Applications' complexity and ambition scale with increases in processing power and network performance. For example, the last few years have seen a proliferation of distributed shared memory systems [25, 26, 28], real-time video and voice applications [45], parallel applications [11, 34], and tightly-coupled distributed systems <ref> [2, 36, 39] </ref>. Unfortunately, although raw CPU and networking hardware speeds have increased, this increase is not reaching applications: networking software and memory subsystem performance already limit applications and will only do so more in the future [10, 13, 39]. <p> The next section reports on the benefits of using ASHs. Like other systems <ref> [15, 16, 29, 36, 40] </ref>, all the protocols are implemented in user space. The main point to take from the results in this section is that our implementation without ASHs performs well and is competitive with the best systems reported in the literature. <p> DPF is an order of magnitude faster than the highest performance packet filter engines (MPF [47] and PATHFINDER [3]) in the literature. Similarly to other systems <ref> [15, 34, 36] </ref>, the AN2 device is securely exported by using the ATM connection identifier to demulti-plex packets. Processes bind to a virtual circuit identifier, providing a section of their memory for messages to be DMA'ed to. <p> U-Net (176 vs 66 microseconds), since our experiments are taken on slower machines (40-Mhz vs. 66-Mhz), the AN2 hardware latency is higher than the Fore latency (96 microseconds vs. 42 microseconds), and we have not attempted to rewrite the AN2 firmware to achieve low latency, as was done for U-Net <ref> [36] </ref>. <p> The general structure is similar to other implementations of user-level protocols <ref> [16, 36] </ref>. The UDP implementation is a straightforward implementation of the UDP protocol as specified in RFC768. Similarly, the TCP implementation is a library-based implementation of RFC793. <p> Using the AN2 interface, UDP latencies are about 31 microseconds higher than the raw user-level latencies. This difference is because the UDP library allocates send buffers, and initializes IP and UDP fields. Our implementation seems to have lower overhead than U-Net <ref> [36] </ref>; the U-Net implementation adds 73 microseconds on a 66-Mhz processor while our implementation adds 52 microseconds on a 40-Mhz processor (even though, unlike their numbers, our checksum and memory copy are not integrated for this measurement). <p> Past systems precluded protected, low-latency control transfer, or heavily relied on user-level polling to achieve performance (e.g., in U-Net using signals to indicate the arrival of a message instead of polling adds 30 microseconds to the 65-microsecond roundtrip latency <ref> [36] </ref>). The cost of control transfers is sufficiently high that recently a dichotomy has been drawn between control and data transfer in the interests of constructing systems to efficiently perform just data transfer [39]. <p> Small and Seltzer compare a number of approaches to safely executing untrusted code [35]. Message vectoring Message vectoring has been a popular focus of the networking community <ref> [14, 15, 16, 36] </ref>. The main difference between our work and previous work is that ASHs can perform application-specific computation at message arrival. By using application-state and domain knowledge these handlers can perform operations difficult in the context of static protocol specifications. <p> Active messages on parallel machines do not worry about issues of software protection. Several user-level AM implementations for networks of workstations have recently become available <ref> [30, 36] </ref>. U-Net designed for ATM networks, does provide protection, but only at a cost of higher latency: messages are not executed until the corresponding process happens to be scheduled by the kernel [36]. HPAM is designed for HP workstations connected via an FDDI layer. <p> Several user-level AM implementations for networks of workstations have recently become available [30, 36]. U-Net designed for ATM networks, does provide protection, but only at a cost of higher latency: messages are not executed until the corresponding process happens to be scheduled by the kernel <ref> [36] </ref>. HPAM is designed for HP workstations connected via an FDDI layer. It makes the optimistic assumption that incoming messages are intended for the currently running process; messages intended for other processes are copied multiple times.
Reference: [37] <author> D.L. Tennenhouse and D.J. Wetherall. </author> <title> Towards an active network architecture. </title> <booktitle> In Proc. Multimedia, Computing, and Networking 96, </booktitle> <month> January </month> <year> 1996. </year>
Reference-contexts: No numbers are reported yet for TCP. With the advent of HotJava and Java [22], code importation in the form of mobile code has received a lot of press. Recently Tennenhouse and Wetherall have proposed to use mobile code to build Active Networks <ref> [37] </ref>; in an active network, protocols are replaced by programs, which are safely executed in the operating system on message arrival. Small and Seltzer compare a number of approaches to safely executing untrusted code [35].
Reference: [38] <author> C.A. Thekkath and H.M. Levy. </author> <title> Limits to low-latency communication on high-speed networks. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(2) </volume> <pages> 179-203, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Table 1 shows the roundtrip latency achieved using the Ethernet and AN2 interfaces to send and receive from user space a 4-byte message between two DECstation 5000/240s. For this configuration, the Ethernet number (279 microseconds) is close to the limits measured for user-level low-latency communication in <ref> [38] </ref>; an exact comparison is complicated since the limits were measured on DECstation 5000/200s. For the AN2 interface, the table also compares the user-level version to the best in-kernel version we were able to write. <p> Using larger train sizes increases the throughput. On the Ethernet, both UDP latency and throughput are modulo process speed differences about the same as the fastest implementation reported in the literature <ref> [38] </ref>. Using the AN2 interface, UDP latencies are about 31 microseconds higher than the raw user-level latencies. This difference is because the UDP library allocates send buffers, and initializes IP and UDP fields.
Reference: [39] <author> C.A. Thekkath, H.M. Levy, and E.D. Lazowska. </author> <title> Separating data and control transfer in distributed operating systems. </title> <booktitle> In Sixth International Conference on Architecture Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 2-11, </pages> <address> San Jose, CA, USA, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: 1 Introduction Applications' complexity and ambition scale with increases in processing power and network performance. For example, the last few years have seen a proliferation of distributed shared memory systems [25, 26, 28], real-time video and voice applications [45], parallel applications [11, 34], and tightly-coupled distributed systems <ref> [2, 36, 39] </ref>. Unfortunately, although raw CPU and networking hardware speeds have increased, this increase is not reaching applications: networking software and memory subsystem performance already limit applications and will only do so more in the future [10, 13, 39]. <p> Unfortunately, although raw CPU and networking hardware speeds have increased, this increase is not reaching applications: networking software and memory subsystem performance already limit applications and will only do so more in the future <ref> [10, 13, 39] </ref>. This paper addresses the important problem of delivering hardware-level network performance to applications by introducing application-specific safe message handlers (ASHs), which are user-written upcalls [8] that are safely and efficiently executed in the kernel in response to a message arrival. <p> This ability allows them to perform control operations at message reception, implementing such computational actions as traditional active messages [43] or remote lock acquisition in a distributed shared memory system. Even recently, low-overhead control transfer had been considered to be infeasible to implement <ref> [39] </ref>. We have also integrated support for dynamic integrated layer processing and dynamic protocol composition into the ASH system. <p> The send part of a layer (if any) is similarly structured to the receive path, only without an abort handler. implements naive remote writes <ref> [39] </ref>. The ASH extracts the message destination address and length from the message (using the consume library call). It then copies the payload to the destination address, also using the consume call. The message is not passed up further. <p> To address this bottleneck, applications must be able to direct message placement, and to exploit ILP during copying. We examine each below. 5.1.1 Avoiding message copies Message copies cripple networking performance <ref> [1, 10, 39] </ref>. However, most network systems make little provision for application-directed data transfer. This results in needless data copies as incoming messages are copied from network buffers to intermediate buffers (e.g., BSD's mbufs [27]) and then copied to their eventual destination. <p> The remote write, modeled after that of Thekkath et al. <ref> [39] </ref>, reads the segment number, offset, and size from the message, uses address translation tables to determine the correct place to write the data to, and then writes the data (assuming the request is valid). <p> The cost of control transfers is sufficiently high that recently a dichotomy has been drawn between control and data transfer in the interests of constructing systems to efficiently perform just data transfer <ref> [39] </ref>. ASHs remove the restrictive cost of control transfer for those operations that can be expressed in terms of ASHs. We believe that the expressiveness of ASHs is sufficient for most operations subject to low-latency requirements.
Reference: [40] <author> C.A. Thekkath, T.D. Nguyen, E. Moy, and E. Lazowska. </author> <title> Implementing network protocols at user level. </title> <booktitle> In ACM Communication Architectures, Protocols, and Applications (SIGCOMM '93), </booktitle> <pages> pages 64-73, </pages> <address> San Francisco, CA, USA, </address> <month> October </month> <year> 1993. </year>
Reference-contexts: The next section reports on the benefits of using ASHs. Like other systems <ref> [15, 16, 29, 36, 40] </ref>, all the protocols are implemented in user space. The main point to take from the results in this section is that our implementation without ASHs performs well and is competitive with the best systems reported in the literature. <p> In summary, the base performance of our system for UDP and TCP is in the same ballpark or is better than most high-performance user-level and in-kernel implementations <ref> [15, 16, 21, 29, 40] </ref>. 5 Using application-specific handlers In this section, we examine how application-specific safe handlers can be exploited to achieve good throughput, data transfer latency and control transfer latency. Many of our experiments are influenced by Clark and Tennenhouse [10].
Reference: [41] <author> C. Tschudin. </author> <title> Flexible protocol stacks. </title> <booktitle> In ACM Communication Architectures, Protocols, and Applications (SIGCOMM '91), </booktitle> <pages> pages 197-204, </pages> <address> Zurich, Switzerland, </address> <month> September </month> <year> 1991. </year>
Reference-contexts: Furthermore, many systems compose protocols at runtime <ref> [5, 23, 24, 41, 42] </ref>, making static ILP infeasible. <p> The tradeoffs discussed there are applicable here. ILP and protocol composition There have been many instances of ad hoc ILP, for example, in many networking kernels [9]. There is also quite a bit of work on protocol composition <ref> [5, 23, 24, 41, 42] </ref>. The first system to provide an automatic modular mechanism for ILP is Abbott and Peterson [1]. They describe an ILP system that composes macros into integrated loops at compile time, eliminating multiple data traversals.
Reference: [42] <author> R. van Renesse, K.P. Birman, R. Friedman, M. Hayden, and D. Karr. </author> <title> A framework for protocol composition in Horus. </title> <booktitle> In Proceedings of Fourteenth ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 138-150, </pages> <address> Ottawa, Ontario, Canada, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: Furthermore, many systems compose protocols at runtime <ref> [5, 23, 24, 41, 42] </ref>, making static ILP infeasible. <p> The tradeoffs discussed there are applicable here. ILP and protocol composition There have been many instances of ad hoc ILP, for example, in many networking kernels [9]. There is also quite a bit of work on protocol composition <ref> [5, 23, 24, 41, 42] </ref>. The first system to provide an automatic modular mechanism for ILP is Abbott and Peterson [1]. They describe an ILP system that composes macros into integrated loops at compile time, eliminating multiple data traversals.
Reference: [43] <author> T. von Eicken, D.E. Culler, S.C. Goldstein, and K.E. Schauser. </author> <title> Active messages: a mechanism for integrated communication and computation. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <pages> pages 256-267, </pages> <address> Gold Coast, Qld., Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: Control initiation ASHs can perform general computation. This ability allows them to perform control operations at message reception, implementing such computational actions as traditional active messages <ref> [43] </ref> or remote lock acquisition in a distributed shared memory system. Even recently, low-overhead control transfer had been considered to be infeasible to implement [39]. We have also integrated support for dynamic integrated layer processing and dynamic protocol composition into the ASH system. <p> Examples include remote lock allocation, reference counting, voting, global barriers, object location queries, and method invocations. The need for low-latency remote computation is so overwhelming that the parallel community has spawned a new paradigm of programming built around the concept of active messages <ref> [43] </ref>: an efficient, unprotected transfer of control to the application in the interrupt handler. A key benefit of ASHs is that because the runtime of downloaded code is bounded, they can be run in situations when performing a full context switch to an unscheduled application is impractical. <p> Nevertheless, their simple interface is easy to implement and tune; it remains to be seen if the expressiveness we provide is superior to it for real applications on real systems. In the parallel community the concept of active messages <ref> [43] </ref> has gained great popularity, since it dramatically decreases latency by executing the required code directly in the message handler. Active messages on parallel machines do not worry about issues of software protection. Several user-level AM implementations for networks of workstations have recently become available [30, 36].
Reference: [44] <author> R. Wahbe, S. Lucco, T. Anderson, and S. Graham. </author> <title> Efficient software-based fault isolation. </title> <booktitle> In Proceedings of the Fourteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 203-216, </pages> <address> Ashville, NC, USA, </address> <month> December </month> <year> 1993. </year>
Reference-contexts: The ASH system for the MIPS bounds execution time using a framework inspired by Deutsch [12]. Exceptions are prevented using runtime and static checks (as is done in existing packet-filters [31, 47]). Wild memory references are prevented using a combination of address-space fire-walls and sandboxing <ref> [44] </ref>. Wild jumps are prevented using language support. We examine each technique in further detail below. <p> To prevent this, we force all non-message loads and stores to have user-level addresses, using the code inspection (sandboxing) techniques of Wahbe et al. <ref> [44] </ref>. Similarly, we mask the lower bits of memory operations to prevent unaligned exceptions. Loads of kernel-level message data are performed only through specialized function calls: in this way the sandboxer knows no memory operations to kernel-level data (i.e., the received message buffer) should occur. <p> Those that can tolerate more latency can use the flexibility of the upcall; those that cannot will be confined to ASHs. Code importation There are a number of clear antecedents to our work: Deutsch's seminal paper [12] and Wahbe et al.'s modern revisitation of safe code importation <ref> [44] </ref> influenced our ideas strongly, as did Mogul's original packet filter paper [31].
Reference: [45] <author> I. Wakeman, A. Ghosh, J. Crowcroft, V. Jacobson, and S. Floyd. </author> <title> Implementing real time packet forwarding policies using Streams. </title> <booktitle> In Proceedings USENIX Winter 1995 Technical Conference, </booktitle> <pages> pages 71-82, </pages> <address> New Orleans, LA, USA, </address> <month> January </month> <year> 1995. </year>
Reference-contexts: 1 Introduction Applications' complexity and ambition scale with increases in processing power and network performance. For example, the last few years have seen a proliferation of distributed shared memory systems [25, 26, 28], real-time video and voice applications <ref> [45] </ref>, parallel applications [11, 34], and tightly-coupled distributed systems [2, 36, 39]. Unfortunately, although raw CPU and networking hardware speeds have increased, this increase is not reaching applications: networking software and memory subsystem performance already limit applications and will only do so more in the future [10, 13, 39].
Reference: [46] <author> D.A. Wallach, W.C. Hsieh, K.L. Johnson, M.F. Kaashoek, and W.E. Weihl. </author> <title> Optimistic active messages: A mechanism for scheduling communication with computation. </title> <booktitle> In Proceedings of the 5th Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Santa Barbara, CA, USA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Issues about schedulability and when and how a message handler should abort have been recently explored in Optimistic Active Messages <ref> [46] </ref>. The tradeoffs discussed there are applicable here. ILP and protocol composition There have been many instances of ad hoc ILP, for example, in many networking kernels [9]. There is also quite a bit of work on protocol composition [5, 23, 24, 41, 42].
Reference: [47] <author> M. Yuhara, B.N. Bershad, C. Maeda, and J.E.B. Moss. </author> <title> Efficient packet demultiplexing for multiple endpoints and large messages. </title> <booktitle> In Proceedings of the Winter 1994 USENIX Conference, </booktitle> <address> San Francisco, CA, USA, </address> <month> January </month> <year> 1994. </year> <month> 13 </month>
Reference-contexts: We describe these software techniques here in detail. The ASH system for the MIPS bounds execution time using a framework inspired by Deutsch [12]. Exceptions are prevented using runtime and static checks (as is done in existing packet-filters <ref> [31, 47] </ref>). Wild memory references are prevented using a combination of address-space fire-walls and sandboxing [44]. Wild jumps are prevented using language support. We examine each technique in further detail below. <p> DPF is an order of magnitude faster than the highest performance packet filter engines (MPF <ref> [47] </ref> and PATHFINDER [3]) in the literature. Similarly to other systems [15, 34, 36], the AN2 device is securely exported by using the ATM connection identifier to demulti-plex packets. Processes bind to a virtual circuit identifier, providing a section of their memory for messages to be DMA'ed to.
References-found: 47

