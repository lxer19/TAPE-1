URL: ftp://ftp.pmg.lcs.mit.edu/pub/castro/icpp96.ps.gz
Refering-URL: http://www.pmg.lcs.mit.edu/~castro/pubs.html
Root-URL: 
Email: email: (miguel, mds, msc, pjg)@inesc.pt  
Title: EFFICIENT AND FLEXIBLE OBJECT SHARING  
Author: Miguel Castro Manuel Sequeira Manuel Costa Paulo Guedes R. Alves Redol , Lisboa, PORTUGAL 
Affiliation: IST INESC  
Abstract: DiSOM is a software-based distributed shared memory (DSM) system, which supports intra- and inter-application sharing in heterogeneous networks of multiprocessor workstations. Unlike previous DSM systems, DiSOM provides fine-grained control over communication while retaining a simple shared memory model. It achieves this by using an update-based implementation of entry consistency, semaphores, remote object invocation, dynamic decomposition of objects and object-oriented language mechanisms. These techniques allow programmers to exploit application-specific knowledge to improve performance. A comparison between DiSOM and TreadMarks, a state-of-the-art DSM system, shows that on average DiSOM executes 33% faster, and sends 69% fewer messages and 38% less data. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Adve and M. Hill. </author> <title> A Unified Formalization of Four Shared-Memory Models. </title> <journal> IEEE TPDS, </journal> <month> June </month> <year> 1993. </year>
Reference-contexts: If a program satisfies this contract then the system guarantees that memory appears to be sequentially consistent [13]. The main difference between entry consistency and other weak consistency models, like weak and release consistency <ref> [1, 6, 12] </ref>, is that in entry consistency each synchronization object s has a set of explicitly associated objects, s:associated, and the operations executed on s only order accesses to objects in s:associated. In other weak consistency models this relation is implicit, and synchronization operations order accesses to arbitrary objects.
Reference: [2] <author> H. Bal and A. Tanenbaum. </author> <title> Distributed Programming with Shared Data. </title> <booktitle> In IEEE Conference on Computer Languages, </booktitle> <year> 1988. </year>
Reference-contexts: Our study shows that DiSOM has significantly better performance than TreadMarks. The reason for this apparent contradiction is that, unlike Midway, DiSOM uses semaphores, remote object invocation and object-oriented language techniques to provide programmers with further control over communication. Orca <ref> [2] </ref> and SAM [17] use objects as the unit of memory consistency like DiSOM. Orca uses compile time analysis to decide whether or not to replicate an object. Objects that are not replicated are accessed using remote procedure calls. <p> The most common example of this problem is that of a large matrix where groups of lines or rows are independently shared. One solution to this problem is to force the programmer to statically decompose the matrix into objects that are independently shared <ref> [2, 17] </ref>. However, decomposing an object to achieve this goal frequently requires a significant programming effort, does not adapt to dynamic changes in the sharing patterns, and can lead to poor performance. DiSOM handles the problem by allowing programmers to dynamically decompose an object into regions, i.e. subsets of fields.
Reference: [3] <author> B. Bershad, M. Zekauskas, and W. Sawdon. </author> <title> The Midway Distributed Shared Memory System. </title> <booktitle> In COMPCON, </booktitle> <month> Febru-ary </month> <year> 1993. </year>
Reference-contexts: DiSOM offers a programming model where concurrent threads synchronize explicitly and communicate by executing operations on shared objects. The objects are kept consistent according to a variant of the entry consistency <ref> [3] </ref> memory model. This model requires shared objects to be explicitly associated with synchronization objects; and consistency is guaranteed as long as an access to a shared object is enclosed between an acquire and a release on a synchronization object associated with the shared object. <p> This causes the system to frequently send more messages and data than needed. Midway <ref> [3] </ref> introduced the entry consistency memory model, implemented it using an update protocol, and made some preliminary comparisons with eager release consistency. This paper presents one of the first two comparison studies between an entry consistency based system and a lazy release consistency based system. <p> A wait operation completes when the boolean variable of the thread executing the operation is true, and it resets the boolean variable. DiSOM uses the synchronization operations to drive the memory consistency protocol that implements the entry consistency <ref> [3] </ref> memory model. The memory consistency model is best described as a contract between the system and the application program.
Reference: [4] <author> J. Carter, J. Bennett, and W. Zwaenepoel. </author> <title> Implementation and Performance of Munin. </title> <booktitle> In SOSP, </booktitle> <month> October </month> <year> 1991. </year>
Reference-contexts: INTRODUCTION Distributed shared memory (DSM) systems <ref> [14, 10, 16, 9, 7, 4] </ref> offer the abstraction of a centralized memory that is shared by all the processors in a distributed system. This abstraction simplifies programming because it makes communication implicit. <p> RELATED WORK The first software distributed shared memory systems [14, 10, 16, 9] used pages or coarse-grained segments as the unit of coherence, and had a sequential consistency [13] memory model. A second generation of DSM systems explored relaxed memory consistency models to improve performance. Munin <ref> [4] </ref> provided a software implementation of release consistency [6] and TreadMarks introduced lazy release consistency [7]. In both these systems the unit of coherence is a virtual memory page and they support concurrent writers on the same coherence unit. <p> At the end of each phase, a thread signals the semaphores corresponding to the boundary regions just computed and waits in its neighbor's semaphores. The TSP application solves the well known traveling salesperson problem in a graph of 19 cities, using a branch and bound algorithm <ref> [4] </ref>. The shared data structures in TSP are the minimum length tour encountered so far, an array of structures describing partially evaluated and unused tours, a priority queue with pointers to partially evaluated tours, and a stack with pointers to unused tours.
Reference: [5] <author> B. Liskov et al. </author> <title> Implementation of Argus. </title> <booktitle> In SOSP, </booktitle> <month> Novem-ber </month> <year> 1987. </year>
Reference-contexts: In DiSOM each object class defines a pack and an unpack method. These methods handle marshaling and unmarshaling of the object state and perform the needed conversions. This simple technique has been used by several object based distributed systems, for example Argus <ref> [5] </ref>. Conversions are performed using an external data representation policy that avoids the conversion when communicating processes have the same architecture. Distributed Read-Write Locks The distributed read-write lock algorithm implemented by the EcObject class is a variant of Li's dynamic distributed manager with distributed copy sets [14].
Reference: [6] <author> K. Gharachorloo et al. </author> <title> Memory Consistency and Event Ordering in Scalable Shared-Memory Multiprocessors. </title> <booktitle> In ISCA, </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: A second generation of DSM systems explored relaxed memory consistency models to improve performance. Munin [4] provided a software implementation of release consistency <ref> [6] </ref> and TreadMarks introduced lazy release consistency [7]. In both these systems the unit of coherence is a virtual memory page and they support concurrent writers on the same coherence unit. <p> If a program satisfies this contract then the system guarantees that memory appears to be sequentially consistent [13]. The main difference between entry consistency and other weak consistency models, like weak and release consistency <ref> [1, 6, 12] </ref>, is that in entry consistency each synchronization object s has a set of explicitly associated objects, s:associated, and the operations executed on s only order accesses to objects in s:associated. In other weak consistency models this relation is implicit, and synchronization operations order accesses to arbitrary objects.
Reference: [7] <author> P. Keleher et al. Treadmarks: </author> <title> Distributed Shared Memory on Standard Workstations and Operating Systems. </title> <booktitle> In Winter USENIX, </booktitle> <month> January </month> <year> 1994. </year>
Reference-contexts: INTRODUCTION Distributed shared memory (DSM) systems <ref> [14, 10, 16, 9, 7, 4] </ref> offer the abstraction of a centralized memory that is shared by all the processors in a distributed system. This abstraction simplifies programming because it makes communication implicit. <p> This allows programmers to choose between function-shipping and data-shipping communication. This mechanism can be used to reduce the number of messages sent and the amount of data transferred, as shown in the evaluation section. This paper compares the performance and programming complexity of DiSOM and TreadMarks <ref> [7] </ref>, a state-of-the-art distributed shared memory system that implements lazy release consistency. The measurements were obtained by running the same set of applications both on DiSOM and on TreadMarks on the same cluster of workstations. <p> A second generation of DSM systems explored relaxed memory consistency models to improve performance. Munin [4] provided a software implementation of release consistency [6] and TreadMarks introduced lazy release consistency <ref> [7] </ref>. In both these systems the unit of coherence is a virtual memory page and they support concurrent writers on the same coherence unit. <p> It redefines the pack and unpack methods to transfer only the updates, and uses an instrumented store operator to set the dirty bits. EVALUATION We evaluated DiSOM's performance and the complexity of its programming model by comparing it with TreadMarks <ref> [7] </ref>. The measurements were obtained using four shared memory parallel applications: Matrix Multiply (MM), Successive Over-Relaxation (SOR), Traveling Salesperson (TSP) and Water (Water). The code of SOR, TSP and Water was ported from TreadMarks. <p> The regions of C are associated with a barrier that is used to synchronize threads at the end of the computation. This barrier is the only synchronization used in TreadMark's implementation. SOR is an implementation of the red-black successive over-relaxation algorithm <ref> [7] </ref>. SOR's input is a 1024 fi 512 single precision floating point matrix and it performs 106 iterations. The matrix is statically divided in stripes with an equal number of rows and each stripe is assigned to a different thread.
Reference: [8] <author> S. Adve et al. </author> <title> A Comparison of Entry Consistency and Lazy Release Consistency Implementations. </title> <booktitle> In IEEE HPCA, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: Midway [3] introduced the entry consistency memory model, implemented it using an update protocol, and made some preliminary comparisons with eager release consistency. This paper presents one of the first two comparison studies between an entry consistency based system and a lazy release consistency based system. The other study <ref> [8] </ref> compared TreadMarks with a system very similar to Midway, and concluded that there was no clear winner in terms of performance. Our study shows that DiSOM has significantly better performance than TreadMarks.
Reference: [9] <author> S. Zhou et al. </author> <title> Heretogeneous Distributed Shared Memory. </title> <journal> IEEE TPDS, </journal> <month> September </month> <year> 1992. </year>
Reference-contexts: INTRODUCTION Distributed shared memory (DSM) systems <ref> [14, 10, 16, 9, 7, 4] </ref> offer the abstraction of a centralized memory that is shared by all the processors in a distributed system. This abstraction simplifies programming because it makes communication implicit. <p> The rest of the paper is organized as follows. First, we discuss related work. Next, we describe the programming model and its implementation. Later, we evaluate the performance and programmability of DiSOM. RELATED WORK The first software distributed shared memory systems <ref> [14, 10, 16, 9] </ref> used pages or coarse-grained segments as the unit of coherence, and had a sequential consistency [13] memory model. A second generation of DSM systems explored relaxed memory consistency models to improve performance.
Reference: [10] <author> B. Fleish and G. Popek. </author> <title> Mirage: A Coherent Distributed Virtual Memory Design. </title> <booktitle> In SOSP, </booktitle> <month> December </month> <year> 1989. </year>
Reference-contexts: INTRODUCTION Distributed shared memory (DSM) systems <ref> [14, 10, 16, 9, 7, 4] </ref> offer the abstraction of a centralized memory that is shared by all the processors in a distributed system. This abstraction simplifies programming because it makes communication implicit. <p> The rest of the paper is organized as follows. First, we discuss related work. Next, we describe the programming model and its implementation. Later, we evaluate the performance and programmability of DiSOM. RELATED WORK The first software distributed shared memory systems <ref> [14, 10, 16, 9] </ref> used pages or coarse-grained segments as the unit of coherence, and had a sequential consistency [13] memory model. A second generation of DSM systems explored relaxed memory consistency models to improve performance.
Reference: [11] <author> A. Hosking and J. Moss. </author> <title> Protection Traps and Alternatives for Memory Management of an Object-Oriented Language. </title> <booktitle> In SOSP, </booktitle> <month> December </month> <year> 1993. </year>
Reference-contexts: DiSOM uses both the IP address of the machine where the process is running and the TCP/IP port of the process to make the object identifiers globally unique. Note that the identifiers will be globally unique even across different applications. Our swizzling scheme is a form of node marking <ref> [11] </ref>, i.e. when the memory consistency protocol brings an object's state into the address space of a process, all the references in the object are converted.
Reference: [12] <author> P. Keleher, A. Cox, and W. Zwaenepoel. </author> <title> Lazy Release Consistency for Software Distributed Shared Memory. </title> <booktitle> In ISCA, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: If a program satisfies this contract then the system guarantees that memory appears to be sequentially consistent [13]. The main difference between entry consistency and other weak consistency models, like weak and release consistency <ref> [1, 6, 12] </ref>, is that in entry consistency each synchronization object s has a set of explicitly associated objects, s:associated, and the operations executed on s only order accesses to objects in s:associated. In other weak consistency models this relation is implicit, and synchronization operations order accesses to arbitrary objects.
Reference: [13] <author> L. Lamport. </author> <title> How to Make a Multiprocessor Computer That Correctly Executes Multiprocess Programs. </title> <journal> IEEE TOC, </journal> <month> September </month> <year> 1979. </year>
Reference-contexts: Next, we describe the programming model and its implementation. Later, we evaluate the performance and programmability of DiSOM. RELATED WORK The first software distributed shared memory systems [14, 10, 16, 9] used pages or coarse-grained segments as the unit of coherence, and had a sequential consistency <ref> [13] </ref> memory model. A second generation of DSM systems explored relaxed memory consistency models to improve performance. Munin [4] provided a software implementation of release consistency [6] and TreadMarks introduced lazy release consistency [7]. <p> If a program satisfies this contract then the system guarantees that memory appears to be sequentially consistent <ref> [13] </ref>. The main difference between entry consistency and other weak consistency models, like weak and release consistency [1, 6, 12], is that in entry consistency each synchronization object s has a set of explicitly associated objects, s:associated, and the operations executed on s only order accesses to objects in s:associated.
Reference: [14] <author> K. Li and P. Hudak. </author> <title> Memory Coherence in Shared Virtual Memory Systems. </title> <journal> ACM TOCS, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: INTRODUCTION Distributed shared memory (DSM) systems <ref> [14, 10, 16, 9, 7, 4] </ref> offer the abstraction of a centralized memory that is shared by all the processors in a distributed system. This abstraction simplifies programming because it makes communication implicit. <p> The rest of the paper is organized as follows. First, we discuss related work. Next, we describe the programming model and its implementation. Later, we evaluate the performance and programmability of DiSOM. RELATED WORK The first software distributed shared memory systems <ref> [14, 10, 16, 9] </ref> used pages or coarse-grained segments as the unit of coherence, and had a sequential consistency [13] memory model. A second generation of DSM systems explored relaxed memory consistency models to improve performance. <p> Conversions are performed using an external data representation policy that avoids the conversion when communicating processes have the same architecture. Distributed Read-Write Locks The distributed read-write lock algorithm implemented by the EcObject class is a variant of Li's dynamic distributed manager with distributed copy sets <ref> [14] </ref>. Li used this algorithm to keep memory coherent. We modified it to implement concurrent read/exclusive write synchronization of multiple threads running in multiple processes in a distributed system. The algorithm associates two types of tokens with each lock, the write token and the read token.
Reference: [15] <author> N. Neves, M. Castro, and P. Guedes. </author> <title> A Checkpoint Protocol for an Entry Consistent Shared Memory System. </title> <booktitle> In PODC, </booktitle> <month> August </month> <year> 1994. </year>
Reference-contexts: Several threads may execute in each process. The communication channels between the processes are assumed to be reliable and FIFO. We assume no process failures in this paper. In a previous paper <ref> [15] </ref>, we described a checkpointing protocol that allows DiSOM to support single failures efficiently. Shared Address Space Most distributed shared memory systems achieve uniform naming by mapping each shared data item to the same virtual memory address in each process.
Reference: [16] <author> U. Ramachandran and M. Khalidi. </author> <title> An Implementation of Distributed Shared Memory. </title> <booktitle> In Distributed and Multiprocessor Systems Workshop, </booktitle> <month> October </month> <year> 1989. </year>
Reference-contexts: INTRODUCTION Distributed shared memory (DSM) systems <ref> [14, 10, 16, 9, 7, 4] </ref> offer the abstraction of a centralized memory that is shared by all the processors in a distributed system. This abstraction simplifies programming because it makes communication implicit. <p> The rest of the paper is organized as follows. First, we discuss related work. Next, we describe the programming model and its implementation. Later, we evaluate the performance and programmability of DiSOM. RELATED WORK The first software distributed shared memory systems <ref> [14, 10, 16, 9] </ref> used pages or coarse-grained segments as the unit of coherence, and had a sequential consistency [13] memory model. A second generation of DSM systems explored relaxed memory consistency models to improve performance.
Reference: [17] <author> D. Scales and M. Lam. </author> <title> The Design and Evaluation of a Shared Object System for Distributed Memory Machines. </title> <booktitle> In OSDI, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: Our study shows that DiSOM has significantly better performance than TreadMarks. The reason for this apparent contradiction is that, unlike Midway, DiSOM uses semaphores, remote object invocation and object-oriented language techniques to provide programmers with further control over communication. Orca [2] and SAM <ref> [17] </ref> use objects as the unit of memory consistency like DiSOM. Orca uses compile time analysis to decide whether or not to replicate an object. Objects that are not replicated are accessed using remote procedure calls. <p> The most common example of this problem is that of a large matrix where groups of lines or rows are independently shared. One solution to this problem is to force the programmer to statically decompose the matrix into objects that are independently shared <ref> [2, 17] </ref>. However, decomposing an object to achieve this goal frequently requires a significant programming effort, does not adapt to dynamic changes in the sharing patterns, and can lead to poor performance. DiSOM handles the problem by allowing programmers to dynamically decompose an object into regions, i.e. subsets of fields.
Reference: [18] <author> J. Singh, W. Weber, and A. Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory. </title> <type> Technical Report CSL-TR-91-469, </type> <institution> Stanford University, </institution> <year> 1991. </year>
Reference-contexts: The array of tours and the priority queue are large objects that are sparsely modified between synchronization operations. Therefore, we used the version of the array class that implements fine-grained access detection. Water is a N-body molecular dynamics simulation from the SPLASH benchmark suite <ref> [18] </ref>. It calculates forces and potentials in a system of water molecules in the liquid state. We measured 5 steps of a system with 343 molecules. The main data structures are an array of structures describing molecules and a set of global sums. <p> Work is partitioned statically by assigning a set of contiguous molecules in the array to each thread, and having each thread compute the interactions between its molecules and the 343=2 molecules that follow them in the array. We used the optimization suggested in <ref> [18] </ref> of collecting the changes to the molecules in private memory and updating the molecules only at the end of the inter-molecular forces calculation phase. The TreadMarks' implementation uses a lock per molecule to synchronize updates, and uses barriers before and after the phases that compute inter-molecular interactions. <p> Remote write operations are used to update the molecules at the end of the phase that computes inter-molecular forces. Since only approximately 1=4 of the object that describes a molecule is shared <ref> [18] </ref>, DiSOM's implementation uses specialized versions of the pack and unpack methods that only transmit the shared fields.
Reference: [19] <author> P. Wilson and S. Kakkad. </author> <title> Pointer Swizzling at Page Fault Time: Efficiently and Compatibly Supporting Huge Address Spaces on Standard Hardware. </title> <booktitle> In IWOOOS, </booktitle> <month> October </month> <year> 1992. </year>
Reference-contexts: Furthermore, existing systems that use this technique coordinate address space layout only within a single application and therefore fail to support inter-application sharing. Since DiSOM supports sharing across different architectures, it uses a form of pointer swizzling <ref> [19] </ref> to achieve uniform object naming across different processes. The system assigns a global identifier to each shared object and automatically converts between the language level reference and the global identifier when the reference is exchanged in a message.
Reference: [20] <author> M. Zekauskas, W. Sawdon, and B. Bershad. </author> <title> Software Write Detection for Distributed Shared Memory. </title> <booktitle> In OSDI, </booktitle> <month> Novem-ber </month> <year> 1994. </year>
Reference-contexts: A concrete example concerns the implementation of fine-grained access detection. DiSOM's memory model was defined to allow object-level access detection using the calls to the synchronization primitives. Therefore, there is no need to use page faults or software dirty bits <ref> [20] </ref> to detect updates to shared objects. However, if objects are large and sparsely written (e.g. large arrays), this simple technique may transfer considerably more data than needed to ensure coherency. This happens because the update protocol conservatively transfers the entire object state to an acquiring thread. <p> This happens because the update protocol conservatively transfers the entire object state to an acquiring thread. This problem is solved by providing a special array class that implements a software dirty bit scheme similar to Midway <ref> [20] </ref>. This class is a subclass of the regular array class. It redefines the pack and unpack methods to transfer only the updates, and uses an instrumented store operator to set the dirty bits. <p> Its performance was better than TreadMark's for a small number of processors but significantly worse for larger numbers of processors. One solution to this problem is not to associate objects with barriers and instead acquire the objects for reading after the barrier crossing. This approach was used in <ref> [20] </ref>. It avoids sending more data than needed but it can send significantly more messages. For example, in Water with 8 processors it would send 2.5 times more messages than DiSOM and 1.3 times more messages than TreadMarks. Therefore, semaphores are a more efficient solution.
References-found: 20

