URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-95-1265/CS-TR-95-1265.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-95-1265/
Root-URL: http://www.cs.wisc.edu
Email: galileo@cs.wisc.edu  
Title: Simulation of the SCI Transport Layer on the Wisconsin Wind Tunnel  
Author: Douglas C. Burger and James R. Goodman 
Address: 1210 West Dayton Street Madison, Wisconsin 53706 USA  
Affiliation: Computer Sciences Department University of Wisconsin-Madison  
Abstract: Parallel simulation of parallel machines is fast becoming a critical technique for the evaluation of new parallel architectures and architectural extensions. Fast and accurate simulation of the interconnection network in parallel simulators is extremely difficult, but also extremely important. In this report, we describe an extension to the Wis-consin Wind Tunnel that simulates the transport layer of the Scalable Coherent Interface. This module enables an evaluation of switch designs and network topologies that uses real parallel codes. It also enables the exploration of architectural and protocol optimization effects on network performance. Finally, our extension increases confidence in SCI-related results that were obtained without detailed network simulation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Douglas C. Burger and David A. Wood. </author> <title> Accuracy vs. Performance in Parallel Simulation of Interconnection Networks. </title> <booktitle> In Proceedings of the 9th International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1995. </year>
Reference-contexts: This centralization is a severe limitation, which inates simulation time substantially. The simulator is nevertheless critical for validating whatever less-expensive network models are used (a detailed discussion of relevant trade-offs appears elsewhere <ref> [1] </ref>). It also allows design parameters of SCI-based networks to be evaluated. Finally, the network simulator enables the measurement effects produced by architectural and protocol optimizations that change the network load or contention distributions. <p> If the quantum latency is too large, the network simulator may not send the header unit to the intended recipient before the target receipt event was to occur, causing an error. Smaller quanta can greatly inate simulation time, as the number of inter-quanta synchronizations per target cycle increases <ref> [1] </ref>. 4.3 Resource latencies The physical times that symbols take to move across network resources are listed in Table 1. is a base factor that accounts for the speed of the hardware.
Reference: [2] <author> David L. Chaiken and Anant Agarwal. </author> <title> Software-Extended Coherent Shared Memory: Performance and Cost. </title> <booktitle> In Proceedings of the 21th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 314324, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Executing such simulations on uniprocessors is prohibitively expensive for target machines of any significant size. Physical memory is the prime limiting factor, but even with enough memory, such simulations run extremely slowly. An increasingly popular solution is to parallelize the simulations <ref> [2, 4, 11] </ref>, permitting substantially larger simulations to be performed. Parallelizing the simulation, however, creates at least one significant problem: the efficient communication of target state between physical nodes of the host machine.
Reference: [3] <author> R.C. Covington, S. Madala, V. Mehta, J.R. Jump, and J.B. Sin-clair. </author> <title> The Rice Parallel Processing Testbed. </title> <booktitle> In Proceedings of the 1988 ACM SIGMETRICS Conference on Measurements and Modeling of Computer Systems, </booktitle> <pages> pages 411, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: The Wisconsin Wind Tunnel (WWT) [11] is one such parallel simulator, which runs on a Thinking Machines CM-5. WWT uses conservative, discrete-event simulation [5, 8, 10, 15] to accurately calculate the logical execution time of the target application. Superior performance is obtained through direct execution <ref> [3] </ref> of identical target and host instructions on the native CM-5 hardware. WWTs solution to the problem of communicating simulation state is to guarantee windows of target time during which a node can perform simulation without requiring state from other physical nodes.
Reference: [4] <author> Helen Davis, Stephen R. Goldschmidt, and John Hennessy. </author> <title> Multiprocessor Simulation and Tracing Using Tango. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <pages> pages II99107, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Executing such simulations on uniprocessors is prohibitively expensive for target machines of any significant size. Physical memory is the prime limiting factor, but even with enough memory, such simulations run extremely slowly. An increasingly popular solution is to parallelize the simulations <ref> [2, 4, 11] </ref>, permitting substantially larger simulations to be performed. Parallelizing the simulation, however, creates at least one significant problem: the efficient communication of target state between physical nodes of the host machine.
Reference: [5] <author> Richard M. Fujimoto. </author> <title> Parallel Discrete Event Simulation. </title> <journal> Communications of the ACM, </journal> <volume> 33(10):3053, </volume> <month> October </month> <year> 1990. </year>
Reference-contexts: Parallelizing the simulation, however, creates at least one significant problem: the efficient communication of target state between physical nodes of the host machine. The Wisconsin Wind Tunnel (WWT) [11] is one such parallel simulator, which runs on a Thinking Machines CM-5. WWT uses conservative, discrete-event simulation <ref> [5, 8, 10, 15] </ref> to accurately calculate the logical execution time of the target application. Superior performance is obtained through direct execution [3] of identical target and host instructions on the native CM-5 hardware.
Reference: [6] <author> Ross E. Johnson and James R. Goodman. </author> <title> Interconnect Topologies with Point-to-Point Rings. </title> <type> Technical Report 1058, </type> <institution> Computer Sciences Department, University of Wisconsin-Madison, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: To prevent deadlock, responses and requests are queued in logically separate queues. Request packets are only processed when the response queue is empty. 3 Physical model While the SCI standard defines what types of packets may be sent, considerable latitude exists in defining the network topology <ref> [6] </ref>, the physical structure of the switches, and the processor/network interface. In this section we describe the physical assumptions on which our simulator are based. The topology simulated is a k-ary n-cube of rings. The dimensionality of the network is an input parameter, as is the number of processing elements.
Reference: [7] <author> Alain Kgi, Nagi Aboulenein, Douglas C. Burger, and James R. Goodman. </author> <title> Techniques for Reducing the Overheads of Shared-Memory Multiprocessing. </title> <type> Technical Report 1266, </type> <institution> Computer Sciences Department, University of Wisconsin-Madison, </institution> <year> 1995. </year>
Reference-contexts: The relations shown in Table 1 were arrived at by estimating the amount of hardware needed to perform each function, and normalizing all latencies to . In the study for which this simulator was first used <ref> [7] </ref>, was set to be 9 cycles. These delays, however, are measured in network cycles. <p> As the network is not necessarily clocked at the same rate as the processor, a scaling factor was introduced that adjusts network latencies to be converted to the number of instructions issued in the same amount of time. In a previous study <ref> [7] </ref>, we introduced two scaling factors. <p> to channel Channel to channel Channel to bypass buffer Bypass buffer to channel Agent delay Channel to queue Queue to processor 2 DELAY DELAY 3 ( ) 3 1 DELAY 1+( ) 2 DELAY 2 ( ) 2 3 DELAY 2 DELAY ( ) 3 2 DELAY 6 nous ushes <ref> [7] </ref>. Unless otherwise specified, simulated networks have 4 slots per queue, and the speed of the network switch is cycles. 5.1 Network effect on execution time latency experiments versus the same experiments using SCI network simulation (henceforth called ns). Two sets of constant latency experiments were run.
Reference: [8] <author> Boris D. Lubachevsky. </author> <title> Efficient Distributed Event-Driven Simulations of Multiple-Loop Networks. </title> <journal> Communications of the ACM, </journal> <volume> 32(2):111123, </volume> <month> January </month> <year> 1989. </year>
Reference-contexts: Parallelizing the simulation, however, creates at least one significant problem: the efficient communication of target state between physical nodes of the host machine. The Wisconsin Wind Tunnel (WWT) [11] is one such parallel simulator, which runs on a Thinking Machines CM-5. WWT uses conservative, discrete-event simulation <ref> [5, 8, 10, 15] </ref> to accurately calculate the logical execution time of the target application. Superior performance is obtained through direct execution [3] of identical target and host instructions on the native CM-5 hardware.
Reference: [9] <author> John M. Mellor-Crummey and Michael L. Scott. </author> <title> Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(1):2165, </volume> <month> February </month> <year> 1991. </year>
Reference-contexts: Ocean is a hydrodynamic simulation, which was run for a 98x98 input grid over two simulated days. All of our experiments used the base SCI coherence protocol with MCS locks <ref> [9] </ref>, 64-byte cache blocks, 32 processors, and asynchro Path Latency Processor to queue Table 1: Latencies through the network Queue to channel Channel to channel Channel to bypass buffer Bypass buffer to channel Agent delay Channel to queue Queue to processor 2 DELAY DELAY 3 ( ) 3 1 DELAY 1+(
Reference: [10] <author> David Nicol. </author> <title> Conservative Parallel Simulation of Priority Class Queueing Networks. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(3):398412, </volume> <month> May </month> <year> 1992. </year>
Reference-contexts: Parallelizing the simulation, however, creates at least one significant problem: the efficient communication of target state between physical nodes of the host machine. The Wisconsin Wind Tunnel (WWT) [11] is one such parallel simulator, which runs on a Thinking Machines CM-5. WWT uses conservative, discrete-event simulation <ref> [5, 8, 10, 15] </ref> to accurately calculate the logical execution time of the target application. Superior performance is obtained through direct execution [3] of identical target and host instructions on the native CM-5 hardware.
Reference: [11] <author> Steven K. Reinhardt, Mark D. Hill, James R. Larus, Alvin R. Leb-eck, James C. Lewis, and David A. Wood. </author> <title> The Wisconsin Wind Tunnel: Virtual Prototyping of Parallel Computers. </title> <booktitle> In Proceedings of the 1993 ACM SIGMETRICS Conference on Measurements and Modeling of Computer Systems, </booktitle> <pages> pages 4860, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Executing such simulations on uniprocessors is prohibitively expensive for target machines of any significant size. Physical memory is the prime limiting factor, but even with enough memory, such simulations run extremely slowly. An increasingly popular solution is to parallelize the simulations <ref> [2, 4, 11] </ref>, permitting substantially larger simulations to be performed. Parallelizing the simulation, however, creates at least one significant problem: the efficient communication of target state between physical nodes of the host machine. <p> An increasingly popular solution is to parallelize the simulations [2, 4, 11], permitting substantially larger simulations to be performed. Parallelizing the simulation, however, creates at least one significant problem: the efficient communication of target state between physical nodes of the host machine. The Wisconsin Wind Tunnel (WWT) <ref> [11] </ref> is one such parallel simulator, which runs on a Thinking Machines CM-5. WWT uses conservative, discrete-event simulation [5, 8, 10, 15] to accurately calculate the logical execution time of the target application.
Reference: [12] <author> Steven L. Scott, James R. Goodman, and Mary K. Vernon. </author> <title> Performance of the SCI Ring. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 403414, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: For higher dimensions this is clearly prohibitively expensive, but the control is much simpler than for merging multiple physical queues. Such a merger would require allowing multiple high-speed channels to simultaneously write variable-sized packets into a single queue. A previous study <ref> [12] </ref> assumed that packets sent from a queue were placed into an active buffer to await the returning echo. The queues assumed here hold a sent packet in the queue while its transmission is pending.
Reference: [13] <author> Jaswinder Pal Singh, Wolf-Dietrich Weber, and Anoop Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared Memory. Computer Architecture News, </title> <address> 20(1):544, </address> <month> March </month> <year> 1992. </year>
Reference-contexts: We explore three network design parameters, and finally measure the performance of WWT using the SCI network simulator instead of a constant network latency model. All experimental results presented in this section were obtained by running Ocean, one of the SPLASH <ref> [13] </ref> benchmarks, as the target application. Ocean is a hydrodynamic simulation, which was run for a 98x98 input grid over two simulated days.
Reference: [14] <institution> IEEE Computer Society. IEEE Standard for Scalable Coherent Interface (SCI). IEEE Std 1596-1992, </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: Consequently, the network must be considered when parallel systems are evaluated through simulation. This report describes a network simulator that extends WWT with a network based on the Scalable Coherent Interface <ref> [14] </ref> transport layer. The simulator is a cycle-by cycle event-driven module that runs on one centralized This work is supported in part by NSF Grant CCR-9207971, generous funding from the Apple Computer Advanced Technology Group, and donations from Thinking Machines Corporation. <p> A node accumulates go bits during its recovery phase to throttle other nodes, so that it is never starved for bandwidth when ring utilization is high. For a detailed description of the ow control algorithm the reader is referred to the standard document <ref> [14] </ref>. The elasticity buffer, which nodes use to maintain synchronism with adjacent channels (clocks are synchronized with a phase-lock loop), is also not represented in the simulator. 4 Implementation of network simulation In this section we first describe the original WWT network model.
Reference: [15] <author> Jeff S. Steinman. </author> <title> Breathing Time Warp. </title> <booktitle> In Proceedings of Parallel and Distributed Simulation, </booktitle> <pages> pages 109118, </pages> <year> 1993. </year>
Reference-contexts: Parallelizing the simulation, however, creates at least one significant problem: the efficient communication of target state between physical nodes of the host machine. The Wisconsin Wind Tunnel (WWT) [11] is one such parallel simulator, which runs on a Thinking Machines CM-5. WWT uses conservative, discrete-event simulation <ref> [5, 8, 10, 15] </ref> to accurately calculate the logical execution time of the target application. Superior performance is obtained through direct execution [3] of identical target and host instructions on the native CM-5 hardware.
References-found: 15

