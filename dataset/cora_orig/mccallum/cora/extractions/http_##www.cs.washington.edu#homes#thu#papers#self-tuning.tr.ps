URL: http://www.cs.washington.edu/homes/thu/papers/self-tuning.tr.ps
Refering-URL: http://www.cs.washington.edu/homes/thu/papers/pps.abstract.html
Root-URL: 
Title: Maximizing Speedup Through Self-Tuning of Processor Allocation  
Author: Thu D. Nguyen, Raj Vaswani, and John Zahorjan 
Note: This work was supported in part by the National Science Foundation (Grants CCR-9123308 and CCR-9200832) and the Washington Technology Center.  
Address: Box 352350  Seattle, WA 98195-2350 USA  
Affiliation: Department of Computer Science and Engineering,  University of Washington  
Abstract: Technical Report UW-CSE-95-09-02 October 3, 1995 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. A. Alverson and D. Notkin. </author> <title> Program Structuring for Effective Parallel Portability. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(9) </volume> <pages> 1041-59, </pages> <month> Sept. </month> <year> 1993. </year>
Reference-contexts: Furthermore, we believe that, with some modifications, our algorithm could also be applicable to non-iterative applications that use general runtime work organization paradigms, such as work heap <ref> [1, 12, 19] </ref>. 2 Section 4 defines the basic self-tuning schemes we employ, and Section 5 examines their performance experimentally. Section 6 extends the basic schemes to allow more fine grained scheduling decisions, and examines the implications to performance. Section 7 discusses related work. <p> a family of self-tuning algorithms that assumes no knowledge of application structure other than the fact that the application is iterative. 4.1 A Basic Self-Tuning Algorithm We start with two assumptions: * Speedup is a function of a single variable; S (p) : I ! R, where the domain is <ref> [1; P ] </ref>. * S (p) can be calculated for any p, 1 p P , by measuring the efficiency E (p) of any one iteration and solving equation 2.
Reference: [2] <author> T. B. Brecht and K. Guha. </author> <title> Using Parallel Program Characteristics in Dynamic Multiprocessor Allocation Policies. </title> <address> ftp://www.cs.yorku.ca/pub/brecht/Brecht Guha.ps, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: Majumdar et. al. [9], Chiang et. al [3], Leutenegger and Vernon [8], Sevcik [15, 16], Ghosal et. al. [5], Rosti et. al. [14] and others have proposed using application characteristics such as efficiency, speedup, and average parallelism to improve the performance of static processor schedulers. More recently, Brecht <ref> [2] </ref> has proposed that application characteristics such as efficiency and execution time can profitably be used by dynamic processor schedulers as well. All of these studies, however, assume the availability of accurate historical performance data, provided to the scheduler simultaneously with job submission.
Reference: [3] <author> S.-H. Chiang, R. K. Mansharamani, and M. K. Vernon. </author> <title> Use of Application Characteristics and Limited Preemption for Run-to-Completion Parallel Processor Scheduling Policies. </title> <booktitle> In Proceedings of the ACM SIGMETRICS Conference, </booktitle> <pages> pages 33-44, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: This work differs from ours in that its goal is to determine an appropriate allocation to each of several simultaneously scheduled jobs, typically with the goal of minimizing average response time. Majumdar et. al. [9], Chiang et. al <ref> [3] </ref>, Leutenegger and Vernon [8], Sevcik [15, 16], Ghosal et. al. [5], Rosti et. al. [14] and others have proposed using application characteristics such as efficiency, speedup, and average parallelism to improve the performance of static processor schedulers.
Reference: [4] <author> E. C. Cooper and R. P. Draves. </author> <title> C Threads. </title> <type> Technical Report CMU-CS-88-154, </type> <institution> Department of Computer Science, Carnegie-Mellon University, </institution> <month> June </month> <year> 1988. </year>
Reference-contexts: This information is made available to the system and user jobs through a set of read-only registers. The KSR-2 runs a variant of the OSF/1 UNIX operating system. We use the KSR KAP preprocessor to parallelize sequential applications, and the KSR PRESTO runtime system and CThreads <ref> [4] </ref>, an efficient user-level threads package, as the vehicles of parallelism.
Reference: [5] <author> D. Ghosal, G. Serazzi, and S. K. Tripathi. </author> <title> The Processor Working Set and its Use in Scheduling Multiprocessor Systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17(5) </volume> <pages> 443-53, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: This work differs from ours in that its goal is to determine an appropriate allocation to each of several simultaneously scheduled jobs, typically with the goal of minimizing average response time. Majumdar et. al. [9], Chiang et. al [3], Leutenegger and Vernon [8], Sevcik [15, 16], Ghosal et. al. <ref> [5] </ref>, Rosti et. al. [14] and others have proposed using application characteristics such as efficiency, speedup, and average parallelism to improve the performance of static processor schedulers.
Reference: [6] <author> T. E. Jeremiassen. </author> <title> Using Compile-Time Analysis and Transformation to Reduce False Sharing on Shared-Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> University of Washington, </institution> <year> 1995. </year>
Reference-contexts: Table 1: Benchmark Applications (hc = hand-coded, cps = compiler-parallelized sequential). #Iterations is the number of iterations for which the program executes; Runtime is the execution time in seconds on a single processor. hand-coded and compiler-parallelized applications and automatically insert such calls. Jeremiassen <ref> [6] </ref> has shown that it is possible to automatically detect phase behavior for many hand-coded applications. For our application suite, only FLO52 had a non-obvious iterative structure: its useful iterative loop is hidden in an outer loop that drives the program to solve multiple data sets.
Reference: [7] <author> S. Kirkpatrick, J. C. D. Gellat, and M. P. Vecchi. </author> <title> Optimization by Simulated Annealing. </title> <journal> Science, </journal> <volume> 220(4598) </volume> <pages> 671-680, </pages> <month> May </month> <year> 1983. </year>
Reference-contexts: To account for the relationship of the performance in one phase to the allocation decisions made for other phases, a more complicated search procedure is required. In particular, we propose a randomized search technique, called inter-dependent multi-phase self-tuning, based on simulated annealing <ref> [7] </ref>. 10 6.1 Inter-dependent Multi-Phase Self-Tuning The basic operation of this technique is to choose an initial candidate allocation vector, to evaluate the performance at this vector, to select a modified vector to consider, to evaluate performance at the modified vector, and then to accept or reject the modified vector as
Reference: [8] <author> S. T. Leutenegger and M. K. Vernon. </author> <title> The Performance of Multiprogrammed Multiprocessor Scheduling Policies. </title> <booktitle> In Proceedings of the ACM SIGMETRICS Conference, </booktitle> <pages> pages 226-236, </pages> <month> May </month> <year> 1990. </year> <month> 13 </month>
Reference-contexts: This work differs from ours in that its goal is to determine an appropriate allocation to each of several simultaneously scheduled jobs, typically with the goal of minimizing average response time. Majumdar et. al. [9], Chiang et. al [3], Leutenegger and Vernon <ref> [8] </ref>, Sevcik [15, 16], Ghosal et. al. [5], Rosti et. al. [14] and others have proposed using application characteristics such as efficiency, speedup, and average parallelism to improve the performance of static processor schedulers.
Reference: [9] <author> S. Majumdar, D. L. Eager, and R. B. Bunt. </author> <title> Scheduling in Multiprogrammed Parallel Systems. </title> <booktitle> In Proceedings of the ACM SIGMETRICS Conference, </booktitle> <pages> pages 104-113, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: This work differs from ours in that its goal is to determine an appropriate allocation to each of several simultaneously scheduled jobs, typically with the goal of minimizing average response time. Majumdar et. al. <ref> [9] </ref>, Chiang et. al [3], Leutenegger and Vernon [8], Sevcik [15, 16], Ghosal et. al. [5], Rosti et. al. [14] and others have proposed using application characteristics such as efficiency, speedup, and average parallelism to improve the performance of static processor schedulers.
Reference: [10] <author> C. McCann, R. Vaswani, and J. Zahorjan. </author> <title> A Dynamic Processor Allocation Policy for Multiprogrammed Shared-Memory Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(2) </volume> <pages> 146-178, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: All of these studies, however, assume the availability of accurate historical performance data, provided to the scheduler simultaneously with job submission. Furthermore, 12 they focus on overall system performance, as opposed to the performance of individual applications. McCann et. al. <ref> [10] </ref> have proposed a dynamic scheduler that uses application provided runtime idleness information to dynamically adjust processor allocations to improve processor utilization.
Reference: [11] <author> G. P. McCormick. </author> <title> Nonlinear Programming. </title> <publisher> John Wiley & Sons, Inc., </publisher> <year> 1983. </year>
Reference-contexts: For such applications, dynamic adjustments of processor allocations can lead to better performance. Our runtime system employs a simple search procedure on application speedup curves as the basis of its actions. The search technique is an adaptation of the method of golden sections <ref> [11] </ref>, which is an efficient technique to find the maximum of a unimodal function 1 within a finite interval. The primary modifications to this technique are to account for the fact that speedup functions, of course, are not necessarily unimodal. <p> Thus, multiple evaluations of S (p) at p's far away from the optimal value can lengthen the very execution that we are trying to minimize. As previously mentioned, we base our current implementation of self-tuning on a simple optimization technique, the method of golden sections (MGS) <ref> [11] </ref>, which searches for the maximum of a function over 5 a 1 a 0 , b 1 d 1 0:618 (d 1 a 1 ), c 1 b 0 , d 1 c 0 ; (c) S (b 1 ) &lt; S (c 1 ) =&gt; a 2 b 1
Reference: [12] <author> P. Moller-Nielsen and J. Staunstrup. Problem-Heap: </author> <title> a Paradigm for Multiprocessor Algorithms. </title> <journal> Parallel Computing, </journal> <volume> 4(1) </volume> <pages> 63-74, </pages> <month> Feb. </month> <year> 1987. </year>
Reference-contexts: Furthermore, we believe that, with some modifications, our algorithm could also be applicable to non-iterative applications that use general runtime work organization paradigms, such as work heap <ref> [1, 12, 19] </ref>. 2 Section 4 defines the basic self-tuning schemes we employ, and Section 5 examines their performance experimentally. Section 6 extends the basic schemes to allow more fine grained scheduling decisions, and examines the implications to performance. Section 7 discusses related work.
Reference: [13] <author> T. D. Nguyen, R. Vaswani, and J. Zahorjan. </author> <title> On Scheduling Implications of Application Characteristics. </title> <type> Technical report, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <note> in preparation. </note>
Reference-contexts: However, measurements of a large variety of parallel benchmarks, including both finely tuned hand-coded programs and compiler-parallelized sequential programs, show that, except for very local variations, speedup is typically unimodal over fairly substantial ranges of processors <ref> [13] </ref>. Therefore, we rely on the golden sections technique to find the optimum allocation over a local subinterval, and on our heuristic extensions to locate the local subinterval in which the global maximum lies. <p> In <ref> [13] </ref>, we found that five of the ten SPLASH applications and all seven of the Perfect Club applications we could compile are iterative. <p> We have shown previously <ref> [13] </ref> that we can accurately predict application efficiency by measuring only system overhead, idleness, and processor stall; parallelization overhead is typically small. Thus, we require only estimates of the latter three components to accurately assess efficiency. <p> On the other hand, long term changes in speedup can lead to noticeable performance degradation. As can be seen in Figure 3, the performance of USAero, an application whose speedup does change with time <ref> [13] </ref>, is indeed enhanced by change-driven self-tuning. Time-driven self-tuning is not useful for the applications we studied. Time-driven self-tuning is meant to address the situation in which a job's speedup changes in the middle of self-tuning but stabilizes before self-tuning completes, possibly trapping change-driven self-tuning into a poor allocation choice.
Reference: [14] <author> E. Rosti, E. Smirni, L. W. Dowdy, G. Serazzi, and B. M. Carlson. </author> <title> Robust Partitioning Policies of Multiprocessor Systems. Performance Evaluation, </title> <address> 19(2-3):141-65, </address> <month> Mar. </month> <year> 1994. </year>
Reference-contexts: Majumdar et. al. [9], Chiang et. al [3], Leutenegger and Vernon [8], Sevcik [15, 16], Ghosal et. al. [5], Rosti et. al. <ref> [14] </ref> and others have proposed using application characteristics such as efficiency, speedup, and average parallelism to improve the performance of static processor schedulers. More recently, Brecht [2] has proposed that application characteristics such as efficiency and execution time can profitably be used by dynamic processor schedulers as well.
Reference: [15] <author> K. C. Sevcik. </author> <title> Characterizations of Parallelism in Applications and their Use in Scheduling. </title> <booktitle> In Proceedings of the ACM SIGMETRICS Conference, </booktitle> <pages> pages 171-180, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: This work differs from ours in that its goal is to determine an appropriate allocation to each of several simultaneously scheduled jobs, typically with the goal of minimizing average response time. Majumdar et. al. [9], Chiang et. al [3], Leutenegger and Vernon [8], Sevcik <ref> [15, 16] </ref>, Ghosal et. al. [5], Rosti et. al. [14] and others have proposed using application characteristics such as efficiency, speedup, and average parallelism to improve the performance of static processor schedulers.
Reference: [16] <author> K. C. Sevcik. </author> <title> Application Scheduling and Processor Allocation in Multiprogrammed Parallel Processing Systems. </title> <type> Technical Report CSRI-282, </type> <institution> Computer Systems Research Institute, University of Toronto, </institution> <month> Mar. </month> <year> 1993. </year>
Reference-contexts: This work differs from ours in that its goal is to determine an appropriate allocation to each of several simultaneously scheduled jobs, typically with the goal of minimizing average response time. Majumdar et. al. [9], Chiang et. al [3], Leutenegger and Vernon [8], Sevcik <ref> [15, 16] </ref>, Ghosal et. al. [5], Rosti et. al. [14] and others have proposed using application characteristics such as efficiency, speedup, and average parallelism to improve the performance of static processor schedulers.
Reference: [17] <author> A. Torn and A. Zilinskas. </author> <title> Global Optimization. </title> <publisher> Springer-Verlag, </publisher> <year> 1987. </year>
Reference-contexts: We found that the first assumption was never violated in our experimental environment. In theory, though, self-tuning could be extended in several ways to deal with erroneous exclusion of the correct interval. Possible extensions include randomly evaluating S (p) outside the current search interval, employing a covering technique <ref> [17] </ref> by evaluating S (p) at regular intervals within [S (P ); P ], etc. We did not implement any of these extensions because our basic algorithm did not encounter any difficulties of this sort in our experimental environment.
Reference: [18] <author> A. Tucker and A. Gupta. </author> <title> Process Control and Scheduling Issues for Multiprogrammed Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 159-166, </pages> <month> Dec. </month> <year> 1989. </year>
Reference-contexts: The Perfect Club applications and USAero were run on their default data sets. To avoid loss of efficiency due to dynamic adjustment of processor allocations, we modified all hand-coded applications to dynamically match the number of threads to the number of processors at the beginning of each iteration <ref> [18] </ref>. For compiler-parallelized applications, we were able to insert this change into the KSR PRESTO runtime system, and so required no changes to the applications nor the compiler. We also rely on the application to call appropriate runtime routines at the beginning and end of each iteration.
Reference: [19] <author> M. Vandevoorde and E. Roberts. WorkCrews: </author> <title> an Abstraction for Controlling Parallelism. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 17(4) </volume> <pages> 347-66, </pages> <month> Aug. </month> <year> 1988. </year> <month> 14 </month>
Reference-contexts: Furthermore, we believe that, with some modifications, our algorithm could also be applicable to non-iterative applications that use general runtime work organization paradigms, such as work heap <ref> [1, 12, 19] </ref>. 2 Section 4 defines the basic self-tuning schemes we employ, and Section 5 examines their performance experimentally. Section 6 extends the basic schemes to allow more fine grained scheduling decisions, and examines the implications to performance. Section 7 discusses related work.
References-found: 19

