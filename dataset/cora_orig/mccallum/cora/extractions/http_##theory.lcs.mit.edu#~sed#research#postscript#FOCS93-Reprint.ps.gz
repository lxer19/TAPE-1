URL: http://theory.lcs.mit.edu/~sed/research/postscript/FOCS93-Reprint.ps.gz
Refering-URL: http://theory.lcs.mit.edu/~sed/research/FOCS93-abs.html
Root-URL: 
Email: net address: jaa@theory.lcs.mit.edu  net address: sed@das.harvard.edu  
Title: General Bounds on Statistical Query Learning and PAC Learning with Noise via Hypothesis Boosting  
Author: Javed A. Aslam Scott E. Decatur 
Address: Cambridge, MA 02139  Cambridge, MA 02138  
Affiliation: Laboratory for Computer Science Massachusetts Institute of Technology  Aiken Computation Laboratory Harvard University  
Date: 1993  1  
Note: Appearing in 34th Symposium on Foundations of Computer Science,  and the inverse of the minimum tolerance by  Author was supported by DARPA Contract N00014-87-K-825 and by NSF Grant CCR-89-14428. Author's  Author was supported by an NDSEG Fellowship and by NSF Grant CCR-92-00884. Author's  
Abstract: We derive general bounds on the complexity of learning in the Statistical Query model and in the PAC model with classification noise. We do so by considering the problem of boosting the accuracy of weak learning algorithms which fall within the Statistical Query model. This new model was introduced by Kearns [12] to provide a general framework for efficient PAC learning in the presence of classification noise. We first show a general scheme for boosting the accuracy of weak SQ learning algorithms, proving that weak SQ learning is equivalent to strong SQ learning. The boosting is efficient and is used to show our main result of the first general upper bounds on the complexity of strong SQ learning. Specifically, we derive simultaneous upper bounds with respect to * on the number of queries, O(log 2 1 * ), the Vapnik-Chervonenkis dimension of the query space, O(log 1 * ), and the inverse of the minimum tolerance, O( 1 * log 1 * ). In addition, we show that these general upper bounds are nearly optimal by describing a class of learning problems for which we simultaneously lower bound the number of queries by (log 1 * ) We further apply our boosting results in the SQ model to learning in the PAC model with classification noise. Since nearly all PAC learning algorithms can be cast in the SQ model, we can apply our boosting techniques to convert these PAC algorithms into highly efficient SQ algorithms. By simulating these efficient SQ algorithms in the PAC model with classification noise, we show that nearly all PAC algorithms can be converted into highly efficient PAC algorithms which tolerate classification noise. We give an upper bound on the sample complexity of these noise-tolerant PAC algorithms which is nearly optimal with respect to the noise rate. We also give upper bounds on space complexity and hypothesis size and show that these two measures are in fact independent of the noise rate. We note that the running times of these noise-tolerant PAC algorithms are efficient. This sequence of simulations also demonstrates that it is possible to boost the accuracy of nearly all PAC algorithms even in the presence of noise. This provides a partial answer to an open problem of Schapire [15] and the first theoretical evidence for an empirical result of Drucker, Schapire and Simard [4]. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Dana Angluin. </author> <title> Computational learning theory: Sur vey and selected bibliography. </title> <booktitle> In Proceedings of the Twenty-Fourth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 351-369, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Two standard complexity measures studied in the PAC model are sample complexity and time complexity. Efficient PAC learning algorithms have been developed for many function classes <ref> [1] </ref>, and PAC learning continues to be a popular model of machine learning. The model of learning stated above is often referred to as strong learning since the learner could be required to output an arbitrarily accurate hypothesis depending on the accuracy parameter.
Reference: [2] <author> Dana Angluin and Philip Laird. </author> <title> Learning from noisy examples. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 343-370, </pages> <year> 1988. </year>
Reference-contexts: In fact, most of the standard PAC learning algorithms would fail if even a small number of the labelled examples given to the learning algorithm were "noisy". A popular noise model for both theoretical and experimental research is the classification noise model introduced by Angluin and Laird <ref> [2, 13] </ref>. In this model, each example received by the learner is mis-labelled randomly and independently with some fixed probability. <p> In this model, each example received by the learner is mis-labelled randomly and independently with some fixed probability. While a limited number of efficient PAC algorithms have been developed which can tolerate classification noise <ref> [2, 9, 14] </ref>, no general framework for efficient learning 1 in the presence of classification noise was known until Kearns introduced the Statistical Query model [12]. In the SQ model, the labelled example oracle of the standard PAC model is replaced by a statistics oracle. <p> Upon gathering a sufficient number of statistics, the SQ algorithm returns an hypothesis of the desired accuracy. Since calls to the statistics oracle can be simulated 1 Angluin and Laird <ref> [2] </ref> introduced a general framework for learning in the presence of classification noise. <p> A popular model of noise for both experimental and theoretical purposes was introduced by Angluin and Laird <ref> [2, 13] </ref>. In the classification noise model, the labelled example oracle EX (f; D) is replaced by a noisy example oracle EX (f; D). Each time the noisy example oracle is called, an example x 2 X is drawn according to D. <p> efficient if its running time is polynomial in the usual PAC learning parameters as well as 1 12 b . 2.3 The Statistical Query Model While a limited number of PAC algorithms have been developed which can tolerate arbitrary amounts of classification noise (up to the information-theoretic limit of 1/2) <ref> [2, 9, 14] </ref>, no general framework for efficient learning in the presence of classification noise was known until Kearns introduced the Statistical Query model [12]. In the SQ model, the example oracle EX (f; D) from the standard PAC model is replaced by a statistics oracle STAT (f; D).
Reference: [3] <author> Scott E. Decatur. </author> <title> Statistical queries and faulty PAC oracles. </title> <booktitle> In Proceedings of the Sixth Annual ACM Workshop on Computational Learning Theory. </booktitle> <publisher> ACM Press, </publisher> <year> 1993. </year>
Reference-contexts: that calls to the statistics oracle STAT (f; D) can also be efficiently simulated (with high probability) by a sample drawn from a noisy example oracle EX (f; D). 3 The size of the sample 3 The statistics oracle can actually be simulated by a variety of "faulty" example oracles <ref> [3] </ref>. 4 required is polynomial in 1=t , 1=(1 2 b ) and log 1=ffi.
Reference: [4] <author> Harris Drucker, Robert Schapire, and Patrice Simard. </author> <title> Improving performance in neural networks using a boosting algorithm. </title> <booktitle> In Advances in Neural Information Processing Systems. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: This provides a partial answer to an open problem of Schapire [15] on whether boosting techniques can be used in the presence of noise. It also provides the first theoretical evidence for an empirical result obtained by Drucker, Schapire and Simard <ref> [4] </ref> on improving the performance of a neural network in the presence of noise. By creating efficient SQ algorithms and simulating them in the PAC model with classification noise, we effectively show that nearly every PAC algorithm can be converted into a highly efficient PAC algorithm which tolerates classification noise.
Reference: [5] <author> Andrzej Ehrenfeucht, David Haussler, Michael Kearns, and Leslie Valiant. </author> <title> A general lower bound on the number of examples needed for learning. </title> <journal> Information and Computation, </journal> <volume> 82(3) </volume> <pages> 247-251, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: One can show that by using real-valued 's, the query complexity of boosting in the SQ model can be somewhat reduced [8]; however, the complexity of the PAC simulation is not significantly improved. Ehrenfeucht, et al. <ref> [5] </ref> have shown that the sample complexity of PAC learning depends at least linearly on 1=*, and clearly this bound holds for learning in the presence of classification noise as well.
Reference: [6] <author> Yoav Freund. </author> <title> Boosting a weak learning algorithm by majority. </title> <booktitle> In Proceedings of the Third Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 202-216. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: A fundamental and surprising result first shown by Schapire [15, 16] and later improved upon by Freund <ref> [6, 7] </ref> states that any algorithm which efficiently weakly learns can be transformed into an algorithm which efficiently strongly learns. These results have important consequences for PAC learning, including providing upper bounds on the time and sample complexities of strong learning. <p> They both create a strong hypothesis by combining many hypotheses obtained from the weak learning algorithm. The boosting mechanisms derive their power by presenting the different calls to the weak learning algorithm with modified versions of the original distribution over labelled examples. Freund <ref> [6, 7] </ref> has developed two similar schemes (which we call Scheme 1 and Scheme 2) for boosting a weak learning algorithm into a strong learning algorithm. One is more efficient with respect to * while the other is more efficient with respect to fl.
Reference: [7] <author> Yoav Freund. </author> <title> An improved boosting algorithm and its implications on learning complexity. </title> <booktitle> In Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, </booktitle> <pages> pages 391-398. </pages> <publisher> ACM Press, </publisher> <year> 1992. </year>
Reference-contexts: A fundamental and surprising result first shown by Schapire [15, 16] and later improved upon by Freund <ref> [6, 7] </ref> states that any algorithm which efficiently weakly learns can be transformed into an algorithm which efficiently strongly learns. These results have important consequences for PAC learning, including providing upper bounds on the time and sample complexities of strong learning. <p> Thus, we show that weak SQ learning is equivalent to strong SQ learning. We use the technique of "boosting by majority" <ref> [7] </ref> which is nearly optimal in terms of its dependence on hypothesis accuracy. In the SQ model, as in the PAC model, the boosting result allows us to derive upper bounds 2 on many complexity measures of learning. <p> They both create a strong hypothesis by combining many hypotheses obtained from the weak learning algorithm. The boosting mechanisms derive their power by presenting the different calls to the weak learning algorithm with modified versions of the original distribution over labelled examples. Freund <ref> [6, 7] </ref> has developed two similar schemes (which we call Scheme 1 and Scheme 2) for boosting a weak learning algorithm into a strong learning algorithm. One is more efficient with respect to * while the other is more efficient with respect to fl.
Reference: [8] <author> Yoav Freund. </author> <type> Personal communication, </type> <year> 1993. </year>
Reference-contexts: Estimating the expectation of real-valued 's in the presence of noise requires new techniques which we describe in the full paper. One can show that by using real-valued 's, the query complexity of boosting in the SQ model can be somewhat reduced <ref> [8] </ref>; however, the complexity of the PAC simulation is not significantly improved. Ehrenfeucht, et al. [5] have shown that the sample complexity of PAC learning depends at least linearly on 1=*, and clearly this bound holds for learning in the presence of classification noise as well.
Reference: [9] <author> Sally A. Goldman, Michael J. Kearns, and Robert E. Schapire. </author> <title> Exact identification of circuits using fixed points of amplification functions. </title> <booktitle> In Proceedings of the 31st Symposium on Foundations of Computer Science, </booktitle> <pages> pages 193-202. </pages> <publisher> IEEE, </publisher> <month> October </month> <year> 1990. </year>
Reference-contexts: In this model, each example received by the learner is mis-labelled randomly and independently with some fixed probability. While a limited number of efficient PAC algorithms have been developed which can tolerate classification noise <ref> [2, 9, 14] </ref>, no general framework for efficient learning 1 in the presence of classification noise was known until Kearns introduced the Statistical Query model [12]. In the SQ model, the labelled example oracle of the standard PAC model is replaced by a statistics oracle. <p> efficient if its running time is polynomial in the usual PAC learning parameters as well as 1 12 b . 2.3 The Statistical Query Model While a limited number of PAC algorithms have been developed which can tolerate arbitrary amounts of classification noise (up to the information-theoretic limit of 1/2) <ref> [2, 9, 14] </ref>, no general framework for efficient learning in the presence of classification noise was known until Kearns introduced the Statistical Query model [12]. In the SQ model, the example oracle EX (f; D) from the standard PAC model is replaced by a statistics oracle STAT (f; D).
Reference: [10] <author> Sally A. Goldman, Michael J. Kearns, and Robert E. Schapire. </author> <title> On the sample complexity of weak learning. </title> <booktitle> In Proceedings of COLT '90, </booktitle> <pages> pages 217-231. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: We finally note that the time complexity of learning F is polyno-mially bounded in all relevant learning parameters. 7 7 Discussion Throughout this paper, we have assumed that the hypothesis classes used by all weak learning algorithms are composed solely of deterministic hypotheses. However, Goldman, Kearns and Schapire <ref> [10] </ref> have shown that in many cases, algorithms which are allowed to output probabilistic hypotheses are more efficient than algorithms which are required to output deterministic hypotheses. By allowing weak learning algorithms to output probabilistic hypotheses, our boosting algorithm may construct probabilistic 's.
Reference: [11] <author> David Helmbold, Robert Sloan, and Manfred K. </author> <title> War muth. Learning integer lattices. </title> <journal> SIAM Journal on Computing, </journal> <volume> 21(2) </volume> <pages> 240-266, </pages> <year> 1992. </year>
Reference-contexts: In fact such a reduction cannot exist since, while the class of parity functions is known to be PAC learnable <ref> [11] </ref>, Kearns has shown that this class is provably unlearnable in the SQ model. Kearns' technique for converting PAC algorithms to SQ algorithms consists of a few general rules, but each PAC algorithm must be examined in turn and converted to an SQ algorithm individually.
Reference: [12] <author> Michael Kearns. </author> <title> Efficient noise-tolerant learning from statistical queries. </title> <booktitle> In Proceedings of the Twenty-Fifth Annual ACM Symposium on Theory of Computing, </booktitle> <year> 1993. </year>
Reference-contexts: While a limited number of efficient PAC algorithms have been developed which can tolerate classification noise [2, 9, 14], no general framework for efficient learning 1 in the presence of classification noise was known until Kearns introduced the Statistical Query model <ref> [12] </ref>. In the SQ model, the labelled example oracle of the standard PAC model is replaced by a statistics oracle. <p> The time and sample complexities of the simulation of an SQ algorithm in the PAC model are directly affected by these measures; therefore we would like to determine these measures as accurately as possible. Kearns <ref> [12] </ref> has demonstrated two important properties of the SQ model which make it worthy of study. <p> A learning algorithm is said to be efficient if its running time is polynomial in 1=*, 1=ffi, n and size (f ). We formally define PAC learning as follows (adapted from Kearns <ref> [12] </ref>): Definition 1 (Strong PAC learning) Let F and H be function classes defined over X. <p> Model While a limited number of PAC algorithms have been developed which can tolerate arbitrary amounts of classification noise (up to the information-theoretic limit of 1/2) [2, 9, 14], no general framework for efficient learning in the presence of classification noise was known until Kearns introduced the Statistical Query model <ref> [12] </ref>. In the SQ model, the example oracle EX (f; D) from the standard PAC model is replaced by a statistics oracle STAT (f; D). <p> The size of the sample required will be polynomial in 1=t and log 1=ffi, and the simulation time will also be dependent on the time required to evaluate . We formally define efficient learning in the Statistical Query model as follows (adapted from Kearns <ref> [12] </ref>): Definition 2 (Strong SQ Learning) Let F and H be function classes defined over X. <p> Both N and t 0 are independent of *. Further analysis of the boosting scheme shows that the dependence on * of the time complexity, space complexity and hypothesis size of strong SQ learning are O (log 2 1 * ) and O (log 1 * ), respectively. Kearns <ref> [12] </ref> has shown a general lower bound of (d= log d) queries each of tolerance O (*) for learning any concept class of VC-dimension d in the SQ model.
Reference: [13] <author> Philip D. Laird. </author> <title> Learning from Good and Bad Data. </title> <booktitle> Kluwer international series in engineering and computer science. </booktitle> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, </address> <year> 1988. </year>
Reference-contexts: In fact, most of the standard PAC learning algorithms would fail if even a small number of the labelled examples given to the learning algorithm were "noisy". A popular noise model for both theoretical and experimental research is the classification noise model introduced by Angluin and Laird <ref> [2, 13] </ref>. In this model, each example received by the learner is mis-labelled randomly and independently with some fixed probability. <p> A popular model of noise for both experimental and theoretical purposes was introduced by Angluin and Laird <ref> [2, 13] </ref>. In the classification noise model, the labelled example oracle EX (f; D) is replaced by a noisy example oracle EX (f; D). Each time the noisy example oracle is called, an example x 2 X is drawn according to D. <p> Ehrenfeucht, et al. [5] have shown that the sample complexity of PAC learning depends at least linearly on 1=*, and clearly this bound holds for learning in the presence of classification noise as well. Laird <ref> [13] </ref> has developed a general technique for learning in the presence of classification noise whose sample complexity depends only linearly on 1=*; however, this technique does not yield computationally efficient algorithms. The upper bound given in Theorem 5 has a roughly quadratic dependence on 1=*.
Reference: [14] <author> Yasubumi Sakakibara. </author> <title> Algorithmic Learning of For mal Languages and Decision Trees. </title> <type> PhD thesis, </type> <institution> Tokyo Institute of Technology, </institution> <month> October </month> <year> 1991. </year> <institution> (International Institute for Advanced Study of Social Information Science, Fujitsu Laboratories Ltd, </institution> <note> Research Report IIAS-RR-91-22E). </note>
Reference-contexts: In this model, each example received by the learner is mis-labelled randomly and independently with some fixed probability. While a limited number of efficient PAC algorithms have been developed which can tolerate classification noise <ref> [2, 9, 14] </ref>, no general framework for efficient learning 1 in the presence of classification noise was known until Kearns introduced the Statistical Query model [12]. In the SQ model, the labelled example oracle of the standard PAC model is replaced by a statistics oracle. <p> efficient if its running time is polynomial in the usual PAC learning parameters as well as 1 12 b . 2.3 The Statistical Query Model While a limited number of PAC algorithms have been developed which can tolerate arbitrary amounts of classification noise (up to the information-theoretic limit of 1/2) <ref> [2, 9, 14] </ref>, no general framework for efficient learning in the presence of classification noise was known until Kearns introduced the Statistical Query model [12]. In the SQ model, the example oracle EX (f; D) from the standard PAC model is replaced by a statistics oracle STAT (f; D).
Reference: [15] <author> Robert E. Schapire. </author> <title> The strength of weak learnability. </title> <journal> Machine Learning, </journal> <volume> 5(2) </volume> <pages> 197-227, </pages> <year> 1990. </year>
Reference-contexts: A variant of strong learning called weak learning is identical except that there is no accuracy parameter, and the output hypothesis need only have error rate slightly less than 1/2 (i.e. slightly better than random guessing). A fundamental and surprising result first shown by Schapire <ref> [15, 16] </ref> and later improved upon by Freund [6, 7] states that any algorithm which efficiently weakly learns can be transformed into an algorithm which efficiently strongly learns. These results have important consequences for PAC learning, including providing upper bounds on the time and sample complexities of strong learning. <p> This provides a partial answer to an open problem of Schapire <ref> [15] </ref> on whether boosting techniques can be used in the presence of noise. It also provides the first theoretical evidence for an empirical result obtained by Drucker, Schapire and Simard [4] on improving the performance of a neural network in the presence of noise.
Reference: [16] <author> Robert E. Schapire. </author> <title> The Design and Analysis of Ef ficient Learning Algorithms. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1992. </year>
Reference-contexts: A variant of strong learning called weak learning is identical except that there is no accuracy parameter, and the output hypothesis need only have error rate slightly less than 1/2 (i.e. slightly better than random guessing). A fundamental and surprising result first shown by Schapire <ref> [15, 16] </ref> and later improved upon by Freund [6, 7] states that any algorithm which efficiently weakly learns can be transformed into an algorithm which efficiently strongly learns. These results have important consequences for PAC learning, including providing upper bounds on the time and sample complexities of strong learning.
Reference: [17] <author> Hans Ulrich Simon. </author> <title> General bounds on the number of examples needed for learning probabilistic concepts. </title> <booktitle> In Proceedings of the Sixth Annual ACM Workshop on Computational Learning Theory. </booktitle> <publisher> ACM Press, </publisher> <year> 1993. </year>
Reference-contexts: Simon <ref> [17] </ref> has shown that the sample complexity of PAC learning with classification noise is ( 1 *(12) 2 ). Therefore our sample complexity bound is roughly optimal with respect to b . Note that the hypothesis size is independent of the noise rate.
Reference: [18] <author> Leslie G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Commu nications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <month> November </month> <year> 1984. </year>
Reference-contexts: We show that it is possible to improve the accuracy of weak learning algorithms in the Statistical Query model to any arbitrary accuracy, and we derive a number of interesting consequences from this result. Since Valiant's introduction of the Probably Approximately Correct model of learning <ref> [18] </ref>, PAC learning has proven to be an interesting and well studied model of machine learning. In an instance of PAC learning, a learner is given the task of determining a close approximation of an unknown f0; 1g-valued target function f from labelled examples of that function.
References-found: 18

