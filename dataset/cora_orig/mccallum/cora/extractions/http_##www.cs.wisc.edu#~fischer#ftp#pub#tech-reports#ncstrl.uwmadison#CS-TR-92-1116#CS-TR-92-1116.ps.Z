URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-92-1116/CS-TR-92-1116.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-92-1116/
Root-URL: http://www.cs.wisc.edu
Title: Realistic Analysis of Parallel Dynamic Programming Algorithms  
Author: Gary Lewandowski Anne Condon Eric Bach 
Note: It is likely that the techniques used here can be used in the analysis of other algorithms that use barriers or pipelining techniques.  
Date: October 1992  
Address: Madison  
Affiliation: Computer Science Department University of Wisconsin at  
Abstract: We examine a very simple asynchronous model of parallel computation that assumes the time to compute a task is random, following some probability distribution. The goal of this model is to capture the effects of unexpected delays on processors. Using techniques from queueing theory and occupancy problems, we use this model to analyze two parallel dynamic programming algorithms. We show that this model is both simple to analyze and realistic in the sense that the analysis corresponds to experimental results on a shared memory parallel machine. The algorithms we consider are a pipeline algorithm, where each processor i computes in order the entries of rows i, i + p and so on, where p is the number of processors; and a diagonal algorithm, where entries along each diagonal extending from the left to the top of the table are computed in turn. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Almquist, K., R. J. Anderson and E. D. Lazowska. </author> <title> The measured performance of parallel dynamic programming implementations, </title> <booktitle> Proceedings of the International Conference on Parallel Processing, III, </booktitle> <pages> pages 76-79, </pages> <year> 1989. </year>
Reference-contexts: j m. (The entries in the first row and column have only 1 predecessor.) Dynamic programming algorithms are good to study because dynamic programming is a very common algorithmic technique (it is used, for example, to solve the Knapsack problem [9], the Longest Common Substring problem [23], queueing network models <ref> [1] </ref>, and DNA sequence alignment [17]), and there are many possible parallel algorithms to implement it. The two dynamic programming algorithms we will examine are the pipeline and diagonal algorithms. <p> A processor can compute an entry as soon as its predecessors are computed. (We assume that processors can test whether the predecessors of an entry are already computed, using locks, for example.) Almquist et al. <ref> [1] </ref> used this algorithm in solving the longest common substring problem and a problem on queueing network models. In the diagonal algorithm, entries along each diagonal extending from the left side to the top of the table, are computed in turn.
Reference: [2] <author> Anderson, R. J., P. Beame and W. L. Ruzzo. </author> <title> Low overhead parallel schedules for task graphs, </title> <booktitle> Proceedings of the 2nd Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 11-21, </pages> <year> 1990. </year>
Reference-contexts: One model, used by Anderson et al. <ref> [2] </ref> in analyzing dynamic programming algorithms on small, asynchronous parallel machines, allows an adversary to control the delays of the processors. Such a model may be useful for predicting the worst case performance of an algorithm.
Reference: [3] <author> Arjas, E. and T. Lehtonen. </author> <title> Approximating many server queues by single server queues, </title> <journal> Mathematics of Operations Research 3, </journal> <pages> pages 205-233, </pages> <year> 1978. </year>
Reference-contexts: See Arjas and Lehtonen <ref> [3] </ref> for a simple proof of Lemma 3.3. We extend these to prove a third useful lemma.
Reference: [4] <author> Cole, R. and O. Zajicek. </author> <title> The expected advantage of asynchrony, </title> <booktitle> Proceedings of the 2nd Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 85-94, </pages> <year> 1990. </year> <month> 25 </month>
Reference-contexts: Such a model may be useful for predicting the worst case performance of an algorithm. Another approach is to make the running time of a task in the computation a random variable. Nishimura [19], Cole and Zajicek <ref> [4] </ref> and Martel et al. [16] described general models of asynchronous parallel computation with such random delays. This approach appears to be a promising one, when one wants to estimate the performance of an algorithm on an average run, rather than in the worst case.
Reference: [5] <author> Fortune, S., J. Wylie. </author> <title> Parallelism in random access machines, </title> <booktitle> Proceedings of the 10th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 114-118, </pages> <year> 1978. </year>
Reference-contexts: The two dynamic programming algorithms we will examine are the pipeline and diagonal algorithms. While experiments show that their performance is generally much different on the same input data, synchronous models of parallel computation, such as the PRAM <ref> [5] </ref> [8] [21], give the same time complexity for both algorithms. A useful asynchronous model must capture the difference. In the pipeline algorithm, when the number of processors is p, the ith processor computes the entries in rows i; i+p; : : :, in order.
Reference: [6] <author> Fromm, H., U. Hercksen, U. Herzog, K. John, R. Klar and W. Kleinoder. </author> <title> Experiences with performance measurement and modeling of a processor array, </title> <journal> IEEE Transactions on Computers, </journal> <volume> 32(1), </volume> <pages> pages 15-31, </pages> <year> 1983. </year>
Reference-contexts: There is precedent for the use of the exponential distribution in predicting the performance of parallel programs. As early as 1983, Fromm et al. <ref> [6] </ref> used a stochastic model to analyze the performance of a parallel system, the Erlangen General Processor Array. The time needed to execute an instruction in this system depends on delays due to memory conflicts. Thus, the time for an instruction is modeled as a random variable.
Reference: [7] <author> Glynn, P. W. and Whitt, W. </author> <title> Departures from many queues in series, </title> <type> Technical Report Number 60, </type> <institution> Department of Operations Research, Stanford University. </institution>
Reference-contexts: In future work, we would like to extend the analysis for the pipeline case for some non-exponential distributions, as in the diagonal algorithm. A theorem by Glynn and Whitt <ref> [7] </ref> can be used to get a very weak upper bound of (mdn=pe)= + O (nm 1a=2 ), when p = fi (m a ), for 0 &lt; a 1.
Reference: [8] <author> Goldschlager, L.M. </author> <title> A unified approach to models of synchronous parallel machines, </title> <booktitle> Proceedings of the 10th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 89-94, </pages> <year> 1978. </year>
Reference-contexts: The two dynamic programming algorithms we will examine are the pipeline and diagonal algorithms. While experiments show that their performance is generally much different on the same input data, synchronous models of parallel computation, such as the PRAM [5] <ref> [8] </ref> [21], give the same time complexity for both algorithms. A useful asynchronous model must capture the difference. In the pipeline algorithm, when the number of processors is p, the ith processor computes the entries in rows i; i+p; : : :, in order.
Reference: [9] <author> Horowitz, E. and S. Sahni. </author> <title> Computing partitions with applications to the knapsack problem, </title> <journal> Journal ACM 21, </journal> <pages> pages 277-292, </pages> <year> 1974. </year>
Reference-contexts: j 1) and (i; j 1); 1 i n; 1 j m. (The entries in the first row and column have only 1 predecessor.) Dynamic programming algorithms are good to study because dynamic programming is a very common algorithmic technique (it is used, for example, to solve the Knapsack problem <ref> [9] </ref>, the Longest Common Substring problem [23], queueing network models [1], and DNA sequence alignment [17]), and there are many possible parallel algorithms to implement it. The two dynamic programming algorithms we will examine are the pipeline and diagonal algorithms.
Reference: [10] <author> Kruskal, C.P. and A. Weiss. </author> <title> Allocating independent subtasks on parallel processors, </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> Vol SE-11, No. 10, </volume> <pages> pages 1001-1016, </pages> <year> 1985. </year>
Reference-contexts: Our work on the diagonal algorithm extends their results on series-parallel graphs, while our work on the pipeline algorithm provides tools for predicting the performance of task graphs with a mesh structure, where dependence between the tasks is much more complex than in a series-parallel graph. Kruskal and Weiss <ref> [10] </ref> analyzed the expected running time of p processors working on a pool of n subtasks. Each subtask could be done independently. <p> When p is between these two extremes, we extend a result of Kruskal and Weiss <ref> [10] </ref> on the expected running time of p processors on k subtasks to obtain an upper bound. The following summarizes the resulting asymptotic estimates on the expected running time of the diagonal algorithm. <p> Normal restricted to positive values), Uniform and constant distributions. This assumption is sufficient to extend the analysis in the cases where the number of processors used is more than a constant but less than the number of rows in the table. This analysis uses results from Kruskal and Weiss <ref> [10] </ref>. We begin by presenting these results. Let Y 1 ; : : : ; Y p be random variables with the common distribution function G (x) = P (Y x). Definition 4.1 The characteristic maximum m p def 17 Kruskal and Weiss [10] proved the following theorem based on results <p> analysis uses results from Kruskal and Weiss <ref> [10] </ref>. We begin by presenting these results. Let Y 1 ; : : : ; Y p be random variables with the common distribution function G (x) = P (Y x). Definition 4.1 The characteristic maximum m p def 17 Kruskal and Weiss [10] proved the following theorem based on results by Lai and Robbins [11].
Reference: [11] <author> Lai, T.L. and H. Robbins, </author> <title> Maximally dependent random variables, </title> <booktitle> Proceedings of the National Academy Sciences, </booktitle> <volume> vol 73, no. 2, </volume> <pages> pages 286-288, </pages> <year> 1976. </year>
Reference-contexts: Let Y 1 ; : : : ; Y p be random variables with the common distribution function G (x) = P (Y x). Definition 4.1 The characteristic maximum m p def 17 Kruskal and Weiss [10] proved the following theorem based on results by Lai and Robbins <ref> [11] </ref>.
Reference: [12] <author> Lander, E., J. P. Mesirov and W. Taylor IV. </author> <title> Study of protein sequence comparison metrics on the Connection Machine CM-2, </title> <journal> The Journal of Supercomputing, </journal> <volume> 3, </volume> <pages> pages 255-269, </pages> <year> 1989. </year>
Reference-contexts: Within each diagonal, each processor computes approximately 1=p of the entries. The computation of the entries along a diagonal is not started until all entries along the previous diagonal are computed. This can be accomplished using barriers, for example. Lander et al. <ref> [12] </ref> proposed this parallel algorithm for protein sequence alignment. The difference between the algorithms is that in the diagonal algorithm, all processors are forced to wait at a barrier, whereas in the pipeline algorithm, a processor can compute an entry once 2 its predecessors are computed.
Reference: [13] <author> Lavenberg, S. S. and M. Reiser. </author> <title> Stationary state probabilities at arrival instants for closed queueing networks with multiple types of customers, </title> <journal> Journal of Applied Probability, </journal> <pages> pages 1048-1061, </pages> <year> 1980. </year>
Reference-contexts: We assume that the system is in steady-state and that service times are all exponentially distributed with mean 1=. The following lemma, which follows from a result of Lavenberg and Reiser <ref> [13] </ref>, gives the expected time for a customer to be served s times.
Reference: [14] <author> Mak, V. W. and S. F. Lundstrom. </author> <title> Predicting performance of parallel computations, </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(3), </volume> <pages> pages 257-270, </pages> <year> 1990. </year>
Reference-contexts: Different distributions were considered, including constant, exponential and "phase-type" distributions, and the results of the analysis were compared with experimental results. The authors concluded that the analysis using the exponential distribution compared favorably with experimental results. Another more recent example is the work of Mak and Lundstrom <ref> [14] </ref>, who describe analytic models for predicting the performance of a parallel program represented as a task graph with series-parallel structure, where the time to execute a task is exponentially distributed.
Reference: [15] <author> Marshall, A.W. and I. Olkin, </author> <title> Inequalities: Theory of majorization and its applications, </title> <address> New York: </address> <publisher> Academic Press, </publisher> <year> 1979. </year>
Reference-contexts: u v, where u, v are real vectors in R i1 , Then, (X 1 ; : : :; X n ) (Y 1 ; : : : ; Y n ). 8 Lemma 3.2 is a special case of Lemma 3.3; both can be found in Marshall and Olkin <ref> [15] </ref>. See Arjas and Lehtonen [3] for a simple proof of Lemma 3.3. We extend these to prove a third useful lemma.
Reference: [16] <author> Martel, C., R. Subramonian and A. Park. </author> <title> Asynchronous PRAMS are (almost) as good as synchronous PRAMS, </title> <booktitle> Proceedings of the 31st Annual Symposium on the Foundations of Computer Science, </booktitle> <pages> pages 590-599, </pages> <year> 1990. </year>
Reference-contexts: Such a model may be useful for predicting the worst case performance of an algorithm. Another approach is to make the running time of a task in the computation a random variable. Nishimura [19], Cole and Zajicek [4] and Martel et al. <ref> [16] </ref> described general models of asynchronous parallel computation with such random delays. This approach appears to be a promising one, when one wants to estimate the performance of an algorithm on an average run, rather than in the worst case. Our simple model follows this approach.
Reference: [17] <author> Needleman, S. B., </author> <title> C.D. Wunsch. A general method applicable to the search for similarity in the amino acid sequence of two proteins, </title> <journal> Journal of Molecular Biology 48, </journal> <pages> pages 443-454, </pages> <year> 1970. </year>
Reference-contexts: the first row and column have only 1 predecessor.) Dynamic programming algorithms are good to study because dynamic programming is a very common algorithmic technique (it is used, for example, to solve the Knapsack problem [9], the Longest Common Substring problem [23], queueing network models [1], and DNA sequence alignment <ref> [17] </ref>), and there are many possible parallel algorithms to implement it. The two dynamic programming algorithms we will examine are the pipeline and diagonal algorithms. <p> The diagonal algorithm was implemented using barriers, the pipeline using locks. The application implemented as an example of the algorithms is the Needleman-Wunsch algorithm for aligning two DNA sequences <ref> [17] </ref>. In aligning the two sequences, gaps may be inserted in one or the other of the sequences in order to make the overall alignment better. The best alignment is determined by scoring all possible alignments using a dynamic programming algorithm.
Reference: [18] <author> Newman, D. J. and L. Shepp. </author> <title> The double dixie cup problem. </title> <journal> The American Mathematical Monthly, </journal> <volume> 67, </volume> <pages> pages 58-61, </pages> <year> 1960. </year>
Reference-contexts: If k = ffp and p ! 1 then E [Occ (p; k)] pk (1 + O ( p Proof: 1 and 2 follow directly from results of Newman and Shepp <ref> [18] </ref>. We give a brief outline of the proof of 3. Suppose k = ffp. We first estimate the number, N , of balls needed to ensure that with probability at least 1 1=p, all bins have at least k balls.
Reference: [19] <author> Nishimura, N. </author> <title> Asynchronous shared memory parallel computation. </title> <booktitle> Proceedings of the 2nd Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 76-84, </pages> <year> 1990. </year>
Reference-contexts: Such a model may be useful for predicting the worst case performance of an algorithm. Another approach is to make the running time of a task in the computation a random variable. Nishimura <ref> [19] </ref>, Cole and Zajicek [4] and Martel et al. [16] described general models of asynchronous parallel computation with such random delays. This approach appears to be a promising one, when one wants to estimate the performance of an algorithm on an average run, rather than in the worst case.
Reference: [20] <author> Purdom, P. W. and C. A. Brown. </author> <title> The Analysis of Algorithms, </title> <publisher> Holt, Rhinehart and Winston, </publisher> <year> 1985. </year>
Reference: [21] <author> Savitch, W.J., M. Stimson. </author> <title> Time bounded random access machines with parallel processing, </title> <journal> Journal ACM, </journal> <volume> 26, </volume> <pages> pages 103-118, </pages> <year> 1979. </year>
Reference-contexts: The two dynamic programming algorithms we will examine are the pipeline and diagonal algorithms. While experiments show that their performance is generally much different on the same input data, synchronous models of parallel computation, such as the PRAM [5] [8] <ref> [21] </ref>, give the same time complexity for both algorithms. A useful asynchronous model must capture the difference. In the pipeline algorithm, when the number of processors is p, the ith processor computes the entries in rows i; i+p; : : :, in order.
Reference: [22] <author> Solomon, F. </author> <title> Residual lifetimes in random parallel systems, </title> <journal> Mathematics Magazine, </journal> <volume> 63(1), </volume> <pages> pages 37-48, </pages> <year> 1990. </year>
Reference-contexts: This is known to be (1=)H j , where H j is the jth harmonic number (Solomon <ref> [22] </ref>). 4.1 Lower Bound We now obtain a lower bound on the expected time of the diagonal algorithm. We first obtain a lower bound on T (p; j).
Reference: [23] <author> Wagner, R.A., M.J. Fischer, </author> <title> The string-to-string correction problem, </title> <journal> Journal ACM 21, </journal> <pages> pages 168--173, </pages> <year> 1974. </year>
Reference-contexts: 1 i n; 1 j m. (The entries in the first row and column have only 1 predecessor.) Dynamic programming algorithms are good to study because dynamic programming is a very common algorithmic technique (it is used, for example, to solve the Knapsack problem [9], the Longest Common Substring problem <ref> [23] </ref>, queueing network models [1], and DNA sequence alignment [17]), and there are many possible parallel algorithms to implement it. The two dynamic programming algorithms we will examine are the pipeline and diagonal algorithms.

References-found: 23

