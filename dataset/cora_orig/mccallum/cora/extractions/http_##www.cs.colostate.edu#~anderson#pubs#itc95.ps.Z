URL: http://www.cs.colostate.edu/~anderson/pubs/itc95.ps.Z
Refering-URL: http://www.cs.colostate.edu/~anderson/pubs/pubs.html
Root-URL: 
Email: anderson@cs.colostate.edu avm@cs.colostate.edu mraz@kirk.usafa.af.mil  
Title: On the Use of Neural Networks to Guide Software Testing Activities  
Author: Charles Anderson Anneliese von Mayrhauser Rick Mraz 
Address: Fort Collins, CO 80523 Fort Collins, CO 80523 Colorado Springs, CO  
Affiliation: Colorado State University Colorado State University US Air Force Academy Computer Science Department Computer Science Department Computer Science Department  
Abstract: As test case automation increases, the volume of tests can become a problem. Further, it may not be immediately obvious whether the test generation tool generates effective test cases. Indeed, it might be useful to have a mechanism that is able to learn, based on past history, which test cases are likely to yield more failures versus those that are not likely to uncover any. We present experimental results on using a neural network for pruning a testcase set while preserving its effectiveness. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Charles W. Anderson, Judy A. Franklin, and Richard S. Sutton. </author> <title> Learning a Nonlinear Model of a Manufacturing Process Using Multilayer Connectionist Networks, </title> <booktitle> Proc. of the 5th IEEE International Symposium on Intelligent Control, </booktitle> <address> Philadelphia, PA, </address> <month> Septem-ber </month> <year> 1990, </year> <pages> pp. 404-409. </pages>
Reference-contexts: Then, we trained the network to recognize relationships between test case descriptors and faults. Once trained, the network acts as a fault predictor for new test cases. Related work includes the use of neural networks to predict quality in a manufacturing process <ref> [1] </ref>. Section 2 provides background on neural network classifiers and their use to predict failures. Section 3 introduces the experimental design to evaluate how well neural net works work when used as test effectiveness predictors.
Reference: [2] <author> B. </author> <title> Beizer; Software Testing Techniques, </title> <publisher> Van Nostrand Reinhold, </publisher> <address> New York, NY, </address> <year> 1990. </year>
Reference-contexts: 1 INTRODUCTION Software testing still consumes major resources in a software product's lifecycle. A myriad of testing methods exist ranging from white box testing methods for unit and integration testing to grey and black box methods for system testing <ref> [17, 2] </ref>. Methods are usually associated with testing criteria, that identify what to test and how we know that we are done. Examples include branch testing, dataflow testing, etc. Coverage metrics measure compliance with testing criteria and identify which parts of the code have not been adequately tested.
Reference: [3] <author> DeMillo, R., A.; Offutt, A., J.; </author> <title> Constraint-Based Au--tomatic Test Data Generation, </title> <journal> IEEE Transactions on Software Engineering SE-17, </journal> <volume> 9(Sept. </volume> <year> 1991), </year> <pages> pp. 900-910. </pages>
Reference-contexts: This should come as no surprise, since the general test data generation problem is undecidable [8]. In practice, many test data generation systems make simplifying assumptions, either as to the power of the language for which they generate test data <ref> [3, 7, 26] </ref>, or as to which information they consider in driving test data generation (e.g., Robert Poston's T tool). <p> Thus experimental evaluation of testing techniques is also important. Examples of such evaluations include [24, 25, 6]. When the number of tests to be run can become very large, test case reduction becomes desirable. Examples of reducing the number of test cases are <ref> [3, 22] </ref>. Prudent test case reduction prunes a test set by eliminating those test cases that are not likely to yield any failures. Figure 1 summarizes our discussion. This paper concentrates on experimental effectiveness analysis and prediction.
Reference: [4] <author> Laurene Fausett. </author> <title> Fundamentals of Neural Networks. </title> <publisher> Prentice Hall: </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1994. </year>
Reference-contexts: Processing elements are interconnected by a network of weighted connections that encode network knowledge. Neural networks are highly parallel and exercise distributed control. They emphasize automatic learning. Neural networks have been used as memories, pattern recall devices, pattern classifiers, and general function mapping engines <ref> [4, 9, 27] </ref>. Test effectiveness evaluation concentrates on their use as pattern classifiers. A classifier maps input vectors to output vectors in two phases. The network learns the input-output classification from a set of training vectors. <p> A net with too few hidden units cannot learn the mapping to the required accuracy. Too many hidden units allow the net to memorize the training data and does not generalize well to new data. Backpropagation is the most popular training algorithm for multilayer neural networks <ref> [4, 9, 27] </ref>. The algorithm initializes the network with a random set of weights, and the network trains from a set of input-output pairs. Each pair requires a two-stage learning algorithm: forward pass and backward pass. <p> First, the error passes from the output layer to the hidden layer updating output weights. Next, each hidden unit calculates an error based on the error from each output unit. The error from the hidden units updates input weights. Details about the Backpropagation algorithm can be found in <ref> [4, 9, 27] </ref>. One training epoch passes when the network sees all input-output pairs in the training set. Training stops when the sum squared error is acceptable or when a predefined number of epochs passes.
Reference: [5] <author> R. Hamlet, R. </author> <title> Taylor; Partition Testing does not inspire confidence, </title> <journal> IEEE Transactions on Software Engineering vol. </journal> <volume> 16, no. 12(Dec. </volume> <year> 1990), </year> <pages> pp. 1402-1411. </pages>
Reference: [6] <author> P. Frankl, S. </author> <title> Weiss; An experimental comparison of the effectiveness of branch testing and data flow testing, </title> <journal> Transactions on Software Engineering vol. </journal> <volume> 19, no. </volume> <month> 8(August </month> <year> 1993), </year> <pages> pp. 774-787. </pages>
Reference-contexts: Analytical evaluation considers whether the testing criteria meet adequacy axioms [12, 23]. They describe properties any testing criteria should have to be considered adequate. They do not, however, guarantee high failure yields. Thus experimental evaluation of testing techniques is also important. Examples of such evaluations include <ref> [24, 25, 6] </ref>. When the number of tests to be run can become very large, test case reduction becomes desirable. Examples of reducing the number of test cases are [3, 22].
Reference: [7] <author> Higashino, T., v. Bochman, G.; </author> <title> Automatic Analysis and Test Case Derivation for a Restricted Class of LOTOS Expressions with Data Parameters, </title> <journal> IEEE Transactions on Software Engineering SE-20, </journal> <month> 1(Jan-uary </month> <year> 1994), </year> <pages> pp. 29-42. </pages>
Reference-contexts: This should come as no surprise, since the general test data generation problem is undecidable [8]. In practice, many test data generation systems make simplifying assumptions, either as to the power of the language for which they generate test data <ref> [3, 7, 26] </ref>, or as to which information they consider in driving test data generation (e.g., Robert Poston's T tool).
Reference: [8] <author> W. </author> <title> Howden; Functional Testing and Analysis, </title> <publisher> McGraw-Hill, </publisher> <address> New York, NY, </address> <year> 1987. </year>
Reference-contexts: Examples are test generation tools based on symbolic execution that ensure various types of white box testing coverage [16]. This should come as no surprise, since the general test data generation problem is undecidable <ref> [8] </ref>. In practice, many test data generation systems make simplifying assumptions, either as to the power of the language for which they generate test data [3, 7, 26], or as to which information they consider in driving test data generation (e.g., Robert Poston's T tool).
Reference: [9] <editor> J.L. McClelland, D.E. Rumelhart, </editor> <booktitle> and the PDP Research Group. Parallel Distributed Processing: Exploration in the Microstructure of Cognition, </booktitle> <volume> vol 1. </volume> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: Section 6 summarizes our conclusions. 2 BACKGROUND ON NEURAL NETWORKS 2.1 CLASSIFIERS Neural network methods were developed to model the neural architecture and computation of the human brain <ref> [9] </ref>. A neural network consists of simple neuron-like processing elements. Processing elements are interconnected by a network of weighted connections that encode network knowledge. Neural networks are highly parallel and exercise distributed control. They emphasize automatic learning. <p> Processing elements are interconnected by a network of weighted connections that encode network knowledge. Neural networks are highly parallel and exercise distributed control. They emphasize automatic learning. Neural networks have been used as memories, pattern recall devices, pattern classifiers, and general function mapping engines <ref> [4, 9, 27] </ref>. Test effectiveness evaluation concentrates on their use as pattern classifiers. A classifier maps input vectors to output vectors in two phases. The network learns the input-output classification from a set of training vectors. <p> A net with too few hidden units cannot learn the mapping to the required accuracy. Too many hidden units allow the net to memorize the training data and does not generalize well to new data. Backpropagation is the most popular training algorithm for multilayer neural networks <ref> [4, 9, 27] </ref>. The algorithm initializes the network with a random set of weights, and the network trains from a set of input-output pairs. Each pair requires a two-stage learning algorithm: forward pass and backward pass. <p> First, the error passes from the output layer to the hidden layer updating output weights. Next, each hidden unit calculates an error based on the error from each output unit. The error from the hidden units updates input weights. Details about the Backpropagation algorithm can be found in <ref> [4, 9, 27] </ref>. One training epoch passes when the network sees all input-output pairs in the training set. Training stops when the sum squared error is acceptable or when a predefined number of epochs passes.
Reference: [10] <author> T. </author> <title> Ostrand, M. </title> <journal> Balcer; Category-partition testing Communications of the ACM vol. </journal> <volume> 31, no. </volume> <month> 6(June </month> <year> 1988), </year> <pages> pp. 676-687. </pages>
Reference: [11] <author> Robert G.D. Steel and James H. Torrie. </author> <title> Principles and Procedures of Statistics: A Biometrical Approach, second edition. </title> <publisher> McGraw-Hill: </publisher> <address> New York, </address> <year> 1980. </year>
Reference-contexts: We used LOOM to experimentally calculate the best network topology (number of hidden units). Table 5 lists the best topology 1 Scaling reduces the side effects of scale differences between parameters. Linear, square root, logarithm, and general data transformation are typical scaling methods <ref> [11] </ref>.
Reference: [12] <author> A. Parrish, S. </author> <title> Zweben; Analysis and Refinement of Software Test Data Adequacy Properties, </title> <journal> IEEE Transactions on Software Engineering vol. </journal> <volume> 17, no. </volume> <month> 6(June </month> <year> 1991), </year> <pages> pp. 565-581. </pages>
Reference-contexts: Simplification is obviously not a problem when test data are effective (reveal faults). This makes it very important to evaluate a test generation method experimentally and/or analytically. Analytical evaluation considers whether the testing criteria meet adequacy axioms <ref> [12, 23] </ref>. They describe properties any testing criteria should have to be considered adequate. They do not, however, guarantee high failure yields. Thus experimental evaluation of testing techniques is also important. Examples of such evaluations include [24, 25, 6].
Reference: [13] <author> Rapps, S.; Weyuker, E., J.; </author> <title> Data Flow Analysis Techniques for Test Data Selection Procs. </title> <booktitle> Sixth Int. Conf. Software Engineering, IEEE Computer Society, </booktitle> <address> Tokyo, Japan, </address> <month> Sept. </month> <year> 1982, </year> <pages> pp. 272-277. </pages>
Reference-contexts: An example of a coverage analyzer is the Free Software Foundation's gct tool. Often, testing methods are associated with their own test case or test suite generation tools (e. g. ASSET [14] and dataflow testing <ref> [13] </ref>, SLEUTH [19] and domain based testing [21]). Test generation tools may aim for forcing compliance or only approximate it (then coverage measurement tools evaluate how well test data meets the testing criterion).
Reference: [14] <author> Rapps, S.; Weyuker, E., J.; </author> <title> Selecting Software Test Data using Data Flow Information, </title> <journal> IEEE Transactions on Software Engineering SE-11, </journal> <month> 4(April </month> <year> 1985), </year> <pages> pp. 367-375. </pages>
Reference-contexts: An example of a coverage analyzer is the Free Software Foundation's gct tool. Often, testing methods are associated with their own test case or test suite generation tools (e. g. ASSET <ref> [14] </ref> and dataflow testing [13], SLEUTH [19] and domain based testing [21]). Test generation tools may aim for forcing compliance or only approximate it (then coverage measurement tools evaluate how well test data meets the testing criterion).
Reference: [15] <author> StorageTek, </author> <title> StorageTek 4400 Operator's Guide, Host Software Component (VM) Rel 1.2.0, </title> <address> StorageTek, </address> <year> 1992. </year>
Reference: [16] <author> Gmeiner, L.; Voges, U.; von Mayrhauser, A.; </author> <title> SADAT-An Automated Testing Tool, </title> <journal> IEEE Transactions on Software Engineering SE-6, </journal> <month> 3(May </month> <year> 1980), </year> <pages> pp. 286-290. </pages>
Reference-contexts: Examples are test generation tools based on symbolic execution that ensure various types of white box testing coverage <ref> [16] </ref>. This should come as no surprise, since the general test data generation problem is undecidable [8].
Reference: [17] <author> A. </author> <title> von Mayrhauser; Software Engineering: Methods and Management, </title> <publisher> Academic Press, </publisher> <address> Boston, MA, </address> <year> 1990. </year>
Reference-contexts: 1 INTRODUCTION Software testing still consumes major resources in a software product's lifecycle. A myriad of testing methods exist ranging from white box testing methods for unit and integration testing to grey and black box methods for system testing <ref> [17, 2] </ref>. Methods are usually associated with testing criteria, that identify what to test and how we know that we are done. Examples include branch testing, dataflow testing, etc. Coverage metrics measure compliance with testing criteria and identify which parts of the code have not been adequately tested.
Reference: [18] <author> A. von Mayrhauser, S. </author> <title> Crawford-Hines; Automated Testing Support for a Robot Tape Library, Procs. </title> <booktitle> IEEE Computer Society International Symp. on Software Reliability Engineering, </booktitle> <address> Denver, </address> <month> Nov. </month> <year> 1993, </year> <pages> pp. 6-14. </pages>
Reference: [19] <author> Anneliese von Mayrhauser, Jeff Walls, and Richard Mraz, </author> <title> Testing Applications Using Domain Based Testing and Sleuth, </title> <booktitle> Proceedings of the Fifth International Software Reliability Engineering Symposium, </booktitle> <address> Monterey, </address> <month> November </month> <year> 1994, </year> <pages> pp. 206-215. </pages>
Reference-contexts: An example of a coverage analyzer is the Free Software Foundation's gct tool. Often, testing methods are associated with their own test case or test suite generation tools (e. g. ASSET [14] and dataflow testing [13], SLEUTH <ref> [19] </ref> and domain based testing [21]). Test generation tools may aim for forcing compliance or only approximate it (then coverage measurement tools evaluate how well test data meets the testing criterion). <p> Section 2 provides background on neural network classifiers and their use to predict failures. Section 3 introduces the experimental design to evaluate how well neural net works work when used as test effectiveness predictors. As example testing technique we use Domain Based Testing <ref> [19] </ref> and its associated test data generation tool SLEUTH [21]. Section 4 describes the results of the experiment. The results are useful in two ways. First, they point out which of the generated test suites are likely to trigger what types of incidents. <p> Comparing the two, we measure how well the neural net acts as a test case effectiveness predictor. Phase 3 EXPERIMENT DESIGN We conducted an empirical study to show the effectiveness of a neural net fault predictor. The study used the DBT test generation tool Sleuth to generate test data <ref> [21, 19] </ref>. Using test case metrics, a synthetic test oracle evaluated each test case for error classification. The neural net trained on test metric input patterns and mapped them to the test oracle's error classification.
Reference: [20] <author> Anneliese von Mayrhauser, Richard Mraz, Jeff Walls, and Pete Ocken. </author> <title> Domain Based Testing: Increasing Test Case Reuse, </title> <booktitle> Proc. of the International Conference on Computer Design, </booktitle> <address> Boston, </address> <month> October </month> <year> 1994, </year> <pages> pp. 484-491. </pages>
Reference: [21] <author> Anneliese von Mayrhauser, Jeff Walls, and Richard Mraz. Sleuth: </author> <title> A Domain Based Testing Tool, </title> <booktitle> Proc. of the International Test Conference, </booktitle> <month> October, </month> <year> 1994. </year> <note> [22] von Mayrhauser, </note> <author> A.; Anderson, Ch.; Mraz, R.; </author> <title> Using A Neural Network to Predict Test Case Effectiveness, Procs. </title> <booktitle> IEEE Aerospace Applications Conference, </booktitle> <address> Snowmass, CO, </address> <month> Feb. </month> <year> 1995 </year>
Reference-contexts: An example of a coverage analyzer is the Free Software Foundation's gct tool. Often, testing methods are associated with their own test case or test suite generation tools (e. g. ASSET [14] and dataflow testing [13], SLEUTH [19] and domain based testing <ref> [21] </ref>). Test generation tools may aim for forcing compliance or only approximate it (then coverage measurement tools evaluate how well test data meets the testing criterion). <p> Section 3 introduces the experimental design to evaluate how well neural net works work when used as test effectiveness predictors. As example testing technique we use Domain Based Testing [19] and its associated test data generation tool SLEUTH <ref> [21] </ref>. Section 4 describes the results of the experiment. The results are useful in two ways. First, they point out which of the generated test suites are likely to trigger what types of incidents. <p> Comparing the two, we measure how well the neural net acts as a test case effectiveness predictor. Phase 3 EXPERIMENT DESIGN We conducted an empirical study to show the effectiveness of a neural net fault predictor. The study used the DBT test generation tool Sleuth to generate test data <ref> [21, 19] </ref>. Using test case metrics, a synthetic test oracle evaluated each test case for error classification. The neural net trained on test metric input patterns and mapped them to the test oracle's error classification.
Reference: [23] <author> E. </author> <title> Weyuker; Axiomatizing Software Test Data Adequacy, </title> <journal> IEEE Transactions on Software Engineering vol. </journal> <volume> 12, no. 12(Dec. </volume> <year> 1986), </year> <pages> pp. 1128-1138. </pages>
Reference-contexts: Simplification is obviously not a problem when test data are effective (reveal faults). This makes it very important to evaluate a test generation method experimentally and/or analytically. Analytical evaluation considers whether the testing criteria meet adequacy axioms <ref> [12, 23] </ref>. They describe properties any testing criteria should have to be considered adequate. They do not, however, guarantee high failure yields. Thus experimental evaluation of testing techniques is also important. Examples of such evaluations include [24, 25, 6].
Reference: [24] <author> E. </author> <title> Weyuker; The Cost of Data Flow Testing: An Empirical Study, </title> <journal> IEEE Transactions on Software Engineering vol. </journal> <volume> 16, no. 2(Feb. </volume> <year> 1990), </year> <pages> pp. 121-128. </pages>
Reference-contexts: Analytical evaluation considers whether the testing criteria meet adequacy axioms [12, 23]. They describe properties any testing criteria should have to be considered adequate. They do not, however, guarantee high failure yields. Thus experimental evaluation of testing techniques is also important. Examples of such evaluations include <ref> [24, 25, 6] </ref>. When the number of tests to be run can become very large, test case reduction becomes desirable. Examples of reducing the number of test cases are [3, 22].
Reference: [25] <author> E. </author> <title> Weyuker; More Experience with Data Flow Testing, </title> <journal> IEEE Transactions on Software Engineering vol. </journal> <volume> 19, no. 9(Sep. </volume> <year> 1993), </year> <pages> pp. 912-919. </pages>
Reference-contexts: Analytical evaluation considers whether the testing criteria meet adequacy axioms [12, 23]. They describe properties any testing criteria should have to be considered adequate. They do not, however, guarantee high failure yields. Thus experimental evaluation of testing techniques is also important. Examples of such evaluations include <ref> [24, 25, 6] </ref>. When the number of tests to be run can become very large, test case reduction becomes desirable. Examples of reducing the number of test cases are [3, 22].
Reference: [26] <author> Weyuker, E., Goradia, T., Singh, A.; </author> <title> Automatically Generating Test Data from a Boolean Specification, </title> <journal> IEEE Transactions on Software Engineering SE-20, </journal> <month> 5(May </month> <year> 1994), </year> <pages> pp. 353-363. </pages>
Reference-contexts: This should come as no surprise, since the general test data generation problem is undecidable [8]. In practice, many test data generation systems make simplifying assumptions, either as to the power of the language for which they generate test data <ref> [3, 7, 26] </ref>, or as to which information they consider in driving test data generation (e.g., Robert Poston's T tool).
Reference: [27] <author> Jacek M. Zurada. </author> <title> Introduction to Artificial Neural Systems. </title> <publisher> West Publishing: </publisher> <address> St. Paul, </address> <year> 1992. </year>
Reference-contexts: Processing elements are interconnected by a network of weighted connections that encode network knowledge. Neural networks are highly parallel and exercise distributed control. They emphasize automatic learning. Neural networks have been used as memories, pattern recall devices, pattern classifiers, and general function mapping engines <ref> [4, 9, 27] </ref>. Test effectiveness evaluation concentrates on their use as pattern classifiers. A classifier maps input vectors to output vectors in two phases. The network learns the input-output classification from a set of training vectors. <p> A net with too few hidden units cannot learn the mapping to the required accuracy. Too many hidden units allow the net to memorize the training data and does not generalize well to new data. Backpropagation is the most popular training algorithm for multilayer neural networks <ref> [4, 9, 27] </ref>. The algorithm initializes the network with a random set of weights, and the network trains from a set of input-output pairs. Each pair requires a two-stage learning algorithm: forward pass and backward pass. <p> First, the error passes from the output layer to the hidden layer updating output weights. Next, each hidden unit calculates an error based on the error from each output unit. The error from the hidden units updates input weights. Details about the Backpropagation algorithm can be found in <ref> [4, 9, 27] </ref>. One training epoch passes when the network sees all input-output pairs in the training set. Training stops when the sum squared error is acceptable or when a predefined number of epochs passes.
References-found: 26

