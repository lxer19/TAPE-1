URL: http://ini.cs.tu-berlin.de/~ao/pubs/pitfalls_submitted.ps.gz
Refering-URL: http://ini.cs.tu-berlin.de/~ao/pubs-e.html
Root-URL: http://ini.cs.tu-berlin.de/~ao/pubs-e.html
Title: Performance Evaluation of Feedforward Networks Using Computational Methods  
Author: Stefan M. Ruger Arnfried Ossen 
Date: 1995  
Note: Submitted to NEURAP  
Address: Sekr. FR 5-9  10 587 Berlin, Germany  
Affiliation: Informatik,  Technische Universitat Berlin  
Abstract: We will demonstrate that the performance evaluation of feedforward neural networks using computational methods may result in overly pessimistic estimates of the prediction error. In fact, they capture unwanted variability in the distribution of weights, introduced by local maxima in the likelihood function in connection with deficiencies of gradient based learning procedures. An analysis of the influence of local maxima is hampered due to a nontrivial algebraic structure of the weight space; we will show that typical feedforward networks exhibit a large number of symmetries due to a nontrivial symmetry group acting on the weight space. We will present an algorithm which divides out these symmetries. In the resulting much smaller effective weight space, clustering algorithms may be used to improve the assessment of prediction errors. We will demonstrate that this method can be successfully applied. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Wray L. Buntine and Andreas S. Weigend. </author> <title> Calculating second derivatives on feed-forward networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> <year> 1991. </year>
Reference-contexts: [2], we obtain an approximation of the standard error estimation ^se (out ^w (x)) by i ^w ^ I 1 r w out (x)j ^w : There are efficient algorithms, which exploit the feed-forward structure of the network, to compute the Hes sian and the inverse Hessian of the log-likelihood <ref> [1, 3] </ref>. 3.2 Bootstrap Approach The bootstrap method [2] is based on re-estimations of the parameter vector on B bootstrap samples of the training set.
Reference: [2] <author> Bradley Efron and Robert J. Tibshirani. </author> <title> An Introduction to the Bootstrap, </title> <booktitle> volume 57 of Monographs on Statistics and Applied Probability. </booktitle> <publisher> Chapman & Hall, </publisher> <year> 1993. </year>
Reference-contexts: In general, there are several approximate methods to estimate the standard error of feedforward networks. The development of ever more complicated statistical learning procedures and powerful computers has led to computational approaches to statistical inference like jackknife, cross-validation and bootstrap <ref> [2] </ref>. <p> Using the delta method <ref> [2] </ref>, we obtain an approximation of the standard error estimation ^se (out ^w (x)) by i ^w ^ I 1 r w out (x)j ^w : There are efficient algorithms, which exploit the feed-forward structure of the network, to compute the Hes sian and the inverse Hessian of the log-likelihood [1, <p> error estimation ^se (out ^w (x)) by i ^w ^ I 1 r w out (x)j ^w : There are efficient algorithms, which exploit the feed-forward structure of the network, to compute the Hes sian and the inverse Hessian of the log-likelihood [1, 3]. 3.2 Bootstrap Approach The bootstrap method <ref> [2] </ref> is based on re-estimations of the parameter vector on B bootstrap samples of the training set.
Reference: [3] <author> Babak Hassibi and David G. Stork. </author> <title> Second order derivatives for network pruning. </title> <editor> In Stephen Jose Hanson, Jack D. Cowan, and C. Lee Giles, editors, </editor> <booktitle> Advances in Neural Information Processing Systems V. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: [2], we obtain an approximation of the standard error estimation ^se (out ^w (x)) by i ^w ^ I 1 r w out (x)j ^w : There are efficient algorithms, which exploit the feed-forward structure of the network, to compute the Hes sian and the inverse Hessian of the log-likelihood <ref> [1, 3] </ref>. 3.2 Bootstrap Approach The bootstrap method [2] is based on re-estimations of the parameter vector on B bootstrap samples of the training set.
Reference: [4] <author> Robert Hecht-Nielsen. </author> <title> On the algebraic structure of feedforward network weight spaces. </title> <editor> In R. Eck-miller, editor, </editor> <booktitle> Advanced Neural Computers, Ams-terdam, 1990. </booktitle> <publisher> Elsevier. </publisher>
Reference-contexts: There have been attempts to study the algebraic structure of weight spaces <ref> [4] </ref>; our analysis is more general and the above search set W is much smaller than the one given in [4]. In a certain sense W represents a possible way to define a minimal search set with respect to S. The space R E of weight vectors is highly redundant. <p> There have been attempts to study the algebraic structure of weight spaces <ref> [4] </ref>; our analysis is more general and the above search set W is much smaller than the one given in [4]. In a certain sense W represents a possible way to define a minimal search set with respect to S. The space R E of weight vectors is highly redundant.
Reference: [5] <author> Ian D. Macdonald. </author> <title> The theory of groups. </title> <publisher> Oxford University Press, </publisher> <year> 1975. </year>
Reference-contexts: We have given an example using k-means clustering. Appendix: Symmetry Group of the Weight Space The symmetries of the network function, which arise in section 2, are best described and analyzed in terms W13 W12 their Orbits under S. of their corresponding groups, see e. g. <ref> [5] </ref>. Let (M ) denote the permutation group of a set M . Recall that every permutation can be written in terms of transpositions.
Reference: [6] <author> Robert Tibshirani. </author> <title> A comparison of some error estimates for neural networks. </title> <type> Technical report, </type> <institution> University of Toronto, Dep. of Statistics, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: The development of ever more complicated statistical learning procedures and powerful computers has led to computational approaches to statistical inference like jackknife, cross-validation and bootstrap [2]. Recent studies suggest that computational methods perform best, "partly because they capture variability due to the choice of starting weights." <ref> [6] </ref> However, we will argue that their naive use can be misleading. 2 Weights and Symmetries Most learning algorithms for feedforward networks try to minimize a certain cost function, which depends on a weight vector. <p> The training multiset D is f (x 1 ; y 1 ); : : : ; (x 33 ; y 33 )g (see Figure 3). 3.1 Likelihood Approach The classic approach of standard error estimation follows likelihood theory <ref> [6] </ref>.
References-found: 6

