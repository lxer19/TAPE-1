URL: http://react-ilp.cs.nyu.edu/publications/schedule.ps
Refering-URL: http://react-ilp.cs.nyu.edu/publications/index.html
Root-URL: http://www.cs.nyu.edu
Title: A fast algorithm for scheduling time-constrained instructions on processors with ILP  
Author: Allen Leung Krishna V. Palem Amir Pnueli 
Keyword: Compiler-optimizations, embedded applications, instruction level parallelism, instruction scheduling.  
Note: Supported in part by awards from the Hewlett-Packard Corporation, Panasonic American Laboratories, the IBM corporation and DARPA contract no. DABT63-96-C-0049.  
Address: 251 Mercer St., New York, NY 10012  251 Mercer St., New York, NY 10012  76100 Rehovot, Israel  
Affiliation: Courant Institute of Mathematical Sciences  Courant Institute of Mathematical Sciences  Weizmann Institute of Science  
Abstract: Instruction scheduling is central to achieving performance in modern processors with instruction level parallelism (ILP). Classical work in this area has spanned the theoretical foundations of algorithms for instruction scheduling with provable optimality, as well as heuristic approaches with experimentally validated performance improvements. Typically, the theoretical foundations are developed in the context of basic-blocks of code. In this paper, we provide the theoretical foundations for scheduling basic-blocks of instructions with time-constraints, which can play an important role in compile-time ILP optimizations in embedded applications. We present an algorithm for scheduling unit-execution-time instructions on machines with multiple pipelines, in the presence of precedence constraints, release-times, deadlines, and latencies l ij between any pairs of instructions i and j. Our algorithm runs in time O(n 3 ff(n)), where ff(n) is the functional inverse of the Ackermann function. It can be used construct feasible schedules for two classes of instances: (1) one pipeline and the latencies between instructions are restricted to the values of 0 and 1, and (2) arbitrary number of pipelines and monotone-interval order precedences. Our result can be seen as a natural extension of previous work on instruction scheduling for pipelined machines in the presence of deadlines. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Bernstein and I. Gertner. </author> <title> Scheduling expressions on a pipelined processor with a maximal delay of one cycle. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 11(1):5766, </volume> <year> 1989. </year>
Reference-contexts: Typically, such a processor can issue multiple instructions in a single cycle on multiple pipelined functional units. In particular, the core case of instruction-scheduling for basic-blocks is very well-understood and provably optimal solutions and heuristics <ref> [1, 10, 16, 22] </ref> have been validated in product-quality compilers. With the ever-decreasing cost and improving performance of modern RISC microprocessors, they are being used more and more as controllers of automobile braking systems, monitors of intensive care rooms in hospitals, telemetry and a variety of other embedded applications. <p> With possibly unequal latencies as is the case with RISC processors, we can specialize our algorithm to get polynomially solvable results due to Palem and Simons [16], as well as those due to Bernstein and Gertner <ref> [1] </ref> for deadline-free instances. Again, release-times are not considered. 3. <p> Thus a faster algorithm exists. The Bern stein/Gertner <ref> [1] </ref> rank function can be computed in O (n 2 ) time and is optimal for this problem.
Reference: [2] <author> P. Brucker and S. Knust. </author> <title> Complexity results for single-machine problems with positive finish-start time-lags. Os-nabruecker Schriften zur Mathematik, Reihe P, </title> <journal> Nr. </journal> <volume> 202, </volume> <year> 1998. </year>
Reference-contexts: In the notation of [9] and <ref> [2] </ref>, this is the problem P jprec (l ij ); r i ; p i = 1jL max Example: Let us consider the example DAG in Figure 1. The latencies are shown as labels next to the edges. <p> A similar problem with unit-latencies, 1 j prec (l ij = 1); p j 2 IN + j C max , has been addressed in [5], using the Coffman/Graham [4] lexicographical rank function for list-scheduling priorities. (See also the recent work of Brucker and Knust <ref> [2] </ref> for other related results.) The optimality proof makes use of a certain rigid structure of list-schedules generated by this rank function, and is quite involved.
Reference: [3] <author> J. Bruno, J. Jones, and K. </author> <title> So. Deterministic scheduling with pipelined processors. </title> <journal> IEEE Transactions of Computers, </journal> <volume> C-29:308316, </volume> <month> Apr. </month> <year> 1980. </year> <title> 7 We do not consider the measure (tardiness + 1) as satisfactory although it remains finite everywhere. </title>
Reference-contexts: Interestingly, our algorithm also unifies several previously known results involving latencies, deadlines and release-times. We group these results into three parts. 1. In the first group, we have a range of scheduling problems introduced by Bruno, Jones and So wherein all the latencies are equal, and possibly zero <ref> [3] </ref>; the latter case yields the well-known multiprocessor scheduling problem. Furthermore, we do not consider release-times; only deadlines are permitted. <p> Algorithms and optimality proofs for classes (III) and (IV) have previously appeared in [8] and <ref> [3] </ref>. We repeat the proof for (III) here for comparison. Proof Sketch: Suppose we are given a feasible instance but list-scheduling produces an infeasible schedule . Let instruction i be an instruction with (i) d 0 i such that (i) is minimized. <p> Remarks Decision questions arising out of parametric variations of the polynomially solvable instruction-scheduling variations for RISC machines and pipelines that we identified in Section 3 are known to be NP-complete <ref> [3, 10, 15] </ref>. Some interesting and challenging open questions that remain unresolved in this domain and that arise out of the new work are: 1.
Reference: [4] <author> E. Coffman and R. Graham. </author> <title> Optimal scheduling for two-processor systems. </title> <journal> Acta Informatica, </journal> <volume> 1:200213, </volume> <year> 1972. </year>
Reference-contexts: Our algorithm specializes to most of the previously known cases such as the classical two-processor scheduling due to Coffman and Graham <ref> [4] </ref>, again via the unification in [15]. 4. Computing the Modified Deadlines The modified deadlines computation algorithm we describe below repeatedly invokes a backward scheduling subroutine backschedule, which is a generalization of the backward scheduling process described in Palem and Si-mons [16]. 4.1. <p> A similar problem with unit-latencies, 1 j prec (l ij = 1); p j 2 IN + j C max , has been addressed in [5], using the Coffman/Graham <ref> [4] </ref> lexicographical rank function for list-scheduling priorities. (See also the recent work of Brucker and Knust [2] for other related results.) The optimality proof makes use of a certain rigid structure of list-schedules generated by this rank function, and is quite involved.
Reference: [5] <author> L. Finta and Z. Liu. </author> <title> Single machine scheduling subject to precedence delays. </title> <journal> Discrete Applied Mathematics, </journal> <volume> 70:247 266, </volume> <year> 1996. </year>
Reference-contexts: We can generalize the recent result due to Finta and Liu <ref> [5] </ref> and show that modified deadlines computed with our algorithm can be used with list-scheduling to minimize the makespan of a schedule on one processor with arbitrary precedence constraints, individual processing times p i 1 and zero or unit latencies; the work in [5] proves this fact for unit latencies only. <p> recent result due to Finta and Liu <ref> [5] </ref> and show that modified deadlines computed with our algorithm can be used with list-scheduling to minimize the makespan of a schedule on one processor with arbitrary precedence constraints, individual processing times p i 1 and zero or unit latencies; the work in [5] proves this fact for unit latencies only. 2. Our algorithm specializes to most of the previously known cases such as the classical two-processor scheduling due to Coffman and Graham [4], again via the unification in [15]. 4. <p> A similar problem with unit-latencies, 1 j prec (l ij = 1); p j 2 IN + j C max , has been addressed in <ref> [5] </ref>, using the Coffman/Graham [4] lexicographical rank function for list-scheduling priorities. (See also the recent work of Brucker and Knust [2] for other related results.) The optimality proof makes use of a certain rigid structure of list-schedules generated by this rank function, and is quite involved.
Reference: [6] <author> G. N. Frederickson. </author> <title> Scheduling unit-time tasks with integer release times and deadlines. </title> <type> Technical Report CS-81-27, </type> <institution> Dept. of Computer Science, Penn. State University, </institution> <year> 1982. </year>
Reference-contexts: The running times of the algorithm can be improved to O (n 3 ) in both cases using the techniques identified by Gabow and Tarjan [7], and by Frederickson <ref> [6] </ref>, using possibly more than O (n 2 ) space 1 . Interestingly, our algorithm also unifies several previously known results involving latencies, deadlines and release-times. We group these results into three parts. 1. <p> The time complexity of our algorithm can be further improved to O ((m + n)n 2 ), where m is the number of pipelines, using Frederickson's <ref> [6] </ref> algorithm for scheduling unit-time tasks with integer release-times and deadlines, which can be made to run in O (n) time using the off-line union-find algorithm of Gabow and Tarjan [7]. Since m is typically much smaller than n, this is an improvement.
Reference: [7] <author> H. N. Gabow and R. Tarjan. </author> <title> A linear-time algorithm for a special case of disjoint set union. </title> <booktitle> Proc. of ACM Symposium on Theory of Computation, </booktitle> <pages> pages 246251, </pages> <year> 1983. </year>
Reference-contexts: The running times of the algorithm can be improved to O (n 3 ) in both cases using the techniques identified by Gabow and Tarjan <ref> [7] </ref>, and by Frederickson [6], using possibly more than O (n 2 ) space 1 . Interestingly, our algorithm also unifies several previously known results involving latencies, deadlines and release-times. We group these results into three parts. 1. <p> can be further improved to O ((m + n)n 2 ), where m is the number of pipelines, using Frederickson's [6] algorithm for scheduling unit-time tasks with integer release-times and deadlines, which can be made to run in O (n) time using the off-line union-find algorithm of Gabow and Tarjan <ref> [7] </ref>. Since m is typically much smaller than n, this is an improvement. In fact, the maintenance of the sorted lists L r and L d in Algorithm 2 becomes unnecessary when this list-scheduling algorithm is used as the subroutine.
Reference: [8] <author> M. Garey and D. Johnson. </author> <title> Two-processor scheduling with start-times and deadlines. </title> <journal> SIAM Journal of Computing, </journal> <volume> 6(3):416426, </volume> <month> Sept. </month> <year> 1977. </year>
Reference-contexts: In doing so, we unify and generalize the earlier work due to Palem and Simons [16] on instruction scheduling with deadlines, and that of Garey and John-son on scheduling tasks with release-times and deadlines on two identical processors <ref> [8] </ref>. Running in O (n 3 ff (n)) time, our algorithm is guaranteed to find feasible schedules for arbitrary basic-blocks of code, for such RISC ma chines as the IBM-801 [18], the Berkeley RISC [13] and Stanford MIPS [11] processors. <p> Again, release-times are not considered. 3. Moving to the third group, with release-times, our algorithm can be specialized to the case where all the latencies are zero, and m = 2 identical processors; the algorithm for this case is due to an early result by Garey and Johnson <ref> [8] </ref>, which runs in O (n 3 ) time. Ignoring both release-times and deadlines and consider ing the makespan minimization case, 1 The algorithm can use space that is proportional to the difference between the values of the largest of all deadlines, and the smallest of all release-times. 1. <p> Algorithms and optimality proofs for classes (III) and (IV) have previously appeared in <ref> [8] </ref> and [3]. We repeat the proof for (III) here for comparison. Proof Sketch: Suppose we are given a feasible instance but list-scheduling produces an infeasible schedule . Let instruction i be an instruction with (i) d 0 i such that (i) is minimized.
Reference: [9] <author> R. L. Graham, E. L. Lawler, J. K. Lenstra, and A. H. G. R. Kan. </author> <title> Optimization and approximation in deterministic sequencing and scheduling: A survey. </title> <journal> Annals of Discrete Mathematics, </journal> <volume> 5:287326, </volume> <year> 1979. </year>
Reference-contexts: In the notation of <ref> [9] </ref> and [2], this is the problem P jprec (l ij ); r i ; p i = 1jL max Example: Let us consider the example DAG in Figure 1. The latencies are shown as labels next to the edges.
Reference: [10] <author> J. Hennessy and T. Gross. </author> <title> Postpass code optimization of pipeline constraints. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 5(3), </volume> <year> 1983. </year>
Reference-contexts: Typically, such a processor can issue multiple instructions in a single cycle on multiple pipelined functional units. In particular, the core case of instruction-scheduling for basic-blocks is very well-understood and provably optimal solutions and heuristics <ref> [1, 10, 16, 22] </ref> have been validated in product-quality compilers. With the ever-decreasing cost and improving performance of modern RISC microprocessors, they are being used more and more as controllers of automobile braking systems, monitors of intensive care rooms in hospitals, telemetry and a variety of other embedded applications. <p> Feasible schedules are then computed by list-scheduling the instructions in non-decreasing order of their modified deadlines. Staying within the list-scheduling framework is appealing and holds promise since it has been successfully validated in pragmatic settings via stable and practically efficient implementations <ref> [10, 22] </ref>. The proof that for problem classes of interest, list-scheduling using our modified deadlines always yields feasible schedules whenever such schedules exist, is sketched in Section 5. In Section 3, we relate our work with previous research in the rich area of deterministic scheduling. <p> These restrictions on the latencies model early RISC processors such as the IBM-801, Berkeley RISC and Stan ford MIPS processors <ref> [10, 22] </ref>. 2. for DAGs that are monotone interval-orders, and on machines with an arbitrary number m of pipelines. <p> Remarks Decision questions arising out of parametric variations of the polynomially solvable instruction-scheduling variations for RISC machines and pipelines that we identified in Section 3 are known to be NP-complete <ref> [3, 10, 15] </ref>. Some interesting and challenging open questions that remain unresolved in this domain and that arise out of the new work are: 1.
Reference: [11] <author> J. Hennessy, N. Jouppi, J. Gill, F. Baskett, A. Strong, T. Gross, C. Rowen, and J. Leonard. </author> <title> The MIPS machine. </title> <booktitle> Proceedings IEEE Compcon, </booktitle> <pages> pages 27, </pages> <month> February </month> <year> 1982. </year>
Reference-contexts: Running in O (n 3 ff (n)) time, our algorithm is guaranteed to find feasible schedules for arbitrary basic-blocks of code, for such RISC ma chines as the IBM-801 [18], the Berkeley RISC [13] and Stanford MIPS <ref> [11] </ref> processors. Also, our algorithm can in the same time bound find feasible schedules whenever such schedules exist, for basic-blocks whose data-dependence graphs are monotone interval-orders [16, 17]; in this case, the RISC processor can have a variable number m of RISC pipelines, each with k stages in it.
Reference: [12] <author> J. R. Jackson. </author> <title> Scheduling a production line to minimize maximum tardiness. </title> <type> Technical Report 43, </type> <institution> Management Science Research Project, University of California, </institution> <year> 1955. </year>
Reference-contexts: The problem then simplifies to a decision problem of scheduling unit-time tasks with release-times and deadlines (without precedence and latency constraints). This well-known problem can be easily solved by list-scheduling with Jack-son's rule <ref> [12] </ref>: i.e. schedule a ready instruction with the earliest deadline whenever possible. Since this decision problem has to be solved repeatedly during the backward scheduling process, it is important to minimize its complexity.
Reference: [13] <author> M. Katevenis. </author> <title> Reduced Instruction Set Computer Architecture for VLSI. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1984. </year>
Reference-contexts: Running in O (n 3 ff (n)) time, our algorithm is guaranteed to find feasible schedules for arbitrary basic-blocks of code, for such RISC ma chines as the IBM-801 [18], the Berkeley RISC <ref> [13] </ref> and Stanford MIPS [11] processors.
Reference: [14] <author> J.-K. Lenstra, D. Shmoys, and E. Tardos. </author> <title> Approximation algorithms for scheduling unrelated parallel machines. </title> <booktitle> Proceedings of the 28th IEEE Conference on Foundation of Computer Science, </booktitle> <pages> pages 217224, </pages> <year> 1987. </year>
Reference-contexts: It will valuable to adapt powerful techniques rooted in polytope rounding <ref> [14] </ref> and related methods (for a recent update please also see [19]) to design approximation algorithms that can give better ratios in the context of RISC pipelines, for instructions with arbitrary precedence constraints. 3.
Reference: [15] <author> K. Palem. </author> <title> On the complexity of precedence constrained scheduling. </title> <type> Technical Report TR-86-11, </type> <institution> University of Texas, Austin, TX, </institution> <year> 1986. </year>
Reference-contexts: Furthermore, we do not consider release-times; only deadlines are permitted. We derive this fact by specializing our algorithm to yield the framework described in Palem's thesis <ref> [15] </ref> which unified and generalized all the previously known re sults in this category. 2. <p> Our algorithm specializes to most of the previously known cases such as the classical two-processor scheduling due to Coffman and Graham [4], again via the unification in <ref> [15] </ref>. 4. Computing the Modified Deadlines The modified deadlines computation algorithm we describe below repeatedly invokes a backward scheduling subroutine backschedule, which is a generalization of the backward scheduling process described in Palem and Si-mons [16]. 4.1. A Backward Schedule Formally, backward scheduling solves the following maximization problem. <p> Remarks Decision questions arising out of parametric variations of the polynomially solvable instruction-scheduling variations for RISC machines and pipelines that we identified in Section 3 are known to be NP-complete <ref> [3, 10, 15] </ref>. Some interesting and challenging open questions that remain unresolved in this domain and that arise out of the new work are: 1.
Reference: [16] <author> K. Palem and B. Simons. </author> <title> Scheduling time-critical instructions on RISC machines. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 5(3), </volume> <year> 1993. </year>
Reference-contexts: Typically, such a processor can issue multiple instructions in a single cycle on multiple pipelined functional units. In particular, the core case of instruction-scheduling for basic-blocks is very well-understood and provably optimal solutions and heuristics <ref> [1, 10, 16, 22] </ref> have been validated in product-quality compilers. With the ever-decreasing cost and improving performance of modern RISC microprocessors, they are being used more and more as controllers of automobile braking systems, monitors of intensive care rooms in hospitals, telemetry and a variety of other embedded applications. <p> In doing so, we unify and generalize the earlier work due to Palem and Simons <ref> [16] </ref> on instruction scheduling with deadlines, and that of Garey and John-son on scheduling tasks with release-times and deadlines on two identical processors [8]. <p> Also, our algorithm can in the same time bound find feasible schedules whenever such schedules exist, for basic-blocks whose data-dependence graphs are monotone interval-orders <ref> [16, 17] </ref>; in this case, the RISC processor can have a variable number m of RISC pipelines, each with k stages in it. A crucial technical idea on which our algorithm rests is that of a stable and consistent modified deadline, that we associate with each instruction. <p> With possibly unequal latencies as is the case with RISC processors, we can specialize our algorithm to get polynomially solvable results due to Palem and Simons <ref> [16] </ref>, as well as those due to Bernstein and Gertner [1] for deadline-free instances. Again, release-times are not considered. 3. <p> Computing the Modified Deadlines The modified deadlines computation algorithm we describe below repeatedly invokes a backward scheduling subroutine backschedule, which is a generalization of the backward scheduling process described in Palem and Si-mons <ref> [16] </ref>. 4.1. A Backward Schedule Formally, backward scheduling solves the following maximization problem. INPUT: an instruction i, a set of instructions S distinct from i, release-times r 0 and deadlines d 0 on S [ fig. <p> This case has the potential for being quite useful in practice. 2. Still staying with the makespan minimization version, typical approximation results use extensions of the classical algorithm of Graham <ref> [16] </ref> and tend to approach a constant factor ratio of 2 from below, as the problem parameters such as the number of functional units m increase.
Reference: [17] <author> C. Papadimitriou and M. Yannakakis. </author> <title> Scheduling interval-ordered tasks. </title> <journal> SIAM Journal of Computing, </journal> <volume> 8:405409, </volume> <year> 1979. </year>
Reference-contexts: Also, our algorithm can in the same time bound find feasible schedules whenever such schedules exist, for basic-blocks whose data-dependence graphs are monotone interval-orders <ref> [16, 17] </ref>; in this case, the RISC processor can have a variable number m of RISC pipelines, each with k stages in it. A crucial technical idea on which our algorithm rests is that of a stable and consistent modified deadline, that we associate with each instruction.
Reference: [18] <author> G. Radin. </author> <title> The 801 minicomputer. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 27(3):237246, </volume> <year> 1983. </year>
Reference-contexts: Running in O (n 3 ff (n)) time, our algorithm is guaranteed to find feasible schedules for arbitrary basic-blocks of code, for such RISC ma chines as the IBM-801 <ref> [18] </ref>, the Berkeley RISC [13] and Stanford MIPS [11] processors.
Reference: [19] <author> A. Schulz. </author> <title> Polytopes and Scheduling. </title> <type> PhD thesis, </type> <institution> Fach-bereich Mathematik, Technische Universitat Berlin, </institution> <address> Berlin, Germany, </address> <year> 1996. </year>
Reference-contexts: It will valuable to adapt powerful techniques rooted in polytope rounding [14] and related methods (for a recent update please also see <ref> [19] </ref>) to design approximation algorithms that can give better ratios in the context of RISC pipelines, for instructions with arbitrary precedence constraints. 3. Designing fast approximation algorithms, ideally with (small) constant fraction ratios, within the context of tasks with deadlines.
Reference: [20] <author> R. E. Tarjan. </author> <title> Efficiency of a good but not linear set union algorithm. </title> <journal> J. ACM, </journal> <volume> 22:215225, </volume> <month> April </month> <year> 1975. </year>
Reference-contexts: This variant of the algo rithm can be implemented using the union-find <ref> [20] </ref> data structure in O (nff (n)) time and O (n) space 2 . Our fast backward scheduling algorithm is described be low. Algorithm 1 (Backward Schedule backschedule) 1.
Reference: [21] <author> J. D. Ullman. </author> <title> NP-complete scheduling problems. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 10:384393, </volume> <year> 1975. </year>
Reference-contexts: We restrict our attention to only single processor problems, since precedence constrained multiprocessor scheduling is NP-complete, even when restricted to two processors and processing times of 1 and 2 <ref> [21] </ref>.
Reference: [22] <author> H. Warren. </author> <title> Instruction scheduling for the IBM RISC system/6k processors. </title> <journal> IBM Journal of Research and Development, </journal> <pages> pages 8592, </pages> <year> 1990. </year>
Reference-contexts: Typically, such a processor can issue multiple instructions in a single cycle on multiple pipelined functional units. In particular, the core case of instruction-scheduling for basic-blocks is very well-understood and provably optimal solutions and heuristics <ref> [1, 10, 16, 22] </ref> have been validated in product-quality compilers. With the ever-decreasing cost and improving performance of modern RISC microprocessors, they are being used more and more as controllers of automobile braking systems, monitors of intensive care rooms in hospitals, telemetry and a variety of other embedded applications. <p> Feasible schedules are then computed by list-scheduling the instructions in non-decreasing order of their modified deadlines. Staying within the list-scheduling framework is appealing and holds promise since it has been successfully validated in pragmatic settings via stable and practically efficient implementations <ref> [10, 22] </ref>. The proof that for problem classes of interest, list-scheduling using our modified deadlines always yields feasible schedules whenever such schedules exist, is sketched in Section 5. In Section 3, we relate our work with previous research in the rich area of deterministic scheduling. <p> These restrictions on the latencies model early RISC processors such as the IBM-801, Berkeley RISC and Stan ford MIPS processors <ref> [10, 22] </ref>. 2. for DAGs that are monotone interval-orders, and on machines with an arbitrary number m of pipelines.
References-found: 22

