URL: http://www.eecs.umich.edu/techreports/systems/cspl/cspl-316.ps.gz
Refering-URL: http://www.eecs.umich.edu/systems/TechReportList.html
Root-URL: http://www.eecs.umich.edu
Title: Generalized Proximal Point Algorithms and Bundle Implementations  
Author: Stephane Chretien and Alfred O. Hero 
Date: 316, Aug. 1998  
Address: Ann Arbor, MI 48109-2122  
Affiliation: COMMUNICATIONS SIGNAL PROCESSING LABORATORY Department of Electrical Engineering and Computer Science The University of Michigan  
Pubnum: Technical Report No.  
Abstract: In this paper, we present a study of the proximal point algorithm using very general regularizations for minimizing possibly nondifferentiable and nonconvex locally Lipschitz functions. We deduce from the proximal point scheme simple and implementable bundle methods for the convex and nonconvex cases. The originality of our bundle method is that the bundle information incorporates the subgradients of both the objective and the regularization function. The resulting method opens up a broad class of regularizations which are not restricted to quadratic, convex or even differentiable functions. Keywords: mathematical programming, proximal point, bundle methods, nonsmooth regularization This work was partially supported by AFOSR F49620-96-0028. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. H. Bauschke and J. M. Borwein. </author> <title> On projection algorithms for solving convex feasibility problems. </title> <journal> SIAM Review, </journal> <volume> 38(3) </volume> <pages> 367-426, </pages> <year> 1996. </year>
Reference-contexts: To illustrate, consider the following one dimensional example, f (y) = x fl y; if y x fl 2 (x fl y); if y &gt; x fl We then have @f (x fl ) = <ref> [1; 1 2 ] </ref> and @(x fl ; x fl ) = [1; 1]. On the other hand, @F (x fl ; x fl ) = [2; 1 2 ]. <p> To illustrate, consider the following one dimensional example, f (y) = x fl y; if y x fl 2 (x fl y); if y &gt; x fl We then have @f (x fl ) = [1; 1 2 ] and @(x fl ; x fl ) = <ref> [1; 1] </ref>. On the other hand, @F (x fl ; x fl ) = [2; 1 2 ].
Reference: [2] <author> A. Bihain. </author> <title> Optimization of upper semidifferentiable functions. </title> <journal> Journal of Optimization Theory and Applications, </journal> <volume> 44 </volume> <pages> 545-568, </pages> <year> 1984. </year>
Reference-contexts: x2R n f (x): (1) One of the most widely studied methods for solving nondifferentiable optimization problems is the bundle method first proposed by Lemarechal [14] and Wolfe [31] for convex minimization and further developed by Mi*in [19, 20] and Kiwiel [10, 11, 13] for the nonconvex case; see also <ref> [2] </ref>, [17], [26] and the references therein. The bundle method can be interpreted as a cutting plane algorithm stabilized by a quadratic penalty or regularization. <p> On the other hand, @F (x fl ; x fl ) = <ref> [2; 1 2 ] </ref>.
Reference: [3] <author> J. F. Bonnans, J.-Ch. Gilbert, C. Lemarechal, and C. Sagastizabal. </author> <title> Optimization numerique. </title> <journal> Aspects theoriques et pratiques, </journal> <volume> volume 27. </volume> <publisher> Springer Verlag, </publisher> <year> 1997. </year> <title> Series : Mathematiques et Applications. Chretien and Hero, "Generalized proximal methods : : : " 22 </title>
Reference-contexts: Finally, increase k by 1 and go to Step 1. We now establish some preliminary results concerning this algorithm. The following lemmas are standard in the analysis of bundle methods. In particular, the following lemma expresses proximal step (24) in the form of a dual quadratic program: Lemma 7 <ref> [3] </ref> Consider the constrained minimization problem u =argmin u2R k n 1 j j2J k X u j ff k (25) subject to (26) u 2 k = z 2 (0; 1) k j j2J k Then, (i) y k+1 = x k k j2J k where y k+1 solves the <p> step 2 of Algorithm 1 f (x k ) (y k+1 ) 2 1 fi X u j s j fi + j2J k j Recalling that u is solution to the quadratic program (29), we have * k = j2J k j Therefore (32) is equivalent to (see also <ref> [3] </ref>) ffi k = * k + 2 k With this result in hand, we obtain the following lemma. Chretien and Hero, "Generalized proximal methods : : : " 12 Lemma 8 Let p k be the direction defined by (28). <p> We will also need a more technical result the proof of which can be found in <ref> [3] </ref>. Lemma 10 Let f and satisfy assumptions 1 and 2. Let K be the set of indices where a descent step is taken. Let f fl = lim k!1;k2K f (x k ). Then X ffi k m The following convergence analysis is divided into two parts.
Reference: [4] <author> F. H. Clarke. </author> <title> Optimization and nonsmooth analysis. </title> <publisher> Wiley-Interscience, </publisher> <year> 1983. </year>
Reference-contexts: The case of convex function is discussed first in Section 5 for the sake of clarity in the exposition. Then, algorithmic refinements, including a linesearch, are introduced in section 6 in order to accomodate the nonconvex case. We recall the following notations and definitions <ref> [4] </ref>. The inner product on R n is denoted by h:; :i and the associated norm is j:j. The convex hull of a set S is denoted conv (S).
Reference: [5] <author> A. P. Dempster, N. M. Laird, and D. B. Rubin. </author> <title> Maximum likelihood for incomplete data via the EM algorithm (with discussion). </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> 39 </volume> <pages> 1-38, </pages> <year> 1977. </year>
Reference-contexts: Example 1 (EM-type algorithms for maximum likelihood estimation) We show here that the case of Kullback regularization results in a proximal point method which is a generalization of the well known Expectation Maximization (EM) algorithm for maximum likelihood estimation <ref> [5] </ref>. Consider as in [5] the sample spaces 1 and 2 on which one defined the random variables V 1 and V 2 with respective probability densities p 1 (v 1 ; x) and p 2 (v 2 ; x), both indexed by an unknown parameter x 2 R n to <p> Example 1 (EM-type algorithms for maximum likelihood estimation) We show here that the case of Kullback regularization results in a proximal point method which is a generalization of the well known Expectation Maximization (EM) algorithm for maximum likelihood estimation <ref> [5] </ref>. Consider as in [5] the sample spaces 1 and 2 on which one defined the random variables V 1 and V 2 with respective probability densities p 1 (v 1 ; x) and p 2 (v 2 ; x), both indexed by an unknown parameter x 2 R n to be estimated. <p> When k = 1, recursion (6) is identical to the EM algorithm introduced in <ref> [5] </ref> where V 2 is the incomplete data and V 1 is the complete data. Furthermore, as implied by Lemma 4.1 below the recursion (6) monotonically increases the log-likelihood log p 2 (v 2 ; y) as does the standard EM algorithm of [5]. <p> identical to the EM algorithm introduced in <ref> [5] </ref> where V 2 is the incomplete data and V 1 is the complete data. Furthermore, as implied by Lemma 4.1 below the recursion (6) monotonically increases the log-likelihood log p 2 (v 2 ; y) as does the standard EM algorithm of [5]. A special case of (5) is the case of Laplacian data, p 2 (v 2 ; x) = 2 When the complete data V 1 is also chosen as Laplacian, it is easy to show that the Kullback regularization given by (4) is nonsmooth and nonconvex.
Reference: [6] <author> M. Godard. </author> <title> Self-recovering equalization and carrier tracking in two-dimensional data communication systems. </title> <journal> IEEE Trans. Commun., </journal> <volume> 28 </volume> <pages> 1867-1875, </pages> <year> 1980. </year>
Reference-contexts: For odd p or q, the function is nondifferentiable. A popular algorithm for implementing (54) is the well known constant modulus algorithm (CMA) <ref> [6, 30, 27] </ref>. The objective function has the shape represented in Figure 1, for the case n = 1, p = 2, q = 1.
Reference: [7] <author> J. B. Hirriart Urruty and C. </author> <title> Lemarechal. Convex Analysis and Minimization Algorithms I-II. </title> <publisher> Springer Verlag, </publisher> <year> 1993. </year> <institution> Grundlehren der mathematischen Wissenschaften 306. </institution>
Reference-contexts: In the two next sections, we use the bundle framework introduced by Lemarechal <ref> [7] </ref> to make this approach tractable in applications. The present section introduces the main ideas governing incorporation of nonquadratic regularization functions into the bundle mechanism in the case where the functions are convex. <p> x x k i * k ; which is equivalent to p k 2 @ * k f (x k ) + k (x k ; x k ) : This last result shows in particular the well known fact that bundle methods may be interpreted as *subgradient methods (see <ref> [7] </ref>). The main feature of bundle methods is therefore the control of the parameter * k via (34) and the descent step/null step strategy using the expected decrease ffi k . We now discuss convergence of this method to a minimizer of f . <p> Secondly, if the linesearch only leads to a null step, the linesearch provides a systematic refinement of the local subgradient information. Indeed, as discussed in <ref> [7] </ref> and [19], under some assumptions of weak upper semi-smothness, a new candidate is obtained which satisfies hs k+1 ; d k i &gt; 0 when a null step is taken. The question of optimality is solved in the following manner.
Reference: [8] <author> A. N. Iusem, B. Svaiter, and M. Teboulle. </author> <title> Entropy-like proximal methods in convex programming. </title> <journal> Mathematics of Operations Research, </journal> <volume> 19 </volume> <pages> 790-814, </pages> <year> 1994. </year>
Reference-contexts: Example 2 (Methods of multipliers) In [24] Rockafellar shows that the proximal point approach can be applied to the dual of a constrained optimization problem to yield interesting classes of multiplier methods. Subsequent studies <ref> [28, 29, 8] </ref> have demonstrated the benefit of using nonquadratic regularization functions. Among the possible choices for regularization functions proposed in [28] is the OEdivergence d OE (x; x 0 ) = j=1 x 0 x j where, in [28], OE was assumed strictly convex. <p> The proximal point algorithm applied to the dual takes the form (see <ref> [8, section 6] </ref>) p 0 2 R m p k+1 = argmax p0 fc (p) k d OE (p k ; p)g with c (p) being the dual functional defined by inf x2R n L (x; p) where L (x; p) is the Lagrangian L (x; p) = f (x) + <p> L (x; p) = f (x) + i=1 p i g i (x) if p i 0; 8i 2 f1; : : : ; mg 1 otherwise Thus, one obtains convergence of (x k ) k2N and (p k ) k2N to the solution of problem (7) (for instance, see <ref> [8] </ref>). Using different choices for the function OE, some well known multiplier methods can be recovered. Our generalization of OE to nonconvex functions opens up many new possibilities. Further examples of nonsmooth and nonconvex regularizations have also recently been studied in the context of inverse problems in [21] and [22].
Reference: [9] <author> K. C. Kiwiel. </author> <title> An aggregate subgradient method for nonsmooth convex minimization. </title> <journal> Mathematical Programming, </journal> <volume> 27 </volume> <pages> 320-341, </pages> <year> 1983. </year>
Reference-contexts: Proof: Due to Lemma 16 and Assumption 3 (i), the function f (:) + k (x k ; :) is weakly upper semismooth. Therefore, the proof of Theorem 4.1 (a) in [19] can be easily adapted to our linesearch procedure. The following lemma due to Kiwiel <ref> [9] </ref> will be used in the proof of Theorem 3. In particular, given * &gt; 0, this lemma implies existence of a finite number of successive null steps without reset such that jd k j &lt; *. <p> The following lemma due to Kiwiel [9] will be used in the proof of Theorem 3. In particular, given * &gt; 0, this lemma implies existence of a finite number of successive null steps without reset such that jd k j &lt; *. Lemma 18 <ref> [9, 10] </ref> Let w k = 1 2 jd k j 2 + ff k and assume t k L = 0 and r k a = 0.
Reference: [10] <author> K. C. Kiwiel. </author> <title> A linearization algorithm for nonsmooth minimization. </title> <journal> Mathematics of Operations Research, </journal> <volume> 10(2) </volume> <pages> 185-194, </pages> <year> 1985. </year>
Reference-contexts: nonconvex function f (x) on R n , i.e. x2R n f (x): (1) One of the most widely studied methods for solving nondifferentiable optimization problems is the bundle method first proposed by Lemarechal [14] and Wolfe [31] for convex minimization and further developed by Mi*in [19, 20] and Kiwiel <ref> [10, 11, 13] </ref> for the nonconvex case; see also [2], [17], [26] and the references therein. The bundle method can be interpreted as a cutting plane algorithm stabilized by a quadratic penalty or regularization. <p> In the case where f is nonconvex, the following polyhedral approximation is usually chosen, as in <ref> [10, 11, 13] </ref>, ^ f (x) = f (x k ) + max fff k where ff k ff (x; y) = jf (x) f (y) hg (y); x k yij: One fruitful interpretation of the bundle method is to consider iteration (2) as an implementable approximation of the well known <p> For any x 2 R n and any * 0 the Goldstein *-subdifferential <ref> [10] </ref> of f at x is the set @ * f (x) = convf@f (y) j jy xj *g: The multivalued function (x; *) 7! @ * f (x) is locally bounded and upper semicontinuous, i.e., x k ! x, * k ! *, p k 2 @ * k f <p> The main ideas remain the same as in the convex situation. Nevertheless, several modifications need to be introduced in order to overcome the difficulties associated with nonconvexity. The algorithmic structure used in the sequel is similar to the one proposed by Kiwiel <ref> [10] </ref> and therefore inherits useful convergence properties. This allows us to concentrate on the particular problems induced by the use of our generalized regularization. We first introduce the main characteristics of the method. <p> In other words, jp k j should not vanish faster than a fraction of the locaty measure. Convergence of p k towards zero is obtained using the same type of proof as in <ref> [10] </ref> which implies convergence of a k to zero. 6.2 A generalized proximal bundle method for nonconvex functions In the sequel, at iteration k we will use the notation s (y) for any element of @f (y) + k @(x k ; y) and ff (x; y) = jf (x) f <p> Proof: Due to assumption 1 (i) of inf-compactness, and the fact that algorithm 2 is a descent method, the sequence (x k ) k2N is bounded. The remainder of the proof can be easily adapted from <ref> [10, Lemma 3.3] </ref>. <p> and x = y = x, we have p fl Finally, (48), (49) and (50) together give 0 2 @f (x) + + @(x; x): (51) The proof is then easily completed using Lemma 2. 6.4 Convergence Convergence of Algorithm 2 is established using the properties of Algorithm 2.1 in <ref> [10] </ref>, to which it is structurally very close, and the preliminary properties established in the previous section. In order to adapt the study of [10] to our generalized proximal bundle method, we make the following important observation. <p> x): (51) The proof is then easily completed using Lemma 2. 6.4 Convergence Convergence of Algorithm 2 is established using the properties of Algorithm 2.1 in <ref> [10] </ref>, to which it is structurally very close, and the preliminary properties established in the previous section. In order to adapt the study of [10] to our generalized proximal bundle method, we make the following important observation. The behavior of the method principally relies on the output of the linesearch (Procedure 1). <p> k ) f (y k+1 ) k m L t L v k : Due to positivity of , we obtain f (x k ) f (y k+1 ) m L t L v k : (52) This last equation corresponds exactly to the output given by Algorithm 2.1 of <ref> [10] </ref> in the descent case. <p> k+1 ) m R t R v k ; which is equivalent to F k (x k ; x k ) F k (x k ; y k+1 ) m R t R v k : This corresponds exactly to the output of the linesearch procedure in Algorithm 2.1 of <ref> [10] </ref> when f (:) is replaced by F k (x k ; :). Using these strong similarities between the two methods, we now establish convergence of our procedure. The first step is study the linesearch procedure. In particular, we will need the following simple lemma. <p> The following lemma due to Kiwiel [9] will be used in the proof of Theorem 3. In particular, given * &gt; 0, this lemma implies existence of a finite number of successive null steps without reset such that jd k j &lt; *. Lemma 18 <ref> [9, 10] </ref> Let w k = 1 2 jd k j 2 + ff k and assume t k L = 0 and r k a = 0. <p> Assume in addition that only a finite number of descent step is taken and let k 0 denote the index of the last descent step. Then x k 0 is a stationary point of f . Proof: Using the same type of proof as in Lemma 3.4 of <ref> [10] </ref>, we obtain that lim p k = 0 and lim a k = 0: Therefore, Lemma 15 implies the desired result. The case of an infinite number of descent steps follows. Lemma 20 Assume that f and satisfy the assumptions in lemma 19. <p> Proof: Since the linesearch procedure 1 gives equation (52) in the case of a descent step, the sequence (f (x k )) k2N is nondecreasing. Furthermore, one can easily check that the proof of lemma 3.5 in <ref> [10] </ref> can also be adapted to our method. Therefore, we obtain lim p k = 0 and lim a k = 0: Now, take any convergent subsequence from (x k ) k2N and let x denote its limit. Then, Lemma 15 implies the desired result.
Reference: [11] <author> K. C. Kiwiel. </author> <title> A method of linearization for linearly constrained nonconvex nonsmooth minimization. </title> <journal> Mathematical Programming, </journal> <volume> 34 </volume> <pages> 175-187, </pages> <year> 1986. </year>
Reference-contexts: nonconvex function f (x) on R n , i.e. x2R n f (x): (1) One of the most widely studied methods for solving nondifferentiable optimization problems is the bundle method first proposed by Lemarechal [14] and Wolfe [31] for convex minimization and further developed by Mi*in [19, 20] and Kiwiel <ref> [10, 11, 13] </ref> for the nonconvex case; see also [2], [17], [26] and the references therein. The bundle method can be interpreted as a cutting plane algorithm stabilized by a quadratic penalty or regularization. <p> In the case where f is nonconvex, the following polyhedral approximation is usually chosen, as in <ref> [10, 11, 13] </ref>, ^ f (x) = f (x k ) + max fff k where ff k ff (x; y) = jf (x) f (y) hg (y); x k yij: One fruitful interpretation of the bundle method is to consider iteration (2) as an implementable approximation of the well known
Reference: [12] <author> K. C. Kiwiel. </author> <title> Proximity control in bundle methods for convex nondifferentiable minimization. </title> <journal> Mathematical Programming, </journal> <volume> 46 </volume> <pages> 105-122, </pages> <year> 1990. </year>
Reference-contexts: They can be interpreted as implementations of stepwise approximations of the proximal point algorithm <ref> [16, 12] </ref>. However, to our knowledge, bundle implementations of general nonquadratic regularizations have not been considered in the litterature. This may be due to be fact that bundle methods are essentially sequential quadratic programs, which seems to exclude the possibility of more general regularization functions.
Reference: [13] <author> K. C. Kiwiel. </author> <title> Restricted step and Levenberg-Marquard techniques in proximal bundle methods for nonconvex nondif-ferentiable optimization. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 6(1) </volume> <pages> 227-249, </pages> <year> 1996. </year>
Reference-contexts: nonconvex function f (x) on R n , i.e. x2R n f (x): (1) One of the most widely studied methods for solving nondifferentiable optimization problems is the bundle method first proposed by Lemarechal [14] and Wolfe [31] for convex minimization and further developed by Mi*in [19, 20] and Kiwiel <ref> [10, 11, 13] </ref> for the nonconvex case; see also [2], [17], [26] and the references therein. The bundle method can be interpreted as a cutting plane algorithm stabilized by a quadratic penalty or regularization. <p> In the case where f is nonconvex, the following polyhedral approximation is usually chosen, as in <ref> [10, 11, 13] </ref>, ^ f (x) = f (x k ) + max fff k where ff k ff (x; y) = jf (x) f (y) hg (y); x k yij: One fruitful interpretation of the bundle method is to consider iteration (2) as an implementable approximation of the well known <p> Thus, we can extract convergent subsequences (p oe (k) oe (fl (k)) 2 ) k2N with respective limits p fl 2 such that p fl 2 = 0: (48) On the other hand, using <ref> [13, lemma 3.1] </ref>, we obtain p fl Furthermore, since lim k!1 x k = x, we have lim k!1 b k = 0 and thus lim k!1 q (a k + b k ; b k + a k ) + a k = 0, due to Assumption 3 (ii) c. <p> Using <ref> [13, lemma 3.1] </ref> and the fact that + = lim sup k!1 + k one easily obtains p fl Now, due to quazisymmetry Assumption 3 (ii) in the case a = 0 and x = y = x, we have p fl Finally, (48), (49) and (50) together give 0 2 <p> Algorithmic refinements of the basic regularized bundle method proposed in this paper will be investigated in future work. Notice that the theoretical results demonstrated in Section 6 may apply to more sophisticated approaches recently developed for the standard Moreau-Yosida regularization, including trust region techniques <ref> [13] </ref> and second order methods [16] in the case of convex objective and regularization functions. 8 Conclusion This paper developed a convergence analysis for the proximal point algorithm with general regularization functions. Strong convergence results were obtained without any assumption on differentiability nor convexity of the regularization.
Reference: [14] <author> C. </author> <title> Lemarechal. An extention of Davidon methods to nondifferentiable problems. </title> <journal> Mathematical Programming Study, </journal> <volume> 3 </volume> <pages> 145-173, </pages> <note> 1975. </note> <author> M. Balinski and P. Wolfe, </author> <title> Eds. </title>
Reference-contexts: Introduction In this paper, we address the problem of minimizing a locally Lipschitz possibly nondifferentiable and nonconvex function f (x) on R n , i.e. x2R n f (x): (1) One of the most widely studied methods for solving nondifferentiable optimization problems is the bundle method first proposed by Lemarechal <ref> [14] </ref> and Wolfe [31] for convex minimization and further developed by Mi*in [19, 20] and Kiwiel [10, 11, 13] for the nonconvex case; see also [2], [17], [26] and the references therein. The bundle method can be interpreted as a cutting plane algorithm stabilized by a quadratic penalty or regularization.
Reference: [15] <editor> C. Lemarechal and C. Sagastizabal. </editor> <title> A class of variable metric bundle methods. </title> <note> Research Report INRIA 2128, </note> <year> 1993. </year>
Reference-contexts: Then, x k 0 is a minimizer of f . Proof: The proof is easily adapted from <ref> [15] </ref>. The definition of ffi k gives f (x k ) ffi k = max ff (y j ) + k (x k ; y j ) + hs j ; y k+1 y j ig + 2 Take k k 0 .
Reference: [16] <editor> C. Lemarechal and C. Sagastizabal. </editor> <title> Practical aspects of the moreau yosida regularization: </title> <journal> Theoretical prelimiaries. SIAM Journal on Optimization, </journal> <volume> 7(2) </volume> <pages> 867-895, </pages> <year> 1997. </year>
Reference-contexts: The proximal point algorithm and the bundle method share the same property of solving a sequence of minimization subproblems incorporating a quadratic penalty, also denoted Moreau-Yosida regularization <ref> [16] </ref>. In this paper, we address the study of the proximal point algorithm and its bundle implementations using very general nonquadratic penalty functions. In particular, we establish convergence for a class of locally Lipschitz regularizations without any convexity nor differentiability assumptions. <p> They can be interpreted as implementations of stepwise approximations of the proximal point algorithm <ref> [16, 12] </ref>. However, to our knowledge, bundle implementations of general nonquadratic regularizations have not been considered in the litterature. This may be due to be fact that bundle methods are essentially sequential quadratic programs, which seems to exclude the possibility of more general regularization functions. <p> Algorithmic refinements of the basic regularized bundle method proposed in this paper will be investigated in future work. Notice that the theoretical results demonstrated in Section 6 may apply to more sophisticated approaches recently developed for the standard Moreau-Yosida regularization, including trust region techniques [13] and second order methods <ref> [16] </ref> in the case of convex objective and regularization functions. 8 Conclusion This paper developed a convergence analysis for the proximal point algorithm with general regularization functions. Strong convergence results were obtained without any assumption on differentiability nor convexity of the regularization.
Reference: [17] <editor> C. Lemarechal, J. J. Strodiot, and A. Bihain. </editor> <title> On a bundle algorithm for nonsmooth optimization. </title> <journal> Nonlinear Programming, </journal> <volume> 4 </volume> <pages> 245-281, </pages> <year> 1981. </year> <editor> O. L. Mangasarian, R. R. Meyer and S. M. Robinson, </editor> <publisher> Eds. </publisher>
Reference-contexts: n f (x): (1) One of the most widely studied methods for solving nondifferentiable optimization problems is the bundle method first proposed by Lemarechal [14] and Wolfe [31] for convex minimization and further developed by Mi*in [19, 20] and Kiwiel [10, 11, 13] for the nonconvex case; see also [2], <ref> [17] </ref>, [26] and the references therein. The bundle method can be interpreted as a cutting plane algorithm stabilized by a quadratic penalty or regularization.
Reference: [18] <editor> B. Martinet. Regularisation d'inequation variationnelles par approximations successives. Revue Francaise d'Informatique et de Recherche Operationnelle, </editor> <volume> 3 </volume> <pages> 154-179, </pages> <year> 1970. </year>
Reference-contexts: In the original form <ref> [18, 25] </ref>, the proximal point algorithm is defined by the reccurence x k+1 = argmin y2R n ff (y) + k jx yj 2 g: where ( k ) k2N is a sequence of positive relaxation parameters.
Reference: [19] <author> R. Mi*in. </author> <title> An algorithm for constrained optimization with semismooth functions. </title> <journal> Mathematics of Operations Research, </journal> <volume> 2 </volume> <pages> 191-207, </pages> <year> 1977. </year>
Reference-contexts: Lipschitz possibly nondifferentiable and nonconvex function f (x) on R n , i.e. x2R n f (x): (1) One of the most widely studied methods for solving nondifferentiable optimization problems is the bundle method first proposed by Lemarechal [14] and Wolfe [31] for convex minimization and further developed by Mi*in <ref> [19, 20] </ref> and Kiwiel [10, 11, 13] for the nonconvex case; see also [2], [17], [26] and the references therein. The bundle method can be interpreted as a cutting plane algorithm stabilized by a quadratic penalty or regularization. <p> We will also need the notion of weak upper semismoothness introduced by Mi*in <ref> [19] </ref>. <p> Secondly, if the linesearch only leads to a null step, the linesearch provides a systematic refinement of the local subgradient information. Indeed, as discussed in [7] and <ref> [19] </ref>, under some assumptions of weak upper semi-smothness, a new candidate is obtained which satisfies hs k+1 ; d k i &gt; 0 when a null step is taken. The question of optimality is solved in the following manner. <p> Lemma 17 The linesearch (Procedure 1) terminates in a finite number of steps. Proof: Due to Lemma 16 and Assumption 3 (i), the function f (:) + k (x k ; :) is weakly upper semismooth. Therefore, the proof of Theorem 4.1 (a) in <ref> [19] </ref> can be easily adapted to our linesearch procedure. The following lemma due to Kiwiel [9] will be used in the proof of Theorem 3.
Reference: [20] <author> R. Mi*in. </author> <title> A modification and extension of Lemarechal's algorithm for nonsmooth minimization. nondifferential and variational techniques in optimization. </title> <journal> Mathematical Programming Study, </journal> <volume> 17 </volume> <pages> 77-90, </pages> <year> 1982. </year> <title> D.C. </title> <editor> Sorensen and R.-J.B. Wets, </editor> <publisher> Eds. </publisher>
Reference-contexts: Lipschitz possibly nondifferentiable and nonconvex function f (x) on R n , i.e. x2R n f (x): (1) One of the most widely studied methods for solving nondifferentiable optimization problems is the bundle method first proposed by Lemarechal [14] and Wolfe [31] for convex minimization and further developed by Mi*in <ref> [19, 20] </ref> and Kiwiel [10, 11, 13] for the nonconvex case; see also [2], [17], [26] and the references therein. The bundle method can be interpreted as a cutting plane algorithm stabilized by a quadratic penalty or regularization.
Reference: [21] <author> M. </author> <month> Nikolova. </month> <institution> Estimees localement fortement homogenes. Comptes Rendus de l'Academie des Sciences, Paris, Serie I, </institution> <month> 325 </month> <pages> 665-670, </pages> <year> 1997. </year>
Reference-contexts: Using different choices for the function OE, some well known multiplier methods can be recovered. Our generalization of OE to nonconvex functions opens up many new possibilities. Further examples of nonsmooth and nonconvex regularizations have also recently been studied in the context of inverse problems in <ref> [21] </ref> and [22]. The outline of the paper is the following. In Section 2 the generalized proximal point algorithm is introduced for a wide class of possible regularizations. In Section 3, the fixed points of the method are studied.
Reference: [22] <author> M. Nikolova. </author> <title> Local strong homogeneity using a regularized estimator. </title> <type> Internal Report, </type> <institution> UFR Mathematiques et Infor-matiques, Universite Rene Descartes, </institution> <address> Paris 5, </address> <year> 1997. </year>
Reference-contexts: Using different choices for the function OE, some well known multiplier methods can be recovered. Our generalization of OE to nonconvex functions opens up many new possibilities. Further examples of nonsmooth and nonconvex regularizations have also recently been studied in the context of inverse problems in [21] and <ref> [22] </ref>. The outline of the paper is the following. In Section 2 the generalized proximal point algorithm is introduced for a wide class of possible regularizations. In Section 3, the fixed points of the method are studied.
Reference: [23] <author> A. M. Ostrowski. </author> <title> Solution of equations and systems of equations. </title> <publisher> Academic, </publisher> <address> New York, </address> <year> 1966. </year>
Reference-contexts: Chretien and Hero, "Generalized proximal methods : : : " 9 Proof: This result follows directly from the fact that lim k!1 jx k+1 x k j = 0 and from <ref> [23, Theorem 28.1] </ref>. Corollary 1 Suppose, in addition to the assumptions of Theorem 1, that f (x) is strictly convex in an open neighborhood N of an accumulation point x fl of (x k ) k2N .
Reference: [24] <author> R. T. Rockafellar. </author> <title> Augmented lagragians and application of the proximal point algorithm in convex programming. </title> <journal> Mathematics of Operations Research, </journal> <volume> 1 </volume> <pages> 96-116, </pages> <year> 1976. </year>
Reference-contexts: Example 2 (Methods of multipliers) In <ref> [24] </ref> Rockafellar shows that the proximal point approach can be applied to the dual of a constrained optimization problem to yield interesting classes of multiplier methods. Subsequent studies [28, 29, 8] have demonstrated the benefit of using nonquadratic regularization functions.
Reference: [25] <author> R. T. Rockafellar. </author> <title> Monotone operators and the proximal point algorithm. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 14 </volume> <pages> 877-898, </pages> <year> 1976. </year>
Reference-contexts: In the original form <ref> [18, 25] </ref>, the proximal point algorithm is defined by the reccurence x k+1 = argmin y2R n ff (y) + k jx yj 2 g: where ( k ) k2N is a sequence of positive relaxation parameters.
Reference: [26] <author> H. Schramm and J. </author> <title> Zowe. A version of the bundle idea for minimizing a nonsmooth function: conceptual idea, convergence analysis, numerical results. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 2(1) </volume> <pages> 121-152, </pages> <year> 1992. </year> <title> Chretien and Hero, "Generalized proximal methods : : : " 23 </title>
Reference-contexts: f (x): (1) One of the most widely studied methods for solving nondifferentiable optimization problems is the bundle method first proposed by Lemarechal [14] and Wolfe [31] for convex minimization and further developed by Mi*in [19, 20] and Kiwiel [10, 11, 13] for the nonconvex case; see also [2], [17], <ref> [26] </ref> and the references therein. The bundle method can be interpreted as a cutting plane algorithm stabilized by a quadratic penalty or regularization.
Reference: [27] <author> O. Tanrikulu, A. G. Constantinides, and J. A. Chambers. </author> <title> New normalized constant modulus algorithms with relaxation. </title> <journal> IEEE Sig. Proc. Letters, </journal> <volume> 4(9) </volume> <pages> 256-258, </pages> <year> 1997. </year>
Reference-contexts: For odd p or q, the function is nondifferentiable. A popular algorithm for implementing (54) is the well known constant modulus algorithm (CMA) <ref> [6, 30, 27] </ref>. The objective function has the shape represented in Figure 1, for the case n = 1, p = 2, q = 1.
Reference: [28] <author> M. Teboulle. </author> <title> Entropic proximal mappings with application to nonlinear programming. </title> <journal> Mathematics of Operations Research, </journal> <volume> 17 </volume> <pages> 670-690, </pages> <year> 1992. </year>
Reference-contexts: Example 2 (Methods of multipliers) In [24] Rockafellar shows that the proximal point approach can be applied to the dual of a constrained optimization problem to yield interesting classes of multiplier methods. Subsequent studies <ref> [28, 29, 8] </ref> have demonstrated the benefit of using nonquadratic regularization functions. Among the possible choices for regularization functions proposed in [28] is the OEdivergence d OE (x; x 0 ) = j=1 x 0 x j where, in [28], OE was assumed strictly convex. <p> Subsequent studies [28, 29, 8] have demonstrated the benefit of using nonquadratic regularization functions. Among the possible choices for regularization functions proposed in <ref> [28] </ref> is the OEdivergence d OE (x; x 0 ) = j=1 x 0 x j where, in [28], OE was assumed strictly convex. <p> Subsequent studies [28, 29, 8] have demonstrated the benefit of using nonquadratic regularization functions. Among the possible choices for regularization functions proposed in <ref> [28] </ref> is the OEdivergence d OE (x; x 0 ) = j=1 x 0 x j where, in [28], OE was assumed strictly convex. In particular, consider the convex program min x2R n f (x) subject to g i (x) 0; i 2 f1; : : : ; mg (7) where f and g 1 ; : : : ; g m are convex functions.
Reference: [29] <author> M. Teboulle. </author> <title> Convergence of proximal-like algorithms. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 7 </volume> <pages> 1069-1083, </pages> <year> 1997. </year>
Reference-contexts: Example 2 (Methods of multipliers) In [24] Rockafellar shows that the proximal point approach can be applied to the dual of a constrained optimization problem to yield interesting classes of multiplier methods. Subsequent studies <ref> [28, 29, 8] </ref> have demonstrated the benefit of using nonquadratic regularization functions. Among the possible choices for regularization functions proposed in [28] is the OEdivergence d OE (x; x 0 ) = j=1 x 0 x j where, in [28], OE was assumed strictly convex.
Reference: [30] <author> J. R. Treichler and M. G. Larimore. </author> <title> New processing techniques based on the constant modulus adaptive algorithm. </title> <journal> IEEE Trans. ASSP, </journal> <volume> 33 </volume> <pages> 420-431, </pages> <year> 1985. </year>
Reference-contexts: For odd p or q, the function is nondifferentiable. A popular algorithm for implementing (54) is the well known constant modulus algorithm (CMA) <ref> [6, 30, 27] </ref>. The objective function has the shape represented in Figure 1, for the case n = 1, p = 2, q = 1.

References-found: 30

