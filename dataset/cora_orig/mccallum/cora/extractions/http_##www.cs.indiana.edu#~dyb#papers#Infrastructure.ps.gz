URL: http://www.cs.indiana.edu/~dyb/papers/Infrastructure.ps.gz
Refering-URL: http://www.cs.indiana.edu/hyplan/dyb.html
Root-URL: http://www.cs.indiana.edu
Email: burgerrg@sagian.com dyb@cs.indiana.edu  
Title: An Infrastructure for Profile-Driven Dynamic Recompilation  
Phone: Telephone: Int'l. 908-562-3966.  
Author: Hoes Lane P. O. Robert G. Burger R. Kent Dybvig P. O. 
Affiliation: SAGIAN Computer Science Department A Division of Beckman Coulter Indiana University  
Address: Box 1331, Piscataway, NJ 08855-1331, USA.  Box 78668 Lindley Hall 415 Indianapolis, IN 46278 Bloomington, IN 47405  
Note: Copyright Permissions IEEE Service Center 445  
Abstract: c fl 1998 IEEE. Published in the Proceedings of ICCL'98, May 1998, Chicago, Illinois. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works, must be obtained from the IEEE. Contact: Manager, Copyrights and Abstract Dynamic optimization of computer programs can dramatically improve their performance on a variety of applications. This paper presents an efficient infrastructure for dynamic recompilation that can support a wide range of dynamic optimizations including profile-driven optimizations. The infrastructure allows any section of code to be optimized and regenerated on-the-fly, even code for currently active procedures. The infrastructure incorporates a low-overhead edge-count profiling strategy that supports first-class continuations and reinstrumentation of active procedures. Profiling instrumentation can be added and removed dynamically, and the data can be displayed graphically in terms of the original source to provide use ful feedback to the programmer.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Michael Ashley and R. Kent Dybvig. </author> <title> An efficient implementation of multiple return values in Scheme. </title> <booktitle> In Proceedings of the ACM SIGPLAN '94 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 140 149, </pages> <month> June </month> <year> 1994. </year> <month> 9 </month>
Reference-contexts: after the code object has been recompiled and collected. (It would be possible to relax this restriction.) In order to support multiple return values, first-class continuations, and garbage collection efficiently, four words of data are placed in the instruction stream immediately before the single-value return point from each non-tail call <ref> [17, 1, 5] </ref>. The live mask is a bit vector describing which frame locations contain live data. The code pointer is used to find the code object associated with a given return address.
Reference: [2] <author> Thomas Ball and James R. Larus. </author> <title> Optimally profiling and tracing programs. </title> <booktitle> In Proceedings of the 19th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, </booktitle> <pages> pages 5970, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: Since many optimizations benefit from profiling information, we have included support for profiling as an integral part of the recompilation infrastructure. Instrumentation for profiling can be inserted or removed dynamically, again by regenerating code on-the-fly. A variant of Ball and Larus's low-overhead edge-count profiling strategy <ref> [2] </ref>, extended to support first-class continuations and reinstrumentation of active procedures, is used to obtain accurate execution counts for all basic blocks in instrumented code. <p> Section 4 gives some performance data for the profiler and dynamic recompiler. Section 5 describes related work. Section 6 summarizes our results and discusses future work. 2 Edge-Count Profiling This section describes a low-overhead edge-count profiling strategy based on one described by Ball and Larus <ref> [2] </ref>. Like Ball and Larus's, it minimizes the total number of profile counter increments at run time. Unlike Ball and Larus's, it supports first-class continuations and reinstru-mentation of active procedures. Additionally, our strategy employs a fast log-linear algorithm to determine optimal counter placement. <p> The edge from the exit block to the entry block does not correspond to an actual instruction in the procedure, so it cannot be instrumented. Consequently, the maximal spanning tree algorithm is seeded with this edge, and the resulting spanning tree is still maximal <ref> [2] </ref>. &gt; (call/cc (lambda (k) (fact 5 k))) j (fact 5 &lt;proc&gt;) j j (fact 4 &lt;proc&gt;) j j j (fact 3 &lt;proc&gt;) j j j j (fact 2 &lt;proc&gt;) j j j j j (fact 1 &lt;proc&gt;) 1 incorrect counts (in bold) all counts correct without an exit edge: with <p> Another benefit of this algorithm is that it adds uninstru-mented edges to the tree in precisely the reverse order for which their weights need to be computed using the conservation of flow property. As a result, count propagation does not require a separate depth-first search as described in <ref> [2] </ref>. Instead, the maximal spanning tree algorithm generates the list used to propagate the counts quickly and easily. This list is especially important to the garbage collector, which must propagate the counts of recompiled procedures (see Sections 3.1 and 3.3). <p> Consequently, there are two choices for measuring the weights of exit edges. First, we could modify continuation invocation to update the weights directly. Ball and Larus use this technique for exit-only continuations by incrementing the weights of the exit edges associated with each activation that will exit prematurely <ref> [2] </ref>. This approach would support fully general continuations if it would also decrement the weights of the exit edges associated with each activation that would be reinstated. <p> Because the Alpha performs dynamic branch prediction, the 6% reduction in run time is likely due mostly to a reduction in instruction cache misses. 5 Related Work Our edge-count profiling algorithm is based on one described by Ball and Larus <ref> [2] </ref>, who applied their algorithm in a traditional static compilation environment. We have extended their algorithm to support first-class continuations and reinstrumentation of active procedures.
Reference: [3] <author> Anders Bondorf. </author> <note> Similix Manual, System Version 5.0. </note> <institution> DIKU, University of Copenhagen, Denmark, </institution> <year> 1993. </year>
Reference-contexts: Performance To assess both compile-time and run-time performance of the dynamic recompiler and profiler, we used a set of benchmarks comprised of Chez Scheme 5.0g recompiling itself, Soft Scheme [29] checking its pattern matcher, Digital Design Derivation System 1.0 [4] deriving a Scheme CPU [6], Similix 5.0 partially evaluating itself <ref> [3] </ref>, and the Gambit-C 2.3.1 benchmark suite. All measurements were taken on a DEC Alpha 3000/600 running Digital UNIX V4.0A and are reported in detail elsewhere [7]. Initial instrumentation has an average run-time overhead of 50% and an average compile-time overhead of 11%.
Reference: [4] <author> Bhaskar Bose. </author> <title> DDDA transformation system for Digital Design Derivation. </title> <type> Technical Report 331, </type> <institution> Indiana University, Computer Science Department, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: its reloc field changed to point to the translation table. 4 Performance To assess both compile-time and run-time performance of the dynamic recompiler and profiler, we used a set of benchmarks comprised of Chez Scheme 5.0g recompiling itself, Soft Scheme [29] checking its pattern matcher, Digital Design Derivation System 1.0 <ref> [4] </ref> deriving a Scheme CPU [6], Similix 5.0 partially evaluating itself [3], and the Gambit-C 2.3.1 benchmark suite. All measurements were taken on a DEC Alpha 3000/600 running Digital UNIX V4.0A and are reported in detail elsewhere [7].
Reference: [5] <author> Carl Bruggeman, Oscar Waddell, and R. Kent Dybvig. </author> <title> Representing control in the presence of one-shot continuations. </title> <booktitle> In Proceedings of the ACM SIGPLAN '96 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 99107, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Second, we could seed the maximal spanning tree algorithm with all the exit edges. The resulting spanning tree might not be maximal, but it would be maximal among spanning trees that include all the exit edges. Our system's segmented stack implementation supports constant-time continuation invocation <ref> [17, 5] </ref>. Implementing the first approach would destroy this property. Moreover, if any procedure is profiled, the system must traverse all activations to be sure it finds all profiled ones. <p> after the code object has been recompiled and collected. (It would be possible to relax this restriction.) In order to support multiple return values, first-class continuations, and garbage collection efficiently, four words of data are placed in the instruction stream immediately before the single-value return point from each non-tail call <ref> [17, 1, 5] </ref>. The live mask is a bit vector describing which frame locations contain live data. The code pointer is used to find the code object associated with a given return address.
Reference: [6] <author> Robert G. Burger. </author> <title> The Scheme Machine. </title> <type> Technical Report 413, </type> <institution> Indiana University, Computer Science Department, </institution> <month> August </month> <year> 1994. </year>
Reference-contexts: point to the translation table. 4 Performance To assess both compile-time and run-time performance of the dynamic recompiler and profiler, we used a set of benchmarks comprised of Chez Scheme 5.0g recompiling itself, Soft Scheme [29] checking its pattern matcher, Digital Design Derivation System 1.0 [4] deriving a Scheme CPU <ref> [6] </ref>, Similix 5.0 partially evaluating itself [3], and the Gambit-C 2.3.1 benchmark suite. All measurements were taken on a DEC Alpha 3000/600 running Digital UNIX V4.0A and are reported in detail elsewhere [7]. Initial instrumentation has an average run-time overhead of 50% and an average compile-time overhead of 11%.
Reference: [7] <author> Robert G. Burger. </author> <title> Efficient Compilation and Profile-Driven Dynamic Recompilation in Scheme. </title> <type> PhD thesis, </type> <institution> Indiana University, </institution> <month> March </month> <year> 1997. </year>
Reference-contexts: All measurements were taken on a DEC Alpha 3000/600 running Digital UNIX V4.0A and are reported in detail elsewhere <ref> [7] </ref>. Initial instrumentation has an average run-time overhead of 50% and an average compile-time overhead of 11%. Without block reordering, optimal count placement reduces the average run-time overhead to 37%, and the average recompile time is only 15% of the base compile time.
Reference: [8] <author> Chaig Chambers, Susan J. Eggers, Joel Auslander, Matthai Philipose, Markus Mock, and Przemyslaw Pardyak. </author> <title> Automatic dynamic compilation support for event dispatching in extensible systems. </title> <booktitle> In WCSSS'96 Workshop on Compiler Support for System Software, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: Recent research has shown that dynamic compilation can dramatically improve the performance of a wide range of applications including network packet demultiplexing, sparse matrix computations, pattern matching, and mobile code <ref> [10, 8, 13, 16, 22, 23] </ref>. Dynamic optimization works when a program is staged in such a way that the cost of dynamic recompilation can be amortized over many runs of the optimized code [21]. <p> Since determining the optimal ordering is NP-complete, they use a greedy closest is best strategy. Several recent research projects have focused on lightweight run-time code generation <ref> [8, 15, 22, 20, 25] </ref>, with code generation costs on the order of five to 100 instructions executed for each instruction generated. Although the overhead inherent in our model is greater, the potential benefits are greater as well.
Reference: [9] <author> Craig Chambers. </author> <title> The Design and Implementation of the SELF Compiler, an Optimizing Compiler for Object-Oriented Programming Languages. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> April </month> <year> 1992. </year>
Reference-contexts: Although the overhead inherent in our model is greater, the potential benefits are greater as well. Little attention has previously been paid to profile-driven dynamic optimizations, with the notable exception of work on Self <ref> [9, 18] </ref>. This work uses a specialized form of profiling to guide generation of special-purpose code to avoid generic dispatch overhead.
Reference: [10] <author> Craig Chambers and David Ungar. </author> <title> Customization: Optimizing compiler technology for SELF, a dynamically-typed object-oriented programming language. </title> <booktitle> In Proceedings of the ACM SIGPLAN '89 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 146160, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: Recent research has shown that dynamic compilation can dramatically improve the performance of a wide range of applications including network packet demultiplexing, sparse matrix computations, pattern matching, and mobile code <ref> [10, 8, 13, 16, 22, 23] </ref>. Dynamic optimization works when a program is staged in such a way that the cost of dynamic recompilation can be amortized over many runs of the optimized code [21].
Reference: [11] <editor> William Clinger and Jonathan Rees (editors). </editor> <title> Revised 4 report on the algorithmic language Scheme. LISP Pointers, </title> <address> IV(3):155, </address> <month> JulySeptember </month> <year> 1991. </year>
Reference-contexts: Even when there are no continuations, reinstrumenting a procedure while it has activations on the stack is problematic because some of its calls have not returned yet. illustrates the effects of nonlocal exit and re-entry using Scheme's call-with-current-continuation (call/cc) function <ref> [11] </ref>. Ball and Larus handle the restricted class of exit-only continuations, e.g., setjmp/longjmp in C, by adding to each call block an edge pointing to the exit block. The weight of an exit edge represents the number of times its associated call exits prematurely.
Reference: [12] <author> William D. Clinger and Lars Thomas Hansen. </author> <title> Lambda, the ultimate label, or a simple optimizing compiler for Scheme. </title> <booktitle> In Proceedings of the 1994 ACM Conference on LISP and Functional Programming, </booktitle> <pages> pages 128139, </pages> <year> 1994. </year>
Reference-contexts: For example, profile data can be used to measure how many times a procedure is called versus how many times it is created, and this ratio could be used to guide lambda lifting <ref> [12] </ref>. Combined with an estimate of the cost of generating code for the procedure, this ratio could also help determine when run-time code generation [22] would be profitable. Our system can also be extended to recompile (specialize) a closure based on the run-time values of its free variables.
Reference: [13] <author> L. Peter Deutsch and Allan M. Schiffman. </author> <title> Efficient implementation of the Smalltalk80 system. </title> <booktitle> In Proceedings of the 11th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 297302, </pages> <month> January </month> <year> 1984. </year>
Reference-contexts: Recent research has shown that dynamic compilation can dramatically improve the performance of a wide range of applications including network packet demultiplexing, sparse matrix computations, pattern matching, and mobile code <ref> [10, 8, 13, 16, 22, 23] </ref>. Dynamic optimization works when a program is staged in such a way that the cost of dynamic recompilation can be amortized over many runs of the optimized code [21].
Reference: [14] <author> R. Kent Dybvig, David Eby, and Carl Bruggeman. </author> <title> Don't stop the BIBOP: Flexible and efficient storage management for dynamically typed languages. </title> <type> Technical Report 400, </type> <institution> Indiana University, Computer Science Department, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: Since our collector is generational, we must address the problem of potential cross-generational pointers from obsolete to new code objects. Our segmented heap model allows us to allocate new objects in older generations when necessary <ref> [14] </ref>; thus, we always allocate new code objects in the same generation as the corresponding obsolete code objects.
Reference: [15] <author> Dawson R. Engler. </author> <title> VCODE: A retargetable, extensible, very fast dynamic code generation system. </title> <booktitle> In Proceedings of the ACM SIGPLAN '96 Conference on Programming Language Design and Implementation, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: Since determining the optimal ordering is NP-complete, they use a greedy closest is best strategy. Several recent research projects have focused on lightweight run-time code generation <ref> [8, 15, 22, 20, 25] </ref>, with code generation costs on the order of five to 100 instructions executed for each instruction generated. Although the overhead inherent in our model is greater, the potential benefits are greater as well.
Reference: [16] <author> Dawson R. Engler, Deborah Wallach, and M. Frans Kaashoek. </author> <title> Efficient, safe, application-specific message processing. </title> <type> Technical Memorandum MIT/LCS/TM533, </type> <institution> MIT Laboratory for Computer Science, </institution> <month> March </month> <year> 1995. </year>
Reference-contexts: Recent research has shown that dynamic compilation can dramatically improve the performance of a wide range of applications including network packet demultiplexing, sparse matrix computations, pattern matching, and mobile code <ref> [10, 8, 13, 16, 22, 23] </ref>. Dynamic optimization works when a program is staged in such a way that the cost of dynamic recompilation can be amortized over many runs of the optimized code [21].
Reference: [17] <author> Robert Hieb, R. Kent Dybvig, and Carl Bruggeman. </author> <title> Representing control in the presence of first-class continuations. </title> <booktitle> In Proceedings of the ACM SIGPLAN '90 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 6677, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Second, we could seed the maximal spanning tree algorithm with all the exit edges. The resulting spanning tree might not be maximal, but it would be maximal among spanning trees that include all the exit edges. Our system's segmented stack implementation supports constant-time continuation invocation <ref> [17, 5] </ref>. Implementing the first approach would destroy this property. Moreover, if any procedure is profiled, the system must traverse all activations to be sure it finds all profiled ones. <p> after the code object has been recompiled and collected. (It would be possible to relax this restriction.) In order to support multiple return values, first-class continuations, and garbage collection efficiently, four words of data are placed in the instruction stream immediately before the single-value return point from each non-tail call <ref> [17, 1, 5] </ref>. The live mask is a bit vector describing which frame locations contain live data. The code pointer is used to find the code object associated with a given return address.
Reference: [18] <author> Urs Holze and David Ungar. </author> <title> Optimizing dynamically-dispatched calls with run-time type feedback. </title> <booktitle> In Proceedings of the ACM SIGPLAN '94 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 326 335, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Although the overhead inherent in our model is greater, the potential benefits are greater as well. Little attention has previously been paid to profile-driven dynamic optimizations, with the notable exception of work on Self <ref> [9, 18] </ref>. This work uses a specialized form of profiling to guide generation of special-purpose code to avoid generic dispatch overhead.
Reference: [19] <author> David Keppel, Susan J. Eggers, and Robert R. Henry. </author> <title> Evaluating runtime-compiled value-specific optimizations. </title> <type> Technical Report 93-11-02, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <month> Novem-ber </month> <year> 1993. </year>
Reference-contexts: Keppel has investigated the application of run-time code generation to value-specific optimizations, in which code is special-cased to particular input values <ref> [19] </ref>. Although we have so far focused on profile-driven optimizations, we believe that our infrastructure is well suited to value-specific optimizations as well. 6 Conclusions We have described an efficient infrastructure for dynamic recompilation that incorporates a low-overhead edge-count profiling strategy supporting first-class continuations and reinstrumentation of active procedures.
Reference: [20] <author> Mark Leone. </author> <title> A Principled and Practical Approach to Run-Time Code Generation. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <year> 1997. </year> <note> In preparation. </note>
Reference-contexts: Since determining the optimal ordering is NP-complete, they use a greedy closest is best strategy. Several recent research projects have focused on lightweight run-time code generation <ref> [8, 15, 22, 20, 25] </ref>, with code generation costs on the order of five to 100 instructions executed for each instruction generated. Although the overhead inherent in our model is greater, the potential benefits are greater as well.
Reference: [21] <author> Mark Leone and R. Kent Dybvig. Dynamo: </author> <title> A staged compiler architecture for dynamic program optimization. </title> <type> Technical Report 490, </type> <institution> Indiana University, Computer Science Department, </institution> <month> September </month> <year> 1997. </year>
Reference-contexts: Dynamic optimization works when a program is staged in such a way that the cost of dynamic recompilation can be amortized over many runs of the optimized code <ref> [21] </ref>. This paper presents an infrastructure for dynamic re compilation of computer programs that permits optimiza fl This material is based on work supported in part by the National Science Foundation under grant numbers CDA-9312614 and CCR-9711269. Robert G.
Reference: [22] <author> Mark Leone and Peter Lee. </author> <title> Optimizing ML with run-time code generation. </title> <booktitle> In Proceedings of the ACM SIGPLAN '96 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 137148, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Recent research has shown that dynamic compilation can dramatically improve the performance of a wide range of applications including network packet demultiplexing, sparse matrix computations, pattern matching, and mobile code <ref> [10, 8, 13, 16, 22, 23] </ref>. Dynamic optimization works when a program is staged in such a way that the cost of dynamic recompilation can be amortized over many runs of the optimized code [21]. <p> Since determining the optimal ordering is NP-complete, they use a greedy closest is best strategy. Several recent research projects have focused on lightweight run-time code generation <ref> [8, 15, 22, 20, 25] </ref>, with code generation costs on the order of five to 100 instructions executed for each instruction generated. Although the overhead inherent in our model is greater, the potential benefits are greater as well. <p> Combined with an estimate of the cost of generating code for the procedure, this ratio could also help determine when run-time code generation <ref> [22] </ref> would be profitable. Our system can also be extended to recompile (specialize) a closure based on the run-time values of its free variables.
Reference: [23] <author> Henry Massalin. </author> <title> Synthesis: An Efficient Implementation of Fundamental Operating System Services. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Columbia University, </institution> <year> 1992. </year>
Reference-contexts: Recent research has shown that dynamic compilation can dramatically improve the performance of a wide range of applications including network packet demultiplexing, sparse matrix computations, pattern matching, and mobile code <ref> [10, 8, 13, 16, 22, 23] </ref>. Dynamic optimization works when a program is staged in such a way that the cost of dynamic recompilation can be amortized over many runs of the optimized code [21].
Reference: [24] <author> Karl Pettis and Robert C. Hansen. </author> <title> Profile guided code positioning. </title> <booktitle> In Proceedings of the ACM SIGPLAN '90 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 1627, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: As a proof of concept, we have used the recompilation infrastructure and profiling information to support run-time reordering of basic blocks to reduce the number mis-predicted branches and instruction cache misses, using a variant of Pettis and Hansen's basic-block reordering algorithm <ref> [24] </ref>. The mechanisms described in this paper are directly applicable to garbage-collected languages such as Java, ML, Scheme, and Smalltalk in which all references to a proce-dure may be found and relocated at run time. <p> Section 3.3 describes how the garbage collector uses the modified representation to replace code objects with recompiled ones. Section 3.4 presents the recompilation process, which uses a variant of Pettis and Hansen's basic-block reordering algorithm to reduce the number of mispredicted branches and instruction cache misses <ref> [24] </ref>. 3.1 Overview Dynamic recompilation proceeds in three phases. First, the candidate procedures are identified, either by the user or by a program that selects among all the procedures in the heap. Second, these procedures are recompiled and linked to separate, new procedures. <p> We use a variant of Pettis and Hansen's basic-block reordering algorithm to reduce the number of mispredicted branches and instruction cache misses <ref> [24] </ref>. We also use the edge-count profile data to decrease profiling overhead by re-running the maximal spanning tree algorithm to improve counter placement. The block reordering algorithm proceeds in two steps. <p> We have used Pettis and Hansen's intraprocedural block-reordering strategy with only minor modifications to apply it in the context of dynamic recompilation. Samples [26] explores a similar intraprocedural algorithm that reduces instruction cache miss rates by up to 50%. Pettis and Hansen <ref> [24] </ref> also describe an interprocedural algorithm that places procedures in memory such that those that execute close together in time will also be close together in memory. Since determining the optimal ordering is NP-complete, they use a greedy closest is best strategy.
Reference: [25] <author> Massimiliano Poletto, Dawson R. Engler, and M. Frans Kaashoek. tcc: </author> <title> A template-based compiler for `C. </title> <booktitle> In WCSSS'96 Workshop on Compiler Support for System Software, </booktitle> <pages> pages 17, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: Since determining the optimal ordering is NP-complete, they use a greedy closest is best strategy. Several recent research projects have focused on lightweight run-time code generation <ref> [8, 15, 22, 20, 25] </ref>, with code generation costs on the order of five to 100 instructions executed for each instruction generated. Although the overhead inherent in our model is greater, the potential benefits are greater as well.
Reference: [26] <author> A. </author> <title> Dain Samples. Profile-driven compilation. </title> <type> Technical Report 627, </type> <institution> University of California, Berkeley, </institution> <year> 1991. </year>
Reference-contexts: We have also identified an algorithm for determining optimal counter placement that is faster and eliminates the need for a separate depth-first search for count propagation. We have used Pettis and Hansen's intraprocedural block-reordering strategy with only minor modifications to apply it in the context of dynamic recompilation. Samples <ref> [26] </ref> explores a similar intraprocedural algorithm that reduces instruction cache miss rates by up to 50%. Pettis and Hansen [24] also describe an interprocedural algorithm that places procedures in memory such that those that execute close together in time will also be close together in memory.
Reference: [27] <author> Robert Sedgewick. </author> <title> Algorithms, chapter 31. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> second edition, </address> <year> 1988. </year>
Reference-contexts: Finally, the compiler inserts counter increments into the generated code, placing them into existing blocks whenever possible. For efficiency, our compiler uses the priority-first search algorithm for finding a maximal spanning tree <ref> [27] </ref>. Its worst-case behavior is O ((E + B) log B), where B is the number of blocks and E is the number of edges. Since each block has no more than two outgoing edges, E is O (B).
Reference: [28] <author> Richard L. </author> <title> Sites, editor. Alpha Architecture Reference Manual. </title> <publisher> Digital Press, </publisher> <year> 1992. </year>
Reference-contexts: To assess the effectiveness of the block-reordering algorithm, we measured the number of mispredicted conditional branches and the number of unconditional branches. The Alpha architecture encourages hardware and compiler implementors to predict backward conditional branches taken and forward conditional branches not taken <ref> [28] </ref>. Current Alpha implementations use this static model as a starting point for dynamic branch prediction. We computed the mispredicted branch percentage using the static model as a metric for determining the effectiveness of the block-reordering algorithm.
Reference: [29] <author> Andrew K. Wright and Robert Cartwright. </author> <title> A practical soft type system for Scheme. </title> <booktitle> In Proceedings of the 1994 ACM Conference on LISP and Functional Programming, </booktitle> <pages> pages 250262, </pages> <year> 1994. </year>
Reference-contexts: and the old code object is marked obsolete and has its reloc field changed to point to the translation table. 4 Performance To assess both compile-time and run-time performance of the dynamic recompiler and profiler, we used a set of benchmarks comprised of Chez Scheme 5.0g recompiling itself, Soft Scheme <ref> [29] </ref> checking its pattern matcher, Digital Design Derivation System 1.0 [4] deriving a Scheme CPU [6], Similix 5.0 partially evaluating itself [3], and the Gambit-C 2.3.1 benchmark suite. All measurements were taken on a DEC Alpha 3000/600 running Digital UNIX V4.0A and are reported in detail elsewhere [7].
References-found: 29

