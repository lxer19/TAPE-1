URL: http://www.cs.utah.edu/~cs686/Previous/s96/SOSP95-hive.ps
Refering-URL: http://www.cs.utah.edu/~cs686/Previous/s96/
Root-URL: 
Abstract: Reliability and scalability are major concerns when designing operating systems for large-scale shared-memory multiprocessors. In this paper we describe Hive, an operating system with a novel kernel architecture that addresses these issues. Hive is structured as an internal distributed system of independent kernels called cells. This improves reliability because a hardware or software fault damages only one cell rather than the whole system, and improves scalability because few kernel resources are shared by processes running on different cells. The Hive prototype is a complete implementation of UNIX SVR4 and is targeted to run on the Stanford FLASH multiprocessor. This paper focuses on Hives solution to the following key challenges: (1) fault containment, i.e. confining the effects of hardware or software faults to the cell where they occur, and (2) memory sharing among cells, which is required to achieve application performance competitive with other multiprocessor operating systems. Fault containment in a shared-memory multiprocessor requires defending each cell against erroneous writes caused by faults in other cells. Hive prevents such damage by using the FLASH firewall, a write permission bit-vector associated with each page of memory, and by discarding potentially corrupt pages when a fault is detected. Memory sharing is provided through a unified file and virtual memory page cache across the cells, and through a unified free page frame pool. We report early experience with the system, including the results of fault injection and performance experiments using SimOS, an accurate simulator of FLASH. The effects of faults were contained to the cell in which they occurred in all 49 tests where we injected fail-stop hardware faults, and in all 20 tests where we injected kernel data corruption. The Hive prototype executes test workloads on a four-processor four-cell system with between 0% and 11% slowdown as compared to SGI IRIX 5.2 (the version of UNIX on which it is based). 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. Anderson, D. Culler, and D. Patterson. </author> <title> A Case for NOW (Networks of Workstations). </title> <booktitle> IEEE Micro 15(1) </booktitle> <pages> 54-64, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: The NOW project at U.C. Berkeley is studying how to couple a cluster of workstations more tightly for better resource sharing <ref> [1] </ref>. The hardware they assume for a NOW environment does not provide shared memory, so they do not face the challenge of wild writes or the opportunity of directly accessing remote memory.
Reference: [2] <author> J. Bartlett, J. Gray, and B. Horst. </author> <title> Fault Tolerance in Tandem Computer Systems. </title> <booktitle> In Evolution of Fault-Tolerant Computing, </booktitle> <pages> pp. 55-76, </pages> <publisher> Springer-Verlag, </publisher> <year> 1987. </year>
Reference-contexts: New microkernels such as the Exokernel [6] and the Cache Kernel [4] may be small enough to provide reliability. An alternative reliability strategy would be to use traditional fault-tolerant operating system implementation techniques. Previous systems such as Tandem Guardian <ref> [2] </ref> provide a much stronger reliability guarantee than fault containment. However, full fault tolerance requires replication of computation, so it uses the available hardware resources inefficiently. While this is appropriate when supporting applications that cannot tolerate partial failures, it is not acceptable for performance-oriented and cost-sensitive multiprocessor environments.
Reference: [3] <author> D. Chaiken and A. Agarwal. </author> <title> Software-Extended Coherent Shared Memory: Performance and Cost. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 314-324, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: for small-scale machines are difficult to scale to Hive: Fault Containment for Shared-Memory Multiprocessors John Chapin, Mendel Rosenblum, Scott Devine, Tirthankar Lahiri, Dan Teodosiu, and Anoop Gupta Computer Systems Laboratory Stanford University, Stanford CA 94305 http://www-flash.stanford.edu the large shared-memory multiprocessors that can now be built (Stanford DASH [11], MIT Alewife <ref> [3] </ref>, Convex Exemplar [5]). In this paper we describe Hive, an operating system designed for large-scale shared-memory multiprocessors. Hive is fundamentally different from previous monolithic and microkernel SMP OS implementations: it is structured as an internal distributed system of independent kernels called cells.
Reference: [4] <author> D. Cheriton and K. Duda. </author> <title> A Caching Model of Operating System Kernel Functionality. </title> <booktitle> In Proceedings of the First USENIX Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pp. 179-193, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: A microkernel could support a distributed system like Hive and prevent wild writes, as discussed in Section 2. However, existing microkernels such as Mach [15] are large and complex enough that it is difficult to trust their correctness. New microkernels such as the Exokernel [6] and the Cache Kernel <ref> [4] </ref> may be small enough to provide reliability. An alternative reliability strategy would be to use traditional fault-tolerant operating system implementation techniques. Previous systems such as Tandem Guardian [2] provide a much stronger reliability guarantee than fault containment.
Reference: [5] <author> Convex Computer Corporation. </author> <title> Convex Exemplar: System Overview. Order No. </title> <address> 080-002293-000, </address> <year> 1994. </year>
Reference-contexts: are difficult to scale to Hive: Fault Containment for Shared-Memory Multiprocessors John Chapin, Mendel Rosenblum, Scott Devine, Tirthankar Lahiri, Dan Teodosiu, and Anoop Gupta Computer Systems Laboratory Stanford University, Stanford CA 94305 http://www-flash.stanford.edu the large shared-memory multiprocessors that can now be built (Stanford DASH [11], MIT Alewife [3], Convex Exemplar <ref> [5] </ref>). In this paper we describe Hive, an operating system designed for large-scale shared-memory multiprocessors. Hive is fundamentally different from previous monolithic and microkernel SMP OS implementations: it is structured as an internal distributed system of independent kernels called cells.
Reference: [6] <author> D. Engler, M.F. Kaashoek, and J. OToole Jr. Exokernel: </author> <title> An Operating System Architecture For Application-Level Resource Management. </title> <booktitle> In Proceedings of the Fifteenth ACM Symposium on Operating Systems Principles, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: A microkernel could support a distributed system like Hive and prevent wild writes, as discussed in Section 2. However, existing microkernels such as Mach [15] are large and complex enough that it is difficult to trust their correctness. New microkernels such as the Exokernel <ref> [6] </ref> and the Cache Kernel [4] may be small enough to provide reliability. An alternative reliability strategy would be to use traditional fault-tolerant operating system implementation techniques. Previous systems such as Tandem Guardian [2] provide a much stronger reliability guarantee than fault containment.
Reference: [7] <author> J. Gray and A. Reuter. </author> <title> Transaction Processing: Concepts and Techniques. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: Software faults: The presence of shared memory makes each cell vulnerable to wild writes resulting from software faults in other cells. Wild writes are not a negligible problem. Studies have shown that software faults are more common than hardware faults in current systems <ref> [7] </ref>. When a software fault occurs, a wild write can easily follow.
Reference: [8] <author> J. Heinlein, K. Gharachorloo, S. Dresser, and A. Gupta. </author> <title> Integration of Message Passing and Shared Memory in the Stanford FLASH Multiprocessor. </title> <booktitle> In Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 38-50, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Separate receive queues are provided on each node for request and reply messages, making deadlock avoidance easy. An early version of the message send primitive is described in detail in <ref> [8] </ref>. The Hive RPC subsystem built on top of SIPS is much leaner than the ones in previous distributed systems. No retransmission or duplicate suppression is required because the primitive is reliable.
Reference: [9] <author> D. Kotz, S. Toh, and S. Radhakrishnan. </author> <title> A Detailed Simulation of the HP 97560 Disk Drive. </title> <type> Technical Report PCS-TR94-20, </type> <institution> Dartmouth University, </institution> <year> 1994. </year>
Reference-contexts: An interprocessor interrupt (IPI) is delivered 700 ns after it is requested, while a SIPS message requires an IPI latency plus 300 ns when the receiving processor accesses the data. Disk latency is computed for each access using an experimentally-validated model of an HP 97560 disk drive <ref> [9] </ref>. SimOS models both DMA latency and the memory controller occupancy required to transfer data from the disk controller to main memory. There are two inaccuracies in the machine model that affect our performance numbers.
Reference: [10] <author> J. Kuskin, D. Ofelt, M. Heinrich, J. Heinlein, R. Simoni, K. Gharac-horloo, J. Chapin, D. Nakahira, J. Baxter, M. Horowitz, A. Gupta, M. Rosenblum, and J. Hennessy. </author> <title> The Stanford FLASH Multiprocessor. </title> <booktitle> In Proceedings of the 21st International Symposium on Computer Architecture, </booktitle> <pages> pp. 302-313, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: In this paper, we focus on Hives solution to the fault containment problem and on its solution to a key resource sharing problem, sharing memory across cell boundaries. The solutions rely on hardware as well as software mechanisms: we have designed Hive in conjunction with the Stanford FLASH multiprocessor <ref> [10] </ref>, which has enabled us to add hardware support in a few critical areas. Hives fault containment strategy has three main components. Each cell uses firewall hardware provided by FLASH to defend most of its memory pages against wild writes.
Reference: [11] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. Lam. </author> <title> The Stanford DASH Multiprocessor. </title> <booktitle> Computer 25(3) </booktitle> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: OS) commonly used for small-scale machines are difficult to scale to Hive: Fault Containment for Shared-Memory Multiprocessors John Chapin, Mendel Rosenblum, Scott Devine, Tirthankar Lahiri, Dan Teodosiu, and Anoop Gupta Computer Systems Laboratory Stanford University, Stanford CA 94305 http://www-flash.stanford.edu the large shared-memory multiprocessors that can now be built (Stanford DASH <ref> [11] </ref>, MIT Alewife [3], Convex Exemplar [5]). In this paper we describe Hive, an operating system designed for large-scale shared-memory multiprocessors. Hive is fundamentally different from previous monolithic and microkernel SMP OS implementations: it is structured as an internal distributed system of independent kernels called cells.
Reference: [12] <author> A. Nowatzyk, G. Aybay, M. Browne, E. Kelly, D. Lee, and M. Park. </author> <title> The S3.mp Scalable Shared Memory Multiprocessor. </title> <booktitle> In Proceedings of 27th Hawaii International Conference on Systems Sciences, </booktitle> <pages> pp. 144-153, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: Different cells can even run different kernel code if their resource management mechanisms are incompatible or the machines hardware is heterogenous. Support for CC-NOW: Researchers have proposed workstation add-on cards that will provide cache-coherent shared memory across local-area networks <ref> [12] </ref>. Also, the FLASH architecture may eventually be distributed to multiple desktops. Both approaches would create a cache-coherent network of workstations (CC-NOW). The goal of a CC-NOW is a system with the fault isolation and administrative independence characteristic of a workstation cluster, but the resource sharing characteristic of a multiprocessor.
Reference: [13] <author> J. Ousterhout, A. Cherenson, F. Douglis, M. Nelson, and B. Welch. </author> <title> The Sprite Network Operating System. </title> <booktitle> Computer 21(2) </booktitle> <pages> 23-36, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: However, much of their work is directly applicable to improving the resource management policies of a system like Hive. The internal distributed system of Hive requires it to synthesize a single-system image from multiple kernels. The single-system image problem has been studied in depth by other researchers (Sprite <ref> [13] </ref>, Locus [14], OSF/1 AD TNC [23]). Hive reuses some of the techniques developed in Sprite and Locus. 10 Concluding Remarks Fault containment is a key technique that will improve the reliability of large-scale shared-memory multiprocessors used as general-purpose compute servers.
Reference: [14] <editor> G. Popek and B. Walker (eds.). </editor> <title> The LOCUS Distributed System Architecture. </title> <publisher> MIT Press, </publisher> <year> 1985. </year>
Reference-contexts: The internal distributed system of Hive requires it to synthesize a single-system image from multiple kernels. The single-system image problem has been studied in depth by other researchers (Sprite [13], Locus <ref> [14] </ref>, OSF/1 AD TNC [23]). Hive reuses some of the techniques developed in Sprite and Locus. 10 Concluding Remarks Fault containment is a key technique that will improve the reliability of large-scale shared-memory multiprocessors used as general-purpose compute servers.
Reference: [15] <author> R. Rashid, A. Tevanian, Jr., M. Young, D. Golub, R. Baron, D. Black, W. Bolosky, and J. Chew. </author> <title> Machine-Independent Virtual Memory Management for Paged Uniprocessor and Multiprocessor Architectures. </title> <journal> IEEE Transactions on Computers 37(8) </journal> <pages> 896-908, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: The interesting difference is the mechanism for finding the requested page when a process takes a page fault. In IRIX, anonymous pages are managed in copy-on-write trees, similar to the MACH approach <ref> [15] </ref>. An anonymous page is allocated when a process writes to a page of its address space that is shared copy-on-write with its parent. The new page is recorded at the current leaf of the copy-on-write tree. <p> Reliability is one of the goals of microkernel research. A microkernel could support a distributed system like Hive and prevent wild writes, as discussed in Section 2. However, existing microkernels such as Mach <ref> [15] </ref> are large and complex enough that it is difficult to trust their correctness. New microkernels such as the Exokernel [6] and the Cache Kernel [4] may be small enough to provide reliability. An alternative reliability strategy would be to use traditional fault-tolerant operating system implementation techniques.
Reference: [16] <author> A. Ricciardi and K. Birman. </author> <title> Using Process Groups To Implement Failure Detection in Asynchronous Environments. </title> <booktitle> In Proceedings of the Tenth Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pp. 341-353, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: The distributed agreement algorithm is an instance of the well-studied group membership problem, so Hive will use a standard algorithm (probably <ref> [16] </ref>). This is not implemented yet and is simulated by an oracle for the experiments reported in this paper. Recovery algorithms: Given consensus on the live set of cells, each cell runs recovery algorithms to clean up dangling references and determine which processes must be killed.
Reference: [17] <author> M. Rosenblum, E. Bugnion, S. Herrod, E. Witchel, and A. Gupta. </author> <title> The Impact of Architectural Trends on Operating System Performance. </title> <booktitle> In Proceedings of the Fifteenth ACM Symposium on Operating Systems Principles, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: We believe that this environment is a good way to develop an operating system that requires hardware features not available on current machines. It is also an excellent performance measurement and debugging environment <ref> [17] </ref>. 7.2 Simulated machine We simulate a machine similar in performance to an SGI Challenge multiprocessor, with four 200-MHz MIPS R4000-class processors, 128 MB of memory, four disk controllers each with one attached disk, four ethernet interfaces, and four consoles.
Reference: [18] <author> M. Rosenblum, S. Herrod, E. Witchel, and A. Gupta. </author> <title> Fast and Accurate Multiprocessor Simulation: The SimOS Approach. </title> <booktitle> IEEE Parallel and Distributed Technology 3(4), </booktitle> <month> Fall </month> <year> 1995. </year>
Reference-contexts: FLASH makes memory sharing safe by providing timeouts and checks on memory accesses. The current prototype of Hive is based on and remains binary compatible with IRIX 5.2 (a version of UNIX SVR4 from Silicon Graphics, Inc.). Because FLASH is not available yet, we used the SimOS hardware simulator <ref> [18] </ref> to develop and test Hive. Our early experiments using SimOS demonstrate that: Hive can survive the halt of a processor or the failure of a range of memory. <p> In physical-level sharing, one cell transfers control over a page frame to another. One page might be shared in both ways at the same time. fully-implemented system will perform as well as previous UNIX kernels. The performance measurements reported in the following sections were obtained using SimOS <ref> [18] </ref>. We model a machine similar in performance to an SGI Challenge multiprocessor with four 200-MHz MIPS R4000 processors and a 700 nanosecond main memory access latency. We use two types of workloads, characteristic of the two environments we expect to be most common for Hive. <p> First we describe SimOS and the machine model used for our experiments in more detail. Next we present the results of performance experiments, fail-stop hardware fault experiments, and software fault experiments. 7.1 SimOS environment SimOS <ref> [18] </ref> is a complete machine simulator detailed enough to provide an accurate model of the FLASH hardware. It can also run in a less-accurate mode where it is fast enough (on an SGI Challenge) to boot the operating system quickly and execute interactive applications in real time.
Reference: [19] <author> M. Sullivan and M. Stonebraker. </author> <title> Improving Software Fault Tolerance in Highly Available Database Systems. </title> <institution> Technical reports UCB/ERL M90/11, University of California, Berkeley, 1990, and UCB/ERL M91/56, </institution> <year> 1991. </year>
Reference-contexts: We know of no other operating systems that try to contain the effects of wild writes without giving up standard multiprocessor resource sharing. Sullivan and Stonebraker considered the problem in the context of database implementations <ref> [19] </ref>, but the strategies they used are focused on a transactional environment and thus are not directly applicable to a standard commercial operating system. Reliability is one of the goals of microkernel research.
Reference: [20] <author> M. Sullivan and R. Chillarege. </author> <title> Software Defects and Their Impact on System AvailabilityA Study of Field Failures in Operating Systems. </title> <booktitle> In Proceedings of the 21st International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pp. 2-9, </pages> <month> June </month> <year> 1991. </year> <month> 15 </month>
Reference-contexts: When a software fault occurs, a wild write can easily follow. One study found that among 3000 severe bugs reported in IBM operating systems over a five-year period, between 15 and 25 percent caused wild writes <ref> [20] </ref>. 3 Unfortunately, existing shared-memory multiprocessors do not provide a mechanism to prevent wild writes. The only mechanism that can halt a write request is the virtual address translation hardware present in each processor, which is under the control of the very software whose faults must be protected against.
Reference: [21] <author> R. Unrau, O. Krieger, B. Gamsa, and M. Stumm. </author> <title> Hierarchical Clustering: A Structure for Scalable Multiprocessor Operating System Design. </title> <journal> Journal of Supercomputing </journal> 9(1/2):105-134, March 1995. 
Reference-contexts: Another way to look at Hive is as a distributed system where memory and other resources are freely shared between the kernels. This approach to achieving scalability in a multiprocessor operating system has been previously explored by the Hurricane 14 project <ref> [21] </ref>. Although Hurricane is a microkernel that does not implement full SMP OS functionality or fault containment, and does not use shared memory between the separate kernels, its implementation strategies are close to those developed independently in Hive. The NOW project at U.C.
Reference: [22] <author> S. Woo, M. Ohara, E. Torrie, J.P. Singh, and A. Gupta. </author> <title> The SPLASH-2 Programs: Characterization and Methodological Considerations. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 24-36, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: These workloads are characteristic of the two ways we expect Hive to be used. Raytrace and ocean (taken from the Splash-2 suite <ref> [22] </ref>) are parallel scientific applications that use the system in ways characteristic of supercomputer environments. Pmake (parallel make) is characteristic of use as a multi-programmed compute server. In all cases the file cache was warmed up before running the workloads.
Reference: [23] <author> R. Zajcew, P. Roy, D. Black, C. Peak, P. Guedes, B. Kemp, J. Lo Verso, M. Leibensperger, M. Barnett, F. Rabii, and D. Netterwala. </author> <title> An OSF/1 Unix for Massively Parallel Multicomputers. </title> <booktitle> In Proceedings of the Winter 1993 USENIX Conference, </booktitle> <pages> pp. 449-468, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: The internal distributed system of Hive requires it to synthesize a single-system image from multiple kernels. The single-system image problem has been studied in depth by other researchers (Sprite [13], Locus [14], OSF/1 AD TNC <ref> [23] </ref>). Hive reuses some of the techniques developed in Sprite and Locus. 10 Concluding Remarks Fault containment is a key technique that will improve the reliability of large-scale shared-memory multiprocessors used as general-purpose compute servers. The challenge is to provide better reliability than current multiprocessor operating systems without reducing performance.
References-found: 23

