URL: http://www.daimi.aau.dk/~depenau/PUB/dapp.ps
Refering-URL: http://www.daimi.aau.dk/~depenau/PUB/pub.html
Root-URL: http://www.daimi.aau.dk
Title: Soft-Monotonic Error Functions  
Author: Martin Moller and Jan Depenau 
Address: Ny Munkegade, Build. 540, DK-8000 Arhus C, Denmark  
Affiliation: Denmark and Computer Science Department, Arhus University,  
Note: TERMA Elektronik AS, Hovmarken 4, DK-8520 Lystrup,  
Abstract: It is well known that the least mean square error function and the entropy error function are Bayes optimal. Satisfying Bayes optimality criteria does not give any information about convergence properties, trajectories in weight space (e.g., if training often leads to local minima or flat regions in weight space), or generalization ability when trained on smaller sets of data. The problem with these error functions is that they are not monotonic with respect to classification, i.e., minimization of the error functions does not imply minimization of misclassifications. This paper proposes two new error functions, that exhibits a form of soft-monotonicity, where the monotonic behavior is dependent on the values of certain parameters associated with the functions. Through several experiments, it is shown that these functions can improve convergence and generalization. 
Abstract-found: 1
Intro-found: 1
Reference: [Battiti 92] <author> R. </author> <month> Battiti </month> <year> (1992), </year> <title> First and Second-Order Methods for Learning: between Steepest descent and Newton's Method, </title> <journal> Neural Computation, </journal> <volume> Vol. 4 (2), </volume> <pages> pp. 141-167. </pages>
Reference-contexts: Each centerpoint and its distortions were then randomly assigned to one out of two possible classes. It is widely recognized that the class of conjugate gradient algorithms are well suited for learning algorithms because of their ability to gain second order information without too much calculation work <ref> [Battiti 92] </ref>.
Reference: [Buntine 91] <author> W. </author> <title> Buntine and A.S. Weigend (1991), Bayesian Back-Propagation, </title> <journal> Complex Systems, </journal> <volume> Vol. 5, </volume> <pages> pp. 603-643. </pages>
Reference-contexts: But when the training set is small this approximation can be poor <ref> [Buntine 91] </ref>, and it is necessary to impose constraints on the network solutions. This is in a Bayesian perspective the same as choosing appropriate priors which is strongly related to penalty terms or regularizers in statistical literature.
Reference: [Hampshire 92] <author> J.B. </author> <title> Hampshire (1992), A Differential Theory of Learning for Statistical Pattern Recognition with Connectionist Models, </title> <type> Ph.D. Thesis, </type> <institution> School of Computer Science, Carnegie Mellon University. </institution>
Reference-contexts: This is in a Bayesian perspective the same as choosing appropriate priors which is strongly related to penalty terms or regularizers in statistical literature. The problem with the least mean square error function can be illustrated by the following simple figure <ref> [Hampshire 92] </ref>. Consider a network with two output units having output between 0 and 1. The outputs are mapped onto the x- and y-axis respectively. If the desired target pattern is (1 0) then all outputs to the right of the line y = x can be considered correct. <p> A way to avoid suboptimal solutions is to strictly minimize the number of misclassifications. Hampshire defines such an approach that works for binary classification problems <ref> [Hampshire 92] </ref>. We present a more general approach that involves a soft minimization of misclassifications.
Reference: [Moller 93] <author> M. </author> <title> Moller (1993), A Scaled Conjugate Gradient Algorithm for Fast Supervised Learning, Neural Networks, </title> <journal> June, </journal> <volume> Vol. 6, No. 4, </volume> <pages> pp. 525-533. </pages>
Reference-contexts: It is widely recognized that the class of conjugate gradient algorithms are well suited for learning algorithms because of their ability to gain second order information without too much calculation work [Battiti 92]. One, the Scaled Conjugate Gradient algorithm <ref> [Moller 93] </ref>, has especially low calculation costs, and has for that reason been used in the experiments to follow. 3.1 Training The three error functions were tested on dimension 8,10,12,14,16 and 18 running 5 different runs on each dimension using a 3 layer network with N hidden units.
Reference: [Seber and Wild 89] <author> G.A.F. Seber and C.J. </author> <title> Wild (1989), Non-linear Regression, </title> <publisher> John Wiley and Sons, </publisher> <address> New York. </address> <month> 467 </month>
Reference-contexts: This gives a sort of balanced distribution of the errors. For regression problems it is well known in statistics that a balanced set of errors can yield better generalization, this is often referred to as variance heterogeneity <ref> [Seber and Wild 89] </ref>. It is an open question whether this is true also for classification problems. In the limit when ff increases to infinity, the exponential error function is monotonic.
References-found: 5

