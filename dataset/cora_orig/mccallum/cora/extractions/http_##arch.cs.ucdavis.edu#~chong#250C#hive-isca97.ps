URL: http://arch.cs.ucdavis.edu/~chong/250C/hive-isca97.ps
Refering-URL: http://arch.cs.ucdavis.edu/~chong/250C.html
Root-URL: http://www.cs.ucdavis.edu
Title: Abstract  
Abstract: Current shared-memory multiprocessors are inherently vulnerable to faults: any significant hardware or system software fault causes the entire system to fail. Unless provisions are made to limit the impact of faults, users will perceive a decrease in reliability when they entrust their applications to larger machines. This paper shows that fault containment techniques can be effectively applied to scalable shared-memory multiprocessors to reduce the reliability problems created by increased machine size. The primary goal of our approach is to leave normal-mode performance unaffected. Rather than using expensive fault-tolerance techniques to mask the effects of data and resource loss, our strategy is based on limiting the damage caused by faults to only a portion of the machine. After a hardware fault, we run a distributed recovery algorithm that allows normal operation to be resumed in the functioning parts of the machine. Our approach is implemented in the Stanford FLASH multiprocessor. Using a detailed hardware simulator, we have performed a number of fault injection experiments on a FLASH system running Hive, an operating system designed to support fault containment. The results we report validate our approach and show that in conjunction with an operating system like Hive, we can improve the reliability seen by unmodified applications without substantial performance cost. Simulation results suggest that our algorithms scale well for systems up to 128 processors. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Aingworth, C. Chekuri, and R. Motwani. </author> <title> Fast Estimation of Diameter and Shortest Paths (without Matrix Multiplication). </title> <booktitle> In Proceedings of the Seventh Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pp. 547-553, </pages> <year> 1996. </year>
Reference-contexts: Still better approximations to the diameter can be computed in linear time, as shown in <ref> [1] </ref>. The BFT computations have to be carefully scheduled in order to avoid slowing down the dissemination phase.
Reference: [2] <author> P. Bernstein. </author> <title> Sequoia: A Fault-Tolerant Tightly Coupled Multiprocessor for Transaction Processing. </title> <booktitle> IEEE Computer 21(2), </booktitle> <month> February </month> <year> 1988. </year>
Reference-contexts: Furthermore, due to an ad-hoc design of the error recovery component, C.mmp was not able to handle many errors. The Sequoia shared-memory multiprocessor <ref> [2] </ref> provided fault tolerance by using a checkpointing scheme. In this scheme, main memory stores two copies of all application data. Any writeback requires the entire cache to be ushed twice (under OS control), once to each memory copy.
Reference: [3] <author> J. Chapin, M. Rosenblum, S. Devine, T. Lahiri, D. Teodosiu, and A. Gupta. Hive: </author> <title> Fault Containment for Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the 15th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pp. 12-25, </pages> <month> December </month> <year> 1995. </year>
Reference-contexts: Finally, we discuss how these failures can affect the operating system. The impact of operating system software errors lies beyond the scope of this paper; see <ref> [3] </ref> and [18] for a detailed treatment of this topic. 3.1 Direct Impact of Faults As shared-memory multiprocessor systems scale to larger numbers of processors, the probability of a hardware fault increases. <p> The firewall allows cells to protect their data against speculative writes, and helps defend against wild writes resulting from system software crashes in other cells <ref> [3] </ref>. A wild write issued by the processor has the potential to disrupt the operation of that nodes MAGIC, as the node controllers code, internal data structures, and cache coherence protocol state are all stored in a region of the nodes memory. <p> After the recovery algorithm completes it is possible to resume normal operation in the functioning parts of the machine; the operating system is informed that the recovery algorithm has run and performs its own recovery actions <ref> [3] </ref> before allowing user-level processes to continue execution. To avoid single points of failure, the recovery algorithm runs as a distributed algorithm over all the functioning nodes in the machine. <p> The simpler model does not expose any of the effects related to incorrect speculation, but should otherwise demonstrate all the effects seen on the real machine. The machine configurations used in our experiments are shown in Table 5.1. For the end-to-end recovery experiments we booted the Hive operating system <ref> [3] </ref> in an 8-cell configuration with one processor and 16 MB of main memory per cell. We ran a parallel make benchmark that compiles eight of the GnuChess 4.0 files, with each compile executing on a different cell. <p> Detailed system simulations show that the average increase in intercell write cache miss latency due to the firewall is less than 7% of the fastest internode write cache miss <ref> [3] </ref>. Since intercell write cache misses make up a small fraction of the cache coherence operations and since the R10000 processors provide latency-hiding mechanisms, the degradation in application performance lies in the noise for most workloads.
Reference: [4] <author> M. Galles. </author> <title> Scalable Pipelined Interconnect for Distributed Endpoint Routing: The SGI SPIDER Chip. </title> <booktitle> Presented at Hot Interconnects Symposium IV, </booktitle> <month> August </month> <year> 1996. </year>
Reference-contexts: Handlers running at the home node for a cache line manage the cache coherence protocol state of that line. Nodes in FLASH communicate through a high-speed, reliable, ow-controlled interconnect that is based on Silicon Graphicss CrayLink technology and SPIDER router chip <ref> [4] </ref>. For simplicity, in this paper we shall assume that the nodes are arranged in a two-dimensional mesh. Although such a topology could be realized with the SPIDER routers, FLASH actually uses a hierarchical fat hypercube topology that has a smaller diameter than a mesh with FIGURE 2.1.
Reference: [5] <author> C. Glass and L. Ni. </author> <title> Fault-Tolerant Wormhole Routing in Meshes without Virtual Channels. </title> <journal> IEEE Transactions on Parallel and Distributed Systems 7(6), </journal> <pages> pp. 620-636, </pages> <month> June </month> <year> 1996. </year>
Reference-contexts: After the interconnect has drained, the routing tables are reprogrammed such that normal coherence traffic will be routed around the failed regions. could lead to deadlock. We use the turn method and an approach similar to the one described in <ref> [5] </ref> and [21] to guarantee that the routing is deadlock free. Although this approach is able to produce a deadlock-free rerouting for many of the common failure cases, a general solution is still an open research issue; the details are outside the scope of this paper.
Reference: [6] <author> J. R. Goodman, M. K. Vernon, and P. J. Woest. </author> <title> Efficient Synchronization Primitives for Large-Scale Cache-Coherent Multiprocessors. </title> <booktitle> In Proceedings of the 3rd International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 64-73, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: The two phases are implemented as (fault-tolerant) barriers using the BFT computed during dissemination <ref> [6] </ref>. In practice, we have not observed any case in which the agreement protocol did not commit in the first round. After all the traffic has drained out of the interconnect, it is safe to reprogram the routing tables so that new traffic will be routed around the failed regions.
Reference: [7] <author> M. Heinrich, J. Kuskin, D. Ofelt, J. Heinlein, J. Baxter, J.P. Singh, R. Simoni, K. Gharachorloo, D. Nakahira, M. Horowitz, A. Gupta, M. Rosenblum, and J. Hennessy. </author> <title> The Performance Impact of Flexibility in the Stanford FLASH Multiprocessor. </title> <booktitle> In Proceedings of the 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 274-285, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: We conclude this section with scalability results. 5.1 Simulation Environment We debugged and tested our implementation of fault recovery using the SimOS [19] and FlashLite <ref> [7] </ref> simulators, since the FLASH hardware is not available at the time of writing. FlashLite is an accurate model of the FLASH memory system that models in detail both the node controllers and the CrayLink interconnect.
Reference: [8] <author> Y. A. Khalidi, J. M. Bernabeu, V. Matena, K. Shirriff, and M. Thadani. </author> <title> Solaris MC: A Multi Computer OS. </title> <booktitle> In Proceedings of the USENIX 1996 Annual Technical Conference, </booktitle> <pages> pp. 191-204, </pages> <month> January </month> <year> 1996. </year>
Reference-contexts: A general survey of checkpointing techniques for distributed shared memory can be found in [14]. Fault containment has been implemented in several network operating systems such as Locus [16], Sprite [17], and Solaris MC <ref> [8] </ref>. Due to a much weaker coupling between nodes and to the absence of shared memory, these systems do not have to cope with the problems described in Section 3.1 and Section 3.2.
Reference: [9] <author> J. Kuskin, D. Ofelt, M. Heinrich, J. Heinlein, R. Simoni, K. Gharac-horloo, J. Chapin, D. Nakahira, J. Baxter, M. Horowitz, A. Gupta, M. Rosenblum, and J. Hennessy. </author> <title> The Stanford FLASH Multiprocessor. </title> <booktitle> In Proceedings of the 21st International Symposium on Computer Architecture, </booktitle> <pages> pp. 302-313, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: The programmable node controller, called MAGIC, manages cache coherence between nodes, as well as all other communication within the node and with other nodes <ref> [9] </ref>. MAGIC (Figure 2.2) contains a statically scheduled, dual-issue RISC processor core that executes code sequences called handlers. The handlers update protocol state, control communication, and manage MAGICs resources. <p> By changing the MAGIC handlers it is possible to use a variety of different cache coherence protocols in FLASH. In this paper we assume the use of a directory-based protocol <ref> [9] </ref> which assigns each 128-byte memory line to a fixed home node, where the protocol state for that line is stored. Memory data from one node may be cached by the processor of any other node.
Reference: [10] <author> J. Laudon, and D. Lenoski. </author> <title> The SGI Origin 2000: A ccNUMA Highly Scalable Server. </title> <booktitle> In Proceedings of the 24th Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: 1 Introduction Scalable shared-memory multiprocessors are becoming an increasingly common computing platform. Several companies, including HP-Convex [20], Sequent [11], and Silicon Graphics <ref> [10] </ref>, are shipping multiprocessor systems with configurations of up to a few hundred processing nodes. However, current shared-memory multiprocessors are inherently vulnerable to faults: any significant hardware or system software fault will cause the entire system to fail.
Reference: [11] <author> T. Lovett and R. Clapp. STiNG: </author> <title> A CC-NUMA Computer System for the Commercial Marketplace. </title> <booktitle> In Proceedings of the 23rd Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 308-317, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: 1 Introduction Scalable shared-memory multiprocessors are becoming an increasingly common computing platform. Several companies, including HP-Convex [20], Sequent <ref> [11] </ref>, and Silicon Graphics [10], are shipping multiprocessor systems with configurations of up to a few hundred processing nodes. However, current shared-memory multiprocessors are inherently vulnerable to faults: any significant hardware or system software fault will cause the entire system to fail.
Reference: [12] <author> N. Lynch. </author> <title> Distributed Algorithms. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1995. </year>
Reference: [13] <author> C. Morin, A. Gefaut, M. Banatre, and A. Kernarrec. </author> <title> COMA: an Opportunity for Building Fault-tolerant Scalable Shared Memory Multiprocessors. </title> <booktitle> In Proceedings of the 23rd Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 56-65, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: An approach for fault tolerance in COMA (Cache Only Memory Architecture) systems has been proposed in <ref> [13] </ref>, but has not been implemented. The foremost drawback of this scheme is the high overhead for maintaining copies of memory lines. <p> A further difficulty consists in determining whether non-idempotent operations (such as uncached I/O operations) have completed before a fault since rolling back to a previously saved state and reissuing those operations may violate their exactly-once semantics. The approach described in <ref> [13] </ref> can only cope with the failure of one processor, requires a spare processor to resume operation, and cannot handle interconnect failures. A general survey of checkpointing techniques for distributed shared memory can be found in [14].
Reference: [14] <author> C. Morin, and I. Puaut. </author> <title> A Survey of Recoverable Distributed Shared Memory Systems. </title> <type> IRISA Technical Report 975, </type> <month> December </month> <year> 1995. </year>
Reference-contexts: The approach described in [13] can only cope with the failure of one processor, requires a spare processor to resume operation, and cannot handle interconnect failures. A general survey of checkpointing techniques for distributed shared memory can be found in <ref> [14] </ref>. Fault containment has been implemented in several network operating systems such as Locus [16], Sprite [17], and Solaris MC [8].
Reference: [15] <author> V. P. Nelson. </author> <title> Fault-tolerant computing: fundamental concepts. </title> <booktitle> IEEE Computer 23(7), </booktitle> <pages> pp. 19-25, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. In Proceedings of 24th ISCA, June 2-4, 1997, Denver, Colorado. small portion of a system <ref> [15] </ref>. In a computing system that provides fault containment, the chance of failure for a task depends only on the amount of resources that the task uses, not on the size of the entire system.
Reference: [16] <author> G. Popek and B. Walker. </author> <title> The LOCUS Distributed System Architecture. </title> <publisher> MIT Press, </publisher> <year> 1985. </year>
Reference-contexts: A general survey of checkpointing techniques for distributed shared memory can be found in [14]. Fault containment has been implemented in several network operating systems such as Locus <ref> [16] </ref>, Sprite [17], and Solaris MC [8]. Due to a much weaker coupling between nodes and to the absence of shared memory, these systems do not have to cope with the problems described in Section 3.1 and Section 3.2.
Reference: [17] <author> J. K. Ousterhout, A. R. Cherenson, F. Douglis, M. N. Nelson, and B. B. Welch. </author> <title> The Sprite network operating system. </title> <booktitle> IEEE Computer 21(2), </booktitle> <pages> pp. 23-36, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: A general survey of checkpointing techniques for distributed shared memory can be found in [14]. Fault containment has been implemented in several network operating systems such as Locus [16], Sprite <ref> [17] </ref>, and Solaris MC [8]. Due to a much weaker coupling between nodes and to the absence of shared memory, these systems do not have to cope with the problems described in Section 3.1 and Section 3.2.
Reference: [18] <author> M. Rosenblum, J. Chapin, D. Teodosiu, S. Devine, T. Lahiri, and A. Gupta. </author> <title> Implementing Efficient Fault Containment for Multiprocessors. </title> <journal> Communications of the ACM 39(9), </journal> <month> September </month> <year> 1996, </year> <pages> pp. 52-61. </pages>
Reference-contexts: Finally, we discuss how these failures can affect the operating system. The impact of operating system software errors lies beyond the scope of this paper; see [3] and <ref> [18] </ref> for a detailed treatment of this topic. 3.1 Direct Impact of Faults As shared-memory multiprocessor systems scale to larger numbers of processors, the probability of a hardware fault increases.
Reference: [19] <author> M. Rosenblum, S. Herrod, E. Witchel, and A. Gupta. </author> <title> Complete Computer Simulation: The SimOS Approach. </title> <booktitle> IEEE Parallel and Distributed Technology 3(4), </booktitle> <pages> pp. 34-43, </pages> <month> Winter </month> <year> 1995. </year>
Reference-contexts: We have added support to the FLASH multiprocessor [7][9] to confine the effects of most hardware and system software faults. Using the SimOS simulation environment <ref> [19] </ref>, we have validated our approach by performing a number of fault injection experiments on a detailed simulation of the machine. Achieving fault containment in a shared-memory multiprocessor requires careful design of both its hardware and its operating system. <p> We conclude this section with scalability results. 5.1 Simulation Environment We debugged and tested our implementation of fault recovery using the SimOS <ref> [19] </ref> and FlashLite [7] simulators, since the FLASH hardware is not available at the time of writing. FlashLite is an accurate model of the FLASH memory system that models in detail both the node controllers and the CrayLink interconnect.
Reference: [20] <author> T. Sterling, P. Merkey, and D. Savarese. </author> <title> Improving Application Performance on the HP/Convex Exemplar. </title> <booktitle> IEEE Computer 29(12), </booktitle> <pages> pp. 50-55, </pages> <month> December </month> <year> 1996. </year>
Reference-contexts: 1 Introduction Scalable shared-memory multiprocessors are becoming an increasingly common computing platform. Several companies, including HP-Convex <ref> [20] </ref>, Sequent [11], and Silicon Graphics [10], are shipping multiprocessor systems with configurations of up to a few hundred processing nodes. However, current shared-memory multiprocessors are inherently vulnerable to faults: any significant hardware or system software fault will cause the entire system to fail.
Reference: [21] <author> J. Vounckx et al. </author> <title> Fault-Tolerant Compact Routing based on Reduced Structural Information in Wormhole-Switching based Networks. </title> <booktitle> In Proceedings of the Colloquium on Structural Information and Communication Complexity, </booktitle> <month> May </month> <year> 1994. </year>
Reference-contexts: After the interconnect has drained, the routing tables are reprogrammed such that normal coherence traffic will be routed around the failed regions. could lead to deadlock. We use the turn method and an approach similar to the one described in [5] and <ref> [21] </ref> to guarantee that the routing is deadlock free. Although this approach is able to produce a deadlock-free rerouting for many of the common failure cases, a general solution is still an open research issue; the details are outside the scope of this paper.
Reference: [22] <editor> D. Weaver and T. Germond, eds. </editor> <title> The SPARC Architecture Manual, Version 9. </title> <publisher> Prentice-Hall, Inc., </publisher> <year> 1994. </year>
Reference-contexts: Rather than using an NMI on the R10000, MAGIC forces a processor interface bus parity error which raises a Cache Error that has most of the necessary properties. This solution is not optimal, 1. An example of appropriate support is the RED_state found in the UltraSPARC architecture <ref> [22] </ref>. FIGURE 4.3. Failure detection in FLASH. In this example, A receives no reply to a memory request it has sent to B, since B has failed.
Reference: [23] <author> W. Weber et al. </author> <title> The Mercury Interconnect Architecture: A Cost-effective Infrastructure for High-performance Servers. </title> <booktitle> In Proceedings of the 24th Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: Any instructions added to this handler for end-to-end checks would slow it down considerably and degrade the performance of the machine. On the other hand, a hardware implementation of end-to-end reliability can be quite effective, as shown in <ref> [23] </ref>. If a truncated message is received by a node controller, it is not generally possible to discard the message. To reduce the latency of memory operations in FLASH, data is pipelined as it passes through the memory, the interconnect, and MAGIC. <p> Node failures are detected in a lazy fashion, while transient network problems are handled by an end-to-end transmission protocol. The HAL multiprocessor provides an efficient hardware implementation of an end-to-end reliable protocol for coherence traffic <ref> [23] </ref>. The advantage of end-to-end reliability for fault containment consists in eliminating the loss of coherence packets that try to traverse a failed area, since these packets are resent by the hardware after the connectivity of the interconnect is restored.
Reference: [24] <author> W. Wulf, R. Levin, and S. P. Harbison. HYDRA/C.mmp: </author> <title> An Experimental Computer System. </title> <publisher> McGraw-Hill, </publisher> <year> 1981. </year>
Reference-contexts: This solution requires exposing most of the state of the node controller to allow the processor to take control of its operation during recovery. 6.3 Related Work An early effort at recovering from failures in the C.mmp multiprocessor is described in <ref> [24] </ref>. C.mmp attempted to diagnose the system after a fault and excluded any failed resources before resuming the unaffected processes. However, C.mmp did not have to deal with most of the issues described in this paper, since it did not use caches and since it had a crossbar interconnect.
Reference: [25] <author> K. Yeager. </author> <title> The MIPS R10000 Superscalar Microprocessor. </title> <booktitle> IEEE Micro 16(2), </booktitle> <pages> pp. 28-41, </pages> <month> April </month> <year> 1996. </year>
Reference-contexts: Each node in the machine contains a MIPS R10000 processor <ref> [25] </ref> and its second-level cache, a portion of the machines distributed main memory, a programmable node controller, and attached I/O devices such as disks or network adapters.
References-found: 25

