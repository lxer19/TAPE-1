URL: ftp://cse.ogi.edu/pub/tech-reports/1996/96-008.ps.gz
Refering-URL: ftp://cse.ogi.edu/pub/tech-reports/README.html
Root-URL: http://www.cse.ogi.edu
Title: Demand-Driven Interprocedural Constant Propagation: Implementation and Evaluation solve the interprocedural constant propagation problem with an
Author: Tito Autrey P. O. 
Note: Burke and Cytron  Grove and Torczon  
Address: Box 91000, Portland, OR 97291-1000 USA  
Affiliation: Department of Computer Science and Engineering Oregon Graduate Institute of Science Technology  
Pubnum: [CCKT86, GT93].  
Email: tito@cse.ogi.edu  
Date: June 1, 1995  
Abstract: We have developed a hybrid algorithm for interprocedural constant propagation combining two prior methods with a new demand-driven approach. We modified a prior intraprocedural constant propagator to use incrementally in a demand-driven interprocedural framework. We compare our algorithm to three prior interproce-dural methods. Wegman and Zadeck solve the intraprocedural constant propagation problem with an optimistic algorithm [WZ91]. Their algorithm solves the sparse conditional constant problem. The interprocedural version of their algorithm links the Static Single Assignment graphs of all procedures together and runs their intraprocedural algorithm over the single SSA graph. We show that our interprocedural algorithm, Demand-driven With Incremental Modification (DWIM), is fast and finds the same number of constants as the best interprocedural constant propagator in use today. We know of no implementations of either interprocedural Wegman-Zadeck or Burke-Cytron algorithms. Over a set of standard benchmarks we find 46.4% more interprocedural constants compared to intraprocedural constants. 
Abstract-found: 1
Intro-found: 1
Reference: <institution> References </institution>
Reference: [All72] <author> F. E. Allen. </author> <title> A Catalogue of Optimizing Transformations, </title> <address> pages 1-30. </address> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NH, </address> <year> 1972. </year>
Reference-contexts: Constant propagation makes other analyses simpler and more exact which leads to useful indirect effects. For example, constant 1 There are two forms of dead code. This form of dead code is called unreachable code. The other form is code whose results are not used, called unused code <ref> [All72] </ref>. 1 loop limits enhance dependence analysis, enhance parallelization heuristics [EB91], and may remove run-time range checks [Kol94]. It may also make subscript expressions linear which speeds analysis and can enable loop transformations [SLY90]. It may make array section analysis simpler [MS93].
Reference: [ASU86] <author> A. V. Aho, R. Sethi, and J. D. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1986. </year>
Reference-contexts: Other datatypes, such as structures, bit and string constants could all be done, but folklore holds that they do not occur frequently enough to be worth the effort. Constant propagation is computed on a data-flow graph representation of a procedure, frequently classic use-def and def-use chains <ref> [ASU86] </ref>. A lattice is used to cast constant propagation as a global data-flow problem. Each variable use or definition is represented by a value from Kildall's three-level constant propagation lattice [Kil73]. Top, &gt;, represents an as yet unknown constant value. Bottom, ?, represents a nonconstant value.
Reference: [BC86] <author> Michael Burke and Ron Cytron. </author> <title> Interprocedural dependence analysis and paral-lelization. </title> <booktitle> In Proceedings of the ACM SIGPLAN '86 Symposium on Compiler Construction, </booktitle> <pages> pages 162-175, </pages> <month> July </month> <year> 1986. </year>
Reference-contexts: Burke and Cytron's inter-procedural algorithm is tailored to minimize the working set of memory needed. They describe the use of an incremental intraprocedural propagator, iterating both backward and forward over the call graph <ref> [BC86] </ref>. On the practical front, Grove and Torczon have demonstrated that procedure summary blocks with jump functions find a very high percentage of the available constants [GT93]. RJFs also capture modification by a constant as a result of a procedure call. <p> The difference can be accounted for by our lack of support for global variables and RJFs. Burke and Cytron note that interprocedural constant propagation can be modeled as incremental intraprocedural constant propagation <ref> [BC86] </ref>. They point out that optimistic algorithms, which assume a value is constant until proven otherwise, such as WZ-SCC, are ill-suited to incremental propagation.
Reference: [CCKT86] <author> David Callahan, Keith D. Cooper, Ken Kennedy, and Linda Torczon. </author> <title> Interprocedu-ral constant propagation. </title> <booktitle> In Proceedings of the ACM SIGPLAN '86 Symposium on Compiler Construction, </booktitle> <pages> pages 152-161, </pages> <month> July </month> <year> 1986. </year>
Reference-contexts: We implemented the following four analyses to support our study of interproce-dural constant propagation: Call Graph Construction, Alias Analysis, MOD and REF Analysis, and Jump Function identification. The analyses store some of their results in a procedure summary block which is referenced by later analysis and optimization stages <ref> [CCKT86] </ref>. <p> value can be retained in a register across the call; G5 is also modified so its value must also be reloaded after the call. 3.4 Jump Functions Jump functions 4 are a concise way to express the value of arguments in terms of literals and constant flow through a procedure <ref> [CCKT86] </ref>. The flow of constants in a procedure p to a call site s for each actual argument and global variable (called global argument for this discussion) x, is expressed by a forward jump function (FJF) F x s . <p> Callahan et al. describe a method for efficient interprocedural constant propagation that easily supports separate compilation, but provide no evaluation of its effectiveness <ref> [CCKT86] </ref>. They use procedure summary blocks so that they do not need the procedure text available when they perform interprocedural analysis. They expand the idea of forward and return Jump Functions.
Reference: [CD93] <author> Charles Consel and Olivier Danvy. </author> <title> Tutorial notes on partial evaluation. </title> <booktitle> In Proceedings of the Twentieth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <year> 1993. </year> <month> 16 </month>
Reference-contexts: The envision this approach being more effective in a functional language setting. Constant propagation is one of the primary tools used in partial evaluation <ref> [CD93] </ref>. In partial evaluation a program as specialized with respect to some of its inputs and a new one is generate by a partial evaluation function.
Reference: [CFR + 89] <author> Ron Cytron, Jeanne Ferrante, Barry K. Rosen, Mark N. Wegman, and F. Kenneth Zadeck. </author> <title> An efficient method of computing static single assignment form. </title> <booktitle> In Proceedings of the Sixteenth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 25-35, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: Modern approaches to constant propagation are performed on sparse graphs. One of the most common sparse graphs is the Static Single Assignment (SSA) form <ref> [CFR + 89] </ref>. The Wegman-Zadeck Sparse Conditional Constant propagator (WZ-SCC) is a worklist algorithm [WZ91]. It has two lists, one of variable uses and the other of edges in the CFG.
Reference: [CK84] <author> Keith D. Cooper and Ken Kennedy. </author> <title> Efficient computation of flow-insensitive inter-procedural summary information. </title> <booktitle> In Proceedings of the ACM SIGPLAN '84 Symposium on Compiler Construction, </booktitle> <pages> pages 247-258, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: All this information is needed so that in procedure J the compiler does not try to load G1 into a register before doing the store to F3. 3.3 MOD and REF Analysis MOD and REF analysis computes MOD (p) and REF (p) sets for all procedures p <ref> [CK84, CK87, CK88] </ref>. MOD represents the side effects or externally visible write set of a procedure. REF represents the set of referenced formal parameters and global variables. During parsing we set DMOD (p) to be all formal or global parameters that are directly modified by the body of procedure p.
Reference: [CK86] <author> Keith D. Cooper and Ken Kennedy. </author> <title> Fast interprocedural alias analysis. </title> <booktitle> In Proceedings of the Sixteenth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 29-41, </pages> <month> February </month> <year> 1986. </year>
Reference-contexts: Much of the original work we draw upon was performed by Keith Cooper at Rice <ref> [Coo85, CK88, CK86] </ref>. The analysis uses two data structures which are built on top of the call graph. The algorithm for computing the formal-global binding graph, fi, is initialized by examining all call sites in all procedures.
Reference: [CK87] <author> Keith D. Cooper and Ken Kennedy. </author> <title> Efficient computation of flow-insensitive in-terprocedural summary infomation (a correction). </title> <type> Technical Report TR87-60, </type> <institution> Rice University, </institution> <year> 1987. </year>
Reference-contexts: All this information is needed so that in procedure J the compiler does not try to load G1 into a register before doing the store to F3. 3.3 MOD and REF Analysis MOD and REF analysis computes MOD (p) and REF (p) sets for all procedures p <ref> [CK84, CK87, CK88] </ref>. MOD represents the side effects or externally visible write set of a procedure. REF represents the set of referenced formal parameters and global variables. During parsing we set DMOD (p) to be all formal or global parameters that are directly modified by the body of procedure p.
Reference: [CK88] <author> Keith D. Cooper and Ken Kennedy. </author> <title> Interprocedural side-effect analysis in linear time. </title> <booktitle> In Proceedings of the ACM SIGPLAN '88 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 57-66, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Much of the original work we draw upon was performed by Keith Cooper at Rice <ref> [Coo85, CK88, CK86] </ref>. The analysis uses two data structures which are built on top of the call graph. The algorithm for computing the formal-global binding graph, fi, is initialized by examining all call sites in all procedures. <p> All this information is needed so that in procedure J the compiler does not try to load G1 into a register before doing the store to F3. 3.3 MOD and REF Analysis MOD and REF analysis computes MOD (p) and REF (p) sets for all procedures p <ref> [CK84, CK87, CK88] </ref>. MOD represents the side effects or externally visible write set of a procedure. REF represents the set of referenced formal parameters and global variables. During parsing we set DMOD (p) to be all formal or global parameters that are directly modified by the body of procedure p.
Reference: [Coo85] <author> Keith D. Cooper. </author> <title> Analyzing aliases of reference formal parameters. </title> <booktitle> In Proceedings of the Twelfthh Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 281-290, </pages> <month> January </month> <year> 1985. </year>
Reference-contexts: Much of the original work we draw upon was performed by Keith Cooper at Rice <ref> [Coo85, CK88, CK86] </ref>. The analysis uses two data structures which are built on top of the call graph. The algorithm for computing the formal-global binding graph, fi, is initialized by examining all call sites in all procedures.
Reference: [EB91] <author> Rudolf Eigenmann and William Blume. </author> <title> An effectiveness study of parallelizing compiler techniques. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1991. </year>
Reference-contexts: For example, constant 1 There are two forms of dead code. This form of dead code is called unreachable code. The other form is code whose results are not used, called unused code [All72]. 1 loop limits enhance dependence analysis, enhance parallelization heuristics <ref> [EB91] </ref>, and may remove run-time range checks [Kol94]. It may also make subscript expressions linear which speeds analysis and can enable loop transformations [SLY90]. It may make array section analysis simpler [MS93]. Constant propagation is generally only performed on integer and logical values.
Reference: [GT93] <author> Dan Grove and Linda Torczon. </author> <title> Interprocedural constant propagation: A study of jump function implementations. </title> <booktitle> In Proceedings of the ACM SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 90-99, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: They summarize the effect on the formal and global parameters of executing the whole procedure. The type of a jump function is an element of the constant propagation lattice. According to Grove and Torczon there are four FJFs of interest <ref> [GT93] </ref>. The literal constant FJF (LCFJF) identifies actual arguments that are literal constants. In Figure 5, formal A in procedure R has value 1 and no other constants are found. Because global variables are lexically variables and not literals, this FJF is unable to identify globals as constant. <p> They describe the use of an incremental intraprocedural propagator, iterating both backward and forward over the call graph [BC86]. On the practical front, Grove and Torczon have demonstrated that procedure summary blocks with jump functions find a very high percentage of the available constants <ref> [GT93] </ref>. RJFs also capture modification by a constant as a result of a procedure call. This information can be applied on a call site by call site basis which is more powerful than simply merging the results of all call sites together. <p> The expression evaluator handles aliasing. They trade completeness against support for separate compilation and they choose data structures that give them time linear in the size of the call graph with some reasonable assumptions. Grove and Torczon evaluate the effectiveness of the various types of Jump Functions <ref> [GT93] </ref>. Their algorithm only handles integer constants and it does not track them in and out of arrays. First, in a bottom-up walk of the call graph, it builds RJFs from an SSA graph, then it destroys the SSA graph.
Reference: [HK92] <author> Mary W. Hall and Ken Kennedy. </author> <title> Efficient call graph analysis. </title> <journal> ACM Letters on Programming Languages and Systems, </journal> <volume> 1(3) </volume> <pages> 227-242, </pages> <month> Sept </month> <year> 1992. </year>
Reference-contexts: Procedure formals are formal parameters that are bound to actual arguments which are procedure constants or procedure formals, as opposed to procedure variables which can be assigned procedure values at any point in the program. We implemented the algorithm described by Hall and Kennedy <ref> [HK92] </ref> which is complete and fast for languages with procedure formals. It finds the maximal Boundto (pf ) = fc : c is procedure constant bound to pf along some possible execution pathg set for all procedure formals.
Reference: [HP90] <author> John Hennessy and David Patterson. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: Induction variable analysis uses propagation results as input, and folding and propagation use induction expressions as input, so they can all be performed concurrently. Most constants are small integers <ref> [HP90] </ref>. This allows for a more compact encoding of a program by placing constants in an instruction immediate field rather than loading them explicitly from memory. Constant propagation makes other analyses simpler and more exact which leads to useful indirect effects.
Reference: [Kil73] <author> G. Kildall. </author> <title> A unified approach to global program optimization. </title> <booktitle> In Proceedings of the First Annual ACM Symposium on Principles of Programming Languages, </booktitle> <year> 1973. </year>
Reference-contexts: Constant propagation is computed on a data-flow graph representation of a procedure, frequently classic use-def and def-use chains [ASU86]. A lattice is used to cast constant propagation as a global data-flow problem. Each variable use or definition is represented by a value from Kildall's three-level constant propagation lattice <ref> [Kil73] </ref>. Top, &gt;, represents an as yet unknown constant value. Bottom, ?, represents a nonconstant value. The constant values constitute the middle layer of the lattice. The lattice meet operator, u, is defined in the usual way.
Reference: [Kol94] <author> Priyadarshan Kolte. </author> <title> Optimization of array subscript range checks. </title> <type> Technical report, </type> <institution> Oregon Graduate Institute, </institution> <year> 1994. </year>
Reference-contexts: This form of dead code is called unreachable code. The other form is code whose results are not used, called unused code [All72]. 1 loop limits enhance dependence analysis, enhance parallelization heuristics [EB91], and may remove run-time range checks <ref> [Kol94] </ref>. It may also make subscript expressions linear which speeds analysis and can enable loop transformations [SLY90]. It may make array section analysis simpler [MS93]. Constant propagation is generally only performed on integer and logical values.
Reference: [KU77] <author> J. B. Kam and J. D. Ullman. </author> <title> Monotone data flow analysis frameworks. </title> <journal> Acta Infor-matica, </journal> <volume> 7 </volume> <pages> 305-317, </pages> <year> 1977. </year>
Reference-contexts: - Section 3; interprocedural constant propagation in general and DWIM in particular - Section 4; our evaluation experiments and results Section 5; related work Section 6; future work Section 7, and we summarize in Section 8. 2 Constant Propagation It is well-known that the general constant propagation problem is undecidable <ref> [KU77] </ref>. Solutions to the restricted decidable sub-problem still yield useful results. Constant propagation has several direct effects. It may mark edges as non-executable in the control flow graph (CFG) by compile-time (static binding time) predicate evaluation.
Reference: [MS93] <author> Robert Metzger and Sean Stroud. </author> <title> Interprocedural constant propagation: An empirical study. </title> <journal> ACM Letters on Programming Languages and Systems, </journal> <volume> 2(4) </volume> <pages> 213-232, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: It may also make subscript expressions linear which speeds analysis and can enable loop transformations [SLY90]. It may make array section analysis simpler <ref> [MS93] </ref>. Constant propagation is generally only performed on integer and logical values. This is because they produce the biggest payoffs in terms of loop bounds, array subscripts and predicate values. <p> The Convex Application Compiler does perform FP constant propagation by assuming the compiler and target architectures are the same and tracking when the rounding mode is not known to be important <ref> [MS93] </ref>. Tracking constant array element values is a fruitful area of current research. Other datatypes, such as structures, bit and string constants could all be done, but folklore holds that they do not occur frequently enough to be worth the effort. <p> There is a distinct shortage of papers evaluating the effectiveness or cost of performing interprocedural analysis, optimization and in particular constant propagation. Grove and Torczon have evaluated the ParaScope 8 research compiler, and Metzger and Stroud have evaluated the Convex Application Compiler, a commercial compiler <ref> [MS93] </ref>. We will make comparisons to the Rice group under Ken Kennedy, to Wegman and Zadeck's extension of their intraprocedural SCC algorithm, to Burke and Cytron's work and to partial evaluation techniques.
Reference: [MW93] <author> Herbert G. Mayer and Michael Wolfe. </author> <title> Interprocedural alias analysis: Implementation and empirical results. </title> <journal> Software- Practice and Experience, </journal> <volume> 23(11) </volume> <pages> 1201-1233, </pages> <month> November </month> <year> 1993. </year> <month> 17 </month>
Reference-contexts: These occur when a procedure calls itself recursively with the formals used as actuals in the same position. These edges add no information so they may be safely ignored. The graph turns out to be unnecessarily large and complex to compute directly <ref> [MW93] </ref>.
Reference: [SLY90] <author> Zhiyu Shen, Zhiyuan Li, and Pen-Chung Yew. </author> <title> An empirical study of Fortran pro-grams for parallelizing compilers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(3) </volume> <pages> 356-364, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: The other form is code whose results are not used, called unused code [All72]. 1 loop limits enhance dependence analysis, enhance parallelization heuristics [EB91], and may remove run-time range checks [Kol94]. It may also make subscript expressions linear which speeds analysis and can enable loop transformations <ref> [SLY90] </ref>. It may make array section analysis simpler [MS93]. Constant propagation is generally only performed on integer and logical values. This is because they produce the biggest payoffs in terms of loop bounds, array subscripts and predicate values.
Reference: [SW94] <author> Eric Stoltz and Michael Wolfe. </author> <title> Constant propagation: A fresh, demand-driven look. </title> <booktitle> In Symposium on Applied Computing. ACM SIGAPP, </booktitle> <month> March </month> <year> 1994. </year>
Reference-contexts: Each use obtains its value from the meet of the reaching defs. In SSA graphs there is only one reaching def, except at nodes. Stoltz has shown that demand-driven constant propagation solvers with Gated Single Assignment are faster than WZ-SCC <ref> [SW94] </ref>. 3 Interprocedural Analysis Various interprocedural analyses are needed to enable interprocedural constant propagation. We implemented the following four analyses to support our study of interproce-dural constant propagation: Call Graph Construction, Alias Analysis, MOD and REF Analysis, and Jump Function identification.

References-found: 23

