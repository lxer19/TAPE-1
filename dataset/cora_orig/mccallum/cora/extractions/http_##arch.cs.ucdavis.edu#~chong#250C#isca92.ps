URL: http://arch.cs.ucdavis.edu/~chong/250C/isca92.ps
Refering-URL: http://arch.cs.ucdavis.edu/~chong/250C.html
Root-URL: http://www.cs.ucdavis.edu
Email: ftve,culler,sethg,schauserg@cs.berkeley.edu  
Title: Active Messages: a Mechanism for Integrated Communication and Computation 1  
Author: Thorsten von Eicken David E. Culler Seth Copen Goldstein Klaus Erik Schauser 
Address: Berkeley, CA 94720  
Affiliation: University of California,  
Abstract: Report No. UCB/CSD 92/#675, March 1992 Computer Science Division EECS Abstract The design challenge for large-scale multiprocessors is (1) to minimize communication overhead, (2) allow communication to overlap computation, and (3) coordinate the two without sacrificing processor cost/performance. We show that existing message passing multiprocessors have unnecessarily high communication costs. Research prototypes of message driven machines demonstrate low communication overhead, but poor processor cost/performance. We introduce a simple communication mechanism, Active Messages, show that it is intrinsic to both architectures, allows cost effective use of the hardware, and offers tremendous flexibility. Implementations on nCUBE/2 and CM-5 are described and evaluated using a split-phase shared-memory extension to C, Split-C. We further show that active messages are sufficient to implement the dynamically scheduled languages for which message driven machines were designed. With this mechanism, latency tolerance becomes a programming/compiling concern. Hardware support for active messages is desirable and we outline a range of enhancements to mainstream processors. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Arvind and R. A. </author> <title> Iannucci. Two Fundamental Issues in Multiprocessing. </title> <booktitle> In Proc. of DFVLR - Conf. 1987 on Par. Proc. in Science and Eng., </booktitle> <address> Bonn-Bad Godesberg, </address> <publisher> W. </publisher> <address> Germany, </address> <month> June </month> <year> 1987. </year>
Reference-contexts: Much like a traditional pipeline, the sender blocks until the message can be injected into the network and the handler executes immediately on arrival. Tolerating communication latency has been raised as a fundamental architectural issue <ref> [1] </ref>; this is not quite correct. The real architectural issue is to provide the ability to overlap communication and computation, which, in-turn, requires low-overhead asynchronous communication. Tolerating latency then becomes a programming problem: a communication must be initiated sufficiently in advance of the use of its result.
Reference: [2] <author> B. N. Bershad, T. E. Anderson, E. D. Lazowska, and H. M. Levy. </author> <title> Lightweight Remote Procedure Call. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 8(1), </volume> <month> February </month> <year> 1990. </year>
Reference-contexts: The resources invested in message handling serve to maintain the efficiency of the background computation. 5 Related work The work presented in this paper is similar in character to the recent development of optimized RPC mechanisms in the operating system research community <ref> [19, 2] </ref>. Both attempt to reduce the communication layer functionality to the minimum required and carefully analyze and optimize the frequent case. However, the time scales and the operating system involvement are radically different in the two arenas.
Reference: [3] <author> D. Culler, A. Sah, K. Schauser, T. von Eicken, and J. Wawrzynek. </author> <title> Fine-grain Parallelism with Minimal Hardware Support: A Compiler-Controlled Threaded Abstract Machine. </title> <booktitle> In Proc. of 4th Int. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> Santa-Clara, CA, </address> <month> April </month> <year> 1991. </year> <note> (Also available as Technical Report UCB/CSD 91/591, </note> <institution> CS Div., University of California at Berkeley). </institution>
Reference-contexts: Contrary to expectation this does not necessarily result in lower performance than a direct hardware implementation because software handlers can exploit and optimize special cases. TAM <ref> [3] </ref> (Threaded Abstract Machine), a fine-grain parallel execution model based on Active Messages, goes one step further and requires the compiler to help manage memory allocation and scheduling. It is currently used as a compilation target for implicitly parallel languages such as Id90.
Reference: [4] <author> D. E. Culler and Arvind. </author> <title> Resource Requirements of Dataflow Programs. </title> <booktitle> In Proc. of the 15th Ann. Int. Symp. on Comp. Arch., </booktitle> <pages> pages 141-150, </pages> <address> Hawaii, </address> <month> May </month> <year> 1988. </year>
Reference-contexts: In general, it cannot be released in simple FIFO or LIFO order. Moreover, the size of the scheduling queue does not depend on the rate at which messages arrive or handlers are executed, but on the amount of excess parallelism in the program <ref> [4] </ref>. Given that the excess parallelism can grow arbitrarily (as can the conventional call stack) it is impractical to set aside a fraction of memory for the message queue, rather it must be able to grow to the size of available memory.
Reference: [5] <author> W. Dally and et al. </author> <title> Architecture of a Message-Driven Processor. </title> <booktitle> In Proc. of the 14th Annual Int. Symp. on Comp. Arch., </booktitle> <pages> pages 189-196, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: Message passing machines, including the nCUBE/2, iPSC/2, iPSC/860 and others, treat the network essentially as a fast I/O device. Message driven architectures, including Monsoon [18, 17] and the J-Machine <ref> [5] </ref>, integrate the network deeply into the processor. Message reception is part of the basic instruction scheduling mechanism and message send is supported directly in the execution unit. Section 2 examines current message passing machines in detail.
Reference: [6] <author> W. Dally and et al. </author> <title> The J-Machine: A Fine-Grain Concurrent Computer. </title> <booktitle> In IFIP Congress, </booktitle> <year> 1989. </year>
Reference-contexts: This ability to suspend requires general allocation and scheduling on message arrival and is the key difference with respect to Active Messages. In the case of the J-Machine, the programming model is put forward in object-oriented language terms <ref> [6] </ref>: the handler is a method, the data holds the arguments for the method and usually one of them names the object the method is to operate on. In a functional language view, the message is a closure with a code pointer and all arguments of the closure.
Reference: [7] <author> J. J. Dongarra. </author> <title> Performance of Various Computers Using Standard Linear Equations Software. </title> <type> Technical Report CS-89-85, </type> <institution> Computer Science Dept., Univ. of Tennessee, </institution> <address> Knoxville, TN 37996, </address> <month> December </month> <year> 1991. </year>
Reference-contexts: entire volume of communication must be allocated for the duration of the computation phase! Machine T s T b T fp [s/mesg] [s/byte] [s/flop] iPSC [8] 4100 2.8 25 nCUBE/10 [8] 400 2.6 8.3 iPSC/2 [8] 700 0.36 3.4 390y 0.2 nCUBE/2 160 0.45 0.50 iPSC/860 [13] 160 0.36 0.033 <ref> [7] </ref> 60y 0.5 y: messages up to 100 bytes z: blocking send/receive Table 1: Asynchronous send and receive overheads in existing message passing machines.
Reference: [8] <author> T. H. Dunigan. </author> <title> Performance of a Second Generation Hypercube. </title> <type> Technical Report ORNL/TM-10881, </type> <institution> Oak Ridge Nat'l Lab, </institution> <month> November </month> <year> 1988. </year>
Reference-contexts: Data can be exchanged while computing by executing all sends before the computation phase and all receives afterwards. Note that buffer space for the entire volume of communication must be allocated for the duration of the computation phase! Machine T s T b T fp [s/mesg] [s/byte] [s/flop] iPSC <ref> [8] </ref> 4100 2.8 25 nCUBE/10 [8] 400 2.6 8.3 iPSC/2 [8] 700 0.36 3.4 390y 0.2 nCUBE/2 160 0.45 0.50 iPSC/860 [13] 160 0.36 0.033 [7] 60y 0.5 y: messages up to 100 bytes z: blocking send/receive Table 1: Asynchronous send and receive overheads in existing message passing machines. <p> Note that buffer space for the entire volume of communication must be allocated for the duration of the computation phase! Machine T s T b T fp [s/mesg] [s/byte] [s/flop] iPSC <ref> [8] </ref> 4100 2.8 25 nCUBE/10 [8] 400 2.6 8.3 iPSC/2 [8] 700 0.36 3.4 390y 0.2 nCUBE/2 160 0.45 0.50 iPSC/860 [13] 160 0.36 0.033 [7] 60y 0.5 y: messages up to 100 bytes z: blocking send/receive Table 1: Asynchronous send and receive overheads in existing message passing machines. <p> Note that buffer space for the entire volume of communication must be allocated for the duration of the computation phase! Machine T s T b T fp [s/mesg] [s/byte] [s/flop] iPSC <ref> [8] </ref> 4100 2.8 25 nCUBE/10 [8] 400 2.6 8.3 iPSC/2 [8] 700 0.36 3.4 390y 0.2 nCUBE/2 160 0.45 0.50 iPSC/860 [13] 160 0.36 0.033 [7] 60y 0.5 y: messages up to 100 bytes z: blocking send/receive Table 1: Asynchronous send and receive overheads in existing message passing machines.
Reference: [9] <author> G. Fox. </author> <title> Programming Concurrent Processors. </title> <publisher> Addison Wesley, </publisher> <year> 1989. </year>
Reference-contexts: what balance is required between processor and network performance. 1.1 Algorithmic communication model The most common cost model used in algorithm design for large-scale multiprocessors assumes the program alternates between computation and communication phases and that communication requires time linear in the size of the message, plus a start-up cost <ref> [9] </ref>. <p> In the synchronous, or crystalline <ref> [9] </ref> form, send and receive are blocking the send blocks until the corresponding receive is executed and only then is data transferred. The main advantage of the blocking send/receive model is its simplicity.
Reference: [10] <author> R. H. Halstead, Jr. </author> <title> Multilisp: A Language for Concurrent Symbolic Computation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(4) </volume> <pages> 501-538, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: Simpler, more traditional, processors can be built without unduly compromising either the communication or the processing performance. 3.1 Intended programming model The main driving force behind message driven architectures is to support languages with dynamic parallelism, such as Id90 [15], Multilisp <ref> [10] </ref>, and CST [12]. Computation is driven by messages, which contain the name of a handler and some data. On message arrival, storage for the message is allocated in a scheduling 12 queue.
Reference: [11] <author> Dana S. Henry and Christopher F. Joerg. </author> <title> The Network Interface Chip. </title> <type> Technical Report CSG Memo, 331, </type> <institution> MIT Lab for Comp. Sci., </institution> <type> 545 Tech. </type> <institution> Square, </institution> <address> Cambridge, MA, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Also, multiple requests are often sent with mostly identical return addresses. Keeping additional context information such as the current frame pointer and a code base pointer in the network interface can further accelerate the formatting of requests. The NIC <ref> [11] </ref> network interface contains 5 input and 5 output registers which are used to set-up and consume messages. Output registers retain their value after a message is sent, so that consecutive messages with identical parts can be sent cheaply.
Reference: [12] <author> W. Horwat, A. A. Chien, and W. J. Dally. </author> <title> Experience with CST: </title> <booktitle> Programming and Implementation. In Proc. of the ACM SIGPLAN '89 Conference on Programming Language Design and Implementation, </booktitle> <year> 1989. </year>
Reference-contexts: Simpler, more traditional, processors can be built without unduly compromising either the communication or the processing performance. 3.1 Intended programming model The main driving force behind message driven architectures is to support languages with dynamic parallelism, such as Id90 [15], Multilisp [10], and CST <ref> [12] </ref>. Computation is driven by messages, which contain the name of a handler and some data. On message arrival, storage for the message is allocated in a scheduling 12 queue. When the message reaches the head of the queue, the handler is executed with the data as arguments.
Reference: [13] <author> Intel. </author> <type> Personal communication, </type> <year> 1991. </year>
Reference-contexts: buffer space for the entire volume of communication must be allocated for the duration of the computation phase! Machine T s T b T fp [s/mesg] [s/byte] [s/flop] iPSC [8] 4100 2.8 25 nCUBE/10 [8] 400 2.6 8.3 iPSC/2 [8] 700 0.36 3.4 390y 0.2 nCUBE/2 160 0.45 0.50 iPSC/860 <ref> [13] </ref> 160 0.36 0.033 [7] 60y 0.5 y: messages up to 100 bytes z: blocking send/receive Table 1: Asynchronous send and receive overheads in existing message passing machines.
Reference: [14] <author> D. Johnson. </author> <title> Trap Architectures for Lisp Systems. </title> <booktitle> In Proc. of the 1990 ACM conf. on Lisp and Functional Programming, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: For example, on Sparc or Mips a message-ready signal can be attached to the coprocessor condition code input and polled using a branch on coprocessor condition instruction. User-level interrupts: User-level traps have been proposed to handle exceptions in dynamically typed programming languages <ref> [14] </ref> and floating-point computations. For Active Messages, user-level interrupts need only occur between instructions. However, an incoming message may not be for the currently running user process and the network interface should interrupt to the kernel in this case.
Reference: [15] <author> R. S. Nikhil. </author> <title> The Parallel Programming Language Id and its Compilation for Parallel Machines. </title> <booktitle> In Proc. Workshop on Massive Parallelism, </booktitle> <address> Amalfi, Italy, October 1989. </address> <publisher> Academic Press, </publisher> <year> 1991. </year> <note> Also: CSG Memo 313, </note> <institution> MIT Laboratory for Computer Science, 545 Technology Square, </institution> <address> Cambridge, MA 02139, USA. </address>
Reference-contexts: Simpler, more traditional, processors can be built without unduly compromising either the communication or the processing performance. 3.1 Intended programming model The main driving force behind message driven architectures is to support languages with dynamic parallelism, such as Id90 <ref> [15] </ref>, Multilisp [10], and CST [12]. Computation is driven by messages, which contain the name of a handler and some data. On message arrival, storage for the message is allocated in a scheduling 12 queue.
Reference: [16] <author> R. S. Nikhil, G. M. Papadopoulos, and Arvind. </author> <title> *T: A Killer Micro for A Brave New World. </title> <type> Technical Report CSG Memo 325, </type> <institution> MIT Lab for Comp. Sci., </institution> <type> 545 Tech. </type> <institution> Square, </institution> <address> Cambridge, MA, </address> <month> January </month> <year> 1991. </year>
Reference-contexts: On reception, the message head (i.e., the handler address and possibly a process id) can be checked using the normal memory management system. Frequent message accelerators: A well-designed network interface allows the most frequent message types to be issued quickly. For example in the *T <ref> [16] </ref> proposal, issuing a global memory fetch takes a single store double instruction (the network interface is memory mapped). The 64-bit data value is interpreted as a global address and translated in the network interface into a node/local-address pair.
Reference: [17] <author> G. M. Papadopoulos. </author> <title> Implementation of a General Purpose Dataflow Multiprocessor. </title> <type> Technical Report TR432, </type> <institution> MIT Lab for Comp. Sci., </institution> <type> 545 Tech. </type> <institution> Square, </institution> <address> Cambridge, MA, </address> <month> September </month> <year> 1988. </year> <type> (PhD Thesis, </type> <institution> Dept. of EECS, MIT). </institution>
Reference-contexts: Message passing machines, including the nCUBE/2, iPSC/2, iPSC/860 and others, treat the network essentially as a fast I/O device. Message driven architectures, including Monsoon <ref> [18, 17] </ref> and the J-Machine [5], integrate the network deeply into the processor. Message reception is part of the basic instruction scheduling mechanism and message send is supported directly in the execution unit. Section 2 examines current message passing machines in detail.
Reference: [18] <author> G. M. Papadopoulos and D. E. Culler. Monsoon: </author> <title> an Explicit Token-Store Architecture. </title> <booktitle> In Proc. of the 17th Annual Int. Symp. on Comp. Arch., </booktitle> <address> Seattle, Washington, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: Message passing machines, including the nCUBE/2, iPSC/2, iPSC/860 and others, treat the network essentially as a fast I/O device. Message driven architectures, including Monsoon <ref> [18, 17] </ref> and the J-Machine [5], integrate the network deeply into the processor. Message reception is part of the basic instruction scheduling mechanism and message send is supported directly in the execution unit. Section 2 examines current message passing machines in detail. <p> In a functional language view, the message is a closure with a code pointer and all arguments of the closure. Monsoon is usually described from the dataflow perspective <ref> [18] </ref> and messages carry tokens formed of an instruction pointer, a frame pointer and one piece of data. The data value is one of the operands of the specified instruction, the other is referenced relative to the frame pointer.
Reference: [19] <author> A. Thekkath and H. M. Levy. </author> <title> Limits to Low-Latency RPC. </title> <type> Technical Report TR 91-06-01, </type> <institution> Dept. of Computer Science and Engineering, University of Washington, </institution> <address> Seattle WA 98195, </address> <year> 1991. </year> <month> 20 </month>
Reference-contexts: The resources invested in message handling serve to maintain the efficiency of the background computation. 5 Related work The work presented in this paper is similar in character to the recent development of optimized RPC mechanisms in the operating system research community <ref> [19, 2] </ref>. Both attempt to reduce the communication layer functionality to the minimum required and carefully analyze and optimize the frequent case. However, the time scales and the operating system involvement are radically different in the two arenas.
References-found: 19

