URL: http://robotics.stanford.edu/~ronnyk/eodg.ps
Refering-URL: http://robotics.stanford.edu/users/ronnyk/ronnyk-bib.html
Root-URL: 
Email: fronnyk,jamieg@CS.Stanford.EDU  
Title: Oblivious Decision Trees, Graphs, and Top-Down Pruning  
Author: Ron Kohavi and Chia-Hsin Li 
Address: Stanford, CA. 94305  
Affiliation: Computer Science Department Stanford University  
Date: 1995  
Note: Appears in the International Joint Conference on Artificial Intelligence (IJCAI),  
Abstract: We describe a supervised learning algorithm, EODG, that uses mutual information to build an oblivious decision tree. The tree is then converted to an Oblivious read-Once Decision Graph (OODG) by merging nodes at the same level of the tree. For domains that are appropriate for both decision trees and OODGs, performance is approximately the same as that of C4.5, but the number of nodes in the OODG is much smaller. The merging phase that converts the oblivious decision tree to an OODG provides a new way of dealing with the replication problem and a new pruning mechanism that works top down starting from the root. The pruning mechanism is well suited for finding symmetries and aids in recovering from splits on irrelevant features that may happen during the tree construction.
Abstract-found: 1
Intro-found: 1
Reference: <author> Breiman, L., Friedman, J. H., Olshen, R. A. & Stone, C. J. </author> <year> (1984), </year> <title> Classification and Regression Trees, </title> <publisher> Wadsworth International Group. </publisher>
Reference-contexts: 1 Introduction Decision trees provide a hypothesis space for supervised machine learning algorithms that is well suited for many datasets encountered in the real world <ref> (Breiman, Fried-man, Olshen & Stone 1984, Quinlan 1993) </ref>.The tree structure, used to represent concepts suffers from some well-known problems, most notably the replication problem: disjunctive concepts such as (A ^ B) _ (C ^ D) are inefficiently represented (Pagallo & Haussler 1990). <p> A related problem, the fragmentation problem, sometimes called over-branching or over-partitioning (Fayyad 1991), is specific to the top-down recursive partitioning that is used to build the trees. In the common top-down decision tree construction algorithms including CART <ref> (Breiman et al. 1984) </ref>, ID3, and C4.5 (Quinlan 1993), the training set is partitioned according to a test at each node (usually a test on a single feature).
Reference: <author> Bryant, R. E. </author> <year> (1986), </year> <title> "Graph-based algorithms for boolean function manipulation", </title> <journal> IEEE Transactions on Computers C-35(8), </journal> <pages> 677-691. </pages>
Reference-contexts: Kohavi (1994a) introduced Oblivious read-Once Decision Graphs (OODGs) as an alternative representation structure for supervised classification learning. OODGs retain most of the advantages of decision trees, while overcoming the above problems. OODGs are similar to Ordered Binary Decision Diagrams <ref> (Bryant 1986) </ref>, which have been used in the engineering community to represent state-graph models of systems, allowing verification of finite-state systems with up to 10 120 states. Much of the work on OBDDs carries over to OODGs. We refer the reader to Kohavi (1994a) for a discussion of related work.
Reference: <author> Cover, T. M. & Thomas, J. A. </author> <year> (1991), </year> <title> Elements of Information Theory, </title> <publisher> John Wiley & Sons, Inc. </publisher>
Reference: <author> Fayyad, U. M. </author> <year> (1991), </year> <title> On the induction of decision trees for multiple concept learning, </title> <type> PhD thesis, </type> <institution> EECS Dept, Michigan University. </institution>
Reference-contexts: A related problem, the fragmentation problem, sometimes called over-branching or over-partitioning <ref> (Fayyad 1991) </ref>, is specific to the top-down recursive partitioning that is used to build the trees.
Reference: <author> Fayyad, U. M. </author> <year> (1994), </year> <title> Branching on attribute values in decision tree generation, </title> <booktitle> in "Proceedings of the twelfth national conference on artificial intelligence", </booktitle> <publisher> AAAI Press and MIT Press, </publisher> <pages> pp. 601-606. </pages>
Reference: <author> Fayyad, U. M. & Irani, K. B. </author> <year> (1993), </year> <title> Multi-interval dis-cretization of continuous attributes for classification learning, </title> <editor> in R. Bajcsy, ed., </editor> <booktitle> "Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence", </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kohavi, R. </author> <year> (1994a), </year> <title> Bottom-up induction of oblivious, read-once decision graphs, </title> <booktitle> in "Proceedings of the European Conference on Machine Learning". </booktitle>
Reference-contexts: An oblivious decision graph is a levelled graph such that all nodes at a given level are labelled by the same feature. Previous work on OODGs <ref> (Kohavi 1994a, Kohavi 1994b) </ref> defined read-once as allowing a feature to be tested only once. For continuous features, this restriction is is inappropriate; thus we allow continuous features to be tested at different levels as long as the thresholds are different. An OODG is an oblivious, read-once decision graph.
Reference: <institution> Available by anonymous ftp from starry.Stanford.EDU:pub/ronnyk/euroML94.ps. </institution>
Reference: <author> Kohavi, R. </author> <year> (1994b), </year> <title> Bottom-up induction of oblivious, read-once decision graphs : strengths and limitations, </title> <booktitle> in "Twelfth National Conference on Artificial Intelligence", </booktitle> <pages> pp. 613-618. </pages> <note> Available by anonymous ftp from Starry.Stanford.EDU:pub/ronnyk/aaai94.ps. </note>
Reference-contexts: Much of the work on OBDDs carries over to OODGs. We refer the reader to Kohavi (1994a) for a discussion of related work. The HOODG algorithm for bottom-up induction of OODGs was shown to be effective for nominal features <ref> (Kohavi 1994b) </ref>, but the algorithm could not cope with continuous features and lacked a global measure of improvement that could help cope with irrelevant features. In this paper, we introduce EODG (Entropy-based Oblivious Decision Graphs), a top-down induction algorithm for inducing oblivious read-once decision graphs.
Reference: <author> Murphy, P. M. & Aha, D. W. </author> <year> (1994), </year> <title> UCI repository of machine learning databases, For information contact ml-repository@ics.uci.edu. </title>
Reference-contexts: The last column indicates the time in CPU seconds on a Sparc-10 for EODG to train and test once (equivalent to one fold in ten-fold CV). 4 Experiments To estimate the performance accuracy of EODG, we chose a few artificial datasets and real-world datasets from the UC Irvine repository <ref> (Murphy & Aha 1994) </ref>. The artificial datasets are standard benchmark datasets such as the monk problems (Thrun etal. 1991); from the real-world domains, we chose ones that had at least 200 instances.
Reference: <author> Pagallo, G. & Haussler, D. </author> <year> (1990), </year> <title> "Boolean feature discovery in empirical learning", </title> <booktitle> Machine Learning 5, </booktitle> <pages> 71-99. </pages>
Reference-contexts: that is well suited for many datasets encountered in the real world (Breiman, Fried-man, Olshen & Stone 1984, Quinlan 1993).The tree structure, used to represent concepts suffers from some well-known problems, most notably the replication problem: disjunctive concepts such as (A ^ B) _ (C ^ D) are inefficiently represented <ref> (Pagallo & Haussler 1990) </ref>. A related problem, the fragmentation problem, sometimes called over-branching or over-partitioning (Fayyad 1991), is specific to the top-down recursive partitioning that is used to build the trees.
Reference: <author> Quinlan, J. R. </author> <year> (1993), </year> <title> C4.5: Programs for Machine Learning, </title> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, California. </address>
Reference-contexts: A related problem, the fragmentation problem, sometimes called over-branching or over-partitioning (Fayyad 1991), is specific to the top-down recursive partitioning that is used to build the trees. In the common top-down decision tree construction algorithms including CART (Breiman et al. 1984), ID3, and C4.5 <ref> (Quinlan 1993) </ref>, the training set is partitioned according to a test at each node (usually a test on a single feature). After a few splits, the number of instances at the node diminishes to a point where distinguishing between the actual pattern (signal) and random events (noise) becomes difficult. <p> We now describe the phases in detail. 3.1 Top-down Construction Using Mutual Information The top-down construction of the ODT is performed using mutual information in a manner similar to c4.5 <ref> (Quinlan 1993) </ref>. The main difference is that instead of doing recursive partitioning, i.e., finding a split and constructing the subtree recursively, an iterative process is used. <p> Compatible subtree are 0-compatible. When two nodes are considered for merging, we allow them to misclassify k instances, where k is defined similarly to the pruning method used in C4.5 <ref> (Quinlan 1993) </ref>: the number of misclassifications of each subtree is increased to the high-value of a confidence interval, and the merge takes place only if a similar increase in the merged subgraph results in fewer misclassifications than the sum of the adjusted misclassifications of the children. <p> The EODG algorithm can be improved along a few directions. We have not implemented handling of unknown values, but we believe that an approach similar to that used in decision trees <ref> (Quinlan 1993) </ref> should work well. We are currently conducting tests on a single fea ture at a node; other tests, such as oblique splits are possible.
Reference: <author> Spackman, A. K. </author> <year> (1988), </year> <title> Learning categorical criteria in biomedical domains, </title> <booktitle> in "Proceedings of the Fifth International Machine Learning Conference", </booktitle> <publisher> Mor-gan Kaufmann, </publisher> <pages> pp. 36-46. </pages>
Reference-contexts: The smallest decision trees for most symmetric functions, such as majority, "m of n," and parity, require exponentially-sized trees. While parity is unlikely to occur in practice, "m of n" is more common <ref> (Spackman 1988) </ref>. Induction algorithms that search for small trees tend to generalize poorly on these functions. Kohavi (1994a) introduced Oblivious read-Once Decision Graphs (OODGs) as an alternative representation structure for supervised classification learning. OODGs retain most of the advantages of decision trees, while overcoming the above problems.
Reference: <author> Thrun etal. </author> <year> (1991), </year> <title> The monk's problems: A performance comparison of different learning algorithms, </title> <type> Technical Report CMU-CS-91-197, </type> <institution> Carnegie Mel-lon University. </institution>
Reference-contexts: The artificial datasets are standard benchmark datasets such as the monk problems <ref> (Thrun etal. 1991) </ref>; from the real-world domains, we chose ones that had at least 200 instances. Because we have not implemented classification of instances with unknown values, we removed all such instances from the datasets tested. Table 1 shows the characteristics of the different domains used.
References-found: 14

