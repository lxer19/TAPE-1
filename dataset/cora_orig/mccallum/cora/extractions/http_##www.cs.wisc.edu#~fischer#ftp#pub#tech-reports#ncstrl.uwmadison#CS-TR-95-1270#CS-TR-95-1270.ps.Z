URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-95-1270/CS-TR-95-1270.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-95-1270/
Root-URL: http://www.cs.wisc.edu
Email: fjussi,mirong@cs.wisc.edu  
Title: Disk-Tape Joins: Synchronizing Disk and Tape Access  
Author: Jussi Myllymaki Miron Livny 
Keyword: tertiary storage, join methods, concurrent I/O  
Affiliation: Computer Sciences Department University of Wisconsin-Madison  
Abstract: Today large amounts of data are stored on tertiary storage media such as magnetic tapes and optical disks. DBMSs typically operate only on magnetic disks since they know how to maneuver disks and how to optimize accesses on them. Tertiary devices present a problem for DBMSs since these devices have dismountable media and have very different operational characteristics compared to magnetic disks. For instance, most tape drives offer very high capacity at low cost but are accessed sequentially, involve lengthy latencies, and deliver lower bandwidth. Typically, the scope of a DBMS's query optimizer does not include tertiary devices, and the DBMS might not even know how to control and operate upon tertiary-resident data. In a three-level hierarchy of storage devices (main memory, disk, tape), the typical solution is to elevate tape-resident data to disk devices, thus bringing such data into the DBMS' control, and then to perform the required operations on disk. This requires additional space on disk and may not give the lowest response time possible. With this challenge in mind, we studied the trade-offs between memory and disk requirements and the execution time of a join with the help of two well-known join methods. The conventional, disk-based Nested Block Join and Hybrid Hash Join were modified to operate directly on tapes. An experimental implementation of the modified algorithms gave us more insight into how the algorithms perform in practice. Our performance analysis shows that a DBMS desiring to operate on tertiary storage will benefit from special algorithms that operate directly on tape-resident data and take into account and exploit the mismatch in disk and tape characteristics. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Blasgen and K. Eswaran. </author> <title> Storage and access in relational data bases. </title> <journal> IBM Syst. J., </journal> <volume> 16(4) </volume> <pages> 363-377, </pages> <year> 1977. </year>
Reference-contexts: Joining two relations is one of the most common operations in a relational DBMS and one of the most costly if done naively. Since the seminal paper on computing joins of relations by Blasgen and Eswaran <ref> [1] </ref>, the database research community has shown great interest in optimizing joins. 2 On the other hand, more expensive tape drives with very short stop/start latencies, such as an IBM 3490, can make drive motions completely transparent to the application. 3 An important subclass of joins are ad hoc joins which <p> Most studies on disk-based joins employ a system model comprising main memory and disks where disks are represented by a transfer-only cost model <ref> [1, 2, 4, 8, 11] </ref>. In such a cost model the number of pages transferred is the cost metric while the latency penalty of small requests (seek and platter rotation delay) is disregarded. <p> Our analysis does not include the I/O cost and memory requirement of the final output stream from the join. Output costs are typically not considered in join method analyses since they are the same for all methods <ref> [1, 4, 5, 7, 11] </ref>. Before the join operation begins, the tape which holds S has already been inserted and loaded into the tape drive. Since S fits on a single tape, no media changes are required.
Reference: [2] <author> K. Bratbergsengen. </author> <title> Hashing methods and relational algebra operations. </title> <booktitle> In Proc. Conf. Very Large Databases, </booktitle> <pages> pages 323-333, </pages> <address> Singapore, </address> <month> Aug. </month> <year> 1984. </year>
Reference-contexts: Most studies on disk-based joins employ a system model comprising main memory and disks where disks are represented by a transfer-only cost model <ref> [1, 2, 4, 8, 11] </ref>. In such a cost model the number of pages transferred is the cost metric while the latency penalty of small requests (seek and platter rotation delay) is disregarded.
Reference: [3] <author> B. W. Culp, D. R. Domel, W. T. Gregory, J. J. Kato, G. C. Melton, K. A. Proehl, D. W. Ruska, V. K. Russon, and P. </author> <title> Way. Streaming tape drive control electronics. </title> <journal> Hewlett-Packard J., </journal> <volume> 39(3) </volume> <pages> 43-54, </pages> <month> June </month> <year> 1988. </year> <month> 22 </month>
Reference-contexts: It has an internal read-ahead, speed-matching buffer which allows the system to transfer data in requests as small as one block without performance deterioration. The tape drive tries to keep its read-ahead buffer full at all times by reading as many blocks from tape as possible <ref> [3, 9, 13] </ref>. When the buffer fills up or a high-water mark is hit, the tape drive slows down and stops (ramp down), and the tape is repositioned so that the read/write head is at the first tape block that did not fit into the buffer.
Reference: [4] <author> D. J. DeWitt, R. H. Katz, F. Olken, L. D. Shapiro, M. R. Stonebraker, and D. Wood. </author> <title> Im--plementation techniques for main memory database systems. </title> <booktitle> In Proc. ACM SIGMOD, </booktitle> <pages> pages 1-8, </pages> <address> Boston, MA, </address> <month> June </month> <year> 1984. </year>
Reference-contexts: Most studies on disk-based joins employ a system model comprising main memory and disks where disks are represented by a transfer-only cost model <ref> [1, 2, 4, 8, 11] </ref>. In such a cost model the number of pages transferred is the cost metric while the latency penalty of small requests (seek and platter rotation delay) is disregarded. <p> The smaller relation, R, is on disk. We consider only the case where R does not fit in main memory. If it does fit, the simplest approach is to read and hash R into memory, and then scan S. We refer to this case as Simple Hash Join <ref> [4] </ref> from tape. In the disk-tape version of Nested Block Join, the tape-resident relation (which is also the larger one) is selected as the outer relation. In the conventional, disk-based Nested Block Join the outer relation is always the smaller one. <p> M R blocks of main memory are assigned for R tuples, while M S blocks are assigned to store tuples from S. The space overhead incurred when building a hash table for a set of data is denoted by factor F <ref> [4] </ref>. A summary of the notation appears in Table 2. 5.1 Nested Block Join The conventional Nested Block Join (NB) [8] can be used for joining tape-resident S with disk-resident R as follows. <p> Our analysis does not include the I/O cost and memory requirement of the final output stream from the join. Output costs are typically not considered in join method analyses since they are the same for all methods <ref> [1, 4, 5, 7, 11] </ref>. Before the join operation begins, the tape which holds S has already been inserted and loaded into the tape drive. Since S fits on a single tape, no media changes are required.
Reference: [5] <author> L. M. Haas, M. J. Carey, and M. Livny. </author> <title> SEEKing the truth about ad hoc join costs. </title> <type> Technical Report 1148, </type> <institution> Univ. of Wisconsin at Madison, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: In other studies [7] the cost metric has been the number of multi-page I/O requests, regardless of request size. A detailed cost model which combines both cost metrics has been considered in <ref> [5] </ref>. A tape join study done as part of the Sequioa 2000 project [12] assumes a configuration of two tapes and one tape drive, and hence focuses on media switch delays [10]. The study employs a transfer-only cost model for tape drives, counting the number of chunks, or extents, transferred. <p> Memory size, relative to the size of relations, is typically a key factor in selecting a join method. For disk-based equi-joins, Hybrid Hash Join is commonly viewed as the method of choice when only a small amount of main memory is available, compared to the smaller relation. In <ref> [5] </ref> it is suggested that when a moderate fraction of the smaller relation can fit in memory, Nested Block Join with optimal buffer allocation provides better performance. <p> We assume that all disk accesses are multi-page I/O requests. The cost of a disk access is therefore derived by counting the number of blocks transferred. The cost of seek and latency delays is ignored. As shown in <ref> [5] </ref>, disk seeks and latency play a relatively minor role compared to transfer cost when disk requests are at least moderately large. In our model, the size of all disk requests is assumed to be at least 30 blocks, making seek and latency costs negligible 3 . <p> Our analysis does not include the I/O cost and memory requirement of the final output stream from the join. Output costs are typically not considered in join method analyses since they are the same for all methods <ref> [1, 4, 5, 7, 11] </ref>. Before the join operation begins, the tape which holds S has already been inserted and loaded into the tape drive. Since S fits on a single tape, no media changes are required. <p> It is very similar to the equation derived for the conventional, disk-based Nested Block Join in <ref> [5] </ref> although the scenarios are different. 6 For example, if jRj = 10 M and M R = 0:1 M, relation rocking would save M R jRj = 1% in disk I/O. 12 Table 4: Resource Requirements and I/O Cost bounding algorithm required disk space required memory I/O cost device Nested
Reference: [6] <author> L. M. Haas, M. J. Carey, and M. Livny. </author> <title> Tapes hold data too: Challenges of tuples on tertiary store. </title> <booktitle> In Proc. ACM SIGMOD, </booktitle> <pages> pages 413-417, </pages> <address> Washington, D.C., </address> <month> May </month> <year> 1993. </year>
Reference-contexts: A typical query optimizer of a database management system (DBMS) knows how to deal with only two types of storage devices|primary (i.e. main memory) and secondary (i.e. magnetic disks). Tertiary-resident data is commonly perceived as archived data <ref> [6] </ref> which means that the DBMS is not capable of maneuvering tertiary devices directly. Using services of the operating system, the DBMS first moves all tertiary-resident data to disk, and then optimizes and processes the query. <p> An experimental implementation is described in Section 9. Section 10 describes future work and Section 11 concludes this paper. 2 Related Work Prompted by a SIGMOD Database Challenges paper which highlighted the importance of DBMS access to and control of tertiary storage <ref> [6] </ref>, we set out to study the factors determining the performance of tertiary join operations. In particular, we look at magnetic tape devices which seem particularly poorly suited for joins, and are indeed commonly viewed as "second class citizens" in DBMSs, i.e. suitable for o*ine or nearline processing only.
Reference: [7] <author> R. B. Hagmann. </author> <title> An observation on database buffering performance metrics. </title> <booktitle> In Proc. Conf. Very Large Databases, </booktitle> <pages> pages 289-293, </pages> <address> Kyoto, Japan, </address> <month> Aug. </month> <year> 1986. </year>
Reference-contexts: In such a cost model the number of pages transferred is the cost metric while the latency penalty of small requests (seek and platter rotation delay) is disregarded. In other studies <ref> [7] </ref> the cost metric has been the number of multi-page I/O requests, regardless of request size. A detailed cost model which combines both cost metrics has been considered in [5]. <p> Our analysis does not include the I/O cost and memory requirement of the final output stream from the join. Output costs are typically not considered in join method analyses since they are the same for all methods <ref> [1, 4, 5, 7, 11] </ref>. Before the join operation begins, the tape which holds S has already been inserted and loaded into the tape drive. Since S fits on a single tape, no media changes are required.
Reference: [8] <author> W. Kim. </author> <title> A new way to compute the product and join of relation. </title> <booktitle> In Proc. ACM SIGMOD, </booktitle> <pages> pages 179-187, </pages> <address> Santa Monica, CA, </address> <month> May </month> <year> 1980. </year>
Reference-contexts: completely eliminated, how does it affect processing time? How does the amount of main memory space allocated to the join operator affect the interplay between disk space and processing time? In this paper we examine these questions with the help of two well-known ad hoc join methods: Nested Block Join <ref> [8] </ref> and Hybrid Hash Join [4,11]. Two join algorithms for tape-resident data that are based on these methods are presented and used to profile the interplay between the amount of disk and memory space allocated to a join operator and how long it takes to process the join. <p> Most studies on disk-based joins employ a system model comprising main memory and disks where disks are represented by a transfer-only cost model <ref> [1, 2, 4, 8, 11] </ref>. In such a cost model the number of pages transferred is the cost metric while the latency penalty of small requests (seek and platter rotation delay) is disregarded. <p> The space overhead incurred when building a hash table for a set of data is denoted by factor F [4]. A summary of the notation appears in Table 2. 5.1 Nested Block Join The conventional Nested Block Join (NB) <ref> [8] </ref> can be used for joining tape-resident S with disk-resident R as follows. An M S -size chunk of S is read from tape to memory, hashed, and joined with the entire R. This process is then iterated until S has been exhausted. <p> Each cycle has an associated time constraint: Reading R from disk should be faster than reading S i from tape. If the constraint holds, the tape drive is streaming and the join is bound by the speed 8 of the tape drive. NBT uses relation rocking <ref> [8, 9] </ref> to save on disk transfers.
Reference: [9] <author> D. Knuth. </author> <title> The Art of Computer Programming, Vol. III: Sorting and Searching. </title> <publisher> Addison-Wesley Publishing Co., </publisher> <address> Redwood City, CA, </address> <year> 1973. </year>
Reference-contexts: It has an internal read-ahead, speed-matching buffer which allows the system to transfer data in requests as small as one block without performance deterioration. The tape drive tries to keep its read-ahead buffer full at all times by reading as many blocks from tape as possible <ref> [3, 9, 13] </ref>. When the buffer fills up or a high-water mark is hit, the tape drive slows down and stops (ramp down), and the tape is repositioned so that the read/write head is at the first tape block that did not fit into the buffer. <p> Each cycle has an associated time constraint: Reading R from disk should be faster than reading S i from tape. If the constraint holds, the tape drive is streaming and the join is bound by the speed 8 of the tape drive. NBT uses relation rocking <ref> [8, 9] </ref> to save on disk transfers.
Reference: [10] <author> S. Sarawagi and M. Stonebraker. </author> <title> Single query optimization for tertiary memory. </title> <type> Technical Report 45, </type> <institution> Univ. of California at Berkeley, </institution> <month> Mar. </month> <year> 1994. </year>
Reference-contexts: A detailed cost model which combines both cost metrics has been considered in [5]. A tape join study done as part of the Sequioa 2000 project [12] assumes a configuration of two tapes and one tape drive, and hence focuses on media switch delays <ref> [10] </ref>. The study employs a transfer-only cost model for tape drives, counting the number of chunks, or extents, transferred. CPU and disk I/O costs are ignored in the cost model. Memory size, relative to the size of relations, is typically a key factor in selecting a join method.
Reference: [11] <author> L. Shapiro. </author> <title> Join processing in database systems with large main memories. </title> <journal> ACM Trans. Database Syst., </journal> <volume> 11(3) </volume> <pages> 239-264, </pages> <month> Sept. </month> <year> 1986. </year>
Reference-contexts: Most studies on disk-based joins employ a system model comprising main memory and disks where disks are represented by a transfer-only cost model <ref> [1, 2, 4, 8, 11] </ref>. In such a cost model the number of pages transferred is the cost metric while the latency penalty of small requests (seek and platter rotation delay) is disregarded. <p> Our analysis does not include the I/O cost and memory requirement of the final output stream from the join. Output costs are typically not considered in join method analyses since they are the same for all methods <ref> [1, 4, 5, 7, 11] </ref>. Before the join operation begins, the tape which holds S has already been inserted and loaded into the tape drive. Since S fits on a single tape, no media changes are required. <p> Taking into account M S = 0:9M and the fudge factor F , the disk space requirement is M 1:8=F . 14 hh nbt nbt III jRjF Relative Cost 10.80.60.40.2 1.5 III db mb nb jRjF Relative Cost 10.80.60.40.2 1.5 6.5 Hybrid Hash Join In <ref> [11] </ref>, it is shown that HH requires at least p jRjF blocks of memory if recursive partitioning is to be avoided.
Reference: [12] <author> M. Stonebraker. </author> <title> An overview of the Sequioa 2000 project. </title> <type> Technical Report 5, </type> <institution> Univ. of California at Berkeley, </institution> <month> Dec. </month> <year> 1991. </year>
Reference-contexts: In other studies [7] the cost metric has been the number of multi-page I/O requests, regardless of request size. A detailed cost model which combines both cost metrics has been considered in [5]. A tape join study done as part of the Sequioa 2000 project <ref> [12] </ref> assumes a configuration of two tapes and one tape drive, and hence focuses on media switch delays [10]. The study employs a transfer-only cost model for tape drives, counting the number of chunks, or extents, transferred. CPU and disk I/O costs are ignored in the cost model.
Reference: [13] <author> R. Thomas. </author> <title> Cache memory splits computer and tape operations. </title> <booktitle> Computer Design, </booktitle> <volume> 24(13) </volume> <pages> 89-93, </pages> <month> Oct. </month> <year> 1985. </year>
Reference-contexts: It has an internal read-ahead, speed-matching buffer which allows the system to transfer data in requests as small as one block without performance deterioration. The tape drive tries to keep its read-ahead buffer full at all times by reading as many blocks from tape as possible <ref> [3, 9, 13] </ref>. When the buffer fills up or a high-water mark is hit, the tape drive slows down and stops (ramp down), and the tape is repositioned so that the read/write head is at the first tape block that did not fit into the buffer.
References-found: 13

