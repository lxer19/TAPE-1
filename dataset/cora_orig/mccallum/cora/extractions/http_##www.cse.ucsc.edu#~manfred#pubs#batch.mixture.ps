URL: http://www.cse.ucsc.edu/~manfred/pubs/batch.mixture.ps
Refering-URL: http://www.cse.ucsc.edu/~manfred/pubs.html
Root-URL: http://www.cse.ucsc.edu
Email: dph@cse.ucsc.edu  schapire@research.att.com  singer@research.att.com  manfred@cse.ucsc.edu  
Title: A Comparison of New and Old Algorithms for A Mixture Estimation Problem  
Author: DAVID P. HELMBOLD ROBERT E. SCHAPIRE YORAM SINGER MANFRED K. WARMUTH 
Address: Santa Cruz, CA 95064  600 Mountain Avenue, Murray Hill, NJ 07974  600 Mountain Avenue, Murray Hill, NJ 07974  Santa Cruz, CA 95064  
Affiliation: Computer and Information Sciences, University of California,  AT&T Bell Laboratories,  AT&T Bell Laboratories,  Computer and Information Sciences, University of California,  
Abstract: We investigate the problem of estimating the proportion vector which maximizes the likelihood of a given sample for a mixture of given densities. We adapt a framework developed for supervised learning and give simple derivations for many of the standard iterative algorithms like gradient projection and EM. In this framework, the distance between the new and old proportion vectors is used as a penalty term. The square distance leads to the gradient projection update, and the relative entropy to a new update which we call the exponentiated gradient update (EG ). Curiously, when a second order Taylor expansion of the relative entropy is used, we arrive at an update EM which, for = 1, gives the usual EM update. Experimentally, both the EM -update and the EG -update for &gt; 1 outperform the EM algorithm and its variants. We also prove a polynomial bound on the worst-case global rate of convergence of the EG algorithm. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> N. Abe, J. Takeuchi, and M. K. Warmuth. </author> <title> Polynomial learnability of probablistic concepts with respect to the Kullback-Leibler divergence. </title> <booktitle> In Proceedings of the Fourth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 277-289. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: Similar observations were made in the supervised setting [7], [8]. We also show below how to obtain bounds when the entries in the matrix have zero-valued components. We essentially average the data matrix with a uniform matrix (this *-Bayesian averaging was also used in <ref> [1] </ref>) and then use the averaged matrix to run our algorithm. <p> Z t = i=1 P Y exp x p;i N X w t;i p=1 exp x p;i 1=P Since x t;i 2 <ref> [0; 1] </ref> and since fi x 1 (1 fi)x for fi &gt; 0 and x 2 [0; 1] we can upper bound the right-hand side by: N X w t;i p=1 1 1 exp 1=P N X P Y w t x p w t;i x p;i We will need the <p> Z t = i=1 P Y exp x p;i N X w t;i p=1 exp x p;i 1=P Since x t;i 2 <ref> [0; 1] </ref> and since fi x 1 (1 fi)x for fi &gt; 0 and x 2 [0; 1] we can upper bound the right-hand side by: N X w t;i p=1 1 1 exp 1=P N X P Y w t x p w t;i x p;i We will need the following fact: For non-negative numbers A i;p , 12 D.P. HELMBOLD, R.E. SCHAPIRE, Y. <p> w t;i 1 exp 1=P yields an upper bound on Z t of P Y N X w t x p w t;i x p;i (11) P Y w t x p : To further bound ln Z t , we apply the following: Lemma 1 For all ff 2 <ref> [0; 1] </ref> and x 2 R, ln (1 ff (1 e x )) ffx + x 2 =8 : Proof: Fix ff 2 [0; 1], and let f (x) = ffx + x 2 =8 ln (1 ff (1 e x )) : We wish to show that f (x) 0. <p> x p;i (11) P Y w t x p : To further bound ln Z t , we apply the following: Lemma 1 For all ff 2 <ref> [0; 1] </ref> and x 2 R, ln (1 ff (1 e x )) ffx + x 2 =8 : Proof: Fix ff 2 [0; 1], and let f (x) = ffx + x 2 =8 ln (1 ff (1 e x )) : We wish to show that f (x) 0. <p> When some of the components x p;i are zero, or very close to zero, we can use the following algorithm which is parameterized by a real number ff 2 <ref> [0; 1] </ref>. Let ~ x p = (1 ff=N )x p + (ff=N )1 where 1 is the all 1's vector. <p> Since x p;i 2 <ref> [r; 1] </ref>, and assuming that w t;i 0, it follows that this is bounded by P p=1 w t x p + 2r 2 : Thus, summing over all t T , we get 1 ku w T+1 k 2 2 T X P X ln u x p 2 N <p> Furthermore, we found that the learning rates used for deriving the bounds in the previous section ware too conservative. For the experiments reported in this section we used a fixed scheduling with a higher learning rate in the range <ref> [1; 5] </ref>. All these phenomena are depicted at the top part of Figure 3. The gradient ascent update with exponential parameterization appears inferior to all other methods. A good fixed scheduling for that method is difficult to obtain as the optimal learning rate has large oscillations. <p> This algorithm's performance was analyzed in the PAC model in <ref> [1] </ref>. 5. In one form, Holder's inequality states that, for non-negative a i , b i , X a i b i X a p ! 1=p i i for any positive p; q satisfying 1=p + 1=q = 1. 6.
Reference: 2. <author> J. Bridle. </author> <title> Probabilistic interpretation of feedforward classification network outputs with relationships to statistical pattern recognition. </title> <editor> In F. Fogelman-Souli and J. Herault, editors, Neuro-Computing: </editor> <booktitle> Algorithms, Architectures, and Applications. </booktitle> <address> New York: </address> <publisher> Springer Verlag, </publisher> <year> 1989. </year>
Reference-contexts: The contour lines for d RE are deformed ellipses that bend towards the vertices of the triangular feasible region. One can also get an update by re-parameterizing the probability vectors and doing unconstrained gradient ascent in the new parameter space. We use the standard exponential parameterization <ref> [2] </ref>: w i = e r i = P N j=1 e r j and maximize the function A COMPARISON OF NEW AND OLD ALGORITHMS 7 d RE (w t+1 jjw t ) (second row) and d 2 (w t+1 jjw t ) (third row) as a function of w t+1
Reference: 3. <author> T. </author> <title> Cover. Universal portfolios. </title> <journal> Mathematical Finance, </journal> <volume> 1(1) </volume> <pages> 1-29, </pages> <year> 1991. </year>
Reference-contexts: Experimentally, this version outperforms the known on-line versions of EM which is the EM algorithm with = 1 (The latter algorithm is called Generalized-EM). ************YORAM - ref missing*************** We have also applied the on-line version of our algorithms to a portfolio selection problem investigated by Cover <ref> [3] </ref>. Although Cover's analytical bounds appear better than ours, preliminary experimental results indicate that EM and EG outperform Cover's algorithm on historical stock market data. Furthermore, our algorithms are computationally efficient while Cover's algorithm is exponential in the number of possible investments.
Reference: 4. <author> A. P. Dempster, N. M. Laird, and D. B. Rubin. </author> <title> Maximum-likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> B39:1-38, </volume> <year> 1977. </year>
Reference-contexts: Most of the common techniques to solve this problem are based on either gradient ascent iterative schemes [9] or on the Expectation Maximization (EM) algorithm for parameter estimation from incomplete data <ref> [4] </ref>, [13]. We derive the standard iterative algorithms for the unsupervised mixture proportions estimation problem by placing them in a common hill-climbing framework. This framework is analogous to the one developed by Kivinen and Warmuth [7] 2 D.P. HELMBOLD, R.E. SCHAPIRE, Y. SINGER, M.K. WARMUTH for supervised on-line learning.
Reference: 5. <author> R. O. Duda and P. E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <year> 1973. </year>
Reference-contexts: 1. Introduction The problem of maximum-likelihood (ML) estimation of a mixture of densities is an important and well known learning problem <ref> [5] </ref>. ML estimators are asymptotically unbiased and are a basic tool for other more complicated problems such as clustering and learning hidden Markov models. We investigate the ML-estimation problem when the densities are given and only the mixture proportions are unknown. <p> Furthermore, we found that the learning rates used for deriving the bounds in the previous section ware too conservative. For the experiments reported in this section we used a fixed scheduling with a higher learning rate in the range <ref> [1; 5] </ref>. All these phenomena are depicted at the top part of Figure 3. The gradient ascent update with exponential parameterization appears inferior to all other methods. A good fixed scheduling for that method is difficult to obtain as the optimal learning rate has large oscillations.
Reference: 6. <author> G. H. Golub and C. F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> Johns-Hopkins University Press, </publisher> <year> 1989. </year>
Reference-contexts: The conjugate gradient search is a method for iteratively searching a quadratic cost function [9], <ref> [6] </ref>. When the cost function is non-quadratic, as is the likelihood function in our case, a variant of the conjugate gradient method can be devised.
Reference: 7. <author> J. Kivinen and M. K. Warmuth. </author> <title> Additive versus exponentiated gradient updates. </title> <booktitle> In Proceedings of the Twenty-Seventh Annual ACM Symposium on Theory of Computing, </booktitle> <year> 1995. </year> <note> 24 D.P. </note> <author> HELMBOLD, R.E. SCHAPIRE, Y. SINGER, </author> <title> M.K. </title> <type> WARMUTH </type>
Reference-contexts: We derive the standard iterative algorithms for the unsupervised mixture proportions estimation problem by placing them in a common hill-climbing framework. This framework is analogous to the one developed by Kivinen and Warmuth <ref> [7] </ref> 2 D.P. HELMBOLD, R.E. SCHAPIRE, Y. SINGER, M.K. WARMUTH for supervised on-line learning. Our goal is to maximize the log likelihood of the observations as a function of the mixture vector w, denoted by LogLike (w). This is computationally hard and requires iterative methods. <p> The Updates Kivinen and Warmuth <ref> [7] </ref> studied a general framework for on-line learning in which they derived algorithms for a broad class of loss functions. Here, we apply their method specifically to negative log-likelihood. <p> More precisely, we use the same distance function that motivates the update as a potential function to obtain worst-case cumulative loss bounds over sequences of updates (similar to the methods applied to the supervised case <ref> [7] </ref>). The natural loss of a mixture vector w t for our problem is LogLike (w t ). Note that this loss is unbounded since the likelihood for w t is zero when there is some x p for which w t x p = 0. <p> In the supervised case, one can obtain firm worst-case loss bounds with respect to the square loss for various updates by analyzing the progress <ref> [7] </ref>. But the square loss is bounded and it is not surprising that it is much harder to obtain strong loss bounds for our (unbounded loss) unsupervised setting. <p> HELMBOLD, R.E. SCHAPIRE, Y. SINGER, M.K. WARMUTH versus the square-root growth of the latter family. Similar observations were made in the supervised setting <ref> [7] </ref>, [8]. We also show below how to obtain bounds when the entries in the matrix have zero-valued components. We essentially average the data matrix with a uniform matrix (this *-Bayesian averaging was also used in [1]) and then use the averaged matrix to run our algorithm. <p> In the on-line setting each iteration typically uses only a single observation. It is therefore desirable to preserve information about the previous observations while improving the likelihood of the current observation. 2. A similar update for the case of linear regression was first given by Kivinen and Warmuth <ref> [7] </ref>. 3. i=1 w t;i rL (w t ) i = i=1 P p=1 x p w t = 1 P P w t x p 4. This algorithm's performance was analyzed in the PAC model in [1]. 5.
Reference: 8. <author> J. Kivinen and M. K. Warmuth. </author> <title> The perceptron algorithm vs. winnow: linear vs. logarithmic mistake bounds when few input variables are relevant. </title> <booktitle> In Proceedings of the Eighth Annual Workshop on Computational Learning Theory, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: HELMBOLD, R.E. SCHAPIRE, Y. SINGER, M.K. WARMUTH versus the square-root growth of the latter family. Similar observations were made in the supervised setting [7], <ref> [8] </ref>. We also show below how to obtain bounds when the entries in the matrix have zero-valued components. We essentially average the data matrix with a uniform matrix (this *-Bayesian averaging was also used in [1]) and then use the averaged matrix to run our algorithm.
Reference: 9. <author> D. G. Luenberger. </author> <title> Linear and Nonlinear Programming. </title> <publisher> Addison-Wesley, </publisher> <year> 1984. </year>
Reference-contexts: Most of the common techniques to solve this problem are based on either gradient ascent iterative schemes <ref> [9] </ref> or on the Expectation Maximization (EM) algorithm for parameter estimation from incomplete data [4], [13]. We derive the standard iterative algorithms for the unsupervised mixture proportions estimation problem by placing them in a common hill-climbing framework. <p> In particular, the standard EM algorithm (using = 1) has the property that the non-negativity constraints are always preserved. 4. Convergence and Progress In this section we discuss the convergence properties of the algorithms. Using standard methods, as in <ref> [9] </ref>, it can be shown that, given certain assumptions, all updates described in the previous section converge locally to an optimal ML solution, provided that the current mixture vector w t is close to the ML solution. <p> The conjugate gradient search is a method for iteratively searching a quadratic cost function <ref> [9] </ref>, [6]. When the cost function is non-quadratic, as is the likelihood function in our case, a variant of the conjugate gradient method can be devised.
Reference: 10. <author> R. M. Neal and G. E. Hinton. </author> <title> A new view of the EM algorithm that justifies incremental and other variants. </title> <type> Unpublished manuscript, </type> <year> 1993. </year>
Reference-contexts: The derivations of the learning rules using the above framework are simple and can readily be applied to other settings. They are similar to previous derivations found in the literature [13], <ref> [10] </ref>. 2. Definitions and Problem Statement Let R represent the real numbers. We say a vector v = (v 1 ; :::; v N ) 2 R N is a probability vector if, 8i : v i 0 and P n i=1 v i = 1. <p> Later iterations use a lower learning rate to aid convergence. The EM algorithm is in fact a limiting case of a more general approach usually called Generalized EM (GEM). **********YORAM - ref missing Neal and Hinton <ref> [10] </ref> considered an extention of EM which involves examining only a portion of the observation matrix X on each iteration.
Reference: 11. <author> B. C. Peters and H. F. Walker. </author> <title> An iterative procedure for obtaining maximum-likelihood estimates of the parameters for a mixture of normal distributions. </title> <journal> SIAM Journal of Applied Mathematics, </journal> <volume> 35 </volume> <pages> 362-378, </pages> <year> 1978. </year>
Reference-contexts: Our experimental evidence suggests that setting &gt; 1 results in a more effective update. These results agree with the infinitesimal analysis in the limit of n ! 1 based on a stochastic approximation approach <ref> [11] </ref>, [12], [13]. For the exponentiated gradient algorithm, we are able to prove rigorous polynomial bounds on the number of iterations needed to get an arbitrarily good ML-estimator. <p> The EM 1 update can be motivated by the likelihood equations, and the generalization to arbitrary was studied by Peters and Walker <ref> [11] </ref>, [12]. Since the 2 distance approximates the relative entropy it may not be surprising that the EM -update (7) also approximates the EG -update (6).
Reference: 12. <author> B. C. Peters and H. F. Walker. </author> <title> The numerical evaluation of the maximum-likelihood estimates of a subset of mixture proportions. </title> <journal> SIAM Journal of Applied Mathematics, </journal> <volume> 35 </volume> <pages> 447-452, </pages> <year> 1978. </year>
Reference-contexts: Our experimental evidence suggests that setting &gt; 1 results in a more effective update. These results agree with the infinitesimal analysis in the limit of n ! 1 based on a stochastic approximation approach [11], <ref> [12] </ref>, [13]. For the exponentiated gradient algorithm, we are able to prove rigorous polynomial bounds on the number of iterations needed to get an arbitrarily good ML-estimator. <p> The EM 1 update can be motivated by the likelihood equations, and the generalization to arbitrary was studied by Peters and Walker [11], <ref> [12] </ref>. Since the 2 distance approximates the relative entropy it may not be surprising that the EM -update (7) also approximates the EG -update (6). <p> Using standard methods, as in [9], it can be shown that, given certain assumptions, all updates described in the previous section converge locally to an optimal ML solution, provided that the current mixture vector w t is close to the ML solution. Moreover, using similar techniques, as in <ref> [12] </ref>, [13], it can be shown that it is better to use a learning rate &gt; 1 rather than the rate = 1. This implies that the EM algorithm is not optimal for this family of update rules.
Reference: 13. <author> R. A. Redner and H. F. Walker. </author> <title> Mixture densities, maximum likelihood, and the EM algorithm. </title> <journal> Siam Review, </journal> <volume> 26 </volume> <pages> 195-239, </pages> <year> 1984. </year>
Reference-contexts: Most of the common techniques to solve this problem are based on either gradient ascent iterative schemes [9] or on the Expectation Maximization (EM) algorithm for parameter estimation from incomplete data [4], <ref> [13] </ref>. We derive the standard iterative algorithms for the unsupervised mixture proportions estimation problem by placing them in a common hill-climbing framework. This framework is analogous to the one developed by Kivinen and Warmuth [7] 2 D.P. HELMBOLD, R.E. SCHAPIRE, Y. SINGER, M.K. WARMUTH for supervised on-line learning. <p> Our experimental evidence suggests that setting &gt; 1 results in a more effective update. These results agree with the infinitesimal analysis in the limit of n ! 1 based on a stochastic approximation approach [11], [12], <ref> [13] </ref>. For the exponentiated gradient algorithm, we are able to prove rigorous polynomial bounds on the number of iterations needed to get an arbitrarily good ML-estimator. However, this result assumes that there is a positive lower bound on the probability of each sample point under each of the given distributions. <p> The derivations of the learning rules using the above framework are simple and can readily be applied to other settings. They are similar to previous derivations found in the literature <ref> [13] </ref>, [10]. 2. Definitions and Problem Statement Let R represent the real numbers. We say a vector v = (v 1 ; :::; v N ) 2 R N is a probability vector if, 8i : v i 0 and P n i=1 v i = 1. <p> Using standard methods, as in [9], it can be shown that, given certain assumptions, all updates described in the previous section converge locally to an optimal ML solution, provided that the current mixture vector w t is close to the ML solution. Moreover, using similar techniques, as in [12], <ref> [13] </ref>, it can be shown that it is better to use a learning rate &gt; 1 rather than the rate = 1. This implies that the EM algorithm is not optimal for this family of update rules.
Reference: 14. <author> M. K. Warmuth and Y. Singer. </author> <title> Training algorithms for hidden markov models using entropy based distance functions. </title> <note> To appear in NIPS 96, </note> <year> 1996. </year>
Reference-contexts: One important area for future research is identifying good distance functions when the parameters do not form a probability vector. We have already applied our methodology for deriving updates to more complicated mixture estimation problems such as training hidden Markov models <ref> [14] </ref> and we are currently applying this methodology to mixtures of Gaussians with arbitrary mean and variance. In this more complicated setting we need distance functions that depend on the means and variances given to the Gaussians as well as the mixture probabilities assigned to them.
References-found: 14

