URL: http://www.cs.umd.edu/~tseng/papers/ics95.ps
Refering-URL: http://www.cs.umd.edu/~tseng/papers.html
Root-URL: 
Title: Unified Compilation Techniques for Shared and Distributed Address Space Machines  
Author: Chau-Wen Tseng, Jennifer M. Anderson, Saman P. Amarasinghe and Monica S. Lam 
Address: Stanford, CA 94305-4070  
Affiliation: Computer Systems Laboratory Stanford University  
Abstract: Parallel machines with shared address spaces are easy to program because they provide hardware support that allows each processor to transparently access non-local data. However, obtaining scalable performance can be difficult due to memory access and synchronization overhead. In this paper, we use profiling and simulation studies to identify the sources of parallel overhead. We demonstrate that compilation techniques for distributed address space machines can be very effective when used in compilers for shared address space machines. Automatic data decomposition can co-locate data and computation to improve locality. Data reorganization transformations can reduce harmful cache effects. Communication analysis can eliminate barrier synchronization. We present a set of unified compilation techniques that exemplify this convergence in compilers for shared and distributed address space machines, and illustrate their effectiveness using two example applications. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. P. Amarasinghe and M. S. Lam. </author> <title> Communication optimization and code generation for distributed memory machines. </title> <booktitle> In Proceedings of the SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 126-138, </pages> <address> Albuquerque, NM, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: to be important as the number of processors increases. 5 Related Work Our work builds upon shared-memory compiler algorithms for identifying parallelism [5, 12, 30] and performing program transformations [32, 33], as well as distributed-memory compilation techniques to select data decompositions [3] and explicitly manage address translation and data movement <ref> [1, 15] </ref>. Many previous researchers have examined performance issues on shared-memory architectures. Singh et al. [25] applied optimizations similar to those we considered by hand to representative computations in order to gain good performance.
Reference: [2] <author> J. M. Anderson, S. P. Amarasinghe, and M. S. Lam. </author> <title> Data and computation transformations for multiprocessors. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: Once the data decomposition is known, a simple algorithm exists to transform the data layout to make the region accessed by each processor contiguous <ref> [2] </ref>. Sometimes it is necessary to change the data decompositions dynamically as the program executes to minimize communication. A compiler for shared address space machines may choose to change the data layout accordingly at run time.
Reference: [3] <author> J. M. Anderson and M. S. Lam. </author> <title> Global optimizations for parallelism and locality on scalable parallel machines. </title> <booktitle> In Proceedings of the SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 112-125, </pages> <address> Albuquerque, NM, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: The user specifies the data-to-processor mapping using a language such as HPF [14], and the compiler infers the computation mapping by using the owner-computes rule [15]. Recently, a number of algorithms for finding data and/or computation decompositions automatically have been proposed <ref> [3, 4, 11, 24] </ref>. For shared address space machines, there is no need to find the data decompositions per se, since it is not necessary to allocate arrays in separate local address spaces. <p> Synchronization optimizations grow to be important as the number of processors increases. 5 Related Work Our work builds upon shared-memory compiler algorithms for identifying parallelism [5, 12, 30] and performing program transformations [32, 33], as well as distributed-memory compilation techniques to select data decompositions <ref> [3] </ref> and explicitly manage address translation and data movement [1, 15]. Many previous researchers have examined performance issues on shared-memory architectures. Singh et al. [25] applied optimizations similar to those we considered by hand to representative computations in order to gain good performance.
Reference: [4] <author> B. Bixby, K. Kennedy, and U. Kremer. </author> <title> Automatic data layout using 0-1 integer programming. </title> <booktitle> In Proceedings of the International Conference on Parallel Architectures and Compilation Techniques (PACT), </booktitle> <pages> pages 111-122, </pages> <address> Montreal, Canada, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: The user specifies the data-to-processor mapping using a language such as HPF [14], and the compiler infers the computation mapping by using the owner-computes rule [15]. Recently, a number of algorithms for finding data and/or computation decompositions automatically have been proposed <ref> [3, 4, 11, 24] </ref>. For shared address space machines, there is no need to find the data decompositions per se, since it is not necessary to allocate arrays in separate local address spaces.
Reference: [5] <author> W. Blume et al. </author> <title> Polaris: The next generation in parallelizing compilers,. </title> <booktitle> In Proceedings of the Seventh Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Ithaca, NY, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: Reducing false sharing is very significant in certain cases. Synchronization optimizations grow to be important as the number of processors increases. 5 Related Work Our work builds upon shared-memory compiler algorithms for identifying parallelism <ref> [5, 12, 30] </ref> and performing program transformations [32, 33], as well as distributed-memory compilation techniques to select data decompositions [3] and explicitly manage address translation and data movement [1, 15]. Many previous researchers have examined performance issues on shared-memory architectures.
Reference: [6] <author> W. J. Bolosky and M. L. Scott. </author> <title> False sharing and its effect on shared memory performance. </title> <booktitle> In Proceedings of the USENIX Symposium on Experiences with Distributed and Multiprocessor Systems (SEDMS IV), </booktitle> <pages> pages 57-71, </pages> <address> San Diego, CA, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: Many previous researchers have examined performance issues on shared-memory architectures. Singh et al. [25] applied optimizations similar to those we considered by hand to representative computations in order to gain good performance. Others evaluated the benefits of co-locating data and computation [9, 22], as well as false sharing <ref> [6, 8, 10, 21, 27] </ref>. These approaches focused on individual optimizations and were generally applied by hand. In contrast, we have shown how they may be implemented in a compiler by adapting well-known techniques from distributed-memory compilers for shared-memory machines.
Reference: [7] <author> R. Cytron, J. Lipkis, and E. Schonberg. </author> <title> A compiler-assisted approach to SPMD execution. </title> <booktitle> In Proceedings of Supercomputing '90, </booktitle> <pages> pages 398-406, </pages> <address> New York, NY, </address> <month> November </month> <year> 1990. </year>
Reference-contexts: Sequential parts of the program are executed by a single master thread, as in traditional compilers for shared address space machines. However, by employing a hybrid SPMD model, the computation assigned to each worker processor can span a series of parallel loops <ref> [7] </ref>. Barriers may be eliminated if they are unnecessary, or they may be replaced with more efficient point-to-point synchronization operations [29]. Barrier optimization requires analysis of dependences across loop boundaries. <p> Compared to his work we examine actual instances of compiler techniques and evaluate their impact on performance. Cytron et al. <ref> [7] </ref> were the first to consider mixing fork-join and SPMD models. Their goal was to eliminate thread startup overhead and increase opportunities for privatization; they carefully considered safety conditions. In comparison, we use local SPMD regions to enable compile-time computation partition and synchronization elimination through communication analysis.
Reference: [8] <author> S. J. Eggers and T. E. Jeremiassen. </author> <title> Eliminating false sharing. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <pages> pages 377-381, </pages> <address> St. Charles, IL, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: Many previous researchers have examined performance issues on shared-memory architectures. Singh et al. [25] applied optimizations similar to those we considered by hand to representative computations in order to gain good performance. Others evaluated the benefits of co-locating data and computation [9, 22], as well as false sharing <ref> [6, 8, 10, 21, 27] </ref>. These approaches focused on individual optimizations and were generally applied by hand. In contrast, we have shown how they may be implemented in a compiler by adapting well-known techniques from distributed-memory compilers for shared-memory machines.
Reference: [9] <author> J. Fang and M. Lu. </author> <title> A solution of cache ping-pong problem in RISC based parallel processing systems. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: Many previous researchers have examined performance issues on shared-memory architectures. Singh et al. [25] applied optimizations similar to those we considered by hand to representative computations in order to gain good performance. Others evaluated the benefits of co-locating data and computation <ref> [9, 22] </ref>, as well as false sharing [6, 8, 10, 21, 27]. These approaches focused on individual optimizations and were generally applied by hand. In contrast, we have shown how they may be implemented in a compiler by adapting well-known techniques from distributed-memory compilers for shared-memory machines.
Reference: [10] <author> E. D. Granston and H. Wishoff. </author> <title> Managing pages in shared virtual memory systems: Getting the compiler into the game. </title> <booktitle> In Proceedings of the 1993 ACM International Conference on Supercomputing, </booktitle> <address> Tokyo, Japan, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: Many previous researchers have examined performance issues on shared-memory architectures. Singh et al. [25] applied optimizations similar to those we considered by hand to representative computations in order to gain good performance. Others evaluated the benefits of co-locating data and computation [9, 22], as well as false sharing <ref> [6, 8, 10, 21, 27] </ref>. These approaches focused on individual optimizations and were generally applied by hand. In contrast, we have shown how they may be implemented in a compiler by adapting well-known techniques from distributed-memory compilers for shared-memory machines.
Reference: [11] <author> M. Gupta and P. Banerjee. </author> <title> Demonstration of automatic data partitioning techniques for parallelizing compilers on multicomputers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(2) </volume> <pages> 179-193, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: The user specifies the data-to-processor mapping using a language such as HPF [14], and the compiler infers the computation mapping by using the owner-computes rule [15]. Recently, a number of algorithms for finding data and/or computation decompositions automatically have been proposed <ref> [3, 4, 11, 24] </ref>. For shared address space machines, there is no need to find the data decompositions per se, since it is not necessary to allocate arrays in separate local address spaces.
Reference: [12] <author> M. W. Hall, S. Amarasinghe, and B. Murphy. </author> <title> Interprocedural analysis for parallelization: Design and experience. </title> <booktitle> In Proceedings of the Seventh SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 650-655, </pages> <address> San Francisco, CA, </address> <month> February </month> <year> 1995. </year>
Reference-contexts: The SUIF compiler system has many of the common optimizations found in commercial compiler systems. A distinctive feature of this compiler system is that it is capable of finding parallelism across procedure boundaries. The compiler contains a large set of interprocedural analyses <ref> [12] </ref>: symbolic analysis, data dependence, array privatization [30], as well as reductions to both scalar and array variables. The compiler also contains some of the novel optimizations described in this paper. <p> The study analyzed the performance of a collection of programs from the SPEC, NAS and RICEPS benchmark suites that are successfully parallelized by the SUIF interprocedural parallelizer <ref> [12] </ref>. To quantify the success of parallelization, we define parallel coverage to be the percentage of instructions executed within parallel regions. <p> Reducing false sharing is very significant in certain cases. Synchronization optimizations grow to be important as the number of processors increases. 5 Related Work Our work builds upon shared-memory compiler algorithms for identifying parallelism <ref> [5, 12, 30] </ref> and performing program transformations [32, 33], as well as distributed-memory compilation techniques to select data decompositions [3] and explicitly manage address translation and data movement [1, 15]. Many previous researchers have examined performance issues on shared-memory architectures.
Reference: [13] <author> M. Heinrich, J. Kuskin, D. Ofelt, J. Heinlein, J. P. Singh, R. Simoni, K. Gharachorloo, J. Baxter, D. Nakahira, M. Horowitz, A. Gupta, M. Rosenblum, and J. Hennessy. </author> <title> The performance impact of flexibility in the Stanford FLASH multiprocessor. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VI), </booktitle> <pages> pages 274-284, </pages> <month> Oc-tober </month> <year> 1994. </year>
Reference-contexts: The target machine used for this study is an advanced directory-based cache-coherent non-uniform memory access (CC-NUMA) multiprocessor, similar to the FLASH multiprocessor <ref> [13] </ref>. It has a 200 MHz processor, a 100 MHz 256-bit local memory bus and a 200 MHz 16-bit wide mesh network interconnect. Each processor has a single 128KB, 4-way set-associative cache, whose cache line size is 128 bytes. <p> Amortization of communication overhead. An important optimization used in compilers for distributed address space machines is to aggregate communication into large messages so as to amortize the high message passing overhead. This analysis may be applied for shared address space machines that provide efficient block data transfer <ref> [13, 17] </ref>. This technique is also useful for moving a large collection of data on separate caches lines. The compiler can pack the data to be communicated into contiguous locations, and thus reduce the number of cache lines transferred.
Reference: [14] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran language specification. </title> <booktitle> Scientific Programming, </booktitle> <address> 2(1-2):1-170, </address> <year> 1993. </year>
Reference-contexts: This problem requires global analysis be applied across different loop nests to determine which loops to parallelize. A popular approach to this problem is to leave the primary responsibility to the user. The user specifies the data-to-processor mapping using a language such as HPF <ref> [14] </ref>, and the compiler infers the computation mapping by using the owner-computes rule [15]. Recently, a number of algorithms for finding data and/or computation decompositions automatically have been proposed [3, 4, 11, 24].
Reference: [15] <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: A popular approach to this problem is to leave the primary responsibility to the user. The user specifies the data-to-processor mapping using a language such as HPF [14], and the compiler infers the computation mapping by using the owner-computes rule <ref> [15] </ref>. Recently, a number of algorithms for finding data and/or computation decompositions automatically have been proposed [3, 4, 11, 24]. For shared address space machines, there is no need to find the data decompositions per se, since it is not necessary to allocate arrays in separate local address spaces. <p> to be important as the number of processors increases. 5 Related Work Our work builds upon shared-memory compiler algorithms for identifying parallelism [5, 12, 30] and performing program transformations [32, 33], as well as distributed-memory compilation techniques to select data decompositions [3] and explicitly manage address translation and data movement <ref> [1, 15] </ref>. Many previous researchers have examined performance issues on shared-memory architectures. Singh et al. [25] applied optimizations similar to those we considered by hand to representative computations in order to gain good performance.
Reference: [16] <author> K. Kennedy and K. S. M c Kinley. </author> <title> Optimizing for parallelism and data locality. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <address> Washington, DC, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Traditionally, research in compilers for shared address space machines has focused mainly on the extraction of parallelism in sequential programs. Only recently have research compilers started to use some loop-level optimizations to enhance data locality and minimize communication <ref> [16, 26, 32] </ref>. On the other hand, the main focus in compilers for distributed address space machines is in managing the distributed data. From the distribution of the data, the compiler derives the computation assignment and the communication optimizations. <p> This condition is similar to the strip-mine and interchange transformation used to increase granularity of parallelism while preserving data locality <ref> [16, 32] </ref>. For TOMCATV, this optimization can be applied to the COMP DECOMP versions of the program to move barriers following inner parallel loops outwards. More advanced synchronization optimizations are able to eliminate several barriers between adjacent parallel loop nests, reducing the number of barrier executions by half.
Reference: [17] <author> D. Kranz, K. Johnson, A. Agarwal, J. Kubiatowicz, and B. H. Lim. </author> <title> Integrating message-passing and shared-memory: Early experience. </title> <booktitle> In Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 54-63, </pages> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Amortization of communication overhead. An important optimization used in compilers for distributed address space machines is to aggregate communication into large messages so as to amortize the high message passing overhead. This analysis may be applied for shared address space machines that provide efficient block data transfer <ref> [13, 17] </ref>. This technique is also useful for moving a large collection of data on separate caches lines. The compiler can pack the data to be communicated into contiguous locations, and thus reduce the number of cache lines transferred. <p> For instance, DASH provides an operation where the producer can initiate updates to remote copies to reduce latency. However, this application of distributed address space compilation technique has been shown to be useful mainly for implementations of global synchronization primitives such as barriers and accumulations <ref> [17] </ref>. 4 Case Studies We have implemented many of the optimizations described in this paper in the SUIF compilation system [31]. The SUIF compiler takes as input either Fortran or C, performs multiple optimization passes, then generates C code that can be linked with a portable run-time library. <p> Their goal was to eliminate thread startup overhead and increase opportunities for privatization; they carefully considered safety conditions. In comparison, we use local SPMD regions to enable compile-time computation partition and synchronization elimination through communication analysis. Researchers using the Alewife multiprocessor compared the benefits of message-passing and shared memory <ref> [17] </ref>; they found message passing to be advantageous mainly for improving synchronization primitives by coupling synchronization and data movement. 6 Conclusions In this paper, we examined issues in compiling for shared address space architectures. Despite their programmability, it can be difficult to achieve scalable speedups.
Reference: [18] <author> Kuck & Associates, Inc. </author> <title> KAP User's Guide. </title> <address> Champaign, IL 61820, </address> <year> 1988. </year>
Reference-contexts: We compiled the C programs produced by SUIF using gcc version 2.5.8 at optimization level -O3. Performance is presented as speedups over the best sequential version of each program. 4.2 Evaluation The basic SUIF compiler has capabilities similar to traditional shared-memory compilers such as KAP <ref> [18] </ref>. These abilities include parallelizing outer loops, scheduling iterations across processors evenly in contiguous blocks, and performing program transformations such loop permutation to enhance data locality. We consider this system to be our baseline compiler, and label its performance as BASE in each figure.
Reference: [19] <author> J. Larus. </author> <title> Compiling for shared-memory and message-passing computers. </title> <journal> ACM Letters on Programming Languages and Systems, </journal> <volume> 2(1-4):165-180, </volume> <month> March-December </month> <year> 1993. </year>
Reference-contexts: In contrast, we have shown how they may be implemented in a compiler by adapting well-known techniques from distributed-memory compilers for shared-memory machines. Larus has compared implementing global address spaces in software using a distributed-memory compiler compared to hardware-based implementations <ref> [19] </ref>. He speculates that distributed-memory compilers are desirable because they can more closely exploit underlying architectural features in certain key cases; however, shared-memory hardware is desirable in the cases where the compiler fails. Compared to his work we examine actual instances of compiler techniques and evaluate their impact on performance.
Reference: [20] <author> D. Lenoski, J. Laudon, T. Joe, D. Nakahira, L. Stevens, A. Gupta, and J. Hennessy. </author> <title> The DASH prototype: Implementation and performance. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <pages> pages 92-105, </pages> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: Table 1 shows some of the programs included in the study; all these programs have a parallel coverage of at least 90% as measured by pixie on the Stanford DASH multiprocessor <ref> [20] </ref>. <p> The 4-way version was the same cache model used for collecting our previous simulation results. Speedups for each program were obtained on the DASH multiprocessor, a cache-coherent NUMA architecture <ref> [20] </ref>. The machine we used for our experiments consists of 32 processors, organized into 8 clusters of 4 processors each. Each processor is a 33MHz MIPS R3000, and has a 64KB first-level cache and a 256KB second-level cache.
Reference: [21] <author> H. Li and K. C. Sevcik. NUMACROS: </author> <title> Data parallel programming on NUMA multiprocessors. </title> <booktitle> In Proceedings of the USENIX Symposium on Experiences with Distributed and Multiprocessor Systems (SEDMS IV), </booktitle> <pages> pages 247-263, </pages> <address> San Diego, CA, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: Many previous researchers have examined performance issues on shared-memory architectures. Singh et al. [25] applied optimizations similar to those we considered by hand to representative computations in order to gain good performance. Others evaluated the benefits of co-locating data and computation [9, 22], as well as false sharing <ref> [6, 8, 10, 21, 27] </ref>. These approaches focused on individual optimizations and were generally applied by hand. In contrast, we have shown how they may be implemented in a compiler by adapting well-known techniques from distributed-memory compilers for shared-memory machines.
Reference: [22] <author> E. Markatos and T. LeBlanc. </author> <title> Using processor affinity in loop scheduling on shared-memory multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 5(4) </volume> <pages> 379-400, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Many previous researchers have examined performance issues on shared-memory architectures. Singh et al. [25] applied optimizations similar to those we considered by hand to representative computations in order to gain good performance. Others evaluated the benefits of co-locating data and computation <ref> [9, 22] </ref>, as well as false sharing [6, 8, 10, 21, 27]. These approaches focused on individual optimizations and were generally applied by hand. In contrast, we have shown how they may be implemented in a compiler by adapting well-known techniques from distributed-memory compilers for shared-memory machines.
Reference: [23] <author> T. Mowry, M. S. Lam, and A. Gupta. </author> <title> Design and evaluation of a compiler algorithm for prefetching. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-V), </booktitle> <pages> pages 62-73, </pages> <address> Boston, MA, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: Due to the high cache miss penalty, even recent microprocessors include prefetch operations in their instruction sets so that cache miss penalties can be overlapped with computation of other data. Because prefetches incur additional software instruction overhead, it is thus necessary to avoid issuing unnecessary prefetch operations <ref> [23] </ref>. Communication analysis similar to what is used for distributed address space machines is thus also useful for compilers for shared address space machines. Prefetch algorithms are different in some ways. For example, we must also issue prefetches to local data that miss in the cache.
Reference: [24] <author> T. J. Sheffler, R. Schreiber, J. R. Gilbert, and S. Chatterjee. </author> <title> Aligning parallel arrays to reduce communication. </title> <booktitle> In Frontiers '95: The 5th Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 324-331, </pages> <address> McLean, VA, </address> <month> February </month> <year> 1995. </year>
Reference-contexts: The user specifies the data-to-processor mapping using a language such as HPF [14], and the compiler infers the computation mapping by using the owner-computes rule [15]. Recently, a number of algorithms for finding data and/or computation decompositions automatically have been proposed <ref> [3, 4, 11, 24] </ref>. For shared address space machines, there is no need to find the data decompositions per se, since it is not necessary to allocate arrays in separate local address spaces.
Reference: [25] <author> J.P. Singh, T. Joe, A. Gupta, and J. L. Hennessy. </author> <title> An empirical comparison of the Kendall Square Research KSR-1 and Stanford DASH multiprocessors. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 214-225, </pages> <address> Portland, OR, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: Many previous researchers have examined performance issues on shared-memory architectures. Singh et al. <ref> [25] </ref> applied optimizations similar to those we considered by hand to representative computations in order to gain good performance. Others evaluated the benefits of co-locating data and computation [9, 22], as well as false sharing [6, 8, 10, 21, 27].
Reference: [26] <author> O. Temam, E. D. Granston, and W. Jalby. </author> <title> To copy or not to copy: A compile-time technique for assessing when data copying should be used to eliminate cache conflicts. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 410-419, </pages> <address> Portland, OR, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: Traditionally, research in compilers for shared address space machines has focused mainly on the extraction of parallelism in sequential programs. Only recently have research compilers started to use some loop-level optimizations to enhance data locality and minimize communication <ref> [16, 26, 32] </ref>. On the other hand, the main focus in compilers for distributed address space machines is in managing the distributed data. From the distribution of the data, the compiler derives the computation assignment and the communication optimizations.
Reference: [27] <author> J. Torrellas, M. S. Lam, and J. L. Hennessy. </author> <title> False sharing and spatial locality in multiprocessor caches. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 43(6) </volume> <pages> 651-663, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Many previous researchers have examined performance issues on shared-memory architectures. Singh et al. [25] applied optimizations similar to those we considered by hand to representative computations in order to gain good performance. Others evaluated the benefits of co-locating data and computation [9, 22], as well as false sharing <ref> [6, 8, 10, 21, 27] </ref>. These approaches focused on individual optimizations and were generally applied by hand. In contrast, we have shown how they may be implemented in a compiler by adapting well-known techniques from distributed-memory compilers for shared-memory machines.
Reference: [28] <author> E. Torrie, C-W. Tseng, M. Martonosi, and M. W. Hall. </author> <title> Evaluating the impact of advanced memory systems on compiler-parallelized codes. </title> <booktitle> In Proceedings of the International Conference on Parallel Architectures and Compilation Techniques (PACT), </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: To discover why speedups are lower than expected, we consider a recent simulation study by Torrie et al. that evaluates the impact of advanced memory systems on the behavior of compiler-parallelized programs <ref> [28] </ref>. The study analyzed the performance of a collection of programs from the SPEC, NAS and RICEPS benchmark suites that are successfully parallelized by the SUIF interprocedural parallelizer [12]. To quantify the success of parallelization, we define parallel coverage to be the percentage of instructions executed within parallel regions. <p> Barriers are executed following each parallel loop to ensure that all processors have completed before the master continues. The simulation study found that even if the compiler is able to detect significant parallelism, the performance obtained with this simple back-end strategy can be rather poor <ref> [28] </ref>. The target machine used for this study is an advanced directory-based cache-coherent non-uniform memory access (CC-NUMA) multiprocessor, similar to the FLASH multiprocessor [13]. It has a 200 MHz processor, a 100 MHz 256-bit local memory bus and a 200 MHz 16-bit wide mesh network interconnect. <p> We ran the pixified programs on DASH with 16 processors and tallied the instruction counts for each processor. The data in this figure can only partially attribute for the idle cycles shown in Figure 2. Here we ran the entire program, whereas in the simulation study <ref> [28] </ref> a limited number of time steps were run over the full data set. Sequential execution. As prescribed by Amdahl's law, parallel performance is heavily dependent on having a small sequential execution bottleneck. <p> Since each processor only operates on a subset of the data, the addresses accessed by each processor are often distributed throughout the shared address space, thus potentially leading to significant conflict misses. In the simulation study of compiler-parallelized codes, the causes of cache misses were analyzed <ref> [28] </ref> and are shown in Figure 4. The experiment classifies each miss as either a cold miss, which occurs the first time each processor accesses data, a true sharing miss, a false sharing miss, and finally a replacement miss, which may either be a conflict miss or a capacity miss. <p> We also examine speedups on the Stanford DASH multiprocessor to evaluate how the optimizations affect performance on a real machine. 4.1 Experimental Setup We obtained statistics on the memory and synchronization behavior of each example program with the MemSpy simulator used by Torrie et al. <ref> [28] </ref>. We simulated two cache configurations for 16 processors: 128KB, 128 byte cache line, and either direct-mapped or 4-way set associative LRU caches. The 4-way version was the same cache model used for collecting our previous simulation results.
Reference: [29] <author> C-W. Tseng. </author> <title> Compiler optimizations for eliminating barrier synchronization. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: However, by employing a hybrid SPMD model, the computation assigned to each worker processor can span a series of parallel loops [7]. Barriers may be eliminated if they are unnecessary, or they may be replaced with more efficient point-to-point synchronization operations <ref> [29] </ref>. Barrier optimization requires analysis of dependences across loop boundaries. While inter-loop analysis is not used in conventional compilers for shared address space machines, such analysis is required to generate the necessary communication code for distributed address space machines.
Reference: [30] <author> P. Tu and D. Padua. </author> <title> Automatic array privatization. </title> <booktitle> In Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: The SUIF compiler system has many of the common optimizations found in commercial compiler systems. A distinctive feature of this compiler system is that it is capable of finding parallelism across procedure boundaries. The compiler contains a large set of interprocedural analyses [12]: symbolic analysis, data dependence, array privatization <ref> [30] </ref>, as well as reductions to both scalar and array variables. The compiler also contains some of the novel optimizations described in this paper. <p> Reducing false sharing is very significant in certain cases. Synchronization optimizations grow to be important as the number of processors increases. 5 Related Work Our work builds upon shared-memory compiler algorithms for identifying parallelism <ref> [5, 12, 30] </ref> and performing program transformations [32, 33], as well as distributed-memory compilation techniques to select data decompositions [3] and explicitly manage address translation and data movement [1, 15]. Many previous researchers have examined performance issues on shared-memory architectures.
Reference: [31] <author> R. P. Wilson, R. S. French, C. S. Wilson, S. P. Amarasinghe, J. M. Anderson, S. W. K. Tjiang, S.-W. Liao, C.-W. Tseng, M. W. Hall, M. S. Lam, and J. L. Hennessy. </author> <title> SUIF: An infrastructure for research on parallelizing and optimizing compilers. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 29(12) </volume> <pages> 31-37, </pages> <month> December </month> <year> 1994. </year>
Reference-contexts: Throughout the paper, we attempt to substantiate our argument with experimental measurements. These experiments are based on the SUIF (Stanford University Intermediate Format) parallelizing compiler <ref> [31] </ref>. The SUIF compiler system has many of the common optimizations found in commercial compiler systems. A distinctive feature of this compiler system is that it is capable of finding parallelism across procedure boundaries. <p> However, this application of distributed address space compilation technique has been shown to be useful mainly for implementations of global synchronization primitives such as barriers and accumulations [17]. 4 Case Studies We have implemented many of the optimizations described in this paper in the SUIF compilation system <ref> [31] </ref>. The SUIF compiler takes as input either Fortran or C, performs multiple optimization passes, then generates C code that can be linked with a portable run-time library. Currently only the shared address space machine code generator is in place. Figure 5 illustrates the basic architecture of the compiler.
Reference: [32] <author> M. E. Wolf and M. S. Lam. </author> <title> A loop transformation theory and an algorithm to maximize parallelism. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 452-471, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Traditionally, research in compilers for shared address space machines has focused mainly on the extraction of parallelism in sequential programs. Only recently have research compilers started to use some loop-level optimizations to enhance data locality and minimize communication <ref> [16, 26, 32] </ref>. On the other hand, the main focus in compilers for distributed address space machines is in managing the distributed data. From the distribution of the data, the compiler derives the computation assignment and the communication optimizations. <p> This condition is similar to the strip-mine and interchange transformation used to increase granularity of parallelism while preserving data locality <ref> [16, 32] </ref>. For TOMCATV, this optimization can be applied to the COMP DECOMP versions of the program to move barriers following inner parallel loops outwards. More advanced synchronization optimizations are able to eliminate several barriers between adjacent parallel loop nests, reducing the number of barrier executions by half. <p> Reducing false sharing is very significant in certain cases. Synchronization optimizations grow to be important as the number of processors increases. 5 Related Work Our work builds upon shared-memory compiler algorithms for identifying parallelism [5, 12, 30] and performing program transformations <ref> [32, 33] </ref>, as well as distributed-memory compilation techniques to select data decompositions [3] and explicitly manage address translation and data movement [1, 15]. Many previous researchers have examined performance issues on shared-memory architectures.
Reference: [33] <author> M. J. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1989. </year>
Reference-contexts: Reducing false sharing is very significant in certain cases. Synchronization optimizations grow to be important as the number of processors increases. 5 Related Work Our work builds upon shared-memory compiler algorithms for identifying parallelism [5, 12, 30] and performing program transformations <ref> [32, 33] </ref>, as well as distributed-memory compilation techniques to select data decompositions [3] and explicitly manage address translation and data movement [1, 15]. Many previous researchers have examined performance issues on shared-memory architectures.
References-found: 33

