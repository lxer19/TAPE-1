URL: ftp://ftp-pubs.lcs.mit.edu/pub/lcs-pubs/tr.outbox/MIT-LCS-TR-574.ps.gz
Refering-URL: ftp://ftp-pubs.lcs.mit.edu/pub/lcs-pubs/listings/newcat.html
Root-URL: 
Title: Distributed Garbage Collection in a Client-Server, Transactional, Persistent Object System  
Author: by Umesh Maheshwari Barbara H. Liskov Campbell L. Searle 
Degree: B.Tech., Indian Institute of Technology, Delhi, 1990 Submitted to the Department of Electrical Engineering and Computer Science in partial fulfillment of the requirements for the degree of Master of Science in Electrical Engineering and Computer Science at the  All rights reserved. Author  Certified by  Professor of Software Science and Engineering Thesis Supervisor Accepted by  Chairman, Departmental Committee on Graduate Students  
Date: February 1993  February 8, 1993  
Affiliation: Massachusetts Institute of Technology  c Massachusetts Institute of Technology 1993.  Department of Electrical Engineering and Computer Science  NEC  
Abstract-found: 0
Intro-found: 1
Reference: [Ady93] <author> A. Adya. </author> <title> A Distributed Commit Protocol for Optimistic Concurrency Control, </title> <type> Master's thesis, </type> <institution> Massachussetts Institute of Technology, </institution> <year> 1993 </year> <month> (forthcoming). </month>
Reference-contexts: The front end selects a participant as the coordinator, and sends it the necessary information about the transaction, including copies of newly persistent objects and modified objects. The coordinator coordinates the commit with other participants. The 2-phase commit protocol proceeds as follows <ref> [Gra78, Ady93] </ref>: Phase 1 (or, the prepare phase) initiate : The coordinator assigns the transaction a tid that is unique across Thor. All messages 52 sent on behalf of the transaction are stamped with its tid. The coordinator sends out prepare messages to the participants. <p> To resolve the problem, we exploit two attributes of the distributed commit protocol in Thor <ref> [Ady93] </ref>: 1. Each transaction is assigned a tid by its coordinator that is unique across the Thor world. The tid consists of a timestamp generated locally by the clock at the coordinator's node, which is 77 augmented with the OR-id of the coordinator for distinctness. <p> The distributed commit protocol in Thor sets the prepare threshold at a level that makes the GC protocol particularly efficient. The prepare threshold at an OR is set to the install watermark, that is, the highest tid of any transaction that has been installed at that OR <ref> [Ady93] </ref> 1 . We show below that this leads to an efficient removal of OR table entries. Consider an or-surrogate at the receiver and a matching entry in the owner's OR table that is stamped with the tid of transaction X .
Reference: [Ali84] <author> K. A. M. Ali. </author> <title> Garbage Collection Schemes for Distributed Storage Systems. </title> <booktitle> Proceedings of Workshop on Implementation of Functional Languages, </booktitle> <pages> pages 422-428, </pages> <address> Aspe-nas, Sweden, </address> <month> Feb </month> <year> 1985. </year>
Reference-contexts: Multiple roots on different nodes can be handled by waiting until all roots have sent out an ack message. One node is selected as the leader, which checks for this condition and notifies all other nodes <ref> [Ali84] </ref>. 21 2.2.2 Criticism Some of the marking messages sent between two nodes can be batched together into one packet. For example, in Figure 2-4, m1 and m2 can be batched. <p> The following subsections discuss various forms of reference tracking, focusing on their increase and decrease protocols. 26 2.4.1 Reference Flagging In this scheme, the only information kept is which local objects are referenced by one or more remote references <ref> [Ali84, Juu90] </ref>. Such objects are distinguished by the presence of an inlist entry; the entry does not contain any other information. This is similar to using a 1-bit wide reference count. <p> A bad selection results in wasted computation. Another drawback is that each object needs to have an extra field beside the one used to hold the actual reference count. 2.5.3 Complementary Tracing The idea here is to invoke a global tracing periodically to collect circular garbage <ref> [Ali84, Juu90] </ref>. The drawbacks of this scheme are the same as those of global tracing itself (Section 2.2.2), except with reduced severity because tracing as a complementary scheme is run infrequently, and its responsibility is limited to collecting circular garbage.
Reference: [Bar78] <author> J. F. Bartlett. </author> <title> Compacting Garbage Collection with Ambiguous Roots. </title> <type> Techincal Report 88/2, </type> <institution> Digital Western Research Laboratory, </institution> <address> Palo Alto CA, </address> <month> Feb </month> <year> 1988. </year>
Reference: [Bev87] <author> D. I. Bevan. </author> <title> Distributed Garbage Collection Using Reference Counting. </title> <booktitle> Lecture Notes in Computer Science 259, </booktitle> <pages> pages 176-187, </pages> <publisher> Springer-Verlag, </publisher> <month> Jun </month> <year> 1987. </year>
Reference-contexts: In the parallel scheme, a race condition between the two messages cannot be avoided using simple timestamp protocols, because 1 The use of reference listing preempts the use of some other techniques, like weighted reference counting <ref> [Wen79, Bev87] </ref>. As discussed in Section 2.4.3, weighted reference counting requires the use of reliable decrement messages, and suffers from the weight-underflow problem. 88 the insert message is sent by the coordinator while the trim message is sent by the receiver itself.
Reference: [Bis77] <author> P. B. Bishop. </author> <title> Computer Systems with a Very Large Address Space, and Garbage Collection. </title> <type> Technical Report MIT/LCS/TR-178, </type> <institution> MIT Laboratory for Computer Science, </institution> <address> Cambridge MA, </address> <month> May </month> <year> 1977. </year>
Reference-contexts: This section will ignore issues pertinent to distributed systems, such as message passing and fault-tolerance; they will be introduced in the next section. Indeed, the use of separate areas is also useful in singe-site systems with large address spaces <ref> [Bis77] </ref>: 1. The mutator need not wait for the collection of the entire address space. <p> It tracks incoming remote references in some form or other. In general, an inlist entry contains a reference to the local object and some information about incoming remote references to that object. Different schemes 2 This idea can be traced back to <ref> [Bis77] </ref>, which uses links for reference counting in one direction, and cables for tracing in the other. 25 result in differences in the message passing protocol and fault tolerance. <p> is not described here. (Caution: the scheme is not related in any way to the generational collection described in Section 2.3.2.) 2.4.4 Reference Listing Here, instead of keeping just a count, the inlist entry for an object keeps the list of all nodes that contain a remote reference to it <ref> [Bis77, SGP90] </ref>. Assume that the nodes are identified by distinct node-ids. It is common in the literature to break the inlist entry into separate elements, each containing a reference to the local object and a single node-id from where the object is referenced. <p> With respect to Figure 2-13, the collection of the cycle that passes through Node B and C should not require cooperation from Node A. 2.5.1 Object Migration The aim here is to consolidate an unreachable distributed cycle into a single area where it can be collected by the local collector <ref> [Bis77, SGP90] </ref>. This is implemented as follows: if an object is 37 unreachable from the local primary roots but is remotely referenced, it is moved to a node that references it. The local GC helps in finding such objects.
Reference: [Che70] <author> C. J. </author> <title> Cheney. A Nonrecursive List Compacting Algorithm. </title> <journal> Communications of the ACM, </journal> <volume> 13(11), </volume> <pages> pages 677-678, </pages> <month> Nov </month> <year> 1970. </year>
Reference-contexts: Each object reached during the traversal is recorded to be live and scanned for references to other objects. Objects that are not reached by the end of the traversal are garbage. Two popular schemes of this kind are mark-and-sweep and copy-collection <ref> [Che70] </ref>. The first scheme records reachable objects by marking them, and collects all objects that remain unmarked into a free-list. It needs a GC queue (or a stack) to run a breadth-first (or depth first) marking algorithm.
Reference: [Chr84] <author> T. W. Christopher. </author> <title> Reference Count Garbage Collection. </title> <journal> Software Practice and Experience, </journal> <volume> 14(6), </volume> <pages> pages 503-507, </pages> <month> Jun </month> <year> 1984. </year>
Reference-contexts: In [LPQ92], the group derives the information from the reference counts in the inlists at each node. It uses <ref> [Chr84] </ref> to compute precisely how many times an inlist entry is referenced from outside the group; if this is non-zero, the entry is included in the group's roots. We claim that the same could be achieved more easily if reference listing were used (Section 2.4.4).
Reference: [Day93] <author> M. Day. </author> <title> Managing a Cache of Swizzled Objects and Surrogates, </title> <type> Ph.D. thesis, </type> <institution> Mas-sachussetts Institute of Technology, </institution> <year> 1993 </year> <month> (forthcoming). </month>
Reference-contexts: Besides deleting unreachable objects, the front end can create extra space by turning reachable objects that are unlikely to be accessed into fe-surrogates. This is the reverse of caching; we call it shrinking because it reduces actual objects to surrogates <ref> [Day93] </ref>. (Only persistent objects that have not been modified during the current transaction can be shrunk in this way.) When an object is turned into a surrogate, its contents are removed, which in turn may cause other objects to become inaccessible. 3 There are other uses for such a table in
Reference: [DLMSS78] <author> E. W. Dijkstra, L. Lamport, A. J. Martin, C. S. Schloten, and E. F. M. Steffens. </author> <title> On-the-fly Garbage Collection: An Exercise in Cooperation. </title> <journal> Communications of the ACM, </journal> <volume> 21(11), </volume> <pages> pages 966-975, </pages> <month> Nov </month> <year> 1978. </year>
Reference-contexts: On the other hand, tracing is invoked as a corrective measure when the free memory nears exhaustion, and it may lead to a long GC interruption unless an incremental or concurrent version is employed <ref> [DLMSS78] </ref>. A related point is that tracing must make a global search starting from the roots before any garbage can be collected.
Reference: [DS80] <author> E. W. Dijkstra, and C. S. Schloten. </author> <title> Termination Detection for Diffusing Computation. </title> <journal> Information Processin Letters, </journal> <volume> Vol. 11, No. 1, </volume> <month> Aug </month> <year> 1980. </year>
Reference-contexts: That is, even after a GC queue is empty, an incoming marking message may initiate marking again. We describe one solution below. 2.2.1 Marking-Tree This scheme is based on the distributed termination detection of diffusive computation <ref> [DS80] </ref>. For the time being, assume that there is only one root object. Also, for simplicity, the concept of marking messages is generalized to include local references: when an object is marked, it sends marking messages along every contained reference, remote or local [HK82].
Reference: [GC89] <author> C. Gray, and D. Cheriton. Leases: </author> <title> An Efficient Fault-Tolerant Mechanism for Distributed File Cache Consistency. </title> <booktitle> Proceedings of the Twelfth ACM Symposium on Operating System Principles, </booktitle> <pages> pages 202-210, </pages> <year> 1989. </year> <month> 91 </month>
Reference-contexts: But then, the local collection cannot be deferred for an unbounded span of time. One solution to this is to use leases <ref> [GC89] </ref> as discussed below. 4.5.3 Leased FE Tables In this scheme, the OR guarantees to maintain the FE table for a limited time. We say that the lease for the FE table expires after that time. <p> The scan-on-need scheme described in Section 4.3.2 records only the objects composing the block. The references contained in these objects are included in the FE table only when the stable copy of the object is about to be modified. * The FE tables are maintained on a lease basis <ref> [GC89] </ref>; that is, an OR guarantees to maintain an FE table only until its lease expires. When the OR recovers from a crash, it only needs to wait for the lease period to pass before doing a local GC.
Reference: [Gol89] <author> B. Goldberg. </author> <title> Generational Reference Counting: A Reduced Communication Dis--tributed Storage Scheme. </title> <booktitle> ACM SIGPLAN Programming Languages Design and Implementation, </booktitle> <pages> pages 313-321, </pages> <address> portland OR, </address> <month> Jun </month> <year> 1989. </year>
Reference-contexts: When the receiver accesses its reference, which points to the sender, the sender redirects the access to the owner. There is another scheme similar in spirit to weighted reference counting, called generational reference counting <ref> [Gol89] </ref>, which is not described here. (Caution: the scheme is not related in any way to the generational collection described in Section 2.3.2.) 2.4.4 Reference Listing Here, instead of keeping just a count, the inlist entry for an object keeps the list of all nodes that contain a remote reference to
Reference: [Gra78] <author> J. N. Gray. </author> <booktitle> Notes on Database Operating Systems. Operating Systems: An Advanced Course (Lecture Notes in Computer Science 60), </booktitle> <pages> pages 393-481, </pages> <publisher> Springer-Verlag, </publisher> <year> 1978. </year>
Reference-contexts: When the time comes to commit the transaction, the client sends copies of the modified and the new objects to a server designated as the coordinator of the transaction. It is the coordinator's responsibility to execute the 2-phase commit protocol <ref> [Gra78] </ref>. As a part of this protocol, it distributes the copies of modified and new objects to the servers where they belong. These servers, the participants of the 2-phase commit, incorporate the object copies after the commit succeeds. <p> The front end selects a participant as the coordinator, and sends it the necessary information about the transaction, including copies of newly persistent objects and modified objects. The coordinator coordinates the commit with other participants. The 2-phase commit protocol proceeds as follows <ref> [Gra78, Ady93] </ref>: Phase 1 (or, the prepare phase) initiate : The coordinator assigns the transaction a tid that is unique across Thor. All messages 52 sent on behalf of the transaction are stamped with its tid. The coordinator sends out prepare messages to the participants.
Reference: [HK82] <author> P. Hudak, and R. Keller. </author> <title> Garbage Collection and Task Deletion in Distributed Applicative Processing Systems. </title> <booktitle> ACM Symposium on Lisp and Functional Programming, </booktitle> <pages> pages 168-178, </pages> <month> Aug </month> <year> 1982. </year>
Reference-contexts: For the time being, assume that there is only one root object. Also, for simplicity, the concept of marking messages is generalized to include local references: when an object is marked, it sends marking messages along every contained reference, remote or local <ref> [HK82] </ref>. When an object first gets a marking message, it remembers the sender object in its own parent field. Then it sends out a marking message for each of its contained references, and maintains a count of these messages in its count field. <p> If a marked object gets another marking message, it sends back an ack right away (Figure 2-3). The ack message from the root indicates the completion of marking. Note that the parent fields make a tree-like structure, called a marking-tree in <ref> [HK82] </ref>, which replaces the GC queue. Multiple roots on different nodes can be handled by waiting until all roots have sent out an ack message.
Reference: [Hug85] <author> J. Hughes. </author> <title> A Distributed Garbage Collection Algorithm. </title> <booktitle> Functional Programming and Computer Architecture (Lecture Notes in Computer Science 201), </booktitle> <pages> pages 256-272, </pages> <publisher> Springer-Verlag, </publisher> <month> Sep </month> <year> 1985. </year>
Reference-contexts: The next few subsections describe the use of more sophisticated techniques that improve upon the fault tolerance of plain tracing. 2.5.4 Tracing with Timestamps In conventional global tracing, nodes have to synchronize their mark and sweep phases: no garbage can be swept until marking is complete at all nodes. <ref> [Hug85] </ref> uses propagation of timestamps instead of marks to do away with the synchronization. The usual local collection, which traces the local space starting from the primary roots and the inlist, also plays the role of propagating the time-stamps on behalf of global tracing. <p> As a result, global tracing turns into a smooth, albeit slow, ongoing process executed incrementally through local collections. However, it still remains the case that a node that is inaccessible or unwilling to do local collection will ultimately hold up the entire global tracing. The algorithm as described in <ref> [Hug85] </ref> uses reference flagging augmented with timestamping. That is, each inlist entry contains a timestamp, but no other information such as the reference count. Consequently, the scheme relies on global tracing to collect all distributed garbage, not just circular garbage (Section 2.4.1). <p> When a node increases the timestamp of an inlist entry, it records the fact that it has not propagated the increased timestamp. To this end, each node maintains a timestamp, called redo in <ref> [Hug85] </ref>, such that all timestamps less than or equal to that are guaranteed to have been propagated. Therefore, in increasing an entry's timestamp, the redo is set to the entry's old timestamp, if that is lower than its current value. <p> However, it is tricky to find the global minimum of the redo's at any time because redo values keep bobbing up and down. (Note that while timestamps of inlist and outlist entries can only increase, the redo value of a node often decreases on the arrival of a marking message.) <ref> [Hug85] </ref> modifies a distributed termination detection algorithm [Ran83] to compute the global minimum, but other distributed snapshot algorithms could be used as well. [Ran83] presumes the use of global synchronized clocks and instantaneous messages. We believe that apart from the use of this algorithm, the technique in [Hug85] will work just <p> a marking message.) <ref> [Hug85] </ref> modifies a distributed termination detection algorithm [Ran83] to compute the global minimum, but other distributed snapshot algorithms could be used as well. [Ran83] presumes the use of global synchronized clocks and instantaneous messages. We believe that apart from the use of this algorithm, the technique in [Hug85] will work just as well without these two requirements. If the clocks on different nodes are out of synchronization, that will only affect the performance by further delaying the collection of garbage. <p> This scheme is not viable, however, because to provide the connectivity information, the nodes may have to traverse their local spaces multiple times. [LL92] therefore employs the technique used in <ref> [Hug85] </ref> instead. The nodes do not have to communicate with each other for the purpose of GC. The communication with the service can be performed in the background. Further, having a special server to detect distribute garbage offloads work from other nodes. <p> Simply stated, all inlist entries except those that stand for other member nodes of the same group are included in the group's roots. Once the group roots are found, the group members mark the included inlist entries to distinguish them from the rest (Figure 2-17). Just as <ref> [Hug85] </ref> used local tracing to propagate timestamps, [LPQ92] uses them to carry out the group-wide tracing. But unlike [Hug85], the mark and the sweep phases are separated. <p> Once the group roots are found, the group members mark the included inlist entries to distinguish them from the rest (Figure 2-17). Just as <ref> [Hug85] </ref> used local tracing to propagate timestamps, [LPQ92] uses them to carry out the group-wide tracing. But unlike [Hug85], the mark and the sweep phases are separated. In the mark phase, the local collection is expected to propagate marks from the group roots to the outlist 45 entries reachable from them.
Reference: [Juu90] <author> N. C. Juul. </author> <title> A Distributed Faulting Garbage Collector for Emerald. </title> <booktitle> OOPSLA Workshop on Garbage Collection, </booktitle> <year> 1990. </year>
Reference-contexts: The following subsections discuss various forms of reference tracking, focusing on their increase and decrease protocols. 26 2.4.1 Reference Flagging In this scheme, the only information kept is which local objects are referenced by one or more remote references <ref> [Ali84, Juu90] </ref>. Such objects are distinguished by the presence of an inlist entry; the entry does not contain any other information. This is similar to using a 1-bit wide reference count. <p> A bad selection results in wasted computation. Another drawback is that each object needs to have an extra field beside the one used to hold the actual reference count. 2.5.3 Complementary Tracing The idea here is to invoke a global tracing periodically to collect circular garbage <ref> [Ali84, Juu90] </ref>. The drawbacks of this scheme are the same as those of global tracing itself (Section 2.2.2), except with reduced severity because tracing as a complementary scheme is run infrequently, and its responsibility is limited to collecting circular garbage.
Reference: [KR81] <author> H. T. Kung, J. T. Robinson. </author> <title> On Optimistic Methods for Concurrency Control. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 6(2), </volume> <pages> pages 213-226, </pages> <month> Jun </month> <year> 1981. </year>
Reference-contexts: Transactions are serialized using optimistic concurrency control: the front ends might fetch inconsistent data, so the transactions have to be validated at commit time <ref> [KR81] </ref>. A transaction aborts if the validation fails. The front end tracks the persistent objects read and modified during the current transaction. At commit time, it collects the (to-be) newly persistent objects by tracing for temporary objects reachable from the set of modified objects.
Reference: [LDS92] <author> B. Liskov, M. Day, and L. Shrira. </author> <title> Distributed Object Management in Thor. Distributed Object Management, </title> <editor> ed. M. T. Ozsu, U. Dayal, and P. Valduriez, </editor> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: Object migration is problematic if references are location dependent because the references pointing to the old location must be taken care of. A technique sometimes used is indirection: leave a surrogate at the old location that stores a reference to the new location <ref> [LDS92] </ref>. However, indirection will not help in consolidating the cycle because, in effect, the cycle still goes through the object's old node (Figure 2-15). <p> Thor can be used in a heterogenous distributed system and it allows programs written in different languages to share objects <ref> [Lis92, LDS92] </ref>. This chapter describes the system architecture of Thor, but the description is restricted to what is relevant to the design proposed in this thesis. The setting and the terminology introduced herein will be assumed in later chapters. <p> Outlists, on the other hand can be computed from the stably stored or-surrogates. Nonetheless, we maintain the outlists stably because it is cheap to do so. 85 Chapter 6 Conclusions This thesis has presented a design for distributed GC in a new object-oriented database system called Thor <ref> [LDS92] </ref>. In Thor, a front end (FE) running on behalf of a client invokes operations on copies of objects fetched from multiple object repositories (ORs). Our design accounts for caching and prefetching of objects done by the front end.
Reference: [LH83] <author> H. Lieberman, and C. Hewitt. </author> <title> A Real-time Garbage Collector Based on the Lifetimes of Objects. </title> <journal> Communications of the ACM, </journal> <volume> 26(6), </volume> <pages> pages 419-429, </pages> <month> Jun </month> <year> 1983. </year>
Reference-contexts: This will result in fewer inter-area references, and less inter-area circular garbage. 2.3.2 Generational Collection Generational collection optimizes the GC in separate areas by tuning it to the lifetimes of objects <ref> [LH83] </ref>. In doing so it exploits two facts: 1. Newly created objects have a higher chance of becoming garbage than those that have already survived many collections.
Reference: [LL86] <author> B. Liskov, and R. Ladin. </author> <title> Highly Available Distributed Services and Fault Tolerant Distributed Garbage Collection. </title> <booktitle> Proceedings of the 5th Symposium on the Principles of Distributed Computing, ACM, </booktitle> <pages> pages 29-39, </pages> <address> Vancouver, Canada, </address> <month> Aug </month> <year> 1986. </year>
Reference-contexts: message for a reference it sent out to another node until it has received the ack is to store all such references in a separate data structure, called the translist, whose entries include the receiver node to which the reference was sent. (The concept of a translist is borrowed from <ref> [LL86] </ref>, where it is used in a somewhat different context.) Entries are deleted from the translist when the sender receives an ack from the receiver (or the owner, depending upon the exact protocol). <p> Conventional distributed systems using reference tracking for distributed GC are able to eliminate synchronous insert messages by using techniques discussed in Section 2.4, which include the use of a translist <ref> [LL86] </ref> and strong-weak pointers [SDP92]. 1 These techniques could not be applied in Thor because of the following: 1. When the sender sends a reference to the front end, it does not know which ORs will finally receive a copy.
Reference: [LL92] <author> R. Ladin, and B. Liskov. </author> <title> Garbage Collection of a Distributed Heap. </title> <booktitle> Int. Conference on Distributed Computing Systems, </booktitle> <pages> pages 708-715, </pages> <address> Yokohoma, Japan, </address> <month> Jun </month> <year> 1992. </year>
Reference-contexts: Another drawback is that the execution of the distributed algorithm to compute global minimum redo is costly and slows down the collection of garbage. 2.5.5 Centralized Server This scheme makes use of a logically centralized service that tracks all inter-node references <ref> [LL92] </ref>. The implementation of the service may actually be distributed, and may use replication for high availability and reliability, but it appears as if it were run by one server [LLSG90]. <p> This scheme is not viable, however, because to provide the connectivity information, the nodes may have to traverse their local spaces multiple times. <ref> [LL92] </ref> therefore employs the technique used in [Hug85] instead. The nodes do not have to communicate with each other for the purpose of GC. The communication with the service can be performed in the background. Further, having a special server to detect distribute garbage offloads work from other nodes.
Reference: [LLSG90] <author> R. Ladin, B. Liskov, L. Shrira, S. Ghemawat. </author> <title> Lazy Replication: Exploiting the Semantics of Distributed Services. </title> <booktitle> Proceedings of the 9th ACM Symposium on the Principles of Distributed Computing, ACM, </booktitle> <address> Canada, </address> <month> Aug </month> <year> 1990. </year>
Reference-contexts: The implementation of the service may actually be distributed, and may use replication for high availability and reliability, but it appears as if it were run by one server <ref> [LLSG90] </ref>. Nodes communicate with the service to provide it with information of their outlists and translists (Section 2.4.2), typically, soon after they have performed a local collection. They also query it for the accessibility of their inlist entries.
Reference: [LQP92] <author> B. Lang, C. Queinnec, and J. Piquer. </author> <title> Garbage Collecting the World. </title> <booktitle> Proceedings of the 19th Annual ACM SIGPLAN-SIGACT Symp. on Principles of Programming Languages, </booktitle> <pages> pages 39-50, </pages> <address> Albuquerque, </address> <month> Jan </month> <year> 1992. </year>
Reference-contexts: Further, the proposed design does not include a scheme for the collection of distributed circular garbage. It is possible, though admittedly tricky, to augment the design by either tracing <ref> [LQP92] </ref>, or forced migration of certain objects [SGP90] to collect circular garbage. These schemes are discussed in Section 2.5. 1.5 Thesis Outline Chapter 2 provides a study of various distributed GC algorithms. The schemes proposed in this thesis have borrowed many ideas from the analysis in this chapter. <p> The main idea is to reuse the concept of separate areas at a higher level: any set of nodes can decide to form a group, and perform a group-wide tracing that collects all cycles within the group <ref> [LQP92] </ref>. But unlike areas, groups of nodes can be formed and dismantled dynamically, although it may be desirable to form some groups statically and never dismantle them. <p> The drawback is that the strategy might result in load imbalance among the ORs. * Hierarchical grouping of ORs such that tracing can be performed in each group independently of other ORs <ref> [LQP92] </ref>; this is discussed in Section 2.5.6. Groups are made on the basis of expected locality in inter-OR references; for instance, all ORs on the same local area network could be grouped together. When a group is traced, all circular garbage lying within the group is collected.
Reference: [LS79] <author> B. W. Lampson, and H. E. Sturgis. </author> <title> Crash Recovery in a Distributed Data Storage System. </title> <type> Technical Report, </type> <institution> Xerox Palo Alto Research Center, </institution> <month> Apr </month> <year> 1979. </year> <month> 92 </month>
Reference-contexts: The effects of committed 14 transactions survive crashes. Some of the information that supports distributed GC must survive crashes too, while the rest can be recomputed on recovery. Updates of the GC information may therefore incur stable-storage writes in addition to those required for durability of transactions <ref> [LS79] </ref>.
Reference: [Lis88] <author> B. Liskov. </author> <title> Distributed Programming in Argus. </title> <journal> Communications of the ACM, </journal> <volume> 31(3), </volume> <pages> pages 300-312, </pages> <month> Mar </month> <year> 1988. </year>
Reference-contexts: Th is implemented in a high level language called Argus, which supports abstract data types, garbage collection, remote procedure calls, and transactions <ref> [Lis88] </ref>. However, the layer of abstraction introduced by Argus makes Th unsuitable as a test bed for evaluating performance in terms of real time. For instance, Argus forces the use of reliable remote procedure calls where unreliable messages could have been used.
Reference: [Lis92] <author> B. Liskov. </author> <title> Preliminary Design of the Thor Object-Oriented Database System. Programming Methodology Group Memo 74, </title> <institution> MIT Laboratory for Computer Science, </institution> <month> Mar </month> <year> 1992. </year>
Reference-contexts: Thor can be used in a heterogenous distributed system and it allows programs written in different languages to share objects <ref> [Lis92, LDS92] </ref>. This chapter describes the system architecture of Thor, but the description is restricted to what is relevant to the design proposed in this thesis. The setting and the terminology introduced herein will be assumed in later chapters.
Reference: [Lis93] <author> B. Liskov. </author> <title> Practical Uses of Synchronized Clocks in Distributed Systems. </title> <booktitle> Distributed Computing, </booktitle> <year> 1993 </year> <month> (forthcoming). </month>
Reference-contexts: Our scheme relies on loosely synchronized clocks to guarantee correct behavior. Bounded differences in clock times can be handled by conservatively adjusting the lease period at the two ends. <ref> [Lis93] </ref> suggests a technique to implement leases using loosely synchronized clock rates instead of the absolute time maintained by the clocks. We could not employ that technique because the commit deadline included in commit requests needs to be an absolute time limit.
Reference: [Mos90] <author> J. E. B. Moss. </author> <title> Design of the Mneme Persistent Object Store. </title> <journal> ACM Tran. on Information Systems, </journal> <volume> 8(2), </volume> <pages> pages 103-139, </pages> <month> Apr </month> <year> 1990. </year>
Reference-contexts: As described earlier, objects at the OR contain references in the form of orefs. It is the front end's job to convert them into direct memory pointers to cached objects for better performance; the conversion is called swizzling <ref> [Mos90] </ref>. To do swizzling, the front end maintains a swizzle table, which maps xrefs to memory pointers for the cached objects. If an oref refers to an object that the front end has already cached, it replaces the oref with a memory pointer to the object.
Reference: [OL88] <author> B. M. Oki, and B. Liskov. </author> <title> Viewstamped Replication: A New Primary Copy Method to Support Highly Available Distributed Systems. </title> <booktitle> Proceedings of the 7th ACM Symposium on Principles of Distributed Computing, ACM, </booktitle> <month> Aug </month> <year> 1988. </year>
Reference-contexts: We shall often refer to non-surrogate objects as actual objects. Each OR has a root directory object. The root directory contains references to other objects, presumably other directories. The ORs use some kind of stable storage mechanism to store their objects. Our plan is to use primary copy replication <ref> [OL88] </ref>. Nonetheless, in the rest of this thesis we shall simply use the terminology commonly used for disks. 3.2 Front Ends A front end is created for each client program. It fetches copies of the required objects from their ORs into its local cache, and runs operations on them.
Reference: [Piq91] <author> J. M. Piquer. </author> <title> Indirect Reference Counting: A Distributed Garbage Collection Algorithm. </title> <booktitle> PARLE '91 Parallel Architecture and Languages (Lecture Notes in Computer Science 505), </booktitle> <pages> pages 150-165, </pages> <publisher> Springer-Verlag, </publisher> <month> Jun </month> <year> 1991. </year>
Reference-contexts: Partial failures include crashes of individual nodes, and failures 1 In the literature, authors sometimes distinguish between creation of the first copy of the remote reference by the owner, and its subsequent duplication by other senders <ref> [Piq91] </ref>. This thesis, however, does not distinguish between the two. in message delivery: messages can be delayed, lost, duplicated, and delivered out of order, or there might be a network partition, in which a group of nodes becomes virtually discon nected from the rest. 3. <p> Later, in the background, a separate inlist entry is made for the receiver, and then its strong pointer can be changed to point directly to the object. This allows the collection of the outlist entry at the sender (Figure 2-12 (C)). <ref> [Piq91] </ref> uses this technique in conjunction with reference counting, although it is modeled differently. [SDP92] uses the technique in conjunction with reference listing. One drawback of using strong-weak pointers is that every reference included in mutator messages actually occupies the size of two references.
Reference: [Ran83] <author> S. P. Rana. </author> <title> A Distributed Solution to the Distributed Termination Problem. </title> <journal> Information Processing Letters, </journal> <volume> Vol. 17, </volume> <pages> pages 43-46, </pages> <month> Jul </month> <year> 1983. </year>
Reference-contexts: global minimum of the redo's at any time because redo values keep bobbing up and down. (Note that while timestamps of inlist and outlist entries can only increase, the redo value of a node often decreases on the arrival of a marking message.) [Hug85] modifies a distributed termination detection algorithm <ref> [Ran83] </ref> to compute the global minimum, but other distributed snapshot algorithms could be used as well. [Ran83] presumes the use of global synchronized clocks and instantaneous messages. We believe that apart from the use of this algorithm, the technique in [Hug85] will work just as well without these two requirements. <p> (Note that while timestamps of inlist and outlist entries can only increase, the redo value of a node often decreases on the arrival of a marking message.) [Hug85] modifies a distributed termination detection algorithm <ref> [Ran83] </ref> to compute the global minimum, but other distributed snapshot algorithms could be used as well. [Ran83] presumes the use of global synchronized clocks and instantaneous messages. We believe that apart from the use of this algorithm, the technique in [Hug85] will work just as well without these two requirements.
Reference: [SDP92] <author> M. Shapiro, P. Dickman, and D. Plainfosse. </author> <title> Robust, Distributed References and Acyclic garbage Collection. </title> <booktitle> Symposium on Principles of Distributed Computing, </booktitle> <address> Vancouver, Canada, </address> <month> Aug </month> <year> 1992. </year>
Reference-contexts: This allows the collection of the outlist entry at the sender (Figure 2-12 (C)). [Piq91] uses this technique in conjunction with reference counting, although it is modeled differently. <ref> [SDP92] </ref> uses the technique in conjunction with reference listing. One drawback of using strong-weak pointers is that every reference included in mutator messages actually occupies the size of two references. This is awkward if the mutator message is carrying 35 36 an object that contains references. <p> This is useful for streaming extra blocks in the background after the requested object has been sent (Section 3.2). Such blocks can be sent using unreliable messages. A timestamping scheme suited for this purpose is given below. It is similar to the one used in <ref> [SDP92] </ref>. The OR timestamps each block sent out. It maintains the timestamp of the latest block sent to a front end as T max [F E] , for each front end it has a session with. The timestamp required for the purpose is simply a monotonically increasing identifier. <p> This is because the or-surrogate at OR 1 is protected by the FE table, which in turn protects the referenced object through the OR table for OR 1 at OR 2 . This scheme is conceptually similar to the strong-weak pointers scheme <ref> [SDP92] </ref>. * Recording all references contained in the block into the FE table would delay the fetch request and inflate the FE table. The scan-on-need scheme described in Section 4.3.2 records only the objects composing the block. <p> Conventional distributed systems using reference tracking for distributed GC are able to eliminate synchronous insert messages by using techniques discussed in Section 2.4, which include the use of a translist [LL86] and strong-weak pointers <ref> [SDP92] </ref>. 1 These techniques could not be applied in Thor because of the following: 1. When the sender sends a reference to the front end, it does not know which ORs will finally receive a copy. <p> In Section 5.2.2 we described a timestamp protocol that solves this problem by exploiting some features of the distributed commit protocol in Thor. The protocol is comparable to the one used in <ref> [SDP92] </ref>. 6.2 Future Work 6.2.1 A Formal Proof In this thesis we presented informal and operational arguments to validate parts of the proposed design. It is desirable to have a formal proof that covers the entire design the FE-OR protocol as well as the OR-OR protocol.
Reference: [SGP90] <author> M. Shapiro, O. Gruber, and D. Plainfosse. </author> <title> A Garbage Detection Protocol for a Realistic Distributed Object-Support System. </title> <type> Research Report 1320, </type> <institution> INRIA-Rocquencourt, </institution> <month> Nov </month> <year> 1990. </year>
Reference-contexts: Further, the proposed design does not include a scheme for the collection of distributed circular garbage. It is possible, though admittedly tricky, to augment the design by either tracing [LQP92], or forced migration of certain objects <ref> [SGP90] </ref> to collect circular garbage. These schemes are discussed in Section 2.5. 1.5 Thesis Outline Chapter 2 provides a study of various distributed GC algorithms. The schemes proposed in this thesis have borrowed many ideas from the analysis in this chapter. <p> is not described here. (Caution: the scheme is not related in any way to the generational collection described in Section 2.3.2.) 2.4.4 Reference Listing Here, instead of keeping just a count, the inlist entry for an object keeps the list of all nodes that contain a remote reference to it <ref> [Bis77, SGP90] </ref>. Assume that the nodes are identified by distinct node-ids. It is common in the literature to break the inlist entry into separate elements, each containing a reference to the local object and a single node-id from where the object is referenced. <p> As before, the delete message is still timestamped by the node that held the reference, namely, the receiver. Since the clocks of the owner and the holder may not be synchronized, a somewhat more complicated protocol is required (see <ref> [SGP90] </ref>). Another way in which reference listing provides extra fault-tolerance is that a node can send query messages to the nodes for which it has inlist entries. Such a message prompts the recipient to send a trim message to the owner. <p> The problem with using indirection is that if the receiver accesses the reference, it is indirected through the sender. One solution is to snap the indirection by communicating to the owner in the background <ref> [SGP90] </ref>. As soon as the receiver has its own inlist entry at the owner, it can switch its reference to point directly to the object. <p> With respect to Figure 2-13, the collection of the cycle that passes through Node B and C should not require cooperation from Node A. 2.5.1 Object Migration The aim here is to consolidate an unreachable distributed cycle into a single area where it can be collected by the local collector <ref> [Bis77, SGP90] </ref>. This is implemented as follows: if an object is 37 unreachable from the local primary roots but is remotely referenced, it is moved to a node that references it. The local GC helps in finding such objects. <p> At present, we are considering two candidate techniques for Thor: * Migration of remotely referenced objects that are not reachable from the local root directory to the ORs from which they are referenced <ref> [SGP90] </ref>; this is discussed in Section 2.5.1. The virtue of this technique is that it makes progress by pairwise communication between the ORs on which the cycle resides.
Reference: [Ves87] <author> S. C. Vestal. </author> <title> Garbage Collection: An Exercise in Distributed, Fault-Tolerant Programming. </title> <type> PhD thesis, </type> <institution> University of Washington, </institution> <month> Jan </month> <year> 1987. </year>
Reference-contexts: Fixes have been suggested to the problems listed above. One of the schemes suggested in <ref> [Ves87] </ref> is to move the logical structure of an object, while its real physical copy remains fixed. Note that the logical structure needs to include contained references if the collection of cyclic garbage is to occur. The real copy is collected only if the logical copy proves to be garbage. <p> If an object is temporarily deleted for the sake of experiment and the reference 39 counts in the object graph adjusted accordingly, then if the reference count of the original object does drop to zero, it must be part of circular garbage <ref> [Ves87] </ref>. This scheme works well only if the local collection is also based on reference counting, because then it is simple to propagate the effects of an experimental deletion.
Reference: [Wen79] <author> K-S Weng. </author> <title> An Abstract Implementation of a Generalized Dataflow Language. </title> <type> Technical Report MIT/LCS/TR 228, </type> <institution> MIT Laboratory for Computer Science, </institution> <address> Cambridge MA, </address> <year> 1979. </year>
Reference-contexts: In the parallel scheme, a race condition between the two messages cannot be avoided using simple timestamp protocols, because 1 The use of reference listing preempts the use of some other techniques, like weighted reference counting <ref> [Wen79, Bev87] </ref>. As discussed in Section 2.4.3, weighted reference counting requires the use of reliable decrement messages, and suffers from the weight-underflow problem. 88 the insert message is sent by the coordinator while the trim message is sent by the receiver itself.
Reference: [Wil92] <author> P. R. Wilson. </author> <title> Uniprocessor Garbage Collection Techniques. </title> <booktitle> 1992 International Workshop on Memory Management, (Lecture Notes in Computer Science 637), </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1992. </year> <month> 93 </month>
References-found: 36

