URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR93339-S.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Author: v. Hanxleden 
Address: P.O. Box 1892 Houston, TX 77251-1892  
Affiliation: Rice University  
Date: #9  October, 1993  
Note: D Newsletter  Reinhard  Center for Research on Parallel Computation  From the Proceedings of the Fourth Workshop on Compilers for Parallel Computers, Delft, The Netherlands, De-cember 1993.  
Pubnum: CRPC-TR93339-S  
Abstract: Handling Irregular Problems with Fortran D | A Preliminary Report 
Abstract-found: 1
Intro-found: 1
Reference: [BS90] <author> H. Berryman and J. Saltz. </author> <title> A manual for PARTI runtime primitives. </title> <type> ICASE Interim Report 13, </type> <institution> Institute for Computer Application in Science and Engineering, Hampton, VA, </institution> <month> September </month> <year> 1990. </year>
Reference-contexts: Then, an "execution phase" (repeatedly) performs the actual computation and uses the communication schedule to exchange data [MSS + 88, KMV90]. The Parti communication library was designed to simplify schedule generation and schedule based communication <ref> [BS90] </ref>. Parti has also been used to implement user-defined irregular distributions [MSS + 88] and to provide a hashed cache for non-local values [MSMB90]. The feasibility of automatically generating inspectors/executors for simple loop nests has been demonstrated by the Arf [WSBH91] and Kali [KM91] compilers.
Reference: [CCH + 88] <author> D. Callahan, K. Cooper, R. Hood, K. Kennedy, and L. Torczon. </author> <title> ParaScope: A parallel programming environ ment. </title> <journal> The International Journal of Supercomputer Applications, </journal> <volume> 2(4) </volume> <pages> 84-99, </pages> <month> Winter </month> <year> 1988. </year> <month> 11 </month>
Reference-contexts: Examples include High Performance Fortran (HPF) [Hig93], Vienna Fortran [CMZ92], and Fortran D [FHK + 90]. A prototype Fortran D compiler targeting MIMD distributed memory machines has been under development at Rice University as part of the ParaScope programming environment <ref> [CCH + 88] </ref>. For regular problems, i.e., applications with relatively simple array subscript functions and fixed communication requirements, this compiler has had considerable successes [HKK + 91, Tse93]. However, irregular problems, such as molecular dynamics or unstructured mesh codes, have proven to be significantly harder to parallelize [SH91].
Reference: [CHK + 92] <author> T. W. Clark, R. v. Hanxleden, K. Kennedy, C. Koelbel, and L. R. Scott. </author> <title> Evaluating paral lel languages for molecular dynamics computations. </title> <booktitle> In Scalable High Performance Computing Conference, </booktitle> <address> Williamsburg, VA, </address> <month> April </month> <year> 1992. </year> <note> Available via anonymous ftp from softlib.rice.edu as pub/CRPC-TRs/reports/CRPC-TR992202-S. </note>
Reference-contexts: However, despite the relative maturity and growing importance of MD, it still presents computational challenges. These stem mainly from the complexity and irregularity of the underlying physical problem, and from various optimizations that speed up the simulations but further complicate the computation <ref> [CHK + 92] </ref>. Consequently, parallelizing MD codes by hand is still an active field of research, and automatic parallelization within a data parallel framework presents an even bigger challenge. Typical MD programs have a broad range of functionality, and each simulation consists of several different phases. <p> Another example is the assignment to j, which became dead due to subscript conversion. 5 Summary and conclusions Compiling irregular applications in a data parallel framework presents a challenging problem of growing importance. Previous works have addressed some of the arising problems <ref> [WSBH91, KM91, CHK + 92, HKK + 92, DSvH93, HK93] </ref>, but very little experience has been gained so far with the compilation of large codes with complicated data access patterns and arbitrary control flow.
Reference: [CM90] <author> T. W. Clark and J. A. McCammon. </author> <title> Parallelization of a molecular dynamics non-bonded force algorithm for MIMD architectures. </title> <journal> Computers & Chemistry, </journal> <volume> 14(3) </volume> <pages> 219-224, </pages> <year> 1990. </year>
Reference-contexts: Section 3 describes the analysis phase of the compiler, and Section 4 covers code generation. Section 5 concludes with a brief summary. 2 An example application: Molecular dynamics Molecular dynamics (MD) simulations are computational approaches for studying various kinetic, thermodynamic, mechanistic, and structural properties of molecules <ref> [CM90] </ref>. There exist several MD packages (such as GROMOS, CHARMM, or ARGOS) that are heavily used to simulate biomolecular systems over a broad range of problem sizes and time scales. However, despite the relative maturity and growing importance of MD, it still presents computational challenges.
Reference: [CMZ92] <author> B. Chapman, P. Mehrotra, and H. Zima. </author> <title> Programming in Vienna Fortran. </title> <journal> Scientific Programming, </journal> <volume> 1(1) </volume> <pages> 31-50, </pages> <month> Fall </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Several research projects have aimed at providing a "machine independent parallel programming style," in which the applications programmer uses a dialect of sequential Fortran and annotates it with high level distribution information. Examples include High Performance Fortran (HPF) [Hig93], Vienna Fortran <ref> [CMZ92] </ref>, and Fortran D [FHK + 90]. A prototype Fortran D compiler targeting MIMD distributed memory machines has been under development at Rice University as part of the ParaScope programming environment [CCH + 88].
Reference: [Dah90] <author> D. Dahl. </author> <title> Mapping and compiled communication on the connection machine system. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: The Give-N-Take framework is a generalization of partial redundancy elimination techniques that can be used for analyzing data access requirements and for efficient communication placement in the presence of arbitrary control flow [HK93]. Examples of SIMD specific work are the Communication Compiler <ref> [Dah90] </ref> for the Connection Machine and the loop flattening transformation to allow efficient execution of loop nests with varying inner loop bounds [HK92]. These projects have laid important ground work towards compiling irregular problems under a data parallel, machine independent programming paradigm.
Reference: [DPSM91] <author> R. Das, R. Ponnusamy, J. Saltz, and D. Mavriplis. </author> <title> Distributed memory compiler methods for irregular problems | data copy reuse and runtime partitioning. </title> <type> ICASE Report 91-73, </type> <institution> Institute for Computer Application in Science and Engineering, Hampton, VA, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: An optimization that is not implemented yet but at least conceptionally fairly straightforward is to use incremental schedules for pruning messages in case at least some of the data covered by a reference are already locally available <ref> [DPSM91, HKK + 92] </ref>.
Reference: [DSvH93] <author> R. Das, J. Saltz, and R. v. Hanxleden. </author> <title> Slicing analysis and indirect accesses to distributed arrays. </title> <booktitle> In Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year> <note> Available via anonymous ftp from softlib.rice.edu as pub/CRPC-TRs/reports/CRPC-TR93319-S. </note>
Reference-contexts: Slicing analysis is a technique for generating inspectors and pre-inspectors for multiple levels of indirection <ref> [DSvH93] </ref>. The Give-N-Take framework is a generalization of partial redundancy elimination techniques that can be used for analyzing data access requirements and for efficient communication placement in the presence of arbitrary control flow [HK93]. <p> This can be thought of as the set of data that are referenced when inspecting for a subscript; a more detailed discussion can be found elsewhere <ref> [DSvH93] </ref>. For example, a subscript that is itself an indirection array lookup depends on the indirection array. <p> Part of the modifications is to replace irregular subscripts by references to trace arrays <ref> [DSvH93] </ref>. These arrays contain traces of the subscripts encountered during inspection, localized from global to local name space. Trace arrays are one of several possible options to perform the name space conversion. <p> Another example is the assignment to j, which became dead due to subscript conversion. 5 Summary and conclusions Compiling irregular applications in a data parallel framework presents a challenging problem of growing importance. Previous works have addressed some of the arising problems <ref> [WSBH91, KM91, CHK + 92, HKK + 92, DSvH93, HK93] </ref>, but very little experience has been gained so far with the compilation of large codes with complicated data access patterns and arbitrary control flow.
Reference: [FHK + 90] <author> G. C. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, C. Tseng, and M. Wu. </author> <title> Fortran D lan guage specification. </title> <type> Technical Report TR90-141, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> December </month> <year> 1990. </year> <note> Revised April, </note> <year> 1991. </year>
Reference-contexts: 1 Introduction Several research projects have aimed at providing a "machine independent parallel programming style," in which the applications programmer uses a dialect of sequential Fortran and annotates it with high level distribution information. Examples include High Performance Fortran (HPF) [Hig93], Vienna Fortran [CMZ92], and Fortran D <ref> [FHK + 90] </ref>. A prototype Fortran D compiler targeting MIMD distributed memory machines has been under development at Rice University as part of the ParaScope programming environment [CCH + 88].
Reference: [GS93] <author> M. Gupta and E. Schonberg. </author> <title> A framework for exploiting data availability to optimize communication. </title> <booktitle> In Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: An option not considered here is that p might receive d from any processor that has a valid copy of d <ref> [GS93] </ref>. Since the owner computes rule is not strictly applied, any processor p may define d. If p 6= owner (d) and d will be referenced by some processor q, q 6= p, then p must send d to owner (d) after defining d.
Reference: [Hav93] <author> P. Havlak. </author> <title> Construction of thinned gate single-assignment form. </title> <booktitle> In Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: Value numbers, VN := f vn j e an expression in P , vn = val (e), the value number of e.g. Value numbers are computed for both constant and non-constant expressions and are closely related to the SSA information <ref> [Hav93] </ref>. They provide for example information about whether an expression is an immediate or auxiliary induction variable or a linear combination thereof.
Reference: [HHKT92] <author> M. W. Hall, S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Interprocedural compilation of Fortran D for MIMD distributed-memory machines. </title> <booktitle> In Proceedings of Supercomputing '92, </booktitle> <address> Minneapolis, MN, </address> <month> November </month> <year> 1992. </year>
Reference-contexts: An important interprocedural information provided by the regular compiler is reaching decomposition analysis, which propagates Fortran D specific decomposition information from callers to callees <ref> [HHKT92] </ref>. 3.3 The data flow universe for communication analysis In preparation for analyzing the communication requirements of P , the irregular compiler first determines IREFS, the set of both regular and irregular references to arrays that are accessed irregularly somewhere, and then computes KEYS, the data flow universe.
Reference: [Hig93] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran language specification, version 1.0. </title> <type> Technical Report CRPC-TR92225, </type> <institution> Center for Research on Parallel Computation, Rice University, Houston, TX, </institution> <year> 1992 </year> <month> (revised Jan. </month> <year> 1993). </year> <note> To appear in Scientific Programming, </note> <month> July </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Several research projects have aimed at providing a "machine independent parallel programming style," in which the applications programmer uses a dialect of sequential Fortran and annotates it with high level distribution information. Examples include High Performance Fortran (HPF) <ref> [Hig93] </ref>, Vienna Fortran [CMZ92], and Fortran D [FHK + 90]. A prototype Fortran D compiler targeting MIMD distributed memory machines has been under development at Rice University as part of the ParaScope programming environment [CCH + 88].
Reference: [HK92] <author> R. v. Hanxleden and K. Kennedy. </author> <title> Relaxing SIMD control flow constraints using loop transforma tions. </title> <booktitle> In Proceedings of the ACM SIGPLAN '92 Conference on Program Language Design and Implementation, </booktitle> <address> San Francisco, CA, </address> <month> June </month> <year> 1992. </year> <note> Available via anonymous ftp from softlib.rice.edu as pub/CRPC-TRs/reports/CRPC-TR92207-S. </note>
Reference-contexts: Examples of SIMD specific work are the Communication Compiler [Dah90] for the Connection Machine and the loop flattening transformation to allow efficient execution of loop nests with varying inner loop bounds <ref> [HK92] </ref>. These projects have laid important ground work towards compiling irregular problems under a data parallel, machine independent programming paradigm. However, they tended to be limited either in their generality or in the degree to which they were implemented and practical difficulties were addressed.
Reference: [HK93] <author> R. v. Hanxleden and K. Kennedy. </author> <title> A code placement framework and its application to communication gen eration. </title> <type> Technical Report CRPC-TR93337-S, </type> <note> Center for Research on Parallel Computation, October 1993. Available via anonymous ftp from softlib.rice.edu as pub/CRPC-TRs/reports/CRPC-TR93337-S. </note>
Reference-contexts: Slicing analysis is a technique for generating inspectors and pre-inspectors for multiple levels of indirection [DSvH93]. The Give-N-Take framework is a generalization of partial redundancy elimination techniques that can be used for analyzing data access requirements and for efficient communication placement in the presence of arbitrary control flow <ref> [HK93] </ref>. Examples of SIMD specific work are the Communication Compiler [Dah90] for the Connection Machine and the loop flattening transformation to allow efficient execution of loop nests with varying inner loop bounds [HK92]. <p> The actual data flow equations, formal correctness and optimality criteria, etc., are described elsewhere <ref> [HK93] </ref>. A minor, but in practice interesting point is that in some cases communication might be placed at synthetic nodes, i.e., nodes that correspond to not-yet existing basic blocks. <p> This process does not make use of the Give-N-Take analysis yet, one of the reasons being that Give-N-Take so far does not include dependence analysis. That alone could be changed relatively easily <ref> [HK93] </ref>, but there are also other issues that deserve special attention when generating regular communications from the information provided by Give-N-Take. For example, message tags have to be generated for matching separate sends and receives, which might require the construction of SEND/RECV equivalence classes in case of complicated control flow. <p> Another example is the assignment to j, which became dead due to subscript conversion. 5 Summary and conclusions Compiling irregular applications in a data parallel framework presents a challenging problem of growing importance. Previous works have addressed some of the arising problems <ref> [WSBH91, KM91, CHK + 92, HKK + 92, DSvH93, HK93] </ref>, but very little experience has been gained so far with the compilation of large codes with complicated data access patterns and arbitrary control flow.
Reference: [HKK + 91] <author> S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, and C. Tseng. </author> <title> An overview of the Fortran D programming system. </title> <booktitle> In Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Santa Clara, CA, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: A prototype Fortran D compiler targeting MIMD distributed memory machines has been under development at Rice University as part of the ParaScope programming environment [CCH + 88]. For regular problems, i.e., applications with relatively simple array subscript functions and fixed communication requirements, this compiler has had considerable successes <ref> [HKK + 91, Tse93] </ref>. However, irregular problems, such as molecular dynamics or unstructured mesh codes, have proven to be significantly harder to parallelize [SH91].
Reference: [HKK + 92] <author> R. v. Hanxleden, K. Kennedy, C. Koelbel, R. Das, and J. Saltz. </author> <title> Compiler analysis for irregular prob lems in Fortran D. </title> <booktitle> In Proceedings of the Fifth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> New Haven, CT, </address> <month> August </month> <year> 1992. </year> <note> Available via anonymous ftp from softlib.rice.edu as pub/CRPC-TRs/reports/CRPC-TR92287-S. </note>
Reference-contexts: (n) ffi ; // Redefining data or indirection arrays blocks Reads Read:GIVE init (n) := DEF (n) " : // Reads come "for free" from local definitions Here DEF (n) ffi and DEF (n) " are the data that are either partially or fully enclosed by references in DEF (n) <ref> [HKK + 92] </ref>. <p> An optimization that is not implemented yet but at least conceptionally fairly straightforward is to use incremental schedules for pruning messages in case at least some of the data covered by a reference are already locally available <ref> [DPSM91, HKK + 92] </ref>. <p> Another example is the assignment to j, which became dead due to subscript conversion. 5 Summary and conclusions Compiling irregular applications in a data parallel framework presents a challenging problem of growing importance. Previous works have addressed some of the arising problems <ref> [WSBH91, KM91, CHK + 92, HKK + 92, DSvH93, HK93] </ref>, but very little experience has been gained so far with the compilation of large codes with complicated data access patterns and arbitrary control flow.
Reference: [KM91] <author> C. Koelbel and P. Mehrotra. </author> <title> Programming data parallel algorithms on distributed memory machines using Kali. </title> <booktitle> In Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Parti has also been used to implement user-defined irregular distributions [MSS + 88] and to provide a hashed cache for non-local values [MSMB90]. The feasibility of automatically generating inspectors/executors for simple loop nests has been demonstrated by the Arf [WSBH91] and Kali <ref> [KM91] </ref> compilers. Run time iteration graphs fl From the Proceedings of the Fourth Workshop on Compilers for Parallel Computers, Delft, The Netherlands, December 1993. <p> Another example is the assignment to j, which became dead due to subscript conversion. 5 Summary and conclusions Compiling irregular applications in a data parallel framework presents a challenging problem of growing importance. Previous works have addressed some of the arising problems <ref> [WSBH91, KM91, CHK + 92, HKK + 92, DSvH93, HK93] </ref>, but very little experience has been gained so far with the compilation of large codes with complicated data access patterns and arbitrary control flow.
Reference: [KMV90] <author> C. Koelbel, P. Mehrotra, and J. Van Rosendale. </author> <title> Supporting shared data structures on distributed memory machines. </title> <booktitle> In Proceedings of the Second ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Seattle, WA, </address> <month> March </month> <year> 1990. </year>
Reference-contexts: First, an "inspection phase" determines what data have to be communicated within a certain loop and generates a communication schedule. Then, an "execution phase" (repeatedly) performs the actual computation and uses the communication schedule to exchange data <ref> [MSS + 88, KMV90] </ref>. The Parti communication library was designed to simplify schedule generation and schedule based communication [BS90]. Parti has also been used to implement user-defined irregular distributions [MSS + 88] and to provide a hashed cache for non-local values [MSMB90].
Reference: [MSMB90] <author> S. Mirchandaney, J. Saltz, P. Mehrotra, and H. Berryman. </author> <title> A scheme for supporting automatic data migration on multicomputers. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: The Parti communication library was designed to simplify schedule generation and schedule based communication [BS90]. Parti has also been used to implement user-defined irregular distributions [MSS + 88] and to provide a hashed cache for non-local values <ref> [MSMB90] </ref>. The feasibility of automatically generating inspectors/executors for simple loop nests has been demonstrated by the Arf [WSBH91] and Kali [KM91] compilers. Run time iteration graphs fl From the Proceedings of the Fourth Workshop on Compilers for Parallel Computers, Delft, The Netherlands, December 1993.
Reference: [MSS + 88] <author> R. Mirchandaney, J. Saltz, R. Smith, D. Nicol, and K. Crowley. </author> <title> Principles of runtime support for parallel processors. </title> <booktitle> In Proceedings of the Second International Conference on Supercomputing, </booktitle> <pages> pages 140-152, </pages> <address> St. Malo, France, </address> <month> July </month> <year> 1988. </year>
Reference-contexts: First, an "inspection phase" determines what data have to be communicated within a certain loop and generates a communication schedule. Then, an "execution phase" (repeatedly) performs the actual computation and uses the communication schedule to exchange data <ref> [MSS + 88, KMV90] </ref>. The Parti communication library was designed to simplify schedule generation and schedule based communication [BS90]. Parti has also been used to implement user-defined irregular distributions [MSS + 88] and to provide a hashed cache for non-local values [MSMB90]. <p> Then, an "execution phase" (repeatedly) performs the actual computation and uses the communication schedule to exchange data [MSS + 88, KMV90]. The Parti communication library was designed to simplify schedule generation and schedule based communication [BS90]. Parti has also been used to implement user-defined irregular distributions <ref> [MSS + 88] </ref> and to provide a hashed cache for non-local values [MSMB90]. The feasibility of automatically generating inspectors/executors for simple loop nests has been demonstrated by the Arf [WSBH91] and Kali [KM91] compilers.
Reference: [PSC93] <author> R. Ponnusamy, J. Saltz, and A. Choudhary. </author> <title> Runtime compilation techniques for data partitioning and com munication schedule reuse. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <address> Portland, OR, </address> <month> November </month> <year> 1993. </year> <month> 12 </month>
Reference-contexts: D Newsletter #9, also available via anonymous ftp from softlib.rice.edu as pub/CRPC-TRs/reports/CRPC-TR93339-S. y This work is supported by an IBM fellowship, and by the National Aeronautics and Space Administration/the National Science Foundation under grant #ASC-9349459. 1 can assist in improving load balance and access locality when distributing data across processors <ref> [PSC93] </ref>. Slicing analysis is a technique for generating inspectors and pre-inspectors for multiple levels of indirection [DSvH93]. The Give-N-Take framework is a generalization of partial redundancy elimination techniques that can be used for analyzing data access requirements and for efficient communication placement in the presence of arbitrary control flow [HK93].
Reference: [SH91] <author> J. P. Singh and J. L. Hennessy. </author> <title> An empirical investigation of the effectiveness and limitations of automatic parallelization. </title> <booktitle> In Proceedings of the International Symposium on Shared Memory Multiprocessing, </booktitle> <address> Tokyo, Japan, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: For regular problems, i.e., applications with relatively simple array subscript functions and fixed communication requirements, this compiler has had considerable successes [HKK + 91, Tse93]. However, irregular problems, such as molecular dynamics or unstructured mesh codes, have proven to be significantly harder to parallelize <ref> [SH91] </ref>. Some of the obstacles encountered are: * Lack of compile time knowledge about where and which data have to be communicated. * Limited access locality. * Large communication requirements. * Poor load balance.
Reference: [Tse93] <author> C. Tseng. </author> <title> An Optimizing Fortran D Compiler for MIMD Distributed-Memory Machines. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: A prototype Fortran D compiler targeting MIMD distributed memory machines has been under development at Rice University as part of the ParaScope programming environment [CCH + 88]. For regular problems, i.e., applications with relatively simple array subscript functions and fixed communication requirements, this compiler has had considerable successes <ref> [HKK + 91, Tse93] </ref>. However, irregular problems, such as molecular dynamics or unstructured mesh codes, have proven to be significantly harder to parallelize [SH91].
Reference: [WSBH91] <author> J. Wu, J. Saltz, H. Berryman, and S. Hiranandani. </author> <title> Distributed memory compiler design for sparse problems. </title> <type> ICASE Report 91-13, </type> <institution> Institute for Computer Application in Science and Engineering, Hampton, VA, </institution> <month> January </month> <year> 1991. </year>
Reference-contexts: Parti has also been used to implement user-defined irregular distributions [MSS + 88] and to provide a hashed cache for non-local values [MSMB90]. The feasibility of automatically generating inspectors/executors for simple loop nests has been demonstrated by the Arf <ref> [WSBH91] </ref> and Kali [KM91] compilers. Run time iteration graphs fl From the Proceedings of the Fourth Workshop on Compilers for Parallel Computers, Delft, The Netherlands, December 1993. <p> Another example is the assignment to j, which became dead due to subscript conversion. 5 Summary and conclusions Compiling irregular applications in a data parallel framework presents a challenging problem of growing importance. Previous works have addressed some of the arising problems <ref> [WSBH91, KM91, CHK + 92, HKK + 92, DSvH93, HK93] </ref>, but very little experience has been gained so far with the compilation of large codes with complicated data access patterns and arbitrary control flow.
References-found: 25

