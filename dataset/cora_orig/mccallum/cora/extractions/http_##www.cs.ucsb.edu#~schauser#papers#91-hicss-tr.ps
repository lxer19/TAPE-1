URL: http://www.cs.ucsb.edu/~schauser/papers/91-hicss-tr.ps
Refering-URL: http://www.cs.ucsb.edu/~schauser/papers/
Root-URL: http://www.cs.ucsb.edu
Title: Evaluation of a "Stall" Cache: An Efficient Restricted On-chip Instruction Cache  
Author: Klaus Erik Schauser Krste Asanovic David A. Patterson Edward H. Frank 
Address: Berkeley, CA 94720  Mountain View, California  
Affiliation: Computer Science Division, EECS Department University of California, Berkeley  Sun Microsystems  
Abstract: In this report we compare the cost and performance of a new kind of restricted instruction cache architecture | the stall cache | against several other conventional cache architectures. The stall cache minimizes the size of an on-chip instruction cache by caching only those instructions whose instruction fetch phase collides with the memory access phase of a preceding load or store instruction. Many existing machines provide a single cycle external cache memory [6, 17, 2]. Our results show that, under this assumption, the stall cache always outperforms an equivalent sized on-chip instruction cache, reducing external memory access stalls by approximately 10%. In addition we present results for a system using an on-chip data cache, and for one with a double width data bus and short instruction prefetch buffer. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal and M. Huffman. </author> <title> Blocking: Exploiting Spatial Locality for Trace Compaction. </title> <booktitle> In Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 48-57. </pages> <publisher> ACM SIGMETRIC, </publisher> <month> May </month> <year> 1990. </year>
Reference-contexts: The advantage of this scheme is that with only a small fraction of the total address trace the original program behavior can be reproduced. In <ref> [1] </ref> and [24] alternative schemes are presented for trace compaction and efficient trace-driven simulation. We chose the simpler approach because it was satisfactory for our purposes, saving both disk space and cache simulation time.
Reference: [2] <author> A. V. Bechtolsheim and E. H. Frank. </author> <title> Sun's SPARCsta-tion 1: A Workstation for the 1990s. </title> <booktitle> In COMPCON, </booktitle> <pages> pages 184-188, </pages> <month> February </month> <year> 1990. </year>
Reference: [3] <author> E. W. Brown, A. Agrawal, T. Creary, M. F. Klein, D. Murata, and J. Petolino. </author> <title> Implementing SPARC in ECL. </title> <journal> MICRO, </journal> <volume> 10(2) </volume> <pages> 10-22, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: Therefore the stall cache is a restricted instruction cache employing a similar philosophy as a branch target buffer [15], but for data access stalls instead of branch stalls. The single double-width memory port, as shown in without the overhead of two sets of address lines <ref> [3] </ref>. We assume that instructions occupy only one word each, and hence that every external cache access can return 2 Buffer two instructions. <p> This is because the buffer must be flushed on taken branches, limiting the effectiveness of larger buffers. Our findings are consistent with those for the ECL SPARC <ref> [3] </ref>. Small instruction buffers can give the same performance improvement as a 1 Kbyte instruction cache. We were interested in determining the performance/cost of larger block sizes for direct mapped caches with single word sub-block placement.
Reference: [4] <institution> Systems Performance Evaluation Cooperative. </institution> <note> SPEC Benchmark Suite Release 1.0. SPEC Newsletter, 1(1), Fall 1989. </note>
Reference-contexts: The execution time is Cycles I-buffer = I-count + Buffer stalls 4 Methodology We used trace driven simulation to evaluate the different configurations [21]. We took our traces from the SPEC benchmark suite running on a MIPS R3000 <ref> [4, 5] </ref>. The SPEC benchmark suite consists of ten different programs, 4 written in C and 6 in FORTRAN. The number of references generated by execution of these programs reach into the tens of billions on the MIPS R3000.
Reference: [5] <author> K. Dixit and V. Metha. </author> <title> Analyzing Performance using SPEC Release 1.0 Benchmarks. </title> <journal> SPEC Newsletter, </journal> <volume> 1(2), </volume> <month> Spring </month> <year> 1990. </year>
Reference-contexts: The execution time is Cycles I-buffer = I-count + Buffer stalls 4 Methodology We used trace driven simulation to evaluate the different configurations [21]. We took our traces from the SPEC benchmark suite running on a MIPS R3000 <ref> [4, 5] </ref>. The SPEC benchmark suite consists of ten different programs, 4 written in C and 6 in FORTRAN. The number of references generated by execution of these programs reach into the tens of billions on the MIPS R3000.
Reference: [6] <author> M. Forsyth, S. Mangelsdorf, E. DeLano, C. Gleason, J. Yetter, and D. Steiss. </author> <title> CMOS PA-RISC Processor for a New Family of Workstations. </title> <booktitle> In COMPCON, </booktitle> <pages> pages 202-207, </pages> <month> February </month> <year> 1991. </year>
Reference: [7] <author> E. H. Frank and M. Namjoo. </author> <title> Apparatus and Method for Providing a Stall Cache | Patent Disclosure. </title> <type> Technical report, </type> <institution> SUN Microsystems, </institution> <year> 1989. </year> <month> 10 </month>
Reference-contexts: One solution is to have separate data and instruction busses; another possibility is to have caches on chip. In this report we evaluate the cost and performance of a number of existing solutions for removing these stalls against a new proposal: the stall cache <ref> [7] </ref>.
Reference: [8] <author> J. L. Hennessy. </author> <title> VLSI Processor Architecture. </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-33(11):1221-1246, </volume> <month> December </month> <year> 1984. </year>
Reference-contexts: 1 Introduction RISC instruction sets are designed to facilitate pipelin-ing, with many architectures aiming to issue at least one instruction per clock cycle <ref> [19, 20, 8, 18] </ref>. To sustain this instruction bandwidth, most RISC machines employ some form of instruction cache. Current device technology does not allow large on-chip caches, and so many designs employ a large external cache to achieve low miss-rates over a wide range of applications.
Reference: [9] <author> J. L. Hennessy and D. A. Patterson. </author> <title> Computer Architecture | A Quantative Approach. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: Usually variables and intermediate results can be kept in registers, reducing the number of data accesses as compared to memory-memory or register-memory architectures. However, studies have shown that loads and stores still account for around 25-40% of all instructions executed <ref> [9] </ref>. If a processor is to avoid memory access stalls, the memory subsystem must be capable of delivering at least one instruction word every cycle while servicing data accesses.
Reference: [10] <author> M. D. Hill. DineroIII Documentation, </author> <title> Unpublished Unix-style Man Pages. </title> <institution> Computer Science Division (EECS), University of California, Berkeley, </institution> <month> October </month> <year> 1985. </year>
Reference-contexts: We ran each instrumented benchmark program twice; once to count the total number of instructions executed, and again to take the evenly spaced samples. We then used this stored sequence of samples as input to our cache simulations. The main simulation tool used was the dinero cache simulator <ref> [10, 11] </ref>. We had to make a number of modifications to dinero to record coincident instruction and data misses. We also wrote an instruction buffer simulator that reads dinero format traces. 5 Results In Section 5.1 we present dynamic memory reference statistics for the individual benchmarks.
Reference: [11] <author> M. D. Hill. </author> <title> Aspects of Cache Memory and Instruction Buffer Performance. </title> <type> PhD thesis, </type> <institution> Computer Science Division (EECS), University of California, Berke-ley, </institution> <month> November </month> <year> 1987. </year> <note> Available as Tech. Report No. UCB/CSD 87/381. </note>
Reference-contexts: We ran each instrumented benchmark program twice; once to count the total number of instructions executed, and again to take the evenly spaced samples. We then used this stored sequence of samples as input to our cache simulations. The main simulation tool used was the dinero cache simulator <ref> [10, 11] </ref>. We had to make a number of modifications to dinero to record coincident instruction and data misses. We also wrote an instruction buffer simulator that reads dinero format traces. 5 Results In Section 5.1 we present dynamic memory reference statistics for the individual benchmarks.
Reference: [12] <author> M. D. Hill. </author> <title> A Case for Direct Mapped Caches. </title> <journal> Computer, </journal> <volume> 21(12) </volume> <pages> 25-40, </pages> <month> December </month> <year> 1988. </year>
Reference-contexts: For the data cache, we see that larger block sizes lower performance. This is explained by the poor spatial locality of the data reference streams. Increasing associativity can improve cache performance, but adds to cost by increasing tag area and complicating control logic. In <ref> [12] </ref> Hill shows how the added logic may slow cache cycle time and negate the bene fits of increasing associativity, here we did not attempt to account for these effects.
Reference: [13] <author> G. Kane. </author> <title> MIPS RISC Architecture (R2000/R3000). </title> <publisher> Prentice Hall, </publisher> <year> 1989. </year>
Reference-contexts: Sources of stall cycles include load delays, branch delays, and floating point stalls in addition to memory system stalls. All our results are for the MIPS R3000 <ref> [13] </ref>. Compilers for this architecture must fill unused load and branch delay slots with NOP instructions since there are no hardware interlocks. Thus for the MIPS architecture the instruction count includes these delay stall cycles.
Reference: [14] <author> S. Laha, J. H. Patel, and R. K. Iyer. </author> <title> Accurate Low-cost Methods for Performance Evaluation of Cache Memory Systems. </title> <journal> IEEE Trans. on Computers, </journal> <volume> 37(11) </volume> <pages> 1325-1336, </pages> <month> November </month> <year> 1988. </year>
Reference-contexts: Our solution was to adopt a sampling technique similar to Laha et al <ref> [14] </ref> to generate a shorter | and hence storable | trace. At evenly spaced intervals over the entire execution of the benchmark we take samples of the address trace. The results in [14] show that as few as 35 samples can give accurate estimates of both the mean value and the <p> Our solution was to adopt a sampling technique similar to Laha et al <ref> [14] </ref> to generate a shorter | and hence storable | trace. At evenly spaced intervals over the entire execution of the benchmark we take samples of the address trace. The results in [14] show that as few as 35 samples can give accurate estimates of both the mean value and the distribution of the miss ratio. The advantage of this scheme is that with only a small fraction of the total address trace the original program behavior can be reproduced. <p> This scheme worked well for all the benchmark programs except tomcatv. This is a very small FORTRAN program, exhibiting strong periodicity that unfortunately correlated with our sampling frequency. In <ref> [14] </ref> this problem is mentioned and sampling at random intervals is recommended for these cases. We repeated the sampling of tomcatv using 37 equally spaced samples, and this simple alternative approach gave good results. The espresso benchmark is the only SPEC 4 program to require multiple runs using different inputs.
Reference: [15] <author> J. K. F. Lee and A. J. Smith. </author> <title> Branch Prediction Strategies and Branch Target Buffer Design. </title> <journal> Computer, </journal> <volume> 17(1) </volume> <pages> 6-22, </pages> <month> January </month> <year> 1984. </year>
Reference-contexts: If the next instruction is not in the stall cache, we add a stall cycle to the pipeline to fetch the missing instruction from external cache. Therefore the stall cache is a restricted instruction cache employing a similar philosophy as a branch target buffer <ref> [15] </ref>, but for data access stalls instead of branch stalls. The single double-width memory port, as shown in without the overhead of two sets of address lines [3]. We assume that instructions occupy only one word each, and hence that every external cache access can return 2 Buffer two instructions.
Reference: [16] <author> MIPS. </author> <title> pixie documentation. </title> <journal> Unpublished Unix-style man pages. </journal>
Reference-contexts: We compiled the benchmark programs using version 2.11 of the MIPS cc and FORTRAN compilers for a MIPS M/2000 running RISC/os 4.50. We used pixie to instrument the object code to generate the instruction and data references <ref> [16] </ref>. We ran each instrumented benchmark program twice; once to count the total number of instructions executed, and again to take the evenly spaced samples. We then used this stored sequence of samples as input to our cache simulations.
Reference: [17] <author> M. J. K Nielsen. </author> <title> DECstation 5000 Model 200. </title> <booktitle> In COM-PCON, </booktitle> <pages> pages 220-225, </pages> <month> February </month> <year> 1991. </year>
Reference: [18] <author> D. A. Patterson. </author> <title> Reduced Instruction Set Computers. </title> <journal> Communications of the Association for Computing Machinery, </journal> <volume> 28(1) </volume> <pages> 8-21, </pages> <month> Januar </month> <year> 1985. </year>
Reference-contexts: 1 Introduction RISC instruction sets are designed to facilitate pipelin-ing, with many architectures aiming to issue at least one instruction per clock cycle <ref> [19, 20, 8, 18] </ref>. To sustain this instruction bandwidth, most RISC machines employ some form of instruction cache. Current device technology does not allow large on-chip caches, and so many designs employ a large external cache to achieve low miss-rates over a wide range of applications.
Reference: [19] <author> D. A. Patterson and D. R. Ditzel. </author> <title> The Case for the Reduced Instruction Set Computer. </title> <journal> Computer Architecture News, </journal> <volume> 8(6) </volume> <pages> 25-33, </pages> <month> October </month> <year> 1980. </year>
Reference-contexts: 1 Introduction RISC instruction sets are designed to facilitate pipelin-ing, with many architectures aiming to issue at least one instruction per clock cycle <ref> [19, 20, 8, 18] </ref>. To sustain this instruction bandwidth, most RISC machines employ some form of instruction cache. Current device technology does not allow large on-chip caches, and so many designs employ a large external cache to achieve low miss-rates over a wide range of applications.
Reference: [20] <author> G. Radin. </author> <title> The 801 Minicomputer. </title> <booktitle> In Proc. Symposium Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 39-47, </pages> <month> March </month> <year> 1982. </year>
Reference-contexts: 1 Introduction RISC instruction sets are designed to facilitate pipelin-ing, with many architectures aiming to issue at least one instruction per clock cycle <ref> [19, 20, 8, 18] </ref>. To sustain this instruction bandwidth, most RISC machines employ some form of instruction cache. Current device technology does not allow large on-chip caches, and so many designs employ a large external cache to achieve low miss-rates over a wide range of applications.
Reference: [21] <author> A. J. Smith. </author> <title> Cache Memories. </title> <journal> Computing Surveys, </journal> <volume> 14(3) </volume> <pages> 473-530, </pages> <month> September </month> <year> 1982. </year>
Reference-contexts: The buffer is flushed on taken branches. Stall cycles only occur if the buffer is empty when a load or store instruction requires a memory access. The execution time is Cycles I-buffer = I-count + Buffer stalls 4 Methodology We used trace driven simulation to evaluate the different configurations <ref> [21] </ref>. We took our traces from the SPEC benchmark suite running on a MIPS R3000 [4, 5]. The SPEC benchmark suite consists of ten different programs, 4 written in C and 6 in FORTRAN. <p> To assess the effects of changing associativity we varied associativity from direct mapped to 8-way for caches with single word blocks. Previous studies have shown little performance improvement for degrees of associativity greater than 8-way <ref> [21] </ref>. 5.3 Speedup Calculation We measure performance as the speedup of a configuration relative to the time required for the unenhanced processor Speedup Configuration = Cycles Unenhanced Cycles Configuration This speedup should be reduced to take into account floating point stalls and external cache misses.
Reference: [22] <author> J. E. Smith and J. R. Goodman. </author> <title> A Study of Instruction Cache Organizations and Replacement Policies. </title> <booktitle> In Proc. Tenth Symposium on Computer Architecture, </booktitle> <pages> pages 132-137, </pages> <month> June </month> <year> 1983. </year>
Reference-contexts: The replacement policy was LRU. Increasing associativity improves performance/cost for stall caches larger than 512 bytes, with negligible effect on smaller caches. The performance/cost of the instruction cache decreases with increasing associativity for small caches. We believe this is due to the same reasons as cited in <ref> [22] </ref>. A high degree of associativity combined with an LRU replacement policy can cause a higher miss rate in instruction loops that exceed the cache capacity.
Reference: [23] <author> H. Vlahos and V. Milutinovic. </author> <title> GaAs Microprocessors and Digital Systems. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 28-56, </pages> <month> Febru-ary </month> <year> 1988. </year>
Reference-contexts: It is likely that this style of design will remain popular for device technologies where large on-chip caches are not feasible. In particular, device technologies such as GaAs offer high speed but relatively low integration density <ref> [23] </ref>. On most RISC architectures, the only instructions that access memory are loads and stores. Usually variables and intermediate results can be kept in registers, reducing the number of data accesses as compared to memory-memory or register-memory architectures.

References-found: 23

