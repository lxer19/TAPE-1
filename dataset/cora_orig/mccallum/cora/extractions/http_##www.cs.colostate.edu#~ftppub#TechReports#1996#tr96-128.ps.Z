URL: http://www.cs.colostate.edu/~ftppub/TechReports/1996/tr96-128.ps.Z
Refering-URL: http://www.cs.colostate.edu/~ftppub/
Root-URL: 
Email: malaiya@cs.colostate.edu  Rick Karcich@stortek.com  
Phone: (303) 673-6223  Phone: (970) 491-5792 Fax: (970) 491-2466  
Title: Software Test Coverage and Reliability  
Author: Yashwant K. Malaiya Naixin Li James M. Bieman Rick Karcich Bob Skibbe Y. Malaiya and N. Li 
Note: are partly supported by a BMDO funded project monitored by ONR J. Bieman is supported, in part, by the NASA Langley Research Center, the Colorado Advanced Software Institute (CASI) and Storage Technology Inc. CASI is supported in part by the Colorado Advanced Technology Institute (CATI). CATI promotes advanced technology teaching and research at universities in Colorado for the purpose of economic development.  
Web: WWW: http://www.cs.colostate.edu  
Address: Fort Collins, CO 80523  2270 South 88th Street Louisville, CO 80028-2286  Fort Collins, CO 80523-1873  
Affiliation: Computer Science  Computer Science Dept. Colorado State University  StorageTek  Computer Science Department Colorado State University  
Pubnum: Technical Report  Technical Report CS-96-128  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> J. R. Adams. </author> <title> Software reliability predictions are practical today. </title> <booktitle> In 7th Annual Software Reliability Symposium, </booktitle> <address> Denver, Colorado, </address> <month> May </month> <year> 1989. </year>
Reference-contexts: Discrete detectability profiles have been calculated for several small and large combinational circuits. Continuous detectability profiles for some benchmark circuits have been estimated [27]. Similar data for some software systems is also available <ref> [1] </ref>. Fortunately, it is possible to obtain an approximation for the detectability profiles.
Reference: [2] <author> H. Agrawal, J. Horgan, E. Krauser, and S. </author> <title> London. A testing-based model and risk browser for C. </title> <booktitle> In Int'l Conf. Rel. Qual. Control & Risk Assessment, </booktitle> <pages> pages 1-7, </pages> <month> Oct. </month> <year> 1993. </year>
Reference-contexts: Branch coverage may be an adequate measure in many cases, since about 80% branch coverage often produces acceptable results [11]. However for testing of individual modules or for highly reliable software p-use may be better. Researchers suggests the use of a weighted risk measure <ref> [2, 24, 21] </ref>. Weights are chosen on the basis of relative significance of each measure. As we find, structural coverage measures tend to be strongly correlated, and thus a weighted average may not provide more information than a single measure. We need to identify more independent measures.
Reference: [3] <author> B. Beizer. </author> <title> Software Testing Techniques. </title> <publisher> Van Nostrand Reinhold, </publisher> <address> 2nd edition, </address> <year> 1990. </year>
Reference-contexts: The needs of early reliability measurement and modeling are not met by common testing practices. The focus of testing is on finding defects, and defects can be often found much faster by non-random methods <ref> [3] </ref>. Testing is directed towards inputs and program components where errors are more likely. For example, testing may be conducted to ensure that particular portions of the program and/or boundary cases are covered. Models that can measure and predict reliability based on the status of non-random testing are clearly needed. <p> The model given by Equation 9 fits the data well as indicated by the LSE values. The data shows that C 1 &gt; C 2 &gt; C 4 , which is expected. Complete decision coverage implies complete block coverage, and complete p-uses coverage implies complete decision coverage <ref> [3, 9, 22] </ref>. The c-uses coverage has no such relation relative to the other metrics. Table 5 summarizes the result for DS2. Nine faults were revealed by application of 1196 tests; we assume that one fault (i.e. 10%) is still undetected.
Reference: [4] <author> J. Bieman and J. Schultz. </author> <title> An empirical evaluation (and specification) of the all-du-paths testing criterion. </title> <journal> Software Engineering Journal, </journal> <pages> pages 43-51, </pages> <month> Jan. </month> <year> 1992. </year>
Reference-contexts: Table 1 shows the maximum number of tests [32] to satisfy these criteria. The upper bound for all-du-paths was reached in one subroutine out of 143 considered by Bieman and Schultz <ref> [4] </ref>. 3 Detectability Profiles of Enumerables The coverage achieved by a set of tests depends not only on the number and types of tests applied (or, equivalently, the testing time) but also on the distribution of testability values of the enumerables.
Reference: [5] <author> M. Chen, J. Horgan, A. Mathur, and V. Rego. </author> <title> A time/structure based model for estimating software reliability. </title> <type> Technical Report SERC-TR-117-P, </type> <institution> Purdue University, </institution> <month> Dec. </month> <year> 1992. </year>
Reference-contexts: Vouk's experimental results suggest the use of a more general Weibull distribution. He observes that, in terms of error removal capability, the relative power of the coverage measures fblock:p-use:DUD-chainsg is f1:2:6g. Chen et al <ref> [5] </ref> add structural coverage to traditional time-based software reliability models (SRMs). Their model excludes test cases that do not increase coverage. The adjusted test effort 2 data is used to fit existing time-based models to avoid the overestimation of traditional time-based SRMs due to the saturation effect of testing strategies.
Reference: [6] <author> L. A. Clarke, A. Podgurski, D. J. Richardson, and S. J. Zeil. </author> <title> A formal evaluation of data flow path selection criteria. </title> <journal> IEEE Trans. Software Engineering, </journal> <volume> SE-15(11):1318-1332, </volume> <month> Nov. </month> <year> 1989. </year>
Reference-contexts: Test coverage can also be measured in other units, such as context, required k-tuple, etc. A formal evaluation of different test coverage criteria was presented by Clarke <ref> [6] </ref>. When such a unit is exercised, it is possible that one or more associated faults may be detected. Counting the number of units covered gives us a measure of the extent of sampling.
Reference: [7] <author> S. Dalal, J. Hogan, and J. Kettenring. </author> <title> Reliable software and communications: software quality, reliability and safety. </title> <booktitle> In 15 Int'l Conf. Software Engineering, </booktitle> <pages> pages 425-435, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: They try a variety of models to fit the data. The best fit is obtained using the Goel and Okumoto's exponential model. Dalal, Horgan and Ketterring <ref> [7] </ref> examine the correlation between test coverage and the error removal rate. They give a scatter plot of the number of faults detected during system testing versus the block coverage achieved during unit testing.
Reference: [8] <author> P. Frankl and N. Weiss. </author> <title> An experimental comparison of the effectiveness of branch testing and data flow testing. </title> <journal> IEEE Trans. Software Engineering, </journal> <pages> pages 774-787, </pages> <month> Aug. </month> <year> 1993. </year>
Reference-contexts: Their model is equivalent to the Goel-Okumoto model [25]. They also derive an exponential model relating the covering frequency to the error removal ratio. The model relies on prior knowledge of the error distribution over different functional groups in a product. Frankl and Weiss <ref> [8] </ref> compare the fault exposing capability of branch coverage and data flow coverage criteria. They find that for 4 out of 7 programs, the effectiveness of a test in exposing an error is positively correlated with the two coverage measures.
Reference: [9] <author> P. Frankl and E. Weyuker. </author> <title> An applicable family of data flow testing criteria. </title> <journal> IEEE Trans. Software Engineering, </journal> <pages> pages 1483-1498, </pages> <month> Oct. </month> <year> 1988. </year>
Reference-contexts: The model given by Equation 9 fits the data well as indicated by the LSE values. The data shows that C 1 &gt; C 2 &gt; C 4 , which is expected. Complete decision coverage implies complete block coverage, and complete p-uses coverage implies complete decision coverage <ref> [3, 9, 22] </ref>. The c-uses coverage has no such relation relative to the other metrics. Table 5 summarizes the result for DS2. Nine faults were revealed by application of 1196 tests; we assume that one fault (i.e. 10%) is still undetected.
Reference: [10] <author> F. D. Frate, P. Garg, A. P. Mathur, and A. Pasquini. </author> <title> Experiments to investigate the correlation between code coverage and software reliability. </title> <type> Technical Report SERC-TR-162-P, </type> <institution> Purdue University, Software Engineering Research Center, Purdue University, West Lafayette, Indiana 47907, </institution> <month> Apr. </month> <year> 1995. </year>
Reference-contexts: In very small programs, if the bugs are being fixed as they found, the behavior will be similar to evolving programs. This is because fixing each bug can replace a significant fraction of the code, thus possibly reducing coverage <ref> [10] </ref>. 7 Model Parameters Researchers have shown that the logarithmic model generally works well [17, 20], however interpretation of its parameters is difficult. One interpretation given by Malaiya et al [18] is described by Equations 10 and 11. The same interpretation may be applicable for enumerables other than defects.
Reference: [11] <author> R. E. Grady. </author> <title> Practical Software Metrics for Project Management and Process Improvement. </title> <publisher> Prentice-Hall, </publisher> <year> 1992. </year> <pages> pp. 58-60. </pages>
Reference-contexts: At 50% branch coverage the fault coverage is still quite low (about 10%), however with only 84% branch coverage, 90% fault coverage is obtained. The branch coverage shows saturation at about 84%. This supports the view that 80% branch coverage is often adequate <ref> [11] </ref>. perspective. The computed values are from the number of tests (traditional reliability growth mod eling), and test coverage measures C 1 , C 2 , C 3 and C 4 using Equation 12. The calculated values are all quite close, showing that coverage based modeling can replace time-based modeling. <p> If the requirements are such that 100% block coverage is not enough, branch or p-use coverage may be more appropriate. Branch coverage may be an adequate measure in many cases, since about 80% branch coverage often produces acceptable results <ref> [11] </ref>. However for testing of individual modules or for highly reliable software p-use may be better. Researchers suggests the use of a weighted risk measure [2, 24, 21]. Weights are chosen on the basis of relative significance of each measure.
Reference: [12] <author> H. Hecht and P. Crane. </author> <title> Rare conditions and their effect on software failures. </title> <booktitle> In Annual Reliability and Maintainability Symp., </booktitle> <pages> pages 334-337, </pages> <month> Jan. </month> <year> 1994. </year>
Reference-contexts: These modules rarely run <ref> [12] </ref>, and are notoriously difficult to test. Yet, they are critical components of a system that must be highly reliable. Only by forcing the coverage of such critical components, can reliability be predicted at very high levels. Intuition and empirical evidence suggests that test coverage must be related to reliability.
Reference: [13] <author> M. Hutchings, H. Foster, T. Goradia, and T. </author> <title> Ostrand. Experiments on the effectiveness of dataflow- and controlflow-based test adequacy criteria. </title> <booktitle> In Int'l Conf. Software Engineering, </booktitle> <pages> pages 191-200, </pages> <year> 1994. </year>
Reference-contexts: They observed complex relationships between test coverage growth and the probability of exposing an error for a test set. Since the seven programs they used are very small and they only considered subtle errors, the result may not be directly applicable to large commercial software systems. Hutchins, et al <ref> [13] </ref> study detection effectiveness of test sets with different coverage values for realistic seeded faults. They find that a test set with higher coverage has higher per-test detection probability. They also showed that 100% coverage using a specific measure may not detect all the faults.
Reference: [14] <author> N. Li and Y. K. Malaiya. </author> <title> Fault exposure ratio and software reliability. </title> <booktitle> In 3rd Workshop on Issues in Software Reliability and Testing, pages 6.3.1-6.3.18, </booktitle> <address> Boulder, Colorado, </address> <month> Oct. </month> <year> 1993. </year>
Reference-contexts: An empirical method to estimate the initial fault exposure ratio K 0 (0) has been suggested by Li and Malaiya <ref> [14] </ref>. Estimation of a i remains an open problem. The second parameter is given by, b i = a 0 T s (14) The single test execution time T s depends on the program size and its structure.
Reference: [15] <author> N. Li and Y. K. Malaiya. </author> <title> Robust: A next generation software reliability engineering tool. </title> <booktitle> In Proc. Int'l Symp. on Software Reliability Engineering, </booktitle> <address> France, </address> <month> Oct. </month> <note> 1995 (to appear). </note>
Reference-contexts: The results are consistent with the analytical coverage inclusion relationships. Our model is simple and easily explained, and is thus 15 suitable for industrial use. A preliminary implementation of the model as part of the ROBUST tool is presented in <ref> [15] </ref>. The model given by Equation 12 can be used in two different ways. Extrapolation requires collecting data for part of the testing process, which is then used to estimate the applicable parameter values. These are used for making projections for planning the rest of the test effort.
Reference: [16] <author> M. Lyu, J. Horgan, and S. </author> <title> London. A coverage analysis tool for the effectiveness of software testing. </title> <booktitle> In Int'l Symposium on Software Reliability Engineering, </booktitle> <pages> pages 25-34, </pages> <year> 1993. </year>
Reference-contexts: The first data set, DS1, is from a multiple-version automatic airplane landing system <ref> [16] </ref>.
Reference: [17] <author> Y. Malaiya, N. Karunanithi, and P. Verma. </author> <title> Predictability of software reliability models. </title> <journal> IEEE Trans. Reliability, </journal> <pages> pages 539-546, </pages> <month> Dec. </month> <year> 1992. </year> <month> 17 </month>
Reference-contexts: This hypothesis follows directly from the Logarithmic model, which has been shown to have superior predictive capability compared with other 2 parameter models <ref> [17] </ref>. <p> This is because fixing each bug can replace a significant fraction of the code, thus possibly reducing coverage [10]. 7 Model Parameters Researchers have shown that the logarithmic model generally works well <ref> [17, 20] </ref>, however interpretation of its parameters is difficult. One interpretation given by Malaiya et al [18] is described by Equations 10 and 11. The same interpretation may be applicable for enumerables other than defects.
Reference: [18] <author> Y. Malaiya, A. von Mayrhauser, and P. Srimani. </author> <title> An examination of fault exposure ratio. </title> <journal> IEEE Trans. Software Engineering, </journal> <pages> pages 1087-1094, </pages> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: The detectability profile is the distribution of detectability values in the system under test. The detectability profiles were introduced by Malaiya and Yang [19]. They have been used to characterize testing of hardware [31] and software <ref> [18] </ref>. A continuous version of the detectability profile was defined by Seth, Agrawal and Farhat [27]. For convenience, we use the normalized detectability profile (NDP) as defined below. <p> In actual practice, a test 7 case is selected in order to exercise a functionality or enumerable that has remained untested so far. This process makes actual testing more directed and hence more efficient than random testing. Malaiya, von Mayrhauser and Srimani <ref> [18] </ref> show that this non-random process leads to a defect finding behavior described by the logarithmic growth model [20]. Their analysis gives an interpretation for the model parameters. The coverage growth of an enumerable-type depends on the detectability profile of the type and the test selection strategy. <p> This thus takes into account the fact that 100% coverage of one enumerable does not mean exhaustive exercising of the system. For defects (i = 0), the parameters fi 0 0 and fi 1 1 have the following interpretation <ref> [18] </ref>. fi 0 K 0 (0)N 0 (0) (10) fi 0 where K 0 (0) is the exposure ratio at time t = 0, T L is the linear execution time and a 0 is a parameter that describes the variation in the exposure ratio. <p> This is because fixing each bug can replace a significant fraction of the code, thus possibly reducing coverage [10]. 7 Model Parameters Researchers have shown that the logarithmic model generally works well [17, 20], however interpretation of its parameters is difficult. One interpretation given by Malaiya et al <ref> [18] </ref> is described by Equations 10 and 11. The same interpretation may be applicable for enumerables other than defects. <p> Thus it is possible that they may be estimable with reasonable accuracy. Extensive data collection is needed to examine this question in detail. 8 Defect density and reliability Since the failure intensity is proportional to the number of defects <ref> [18] </ref>, we have, = T L Where K is the overall value of fault exposure ratio. Let N 0 be the total number of faults initially present in the program and there is no new fault introduced during testing process.
Reference: [19] <author> Y. Malaiya and S. Yang. </author> <title> The coverage problem for random testing. </title> <booktitle> In Int'l Test Conference, </booktitle> <pages> pages 237-242, </pages> <month> Oct. </month> <year> 1984. </year>
Reference-contexts: Definition: Detectability of an enumerable d j l is the probability that the l-th enumerable of type j will be exercised by a randomly chosen test. The detectability profile is the distribution of detectability values in the system under test. The detectability profiles were introduced by Malaiya and Yang <ref> [19] </ref>. They have been used to characterize testing of hardware [31] and software [18]. A continuous version of the detectability profile was defined by Seth, Agrawal and Farhat [27]. For convenience, we use the normalized detectability profile (NDP) as defined below. <p> P 1 j di = 1 since all fractions added will be unity. A detectability value of 0 is possible, since a branch might be infeasible, or a defect might not 5 be testable due to redundancy in implementation. Researchers have compiled detectability profiles of several digital circuits <ref> [19, 31] </ref> and software systems [28]. If the number of enumerables is large, a continuous function can approximate the discrete NDP. <p> Hence, it can be used to calculate expected coverage when a given number of tests have been applied. Here, we assume that testing is random, i.e. any single test is selected randomly with replacement. Malaiya and Yang <ref> [19] </ref>, and Wagner et al [31] show that the expected coverage of the enumerables of type j is C j (n) = 1 i=1 j j provided testing is random.
Reference: [20] <author> J. D. Musa, A. Iannino, and K. Okumoto. </author> <title> Software Reliability Measurement, Prediction, Applications. </title> <publisher> McGraw-Hill, </publisher> <year> 1987. </year>
Reference-contexts: 1 Introduction Developers can achieve the target reliability of software systems in a predictable way by evaluating reliability during development. By evaluating and projecting reliability growth, the developers can optimally allocate resources to meet a deadline with the target reliability <ref> [20] </ref>. Reliability can be estimated as soon as running code exists. To quantify reliability during testing, the code (or portion of code) is executed using inputs randomly selected following some distribution. fl Y. Malaiya and N. Li are partly supported by a BMDO funded project monitored by ONR y J. <p> This process makes actual testing more directed and hence more efficient than random testing. Malaiya, von Mayrhauser and Srimani [18] show that this non-random process leads to a defect finding behavior described by the logarithmic growth model <ref> [20] </ref>. Their analysis gives an interpretation for the model parameters. The coverage growth of an enumerable-type depends on the detectability profile of the type and the test selection strategy. <p> This is because fixing each bug can replace a significant fraction of the code, thus possibly reducing coverage [10]. 7 Model Parameters Researchers have shown that the logarithmic model generally works well <ref> [17, 20] </ref>, however interpretation of its parameters is difficult. One interpretation given by Malaiya et al [18] is described by Equations 10 and 11. The same interpretation may be applicable for enumerables other than defects.
Reference: [21] <author> A. M. Neufelder. </author> <title> Ensuring Software Reliability. </title> <publisher> Marcel Dekker Inc., </publisher> <year> 1993. </year> <pages> pp. 137-140. </pages>
Reference-contexts: They find that a test set with higher coverage has higher per-test detection probability. They also showed that 100% coverage using a specific measure may not detect all the faults. The Leone test coverage model given in <ref> [21] </ref> is a weighted average of four different coverage metrics achieved during test phases: lines of executable code, independent test paths, functions/requirements, and hazard test cases. The weighted average is used as an indicator of software reliability. <p> Branch coverage may be an adequate measure in many cases, since about 80% branch coverage often produces acceptable results [11]. However for testing of individual modules or for highly reliable software p-use may be better. Researchers suggests the use of a weighted risk measure <ref> [2, 24, 21] </ref>. Weights are chosen on the basis of relative significance of each measure. As we find, structural coverage measures tend to be strongly correlated, and thus a weighted average may not provide more information than a single measure. We need to identify more independent measures.
Reference: [22] <author> S. Ntafos. </author> <title> A comparision of some structural testing strategies. </title> <journal> IEEE Trans. Software Engineering, </journal> <pages> pages 868-874, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: The model given by Equation 9 fits the data well as indicated by the LSE values. The data shows that C 1 &gt; C 2 &gt; C 4 , which is expected. Complete decision coverage implies complete block coverage, and complete p-uses coverage implies complete decision coverage <ref> [3, 9, 22] </ref>. The c-uses coverage has no such relation relative to the other metrics. Table 5 summarizes the result for DS2. Nine faults were revealed by application of 1196 tests; we assume that one fault (i.e. 10%) is still undetected.
Reference: [23] <author> P. Piwowarshki, M. Ohba, and J. Caruso. </author> <title> Coverage measurement experience during function test. </title> <booktitle> In International Conference on Software Engineering, </booktitle> <pages> pages 287-300, </pages> <address> Baltimore, Maryland, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Their model excludes test cases that do not increase coverage. The adjusted test effort 2 data is used to fit existing time-based models to avoid the overestimation of traditional time-based SRMs due to the saturation effect of testing strategies. Assuming random testing, Piwowarski, Ohba and Caruso <ref> [23] </ref> analyze block coverage growth during function test, and derive an exponential model relating the number of tests to block coverage. Their model is equivalent to the Goel-Okumoto model [25]. They also derive an exponential model relating the covering frequency to the error removal ratio.
Reference: [24] <author> R. Poston. </author> <title> The power of simple software testing metrics. </title> <booktitle> In Int'l Conf. on Software Engineering, </booktitle> <year> 1993. </year>
Reference-contexts: The model assumes that full coverage of all four metrics implies that the software tested is 100% reliable. In reality, such software may have some remaining faults. A similar approach, but with different coverage metrics, was taken to provide a test quality report <ref> [24] </ref>. In this paper, we explore the connection between test coverage and reliability. We develop a model that relates test coverage to defect coverage. With this model we can estimate the defect density. <p> Branch coverage may be an adequate measure in many cases, since about 80% branch coverage often produces acceptable results [11]. However for testing of individual modules or for highly reliable software p-use may be better. Researchers suggests the use of a weighted risk measure <ref> [2, 24, 21] </ref>. Weights are chosen on the basis of relative significance of each measure. As we find, structural coverage measures tend to be strongly correlated, and thus a weighted average may not provide more information than a single measure. We need to identify more independent measures.
Reference: [25] <author> J. Ramsey and V. Basili. </author> <title> Analyzing the test process using structural coverage. </title> <booktitle> In Int'l Conf. on Software Engineering, </booktitle> <pages> pages 306-312, </pages> <year> 1985. </year>
Reference-contexts: With the same test effort (measured in CPU execution time or calendar time), a less effective test strategy may be less efficient in finding defects. Measuring test-coverage is usually an intrusive approach, however the available tools now allow it to be done automatically. Ramsey and Basili <ref> [25] </ref> investigate different permutations of the same test set and collected data relating the number of tests to statement coverage growth. They try a variety of models to fit the data. The best fit is obtained using the Goel and Okumoto's exponential model. <p> Assuming random testing, Piwowarski, Ohba and Caruso [23] analyze block coverage growth during function test, and derive an exponential model relating the number of tests to block coverage. Their model is equivalent to the Goel-Okumoto model <ref> [25] </ref>. They also derive an exponential model relating the covering frequency to the error removal ratio. The model relies on prior knowledge of the error distribution over different functional groups in a product. Frankl and Weiss [8] compare the fault exposing capability of branch coverage and data flow coverage criteria.
Reference: [26] <author> S. Rapps and E. Weyuker. </author> <title> Selecting software test data using data flow information. </title> <journal> IEEE Trans. Software Engineering, </journal> <volume> SE-11(4):367-375, </volume> <month> Apr. </month> <year> 1985. </year>
Reference-contexts: A c-use path is a path through a program from each point where the value of a variable is modified to each c-use (without the variable being modified along the path) <ref> [26] </ref>. * P-use coverage: the fraction of the total number of p-uses that have been covered by one p-use path during testing. <p> A p-use path is a path from each point where the value of a variable is modified to each p-use, a use in a predicate or decision (without modifications to the variable along the path) <ref> [26] </ref>. Test coverage can also be measured in other units, such as context, required k-tuple, etc. A formal evaluation of different test coverage criteria was presented by Clarke [6]. When such a unit is exercised, it is possible that one or more associated faults may be detected.
Reference: [27] <author> S. Seth, V. Agrawal, and H. Farhat. </author> <title> A statistical theory of digital circuit testability. </title> <journal> IEEE Trans. on Computer, </journal> <pages> pages 582-586, </pages> <month> Apr. </month> <year> 1990. </year>
Reference-contexts: The detectability profiles were introduced by Malaiya and Yang [19]. They have been used to characterize testing of hardware [31] and software [18]. A continuous version of the detectability profile was defined by Seth, Agrawal and Farhat <ref> [27] </ref>. For convenience, we use the normalized detectability profile (NDP) as defined below. <p> Malaiya and Yang [19], and Wagner et al [31] show that the expected coverage of the enumerables of type j is C j (n) = 1 i=1 j j provided testing is random. The same result holds for continuous NDP <ref> [27] </ref> C j (n) = 1 0 Actual testing of software is more likely to be pseudo-random, since a test once applied will not be repeated. In such cases, random testing is an approximation. This approximation is reasonable, except when coverage approaches 100%. Equations 3 and 4 give expected coverage. <p> Obtaining exact detectability profiles requires a lot of computation since the detectability of each defect needs to be computed. Discrete detectability profiles have been calculated for several small and large combinational circuits. Continuous detectability profiles for some benchmark circuits have been estimated <ref> [27] </ref>. Similar data for some software systems is also available [1]. Fortunately, it is possible to obtain an approximation for the detectability profiles.
Reference: [28] <author> M. Trachtenberg. </author> <title> Why failure rates observe Zipf's law in operational software. </title> <journal> IEEE Trans. Reliability, </journal> <pages> pages 386-389, </pages> <month> Sept. </month> <year> 1992. </year>
Reference-contexts: A detectability value of 0 is possible, since a branch might be infeasible, or a defect might not 5 be testable due to redundancy in implementation. Researchers have compiled detectability profiles of several digital circuits [19, 31] and software systems <ref> [28] </ref>. If the number of enumerables is large, a continuous function can approximate the discrete NDP. <p> During much of the testing, the shape of the profile will appear like the bottom curve in Figure 2, regardless of the initial profile. Trachtecnberg's justification of the Zipf's law supports a similar shape <ref> [28] </ref>. Available results for hardware components suggest that initial detectability profiles may be of the form p j (x) = (mj + 1)(1 x) mj (5) where mj is a parameter. The factors (mj + 1) ensure that the area under the initial profile curve is unity.
Reference: [29] <author> J. Voas and K. Miller. </author> <title> Improving the software development process using testability research. </title> <booktitle> In 3rd Int'l Symp. Software Reliability Engineering, </booktitle> <pages> pages 114-121, </pages> <year> 1992. </year>
Reference-contexts: A statement which is reached more easily is more testable. Such statements are likely to get covered (i.e. exercised at lease once) with only a small number of tests. Testability also depends on the likelihood that a fault that is reached actually causes a failure <ref> [29] </ref>. A statement that is executed only in rare situations has low testability. It may not get exercised by most of the tests that are normally applied. As testing progresses, the distribution of testability values will shift. The easy-to-test enumerables are covered early during testing, and are removed from consideration.
Reference: [30] <author> M. Vouk. </author> <title> Using reliability models during testing with non-operational profiles. </title> <booktitle> In 2nd Bell-core/Purdue Workshop Issues in Software Reliability Estimation, </booktitle> <pages> pages 103-111, </pages> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: They give a scatter plot of the number of faults detected during system testing versus the block coverage achieved during unit testing. The plot shows that modules covered more thoroughly during unit testing are much less likely to contain errors. Vouk <ref> [30] </ref> finds that the relation between structural coverage and fault coverage is a variant of the Rayleigh distribution. He assumes that the fault detection rate during testing is proportional to the number of faults present in the software and test coverage values including block, branch, data-flow, and functional group coverage. <p> The data used is for integration and acceptance test phases, where 66 defects were found. One additional defect was found during operational testing. The next three data sets, DS2, DS3, and DS4 are from a NASA supported project implementing sensor management in inertial navigation system <ref> [30] </ref>. The last data set DS5 is from an evolving software system containing a large number of modules. As an example, the data set DS3 is reproduced in Table 3.
Reference: [31] <author> K. Wagnor, C. Chin, and E. McCluskey. </author> <title> Pseudorandom testing. </title> <journal> IEEE Trans. Computer, </journal> <pages> pages 332-343, </pages> <month> Mar. </month> <year> 1987. </year>
Reference-contexts: The detectability profile is the distribution of detectability values in the system under test. The detectability profiles were introduced by Malaiya and Yang [19]. They have been used to characterize testing of hardware <ref> [31] </ref> and software [18]. A continuous version of the detectability profile was defined by Seth, Agrawal and Farhat [27]. For convenience, we use the normalized detectability profile (NDP) as defined below. <p> P 1 j di = 1 since all fractions added will be unity. A detectability value of 0 is possible, since a branch might be infeasible, or a defect might not 5 be testable due to redundancy in implementation. Researchers have compiled detectability profiles of several digital circuits <ref> [19, 31] </ref> and software systems [28]. If the number of enumerables is large, a continuous function can approximate the discrete NDP. <p> Hence, it can be used to calculate expected coverage when a given number of tests have been applied. Here, we assume that testing is random, i.e. any single test is selected randomly with replacement. Malaiya and Yang [19], and Wagner et al <ref> [31] </ref> show that the expected coverage of the enumerables of type j is C j (n) = 1 i=1 j j provided testing is random.
Reference: [32] <author> E. Weyuker. </author> <title> More experience with data flow testing. </title> <journal> IEEE Trans. Software Engineering, </journal> <pages> pages 912-919, </pages> <month> Sept. </month> <year> 1993. </year> <month> 18 </month>
Reference-contexts: A small number of enumerables may not be reachable in practice. We assume that the fraction of such enumerables is negligible. 4 Table 1: The complexity (test length) for achieving different coverage criteria <ref> [32] </ref> Coverage Criterion Upper bound All-Blocks d + 1 All-Branches d + 1 All-P-Uses 1 4 (d 2 + 4d + 3) All-Defs m + (i fi n) All-P-Uses/Some-C-Uses 1 4 (d 2 + 4d + 3) All-C-Uses/Some-P-Uses 1 4 (d 2 + 4d + 3) All-Uses 1 4 (d 2 <p> Weyuker. If there is a directed path from criteria A to criteria B, then test sets that meet criteria A (achievement of complete coverage) are guaranteed to satisfy criteria B. Table 1 shows the maximum number of tests <ref> [32] </ref> to satisfy these criteria.
References-found: 32

