URL: http://www.iro.umontreal.ca/~lisa/pointeurs/iohmms.ps
Refering-URL: http://www.iro.umontreal.ca/labs/neuro/bib/journals/journals.html
Root-URL: http://www.iro.umontreal.ca
Title: Input/Output HMMs for Sequence Processing  
Author: Yoshua Bengio Paolo Frasconi 
Date: September 4, 1995  
Address: Montreal, Qc H3C-3J7 50139 Firenze (Italy)  
Affiliation: Dept. Informatique et Dipartimento di Sistemi Recherche Operationnelle e Informatica Universite de Montreal Universita di Firenze  
Abstract: We consider problems of sequence processing and propose a solution based on a discrete state model in order to represent past context. We introduce a recurrent connectionist architecture having a modular structure that associates a subnetwork to each state. The model has a statistical interpretation we call Input/Output Hidden Markov Model (IOHMM). It can be trained by the EM or GEM algorithms, considering state trajectories as missing data, which decouples temporal credit assignment and actual parameter estimation. The model presents similarities to hidden Markov models (HMMs), but allows us to map input sequences to output sequences, using the same processing style as recurrent neural networks. IOHMMs are trained using a more discriminant learning paradigm than HMMs, while potentially taking advantage of the EM algorithm. We demonstrate that IOHMMs are well suited for solving grammatical inference problems on a benchmark problem. Experimental results are presented for the seven Tomita grammars, showing that these adaptive models can attain excellent generalization. fl also, AT&T Bell Laboratories, Holmdel, NJ
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Waibel, T. Hanazawa, G. Hinton, K. Shikano, and K. Lang, </author> <title> "Phoneme recognition using time-delay neural networks," </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> vol. 37, </volume> <pages> pp. 328-339, </pages> <year> 1989. </year>
Reference-contexts: Feedforward neural networks are inadequate in many of these cases because of the absence of a memory mechanism that can retain past information in a flexible way. Even if these models include delays in their connections <ref> [1] </ref>, the duration of the temporal contingencies that can be captured is fixed a priori by the architecture rather than being inferred from data. Furthermore, for some tasks, the appropriate size of the input window (or delays) varies during the sequence or from sequence to sequence. <p> Each state network was composed of a single layer of n neurons with a softmax function at their outputs. Input symbols were encoded by two-dimensional index vectors (i.e., u t = <ref> [1; 0] </ref> 0 for the symbol 0 and u t = [0; 1] 0 for the symbol 1). The total number of free parameters is thus 2n 2 + n. In the experiments we measured convergence and generalization performance using different sizes for the recurrent architecture. <p> Each state network was composed of a single layer of n neurons with a softmax function at their outputs. Input symbols were encoded by two-dimensional index vectors (i.e., u t = [1; 0] 0 for the symbol 0 and u t = <ref> [0; 1] </ref> 0 for the symbol 1). The total number of free parameters is thus 2n 2 + n. In the experiments we measured convergence and generalization performance using different sizes for the recurrent architecture. For each setting we ran 20 trials with different seeds for the initial weights.
Reference: [2] <author> D. Seidl and D. Lorenz, </author> <title> "A structure by which a recurrent neural network can approximate a nonlinear dynamic 40 system," </title> <booktitle> in Proceedings of the International Joint Conference on Neural Networks, </booktitle> <volume> vol. 2, </volume> <pages> pp. 709-714, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Furthermore, for some tasks, the appropriate size of the input window (or delays) varies during the sequence or from sequence to sequence. Recurrent neural networks, on the other hand, allow one to model arbitrary dynamical systems <ref> [2, 3] </ref> and can store and retrieve contextual information in a very flexible way, i.e., for durations that are not fixed a priori and that can vary from one sequence to another.
Reference: [3] <author> E. Sontag, </author> <title> "Systems combining linearity and saturations and relations to neural networks," </title> <type> Tech. Rep. </type> <institution> SYCON-92-01, Rutgers Center for Systems and Control, </institution> <year> 1992. </year>
Reference-contexts: Furthermore, for some tasks, the appropriate size of the input window (or delays) varies during the sequence or from sequence to sequence. Recurrent neural networks, on the other hand, allow one to model arbitrary dynamical systems <ref> [2, 3] </ref> and can store and retrieve contextual information in a very flexible way, i.e., for durations that are not fixed a priori and that can vary from one sequence to another.
Reference: [4] <author> D. Rumelhart, G. Hinton, and R. Williams, </author> <title> "Learning internal representations by error propagation," in Parallel Distributed Processing (D. </title> <editor> Rumelhart and J. McClelland, eds.), </editor> <volume> vol. 1, ch. 8, </volume> <pages> pp. 318-362, </pages> <address> Cambridge: </address> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: Up to now, research efforts on supervised learning for recurrent networks have been almost exclusively focused on gradient descent methods and a continuous state-space. Numerous algorithms are available for computing the gradient. For example, the back-propagation through time (BPTT) algorithm <ref> [4, 5] </ref> is a straightforward generalization of back-propagation that allows one to compute the complete gradient in fully recurrent networks. The real time recurrent learning (RTRL) algorithm [6, 7, 8] is local in time and produces a partial gradient after each time step, thus allowing on-line weights updating.
Reference: [5] <author> B. Pearlmutter, </author> <title> "Learning state space trajectories in recurrent neural networks," </title> <journal> Neural Computation, </journal> <volume> vol. 1, </volume> <pages> pp. 263-269, </pages> <year> 1989. </year>
Reference-contexts: Up to now, research efforts on supervised learning for recurrent networks have been almost exclusively focused on gradient descent methods and a continuous state-space. Numerous algorithms are available for computing the gradient. For example, the back-propagation through time (BPTT) algorithm <ref> [4, 5] </ref> is a straightforward generalization of back-propagation that allows one to compute the complete gradient in fully recurrent networks. The real time recurrent learning (RTRL) algorithm [6, 7, 8] is local in time and produces a partial gradient after each time step, thus allowing on-line weights updating.
Reference: [6] <author> G. Kuhn, </author> <title> "A first look at phonetic discrimination using connectionist models with recurrent links." </title> <institution> CCRP - IDA SCIMP working paper No.4/87, Institute for Defense Analysis, Princeton, NJ, </institution> <year> 1987. </year>
Reference-contexts: Numerous algorithms are available for computing the gradient. For example, the back-propagation through time (BPTT) algorithm [4, 5] is a straightforward generalization of back-propagation that allows one to compute the complete gradient in fully recurrent networks. The real time recurrent learning (RTRL) algorithm <ref> [6, 7, 8] </ref> is local in time and produces a partial gradient after each time step, thus allowing on-line weights updating. Another algorithm was proposed for training local feedback recurrent networks [9, 10].
Reference: [7] <author> A. Robinson and F. Fallside, </author> <title> "Static and dynamic error propagation networks with application to speech coding," </title> <booktitle> in Neural Information Processing Systems (D. </booktitle> <editor> Anderson, ed.), </editor> <address> (Denver, </address> <publisher> CO), </publisher> <pages> pp. 632-641, </pages> <institution> American Institute of Physics, </institution> <address> New York, </address> <year> 1988. </year>
Reference-contexts: Numerous algorithms are available for computing the gradient. For example, the back-propagation through time (BPTT) algorithm [4, 5] is a straightforward generalization of back-propagation that allows one to compute the complete gradient in fully recurrent networks. The real time recurrent learning (RTRL) algorithm <ref> [6, 7, 8] </ref> is local in time and produces a partial gradient after each time step, thus allowing on-line weights updating. Another algorithm was proposed for training local feedback recurrent networks [9, 10].
Reference: [8] <author> R. Williams and D. Zipser, </author> <title> "A learning algorithm for continually running fully recurrent neural networks," </title> <journal> Neural Computation, </journal> <volume> vol. 1, </volume> <pages> pp. 270-280, </pages> <year> 1989. </year>
Reference-contexts: Numerous algorithms are available for computing the gradient. For example, the back-propagation through time (BPTT) algorithm [4, 5] is a straightforward generalization of back-propagation that allows one to compute the complete gradient in fully recurrent networks. The real time recurrent learning (RTRL) algorithm <ref> [6, 7, 8] </ref> is local in time and produces a partial gradient after each time step, thus allowing on-line weights updating. Another algorithm was proposed for training local feedback recurrent networks [9, 10].
Reference: [9] <author> M. Gori, Y. Bengio, and R. De Mori, </author> <title> "BPS: A learning algorithm for capturing the dynamical nature of speech," </title> <booktitle> in Proceedings of the International Joint Conference on Neural Networks, </booktitle> <address> (Washington D.C.), </address> <pages> pp. 643-644, </pages> <publisher> IEEE, </publisher> <address> New York, </address> <year> 1989. </year>
Reference-contexts: The real time recurrent learning (RTRL) algorithm [6, 7, 8] is local in time and produces a partial gradient after each time step, thus allowing on-line weights updating. Another algorithm was proposed for training local feedback recurrent networks <ref> [9, 10] </ref>. It is also local in time, but requires computation only proportional to the number of weights, like back-propagation through time. Local feedback recurrent networks are suitable for implementing short-term memories but they have limited representational power for dealing with general sequences [11, 12].
Reference: [10] <author> M. Mozer, </author> <title> "A focused back-propagation algorithm for temporal pattern recognition," </title> <journal> Complex Systems, </journal> <volume> vol. 3, </volume> <pages> pp. 349-381, </pages> <year> 1989. </year>
Reference-contexts: The real time recurrent learning (RTRL) algorithm [6, 7, 8] is local in time and produces a partial gradient after each time step, thus allowing on-line weights updating. Another algorithm was proposed for training local feedback recurrent networks <ref> [9, 10] </ref>. It is also local in time, but requires computation only proportional to the number of weights, like back-propagation through time. Local feedback recurrent networks are suitable for implementing short-term memories but they have limited representational power for dealing with general sequences [11, 12].
Reference: [11] <author> P. Frasconi, M. Gori, and G. </author> <title> Soda, "Local feedback multi-layered networks," </title> <journal> Neural Computation, </journal> <volume> vol. 4, no. 1, </volume> <pages> pp. 120-130, </pages> <year> 1992. </year>
Reference-contexts: It is also local in time, but requires computation only proportional to the number of weights, like back-propagation through time. Local feedback recurrent networks are suitable for implementing short-term memories but they have limited representational power for dealing with general sequences <ref> [11, 12] </ref>. However, practical difficulties have been reported in training recurrent neural networks to perform tasks 2 in which the temporal contingencies present in the input/output sequences span long intervals [13, 14, 15].
Reference: [12] <author> A. Tsoi and A. </author> <title> Back, "Locally recurrent globally feedforward networks, a critical review of architectures," </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 5, no. 2, </volume> <pages> pp. 229|239, </pages> <year> 1994. </year>
Reference-contexts: It is also local in time, but requires computation only proportional to the number of weights, like back-propagation through time. Local feedback recurrent networks are suitable for implementing short-term memories but they have limited representational power for dealing with general sequences <ref> [11, 12] </ref>. However, practical difficulties have been reported in training recurrent neural networks to perform tasks 2 in which the temporal contingencies present in the input/output sequences span long intervals [13, 14, 15].
Reference: [13] <author> Y. Bengio, P. Simard, and P. Frasconi, </author> <title> "Learning long-term dependencies with gradient descent is difficult," </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 5, no. 2, </volume> <pages> pp. 157-166, </pages> <year> 1994. </year> <month> 41 </month>
Reference-contexts: However, practical difficulties have been reported in training recurrent neural networks to perform tasks 2 in which the temporal contingencies present in the input/output sequences span long intervals <ref> [13, 14, 15] </ref>. In fact, it can be proved that any parametric dynamical system with a non-linear recurrence (such as a recurrent neural network) will be increasingly difficult to train with gradient descent as the duration of the dependencies to be captured increases [13]. <p> In fact, it can be proved that any parametric dynamical system with a non-linear recurrence (such as a recurrent neural network) will be increasingly difficult to train with gradient descent as the duration of the dependencies to be captured increases <ref> [13] </ref>. This is a problem with the gradient of the error function and thus it persists regardless of what gradient computation algorithm (such as RTRL or BPTT) is employed. A common heuristic solution is to start training on shorter sequences, and then incrementally train on longer sequences. <p> A common heuristic solution is to start training on shorter sequences, and then incrementally train on longer sequences. In general, however, the rules needed to deal with long term dependencies might not be present in short sequences. Previous work on alternative training algorithms <ref> [16, 13] </ref> suggests that the root of the problem lies in the essentially discrete nature of the process of storing contextual information for an indefinite amount of time. A potential solution to this problem is to propagate, backward in time, targets in state space, rather than differential error information. <p> Instead, the recurrence loop of IOHMMs is purely linear. It has been shown that such a non-linearity in the loop makes very difficult the learning of long-term context <ref> [13] </ref> (see next section for a discussion of learning long-term dependencies). Learning in the MC architecture uses approximated gradient ascent to optimize the likelihood, in contrast to the EM supervised learning algorithm proposed by Jordan & Jacobs (1994) for the HME. <p> In the case of recurrent networks trained by gradient descent, credit assignment through time is represented by a sequence of gradients of the error function with respect to the state of the sigmoidal units. However, many researchers have found this procedure ineffective for assigning credit over long temporal spawns <ref> [13, 14, 15] </ref>. In the case of IOHMMs trained by the EM algorithm, credit assignment through time is represented by the sequences of posterior (i.e., after having observed the data) probabilities P (x t =i j u t 1 ). <p> In the following we summarize the main results on the problem of learning long-term dependencies with Markovian models, which include IOHMMs and HMMs. A formal analysis of this problem can be found in [44]. 5.1 Temporal Credit Assignment In previous work <ref> [13] </ref> we found theoretical reasons for the difficulty in training parametric non-linear dynamical systems to capture long-term dependencies. For such systems, the dynamical evolution is controlled by a non-linear iterated map a t = M (a t1 ; u t ), with a t a continuous state vector. <p> These two simple benchmarks were used in <ref> [13] </ref> to compare the long-term learning capabilities of recurrent networks trained by back-propagation and five other alternative algorithms. <p> In the tables, "p-n" stands for pseudo-Newton [53]. Time-weighted pseudo-newton is a variation in which derivatives with respect to the instantiation of a parameter at a particular time step are weighted by the inverse of the corresponding second derivatives <ref> [13] </ref>. Multigrid is similar to simulated annealing with constant temperature 0. The discrete error propagation algorithm [13] attempts to propagate backwards discrete error information in a recurrent network with discrete units. <p> Time-weighted pseudo-newton is a variation in which derivatives with respect to the instantiation of a parameter at a particular time step are weighted by the inverse of the corresponding second derivatives <ref> [13] </ref>. Multigrid is similar to simulated annealing with constant temperature 0. The discrete error propagation algorithm [13] attempts to propagate backwards discrete error information in a recurrent network with discrete units. Each column of the tables corresponds to a value of the maximum sequence length T for a given set of trials. <p> On two test problems, in which the span of the temporal dependencies can be controlled, we have found that IOHMMs learn long-term dependencies more effectively than back-propagation and other alternative algorithms described in <ref> [13, 18] </ref>.
Reference: [14] <author> M. C. Mozer, </author> <title> "The induction of multiscale temporal structure," </title> <booktitle> in Advances in Neural Information Processing Systems 4 (J. </booktitle> <editor> Moody, S. Hanson, and R. Lipmann, eds.), </editor> <address> (San Mateo, CA), </address> <pages> pp. 275-282, </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: However, practical difficulties have been reported in training recurrent neural networks to perform tasks 2 in which the temporal contingencies present in the input/output sequences span long intervals <ref> [13, 14, 15] </ref>. In fact, it can be proved that any parametric dynamical system with a non-linear recurrence (such as a recurrent neural network) will be increasingly difficult to train with gradient descent as the duration of the dependencies to be captured increases [13]. <p> In the case of recurrent networks trained by gradient descent, credit assignment through time is represented by a sequence of gradients of the error function with respect to the state of the sigmoidal units. However, many researchers have found this procedure ineffective for assigning credit over long temporal spawns <ref> [13, 14, 15] </ref>. In the case of IOHMMs trained by the EM algorithm, credit assignment through time is represented by the sequences of posterior (i.e., after having observed the data) probabilities P (x t =i j u t 1 ).
Reference: [15] <author> R. Rohwer, </author> <title> "The time dimension of neural network models," </title> <journal> ACM Sigart Bulleting, </journal> <volume> vol. 5, </volume> <pages> pp. 36-44, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: However, practical difficulties have been reported in training recurrent neural networks to perform tasks 2 in which the temporal contingencies present in the input/output sequences span long intervals <ref> [13, 14, 15] </ref>. In fact, it can be proved that any parametric dynamical system with a non-linear recurrence (such as a recurrent neural network) will be increasingly difficult to train with gradient descent as the duration of the dependencies to be captured increases [13]. <p> In the case of recurrent networks trained by gradient descent, credit assignment through time is represented by a sequence of gradients of the error function with respect to the state of the sigmoidal units. However, many researchers have found this procedure ineffective for assigning credit over long temporal spawns <ref> [13, 14, 15] </ref>. In the case of IOHMMs trained by the EM algorithm, credit assignment through time is represented by the sequences of posterior (i.e., after having observed the data) probabilities P (x t =i j u t 1 ).
Reference: [16] <author> Y. Bengio, P. Frasconi, and P. Simard, </author> <title> "The problem of learning long-term dependencies in recurrent networks," </title> <booktitle> in IEEE International Conference on Neural Networks, </booktitle> <address> (San Francisco), </address> <pages> pp. 1183-1195, </pages> <publisher> IEEE Press, </publisher> <year> 1993. </year> <type> (invited paper). </type>
Reference-contexts: A common heuristic solution is to start training on shorter sequences, and then incrementally train on longer sequences. In general, however, the rules needed to deal with long term dependencies might not be present in short sequences. Previous work on alternative training algorithms <ref> [16, 13] </ref> suggests that the root of the problem lies in the essentially discrete nature of the process of storing contextual information for an indefinite amount of time. A potential solution to this problem is to propagate, backward in time, targets in state space, rather than differential error information.
Reference: [17] <author> R. Rohwer, </author> <title> "The "moving targets" training algorithm," </title> <booktitle> in Advances in Neural Information Processing Systems 2 (D. </booktitle> <editor> Touretzky, ed.), </editor> <address> (Denver, </address> <publisher> CO), </publisher> <pages> pp. 558-565, </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, </address> <year> 1990. </year>
Reference-contexts: If each iteration is guaranteed to produce some improvements in the approximation of the "true" targets, then the process may converge to some useful solution with regard to the output targets specified by supervision. One of the first related approaches is probably the moving target algorithm by Rohwer <ref> [17] </ref>. The moving target approach consists in formulating supervised learning as an optimization problem in the joint space of temporal targets and adjustable parameters (connection weights). Rohwer proposed a solution based on gradient descent and demonstrated experimentally that some difficult credit assignment tasks could be solved.
Reference: [18] <author> Y. Bengio and P. Frasconi, </author> <title> "Credit assignment through time: Alternatives to backpropagation," </title> <booktitle> in Advances in Neural Information Processing Systems 6 (J. </booktitle> <editor> Cowan, G. Tesauro, and J. Alspector, eds.), </editor> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: Rohwer proposed a solution based on gradient descent and demonstrated experimentally that some difficult credit assignment tasks could be solved. However, for more difficult problems, the method got stuck very often in local minima and no useful solution could be 3 obtained. Extending previous work <ref> [18] </ref>, in this paper we propose a statistical approach to target propagation, based on the EM algorithm. We consider a parametric dynamical system having n discrete states and we introduce a modular architecture, with subnetworks associated to discrete states. <p> Experiments on artificial tasks <ref> [18] </ref> have shown that a simplified version of the approach presented here can deal with long-term dependencies more effectively than recurrent networks trained with back-propagation through time or other alternative algorithms. The model used in [18] has very limited representational capabilities and can only map an input sequence to a final <p> Experiments on artificial tasks <ref> [18] </ref> have shown that a simplified version of the approach presented here can deal with long-term dependencies more effectively than recurrent networks trained with back-propagation through time or other alternative algorithms. The model used in [18] has very limited representational capabilities and can only map an input sequence to a final discrete state. In the present paper we describe an extended architecture that allows one to fully specify both the input and output portions of data, as required by the supervised learning paradigm. <p> For example, the fraction of converged trials for grammars 3, 4, and 5 is small and the difficulty of discovering the optimal solution might become a serious restriction for tasks involving a large number of states. In other experiments <ref> [18] </ref> we noticed that restricting the connectivity of the transition graph can significantly help to remove problems of convergence. Of course, this approach can be effectively exploited only if some prior knowledge about the state space is available. <p> On two test problems, in which the span of the temporal dependencies can be controlled, we have found that IOHMMs learn long-term dependencies more effectively than back-propagation and other alternative algorithms described in <ref> [13, 18] </ref>.
Reference: [19] <author> A. P. Dempster, N. M. Laird, and D. B. Rubin, </author> <title> "Maximum-likelihood from incomplete data via the EM algorithm," </title> <journal> Journal of Royal Statistical Society B, </journal> <volume> vol. 39, </volume> <pages> pp. 1-38, </pages> <year> 1977. </year>
Reference-contexts: We consider a parametric dynamical system having n discrete states and we introduce a modular architecture, with subnetworks associated to discrete states. The architecture can be interpreted as a statistical model and can be trained by the EM or generalized EM (GEM) algorithms of Dempster, Laird, & Rubin <ref> [19] </ref>, by considering the internal state trajectories as missing data. In this way learning is factored into a temporal credit assignment subproblem and a static learning subproblem that consists in fitting parameters to the next-state and output mappings defined by the estimated trajectories. <p> Let us first briefly describe the EM algorithm. 3.1 The EM Algorithm EM (estimation-maximization) is an iterative approach to maximum likelihood estimation (MLE), originally proposed in <ref> [19] </ref>. Each iteration is composed of two steps: an estimation (E) step and a maximization 2 In the following, in order to simplify the notation, the sequence index p may be omitted. 12 (M) step. <p> (fi (k) ) where M () is such that Q (M (fi (k) ); fi (k) ) Q (fi (k) ; fi (k) ); The following theorem guarantees the convergence of EM and GEM algorithms to a (possibly local) maximum of the (incomplete data) likelihood: Theorem 1 (Dempster et al. <ref> [19] </ref>) For each GEM algorithm L (M (fi); D) L (fi; D) (15) where the equality holds if and only if Q (M (fi); fi) = Q (fi; fi): (16) 3.2 EM for Training IOHMMs In order to apply EM to IOHMMs we begin by noting that the variable X ,
Reference: [20] <author> L. E. Baum, T. Petrie, G. Soules, and N. Weiss, </author> <title> "A maximization technique occuring in the statistical analysis of probabilistic functions of Markov chains," </title> <journal> Ann. Math. Statistic., </journal> <volume> vol. 41, </volume> <pages> pp. 164-171, </pages> <year> 1970. </year>
Reference-contexts: In order to iteratively tune parameters with the EM or GEM algorithms, the system propagates forward and backward a discrete distribution over the n states, resulting in a procedure similar to the Baum-Welsh algorithm used to train standard hidden Markov models (HMMs) <ref> [20, 21, 22] </ref>. <p> Intuitively, the parameters are updated as if the estimation step of EM had provided soft targets for the outputs of the 2n subnetworks, for each time t. 4 Comparisons 4.1 Standard Hidden Markov Models The model proposed here is a natural extension of HMMs <ref> [20, 21, 22] </ref>: the distribution of the output sequence is conditioned on an input sequence. Furthermore, we propose to parameterize the next-state and output distributions with complex modules such as artificial neural networks. The most typical applications of standard HMMs are in automatic speech recognition [21, 37].
Reference: [21] <author> S. Levinson, L. Rabiner, and M. Sondhi, </author> <title> "An introduction to the application of the theory of probabilistic functions of a Markov process to automatic speech recognition," </title> <journal> Bell System Technical Journal, </journal> <volume> vol. 64, no. 4, </volume> <pages> pp. 1035-1074, </pages> <year> 1983. </year>
Reference-contexts: In order to iteratively tune parameters with the EM or GEM algorithms, the system propagates forward and backward a discrete distribution over the n states, resulting in a procedure similar to the Baum-Welsh algorithm used to train standard hidden Markov models (HMMs) <ref> [20, 21, 22] </ref>. <p> Intuitively, the parameters are updated as if the estimation step of EM had provided soft targets for the outputs of the 2n subnetworks, for each time t. 4 Comparisons 4.1 Standard Hidden Markov Models The model proposed here is a natural extension of HMMs <ref> [20, 21, 22] </ref>: the distribution of the output sequence is conditioned on an input sequence. Furthermore, we propose to parameterize the next-state and output distributions with complex modules such as artificial neural networks. The most typical applications of standard HMMs are in automatic speech recognition [21, 37]. <p> Furthermore, we propose to parameterize the next-state and output distributions with complex modules such as artificial neural networks. The most typical applications of standard HMMs are in automatic speech recognition <ref> [21, 37] </ref>. In these cases each lexical unit is associated to one model M i . During recognition one computes for each model the probability P (y T 1 j M i ) of having generated the observed acoustic sequence y T 1 .
Reference: [22] <author> A. Poritz, </author> <title> "Hidden Markov models: a guided tour," </title> <booktitle> in Proc. Int. Conf. Acoustics, Speech, and Signal Processing, </booktitle> <pages> pp. 7-13, </pages> <year> 1988. </year>
Reference-contexts: In order to iteratively tune parameters with the EM or GEM algorithms, the system propagates forward and backward a discrete distribution over the n states, resulting in a procedure similar to the Baum-Welsh algorithm used to train standard hidden Markov models (HMMs) <ref> [20, 21, 22] </ref>. <p> Intuitively, the parameters are updated as if the estimation step of EM had provided soft targets for the outputs of the 2n subnetworks, for each time t. 4 Comparisons 4.1 Standard Hidden Markov Models The model proposed here is a natural extension of HMMs <ref> [20, 21, 22] </ref>: the distribution of the output sequence is conditioned on an input sequence. Furthermore, we propose to parameterize the next-state and output distributions with complex modules such as artificial neural networks. The most typical applications of standard HMMs are in automatic speech recognition [21, 37].
Reference: [23] <author> A. Kehagias, </author> <title> "Stochastic recurrent networks: Prediction and classification of time series," </title> <type> tech. rep., </type> <institution> Brown University. Division of Applied Mathematics, </institution> <address> Providence, RI 02912, </address> <year> 1991. </year>
Reference-contexts: Another connectionist model extending hidden Markov models to process discrete input and output streams was proposed in <ref> [23] </ref>, for modeling the distribution of an output sequence y given an input sequence u. Other interesting related models are the various hybrids of neural networks and HMMs that have been proposed in the literature (such as [29, 30, 31, 32]).
Reference: [24] <author> H. Leprieur and P. Haffner, </author> <title> "Discriminant learning with minimum memory loss for improved non-vocabulary rejection," </title> <booktitle> in EUROSPEECH'95, </booktitle> <address> (Madrid, Spain), </address> <year> 1995. </year>
Reference-contexts: Another advantage of more discriminant training criteria is that they tend to be more robust to incorrectness of the model, and for this reason sometimes perform better <ref> [24, 25] </ref>. Both the input and output sequences can be multivariate, discrete or continuous. Thus IOHMMs can perform sequence regression (y continuous) or classification (y discrete).
Reference: [25] <author> Y. Bengio, Y. LeCun, and D. Henderson, </author> <title> "Globally trained handwritten word recognizer using spatial representation, space displacement neural networks and hidden Markov models," </title> <booktitle> in Advances in Neural Information Processing Systems 6 (J. </booktitle> <editor> Cowan, G. Tesauro, and J. Alspector, </editor> <booktitle> eds.), </booktitle> <pages> pp. 937-944, </pages> <year> 1994. </year> <month> 42 </month>
Reference-contexts: Another advantage of more discriminant training criteria is that they tend to be more robust to incorrectness of the model, and for this reason sometimes perform better <ref> [24, 25] </ref>. Both the input and output sequences can be multivariate, discrete or continuous. Thus IOHMMs can perform sequence regression (y continuous) or classification (y discrete).
Reference: [26] <author> G. J. McLachlan and K. E. Basford, </author> <title> Mixture models: Inference and applications to clustering. </title> <publisher> Marcel Dekker, </publisher> <year> 1988. </year>
Reference-contexts: Therefore, a simple way to obtain an IOHMM from an HMM is to make the output and transition probabilities function of an input u t at each time step t. The output distribution P (y t j u t ) is obtained as a mixture of probabilities <ref> [26] </ref>, in which each component is conditional on a particular discrete state, and the mixing proportions are the current state probabilities, conditional on the input. Hence, the model has also interesting connections to the mixture of experts (ME) architecture by Jacobs, Jordan, Nowlan & Hinton [27].
Reference: [27] <author> R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton, </author> <title> "Adaptive mixture of local experts," </title> <journal> Neural Computation, </journal> <volume> vol. 3, </volume> <pages> pp. 79-87, </pages> <year> 1991. </year>
Reference-contexts: Hence, the model has also interesting connections to the mixture of experts (ME) architecture by Jacobs, Jordan, Nowlan & Hinton <ref> [27] </ref>. <p> A similar interpretation of second order networks, although limited to symbolic inputs, was proposed in [41]. 4.3 Adaptive Mixtures of Experts Adaptive mixtures of experts (ME) <ref> [27] </ref> and hierarchical mixtures of experts (HME) [42] have been introduced as a divide and conquer approach to supervised learning in static connectionist models. <p> It can be seen both as an extension of standard Hidden Markov Models (HMMs), with a conditioning input sequence, and as an extension of the mixture of experts (ME) model <ref> [27] </ref>, with a constrained linear feedback loop and two sets of experts (for predicting the output and for predicting the next state).
Reference: [28] <author> T. W. Cacciatore and S. J. Nowlan, </author> <title> "Mixtures of controllers for jump linear and non-linear plants," </title> <booktitle> in Advances in Neural Information Processing Systems 6 (J. </booktitle> <editor> Cowan, G. Tesauro, and J. Alspector, eds.), </editor> <address> (San Mateo, CA), </address> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: As in the mixture of experts, the task decomposition is smooth. Unlike the related approach of <ref> [28] </ref>, the gating (or switching) between experts is provided by expert modules (one per state i) computing the state transition distribution P (x t j x t1 =i; u t ) (conditioned on the current input). <p> y = P j g j y j where the weights g j are computed as a parametric function 24 of the inputs by a separate subnetwork (gating network) that assigns responsibility to different experts for different regions of the input space. j t Network Gating Recently, Cacciatore & Nowlan <ref> [28] </ref> have proposed a recurrent extension to the ME architecture, called mixture of controllers (MC), in which the gating network has feedback connections, thus taking temporal context into account. The MC architecture is shown in Figure 3.
Reference: [29] <author> H. Bourlard and C. Wellekens, </author> <title> "Links between hidden Markov models and multilayer perceptrons," </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> vol. 12, </volume> <pages> pp. 1167-1178, </pages> <year> 1990. </year>
Reference-contexts: Other interesting related models are the various hybrids of neural networks and HMMs that have been proposed in the literature (such as <ref> [29, 30, 31, 32] </ref>). <p> For example in [31] the observations used by the HMM are generated by a recurrent neural network. Bourlard et al. <ref> [29, 30] </ref> use a feedforward network to estimate state probabilities, conditioned on the acoustic sequence. Instead of deriving an exact EM or GEM algorithm, they apply Viterbi decoding in order to estimate the most likely state trajectory, thereafter used as a target sequence for the feedforward network.
Reference: [30] <author> H. Bourlard and N. Morgan, </author> <title> Connectionist Speech Recognition. A Hybrid Approach, </title> <booktitle> vol. 247 of The Kluwer international series in engineering and computer science. </booktitle> <address> Boston: </address> <publisher> Kluwer Academic Publishers, </publisher> <year> 1993. </year>
Reference-contexts: Other interesting related models are the various hybrids of neural networks and HMMs that have been proposed in the literature (such as <ref> [29, 30, 31, 32] </ref>). <p> The other fundamental difference is in the learning procedure. While interesting for their capabilities of modeling sequential phenomena, a weakness of standard HMMs is their poor discrimination power when trained by maximum likelihood estimation (MLE) <ref> [30] </ref>. Consider, for example, the application of HMMs to speech recognition. In the MLE framework, each lexical unit model (corresponding to a word or a phoneme) is trained to fit the distribution of that particular unit. <p> For example in [31] the observations used by the HMM are generated by a recurrent neural network. Bourlard et al. <ref> [29, 30] </ref> use a feedforward network to estimate state probabilities, conditioned on the acoustic sequence. Instead of deriving an exact EM or GEM algorithm, they apply Viterbi decoding in order to estimate the most likely state trajectory, thereafter used as a target sequence for the feedforward network.
Reference: [31] <author> Y. Bengio, R. De Mori, G. Flammia, and R. Kompe, </author> <title> "Global optimization of a neural network-hidden Markov model hybrid," </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 3, no. 2, </volume> <pages> pp. 252-259, </pages> <year> 1992. </year>
Reference-contexts: Other interesting related models are the various hybrids of neural networks and HMMs that have been proposed in the literature (such as <ref> [29, 30, 31, 32] </ref>). <p> For example in <ref> [31] </ref> the observations used by the HMM are generated by a recurrent neural network. Bourlard et al. [29, 30] use a feedforward network to estimate state probabilities, conditioned on the acoustic sequence.
Reference: [32] <author> E. Levin, </author> <title> "Word recognition using hidden control neural architecture," </title> <booktitle> in International Conference on Acoustics, Speech and Signal Processing, </booktitle> <address> (Albuquerque, NM), </address> <pages> pp. 433-436, </pages> <year> 1990. </year>
Reference-contexts: Other interesting related models are the various hybrids of neural networks and HMMs that have been proposed in the literature (such as <ref> [29, 30, 31, 32] </ref>).
Reference: [33] <author> J. Bridle, </author> <title> "Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition," in Neuro-computing: Algorithms, Architectures, </title> <editor> and Applications (F. Fogelman-Soulie and J. Herault, eds.), </editor> <address> New York: </address> <publisher> Springer-Verlag, </publisher> <year> 1989. </year>
Reference-contexts: To guarantee that the variables ' ij;t are positive and summing to 1, the softmax function <ref> [33] </ref> is used in the last layer: ' ij;t = X e a `j;t where a ij;t are intermediate variables that can be thought of as the activations (e.g, weighted sums) of the 8 output units of subnetwork N j .
Reference: [34] <author> J. Pearl, </author> <title> Probabilistic Reasoning in Intelligent Systems : Networks of Plausible Inference. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: Rather than listing a set of conditional independency assumptions, we prefer to express dependencies using a graphical representation. A dependency model M can be represented by means of a directed acyclic graph (DAG), called Bayesian network of M . A formal definition of Bayesian networks can be found in <ref> [34] </ref>. In practice a Bayesian network is constructed by allocating one node for each variable in S and by creating one edge A ! B for each variable A that is believed to have a direct causal impact on B.
Reference: [35] <author> J. Bridle, </author> <title> "Training stochastic model recognition algorithms as networks can lead to maximum mutual information estimation of parameters," </title> <booktitle> in Advances in Neural Information Processing Systems 2 (D. </booktitle> <editor> Touretzky, </editor> <publisher> ed.), </publisher> <pages> pp. 211-217, </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: During training, a backward linear recursion (equation 27) is performed, that is equivalent to back-propagating through time gradients of the likelihood with respect to the ff's (fi i;t = @L @ff i;t , see also <ref> [35] </ref>). Notice that the sums in equations (24), (27) and (25) can be constrained by the transition graph underlying the model. 17 3.2.2 The Maximization Step Each iteration of the EM algorithm requires to maximize Q (fi; fi (k) ). <p> It has been pointed out that supervised learning in neural networks and discriminant learning criteria like MMI are actually strictly related <ref> [35] </ref>. Unfortunately, MMI training of standard HMMs can only be done with gradient ascent. On the other hand, for IOHMMs, the parameter adjusting procedure is based on MLE and EM can be used.
Reference: [36] <author> P. Baldi and Y. Chauvin, </author> <title> "Smooth on-line learning algorithms for hidden Markov models," </title> <journal> Neural Computation, </journal> <volume> vol. 6, no. 4, </volume> <pages> pp. 307-318, </pages> <year> 1994. </year>
Reference-contexts: However, the parameterization of transition probabilities through layers of neural units makes the learning algorithm smooth and suitable for use in on-line mode (i.e., updating the parameters after each sequence presentation, rather than accumulating parameter change information over the whole training set). This is a desirable property <ref> [36] </ref> and may often help to speed up learning.
Reference: [37] <author> L. R. Rabiner, </author> <title> "A tutorial on hidden Markov models and selected applications in speech recognition," </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> vol. 77, no. 2, </volume> <pages> pp. 257-286, </pages> <year> 1989. </year> <month> 43 </month>
Reference-contexts: Furthermore, we propose to parameterize the next-state and output distributions with complex modules such as artificial neural networks. The most typical applications of standard HMMs are in automatic speech recognition <ref> [21, 37] </ref>. In these cases each lexical unit is associated to one model M i . During recognition one computes for each model the probability P (y T 1 j M i ) of having generated the observed acoustic sequence y T 1 . <p> A similar approach could be used to choose good topologies of the transition graph in Markovian models (e.g., HMMs or IOHMMs). For example, structured left-to-right HMMs have been introduced in speech recognition with a topology that is based on elementary considerations about speech production and the structure of language <ref> [52, 37] </ref>. 28 5.2 Reducing Credit Diffusion by Penalized Likelihood As outlined in [44], the undesired diffusion of temporal credit depends on the fast convergence of the rank of the product of n successive matrices of transition probabilities as n increases.
Reference: [38] <author> P. Brown, </author> <title> The Acoustic-Modeling problem in Automatic Speech Recognition. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Carnegie-Mellon University, </institution> <year> 1987. </year>
Reference-contexts: Each model learns from positive examples only, without being informed by the teacher of what classes it will have to compete with. An approach that has been found useful to improve discrimination in HMMs is based on maximum mutual information (MMI) training <ref> [38] </ref>. When using MMI, the parameters for a given model are adjusted taking into account the likelihoods of all the models and not only the likelihood of the model for the correct class (as with the MLE criterion).
Reference: [39] <author> K. Hornik, M. Stinchcombe, and H. White, </author> <title> "Multilayer feedforward networks are universal approximators," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 2, </volume> <pages> pp. 359-366, </pages> <year> 1989. </year>
Reference-contexts: multilayered state networks with enough hidden units are used, then, because of the 6 In fact, the softmax function is used (eq. 2) and lim fl!1 e fla i P ` e fla ` equals to 1 if i = argmax ` a ` and 0 otherwise. 23 universality results <ref> [39] </ref>, the regions ij can be arbitrarily shaped. When the output units of the state networks are not saturated (i.e. transition probabilities are not exactly 0 or 1), we can obtain a similar interpretation, except that the regions ij have soft boundaries.
Reference: [40] <author> C. L. Giles and C. W. Omlin, </author> <title> "Inserting rules into recurrent neural networks," in Neural Networks for Signal Processing II, </title> <booktitle> Proceedings of the 1992 IEEE workshop (Kung, Fallside, Sorenson, and Kamm, eds.), </booktitle> <pages> pp. 13-22, </pages> <publisher> IEEE Press, </publisher> <year> 1992. </year>
Reference-contexts: Because of the multiplicative links, there are some analogies between our architecture and second order recurrent networks that encode discrete states <ref> [40] </ref>. A second order network with n state units and m inputs evolves according to the equation x t = f @ j=1 1 where W j ; j = 1; : : :n are n by m matrices of weights. <p> An IOHMM that uses one-layered state subnetworks would evolve, instead, with the linear recurrence i t = j=1 Following <ref> [40] </ref>, a second order network can represent discrete states by "one-hot" encoding: x i;t = 1 if the state at time t is i, and x i;t = 0 otherwise.
Reference: [41] <author> G. Z. Sun, H. H. Chen, Y. C. Lee, and C. L. Giles, </author> <title> "Recurrent neural networks, hidden Markov models and stochastic grammars," </title> <booktitle> in Proc. Int. Joint Conference on Neural Networks, vol. I, </booktitle> <address> (San Diego CA), </address> <pages> pp. 729-734, </pages> <year> 1990. </year>
Reference-contexts: A similar interpretation of second order networks, although limited to symbolic inputs, was proposed in <ref> [41] </ref>. 4.3 Adaptive Mixtures of Experts Adaptive mixtures of experts (ME) [27] and hierarchical mixtures of experts (HME) [42] have been introduced as a divide and conquer approach to supervised learning in static connectionist models.
Reference: [42] <author> M. I. Jordan and R. A. Jacobs, </author> <title> "Hierarchies of adaptive experts," </title> <booktitle> in Advances in Neural Information Processing Systems 4 (J. </booktitle> <editor> Moody, S. Hanson, and R. Lipmann, eds.), </editor> <address> (San Mateo, CA), </address> <pages> pp. 985-992, </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: A similar interpretation of second order networks, although limited to symbolic inputs, was proposed in [41]. 4.3 Adaptive Mixtures of Experts Adaptive mixtures of experts (ME) [27] and hierarchical mixtures of experts (HME) <ref> [42] </ref> have been introduced as a divide and conquer approach to supervised learning in static connectionist models. A mixture of experts is composed by a modular set of subnetworks (experts) that compete to gain responsibility in modeling outputs in a given region of input space.
Reference: [43] <author> J. Elman, </author> <title> "Finding structure in time," </title> <journal> Cognitive Science, </journal> <volume> vol. 14, </volume> <pages> pp. 179-211, </pages> <year> 1990. </year>
Reference-contexts: Learning in the MC architecture uses approximated gradient ascent to optimize the likelihood, in contrast to the EM supervised learning algorithm proposed by Jordan & Jacobs (1994) for the HME. The approximation of gradient is based on one step truncated back-propagation through time (somehow similar to Elman's approach <ref> [43] </ref>) and allows online updating for continually running sequences, which is useful for control tasks.
Reference: [44] <author> Y. Bengio and P. Frasconi, </author> <title> "Diffusion of credit in markovian models," </title> <booktitle> in Advances in Neural Information Processing Systems 7 (G. </booktitle> <editor> Tesauro, D. S. Touretzky, and J. Alspector, eds.), </editor> <address> (San Mateo, CA), </address> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: In the following we summarize the main results on the problem of learning long-term dependencies with Markovian models, which include IOHMMs and HMMs. A formal analysis of this problem can be found in <ref> [44] </ref>. 5.1 Temporal Credit Assignment In previous work [13] we found theoretical reasons for the difficulty in training parametric non-linear dynamical systems to capture long-term dependencies. <p> In this case, the norm of the Jacobian of the state transition function is constrained to be exactly one. Like in recurrent networks, learning in non-deterministic Markovian models generally becomes increasingly difficult as the span of the temporal dependencies increases <ref> [44] </ref>. However, a very important qualitative difference is that in Markovian models long-term storing and temporal credit assignment are not necessarily incompatible: they either both occur or are both impractical. They both occur in the very special case of an essentially deterministic model. <p> Such a situation can be found for example in problems of grammar inference in which the input/output data is essentially deterministic (as with the task studied in section 6). An analysis of this problem of credit assignment is presented in <ref> [44] </ref>, in which we study the problem from a theoretical point of view, applying established mathematical results on Markov chains [45] to the problem of learning long term dependencies in homogeneous and non-homogeneous HMMs. <p> On the other hand an IOHMM can perform a large class of interesting computations (such as grammar inference) with this same constraint, because the transition probabilities can vary at each time step depending on the input sequence. The analyses reported in <ref> [44] </ref> also suggest that fully connected transition graphs have the worst behavior from the point of view of temporal credit propagation. The transition graph can be constrained using some prior knowledge on the problem. <p> For example, structured left-to-right HMMs have been introduced in speech recognition with a topology that is based on elementary considerations about speech production and the structure of language [52, 37]. 28 5.2 Reducing Credit Diffusion by Penalized Likelihood As outlined in <ref> [44] </ref>, the undesired diffusion of temporal credit depends on the fast convergence of the rank of the product of n successive matrices of transition probabilities as n increases. The rate of rank lossage can be reduced by controlling the norm of the eigenvalues of the transition matrices t . <p> On two test problems, in which the span of the temporal dependencies can be controlled, we have found that IOHMMs learn long-term dependencies more effectively than back-propagation and other alternative algorithms described in [13, 18]. An analysis of the problem of credit assignment through time in HMMs and IOHMMs <ref> [44] </ref> explains why they could solve this problem better than recurrent networks (which 39 have a non-linearity in the recurrence loop), and revealed that best results would be obtained when transition probabilities are near 0 or 1.
Reference: [45] <author> E. Seneta, </author> <title> Nonnegative Matrices and Markov Chains. </title> <address> New York: </address> <publisher> Springer, </publisher> <year> 1981. </year>
Reference-contexts: An analysis of this problem of credit assignment is presented in [44], in which we study the problem from a theoretical point of view, applying established mathematical results on Markov chains <ref> [45] </ref> to the problem of learning long term dependencies in homogeneous and non-homogeneous HMMs. Although the analysis is the same for both ordinary HMMs and IOHMMs, there is a very important difference in the simplest cure, which is to have transition probabilities near 0 or 1.
Reference: [46] <author> Y. S. Abu-Mostafa, </author> <title> "Learning from hints in neural networks," </title> <journal> Journal of Complexity, </journal> <volume> vol. 6, </volume> <pages> pp. 192-198, </pages> <year> 1990. </year>
Reference-contexts: The transition graph can be constrained using some prior knowledge on the problem. There are two main benefits that can be gained by introducing prior knowledge into an adaptive model: improving generalization to new instances and simplifying learning <ref> [46, 47, 48] </ref>. Techniques for injecting prior knowledge into recurrent neural networks have been proposed by many researchers [49, 50, 51]. In these cases the domain knowledge is supposed to be available as a collection of transition rules for a finite automaton.
Reference: [47] <author> G. G. Towell, J. W. Shavlik, and M. O. Noordewier, </author> <title> "Refinement of approximate domain theories by knowledge-based neural networks," </title> <booktitle> in Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <address> (Boston, MA), </address> <pages> pp. 861-866, </pages> <year> 1990. </year>
Reference-contexts: The transition graph can be constrained using some prior knowledge on the problem. There are two main benefits that can be gained by introducing prior knowledge into an adaptive model: improving generalization to new instances and simplifying learning <ref> [46, 47, 48] </ref>. Techniques for injecting prior knowledge into recurrent neural networks have been proposed by many researchers [49, 50, 51]. In these cases the domain knowledge is supposed to be available as a collection of transition rules for a finite automaton.
Reference: [48] <author> V. Tresp, J. Hollatz, and S. Ahmad, </author> <title> "Network structuring and training using rule-based knowledge," </title> <booktitle> in Advances in Neural Information Processing Systems 5 (S. </booktitle> <editor> J. Hanson, J. D. Cowan, and C. L. Giles, eds.), </editor> <address> San Mateo, CA: </address> <publisher> Morgan Kaufman Publishers, </publisher> <year> 1993. </year>
Reference-contexts: The transition graph can be constrained using some prior knowledge on the problem. There are two main benefits that can be gained by introducing prior knowledge into an adaptive model: improving generalization to new instances and simplifying learning <ref> [46, 47, 48] </ref>. Techniques for injecting prior knowledge into recurrent neural networks have been proposed by many researchers [49, 50, 51]. In these cases the domain knowledge is supposed to be available as a collection of transition rules for a finite automaton.
Reference: [49] <author> C. W. Omlin and C. L. Giles, </author> <title> "Training second-order recurrent neural networks using hints," </title> <booktitle> in Machine Learning: Proc. of the Ninth Int. Conference (D. </booktitle> <editor> Sleeman and P. Edwards, eds.), </editor> <address> (San Mateo CA), </address> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year> <month> 44 </month>
Reference-contexts: There are two main benefits that can be gained by introducing prior knowledge into an adaptive model: improving generalization to new instances and simplifying learning [46, 47, 48]. Techniques for injecting prior knowledge into recurrent neural networks have been proposed by many researchers <ref> [49, 50, 51] </ref>. In these cases the domain knowledge is supposed to be available as a collection of transition rules for a finite automaton. A similar approach could be used to choose good topologies of the transition graph in Markovian models (e.g., HMMs or IOHMMs).
Reference: [50] <author> R. Maclin and J. W. Shawlik, </author> <title> "Refining domain theories expressed as finite-state automata," </title> <booktitle> in Machine Learning: Proceedings of the Eighth International Workshop (L. </booktitle> <editor> Birnbaum and G. Collins, eds.), </editor> <address> (San Mateo CA), </address> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: There are two main benefits that can be gained by introducing prior knowledge into an adaptive model: improving generalization to new instances and simplifying learning [46, 47, 48]. Techniques for injecting prior knowledge into recurrent neural networks have been proposed by many researchers <ref> [49, 50, 51] </ref>. In these cases the domain knowledge is supposed to be available as a collection of transition rules for a finite automaton. A similar approach could be used to choose good topologies of the transition graph in Markovian models (e.g., HMMs or IOHMMs).
Reference: [51] <author> P. Frasconi, M. Gori, M. Maggini, and G. </author> <title> Soda, "Unified integration of explicit rules and learning by example in recurrent networks," </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> vol. 7, no. 2, </volume> <pages> pp. 340-346, </pages> <year> 1995. </year> <note> (in press). </note>
Reference-contexts: There are two main benefits that can be gained by introducing prior knowledge into an adaptive model: improving generalization to new instances and simplifying learning [46, 47, 48]. Techniques for injecting prior knowledge into recurrent neural networks have been proposed by many researchers <ref> [49, 50, 51] </ref>. In these cases the domain knowledge is supposed to be available as a collection of transition rules for a finite automaton. A similar approach could be used to choose good topologies of the transition graph in Markovian models (e.g., HMMs or IOHMMs).
Reference: [52] <author> R. Bakis, </author> <title> "Continuous speech recognition via centisecond acoustic states," </title> <booktitle> in 19st Meeting of the Acoustic Society of America, </booktitle> <month> April </month> <year> 1976. </year>
Reference-contexts: A similar approach could be used to choose good topologies of the transition graph in Markovian models (e.g., HMMs or IOHMMs). For example, structured left-to-right HMMs have been introduced in speech recognition with a topology that is based on elementary considerations about speech production and the structure of language <ref> [52, 37] </ref>. 28 5.2 Reducing Credit Diffusion by Penalized Likelihood As outlined in [44], the undesired diffusion of temporal credit depends on the fast convergence of the rank of the product of n successive matrices of transition probabilities as n increases.
Reference: [53] <author> S. Becker and Y. LeCun, </author> <title> "Improving the convergence of back-propagation learning with second order methods," </title> <booktitle> in Proceedings of the 1988 Connectionist Models Summer School (D. </booktitle> <editor> Touretzky, G. Hinton, and T. Sejnowski, eds.), </editor> <booktitle> (Pittsburg 1988), </booktitle> <pages> pp. 29-37, </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, </address> <year> 1989. </year>
Reference-contexts: In the tables, "p-n" stands for pseudo-Newton <ref> [53] </ref>. Time-weighted pseudo-newton is a variation in which derivatives with respect to the instantiation of a parameter at a particular time step are weighted by the inverse of the corresponding second derivatives [13]. Multigrid is similar to simulated annealing with constant temperature 0.
Reference: [54] <author> D. Angluin and C. Smith, </author> <title> "Inductive inference: Theory and methods," </title> <journal> Computing Surveys, </journal> <volume> vol. 15, no. 3, </volume> <pages> pp. 237-269, </pages> <year> 1983. </year>
Reference-contexts: It can be considered as a prototype for more complex language processing problems. However, even in the "simplest" case, i.e., regular grammars, the task can be proved to be NP-complete <ref> [54] </ref>. Many researchers [55, 56, 57] have approached grammatical inference with recurrent networks. These studies demonstrate that second-order neural networks can be trained to approximate the behavior of finite state automata (FSA).
Reference: [55] <author> C. L. Giles, C. B. Miller, D. Chen, H. H. Chen, G. Z. Sun, and Y. C. Lee, </author> <title> "Learning and extracted finite state automata with second-order recurrent neural networks," </title> <journal> Neural Computation, </journal> <volume> vol. 4, no. 3, </volume> <pages> pp. 393-405, </pages> <year> 1992. </year>
Reference-contexts: It can be considered as a prototype for more complex language processing problems. However, even in the "simplest" case, i.e., regular grammars, the task can be proved to be NP-complete [54]. Many researchers <ref> [55, 56, 57] </ref> have approached grammatical inference with recurrent networks. These studies demonstrate that second-order neural networks can be trained to approximate the behavior of finite state automata (FSA). However, memories learned in this way appear to lack robustness and noisy dynamics become dominant for long input strings. <p> 2m+1 as a substring 4 string does not contain 000 as a substring 5 string contains an even number of 01's and 10's 6 number of 0's number of 1's is a multiple of 3 7 0 ? 1 ? 0 ? 1 ? automata rules from the trained network <ref> [55, 57] </ref>. In many cases, it has been shown that the extracted automaton outperforms the trained network. Although FSA extraction procedures are relatively easy to devise for symbolic inputs, they may be more difficult to apply in tasks involving a sub-symbolic or continuous input space, such as in speech recognition. <p> We report experimental results on the application of IOHMMs to a set of regular grammars introduced by Tomita [60] and afterwards used by other researchers as a benchmark to measure the accuracy of inference methods based on recurrent networks <ref> [59, 55, 58, 56, 57] </ref>. The grammars use the binary alphabet f0; 1g and are reported in Table 5. For each grammar, Tomita also defined a small set of labeled strings to be used as training data. <p> Grammar Sizes Frequency of Accuracies n ? FSA min Convergence Average Worst Best W&K Best 1 2 2 .600 1.000 1.000 1.000 1.000 3 7 5 .150 .867 .775 1.000 .783 5 4 4 .100 1.000 1.000 1.000 .668 7 3 5 .450 .856 .815 1.000 .557 state. In <ref> [55] </ref> this problem is circumvented by appending a special "end" symbol to each string. However, in our case this would increase the number of parameters. <p> For comparison, in the last column of Table 6 we reproduce the results reported by Watrous & Kuhn [57] in the best of five trials. Other researchers also obtained interesting results, although they are not directly comparable because of the use of larger 7 training sets <ref> [59, 55] </ref> or different experimental conditions [58].
Reference: [56] <author> J. B. Pollack, </author> <title> "The induction of dynamical recognizers," </title> <journal> Machine Learning, </journal> <volume> vol. 7, no. 2, </volume> <pages> pp. 196-227, </pages> <year> 1991. </year>
Reference-contexts: It can be considered as a prototype for more complex language processing problems. However, even in the "simplest" case, i.e., regular grammars, the task can be proved to be NP-complete [54]. Many researchers <ref> [55, 56, 57] </ref> have approached grammatical inference with recurrent networks. These studies demonstrate that second-order neural networks can be trained to approximate the behavior of finite state automata (FSA). However, memories learned in this way appear to lack robustness and noisy dynamics become dominant for long input strings. <p> We report experimental results on the application of IOHMMs to a set of regular grammars introduced by Tomita [60] and afterwards used by other researchers as a benchmark to measure the accuracy of inference methods based on recurrent networks <ref> [59, 55, 58, 56, 57] </ref>. The grammars use the binary alphabet f0; 1g and are reported in Table 5. For each grammar, Tomita also defined a small set of labeled strings to be used as training data.
Reference: [57] <author> R. L. Watrous and G. M. Kuhn, </author> <title> "Induction of finite-state languages using second-order recurrent networks," </title> <journal> Neural Computation, </journal> <volume> vol. 4, no. 3, </volume> <pages> pp. 406-414, </pages> <year> 1992. </year>
Reference-contexts: It can be considered as a prototype for more complex language processing problems. However, even in the "simplest" case, i.e., regular grammars, the task can be proved to be NP-complete [54]. Many researchers <ref> [55, 56, 57] </ref> have approached grammatical inference with recurrent networks. These studies demonstrate that second-order neural networks can be trained to approximate the behavior of finite state automata (FSA). However, memories learned in this way appear to lack robustness and noisy dynamics become dominant for long input strings. <p> 2m+1 as a substring 4 string does not contain 000 as a substring 5 string contains an even number of 01's and 10's 6 number of 0's number of 1's is a multiple of 3 7 0 ? 1 ? 0 ? 1 ? automata rules from the trained network <ref> [55, 57] </ref>. In many cases, it has been shown that the extracted automaton outperforms the trained network. Although FSA extraction procedures are relatively easy to devise for symbolic inputs, they may be more difficult to apply in tasks involving a sub-symbolic or continuous input space, such as in speech recognition. <p> We report experimental results on the application of IOHMMs to a set of regular grammars introduced by Tomita [60] and afterwards used by other researchers as a benchmark to measure the accuracy of inference methods based on recurrent networks <ref> [59, 55, 58, 56, 57] </ref>. The grammars use the binary alphabet f0; 1g and are reported in Table 5. For each grammar, Tomita also defined a small set of labeled strings to be used as training data. <p> This figure shows the generalization accuracy (triangles) and the frequency of convergence to zero errors on the training set (squares), for each of the grammars, with a comparison to the best result of 5 trials obtained by Watrous & Kuhn <ref> [57] </ref> with a second-order recurrent network (dashed horizontal line) on the same data. We see that most of the IOHMM trials performed better than the best of 5 recurrent network trials and that the best IOHMM trial always generalized perfectly (unlike the best recurrent network). <p> These results compare very favorably to those obtained with second-order networks trained by gradient descent, when using the training sets proposed by Tomita. For comparison, in the last column of Table 6 we reproduce the results reported by Watrous & Kuhn <ref> [57] </ref> in the best of five trials. Other researchers also obtained interesting results, although they are not directly comparable because of the use of larger 7 training sets [59, 55] or different experimental conditions [58].
Reference: [58] <author> M. Gori, M. Maggini, and G. </author> <title> Soda, "Insertion of finite state automata into recurrent radial basis function networks," </title> <type> Tech. Rep. </type> <institution> DSI-17/93, Universita di Firenze (Italy), </institution> <year> 1993. </year> <note> (submitted). </note>
Reference-contexts: Moreover, the complexity of the discrete state space produced by the FSA extraction procedure may grow intolerably if the continuous network has learned a representation involving chaotic attractors. Other researchers have attempted to encourage a finite-state representation via regularization <ref> [58] </ref> or by integrating clustering techniques in the training procedure [59]. <p> We report experimental results on the application of IOHMMs to a set of regular grammars introduced by Tomita [60] and afterwards used by other researchers as a benchmark to measure the accuracy of inference methods based on recurrent networks <ref> [59, 55, 58, 56, 57] </ref>. The grammars use the binary alphabet f0; 1g and are reported in Table 5. For each grammar, Tomita also defined a small set of labeled strings to be used as training data. <p> Other researchers also obtained interesting results, although they are not directly comparable because of the use of larger 7 training sets [59, 55] or different experimental conditions <ref> [58] </ref>. In most of the successful trials we observed that the model learned a "deterministic" behavior, i.e., the transition probabilities were asymptotically converging either to 0 or to 1 (exact values of 0 or 1 would require to develop infinite weights because of the softmax function).
Reference: [59] <author> S. Das and M. C. Mozer, </author> <title> "A unified gradient-descent/clustering architecture for finite state machine induction," </title> <booktitle> in Advances in Neural Information Processing Systems 6 (J. </booktitle> <editor> Cowan, G. Tesauro, and J. Alspector, eds.), </editor> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: Moreover, the complexity of the discrete state space produced by the FSA extraction procedure may grow intolerably if the continuous network has learned a representation involving chaotic attractors. Other researchers have attempted to encourage a finite-state representation via regularization [58] or by integrating clustering techniques in the training procedure <ref> [59] </ref>. We report experimental results on the application of IOHMMs to a set of regular grammars introduced by Tomita [60] and afterwards used by other researchers as a benchmark to measure the accuracy of inference methods based on recurrent networks [59, 55, 58, 56, 57]. <p> We report experimental results on the application of IOHMMs to a set of regular grammars introduced by Tomita [60] and afterwards used by other researchers as a benchmark to measure the accuracy of inference methods based on recurrent networks <ref> [59, 55, 58, 56, 57] </ref>. The grammars use the binary alphabet f0; 1g and are reported in Table 5. For each grammar, Tomita also defined a small set of labeled strings to be used as training data. <p> For comparison, in the last column of Table 6 we reproduce the results reported by Watrous & Kuhn [57] in the best of five trials. Other researchers also obtained interesting results, although they are not directly comparable because of the use of larger 7 training sets <ref> [59, 55] </ref> or different experimental conditions [58].
Reference: [60] <author> M. Tomita, </author> <title> "Dynamic construction of finite-state automata from examples using hill-climbing," </title> <booktitle> in Proceedings of the Fourth Annual Cognitive Science Conference, </booktitle> <address> (Ann Arbor, MI), </address> <pages> pp. 105-108, </pages> <year> 1982. </year>
Reference-contexts: Other researchers have attempted to encourage a finite-state representation via regularization [58] or by integrating clustering techniques in the training procedure [59]. We report experimental results on the application of IOHMMs to a set of regular grammars introduced by Tomita <ref> [60] </ref> and afterwards used by other researchers as a benchmark to measure the accuracy of inference methods based on recurrent networks [59, 55, 58, 56, 57]. The grammars use the binary alphabet f0; 1g and are reported in Table 5. <p> For comparison, in Table 6 we also report for each grammar the number of states of the minimal recognizing FSA <ref> [60] </ref>. We tested the trained IOHMMs on a corpus of 2 13 1 binary strings of length T 12. The final results are numerically summarized in Table 6. The column "Convergence" reports the fraction of trials that succeeded to separate the training set. <p> Indeed, for grammars 1,4,5, and 6, we found that the trained IOHMMs behave exactly like the minimal 7 We used the training sets defined by Tomita <ref> [60] </ref>. 37 recognizing FSA (see Figure 6). In some cases, however, the IOHMM learned a different representation. In particular, for grammar 7 we found a model with three states that correctly classify all the test strings. This is interesting because the minimal FSA for grammar 7 has five states. <p> The results obtained in recent experiments suggest that IOHMMs are appropriate for solving grammatical inference problems. In particular, for the benchmark problem proposed by Tomita <ref> [60] </ref>, IOHMMs compare favorably to second order nets trained by gradient descent, in terms of generalization performance. Future work will have to address extensions of this model in several directions.
Reference: [61] <author> J. E. Hopcroft and J. D. Ullman, </author> <title> Introduction to Automata Theory, </title> <booktitle> Languages, and Computation. </booktitle> <address> Reading, MA: </address> <publisher> Addison-Wesley Publishing Company, Inc., </publisher> <year> 1979. </year> <month> 45 </month>
Reference-contexts: In [55] this problem is circumvented by appending a special "end" symbol to each string. However, in our case this would increase the number of parameters. The task of accepting strings can be solved by a Moore finite state machine <ref> [61] </ref>, in which the output is function of the state only (i.e., strings are accepted or rejected depending on what final state is reached). Hence, we did not apply external inputs to the output networks, that reduced to one unit fed by a bias input.
References-found: 61

