URL: http://www.cs.huji.ac.il/labs/learning/Papers/acms_nips.ps.gz
Refering-URL: http://www.cs.huji.ac.il/labs/learning/Papers/MLT_list.html
Root-URL: 
Email: E-mail: franni,fshai,tishbyg@cs.huji.ac.il  
Title: Agnostic Classification of Markovian Sequences  
Author: Ran El-Yaniv Shai Fine Naftali Tishby 
Keyword: Category: Algorithms.  
Address: Jerusalem 91904, Israel  
Affiliation: Institute of Computer Science and Center for Neural Computation The Hebrew University  
Abstract: Classification of finite sequences without explicit knowledge of their statistical nature is a fundamental problem with many important applications. We propose a new information theoretic approach to this problem which is based on the following ingredients: (i) sequences are similar when they are likely to be generated by the same source; (ii) cross entropies can be estimated via "universal compression"; (iii) Markovian sequences can be asymptotically-optimally merged. With these ingredients we design a method for the classification of discrete sequences whenever they can be compressed. We introduce the method and illustrate its application for hierarchical clustering of languages and for estimating similarities of protein sequences.
Abstract-found: 1
Intro-found: 1
Reference: [BE97] <author> R. Bachrach and R. El-Yaniv, </author> <title> An Improved Measure of Relative Entropy Between Individual Sequences, </title> <type> unpublished manuscript. </type>
Reference-contexts: We apply an improvement of the method of Ziv and Merhav [ZM93] for the estimation of the two cross-entropies using the Lempel-Ziv algorithm given two sample sequences <ref> [BE97] </ref>. Notice that our estimation of D is as good as the compression method used, namely, closer to optimal compression yields better estimation of the similarity measure.
Reference: [CK81] <author> I. Csiszar and J. Krorner. </author> <title> Information Theory: Coding Theorems for Discrete Memoryless Systems, </title> <publisher> Academic Press, </publisher> <month> New-York </month> <year> 1981. </year>
Reference-contexts: The symmetric divergence, i.e. D (p; q) = D KL [pjjq] + D KL [qjjp], suffers from similar sensitivity problems and lacks the clear statistical meaning. 2.1 The "two sample problem" Direct Bayesian arguments, or alternately the method of types <ref> [CK81] </ref>, suggest that the probability that there exists one source distribution ^ M for two independently drawn samples, x and y [Leh59], is proportional to Z Z d (M ) 2 (jxjD KL [p x jjM]+jyjD KL [p y jjM]) ; (2) where d (M ) is a prior density of
Reference: [CT91] <author> T. M. Cover and J. A. Thomas. </author> <title> Elements of Information Theory, </title> <publisher> John Wiley & Sons, </publisher> <month> New-York </month> <year> 1991. </year>
Reference-contexts: Estimating the log-likelihood of a sequence-sample over a discrete alphabet by a statistical model can be done through the Cross Entropy or Kullback-Leibler Divergence <ref> [CT91] </ref> between the sample empirical distribution p and model distribution q, defined as: D KL (pjjq) = X p () log q () The KL-divergence, however, has some serious practical drawbacks. <p> Since cross entropies, D KL , express code-length differences, they can be estimated using any efficient compression algorithm for the two sequences. The existence of "universal" compression methods, such as the Lempel-Ziv algorithm (see e.g. <ref> [CT91] </ref>) which are provably asymptotically optimal for any sequence, give us the means for asymptotically optimal estimation of D , provided that we can obtain a typical sequence of the most-likely joint source, M .
Reference: [EFT97] <author> R. El-Yaniv, S. Fine and N. Tishby. </author> <title> Classifying Markovian Sources, </title> <booktitle> in preparations, </booktitle> <year> 1997. </year>
Reference-contexts: Here we apply the method to hierarchical clustering of short text segments in 18 European languages and to evaluation of similarities of protein sequences. A complete analysis of the method, with further applications, will be presented elsewhere <ref> [EFT97] </ref>. 2 Measuring the statistical similarity of sequences Estimating the statistical similarity of two individual sequences is traditionally done by training a statistical model for each sequence and then measuring the likelihood of the other sequence by the model. <p> The function D (p; q) is an extension of the Jensen-Shannon divergence (see e.g. [Lin91]) and satisfies many useful analytic properties, such as symmetry and bound edness on both sides by the L 1 -norm, in addition to its clear statistical meaning. See <ref> [Lin91, EFT97] </ref> for a more complete discussion of this measure. 2.2 Estimating the D similarity measure The key component of our classification method is the estimation of D for individual finite sequences, without an explicit model distribution. <p> These algorithms are different from most existing approaches because they rely only on the sequenced data, similar to universal compression, without explicit modeling assumptions. Further details, analysis, and applications of the method will be presented elsewhere <ref> [EFT97] </ref>. 4.1 Merging and synthesis of sequences An immediate application of the source merging algorithm is for synthesis of typical sequences of the joint source from some given data sequences, without any access to an explicit model of the source. To illustrate this point consider the sequence in Figure 2.
Reference: [HH92] <author> S. Henikoff and J. G. </author> <month> Henikoff </month> <year> (1992). </year> <title> Amino acid substitution matrices from protein blocks. </title> <booktitle> Proc. </booktitle> <institution> Natl. Acad. Sci. </institution> <address> USA 89, </address> <pages> 10915-10919. </pages>
Reference-contexts: Darker gray represent higher similarity. In another experiment we considered all the 200 proteins of the Kinase family and computed the pairwise distances of these proteins using the agnostic algorithm. For comparison we computed the pairwise similarities of these sequences using the widely used Smith-Waterman algorithm (see e.g. <ref> [HH92] </ref>). 2 The resulting agnostic similarities, computed with no biological information whatsoever, are very similar to the Smith-Waterman similarities. 3 Furthermore, our agnostic measure discovered some biological similarities not detected by the Smith-Waterman method. 4.3 Agnostic classification of languages The sample of the joint source of two sequences can be considered
Reference: [Leh59] <author> E. L. Lehmann. </author> <title> Testing Statistical Hypotheses, </title> <publisher> John Wiley & Sons, </publisher> <month> New-York </month> <year> 1959. </year>
Reference-contexts: The third ingredient is a novel and simple randomized sequence merging algorithm which provably generates a typical sequence of the most likely joint source of the sequences, under the above Markovian approximation assumption. Our similarity measure is also motivated by the known "two sample problem" <ref> [Leh59] </ref> of estimating the probability that two given samples are taken from the same distribution. In the i.i.d. (Bernoulli) case this problem was thoroughly investigated and the optimal statistical test is given by the sum of the empirical cross entropies between the two samples and their most likely joint source. <p> + D KL [qjjp], suffers from similar sensitivity problems and lacks the clear statistical meaning. 2.1 The "two sample problem" Direct Bayesian arguments, or alternately the method of types [CK81], suggest that the probability that there exists one source distribution ^ M for two independently drawn samples, x and y <ref> [Leh59] </ref>, is proportional to Z Z d (M ) 2 (jxjD KL [p x jjM]+jyjD KL [p y jjM]) ; (2) where d (M ) is a prior density of all candidate distributions, p x and p y are the empirical (sample) distributions, and jxj and jyj are the corresponding sample
Reference: [Lin91] <author> J. Lin, </author> <year> 1991. </year> <title> Divergence measures based on the Shannon entropy. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 37(1) </volume> <pages> 145-151. </pages>
Reference-contexts: The function D (p; q) is an extension of the Jensen-Shannon divergence (see e.g. <ref> [Lin91] </ref>) and satisfies many useful analytic properties, such as symmetry and bound edness on both sides by the L 1 -norm, in addition to its clear statistical meaning. <p> The function D (p; q) is an extension of the Jensen-Shannon divergence (see e.g. [Lin91]) and satisfies many useful analytic properties, such as symmetry and bound edness on both sides by the L 1 -norm, in addition to its clear statistical meaning. See <ref> [Lin91, EFT97] </ref> for a more complete discussion of this measure. 2.2 Estimating the D similarity measure The key component of our classification method is the estimation of D for individual finite sequences, without an explicit model distribution.
Reference: [ZM93] <author> J. Ziv and N. Merhav, </author> <year> 1993. </year> <title> A Measure of Relative Entropy Between Individual Sequences with Application to Universal Classification, </title> <journal> IEEE Transactions on Information Theory, </journal> <month> 39(4). </month> <title> 4 Klingon is a synthetic language that was invented for the Star-Trek TV series. </title>
Reference-contexts: We apply an improvement of the method of Ziv and Merhav <ref> [ZM93] </ref> for the estimation of the two cross-entropies using the Lempel-Ziv algorithm given two sample sequences [BE97]. Notice that our estimation of D is as good as the compression method used, namely, closer to optimal compression yields better estimation of the similarity measure.
References-found: 8

