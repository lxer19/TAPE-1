URL: http://www.cs.nyu.edu/kedem/pubs/darpa92.ps
Refering-URL: http://www.cs.nyu.edu/kedem/pubs.html
Root-URL: http://www.cs.nyu.edu
Title: Methods for Handling Faults and Asynchrony in Parallel Computation  
Author: Z. M. Kedem 
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Y. Aumann and M. Ben-Or, </author> <title> "Asymptotically Optimal PRAM Emulation on Faulty Hypercube," </title> <booktitle> Proc. 32nd IEEE Symp. on Foundations of Computer Science, </booktitle> <pages> 440-446, </pages> <year> 1991. </year>
Reference-contexts: The techniques described in this paper were developed for shared memory machines. However they have been recently extended by Aumann and Ben-Or in the context of distributed memory machines (see <ref> [1] </ref> and [2]) thus proving their applicability to a larger family of parallel machine models and architectures.
Reference: [2] <author> Y. Aumann and M. Ben-Or, </author> <title> "Computing with Faulty Arrays," </title> <booktitle> Proc. 24th ACM Symposium on Theory of Computing, </booktitle> <year> 1992, </year> <note> to appear. </note>
Reference-contexts: The techniques described in this paper were developed for shared memory machines. However they have been recently extended by Aumann and Ben-Or in the context of distributed memory machines (see [1] and <ref> [2] </ref>) thus proving their applicability to a larger family of parallel machine models and architectures.
Reference: [3] <author> R. Bagrodia, M. Chandy, and E. Kwan, </author> <title> "UC: A Language for the Connection Machine," </title> <booktitle> Proc., Supercomputing `90, </booktitle> <pages> 525-534, </pages> <year> 1990. </year>
Reference-contexts: Since the threads interact, barriers are created periodically to synchronize them. In the extreme case, synchronization barriers can be specified after each parallel step; for an example of this possibility, consider the UC programming language <ref> [3] </ref>. In this paper, for simplicity we will assume the need for synchronization after each step. A program written in such a model of a fault-free, completely synchronous machine will be called an ideal program.
Reference: [4] <author> R. Cole and O. Zajicek, </author> <title> "The APRAM: Incorporating Asynchrony into the PRAM Model," </title> <booktitle> Proc. 1989 ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> 170-178, </pages> <year> 1989. </year>
Reference-contexts: The problems due to faults and asynchrony in parallel computing have been recognized for some time by the algorithms community. In this paper we are not able to present in any detail the earlier formal work done in this area; for typical examples, see <ref> [4] </ref>, [10], and [11]. However, the approach taken there did not address the general problem of reliable computing under the most general assumptions on the parallel computers.
Reference: [5] <author> M. Herlihy, </author> <title> "Wait-free Synchronization," </title> <journal> ACM Trans. on Programming Languages and Systems, </journal> <volume> 13(1), </volume> <pages> 124-149, </pages> <year> 1991. </year>
Reference: [6] <author> Z. Kedem and K. Palem, </author> <title> "Transformations for the Automatic Derivation of Resilient Parallel Programs," </title> <booktitle> Proc. of the workshop on Fault Tolerance in Parallel and Distributed Systems, </booktitle> <year> 1992, </year> <note> to appear. </note>
Reference: [7] <author> Z. Kedem, K. Palem, M. Rabin, and A. Raghunathan, </author> <title> "Efficient Program Transformations for Resilient Parallel Computation via Randomization," </title> <booktitle> Proc. 24th ACM Symp. on Theory of Computing, </booktitle> <year> 1992, </year> <note> to appear. </note>
Reference-contexts: As a consequence, such slow processor might install an obsolete value in memory, resulting in the problem we refer to as clobbering. We very briefly sketch the main ideas behind our solution to the clobbering problem; for details see <ref> [7] </ref>. First we will replicate certain "important variables," so that a value is written in several copies; we refer to these as fuzzy variables. We will assure with extremely high probability that at least one copy is not clobbered. <p> Thus, the implementation and updates to the clock will be self-referential. For full description, see again <ref> [7] </ref>. In this section, we found it easier to describe the transformation of the source program into the target program as if it were done separately for each program.
Reference: [8] <author> Z. Kedem, K. Palem, A. Raghunathan, and P. Spirakis, </author> <title> "Combining Tentative and Definite Algorithms for Very Fast Dependable Parallel Computing," </title> <booktitle> Proc. 23rd ACM Symp. on Theory of Computing, </booktitle> <pages> 381-390, </pages> <year> 1991. </year>
Reference-contexts: Returning to the execution of a single (now idempotent) step, we are faced with the number of problems. For purpose of exposition, we first assume that the imperfect machine is fault-prone, but the functioning processors are synchronous. For details, see [9] and <ref> [8] </ref>.
Reference: [9] <author> Z. Kedem, K. Palem, and P. Spirakis, </author> <title> "Efficient Robust Parallel Computations," </title> <booktitle> Proc. 22nd ACM Symp. on Theory of Computing, </booktitle> <pages> 138-148, </pages> <year> 1990. </year>
Reference-contexts: Returning to the execution of a single (now idempotent) step, we are faced with the number of problems. For purpose of exposition, we first assume that the imperfect machine is fault-prone, but the functioning processors are synchronous. For details, see <ref> [9] </ref> and [8].
Reference: [10] <author> P. Kanellakis and A. Shvartsman, </author> <title> "Efficient Parallel Algorithms Can be Made Robust," </title> <booktitle> Proc. 8th ACM Symp. on Principles of Distributed Computing, </booktitle> <pages> 211-222, </pages> <year> 1989. </year>
Reference-contexts: The problems due to faults and asynchrony in parallel computing have been recognized for some time by the algorithms community. In this paper we are not able to present in any detail the earlier formal work done in this area; for typical examples, see [4], <ref> [10] </ref>, and [11]. However, the approach taken there did not address the general problem of reliable computing under the most general assumptions on the parallel computers. For instance, the results presented in [10] for the case of fail-stop processors and in [11] for the case of asynchronous processors addressed only the <p> to present in any detail the earlier formal work done in this area; for typical examples, see [4], <ref> [10] </ref>, and [11]. However, the approach taken there did not address the general problem of reliable computing under the most general assumptions on the parallel computers. For instance, the results presented in [10] for the case of fail-stop processors and in [11] for the case of asynchronous processors addressed only the reliable execution of individual algorithms, by hand-tailoring them to account for the potentially imperfect behavior of the target machine. fl The participants in various phases of the research described in this paper <p> of executing the first program exactly once is the same as executing the second program in such a way that its first instruction is executed at least once and then its second instruction is executed at least once. 4 This primitive is an abstraction of a specific problem considered in <ref> [10] </ref>, the Write-All problem, dealing with filling a vector with a constant value. 5 A processor that reenters a computation after its failure can easily determine what to do by reading a small number of memory locations. developed a mechanism for reliably marking a potentially clobbered copy.
Reference: [11] <author> C. Martel, A. Park, and R. Subramonian, </author> <title> "Fast Asynchronous Algorithms for Shared Memory Parallel Computers," </title> <type> Tech. Rep. </type> <institution> CSE-89-8, Univ. of California-Davis, </institution> <month> 1-17, July 25, </month> <year> 1989. </year>
Reference-contexts: The problems due to faults and asynchrony in parallel computing have been recognized for some time by the algorithms community. In this paper we are not able to present in any detail the earlier formal work done in this area; for typical examples, see [4], [10], and <ref> [11] </ref>. However, the approach taken there did not address the general problem of reliable computing under the most general assumptions on the parallel computers. For instance, the results presented in [10] for the case of fail-stop processors and in [11] for the case of asynchronous processors addressed only the reliable execution <p> done in this area; for typical examples, see [4], [10], and <ref> [11] </ref>. However, the approach taken there did not address the general problem of reliable computing under the most general assumptions on the parallel computers. For instance, the results presented in [10] for the case of fail-stop processors and in [11] for the case of asynchronous processors addressed only the reliable execution of individual algorithms, by hand-tailoring them to account for the potentially imperfect behavior of the target machine. fl The participants in various phases of the research described in this paper were: Zvi Kedem (New York University), Krishna Palem (IBM
Reference: [12] <author> S. Plotkin, </author> <title> "Sticky Bits and Universality of Consensus," </title> <booktitle> Proc. 8th ACM Symp. on Principles of Distributed Computing, </booktitle> <pages> 159-176, </pages> <year> 1989. </year>
Reference: [13] <author> M. Rabin, </author> <title> "Efficient Dispersal of Information for Security, Load Balancing and Fault Tolerance," </title> <journal> J. ACM, </journal> <volume> 36(2), </volume> <pages> 335-348, </pages> <year> 1989. </year>
Reference-contexts: In addition, processors could have local memory, which is lost if this processor fails. Here, processor failures effectively lead to memory failures. Rabin proposed a solution to these problems through information dispersal, see <ref> [13] </ref>. By using dispersal, one can "reconstruct" lost information with very high probability using extremely low amounts of replication of the original information. The techniques described in this paper were developed for shared memory machines.
References-found: 13

