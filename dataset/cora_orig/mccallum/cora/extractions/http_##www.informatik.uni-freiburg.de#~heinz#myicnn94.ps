URL: http://www.informatik.uni-freiburg.de/~heinz/myicnn94.ps
Refering-URL: http://www.informatik.uni-freiburg.de/~heinz/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: email: heinz@informatik.uni-freiburg.de  
Title: Fast Bounded Smooth Regression with Lazy Neural Trees  
Author: Alois P. Heinz 
Address: Rheinstrae 10-12, D-79104 Freiburg, Germany  
Affiliation: Institut fur Informatik, Universitat Freiburg  
Abstract: We propose the lazy neural tree (LNT) as the appropriate architecture for the realization of smooth regression systems. The LNT is a hybrid of a decision tree and a neural network. From the neural network it inherits smoothness of the generated function, incremental adaptability, and conceptual simplicity. From the decision tree it inherits the topology and initial parameter setting as well as a very efficient sequential implementation that out-performs traditional neural network simulations by the order of magnitudes. The enormous speed is achieved by lazy evaluation. A further speed-up can be obtained by the application of a window-ing scheme if the region of interesting results is restricted. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone, </author> <title> Classification and Regression Trees. </title> <address> Belmont, California: </address> <publisher> Wadsworth, </publisher> <year> 1984. </year>
Reference: [2] <author> J. R. Quinlan, </author> <title> "Learning efficient classification procedures and their application to chess end games," in Machine Learning, An Artificial Intelligence Approach (R. </title> <editor> S. Michalski, J. G. Carbonell, and T. M. Mitchell, eds.), ch. </editor> <volume> 15, </volume> <pages> pp. 463-482, </pages> <address> Berlin: </address> <publisher> Springer-Verlag, </publisher> <year> 1984. </year>
Reference: [3] <author> J. R. Quinlan, </author> <title> "Induction of decision trees," </title> <journal> Machine Learning, </journal> <volume> vol. 1, </volume> <pages> pp. 81-106, </pages> <year> 1986. </year>
Reference: [4] <author> L. Atlas, R. Cole, Y. Muthusamy, A. Lipp-man, J. Connor, D. Park, M. El-Sharkawi, and R. J. Marks II, </author> <title> "A performance comparison of trained multilayer perceptrons and trained classification trees," </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> vol. 78, </volume> <pages> pp. 1614-1619, </pages> <month> Oct. </month> <year> 1990. </year>
Reference: [5] <author> D. Knuth and R. Moore, </author> <title> "An analysis of alpha-beta pruning," </title> <journal> Artificial Intelligence, </journal> <volume> vol. 6, </volume> <pages> pp. 293-326, </pages> <year> 1975. </year>
Reference: [6] <author> J. Pearl, </author> <title> Heuristics: Intelligent Search Strategies for Computer Problem Solving. </title> <address> Reading, MA: </address> <publisher> Addison-Wesley, </publisher> <year> 1984. </year>
Reference: [7] <author> I. K. Sethi, </author> <title> "Entropy nets: From decision trees to neural networks," </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> vol. 78, </volume> <pages> pp. 1605-1613, </pages> <month> Oct. </month> <year> 1990. </year>
Reference: [8] <author> I. K. Sethi and M. Otten, </author> <title> "Comparison between entropy net and decision tree classifiers," </title> <booktitle> in Proc. IJCNN | International Joint Conference on Neural Networks, </booktitle> <address> San Diego, Cali-fornia, (Ann Arbor), </address> <pages> pp. </pages> <booktitle> III-63-III-68, IEEE Neural Networks Council, </booktitle> <month> June </month> <year> 1990. </year>
Reference: [9] <author> S. M. Omohundro, </author> <title> "Bumptrees for efficient function, constraint, and classification learning," </title> <booktitle> in Advances in Neural Information Processing Systems 3, Proc. of the third NIPS Conference, Denver, </booktitle> <editor> Colorado (R. L. Lipp-mann, J. E. Moody, and D. S. Touretzky, </editor> <booktitle> eds.), </booktitle> <pages> pp. 693-699, </pages> <publisher> Morgan Kauffmann, </publisher> <month> Nov. </month> <year> 1990. </year>
Reference: [10] <author> J. A. Boyan, </author> <title> "Modular neural networks for learning context-dependent game strategies," </title> <type> Master's thesis, </type> <institution> University of Cambridge, Department of Engineering and Computer Laboratory, </institution> <month> Aug. </month> <year> 1992. </year>
Reference: [11] <author> J. B. Hampshire and A. Waibel, </author> <title> "The Meta-Pi network: Building distributed knowledge representations for robust pattern recognition," </title> <type> Tech. Rep. </type> <institution> CMU-CS-89-166, School of Computer Science, Carnegie Mellon University, Pittsburgh, </institution> <month> Aug. </month> <year> 1989. </year>
Reference: [12] <author> W. W. Armstrong, A. Dwelly, J. D. Liang, D. Lin, and S. Reynolds, </author> <title> "Learning and generalization in adaptive logic networks," in Artificial Neural Networks (T. </title> <editor> Kohonen, K. Makisara, and J. Kangas, </editor> <booktitle> eds.), </booktitle> <pages> pp. 1173-1176, </pages> <publisher> Elsevier Science Publishers B.V. (North Holland), </publisher> <year> 1991. </year>
Reference: [13] <author> J. R. Quinlan, C4.5: </author> <title> Programs for Machine Learning. </title> <booktitle> Machine Learning, </booktitle> <address> San Mateo, Cali-fornia: </address> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1993. </year>
Reference: [14] <author> J. Mingers, </author> <title> "An empirical comparison of selection measures for decision-tree induction," </title> <journal> Machine Learning, </journal> <volume> vol. 3, </volume> <pages> pp. 319-342, </pages> <year> 1989. </year>
Reference: [15] <author> J. Mingers, </author> <title> "An empirical comparison of pruning methods for decision-tree induction," </title> <journal> Machine Learning, </journal> <volume> vol. 4, </volume> <pages> pp. 221-243, </pages> <year> 1989. </year>
Reference: [16] <author> W. Buntine and T. Niblett, </author> <title> "A further comparison of splitting rules for decision-tree induction | technical note," </title> <journal> Machine Learning, </journal> <volume> vol. 8, </volume> <pages> pp. 75-85, </pages> <year> 1992. </year>
Reference: [17] <author> P. J. Werbos, </author> <title> "Backpropagation through time: What it does and how to do it," </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> vol. 78, </volume> <pages> pp. 1550-1560, </pages> <month> Oct. </month> <year> 1990. </year>
Reference: [18] <author> S. E. Fahlman, </author> <title> "An empirical study of learning speed in back-propagation networks," </title> <type> Tech. Rep. </type> <institution> CMU-CS-88-162, Computer Science Department, Carnegie Mellon University, Pitts-burgh, </institution> <month> June </month> <year> 1988. </year>
Reference: [19] <author> D. E. Rumelhart, G. E. Hinton, and R. J. Williams, </author> <title> "Learning internal representations by error propagation," </title> <booktitle> in Parallel Distributed Processing: Explorations in the Microstructure of Cognition, </booktitle> <volume> Vol. 1 (D. </volume> <editor> E. Rumelhart, J. L. McClelland, </editor> <booktitle> and the PDP Research Group, eds.), </booktitle> <pages> pp. 318-362, </pages> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1986. </year>
References-found: 19

