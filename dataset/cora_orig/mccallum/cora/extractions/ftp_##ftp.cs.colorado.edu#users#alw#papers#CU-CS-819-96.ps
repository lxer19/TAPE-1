URL: ftp://ftp.cs.colorado.edu/users/alw/papers/CU-CS-819-96.ps
Refering-URL: http://www.cs.colorado.edu/~alw/RecentPubs.html
Root-URL: http://www.cs.colorado.edu
Email: fjcook,alwg@cs.colorado.edu  
Title: Discovering Models of Software Processes from Event-Based Data  
Author: Jonathan E. Cook and Alexander L. Wolf 
Address: Boulder, CO 80309 USA  
Affiliation: Software Engineering Research Laboratory Department of Computer Science University of Colorado  
Abstract: University of Colorado Department of Computer Science Technical Report CU-CS-819-96 November 1996 ABSTRACT Many software process methods and tools presuppose the existence of a formal model of a process. Unfortunately, developing a formal model for an on-going, complex process can be difficult, costly, and error prone. This presents a practical barrier to the adoption of process technologies, which would be lowered by automated assistance in creating formal models. To this end, we have developed a technique for using event data captured from an ongoing process to generate a formal model of the behavior of that process. We term this kind of data analysis process discovery. Three methods for process discovery, ranging from the purely algorithmic to the purely statistical, are described and compared in this paper. We also discuss the application of the methods in an industrial case study. This work was supported in part by the National Science Foundation under grant CCR-93-02739 and by the Air Force Material Command, Rome Laboratory, and the Defense Advanced Research Projects Agency under Contract Number F30602-94-C-0253. The content of the information does not necessarily reflect the position or the policy of the Government and no official endorsement should be inferred. c fl 1996 Jonathan E. Cook and Alexander L. Wolf
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. Ahonen, K. Mannila, and E. Nikunen. </author> <title> Grammatical Inference and Applications, </title> <booktitle> volume 862 of Lecture Notes in Artificial Intelligence (subseries of LNCS), </booktitle> <pages> pages 153-167. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: This machine is then iteratively merged according to some rules until a final state machine is output as the learned language. Examples of this are the following. 5 * Ahonen et al. <ref> [1] </ref> describe a prefix tree method where states are merged based on previous contexts.
Reference: [2] <author> D. Angluin. </author> <title> Inductive inference of formal languages from positive data. </title> <journal> Information and Control, </journal> <volume> 45 </volume> <pages> 117-135, </pages> <year> 1980. </year>
Reference-contexts: The survey by Angluin and Smith [5] is a broad look at inductive inference learning. Pitt's survey [46] is a good starting point for understanding the theoretical complexities involved in learning regular grammars. Further theoretical work on language learning is also available <ref> [2, 36, 40, 41, 48] </ref>. Angluin and Smith acknowledge the gap between theoretical results and practical application of inference methods: The most significant open problem in the field is perhaps not any specific technical question, but the gap between abstract and concrete results.
Reference: [3] <author> D. Angluin. </author> <title> Inference of reversible languages. </title> <journal> Journal of the ACM, </journal> <volume> 29(3) </volume> <pages> 741-765, </pages> <month> July </month> <year> 1982. </year>
Reference-contexts: Examples of this are the following. 5 * Ahonen et al. [1] describe a prefix tree method where states are merged based on previous contexts. That is, if two states are entered from the same k-length contexts, then they are merged. * Angluin <ref> [3] </ref> merges states of a prefix tree based on a notion of k-reversibility, which restricts the class of languages the algorithm can infer.
Reference: [4] <author> D. Angluin. </author> <title> Learning regular sets from queries and counter-examples. </title> <journal> Information and Computation, </journal> <volume> 75 </volume> <pages> 87-106, </pages> <year> 1987. </year>
Reference-contexts: Results for grammars, including DFAs, are more negative, but less definite. As Pitt reports: If DFAs are polynomially approximately predictable, then there is a probabilistic poly nomial time algorithm for inverting the RSA encryption function, for factoring Blum integers, and for deciding quadratic residues [46]. Angluin <ref> [4] </ref> phrases the learning problem in terms of an oracle. An algorithm can make a fixed set of queries to the oracle, the basic two being "Is this string accepted by the correct grammar?" and "Is this grammar equivalent to the correct grammar?".
Reference: [5] <author> D. Angluin and C.H. Smith. </author> <title> Inductive inference: Theory and methods. </title> <journal> ACM Computing Surveys, </journal> <volume> 15(3) </volume> <pages> 237-269, </pages> <month> September </month> <year> 1983. </year>
Reference-contexts: We address this issue in Section 5.5. To develop our technique, we have cast the process discovery problem in terms of another, previously investigated FSM discovery problem. That problem is the discovery of a grammar for a regular language given example sentences in that language <ref> [5] </ref>. This area of research historically is referred to as grammar inference. If one interprets events as tokens and event streams as sentences in the language of the process, then a natural analog between these areas becomes evident. <p> The PAC model for learning concepts from examples was proposed by Valiant [51]. PAC stands for "Probably Approximately Correct". Probably is defined as within some probability 1 ffi, and 1 This presentation is consistent with others in the field <ref> [5, 46, 49] </ref>. 2 L = fl is not a very interesting language. 4 approximately is defined as within some 1* of the correct solution. <p> With this framework, Angluin gave a polynomial-time algorithm for learning DFAs, and others have extended this to other classes of languages. There is much work describing the computational complexities of various learning paradigms and classes of languages or formulas. The survey by Angluin and Smith <ref> [5] </ref> is a broad look at inductive inference learning. Pitt's survey [46] is a good starting point for understanding the theoretical complexities involved in learning regular grammars. Further theoretical work on language learning is also available [2, 36, 40, 41, 48]. <p> It would be unfortunate if the abstract results proliferated fruitlessly, while the concrete results produced little or nothing of significance beyond their very narrow domains <ref> [5] </ref>. Since the time of their survey, however, research efforts have continued to produce significant and practical results, as we discuss next. 4.3 Practical Inference Techniques There has been some work that has devised or used practical algorithms for grammar inference in the analysis of real-world problem domains.
Reference: [6] <author> G.S. Avrunin, U.A. Buy, J.C. Corbett, L.K. Dillon, and J.C. Wileden. </author> <title> Automated analysis of concurrent systems with the constrained expression toolset. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17(11) </volume> <pages> 1204-1222, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: The use of event data to characterize behavior is already widely accepted in other areas of software engineering, such as program visualization [42], concurrent-system analysis <ref> [6] </ref>, and distributed debugging [10, 25]. The "instant" of an event is relative to the time granularity that is needed or desired; thus, certain activities that are of short duration relative to the time granularity are represented as a single event.
Reference: [7] <author> S. Bandinelli, A. Fuggetta, and C. Ghezzi. </author> <title> Software Process Model Evolution in the SPADE Environe-ment. </title> <journal> IEEE Transactions on Software Engineering, </journal> 19(12) 1128-1144, December 1993. 
Reference-contexts: In response, new methods and tools for supporting various aspects of the software process have been devised. Many of the technologies, including process automation [8, 9, 27, 45, 50], process analysis [33, 34, 37, 47], and process evolution <ref> [7, 35] </ref>, assume the existence of some sort of formal model of a process in order for those technologies to be applied. The need to develop a formal model as a prerequisite to using a new technology is a daunting prospect to the managers of large, on-going projects.
Reference: [8] <author> S. Bandinelli, A. Fuggetta, C. Ghezzi, and L. Lavazza. SPADE: </author> <title> An Environment for Software Process Analysis, Design, </title> <editor> and Enactment. In A. Finkelstein, J. Kramer, and B. Nuseibeh, editors, </editor> <booktitle> Software Process Modeling and Technology, </booktitle> <pages> pages 223-248. </pages> <publisher> Wiley, </publisher> <year> 1994. </year>
Reference-contexts: 1 Introduction The challenge of managing and improving the software process has come to the forefront of software engineering research. In response, new methods and tools for supporting various aspects of the software process have been devised. Many of the technologies, including process automation <ref> [8, 9, 27, 45, 50] </ref>, process analysis [33, 34, 37, 47], and process evolution [7, 35], assume the existence of some sort of formal model of a process in order for those technologies to be applied.
Reference: [9] <author> N.S. Barghouti and G.E. Kaiser. </author> <title> Scaling Up Rule-based Development Environments. </title> <booktitle> In Proceedings of the Third European Software Engineering Conference, number 550 in Lecture Notes in Computer Science, </booktitle> <pages> pages 380-395. </pages> <publisher> Springer-Verlag, </publisher> <month> October </month> <year> 1991. </year>
Reference-contexts: 1 Introduction The challenge of managing and improving the software process has come to the forefront of software engineering research. In response, new methods and tools for supporting various aspects of the software process have been devised. Many of the technologies, including process automation <ref> [8, 9, 27, 45, 50] </ref>, process analysis [33, 34, 37, 47], and process evolution [7, 35], assume the existence of some sort of formal model of a process in order for those technologies to be applied.
Reference: [10] <author> P. Bates. </author> <title> Debugging heterogenous systems using event-based models of behavior. </title> <booktitle> In Proceedings of a Workshop on Parallel and Distributed Debugging, </booktitle> <pages> pages 11-22. </pages> <publisher> ACM Press, </publisher> <month> January </month> <year> 1989. </year>
Reference-contexts: The use of event data to characterize behavior is already widely accepted in other areas of software engineering, such as program visualization [42], concurrent-system analysis [6], and distributed debugging <ref> [10, 25] </ref>. The "instant" of an event is relative to the time granularity that is needed or desired; thus, certain activities that are of short duration relative to the time granularity are represented as a single event.
Reference: [11] <author> A.W. Biermann and J.A. Feldman. </author> <title> On the Synthesis of Finite State Machines from Samples of Their Behavior. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 21(6) </volume> <pages> 592-597, </pages> <month> June </month> <year> 1972. </year>
Reference-contexts: While 0-reversible languages can be inferred in near-linear time, higher k values cause the algorithm to be cubic in the length of the input. * Bierman and Feldman <ref> [11] </ref> describe a prefix tree method where states are merged based on k-length future behaviors. <p> The Ktail method is a purely algorithmic approach that looks at the future behavior to compute a possible current state. We modified and then implemented the theoretical description given by Biermann and Feldman <ref> [11] </ref> to make it less dependent on multiple sequences and to extended it to handle noisy data. In addition, we added post-analysis steps that remove some common overly complex constructs that the basic algorithm tends to leave in a discovered model. 7 ABCABCBACBACABCBACBACABCBACABCABCBACBA 3. <p> The plethora of tuning parameters and the lack of guidelines in setting them are drawbacks as well. 5.2 Ktail The next method is purely algorithmic and based on work by Biermann and Feldman <ref> [11] </ref>. Their formulation is given in terms of sample strings and output values for the FSM. Our formulation of this algorithm does not make use of the output values and is thus presented as just operating on the sample strings themselves.
Reference: [12] <author> G.A. Bolcer and R.N. Taylor. Endeavors: </author> <title> A Process System Integration Infrastructure. </title> <booktitle> In Proceedings of the Fourth International Conference on the Software Process, </booktitle> <pages> pages 76-85. </pages> <publisher> IEEE Computer Society, </publisher> <month> December </month> <year> 1996. </year>
Reference-contexts: The discovered models can also be saved to a file for use by other tools. One such tool that can read Balboa process model files is Endeavors, which is a process model design and execution tool developed by a research group from the University of California at Irvine <ref> [12] </ref>. 25 9 A Trial Use of DaGama The utility of DaGama was readily apparent in an industrial case study conducted at AT&T Bell Laboratories (now Lucent Technologies) [22]. In this section we briefly review how DaGama was employed in the study.
Reference: [13] <author> M.G. Bradac, D.E. Perry, and L.G. Votta. </author> <title> Prototyping a Process Monitoring Experiment. </title> <journal> IEEE Transactions on Software Engineering, </journal> 20(10) 774-784, October 1994. 
Reference-contexts: Hence, just as for any other data analysis technique, the results obtained by discovery methods strongly depend upon the content and quality of the data that are collected. This issue is currently being investigated in the domain of software process <ref> [13, 22, 52, 53] </ref>. 2 3 Problem Statement and Approach Our goal in this work is to use event data, in the form of an event stream, collected from a software process execution to infer a formal model of the behavior of the process.
Reference: [14] <author> D. Brand and P. Zafiropulo. </author> <title> On Communicating Finite-State Machines. </title> <journal> Journal of the ACM, </journal> <volume> 32(2) </volume> <pages> 323-342, </pages> <year> 1983. </year>
Reference-contexts: Once that is done, we can account for the concurrency by extending the model to, for example, communicating finite state machines <ref> [14] </ref>. 17 One approach is to make use of event attributes, such as the identifier of the agent and/or artifact associated with an event. We could preprocess an event stream to separate it into substreams involving particular agents or artifacts and then apply the methods to those individual substreams.
Reference: [15] <author> A. Brazma. </author> <title> Algorithmic Learning Theory, </title> <booktitle> volume 872 of Lecture Notes in Artificial Intelligence (subseries of LNCS), </booktitle> <pages> pages 260-271. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: In this sense, these algorithms make a pass through a sequence, find repetitive patterns, replace all occurrences with the regular expression, then start all over until it is decided that they are done. Two examples are the following. * Brazma et al. <ref> [15, 16, 17] </ref> have been able to construct fast algorithms that learn restricted regular expressions. The restrictions they place on the inferred regular expressions are that both the selection operator and the nesting level of iterations are limited and fixed.
Reference: [16] <author> A. Brazma and K. Cerans. </author> <title> Algorithmic Learning Theory, </title> <booktitle> volume 872 of Lecture Notes in Artificial Intelligence (subseries of LNCS), </booktitle> <pages> pages 76-90. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: In this sense, these algorithms make a pass through a sequence, find repetitive patterns, replace all occurrences with the regular expression, then start all over until it is decided that they are done. Two examples are the following. * Brazma et al. <ref> [15, 16, 17] </ref> have been able to construct fast algorithms that learn restricted regular expressions. The restrictions they place on the inferred regular expressions are that both the selection operator and the nesting level of iterations are limited and fixed.
Reference: [17] <author> A. Brazma, I. Jonassen, I. Eidhammer, and D. Gilbert. </author> <title> Approaches to the automatic discovery of patterns in biosequences. </title> <type> Technical Report TCU/CS/1995/18, </type> <institution> City University (London), </institution> <month> December </month> <year> 1995. </year>
Reference-contexts: In this sense, these algorithms make a pass through a sequence, find repetitive patterns, replace all occurrences with the regular expression, then start all over until it is decided that they are done. Two examples are the following. * Brazma et al. <ref> [15, 16, 17] </ref> have been able to construct fast algorithms that learn restricted regular expressions. The restrictions they place on the inferred regular expressions are that both the selection operator and the nesting level of iterations are limited and fixed.
Reference: [18] <author> R.C. Carrasco and J. Oncina. </author> <title> Grammatical Inference and Applications, </title> <booktitle> volume 862 of Lecture Notes in Artificial Intelligence (subseries of LNCS), </booktitle> <pages> pages 139-152. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1994. </year> <month> 30 </month>
Reference-contexts: Although their method is not directly described as prefix tree merging, it does fall into this class. * Carrasco and Oncina <ref> [18] </ref> describe a statistical method for learning stochastic regular languages, based on state merging from a prefix tree, where states are merged based on statistical likelihood calculations.
Reference: [19] <author> J. Carrol and D. </author> <title> Long. Theory of Finite Automata. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1989. </year>
Reference-contexts: What we seek to infer from the data are recurring patterns of behavior, specifically those involving sequencing, selection, and iteration. For our purposes, finite state machines (FSMs) provide a good starting point for expressing those patterns. By FSMs we mean a nondeterministic, transition-labeled state machine <ref> [19] </ref>. One could consider choosing a more powerful representation than finite state machines, such as push-down automata or even Petri nets. The primary argument against this is that the more powerful the representation, the more complex the discovery problem.
Reference: [20] <author> A. Castellanos, I. Galiano, and E. Vidal. </author> <title> Grammatical Inference and Applications, </title> <booktitle> volume 862 of Lecture Notes in Artificial Intelligence (subseries of LNCS), </booktitle> <pages> pages 93-105. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: Running time is maximally n 3 ; however they claim actual times are near-linear. * Castellanos et al. <ref> [20] </ref> apply a method of this class to learning natural language translators. Another class of techniques is directed more towards iteratively building up a regular expression from the sequences, and then translating that into an FSM (if necessary).
Reference: [21] <author> J.E. Cook. </author> <title> Process Discovery and Validation through Event-Data Analysis. </title> <type> Technical Report CU-CS-817-96, </type> <institution> University of Colorado, University of Colorado, Boulder, Colorado, </institution> <month> November </month> <year> 1996. </year>
Reference-contexts: In other words, the data describing the behavior of a process are viewed as sentences in some language; the grammar of that language is then the formal model of the process. The methods have been implemented as a tool operating on process data sets <ref> [21] </ref> and has been successfully employed in an industrial case study [22]. Although the methods are automated, they still require guidance from a process engineer who is at least somewhat familiar with the particular process under study. <p> DaGama is fit into the Balboa data analysis framework <ref> [21] </ref>, which provides standard data and Number of Markov Ktail Events Time (s) Size (KB) Time (s) Size (KB) 20 1.03 432 1.51 426 60 13.90 1304 2.24 433 Table 4: Time and Space Requirements Versus Number of Events. 24 user interface management capabilities.
Reference: [22] <author> J.E. Cook, L.G. Votta, and A.L. Wolf. </author> <title> A Methodology for Cost-Effective Analysis of In-Place Software Processes. </title> <type> Technical Report CU-CS-825-97, </type> <institution> University of Colorado, University of Colorado, Boulder, Colorado, </institution> <month> January </month> <year> 1997. </year>
Reference-contexts: The methods have been implemented as a tool operating on process data sets [21] and has been successfully employed in an industrial case study <ref> [22] </ref>. Although the methods are automated, they still require guidance from a process engineer who is at least somewhat familiar with the particular process under study. This guidance comes in the form of tuning parameters built into the methods, and from the selection and application of the event data. <p> Hence, just as for any other data analysis technique, the results obtained by discovery methods strongly depend upon the content and quality of the data that are collected. This issue is currently being investigated in the domain of software process <ref> [13, 22, 52, 53] </ref>. 2 3 Problem Statement and Approach Our goal in this work is to use event data, in the form of an event stream, collected from a software process execution to infer a formal model of the behavior of the process. <p> More powerful representations than FSMs are arguably better suited for prescribing a software process. On the other hand, our experience has shown that FSMs are quite convenient and sufficiently powerful for describing historical patterns of actual behavior <ref> [22] </ref>. The drawback to FSMs is that they have no inherent ability to model concurrency. We address this issue in Section 5.5. To develop our technique, we have cast the process discovery problem in terms of another, previously investigated FSM discovery problem. <p> Endeavors, which is a process model design and execution tool developed by a research group from the University of California at Irvine [12]. 25 9 A Trial Use of DaGama The utility of DaGama was readily apparent in an industrial case study conducted at AT&T Bell Laboratories (now Lucent Technologies) <ref> [22] </ref>. In this section we briefly review how DaGama was employed in the study. The study focused on a change request process for a large telecommunications software system.
Reference: [23] <author> J.E. Cook and A.L. Wolf. </author> <title> Toward Metrics for Process Validation. </title> <booktitle> In Proceedings of the Third International Conference on the Software Process, </booktitle> <pages> pages 33-44. </pages> <publisher> IEEE Computer Society, </publisher> <month> October </month> <year> 1994. </year>
Reference-contexts: The goal of the study was to understand whether we could statistically identify process behaviors that correlated with the acceptance or rejection of the fix. The correlation of behaviors was performed by using a process validation technique <ref> [23] </ref>. The technique compares event data collected from a process to a formal model of the process in an effort to measure the deviation of actual process executions from the intended execution.
Reference: [24] <author> J.E. Cook and A.L. Wolf. </author> <title> Automating Process Discovery through Event-Data Analysis. </title> <booktitle> In Proceedings of the 17th International Conference on Software Engineering, </booktitle> <pages> pages 73-82. </pages> <institution> Association for Computer Machinery, </institution> <month> April </month> <year> 1995. </year>
Reference-contexts: In that vein, we have explored methods for automatically deriving a formal model of a process from basic event data collected on the process <ref> [24] </ref>. We term this form of data analysis process discovery, because inherent in every project is a process (whether known or unknown, whether good or bad, and whether stable or erratic) and for every process there is some model that can be devised to describe it.
Reference: [25] <author> J. Cuny, G. Forman, A. Hough, J. Kundu, C. Lin, L. Snyder, and D. Stemple. </author> <title> The adriane debugger: Scalable application of event-based abstraction. </title> <booktitle> In Proceedings of the ACM/ONR Workshop on Parallel and Distributed Debugging, </booktitle> <pages> pages 85-95. </pages> <publisher> ACM Press, </publisher> <month> May </month> <year> 1993. </year>
Reference-contexts: The use of event data to characterize behavior is already widely accepted in other areas of software engineering, such as program visualization [42], concurrent-system analysis [6], and distributed debugging <ref> [10, 25] </ref>. The "instant" of an event is relative to the time granularity that is needed or desired; thus, certain activities that are of short duration relative to the time granularity are represented as a single event.
Reference: [26] <author> S. Das and M.C. Mozer. </author> <title> A Unified Gradient-Descent/Clustering Architecture for Finite State Machine Induction. </title> <booktitle> In Proceedings of the 1993 Conference, number 6 in Advances in Neural Information Processing Systems, </booktitle> <pages> pages 19-26. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: Miclet's survey [43] describes several other techniques for inferring (N)DFAs from positive samples, and is, in general, a good reference for practical inference techniques. 4.4 Inference Using Neural Nets The neural network community has also looked at the problem of grammar inference from positive samples <ref> [26, 54] </ref>. The various methods all consist of defining a recurrent network architecture, and then analyzing the hidden neuron activity to discover the states and transitions for the resulting grammar. The difference among these methods is how they inspect the hidden neurons to infer state information. <p> The methods can be characterized by the way in which they examine those samples. 1. The RNet method is a purely statistical (neural network) approach that looks at the past behavior to characterize a state. For this method we have extended an implementation by Das and Mozer <ref> [26] </ref>. Our extensions allow this method to handle more event types (theirs was restricted to two), and enable the easier extraction of the discovered model from the net. 2. The Ktail method is a purely algorithmic approach that looks at the future behavior to compute a possible current state. <p> RNet was adapted from an implementation by Das and Mozer <ref> [26] </ref>, while Markov and Ktail were implemented especially for DaGama. All three methods produce FSMs in a format compatible with the dot graph layout system [39]. Thus, DaGama provides an end-to-end solution, from event data input to a visual display of the discovered process model.
Reference: [27] <author> W. Deiters and V. Gruhn. </author> <title> Managing Software Processes in the Environment MELMAC. </title> <booktitle> In SIGSOFT '90: Proceedings of the Fourth Symposium on Software Development Environments, </booktitle> <pages> pages 193-205. </pages> <booktitle> ACM SIGSOFT, </booktitle> <month> December </month> <year> 1990. </year>
Reference-contexts: 1 Introduction The challenge of managing and improving the software process has come to the forefront of software engineering research. In response, new methods and tools for supporting various aspects of the software process have been devised. Many of the technologies, including process automation <ref> [8, 9, 27, 45, 50] </ref>, process analysis [33, 34, 37, 47], and process evolution [7, 35], assume the existence of some sort of formal model of a process in order for those technologies to be applied.
Reference: [28] <author> K. El Emam, N. Moukheiber, and N.H. Madhavji. </author> <title> An evaluation of the G/Q/M method. </title> <type> Technical Report MCGILL/SE-94-11, </type> <institution> McGill University, </institution> <year> 1994. </year>
Reference-contexts: This work is more along the lines of a process post-mortem, to analyze by discussion the changes that a process should undergo for the next cycle. 3. Madhavji et al. <ref> [28] </ref> describe a method for eliciting process models from currently executing processes. The method is basically a plan for how a person would enter a development organization, understand their process, and describe the process in a formal way. There is no notion of automation or tool support.
Reference: [29] <author> P.K. Garg and S. Bhansali. </author> <title> Process programming by hindsight. </title> <booktitle> In Proceedings of the 14th International Conference on Software Engineering, </booktitle> <pages> pages 280-293. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> May </month> <year> 1992. </year>
Reference-contexts: We are unaware of any work, other than our own, aimed at investigating techniques for discovering process models from process execution data. Nevertheless, there are three efforts that are at least somewhat related. 28 1. Garg and Bhansali <ref> [29] </ref> describe a method using explanation-based learning to discover as-pects and fragments of the underlying process model from process history data and rules of operations and their effects. This work centers on using a rule base and goals to derive a generalized execution flow from a specific process history.
Reference: [30] <author> P.K. Garg, M. Jazayeri, </author> <title> and M.L. Creech. A meta-process for software reuse, process discovery and evolution. </title> <booktitle> In Proceedings of the 6th International Workshop on Software Reuse, </booktitle> <pages> page [need pages], </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: This work centers on using a rule base and goals to derive a generalized execution flow from a specific process history. By having enough rules, they showed that a complete and correct process fragment could be generated from execution data. 2. Garg et al. <ref> [30] </ref> employ process history analysis, mostly human-centered data validation and analysis, in the context of a meta-process for creating and validating domain specific process and software kits.
Reference: [31] <author> E.M. Gold. </author> <title> Language identification in the limit. </title> <journal> Information and Control, </journal> <volume> 10 </volume> <pages> 447-474, </pages> <year> 1967. </year>
Reference-contexts: The grammar inference problem can now be stated formally as follows. Given some presentation of all or part of fi L , infer a grammar G describing L. 4.2 Previous Complexity Results Gold's "identification in the limit" was the first framework for analyzing the grammar inference problem <ref> [31] </ref>. Identifying in the limit frames the problem in terms of looking at the complete presentation of fi L .
Reference: [32] <author> E.M. Gold. </author> <title> Complexity of automatic identification from given data. </title> <journal> Information and Control, </journal> <volume> 37 </volume> <pages> 302-320, </pages> <year> 1978. </year>
Reference-contexts: If, after some finite presentation of pairs from fi L , the algorithm guesses G correctly, and does not change its guess as the presentation continues, then the algorithm is said to identify G in the limit. Gold <ref> [32] </ref> showed that finding a deterministic finite state automata (DFA) having a minimum number of states, given a presentation of fi L , is NP-hard and, moreover, that it is impossible given just a positive presentation using L . <p> The simplest way to achieve minimality, on the other hand, is to create a single state with self-transitions for each possible input token. As mentioned in Section 4, Gold <ref> [32] </ref> showed that both positive and negative samples are required to construct an accurate and minimal model. Moreover, the samples must be complete in that they fully cover the possible inputs.
Reference: [33] <author> R.M. Greenwood. </author> <title> Using CSP and System Dynamics as Process Engineering Tools. </title> <booktitle> In Proceedings of the Second European Workshop on Software Process Technology, number 635 in Lecture Notes in Computer Science, </booktitle> <pages> pages 138-145. </pages> <publisher> Springer-Verlag, </publisher> <month> September </month> <year> 1992. </year>
Reference-contexts: In response, new methods and tools for supporting various aspects of the software process have been devised. Many of the technologies, including process automation [8, 9, 27, 45, 50], process analysis <ref> [33, 34, 37, 47] </ref>, and process evolution [7, 35], assume the existence of some sort of formal model of a process in order for those technologies to be applied.
Reference: [34] <author> V. Gruhn and R. Jegelka. </author> <title> An Evaluation of FUNSOFT Nets. </title> <booktitle> In Proceedings of the Second European Workshop on Software Process Technology, number 635 in Lecture Notes in Computer Science, </booktitle> <pages> pages 196-214. </pages> <publisher> Springer-Verlag, </publisher> <month> September </month> <year> 1992. </year>
Reference-contexts: In response, new methods and tools for supporting various aspects of the software process have been devised. Many of the technologies, including process automation [8, 9, 27, 45, 50], process analysis <ref> [33, 34, 37, 47] </ref>, and process evolution [7, 35], assume the existence of some sort of formal model of a process in order for those technologies to be applied.
Reference: [35] <author> M.L. Jaccheri and R. Conradi. </author> <title> Techniques for Process Model Evolution in EPOS. </title> <journal> IEEE Transactions on Software Engineering, </journal> 19(12) 1145-1156, December 1993. 
Reference-contexts: In response, new methods and tools for supporting various aspects of the software process have been devised. Many of the technologies, including process automation [8, 9, 27, 45, 50], process analysis [33, 34, 37, 47], and process evolution <ref> [7, 35] </ref>, assume the existence of some sort of formal model of a process in order for those technologies to be applied. The need to develop a formal model as a prerequisite to using a new technology is a daunting prospect to the managers of large, on-going projects.
Reference: [36] <author> S. Jain and A. Sharma. </author> <title> Algorithmic Learning Theory, </title> <booktitle> volume 872 of Lecture Notes in Artificial Intelligence (subseries of LNCS), </booktitle> <pages> pages 349-364. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1994. </year> <month> 31 </month>
Reference-contexts: The survey by Angluin and Smith [5] is a broad look at inductive inference learning. Pitt's survey [46] is a good starting point for understanding the theoretical complexities involved in learning regular grammars. Further theoretical work on language learning is also available <ref> [2, 36, 40, 41, 48] </ref>. Angluin and Smith acknowledge the gap between theoretical results and practical application of inference methods: The most significant open problem in the field is perhaps not any specific technical question, but the gap between abstract and concrete results.
Reference: [37] <author> M.I. Kellner. </author> <title> Software Process Modeling Support for Management Planning and Control. </title> <booktitle> In Proceedings of the First International Conference on the Software Process, </booktitle> <pages> pages 8-28. </pages> <publisher> IEEE Computer Society, </publisher> <month> October </month> <year> 1991. </year>
Reference-contexts: In response, new methods and tools for supporting various aspects of the software process have been devised. Many of the technologies, including process automation [8, 9, 27, 45, 50], process analysis <ref> [33, 34, 37, 47] </ref>, and process evolution [7, 35], assume the existence of some sort of formal model of a process in order for those technologies to be applied. <p> If unit testing fails, the process loops back to the code modification subprocess (since the design modification is assumed to be correct) to redo the previous few process steps. This loop continues until unit testing succeeds and the process completes. Statemate solution <ref> [37] </ref>. The idea is to see how well the methods can reproduce this ideal FSM and, thereby, discover the process model. The data used in the exercise is manually derived from the ideal FSM.
Reference: [38] <author> M.I. Kellner, P.H. Feiler, A. Finkelstein, T. Katayama, L.J. Osterweil, M.H. Penedo, and H.D. Rombach. </author> <title> Software Process Modeling Example Problem. </title> <booktitle> In Proceedings of the 6th International Software Process Workshop, </booktitle> <pages> pages 19-29, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: Section 4 provides needed background on grammar inference. The discovery methods themselves are then described in Section 5. The methods are illustrated in Section 6 using the process community's standard example, the ISPW 6/7 process problem <ref> [38] </ref>. 1 Section 7 presents a comparative evaluation of the methods. Section 8 describes DaGama, the tool implementing the discovery methods. The application of the methods in an industrial case study is reviewed in Section 9. <p> The process is taken from the ISPW 6/7 process problem <ref> [38] </ref>. At a high level, the ISPW 6/7 process proceeds as follows. A change order from the Change Control Board (CCB) triggers the start of the process. The design modification subprocess is scheduled and performed, leading to the scheduling and performing of the code modification subprocess.
Reference: [39] <author> E. </author> <title> Koutsofios and S.C. North. Drawing Graphs with Dot. </title> <institution> AT&T Bell Laboratories, </institution> <month> October </month> <year> 1993. </year>
Reference-contexts: RNet was adapted from an implementation by Das and Mozer [26], while Markov and Ktail were implemented especially for DaGama. All three methods produce FSMs in a format compatible with the dot graph layout system <ref> [39] </ref>. Thus, DaGama provides an end-to-end solution, from event data input to a visual display of the discovered process model. The user interface of DaGama is shown in Figure 12.
Reference: [40] <author> S. Lange, J. Nessel, and R. Wiehagen. </author> <title> Algorithmic Learning Theory, </title> <booktitle> volume 872 of Lecture Notes in Artificial Intelligence (subseries of LNCS), </booktitle> <pages> pages 423-437. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: The survey by Angluin and Smith [5] is a broad look at inductive inference learning. Pitt's survey [46] is a good starting point for understanding the theoretical complexities involved in learning regular grammars. Further theoretical work on language learning is also available <ref> [2, 36, 40, 41, 48] </ref>. Angluin and Smith acknowledge the gap between theoretical results and practical application of inference methods: The most significant open problem in the field is perhaps not any specific technical question, but the gap between abstract and concrete results.
Reference: [41] <author> S. Lange and P. Watson. </author> <title> Algorithmic Learning Theory, </title> <booktitle> volume 872 of Lecture Notes in Artificial Intelligence (subseries of LNCS), </booktitle> <pages> pages 438-452. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: The survey by Angluin and Smith [5] is a broad look at inductive inference learning. Pitt's survey [46] is a good starting point for understanding the theoretical complexities involved in learning regular grammars. Further theoretical work on language learning is also available <ref> [2, 36, 40, 41, 48] </ref>. Angluin and Smith acknowledge the gap between theoretical results and practical application of inference methods: The most significant open problem in the field is perhaps not any specific technical question, but the gap between abstract and concrete results.
Reference: [42] <author> R.J. LeBlanc and A.D. Robbins. </author> <title> Event-Driven Monitoring of Distributed Programs. </title> <booktitle> In Proceedings of the Fifth International Conference on Distributed Computing Systems, </booktitle> <pages> pages 515-522. </pages> <publisher> IEEE Computer Society, </publisher> <month> May </month> <year> 1985. </year>
Reference-contexts: The use of event data to characterize behavior is already widely accepted in other areas of software engineering, such as program visualization <ref> [42] </ref>, concurrent-system analysis [6], and distributed debugging [10, 25]. The "instant" of an event is relative to the time granularity that is needed or desired; thus, certain activities that are of short duration relative to the time granularity are represented as a single event.
Reference: [43] <author> L. Miclet. </author> <title> Syntactic and Structural Pattern Recognition: Theory and Applications, </title> <booktitle> volume 7 of Series in Computer Science, chapter 9: Grammatical Inference, </booktitle> <pages> pages 237-290. </pages> <publisher> World Scientific, </publisher> <address> New Jersey, </address> <year> 1990. </year>
Reference-contexts: Moreover, they have extended this work to handle noisy strings by treating the noise as an edit distance problem, although currently they only allow for a single-token edit, not multi-token gaps. Their work is geared towards analyzing biosequence (i.e., DNA/RNA and protein) data. * Miclet <ref> [43] </ref> describes a method called the uv k w algorithm, where each pass looks for repetitions of substrings (the v k ), chooses the best candidate, and replaces it with an iteration expression. The next pass is then started on the reduced string. <p> The next pass is then started on the reduced string. This method is well-adapted for loops, but has problems with the selection operator as well. Miclet's survey <ref> [43] </ref> describes several other techniques for inferring (N)DFAs from positive samples, and is, in general, a good reference for practical inference techniques. 4.4 Inference Using Neural Nets The neural network community has also looked at the problem of grammar inference from positive samples [26, 54].
Reference: [44] <author> L. Miclet and J. Quinqueton. </author> <title> Syntactic and Structural Pattern Recognition, </title> <booktitle> volume 45 of NATO ASI Series F: Computer and Systems Sciences, </booktitle> <pages> pages 153-171. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: The method uses the concept of Markov models to find the most probable event sequence productions, and algorithmically converts those probabilities into states and state transitions. Although 13 our method is new, there is previous work that has used similar methods. For example, Miclet and Quinqueton <ref> [44] </ref> use sequence probabilities to create FSM recognizers of protein sequences, and then use the Markov models to predict the center point of new protein sequences.
Reference: [45] <author> B. Peuschel and W. Schafer. </author> <title> Concepts and Implementation of a Rule-based Process Engine. </title> <booktitle> In Proceedings of the 14th International Conference on Software Engineering, </booktitle> <pages> pages 262-279. </pages> <publisher> IEEE Computer Society, </publisher> <month> May </month> <year> 1992. </year>
Reference-contexts: 1 Introduction The challenge of managing and improving the software process has come to the forefront of software engineering research. In response, new methods and tools for supporting various aspects of the software process have been devised. Many of the technologies, including process automation <ref> [8, 9, 27, 45, 50] </ref>, process analysis [33, 34, 37, 47], and process evolution [7, 35], assume the existence of some sort of formal model of a process in order for those technologies to be applied.
Reference: [46] <author> L. Pitt. </author> <title> Analogical and Inductive Inference, </title> <booktitle> volume 397 of Lecture Notes in Artificial Intelligence (subseries of LNCS), </booktitle> <pages> pages 18-44. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1989. </year>
Reference-contexts: The PAC model for learning concepts from examples was proposed by Valiant [51]. PAC stands for "Probably Approximately Correct". Probably is defined as within some probability 1 ffi, and 1 This presentation is consistent with others in the field <ref> [5, 46, 49] </ref>. 2 L = fl is not a very interesting language. 4 approximately is defined as within some 1* of the correct solution. <p> Results for grammars, including DFAs, are more negative, but less definite. As Pitt reports: If DFAs are polynomially approximately predictable, then there is a probabilistic poly nomial time algorithm for inverting the RSA encryption function, for factoring Blum integers, and for deciding quadratic residues <ref> [46] </ref>. Angluin [4] phrases the learning problem in terms of an oracle. An algorithm can make a fixed set of queries to the oracle, the basic two being "Is this string accepted by the correct grammar?" and "Is this grammar equivalent to the correct grammar?". <p> There is much work describing the computational complexities of various learning paradigms and classes of languages or formulas. The survey by Angluin and Smith [5] is a broad look at inductive inference learning. Pitt's survey <ref> [46] </ref> is a good starting point for understanding the theoretical complexities involved in learning regular grammars. Further theoretical work on language learning is also available [2, 36, 40, 41, 48].
Reference: [47] <author> M. Saeki, T. Kaneko, and M. Sakamoto. </author> <title> A Method for Software Process Modeling and Description Using LOTOS. </title> <booktitle> In Proceedings of the First International Conference on the Software Process, </booktitle> <pages> pages 90-104. </pages> <publisher> IEEE Computer Society, </publisher> <month> October </month> <year> 1991. </year>
Reference-contexts: In response, new methods and tools for supporting various aspects of the software process have been devised. Many of the technologies, including process automation [8, 9, 27, 45, 50], process analysis <ref> [33, 34, 37, 47] </ref>, and process evolution [7, 35], assume the existence of some sort of formal model of a process in order for those technologies to be applied.
Reference: [48] <author> Y. Sakakibara. </author> <title> Efficient learning of context-free grammars from positive structural examples. </title> <journal> Information and Computation, </journal> <volume> 97 </volume> <pages> 23-60, </pages> <year> 1992. </year>
Reference-contexts: The survey by Angluin and Smith [5] is a broad look at inductive inference learning. Pitt's survey [46] is a good starting point for understanding the theoretical complexities involved in learning regular grammars. Further theoretical work on language learning is also available <ref> [2, 36, 40, 41, 48] </ref>. Angluin and Smith acknowledge the gap between theoretical results and practical application of inference methods: The most significant open problem in the field is perhaps not any specific technical question, but the gap between abstract and concrete results.
Reference: [49] <author> Y. Sakakibara. </author> <title> Grammatical inference: An old and new paradigm. </title> <type> Technical Report ISIS-RR-95-9E, </type> <institution> Institute for Social Information Science, </institution> <month> September </month> <year> 1995. </year>
Reference-contexts: The PAC model for learning concepts from examples was proposed by Valiant [51]. PAC stands for "Probably Approximately Correct". Probably is defined as within some probability 1 ffi, and 1 This presentation is consistent with others in the field <ref> [5, 46, 49] </ref>. 2 L = fl is not a very interesting language. 4 approximately is defined as within some 1* of the correct solution.
Reference: [50] <author> S.M. Sutton, Jr., H. Ziv, D. Heimbigner, H.E. Yessayan, M. Maybee, , L.J. Osterweil, and X. Song. </author> <title> Programming a Software Requirements-specification Process. </title> <booktitle> In Proceedings of the First International Conference on the Software Process, </booktitle> <pages> pages 68-89. </pages> <publisher> IEEE Computer Society, </publisher> <month> October </month> <year> 1991. </year>
Reference-contexts: 1 Introduction The challenge of managing and improving the software process has come to the forefront of software engineering research. In response, new methods and tools for supporting various aspects of the software process have been devised. Many of the technologies, including process automation <ref> [8, 9, 27, 45, 50] </ref>, process analysis [33, 34, 37, 47], and process evolution [7, 35], assume the existence of some sort of formal model of a process in order for those technologies to be applied.
Reference: [51] <author> L.G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27 </volume> <pages> 1134-1142, </pages> <year> 1984. </year>
Reference-contexts: The PAC model for learning concepts from examples was proposed by Valiant <ref> [51] </ref>. PAC stands for "Probably Approximately Correct".
Reference: [52] <author> L.G. Votta and M. Zajac. </author> <title> Design Process Improvement Case Study Using Process Waiver Data. </title> <booktitle> In Proceedings of the Fifth European Software Engineering Conference (ESEC'95), number 989 in Lecture Notes in Computer Science, </booktitle> <pages> pages 44-58. </pages> <publisher> Springer-Verlag, </publisher> <month> September </month> <year> 1995. </year>
Reference-contexts: Hence, just as for any other data analysis technique, the results obtained by discovery methods strongly depend upon the content and quality of the data that are collected. This issue is currently being investigated in the domain of software process <ref> [13, 22, 52, 53] </ref>. 2 3 Problem Statement and Approach Our goal in this work is to use event data, in the form of an event stream, collected from a software process execution to infer a formal model of the behavior of the process.
Reference: [53] <author> A.L. Wolf and D.S. Rosenblum. </author> <title> A Study in Software Process Data Capture and Analysis. </title> <booktitle> In Proceedings of the Second International Conference on the Software Process, </booktitle> <pages> pages 115-124. </pages> <publisher> IEEE Computer Society, </publisher> <month> February </month> <year> 1993. </year>
Reference-contexts: Following Wolf and Rosenblum <ref> [53] </ref>, we use an event-based model of process actions, where an event is used to characterize the dynamic behavior of a process in terms of identifiable, instantaneous actions, such as invoking a development tool or deciding upon the next activity to be performed. <p> Hence, just as for any other data analysis technique, the results obtained by discovery methods strongly depend upon the content and quality of the data that are collected. This issue is currently being investigated in the domain of software process <ref> [13, 22, 52, 53] </ref>. 2 3 Problem Statement and Approach Our goal in this work is to use event data, in the form of an event stream, collected from a software process execution to infer a formal model of the behavior of the process.
Reference: [54] <author> Z. Zeng, R.M. Goodman, and P. Smyth. </author> <title> Learning finite state machines with self-clustering recurrent networks. </title> <journal> Nueral Computation, </journal> <volume> 5 </volume> <pages> 976-990, </pages> <year> 1993. </year> <month> 32 </month>
Reference-contexts: Miclet's survey [43] describes several other techniques for inferring (N)DFAs from positive samples, and is, in general, a good reference for practical inference techniques. 4.4 Inference Using Neural Nets The neural network community has also looked at the problem of grammar inference from positive samples <ref> [26, 54] </ref>. The various methods all consist of defining a recurrent network architecture, and then analyzing the hidden neuron activity to discover the states and transitions for the resulting grammar. The difference among these methods is how they inspect the hidden neurons to infer state information.
References-found: 54

