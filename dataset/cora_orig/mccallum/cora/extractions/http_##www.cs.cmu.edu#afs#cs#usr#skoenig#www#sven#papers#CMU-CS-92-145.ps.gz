URL: http://www.cs.cmu.edu/afs/cs/usr/skoenig/www/sven/papers/CMU-CS-92-145.ps.gz
Refering-URL: http://www.cs.cmu.edu/afs/cs/usr/skoenig/www/sven/abstracts/CMU-CS-92-145.html
Root-URL: 
Title: The Complexity of Real-Time Search  
Author: Sven Koenig 
Note: This research was supported in part by NASA under contract NAGW-1175. The views and conclusions contained in this document are those of the author and should not be interpreted as representing the official policies, either expressed or implied, of NASA or the U.S. government.  
Address: Pittsburgh, PA 15213  
Affiliation: School of Computer Science Carnegie Mellon University  
Date: April 1992  
Pubnum: CMU-CS-92-145  
Abstract: In this paper, we develop new LRTA*-based algorithms for a variety of tasks, and analyze their complexity. The LRTA* algorithm is a real-time search algorithm developed by Korf. It can be used to reach a stationary or moving goal state or to identify all shortest paths from a given start state to a stationary goal state, if the algorithm is reset to the start state when it reaches a goal state. Our algorithms have a search horizon of one and require no internal memory (but must be able to store some information in the states). For example, the bi-directional LRTS algorithm determines optimal universal plans (i.e. finds all optimal paths from all states to a set of stationary goal states), even if reset actions are not available. We show that all tasks studied in this paper can be solved by such LRTA*-based algorithms with only O(n 2 ) action executions for state spaces of size n. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Andrew G. Barto, Steven J. Bradtke, and Satinder P. Singh. </author> <title> Real-time learning and control using asynchronous dynamic programming. </title> <type> Technical Report 91-57, </type> <institution> Department of Computer Science, University of Massachusetts at Amherst, </institution> <year> 1991. </year>
Reference-contexts: The complexities are increased by a factor of O (n), i.e. all tasks discussed here can be solved with O (n 3 ) action executions. The LRTS algorithm has been generalized to probabilistic domains by Barto, Bradtke, and Singh, see <ref> [1] </ref>. It would be interesting to extend the algorithms presented here and their Q-Learning counterparts to probabilistic domains, both for the case that the transition probabilities are known in advance by the algorithm and the case that the algorithm has to perform experiments to learn these probabilities.
Reference: [2] <author> Mark Boddy and Thomas Dean. </author> <title> Solving time-dependent planning problems. </title> <booktitle> In Proceedings of the IJCAI 1989, </booktitle> <pages> pages 979-984. </pages>
Reference-contexts: Then, they execute the first action of the plan, and repeat these two steps in the new state. (A longer description can be found in [5].) The longer they deliberate, the better an action they can potentially select. Revolving planners are therefore good candidates for anytime algorithms (see <ref> [3, 2] </ref>) and can commit to an action very quickly by decreasing the deliberation time to a minimum. <p> These results follow directly from theorem 1 by calculating an upper bound of P * grid worlds (see <ref> [2] </ref>) without obstacles For a one-dimensional grid world (that is, a corridor where one can go to the left or right, except at the ends of the corridor where one has only one choice) the worst-case complexity is O (n).
Reference: [3] <author> Thomas Dean and Mark Boddy. </author> <title> An analysis of time-dependent planning. </title> <booktitle> In Proceedings of the AAAI 1988, </booktitle> <pages> pages 49-54. </pages>
Reference-contexts: Then, they execute the first action of the plan, and repeat these two steps in the new state. (A longer description can be found in [5].) The longer they deliberate, the better an action they can potentially select. Revolving planners are therefore good candidates for anytime algorithms (see <ref> [3, 2] </ref>) and can commit to an action very quickly by decreasing the deliberation time to a minimum.
Reference: [4] <author> Toru Ishida and Richard E. Korf. </author> <title> Moving target search. </title> <booktitle> In Proceedings of the IJCAI 1991, </booktitle> <pages> pages 204-210. </pages>
Reference-contexts: Korf designed various real-time search algorithms: the RTA* (Real-Time A*) algorithm, the LRTA* (Learning Real-Time A*) algorithm, and the MTS (Moving Target Search) algorithm (an application of LRTA* to chasing a moving goal, see <ref> [4] </ref>). 1 The deliberation time of these algorithms is determined by the amount of look-ahead that they perform before they commit to an action. In the following, we focus on an extreme case: LRTA* with a search horizon of one. <p> The number of action executions that are needed in the worst case could easily be derived analogously to a similar analysis of the time complexity of MTS by Korf (for the case that the target does not move), see <ref> [4] </ref>. However, Korf does not take identity actions into account, i.e. actions that do not change the state. Indeed, identity actions do not simplify experimentation and are never part of an optimal path.
Reference: [5] <author> Sven Koenig. </author> <title> Optimal probabilistic and decision-theoretic planning using markovian decision theory. </title> <type> Master's thesis, </type> <institution> University of California at Berkeley, Computer Science Department, </institution> <year> 1991. </year>
Reference-contexts: They are revolving planning methods. Revolving planners first design a partial plan. Then, they execute the first action of the plan, and repeat these two steps in the new state. (A longer description can be found in <ref> [5] </ref>.) The longer they deliberate, the better an action they can potentially select. Revolving planners are therefore good candidates for anytime algorithms (see [3, 2]) and can commit to an action very quickly by decreasing the deliberation time to a minimum.
Reference: [6] <author> Richard E. Korf. </author> <title> Real-time heuristic search: First results. </title> <booktitle> In Proceedings of the AAAI 1987, </booktitle> <pages> pages 133-138. </pages>
Reference-contexts: 1 Introduction Korf introduced the notion of real-time search in <ref> [6, 7, 8] </ref> as an alternative to off-line search. Whereas off-line search algorithms start to execute actions only after they have determined the optimal plan, real-time search algorithms (also called on-line search algorithms) commit to an action as early as possible. They are revolving planning methods.
Reference: [7] <author> Richard E. Korf. </author> <title> Real-time heuristic search: New results. </title> <booktitle> In Proceedings of the AAAI 1988, </booktitle> <pages> pages 139-144. </pages>
Reference-contexts: 1 Introduction Korf introduced the notion of real-time search in <ref> [6, 7, 8] </ref> as an alternative to off-line search. Whereas off-line search algorithms start to execute actions only after they have determined the optimal plan, real-time search algorithms (also called on-line search algorithms) commit to an action as early as possible. They are revolving planning methods.
Reference: [8] <author> Richard E. Korf. </author> <title> Real-time heuristic search. </title> <journal> Artificial Intelligence, </journal> <volume> 42(2-3):189-211, </volume> <month> 3 </month> <year> 1990. </year>
Reference-contexts: 1 Introduction Korf introduced the notion of real-time search in <ref> [6, 7, 8] </ref> as an alternative to off-line search. Whereas off-line search algorithms start to execute actions only after they have determined the optimal plan, real-time search algorithms (also called on-line search algorithms) commit to an action as early as possible. They are revolving planning methods. <p> A perfect path is optimal. Thus, the convergence result implies that, in the 7 The result for the quadratic grid world was first stated by Korf, see <ref> [8] </ref>. 8 In the following, we show how chronological backtracking search can be implemented in invertible state spaces. LRTS behaves like backtracking search, if the tie-breaking rule of the action selection step is changed such that the backtracking action has priority over every other action.
Reference: [9] <author> Joseph C. Pemberton and Richard E. Korf. </author> <title> Incremental path planning on graphs with cycles. </title> <booktitle> In Proceedings of the First Annual AI Planning Systems Conference 1992. </booktitle>
Reference-contexts: the algorithm with external memory, but the information access is restricted to the memory of the current state of the algorithm and the memories of its successors. (Such a restriction is for example realistic for 1 A short overview of other on-line graph search (path planning) methods is given in <ref> [9] </ref>, section 7. 1 an algorithm that navigates an artificial agent from a start location to a goal location.
Reference: [10] <author> Stuart Russell and Eric Wefald. </author> <title> Do the Right Thing Studies in Limited Rationality. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1991. </year>
Reference-contexts: This insight in the trade-off between deliberating and acting is of interest for designing intelligent agents that optimize overall performance (e.g. the sum of deliberation and action time), see for example <ref> [10] </ref>. We proceed as follows. First, we review the assumptions made by LRTS and bi-directional LRTS. Then, we describe LRTS and proceed step-wise from the easiest task, reaching a goal state, to increasingly more demanding tasks.
Reference: [11] <author> M. J. Schoppers. </author> <title> Universal plans for reactive robots in unpredictable environments. </title> <booktitle> In Proceedings of the IJCAI 1987, </booktitle> <pages> pages 1039-1046. </pages>
Reference-contexts: We investigate the problem to find all optimal paths from all states to a set of goal states if the algorithm has no internal memory (but can store information in the states) and has no reset actions available. This is the task of finding optimal universal plans (see <ref> [11] </ref>), a planning approach that is able to deal with uncertain action outcomes and overcomes some of the deficiencies of classical planners by using closed loop plans instead of open loop plans.
Reference: [12] <author> Christopher J. Watkins. </author> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> King's College, Cambridge University, </institution> <year> 1989. </year>
Reference-contexts: Q-Learning <ref> [12] </ref> makes slightly different assumptions. All of the results derived here can easily be adapted to the Q-learning algorithm for deterministic domains. The complexities are increased by a factor of O (n), i.e. all tasks discussed here can be solved with O (n 3 ) action executions.
Reference: [13] <author> Richard Yee. </author> <title> Abstraction in control learning. </title> <type> Technical Report 92-16, </type> <institution> Department of Computer Science, University of Massachusetts at Amherst, </institution> <year> 1992. </year> <month> 26 </month>
Reference-contexts: Then, the forward phase starts in an untagged state and ends as soon as a tagged state is reached, and vice versa for the backward phase. This method is similar to Yee's idea of organizing value propagation for dynamic programming problems, see <ref> [13] </ref>. He suggests to select those states for value updates for which the update replaces a bad value estimate by a good one. When every state is tagged, the algorithm can terminate.
References-found: 13

