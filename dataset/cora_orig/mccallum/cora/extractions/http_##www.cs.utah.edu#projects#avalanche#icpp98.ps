URL: http://www.cs.utah.edu/projects/avalanche/icpp98.ps
Refering-URL: http://www.cs.utah.edu/projects/avalanche/avalanche-publications.html
Root-URL: 
Email: E-mail: fchenchi,retrac,kuramkot,swansong@cs.utah.edu  
Title: ASCOMA An Adaptive Hybrid Shared Memory Architecture  
Author: Chen-Chi Kuo, John Carter, Ravindra Kuramkote, and Mark Swanson 
Address: Salt Lake City, UT 84112  
Affiliation: Department of Computer Science University of Utah  
Abstract: Scalable shared memory multiprocessors traditionally use either a cache coherent non-uniform memory access (CC-NUMA) or simple cache-only memory architecture (S-COMA) memory architecture. Recently, hybrid architectures that combine aspects of both CC-NUMA and S-COMA have emerged. In this paper, we present two improvements over other hybrid architectures. The first improvement is a page allocation algorithm that prefers S-COMA pages at low memory pressures. Once the local free page pool is drained, additional pages are mapped in CC-NUMA mode until they suffer sufficient remote misses to warrant upgrading to S-COMA mode. The second improvement is a page replacement algorithm that dynamically backs off the rate of page remappings from CC-NUMA to S-COMA mode at high memory pressure. This design dramatically reduces the amount of kernel overhead and the number of induced cold misses caused by needless thrashing of the page cache. The resulting hybrid architecture is called adaptive S-COMA (AS-COMA). AS-COMA exploits the best of S-COMA and CC-NUMA, performing like an S-COMA machine at low memory pressure and like a CC-NUMA machine at high memory pressure. AS-COMA outperforms CC-NUMA under almost all conditions, and outperforms other hybrid architectures by up to 17% at low memory pressure and up to 90% at high memory pressure. *Mark Swanson is now at Intel Corporation. Current email addresses: Mark R Swanson@ccm.dp.intel.com This research was supported in part by the Space and Naval Warfare Systems Command (SPAWAR) and the Advanced Research Projects Agency (ARPA), under SPAWAR contract No.#N0039-95-C-0018 and ARPA Order No.#B990. The views and conclusions contained herein are those of the authors and should not be interpreted as necessariy representing the official policies or endorsements, either expressed or implied, of DARPA, the Air Force Research Laboratory, or the US Government. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> W. Bolosky, R. Fitzgerald, and M. Scott. </author> <title> Simple but effective techniques for NUMA memory management. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating Systems Principles, pages 1931, </booktitle> <month> Dec. </month> <year> 1989. </year>
Reference-contexts: Applications that suffer a large number of conflict misses to remote data, e.g., due to the limited amount of caching of remote data, perform poorly on CC-NUMAs [3]. Unfortunately, these applications are fairly common [3, 10]. Careful page allocation <ref> [1, 7] </ref>, migration [13], or replication [13] can alleviate this problem by selecting or modifying the choice of home node for a given page of data, but these techniques have to date only been successful for read-only or non-shared pages.
Reference: [2] <author> S. Chandra, J. Larus, and A. Rogers. </author> <booktitle> Where is time spent in message-passing and shared-memory programs? In Proceedings of the 6th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 61 73, </pages> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: Benchmark Programs We used six programs to conduct our study: barnes, fft, lu, ocean, and radix from the SPLASH-2 benchmark suite [14] and em3d from a shared memory implementation of the Split-C benchmark <ref> [2] </ref>. Table 2 shows the inputs used for each test program. The column labeled Total Home pages indicates the total number of shared data pages initially allocated at all nodes.
Reference: [3] <author> B. Falsafi and D. Wood. </author> <title> Reactive NUMA: A design for unifying S-COMA and CC-NUMA. </title> <booktitle> In Proceedings of the 24th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 229240, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: In this paper, we concentrate on the second and third issues, namely reducing the frequency of remote memory accesses while ensuring that the software overhead required to do this remains modest. Previous studies have tended to ignore the impact of software overhead <ref> [3, 9, 10] </ref>, but our findings in dicate that the effect of this factor can be dramatic. Scalable shared memory multiprocessors traditionally use either a cache coherent non-uniform memory access (CC-NUMA) architecture [5, 6, 12] or a simple cache-only memory architecture (S-COMA) [10]. <p> Although this ability to cache remote data in local memory can dramatically reduce the number of remote memory operations, page management can be expensive. Recently, hybrid architectures that combine aspects of both CC-NUMA and S-COMA have emerged, such as the Wisconsin reactive CC-NUMA (R-NUMA) <ref> [3] </ref> and the USC victim cache NUMA (VC-NUMA) [9]. Intuitively, these hybrid systems attempt to map the remote pages for which there are the highest number of conflict misses to local S-COMA pages, so-called hot pages, thereby eliminating the greatest number of expensive remote operations. <p> Remote data can only be cached in the processor cache (s) or an optional remote access cache (RAC) on the DSM controller. Applications that suffer a large number of conflict misses to remote data, e.g., due to the limited amount of caching of remote data, perform poorly on CC-NUMAs <ref> [3] </ref>. Unfortunately, these applications are fairly common [3, 10]. <p> Applications that suffer a large number of conflict misses to remote data, e.g., due to the limited amount of caching of remote data, perform poorly on CC-NUMAs [3]. Unfortunately, these applications are fairly common <ref> [3, 10] </ref>. Careful page allocation [1, 7], migration [13], or replication [13] can alleviate this problem by selecting or modifying the choice of home node for a given page of data, but these techniques have to date only been successful for read-only or non-shared pages. <p> Sources of this overhead include the time spent context switching between the user application and the pageout daemon, flushing blocks from the victim page (s), and remapping pages. 2.4. Hybrid DSM Architectures Two hybrid CC-NUMA/S-COMA architectures have been proposed: R-NUMA <ref> [3] </ref> and VC-NUMA [9]. We describe these architectures in this section. The basic architecture of an R-NUMA machine [3] is that of a CC-NUMA machine. <p> Hybrid DSM Architectures Two hybrid CC-NUMA/S-COMA architectures have been proposed: R-NUMA <ref> [3] </ref> and VC-NUMA [9]. We describe these architectures in this section. The basic architecture of an R-NUMA machine [3] is that of a CC-NUMA machine. However, unlike CC-NUMA, which wastes local physical memory not required to hold home pages, R-NUMA uses this otherwise unused storage to cache frequently accessed remote pages, as in S-COMA. <p> This causes the DSM engine on the requesting node to interrupt the processor with an indication that a particular page should be remapped to a local S-COMA page. In a recent study <ref> [3] </ref>, R-NUMA's flexibility and intelligent selection of pages to map in S-COMA mode caused it to outperform the best of pure CC-NUMA and pure S-COMA by up to 37% on some applications. <p> Although R-NUMA frequently outperforms both CC-NUMA and S-COMA, it was also observed to perform as much as 57% worse on some applications <ref> [3] </ref>. This poor performance can be attributed to two problems. First, R NUMA initially maps all pages in CC-NUMA mode, and only upgrades them to S-COMA mode after some number of remote refetches occur, which introduces needless remote refetches when memory pressure is low. <p> We, therefore, model a single 8-kilobyte direct-mapped processor cache to compensate for the small size of the data sets, which is consistent with previous studies of hybrid architectures <ref> [3, 9] </ref>. We modeled a 4-bank main memory controller that can supply data from local memory in 58 cycles. The size of main memory and the amount of free memory used for page caching was varied to test the different models under varying memory pressures.
Reference: [4] <author> C.-C. Kuo, J. Carter, R. Kuramkote, and M. Swanson. AS-COMA: </author> <title> An adaptive hybrid shared memory architecture. </title> <type> Technical report, </type> <institution> University of Utah Computer Science Department, </institution> <month> March </month> <year> 1998. </year>
Reference-contexts: K-BASE: performing essential kernel operations (i.e., those required by all architectures). K-OVERHD: performing architecture-specific kernel operations, such as remapping pages and handling relocation interrupts. U-INSTR and U-LC were satisfied, can be found in the technical report version of this paper <ref> [4] </ref>. We simulated the applications across a range of memory pressures between 10% and 90%. Only one result is shown for CC-NUMA, since it is not affected by memory pressure. <p> The behavior of em3d shows the danger of focusing solely on reducing remote conflict misses <ref> [4] </ref> when designing a memory architecture. As shown in Figure 1, the performance of em3d on the hybrid architectures is quite sensitive to memory pressure. R-NUMA outperforms CC-NUMA until memory pressure approaches 70%, after which time its performance drops quickly. <p> Thus, above 70% memory pressure, R-NUMA begins to thrash and its performance degrades badly. This performance dropoff occurs even though there are significantly fewer remote conflict misses in R-NUMA than in CC-NUMA or AS-COMA <ref> [4] </ref>. The cost of constantly remapping pages between CC-NUMA and S-COMA mode and the increase in remote cold misses overwhelms the benefit of the reduced number of remote conflict misses. This behavior emphasizes the importance of detecting thrashing and reducing the rate of remappings when it occurs. <p> This results in more remote conflict/capacity misses than the other hybrid architectures, but it reduces the number of cold misses caused by flushing pages during remapping and the kernel overhead associated with handling interrupts and remapping <ref> [4] </ref>. As a result, AS-COMA outperforms VC-NUMA by 31% and R-NUMA by 53% at 90% memory pressure. Moreover, despite having only a small page cache available to it and a remote working set larger than this cache, AS-COMA outperforms CC-NUMA. Barnes exhibits very high spatial locality. <p> The source of this performance degradation is the same as in em3d and barnes increasing kernel overhead and (to a lesser degree) induced cold misses. Once again, R-NUMA induces fewer remote accesses than CC-NUMA <ref> [4] </ref>, but the kernel overhead required to support page relocation is such that R-NUMA underperforms CC-NUMA by 75% at 70% memory pressure and by almost a factor of two at 2 Note that barnes is very compute-intensive, and a problem size that can be simulated in a reasonable amount of time <p> All three applications only have a small set of hot pages, which can be easily replicated using a small page cache, or references to remote pages are so localized that the small (128-byte) RAC in our simulation was able to satisfy a high percentage of remote accesses <ref> [4] </ref>. As a result, thrashing never occurs and the various backoff schemes are not invoked. Thus, the performance of the three hybrid algorithms is almost identical.
Reference: [5] <author> J. Laudon and D. Lenoski. </author> <title> The SGI Origin: A ccNUMA highly scalable server. </title> <booktitle> In SIGARCH97, </booktitle> <pages> pages 241251, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: The designers of high-end commercial DSM systems such as the SUN UE10000 [12] and SGI Origin 2000 <ref> [5] </ref> have put considerable effort into reducing the remote memory latency by developing specialized high speed interconnects. These efforts can reduce the ratio of remote to local memory latency to as low as 2:1, but they require expensive hardware available only on high-end servers costing hundreds of thousands of dollars. <p> Previous studies have tended to ignore the impact of software overhead [3, 9, 10], but our findings in dicate that the effect of this factor can be dramatic. Scalable shared memory multiprocessors traditionally use either a cache coherent non-uniform memory access (CC-NUMA) architecture <ref> [5, 6, 12] </ref> or a simple cache-only memory architecture (S-COMA) [10]. In a CC-NUMA, the amount of remote shared data that can be replicated on a node is limited by the size of a node's processor cache (s) and remote access cache (RAC) [6]. <p> T pagecache and T remote represent the latency of fetching a line from the local page cache or remote memory, respectively. T overhead represents the soft-ware overheads of the S-COMA and the hybrid models to support page remapping. 2.2. CC-NUMA In CC-NUMA <ref> [5, 6, 12] </ref>, the first page access on each node to a particular page causes a page fault, at which time the local TLB and page table are loaded with a page translation to the appropriate global physical page. <p> To reduce this overhead, designers of some such systems have adopted high speed interconnects to reduce T remote <ref> [5, 12] </ref>. 2.3. S-COMA In the S-COMA model [10], the DSM controller and operating system cooperate to provide access to remotely homed data. S-COMA's aggressive use of local memory to replicate remote shared data can completely eliminate N remote when the memory pressure on a node is low.
Reference: [6] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W.-D. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. S. Lam. </author> <title> The Stanford DASH multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3):63 79, </volume> <month> Mar. </month> <year> 1992. </year>
Reference-contexts: Previous studies have tended to ignore the impact of software overhead [3, 9, 10], but our findings in dicate that the effect of this factor can be dramatic. Scalable shared memory multiprocessors traditionally use either a cache coherent non-uniform memory access (CC-NUMA) architecture <ref> [5, 6, 12] </ref> or a simple cache-only memory architecture (S-COMA) [10]. In a CC-NUMA, the amount of remote shared data that can be replicated on a node is limited by the size of a node's processor cache (s) and remote access cache (RAC) [6]. <p> In a CC-NUMA, the amount of remote shared data that can be replicated on a node is limited by the size of a node's processor cache (s) and remote access cache (RAC) <ref> [6] </ref>. S-COMA architectures employ any unused DRAM on a node as a cache for remote data [10]. However, the performance of pure S-COMA machines is heavily dependent on the memory pressure of a particular application. <p> T pagecache and T remote represent the latency of fetching a line from the local page cache or remote memory, respectively. T overhead represents the soft-ware overheads of the S-COMA and the hybrid models to support page remapping. 2.2. CC-NUMA In CC-NUMA <ref> [5, 6, 12] </ref>, the first page access on each node to a particular page causes a page fault, at which time the local TLB and page table are loaded with a page translation to the appropriate global physical page.
Reference: [7] <author> M. Marchetti, L. Kontothonassis, R. Bianchini, and M. Scott. </author> <title> Using simple page placement policies to reduce the code of cache fills in coherent shared-memory systems. </title> <booktitle> In Proceedings of the Ninth ACM/IEEE International Parallel Processing Symposium (IPPS), </booktitle> <month> Apr. </month> <year> 1995. </year>
Reference-contexts: Applications that suffer a large number of conflict misses to remote data, e.g., due to the limited amount of caching of remote data, perform poorly on CC-NUMAs [3]. Unfortunately, these applications are fairly common [3, 10]. Careful page allocation <ref> [1, 7] </ref>, migration [13], or replication [13] can alleviate this problem by selecting or modifying the choice of home node for a given page of data, but these techniques have to date only been successful for read-only or non-shared pages. <p> All three hybrid architectures we study adopt BSD4.4's page allocation mechanism and paging policy [8] with minor modifications. Free min and free target (see Section 3) were set to 5% and 7% of total memory, respectively. We extended the first touch allocation algorithm <ref> [7] </ref> to distribute home pages equally. The modeled processor and DSM engine are clocked at 120MHz. The system bus modeled is HP's Runway bus, which is also clocked at 120MHz. All cycle counts reported herein are with respect to this clock.
Reference: [8] <author> M. Mckusick, K. Bostic, M. Karels, and J. </author> <title> Quarterman. </title> <booktitle> The Design and Implementation of the 4.4BSD operating system, chapter 5 Memory Management, </booktitle> <pages> pages 117190. </pages> <publisher> Addison-Wesley Publishing Company Inc, </publisher> <year> 1996. </year>
Reference-contexts: It includes a kernel based on 4.4BSD that provides scheduling, interrupt handling, memory management, and limited system call capabilities. The modeled physical page size is 4 kilobytes. All three hybrid architectures we study adopt BSD4.4's page allocation mechanism and paging policy <ref> [8] </ref> with minor modifications. Free min and free target (see Section 3) were set to 5% and 7% of total memory, respectively. We extended the first touch allocation algorithm [7] to distribute home pages equally. The modeled processor and DSM engine are clocked at 120MHz.
Reference: [9] <author> A. Moga and M. Dubois. </author> <title> The effectiveness of SRAM network caches in clustered DSMs. </title> <booktitle> In Proceedings of the Fourth Annual Symposium on High Performance Computer Architecture, </booktitle> <year> 1998. </year>
Reference-contexts: In this paper, we concentrate on the second and third issues, namely reducing the frequency of remote memory accesses while ensuring that the software overhead required to do this remains modest. Previous studies have tended to ignore the impact of software overhead <ref> [3, 9, 10] </ref>, but our findings in dicate that the effect of this factor can be dramatic. Scalable shared memory multiprocessors traditionally use either a cache coherent non-uniform memory access (CC-NUMA) architecture [5, 6, 12] or a simple cache-only memory architecture (S-COMA) [10]. <p> Recently, hybrid architectures that combine aspects of both CC-NUMA and S-COMA have emerged, such as the Wisconsin reactive CC-NUMA (R-NUMA) [3] and the USC victim cache NUMA (VC-NUMA) <ref> [9] </ref>. Intuitively, these hybrid systems attempt to map the remote pages for which there are the highest number of conflict misses to local S-COMA pages, so-called hot pages, thereby eliminating the greatest number of expensive remote operations. All other remote pages are mapped in CC-NUMA mode. <p> Sources of this overhead include the time spent context switching between the user application and the pageout daemon, flushing blocks from the victim page (s), and remapping pages. 2.4. Hybrid DSM Architectures Two hybrid CC-NUMA/S-COMA architectures have been proposed: R-NUMA [3] and VC-NUMA <ref> [9] </ref>. We describe these architectures in this section. The basic architecture of an R-NUMA machine [3] is that of a CC-NUMA machine. <p> When memory pressure is high, and the number of hot pages exceeds the number of free pages available for caching them, this behavior results in frequent expensive page remappings for little value. This leads to performance worse than CC-NUMA, which never remaps pages. VC-NUMA <ref> [9] </ref> treats its RAC as a victim cache for the processor cache (s). However, this solution requires significant modifications to the processor cache controller and bus protocol. <p> We, therefore, model a single 8-kilobyte direct-mapped processor cache to compensate for the small size of the data sets, which is consistent with previous studies of hybrid architectures <ref> [3, 9] </ref>. We modeled a 4-bank main memory controller that can supply data from local memory in 58 cycles. The size of main memory and the amount of free memory used for page caching was varied to test the different models under varying memory pressures. <p> Cache and Network Characteristics of this study. Thus, the results reported for VC-NUMA are only relevant for evaluating its relocation strategy, and not the value of treating the RAC as a victim cache <ref> [9] </ref>. 4.2. Benchmark Programs We used six programs to conduct our study: barnes, fft, lu, ocean, and radix from the SPLASH-2 benchmark suite [14] and em3d from a shared memory implementation of the Split-C benchmark [2]. Table 2 shows the inputs used for each test program. <p> In particular, VC-NUMA only checks its backoff indicator when an average of two replacements per cached page have occurred, which is not sufficiently often to avoid thrashing. As shown in the previous study <ref> [9] </ref>, VC-NUMA does not significantly outperform R-NUMA until memory pressure exceeds 87.5%. Once again, AS-COMA's adaptive replacement algorithm detects thrashing as soon as it starts to occur, and the resulting back-off mechanism causes performance to degrade only slightly as memory pressure increases.
Reference: [10] <author> A. Saulsbury, T. Wilkinson, J. Carter, and A. Landin. </author> <title> An argument for Simple COMA. </title> <booktitle> In Proceedings of the First Annual Symposium on High Performance Computer Architecture, </booktitle> <pages> pages 276285, </pages> <month> Jan. </month> <year> 1995. </year>
Reference-contexts: In this paper, we concentrate on the second and third issues, namely reducing the frequency of remote memory accesses while ensuring that the software overhead required to do this remains modest. Previous studies have tended to ignore the impact of software overhead <ref> [3, 9, 10] </ref>, but our findings in dicate that the effect of this factor can be dramatic. Scalable shared memory multiprocessors traditionally use either a cache coherent non-uniform memory access (CC-NUMA) architecture [5, 6, 12] or a simple cache-only memory architecture (S-COMA) [10]. <p> Scalable shared memory multiprocessors traditionally use either a cache coherent non-uniform memory access (CC-NUMA) architecture [5, 6, 12] or a simple cache-only memory architecture (S-COMA) <ref> [10] </ref>. In a CC-NUMA, the amount of remote shared data that can be replicated on a node is limited by the size of a node's processor cache (s) and remote access cache (RAC) [6]. S-COMA architectures employ any unused DRAM on a node as a cache for remote data [10]. <p> (S-COMA) <ref> [10] </ref>. In a CC-NUMA, the amount of remote shared data that can be replicated on a node is limited by the size of a node's processor cache (s) and remote access cache (RAC) [6]. S-COMA architectures employ any unused DRAM on a node as a cache for remote data [10]. However, the performance of pure S-COMA machines is heavily dependent on the memory pressure of a particular application. Put simply, memory pressure is a measure of the amount of physical memory in a machine required to hold an application's instructions and data. <p> Applications that suffer a large number of conflict misses to remote data, e.g., due to the limited amount of caching of remote data, perform poorly on CC-NUMAs [3]. Unfortunately, these applications are fairly common <ref> [3, 10] </ref>. Careful page allocation [1, 7], migration [13], or replication [13] can alleviate this problem by selecting or modifying the choice of home node for a given page of data, but these techniques have to date only been successful for read-only or non-shared pages. <p> To reduce this overhead, designers of some such systems have adopted high speed interconnects to reduce T remote [5, 12]. 2.3. S-COMA In the S-COMA model <ref> [10] </ref>, the DSM controller and operating system cooperate to provide access to remotely homed data. S-COMA's aggressive use of local memory to replicate remote shared data can completely eliminate N remote when the memory pressure on a node is low. <p> An improved hybrid architecture, motivated by the analysis above, that performs well regardless of memory pressure is discussed in the following section. 3. Adaptive S-COMA At low memory pressure, S-COMA outperforms CC-NUMA, but the converse is true at high memory pressure <ref> [10] </ref>. Thus, our goal when designing AS-COMA was to develop a memory architecture that performed like pure S-COMA when memory for page caching was plentiful, and like CC-NUMA when it is not. To exploit S-COMA's superior performance at low memory pressures, AS-COMA initially maps pages in S-COMA mode.
Reference: [11] <author> L. Stoller, R. Kuramkote, and M. Swanson. </author> <title> PAINT- PA instruction set interpreter. </title> <type> Technical Report UUCS-96-009, </type> <institution> University of Utah Computer Science Department, </institution> <month> Sept. </month> <year> 1996. </year>
Reference: [12] <author> Sun Microsystems. </author> <title> Ultra Enterprise 10000 System Overview. </title> <address> http://www.sun.com/servers/datacenter/products/starfire. </address>
Reference-contexts: The designers of high-end commercial DSM systems such as the SUN UE10000 <ref> [12] </ref> and SGI Origin 2000 [5] have put considerable effort into reducing the remote memory latency by developing specialized high speed interconnects. <p> Previous studies have tended to ignore the impact of software overhead [3, 9, 10], but our findings in dicate that the effect of this factor can be dramatic. Scalable shared memory multiprocessors traditionally use either a cache coherent non-uniform memory access (CC-NUMA) architecture <ref> [5, 6, 12] </ref> or a simple cache-only memory architecture (S-COMA) [10]. In a CC-NUMA, the amount of remote shared data that can be replicated on a node is limited by the size of a node's processor cache (s) and remote access cache (RAC) [6]. <p> T pagecache and T remote represent the latency of fetching a line from the local page cache or remote memory, respectively. T overhead represents the soft-ware overheads of the S-COMA and the hybrid models to support page remapping. 2.2. CC-NUMA In CC-NUMA <ref> [5, 6, 12] </ref>, the first page access on each node to a particular page causes a page fault, at which time the local TLB and page table are loaded with a page translation to the appropriate global physical page. <p> To reduce this overhead, designers of some such systems have adopted high speed interconnects to reduce T remote <ref> [5, 12] </ref>. 2.3. S-COMA In the S-COMA model [10], the DSM controller and operating system cooperate to provide access to remotely homed data. S-COMA's aggressive use of local memory to replicate remote shared data can completely eliminate N remote when the memory pressure on a node is low.
Reference: [13] <author> B. Verghese, S. Devine, A. Gupta, and M. Rosenblum. </author> <title> Operating system support for improving data locality on CC-NUMA compute servers. </title> <booktitle> In Proceedings of the 7th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: Applications that suffer a large number of conflict misses to remote data, e.g., due to the limited amount of caching of remote data, perform poorly on CC-NUMAs [3]. Unfortunately, these applications are fairly common [3, 10]. Careful page allocation [1, 7], migration <ref> [13] </ref>, or replication [13] can alleviate this problem by selecting or modifying the choice of home node for a given page of data, but these techniques have to date only been successful for read-only or non-shared pages. <p> Applications that suffer a large number of conflict misses to remote data, e.g., due to the limited amount of caching of remote data, perform poorly on CC-NUMAs [3]. Unfortunately, these applications are fairly common [3, 10]. Careful page allocation [1, 7], migration <ref> [13] </ref>, or replication [13] can alleviate this problem by selecting or modifying the choice of home node for a given page of data, but these techniques have to date only been successful for read-only or non-shared pages.
Reference: [14] <author> S. Woo, M. Ohara, E. Torrie, J. Singh, and A. Gupta. </author> <title> The SPLASH-2 programs: Characterization and methodological considerations. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2436, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: The characteristics of the L1 cache, RACs, and network that we modeled are shown in Table 1. For most of the SPLASH2 applications we studied, the data sets provided have a primary working set that fits in an 8-kbyte cache <ref> [14] </ref>. We, therefore, model a single 8-kilobyte direct-mapped processor cache to compensate for the small size of the data sets, which is consistent with previous studies of hybrid architectures [3, 9]. We modeled a 4-bank main memory controller that can supply data from local memory in 58 cycles. <p> Thus, the results reported for VC-NUMA are only relevant for evaluating its relocation strategy, and not the value of treating the RAC as a victim cache [9]. 4.2. Benchmark Programs We used six programs to conduct our study: barnes, fft, lu, ocean, and radix from the SPLASH-2 benchmark suite <ref> [14] </ref> and em3d from a shared memory implementation of the Split-C benchmark [2]. Table 2 shows the inputs used for each test program. The column labeled Total Home pages indicates the total number of shared data pages initially allocated at all nodes.
References-found: 14

