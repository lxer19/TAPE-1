URL: http://www.cs.huji.ac.il/~feit/parsched/p-97-10.ps.gz
Refering-URL: http://www.cs.huji.ac.il/~feit/parsched/parsched97.html
Root-URL: http://www.cs.huji.ac.il
Email: rudolphg@lcs.mit.edu  
Title: Implications of I/O for Gang Scheduled Workloads  
Author: Walter Lee, Matthew Frank, Victor Lee, Kenneth Mackenzie, and Larry Rudolph fwalt, mfrank, wklee, kenmac, 
Web: http://cag-www.lcs.mit.edu/~walt  
Address: Cambridge, MA 02139, U.S.A.  
Affiliation: M.I.T. Laboratory for Computer Science  
Abstract: This paper examines the implications of gang scheduling for general-purpose multiprocessors. The workloads in these environments include both compute-bound parallel jobs, which often require gang scheduling, and I/O-bound jobs, which require high CPU priority to achieve interactive response times. Our results indicate that an effective interactive multiprocessor scheduler must weigh both the benefits and costs of gang scheduling when deciding how to allocate resources to jobs. This paper answers a number of questions about gang scheduling in the context of a variety of synthetic applications and SPLASH benchmarks running on the FUGU scalable multiprocessor workstation. We show that gang scheduling interferes with the performance of I/O-bound jobs, that applications do not benefit equally from gang scheduling, that most real applications can tolerate at least a small amount of scheduling skew without major performance degradation, and that messaging statistics can provide important clues to whether applications require gang scheduling. Taken together these results suggest that a multiprocessor scheduler can deliver interactive response times by dynamically monitoring and adjusting its resource allocation strategy. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Arpaci, A. Dusseau, A. Vahdat, L. Liu, T. Anderson, and D. Pat-terson. </author> <title> The Interaction of Parallel and Sequential Workloads on a Network of Workstations. </title> <booktitle> In Proceedings of Sigmetrics/Performance '95, </booktitle> <pages> pages 267-278, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: Our goal is to identify characteristics of an application which relate to its degree of benefit from gang scheduling. Many studies have measured the benefits of gang scheduling relative to uncoordinated scheduling <ref> [1, 3, 8, 17] </ref>. Our study differs in that we are interested in the marginal benefit of purely gang scheduling environment when compared to a gang scheduling environment with disruptions. In this experiment, we measure the performance of applications under various near-gang scheduling environments on a four-processor machine. <p> This feature simplifies reasonings about performance issues as well as correctness issues like deadlock and livelock. Second, from a performance standpoint, gang scheduling is absolutely necessary for applications that synchronize frequently. Several studies have quantified the benefits of gang scheduling <ref> [1, 3, 8] </ref>. Feitelson and Rudolph [8] demonstrate that gang scheduling benefits applications that perform fine-grain synchronization. Arpaci et al [1] and Mraz [13] observe that the disruption of system daemons in a network of workstation is potentially intolerable without some efforts to synchronize gangs across processors. <p> Second, from a performance standpoint, gang scheduling is absolutely necessary for applications that synchronize frequently. Several studies have quantified the benefits of gang scheduling [1, 3, 8]. Feitelson and Rudolph [8] demonstrate that gang scheduling benefits applications that perform fine-grain synchronization. Arpaci et al <ref> [1] </ref> and Mraz [13] observe that the disruption of system daemons in a network of workstation is potentially intolerable without some efforts to synchronize gangs across processors. Our study confirms these observations and draws detailed conclusions about the causes of slowdown for specific applications.
Reference: [2] <author> J. M. Barton and N. Bitar. </author> <title> A Scalable Multi-Discipline, Multiple-Processor Scheduling Framework for IRIX. </title> <booktitle> In Lecture Notes in Computer Science, </booktitle> <volume> 949, </volume> <pages> pages 45-69, </pages> <address> Santa Barbara, </address> <year> 1995. </year> <title> Springer Ver-lag. Workshop on Parallel Job Scheduling, </title> <booktitle> IPPS '95. </booktitle>
Reference-contexts: Our study confirms these observations and draws detailed conclusions about the causes of slowdown for specific applications. There are costs to gang scheduling as well. Literature focuses on its cost of implementation <ref> [2, 7, 17, 19] </ref>. This cost comes about because gang scheduling requires global coordination and centralized scheduling. But even in a system where these features come for free, gang scheduling still has costs which make its universal use undesirable. <p> The implementation of our scheduler uses a two level distributed hierarchical control structure for efficiency similar to Distributed Hierarchical Control [7, 19]. The scheduler accommodates both ganged and non-ganged jobs simultaneously, as do the IBM SP2 [19], Meiko CS-2 [16], and SGI IRIX <ref> [2] </ref> schedulers. 7 Conclusion In this paper, we make three high level points. First, we demonstrate that, aside from its implementation cost, gang scheduling has other nontrivial costs in the presence of I/O-bound jobs.
Reference: [3] <author> M. Crovella, P. Das, C. Dubnicki, T. LeBlanc, and E. Markatos. </author> <title> Multiprogramming on Multiprocessors. </title> <booktitle> In Proceedings of the third IEEE Symposium on Parallel and Distributed Processing, </booktitle> <year> 1991. </year>
Reference-contexts: Our goal is to identify characteristics of an application which relate to its degree of benefit from gang scheduling. Many studies have measured the benefits of gang scheduling relative to uncoordinated scheduling <ref> [1, 3, 8, 17] </ref>. Our study differs in that we are interested in the marginal benefit of purely gang scheduling environment when compared to a gang scheduling environment with disruptions. In this experiment, we measure the performance of applications under various near-gang scheduling environments on a four-processor machine. <p> Much work has been done to explore and compare the various options [6]. Space-sharing can be very efficient for compute-bound applications and is desirable when permitted by the programming model and application characteristics <ref> [3, 10] </ref>. Timesharing remains desirable for flexibility in debugging and in interleaving I/O with computation. These considerations become more important as multiprocessors become more mainstream. Ousterhout introduces the idea of coscheduling or gang scheduling to improve the performance of parallel applications under timesharing [14]. <p> This feature simplifies reasonings about performance issues as well as correctness issues like deadlock and livelock. Second, from a performance standpoint, gang scheduling is absolutely necessary for applications that synchronize frequently. Several studies have quantified the benefits of gang scheduling <ref> [1, 3, 8] </ref>. Feitelson and Rudolph [8] demonstrate that gang scheduling benefits applications that perform fine-grain synchronization. Arpaci et al [1] and Mraz [13] observe that the disruption of system daemons in a network of workstation is potentially intolerable without some efforts to synchronize gangs across processors.
Reference: [4] <author> R. Cypher, S. Konstantinidou, A. Ho, and P. Messina. </author> <title> A Quantitative Study of Parallel Scientific Applications with Explicit Communication. </title> <booktitle> In The Journal of Supercomputing, </booktitle> <pages> pages 5-24, </pages> <month> January </month> <year> 1996. </year> <month> 11 </month>
Reference-contexts: An ideal scheduler would perform a cost-benefit analysis which gives proper weights to all the issues above. Studies have pointed out that parallel scientific applications may consist of a significant amount of I/O activities due to reading and writing of results <ref> [4, 15] </ref>. I/O activities may also come from paging activities, and Wang [20] notices that even for programs written with a SPMD programming model, there is little coordination of I/O across processing nodes because of data dependencies.
Reference: [5] <author> A. Dusseau, R. Arpaci, , and D. Culler. </author> <title> Effective Distributed Schedul--ing of Parallel Workloads. </title> <booktitle> In Proceedings of ACM SIGMETRICS 1996, </booktitle> <address> Philadelphia, </address> <month> May </month> <year> 1996. </year> <note> ACM. </note>
Reference-contexts: Synchronization rate is the key parameter here. As we reduce synchronization rate, applications will behave more and more embarrassingly parallel-like (see below). We could indeed run an experiment where we vary this rate and observe the change in behavior. But this behavior has been studied before <ref> [5, 8] </ref>, and here we are interested in illustrating the types of behavior at extreme ends of the spectrum. Embarrassingly parallel This type of applications exhibits the same poor behavior as synchronization intensive applications when time is given to or taken away from a single processor.
Reference: [6] <author> D. G. Feitelson. </author> <title> A Survey of Scheduling in Multiprogrammed Parallel Systems. </title> <type> Technical Report IBM/RC 19790(87657), </type> <institution> IBM, </institution> <month> October </month> <year> 1994. </year>
Reference-contexts: Much work has been done to explore and compare the various options <ref> [6] </ref>. Space-sharing can be very efficient for compute-bound applications and is desirable when permitted by the programming model and application characteristics [3, 10]. Timesharing remains desirable for flexibility in debugging and in interleaving I/O with computation. These considerations become more important as multiprocessors become more mainstream.
Reference: [7] <author> D. G. Feitelson and L. Rudolph. </author> <title> Distributed Hierarchical Control for Parallel Processing. </title> <booktitle> In Computer. IEEE, </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: Our study confirms these observations and draws detailed conclusions about the causes of slowdown for specific applications. There are costs to gang scheduling as well. Literature focuses on its cost of implementation <ref> [2, 7, 17, 19] </ref>. This cost comes about because gang scheduling requires global coordination and centralized scheduling. But even in a system where these features come for free, gang scheduling still has costs which make its universal use undesirable. <p> The implementation of our scheduler uses a two level distributed hierarchical control structure for efficiency similar to Distributed Hierarchical Control <ref> [7, 19] </ref>. The scheduler accommodates both ganged and non-ganged jobs simultaneously, as do the IBM SP2 [19], Meiko CS-2 [16], and SGI IRIX [2] schedulers. 7 Conclusion In this paper, we make three high level points.
Reference: [8] <author> D. G. Feitelson and L. Rudolph. </author> <title> Gang Scheduling Performance Benefits for Fine-Grain Synchronization. </title> <journal> In Journal of Parallel and Distributed Computing, </journal> <pages> pages 306-318, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: Our goal is to identify characteristics of an application which relate to its degree of benefit from gang scheduling. Many studies have measured the benefits of gang scheduling relative to uncoordinated scheduling <ref> [1, 3, 8, 17] </ref>. Our study differs in that we are interested in the marginal benefit of purely gang scheduling environment when compared to a gang scheduling environment with disruptions. In this experiment, we measure the performance of applications under various near-gang scheduling environments on a four-processor machine. <p> Synchronization rate is the key parameter here. As we reduce synchronization rate, applications will behave more and more embarrassingly parallel-like (see below). We could indeed run an experiment where we vary this rate and observe the change in behavior. But this behavior has been studied before <ref> [5, 8] </ref>, and here we are interested in illustrating the types of behavior at extreme ends of the spectrum. Embarrassingly parallel This type of applications exhibits the same poor behavior as synchronization intensive applications when time is given to or taken away from a single processor. <p> This feature simplifies reasonings about performance issues as well as correctness issues like deadlock and livelock. Second, from a performance standpoint, gang scheduling is absolutely necessary for applications that synchronize frequently. Several studies have quantified the benefits of gang scheduling <ref> [1, 3, 8] </ref>. Feitelson and Rudolph [8] demonstrate that gang scheduling benefits applications that perform fine-grain synchronization. Arpaci et al [1] and Mraz [13] observe that the disruption of system daemons in a network of workstation is potentially intolerable without some efforts to synchronize gangs across processors. <p> This feature simplifies reasonings about performance issues as well as correctness issues like deadlock and livelock. Second, from a performance standpoint, gang scheduling is absolutely necessary for applications that synchronize frequently. Several studies have quantified the benefits of gang scheduling [1, 3, 8]. Feitelson and Rudolph <ref> [8] </ref> demonstrate that gang scheduling benefits applications that perform fine-grain synchronization. Arpaci et al [1] and Mraz [13] observe that the disruption of system daemons in a network of workstation is potentially intolerable without some efforts to synchronize gangs across processors.
Reference: [9] <author> D. G. Feitelson and L. Rudolph. </author> <title> Coscheduling based on Runtime Identification of Activity Working Sets. </title> <booktitle> In International Journal of Parallel Programming, </booktitle> <pages> pages 135-160, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: A fully dynamic solution to gang scheduling includes the identification of gang members at runtime. Sobalvarro [17] uses individual message arrivals as a cue to the identification of a gang, while Feitelson and Rudolph <ref> [9] </ref> monitor the rate at which shared communication objects are being accessed to determine whether and which processes need to be ganged. The implementation of our scheduler uses a two level distributed hierarchical control structure for efficiency similar to Distributed Hierarchical Control [7, 19].
Reference: [10] <author> A. Gupta, A. Tucker, and L. Stevens. </author> <title> The Impact of Operating System Scheduling Policies and Synchronization Methods of the Performance of Parallel Applications. </title> <booktitle> In Proceedings of 1991 ACM Sigmetrics Conference, </booktitle> <year> 1991. </year>
Reference-contexts: Much work has been done to explore and compare the various options [6]. Space-sharing can be very efficient for compute-bound applications and is desirable when permitted by the programming model and application characteristics <ref> [3, 10] </ref>. Timesharing remains desirable for flexibility in debugging and in interleaving I/O with computation. These considerations become more important as multiprocessors become more mainstream. Ousterhout introduces the idea of coscheduling or gang scheduling to improve the performance of parallel applications under timesharing [14].
Reference: [11] <author> K. L. Johnson, M. F. Kaashoek, and D. A. Wallach. </author> <title> CRL: High-Performance All-Software Distributed Shared Memory. </title> <booktitle> In Proceedings of the 15th ACM Symposium on Operating Systems Principles, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: Enum finds the total number of solutions to the triangle puzzle (a simple board game) by enumerating all possibilities breadth-first. Barnes, Water, and LU are scientific applications from the Splash benchmark suite implemented using CRL, an all-software shared-memory system <ref> [11] </ref>. See Table 1 for statistics describing the applications. Because the applications are non-homogeneous in time, we obtain each data point by taking the average result from 20 runs, each with a different set of time quanta given or taken away.
Reference: [12] <author> K. Mackenzie, J. Kubiatowicz, M. Frank, W. Lee, V. Lee, A. Agarwal, and M. F. Kaashoek. UDM: </author> <title> User Direct Messaging for General-Purpose Multiprocessing. </title> <note> Technical Memo MIT/LCS/TM-556, </note> <month> March </month> <year> 1996. </year>
Reference-contexts: It provides information about the Fugu multiprocessor, the scheduler, and the multiprocessor simulator used by the experiments. Fugu is an experimental, distributed-memory multiprocessor supporting both cache-coherent shared memory and fine-grain message passing communication mechanisms <ref> [12] </ref>. The applications studied in this paper use only the message-passing mechanism. Messages in Fugu have extremely low overhead, costing ~ 10 cycles to send and ~ 100 cycles to process a null active message via an interrupt.
Reference: [13] <author> R. Mraz. </author> <title> Reducing the Variance of Point-to-Point Transfers for Parallel Real-Time Programs. </title> <booktitle> In IEEE Parallel & Distributed Technology, </booktitle> <year> 1994. </year>
Reference-contexts: Second, from a performance standpoint, gang scheduling is absolutely necessary for applications that synchronize frequently. Several studies have quantified the benefits of gang scheduling [1, 3, 8]. Feitelson and Rudolph [8] demonstrate that gang scheduling benefits applications that perform fine-grain synchronization. Arpaci et al [1] and Mraz <ref> [13] </ref> observe that the disruption of system daemons in a network of workstation is potentially intolerable without some efforts to synchronize gangs across processors. Our study confirms these observations and draws detailed conclusions about the causes of slowdown for specific applications. There are costs to gang scheduling as well.
Reference: [14] <author> J. K. Ousterhout. </author> <title> Scheduling Techniques for Concurrent Systems. </title> <booktitle> In 3rd International Conference on Distributed Computing Systems, </booktitle> <pages> pages 22-30, </pages> <year> 1982. </year>
Reference-contexts: Timesharing remains desirable for flexibility in debugging and in interleaving I/O with computation. These considerations become more important as multiprocessors become more mainstream. Ousterhout introduces the idea of coscheduling or gang scheduling to improve the performance of parallel applications under timesharing <ref> [14] </ref>. There are two benefits to gang scheduling. First, from a programmability standpoint, gang scheduling is attractive because it is compatible with conventional programming models, where processes of a parallel application are assumed to be scheduled simultaneously.
Reference: [15] <author> J.M.del Rosario and A. Choudhary. </author> <title> High Performance I/O for parallel computers: problems and prospects. </title> <journal> In IEEE Computers, </journal> <volume> vol. 27, no. 3, </volume> <pages> pages 59-68, </pages> <year> 1994. </year>
Reference-contexts: An ideal scheduler would perform a cost-benefit analysis which gives proper weights to all the issues above. Studies have pointed out that parallel scientific applications may consist of a significant amount of I/O activities due to reading and writing of results <ref> [4, 15] </ref>. I/O activities may also come from paging activities, and Wang [20] notices that even for programs written with a SPMD programming model, there is little coordination of I/O across processing nodes because of data dependencies.
Reference: [16] <author> K. E. Schauser and C. J. Scheiman. </author> <title> Experience with Active Messages on the Meiko CS-2. </title> <booktitle> In Proceedingsof the 9th International Symposium on Parallel Processing, </booktitle> <year> 1995. </year>
Reference-contexts: The implementation of our scheduler uses a two level distributed hierarchical control structure for efficiency similar to Distributed Hierarchical Control [7, 19]. The scheduler accommodates both ganged and non-ganged jobs simultaneously, as do the IBM SP2 [19], Meiko CS-2 <ref> [16] </ref>, and SGI IRIX [2] schedulers. 7 Conclusion In this paper, we make three high level points. First, we demonstrate that, aside from its implementation cost, gang scheduling has other nontrivial costs in the presence of I/O-bound jobs.
Reference: [17] <author> P. G. Sobalvarro and W. E. Weihl. </author> <title> Demand-basedCoschedulingof Parallel Jobs on Multiprogrammed Multiprocessors. </title> <booktitle> In Lecture Notes in Computer Science, </booktitle> <volume> 949, </volume> <pages> pages 106-126, </pages> <address> Santa Barbara, </address> <year> 1995. </year> <title> Springer Verlag. Workshop on Parallel Job Scheduling, </title> <booktitle> IPPS '95. </booktitle>
Reference-contexts: Our goal is to identify characteristics of an application which relate to its degree of benefit from gang scheduling. Many studies have measured the benefits of gang scheduling relative to uncoordinated scheduling <ref> [1, 3, 8, 17] </ref>. Our study differs in that we are interested in the marginal benefit of purely gang scheduling environment when compared to a gang scheduling environment with disruptions. In this experiment, we measure the performance of applications under various near-gang scheduling environments on a four-processor machine. <p> To avoid scheduler-induced load imbalances, interprocess fairness is a good criteria to keep in mind when a gang sched-uler does alternate scheduling. The failure to do so could be a reason why research has found that random alternate scheduling provides little gain to system throughput <ref> [17] </ref>. 6. To first order, there are two types of blocking communication. One is the barrier-like communication used to check that processes have all reached a checkpoint. <p> Our study confirms these observations and draws detailed conclusions about the causes of slowdown for specific applications. There are costs to gang scheduling as well. Literature focuses on its cost of implementation <ref> [2, 7, 17, 19] </ref>. This cost comes about because gang scheduling requires global coordination and centralized scheduling. But even in a system where these features come for free, gang scheduling still has costs which make its universal use undesirable. <p> In our work, we assume that the members of the job are known a priori and concentrate on the problem of deciding whether to gang based on indirect measurements. A fully dynamic solution to gang scheduling includes the identification of gang members at runtime. Sobalvarro <ref> [17] </ref> uses individual message arrivals as a cue to the identification of a gang, while Feitelson and Rudolph [9] monitor the rate at which shared communication objects are being accessed to determine whether and which processes need to be ganged.
Reference: [18] <author> A. Tucker and A. Gupta. </author> <title> Process Control and Scheduling Issues for Multiprogrammed Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating System Principles (SOSP-12), </booktitle> <pages> pages 159-166, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: It is load balanced. Barrier This application consists entirely of synchronizing barriers. Workpile In this set of applications, there is a fixed amount of global work which may be broken into independent units of work <ref> [18] </ref>. The units may then be distributed dynamically to achieve load balancing under a variety of scheduling conditions. We use four such applications, with the granularity of a work unit varying from 14% to 2400% of a quantum.
Reference: [19] <author> F. Wang, H. Franke, M. Papaefthymiou, P. Pattnaik, L. Rudolph, and M. S. Squillante. </author> <title> A Gang Scheduling Design for Multiprogrammed Parallel Computing Environments. </title> <booktitle> In Lecture Notes in Computer Science, </booktitle> <volume> 1162, </volume> <pages> pages 111-125, </pages> <address> Honolulu,Hawaii, </address> <year> 1996. </year> <title> Springer Verlag. Workshop on Parallel Job Scheduling, </title> <booktitle> IPPS '96. </booktitle>
Reference-contexts: Our study confirms these observations and draws detailed conclusions about the causes of slowdown for specific applications. There are costs to gang scheduling as well. Literature focuses on its cost of implementation <ref> [2, 7, 17, 19] </ref>. This cost comes about because gang scheduling requires global coordination and centralized scheduling. But even in a system where these features come for free, gang scheduling still has costs which make its universal use undesirable. <p> The implementation of our scheduler uses a two level distributed hierarchical control structure for efficiency similar to Distributed Hierarchical Control <ref> [7, 19] </ref>. The scheduler accommodates both ganged and non-ganged jobs simultaneously, as do the IBM SP2 [19], Meiko CS-2 [16], and SGI IRIX [2] schedulers. 7 Conclusion In this paper, we make three high level points. <p> The implementation of our scheduler uses a two level distributed hierarchical control structure for efficiency similar to Distributed Hierarchical Control [7, 19]. The scheduler accommodates both ganged and non-ganged jobs simultaneously, as do the IBM SP2 <ref> [19] </ref>, Meiko CS-2 [16], and SGI IRIX [2] schedulers. 7 Conclusion In this paper, we make three high level points. First, we demonstrate that, aside from its implementation cost, gang scheduling has other nontrivial costs in the presence of I/O-bound jobs.
Reference: [20] <author> K. Y. Wang and D. C. Marinescu. </author> <title> Correlation of the paging activity of the individual node programs in the SPMD execution mode. </title> <booktitle> In Proceedings of the Hawaii International Conference on System Sciences, </booktitle> <year> 1995. </year>
Reference-contexts: Studies have pointed out that parallel scientific applications may consist of a significant amount of I/O activities due to reading and writing of results [4, 15]. I/O activities may also come from paging activities, and Wang <ref> [20] </ref> notices that even for programs written with a SPMD programming model, there is little coordination of I/O across processing nodes because of data dependencies. This behavior is consistent with our assumption in the experiments that I/O activities across the processing nodes are independent.
References-found: 20

