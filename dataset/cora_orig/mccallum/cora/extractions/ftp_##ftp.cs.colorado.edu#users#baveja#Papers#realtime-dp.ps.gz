URL: ftp://ftp.cs.colorado.edu/users/baveja/Papers/realtime-dp.ps.gz
Refering-URL: http://www.cs.colorado.edu/~baveja/papers.html
Root-URL: 
Title: Learning to Act using Real-Time Dynamic Programming  
Author: Andrew G. Barto Steven J. Bradtke Satinder P. Singh 
Date: March 22, 1993  
Address: Amherst MA 01003  
Affiliation: Department of Computer Science University of Massachusetts,  
Abstract: fl The authors thank Rich Yee, Vijay Gullapalli, Brian Pinette, and Jonathan Bachrach for helping to clarify the relationships between heuristic search and control. We thank Rich Sutton, Chris Watkins, Paul Werbos, and Ron Williams for sharing their fundamental insights into this subject through numerous discussions, and we further thank Rich Sutton for first making us aware of Korf's research and for his very thoughtful comments on the manuscript. We are very grateful to Dimitri Bertsekas and Steven Sullivan for independently pointing out an error in an earlier version of this article. Finally, we thank Harry Klopf, whose insight and persistence encouraged our interest in this class of learning problems. This research was supported by grants to A.G. Barto from the National Science Foundation (ECS-8912623 and ECS-9214866) and the Air Force Office of Scientific Research, Bolling AFB (AFOSR-89-0526). 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. W. Anderson. </author> <title> Strategy learning with multilayer connectionist representations. </title> <type> Technical Report TR87-509.3, </type> <institution> GTE Laboratories, Incorporated, </institution> <address> Waltham, MA, </address> <year> 1987. </year> <note> (This is a corrected version of the report published in Proceedings of the Fourth International Workshop on Machine Learning,103-114, </note> <institution> 1987, </institution> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann.). </publisher>
Reference-contexts: Because of its compatibility with connectionist learning algorithms, this approach was refined and extended by Sutton [67; 68] and used heuristically in a number of single-agent problem-solving tasks (e.g., Barto, Sutton, and Anderson [2], Anderson <ref> [1] </ref>, and Sutton [67]). The algorithm was implemented as a neuron-like connectionist element called the Adaptive Critic Element [2]. Sutton [68] later called these algorithms Temporal Difference (TD) methods and obtained some theoretical results about their convergence. <p> Real-time algorithms based on policy iteration effectively work by executing an asynchronous form of modified policy iteration concurrently with control. Examples of such methods appear in the pole-balancing system of Barto, Sutton, and Anderson [2; 67] (also refs. <ref> [1; 67] </ref>) and the Dyna-PI method of Sutton [69] (where PI means 38 Policy Iteration). Barto, Sutton, and Watkins [4; 3] discuss the connection between these methods and policy iteration in some detail. <p> The discrepancy supplies the error for any error-correction procedure that approximates functions based on a training set of function samples. This is a form of supervised learning, or learning from examples, and provides the natural way to make use of connectionist networks as shown, for example, by Anderson <ref> [1] </ref> and Tesauro [77]. Parametric approximations of evaluation functions are useful because they can generalize beyond the training data to supply cost estimates for states that have not yet been visited, an important factor for large state sets.
Reference: [2] <author> A. G. Barto, R. S. Sutton, and C. W. Anderson. </author> <title> Neuronlike elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 13 </volume> <pages> 835-846, </pages> <year> 1983. </year> <note> Reprinted in J. </note> <editor> A. Anderson and E. Rosenfeld, Neurocomputing: </editor> <booktitle> Foundations of Research, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1988. </year>
Reference-contexts: Because of its compatibility with connectionist learning algorithms, this approach was refined and extended by Sutton [67; 68] and used heuristically in a number of single-agent problem-solving tasks (e.g., Barto, Sutton, and Anderson <ref> [2] </ref>, Anderson [1], and Sutton [67]). The algorithm was implemented as a neuron-like connectionist element called the Adaptive Critic Element [2]. Sutton [68] later called these algorithms Temporal Difference (TD) methods and obtained some theoretical results about their convergence. <p> compatibility with connectionist learning algorithms, this approach was refined and extended by Sutton [67; 68] and used heuristically in a number of single-agent problem-solving tasks (e.g., Barto, Sutton, and Anderson <ref> [2] </ref>, Anderson [1], and Sutton [67]). The algorithm was implemented as a neuron-like connectionist element called the Adaptive Critic Element [2]. Sutton [68] later called these algorithms Temporal Difference (TD) methods and obtained some theoretical results about their convergence. Following the proposals of Klopf [36; 37], Sutton and Barto [72; 73; 74] developed these methods as models of animal learning. <p> Watkins [81] describes a family of Q-Learning methods in which Q-values are backed up based on information gained over sequences of state transitions. One way to implement this kind of extension is to use the "eligibility trace" idea <ref> [2; 37; 67; 72; 68] </ref> to back up the Q-values of all the state-action pairs experienced in the past, with the magnitudes of the backups decreasing to zero with increasing time in the past. Sutton's [68] TD () algorithms illustrate this idea. <p> Real-time algorithms based on policy iteration effectively work by executing an asynchronous form of modified policy iteration concurrently with control. Examples of such methods appear in the pole-balancing system of Barto, Sutton, and Anderson <ref> [2; 67] </ref> (also refs. [1; 67]) and the Dyna-PI method of Sutton [69] (where PI means 38 Policy Iteration). Barto, Sutton, and Watkins [4; 3] discuss the connection between these methods and policy iteration in some detail. <p> In applying conventional DP to problems involving continuous states and/or actions, the usual practice is to discretize the ranges of the continuous state variables and then use the lookup-table representation (cf. the "boxes" representation used by Michie and Chambers [52] and Barto, Sutton, and Anderson <ref> [2] </ref>).
Reference: [3] <author> A. G. Barto, R. S. Sutton, and C. Watkins. </author> <title> Sequential decision problems and neural networks. </title> <editor> In D. S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <pages> pages 686-693, </pages> <address> San Mateo, CA, 1990. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 58 </pages>
Reference-contexts: Examples of such methods appear in the pole-balancing system of Barto, Sutton, and Anderson [2; 67] (also refs. [1; 67]) and the Dyna-PI method of Sutton [69] (where PI means 38 Policy Iteration). Barto, Sutton, and Watkins <ref> [4; 3] </ref> discuss the connection between these methods and policy iteration in some detail. In this article we do not discuss learning algorithms based on policy iteration because their theory is not yet as well understood as is the theory of learning algorithms based on asynchronous value iteration.
Reference: [4] <author> A. G. Barto, R. S. Sutton, and C. J. C. H. Watkins. </author> <title> Learning and sequential deci-sion making. </title> <editor> In M. Gabriel and J. Moore, editors, </editor> <booktitle> Learning and Computational Neuro-science:Foundations of Adaptive Networks, </booktitle> <pages> pages 539-602. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: Examples of such methods appear in the pole-balancing system of Barto, Sutton, and Anderson [2; 67] (also refs. [1; 67]) and the Dyna-PI method of Sutton [69] (where PI means 38 Policy Iteration). Barto, Sutton, and Watkins <ref> [4; 3] </ref> discuss the connection between these methods and policy iteration in some detail. In this article we do not discuss learning algorithms based on policy iteration because their theory is not yet as well understood as is the theory of learning algorithms based on asynchronous value iteration.
Reference: [5] <author> A.G. Barto. </author> <title> Reinforcement learning and adaptive critic methods. </title> <editor> In D. A. White and D. A. Sofge, editors, </editor> <booktitle> Handbook of Intelligent Control: Neural, Fuzzy, and Adaptive Approaches, </booktitle> <pages> pages 469-491. </pages> <publisher> Van Nostrand Reinhold, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: We have also come across the work Jalali and Ferguson [32], who independently proposed a method similar to Adaptive RTDP. Sutton, Barto, and Williams [75] discussed reinforcement learning from the perspective of DP and adaptive control, and White and Jordan [89] and Barto <ref> [5] </ref> provide additional background and extensive references to current research.
Reference: [6] <author> A.G. Barto and S.P. Singh. </author> <title> On the computational economics of reinforcement learning. </title> <editor> In D. S. Touretzky, J. L. Elman, T. J. Sejnowski, and G. E. Hinton, editors, </editor> <title> Connectionist Models: </title> <booktitle> Proceedings of the 1990 Summer School, </booktitle> <pages> pages 35-44. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: Adaptive optimal control algorithms require mechanisms for resolving these problems, but no mechanism is universally favored. Some of the approaches for which rigorous theoretical results are available are reviewed by Kumar [39], and a variety of more heuristic approaches have been studied by Barto and Singh <ref> [6] </ref>, Kaelbling [34], Moore [55], Schmidhuber [63], Sutton [69], Watkins [81], Thrun [79], and Thrun and Moller [78]. In the following subsections, we describe several non-Bayesian methods for solving Marko-vian decision problems with incomplete information. <p> to state-action pairs formed the basis of Denardo's [24] approach to DP, we have not seen algorithms like Q-Learning for estimating these values that predate Watkins' 1989 dissertation. 33 We depart somewhat in our presentation from the view taken by Watkins [81] and others (e.g., Sutton [69], Barto and Singh <ref> [6] </ref>) of Q-Learning as a method for adaptive on-line control. To emphasize Q-Learning's relationship with asynchronous DP, we first present the basic Q-Learning algorithm as an off-line asynchronous DP method that is unique in not requiring direct access to the state-transition probabilities of the decision problem. <p> Sutton's [68] TD () algorithms illustrate this idea. Attempting to present all of the combinations and variations of Q-Learning methods that have been, or could be, described is well beyond the scope of the present article. Barto and Singh <ref> [6] </ref>, Dayan [20; 21], Lin [47; 46], Moore [57], and Sutton [69] present comparative empirical studies of some of the adaptive algorithms based on Q-Learning. 8 Methods based on Explicit Policy Representations All of the DP-based learning algorithms described above, both non-adaptive and adaptive cases, use an explicit representation of either
Reference: [7] <author> R. Bellman and S. E. Dreyfus. </author> <title> Functional approximations and dynamic programming. Math Tables and Other Aides to Computation, </title> <booktitle> 13 </booktitle> <pages> 247-251, </pages> <year> 1959. </year>
Reference-contexts: There is an extensive literature on function approximation methods and DP, such as multigrid methods and methods using splines and orthogonal polynomials (e.g., Bellman and Dreyfus <ref> [7] </ref>, Bellman, Kalaba, and Kotkin [8], Daniel [18], Kushner and Dupuis [41]). However, most of this literature is devoted to off-line algorithms for cases in which there is a complete model of the decision problem.
Reference: [8] <author> R. Bellman, R. Kalaba, and B. Kotkin. </author> <title> Polynomial approximation|A new computational technique in dynamic programming: Allocation processes. </title> <journal> Mathematical Computation, </journal> <volume> 17 </volume> <pages> 155-161, </pages> <year> 1973. </year>
Reference-contexts: There is an extensive literature on function approximation methods and DP, such as multigrid methods and methods using splines and orthogonal polynomials (e.g., Bellman and Dreyfus [7], Bellman, Kalaba, and Kotkin <ref> [8] </ref>, Daniel [18], Kushner and Dupuis [41]). However, most of this literature is devoted to off-line algorithms for cases in which there is a complete model of the decision problem.
Reference: [9] <author> R. E. Bellman. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ, </address> <year> 1957. </year>
Reference-contexts: Independently of the approaches inspired by Samuel's checkers player, other researchers suggested similar algorithms based on the theory of optimal control, where DP provides important solution methods. As applied to control problems, DP (a term introduced by Bellman <ref> [9] </ref>) consists of methods for successively approximating optimal evaluation functions and decision rules for both deterministic and stochastic problems. <p> This leads to space complexity exponential in the number of state variables, the situation prompting Bellman <ref> [9] </ref> to coin the phrase "curse of dimensionality." The methods described in this article based on asynchronous DP and Q-Learning do not circumvent the curse of dimensionality, although the focusing behavior of Trial-Based RTDP in stochastic shortest path problems with designated start states can reduce the storage requirement if memory is
Reference: [10] <author> D. P. Bertsekas. </author> <title> Distributed dynamic programming. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 27 </volume> <pages> 610-616, </pages> <year> 1982. </year>
Reference-contexts: RTDP is the result of recognizing that Korf's [38] Learning-Real-Time A* (LRTA*) algorithm 1 closely related to a form of DP known as asynchronous DP <ref> [10] </ref>. This novel observation permits us to generalize the ideas behind LRTA* so that they apply to real-time problem solving tasks involving uncertainty. In particular, we apply the theory of asynchronous DP developed by Bertsekas [10] and Bertsekas and Tsitsiklis [12] to show that RTDP converges to optimal solutions when applied <p> A* (LRTA*) algorithm 1 closely related to a form of DP known as asynchronous DP <ref> [10] </ref>. This novel observation permits us to generalize the ideas behind LRTA* so that they apply to real-time problem solving tasks involving uncertainty. In particular, we apply the theory of asynchronous DP developed by Bertsekas [10] and Bertsekas and Tsitsiklis [12] to show that RTDP converges to optimal solutions when applied to several types of real-time problem solving tasks involving uncertainty. <p> In this article, we introduce the fact that the theory of asynchronous DP is applicable to the analysis of DP-based reinforcement learning algorithms. Asynchronous DP algorithms differ from conventional DP algorithms in that they do not have to proceed in systematic exhaustive sweeps of the problem's state set. Bertsekas <ref> [10] </ref> and Bertsekas and Tsitsiklis [12] proved general theorems about the convergence of asynchronous DP applied to discrete-time stochastic optimal control problems. <p> However, it is not organized in terms of systematic successive sweeps of the state set. As proposed by Bertsekas <ref> [10] </ref> and further developed by Bertsekas and Tsitsiklis [12], asynchronous DP is suitable for multi-processor systems with communication time delays and without a common clock.
Reference: [11] <author> D. P. Bertsekas. </author> <title> Dynamic Programming: Deterministic and Stochastic Models. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1987. </year>
Reference-contexts: Many problems of practical importance have been formulated as Markovian decision problems, and extensive treatment of the theory and application of this framework can be found in many books, such as those by Bertsekas <ref> [11] </ref> and Ross [60]. A Markovian decision problem is defined in terms of a discrete-time stochastic dynamic system with finite state set S = f1; : : : ; ng. Time is represented by a sequence of time steps t = 0; 1; : : :. <p> Although the cost of a state need not get closer to its optimal cost on each iteration, the maximum error between f k (i) and f fl (i) over all states i must decrease (e.g., ref. <ref> [11] </ref>). Synchronous DP, as well as the other off-line versions of value iteration we discuss below, generates a sequence of functions that converges to f fl if fl &lt; 1, but it does not explicitly generate a sequence of policies. <p> Because the controller always uses a policy defined by the current evaluation function, it can perform optimally before the evaluation function converges to the optimal evaluation function. Bertsekas <ref> [11] </ref> and Bertsekas and Tsitsiklis [12] give conditions ensuring convergence of synchronous DP for stochastic shortest path problems in the undiscounted case (fl = 1). Using their terminology, a policy is proper if its use implies a nonzero probability of eventually reaching the goal set starting from any state. <p> They use system identification algorithms to update parameters whose values determine the current system model at any time during control. They typically make control decisions under the assumption that the current model is the true model of the system (what control theorists call the certainty equivalence principle <ref> [11] </ref>). Direct methods, on the other hand, form policies without using explicit system models. <p> Unlike the methods addressed in this article, these methods are more closely related to the policy iteration DP algorithm than the value iteration algorithms discussed in Section 5. Policy iteration (see, e.g., Bertsekas <ref> [11] </ref>) alternates two phases: 1) a policy evaluation phase, in which the evaluation function for the current policy is determined, and 2) a policy improvement phase, in which the current policy is updated to be greedy with respect to the current evaluation function.
Reference: [12] <author> D. P. Bertsekas and J. N. Tsitsiklis. </author> <title> Parallel and Distributed Computation: Numerical Methods. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1989. </year>
Reference-contexts: This novel observation permits us to generalize the ideas behind LRTA* so that they apply to real-time problem solving tasks involving uncertainty. In particular, we apply the theory of asynchronous DP developed by Bertsekas [10] and Bertsekas and Tsitsiklis <ref> [12] </ref> to show that RTDP converges to optimal solutions when applied to several types of real-time problem solving tasks involving uncertainty. <p> Asynchronous DP algorithms differ from conventional DP algorithms in that they do not have to proceed in systematic exhaustive sweeps of the problem's state set. Bertsekas [10] and Bertsekas and Tsitsiklis <ref> [12] </ref> proved general theorems about the convergence of asynchronous DP applied to discrete-time stochastic optimal control problems. However, because they were motivated by the suitability of asynchronous DP for parallel processing, they did not relate these results to real-time variants of DP as we do in this article. <p> We consider one set of assumptions for the undiscounted case because the resulting decision problems are closely related to problems to which heuristic search is applied. In these problems, which Bertsekas and Tsitsiklis <ref> [12] </ref> call stochastic shortest path problems (thinking of immediate costs as arc lengths in a graph whose nodes correspond to states), there is an absorbing set of states, i.e., a set of states that once entered is never left, and the immediate cost associated with applying an action to any of <p> Because the controller always uses a policy defined by the current evaluation function, it can perform optimally before the evaluation function converges to the optimal evaluation function. Bertsekas [11] and Bertsekas and Tsitsiklis <ref> [12] </ref> give conditions ensuring convergence of synchronous DP for stochastic shortest path problems in the undiscounted case (fl = 1). Using their terminology, a policy is proper if its use implies a nonzero probability of eventually reaching the goal set starting from any state. <p> When fl &lt; 1, repeated Gauss-Seidel sweeps produce a sequence of functions that converges to f fl . For undiscounted stochastic shortest path problems, the conditions described above that ensure convergence of synchronous DP also ensure convergence of Gauss-Seidel DP <ref> [12] </ref>. Because each cost backup uses the latest costs of the other states, Gauss-Seidel DP generally converges faster than synchronous DP. Furthermore, it should be clear that some state orderings produce faster convergence than others, depending on the problem. <p> it serves as a bridge between synchronous DP and the asynchronous form discussed next. 9 The assumption of positive immediate costs can be weakened to nonnegativity, i.e., c i (u) 0 for all i 2 S and u 2 U (i), if there exists at least one optimal proper policy <ref> [12] </ref>. 21 5.3 Asynchronous Dynamic Programming Asynchronous DP is similar to Gauss-Seidel DP in that it does not back up state costs simultaneously. However, it is not organized in terms of systematic successive sweeps of the state set. As proposed by Bertsekas [10] and further developed by Bertsekas and Tsitsiklis [12], <p> <ref> [12] </ref>. 21 5.3 Asynchronous Dynamic Programming Asynchronous DP is similar to Gauss-Seidel DP in that it does not back up state costs simultaneously. However, it is not organized in terms of systematic successive sweeps of the state set. As proposed by Bertsekas [10] and further developed by Bertsekas and Tsitsiklis [12], asynchronous DP is suitable for multi-processor systems with communication time delays and without a common clock. For each state i 2 S there is a separate processor dedicated to backing up the cost of state i (more generally, each processor may be responsible for a number of states). <p> In practice, this means that the strategy for selecting states for cost backups should never eliminate any state from possible selection in the future. In the undiscounted case (fl = 1), additional assumptions are necessary to ensure convergence. It follows from a result by Bertsekas and Tsitsiklis <ref> [12] </ref> (p. 446) that asynchronous DP converges 22 in undiscounted stochastic shortest path problems if the cost of each state is backed up infinitely often and the conditions given in Section 5.1 for convergence of synchronous DP are met: 1) the initial cost of every goal state is zero, 2) there <p> Our contribution in this article has been to bring to bear on DP-based learning the theory of asynchronous DP as presented by Bertsekas and Tsitsiklis <ref> [12] </ref>. Although the suitability of asynchronous DP for implementation on multi-processor systems motivated this theory, we have made novel use of these results. Applying these results, especially the results on stochastic shortest path problems, to RTDP provides a new theoretical basis for DP-based learning algorithms. <p> After T 1 , therefore, Trial-Based RTDP performs asynchronous DP on a stochastic shortest path problem with state set I that satisfies the conditions of the convergence theorem for asynchronous DP applied to undiscounted stochastic shortest path problems (Bertsekas and Tsitsiklis <ref> [12] </ref>, Proposition 3.3, p. 318). Consequently, Trial-Based RTDP converges to the optimal evaluation function of this stochastic shortest path problem.
Reference: [13] <author> S. J. Bradtke. </author> <title> Reinforcement learning applied to linear quadratic regulation. </title> <booktitle> In Advances in Neural Information Processing 4, </booktitle> <address> San Mateo, CA. </address> <note> Morgan Kaufmann. To appear. </note>
Reference-contexts: Although generalization can be helpful in approximating an optimal evaluation function, it is often detrimental to the convergence of the underlying asynchronous DP algorithm, as pointed out by Watkins [81] and illustrated with a simple example by Bradtke <ref> [13] </ref>. Even if a function approximation scheme can adequately represent the optimal evaluation function when trained on samples from this function, it does not follow that an adequate representation will result from an iterative DP algorithm that uses such an approximation scheme at each stage. <p> Unfortunately these results do not address the problem of representing an evaluation function more compactly than it would be represented in a lookup table. Bradtke <ref> [13] </ref> addresses the problem of learning Q-values that are quadratic functions of a continuous state, but these results are restricted to linear quadratic regulation problems.
Reference: [14] <author> D. Chapman. </author> <title> Penquins can make cake. </title> <journal> AI Magazine, </journal> <volume> 10 </volume> <pages> 45-50, </pages> <year> 1989. </year>
Reference-contexts: A closed-loop control policy (also called a closed-loop control rule, law, or strategy) is a rule specifying each action as a function of current, and possibly past, information about the behavior of the controlled system. It closely corresponds to a "universal plan" [64] as discussed, for example, by Chapman <ref> [14] </ref>, Ginsberg [27], and Schoppers [65]. In control theory, a closed-loop control policy usually specifies each action as a function of the controlled system's current state, not just the current values of observable variables (a distinction whose significance for universal planning is discussed by Chapman [14]). <p> discussed, for example, by Chapman <ref> [14] </ref>, Ginsberg [27], and Schoppers [65]. In control theory, a closed-loop control policy usually specifies each action as a function of the controlled system's current state, not just the current values of observable variables (a distinction whose significance for universal planning is discussed by Chapman [14]). Although closed-loop control is closely associated with negative feedback, which counteracts deviations from desired system behavior, negative feedback control is merely a special case of closed-loop control. When there is no uncertainty, closed-loop control is not in principle more competent than open-loop control.
Reference: [15] <author> D. Chapman and L. P. Kaelbling. </author> <title> Input generalization in delayed reinforcement learning: An algorithm and performance comparisons. </title> <booktitle> In Proceedings of the 1991 International Joint Conference on Artificial Intelligence, </booktitle> <year> 1991. </year> <month> 59 </month>
Reference-contexts: This includes symbolic methods for learning from examples. These methods also generalize beyond the training information, which is derived from the back-up operations of various DP-based algorithms. For example, Chapman and Kaelbling <ref> [15] </ref> and Tan [76] adapt decision-tree methods, and Mahadevan and Connell [49] use a statistical clustering method. Yee [94] discusses function approximation from the perspective of its use with DP-based learning algorithms.
Reference: [16] <author> J. Christensen and R. E. Korf. </author> <title> A unified theory of heuristic evaluation functions and its application to learning. </title> <booktitle> In Proceedings of the Fifth National Conference on Artificial Intelligence AAAI-86, </booktitle> <pages> pages 148-152, </pages> <address> San Mateo, CA, 1986. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Minsky [53; 54] discussed similar ideas in the context of the credit assignment problem for reinforcement learning systems; Hampson [28] independently developed some of these ideas and related them to animal behavior; Christensen and Korf <ref> [16] </ref> experimented with a Samuel-like method for updating evaluation function coefficients using linear regression; and Holland's [30] bucket-brigade algorithm for assigning credit in his classifier systems is closely related to Samuel's method.
Reference: [17] <author> J. H. Curtiss. </author> <title> A theoretical comparison of the efficiencies of two classical methods and a monte carlo method for computing one component of the solution of a set of linear algebraic equations. </title> <editor> In H. A. Meyer, editor, </editor> <booktitle> Symposium on Monte Carlo Methods, </booktitle> <pages> pages 191-233. </pages> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1954. </year>
Reference-contexts: RTDP is closely related to Monte Carlo algorithms that achieve computational efficiency by automatically allocating computation so that, for example, unimportant terms in a sum correspond to very rare events in the computational process <ref> [17] </ref>. For this reason, the asymptotic computational complexity of Monte Carlo methods can exceed that of other methods for some classes of problems. However, Monte Carlo methods are generally not competitive with deterministic methods for small problems or when high-precision answers are required.
Reference: [18] <author> J. W. Daniel. </author> <title> Splines and efficiency in dynamic programming. </title> <journal> Journal of Mathematical Analysis and Applications, </journal> <volume> 54 </volume> <pages> 402-407, </pages> <year> 1976. </year>
Reference-contexts: There is an extensive literature on function approximation methods and DP, such as multigrid methods and methods using splines and orthogonal polynomials (e.g., Bellman and Dreyfus [7], Bellman, Kalaba, and Kotkin [8], Daniel <ref> [18] </ref>, Kushner and Dupuis [41]). However, most of this literature is devoted to off-line algorithms for cases in which there is a complete model of the decision problem. Adapting techniques from this literature to produce approximation methods for RTDP and other DP-based learning algorithms is a challenge for future research.
Reference: [19] <author> C. Darken and J. Moody. </author> <title> Note on learning rate schedule for stochastic optimization. </title> <editor> In R. P. Lippmann, J. E. Moody, and D. S. Touretzky, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 832-838, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This is essentially proved by Watkins [81], and Watkins and Dayan present a revised proof in ref. [82]. Appendix B describes a method for meeting the required learning rate conditions that was developed by Darken and Moody <ref> [19] </ref>. We used this method in obtaining the results for Real-Time Q-Learning on our example problems presented in Section 4.1. One can gain insight into Off-Line Q-Learning by relating it to asynchronous DP. <p> We set ff 0 = 0:5 and t = 300. This equation implements a search-then-converge schedule for each ff t (i; u) as suggested by Darken and Moody <ref> [19] </ref>. They argue that such schedules can achieve good performance in stochastic optimization tasks. It can be shown that this schedule satisfies the hypotheses of the Q-Learning convergence theorem.
Reference: [20] <author> P. Dayan. </author> <title> Navigating through temporal difference. </title> <editor> In R. P. Lippmann, J. E. Moody, and D. S. Touretzky, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 464-470, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Sutton's [68] TD () algorithms illustrate this idea. Attempting to present all of the combinations and variations of Q-Learning methods that have been, or could be, described is well beyond the scope of the present article. Barto and Singh [6], Dayan <ref> [20; 21] </ref>, Lin [47; 46], Moore [57], and Sutton [69] present comparative empirical studies of some of the adaptive algorithms based on Q-Learning. 8 Methods based on Explicit Policy Representations All of the DP-based learning algorithms described above, both non-adaptive and adaptive cases, use an explicit representation of either an evaluation
Reference: [21] <author> P. Dayan. </author> <title> Reinforcing Connectionism: Learning the Statistical Way. </title> <type> PhD thesis, </type> <institution> University of Edinburgh, </institution> <year> 1991. </year>
Reference-contexts: Sutton's [68] TD () algorithms illustrate this idea. Attempting to present all of the combinations and variations of Q-Learning methods that have been, or could be, described is well beyond the scope of the present article. Barto and Singh [6], Dayan <ref> [20; 21] </ref>, Lin [47; 46], Moore [57], and Sutton [69] present comparative empirical studies of some of the adaptive algorithms based on Q-Learning. 8 Methods based on Explicit Policy Representations All of the DP-based learning algorithms described above, both non-adaptive and adaptive cases, use an explicit representation of either an evaluation
Reference: [22] <author> P. Dayan. </author> <title> The convergence of TD() for general . Machine Learning, </title> <booktitle> 8 </booktitle> <pages> 341-362, </pages> <year> 1992. </year>
Reference-contexts: This means that each admissible action must be performed in each state infinitely often in an infinite number of control steps. It is also noteworthy, as pointed out by Dayan <ref> [22] </ref>, that when there is only one admissible action for each state, Real-Time Q-Learning reduces to the TD (0) algorithm investigated by Sutton [68]. <p> To the best of our knowledge, there are only a few theoretical results that directly address 40 the use of generalizing methods with DP-based learning algorithms. The results of Sutton [68] and Dayan <ref> [22] </ref> concern using TD methods to evaluate a given policy as a linear combination of a complete set of linearly independent basis vectors. Unfortunately these results do not address the problem of representing an evaluation function more compactly than it would be represented in a lookup table.
Reference: [23] <author> T. L. Dean and M. P. Wellman. </author> <title> Planning and Control. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: A formalism this abstract is potentially applicable to a wide variety of specific problems, but it is not easy to specify exactly what subproblems within complex systems can best take advantage of these methods. In accord with Dean and Wellman <ref> [23] </ref>, we regard DP-based reinforcement learning as a component technology that addresses some of the issues important for developing sophisticated embedded agents but that by itself does not address all of them. <p> Some concepts and methods from control theory are nevertheless relevant to problems of interest in AI as discussed, for example, by Dean and Wellman <ref> [23] </ref>.
Reference: [24] <author> E. V. Denardo. </author> <title> Contraction mappings in the theory underlying dynamic programming. </title> <journal> SIAM Review, </journal> <volume> 9 </volume> <pages> 165-177, </pages> <year> 1967. </year>
Reference-contexts: He further observed, however, that because these problems had been so intensively studied for over thirty years, it would be surprising if no one had studied them earlier. Although the idea of assigning values to state-action pairs formed the basis of Denardo's <ref> [24] </ref> approach to DP, we have not seen algorithms like Q-Learning for estimating these values that predate Watkins' 1989 dissertation. 33 We depart somewhat in our presentation from the view taken by Watkins [81] and others (e.g., Sutton [69], Barto and Singh [6]) of Q-Learning as a method for adaptive on-line
Reference: [25] <author> M. Gardner. </author> <title> Mathematical games. </title> <publisher> Scientific American, </publisher> <address> 228:108, </address> <month> January </month> <year> 1973. </year>
Reference-contexts: Such problems are examples of minimum-time optimal control problems. 4.1 An Example: The Race Track Problem To illustrate the Markovian decision framework, we formalize a game called Race Track described by Martin Gardner <ref> [25] </ref> that simulates automobile racing. We modify the game, which we use in Section 10 to compare the performance of various DP-based learning algorithms, by considering only a single car and by making it probabilistic.
Reference: [26] <author> D. Gelperin. </author> <title> On the optimality of A*. </title> <journal> Artificial Intelligence, </journal> <volume> 8 </volume> <pages> 69-76, </pages> <year> 1977. </year>
Reference-contexts: The key idea in Dyna is that one can perform the computational steps of a DP algorithm sometimes using information obtained from state transitions actually taken 2 We have found only a few exceptions to this in the heuristic search literature in algorithms proposed by Mer~o [51] and Gelperin <ref> [26] </ref>. Although these algorithms use DP-like backups to update heuristic evaluation functions, they were developed independently of DP. 5 by the system being controlled, and sometimes from hypothetical state transitions simulated using a model of this system.
Reference: [27] <author> M. L. Ginsberg. </author> <title> Universal planning: An (almost) universally bad idea. </title> <journal> AI Magazine, </journal> <volume> 10 </volume> <pages> 40-44, </pages> <year> 1989. </year>
Reference-contexts: It closely corresponds to a "universal plan" [64] as discussed, for example, by Chapman [14], Ginsberg <ref> [27] </ref>, and Schoppers [65]. In control theory, a closed-loop control policy usually specifies each action as a function of the controlled system's current state, not just the current values of observable variables (a distinction whose significance for universal planning is discussed by Chapman [14]).
Reference: [28] <author> S. E. Hampson. </author> <title> Connectionist Problem Solving: Computational Aspects of Biological Learning. </title> <publisher> Birkhauser, </publisher> <address> Boston, </address> <year> 1989. </year>
Reference-contexts: Following the proposals of Klopf [36; 37], Sutton and Barto [72; 73; 74] developed these methods as models of animal learning. Minsky [53; 54] discussed similar ideas in the context of the credit assignment problem for reinforcement learning systems; Hampson <ref> [28] </ref> independently developed some of these ideas and related them to animal behavior; Christensen and Korf [16] experimented with a Samuel-like method for updating evaluation function coefficients using linear regression; and Holland's [30] bucket-brigade algorithm for assigning credit in his classifier systems is closely related to Samuel's method.
Reference: [29] <author> P. E. Hart, N. J. Nilsson, and B. Raphael. </author> <title> A formal basis for the heuristic determination of minimum cost paths. </title> <journal> IEEE Transactions of Systems Science and Cybernetics, </journal> <volume> 4 </volume> <pages> 100-107, </pages> <year> 1968. </year>
Reference-contexts: Although some heuristic search algorithms, such as A* <ref> [29] </ref>, update estimates of the costs to reach states from an initial state (A*'s g function), they typically do not update the heuristic evaluation function estimating the cost to reach a goal from each state (the h function). 2 Despite the fact that DP algorithms are exhaustive in the sense described
Reference: [30] <author> J. H. Holland. </author> <title> Escaping brittleness: The possibility of general-purpose learning algo-rithms applied to rule-based systems. </title> <editor> In R. S. Michalski, J. G. Carbonell, and T. M. Mitchell, editors, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, </booktitle> <volume> Volume II, </volume> <pages> pages 593-623. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1986. </year>
Reference-contexts: [53; 54] discussed similar ideas in the context of the credit assignment problem for reinforcement learning systems; Hampson [28] independently developed some of these ideas and related them to animal behavior; Christensen and Korf [16] experimented with a Samuel-like method for updating evaluation function coefficients using linear regression; and Holland's <ref> [30] </ref> bucket-brigade algorithm for assigning credit in his classifier systems is closely related to Samuel's method. Tesauro's recent TD-Gammon [77], a program using a TD method together with a connectionist network to improve performance in playing backgammon, has achieved remarkable success.
Reference: [31] <author> D. H. Jacobson and D. Q. Mayne. </author> <title> Differential Dynamic Programming. </title> <publisher> Elsevier, </publisher> <address> New York, </address> <year> 1970. </year>
Reference-contexts: This excludes various "differential" approaches, which make use of optimization algorithms related to the connectionist error-backpropagation algorithm (e.g., Jacobson and Mayne <ref> [31] </ref>, Jordan and Jacobs [33], Werbos [83; 88], White and Jordan [89]). The relevance of DP for planning and learning in AI was articulated in Sutton's [69] Dyna architecture.
Reference: [32] <author> A. Jalali and M. Ferguson. </author> <title> Computationally efficient adaptive control algorithms for markov chains. </title> <booktitle> In Proceedings of the 28th Conference on Decision and Control, </booktitle> <pages> pages 1283-1288, </pages> <address> Tampa, Florida, </address> <year> 1989. </year>
Reference-contexts: He used the term Incremental Dynamic Programming to refer to this class of algorithms and discussed many examples. Williams and Baird [91] theoretically analysed additional DP-based algorithms suitable for on-time application. We have also come across the work Jalali and Ferguson <ref> [32] </ref>, who independently proposed a method similar to Adaptive RTDP. Sutton, Barto, and Williams [75] discussed reinforcement learning from the perspective of DP and adaptive control, and White and Jordan [89] and Barto [5] provide additional background and extensive references to current research. <p> To the best of our knowledge, the only other work in which explicit use is made of the theory of asynchronous DP for real-time control is that of Jalali and Ferguson <ref> [32] </ref>. Korf's [38] LRTA* algorithm is a heuristic search algorithm that caches state evaluations so that search performance improves with repeated trials. Evaluations of the states visited by the problem solver are maintained in a hash table. <p> Although Sutton's Dyna architecture [69] focuses on Q-Learning and methods based on policy iteration (Section 8), it also encompasses algorithms such as Adaptive RTDP, as he discusses in ref. [70]. Lin [47; 46] also discusses methods closely related to Adaptive RTDP. In the engineering literature, Jalali and Ferguson <ref> [32] </ref> describe an algorithm that is similar to Adaptive RTDP, although they focus on Markovian decision problems in which performance is measured by the average cost per-time-step instead of the discounted cost we have discussed.
Reference: [33] <author> M. I. Jordan and R. A. Jacobs. </author> <title> Learning to control an unstable system with forward modeling. </title> <editor> In D. S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <address> San Mateo, CA, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This excludes various "differential" approaches, which make use of optimization algorithms related to the connectionist error-backpropagation algorithm (e.g., Jacobson and Mayne [31], Jordan and Jacobs <ref> [33] </ref>, Werbos [83; 88], White and Jordan [89]). The relevance of DP for planning and learning in AI was articulated in Sutton's [69] Dyna architecture.
Reference: [34] <author> L. P. Kaelbling. </author> <title> Learning in Embedded Systems. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1991. </year> <note> Revised version of Teleos Research TR-90-04, </note> <month> June </month> <year> 1990. </year>
Reference-contexts: Adaptive optimal control algorithms require mechanisms for resolving these problems, but no mechanism is universally favored. Some of the approaches for which rigorous theoretical results are available are reviewed by Kumar [39], and a variety of more heuristic approaches have been studied by Barto and Singh [6], Kaelbling <ref> [34] </ref>, Moore [55], Schmidhuber [63], Sutton [69], Watkins [81], Thrun [79], and Thrun and Moller [78]. In the following subsections, we describe several non-Bayesian methods for solving Marko-vian decision problems with incomplete information. <p> This strategy produces exploration that aids identification but can conflict with control. Kaelbling <ref> [34] </ref>, Lin [47], Moore [55], Schmidhuber [63], Sutton [69], Thrun [79], and Thrun and Moller [78] discuss these and other possibilities. 7.3 Q-Learning Q-Learning is a method proposed by Watkins [81] for solving Markovian decision problems with incomplete information. 16 Unlike the indirect adaptive methods discussed above, it is a direct
Reference: [35] <author> S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi. </author> <title> Optimization by simulated annealing. </title> <journal> Science, </journal> <volume> 220 </volume> <pages> 671-680, </pages> <year> 1983. </year>
Reference-contexts: As T increases, these probabilities become more uniform, and as T decreases, the probability of executing fl t (s t ) approaches one, while the probabilities of the other actions approach zero. T acts as a kind of "computational temperature" as used in simulated annealing <ref> [35] </ref> in which T decreases over time. Here it controls the necessary tradeoff between identification and control. At "zero temperature" there is no exploration, and the randomized policy equals the certainty equivalence optimal policy, whereas at "infinite temperature" there is no attempt at control.
Reference: [36] <author> A. H. Klopf. </author> <title> Brain function and adaptive systems|A heterostatic theory. </title> <type> Technical Report AFCRL-72-0164, </type> <institution> Air Force Cambridge Research Laboratories, Bedford, </institution> <address> MA, </address> <year> 1972. </year> <booktitle> A summary appears in Proceedings of the International Conference on Systems, Man, and Cybernetics, 1974, IEEE Systems, Man, and Cybernetics Society, </booktitle> <address> Dallas, TX. </address>
Reference-contexts: The algorithm was implemented as a neuron-like connectionist element called the Adaptive Critic Element [2]. Sutton [68] later called these algorithms Temporal Difference (TD) methods and obtained some theoretical results about their convergence. Following the proposals of Klopf <ref> [36; 37] </ref>, Sutton and Barto [72; 73; 74] developed these methods as models of animal learning.
Reference: [37] <author> A. H. Klopf. </author> <title> The Hedonistic Neuron: A Theory of Memory, Learning, </title> <booktitle> and Intelligence. </booktitle> <address> Hemishere, Washington, D.C., </address> <year> 1982. </year>
Reference-contexts: The algorithm was implemented as a neuron-like connectionist element called the Adaptive Critic Element [2]. Sutton [68] later called these algorithms Temporal Difference (TD) methods and obtained some theoretical results about their convergence. Following the proposals of Klopf <ref> [36; 37] </ref>, Sutton and Barto [72; 73; 74] developed these methods as models of animal learning. <p> Watkins [81] describes a family of Q-Learning methods in which Q-values are backed up based on information gained over sequences of state transitions. One way to implement this kind of extension is to use the "eligibility trace" idea <ref> [2; 37; 67; 72; 68] </ref> to back up the Q-values of all the state-action pairs experienced in the past, with the magnitudes of the backups decreasing to zero with increasing time in the past. Sutton's [68] TD () algorithms illustrate this idea.
Reference: [38] <author> R. E. Korf. </author> <title> Real-time heuristic search. </title> <journal> Artificial Intelligence, </journal> <volume> 42 </volume> <pages> 189-211, </pages> <year> 1990. </year>
Reference-contexts: RTDP is the result of recognizing that Korf's <ref> [38] </ref> Learning-Real-Time A* (LRTA*) algorithm 1 closely related to a form of DP known as asynchronous DP [10]. This novel observation permits us to generalize the ideas behind LRTA* so that they apply to real-time problem solving tasks involving uncertainty. <p> To the best of our knowledge, the only other work in which explicit use is made of the theory of asynchronous DP for real-time control is that of Jalali and Ferguson [32]. Korf's <ref> [38] </ref> LRTA* algorithm is a heuristic search algorithm that caches state evaluations so that search performance improves with repeated trials. Evaluations of the states visited by the problem solver are maintained in a hash table. <p> LRTA* therefore backs up state evaluations in much the same way as do Samuel's algorithm and DP. In fact, as we shall see in what follows, with a slight caveat, LRTA* is the deterministic specialization of asynchronous DP applied on-line. 3 In Korf's <ref> [38] </ref> related Real-Time A* (RTA*) algorithm, the second smallest score is stored. <p> However, unlike a heuristic search algorithm, DP produces an optimal closed-loop policy instead of an open-loop policy for a given initial state. 3.3 Real-Time Heuristic Search Algorithms for real-time heuristic search as defined by Korf <ref> [38] </ref> apply to state-space search problems in which the underlying model is extended to account for the passage of time. The model thus becomes a dynamic system. <p> Thus, whereas a traditional heuristic search algorithm is a design procedure for an open-loop policy, a real-time heuristic search algorithm is a control procedure, and it can accommodate the possibility of closed-loop control. Korf's <ref> [38] </ref> LRTA* algorithm is a kind of receding horizon control because it is an online method for designing a closed-loop policy. <p> We discuss this latter objective for exploration in Section 7. 6.3 RTDP and LRTA* Theorem 3 is a generalization of Korf's <ref> [38] </ref> convergence theorem for LRTA*. RTDP extends LRTA* in two ways: it generalizes LRTA* to stochastic problems, and it includes the option of backing up the costs of many states in the time intervals between the execution of actions. <p> However, in the general case, when RTDP backs up more than one state's cost during each time interval, it makes sense to use the latest estimate of f fl to select an action. An extended form of LRTA* can also be related to RTDP. In his discussion, Korf <ref> [38] </ref> assumes that the evaluation of a state may be augmented by look-ahead search. <p> A number of methods exist for making the lookup-table representation more efficient when it is not necessary to store the costs of all possible states. Hash table methods, as assumed by Korf <ref> [38] </ref> for LRTA*, permit efficient storage and retrieval when the costs of a small enough subset of the possible states need to be stored. <p> Doing this so as to satisfy certain requirements results in the algorithm we call RTDP, a special case of which essentially coincides with Korf's LRTA* <ref> [38] </ref> algorithm. This general approach follows previous research by others in which DP principles have been used for problem solving and learning (e.g., refs. [61; 69; 70; 81; 86; 87]). <p> If this caution is recognized, however, the algorithms described in this article should find wide application as components of sophisticated embedded systems. 55 12 Appendices A. Proof of the Trial-Based RTDP Theorem Here we prove Theorem 3, which extends Korf's <ref> [38] </ref> convergence theorem for LRTA* to Trial-Based RTDP applied to undiscounted stochastic shortest path problems.
Reference: [39] <author> P. R. Kumar. </author> <title> A survey of some results in stochastic adaptive control. </title> <journal> SIAM Journal of Control and Optimization, </journal> <volume> 23 </volume> <pages> 329-380, </pages> <year> 1985. </year>
Reference-contexts: Actions may not be optimal on the basis of prior assumptions and accumulated observations, but the policy should approach an optimal policy in the limit as experience accumulates. Kumar <ref> [39] </ref> surveys the large literature on both classes of methods and conveys the sublety of the issues as well as the sophistication of the existing theoretical results. Here we restrict attention to non-Bayesian methods because they are more practical for large problems. Two types of non-Bayesian methods are distinguished. <p> Direct methods also require exploration and involve these same issues. Adaptive optimal control algorithms require mechanisms for resolving these problems, but no mechanism is universally favored. Some of the approaches for which rigorous theoretical results are available are reviewed by Kumar <ref> [39] </ref>, and a variety of more heuristic approaches have been studied by Barto and Singh [6], Kaelbling [34], Moore [55], Schmidhuber [63], Sutton [69], Watkins [81], Thrun [79], and Thrun and Moller [78]. In the following subsections, we describe several non-Bayesian methods for solving Marko-vian decision problems with incomplete information. <p> It is easy to generate examples in which always following the current certainty equivalence optimal policy prevents convergence to a true optimal policy due to lack of exploration (see, for example, Kumar <ref> [39] </ref>). One of the simplest ways to induce exploratory behavior is to make the controller use randomized policies in which actions are chosen according to probabilities that depend on the current evaluation function.
Reference: [40] <author> V. Kumar and L. N. Kanal. </author> <title> The CDP: A unifying formulation for heuristic search, dynamic programming, and branch-and-bound. </title> <editor> In L. N. Kanal and V. Kumar, editors, </editor> <booktitle> Search in Artificial Intelligence, </booktitle> <pages> pages 1-37. </pages> <publisher> Springer-Verlag, </publisher> <year> 1988. </year>
Reference-contexts: In its most general form, DP applies to optimization problems in which the costs of objects in the search space have a compositional structure that can be exploited to find an object of globally minimum cost without performing exhaustive search. Kumar and Kanal <ref> [40] </ref> discuss DP at this level of generality. However, we restrict attention to DP as it applies to problems in which the objects are state sequences that can be generated in problem-solving or control tasks.
Reference: [41] <author> H. J. Kushner and P. </author> <title> Dupuis. Numerical Methods for Stochastic Control Problems in Continuous Time. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: There is an extensive literature on function approximation methods and DP, such as multigrid methods and methods using splines and orthogonal polynomials (e.g., Bellman and Dreyfus [7], Bellman, Kalaba, and Kotkin [8], Daniel [18], Kushner and Dupuis <ref> [41] </ref>). However, most of this literature is devoted to off-line algorithms for cases in which there is a complete model of the decision problem. Adapting techniques from this literature to produce approximation methods for RTDP and other DP-based learning algorithms is a challenge for future research.
Reference: [42] <author> W. H. Kwon and A. E. Pearson. </author> <title> A modified quadratic cost problem and feedback stabilization of a linear system. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 22 </volume> <pages> 838-842, </pages> <year> 1977. </year>
Reference-contexts: Changing control objectives, on the other hand, often does require policy re-design. One can also design closed-loop policies on-line through repeated on-line design of open-loop policies. This approach has been called receding horizon control <ref> [42; 50] </ref>. For each current state, an open-loop policy is designed with the current state playing the role of the initial state. The design procedure must terminate within the time constraints imposed by on-line operation.
Reference: [43] <author> Y. le Cun. </author> <title> A theoretical framework for back-propagation. </title> <editor> In D. Touretzky, G. Hinton, and T. Sejnowski, editors, </editor> <booktitle> Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <pages> pages 21-28. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1988. </year>
Reference-contexts: Numerical methods applicable to problems involving nonlinear systems and/or nonquadratic costs include gradient methods as well as DP. Whereas gradient methods for optimal control are closely related to some of the gradient descent methods being studied by connectionists (such as the error-backpropagation 9 algorithm <ref> [43; 85; 89] </ref>), DP methods are more closely related to heuristic search. Like a heuristic search algorithm, DP is an off-line procedure for designing an optimal control policy.
Reference: [44] <author> M. Lemmon. </author> <title> Real-time optimal path planning using a distributed computing paradigm. </title> <booktitle> In Proceedings of the American Control Conference, </booktitle> <address> Boston, MA, </address> <year> 1991. </year>
Reference-contexts: Multiprocessor implementations have obvious utility in speeding up DP and thus have practical significance for all the algorithms we discuss below (see, e.g., Lemmon <ref> [44] </ref>). However, our interest in asynchronous DP lies in the fact that it does not require state costs to be backed up in any systematically organized fashion.
Reference: [45] <author> Long-Ji Lin. </author> <title> Programming robots using reinforcement learning and teaching. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 781-786, </pages> <address> Cambridge, MA, 1991. </address> <publisher> MIT Press. </publisher>
Reference-contexts: This skill does not transfer to other tracks due to the specificity with which a track is represented. More realistic applications of DP-based learning to robot navigation requires more abstract states and actions, as in the work of Lin <ref> [45] </ref> and Mahadevan and Connell [49]. 11 Discussion Conventional DP algorithms are of limited utility for problems with large state spaces, such as the combinatorial state spaces of many problems of interest in AI, because they require fully expanding all possible states and storing a cost for each state.
Reference: [46] <author> Long-Ji Lin. </author> <title> Self-improvement based on reinforcement learning, planning and teaching. </title> <editor> In L. A. Birnbaum and G. C. Collins, editors, </editor> <booktitle> Maching Learning: Proceedings of the Eighth International Workshop, </booktitle> <pages> pages 323-327, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: It also makes sense for RTDP to back up the costs of states whose current costs are not yet accurate estimates of their optimal costs but whose successor states do have accurate current costs. Techniques for "teaching" DP-based learning systems by suggesting certain back ups over others (refs. <ref> [46; 90; 80] </ref>) rely on the fact that the order in which the costs of states are backed up can influence the rate of convergence of asynchronous DP, whether applied off- or on-line. <p> Although Sutton's Dyna architecture [69] focuses on Q-Learning and methods based on policy iteration (Section 8), it also encompasses algorithms such as Adaptive RTDP, as he discusses in ref. [70]. Lin <ref> [47; 46] </ref> also discusses methods closely related to Adaptive RTDP. <p> Sutton's [68] TD () algorithms illustrate this idea. Attempting to present all of the combinations and variations of Q-Learning methods that have been, or could be, described is well beyond the scope of the present article. Barto and Singh [6], Dayan [20; 21], Lin <ref> [47; 46] </ref>, Moore [57], and Sutton [69] present comparative empirical studies of some of the adaptive algorithms based on Q-Learning. 8 Methods based on Explicit Policy Representations All of the DP-based learning algorithms described above, both non-adaptive and adaptive cases, use an explicit representation of either an evaluation function or a
Reference: [47] <author> Long-Ji Lin. </author> <title> Self-improving reactive agents: Case studies of reinforcement learning frameworks. </title> <booktitle> In From Animals to Animats: Proceedings of the First International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> pages 297-305, </pages> <address> Cambridge, MA, 1991. </address> <publisher> MIT Press. </publisher>
Reference-contexts: action, the maximum-likelihood estimate of an immediate cost is the observed average of the immediate cost for that state and action. 31 facilitate comparison of algorithms in the simulations described in Section 4.1, we adopt the action-selection method based on the Boltzmann distribution that was used by Watkins [81], Lin <ref> [47] </ref>, and Sutton [69]. This method assigns an execution probability to each admissible action for the current state, where this probability is determined by a rating of each action's utility. <p> Although Sutton's Dyna architecture [69] focuses on Q-Learning and methods based on policy iteration (Section 8), it also encompasses algorithms such as Adaptive RTDP, as he discusses in ref. [70]. Lin <ref> [47; 46] </ref> also discusses methods closely related to Adaptive RTDP. <p> This strategy produces exploration that aids identification but can conflict with control. Kaelbling [34], Lin <ref> [47] </ref>, Moore [55], Schmidhuber [63], Sutton [69], Thrun [79], and Thrun and Moller [78] discuss these and other possibilities. 7.3 Q-Learning Q-Learning is a method proposed by Watkins [81] for solving Markovian decision problems with incomplete information. 16 Unlike the indirect adaptive methods discussed above, it is a direct method because <p> It is also possible to modify the basic Q-Learning method in a variety of ways in order to enhance its efficiency. For example, Lin <ref> [47] </ref> has studied a method in which Real-Time 37 Q-Learning is augmented with model-based Off-Line Q-Learning only if one action does not clearly stand out as preferable according to the current Q-values. <p> Sutton's [68] TD () algorithms illustrate this idea. Attempting to present all of the combinations and variations of Q-Learning methods that have been, or could be, described is well beyond the scope of the present article. Barto and Singh [6], Dayan [20; 21], Lin <ref> [47; 46] </ref>, Moore [57], and Sutton [69] present comparative empirical studies of some of the adaptive algorithms based on Q-Learning. 8 Methods based on Explicit Policy Representations All of the DP-based learning algorithms described above, both non-adaptive and adaptive cases, use an explicit representation of either an evaluation function or a
Reference: [48] <author> Long-Ji Lin. </author> <title> Self-improving reactive agents based on reinforcement learning, planning and teaching. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 293-321, </pages> <year> 1992. </year>
Reference-contexts: Further, as in Gauss-Seidel DP, the order in which states' costs are backed up can influence the rate of convergence in a problem-dependent way. This fact underlies the utility of various strategies for "teaching" DP-based learning algorithms by supplying experience dictating selected orderings of the backups (e.g., Lin <ref> [48] </ref>, Utgoff and Clouse [80], and Whitehead [90]). 6 Dynamic Programming in Real Time The DP algorithms described above are off-line algorithms for solving Markovian decision problems.
Reference: [49] <author> S. Mahadevan and J. Connell. </author> <title> Automatic programming of behavior-based robots using reinforcement learning. </title> <journal> Artificial Intelligence, </journal> <volume> 55 </volume> <pages> 311-365, </pages> <year> 1992. </year>
Reference-contexts: This includes symbolic methods for learning from examples. These methods also generalize beyond the training information, which is derived from the back-up operations of various DP-based algorithms. For example, Chapman and Kaelbling [15] and Tan [76] adapt decision-tree methods, and Mahadevan and Connell <ref> [49] </ref> use a statistical clustering method. Yee [94] discusses function approximation from the perspective of its use with DP-based learning algorithms. <p> This skill does not transfer to other tracks due to the specificity with which a track is represented. More realistic applications of DP-based learning to robot navigation requires more abstract states and actions, as in the work of Lin [45] and Mahadevan and Connell <ref> [49] </ref>. 11 Discussion Conventional DP algorithms are of limited utility for problems with large state spaces, such as the combinatorial state spaces of many problems of interest in AI, because they require fully expanding all possible states and storing a cost for each state.
Reference: [50] <author> D. Q. Mayne and H. Michalska. </author> <title> Receding horizon control of nonlinear systems. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 35 </volume> <pages> 814-824, </pages> <year> 1990. </year>
Reference-contexts: Changing control objectives, on the other hand, often does require policy re-design. One can also design closed-loop policies on-line through repeated on-line design of open-loop policies. This approach has been called receding horizon control <ref> [42; 50] </ref>. For each current state, an open-loop policy is designed with the current state playing the role of the initial state. The design procedure must terminate within the time constraints imposed by on-line operation.
Reference: [51] <author> L. Mer~o. </author> <title> A heuristic search algorithm with modifiable estimate. </title> <journal> Artificial Intelligence, </journal> <volume> 23 </volume> <pages> 13-27, </pages> <year> 1984. </year>
Reference-contexts: The key idea in Dyna is that one can perform the computational steps of a DP algorithm sometimes using information obtained from state transitions actually taken 2 We have found only a few exceptions to this in the heuristic search literature in algorithms proposed by Mer~o <ref> [51] </ref> and Gelperin [26]. Although these algorithms use DP-like backups to update heuristic evaluation functions, they were developed independently of DP. 5 by the system being controlled, and sometimes from hypothetical state transitions simulated using a model of this system.
Reference: [52] <author> D. Michie and R. A. Chambers. </author> <title> BOXES: An experiment in adaptive control. </title> <editor> In E. Dale and D. Michie, editors, </editor> <booktitle> Machine Intelligence 2, </booktitle> <pages> pages 137-152. </pages> <publisher> Oliver and Boyd, </publisher> <year> 1968. </year>
Reference-contexts: In applying conventional DP to problems involving continuous states and/or actions, the usual practice is to discretize the ranges of the continuous state variables and then use the lookup-table representation (cf. the "boxes" representation used by Michie and Chambers <ref> [52] </ref> and Barto, Sutton, and Anderson [2]).
Reference: [53] <author> M. L. Minsky. </author> <title> Theory of Neural-Analog Reinforcement Systems and its Application to the Brain-Model Problem. </title> <type> PhD thesis, </type> <institution> Princeton University, </institution> <year> 1954. </year>
Reference-contexts: Sutton [68] later called these algorithms Temporal Difference (TD) methods and obtained some theoretical results about their convergence. Following the proposals of Klopf [36; 37], Sutton and Barto [72; 73; 74] developed these methods as models of animal learning. Minsky <ref> [53; 54] </ref> discussed similar ideas in the context of the credit assignment problem for reinforcement learning systems; Hampson [28] independently developed some of these ideas and related them to animal behavior; Christensen and Korf [16] experimented with a Samuel-like method for updating evaluation function coefficients using linear regression; and Holland's [30]
Reference: [54] <author> M. L. Minsky. </author> <booktitle> Steps toward artificial intelligence. Proceedings of the Institute of Radio Engineers, </booktitle> <volume> 49 </volume> <pages> 8-30, </pages> <year> 1961. </year> <note> Reprinted in E. </note> <editor> A. Feigenbaum and J. Feldman, editors, </editor> <booktitle> Computers and Thought. </booktitle> <publisher> McGraw-Hill, </publisher> <address> New York, 406-450, </address> <year> 1963. </year> <month> 62 </month>
Reference-contexts: Sutton [68] later called these algorithms Temporal Difference (TD) methods and obtained some theoretical results about their convergence. Following the proposals of Klopf [36; 37], Sutton and Barto [72; 73; 74] developed these methods as models of animal learning. Minsky <ref> [53; 54] </ref> discussed similar ideas in the context of the credit assignment problem for reinforcement learning systems; Hampson [28] independently developed some of these ideas and related them to animal behavior; Christensen and Korf [16] experimented with a Samuel-like method for updating evaluation function coefficients using linear regression; and Holland's [30]
Reference: [55] <author> A. W. Moore. </author> <title> Efficient Memory-Based Learning for Robot Control. </title> <type> PhD thesis, </type> <institution> Uni--versity of Cambridge, </institution> <address> Cambridge, UK, </address> <year> 1990. </year>
Reference-contexts: Some of the approaches for which rigorous theoretical results are available are reviewed by Kumar [39], and a variety of more heuristic approaches have been studied by Barto and Singh [6], Kaelbling [34], Moore <ref> [55] </ref>, Schmidhuber [63], Sutton [69], Watkins [81], Thrun [79], and Thrun and Moller [78]. In the following subsections, we describe several non-Bayesian methods for solving Marko-vian decision problems with incomplete information. <p> This strategy produces exploration that aids identification but can conflict with control. Kaelbling [34], Lin [47], Moore <ref> [55] </ref>, Schmidhuber [63], Sutton [69], Thrun [79], and Thrun and Moller [78] discuss these and other possibilities. 7.3 Q-Learning Q-Learning is a method proposed by Watkins [81] for solving Markovian decision problems with incomplete information. 16 Unlike the indirect adaptive methods discussed above, it is a direct method because it does <p> Hash table methods, as assumed by Korf [38] for LRTA*, permit efficient storage and retrieval when the costs of a small enough subset of the possible states need to be stored. Similarly, using the kd-tree data structure to access state costs, as explored by Moore <ref> [55; 56] </ref>, can provide efficient storage and retrieval of the costs of a finite set of states from a k-dimensional state space. The theoretical results described in this article extend to these methods because they preserve the integrity of the stored costs (assuming hash collisions are resolved).
Reference: [56] <author> A. W. Moore. </author> <title> Variable resolution dynamic programming: Efficiently learning action maps in multivariate real-valued state-spaces. </title> <editor> In L. A. Birnbaum and G. C. Collins, editors, </editor> <booktitle> Maching Learning: Proceedings of the Eighth International Workshop, </booktitle> <pages> pages 333-337, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Hash table methods, as assumed by Korf [38] for LRTA*, permit efficient storage and retrieval when the costs of a small enough subset of the possible states need to be stored. Similarly, using the kd-tree data structure to access state costs, as explored by Moore <ref> [55; 56] </ref>, can provide efficient storage and retrieval of the costs of a finite set of states from a k-dimensional state space. The theoretical results described in this article extend to these methods because they preserve the integrity of the stored costs (assuming hash collisions are resolved).
Reference: [57] <author> A. W. Moore and C. G. Atkeson. </author> <title> Memory-based reinforcement learning: Efficient computation with prioritized sweeping. </title> <booktitle> In Advances in Neural Information Processing 4, </booktitle> <address> San Mateo, CA. </address> <note> Morgan Kaufmann. To appear. </note>
Reference-contexts: A promising approach recently developed by Peng and Williams [58] and Moore and Atkeson <ref> [57] </ref>, which the latter authors call "prioritized sweeping," directs the application of DP backups to the most likely predecessors of states whose costs change significantly. <p> Sutton's [68] TD () algorithms illustrate this idea. Attempting to present all of the combinations and variations of Q-Learning methods that have been, or could be, described is well beyond the scope of the present article. Barto and Singh [6], Dayan [20; 21], Lin [47; 46], Moore <ref> [57] </ref>, and Sutton [69] present comparative empirical studies of some of the adaptive algorithms based on Q-Learning. 8 Methods based on Explicit Policy Representations All of the DP-based learning algorithms described above, both non-adaptive and adaptive cases, use an explicit representation of either an evaluation function or a function giving the
Reference: [58] <author> J. Peng and R. J. Williams. </author> <title> Efficient learning and planning within the dyna framework. </title> <booktitle> In Proceedings of the Second International Conference on Simulation of Adaptive Behavior, </booktitle> <address> Honolulu, HI. </address> <note> To appear. </note>
Reference-contexts: A promising approach recently developed by Peng and Williams <ref> [58] </ref> and Moore and Atkeson [57], which the latter authors call "prioritized sweeping," directs the application of DP backups to the most likely predecessors of states whose costs change significantly.
Reference: [59] <author> M. L. Puterman and M. C. Shin. </author> <title> Modified policy iteration algorithms for discounted Markov decision problems. </title> <journal> Management Science, </journal> <volume> 24 </volume> <pages> 1127-1137, </pages> <year> 1978. </year>
Reference-contexts: Alternatively, explicit matrix inversion methods can be used. Although policy evaluation does not require repeated minimizing over all admissible actions, it can still require too much computation to be practical for large state sets. More feasible is modified policy interation <ref> [59] </ref>, which is policy iteration except that the policy evaluation phase is not executed to completion before each policy improvement phase. Real-time algorithms based on policy iteration effectively work by executing an asynchronous form of modified policy iteration concurrently with control.
Reference: [60] <author> S. Ross. </author> <title> Introduction to Stochastic Dynamic Programming. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: Many problems of practical importance have been formulated as Markovian decision problems, and extensive treatment of the theory and application of this framework can be found in many books, such as those by Bertsekas [11] and Ross <ref> [60] </ref>. A Markovian decision problem is defined in terms of a discrete-time stochastic dynamic system with finite state set S = f1; : : : ; ng. Time is represented by a sequence of time steps t = 0; 1; : : :.
Reference: [61] <author> A. L. Samuel. </author> <title> Some studies in machine learning using the game of checkers. </title> <journal> IBM Journal on Research and Development, </journal> <pages> pages 210-229, </pages> <year> 1959. </year> <note> Reprinted in E. </note> <editor> A. Feigenbaum and J. Feldman, editors, </editor> <booktitle> Computers and Thought, </booktitle> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1963. </year>
Reference-contexts: We conclude in Section 11 with an appraisal of the significance of our approach and discuss some of the open problems. 2 Background A major influence on research leading to current DP-based algorithms has been the method Samuel <ref> [61; 62] </ref> used to modify a heuristic evaluation function for the game of checkers. <p> of a board position likely to arise later in the game: 3 . . . we are attempting to make the score, calculated for the current board position, look like that calculated for the terminal board position of the chain of moves which most probably occur during actual play. (Samuel <ref> [61] </ref>.) As a result of this process of "backing up" board evaluations, the evaluation function should improve in its ability to evaluate the long-term consequences of moves. <p> According to what we mean by adaptive control in this article, even though algorithms like LRTA* and Samuel's algorithm <ref> [61] </ref> are learning algorithms, they are not adaptive control algorithms because they assume the existence of an accurate model of the problem being solved. <p> Despite its not being a real-time computation, we regard the concurrent execution of DP and control in simulation 23 mode to be a form of learning. This is in fact how learning was accomplished in the game--playing programs of Samuel <ref> [61; 62] </ref> and Tesauro [77]. Learning occurred during many simulated games in which these learning systems competed against themselves. Although we emphasize the real-time use of DP-based learning algorithms, the reader should be aware that our discussion also applies to the use of these algorithms in simulation mode. <p> The theoretical results described in this article extend to these methods because they preserve the integrity of the stored costs (assuming hash collisions are resolved). Other approaches to storing evaluation functions use function approximation methods based on parameterized models. For example, in Samuel's <ref> [61] </ref> checkers player, the evaluation function was approximated as a weighted sum of the values of a set of features describing checkerboard configurations. <p> Doing this so as to satisfy certain requirements results in the algorithm we call RTDP, a special case of which essentially coincides with Korf's LRTA* [38] algorithm. This general approach follows previous research by others in which DP principles have been used for problem solving and learning (e.g., refs. <ref> [61; 69; 70; 81; 86; 87] </ref>). Our contribution in this article has been to bring to bear on DP-based learning the theory of asynchronous DP as presented by Bertsekas and Tsitsiklis [12]. <p> We used the term simulation mode to refer to the execution of RTDP and related algorithms during simulated control instead of actual control. DP-based learning in simulation mode is illustrated by Samuel's checkers playing system <ref> [61; 62] </ref>, Tesauro's backgammon playing system [77], and our illustrations of RTDP using the race track problem.
Reference: [62] <author> A. L. Samuel. </author> <title> Some studies in machine learning using the game of checkers. </title> <journal> II|Recent progress. IBM Journal on Research and Development, </journal> <pages> pages 601-617, </pages> <month> November </month> <year> 1967. </year>
Reference-contexts: We conclude in Section 11 with an appraisal of the significance of our approach and discuss some of the open problems. 2 Background A major influence on research leading to current DP-based algorithms has been the method Samuel <ref> [61; 62] </ref> used to modify a heuristic evaluation function for the game of checkers. <p> Despite its not being a real-time computation, we regard the concurrent execution of DP and control in simulation 23 mode to be a form of learning. This is in fact how learning was accomplished in the game--playing programs of Samuel <ref> [61; 62] </ref> and Tesauro [77]. Learning occurred during many simulated games in which these learning systems competed against themselves. Although we emphasize the real-time use of DP-based learning algorithms, the reader should be aware that our discussion also applies to the use of these algorithms in simulation mode. <p> We used the term simulation mode to refer to the execution of RTDP and related algorithms during simulated control instead of actual control. DP-based learning in simulation mode is illustrated by Samuel's checkers playing system <ref> [61; 62] </ref>, Tesauro's backgammon playing system [77], and our illustrations of RTDP using the race track problem.
Reference: [63] <author> J. Schmidhuber. </author> <title> Adaptive confidence and adaptive curiosity. </title> <type> Technical Report FKI-149-91, </type> <institution> Institut fur Informatik, Technische Universitat Munchen, Arcisstr. </institution> <type> 21, 800 Munchen 2, </type> <address> Germany, </address> <year> 1991. </year>
Reference-contexts: Some of the approaches for which rigorous theoretical results are available are reviewed by Kumar [39], and a variety of more heuristic approaches have been studied by Barto and Singh [6], Kaelbling [34], Moore [55], Schmidhuber <ref> [63] </ref>, Sutton [69], Watkins [81], Thrun [79], and Thrun and Moller [78]. In the following subsections, we describe several non-Bayesian methods for solving Marko-vian decision problems with incomplete information. <p> This strategy produces exploration that aids identification but can conflict with control. Kaelbling [34], Lin [47], Moore [55], Schmidhuber <ref> [63] </ref>, Sutton [69], Thrun [79], and Thrun and Moller [78] discuss these and other possibilities. 7.3 Q-Learning Q-Learning is a method proposed by Watkins [81] for solving Markovian decision problems with incomplete information. 16 Unlike the indirect adaptive methods discussed above, it is a direct method because it does not use
Reference: [64] <author> M. J. Schoppers. </author> <title> Universal plans for reactive robots in unpredictable environments. </title> <booktitle> In Proceedings of the Tenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1039-1046, </pages> <address> Menlo Park, CA, </address> <year> 1987. </year>
Reference-contexts: A closed-loop control policy (also called a closed-loop control rule, law, or strategy) is a rule specifying each action as a function of current, and possibly past, information about the behavior of the controlled system. It closely corresponds to a "universal plan" <ref> [64] </ref> as discussed, for example, by Chapman [14], Ginsberg [27], and Schoppers [65].
Reference: [65] <author> M. J. Schoppers. </author> <title> In defense of reaction plans as caches. </title> <journal> AI Magazine, </journal> <volume> 10 </volume> <pages> 51-60, </pages> <year> 1989. </year>
Reference-contexts: It closely corresponds to a "universal plan" [64] as discussed, for example, by Chapman [14], Ginsberg [27], and Schoppers <ref> [65] </ref>. In control theory, a closed-loop control policy usually specifies each action as a function of the controlled system's current state, not just the current values of observable variables (a distinction whose significance for universal planning is discussed by Chapman [14]).
Reference: [66] <author> S. P. Singh and R. C. Yee. </author> <title> An upper bound on the loss from approximate optimal value functions. </title> <type> Technical report. </type> <note> Submitted as Technical Note to Machine Learning. </note>
Reference-contexts: Bradtke [13] addresses the problem of learning Q-values that are quadratic functions of a continuous state, but these results are restricted to linear quadratic regulation problems. However, Singh and Yee <ref> [66] </ref> point out that in the discounted case, small errors in approximating an evaluation function (or a function giving Q-values) lead at worst to small decrements in the performance of a controller using the approximate evaluation function as the basis of control.
Reference: [67] <author> R. S. Sutton. </author> <title> Temporal Credit Assignment in Reinforcement Learning. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, </institution> <address> Amherst, MA, </address> <year> 1984. </year> <month> 63 </month>
Reference-contexts: Because of its compatibility with connectionist learning algorithms, this approach was refined and extended by Sutton <ref> [67; 68] </ref> and used heuristically in a number of single-agent problem-solving tasks (e.g., Barto, Sutton, and Anderson [2], Anderson [1], and Sutton [67]). The algorithm was implemented as a neuron-like connectionist element called the Adaptive Critic Element [2]. <p> Because of its compatibility with connectionist learning algorithms, this approach was refined and extended by Sutton [67; 68] and used heuristically in a number of single-agent problem-solving tasks (e.g., Barto, Sutton, and Anderson [2], Anderson [1], and Sutton <ref> [67] </ref>). The algorithm was implemented as a neuron-like connectionist element called the Adaptive Critic Element [2]. Sutton [68] later called these algorithms Temporal Difference (TD) methods and obtained some theoretical results about their convergence. <p> These are special kinds of the stochastic shortest path problems that address the issue of delayed reinforcement <ref> [67] </ref> in a particularly stark form. Rewards correspond to negative costs in the formalism we are using. In the discounted case when all the rewards are of the same magnitude, an optimal policy produces a shortest path to a rewarding state. <p> Watkins [81] describes a family of Q-Learning methods in which Q-values are backed up based on information gained over sequences of state transitions. One way to implement this kind of extension is to use the "eligibility trace" idea <ref> [2; 37; 67; 72; 68] </ref> to back up the Q-values of all the state-action pairs experienced in the past, with the magnitudes of the backups decreasing to zero with increasing time in the past. Sutton's [68] TD () algorithms illustrate this idea. <p> Real-time algorithms based on policy iteration effectively work by executing an asynchronous form of modified policy iteration concurrently with control. Examples of such methods appear in the pole-balancing system of Barto, Sutton, and Anderson <ref> [2; 67] </ref> (also refs. [1; 67]) and the Dyna-PI method of Sutton [69] (where PI means 38 Policy Iteration). Barto, Sutton, and Watkins [4; 3] discuss the connection between these methods and policy iteration in some detail. <p> Real-time algorithms based on policy iteration effectively work by executing an asynchronous form of modified policy iteration concurrently with control. Examples of such methods appear in the pole-balancing system of Barto, Sutton, and Anderson [2; 67] (also refs. <ref> [1; 67] </ref>) and the Dyna-PI method of Sutton [69] (where PI means 38 Policy Iteration). Barto, Sutton, and Watkins [4; 3] discuss the connection between these methods and policy iteration in some detail.
Reference: [68] <author> R. S. Sutton. </author> <title> Learning to predict by the method of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44, </pages> <year> 1988. </year>
Reference-contexts: Because of its compatibility with connectionist learning algorithms, this approach was refined and extended by Sutton <ref> [67; 68] </ref> and used heuristically in a number of single-agent problem-solving tasks (e.g., Barto, Sutton, and Anderson [2], Anderson [1], and Sutton [67]). The algorithm was implemented as a neuron-like connectionist element called the Adaptive Critic Element [2]. <p> The algorithm was implemented as a neuron-like connectionist element called the Adaptive Critic Element [2]. Sutton <ref> [68] </ref> later called these algorithms Temporal Difference (TD) methods and obtained some theoretical results about their convergence. Following the proposals of Klopf [36; 37], Sutton and Barto [72; 73; 74] developed these methods as models of animal learning. <p> It is also noteworthy, as pointed out by Dayan [22], that when there is only one admissible action for each state, Real-Time Q-Learning reduces to the TD (0) algorithm investigated by Sutton <ref> [68] </ref>. To define a complete adaptive control algorithm making use of Real-Time Q-Learning it is necessary to specify how each action is selected based on the current Q-values. Convergence to an optimal policy requires the same kind of exploration required by indirect methods to facilitate system identification as discussed above. <p> Watkins [81] describes a family of Q-Learning methods in which Q-values are backed up based on information gained over sequences of state transitions. One way to implement this kind of extension is to use the "eligibility trace" idea <ref> [2; 37; 67; 72; 68] </ref> to back up the Q-values of all the state-action pairs experienced in the past, with the magnitudes of the backups decreasing to zero with increasing time in the past. Sutton's [68] TD () algorithms illustrate this idea. <p> Sutton's <ref> [68] </ref> TD () algorithms illustrate this idea. Attempting to present all of the combinations and variations of Q-Learning methods that have been, or could be, described is well beyond the scope of the present article. <p> To the best of our knowledge, there are only a few theoretical results that directly address 40 the use of generalizing methods with DP-based learning algorithms. The results of Sutton <ref> [68] </ref> and Dayan [22] concern using TD methods to evaluate a given policy as a linear combination of a complete set of linearly independent basis vectors. Unfortunately these results do not address the problem of representing an evaluation function more compactly than it would be represented in a lookup table.
Reference: [69] <author> R. S. Sutton. </author> <title> Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <pages> pages 216-224, </pages> <address> San Mateo, CA, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Learning algorithms based on DP employ novel means for improving the computational efficiency of conventional DP algorithms. Werbos [86; 88] and Watkins [81] proposed incremental versions of DP as learning algorithms, and Sutton's Dyna architecture for learning, planning, and reacting <ref> [69; 70] </ref> is based on these principles. <p> This excludes various "differential" approaches, which make use of optimization algorithms related to the connectionist error-backpropagation algorithm (e.g., Jacobson and Mayne [31], Jordan and Jacobs [33], Werbos [83; 88], White and Jordan [89]). The relevance of DP for planning and learning in AI was articulated in Sutton's <ref> [69] </ref> Dyna architecture. <p> Some of the approaches for which rigorous theoretical results are available are reviewed by Kumar [39], and a variety of more heuristic approaches have been studied by Barto and Singh [6], Kaelbling [34], Moore [55], Schmidhuber [63], Sutton <ref> [69] </ref>, Watkins [81], Thrun [79], and Thrun and Moller [78]. In the following subsections, we describe several non-Bayesian methods for solving Marko-vian decision problems with incomplete information. <p> estimate of an immediate cost is the observed average of the immediate cost for that state and action. 31 facilitate comparison of algorithms in the simulations described in Section 4.1, we adopt the action-selection method based on the Boltzmann distribution that was used by Watkins [81], Lin [47], and Sutton <ref> [69] </ref>. This method assigns an execution probability to each admissible action for the current state, where this probability is determined by a rating of each action's utility. <p> Although Sutton's Dyna architecture <ref> [69] </ref> focuses on Q-Learning and methods based on policy iteration (Section 8), it also encompasses algorithms such as Adaptive RTDP, as he discusses in ref. [70]. Lin [47; 46] also discusses methods closely related to Adaptive RTDP. <p> Performing RTDP concurrently with system identification, as in Adaptive RTDP, provides an opportunity to let progress in identification influence the selection of states to which the backup operation is applied. Sutton <ref> [69] </ref> suggested that it can be advantageous to back up the costs of states for which there is good confidence in the accuracy of the estimated state-transition probabilities. <p> This strategy produces exploration that aids identification but can conflict with control. Kaelbling [34], Lin [47], Moore [55], Schmidhuber [63], Sutton <ref> [69] </ref>, Thrun [79], and Thrun and Moller [78] discuss these and other possibilities. 7.3 Q-Learning Q-Learning is a method proposed by Watkins [81] for solving Markovian decision problems with incomplete information. 16 Unlike the indirect adaptive methods discussed above, it is a direct method because it does not use an explicit <p> idea of assigning values to state-action pairs formed the basis of Denardo's [24] approach to DP, we have not seen algorithms like Q-Learning for estimating these values that predate Watkins' 1989 dissertation. 33 We depart somewhat in our presentation from the view taken by Watkins [81] and others (e.g., Sutton <ref> [69] </ref>, Barto and Singh [6]) of Q-Learning as a method for adaptive on-line control. To emphasize Q-Learning's relationship with asynchronous DP, we first present the basic Q-Learning algorithm as an off-line asynchronous DP method that is unique in not requiring direct access to the state-transition probabilities of the decision problem. <p> However, it is also possible to define the successor function sometimes by the real system and sometimes by a system model. For state-action pairs actually experienced during control, the real system provides the successor function; for other state-action pairs, a system model provides an approximate successor function. Sut-ton <ref> [69] </ref> has studied this approach in an algorithm called Dyna-Q, which performs the basic Q-Learning backup using both actual state transitions as well as hypothetical state transitions simulated by a system model. <p> A step of Real-Time Q-Learning is performed based on each actual state transition. This is obviously only one of many possible ways to combine direct and indirect adaptive methods as emphasized in Sutton's discussion of the general Dyna learning architecture <ref> [69] </ref>. It is also possible to modify the basic Q-Learning method in a variety of ways in order to enhance its efficiency. <p> Attempting to present all of the combinations and variations of Q-Learning methods that have been, or could be, described is well beyond the scope of the present article. Barto and Singh [6], Dayan [20; 21], Lin [47; 46], Moore [57], and Sutton <ref> [69] </ref> present comparative empirical studies of some of the adaptive algorithms based on Q-Learning. 8 Methods based on Explicit Policy Representations All of the DP-based learning algorithms described above, both non-adaptive and adaptive cases, use an explicit representation of either an evaluation function or a function giving the Q-values of admissible <p> Real-time algorithms based on policy iteration effectively work by executing an asynchronous form of modified policy iteration concurrently with control. Examples of such methods appear in the pole-balancing system of Barto, Sutton, and Anderson [2; 67] (also refs. [1; 67]) and the Dyna-PI method of Sutton <ref> [69] </ref> (where PI means 38 Policy Iteration). Barto, Sutton, and Watkins [4; 3] discuss the connection between these methods and policy iteration in some detail. <p> Doing this so as to satisfy certain requirements results in the algorithm we call RTDP, a special case of which essentially coincides with Korf's LRTA* [38] algorithm. This general approach follows previous research by others in which DP principles have been used for problem solving and learning (e.g., refs. <ref> [61; 69; 70; 81; 86; 87] </ref>). Our contribution in this article has been to bring to bear on DP-based learning the theory of asynchronous DP as presented by Bertsekas and Tsitsiklis [12].
Reference: [70] <author> R. S. Sutton. </author> <title> Planning by incremental dynamic programming. </title> <editor> In L. A. Birnbaum and G. C. Collins, editors, </editor> <booktitle> Maching Learning: Proceedings of the Eighth International Workshop, </booktitle> <pages> pages 353-357, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Learning algorithms based on DP employ novel means for improving the computational efficiency of conventional DP algorithms. Werbos [86; 88] and Watkins [81] proposed incremental versions of DP as learning algorithms, and Sutton's Dyna architecture for learning, planning, and reacting <ref> [69; 70] </ref> is based on these principles. <p> Even without this on-line model refinement, however, executing a DP algorithm concurrently with the generation of actions has implications for planning in AI, as discussed by Sutton in ref. <ref> [70] </ref>. In this article, we introduce the fact that the theory of asynchronous DP is applicable to the analysis of DP-based reinforcement learning algorithms. Asynchronous DP algorithms differ from conventional DP algorithms in that they do not have to proceed in systematic exhaustive sweeps of the problem's state set. <p> The algorithm we call Real-Time DP (RTDP) results when this interaction has specific characteristics that we present below. Throughout this section we assume that there is a complete and accurate model of the decision problem, the case Sutton <ref> [70] </ref> discusses in relation to planning in AI. In Section 7, we discuss the adaptive case, in which a complete and accurate model of the decision problem is not available. <p> Although Sutton's Dyna architecture [69] focuses on Q-Learning and methods based on policy iteration (Section 8), it also encompasses algorithms such as Adaptive RTDP, as he discusses in ref. <ref> [70] </ref>. Lin [47; 46] also discusses methods closely related to Adaptive RTDP. <p> Doing this so as to satisfy certain requirements results in the algorithm we call RTDP, a special case of which essentially coincides with Korf's LRTA* [38] algorithm. This general approach follows previous research by others in which DP principles have been used for problem solving and learning (e.g., refs. <ref> [61; 69; 70; 81; 86; 87] </ref>). Our contribution in this article has been to bring to bear on DP-based learning the theory of asynchronous DP as presented by Bertsekas and Tsitsiklis [12].
Reference: [71] <author> R. S. Sutton, </author> <title> editor. </title> <journal> A Special Issue of Machine Learning on Reinforcement Learning, </journal> <volume> volume 8. </volume> <booktitle> Machine Learning, </booktitle> <year> 1992. </year> <title> Also published as Reinforcement Learnng, </title> <publisher> Kluwer Academic Press, </publisher> <address> Boston, MA 1992. </address>
Reference-contexts: DP-based learning is the tradeoff between short- and long-term performance: how can an agent learn to improve long-term performance when this may require sacrificing short-term performance? DP-based learning algoithms are examples of reinforcement learning methods by which autonomous agents can improve skills in environments that do not contain explicit teachers <ref> [71] </ref>. In this article we introduce a learning algorithm based on DP, which we call Real-Time Dynamic Programming (RTDP) by which an embedded problem-solving system can improve its long-term performance with experience, and we prove results about its behavior in several different types of problems.
Reference: [72] <author> R. S. Sutton and A. G. Barto. </author> <title> Toward a modern theory of adaptive networks: Expectation and prediction. </title> <journal> Psychological Review, </journal> <volume> 88 </volume> <pages> 135-170, </pages> <year> 1981. </year>
Reference-contexts: The algorithm was implemented as a neuron-like connectionist element called the Adaptive Critic Element [2]. Sutton [68] later called these algorithms Temporal Difference (TD) methods and obtained some theoretical results about their convergence. Following the proposals of Klopf [36; 37], Sutton and Barto <ref> [72; 73; 74] </ref> developed these methods as models of animal learning. <p> Watkins [81] describes a family of Q-Learning methods in which Q-values are backed up based on information gained over sequences of state transitions. One way to implement this kind of extension is to use the "eligibility trace" idea <ref> [2; 37; 67; 72; 68] </ref> to back up the Q-values of all the state-action pairs experienced in the past, with the magnitudes of the backups decreasing to zero with increasing time in the past. Sutton's [68] TD () algorithms illustrate this idea.
Reference: [73] <author> R. S. Sutton and A. G. Barto. </author> <title> A temporal-difference model of classical conditioning. </title> <booktitle> In Proceedings of the Ninth Annual Conference of the Cognitive Science Society, </booktitle> <address> Hillsdale, NJ, 1987. </address> <publisher> Erlbaum. </publisher>
Reference-contexts: The algorithm was implemented as a neuron-like connectionist element called the Adaptive Critic Element [2]. Sutton [68] later called these algorithms Temporal Difference (TD) methods and obtained some theoretical results about their convergence. Following the proposals of Klopf [36; 37], Sutton and Barto <ref> [72; 73; 74] </ref> developed these methods as models of animal learning.
Reference: [74] <author> R. S. Sutton and A. G. Barto. </author> <title> Time-derivative models of pavlovian reinforcement. </title> <editor> In M. Gabriel and J. Moore, editors, </editor> <booktitle> Learning and Computational Neuroscience: Foundations of Adaptive Networks, </booktitle> <pages> pages 497-537. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: The algorithm was implemented as a neuron-like connectionist element called the Adaptive Critic Element [2]. Sutton [68] later called these algorithms Temporal Difference (TD) methods and obtained some theoretical results about their convergence. Following the proposals of Klopf [36; 37], Sutton and Barto <ref> [72; 73; 74] </ref> developed these methods as models of animal learning.
Reference: [75] <author> R. S. Sutton, A. G. Barto, and R. J. Williams. </author> <title> Reinforcement learning is direct adaptive optimal control. </title> <booktitle> In Proceedings of the American Control Conference, </booktitle> <pages> pages 2143-2146, </pages> <address> Boston, MA, </address> <year> 1991. </year>
Reference-contexts: Williams and Baird [91] theoretically analysed additional DP-based algorithms suitable for on-time application. We have also come across the work Jalali and Ferguson [32], who independently proposed a method similar to Adaptive RTDP. Sutton, Barto, and Williams <ref> [75] </ref> discussed reinforcement learning from the perspective of DP and adaptive control, and White and Jordan [89] and Barto [5] provide additional background and extensive references to current research.
Reference: [76] <author> M. Tan. </author> <title> Learning a cost-sensitive internal representation for reinforcement learning. </title> <editor> In L. A. Birnbaum and G. C. Collins, editors, </editor> <booktitle> Maching Learning: Proceedings of the Eighth International Workshop, </booktitle> <pages> pages 358-362, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This includes symbolic methods for learning from examples. These methods also generalize beyond the training information, which is derived from the back-up operations of various DP-based algorithms. For example, Chapman and Kaelbling [15] and Tan <ref> [76] </ref> adapt decision-tree methods, and Mahadevan and Connell [49] use a statistical clustering method. Yee [94] discusses function approximation from the perspective of its use with DP-based learning algorithms.
Reference: [77] <author> G. J. Tesauro. </author> <title> Practical issues in temporal difference learning. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 257-277, </pages> <year> 1992. </year>
Reference-contexts: Tesauro's recent TD-Gammon <ref> [77] </ref>, a program using a TD method together with a connectionist network to improve performance in playing backgammon, has achieved remarkable success. Independently of the approaches inspired by Samuel's checkers player, other researchers suggested similar algorithms based on the theory of optimal control, where DP provides important solution methods. <p> Despite its not being a real-time computation, we regard the concurrent execution of DP and control in simulation 23 mode to be a form of learning. This is in fact how learning was accomplished in the game--playing programs of Samuel [61; 62] and Tesauro <ref> [77] </ref>. Learning occurred during many simulated games in which these learning systems competed against themselves. Although we emphasize the real-time use of DP-based learning algorithms, the reader should be aware that our discussion also applies to the use of these algorithms in simulation mode. <p> This is a form of supervised learning, or learning from examples, and provides the natural way to make use of connectionist networks as shown, for example, by Anderson [1] and Tesauro <ref> [77] </ref>. Parametric approximations of evaluation functions are useful because they can generalize beyond the training data to supply cost estimates for states that have not yet been visited, an important factor for large state sets. <p> Proceeding to larger problems is hampered by the large space requirements of these algorithms if they continue to use lookup tables for storing evaluation functions. Tesauro's TD-Gammon system <ref> [77] </ref> is an encouraging data point for using DP-based learning in conjunction with function approximation methods in problems much larger than those described here, but continued theoretical research is necessary to address the computational complexity of real-time DP algorithms. <p> We used the term simulation mode to refer to the execution of RTDP and related algorithms during simulated control instead of actual control. DP-based learning in simulation mode is illustrated by Samuel's checkers playing system [61; 62], Tesauro's backgammon playing system <ref> [77] </ref>, and our illustrations of RTDP using the race track problem.
Reference: [78] <author> S. B. Thrun and K. Moller. </author> <title> Active exploration in dynamic environments. </title> <editor> In J. E. Moody, S. J. Hanson, and R. P. Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Some of the approaches for which rigorous theoretical results are available are reviewed by Kumar [39], and a variety of more heuristic approaches have been studied by Barto and Singh [6], Kaelbling [34], Moore [55], Schmidhuber [63], Sutton [69], Watkins [81], Thrun [79], and Thrun and Moller <ref> [78] </ref>. In the following subsections, we describe several non-Bayesian methods for solving Marko-vian decision problems with incomplete information. <p> This strategy produces exploration that aids identification but can conflict with control. Kaelbling [34], Lin [47], Moore [55], Schmidhuber [63], Sutton [69], Thrun [79], and Thrun and Moller <ref> [78] </ref> discuss these and other possibilities. 7.3 Q-Learning Q-Learning is a method proposed by Watkins [81] for solving Markovian decision problems with incomplete information. 16 Unlike the indirect adaptive methods discussed above, it is a direct method because it does not use an explicit model of the dynamic system underlying the
Reference: [79] <author> S.B. Thrun. </author> <title> The role of exploration in learning control. </title> <editor> In D. A. White and D. A. Sofge, editors, </editor> <booktitle> Handbook of Intelligent Control: Neural, Fuzzy, and Adaptive Approaches, </booktitle> <pages> pages 527-559. </pages> <publisher> Van Nostrand Reinhold, </publisher> <address> New York, </address> <year> 1992. </year> <month> 64 </month>
Reference-contexts: Some of the approaches for which rigorous theoretical results are available are reviewed by Kumar [39], and a variety of more heuristic approaches have been studied by Barto and Singh [6], Kaelbling [34], Moore [55], Schmidhuber [63], Sutton [69], Watkins [81], Thrun <ref> [79] </ref>, and Thrun and Moller [78]. In the following subsections, we describe several non-Bayesian methods for solving Marko-vian decision problems with incomplete information. <p> This strategy produces exploration that aids identification but can conflict with control. Kaelbling [34], Lin [47], Moore [55], Schmidhuber [63], Sutton [69], Thrun <ref> [79] </ref>, and Thrun and Moller [78] discuss these and other possibilities. 7.3 Q-Learning Q-Learning is a method proposed by Watkins [81] for solving Markovian decision problems with incomplete information. 16 Unlike the indirect adaptive methods discussed above, it is a direct method because it does not use an explicit model of
Reference: [80] <author> P. E. Utgoff and J. A. Clouse. </author> <title> Two kinds of training information for evaluation function learning. </title> <booktitle> In Proceedings of the Ninth Annual Conference on Artificial Intelligence, </booktitle> <pages> pages 596-600, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This fact underlies the utility of various strategies for "teaching" DP-based learning algorithms by supplying experience dictating selected orderings of the backups (e.g., Lin [48], Utgoff and Clouse <ref> [80] </ref>, and Whitehead [90]). 6 Dynamic Programming in Real Time The DP algorithms described above are off-line algorithms for solving Markovian decision problems. <p> It also makes sense for RTDP to back up the costs of states whose current costs are not yet accurate estimates of their optimal costs but whose successor states do have accurate current costs. Techniques for "teaching" DP-based learning systems by suggesting certain back ups over others (refs. <ref> [46; 90; 80] </ref>) rely on the fact that the order in which the costs of states are backed up can influence the rate of convergence of asynchronous DP, whether applied off- or on-line.
Reference: [81] <author> C. J. C. H. Watkins. </author> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> Cambridge University, </institution> <address> Cambridge, England, </address> <year> 1989. </year>
Reference-contexts: Learning algorithms based on DP employ novel means for improving the computational efficiency of conventional DP algorithms. Werbos [86; 88] and Watkins <ref> [81] </ref> proposed incremental versions of DP as learning algorithms, and Sutton's Dyna architecture for learning, planning, and reacting [69; 70] is based on these principles. <p> We also present an extension of RTDP, called Adaptive RTDP, applicable when information is lacking about a problem's structure in addition to its solution. Recognizing that the theory of asynchronous DP is relevant to learning also permits us to provide new insight into Watkins' Q-Learning <ref> [81; 82] </ref> algorithm, another DP-based learning algorithm which is being explored by AI researchers. <p> He called this approach Heuristic Dynamic Programming and has written extensively about it (e.g., refs. [85; 86; 87; 88]). Related algorithms have been discussed by Witten [92; 93], and more recently, Watkins <ref> [81] </ref> extended Sutton's TD algorithms and developed others by explicitly utilizing the theory of DP. He used the term Incremental Dynamic Programming to refer to this class of algorithms and discussed many examples. Williams and Baird [91] theoretically analysed additional DP-based algorithms suitable for on-time application. <p> In other words, is not necessarily a greedy policy with respect to its own evaluation function. To define a greedy policy in this stochastic case we use Watkins' <ref> [81] </ref> "Q" notation, 16 which plays a role in the Q-Learning method described in Section 7.3. <p> Some of the approaches for which rigorous theoretical results are available are reviewed by Kumar [39], and a variety of more heuristic approaches have been studied by Barto and Singh [6], Kaelbling [34], Moore [55], Schmidhuber [63], Sutton [69], Watkins <ref> [81] </ref>, Thrun [79], and Thrun and Moller [78]. In the following subsections, we describe several non-Bayesian methods for solving Marko-vian decision problems with incomplete information. <p> Next, we describe another indirect method that is the simplest modification of the generic indirect method that takes advantage of RTDP. We call this method Adaptive RTDP. The third method we describe is the direct Q-Learning method of Watkins <ref> [81] </ref>. We then briefly describe hybrid direct/indirect methods. 7.1 The Generic Indirect Method Indirect adaptive methods for Markovian decision problems with incomplete information estimate the unknown state-transition probabilities and immediate costs based on the history of state transitions and immediate costs observed while the controller and system interact. <p> state and action, the maximum-likelihood estimate of an immediate cost is the observed average of the immediate cost for that state and action. 31 facilitate comparison of algorithms in the simulations described in Section 4.1, we adopt the action-selection method based on the Boltzmann distribution that was used by Watkins <ref> [81] </ref>, Lin [47], and Sutton [69]. This method assigns an execution probability to each admissible action for the current state, where this probability is determined by a rating of each action's utility. <p> This strategy produces exploration that aids identification but can conflict with control. Kaelbling [34], Lin [47], Moore [55], Schmidhuber [63], Sutton [69], Thrun [79], and Thrun and Moller [78] discuss these and other possibilities. 7.3 Q-Learning Q-Learning is a method proposed by Watkins <ref> [81] </ref> for solving Markovian decision problems with incomplete information. 16 Unlike the indirect adaptive methods discussed above, it is a direct method because it does not use an explicit model of the dynamic system underlying the decision problem. <p> Any policy selecting actions that are greedy with respect to the optimal Q-values is an optimal policy. Thus, if the optimal Q-values are available, an optimal policy can be determined with relatively little computation. 16 Watkins <ref> [81] </ref> actually proposed a family of Q-Learning methods, and what we call Q-Learning in this article is the simplest case, which he called "one-step Q-Learning." He observed that although Q-Learning methods are based on a simple idea, they had not been suggested previously as far as he knew. <p> Although the idea of assigning values to state-action pairs formed the basis of Denardo's [24] approach to DP, we have not seen algorithms like Q-Learning for estimating these values that predate Watkins' 1989 dissertation. 33 We depart somewhat in our presentation from the view taken by Watkins <ref> [81] </ref> and others (e.g., Sutton [69], Barto and Singh [6]) of Q-Learning as a method for adaptive on-line control. <p> This is essentially proved by Watkins <ref> [81] </ref>, and Watkins and Dayan present a revised proof in ref. [82]. Appendix B describes a method for meeting the required learning rate conditions that was developed by Darken and Moody [19]. <p> This can have advantages over Adaptive RTDP when the number of admissible actions is large. However, we use the term Real-Time Q-Learning for the case originally discussed by Watkins <ref> [81] </ref> in which there is no model of the system underlying the decision problem and the real system acts as the successor function. <p> In this case, Off-Line Q-Learning is carried out to backup the Q-values for all of the admissible actions that are "promising" according to the latest Q-values for the current state. Watkins <ref> [81] </ref> describes a family of Q-Learning methods in which Q-values are backed up based on information gained over sequences of state transitions. <p> Although generalization can be helpful in approximating an optimal evaluation function, it is often detrimental to the convergence of the underlying asynchronous DP algorithm, as pointed out by Watkins <ref> [81] </ref> and illustrated with a simple example by Bradtke [13]. <p> Doing this so as to satisfy certain requirements results in the algorithm we call RTDP, a special case of which essentially coincides with Korf's LRTA* [38] algorithm. This general approach follows previous research by others in which DP principles have been used for problem solving and learning (e.g., refs. <ref> [61; 69; 70; 81; 86; 87] </ref>). Our contribution in this article has been to bring to bear on DP-based learning the theory of asynchronous DP as presented by Bertsekas and Tsitsiklis [12]. <p> In addition to indirect adaptive methods, we discussed direct adaptive methods. Direct methods do not form explicit models of the system underlying the decision problem. We described Watkin's <ref> [81] </ref> Q-Learning algorithm, which approximates the optimal evaluation function without forming estimates of state-transition probabilities. Q-Learning instead uses sample state transitions, either generated by a system model or observed during actual control. <p> Real-time Q-Learning additionally requires sequences of learning rate parameters ff t (i; u) (Equation 14) that satisfy the hypotheses of the Q-Learning convergence theorem <ref> [81; 82] </ref>. We defined these sequences as follows. Let ff t (i; u) denote the learning rate parameter used when the Q-value of the state-action pair (i; u) is backed up at time step t.
Reference: [82] <author> C. J. C. H. Watkins and P. </author> <title> Dayan. </title> <journal> Q-learning. Machine Learning, </journal> <volume> 8 </volume> <pages> 279-292, </pages> <year> 1992. </year>
Reference-contexts: We also present an extension of RTDP, called Adaptive RTDP, applicable when information is lacking about a problem's structure in addition to its solution. Recognizing that the theory of asynchronous DP is relevant to learning also permits us to provide new insight into Watkins' Q-Learning <ref> [81; 82] </ref> algorithm, another DP-based learning algorithm which is being explored by AI researchers. <p> This is essentially proved by Watkins [81], and Watkins and Dayan present a revised proof in ref. <ref> [82] </ref>. Appendix B describes a method for meeting the required learning rate conditions that was developed by Darken and Moody [19]. We used this method in obtaining the results for Real-Time Q-Learning on our example problems presented in Section 4.1. <p> Real-time Q-Learning additionally requires sequences of learning rate parameters ff t (i; u) (Equation 14) that satisfy the hypotheses of the Q-Learning convergence theorem <ref> [81; 82] </ref>. We defined these sequences as follows. Let ff t (i; u) denote the learning rate parameter used when the Q-value of the state-action pair (i; u) is backed up at time step t.
Reference: [83] <author> P. J. Werbos. </author> <title> Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences. </title> <type> PhD thesis, </type> <institution> Harvard University, </institution> <year> 1974. </year>
Reference-contexts: This excludes various "differential" approaches, which make use of optimization algorithms related to the connectionist error-backpropagation algorithm (e.g., Jacobson and Mayne [31], Jordan and Jacobs [33], Werbos <ref> [83; 88] </ref>, White and Jordan [89]). The relevance of DP for planning and learning in AI was articulated in Sutton's [69] Dyna architecture.
Reference: [84] <author> P. J. Werbos. </author> <title> Advanced forecasting methods for global crisis warning and models of intelligence. </title> <journal> General Systems Yearbook, </journal> <volume> 22 </volume> <pages> 25-38, </pages> <year> 1977. </year>
Reference-contexts: It is this improvement rather than ultimate convergence to optimality that becomes central. This perspective was taken by Werbos <ref> [84] </ref>, who proposed a method similar to that used by the Adaptive Critic Element within the framework of DP. He called this approach Heuristic Dynamic Programming and has written extensively about it (e.g., refs. [85; 86; 87; 88]).
Reference: [85] <author> P. J. Werbos. </author> <title> Applications of advances in nonlinear sensitivity analysis. </title> <editor> In R. F. Drenick and F. Kosin, editors, </editor> <title> System Modeling an Optimization. </title> <publisher> Springer-Verlag, </publisher> <year> 1982. </year> <booktitle> Proceedings of the Tenth IFIP Conference, </booktitle> <address> New York, </address> <year> 1981. </year>
Reference-contexts: This perspective was taken by Werbos [84], who proposed a method similar to that used by the Adaptive Critic Element within the framework of DP. He called this approach Heuristic Dynamic Programming and has written extensively about it (e.g., refs. <ref> [85; 86; 87; 88] </ref>). Related algorithms have been discussed by Witten [92; 93], and more recently, Watkins [81] extended Sutton's TD algorithms and developed others by explicitly utilizing the theory of DP. He used the term Incremental Dynamic Programming to refer to this class of algorithms and discussed many examples. <p> Numerical methods applicable to problems involving nonlinear systems and/or nonquadratic costs include gradient methods as well as DP. Whereas gradient methods for optimal control are closely related to some of the gradient descent methods being studied by connectionists (such as the error-backpropagation 9 algorithm <ref> [43; 85; 89] </ref>), DP methods are more closely related to heuristic search. Like a heuristic search algorithm, DP is an off-line procedure for designing an optimal control policy.
Reference: [86] <author> P. J. Werbos. </author> <title> Building and understanding adaptive systems: A statistical/numerical approach to factory automation and brain research. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <year> 1987. </year>
Reference-contexts: Learning algorithms based on DP employ novel means for improving the computational efficiency of conventional DP algorithms. Werbos <ref> [86; 88] </ref> and Watkins [81] proposed incremental versions of DP as learning algorithms, and Sutton's Dyna architecture for learning, planning, and reacting [69; 70] is based on these principles. <p> This perspective was taken by Werbos [84], who proposed a method similar to that used by the Adaptive Critic Element within the framework of DP. He called this approach Heuristic Dynamic Programming and has written extensively about it (e.g., refs. <ref> [85; 86; 87; 88] </ref>). Related algorithms have been discussed by Witten [92; 93], and more recently, Watkins [81] extended Sutton's TD algorithms and developed others by explicitly utilizing the theory of DP. He used the term Incremental Dynamic Programming to refer to this class of algorithms and discussed many examples. <p> Doing this so as to satisfy certain requirements results in the algorithm we call RTDP, a special case of which essentially coincides with Korf's LRTA* [38] algorithm. This general approach follows previous research by others in which DP principles have been used for problem solving and learning (e.g., refs. <ref> [61; 69; 70; 81; 86; 87] </ref>). Our contribution in this article has been to bring to bear on DP-based learning the theory of asynchronous DP as presented by Bertsekas and Tsitsiklis [12].
Reference: [87] <author> P. J. Werbos. </author> <title> Generalization of back propagation with applications to a recurrent gas market model. </title> <booktitle> Neural Networks, </booktitle> <volume> 1 </volume> <pages> 339-356, </pages> <year> 1988. </year>
Reference-contexts: This perspective was taken by Werbos [84], who proposed a method similar to that used by the Adaptive Critic Element within the framework of DP. He called this approach Heuristic Dynamic Programming and has written extensively about it (e.g., refs. <ref> [85; 86; 87; 88] </ref>). Related algorithms have been discussed by Witten [92; 93], and more recently, Watkins [81] extended Sutton's TD algorithms and developed others by explicitly utilizing the theory of DP. He used the term Incremental Dynamic Programming to refer to this class of algorithms and discussed many examples. <p> Doing this so as to satisfy certain requirements results in the algorithm we call RTDP, a special case of which essentially coincides with Korf's LRTA* [38] algorithm. This general approach follows previous research by others in which DP principles have been used for problem solving and learning (e.g., refs. <ref> [61; 69; 70; 81; 86; 87] </ref>). Our contribution in this article has been to bring to bear on DP-based learning the theory of asynchronous DP as presented by Bertsekas and Tsitsiklis [12].
Reference: [88] <author> P.J. Werbos. </author> <title> Approximate dynamic programming for real-time control and neural modeling. </title> <editor> In D. A. White and D. A. Sofge, editors, </editor> <booktitle> Handbook of Intelligent Control: Neural, Fuzzy, and Adaptive Approaches, </booktitle> <pages> pages 493-525. </pages> <publisher> Van Nostrand Reinhold, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: Learning algorithms based on DP employ novel means for improving the computational efficiency of conventional DP algorithms. Werbos <ref> [86; 88] </ref> and Watkins [81] proposed incremental versions of DP as learning algorithms, and Sutton's Dyna architecture for learning, planning, and reacting [69; 70] is based on these principles. <p> This perspective was taken by Werbos [84], who proposed a method similar to that used by the Adaptive Critic Element within the framework of DP. He called this approach Heuristic Dynamic Programming and has written extensively about it (e.g., refs. <ref> [85; 86; 87; 88] </ref>). Related algorithms have been discussed by Witten [92; 93], and more recently, Watkins [81] extended Sutton's TD algorithms and developed others by explicitly utilizing the theory of DP. He used the term Incremental Dynamic Programming to refer to this class of algorithms and discussed many examples. <p> This excludes various "differential" approaches, which make use of optimization algorithms related to the connectionist error-backpropagation algorithm (e.g., Jacobson and Mayne [31], Jordan and Jacobs [33], Werbos <ref> [83; 88] </ref>, White and Jordan [89]). The relevance of DP for planning and learning in AI was articulated in Sutton's [69] Dyna architecture.
Reference: [89] <author> D.A. White and M.I. Jordan. </author> <title> Optimal control: A foundation for intelligent control. </title> <editor> In D. A. White and D. A. Sofge, editors, </editor> <booktitle> Handbook of Intelligent Control: Neural, Fuzzy, and Adaptive Approaches, </booktitle> <pages> pages 185-214. </pages> <publisher> Van Nostrand Reinhold, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: We have also come across the work Jalali and Ferguson [32], who independently proposed a method similar to Adaptive RTDP. Sutton, Barto, and Williams [75] discussed reinforcement learning from the perspective of DP and adaptive control, and White and Jordan <ref> [89] </ref> and Barto [5] provide additional background and extensive references to current research. <p> This excludes various "differential" approaches, which make use of optimization algorithms related to the connectionist error-backpropagation algorithm (e.g., Jacobson and Mayne [31], Jordan and Jacobs [33], Werbos [83; 88], White and Jordan <ref> [89] </ref>). The relevance of DP for planning and learning in AI was articulated in Sutton's [69] Dyna architecture. <p> Numerical methods applicable to problems involving nonlinear systems and/or nonquadratic costs include gradient methods as well as DP. Whereas gradient methods for optimal control are closely related to some of the gradient descent methods being studied by connectionists (such as the error-backpropagation 9 algorithm <ref> [43; 85; 89] </ref>), DP methods are more closely related to heuristic search. Like a heuristic search algorithm, DP is an off-line procedure for designing an optimal control policy.
Reference: [90] <author> S. D. Whitehead. </author> <title> Complexity and cooperation in Q-learning. </title> <editor> In L. A. Birnbaum and G. C. Collins, editors, </editor> <booktitle> Maching Learning: Proceedings of the Eighth International Workshop, </booktitle> <pages> pages 363-367, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This fact underlies the utility of various strategies for "teaching" DP-based learning algorithms by supplying experience dictating selected orderings of the backups (e.g., Lin [48], Utgoff and Clouse [80], and Whitehead <ref> [90] </ref>). 6 Dynamic Programming in Real Time The DP algorithms described above are off-line algorithms for solving Markovian decision problems. Although they successively approximate the optimal evaluation function through a sequence of stages, these stages are not related to the time steps of the decision problem being solved. <p> It also makes sense for RTDP to back up the costs of states whose current costs are not yet accurate estimates of their optimal costs but whose successor states do have accurate current costs. Techniques for "teaching" DP-based learning systems by suggesting certain back ups over others (refs. <ref> [46; 90; 80] </ref>) rely on the fact that the order in which the costs of states are backed up can influence the rate of convergence of asynchronous DP, whether applied off- or on-line.
Reference: [91] <author> R. J. Williams and L. C. Baird, III. </author> <title> A mathematical analysis of actor-critic architectures for learning optimal controls through incremental dynamic programming. </title> <booktitle> In Proceedings of the Sixth Yale Workshop on Adaptive and Learning Systems, </booktitle> <pages> pages 96-101, </pages> <address> New Haven, CT, </address> <month> Aug </month> <year> 1990. </year> <month> 65 </month>
Reference-contexts: Related algorithms have been discussed by Witten [92; 93], and more recently, Watkins [81] extended Sutton's TD algorithms and developed others by explicitly utilizing the theory of DP. He used the term Incremental Dynamic Programming to refer to this class of algorithms and discussed many examples. Williams and Baird <ref> [91] </ref> theoretically analysed additional DP-based algorithms suitable for on-time application. We have also come across the work Jalali and Ferguson [32], who independently proposed a method similar to Adaptive RTDP. <p> In this article we do not discuss learning algorithms based on policy iteration because their theory is not yet as well understood as is the theory of learning algorithms based on asynchronous value iteration. However, Williams and Baird <ref> [91] </ref> have made a valuable contribution to this theory by addressing DP algorithms that are asynchronous at a grain finer than that of either asychronous DP or Q-Learning. These algorithms include value iteration, policy iteration, and modified policy iteration as special cases.
Reference: [92] <author> I. H. Witten. </author> <title> An adaptive optimal controller for discrete-time Markov environments. </title> <journal> Information and Control, </journal> <volume> 34 </volume> <pages> 286-295, </pages> <year> 1977. </year>
Reference-contexts: He called this approach Heuristic Dynamic Programming and has written extensively about it (e.g., refs. [85; 86; 87; 88]). Related algorithms have been discussed by Witten <ref> [92; 93] </ref>, and more recently, Watkins [81] extended Sutton's TD algorithms and developed others by explicitly utilizing the theory of DP. He used the term Incremental Dynamic Programming to refer to this class of algorithms and discussed many examples.
Reference: [93] <author> I. H. Witten. </author> <title> Exploring, modelling and controlling discrete sequential environments. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 9 </volume> <pages> 715-735, </pages> <year> 1977. </year>
Reference-contexts: He called this approach Heuristic Dynamic Programming and has written extensively about it (e.g., refs. [85; 86; 87; 88]). Related algorithms have been discussed by Witten <ref> [92; 93] </ref>, and more recently, Watkins [81] extended Sutton's TD algorithms and developed others by explicitly utilizing the theory of DP. He used the term Incremental Dynamic Programming to refer to this class of algorithms and discussed many examples.
Reference: [94] <author> R. C. Yee. </author> <title> Abstraction in control learning. </title> <type> Technical Report 92-16, </type> <institution> Department of Computer Science, University of Massachusetts, </institution> <address> Amherst, MA, </address> <year> 1992. </year> <month> 66 </month>
Reference-contexts: These methods also generalize beyond the training information, which is derived from the back-up operations of various DP-based algorithms. For example, Chapman and Kaelbling [15] and Tan [76] adapt decision-tree methods, and Mahadevan and Connell [49] use a statistical clustering method. Yee <ref> [94] </ref> discusses function approximation from the perspective of its use with DP-based learning algorithms. Despite the large number of studies in which the principles of DP have been combined with generalizing methods for approximating evaluation functions, the theoretical results presented in this article do not automatically extend to these approaches.
References-found: 94

