URL: http://www.cs.dartmouth.edu/~nicol/papers/fft.ps
Refering-URL: http://www.cs.dartmouth.edu/~nicol/papers/papers.html
Root-URL: http://www.cs.dartmouth.edu
Title: Performing Out-of-Core FFTs on Parallel Disk Systems  
Author: Thomas H. Cormen David M. Nicol 
Affiliation: Dartmouth College Department of Computer Science  
Date: (Revised September 1996)  
Note: PCSTR96-294  
Abstract: Dartmouth College Computer Science Technical Report Abstract The Fast Fourier Transform (FFT) plays a key role in many areas of computational science and engineering. Although most one-dimensional FFT problems can be entirely solved entirely in main memory, some important classes of applications require out-of-core techniques. For these, use of parallel I/O systems can improve performance considerably. This paper shows how to perform one-dimensional FFTs using a parallel disk system with independent disk accesses. We present both analytical and experimental results for performing out-of-core FFTs in two ways: using traditional virtual memory with demand paging, and using a provably asymptotically optimal algorithm for the Parallel Disk Model (PDM) of Vitter and Shriver. When run on a DEC 2100 server with a large memory and eight parallel disks, the optimal algorithm for the PDM runs up to 144.7 times faster than in-core methods under demand paging. Moreover, even including I/O costs, the normalized times for the optimal PDM algorithm are competitive, or better than, those for in-core methods even when they run entirely in memory.
Abstract-found: 1
Intro-found: 1
Reference: [AV88] <author> Alok Aggarwal and Jeffrey Scott Vitter. </author> <title> The input/output complexity of sorting and related problems. </title> <journal> Communications of the ACM, </journal> <volume> 31(9) </volume> <pages> 1116-1127, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: The algorithm is a variant of one that was sketched by Vitter and Shriver [VS94], and which achieves the lower bound on complexity proven by Aggarwal and Vitter <ref> [AV88] </ref>. In particular, we show how efficient out-of-core permutation routines can be used throughout the FFT computation. We assess performance by comparison with demand paging; we show analytically and experimentally that well-known in-core FFT algorithms run slowly once the data set size exceeds available in-core memory. <p> Asymptotically, the number of parallel I/O operations is fi N lg N lg min (B;N=B) , which can be shown via simple manipulations to equal the lower bound of N lg min (B;N=B) proven by Aggarwal and Vitter <ref> [AV88] </ref>. Handling general values of N and M If lg M does not divide lg N , then we compensate in the last superlevel.
Reference: [Bai90] <author> David H. Bailey. </author> <title> FFTs in external or hierarchical memory. </title> <journal> The Journal of Supercomputing, </journal> <volume> 4 </volume> <pages> 23-35, </pages> <year> 1990. </year>
Reference-contexts: Stockham's method [Van92, pp. 49-58] eliminates bit-reversal by permuting the N values before each of the lg N stages of the butterfly network. Its memory requirement, however, is twice that of the Cooley-Tukey method. Another method, attributed by Bailey <ref> [Bai90] </ref> to P. Swarztrauber as a variation of an algorithm by Gentleman and Sande, and also attributed to E.
Reference: [Bre69] <author> Norman M. Brenner. </author> <title> Fast Fourier transform of externally stored data. </title> <journal> IEEE Transactions on Audio and Electroacoustics, </journal> <volume> AU-17(2):128-132, </volume> <month> June </month> <year> 1969. </year>
Reference-contexts: Its memory requirement, however, is twice that of the Cooley-Tukey method. Another method, attributed by Bailey [Bai90] to P. Swarztrauber as a variation of an algorithm by Gentleman and Sande, and also attributed to E. Granger by Brenner <ref> [Bre69] </ref>, splits the summation of equation (1) into p N summations each with p N terms. (Here we take N to be a power of 4, but the method can be generalized).
Reference: [CF94] <author> Richard Crandall and Barry Fagin. </author> <title> Discrete weighted transforms and large-integer arithmetic. </title> <journal> Mathematics of Computation, </journal> <volume> 62(205) </volume> <pages> 305-324, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: The project currently requires FFTs with 10 gigapoints, and it desires FFTs with up to 64 gigapoints. Yet another application is for integer multiplication of very large numbers <ref> [CF94] </ref>, which is a key component in the most modern methods of searching for Mersenne prime numbers. FFTs are used in many ways to manipulate data sets, such as convolution/deconvolution, correlation/auto-correlation, filtering, and power spectrum estimation [PFTV88].
Reference: [CGK + 88] <author> Peter Chen, Garth Gibson, Randy H. Katz, David A. Patterson, and Martin Schulze. </author> <title> Two papers on RAIDs. </title> <type> Technical Report UCB/CSD 88/479, </type> <institution> Computer Science Division (EECS), University of California, Berkeley, </institution> <month> December </month> <year> 1988. </year>
Reference-contexts: A more restricted operation is striped I/O, in which the blocks accessed in a given operation must be at the same location on each disk. 2 A block might consist of several sectors of a physical device or, in the case of RAID <ref> [CGK + 88, Gib92, PGK88] </ref>, sectors from several physical devices. 7 D 0 D 1 D 2 D 3 D 4 D 5 D 6 D 7 stripe 0 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 stripe 1 16 17 18 19
Reference: [CH96] <author> Thomas H. Cormen and Melissa Hirschl. </author> <title> Early experiences in evaluating the Parallel Disk Model with the ViC* implementation. </title> <type> Technical Report PCS-TR-293, </type> <institution> Dartmouth College Department of Computer Science, </institution> <month> August </month> <year> 1996. </year> <note> To appear in Parallel Computing. </note>
Reference-contexts: This computation is easy to move into loops and avoids expensive sine and cosine calls. The ViC* interface <ref> [CH96] </ref> provides the appearance of the PDM when performing parallel I/O operations. The interface is portable, and it is implemented as a set of wrappers on top of an existing serial or parallel file system. <p> Times are in seconds, and in italics are the normalized times (the running time divided by N lg N ) in microseconds. The BMMC permutation subroutine is taken from the implementation in <ref> [CH96] </ref>. It calls the ViC* interface to perform striped reads and independent writes. It is carefully optimized for both in-core computation and I/O. Finally, we implemented the FFT algorithm with both synchronous (i.e., blocking) and asynchronous (non-blocking) I/O calls; the ViC* interface supports both.
Reference: [Cla85] <author> Jon F. Claerbout. </author> <title> Imaging the Earth's Interior. </title> <publisher> Blackwell Scientific Publications, </publisher> <year> 1985. </year>
Reference-contexts: Some critical applications require extremely large one-dimensional FFTs, particularly when the subject function exhibits critical phenomena at vastly different time scales and high resolution is required. One such application is seismic analysis <ref> [Cla85] </ref>, where an out-of-core one-dimensional FFT is necessary (as part of a higher dimensional FFT) even when the computer memory has 16 gigabytes of available RAM [Rut96]. Another application is in the area of radio astronomy.
Reference: [CLR90] <author> Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1990. </year>
Reference-contexts: Finally, we summarize in Section 8. 2 In-core FFTs This section reviews Fourier transforms and outlines some well-known FFT methods for in-core computation. For further background on the FFT, see any of the texts <ref> [CLR90, Nus82, Van92] </ref>. Discrete Fourier transforms Fourier transforms are based on complex roots of unity. The principal Nth root of unity is a complex number ! N = e 2i=N , where i = p 1. For any real number u, e iu = cos (u) + i sin (u). <p> Then lg N = 3 stages of butterfly operations are performed, and the results (y 0 ; y 1 ; : : : ; y N1 ) emerge from the right. This figure is taken from <ref> [CLR90, p. 796] </ref>. (DFT) is a vector y = (y 0 ; y 1 ; : : : ; y N1 ) for which y k = j=0 jk We also write y = DFT N (a).
Reference: [CSW94] <author> Thomas H. Cormen, Thomas Sundquist, and Leonard F. Wisniewski. </author> <title> Asymptotically tight bounds for performing BMMC permutations on parallel disk systems. </title> <type> Technical Report PCS-TR94-223, </type> <institution> Dartmouth College Department of Computer Science, </institution> <month> July </month> <year> 1994. </year> <note> Preliminary version appeared in Proceedings of the 5th Annual ACM Symposium on Parallel Algorithms and Architectures. Revised version to appear in SIAM Journal on Computing. 13 </note>
Reference-contexts: Each superlevel consists of N=M mini-butterflies on M values, followed by a (lg M )-bit right-rotation permutation on the entire array. A very efficient algorithm for BMMC permutations on the PDM appears in <ref> [CSW94] </ref>.
Reference: [CT65] <author> J. W. Cooley and J. W. Tukey. </author> <title> An algorithm for the machine calculation of complex Fourier series. </title> <journal> Mathematics of Computation, </journal> <volume> 19 </volume> <pages> 297-301, </pages> <year> 1965. </year>
Reference-contexts: Computing the coefficients of the constituent functions yields a great deal of information about the function. Well-known Fast Fourier Transform (FFT) techniques accomplish the computation in fi (N lg N ) operations. Since the modern discovery of the FFT by Cooley and Tukey in 1965 <ref> [CT65] </ref>, a profusion of FFT methods have been developed, primarily to optimize it for different types of computer architectures such as vector and parallel machines (e.g., see Van Loan [Van92]). <p> The butterfly operations in the sth stage can be organized into N=2 s groups of 2 s operations each. FFT algorithms When the FFT is computed according to Figure 1 in a straightforward manner|left to right and top to bottom|the result is the classic Cooley-Tukey FFT method <ref> [CT65] </ref>. Several other methods have been developed to improve performance on vector machines and in memory hierarchies, by avoiding the bit-reversal permutation to improve locality of reference. Stockham's method [Van92, pp. 49-58] eliminates bit-reversal by permuting the N values before each of the lg N stages of the butterfly network.
Reference: [Fra76] <author> Donald Fraser. </author> <title> Array permutation by index-digit permutation. </title> <journal> Journal of the ACM, </journal> <volume> 23(2) </volume> <pages> 298-309, </pages> <month> April </month> <year> 1976. </year>
Reference-contexts: This relation between M and N is entirely reasonable 4 given contemporary memory sizes and prices. The method does require an out-of-core matrix--transpose subroutine to accomplish steps 1, 4, and 6. Bailey recommends an algorithm by Fraser <ref> [Fra76] </ref> for BPC (bit-permute/complement) permutations on one disk, whereas Brenner details a transposition algorithm. When the problem size just barely exceeds the memory size, Brenner suggests a method developed by W. Ryder. This method, which is a specialization of Swarztrauber's method, eliminates the first two matrix transpositions.
Reference: [Gib92] <author> Garth A. Gibson. </author> <title> Redundant Disk Arrays: Reliable, Parallel Secondary Storage. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1992. </year> <note> Also available as Technical Report UCB/CSD 91/613, </note> <institution> Computer Science Division (EECS), University of California, Berke-ley, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: A more restricted operation is striped I/O, in which the blocks accessed in a given operation must be at the same location on each disk. 2 A block might consist of several sectors of a physical device or, in the case of RAID <ref> [CGK + 88, Gib92, PGK88] </ref>, sectors from several physical devices. 7 D 0 D 1 D 2 D 3 D 4 D 5 D 6 D 7 stripe 0 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 stripe 1 16 17 18 19
Reference: [MGST70] <author> R. Mattson, J. Gecsei, D. Slutz, and I. Traiger. </author> <title> Evaluation techniques for storage hierarchies. </title> <journal> IBM Systems Journal, </journal> <volume> 12(2) </volume> <pages> 78-117, </pages> <year> 1970. </year>
Reference-contexts: To determine whether a given reference to A [j 0 ] causes a page fault, we compute the "stack distance" for the page containing A [j 0 ]. The stack distance <ref> [MGST70] </ref> of a reference to page p is one plus the number of uniquely different pages referenced since the most recent reference to page p.
Reference: [Nus82] <author> Henri J. Nussbaumer. </author> <title> Fast Fourier Transform and Convolution Algorithms. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <note> second edition, </note> <year> 1982. </year>
Reference-contexts: Finally, we summarize in Section 8. 2 In-core FFTs This section reviews Fourier transforms and outlines some well-known FFT methods for in-core computation. For further background on the FFT, see any of the texts <ref> [CLR90, Nus82, Van92] </ref>. Discrete Fourier transforms Fourier transforms are based on complex roots of unity. The principal Nth root of unity is a complex number ! N = e 2i=N , where i = p 1. For any real number u, e iu = cos (u) + i sin (u).
Reference: [PFTV88] <author> W. H. Press, B. P. Flannery, S. A. Teukolsky, and W. T. Vettering. </author> <title> Numerical Recipes in C: </title> <booktitle> The Art of Scientific Computing. </booktitle> <publisher> Cambridge University Press, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: Yet another application is for integer multiplication of very large numbers [CF94], which is a key component in the most modern methods of searching for Mersenne prime numbers. FFTs are used in many ways to manipulate data sets, such as convolution/deconvolution, correlation/auto-correlation, filtering, and power spectrum estimation <ref> [PFTV88] </ref>. Any time the data set is very large and accuracy is essential, very large FFTs are required. The contribution of the present paper is to present an out-of-core FFT algorithm that exploits parallel I/O and to assess its performance.
Reference: [PGK88] <author> David A. Patterson, Garth Gibson, and Randy H. Katz. </author> <title> A case for redundant arrays of inexpensive disks (RAID). </title> <booktitle> In ACM International Conference on Management of Data (SIGMOD), </booktitle> <pages> pages 109-116, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: A more restricted operation is striped I/O, in which the blocks accessed in a given operation must be at the same location on each disk. 2 A block might consist of several sectors of a physical device or, in the case of RAID <ref> [CGK + 88, Gib92, PGK88] </ref>, sectors from several physical devices. 7 D 0 D 1 D 2 D 3 D 4 D 5 D 6 D 7 stripe 0 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 stripe 1 16 17 18 19
Reference: [Rut96] <author> Jeff Rutledge. </author> <title> Private communication, </title> <year> 1996. </year>
Reference-contexts: One such application is seismic analysis [Cla85], where an out-of-core one-dimensional FFT is necessary (as part of a higher dimensional FFT) even when the computer memory has 16 gigabytes of available RAM <ref> [Rut96] </ref>. Another application is in the area of radio astronomy. The High-Speed Data Acquisition and Very Large FFTs Project at Caltech 1 uses FFTs to support searching for fast (millisecond period) pulsars. The project currently requires FFTs with 10 gigapoints, and it desires FFTs with up to 64 gigapoints.
Reference: [Sni81] <author> M. Snir. </author> <title> I/O limitations on multi-chip VLSI systems. </title> <booktitle> In Proceedings of the 19th Allerton Conference on Communication, Control and Computation, </booktitle> <pages> pages 224-233, </pages> <year> 1981. </year>
Reference-contexts: The characteristic matrix is formed by taking the identity matrix and rotating its columns k positions to the right, and rank fl min (k; lg B; lg (N=B)). Redrawing the butterfly Snir <ref> [Sni81] </ref> and is implicitly used in the FFT algorithm of Vitter and Shriver [VS94]. Assume for the moment that lg M divides lg N. As in the Cooley-Tukey method, we start with a bit-reversal permutation.
Reference: [SW95] <author> Roland Sweet and John Wilson. </author> <title> Development of out-of-core fast Fourier transform software for the Connection Machine. </title> <note> URL http://www-math.cudenver.edu/~jwilson/ final report/final report.html, </note> <month> December </month> <year> 1995. </year>
Reference-contexts: The cost of doing so, however, is that the computation time contains a term proportional to N 2 =M , so that if N M , the computation time is very high. Sweet and Wilson <ref> [SW95] </ref> use an extension of Swarztrauber's method to perform FFTs even when N &gt; M 2 on the CM-5 using a Scalable Disk Array (SDA) [TMC92], which appears to the programmer as one large disk.
Reference: [TMC92] <institution> CM-5 scalable disk array. Thinking Machines Corporation glossy, </institution> <month> November </month> <year> 1992. </year>
Reference-contexts: Sweet and Wilson [SW95] use an extension of Swarztrauber's method to perform FFTs even when N &gt; M 2 on the CM-5 using a Scalable Disk Array (SDA) <ref> [TMC92] </ref>, which appears to the programmer as one large disk. The method used by Sweet and Wilson requires an out-of-core bit-reversal permutation, and they use Fraser's algorithm. The algorithm we present in Section 6 fleshes out the details of a sketch given by Vitter and Shriver [VS94].
Reference: [Van92] <author> Charles Van Loan. </author> <title> Computational Frameworks for the Fast Fourier Transform. </title> <publisher> SIAM Press, </publisher> <address> Philadelphia, </address> <year> 1992. </year>
Reference-contexts: Since the modern discovery of the FFT by Cooley and Tukey in 1965 [CT65], a profusion of FFT methods have been developed, primarily to optimize it for different types of computer architectures such as vector and parallel machines (e.g., see Van Loan <ref> [Van92] </ref>). <p> Finally, we summarize in Section 8. 2 In-core FFTs This section reviews Fourier transforms and outlines some well-known FFT methods for in-core computation. For further background on the FFT, see any of the texts <ref> [CLR90, Nus82, Van92] </ref>. Discrete Fourier transforms Fourier transforms are based on complex roots of unity. The principal Nth root of unity is a complex number ! N = e 2i=N , where i = p 1. For any real number u, e iu = cos (u) + i sin (u). <p> Several other methods have been developed to improve performance on vector machines and in memory hierarchies, by avoiding the bit-reversal permutation to improve locality of reference. Stockham's method <ref> [Van92, pp. 49-58] </ref> eliminates bit-reversal by permuting the N values before each of the lg N stages of the butterfly network. Its memory requirement, however, is twice that of the Cooley-Tukey method. Another method, attributed by Bailey [Bai90] to P.
Reference: [VS94] <author> Jeffrey Scott Vitter and Elizabeth A. M. Shriver. </author> <title> Algorithms for parallel memory I: Two-level memories. </title> <journal> Algorithmica, </journal> 12(2/3):110-147, August and September 1994. <volume> 14 </volume>
Reference-contexts: The contribution of the present paper is to present an out-of-core FFT algorithm that exploits parallel I/O and to assess its performance. The algorithm is a variant of one that was sketched by Vitter and Shriver <ref> [VS94] </ref>, and which achieves the lower bound on complexity proven by Aggarwal and Vitter [AV88]. In particular, we show how efficient out-of-core permutation routines can be used throughout the FFT computation. <p> The method used by Sweet and Wilson requires an out-of-core bit-reversal permutation, and they use Fraser's algorithm. The algorithm we present in Section 6 fleshes out the details of a sketch given by Vitter and Shriver <ref> [VS94] </ref>. <p> Nevertheless, we shall see in Section 7 that our explicit out-of-core algorithm runs faster than Swarztrauber's method on the same system for a problem size of N = 2 24 . 5 The Parallel Disk Model This section describes the Parallel Disk Model <ref> [VS94] </ref>. We shall use this model in Section 6 to design an out-of-core FFT algorithm. In the Parallel Disk Model, or PDM, N records are stored on D disks D 0 ; D 1 ; : : : ; D D1 , with N=D records stored on each disk. <p> The characteristic matrix is formed by taking the identity matrix and rotating its columns k positions to the right, and rank fl min (k; lg B; lg (N=B)). Redrawing the butterfly Snir [Sni81] and is implicitly used in the FFT algorithm of Vitter and Shriver <ref> [VS94] </ref>. Assume for the moment that lg M divides lg N. As in the Cooley-Tukey method, we start with a bit-reversal permutation. Then there are lg N= lg M superlevels, where each superlevel consists of N=M separate "mini-butterflies" followed by a (lg M )-bit right-rotation permutation on the entire array.
References-found: 22

