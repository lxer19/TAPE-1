URL: http://www.cs.berkeley.edu/~demmel/cs267/PerformanceMeasurement.ps.gz
Refering-URL: http://www.cs.berkeley.edu/~demmel/cs267/lecture04.html
Root-URL: http://www.cs.berkeley.edu
Title: Computer Performance: A Tutorial  High Performance Scientific  
Author: Lloyd D. Fosdick Carolyn J. C. Schauble Elizabeth R. Jessup 
Address: Boulder  
Affiliation: University of Colorado at  
Note: DRAFT:  
Date: December 19, 1994  December 19, 1994  
Pubnum: Computing  
Abstract-found: 0
Intro-found: 1
Reference: [Adams et al. 92] <author> ADAMS, JEANNE C., WALTER S. BRAINERD, JEANNE T. MARTIN, BRIAN T. SMITH, AND JERROLD L. WAGENER. </author> <year> [1992]. </year> <title> Fortran 90 Handbook. </title> <publisher> Intertext Publications. McGraw-Hill Book Company, </publisher> <address> New York, NY. ISBN 0-07-000406-4. </address>
Reference-contexts: Most manufacturers of vector and parallel computers provide special operating systems and compilers with extensions that exploit the parallel (and vector) mechanisms of the machine. Both Fortran 90 <ref> [Adams et al. 92] </ref> and HPF (High Performance Fortran) [Koelbel et al. 94] are designed to consolidate multiple extensions into a uniform version of the Fortran programming language, allowing additional operations that could be compiled by different machines into parallel or vector operations.
Reference: [Anderson et al. 92] <author> ANDERSON, E., Z. BAI, C. BISCHOF, J. DEMMEL, J. DON-GARRA, J. DUCROZ, A. GREENBAUM, AND S. HAMMARLING. </author> <year> [1992]. </year> <note> LAPACK User's Guide. SIAM, Philadelphia. </note>
Reference-contexts: Many of the reported LINPACK results have been produced using these assembler routines. While LINPACK is still used for benchmarking purposes, it is no longer recommended for solving linear algebra problems. Released in 1992, LAPACK <ref> [Anderson et al. 92] </ref> replaces both LINPACK and EISPACK [Smith et al. 76], the collection of routines for eigenvalue problems introduced in 1970. LAPACK makes use of the newest, most accurate algorithms in numerical linear algebra.
Reference: [Baer 80] <author> BAER, JEAN-LOUP. </author> <year> [1980]. </year> <title> Computer Systems Architecture. </title> <publisher> Computer Society Press. </publisher>
Reference: [Bailey et al. 94] <author> BAILEY, DAVID H., ERIC BARSZCZ, LEONARDO DAGUM, AND HORST D. SIMON. </author> <month> [Mar </month> <year> 1994]. </year> <title> NAS Parallel Benchmark Results 3-94. </title> <type> RNR Technical Report RNR-94-006, </type> <institution> NASA Ames Research Center, NASA Ames Research Center. </institution>
Reference-contexts: Current NAS performance data can be found via the mosaic home page: http://www.nas.nasa.gov/RNR/Parallel/NPB For more information, see <ref> [Bailey et al. 94] </ref> or contact NAS Parallel Benchmarks NAS Systems Division Mail Stop 2588 NASA Ames Research Center Moffett Field, CA 94035 4 The Effect of Optimizing Compilers One of the goals of the original Fortran compiler was to optimize the generated assembly code as much as possible. <p> Performance figures taken from <ref> [Bailey et al. 94] </ref>, [van der Steen 93], and [Dongarra 94].
Reference: [DEC 91] <institution> Digital Equipment Corporation, </institution> <address> Palo Alto, CA 94301. </address> <month> [Dec </month> <year> 1991]. </year> <title> DECstation 5000 Model 240: </title> <type> Technical Overview. </type>
Reference-contexts: Such units include SPECmarks and the other definition of mips. These are discussed in the next section. 2.4 Timing elementary operations In this section we discuss how to time the elementary arithmetic operations on the DECstation 5000/240. This system uses a MIPS R3000 CPU and R3010 FPU chipset <ref> [Kane 88, DEC 91] </ref>, operating at 40 MHz (25 nsec cycle time).
Reference: [Dongarra & Sorensen ] <author> DONGARRA, J. J. AND D. C. SORENSEN. </author> <title> Schedule Users Guide. </title> <institution> Argonne National Laboratory. </institution>
Reference: [Dongarra & Sorensen 87] <author> DONGARRA, JACK J. AND DANNY C. SORENSEN. </author> <year> [1987]. </year> <title> SCHEDULE: Tools for developing and analyzing parallel Fortran programs. </title> <editor> In JAMIESON, LEAH J., DENNIS B. GAN-NON, AND ROBERT J. DOUGLASS, editors, </editor> <booktitle> The Characteristics of Parallel Algorithms, </booktitle> <pages> pages 363-394. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. ISBN 0-262-10036-3. </address>
Reference-contexts: Tracing and visualization tools assist the programmer in studying the execution of a parallel program. Schedule is one such tool. During the execution of a program, it collects information in order to provide data-dependency graphs and visual representations of the flow of given tasks through the processors <ref> [Dongarra & Sorensen , Dongarra & Sorensen 87] </ref>. These graphs display the allocation of different parallel tasks to different processors and so show where performance might be improved by a reallocation of those tasks.
Reference: [Dongarra 94] <author> DONGARRA, J. J. </author> <year> [1994]. </year> <title> Performance of various computers using standard linear equations software. </title> <type> Technical Report CS-89-85, </type> <institution> Oak Ridge National Laboratory, Oak Ridge, TN 37831. </institution> <note> netlib version as of July 1, </note> <year> 1994. </year>
Reference-contexts: One other interesting term used to describe computer performance is the peak performance of a machine. This is simply the maximum number of flops that a machine can theoretically obtain. It is almost never achieved. According to Dongarra <ref> [Dongarra 94] </ref>, The theoretical peak performance is determined by counting the number of floating-point additions and multiplications (in full precision) that can be completed during a period of time, usually the cycle time of the machine. <p> Both of these benchmarks are described in further detail below. The SPECmarks quoted came from various articles posted on comp.benchmarks or from manufacturer's brochures; the LINPACK values came from <ref> [Dongarra 94] </ref>. 3.4.1 LINPACK In the 1970's, a set of Fortran subroutines was developed to solve systems of linear equations. This set of routines is called LINPACK [Dongarra et al. 79]. <p> The SPECmarks quoted came from various articles posted on comp.benchmarks or from manufacturer's brochures; the LINPACK values came from <ref> [Dongarra 94] </ref>. Since LINPACK was popular in many areas of scientific computing, it is logical that one of its routines has become the source of a kernel benchmark for comparing the floating-point performance of different machines. <p> nsec 4.2 nsec Max Memory 1MB 16 MB 128 MB 16 GB Word Size 64 bits 64 bits 64 bits 64 bits Vector Reg Size 64 words 64 words 64 words 64 words Table 3: A comparison of the characteristics of some Cray vector supercom puters, based on data from <ref> [Dongarra 94] </ref> and [Hockney & Jesshope 88]. * n 1=2 The length, m = n 1=2 , of a vector such that R m is equal to R 1 =2. <p> Table 3 summarizes the characteristics of four Cray models while table 4 compares the performance of those four Crays. These figures are based on data from <ref> [Dongarra 94] </ref> and [Hockney & Jesshope 88]. For more information on vector processors, see [Schauble 94]. 6.2 Parallel Computers A parallel computer is a machine with two or more connected processors that may operate in parallel. Such a machine is also called a multiprocessor. <p> 218 822 324 2144 902 10780 Shallow Water Mflops | | 560 | 1532 | | R 1 (~x fl ~y) Mflops 22 70 15000 n 1=2 (~x fl ~y) 18 53 650 Table 4: A comparison of the performance of some Cray vector supercomputers, partially based on data from <ref> [Dongarra 94] </ref>. may act independently of one other on different data, while the processors of an SIMD machine perform the same instruction at the same time but on different data. The category of MIMD multiprocessors is also divided into two subtypes: those with distributed memory and those with shared memory. <p> Performance figures taken from [Bailey et al. 94], [van der Steen 93], and <ref> [Dongarra 94] </ref>.
Reference: [Dongarra et al. 79] <author> DONGARRA, J. J., C. B. MOLER, J. R. BUNCH, AND G. W. STEWART. </author> <year> [1979]. </year> <title> LINPACK User's Guide. </title> <publisher> SIAM, </publisher> <address> Philadelphia. </address> <note> ISBN 0-89871-172-X. CUBoulder : HPSC Course Notes 68 Computer Performance (DRAFT: 19 Dec 1994) </note>
Reference-contexts: The SPECmarks quoted came from various articles posted on comp.benchmarks or from manufacturer's brochures; the LINPACK values came from [Dongarra 94]. 3.4.1 LINPACK In the 1970's, a set of Fortran subroutines was developed to solve systems of linear equations. This set of routines is called LINPACK <ref> [Dongarra et al. 79] </ref>.
Reference: [Dowd 93] <author> DOWD, KEVIN. </author> <year> [1993]. </year> <title> High Performance Computing. </title> <publisher> O'Reilly & Associates, Inc., </publisher> <address> Sebastopol,CA. ISBN 1-56592-032-5. </address>
Reference: [Fox et al. 94] <author> FOX, GEOFFREY C., ROY D. WILLIAMS, AND PAUL C. MESSINA. </author> <title> [1994]. </title> <publisher> Parallel Computing Works! Morgan Kaufmann Publishers, Inc., </publisher> <address> San Francisco, CA. </address>
Reference: [Golub & Van Loan 83] <author> GOLUB, G.H. AND C.F. Van LOAN. </author> <year> [1983]. </year> <title> Matrix Computations. </title> <publisher> The Johns Hopkins Press, </publisher> <address> Baltimore, MD, 1st edition. </address>
Reference-contexts: In some contexts, all floating-point operations are included in the count of flops. In others, only the multiplications and additions are included. (The LINPACK benchmark uses this definition of the flop.) Yet another definition of flop proposed by C.B. Moler <ref> [Golub & Van Loan 83] </ref> includes some time for access of array elements in addition to multiplications and additions.
Reference: [Heath et al. 90] <author> HEATH, M. T., G. A. GEIST, B. W. PEYTON, AND P. H. WOR-LEY. </author> <month> [Oct </month> <year> 1990]. </year> <title> A user's guide to PICL (ORNL/TM-11616). </title> <type> Technical Report TN 37931, </type> <institution> Oak Ridge National Laboratory, Oak Ridge, TN. </institution>
Reference-contexts: Another tool called PICL (Portable Instrumented Communication Library) facilitates the collection of traces of send/receive operations in a distributed-memory MIMD machine. These traces can later plotted by a subtool called ParaGraph to show the parallel execution of the traced program. This display can even be animated <ref> [Heath et al. 90] </ref>. 7 Summary As we have seen, many factors may influence the performance of a computer. Because of this, it is difficult to compare two different architectures.
Reference: [Hennessy & Patterson 90] <author> HENNESSY, JOHN L. AND DAVID A. PATTERSON. </author> <year> [1990]. </year> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, CA. ISBN 1-55860-069-8. </address>
Reference: [Higham 93] <author> HIGHAM, NICHOLAS J. </author> <year> [1993]. </year> <title> Handbook of Writing for the Mathematical Sciences. </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA. ISBN 0-89871-314-5. </address>
Reference: [Hockney & Jesshope 88] <author> HOCKNEY, R. W. AND C. R. JESSHOPE. </author> <year> [1988]. </year> <note> Parallel Computers 2. Adam Hilger. ISBN 0-85274-812-4. </note>
Reference-contexts: Max Memory 1MB 16 MB 128 MB 16 GB Word Size 64 bits 64 bits 64 bits 64 bits Vector Reg Size 64 words 64 words 64 words 64 words Table 3: A comparison of the characteristics of some Cray vector supercom puters, based on data from [Dongarra 94] and <ref> [Hockney & Jesshope 88] </ref>. * n 1=2 The length, m = n 1=2 , of a vector such that R m is equal to R 1 =2. <p> Table 3 summarizes the characteristics of four Cray models while table 4 compares the performance of those four Crays. These figures are based on data from [Dongarra 94] and <ref> [Hockney & Jesshope 88] </ref>. For more information on vector processors, see [Schauble 94]. 6.2 Parallel Computers A parallel computer is a machine with two or more connected processors that may operate in parallel. Such a machine is also called a multiprocessor. <p> Announcements of related conferences and workshops are stream). The first of these includes the common von Neumann computer; this machine performs one instruction at a time on one set of data. The second set is considered to be empty. <ref> [Hockney & Jesshope 88] </ref> CUBoulder : HPSC Course Notes 54 Computer Performance (DRAFT: 19 Dec 1994) included here as well. 6.2.1 Distributed-Memory MIMD Multiprocessors Each of the individual processors of a distributed-memory multiprocessor (DM-MIMD) has a memory unit associated directly with it; that is, each processor has its own local memory.
Reference: [Hoffmann et al. 88] <author> HOFFMANN, G. R., P. N. SWARZTRAUBER, AND R. A. SWEET. </author> <year> [1988]. </year> <title> Aspects of using multiprocessors for meteroro-logical modelling. </title> <editor> In HOFFMAN, G.-R. AND D. F. SNELLING, editors, </editor> <booktitle> Multiprocessing in Meteorological Models, </booktitle> <pages> pages 127-196. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, NY. </address>
Reference-contexts: The purpose of the benchmark is to compare the performance of supercomputers on this code. Results are given both in elapsed time and in Mflops <ref> [Sadourney 75, Hoffmann et al. 88] </ref>. 3.4.8 SPICE SPICE (Simulation Program with Integrated Circuit Emphasis) is a circuit-level simulator developed in the early 1970's at Berkeley.
Reference: [Jessup 94] <author> JESSUP, ELIZABETH R. </author> <year> [1994]. </year> <title> MIMD computing: An introduction. </title> <booktitle> HPSC Course Notes. </booktitle>
Reference-contexts: Another example of a distributed-memory MIMD supercomputer is the CM-5 by Thinking Machines Corp. Each of the processors in this multiprocessor contains four vector-processors; hence, this type of machine can also be described as a DM/V-MIMD multiprocessor. For more information on distributed-memory MIMD multiprocessors, see <ref> [Jessup 94] </ref>. The theoretical peak performance for some of the machines mentioned above is given in table 5. Table 6 on page 61 compares benchmark results for these machines.
Reference: [Kane 88] <author> KANE, GERRY. </author> <year> [1988]. </year> <title> MIPS RISC Architecture. </title> <publisher> Prentice-Hall. </publisher> <address> ISBN 0-13-584749-4. </address> <note> CUBoulder : HPSC Course Notes Computer Performance (DRAFT: 19 Dec 1994) 69 </note>
Reference-contexts: Such units include SPECmarks and the other definition of mips. These are discussed in the next section. 2.4 Timing elementary operations In this section we discuss how to time the elementary arithmetic operations on the DECstation 5000/240. This system uses a MIPS R3000 CPU and R3010 FPU chipset <ref> [Kane 88, DEC 91] </ref>, operating at 40 MHz (25 nsec cycle time).
Reference: [Koelbel et al. 94] <author> KOELBEL, CHARLES H., DAVID B. LOVEMAN, ROBERT S. SCHREIBER, JR. GUY L. STEELE, AND MARY E. ZOSEL. </author> <year> [1994]. </year> <title> The High Performance Fortran Handbook. Scientific and Engineering Computation. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. ISBN 0-262-11185-3. </address>
Reference-contexts: Most manufacturers of vector and parallel computers provide special operating systems and compilers with extensions that exploit the parallel (and vector) mechanisms of the machine. Both Fortran 90 [Adams et al. 92] and HPF (High Performance Fortran) <ref> [Koelbel et al. 94] </ref> are designed to consolidate multiple extensions into a uniform version of the Fortran programming language, allowing additional operations that could be compiled by different machines into parallel or vector operations. Debugging tools for parallel programs are currently under research.
Reference: [McMahon 86] <author> MCMAHON, F. H. </author> <month> [Dec </month> <year> 1986]. </year> <title> The Livermore Fortran Kernels: A computer test of the numerical performance range. </title> <type> Technical Report UCRL-53745, </type> <institution> Lawrence Livermore National Laboratory, Livermore, </institution> <address> CA. </address>
Reference-contexts: A similar version of the kernels exists in C. More information on the benchmark, including both code and results, is available from netlib. Also see <ref> [McMahon 86] </ref>. CUBoulder : HPSC Course Notes 30 Computer Performance (DRAFT: 19 Dec 1994) 3.4.3 Whetstones Designed to compare the floating-point performance of different computers, this benchmark is a single program, originally written in Algol-60. There is a Fortran version as well.
Reference: [Patterson & Hennessy 94] <author> PATTERSON, DAVID A. AND JOHN L. HENNESSY. </author> <year> [1994]. </year> <title> Computer Organization & Design: The Hardware/Software Interface. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, CA. ISBN 1-55860-281-X. </address>
Reference: [Sadourney 75] <author> SADOURNEY, ROBERT. </author> <month> [Apr </month> <year> 1975]. </year> <title> The dynamics of finite-difference models of the shallow-water equations. </title> <journal> Journal of Atmospheric Sciences, </journal> <volume> 32 </volume> <pages> 680-689. </pages>
Reference-contexts: The purpose of the benchmark is to compare the performance of supercomputers on this code. Results are given both in elapsed time and in Mflops <ref> [Sadourney 75, Hoffmann et al. 88] </ref>. 3.4.8 SPICE SPICE (Simulation Program with Integrated Circuit Emphasis) is a circuit-level simulator developed in the early 1970's at Berkeley.
Reference: [Schauble 93] <author> SCHAUBLE, CAROLYN J. C. </author> <year> [1993]. </year> <title> The Connection Machine (CM-2): An introduction. </title> <booktitle> HPSC Course Notes. </booktitle>
Reference-contexts: Table 5 also provides the theoretical peak performance for some SIMD multiprocessors, and ta CUBoulder : HPSC Course Notes 60 Computer Performance (DRAFT: 19 Dec 1994) ble 6 compares benchmark results for these machines. For more information on SIMD multiprocessors, see <ref> [Schauble 93] </ref>. 6.2.5 Performance of Parallel Programs When considering the performance of a parallel program on a given machine, it is usually compared to the performance of the same program on a single processor of that machine.
Reference: [Schauble 94] <author> SCHAUBLE, CAROLYN J. C. </author> <year> [1994]. </year> <title> Vector computing: An introduction. </title> <booktitle> HPSC Course Notes. </booktitle>
Reference-contexts: This means that the startup time would be completely negligible. 3 More detailed information on this topic can be found in the tutorial on vector pro cessing <ref> [Schauble 94] </ref>. <p> Table 3 summarizes the characteristics of four Cray models while table 4 compares the performance of those four Crays. These figures are based on data from [Dongarra 94] and [Hockney & Jesshope 88]. For more information on vector processors, see <ref> [Schauble 94] </ref>. 6.2 Parallel Computers A parallel computer is a machine with two or more connected processors that may operate in parallel. Such a machine is also called a multiprocessor.
Reference: [Smith et al. 76] <author> SMITH, B.T., J.M. BOYLE, J.J. DONGARRA, B.S. GARBOW, Y. IKEBE, V.C. KLEMA, AND C.B. MOLER. </author> <year> [1976]. </year> <title> Matrix Eigensys-tem Routines | EISPACK Guide, </title> <booktitle> volume 6 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer-Verlag, 2nd edition. </publisher>
Reference-contexts: Many of the reported LINPACK results have been produced using these assembler routines. While LINPACK is still used for benchmarking purposes, it is no longer recommended for solving linear algebra problems. Released in 1992, LAPACK [Anderson et al. 92] replaces both LINPACK and EISPACK <ref> [Smith et al. 76] </ref>, the collection of routines for eigenvalue problems introduced in 1970. LAPACK makes use of the newest, most accurate algorithms in numerical linear algebra. It also makes efficient use of memory by dividing large matrix operations into smaller block matrix operations.
Reference: [Stone 87] <editor> STONE, HAROLD S., editor. </editor> <booktitle> [1987]. High-Performance Computer Architecture. </booktitle> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, MA, </address> <note> 2nd edition. ISBN 0-201-52688-3. </note>
Reference: [van der Steen 93] <author> VAN DER STEEN, AAD J. </author> <month> [Dec </month> <year> 1993]. </year> <title> An overview of (almost) available parallel systems. </title> <type> Technical report, </type> <institution> Stichting Na-tionale Computer Faciliteiten, The Netherlands. </institution> <note> Third revised edition, netlib. CUBoulder : HPSC Course Notes 70 Computer Performance (DRAFT: 19 Dec 1994) </note>
Reference-contexts: Table 1 provides a short list of supercomputers, with the name of the machine series, the name of the manufacturer, and the TPP given in Mflops. The figures are taken from <ref> [van der Steen 93] </ref>. Still other units of computer performance measurement refer to specific benchmarks programs. Such units include SPECmarks and the other definition of mips. <p> See <ref> [van der Steen 93] </ref>. sors was done with a butterfly switch. The KSR1 by Kendall Square is also a member of this group of multiprocessors; this architecture links 32 processors in a ring and connects the rings in a tree structure. <p> Performance figures taken from [Bailey et al. 94], <ref> [van der Steen 93] </ref>, and [Dongarra 94].
Reference: [Weicker 84] <author> WEICKER, REINHOLD P. </author> <month> [Oct </month> <year> 1984]. </year> <title> Dhrystone: A synthetic systems programming benchmark. </title> <journal> Communications of the ACM, </journal> <volume> 27(10) </volume> <pages> 1013-1030. </pages> <note> CUBoulder : HPSC Course Notes </note>
Reference-contexts: One problem with this benchmark is the smallness of the loops that often fit into the cache of the machine being tested. This means that the results may show a better performance than the user should expect with larger programs. <ref> [Weicker 84] </ref> Again, these benchmarks are publically available by ftp from netlib@ornl.gov ftp.nosc.mil:pub/aburto More information is available from netlib.
References-found: 29

