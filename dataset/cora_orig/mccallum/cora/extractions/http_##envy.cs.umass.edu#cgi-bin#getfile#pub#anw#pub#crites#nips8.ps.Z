URL: http://envy.cs.umass.edu/cgi-bin/getfile/pub/anw/pub/crites/nips8.ps.Z
Refering-URL: http://envy.cs.umass.edu/People/barto/barto.html
Root-URL: 
Email: crites@cs.umass.edu  barto@cs.umass.edu  
Title: In  Improving Elevator Performance Using Reinforcement Learning  
Author: D. S. Touretzky, M. C. Mozer, and M. E. Hasselmo, Robert H. Crites Andrew G. Barto 
Address: Amherst, MA 01003-4610  Amherst, MA 01003-4610  
Affiliation: Computer Science Department University of Massachusetts  Computer Science Department University of Massachusetts  
Note: eds., Advances in Neural Information Processing Systems 8. MIT Press, Cambridge MA, 1996.  
Abstract: This paper describes the application of reinforcement learning (RL) to the difficult real world problem of elevator dispatching. The elevator domain poses a combination of challenges not seen in most RL research to date. Elevator systems operate in continuous state spaces and in continuous time as discrete event dynamic systems. Their states are not fully observable and they are nonstationary due to changing passenger arrival rates. In addition, we use a team of RL agents, each of which is responsible for controlling one elevator car. The team receives a global reinforcement signal which appears noisy to each agent due to the effects of the actions of the other agents, the random nature of the arrivals and the incomplete observation of the state. In spite of these complications, we show results that in simulation surpass the best of the heuristic elevator control algorithms of which we are aware. These results demonstrate the power of RL on a very large scale stochastic dynamic optimization problem of practical utility.
Abstract-found: 1
Intro-found: 1
Reference: <author> G. Bao, C. G. Cassandras, T. E. Djaferis, A. D. Gandhi, and D. P. Looze. </author> <title> (1994) Elevator Dispatchers for Down Peak Traffic. </title> <type> Technical Report, </type> <institution> ECE Department, University of Massachusetts, </institution> <address> Amherst, MA. </address>
Reference-contexts: Passenger arrivals at each floor are assumed to be Poisson, with arrival rates that vary during the course of the day. Our simulations use a traffic profile <ref> (Bao et al, 1994) </ref> which dictates arrival rates for every 5-minute interval during a typical afternoon down-peak rush hour. Table 1 shows the mean number of passengers arriving at each floor (2-10) during each 5-minute interval who are headed for the lobby. <p> ESA uses queue length information that would not be available in a real elevator system. ESA/nq is a version of ESA that uses arrival rate information to estimate the queue lengths. For more details, see <ref> (Bao et al, 1994) </ref>. These receding horizon controllers are very sophisticated, but also very computationally intensive, such that they would be difficult to implement in real time. RLp and RLd denote the RL controllers, parallel and decentralized.
Reference: <author> S. J. Bradtke and M. O. Duff. </author> <title> (1995) Reinforcement Learning Methods for Continuous-Time Markov Decision Problems. </title> <editor> In: G. Tesauro, D. S. Touretzky and T. K. Leen, eds., </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: In such systems, the constant discount factor fl used in most discrete-time reinforcement learning algorithms is inadequate. This problem can be approached using a variable discount factor that depends on the amount of time between events <ref> (Bradtke & Duff, 1995) </ref>.
Reference: <author> J. Lewis. </author> <title> (1991) A Dynamic Load Balancing Approach to the Control of Multiserver Polling Systems with Applications to Elevator System Dispatching. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, </institution> <address> Amherst, MA. </address>
Reference: <author> K. L. Markey. </author> <title> (1994) Efficient Learning of Multiple Degree-of-Freedom Control Problems with Quasi-independent Q-agents. </title> <editor> In: M. C. Mozer, P. Smolensky, D. S. Touretzky, J. L. Elman and A. S. Weigend, eds., </editor> <booktitle> Proceedings of the 1993 Connectionist Models Summer School. </booktitle> <publisher> Erlbaum Associates, </publisher> <address> Hillsdale, NJ. </address>
Reference-contexts: Therefore, we employed a team of discrete-event Q-learning agents, where each agent is responsible for controlling one elevator car. Another application using a team of Q-learning agents is described in <ref> (Markey, 1994) </ref>. Q (x; a) is defined as the expected infinite discounted return obtained by taking action a in state x and then following an optimal policy (Watkins, 1989). Because of the vast number of states, the Q-values are stored in feedforward neural networks.
Reference: <author> G. Tesauro. </author> <title> (1992) Practical Issues in Temporal Difference Learning. </title> <booktitle> Machine Learning 8 </booktitle> <pages> 257-277. </pages>
Reference: <author> G. Tesauro. </author> <title> (1994) TD-Gammon, a Self-Teaching Backgammon Program, Achieves Master-Level Play. </title> <booktitle> Neural Computation 6 </booktitle> <pages> 215-219. </pages>
Reference: <author> G. Tesauro. </author> <title> (1995) Temporal Difference Learning and TD-Gammon. </title> <journal> Communications of the ACM 38 </journal> <pages> 58-68. </pages>
Reference: <author> C. J. C. H. Watkins. </author> <title> (1989) Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> Cam-bridge University. </institution>
Reference-contexts: Another application using a team of Q-learning agents is described in (Markey, 1994). Q (x; a) is defined as the expected infinite discounted return obtained by taking action a in state x and then following an optimal policy <ref> (Watkins, 1989) </ref>. Because of the vast number of states, the Q-values are stored in feedforward neural networks. The networks receive some state information as input, and produce Q-value estimates as output. We have tested two architectures.
References-found: 8

