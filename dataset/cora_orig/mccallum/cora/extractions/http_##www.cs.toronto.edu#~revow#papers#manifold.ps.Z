URL: http://www.cs.toronto.edu/~revow/papers/manifold.ps.Z
Refering-URL: http://www.cs.utoronto.ca/neuron/elastic.html
Root-URL: 
Email: (hinton@cs.toronto.edu)  (dayan@ai.mit.edu)  (revow@cs.toronto.edu)  
Phone: Phone (416) 978 7453 Fax (416) 978  phone (617) 252 1693, fax (617) 253 2964  
Title: Modelling the Manifolds of Images of Handwritten Digits  
Author: Geoffrey E. Hinton Peter Dayan Michael Revow 
Keyword: principal components, factor analysis, autoencoder, minimum description length, density estimation.  
Note: IEEE trans. on Neural Networks 8(1) 65-74, 1997. 1 Present Address:  
Address: 6 Kings College Road Toronto, Ontario Canada M5S 3H5  1525  E25-229, MIT, Cambridge, MA 02139,  
Affiliation: Department of Computer Science University of Toronto  Center for Biological and Computational Learning, Department of Brain and Cognitive Sciences,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Y. Le Cun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel, </author> <title> "Handwritten digit recognition with a back-propagation network", </title> <booktitle> in Advances in Neural Information Processing Systems, </booktitle> <editor> D. S. Touretzky, Ed., </editor> <booktitle> Denver, 1990, </booktitle> <volume> vol. 2, </volume> <pages> pp. 396-404, </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo. </address>
Reference-contexts: Links with other methods that model the manifold are discussed. I Introduction A standard discriminative approach to digit recognition is to train a classifier to output one of the ten classes based on the input image. The classifier could, for instance, be a multilayer feedforward neural network <ref> [1] </ref>. However, it is also possible to discriminate by fitting a separate probability density model to each class and then picking the class of the model that assigns the highest density to a test image.
Reference: [2] <author> D. E Specht, </author> <title> "Probabilistic neural networks", </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 3, no. 1, </volume> <pages> pp. 109-118, </pages> <year> 1990. </year>
Reference-contexts: We are not claiming that the relative density approach is necessarily better than the discriminative approach, just that it is an alternative worth considering. Certain discriminative methods can be seen in terms of relative densities. For instance, kernel density estimation <ref> [2] </ref>, [3] is a popular non-parametric modelling technique. For this, the probability density for a particular digit is the weighted sum of a collection of kernel functions. The functions all have the same shape, but each is centred on one of the patterns in that class in the training set.
Reference: [3] <author> C. Bishop, </author> <title> Neural networks for pattern recognition, </title> <publisher> Clarendon Press, </publisher> <year> 1995. </year>
Reference-contexts: We are not claiming that the relative density approach is necessarily better than the discriminative approach, just that it is an alternative worth considering. Certain discriminative methods can be seen in terms of relative densities. For instance, kernel density estimation [2], <ref> [3] </ref> is a popular non-parametric modelling technique. For this, the probability density for a particular digit is the weighted sum of a collection of kernel functions. The functions all have the same shape, but each is centred on one of the patterns in that class in the training set.
Reference: [4] <author> B. </author> <title> Widrow, </title> <journal> "The `rubber-mask' technique-I. Pattern Measurement and Analysis", Pattern Recognition, </journal> <volume> vol. 5, </volume> <pages> pp. 175-197, </pages> <year> 1973. </year>
Reference-contexts: The latter two are not true for images of handwritten digits. We sought to build better models for such images on the basis of such information. Elastically deformable templates <ref> [4] </ref>, [5] are one example, and have been shown to model non-normalised images of characters well [6]. Unfortunately, they are also computationally too expensive for normal use. We therefore turned to Gaussian blended linear models, which are computationally much cheaper but are also appropriate for such images.
Reference: [5] <author> M. A. Fischler and R. A. Elschlager, </author> <title> "The representation and matching of pictorial structures", </title> <journal> IEEE Trans. Computers, </journal> <volume> vol. C-22, no. 1, </volume> <pages> pp. 67-92, </pages> <year> 1973. </year>
Reference-contexts: The latter two are not true for images of handwritten digits. We sought to build better models for such images on the basis of such information. Elastically deformable templates [4], <ref> [5] </ref> are one example, and have been shown to model non-normalised images of characters well [6]. Unfortunately, they are also computationally too expensive for normal use. We therefore turned to Gaussian blended linear models, which are computationally much cheaper but are also appropriate for such images.
Reference: [6] <author> M. Revow, C. K. I. Williams, and G. E. Hinton, </author> <title> "Using generative models for handwritten digit recognition", </title> <journal> IEEE Transactions Pattern Analysis and Machine Intellegince, </journal> <volume> vol. 18, no. 6, </volume> <pages> pp. 592-606, </pages> <year> 1996. </year>
Reference-contexts: The latter two are not true for images of handwritten digits. We sought to build better models for such images on the basis of such information. Elastically deformable templates [4], [5] are one example, and have been shown to model non-normalised images of characters well <ref> [6] </ref>. Unfortunately, they are also computationally too expensive for normal use. We therefore turned to Gaussian blended linear models, which are computationally much cheaper but are also appropriate for such images. Simard et al [7] pointed out the locally low-dimensional linear structure underlying these images. <p> Attached to each sub-model's reconstruction is its reconstruction cost relative to the best sub-model (which has cost of zero) . As a comparison, other state of the art methods obtain about 3% error rates <ref> [6] </ref> on the original data which had a mean size of around 45 fi 60 pixels. Thus the images we used had areas about 40 times smaller and so could well have lost information.
Reference: [7] <author> P. Simard, Y. Le Cun, and J. Denker, </author> <title> "Efficient pattern recognition using a new transformation distance.", </title> <booktitle> in Advances in Neural Information Processing Systems 5, </booktitle> <editor> J. D. Cowan S. J. Hanson and C. L. Giles, </editor> <booktitle> Eds., </booktitle> <pages> pp. 50-58. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: Unfortunately, they are also computationally too expensive for normal use. We therefore turned to Gaussian blended linear models, which are computationally much cheaper but are also appropriate for such images. Simard et al <ref> [7] </ref> pointed out the locally low-dimensional linear structure underlying these images. Take the 64-dimensional space of all (normalised) 8 fi 8 images and consider the subset of images of any particular digit, say the digit 2. <p> Considering the effect on an image of small (ie sub-pixel) transformations like these suggests that the surface is locally at least 7 dimensional and probably somewhat more. Different styles of 2 will likely generate separated continuous patches. Simard et al <ref> [7] </ref> used a nearest neighbour method (which, as pointed out, is equivalent to a limiting case of a relative density method) in the space of these 7 dimensional planes, where the distance between two points in the space is the closest distance between the underlying 3 planes in image space. <p> However, FA offers a sounder statistical model of examples, and one might expect it to be more proficient. Section II describes principal components analysis; section III describes factor analysis; section IV shows how to incorporate some of the tangent information that Simard et al <ref> [7] </ref> use to such good effect; and section V shows how the models perform on a large-scale digit recognition task. <p> On the other hand, Simard et al <ref> [7] </ref>, in the method described in the introduction, 16 used an approach that owes more to modelling the local structure of the classes. <p> If the database of training examples had just been expanded, this constraint would not have been applied. There are two differences between this use of the tangent vectors and that in Simard et al <ref> [7] </ref>. One is that for us, the effect of these tangents has a limited spatial extent, whereas for them, the linear manifold about each image extends to infinity. <p> The number of dot products for the tangent distance method depends on the number of tangent directions <ref> [7] </ref>. If 7 tangent directions are used on these data then of the order of 96fiN dot products are needed, or about a factor of 525 times the local model version. 23 layout is the same as in figure 5. <p> The results in the last two columns were obtained with models trained on all 11; 000 training images. highly non-linear effects on the observed images. 9 Third, a priori information about the local structure of the manifolds that comes from knowledge about invariances of digit identities over certain transformations <ref> [7] </ref> is very easy to incorporate into these linear models. Note that FA is just a particular way of limiting the number of parameters that define the covariance matrix used to model data.
Reference: [8] <author> P. Simard, </author> <title> "Efficient computation of complex distance metrcis using hierarchial filtering", </title> <booktitle> in Advances in Neural Information Processing Systems 6, </booktitle> <editor> J. D. Cowan, G. Tesauro, and J Alspector, </editor> <booktitle> Eds., </booktitle> <pages> pp. 168-175. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: the training examples; the maximization (M) phase involves re-estimating the parameters of the linear models in the light of this assignment. 2 This distance is not a metric since it does not satisfy the triangle inequality. 3 Significant performance improvements can be achieved by using various speedup mechanisms, for example <ref> [8] </ref> 4 is captured by 6 locally linear models (right panel) A convenient framework to describe our models comes from the version of neural nets called autoencoders. An autoencoder is a feedforward neural network with a single hidden layer that attempts to reconstruct its input activities at its output.
Reference: [9] <author> G. E. Hinton, M. Revow, and P. Dayan, </author> <title> "Recognizing handwritten digits using mixtures of linear models", </title> <booktitle> in Advances in Neural Information Processing Systems 7, </booktitle> <editor> G. Tesauro, D. S. Touretzky, and T. K. Leen, </editor> <booktitle> Eds., </booktitle> <pages> pp. 1015-1022. </pages> <publisher> MIT Press, </publisher> <address> Cam-bridge MA, </address> <year> 1995. </year>
Reference-contexts: We were therefore forced to mix together numbers of linear models for images of each digit, ie to use a blended linear approximation to the surface (figure 1) <ref> [9] </ref>, [10], [11], [12], [13]. The mixture is fit using either an expectation-maximization (EM) based algorithm [14] or the k-means algorithm, which is actually a limiting case of EM.
Reference: [10] <author> C. Bregler and S. M. Omohundro, </author> <title> "Nonlinear image interpolation using manifold learning", </title> <booktitle> in Advances in Neural Information Processing Systems 7, </booktitle> <editor> G. Tesauro, D. S. Touretzky, and T. K. Leen, </editor> <booktitle> Eds., </booktitle> <pages> pp. 971-980. </pages> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: We were therefore forced to mix together numbers of linear models for images of each digit, ie to use a blended linear approximation to the surface (figure 1) [9], <ref> [10] </ref>, [11], [12], [13]. The mixture is fit using either an expectation-maximization (EM) based algorithm [14] or the k-means algorithm, which is actually a limiting case of EM. <p> The absolute quality of the best reconstruction and the relative qualities of slightly sub-optimal reconstructions are available to reject ambiguous cases. Somewhat similar methods have been used for modelling images of lips for lip reading <ref> [10] </ref>, cartoon-like drawings [12], digit and character recognition [13], [17] and data compression [11]. 8 III Mixtures of Factor Analysers Unfortunately, as noted above, PCA does not provide a correct statistical model for the images because it is not properly normalisable.
Reference: [11] <author> N. Kambhatla and T. K. Leen, </author> <title> "Fast non-linear dimension reduction", </title> <booktitle> in Advances in Neural Information Processing Systems 6, </booktitle> <editor> J. D. Cowan, G. Tesauro, and J. Alspector, </editor> <booktitle> Eds., </booktitle> <pages> pp. 152-159. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: We were therefore forced to mix together numbers of linear models for images of each digit, ie to use a blended linear approximation to the surface (figure 1) [9], [10], <ref> [11] </ref>, [12], [13]. The mixture is fit using either an expectation-maximization (EM) based algorithm [14] or the k-means algorithm, which is actually a limiting case of EM. <p> The absolute quality of the best reconstruction and the relative qualities of slightly sub-optimal reconstructions are available to reject ambiguous cases. Somewhat similar methods have been used for modelling images of lips for lip reading [10], cartoon-like drawings [12], digit and character recognition [13], [17] and data compression <ref> [11] </ref>. 8 III Mixtures of Factor Analysers Unfortunately, as noted above, PCA does not provide a correct statistical model for the images because it is not properly normalisable.
Reference: [12] <author> K. Sung and T. Poggio, </author> <title> Example based learning for view-based human face detection, </title> <note> AI Memo 1521, CBCL paper 112, </note> <year> 1995. </year>
Reference-contexts: We were therefore forced to mix together numbers of linear models for images of each digit, ie to use a blended linear approximation to the surface (figure 1) [9], [10], [11], <ref> [12] </ref>, [13]. The mixture is fit using either an expectation-maximization (EM) based algorithm [14] or the k-means algorithm, which is actually a limiting case of EM. <p> The absolute quality of the best reconstruction and the relative qualities of slightly sub-optimal reconstructions are available to reject ambiguous cases. Somewhat similar methods have been used for modelling images of lips for lip reading [10], cartoon-like drawings <ref> [12] </ref>, digit and character recognition [13], [17] and data compression [11]. 8 III Mixtures of Factor Analysers Unfortunately, as noted above, PCA does not provide a correct statistical model for the images because it is not properly normalisable. <p> Components of the image that lie in the directions of the principal components generate no reconstruction error, and therefore can be added at will without changing the cost. Poggio & Sung <ref> [12] </ref> suggested using as a component of the cost a quantity they called the normalised Mahalanobis distance in the subspace of the principal components.
Reference: [13] <author> T. Hastie and P. Simard, </author> <title> "Learning prototype models for tangent distance", </title> <booktitle> in Advances in Neural Information Processing Systems 7, </booktitle> <editor> G. Tesauro, D. S. Touretzky, and T. K. Leen, </editor> <booktitle> Eds., </booktitle> <pages> pp. 999-1006. </pages> <publisher> MIT Press, </publisher> <year> 1995. </year> <month> 28 </month>
Reference-contexts: We were therefore forced to mix together numbers of linear models for images of each digit, ie to use a blended linear approximation to the surface (figure 1) [9], [10], [11], [12], <ref> [13] </ref>. The mixture is fit using either an expectation-maximization (EM) based algorithm [14] or the k-means algorithm, which is actually a limiting case of EM. <p> The absolute quality of the best reconstruction and the relative qualities of slightly sub-optimal reconstructions are available to reject ambiguous cases. Somewhat similar methods have been used for modelling images of lips for lip reading [10], cartoon-like drawings [12], digit and character recognition <ref> [13] </ref>, [17] and data compression [11]. 8 III Mixtures of Factor Analysers Unfortunately, as noted above, PCA does not provide a correct statistical model for the images because it is not properly normalisable. <p> It would be straightforward to do this during recognition for both PCA and FA; however doing it during learning is computationally more tricky <ref> [13] </ref>. Simard et al's metric would be irrelevant in the limit of very large numbers of training images, since the database itself would contain all the transformations that actually preserve digit identity. In the same limit, the local linear PCA and FA methods would also not benefit from the tangents. <p> Hastie & Simard <ref> [13] </ref>, developed a locally linear mixture model analogous to the one described here, except using two-sided tangent distances during the whole of learning. Tangents would be expected not to help if there are enough data points that they express directly all the actual invariances.
Reference: [14] <author> A. P. Dempster, N. M. Laird, and D. B. Rubin, </author> <title> "Maximum likelihood from incomplete data via the EM algorithm", </title> <journal> Proceedings of the Royal Statistical Society, </journal> <volume> vol. B-39, </volume> <pages> pp. 1-38, </pages> <year> 1977. </year>
Reference-contexts: We were therefore forced to mix together numbers of linear models for images of each digit, ie to use a blended linear approximation to the surface (figure 1) [9], [10], [11], [12], [13]. The mixture is fit using either an expectation-maximization (EM) based algorithm <ref> [14] </ref> or the k-means algorithm, which is actually a limiting case of EM.
Reference: [15] <author> G. E. Hinton and R. Zemel, "Autoencoders, </author> <title> minimum description length and helmholtz free energy", </title> <booktitle> in Advances in Neural Information Processing Systems 6, </booktitle> <editor> J. Cowan, G. Tesauro, and J. Alspector, Eds. </editor> <publisher> Morgan Kauffmann, </publisher> <year> 1994. </year>
Reference-contexts: An autoencoder is a feedforward neural network with a single hidden layer that attempts to reconstruct its input activities at its output. Hinton & Zemel <ref> [15] </ref> and Zemel [16] show how to understand the relationship between statistical modelling and autoencoders. <p> This log-likelihood is based on a model for the image under which the (incorrectly normalised) log probability of image i is <ref> [15] </ref>: 2 2 a=1 a E i m X q i a (3) Minus this quantity can be considered as a cost function for learning the assignments of responsibilities and the principal components. The value of 2 is somewhat arbitrary.
Reference: [16] <author> R. S. Zemel, </author> <title> A minimum description length framework for unsupervised learning, </title> <type> PhD Thesis, </type> <institution> Department of Computer Science, University of Toronto, </institution> <year> 1993. </year>
Reference-contexts: An autoencoder is a feedforward neural network with a single hidden layer that attempts to reconstruct its input activities at its output. Hinton & Zemel [15] and Zemel <ref> [16] </ref> show how to understand the relationship between statistical modelling and autoencoders. <p> Choose initial assignments among the sub-models for each example in the training set (typically at random, or using samples from the initial data); 4 Strictly speaking there is a third component, the model-cost, which encodes the cost of specifying the weights in each model <ref> [16] </ref>. We assume that this cost is the same for all models. 7 2. Perform PCA separately for each sub-model; 3. Reassign patterns to the sub-model that reconstructs them the best; 4. Stop if no patterns have changed sub-model, otherwise return to step 2.
Reference: [17] <author> H. Schwenk and M. Milgram, </author> <title> "Transformation invariant autoassociation with application to handwritten character recognition", </title> <booktitle> in Advances in Neural Information Processing Systems 7, </booktitle> <editor> G. Tesauro, D. S. Touretzky, and T. K. Leen, </editor> <booktitle> Eds., </booktitle> <pages> pp. 991-998. </pages> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: The absolute quality of the best reconstruction and the relative qualities of slightly sub-optimal reconstructions are available to reject ambiguous cases. Somewhat similar methods have been used for modelling images of lips for lip reading [10], cartoon-like drawings [12], digit and character recognition [13], <ref> [17] </ref> and data compression [11]. 8 III Mixtures of Factor Analysers Unfortunately, as noted above, PCA does not provide a correct statistical model for the images because it is not properly normalisable. <p> Note that these local surfaces are chosen to model the images of each digit as best as possible, and not to support the best possible discrimination between them. This local low-dimensional and linear behaviour is what motivated our linear models. Schwenk and Milgram <ref> [17] </ref>, [25] take a slightly different approach and compile down all knowledge about a character into a single prototype trained to directly minimize the distance between the prototype and the tangent planes around each of the training examples. different examples of the same digit, subject to some smooth transformation.
Reference: [18] <author> B. S. Everitt, </author> <title> An introduction to latent variable models, </title> <publisher> Chapman and Hall, </publisher> <year> 1984. </year>
Reference-contexts: Choosing the relative weighting in the overall cost function for the squared reconstruction error E i a and the coding cost is somewhat arbitrary. Factor analysis (FA) is a different way of analysing the covariance matrix of the inputs (see <ref> [18] </ref>, for an excellent introduction) which starts from a proper probabilistic model, and correctly blends the reconstruction cost and a term playing a similar role to this normalised Mahalanobis distance. <p> In terms of the notation introduced above, the standard factor analysis 9 model is written as <ref> [18] </ref>: y i = Gh i + ~ i (4) The hidden-output weights, G, are the factor loadings and the activities of the hidden units, h i , are the factors (the prior over which is Gaussian). <p> It is common to take to be the identity matrix. Model (4) implies that the covariance of the observed variables is given by: C (G; ) = GG T + (5) Under the model, the sample covariance matrix (S) follows a Wishart distribution [19] about C and Everitt <ref> [18] </ref> introduces the function: F (S; C (G; )) = lnjCj + trace (SC 1 ) lnjSj n (6) which, up to some constant factors, is the likelihood. Maximum likelihood FA fits the parameters of the model G and by maximising equation (6).
Reference: [19] <author> K. V. Mardia, J. T. Kent, and J. M. Bibby, </author> <title> Multivariate analysis, </title> <publisher> Academic Press, </publisher> <year> 1979. </year>
Reference-contexts: It is common to take to be the identity matrix. Model (4) implies that the covariance of the observed variables is given by: C (G; ) = GG T + (5) Under the model, the sample covariance matrix (S) follows a Wishart distribution <ref> [19] </ref> about C and Everitt [18] introduces the function: F (S; C (G; )) = lnjCj + trace (SC 1 ) lnjSj n (6) which, up to some constant factors, is the likelihood. Maximum likelihood FA fits the parameters of the model G and by maximising equation (6).
Reference: [20] <author> R. M. Neal and Dayan P., </author> <title> "Factor analysis using delta-rule wake-sleep learning", </title> <type> Tech. Rep., No. 9607. </type> <institution> Dept. of Statistics, University of Toronto. </institution> <note> Available from ftp://ftp.cs.toronto.edu/pub/radford/ws-fa.ps.Z, </note> <year> 1996. </year>
Reference-contexts: This restriction seems 14 reasonable for images, since the axes defined by the input pixels are indeed privileged. Following analysis by Neal and Dayan <ref> [20] </ref> on the relationship between factor analysis, PCA, autoencoders and the Helmholtz machine [21], [22], we actually used an autoencoder to implement factor analysis.
Reference: [21] <author> G. E. Hinton, P. Dayan, B. J. Frey, and R. M. Neal, </author> <title> "The wake-sleep algorithm for unsupervised neural networks", </title> <journal> Science, </journal> <volume> vol. 268, </volume> <pages> pp. 1158-1161, </pages> <year> 1995. </year>
Reference-contexts: This restriction seems 14 reasonable for images, since the axes defined by the input pixels are indeed privileged. Following analysis by Neal and Dayan [20] on the relationship between factor analysis, PCA, autoencoders and the Helmholtz machine <ref> [21] </ref>, [22], we actually used an autoencoder to implement factor analysis.
Reference: [22] <author> P. Dayan, G. E. Hinton, R. M. Neal, and R. S. Zemel, </author> <title> "The Helmholtz machine", </title> <journal> Neural computation, </journal> <volume> vol. 7, no. 7, </volume> <pages> pp. 889-904, </pages> <year> 1995. </year>
Reference-contexts: This restriction seems 14 reasonable for images, since the axes defined by the input pixels are indeed privileged. Following analysis by Neal and Dayan [20] on the relationship between factor analysis, PCA, autoencoders and the Helmholtz machine [21], <ref> [22] </ref>, we actually used an autoencoder to implement factor analysis.
Reference: [23] <author> D.B. Rubin and D. T. Thayer, </author> <title> "EM algorithms for ML factor analysis", </title> <journal> Psychometrika, </journal> <volume> vol. 47, no. 1, </volume> <pages> pp. 69-76, </pages> <year> 1982. </year>
Reference-contexts: The linearity of the model implies that the posterior distribution of the factors h i given the input x i is Gaussian: h i ~ N (Rx i ; ) (7) As Rubin & Thayer <ref> [23] </ref> show in their discussion of the use of EM for FA, the correct values of R and are determined by G and as: R = (G T 1 G + I) 1 G T 1 Following the Helmholtz machine, we call R and parameters of the recognition model, since they
Reference: [24] <author> T. Hastie and R. Tibshirani, </author> <title> "Discriminant adaptive nearest neighbor clasification", </title> <journal> Accepted; IEEE Transactions Pattern Analysis and Machine Intellegince, </journal> <year> 1995. </year>
Reference-contexts: However, the metric that is used to judge proximity can make a substantial difference to the quality of the resulting inference. There are discriminative and maximum likelihood ways to look at this issue. For instance, Hastie and Tibshirani <ref> [24] </ref> choose a metric for the nearest neighbours at a point based on information from local linear discriminant analysis emphasizing directions in which the images from the different classes differ and downplaying directions in which they are similar.
Reference: [25] <author> H. Schwenk and M. Milgram, </author> <title> "Learning discriminant tangent models for handwritten character recognition", in ICANN*95. </title> <publisher> Springer Verlag, </publisher> <year> 1995. </year>
Reference-contexts: Note that these local surfaces are chosen to model the images of each digit as best as possible, and not to support the best possible discrimination between them. This local low-dimensional and linear behaviour is what motivated our linear models. Schwenk and Milgram [17], <ref> [25] </ref> take a slightly different approach and compile down all knowledge about a character into a single prototype trained to directly minimize the distance between the prototype and the tangent planes around each of the training examples. different examples of the same digit, subject to some smooth transformation.
Reference: [26] <author> J. J Hull, </author> <title> "A database for handwritten text recognition research", </title> <journal> IEEE Transactions Pattern Analysis and Machine Intellegince, </journal> <volume> vol. 16, no. 5, </volume> <pages> pp. 550-554, </pages> <year> 1994. </year>
Reference-contexts: belief is that all classes are equally likely then the Bayes decision rule stipulates that we should assign x to the class k fl where: k fl = argmax k P (xjk) We used images from the CEDAR CDROM 1 database of Cities, States, ZIP Codes, Digits, and Alphabetic Characters <ref> [26] </ref>. The br training set of binary segmented digits was subdivided into two sets of size 7,000 and 4,000 respectively. <p> Principal components that explained in excess of 95% were discarded to avoid overfitting. 7 This follows because the weight vectors are mutually orthogonal and so the the generative model inverse is simply its transpose. 21 its were manually removed <ref> [26] </ref>. The training data was also manually screened. It is therefore reasonable to conclude by comparing the validation and goodbs performances that the models did not overfit. 3.0 4.4 1.2 0.0 0.4 9.1 6.3 8.1 5.0 6.6 8 fi 8 image is shown at the bottom.
Reference: [27] <author> J. L. Fleiss, </author> <title> Statistical methods for rates and proportions, Second edition. </title> <publisher> Wiley, </publisher> <year> 1981. </year>
Reference-contexts: The performance of the different methods are presented in Table 1. There are no significant differences between the performances of the different methods at the p &lt; 0:05 level on the bs test set when compared pairwise using a two-tailed McNemar's test <ref> [27] </ref>. In comparing the columns of table 1, it is important to note that the goodbs is a carefully chosen subset of the bs test data when poorly segmented dig 6 This was usually sufficient to explain at least 95% of the training set variance assigned to that sub-model.
Reference: [28] <author> P. Simard, B. Victorri, Y. LeCun, and J. Denker, </author> <title> "A formalism for specifying selected invariances in an adaptive network", </title> <booktitle> in Advances in Neural Information Processing Systems 4, </booktitle> <editor> J.E. Moody, S.J. Hanson, and S. J. Lippmann, </editor> <booktitle> Eds., </booktitle> <pages> pp. 895-903. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year> <month> 29 </month>
Reference-contexts: We also found that the inclusion of tangent vectors did not substantially improve the performance. Our use of tangent vectors is essentially an instantiation of tangent-prop <ref> [28] </ref>, which constrains the output of the network to satisfy appropriate invariances through its directional derivatives. Since our networks are linear, these directional derivatives are particularly simple, allowing the tangent vectors just to be added into the covariance matrices.
References-found: 28

