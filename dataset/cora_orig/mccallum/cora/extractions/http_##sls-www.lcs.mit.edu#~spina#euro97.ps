URL: http://sls-www.lcs.mit.edu/~spina/euro97.ps
Refering-URL: http://sls-www.lcs.mit.edu/~spina/resume.html
Root-URL: 
Email: fspina,zueg@sls.lcs.mit.edu  
Title: AUTOMATIC TRANSCRIPTION OF GENERAL AUDIO DATA: EFFECT OF ENVIRONMENT SEGMENTATION ON PHONETIC RECOGNITION 1  
Author: Michelle S. Spina and Victor W. Zue 
Address: Cambridge, Massachusetts 02139 USA  
Affiliation: Spoken Language Systems Group Laboratory for Computer Science Massachusetts Institute of Technology  
Abstract: The task of automatically transcribing general audio data is very different from those usually confronted by current automatic speech recognition systems. The general goal of our work is to determine the optimal training strategy for recognizing such data. Specifically, we have studied the effects of different speaking environments on a phonetic recognition task using data collected from a radio news program. We found that if a single-recognizer is to be used, it is more effective to use a smaller amount of homogeneous, clean data for training. This approach yielded a decrease in phonetic recognition error rate of over 26% over a system trained with an equivalent amount of data which contained a variety of speaking environments. We found that additional gains can be made with a multiple-recognizer system, trained with environment-specific data. Overall, we found that this approach yielded a decrease in error rate of nearly 2%, with some individual speaking environments' error rate decreasing by over 7%. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anastasakos, T., Kubala, F., Makhoul, J., Schwartz, R. </author> <title> Adaptation to new microphones using tied-mixture normalization, </title> <booktitle> In Proceedings of ICASSP-94, </booktitle> <pages> pages 433-435, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: The overall phonetic recognition error rate was 48.2%. To reduce some of the channel differences between the training and testing data, we investigated using cepstral mean normalization <ref> [1] </ref> as a preprocessing technique. We found that the phonetic error rates were significantly reduced across all of the testing environments, as shown in the second row of Table 2. The resulting overall error rate for this experiment was 45.8%. 3.1.2.
Reference: [2] <author> Garofolo, J., Fiscus, J.G., Fisher, W.M. </author> <title> Design and Preparation of the 1996 Hub-4 Broadcast News Benchmark Test Corpora, </title> <booktitle> In Proceedings of DARPA Speech Recognition Workshop, </booktitle> <month> February </month> <year> 1997. </year>
Reference-contexts: More recently, ASR research has broadened its scope to include the transcription of general audio data (GAD), from sources such as radio or television <ref> [2] </ref>.
Reference: [3] <author> Garofolo, J., Lamel, L., Fisher, W., Fiscus, J., Pallet, D., and Dahlgran, N. </author> <title> The DARPA TIMIT acoustic-phonetic continuous speech corpus CDROM. NTIS order number PB91-505065, </title> <month> October </month> <year> 1990. </year>
Reference-contexts: Environment Training Data Testing Data Clean Speech 151.3 24.9 Music Speech 31.1 8.0 Noisy Speech 30.1 15.8 Field Speech 42.6 3.9 Table 1: Amount of training and testing data available (in minutes) for each speaking environment. Acoustic models were built using the TIMIT <ref> [3] </ref> 61 label set. Results, expressed as phonetic recognition error rates, are collapsed down to the 39 labels typically used by others to report recognition results. The SUMMIT [5] speech recognizer developed by our group was used for these experiments. SUMMIT uses a segment-based framework for its acoustic-phonetic representation.
Reference: [4] <author> Gauvain, J.L., Lamel, L., Adda-Decker, M. </author> <title> Acoustic Modelling in the LIMSI Nov96 Hub4 System, </title> <booktitle> In Proceedings of DARPA Speech Recognition Workshop, </booktitle> <month> February </month> <year> 1997. </year>
Reference-contexts: In [8], Schwartz et al. found that environment-specific training did not prove to be beneficial for an automatic speech recognition task. They determined that gains made from general adaptation techniques applied during both training and testing were significantly larger and resulted in a simpler overall system. However, others <ref> [4] </ref> have found that environment-specific training did result in significant performance gains on the same task. It is therefore unclear how best to handle data with varying acoustic conditions.
Reference: [5] <author> Glass, J., Chang, J., and McCandless, M. </author> <title> A probabilistic framework for feature-based speech recognition, </title> <booktitle> In Proceedings of ICSLP-96, </booktitle> <address> Philadelphia, PA, </address> <month> October </month> <year> 1996. </year>
Reference-contexts: Acoustic models were built using the TIMIT [3] 61 label set. Results, expressed as phonetic recognition error rates, are collapsed down to the 39 labels typically used by others to report recognition results. The SUMMIT <ref> [5] </ref> speech recognizer developed by our group was used for these experiments. SUMMIT uses a segment-based framework for its acoustic-phonetic representation. The feature vector for each segment consisted of MFCC and energy averages over segment thirds as well as two derivatives computed at segment boundaries. Segment duration was also included.
Reference: [6] <author> James, David Anthony. </author> <title> The Application of Classical Information Retrieval Techniques to Spoken Documents. </title> <type> PhD thesis, </type> <institution> Univ. of Cambridge, </institution> <month> February </month> <year> 1995. </year>
Reference-contexts: More recently, ASR research has broadened its scope to include the transcription of general audio data (GAD), from sources such as radio or television [2]. This shift in research focus is largely brought on by the growing need to shift content-based information retrieval from text to speech <ref> [6] </ref>, so that the computer can satisfy requests such as, Play me the speech by President Kennedy in which he said, `Ich bin ein Berliner.' GAD pose new challenges to present-day ASR technology because they often contain extemporaneously-generated, and therefore disfluent speech, with words drawn from a very large vocabulary, and
Reference: [7] <author> Lippmann, R., Martin, E., and Paul, D. </author> <title> Multi-style training for robust isolated-word speech recognition, </title> <booktitle> In Proceedings of ICASSP-87, </booktitle> <pages> pages 4.1-4, </pages> <year> 1987. </year>
Reference-contexts: Multi-Style Training Multi-style training <ref> [7] </ref> incorporates different environment conditions in the training data, thereby reducing the potential mismatch between training and testing conditions. In addition to having a more diverse training set, we are able to utilize a large amount of data to train our acoustic models.
Reference: [8] <author> Schwartz, R., Jin, H., Kubala, F., Matsoukas, S. </author> <title> Modeling those F-conditions Or not, </title> <booktitle> In Proceedings of DARPA Speech Recognition Workshop, </booktitle> <month> February </month> <year> 1997. </year>
Reference-contexts: Such preprocessing would enable the transcription system to utilize the appropriate acoustic models and perhaps even to limit its active vocabulary. Other researchers have investigated environment-specific techniques for acoustic training with varying results. In <ref> [8] </ref>, Schwartz et al. found that environment-specific training did not prove to be beneficial for an automatic speech recognition task. They determined that gains made from general adaptation techniques applied during both training and testing were significantly larger and resulted in a simpler overall system.
Reference: [9] <author> Spina, M.S., and Zue, V.W. </author> <title> Automatic Transcription of General Audio Data: Preliminary Analysis, </title> <booktitle> In Proceedings of ICSLP-96, </booktitle> <address> Philadelphia, PA, </address> <month> October </month> <year> 1996. </year>
Reference-contexts: M. Spina also receives support from Intel Corporation. nel, we have argued that the transcription of GAD would benefit from a preprocessing step that first segmented the signal into acoustically homogeneous chunks <ref> [9] </ref>. Such preprocessing would enable the transcription system to utilize the appropriate acoustic models and perhaps even to limit its active vocabulary. Other researchers have investigated environment-specific techniques for acoustic training with varying results. <p> Silence and field speech are visually distinct from other classes both in terms of energy and spectral shape. Music differs from speech in its fine harmonic structure. Differences in the average spectra of the other three speech categories are more subtle. In <ref> [9] </ref>, we described some preliminary analyses and experiments that we had conducted concerning the transcription of this data. For the NPR-ME corpus, we were able to achieve better than 80% classification accuracy for these seven sound classes on unseen data, using relatively straightforward acoustic measurements and pattern classification techniques. <p> One possible explanation for this result is that many of the noisy speech utterances contain very low levels of background noise and this better fit the clean speech models. This is supported by our early results <ref> [9] </ref> showing that 53.5% of the noisy speech was misclassified as clean speech, indicating that perhaps these utterances should have been considered clean speech. <p> Again, the use of clean speech to develop the acoustic models outperformed the use of multi-style data. 3.2.3. Integrated System The experiments described thus far have assumed that test utterances have been classified perfectly. Our final experiment used the sound classification system described in <ref> [9] </ref> as a preprocessor to classify each test utterance as one of Testing Data Training Data Field Speech Clean Speech 53.3 Music Speech 65.2 Noisy Speech 62.4 Field Speech 59.7 Multi-Style 53.7 Table 4: Summary of field speech phonetic recognition error rates for bandlimited training system. the four speech environments.
Reference: [10] <author> Viterbi, A. </author> <title> Error Bounds for Convolutional Codes and an Asymptotic Optimal Decoding Algorithm, </title> <journal> In IEEE Trans. on Information Theory, </journal> <volume> 13 </volume> <pages> 260-269, </pages> <month> April </month> <year> 1967. </year>
Reference-contexts: Therefore, each file was homogeneous with respect to sound environment. Orthographies and phonetic alignments were generated for each of the files using the orthographic transcriptions and a forced Viterbi search <ref> [10] </ref>. Seven categories were used to characterize the files.
References-found: 10

