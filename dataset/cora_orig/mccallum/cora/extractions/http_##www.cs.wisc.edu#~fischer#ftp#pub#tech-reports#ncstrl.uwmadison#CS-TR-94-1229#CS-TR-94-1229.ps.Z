URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-94-1229/CS-TR-94-1229.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-94-1229/
Root-URL: http://www.cs.wisc.edu
Title: An Evaluation of Directory Protocols for Medium-Scale Shared-Memory Multiprocessors  
Author: Shubhendu S. Mukherjee and Mark D. Hill 
Keyword: Shared-memory multiprocessors, cache coherence, directory protocols, and gray code.  
Address: 1210 West Dayton Street Madison, WI 53706 USA  
Affiliation: Computer Sciences Department University of Wisconsin-Madison  
Note: To appear: Proceedings of the 8th ACM International Conference on Supercomputing,  
Email: shubu@cs.wisc.edu  
Date: July 1994.  
Abstract: This paper considers alternative directory protocols for providing cache coherence in shared-memory multiprocessors with 32 to 128 processors, where the state requirements of Dir N may be considered too large. We consider Dir i B, i = 1; 2; 4, Dir N , Tristate (also called superset), Coarse Vector, and three new protocols. The new protocols|Gray-hardware, Gray-software, Home|are optimizations of Tristate that use gray coding to favor near-neighbor sharing. Our results are the first to compare all these protocols with complete applications (and the first evaluation of Tristate with a non-synthetic workload). Results for three applications|ocean (one-dimensional sharing), appbt (three-dimensional sharing), and barnes (dynamic sharing)|for 128 processors on the Wisconsin Wind Tunnel show that (a) Dir 1 B sends 15 to 43 times as many invalidation messages as Dir N , (b) Gray-software sends 1.0 to 4.7 times as many messages as Dir N , making it better than Tristate, Gray-hardware, and Home, and (c) the choice between Dir i B, Coarse Vector, and Gray-software depends on whether one wants to optimize for few sharers (Dir i B), many sharers (Coarse Vector), or hedge one's bets between both alternatives (Gray-software). 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anant Agarwal, Richard Simoni, Mark Horowitz, and John Hennessy. </author> <title> An Evaluation of Directory Schemes for Cache Coherence. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 280-289, </pages> <year> 1988. </year>
Reference-contexts: When one writable cache copy is outstanding, the sharing code identifies which processor has the block using at least logN bits. 2 Alternative sharing codes have been proposed for identifying the sharers when multiple read-only copies are outstanding. At one extreme is Dir 1 B <ref> [1] </ref> which sends N invalidation messages at each invalidation event with more than one sharer. If there were actually j sharers, j &gt; 1, then N j of the N messages are unnecessary invalidation messages. <p> At the other extreme is Dir N that uses a bit vector to exactly identify the sharers <ref> [1] </ref>. <p> Several proposals also exist that use a smaller sharing code than Dir N , but do not always fall back on broadcast. We call these proposals multicast protocols; others have called them limited broadcast protocols <ref> [1] </ref>. The challenge of designing a multicast protocol lies in minimizing both the sharing code size and the number of unnecessary invalidation messages. In this paper we will study variants of three previously proposed multicast protocols|Dir i B, Tristate, and Coarse Vector. <p> In this paper we will study variants of three previously proposed multicast protocols|Dir i B, Tristate, and Coarse Vector. Dir i B, 1 &lt; i &lt; N , uses i fi logN bits to exactly identify upto i sharers and broadcasts otherwise <ref> [1] </ref>. Coarse Vector uses N=K bits, where a bit is set if any of the processors in a K-processor group cached the block [9]. Tristate [1], also called the superset scheme by Gupta et al. [9], uses a logN digit code requiring 2 bits per digit. <p> Dir i B, 1 &lt; i &lt; N , uses i fi logN bits to exactly identify upto i sharers and broadcasts otherwise <ref> [1] </ref>. Coarse Vector uses N=K bits, where a bit is set if any of the processors in a K-processor group cached the block [9]. Tristate [1], also called the superset scheme by Gupta et al. [9], uses a logN digit code requiring 2 bits per digit. <p> In particular, Tristate has not been evaluated with real benchmarks. Previous studies of directory protocol performance were limited by systems with smaller number of processors|between 4 and 64. Agarwal et al. <ref> [1] </ref> evaluated directory protocols for a small bus-based 3 system using four-processor VAX traces less than two million instructions long. In the same paper, they proposed Tristate without evaluating it.
Reference: [2] <author> J. Archibald and J.-L. Baer. </author> <title> Cache Coherence Protocols: Evaluation Using a Multiprocessor Simulation Model. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 4(4) </volume> <pages> 273-298, </pages> <year> 1986. </year>
Reference-contexts: For these reasons, we assume cache-coherent shared memory in this paper. Many protocols have been proposed for implementing cache coherence. We assume that medium-scale computers are too large to rely on snooping a shared bus <ref> [2] </ref> but small enough that they need not be concerned about asymptotic scalability [10, 12]. A reasonable structure for medium-scale computers is to associate a directory with the memory module in each of the N processor-memory nodes.
Reference: [3] <author> David Bailey, John Barton, Thomas Lasinski, and Horst Si-mon. </author> <title> The NAS Parallel Benchmarks. </title> <type> Report RNR-91-002 Revision 2, </type> <institution> Ames Research Center, </institution> <month> August </month> <year> 1991. </year>
Reference-contexts: This section discusses the benchmarks used in this paper, the platform to perform evaluations, the parallel system assumptions, and the metric for comparing the different protocols. 3.1 Benchmarks The three benchmarks used in this paper are ocean and barnes from the SPLASH suite [16], and appbt, a NAS serial benchmark <ref> [3] </ref> that was paral-lelized by our group. We limited ourselves to three codes|selected as having one-dimensional, three-dimensional, and dynamic sharing|to allow us to focus on qualitative trends and to reduce simulation time. Table 2 summarizes the programs. <p> The number of bits used by Gray-software is fixed at 14 bits. 6 Acknowledgements Members of the Wisconsin Wind Tunnel group provided invaluable support with the Wind Tunnel, CM-5, and the benchmarks. Singh et al. [16] wrote and distributed the SPLASH benchmarks. Bailey et al. <ref> [3] </ref> wrote and distributed the sequential version of the NAS benchmarks. Eric Bach and Suresh Chalasani helped with the literature on mesh-to-hypercube mapping. Doug Burger and David Wood helped develop the initial ideas in this paper.
Reference: [4] <author> Said Bettayeb, Zevi Miller, and I. Hal Sudborough. </author> <title> Embedding Grids in Hypercubes. </title> <journal> Journal of Computer and System Sciences, </journal> (45):340-366, 1992. 
Reference-contexts: Three-dimensional meshes can be embedded with dilations of one (two dimensions are powers of two), two (one dimension is a power of two and using Chan's construction for the other two), three (in many cases <ref> [4] </ref>), and never worse than seven ([8]).
Reference: [5] <author> David Chaiken, John Kubiatowics, and Anant Agarwal. </author> <title> LimitLESS Directories: A Scalable Cache Coherence Scheme. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV), </booktitle> <pages> pages 224-234, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: In the same paper, they proposed Tristate without evaluating it. The MIT Alewife machine uses a Dir N -like protocol, called LimitLESS, which maintains five of the pointers in hardware and the rest in software. Chaiken et al. <ref> [5] </ref> compared LimitLESS against Dir N , using several applications on 16 and 64 processors with 7 to 30 million references per application. They found that LimitLESS's performance is comparable to Dir N . <p> The scope of any experimental study is finite. Our study compares directory protocols that have very similar implementations. Specifically, we examined implementations that differ primarily in how the sharing code is encoded. We chose to exclude protocols that use traps <ref> [5, 11] </ref>, distributed directories [10], directory caching [9], and several other optimizations [13, 14], because setting the plethora of implementation assumptions needed for these alternatives would have compromized the generality of our study.
Reference: [6] <author> David Lars Chaiken. </author> <title> Cache Coherence Protocols for Large-Scale Multiprocessors. </title> <type> Technical Report MIT/LCS/TR-489, </type> <institution> MIT Laboratory for Computer Science, </institution> <month> September </month> <year> 1990. </year>
Reference-contexts: Coarse Vector will, however, send only 4fi K messages, because only four separate bits in the sharing code will be set. y These protocols use an additional logN bits for a counter, since we assume a notifying protocol <ref> [6] </ref> where only positive acknowl edgements are returned on invalidation requests. forms best of four with the same hardware as Tris tate . For ocean, barnes, and appbt, respectively, Gray-software sends 1.0, 1.3, and 4.7 times as many invalidation messages as Dir N . <p> Network topology is ignored and all messages are assumed a fixed latency of 100 processor cycles. Finally, our protocol implementations assume that a directory entry logically keeps a count of the outstanding copies of a block, a processor always notifies the directory when it replaces a block (called notifying <ref> [6] </ref>), and only positive acknowledge-ments are collected at an invalidation event. 3.4 Evaluation Metric The ultimate measure of performance is total program execution time. The Wisconsin Wind Tunnel allows us to calculate total program execution time for a 100-cycle network that ignores contention.
Reference: [7] <author> Mee-Yee Chan. </author> <title> Dilation-2 Embeddings of Grids Into Hyper-cubes. </title> <booktitle> In Proceedings of the 1988 International Conference on Parallel Processing (Vol. III), </booktitle> <pages> pages 295-298, </pages> <year> 1988. </year>
Reference-contexts: Given a d-dimensional mesh and the smallest hypercube with as many nodes as there are vertices in the mesh, what is the best possible mapping of vertices of the mesh to the nodes of the hypercube such that neighbors in the mesh are as close to each other as possible <ref> [7] </ref>. Define dilation as the maximum distance between any two mesh neighbors on the hypercube. Alternately, dilation can also be defined as the maximum number of bit positions in which any two mesh neighbors differ when mapped to the hypercube. <p> The problem can be solved optimally|with dilation 1|if at least d 1 dimensions of a d-dimensional mesh are powers of two. This case occurs commonly in parallel applications, because programmers size their data to fit the machine they run on. For the important case of two-dimensional meshes, Chan <ref> [7] </ref> shows how to automatically construct an 3 Without this wrap-around, the series sums to logN 1. 4 embedding with dilation one or two. Consider a 3 x 5 mesh to be embedded in its smallest hypercube with 16 nodes.
Reference: [8] <author> Mee-Yee Chan. </author> <title> Embedding of d-Dimensional Grids Into Optimal Hyerpcubes. </title> <booktitle> In Proceedings of the First ACM Symposium on Parallel Algorithms and Architectures (SPAA), </booktitle> <pages> pages 52-56, </pages> <year> 1989. </year>
Reference-contexts: embedded with dilations of one (two dimensions are powers of two), two (one dimension is a power of two and using Chan's construction for the other two), three (in many cases [4]), and never worse than seven (<ref> [8] </ref>). Rarely-used higher dimensional meshes can always be embedded with dilation O (dimension) [8]. 2.2 New Protocols Here we discuss specific implementation issues related to the three new protocols|Gray-hardware, Gray-software, and Home. 2.2.1 Gray-hardware code and its inverse This code shows how to do the gray coding in one dimension. The text explains how to do multi-dimensional gray coding.
Reference: [9] <author> Anoop Gupta, Wolf-Dietrich Weber, and Todd Mowry. </author> <title> Reducing Memory and Traffic Requirements for Scalable Directory-Based Cache Coherence Schemes. </title> <booktitle> In Proceedings of the 1990 International Conference on Parallel Processing (Vol. I Architecture), </booktitle> <pages> pages 312-321, </pages> <year> 1990. </year>
Reference-contexts: Dir i B, 1 &lt; i &lt; N , uses i fi logN bits to exactly identify upto i sharers and broadcasts otherwise [1]. Coarse Vector uses N=K bits, where a bit is set if any of the processors in a K-processor group cached the block <ref> [9] </ref>. Tristate [1], also called the superset scheme by Gupta et al. [9], uses a logN digit code requiring 2 bits per digit. <p> Coarse Vector uses N=K bits, where a bit is set if any of the processors in a K-processor group cached the block <ref> [9] </ref>. Tristate [1], also called the superset scheme by Gupta et al. [9], uses a logN digit code requiring 2 bits per digit. The j-th digit of the code is 0 if the j-th bit of all sharers is 0; the digit is 1 if all sharers have 1; the digit is both otherwise. <p> Chaiken et al. [5] compared LimitLESS against Dir N , using several applications on 16 and 64 processors with 7 to 30 million references per application. They found that LimitLESS's performance is comparable to Dir N . Gupta et al. <ref> [9] </ref> compared Coarse Vector with Tristate using a synthetic benchmark, which randomly picked the processors sharing a block, and concluded that Coarse Vector is superior to Tristate, which is contradicted by our results based on three non-synthetic benchmarks. <p> Tristate exploits much of this opportunity by sending 2.7, 1.9, and 5 times as many invalidation message as Dir N . It appears Tristate performs better here than it did for Gupta et al. <ref> [9] </ref>, because sharing in our benchmarks was less random than in their synthetic one. Of the closely-related protocols of Tristate, Gray-hardware, Gray-software, and Home, we recommend Gray-software. <p> The scope of any experimental study is finite. Our study compares directory protocols that have very similar implementations. Specifically, we examined implementations that differ primarily in how the sharing code is encoded. We chose to exclude protocols that use traps [5, 11], distributed directories [10], directory caching <ref> [9] </ref>, and several other optimizations [13, 14], because setting the plethora of implementation assumptions needed for these alternatives would have compromized the generality of our study. We did not examine Dir i NB because it performs poorly without a special mechanism for handling read-only data [17].
Reference: [10] <author> David B. Gustavson. </author> <title> The Scalable Coherent Interface and Related Standards Projects. </title> <journal> IEEE Micro, </journal> <volume> 12(2) </volume> <pages> 10-22, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: For these reasons, we assume cache-coherent shared memory in this paper. Many protocols have been proposed for implementing cache coherence. We assume that medium-scale computers are too large to rely on snooping a shared bus [2] but small enough that they need not be concerned about asymptotic scalability <ref> [10, 12] </ref>. A reasonable structure for medium-scale computers is to associate a directory with the memory module in each of the N processor-memory nodes. <p> The scope of any experimental study is finite. Our study compares directory protocols that have very similar implementations. Specifically, we examined implementations that differ primarily in how the sharing code is encoded. We chose to exclude protocols that use traps [5, 11], distributed directories <ref> [10] </ref>, directory caching [9], and several other optimizations [13, 14], because setting the plethora of implementation assumptions needed for these alternatives would have compromized the generality of our study. We did not examine Dir i NB because it performs poorly without a special mechanism for handling read-only data [17].
Reference: [11] <author> Mark D. Hill, James R. Larus, Steven K. Reinhardt, and David A. Wood. </author> <title> Cooperative Shared Memory: Software and Hardware for Scalable Multiprocessors. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS V), </booktitle> <pages> pages 262-273, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: The scope of any experimental study is finite. Our study compares directory protocols that have very similar implementations. Specifically, we examined implementations that differ primarily in how the sharing code is encoded. We chose to exclude protocols that use traps <ref> [5, 11] </ref>, distributed directories [10], directory caching [9], and several other optimizations [13, 14], because setting the plethora of implementation assumptions needed for these alternatives would have compromized the generality of our study.
Reference: [12] <institution> Kendall Square Research. Kendall Square Research Technical Summary, </institution> <year> 1992. </year>
Reference-contexts: For these reasons, we assume cache-coherent shared memory in this paper. Many protocols have been proposed for implementing cache coherence. We assume that medium-scale computers are too large to rely on snooping a shared bus [2] but small enough that they need not be concerned about asymptotic scalability <ref> [10, 12] </ref>. A reasonable structure for medium-scale computers is to associate a directory with the memory module in each of the N processor-memory nodes.
Reference: [13] <author> Wisam Michael. </author> <title> A Scalable Coherent Cache System With </title>
Reference-contexts: Our study compares directory protocols that have very similar implementations. Specifically, we examined implementations that differ primarily in how the sharing code is encoded. We chose to exclude protocols that use traps [5, 11], distributed directories [10], directory caching [9], and several other optimizations <ref> [13, 14] </ref>, because setting the plethora of implementation assumptions needed for these alternatives would have compromized the generality of our study. We did not examine Dir i NB because it performs poorly without a special mechanism for handling read-only data [17].
References-found: 13

