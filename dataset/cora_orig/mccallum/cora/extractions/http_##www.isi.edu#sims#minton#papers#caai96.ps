URL: http://www.isi.edu/sims/minton/papers/caai96.ps
Refering-URL: http://www.isi.edu/sims/minton/homepage.html
Root-URL: http://www.isi.edu
Email: allen@ptolemy.arc.nasa.gov  minton@isi.edu  
Phone: 2  
Title: Selecting the Right Heuristic Algorithm: Runtime Performance Predictors  
Author: John A. Allen and Steven Minton 
Address: Moffett Field, CA 94035-1000,  4676 Admiralty Way, Marina del Rey, CA 90292-6695,  
Affiliation: 1 Caelum Research Corporation, Mail-Stop 269-2, NASA Ames Research Center,  USC Information Sciences Institute,  
Abstract: It is obvious that, given a problem instance, some heuristic algorithms can perform vastly better than others; however, in most cases the existing literature provides little guidance for choosing the best heuristic algorithm. This paper describes how runtime performance predictors can be used to identify a good algorithm for a particular problem instance. The approach is demonstrated on two families of heuristic al gorithms.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> P. Cheeseman, B. Kanefsky, and W. Taylor. </author> <title> Where the really hard problems are. </title> <booktitle> In Proceedings of the 12 th IJCAI, </booktitle> <pages> pages 331-337, </pages> <address> Sydney, Australia, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: We demonstrate this idea for two very different families of heuristic algorithms: backtracking constraint-propagation and iterative repair tabu search. 2 Finding the Right Heuristic Algorithm is Important Recently, researchers have been interested in identifying "hard" constraint satisfaction problems (e.g., <ref> [1, 10] </ref>). Informally, an problem is hard if no algorithm can solve "most" of its instances "quickly", for suitable definitions of "most" and "quickly". Similarly, a CSP problem is trivial if most CSP algorithms can solve most of its instances quickly.
Reference: 2. <author> F. Glover. </author> <title> Tabu search part I. </title> <journal> ORSA Journal on Computing, </journal> <volume> 1(3) </volume> <pages> 190-206, </pages> <year> 1989. </year>
Reference-contexts: GSAT requires a tabu list. (Selman used a tabu list of length 20. A tabu list of length k keeps track of the last k moves, so they are not repeated by the repair process <ref> [2, 3] </ref>. This can help the algorithm cope with local minima and plateaus.) 3. GSAT's variable-ordering strategy helps significantly. Armed with this knowledge, we were able to create a variation of min-conflicts that does almost as well as GSAT on the graph-coloring instances we tried. <p> For instance, value ordering heuristics do not work this way, and therefore another predictor must be used to gauge whether a value-ordering heuristic is having a positive effect. 4.2 Minimum Conflict Count For our second family of heuristic methods, we consider the use of tabu <ref> [2, 3] </ref> lists by GSAT. As explained in the previous section, tabu lists are used to record the last k moves, so that they will not be repeated.
Reference: 3. <author> F. Glover. </author> <title> Tabu search part II. </title> <journal> ORSA Journal on Computing, </journal> <volume> 2 </volume> <pages> 4-32, </pages> <year> 1990. </year>
Reference-contexts: GSAT requires a tabu list. (Selman used a tabu list of length 20. A tabu list of length k keeps track of the last k moves, so they are not repeated by the repair process <ref> [2, 3] </ref>. This can help the algorithm cope with local minima and plateaus.) 3. GSAT's variable-ordering strategy helps significantly. Armed with this knowledge, we were able to create a variation of min-conflicts that does almost as well as GSAT on the graph-coloring instances we tried. <p> For instance, value ordering heuristics do not work this way, and therefore another predictor must be used to gauge whether a value-ordering heuristic is having a positive effect. 4.2 Minimum Conflict Count For our second family of heuristic methods, we consider the use of tabu <ref> [2, 3] </ref> lists by GSAT. As explained in the previous section, tabu lists are used to record the last k moves, so that they will not be repeated.
Reference: 4. <author> M. Johnston and S. Minton. </author> <title> Analyzing a heuristic strategy for constraint-satisfaction and scheduling. </title> <editor> In M. Zweben and M. Fox, editors, </editor> <booktitle> Intelligent Scheduling, </booktitle> <pages> pages 257-290. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: While N-queens is not intrinsically interesting, it is notable because it has long served as a benchmark for AI search methods. Researchers <ref> [11, 4, 16] </ref> have recently identified heuristic techniques that quickly solve a job shop problem originally proposed by Sadeh [15]. For instance, Johnston found that a combination of a dispatch heuristic and iterative repair search solved most of the instances almost immediately.
Reference: 5. <author> L. Kale. </author> <title> An almost perfect heuristic for the n nonattacking queens problem. </title> <journal> Inf. Process. Lett., </journal> <volume> 34 </volume> <pages> 173-178, </pages> <year> 1990. </year>
Reference-contexts: By making heavy use of constraint propagation techniques, Smith's algorithm completely eliminated search in some cases. Minton et al. [9] have demonstrated that an iterative repair method solves instances of the N-queens problem easily, even if n = 10 6 , and Kale <ref> [5] </ref> found a backtrack version that works well as well. While N-queens is not intrinsically interesting, it is notable because it has long served as a benchmark for AI search methods.
Reference: 6. <author> D. Knuth. </author> <title> Estimating the efficiency of backtrack programs. </title> <journal> Mathematics of Computation, </journal> <volume> 29 </volume> <pages> 121-136, </pages> <year> 1975. </year>
Reference-contexts: This gives us an estimate of the total number of constraint checks that would be required if the entire tree were to be searched, which we refer to as EstT CC. The procedure is based on an iterative sampling approach described by Knuth <ref> [6] </ref> for estimating the size of the search tree, which we adapt for use with depth-first search, as described below. Let CC (n) be the number of constraint checks that occur at node n, and k (n) be the branching factor at node n. <p> The expected number of constraint checks for interleaving four algorithms is calculated as four times the number of constraint checks of the best algorithm. 6 Knuth notes this as well <ref> [6] </ref>. Prob. Inst.
Reference: 7. <author> V. Kumar. </author> <title> Algorithms for constraint-satisfaction problems: a survey. </title> <journal> AI Magazine, </journal> <volume> 13(1) </volume> <pages> 32-44, </pages> <year> 1992. </year>
Reference-contexts: For illustrative purposes, we chose two very different types of algorithms for our demonstration: constraint-propagation methods in a backtracking framework and tabu search in an iterative repair framework. 4.1 Estimated Total Constraint Checks Constraint-propagation (CP) <ref> [12, 7] </ref> is frequently used in conjunction with backtracking search. The idea is simple: whenever a variable is instantiated with a new value, we can prune the domains (the possible values) of the uninstantiated variables using consistency maintenance techniques as follows.
Reference: 8. <author> S. Minton. </author> <title> Integrating heuristics for constraint satisfaction problems: A case study. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <address> San Jose, CA, 1993. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: The difficulty is aggravated when an algorithm incorporates a variety of heuristic mechanisms, as is often the case, since these can interact in unexpected ways. For example, Minton <ref> [8] </ref> found that different backtracking algorithms were appropriate for two distributions of a CSP called Minimal Maximum Matching. The two backtracking algorithms used different variable and value ordering strategies.
Reference: 9. <author> S. Minton, M. Johnston, A. Philips, and P. Laird. </author> <title> Minimizing conficts: a heuristic repair method for constraint satisfaction and scheduling problems. </title> <journal> Artificial Intelligence, </journal> <volume> 58:161 - 205, </volume> <year> 1992. </year>
Reference-contexts: By making heavy use of constraint propagation techniques, Smith's algorithm completely eliminated search in some cases. Minton et al. <ref> [9] </ref> have demonstrated that an iterative repair method solves instances of the N-queens problem easily, even if n = 10 6 , and Kale [5] found a backtrack version that works well as well. <p> The trouble with using gross characteristics is that in many cases we have found that seemingly small differences in an algorithm, or the representation of a problem, can make a big difference in performance. As an example, we describe our recent experience comparing min-conflicts <ref> [9] </ref> and GSAT [17], two iterative-repair algorithms. Min-conflicts begins with an initial assignment. On each iteration, min-conflicts randomly chooses a variable that is in conflict and assigns it the value that minimizes the number of remaining conflicts. GSAT is a descendant of min-conflicts designed for boolean satisfiability problems. <p> This result surprised us, since Minton et al. <ref> [9] </ref> reports that min-conflicts performs relatively poorly on graph-coloring problems! We recently took a look at this discrepancy and were surprised to see that the differences in the algorithms, which are seemingly minor, result in significant differences in performance.
Reference: 10. <author> D. Mitchell, B. Selman, and H. Levesque. </author> <title> Hard and easy distributions of SAT problems. </title> <booktitle> In Proceedings of AAAI-92, </booktitle> <pages> pages 459-465, </pages> <address> San Jose, CA, 1991. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: We demonstrate this idea for two very different families of heuristic algorithms: backtracking constraint-propagation and iterative repair tabu search. 2 Finding the Right Heuristic Algorithm is Important Recently, researchers have been interested in identifying "hard" constraint satisfaction problems (e.g., <ref> [1, 10] </ref>). Informally, an problem is hard if no algorithm can solve "most" of its instances "quickly", for suitable definitions of "most" and "quickly". Similarly, a CSP problem is trivial if most CSP algorithms can solve most of its instances quickly.
Reference: 11. <author> N. Muscettola. HSTS: </author> <title> Integrating planning and scheduling. </title> <editor> In M. Zweben and M. Fox, editors, </editor> <booktitle> Intelligent Scheduling, </booktitle> <pages> pages 169-212. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: While N-queens is not intrinsically interesting, it is notable because it has long served as a benchmark for AI search methods. Researchers <ref> [11, 4, 16] </ref> have recently identified heuristic techniques that quickly solve a job shop problem originally proposed by Sadeh [15]. For instance, Johnston found that a combination of a dispatch heuristic and iterative repair search solved most of the instances almost immediately.
Reference: 12. <author> B. </author> <title> Nadel. Tree search and arc consistency in constraint satisfaction algorithms. </title> <editor> In L. Kanal and V. Kumar, editors, </editor> <booktitle> Search in Artificial Intelligence, </booktitle> <pages> pages 287-342. </pages> <publisher> Springer-Verlag, </publisher> <year> 1988. </year>
Reference-contexts: For illustrative purposes, we chose two very different types of algorithms for our demonstration: constraint-propagation methods in a backtracking framework and tabu search in an iterative repair framework. 4.1 Estimated Total Constraint Checks Constraint-propagation (CP) <ref> [12, 7] </ref> is frequently used in conjunction with backtracking search. The idea is simple: whenever a variable is instantiated with a new value, we can prune the domains (the possible values) of the uninstantiated variables using consistency maintenance techniques as follows. <p> The procedure "revise" is used to enforce arc consistency: applying revise (V; U ) removes values from the domain of V until arc (V; U ) is consistent. Nadel <ref> [12] </ref> describes the following CP heuristics, where U is the variable just instantiated, and V 1 ; : : : ; V n are the uninstantiated variables: 1.
Reference: 13. <author> V. Rao and V. Kumar. </author> <title> Superlinear speedup in state-space search. </title> <booktitle> In Conference on foundations of softwar technology and theoretical computer science, </booktitle> <year> 1988. </year>
Reference-contexts: One possible solution is to run a variety of algorithms in parallel. This can lead to a significant improvement in performance, e.g., super-linear speedup <ref> [13] </ref>, but typically resource limitations prevent us from running more than a few algorithms in parallel. Here we consider an approach, runtime performance prediction (RP P ), whereby we monitor an algorithm as it runs for a short period and predict its performance.
Reference: 14. <author> D. Sabin and E. Freuder. </author> <title> Constradicting conventional wisdom in constraint satisfaction. </title> <editor> In A. Borning, editor, </editor> <booktitle> Proceedings of the 1994 Workshop on Principles and Practice of Constraint Programming, </booktitle> <address> Orcas Island, Washington, 1994. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: In this sense, each successive CP heuristic is more powerful than the preceding one. However, more powerful does not necessarily result in better overall performance. In fact, recently Sabin and Freuder <ref> [14] </ref> have shown that the relative performance of these techniques is problem-dependent. There is a simple model that allows us to understand the basic effects of CP. Simply put, CP techniques increase efficiency by reducing the amount of backtracking required to find a solution.
Reference: 15. <author> N. Sadeh. </author> <title> Look-ahead techniques for micro-opportunist job shop scheduling. </title> <type> Technical Report CMU-CS-91-102, </type> <institution> School of Computer Science, Carnegie Mellon, </institution> <year> 1991. </year>
Reference-contexts: While N-queens is not intrinsically interesting, it is notable because it has long served as a benchmark for AI search methods. Researchers [11, 4, 16] have recently identified heuristic techniques that quickly solve a job shop problem originally proposed by Sadeh <ref> [15] </ref>. For instance, Johnston found that a combination of a dispatch heuristic and iterative repair search solved most of the instances almost immediately.
Reference: 16. <author> N. Sadeh. </author> <title> Micro-opportunistic scheduling: the micro-boss factory scheduler. </title> <editor> In M. Zweben and M. Fox, editors, </editor> <booktitle> Intelligent Scheduling, </booktitle> <pages> pages 99-135. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: While N-queens is not intrinsically interesting, it is notable because it has long served as a benchmark for AI search methods. Researchers <ref> [11, 4, 16] </ref> have recently identified heuristic techniques that quickly solve a job shop problem originally proposed by Sadeh [15]. For instance, Johnston found that a combination of a dispatch heuristic and iterative repair search solved most of the instances almost immediately.
Reference: 17. <author> B. Selman, H. Levesque, and D. Mitchell. </author> <title> A new method for solving hard satisfi-ability problems. </title> <booktitle> In Proceedings of AAAI-92, </booktitle> <pages> pages 440-446, </pages> <address> San Jose, CA, 1992. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: The trouble with using gross characteristics is that in many cases we have found that seemingly small differences in an algorithm, or the representation of a problem, can make a big difference in performance. As an example, we describe our recent experience comparing min-conflicts [9] and GSAT <ref> [17] </ref>, two iterative-repair algorithms. Min-conflicts begins with an initial assignment. On each iteration, min-conflicts randomly chooses a variable that is in conflict and assigns it the value that minimizes the number of remaining conflicts. GSAT is a descendant of min-conflicts designed for boolean satisfiability problems. <p> GSAT is a descendant of min-conflicts designed for boolean satisfiability problems. GSAT differs from min-conflicts in that it selects the variable that eliminates the most conflicts when flipped rather than choosing randomly among conflicted variables. 4 Selman et al. <ref> [17] </ref> claims that GSAT performs well on graph-coloring problems and describes a 17-color problem where GSAT significantly outperforms other algorithms.
Reference: 18. <author> D. Smith. </author> <title> Transformational approach to scheduling. </title> <type> Technical Report KES.U.92.2, </type> <institution> Kestrel Institute, </institution> <year> 1992. </year> <title> This article was processed using the L a T E X macro package with LLNCS style </title>
Reference-contexts: Most of the instances of these challenging problems can be solved quickly, but only if the right CSP method is applied. Some recent studies provide examples of challenging problems | problems that were considered hard until the right method was found: Smith <ref> [18] </ref> describes a DARPA scheduling problem for which he synthesized an algorithm that was orders of magnitude faster than previous methods. By making heavy use of constraint propagation techniques, Smith's algorithm completely eliminated search in some cases.
References-found: 18

