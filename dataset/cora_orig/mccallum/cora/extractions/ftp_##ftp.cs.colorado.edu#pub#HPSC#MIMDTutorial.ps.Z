URL: ftp://ftp.cs.colorado.edu/pub/HPSC/MIMDTutorial.ps.Z
Refering-URL: http://www.cs.colorado.edu/current/courses/materials.hpsc.html
Root-URL: http://www.cs.colorado.edu
Title: Distributed-Memory MIMD Computing: An Introduction  High Performance Scientific  
Author: E. R. Jessup 
Address: Boulder  
Affiliation: University of Colorado at  
Date: September 5, 1995  
Pubnum: Computing  
Abstract-found: 0
Intro-found: 1
Reference: [Almasi & Gottlieb 94] <author> ALMASI, GEORGE S. AND ALLAN GOTTLIEB. </author> <year> [1994]. </year> <title> Highly Parallel Computing. </title> <publisher> The Benjamin/Cummings Publishing Company, Inc., </publisher> <address> Redwood City, CA, 2nd edition. </address>
Reference-contexts: Instead, all nodes were connected to a pair of common data buses. Using the data buses, any one node in the machine could communicate directly with any other node. The NonStop was a computer designed especially for database applications <ref> [Almasi & Gottlieb 94] </ref>. Other designs followed soon after, beginning with the introduction of the Erlangen General Purpose Architecture (EGPA) in 1977 by W. Haendler, F. Hofman, and H. Schneider at the University of Erlangen in Germany. <p> Each node held Intel 8086/8087 processors and 128 Kbytes of memory. The prototype was released in 1982. The prototype Cosmic Cube was followed by improved versions culminating in the Mark III Cosmic Cube completed in 1987. The Mark III uses Motorola 68020 processors <ref> [Almasi & Gottlieb 94, Wilson 94] </ref>. The Cosmic Cube inspired the development of at least two successful commercial hypercube multiprocessors. The first of these was the Intel Personal Supercomputer (the iPSC, later called the iPSC/1). The first iPSC/1's were delivered in 1985 and have up to 128 nodes. <p> Each node had 64 Kbytes of memory and a 32-bit processor. Its host computer had an Intel 80286 processor and ran a special version of Unix that allowed the nCUBE to emulate a machine with a single distributed file system <ref> [Almasi & Gottlieb 94] </ref>. The iPSC/1 and the nCUBE/1 have both spawned new generations of hypercube multiprocessors with more powerful processors and more advanced communication. The iPSC/2 hypercube appeared in 1987 using 80386/80387 processors. <p> The third generation hypercube, the nCUBE/3, was released in 1994. It offers even faster communication and computation and can have up to 65,536 nodes and up to 1 Gbyte of memory per node. The second and third generation nCUBEs have been geared toward the database market <ref> [Almasi & Gottlieb 94, Wyckoff 94, nCUBE 94] </ref>. The mesh multiprocessor has a more scalable design than does the hypercube. In a hypercube multiprocessor with p = 2 d nodes, a node has d nearest neighbors and so d communication wires. <p> : ; d 1 , PARTNERROW = MYROW + p 2 =2 l MYPARTNER = (PARTNERROW,MYCOL) MAXROW = max (MYROW,PARTNERROW) If (MAXROW.le.p 2 =2 l ) then If (MYROW.lt.PARTNERROW) then Send data to MYPARTNER Else Receive data Endif Endif Endfor Endif CUBoulder : HPSC Course Notes Distributed-Memory MIMD Computing 43 <ref> [Almasi & Gottlieb 94] </ref>. The general-purpose Thinking Machines CM-5 architecture is based on a quaternary tree with SPARC processors at the leaves and routing chips at the other tree nodes. Each leaf processor has four parent routing chips.
Reference: [Barnett et al 91] <author> BARNETT, M., D. G. PAYNE, AND R. VAN DE GEIJN. </author> <year> [1991]. </year> <title> Optimal broadcasting in mesh-connected architectures. </title> <type> Technical Report TR-91-38, </type> <institution> Dept. of Computer Science, University of Texas at Austin. </institution>
Reference-contexts: These mesh operations can also be applied if p 1 and p 2 are not powers of 2. For details of the modifications to arbitrary mesh size as well as for the optimizations available under wormhole routing, see <ref> [Barnett et al 91, Barnett et al 94] </ref>.
Reference: [Barnett et al 94] <author> BARNETT, M., D.G. PAYNE, R. VAN DE GEIJN, AND J. WATTS. </author> <year> [1994]. </year> <title> Broadcasting on meshes with worm-hole routing. </title> <note> Submitted to Journal of Parallel and Distributed Computing. </note>
Reference-contexts: These mesh operations can also be applied if p 1 and p 2 are not powers of 2. For details of the modifications to arbitrary mesh size as well as for the optimizations available under wormhole routing, see <ref> [Barnett et al 91, Barnett et al 94] </ref>.
Reference: [Dewar & Smosna 90] <author> DEWAR, R. AND M. SMOSNA. </author> <year> [1990]. </year> <title> Microprocessors: A Programmer's Point of View. </title> <publisher> McGraw-Hill, Inc., </publisher> <address> New York, NY. </address>
Reference-contexts: This is due in part to the almost eight-fold increase in fi=! between the two machines, but it is more greatly influenced by the difficulty of programming the i860 processor. (See <ref> [Dewar & Smosna 90] </ref> for details.) The data for p = 1 show that the theoretical peak performance figures are substantially more realistic for the nCUBE/2 than for the i860-based computers.
Reference: [Dongarra 94] <author> DONGARRA, J. J. </author> <year> [1994]. </year> <title> Performance of various computers using standard linear equations software. </title> <type> Technical Report CS-89-85, </type> <institution> Oak Ridge National Laboratory, Oak Ridge, TN 37831. </institution> <note> netlib version as of November 1, </note> <year> 1994. </year>
Reference-contexts: The parallel program is the solution of the largest linear system that will fit on a given DM-MIMD multiprocessor. This is the Highly Parallel Computing benchmark of <ref> [Dongarra 94] </ref>. Table 3 shows the system size n (number of equations), the megaflops measured for that system solution, and the theoretical peak performance of that machine. <p> Source: <ref> [Dongarra 94] </ref>. the ratio of the measured performance to the theoretical peak. Data are given for machines with p = 1, 2, and 8 nodes. The iPSC/860 was one of the first DM-MIMD multiprocessors with good enough performance to solve realistic scientific problems on a moderate number of processors. <p> CUBoulder : HPSC Course Notes 24 Distributed-Memory MIMD Computing Computer Mbytes Problem Gflops /Node Size nCUBE/2 4 7776 2:4 fi 10 4 iPSC/860 8 12000 2.6 Delta 16 12500 3.5 Paragon (OSF) 32 12000 4.0 Table 4: The Highly Parallel Computing benchmark for some 128-node machines. Sources: <ref> [Dunigan 94, Dongarra 94] </ref>. The main advantage of DM-MIMD multiprocessors is in their large distributed memories and cumulative computing power. The Highly Parallel Computing benchmark demonstrates the full potential of a parallel computer for solving linear systems. <p> Table 4 shows these Gflop ratings and the attainable problem sizes for some of the machines we've examined using 128 nodes. For comparison, a 296-node Paragon (OSF) solves a problem of size 29400 at a rate of 12.5 Gflops <ref> [Dongarra 94] </ref>. In summary, the performance of a DM-MIMD multiprocessor depends on a variety of interacting factors. The theoretical peak performance depends on such standard concerns as the amount of memory and the speed of the processor. When data must be transferred, it is also determined by the communication bandwidth.
Reference: [Dunigan 91] <author> DUNIGAN, T. H. </author> <year> [1991]. </year> <title> Performance of the Intel iPSC/860 and Ncube 6400 hypercubes. </title> <journal> Parallel Computing, </journal> <volume> 17(1991) </volume> <pages> 1285-1302. </pages>
Reference-contexts: The iPSC/860, which replaced the iPSC/2 in 1990, employs wormhole routing by which messages can travel from one processor to any other with almost no delay at the intervening processors. The iPSC/860 uses i860 computation processors. The iPSC/860 and later Intel DM-MIMD multiprocessors all retain the separate communication processors <ref> [Dunigan 91] </ref>. The nCUBE/2 (also called the nCUBE 6400) was released in 1989 and has up to 8192 nodes and up to 64 Mbytes of memory per node. The nCUBE/2 retains the single chip technology that is the hallmark of nCUBE hypercubes but uses a 64-bit processor and wormhole routing. <p> Sources: <ref> [Dunigan 91, Dunigan 92, Dunigan 94] </ref>. <p> A symbol means that data were not available. A symbol ~ means that the data are reported to fewer significant figures than are other entries in the table. Sources: <ref> [Dunigan 91, Dunigan 92, Dunigan 94] </ref>. degradation of bandwidth when the number of hops increases from one to six.
Reference: [Dunigan 92] <author> DUNIGAN, T. H. </author> <year> [1992]. </year> <title> Communication performance of the Intel Touchstone Delta mesh. </title> <type> Technical Report ORNL/TM-11983, </type> <institution> Oak Ridge National Laboratory. </institution>
Reference-contexts: It served as the prototype for the commercial product, the Intel Paragon. The Paragon also uses a two-dimensional mesh interconnection, but its processor is the faster i860XP. Delivery of the Paragon also began in 1991 <ref> [Dunigan 92, Dunigan 94, Wilson 94] </ref>. While Intel and nCUBE have been particularly successful suppliers of CUBoulder : HPSC Course Notes Distributed-Memory MIMD Computing 13 DM-MIMD multiprocessors, they are by no means the only ones. <p> Sources: <ref> [Dunigan 91, Dunigan 92, Dunigan 94] </ref>. <p> A symbol means that data were not available. A symbol ~ means that the data are reported to fewer significant figures than are other entries in the table. Sources: <ref> [Dunigan 91, Dunigan 92, Dunigan 94] </ref>. degradation of bandwidth when the number of hops increases from one to six.
Reference: [Dunigan 94] <author> DUNIGAN, T. H. </author> <year> [1994]. </year> <title> Early experiences and performance of the Intel Paragon. </title> <type> Technical Report ORNL/TM-12194, </type> <institution> Oak Ridge National Laboratory. </institution>
Reference-contexts: It served as the prototype for the commercial product, the Intel Paragon. The Paragon also uses a two-dimensional mesh interconnection, but its processor is the faster i860XP. Delivery of the Paragon also began in 1991 <ref> [Dunigan 92, Dunigan 94, Wilson 94] </ref>. While Intel and nCUBE have been particularly successful suppliers of CUBoulder : HPSC Course Notes Distributed-Memory MIMD Computing 13 DM-MIMD multiprocessors, they are by no means the only ones. <p> Sources: <ref> [Dunigan 91, Dunigan 92, Dunigan 94] </ref>. <p> That is, on computers with wormhole routing, the time to send a message between distant nodes is roughly the same as the time to send it between neighboring nodes <ref> [Dunigan 94] </ref>. The speed of internode communication is reflected in the communication bandwidth of a machine: more bytes of data are communicated per second when message transfer time is fast. <p> A symbol means that data were not available. A symbol ~ means that the data are reported to fewer significant figures than are other entries in the table. Sources: <ref> [Dunigan 91, Dunigan 92, Dunigan 94] </ref>. degradation of bandwidth when the number of hops increases from one to six. <p> If q is large, this delay can be substantial. Contention occurs for a communication link whenever the total amount of data attempting to pass through it exceeds the bandwidth of that link <ref> [Dunigan 94] </ref>. The overall effect of contention on the performance of a parallel program is very difficult to predict in advance, but, in general, it is best to program in a style that limits contention. In sections 3.2-4.2, we discuss methods for programming mesh and hypercube multiprocessors without contention. <p> CUBoulder : HPSC Course Notes 24 Distributed-Memory MIMD Computing Computer Mbytes Problem Gflops /Node Size nCUBE/2 4 7776 2:4 fi 10 4 iPSC/860 8 12000 2.6 Delta 16 12500 3.5 Paragon (OSF) 32 12000 4.0 Table 4: The Highly Parallel Computing benchmark for some 128-node machines. Sources: <ref> [Dunigan 94, Dongarra 94] </ref>. The main advantage of DM-MIMD multiprocessors is in their large distributed memories and cumulative computing power. The Highly Parallel Computing benchmark demonstrates the full potential of a parallel computer for solving linear systems.
Reference: [Fosdick et al 95] <author> FOSDICK, LLOYD D., CAROLYN J. C. SCHAUBLE, AND ELIZ-ABETH R. JESSUP. </author> <year> [1995]. </year> <title> Computer performance: An introduction. </title> <booktitle> HPSC Course Notes. </booktitle>
Reference-contexts: Like that of any other computer, the performance of a DM-MIMD multiprocessor may be classified according to such measures as its Mflop rating and its LINPACK benchmark (discussed in <ref> [Fosdick et al 95] </ref>). In addition, the performance of a DM-MIMD computer is determined by the speed of data communication between nodes. In this section, we discuss the communication performance of several machines.
Reference: [Fox et al 88] <author> FOX, G. C., M. A. JOHNSON, G. A. LYZENGA, S. W. OTTO, J. K. SALMON, AND D. W. WALKER. </author> <year> [1988]. </year> <title> Solving Problems on Concurrent Processors, volume 1. </title> <publisher> Prentice-Hall, Inc., </publisher> <address> Englewood Cliffs, NJ. </address> <note> CUBoulder : HPSC Course Notes 70 Distributed-Memory MIMD Computing </note>
Reference: [Fox et al 94] <author> FOX, GEOFFREY C., ROY D. WILLIAMS, AND PAUL C. MESSINA. </author> <title> [1994]. </title> <publisher> Parallel Computing Works! Morgan Kaufmann Publishers, Inc., </publisher> <address> San Francisco, CA. </address>
Reference: [Geist et al 95] <author> GEIST, AL, ADAM BEGUELIN, JACK DONGARRA, WEICHING JIANG, ROBERT MANCHEK, AND VAIDY SUNDERAM. </author> <year> [1995]. </year> <title> PVM Parallel Virtual Machine: A Users' Guide and Tutorial for Network Parallel Computing. Scientific and Engineering Computation. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: A wholly different type of DM-MIMD multiprocessor is represented by a cluster of workstations. These sets of interconnected workstations are growing in popularity because of their affordability and the substantial computing power of individual workstations. Thanks to such networking software as PVM <ref> [Geist et al 95] </ref> workstation clusters can be programmed in the same style as a more traditional multiprocessor. Because the nodes do not have to be identical, a workstation cluster is an example of a heterogeneous computing environment.
Reference: [Golub & Van Loan 89] <author> GOLUB, G.H. AND C.F. Van LOAN. </author> <year> [1989]. </year> <title> Matrix Computations. </title> <publisher> The Johns Hopkins University Press, </publisher> <address> Baltimore, MD, 2nd edition. </address>
Reference-contexts: We might even want to reconsider how we split up the matrix operations in the first place. For instance, it is often more efficient to split the matrix into blocks than into rows. (See <ref> [Golub & Van Loan 89] </ref> for more information on blocked algorithms.) We may also need to alter the program if a preceding computation distributes the matrix A among the nodes differently than we have required.
Reference: [Gropp et al 94] <author> GROPP, W., E. LUSK, AND S. PIEPER. </author> <month> [Oct </month> <year> 1994]. </year> <title> User's guide for the ANL IBM SP-1. </title> <type> Technical Report ANL/MCS-TM-198, </type> <institution> Argonne National Laboratory, Argonne, IL. </institution>
Reference-contexts: These are instead handled by various parallel programming languages. Nevertheless, the programmer may need to specify the layer to be used by the language and so must be aware of the benefits and disadvantages of the layer types. For more details on the SP1, see <ref> [Gropp et al 94] </ref>. The details of programming a DM-MIMD multiprocessor differ from machine to machine. On some machines, the programmer has much control over such details as the routes traveled by messages, while, on others, such details are hidden in communication libraries.
Reference: [Henning & Volkert 85] <author> HENNING, W. AND J. VOLKERT. </author> <year> [1985]. </year> <booktitle> Programming EGPA systems. In Proceedings of the Fifth International Conference on Distributed Computing Systems, </booktitle> <pages> pages 552-559, </pages> <address> Washington, DC. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: Haendler, F. Hofman, and H. Schneider at the University of Erlangen in Germany. This machine had nodes in a pyramidal structure in which all nodes at the same level of the pyramid formed a torus <ref> [Henning & Volkert 85, Wilson 94] </ref>. CUBoulder : HPSC Course Notes Distributed-Memory MIMD Computing 11 In 1979, the Processor Array eXperiment (PAX-9) was completed by T. Hoshino at the the Institute of Atomic Energy at the University of Kyoto and T. Kawai of Keio University.
Reference: [Hoshino 86] <author> HOSHINO, T. </author> <year> [1986]. </year> <title> An invitation to the world of PAX. </title> <journal> IEEE Computer, </journal> <volume> 19 </volume> <pages> 68-79. </pages>
Reference-contexts: Kawai of Keio University. The PAX-9 was a toroidal array of nine processors. Each node in the array was directly connected to four others. The PAX-9 did not allow the processors to operate fully asynchronously and so permitted a "quasi-MIMD" programming model <ref> [Hoshino 86] </ref>. The closest ancestor of many of today's DM-MIMD multiprocessors was the Cosmic Cube hypercube multiprocessor built at the California Institute of Technology. Development of this machine began in 1981 and was carried out by a research team headed by computer scientist Chuck Seitz and physicist Geoffrey Fox.
Reference: [Hwang 93] <author> HWANG, KAI. </author> <year> [1993]. </year> <title> Advanced Computer Architecture. </title> <publisher> McGraw-Hill, Inc., </publisher> <address> New York, NY. </address>
Reference-contexts: The Omega network is also sometimes called the butterfly network, the flip network, the baseline, or the reverse baseline network [Leighton 92]. An Omega network is not constructed by using single wire connections but rather by means of 2 fi 2 switches. For more implementation details, see <ref> [Hwang 93] </ref>. The IBM SP1 and SP2 use a version of the Omega network. Although the graphs discussed in this section account for those underlying the majority of existing DM-MIMD multiprocessors, there are other possibilities. One example is the pyramidal structure of the EGPA mentioned in section 2.2. <p> Other multiprocessor architectures have identical processors at all nodes but use a completely different interconnection mechanism. For example, the nodes of an Omega network are not connected directly by communication wires, but rather they are connected by means of switches <ref> [Hwang 93] </ref>. The IBM SP1 uses a high speed Omega switch by which a message may be sent quickly between any two nodes of the machine. Thus, the concept of nearest neighbor is not important in programming the IBM SP1.
Reference: [Intel 86] <institution> Intel Corporation, Beaverton, Oregon. </institution> <month> [Sep </month> <year> 1986]. </year> <title> iPSC Technical Description. Order number: </title> <type> 175278-003. </type>
Reference-contexts: The first iPSC/1's were delivered in 1985 and have up to 128 nodes. The iPSC/1 nodes have Intel 80286/80287 processors and up to 4.5 Mbytes of memory. Models with vector processors were also available. The hypercube is operated via an Intel System 310AP microcomputer connected to the nodes <ref> [Intel 86] </ref>. This type of controlling computer is termed the host . In 1985, the first nCUBE hypercube multiprocessor was also released.
Reference: [Intel 89] <institution> Intel Corporation, Beaverton, Oregon. </institution> <month> [Jun </month> <year> 1989]. </year> <title> iPSC/2 Simulator Manual. Order number: </title> <type> 311534-003. </type>
Reference-contexts: This simulator allows you to write and debug hypercube programs without actually using the hypercube multiprocessor. The simulator caused the DEC 5000 workstations in our lab to hang, so we do not support it locally. However, it can be a valuable tool to use on other workstation architectures. See <ref> [Intel 89] </ref> for more information. The simulator is easy to install and use. The source is available from your instructor. CUBoulder : HPSC Course Notes Distributed-Memory MIMD Computing 69
Reference: [Intel 91a] <institution> Intel Corporation, Beaverton, Oregon. </institution> <month> [Apr </month> <year> 1991]. </year> <title> iPSC/2 and iPSC860 Programmer's Reference Manual. Order number: </title> <type> 311708-004. </type>
Reference-contexts: These rules are the key to maintaining your sanity when using bud. In section 8.7, we mention a couple of options to aid in debugging hypercube programs. This appendix covers only those operations and techniques most relevant to programs included in the HPSC materials. For more information, see <ref> [Intel 91a, Intel 91b] </ref>. CUBoulder : HPSC Course Notes 50 Distributed-Memory MIMD Computing 8.1 Writing a node program Bud has compilers for two languages. The Fortran 77 compiler is located in /usr/bin/f77, and the C compiler is in /usr/bin/cc.
Reference: [Intel 91b] <institution> Intel Corporation, Beaverton, Oregon. </institution> <month> [Apr </month> <year> 1991]. </year> <title> iPSC/2 and iPSC/860 User's Guide. Order number: </title> <type> 311532-007. </type>
Reference-contexts: These rules are the key to maintaining your sanity when using bud. In section 8.7, we mention a couple of options to aid in debugging hypercube programs. This appendix covers only those operations and techniques most relevant to programs included in the HPSC materials. For more information, see <ref> [Intel 91a, Intel 91b] </ref>. CUBoulder : HPSC Course Notes 50 Distributed-Memory MIMD Computing 8.1 Writing a node program Bud has compilers for two languages. The Fortran 77 compiler is located in /usr/bin/f77, and the C compiler is in /usr/bin/cc. <p> See <ref> [Intel 91b] </ref> for details. Because bud does not have a CFS, output from a hostless program can only be printed to the screen and not written into files. Sections 8.1-8.3 show only a small fraction of the commands available for carrying out communication steps or controlling the hypercube. <p> Because bud does not have a CFS, output from a hostless program can only be printed to the screen and not written into files. Sections 8.1-8.3 show only a small fraction of the commands available for carrying out communication steps or controlling the hypercube. For a complete list, see <ref> [Intel 91b] </ref>. 8.4 Communication libraries Our sample host and node programs show how the most fundamental send and receive commands may be used to carry out global communication and manipulation of data in a hypercube program. <p> Repeat until done. 8.7 Debugging tools Running a simple example and printing out the intermediate results is often sufficient to debug hypercube programs. However, Intel also provides a concurrent debugger called DECON. More information on it is provided in <ref> [Intel 91b] </ref>. In addition, Intel provides a simulator of the iPSC/2 that runs on workstations. This simulator allows you to write and debug hypercube programs without actually using the hypercube multiprocessor. The simulator caused the DEC 5000 workstations in our lab to hang, so we do not support it locally.
Reference: [Jessup & Neves 94] <author> JESSUP, ELIZABETH R. AND RICH NEVES. </author> <year> [1994]. </year> <title> Using the Paragon at NOAA. </title> <booktitle> HPSC Course Notes. CUBoulder : HPSC Course Notes Distributed-Memory MIMD Computing 71 </booktitle>
Reference-contexts: As is true for the hypercube, mesh-connected multiprocessors also are equipped with library routines that perform the most basic communication operations. (See, for example, <ref> [Jessup & Neves 94] </ref> available via anonymous CUBoulder : HPSC Course Notes 40 Distributed-Memory MIMD Computing CUBoulder : HPSC Course Notes Distributed-Memory MIMD Computing 41 algorithm. ftp in the /pub/HPSC directory at the cs.colorado.edu site.) The user should also beware that when only a subset of the mesh processors are used,
Reference: [Jessup 95a] <author> JESSUP, ELIZABETH R. </author> <year> [1995]. </year> <title> Distributed-memory MIMD computing: An introduction. </title> <booktitle> HPSC Course Notes. </booktitle>
Reference-contexts: Both SIMD and MIMD programs can run on a MIMD multiprocessor. The sample node program of this section is a generalization of the most basic MIMD program introduced in the first section of <ref> [Jessup 95a] </ref>. In that case, two nodes shared the work in summing twenty numbers. In the sample program of this section, p 16 nodes share the work in summing 16 numbers. <p> The result of the global command is returned to every node. The library routines are efficient because they take into account the connectivity and parallelism of the hypercube. For example, the routine GCOLX carries out an alternate direction exchange (ADE) <ref> [Jessup 95a] </ref> to gather a distributed vector on all nodes.
Reference: [Jessup 95b] <author> JESSUP, ELIZABETH R. </author> <year> [1995]. </year> <title> Using the iPSC/2 at CU Boulder. </title> <booktitle> HPSC Course Notes. </booktitle>
Reference-contexts: Hypercube multiprocessors typically come equipped with communication library routines for basic data communication operations. For example, on an iPSC/2 or iPSC/860, a call to the library routine GCOLX with the data vector argument x accumulates distributed elements of x onto all nodes via an ADE. (See <ref> [Jessup 95b] </ref> which is available via anonymous ftp in the /pub/HPSC directory at the cs.colorado.edu site.) It is not unusual, however, to need a routine that combines communication and computation in a way not provided by the libraries.
Reference: [Leighton 92] <author> LEIGHTON, F. THOMSON. </author> <year> [1992]. </year> <title> Introduction to Parallel Algorithms and Architectures: Arrays, Trees, Hypercubes. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, CA. </address>
Reference-contexts: The Omega network is also sometimes called the butterfly network, the flip network, the baseline, or the reverse baseline network <ref> [Leighton 92] </ref>. An Omega network is not constructed by using single wire connections but rather by means of 2 fi 2 switches. For more implementation details, see [Hwang 93]. The IBM SP1 and SP2 use a version of the Omega network. <p> One example is the pyramidal structure of the EGPA mentioned in section 2.2. Processors may also be interconnected by means of a common communication channel or data bus. For more information on graphs and their properties in the context of multiprocessor architectures, see <ref> [Leighton 92] </ref>. 2.2 The evolution of DM-MIMD computers Development of DM-MIMD computers began in the early 1970's. The first example, the Tandem NonStop, was delivered in 1976. In the NonStop, nodes were not connected directly to other nodes. Instead, all nodes were connected to a pair of common data buses. <p> In this way, the graph is embedded into the hypercube. In this section, we review the basic embeddings that are relevant to this tutorial. A more comprehensive description is provided in <ref> [Leighton 92] </ref>. The simplest graph that can be embedded into a hypercube using only nearest neighbor connections is a linear array or ring. This is easy to see in the case of the 2-cube. <p> This numbering scheme can be extended to hypercubes of any dimension by numbering the nodes of the ring according to the binary reflected Gray code <ref> [Leighton 92, Reingold et al 77] </ref>. To construct the Gray code, begin with the binary numbers 0 Flip them over the underscore.
Reference: [Leiserson et al 92] <author> LEISERSON, C.E., Z.S. ABUHAMDEH, D.C. DOUGLAS, C.R. FEYNMANN, M.N. GANMUKHI, J.V. HILL, W.D. HILLIS, B.C. KUSZMAUL, M.A. ST.PIERRE, D.S. WELLS, M.C. WONG, S.-W. YANG, AND R. ZAK. </author> <year> [1992]. </year> <title> The network architecture of the Connection Machine CM-5. </title> <type> Technical Report, </type> <institution> Thinking Machines Corporation. [nCUBE 94] nCUBE Corporation, </institution> <address> Foster City, CA. </address> <month> [Nov </month> <year> 1994]. </year> <title> nCUBE3: The Teraflops Computing Platform. </title>
Reference-contexts: Like a real tree, a tree of this type has "thicker" branches near the root than at the leaves. This architecture is termed a fat tree <ref> [Leiserson et al 92] </ref>. In an architecture like the CM-5, the concept of nearest neighbor no longer strictly applies. <p> On this machine, all but the most expert programmers use library routines for all communication. For more information on the CM-5, see <ref> [Leiserson et al 92] </ref>. Other multiprocessor architectures have identical processors at all nodes but use a completely different interconnection mechanism. For example, the nodes of an Omega network are not connected directly by communication wires, but rather they are connected by means of switches [Hwang 93].
Reference: [Reingold et al 77] <author> REINGOLD, E.M., J. NIEVERGELT, AND N. DEO. </author> <year> [1977]. </year> <title> Combinatorial Algorithms. </title> <publisher> Prentice-Hall, Inc., </publisher> <address> Englewood Cliffs, NJ. </address>
Reference-contexts: This numbering scheme can be extended to hypercubes of any dimension by numbering the nodes of the ring according to the binary reflected Gray code <ref> [Leighton 92, Reingold et al 77] </ref>. To construct the Gray code, begin with the binary numbers 0 Flip them over the underscore.
Reference: [Saad & Schultz 89] <author> SAAD, Y. AND M.H. SCHULTZ. </author> <year> [1989]. </year> <title> Data communication in hypercubes. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 6 </volume> <pages> 115-135. </pages>
Reference-contexts: These same algorithms are applied in reverse to gather data distributed among all nodes into a single node. We also present one more general algorithm to accumulate distributed data in all nodes. More information on data communication in hypercubes is provided in, for example, <ref> [Saad & Schultz 89] </ref>. The simplest broadcast algorithm is based on a linear array.
Reference: [Smith 87] <author> SMITH, H.F. </author> <year> [1987]. </year> <title> Data Structures: Form and Function. </title> <publisher> Harcourt Brace Jovanovich, Inc., </publisher> <address> Orlando, FL. </address>
Reference: [Wilson 94] <author> WILSON, G.V. </author> <month> [Dec </month> <year> 1994]. </year> <title> History of parallel computing. </title> <type> Technical Report TR CSRI-312, </type> <institution> Computing Systems Research Institute, University of Toronto. </institution>
Reference-contexts: Haendler, F. Hofman, and H. Schneider at the University of Erlangen in Germany. This machine had nodes in a pyramidal structure in which all nodes at the same level of the pyramid formed a torus <ref> [Henning & Volkert 85, Wilson 94] </ref>. CUBoulder : HPSC Course Notes Distributed-Memory MIMD Computing 11 In 1979, the Processor Array eXperiment (PAX-9) was completed by T. Hoshino at the the Institute of Atomic Energy at the University of Kyoto and T. Kawai of Keio University. <p> Each node held Intel 8086/8087 processors and 128 Kbytes of memory. The prototype was released in 1982. The prototype Cosmic Cube was followed by improved versions culminating in the Mark III Cosmic Cube completed in 1987. The Mark III uses Motorola 68020 processors <ref> [Almasi & Gottlieb 94, Wilson 94] </ref>. The Cosmic Cube inspired the development of at least two successful commercial hypercube multiprocessors. The first of these was the Intel Personal Supercomputer (the iPSC, later called the iPSC/1). The first iPSC/1's were delivered in 1985 and have up to 128 nodes. <p> It served as the prototype for the commercial product, the Intel Paragon. The Paragon also uses a two-dimensional mesh interconnection, but its processor is the faster i860XP. Delivery of the Paragon also began in 1991 <ref> [Dunigan 92, Dunigan 94, Wilson 94] </ref>. While Intel and nCUBE have been particularly successful suppliers of CUBoulder : HPSC Course Notes Distributed-Memory MIMD Computing 13 DM-MIMD multiprocessors, they are by no means the only ones.
Reference: [Wyckoff 94] <author> WYCKOFF, R., </author> <year> [1994]. </year> <title> nCUBE Corporation, </title> <type> Personal Communication. </type> <note> CUBoulder : HPSC Course Notes </note>
Reference-contexts: The third generation hypercube, the nCUBE/3, was released in 1994. It offers even faster communication and computation and can have up to 65,536 nodes and up to 1 Gbyte of memory per node. The second and third generation nCUBEs have been geared toward the database market <ref> [Almasi & Gottlieb 94, Wyckoff 94, nCUBE 94] </ref>. The mesh multiprocessor has a more scalable design than does the hypercube. In a hypercube multiprocessor with p = 2 d nodes, a node has d nearest neighbors and so d communication wires.
References-found: 31

