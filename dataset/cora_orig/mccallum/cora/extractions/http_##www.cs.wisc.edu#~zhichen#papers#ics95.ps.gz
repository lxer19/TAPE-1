URL: http://www.cs.wisc.edu/~zhichen/papers/ics95.ps.gz
Refering-URL: http://www.cs.wisc.edu/~zhichen/zhichen.html
Root-URL: 
Title: Multiprocessor Scalability Predictions Through Detailed Program Execution Analysis  
Author: Xiaodong Zhang Zhichen Xu 
Address: San Antonio, Texas 78249  
Affiliation: High Performance Computing and Software Laboratory The University of Texas at San Antonio  
Abstract: Scalability measures the ability of a parallel system to improve performance as the size of an application problem and the number of processors involved increase. There are some limits to existing scalability studies. First, the problem size in a computation is not well-defined. Second, the methods used to differentiate algorithmic and architectural scal-abilities are not effective enough. Thirdly, most approaches to scalability study are either highly time-consuming or restricted to simple problem/architecture structures. A major effort of this work is to address these limits. We have extended the latency metric [11] for more complex scaling of problems, and to possibly isolate the scalability of an algorithm from a parallel system. The scalability prediction is based on a semi-empirical approach [10] that significantly reduces the time and cost of measurements and simulation. Our prediction results were validated on the KSR-1 and on the CM-5. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Grama, A. Gupta and V. Kumar, </author> <title> "Isoefficiency function: a scalability metric for parallel algorithms and architectures", </title> <journal> IEEE Parallel & Distributed Technology, </journal> <volume> Vol. 1, No. 3, </volume> <year> 1993, </year> <pages> pp. 12-21. </pages>
Reference-contexts: Furthermore, scalability mea surement time is significantly reduced by our semi-empirical approach. 2.1 Problem Size The existing definitions of problem size represent the amount of meaningful computation that must be carried out to solve a problem, and the input data size <ref> [1, 6, 11] </ref>, which express computation demand and memory requirement as seen by a conventional serial machine. <p> subsequent study of various scalabilities. 2.3 Scalabilities of Parallel Programs and Systems To extract the scalability of the program from a problem-machine combination, we extend the latency metric by defining algorithmic scalability as the amount of the increase in algorithmic latency to keep a desired algorithmic efficiency, E a 2 <ref> [0; 1] </ref>, that is Scale a (E a ; (P; P 0 L a (; ; P ) ; (8) where P 0 is a larger number of processors than P , 0 is the scaled application parameter to keep E a = E a (; 0 ; P 0 )
Reference: [2] <author> J. L. Gustafson, </author> <title> "The consequences of fixed time performance measurement", </title> <booktitle> Proceedings of the 25th Hawaii International Conference on System Sciences. </booktitle> <volume> Vol. III, </volume> <year> 1992, </year> <pages> pp. 113-124. </pages>
Reference: [3] <institution> Kendall Square Research, KSR-1 Technology background, </institution> <year> 1992. </year>
Reference-contexts: Combining both scalabilities, we can indirectly study the overhead patterns inherent in the architecture and its effects on the scalability of a program's execution. The parallel architectures we used are the KSR-1 <ref> [3] </ref> that supports the shared-memory programming model and the CM-5 [8] that supports both message-passing and data-parallel programming models. The problems we used for scalability study are All Pairs Shortest Path, Gauss Elimination and a large Electromagnetic Simulation application [4]. 4.1 Architectural Characteristics The KSR-1 [3], introduced by Kendall Square Research, <p> we used are the KSR-1 <ref> [3] </ref> that supports the shared-memory programming model and the CM-5 [8] that supports both message-passing and data-parallel programming models. The problems we used for scalability study are All Pairs Shortest Path, Gauss Elimination and a large Electromagnetic Simulation application [4]. 4.1 Architectural Characteristics The KSR-1 [3], introduced by Kendall Square Research, is a ring-based, cache coherent, shared-memory multiprocessor system with up to 1088 64-bit custom superscalar RISC processors (20MHz). A basic ring unit in the KSR-1 has 32 processors. The system uses a two-level hierarchy to interconnect 34 of these rings (1088 processors).
Reference: [4] <author> Y. Lu, et. al., </author> <title> "Implementation of electromagnetic scattering from conductors containing loaded slots on the Connection Machine CM-2", </title> <booktitle> Proceeding of the 6th SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <publisher> SIAM Press, </publisher> <year> 1993, </year> <pages> pp. 216-220. </pages>
Reference-contexts: The parallel architectures we used are the KSR-1 [3] that supports the shared-memory programming model and the CM-5 [8] that supports both message-passing and data-parallel programming models. The problems we used for scalability study are All Pairs Shortest Path, Gauss Elimination and a large Electromagnetic Simulation application <ref> [4] </ref>. 4.1 Architectural Characteristics The KSR-1 [3], introduced by Kendall Square Research, is a ring-based, cache coherent, shared-memory multiprocessor system with up to 1088 64-bit custom superscalar RISC processors (20MHz). A basic ring unit in the KSR-1 has 32 processors. <p> In Figure 5, P is the number of processors employed and N is the size of the matrix. The Electromagnetic (EM) simulation program simulates electromagnetic scattering from a conducting plane body <ref> [4] </ref>. In the simulation model, a plane wave from free space, defined as region "A" in the application, is incident on the conducting plane. The conducting plane contains two slots which are connected to a microwave network behind the plane.
Reference: [5] <author> D. Nussbaum, A. Agarwal, </author> <title> "Scalability of parallel machines", </title> <journal> Communication of the ACM, March 1991, </journal> <volume> Vol. 34, No. 3, </volume> <pages> pp. 57-61. </pages>
Reference-contexts: This means the scaling behavior of an application will depend on the way the input parameters are scaled. Second, it is important to study program scalability and architecture scalability as independently as possible. Several proposals have been made to separate the two scalabilities. In <ref> [5] </ref>, algorithm scalability is defined as the maximum achievable speedup on an architecture with an idealized communication structure. This speedup measures the inherent parallelism and overhead patterns of a program for a given size.
Reference: [6] <author> X. Sun and D. T. </author> <title> Rover, "Scalability of parallel algorithm-machine combinations", </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> Vol. 5, No. 6, </volume> <year> 1994, </year> <month> pp.599-613. </month>
Reference-contexts: Furthermore, scalability mea surement time is significantly reduced by our semi-empirical approach. 2.1 Problem Size The existing definitions of problem size represent the amount of meaningful computation that must be carried out to solve a problem, and the input data size <ref> [1, 6, 11] </ref>, which express computation demand and memory requirement as seen by a conventional serial machine.
Reference: [7] <author> A. Sivasubramaniam, A. Singla, U. Ramachandran, H. Venkateswaran, </author> <title> "An approach to scalability study of shared memory parallel systems", </title> <booktitle> Proceedings of the ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <month> May </month> <year> 1994, </year> <pages> pp. 171-179. </pages>
Reference-contexts: However, the same type of latency in this classification may also come from both the program and the machine. It is still difficult to identify and distinguish the performance bottlenecks from the hardware and the algorithm. In <ref> [7] </ref>, parallel computing overheads are classified into algorithmic overhead and interaction overhead, which isolates the overhead from the application program. However, it does not provide direct information about how performance changes when the size of the application and size of the machine changes.
Reference: [8] <author> Thinking Machine Corporation, </author> <title> The Connection Machine CM-5 Technical Summary, </title> <year> 1993. </year>
Reference-contexts: Combining both scalabilities, we can indirectly study the overhead patterns inherent in the architecture and its effects on the scalability of a program's execution. The parallel architectures we used are the KSR-1 [3] that supports the shared-memory programming model and the CM-5 <ref> [8] </ref> that supports both message-passing and data-parallel programming models. <p> A basic ring unit in the KSR-1 has 32 processors. The system uses a two-level hierarchy to interconnect 34 of these rings (1088 processors). Each processor has a 32 MByte cache and a 0.5 MByte subcache. The CM-5 <ref> [8] </ref> is the newest member of the Thinking Machine's Connection Machine family. It is a distributed memory multiprocessor system which can be scaled up to 16K processors and supports both SIMD and MIMD programming models.
Reference: [9] <author> X. Zhang, Z. Xu, L. Sun, </author> <title> "Performance prediction on implicit communication systems", </title> <booktitle> in Proceedings of the Sixth IEEE Symposium of Parallel and Distributed Processing, </booktitle> <publisher> IEEE Computer Society Press, </publisher> <month> Oct. </month> <year> 1994, </year> <pages> pp. 560-568. </pages>
Reference-contexts: To effectively measure and study the various scalabilities, we use a semi-empirical approach that relies on both analytical and experimental methods [10]. This approach has been used to predict the execution times of large application programs on large architectures based on small sample data <ref> [9] </ref>. There are several advantages of the semi-empirical approach in scalability evaluation. First, execution times on real parallel machines and on simulators used for latency measurements are significantly reduced, because analytical modeling is partially applied, and scalability predictions are based on measurement results of small-scale executions of the programs. <p> Similarly, when we represent the program as a thread graph <ref> [9] </ref>, T P p (; ; P ) will be pro portional to the computation demand of one critical path, k (P ), in , that is T M Thus, L a (; ; P ) = C (fi ( k (P ); ) P For the same reason, L a <p> Then we will introduce how this methodology can be adapted to predict parallel performances on the ideal machines. 3.1 Performance Prediction on Real Machines In <ref> [9] </ref>, a fairly complete model is developed to represent both explicit and implicit communications and synchronizations of a parallel program. This approach has achieved a good balance between using analytical techniques and experimental measurements in predicting multiprocessor performance. Our methodology is based on a two-level hierarchical model. <p> The algorithmic scalability of GE and its combination scalability are summarized in Tables 7 and 8. The two types of scalabilities for the EM program are reported in Tables 7 and 8. The data-parallel version of the GE program exploited limited parallelism due to the nature of the computation. <ref> [9] </ref>. Therefore, both algorithmic scalability and the combination scalability are very low (see Tables 7-8). In contrast, the EM program is well suitable to the data-parallel model. Thus, both scalabilities are high (see Tables 9 and 10). <p> the data-parallel GE. (Almost all the computation that is par-allelizable using the shared-memory model is parallelizable in the data-parallel version.) The other two factors that contribute to the high combination scalability are the small percentage of synchronization overhead of the program and the highly scalable Fat-tree networks of the CM-5 <ref> [9] </ref>.
Reference: [10] <author> X. Zhang, Z. Xu, L. Sun, </author> <title> "Semi-empirical multiprocessor performance predictions", </title> <type> Technical Report, </type> <institution> High Performance Computing and Software laboratory, The University of Texas at San Antonio, </institution> <month> March </month> <year> 1995. </year>
Reference-contexts: The second extension to the latency metric is related to scaling methods. In this study, we explicitly keep track how the parameters are scaled. To effectively measure and study the various scalabilities, we use a semi-empirical approach that relies on both analytical and experimental methods <ref> [10] </ref>. This approach has been used to predict the execution times of large application programs on large architectures based on small sample data [9]. There are several advantages of the semi-empirical approach in scalability evaluation. <p> Repeated operations in different iterations in the sequential program are carried out concurrently by virtual processors in the system. 4.3 Validation of the Prediction Methodology Here we give a brief summary of our validation results. In <ref> [10] </ref>, we use simple metrics to study the effectiveness of the semi-empirical approach in predicting the parallel execution times. Our validation results indicate that: * If the program abstraction is sufficiently detailed, the predicted results can be very precise.
Reference: [11] <author> X. Zhang, Y. Yan, K. </author> <title> He, "Latency metric: an experimental method for measuring and evaluating parallel program and architecture scalability", </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Vol. 22, No. 3, </volume> <year> 1994, </year> <pages> pp. 392-410. </pages>
Reference-contexts: Both simulation and analytical methods are based on some assumptions of the architecture and application. Analytical approaches usually describe some asymptotic behavior of a parallel system, and they may only be applicable to simple problem/architecture structures. To address these limits, we make some extensions to the latency metric in <ref> [11] </ref>, and use a semi-empirical approach for scalability study. The first extension to the latency metric is to isolate the scalability of the algorithm from an algorithm-machine combination. <p> The experiments were performed on the KSR-1 and the CM-5. Section 5 concludes the paper. 2 Extensions to the Latency Metric The latency metric <ref> [11] </ref> defines algorithm-machine combination scalability as an average increase of the overhead latency (L) needed to keep its computation efficiency equal to a constant (E) when the size of a parallel program increases from W to W 0 , and the size of the parallel architecture increases from P processors to <p> Furthermore, scalability mea surement time is significantly reduced by our semi-empirical approach. 2.1 Problem Size The existing definitions of problem size represent the amount of meaningful computation that must be carried out to solve a problem, and the input data size <ref> [1, 6, 11] </ref>, which express computation demand and memory requirement as seen by a conventional serial machine. <p> We have shown performance differences of a program on the two different architectures. 5 Conclusion Effective use and refinement of scalability metrics and evaluation methods is a major issue of parallel performance evaluation. To address limitations in the scalability study, we have extended the latency metric in <ref> [11] </ref> by providing a more precise definition of problem size, and by quantifying algorithmic scalability based on algorithmic latency and efficiency. The algorithmic scalability defined in this paper is quite independent of the hardware architecture. To effectively estimate the various scalabilities, a semi-empirical approach is used.
Reference: [12] <author> X. Zhang, Y. Yan and K. </author> <title> He, "Evaluation and measurement of multiprocessor latency patterns", </title> <booktitle> in Proceedings of the 8th International Parallel Processing Symposium, </booktitle> <publisher> IEEE Computer Society Press, </publisher> <month> April, </month> <year> 1994, </year> <pages> pp. 845-852. </pages>
Reference-contexts: An analytical model of a program structure may not precisely and completely describe overhead patterns of the program. The architectural scalability is still, to a great extent, dependent on the application program executed. In <ref> [12] </ref>, the execution overhead latency is further divided into the memory reference latency, the processor idle time, and the parallel primitive execution overhead time, which comprehensively cover the major sources of system/architecture overheads.
References-found: 12

