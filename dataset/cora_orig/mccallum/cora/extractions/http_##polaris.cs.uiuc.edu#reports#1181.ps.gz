URL: http://polaris.cs.uiuc.edu/reports/1181.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: PARALLEL OCEAN CIRCULATION MODELING ON CEDAR  
Author: BY LUIZ ANT ONIO DE ROSE Bach., Universidade de Braslia, 
Degree: THESIS Submitted in partial fulfillment of the requirements for the degree of Master of Science in Computer Science in the Graduate College of the  
Date: 1982  
Address: Braslia,  1992 Urbana, Illinois  
Affiliation: M.Stat., Universidade de  University of Illinois at Urbana-Champaign,  
Abstract-found: 0
Intro-found: 1
Reference: [ADLM88] <author> P. Andrich, P. Delecluse, C. Levy, and G. Madec. </author> <title> A multitasked general circulation model of the ocean. </title> <booktitle> In Proceedings Fourth International Symposium, Cray Research, </booktitle> <pages> pages 407-428, </pages> <year> 1988. </year>
Reference-contexts: Later, Cox [Cox84] further modified Bryan's model to improve the numerical and computational efficiency of the code. Pinardi and Navarra adapted Cox's program to the Mediterranean basin geometry [PN88]. 3.2 Efforts using parallel supercomputers Andrich, Madec et al. in <ref> [AM88, ADLM88] </ref> describe a code written for the Cray 2 applied to sections of the Atlantic and of the Mediterranean. An interesting aspect of their work is their emphasis on efficient elliptic solvers for the two-dimensional mass transport stream function. <p> The second problem was solved by keeping the three-dimensional data out of main memory, and bringing to memory just the slabs necessary to perform the computation. The slab partitioning in vertical sections is still used in the recent version of Semtner [CS88]. Andrich, Madec et al. <ref> [AM88, ADLM88] </ref> using the Cray-2, (and its very large memory), have a different approach, with the use of horizontal slabs for all the horizontal 20 operators and vertical slabs for the vertical operators. For each case, each particular processor computes one slab at a time. <p> Similar results were obtained by other authors, and we can observe that much work is being done to implement new algorithms for the barotropic phase. Andrich, Madec 29 et al. in <ref> [AM88, ADLM88] </ref> implemented a parallel SOR, and a parallel conjugate gradient method for the Cray 2 with four CPUs, obtaining better performance for the latter method.
Reference: [All87] <institution> Alliant Computer Systems Corporation, </institution> <address> 42 Nagog Park, Acton, MA 01720. </address> <note> FX/Fortran Language Manual, </note> <year> 1987. </year>
Reference-contexts: These program segments are called cluster-tasks. System calls are provided by Xylem for starting and stopping tasks, waiting for tasks to finish, and for inter-task synchronization. The Cedar Fortran language [Hoe91, GPHL90, EHJP90] is derived from Alliant's FX/Fortran <ref> [All87] </ref>, which is Fortran 77, with vector constructs such as those proposed for the next Fortran standard (Fortran 90). Cedar Fortran has extensions for memory allocation, concurrency control, multitasking and synchronization. <p> This problem was confirmed by another result in our preliminary study that showed a speedup between 1.5 and 2.5, using Alliant's parallelizing compiler <ref> [All87] </ref> on the Alliant FX/8 with 8 CEs. Similar results were obtained by other authors, and we can observe that much work is being done to implement new algorithms for the barotropic phase. <p> Hrtime is an extension of the Concentrix user and system process time measurements; it times both execution and non-execution process states with 10 sec accuracy. The Alliant Fortran library routine etime returns the elapsed cpu time, also with 10 sec accuracy (see <ref> [All87] </ref>). Timing results are in seconds per timestep, and were derived after running 12 time steps and averaging the last 10 time steps for the model P 8 L 16 , and the last 14 time steps out of 16 in P 4 L 8 . <p> We used scalar optimization, concurrency, vectorization, and associativity transformations as VAST optimization options (see <ref> [All87] </ref>). Table 5.6 also has the run time for the single cluster KAP version running model P 4 L 8 on Cedar 1 .
Reference: [AM88] <author> P. Andrich and G. Madec. </author> <title> Performance evaluation for an ocean general circulation model: vectorization and multitasking. </title> <booktitle> In 1988 International Conference on Supercomputing, </booktitle> <pages> pages 295-302, </pages> <address> St. Malo, France, </address> <month> July </month> <year> 1988. </year> <note> ACM. </note>
Reference-contexts: Later, Cox [Cox84] further modified Bryan's model to improve the numerical and computational efficiency of the code. Pinardi and Navarra adapted Cox's program to the Mediterranean basin geometry [PN88]. 3.2 Efforts using parallel supercomputers Andrich, Madec et al. in <ref> [AM88, ADLM88] </ref> describe a code written for the Cray 2 applied to sections of the Atlantic and of the Mediterranean. An interesting aspect of their work is their emphasis on efficient elliptic solvers for the two-dimensional mass transport stream function. <p> The second problem was solved by keeping the three-dimensional data out of main memory, and bringing to memory just the slabs necessary to perform the computation. The slab partitioning in vertical sections is still used in the recent version of Semtner [CS88]. Andrich, Madec et al. <ref> [AM88, ADLM88] </ref> using the Cray-2, (and its very large memory), have a different approach, with the use of horizontal slabs for all the horizontal 20 operators and vertical slabs for the vertical operators. For each case, each particular processor computes one slab at a time. <p> Similar results were obtained by other authors, and we can observe that much work is being done to implement new algorithms for the barotropic phase. Andrich, Madec 29 et al. in <ref> [AM88, ADLM88] </ref> implemented a parallel SOR, and a parallel conjugate gradient method for the Cray 2 with four CPUs, obtaining better performance for the latter method.
Reference: [Ara66] <author> A. Arakawa. </author> <title> Computational design for long-term numerical integration of the equations of fluid motion: two dimensional imcompressible flow. part 1. </title> <journal> Journal of Computational Physics, </journal> <volume> 1 </volume> <pages> 119-143, </pages> <year> 1966. </year>
Reference-contexts: Nevertheless it is necessary to insure that certain integral constraints are maintained during the numerical solution of the problem. In the absence of dissipative effects, momentum, energy, and the variance of temperature and salinity should be conserved. Arakawa <ref> [Ara66] </ref> was the first to work with the finite difference formulation, maintaining the integral constraints, and this approach was generalized by Bryan [Bry69], allowing the arrangement of the cells to be chosen in any manner that is convenient for the problem. <p> The first constraint that must be satisfied is the mass conservation within each cell. It is shown by Arakawa <ref> [Ara66] </ref> that if integral constraints on energy are maintained, nonlinear instability can be avoided. Considering each cell as a regular rectangular array, then the number of neighbor cells will be 6.
Reference: [Ban76] <author> U. Banerjee. </author> <title> Data dependence in ordinary programs. </title> <type> Master's thesis, </type> <institution> University of Illinois at Urbana-Champaign, Dept. of Computer Science, </institution> <month> November </month> <year> 1976. </year>
Reference-contexts: To simplify the discussion, this description ignores the land issue; It should be remembered however that the original program has conditional code within the loops to avoid computations over land areas. The data dependence analysis <ref> [Ban76, KKP + 81, PW86] </ref> for the loops in this algorithm shows that loops 1 and 5 can be executed as vector concurrent loops, as long as loop 1 precedes loop 5. The problems for parallelization occur with loops 2, 3 and 4.
Reference: [BB87] <author> M. J. Berger and S. H. Bokhari. </author> <title> A partitioning strategy for nonuniform problems on multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36:570-580, </volume> <month> May </month> <year> 1987. </year>
Reference-contexts: Also for multiple processor systems, Reed, Adams, and Patrick [RAP87] use the stencil structure to choose the appropriate partitioning shape. Berger and Bokhari <ref> [BB87] </ref> consider partitioning strategies that balance the workload for problems on a domain with nonuniform work estimates. Slab partitioning in terms of independent longitude/depth (vertical) sections was introduced in ocean simulation codes for Bryan's model.
Reference: [BC67] <author> K. Bryan and M. D. Cox. </author> <title> A numerical investigation of the oceanic general circulation. </title> <address> Tellus, XIX:54-80, </address> <year> 1967. </year>
Reference-contexts: In 1967, the results from the first numerical experiments with primitive equation, three-dimensional ocean model were presented by Bryan and Cox <ref> [BC67] </ref>; these were followed by a description of the physics and numerics involved in the code by Bryan [Bry69]. Although, since that time, ocean models have being developed by several research groups, most of them are derived from the aforementioned original work of Bryan and Cox.
Reference: [BC72] <author> K. Bryan and M. D. Cox. </author> <title> An approximate equation of state for numerical models of ocean circulation. </title> <journal> Journal of Physics Oceanographic, </journal> <volume> 2 </volume> <pages> 510-514, </pages> <year> 1972. </year>
Reference-contexts: ! 1 if z &lt; 0 ; K v = value: (A.10) 64 The equation of state is: = (T; S; z) ; (A:11) where (T; S; z) is taken to be a 9-term, third order polynomial approximation to the Knudsen formula for the density of seawater, as described in <ref> [BC72] </ref>. A.1.2 Boundary conditions The boundary conditions at the ocean surface (z = 0) are 0 A v @z K v @z and where t and t are the zonal and meridional components of surface stress.
Reference: [Bel81] <author> J. Bell. </author> <title> Report of interview with Bert Semtner re: </title> <journal> Oceanic GCM. </journal> <volume> unpublished, </volume> <month> March </month> <year> 1981. </year>
Reference-contexts: Among the design principles cited by <ref> [Bel81, CS88] </ref> for the design of the original program [Sem74], are the following: small number of operations in each loop; data transfer as if only a small amount of main memory were available; and to avoid if statements within loops to improve the performance of the vector statements.
Reference: [BP87] <author> J. L. Bell and G. S. Patterson Jr. </author> <title> Data organization in large numerical computations. </title> <journal> The journal of supercomputing, </journal> <volume> 1 </volume> <pages> 105-136, </pages> <year> 1987. </year> <month> 74 </month>
Reference-contexts: Bell and Patterson <ref> [BP87] </ref> describe data space partitioning strategies for large numerical codes, with a survey of the approach used in these codes. Their focus is on the logical design of data structures for very efficient transfer among multiple memory levels and multiple processors.
Reference: [Bry63] <author> K. Bryan. </author> <title> A numerical investigation of a nonlinear model of a wind-driven ocean. </title> <journal> Journal of Atmospheric Sciences, </journal> <volume> 20 </volume> <pages> 594-606, </pages> <year> 1963. </year>
Reference-contexts: a history of past efforts in numerical ocean simulation, as presented in [WP86, Sem86b], followed by references to some recent work using vector supercomputers, and finishes with the presentation of the original code design. 3.1 History of past efforts The first global numerical ocean simulation models were developed by Bryan <ref> [Bry63] </ref> in the United States, and by Sarkisyan [Sar66] in the Soviet Union.
Reference: [Bry69] <author> K. Bryan. </author> <title> A numerical method for the study of the circulation of the world ocean. </title> <journal> Journal of Computational Physics, </journal> <volume> 4 </volume> <pages> 347-376, </pages> <year> 1969. </year>
Reference-contexts: These models are all based on the primitive equations model designed by Bryan and Cox <ref> [Bry69] </ref>. Two major reorganizations of the code, were due to Cox [Cox84], suitable for vector machines with long start-up times (e.g. Cyber-205), and Semtner [Sem74], for register-to-register vector architectures of the Cray class. <p> In 1967, the results from the first numerical experiments with primitive equation, three-dimensional ocean model were presented by Bryan and Cox [BC67]; these were followed by a description of the physics and numerics involved in the code by Bryan <ref> [Bry69] </ref>. Although, since that time, ocean models have being developed by several research groups, most of them are derived from the aforementioned original work of Bryan and Cox. <p> The continuous equations of the model are presented in Section A.1, following the description given by [Cox84, WP86], and the finite difference formulation is addressed in Section A.2, according to <ref> [Bry69, Cox84] </ref>. <p> In the absence of dissipative effects, momentum, energy, and the variance of temperature and salinity should be conserved. Arakawa [Ara66] was the first to work with the finite difference formulation, maintaining the integral constraints, and this approach was generalized by Bryan <ref> [Bry69] </ref>, allowing the arrangement of the cells to be chosen in any manner that is convenient for the problem. The first constraint that must be satisfied is the mass conservation within each cell.
Reference: [CHA90] <editor> Building an advanced climate model: </editor> <title> Program plan for the CHAMMP climate modeling program. </title> <type> Technical Report DOE/ER-0479T, U.S. </type> <institution> Dept. of Energy, </institution> <address> Washington, D.C., </address> <month> Dec. </month> <year> 1990. </year>
Reference-contexts: With this new model, they reported an improvement of 70% over their original version. A new generation of computationally more demanding models is under development in context of the CHAMMP activity <ref> [CHA90] </ref>.
Reference: [CKS78] <author> S. C. Chen, D. J. Kuck, and A. H. Sameh. </author> <title> Practical parallel band triangular system solvers. </title> <journal> ACM Trans. on Mathematical Software, </journal> <volume> 4(3) </volume> <pages> 270-277, </pages> <month> Sept., </month> <year> 1978. </year>
Reference-contexts: In our algorithm we replaced loop 3 by the Spike algorithm, that will be shortly described in Section 4.3.3. For a more detailed description of the algorithm see <ref> [CKS78] </ref>. 4.3.2 Initialization issues To save memory space, with the use of the same work space for the three-dimensional phase and the relaxation routine, as described in Section 3.3.3, the major part of the initialization work is repeated at each time step. <p> In our version of the relaxation, the initialization consists only of the computation of the initial guess, that is performed with the use of the two previous solutions, and can be executed using all the clusters available. 4.3.3 Spike algorithm The Spike algorithm was proposed by Chen, Kuck and Sameh <ref> [CKS78] </ref> for the parallel solution of linear recurrence systems, (or banded triangular systems). The algorithm is well suited to recurrences of low order and for machines with a limited number of processors. In this section we present the algorithm for first order recurrence relations.
Reference: [Cox84] <author> M. D. Cox. </author> <title> A primitive equation, 3-dimensional model of the ocean. </title> <type> Technical Report 1, </type> <institution> Geophysical Fluid Dynamics Laboratory/NOAA, Princeton University, Princeton, </institution> <address> NJ 08542, </address> <month> August </month> <year> 1984. </year>
Reference-contexts: These models are all based on the primitive equations model designed by Bryan and Cox [Bry69]. Two major reorganizations of the code, were due to Cox <ref> [Cox84] </ref>, suitable for vector machines with long start-up times (e.g. Cyber-205), and Semtner [Sem74], for register-to-register vector architectures of the Cray class. Recently, after a decade of work on organizing the basic code for the use of vector processing, attention is focusing on the exploitation of parallelism. <p> The Ocean General Circulation Model used in this work is based on a basic model of the Geophysical Fluid Dynamics Laboratory (GFDL) <ref> [Cox84] </ref>, as adapted in the Istituto per lo Studio delle Metodologie Geofisiche Ambientali (IMGA-CNR) to the Mediter-ranean basin geometry [PN88]. The model simulates the basic aspects of large-scale, baroclinic ocean circulation, including treatment of irregular bottom topography. <p> Semtner [Sem74] improved the structure of the code, and added various features to the mathematical formulation, such as hole relaxation [Tak74], for treating the islands. Later, Cox <ref> [Cox84] </ref> further modified Bryan's model to improve the numerical and computational efficiency of the code. <p> The code accepts as input several parameters, such as grid size, number of islands, etc., in order to simulate basins with different sizes and characteristics. The main aspects of the implementation of the code on the Cyber 205 at the Geophysical Fluid Dynamics Laboratory <ref> [Cox84] </ref> are presented in this Section. starting with an overview of the grid representation and the program flow, and finishing with the description of the data management. 1 Following the documentation of the program ([Cox84]), the description in this Chapter is for the out-of-core version. <p> (3:4) This corresponds to data stored by the out-of-core model in the five disk files used by the program. 15 CHAPTER 4 DESIGN AND IMPLEMENTATION OF THE CODE ON CEDAR This chapter presents the design and implementation of a parallel version of the basic Ocean General Circulation Model of GFDL <ref> [Cox84] </ref> on CEDAR. The first section review the CEDAR system, describing the major components of its architecture, the operating systems, and the Cedar Fortran language. For a more detailed description of the Cedar system see [KDLS86, EPY89, Yew86, KTV + 91]. <p> This program belongs to the new generation of computationally more demanding ocean models, and will be an excellent test case for Cedar's understanding and performance evaluation. 62 APPENDIX A MATHEMATICAL FORMULATION OF OCEAN SIMULATION The formulation of Bryan's model will be presented here based on Cox's and Semt-ner's modifications <ref> [Sem74, Cox84, PN88] </ref>. The continuous equations of the model are presented in Section A.1, following the description given by [Cox84, WP86], and the finite difference formulation is addressed in Section A.2, according to [Bry69, Cox84]. <p> The continuous equations of the model are presented in Section A.1, following the description given by <ref> [Cox84, WP86] </ref>, and the finite difference formulation is addressed in Section A.2, according to [Bry69, Cox84]. <p> The continuous equations of the model are presented in Section A.1, following the description given by [Cox84, WP86], and the finite difference formulation is addressed in Section A.2, according to <ref> [Bry69, Cox84] </ref>.
Reference: [Cra85] <author> Cray Research Inc. </author> <title> Multitasking User Guide, </title> <month> January </month> <year> 1985. </year>
Reference-contexts: Vector concurrency, conditional vector statements, and vector reduction functions are also provided by the Cedar Fortran language. Cedar Fortran provides facilities for creating and synchronizing cluster-tasks. Three groups of synchronization routines are provided: doacross loop synchronization; Zhu-Yew synchronization primitives (see [ZY87]); and Cray-Style synchronization operations (see <ref> [Cra85] </ref>). 1 doall loops may perform their iterations in any order and synchronization between iterations is not allowed.
Reference: [CS88] <author> R. M. Chervin and A. J. Semtner Jr. </author> <title> An ocean modelling system for supercomputer architectures of the 1990s. </title> <editor> In M. E. Schlesinger, editor, </editor> <booktitle> Proceedings of the NATO Advanced Research Workshop on Climate-Ocean Interaction, </booktitle> <pages> pages 87-95. </pages> <publisher> Kluwer Academic Publishers., </publisher> <year> 1988. </year>
Reference-contexts: Despite the simplicity 8 of the model, their work is very interesting because its point of view is from the computer science perspective, with a detailed discussion of the parallelization and memory management strategies. Recently, Semtner and Chervin modified Semtner's model <ref> [CS88, Sem86a, SC88] </ref> to combine microtasking and vector processing, thus achieving impressive performance in terms of MFLOPS, on a 4-CPU Cray X-MP in several global ocean circulation experiments. Smith, Dukowicz, and Malone [SDM91] implemented Semtner-Chervin's model on the Connection Machine CM-2, obtaining similar performance to the Cray X-MP. <p> Among the design principles cited by <ref> [Bel81, CS88] </ref> for the design of the original program [Sem74], are the following: small number of operations in each loop; data transfer as if only a small amount of main memory were available; and to avoid if statements within loops to improve the performance of the vector statements. <p> The second problem was solved by keeping the three-dimensional data out of main memory, and bringing to memory just the slabs necessary to perform the computation. The slab partitioning in vertical sections is still used in the recent version of Semtner <ref> [CS88] </ref>. Andrich, Madec et al. [AM88, ADLM88] using the Cray-2, (and its very large memory), have a different approach, with the use of horizontal slabs for all the horizontal 20 operators and vertical slabs for the vertical operators. For each case, each particular processor computes one slab at a time.
Reference: [DGG89] <author> L. De Rose, K. Gallivan, and E. Gallopoulos. </author> <title> Trace analysis of the GFDL ocean circulation model: A preliminary study. </title> <type> Technical Report 863, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <address> Urbana IL 61801, </address> <year> 1989. </year>
Reference-contexts: We also present some comparisons with the same model running on the Cray Y/MP, and on the Alliant FX/80, a vector multiprocessor. A preliminary study of this model running on the Alliant 2 FX/8 can be found in <ref> [DGG89] </ref>. <p> Although we know that its execution time is dependent on the convergence parameter, given as an input, we observed in a preliminary study of this ocean model <ref> [DGG89] </ref>, that the relaxation routine was responsible only for a small percentage of the total execution time. We also noticed that a small number of iterations was in general sufficient to provide the required precision.
Reference: [EHJP90] <author> R. Eigenmann, J. Hoeflinger, G. Jaxon, and D. Padua. </author> <title> Cedar Fortran and its restructuring compiler. </title> <type> Technical Report 1041, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <address> Urbana IL 61801, </address> <year> 1990. </year>
Reference-contexts: A Xylem process consists of one or more independently scheduled program segments, that execute asynchronously across the Cedar system. These program segments are called cluster-tasks. System calls are provided by Xylem for starting and stopping tasks, waiting for tasks to finish, and for inter-task synchronization. The Cedar Fortran language <ref> [Hoe91, GPHL90, EHJP90] </ref> is derived from Alliant's FX/Fortran [All87], which is Fortran 77, with vector constructs such as those proposed for the next Fortran standard (Fortran 90). Cedar Fortran has extensions for memory allocation, concurrency control, multitasking and synchronization.
Reference: [Emr85] <author> P. Emrath. </author> <title> Xylem: an operating system for the Cedar multiprocessor. </title> <journal> IEEE Software, </journal> <volume> 2(4) </volume> <pages> 30-37, </pages> <month> July </month> <year> 1985. </year>
Reference-contexts: These switching networks are 2-stage Omega networks built from 8 fi 8 cross-bar switches. For a more complete discussion of the memory system see [ME87]. The Cedar operating system, Xylem, extends the Alliant Concentrix operating system to include multitasking and virtual memory management of the Cedar memory hierarchy (see <ref> [Emr85] </ref>). A Xylem process consists of one or more independently scheduled program segments, that execute asynchronously across the Cedar system. These program segments are called cluster-tasks. System calls are provided by Xylem for starting and stopping tasks, waiting for tasks to finish, and for inter-task synchronization.
Reference: [EPY89] <author> P. Emrath, D. Padua, and P. Yew. </author> <title> Cedar architecture and its software. </title> <booktitle> In 22nd Hawaii International Conference on System Sciences, </booktitle> <year> 1989. </year>
Reference-contexts: The first section review the CEDAR system, describing the major components of its architecture, the operating systems, and the Cedar Fortran language. For a more detailed description of the Cedar system see <ref> [KDLS86, EPY89, Yew86, KTV + 91] </ref>. Section 4.2 contains the design of Cedar code for the baroclinic phase, it discusses the three grid partitioning strategies considered in our design and addresses the different approaches of placing the data structure, to take advantage of Cedar's hierarchical memory.
Reference: [GJT + 91] <author> K. Gallivan, W. Jalby, S. Turner, A. Veidenbaum, and H . Wijshoff. </author> <title> Preliminary basic performance analysis of the cedar multiproces sor memory systems. </title> <booktitle> Proceedings of ICPP'91, </booktitle> <address> St. Charles, IL, I:71-75, </address> <month> August 12-16, </month> <year> 1991. </year> <month> 75 </month>
Reference-contexts: In the GG version, it is not necessary to exchange messages between processors, because all the data is stored in the shared memory. Nevertheless, access times to this memory are slower. As presented by Gallivan et al. in <ref> [GJT + 91] </ref>, the 28 potential improvement by using the prefetch unit to offset latency may bring the global memory access rate close to the cluster memory access rate. The use of Cedar for the GG version is analogous to machines that have large shared memory such as the Cray-2. <p> This can be confirmed by the results presented on Tables 5.7 and 5.12, that show the best execution times obtained using four clusters with models P 4 L 8 and P 8 L 16 respectively. Gallivan et al. in <ref> [GJT + 91] </ref> predicted that the potential improvement by using the prefetch unit to offset latency may bring the global memory access rate close to the cluster memory access rate. <p> Comparing the best results for the GG version with and without prefetch (Tables 5.10 and 5.11 for model P 4 L 8 , and Tables 5.15 and 5.16 for model P 8 L 16 ), we 2 According to <ref> [GJT + 91] </ref>, the global memory access time is 6 times slower than the cache access time, and by using the prefetch unit to offset latency the global memory access time can be improved by a factor of 3. 54 0 2 Factor of slowdown of global memory Slowdown dotted line
Reference: [GPHL90] <author> M. D. Guzzi, D. A. Padua, J. P. Hoeflinger, and D. H. Lawrie. </author> <title> Cedar Fortran and other vector and parallel Fortran dialects. </title> <journal> Journal of Supercomputing, </journal> <pages> pages 37-62, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: A Xylem process consists of one or more independently scheduled program segments, that execute asynchronously across the Cedar system. These program segments are called cluster-tasks. System calls are provided by Xylem for starting and stopping tasks, waiting for tasks to finish, and for inter-task synchronization. The Cedar Fortran language <ref> [Hoe91, GPHL90, EHJP90] </ref> is derived from Alliant's FX/Fortran [All87], which is Fortran 77, with vector constructs such as those proposed for the next Fortran standard (Fortran 90). Cedar Fortran has extensions for memory allocation, concurrency control, multitasking and synchronization.
Reference: [GS89] <author> E. Gallopoulos and Youcef Saad. </author> <title> Parallel Block Cyclic Reduction Algorithm for the Fast Solutio n of Elliptic Equations. </title> <journal> Parallel Computing, </journal> <volume> 10(2) </volume> <pages> 143-160, </pages> <year> 1989. </year>
Reference-contexts: We note that in the meantime, the method developed by Gallopoulos, Saad and Sweet in <ref> [GS89, Swe88] </ref> allow now the effective parallelization of block cyclic reduction, and could be used instead.
Reference: [Hoe91] <author> Jay Hoeflinger. </author> <title> Cedar fortran programmer's handbook. </title> <type> Technical report, </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Superco mputing Res. & Dev., </institution> <month> October </month> <year> 1991. </year> <note> CSRD Report No. 1157. </note>
Reference-contexts: A Xylem process consists of one or more independently scheduled program segments, that execute asynchronously across the Cedar system. These program segments are called cluster-tasks. System calls are provided by Xylem for starting and stopping tasks, waiting for tasks to finish, and for inter-task synchronization. The Cedar Fortran language <ref> [Hoe91, GPHL90, EHJP90] </ref> is derived from Alliant's FX/Fortran [All87], which is Fortran 77, with vector constructs such as those proposed for the next Fortran standard (Fortran 90). Cedar Fortran has extensions for memory allocation, concurrency control, multitasking and synchronization.
Reference: [Kas77] <author> A. Kasahara. </author> <title> Computational aspects of numerical models for weather prediction and climate simulation. </title> <booktitle> Methods in Computational Physics, </booktitle> <volume> 17 </volume> <pages> 2-66, </pages> <year> 1977. </year>
Reference: [KDLS86] <author> D. J. Kuck, E. S. Davidson, D. H. Lawrie, and A. H. Sameh. </author> <title> Parallel supercomputing today and the Cedar approach. </title> <journal> Science, </journal> <volume> 231 </volume> <pages> 967-974, </pages> <month> February </month> <year> 1986. </year>
Reference-contexts: The first section review the CEDAR system, describing the major components of its architecture, the operating systems, and the Cedar Fortran language. For a more detailed description of the Cedar system see <ref> [KDLS86, EPY89, Yew86, KTV + 91] </ref>. Section 4.2 contains the design of Cedar code for the baroclinic phase, it discusses the three grid partitioning strategies considered in our design and addresses the different approaches of placing the data structure, to take advantage of Cedar's hierarchical memory.
Reference: [KKP + 81] <author> D. J. Kuck, R. H. Kuhn, D. A. Padua, B. Leasure, and M . Wolfe. </author> <title> Dependence graphs and compiler optimizations. </title> <booktitle> Proc. of the 8th ACM Symp. on Principles of Programming Languages (POPL), </booktitle> <pages> pages 207-218, </pages> <month> Jan., </month> <year> 1981. </year>
Reference-contexts: To simplify the discussion, this description ignores the land issue; It should be remembered however that the original program has conditional code within the loops to avoid computations over land areas. The data dependence analysis <ref> [Ban76, KKP + 81, PW86] </ref> for the loops in this algorithm shows that loops 1 and 5 can be executed as vector concurrent loops, as long as loop 1 precedes loop 5. The problems for parallelization occur with loops 2, 3 and 4.
Reference: [KTV + 91] <author> J. Konicek, T. Tilton, A. Veidenbaum, C. Zhu, E. Davidson, R. Downing, M. Haney, M. Sharma, P. Yew, P. Farmwald, D. Kuck, D. Lavery, R. Lind-sey, D. Pointer, J. Andrews, T. Beck, T. Murphy, S. Turner, and N. Warter. </author> <title> The organization of the Cedar system. </title> <booktitle> In 1991 Int'l Conference on Parallel processing, </booktitle> <year> 1991. </year>
Reference-contexts: The first section review the CEDAR system, describing the major components of its architecture, the operating systems, and the Cedar Fortran language. For a more detailed description of the Cedar system see <ref> [KDLS86, EPY89, Yew86, KTV + 91] </ref>. Section 4.2 contains the design of Cedar code for the baroclinic phase, it discusses the three grid partitioning strategies considered in our design and addresses the different approaches of placing the data structure, to take advantage of Cedar's hierarchical memory. <p> For testing purposes, this feature can be set off with the use of compiler flags. For more details on the prefetch unit see <ref> [KTV + 91] </ref>. The processors and the global memory are connected via the global interconnection network, that consists of two unidirectional packet-switched networks. These switching networks are 2-stage Omega networks built from 8 fi 8 cross-bar switches. For a more complete discussion of the memory system see [ME87].
Reference: [Kuc87] <author> Kuck and Associates, Inc., Savoy, </author> <title> IL 61874. KAP User's Guide, </title> <booktitle> 4th edition, </booktitle> <year> 1987. </year>
Reference-contexts: Some of these phases were related to the history of Cedar's development. The first step was the conversion of the original code to one cluster Cedar Fortran. This task was done using the KAP <ref> [Kuc87] </ref> source-to-source restructuring program. All subsequent multi-cluster versions were hand written using the one cluster KAP output as a guide.
Reference: [Mal87] <author> A. D. Malony. </author> <title> High resolution process timing user's manual. </title> <type> Technical report, </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Supercomputing Res. & Dev., </institution> <month> June </month> <year> 1987. </year> <note> CSRD Report No. 676. </note>
Reference-contexts: Timings on the Alliant FX/80 are CPU times, also collected with etime, but in multiuser mode. The routines hrcget and hrcdelta are part of a high-resolution timing facility (hrtime) that has been implemented for the Cedar System <ref> [Mal87] </ref>. Hrtime is an extension of the Concentrix user and system process time measurements; it times both execution and non-execution process states with 10 sec accuracy. The Alliant Fortran library routine etime returns the elapsed cpu time, also with 10 sec accuracy (see [All87]).
Reference: [ME87] <author> R. McGrath and P. Emrath. </author> <title> Using memory in the Cedar system. </title> <type> Technical Report 655, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <address> Urbana IL 61801, </address> <year> 1987. </year>
Reference-contexts: The processors and the global memory are connected via the global interconnection network, that consists of two unidirectional packet-switched networks. These switching networks are 2-stage Omega networks built from 8 fi 8 cross-bar switches. For a more complete discussion of the memory system see <ref> [ME87] </ref>. The Cedar operating system, Xylem, extends the Alliant Concentrix operating system to include multitasking and virtual memory management of the Cedar memory hierarchy (see [Emr85]). A Xylem process consists of one or more independently scheduled program segments, that execute asynchronously across the Cedar system.
Reference: [NW87] <author> D. M. Nicol and F. H. Willard. </author> <title> Problem size, parallel architecture, and optimal speedup. </title> <type> Technical Report 87-7, </type> <institution> ICASE Institute for Computer Applications in Science and Engineering, </institution> <month> April </month> <year> 1987. </year> <month> 76 </month>
Reference-contexts: They studied the effects of different partitioning schemes and discretization stencils on interprocessor communication, for both distributed and shared memory architectures. A followup study, using Reed et al. as a framework was done by Nicol and Willard <ref> [NW87] </ref>. They studied the relationship between problem size and architecture, and analytically quantified the relationship between stencil type, partitioning scheme, grid size, execution time and type of communication network. They showed that optimal performance is not always achieved by using all processors available.
Reference: [PDR90] <author> R. C. Pacanowski, K. Dixon, and A. Rosati. GFDL MOM 1.0. </author> <title> Geophysical Fluid Dynamics Laboratory/NOAA, </title> <month> December </month> <year> 1990. </year>
Reference-contexts: A new generation of computationally more demanding models is under development in context of the CHAMMP activity [CHA90]. Another model that considers parameterizations, especially surface energy balance and vertical turbulent mixing, was recently developed by Pacanowski, Dixon, and Rosati <ref> [PDR90] </ref> at the Geophysical Fluid Dynamics Laboratory, for the Cray Y/MP. 3.3 Original code design The original program was first written in Fortran for the IBM 360/91, and was subsequently redesigned for vector computers. <p> The main reason for this approach was because in order to obtain long vectors and thus high performance on vector computers with large startup time for their vector pipelines. Therefore it was 2 At that time, the new GFDL MOM code <ref> [PDR90] </ref> was not yet available. 19 more effective to perform the computation using a large vector containing land points than to use several small vectors containing only the water points. <p> Hence, with the advent of vector supercomputers that have very fast startup time for their pipelines, the considerations about land points became an important issue, and in some of the new models <ref> [PDR90] </ref> this issue is already being considered. In the multiprocessor case, the avoidance of computation over land areas will generate problems of theoretical and practical importance such as new partitioning schemes, load balance, and scheduling.
Reference: [PN88] <author> N. Pinardi and A. Navarra. </author> <title> A brief review of global mediterranean wind-driven general circulation experiments. </title> <type> Technical Report 132, </type> <institution> IMGA-CNR, Modena Italy, </institution> <year> 1988. </year>
Reference-contexts: The Ocean General Circulation Model used in this work is based on a basic model of the Geophysical Fluid Dynamics Laboratory (GFDL) [Cox84], as adapted in the Istituto per lo Studio delle Metodologie Geofisiche Ambientali (IMGA-CNR) to the Mediter-ranean basin geometry <ref> [PN88] </ref>. The model simulates the basic aspects of large-scale, baroclinic ocean circulation, including treatment of irregular bottom topography. It is used in climate studies and also to study the development of mid-ocean eddies. Temperature, salinity and the prediction of currents are the main physical phenomena of interest. <p> Later, Cox [Cox84] further modified Bryan's model to improve the numerical and computational efficiency of the code. Pinardi and Navarra adapted Cox's program to the Mediterranean basin geometry <ref> [PN88] </ref>. 3.2 Efforts using parallel supercomputers Andrich, Madec et al. in [AM88, ADLM88] describe a code written for the Cray 2 applied to sections of the Atlantic and of the Mediterranean. <p> This program belongs to the new generation of computationally more demanding ocean models, and will be an excellent test case for Cedar's understanding and performance evaluation. 62 APPENDIX A MATHEMATICAL FORMULATION OF OCEAN SIMULATION The formulation of Bryan's model will be presented here based on Cox's and Semt-ner's modifications <ref> [Sem74, Cox84, PN88] </ref>. The continuous equations of the model are presented in Section A.1, following the description given by [Cox84, WP86], and the finite difference formulation is addressed in Section A.2, according to [Bry69, Cox84].
Reference: [PW86] <author> D. Padua and M. Wolfe. </author> <title> Advanced compiler optimization for supercomputers. </title> <journal> CACM, </journal> <volume> 29(12) </volume> <pages> 1184-1201, </pages> <month> December, </month> <year> 1986. </year>
Reference-contexts: To simplify the discussion, this description ignores the land issue; It should be remembered however that the original program has conditional code within the loops to avoid computations over land areas. The data dependence analysis <ref> [Ban76, KKP + 81, PW86] </ref> for the loops in this algorithm shows that loops 1 and 5 can be executed as vector concurrent loops, as long as loop 1 precedes loop 5. The problems for parallelization occur with loops 2, 3 and 4.
Reference: [RAP87] <author> D. A. Reed, L. M. Adams, and M. L. Patrick. </author> <title> Stencils and problem partition-ings: Their influence on the performance of multiple processor systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36:845-858, </volume> <month> July </month> <year> 1987. </year>
Reference-contexts: Thune [Thu90] discusses the use of slices and rectangles as partitioning shapes for the case of explicit difference methods on two-dimensional problems, on MIMD computers with distributed memory. Also for multiple processor systems, Reed, Adams, and Patrick <ref> [RAP87] </ref> use the stencil structure to choose the appropriate partitioning shape. Berger and Bokhari [BB87] consider partitioning strategies that balance the workload for problems on a domain with nonuniform work estimates. Slab partitioning in terms of independent longitude/depth (vertical) sections was introduced in ocean simulation codes for Bryan's model. <p> Much work has been done to analyze communication overhead in parallel architectures, especially for the use of explicit difference methods on regular grids. A detailed study of the relationships between stencils, partitioning schemes, architecture, and data structure management was done by Reed, Adams and Patrick <ref> [RAP87] </ref>. They studied the effects of different partitioning schemes and discretization stencils on interprocessor communication, for both distributed and shared memory architectures. A followup study, using Reed et al. as a framework was done by Nicol and Willard [NW87].
Reference: [RM67] <author> R. D. Richtmyer and K. W. Morton. </author> <title> Difference methods for initial value problems. </title> <type> Interscience, </type> <note> second edition, </note> <year> 1967. </year>
Reference-contexts: a max (H) -fl = ^v cos a max (H) and w v is given by ffi z (w v ) = a cos along with the boundary condition w v The frictional terms (F ufl and F vfl ), are delayed by one time step to avoid numerical instability <ref> [RM67] </ref>, and are written as F u fl A h " u N1 + 1 tan 2 u N1 cos 2 # F v fl A h " v N1 + 1 tan 2 v N1 cos 2 # where r 2 fl 1 ffi (cos ffi ) : (A:64) Equation
Reference: [Sar66] <author> A. S. Sarkisyan. </author> <title> Osnovy teorii i raschet okeanicheskyky techeny (Fundamentals of the theory and calculation of ocean currents). </title> <address> Gidrometeoizdat, Moscow, </address> <year> 1966. </year>
Reference-contexts: simulation, as presented in [WP86, Sem86b], followed by references to some recent work using vector supercomputers, and finishes with the presentation of the original code design. 3.1 History of past efforts The first global numerical ocean simulation models were developed by Bryan [Bry63] in the United States, and by Sarkisyan <ref> [Sar66] </ref> in the Soviet Union. In 1967, the results from the first numerical experiments with primitive equation, three-dimensional ocean model were presented by Bryan and Cox [BC67]; these were followed by a description of the physics and numerics involved in the code by Bryan [Bry69].
Reference: [SC88] <author> A. J. Semtner Jr. and R. M. Chervin. </author> <title> A simulation of the global ocean circulation with resolved eddies. </title> <journal> Journal of Geophysical Research, </journal> <volume> 93(C12):15502-15522, </volume> <year> 1988. </year>
Reference-contexts: Despite the simplicity 8 of the model, their work is very interesting because its point of view is from the computer science perspective, with a detailed discussion of the parallelization and memory management strategies. Recently, Semtner and Chervin modified Semtner's model <ref> [CS88, Sem86a, SC88] </ref> to combine microtasking and vector processing, thus achieving impressive performance in terms of MFLOPS, on a 4-CPU Cray X-MP in several global ocean circulation experiments. Smith, Dukowicz, and Malone [SDM91] implemented Semtner-Chervin's model on the Connection Machine CM-2, obtaining similar performance to the Cray X-MP.
Reference: [SDM91] <author> R. D. Smith, J. K. Dukowicz, and R. C. Malone. </author> <title> Massively parallel global ocean modeling. </title> <type> Technical Report LA-UR-91-2583, </type> <institution> Los Alamos National Laboratory, </institution> <address> Los Alamos, New Mexico 87545, </address> <year> 1991. </year>
Reference-contexts: Recently, Semtner and Chervin modified Semtner's model [CS88, Sem86a, SC88] to combine microtasking and vector processing, thus achieving impressive performance in terms of MFLOPS, on a 4-CPU Cray X-MP in several global ocean circulation experiments. Smith, Dukowicz, and Malone <ref> [SDM91] </ref> implemented Semtner-Chervin's model on the Connection Machine CM-2, obtaining similar performance to the Cray X-MP. They also developed and implemented a new formulation of the barotropic equations which involves the surface pressure field rather than the stream function, and used a parallel preconditioned conjugate-gradient method. <p> The code used by Singh and Hennessy in [SH89] was based on the software package FISHPAK [SS75] 8 for the solution of the two-dimensional Poisson equation. Smith, Dukowicz, and Malone <ref> [SDM91] </ref>, on their implementation on the Connection Machine, also focused on the barotropic equations, because approximately two-thirds of the total execution time on their code was being spent on the SOR.
Reference: [Sem74] <author> A. J. Semtner Jr. </author> <title> An oceanic general circulation model with botton topography. </title> <type> Technical Report 9, </type> <institution> UCLA Dept. of Metheorology, </institution> <year> 1974. </year>
Reference-contexts: These models are all based on the primitive equations model designed by Bryan and Cox [Bry69]. Two major reorganizations of the code, were due to Cox [Cox84], suitable for vector machines with long start-up times (e.g. Cyber-205), and Semtner <ref> [Sem74] </ref>, for register-to-register vector architectures of the Cray class. Recently, after a decade of work on organizing the basic code for the use of vector processing, attention is focusing on the exploitation of parallelism. <p> Although, since that time, ocean models have being developed by several research groups, most of them are derived from the aforementioned original work of Bryan and Cox. Semtner <ref> [Sem74] </ref> improved the structure of the code, and added various features to the mathematical formulation, such as hole relaxation [Tak74], for treating the islands. Later, Cox [Cox84] further modified Bryan's model to improve the numerical and computational efficiency of the code. <p> Among the design principles cited by [Bel81, CS88] for the design of the original program <ref> [Sem74] </ref>, are the following: small number of operations in each loop; data transfer as if only a small amount of main memory were available; and to avoid if statements within loops to improve the performance of the vector statements. <p> This program belongs to the new generation of computationally more demanding ocean models, and will be an excellent test case for Cedar's understanding and performance evaluation. 62 APPENDIX A MATHEMATICAL FORMULATION OF OCEAN SIMULATION The formulation of Bryan's model will be presented here based on Cox's and Semt-ner's modifications <ref> [Sem74, Cox84, PN88] </ref>. The continuous equations of the model are presented in Section A.1, following the description given by [Cox84, WP86], and the finite difference formulation is addressed in Section A.2, according to [Bry69, Cox84].
Reference: [Sem86a] <author> A. J. Semtner Jr. </author> <title> Finite-diferrence formulation of a world ocean model. </title> <editor> In J. J. O'Brien, editor, </editor> <booktitle> Proc. NATO Institute of Advanced Physical Oceanographic Numerical Modelling., </booktitle> <pages> pages 187-202. </pages> <address> D. </address> <publisher> Reidel Publishing Co., </publisher> <year> 1986. </year>
Reference-contexts: Despite the simplicity 8 of the model, their work is very interesting because its point of view is from the computer science perspective, with a detailed discussion of the parallelization and memory management strategies. Recently, Semtner and Chervin modified Semtner's model <ref> [CS88, Sem86a, SC88] </ref> to combine microtasking and vector processing, thus achieving impressive performance in terms of MFLOPS, on a 4-CPU Cray X-MP in several global ocean circulation experiments. Smith, Dukowicz, and Malone [SDM91] implemented Semtner-Chervin's model on the Connection Machine CM-2, obtaining similar performance to the Cray X-MP.
Reference: [Sem86b] <author> A. J. Semtner Jr. </author> <title> History and methodology of modelling the circulation of the world ocean. </title> <editor> In J. J. O'Brien, editor, </editor> <booktitle> Proc. NATO Institute of Advanced Physical Oceanographic Numerical Modelling., </booktitle> <pages> pages 23-32. </pages> <address> D. </address> <publisher> Reidel Publishing Co., </publisher> <year> 1986. </year>
Reference-contexts: values for each vertical line, and the barotropic, that consists of the solution of the resulting two-dimensional Poisson equation for the mass transport stream function. 7 CHAPTER 3 OCEAN MODELING HISTORY AND ORIGINAL CODE DESIGN This chapter reviews a history of past efforts in numerical ocean simulation, as presented in <ref> [WP86, Sem86b] </ref>, followed by references to some recent work using vector supercomputers, and finishes with the presentation of the original code design. 3.1 History of past efforts The first global numerical ocean simulation models were developed by Bryan [Bry63] in the United States, and by Sarkisyan [Sar66] in the Soviet Union.
Reference: [SH89] <author> J. P. Singh and J. L. Hennessy. </author> <title> Parallelizing the simulation of ocean eddy currents. </title> <type> Technical Report CSL-TR-89-388, </type> <institution> Computer Systems Laboratory, Stanford University, Stanford, </institution> <address> CA 94305-4055, </address> <month> August </month> <year> 1989. </year> <month> 77 </month>
Reference-contexts: An interesting aspect of their work is their emphasis on efficient elliptic solvers for the two-dimensional mass transport stream function. Singh and Hennessy in <ref> [SH89] </ref> discuss the parallel simulation of a two-level quasi-geotrophic model on a sixteen processor Encore MULTIMAX. Despite the simplicity 8 of the model, their work is very interesting because its point of view is from the computer science perspective, with a detailed discussion of the parallelization and memory management strategies. <p> Andrich, Madec 29 et al. in [AM88, ADLM88] implemented a parallel SOR, and a parallel conjugate gradient method for the Cray 2 with four CPUs, obtaining better performance for the latter method. The code used by Singh and Hennessy in <ref> [SH89] </ref> was based on the software package FISHPAK [SS75] 8 for the solution of the two-dimensional Poisson equation.
Reference: [SS75] <author> P. Swarztrauber and R. Sweet. </author> <title> Efficient fortran subprograms for the solution of elliptic partial differential equations. </title> <type> Technical Report TN/IA-109, NCAR, </type> <institution> National Center for Atmospheric Research, </institution> <month> July </month> <year> 1975. </year>
Reference-contexts: Andrich, Madec 29 et al. in [AM88, ADLM88] implemented a parallel SOR, and a parallel conjugate gradient method for the Cray 2 with four CPUs, obtaining better performance for the latter method. The code used by Singh and Hennessy in [SH89] was based on the software package FISHPAK <ref> [SS75] </ref> 8 for the solution of the two-dimensional Poisson equation. Smith, Dukowicz, and Malone [SDM91], on their implementation on the Connection Machine, also focused on the barotropic equations, because approximately two-thirds of the total execution time on their code was being spent on the SOR.
Reference: [Swe88] <author> R. A. Sweet. </author> <title> A parallel and vector cyclic reduction algorithm. </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> 9(4) </volume> <pages> 761-765, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: We note that in the meantime, the method developed by Gallopoulos, Saad and Sweet in <ref> [GS89, Swe88] </ref> allow now the effective parallelization of block cyclic reduction, and could be used instead.
Reference: [Tak74] <author> K. Takano. </author> <title> A general circulation model for the world ocean. </title> <type> Technical Report 8, </type> <institution> UCLA Dept. of Metheorology, </institution> <year> 1974. </year>
Reference-contexts: Although, since that time, ocean models have being developed by several research groups, most of them are derived from the aforementioned original work of Bryan and Cox. Semtner [Sem74] improved the structure of the code, and added various features to the mathematical formulation, such as hole relaxation <ref> [Tak74] </ref>, for treating the islands. Later, Cox [Cox84] further modified Bryan's model to improve the numerical and computational efficiency of the code. <p> The hole relaxation <ref> [Tak74] </ref>, used for the treatment of the islands, is executed for each island at the end of each SOR iteration.
Reference: [Thu90] <author> Michael Thune. </author> <title> A partitioning strategy for explicit difference methods. </title> <journal> Parallel Computing, </journal> <volume> 15 </volume> <pages> 147-154, </pages> <year> 1990. </year>
Reference-contexts: Bell and Patterson [BP87] describe data space partitioning strategies for large numerical codes, with a survey of the approach used in these codes. Their focus is on the logical design of data structures for very efficient transfer among multiple memory levels and multiple processors. Thune <ref> [Thu90] </ref> discusses the use of slices and rectangles as partitioning shapes for the case of explicit difference methods on two-dimensional problems, on MIMD computers with distributed memory. Also for multiple processor systems, Reed, Adams, and Patrick [RAP87] use the stencil structure to choose the appropriate partitioning shape.
Reference: [WP86] <author> W. M. Washington and C. L. Parkinson. </author> <title> An Introduction to Three-Dimensional Climate Modeling. </title> <publisher> University Science Books, </publisher> <year> 1986. </year>
Reference-contexts: values for each vertical line, and the barotropic, that consists of the solution of the resulting two-dimensional Poisson equation for the mass transport stream function. 7 CHAPTER 3 OCEAN MODELING HISTORY AND ORIGINAL CODE DESIGN This chapter reviews a history of past efforts in numerical ocean simulation, as presented in <ref> [WP86, Sem86b] </ref>, followed by references to some recent work using vector supercomputers, and finishes with the presentation of the original code design. 3.1 History of past efforts The first global numerical ocean simulation models were developed by Bryan [Bry63] in the United States, and by Sarkisyan [Sar66] in the Soviet Union. <p> The continuous equations of the model are presented in Section A.1, following the description given by <ref> [Cox84, WP86] </ref>, and the finite difference formulation is addressed in Section A.2, according to [Bry69, Cox84].
Reference: [Yew86] <author> P. C. Yew. </author> <title> Architecture of the Cedar parallel supercomputer. </title> <type> Technical Report 609, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <address> Urbana IL 61801, </address> <year> 1986. </year>
Reference-contexts: The first section review the CEDAR system, describing the major components of its architecture, the operating systems, and the Cedar Fortran language. For a more detailed description of the Cedar system see <ref> [KDLS86, EPY89, Yew86, KTV + 91] </ref>. Section 4.2 contains the design of Cedar code for the baroclinic phase, it discusses the three grid partitioning strategies considered in our design and addresses the different approaches of placing the data structure, to take advantage of Cedar's hierarchical memory.
Reference: [ZY87] <author> C. Zhu and P. Yew. </author> <title> A scheme to enforce data dependence on large multiprocessor systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-13(6):726-739, </volume> <month> June </month> <year> 1987. </year>
Reference-contexts: Vector concurrency, conditional vector statements, and vector reduction functions are also provided by the Cedar Fortran language. Cedar Fortran provides facilities for creating and synchronizing cluster-tasks. Three groups of synchronization routines are provided: doacross loop synchronization; Zhu-Yew synchronization primitives (see <ref> [ZY87] </ref>); and Cray-Style synchronization operations (see [Cra85]). 1 doall loops may perform their iterations in any order and synchronization between iterations is not allowed. <p> Also, the program was written in a modular way, that it is straightforward to switch to another synchronization mechanism. To construct the busy waiting loops, we used the atomic functions provided by the Zhu-Yew synchronization primitives (see <ref> [ZY87] </ref>). 36 update update update . . . . . - init. . . . . . yes no convergence? . . . . . relax hole relaxhole relax hole relax spike comp res. comp res.comp res. init.init. 37 - - J JJ vort. vort. tracer tracer clinicclinic bootboot rowcrowc r
References-found: 52

