URL: ftp://ftp.idsia.ch/pub/juergen/ml_levin_eira.ps.gz
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00080.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: marco@idsia.ch  juergen@idsia.ch  
Title: Solving POMDPs with Levin Search and EIRA  
Author: Marco Wiering Jurgen Schmidhuber 
Address: Corso Elvezia 36 CH-6900-Lugano (Switzerland)  Corso Elvezia 36 CH-6900-Lugano (Switzerland)  
Affiliation: IDSIA  IDSIA  
Date: 1996  
Note: To appear in Machine Learning: Proceedings of the 13th international conference,  
Abstract: Partially observable Markov decision problems (POMDPs) recently received a lot of attention in the reinforcement learning community. No attention, however, has been paid to Levin's universal search through program space (LS), which is theoretically optimal for a wide variety of search problems including many POMDPs. Experiments in this paper first show that LS can solve partially observable mazes (POMs) involving many more states and obstacles than those solved by various previous authors (here, LS also can easily outperform Q-learning). We then note, however, that LS is not necessarily optimal for "incremental" learning problems where experience with previous problems may help to reduce future search costs. For this reason, we introduce an adaptive extension of LS (ALS) which uses experience to increase probabilities of instructions occurring in successful programs found by LS. To deal with cases where ALS does not lead to long term performance improvement, we use the recent technique of "environment-independent reinforcement acceleration" (EIRA) as a safety belt (EIRA currently is the only known method that guarantees a lifelong history of reward accelerations). Experiments with additional POMs demonstrate: (a) ALS can dramatically reduce the search time consumed by successive calls of LS. (b) Additional significant speed-ups can be obtained by combining ALS and EIRA. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Cliff, D. and Ross, S. </author> <year> (1994). </year> <title> Adding temporary memory to ZCS. </title> <booktitle> Adaptive Behavior, </booktitle> <volume> 3 </volume> <pages> 101-150. </pages>
Reference-contexts: Indeed, in separate POMDP experiments we were already able to show that EIRA can improve standard Q-learning's performance (recall that POMDP applications of Q-learning are not theoretically sound, although many authors do apply Q-variants to POMDPs). Another interesting application area may be the field of bucket-brigade based classifier systems: <ref> (Cliff and Ross, 1994) </ref> show that such systems tend to be unstable and forget good solutions. Here EIRA could unfold its safety belt effect. Acknowledgements Thanks for valuable discussions to Rafa l Sa lustowicz, Sepp Hochreiter, and Jieyu Zhao (supported by SNF grant 21-43'417.95 "incremental self-improvement").
Reference: <author> Dickmanns, D., Schmidhuber, J., and Winklhofer, A. </author> <year> (1987). </year> <title> Der genetische Algorithmus: Eine Im-plementierung in Prolog. </title> <institution> Fortgeschrittenenprak-tikum, Institut fur Informatik, Lehrstuhl Prof. Radig, Technische Universitat Munchen. </institution>
Reference-contexts: Since LS has a principled way of dealing with non-halting programs and time-limits (unlike, e.g., "Genetic Programming"(GP)), LS may also be of interest for researchers working in GP and related fields (the first paper on using GP-like algorithms to evolve assembler-like computer programs was, to the best of our knowledge, <ref> (Dickmanns et al., 1987) </ref>). ALS: single tasks versus multiple tasks.
Reference: <author> Jaakkola, T., Singh, S. P., and Jordan, M. I. </author> <year> (1995). </year> <title> Reinforcement learning algorithm for partially observable markov decision problems. </title> <editor> In Tesauro, G., Touretzky, D. S., and Leen, T. K., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7, to appear. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge MA. </address>
Reference: <author> Kaelbling, L. </author> <year> (1993). </year> <title> Learning in Embedded Systems. </title> <publisher> MIT Press. </publisher>
Reference-contexts: Outline of paper. Section 2 describes LS details. Section 3 presents the heuristic adaptation method (ALS | a simple, adaptive, incremental extension of LS related to the linear reward-inaction algorithm, e.g., <ref> (Kaelbling, 1993) </ref>). Section 4 briefly reviews EIRA and shows how to combine it with ALS.
Reference: <author> Kaelbling, L., Littman, M., and Cassandra, A. </author> <year> (1995). </year> <title> Planning and acting in partially observable stochastic domains. </title> <type> Technical report, </type> <institution> Brown University, Providence RI. </institution>
Reference: <author> Levin, L. A. </author> <year> (1973). </year> <title> Universal sequential search problems. </title> <journal> Problems of Information Transmission, </journal> <volume> 9(3) </volume> <pages> 265-266. </pages>
Reference: <author> Levin, L. A. </author> <year> (1984). </year> <title> Randomness conservation inequalities: Information and independence in mathematical theories. </title> <journal> Information and Control, </journal> <volume> 61 </volume> <pages> 15-37. </pages>
Reference: <author> Li, M. and Vitanyi, P. M. B. </author> <year> (1993). </year> <title> An Introduction to Kolmogorov Complexity and its Applications. </title> <publisher> Springer. </publisher>
Reference-contexts: For instance, suppose there is an algorithm that solves a certain type of maze task in O (n 3 ) steps, where n is a positive integer representing the problem size. Then universal LS will solve the same task in at most O (n 3 ) steps. See <ref> (Li and Vitanyi, 1993) </ref> for an overview. See (Schmidhuber, 1995b) for recent implementations/applications. Search through program space is relevant for "POMDPs". LS is a smart way of performing exhaustive search by "optimally" allocating time to programs computing solution candidates (details in section 2). <p> Outline of paper. Section 2 describes LS details. Section 3 presents the heuristic adaptation method (ALS | a simple, adaptive, incremental extension of LS related to the linear reward-inaction algorithm, e.g., <ref> (Kaelbling, 1993) </ref>). Section 4 briefly reviews EIRA and shows how to combine it with ALS. <p> in order of their Levin complexities Kt (s) = min q flogP M (q) + log t (q; s)g, where q stands for a program that computes s in t (q; s) time steps, and P M (q) is the probability of guessing q according to a fixed Solomonoff-Levin distribution <ref> (Li and Vitanyi, 1993) </ref> on the set of possible programs (in section 3, however, we will make the distribution variable). Optimality. <p> The probability adjustment is controlled by a learning rate fl (0 &lt; fl &lt; 1). ALS is related to the linear reward-inaction algorithm (e.g., <ref> (Kael-bling, 1993) </ref>) | the main difference is: ALS uses LS to search through program space as opposed to single action space. As in section 2, the probability distribution P M is determined by M . Initially, all M ij = 1 r .
Reference: <author> Littman, M. </author> <year> (1994). </year> <title> Memoryless policies: Theoretical limitations and practical results. </title> <editor> In D. Cliff, P. Husbands, J. A. M. and Wilson, S. W., editors, </editor> <booktitle> Proc. of the International Conference on Simulation of Adaptive Behavior: From Animals to Animats 3, </booktitle> <pages> pages 297-305. </pages> <publisher> MIT Press/Bradford Books. </publisher>
Reference: <author> McCallum, R. A. </author> <year> (1993). </year> <title> Overcoming incomplete perception with utile distinction memory. </title> <booktitle> In Machine Learning: Proceedings of the Tenth International Conference. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Amherst, MA. </address>
Reference: <author> McCallum, R. A. </author> <year> (1995). </year> <title> Instance-based utile distinctions for reinforcement learning with hidden state. </title> <editor> In Prieditis, A. and Russell, S., editors, </editor> <booktitle> Machine Learning: Proceedings of the Twelfth International Conference, </booktitle> <pages> pages 387-395. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Francisco, CA. </address>
Reference-contexts: Figure 1 shows a 39fi38-maze with a single start position (S) and a single goal position (G). The maze has many more fields and obstacles than mazes used by previous authors working on POMDPs (for instance, McCallum's maze has only 23 free fields <ref> (McCallum, 1995) </ref>). The goal is to find a program that makes an agent move from S to G. Instructions. Programs can be composed from 9 primitive instructions. These instructions represent the initial bias provided by the programmer (in what follows, superscripts will indicate instruction numbers).
Reference: <author> Ring, M. B. </author> <year> (1994). </year> <title> Continual Learning in Reinforcement Environments. </title> <type> PhD thesis, </type> <institution> University of Texas at Austin, Austin, Texas 78712. </institution>
Reference: <author> Schmidhuber, J. </author> <year> (1995a). </year> <title> Environment-independent reinforcement acceleration. </title> <type> Technical Note IDSIA-59-95, </type> <institution> IDSIA. Invited talk at Hongkong University of Science and Technology. </institution>
Reference-contexts: Theoretical soundness. Using induction, it can be shown that this backtracking procedure ensures that RAC holds after each popping process <ref> (Schmidhuber, 1995a) </ref>. At any given time, EIRA's straight-forward generalization assumption is: modifications that survived the most recent popping process will remain useful.
Reference: <author> Schmidhuber, J. </author> <year> (1996). </year> <title> A general method for incremental self-improvement and multi-agent learning in unrestricted environments. </title> <editor> In Yao, X., editor, </editor> <booktitle> Evolutionary Computation: Theory and Applications. </booktitle> <publisher> Scientific Publ. Co., Singapore. </publisher>
Reference-contexts: None of these, however, can guarantee that the lifelong history of probability modifications will correspond to a lifelong history of reinforcement accelerations. EIRA. The problem above has been addressed recently <ref> (Schmidhuber, 1996) </ref>.
Reference: <author> Schmidhuber, J. H. </author> <year> (1995b). </year> <title> Discovering solutions with low Kolmogorov complexity and high generalization capability. </title> <editor> In Prieditis, A. and Rus-sell, S., editors, </editor> <booktitle> Machine Learning: Proceedings of the Twelfth International Conference, </booktitle> <pages> pages 488-496. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Fran-cisco, CA. </address>
Reference-contexts: Then universal LS will solve the same task in at most O (n 3 ) steps. See (Li and Vitanyi, 1993) for an overview. See <ref> (Schmidhuber, 1995b) </ref> for recent implementations/applications. Search through program space is relevant for "POMDPs". LS is a smart way of performing exhaustive search by "optimally" allocating time to programs computing solution candidates (details in section 2).
Reference: <author> Solomonoff, R. </author> <year> (1986). </year> <title> An application of algorithmic probability to problems in artificial intelligence. </title> <editor> In Kanal, L. N. and Lemmer, J. F., editors, </editor> <booktitle> Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 473-491. </pages> <publisher> Elsevier Science Publishers. </publisher>
Reference: <author> Watanabe, O. </author> <year> (1992). </year> <title> Kolmogorov complexity and computational complexity. </title> <booktitle> EATCS Monographs on Theoretical Computer Science, </booktitle> <publisher> Springer. </publisher>
Reference-contexts: Still, until recently LS has not received much attention except in purely theoretical studies | see, e.g., <ref> (Watanabe, 1992) </ref>. Practical implementation. In our practical LS version, there is an upper bound k on program length (due to obvious storage limitations). a i denotes the address of the i-th instruction.
Reference: <author> Watkins, C. J. C. H. and Dayan, P. </author> <year> (1992). </year> <title> Q-learning. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 279-292. </pages>
Reference-contexts: Some sort of memory is required to disambiguate apparently equal situations encountered on the way to the goal. Q-learning, for instance, is not guaranteed to solve POMDPs (e.g, <ref> (Watkins and Dayan, 1992) </ref>). Our agent, however, can use memory implicit in the state of the execution of its current program to disambiguate ambiguous situations. Measuring time. <p> In a realistic application, however, the time consumed by a robot move would by far exceed the time consumed by a Jump instruction | we omitted this (negligible) cost in the experimental results. Comparison. We compared LS to three variants of Q-learning <ref> (Watkins and Dayan, 1992) </ref> and random search. Random search repeatedly and randomly selects and executes one of the instructions (1-8) until the goal is hit (like with Levin search, the agent is reset to its start position whenever it hits the wall).
References-found: 18

