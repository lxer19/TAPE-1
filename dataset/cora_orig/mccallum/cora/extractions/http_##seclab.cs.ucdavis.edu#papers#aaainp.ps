URL: http://seclab.cs.ucdavis.edu/papers/aaainp.ps
Refering-URL: http://olympus.cs.ucdavis.edu/gsreports/gsreport.94.08.html
Root-URL: http://www.cs.ucdavis.edu
Email: frank@cs.ucdavis.edu  prieditis@cs.ucdavis.edu  
Phone: (916) 758-5925  (916) 752-6958  
Title: Learning in Gradient Descent Scheduling  
Author: Jeremy Frank 
Date: June 13, 1994  
Affiliation: Division of Computer Science University of California at Davis  
Address: Armand Prieditis  Davis, CA. 95616  
Abstract: Local methods are often used to find approximate optimal solutions to scheduling problems. One such method, gradient descent, has proven useful in finding approximate solutions to such problems. This paper presents a way to improve such methods by learning from local searches. Our empirical results show that learning can significantly improve the performance of gradient descent in solving scheduling problems. 
Abstract-found: 1
Intro-found: 1
Reference: [MiLa] <author> S. Minton, P. Laird, M. D. John-ston, A. Phillips. </author> <title> Solving Large Scale Constraint Satisfaction And Scheduling Problems Using A Heuristic Repair Method. </title> <booktitle> Proceedings, AAAI, </booktitle> <year> 1990. </year>
Reference-contexts: 1 Introduction Local search is a popular way to find approximate solutions. For example, [JoPa] discusses local search and local algorithms to solve the Travelling Salesman problem. <ref> [MiLa] </ref> and [TaJo] present local methods of solving scheduling problems. [Br] discusses a local method of solving the Graph Coloring problem. [Se] uses gradient descent methods to solve graph coloring and 3CNF problems. Figure 1 shows an example of local search.
Reference: [TaJo] <author> P. Tadepalli, V. Joshi. </author> <title> Real-Time Scheduling Using Minimin Search. </title> <booktitle> AAAI Spring Symposium on Practical Approaches to Scheduling and Planning, </booktitle> <year> 1992. </year>
Reference-contexts: 1 Introduction Local search is a popular way to find approximate solutions. For example, [JoPa] discusses local search and local algorithms to solve the Travelling Salesman problem. [MiLa] and <ref> [TaJo] </ref> present local methods of solving scheduling problems. [Br] discusses a local method of solving the Graph Coloring problem. [Se] uses gradient descent methods to solve graph coloring and 3CNF problems. Figure 1 shows an example of local search.
Reference: [JaPr] <author> B. Janakiraman, A. </author> <title> Prieditis. Generating Effective Project Scheduling Heuristics By Abstraction and Reconstitution. </title> <booktitle> AAAI Spring Symposium on Practical Approaches to Scheduling and Planning, </booktitle> <year> 1992. </year>
Reference-contexts: The first heuristic is used to create schedules, and statistics are gathered on its performance. The learning algorithm then indicates when the better informed heuristic should be used. 3 The Scheduling Prob lem The scheduling problem considered in this paper is the Constraint Satisfaction Problem [De] <ref> [JaPr] </ref> which is similar to the graph coloring problem [Br]. For the purposes of this paper jobs are assumed to take unit time, and all jobs can be performed in parallel with given resources. We also assumes that the only constraints on jobs are mutual exclusion constraints. <p> A schedule for a project graph is an assignment of direction to the edges of the project graph. The duration of the schedule is the longest path through the graph, which corresponds to the Critical Path heuristic <ref> [JaPr] </ref>. The object is to generate a schedule with a duration less than some parameter d. 4 Gradient Descent Gradient descent algorithms generate an initial schedule, then use a form of local search to find better solutions. Figure 3 shows a schedule for a particular problem.
Reference: [De] <author> R. Dechter. </author> <title> Learning While Searching In Constraint Satisfaction Problems. </title> <booktitle> Proceedings, AAAI, </booktitle> <year> 1986. </year>
Reference-contexts: The first heuristic is used to create schedules, and statistics are gathered on its performance. The learning algorithm then indicates when the better informed heuristic should be used. 3 The Scheduling Prob lem The scheduling problem considered in this paper is the Constraint Satisfaction Problem <ref> [De] </ref> [JaPr] which is similar to the graph coloring problem [Br]. For the purposes of this paper jobs are assumed to take unit time, and all jobs can be performed in parallel with given resources. We also assumes that the only constraints on jobs are mutual exclusion constraints. <p> This is known as rote learning. The algorithm never returns to a schedule on the search path to a local optimum which does not solve the problem. Even if the number of schedules the algorithm searches is small this may be expensive in storage space and search time. <ref> [De] </ref> presents the idea of a minimum conflict set of components that makes a schedule undesirable in some way. We modified this method and combined it with the gradient descent algorithm.
Reference: [Br] <author> D. Brelaz. </author> <title> New Methods to Color the Vertices of a Graph. </title> <journal> Proceedings, CACM, </journal> <year> 1979, </year> <pages> pp. 251-256 6 </pages>
Reference-contexts: 1 Introduction Local search is a popular way to find approximate solutions. For example, [JoPa] discusses local search and local algorithms to solve the Travelling Salesman problem. [MiLa] and [TaJo] present local methods of solving scheduling problems. <ref> [Br] </ref> discusses a local method of solving the Graph Coloring problem. [Se] uses gradient descent methods to solve graph coloring and 3CNF problems. Figure 1 shows an example of local search. <p> The learning algorithm then indicates when the better informed heuristic should be used. 3 The Scheduling Prob lem The scheduling problem considered in this paper is the Constraint Satisfaction Problem [De] [JaPr] which is similar to the graph coloring problem <ref> [Br] </ref>. For the purposes of this paper jobs are assumed to take unit time, and all jobs can be performed in parallel with given resources. We also assumes that the only constraints on jobs are mutual exclusion constraints.
Reference: [ChKa] <author> P. Cheeseman, Bob Kanefsky, William Taylor. </author> <title> Where the Really Hard Problems Are. </title> <booktitle> Proceedings of the 12 th IJCAI, </booktitle> <year> 1991. </year>
Reference: [Zw] <author> M. Zweben, E. Davis. </author> <title> Learning to Improve Iterative Repair Scheduling. </title> <type> Technical Report, </type> <institution> NASA Ames, </institution> <year> 1992. </year>
Reference-contexts: Each state is kept in the list for only a fixed amount of time. Thus the algorithm "learns" not move to states it has seen before, but then "forgets" those states as it learns new ones. <ref> [Zw] </ref> uses learning to choose heuristics driving a local repair algorithm. Two possible heuristics for performing local repairs may be used to create schedules, one of which is better informed than the other. The first heuristic is used to create schedules, and statistics are gathered on its performance.
Reference: [HeDe] <author> A. Hertz, D. deWerra. </author> <title> Using Tabu Search Techniques for Graph Coloring. </title> <journal> Computing, </journal> <volume> vol. 39, </volume> <year> 1987, </year> <pages> pp. 345-351. </pages>
Reference-contexts: In Section 5 we describe the scheduling problem and how our algorithm is used to find approximate 1 solutions. Section 6 presents the empiri-cal results of our tests. Section 7 presents our conclusions and opportunities for future work. 2 Previous Approaches to Learning in Gradient Descent <ref> [HeDe] </ref> discuss a learning gradient descent method to solve graph-coloring problems. Their gradient descent algorithm finds the best neighboring state to move to, even if the state is further from a solution than the current state. A tabu state is one that was seen in a previous iteration. <p> The procedure of randomly generating a schedule and finding a local optimum is repeated until a solution is found. It should be noted that the algorithm can fail to find an optimal solution even should one exist, and is therefore incomplete [Se] <ref> [HeDe] </ref>. Unlike the algorithms in [Se] and [HeDe], this algorithm continues to search until the gradient becomes zero, and does not move along a negative gradient. <p> The procedure of randomly generating a schedule and finding a local optimum is repeated until a solution is found. It should be noted that the algorithm can fail to find an optimal solution even should one exist, and is therefore incomplete [Se] <ref> [HeDe] </ref>. Unlike the algorithms in [Se] and [HeDe], this algorithm continues to search until the gradient becomes zero, and does not move along a negative gradient. The algorithm does most of the work in determining which schedule to move to next since it must determine the duration of all neighboring schedules. <p> The algorithm searches each schedule for minimum conflict sets, which are stored in a database. The database is used to avoid visiting any schedule which contains a conflict set. Unlike <ref> [HeDe] </ref> constraints are stored through the entire run of the program. Since the rote learning algorithm constrains the search only if it attempts to visit an identical schedule, the probability of using any constraint is very small. <p> If an underlying distribution of edges is assumed then intelligent choices of k may be possible. Second, local methods often do not have a global termination criteria. As in <ref> [HeDe] </ref> the algorithm cannot determine whether a problem has a solution or not, and thus does not know when to halt. In this paper the problem was avoided by testing 5 the algorithms on problems with known solutions.
Reference: [Se] <author> B. Selman. </author> <title> A New Method for Solving Hard Satisfiability Problems. </title> <booktitle> Proceedings, AAAI, </booktitle> <year> 1992, </year> <pages> pp. 440-446. </pages>
Reference-contexts: 1 Introduction Local search is a popular way to find approximate solutions. For example, [JoPa] discusses local search and local algorithms to solve the Travelling Salesman problem. [MiLa] and [TaJo] present local methods of solving scheduling problems. [Br] discusses a local method of solving the Graph Coloring problem. <ref> [Se] </ref> uses gradient descent methods to solve graph coloring and 3CNF problems. Figure 1 shows an example of local search. As the figure shows, a solution is compared with other "close" solutions; that is, solutions which are "neighbors" to it. <p> The procedure of randomly generating a schedule and finding a local optimum is repeated until a solution is found. It should be noted that the algorithm can fail to find an optimal solution even should one exist, and is therefore incomplete <ref> [Se] </ref> [HeDe]. Unlike the algorithms in [Se] and [HeDe], this algorithm continues to search until the gradient becomes zero, and does not move along a negative gradient. <p> The procedure of randomly generating a schedule and finding a local optimum is repeated until a solution is found. It should be noted that the algorithm can fail to find an optimal solution even should one exist, and is therefore incomplete <ref> [Se] </ref> [HeDe]. Unlike the algorithms in [Se] and [HeDe], this algorithm continues to search until the gradient becomes zero, and does not move along a negative gradient. The algorithm does most of the work in determining which schedule to move to next since it must determine the duration of all neighboring schedules.
Reference: [JoPa] <author> D. S. Johnson, C. Papadimitriou, M. Yannakakis. </author> <title> How Easy is Local Search? Journal of Computer and System Sciences, </title> <journal> vol. </journal> <volume> 37, </volume> <year> 1988, </year> <pages> pp. 79-100. </pages>
Reference-contexts: 1 Introduction Local search is a popular way to find approximate solutions. For example, <ref> [JoPa] </ref> discusses local search and local algorithms to solve the Travelling Salesman problem. [MiLa] and [TaJo] present local methods of solving scheduling problems. [Br] discusses a local method of solving the Graph Coloring problem. [Se] uses gradient descent methods to solve graph coloring and 3CNF problems. <p> If a neighbor is found to be a "better" solution, then that solution becomes the next solution considered. If no solution is "better" than the current solution then the solution is "locally optimal" <ref> [JoPa] </ref>. Local search may lead to optimal solutions, but for the purposes of this paper it is considered as an approximation method [LaLe]. This paper describes a method to improve a gradient descent scheduler. Section 2 of the paper describes previous work in using learning with gradient descent algorithms.
Reference: [LaLe] <author> E. Lawler, J. Lenstra, A. Rinnooy Kan, D. Shamoys, ed. </author> <title> The Travelling Salesman Problems. </title> <address> New York: </address> <publisher> Wi-ley and Sons, </publisher> <year> 1985. </year> <editor> [MiSe] D. Mitchell, B. Selman, H. Levesque. </editor> <title> Hard and Easy distribution of SAT Problems. </title> <booktitle> Proceedings, AAAI, </booktitle> <year> 1992, </year> <pages> pp. 459-465 </pages>
Reference-contexts: If no solution is "better" than the current solution then the solution is "locally optimal" [JoPa]. Local search may lead to optimal solutions, but for the purposes of this paper it is considered as an approximation method <ref> [LaLe] </ref>. This paper describes a method to improve a gradient descent scheduler. Section 2 of the paper describes previous work in using learning with gradient descent algorithms. Section 3 describes the gradient descent method of local search, and Section 4 describes how learning is added to the algorithm.
Reference: [Pr] <author> A. </author> <title> Prieditis. Machine Discovery of Effective Admissible Heuristics. </title> <journal> Machine Learning Journal, </journal> <note> to appear in 1993. </note>
Reference-contexts: For example, in the scheduling problem, it may be possible to identify other edge assignments as well as those on the Critical Path to store as part of the conflict set. Other enhancements of the heuristics are possible, as well as learning of heuristics <ref> [Pr] </ref>. Lastly, the addition of learning to local methods may prove useful in improving local algorithms to solve other problems such as TSP, 3CNF, and Binpacking. This work is currently in progress.
References-found: 12

