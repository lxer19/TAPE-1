URL: http://www.cs.ualberta.ca/~upal/plan/3/3.ps
Refering-URL: http://www.cs.ualberta.ca/~upal/pub.html
Root-URL: http://www.cs.ualberta.ca/~upal/pub.html
Title: Learned rewrite rules versus learned search control rules to improve plan quality  
Note: Overview of Approach  
Abstract: Domain independent planners can produce better-quality plans through the use of domain-dependent knowledge, typically encoded as search control rules. The planning-by-rewriting approach as been proposed as an alternative to improving plan quality. We present a system called Sys-REWRITE that automatically learns plan rewriting rules and compare it with Sys-SEARCH-CONTROL, a system that automatically learns search control rules for partial order planners. Our results support the usefulness of planning by rewriting to efficiently generate high quality plans and demonstrate a way of automatically learning these rules from analyzing partial-order planning episodes. This work also suggests that there is more mileage to be gained from the automatic acquisition of search-control rules, because the learning opportunities afforded by analyzing the planning process can be translated to search-control rules that are applicable in wider variety of contexts than are rewrite-rules. AI planners must be able to produce high quality plans, and do so efficiently, if they are to be widely deployed in the real-world planning situations. Various approaches have shown that incorporating domain knowledge into domain-independent planners can improve both the efficiency of those planners (Kambham-pati, Katukam, & Qu 1996; Ihrig & Kambhampati 1997; Kautz & Selman 1998) and as well as quality of the plans they produce (Perez 1996; Iwamoto 1994). Traditionally, this knowledge is encoded as search control rules to limit the search for generation of the first viable plan. Recently, Ambite and Knoblock have suggested an alternative approach called planning by rewriting (Ambite & Knoblock 1997). Under this approach, a partial-order planner generates an initial plan, and then a set of rewrite rules are used to transform this plan into a higher-quality plan. Unlike the search control rules for partial order planners (such as those learned by UCPOP+EBL (Kambhampati, Katukam, & Qu 1996) and PIPP (Upal & Elio 1998)) that are defined on the space of partial plans, rewrite rules are defined on the space of complete plans. In addition, it has been argued that plan-rewrite rules are easier to state than search control rules, because they do not require any knowledge of the inner workings of the planning al gorithm (Ambite & Knoblock 1997). That may partially explain why most of the search-control systems have been designed to automatically acquire search-control rules, whereas existing planning by rewriting systems use manually generated rewrite-rules. To date, there has been no comparison of these two techniques to study their strengths and weaknesses. This paper presents an empirical comparison of how the two techniques-search control rules vs rewrite rules- improve plan quality within a partial-order planning framework. Our focus, however, assumes that both rewrite rules as well as search control rules are to be learned as a function of planning experience. We designed two systems, Sys-REWRITE and Sys-SEARCH-CONTROL, that automatically learn to improve quality of the plans produced by the partial order planners. Both systems have the same overall structure, shown in Figure 1, and only differ in their implementation of the last step. For Step 1, both systems use a partial order planning algorithm, POP, of the sort described in (McAllester & Rosenblitt 1991). The learning algorithm, ISL (Intra-Solution Learning algorithm), that the two systems use for Step 2 is similar to that used in (Upal & Elio 1998) and is described in the section that follows. In Step 3, Sys-REWRITE uses the output of Step 2 to create plan-rewrite rules, while Sys-SEARCH-CONTROL uses that information to create search-control rules. The performance component of the two systems necessarily differs, by definition: Sys-SEARCH-CONTROL uses its rules during its planning process, whereas Sys-REWRITE uses the rules after it has completed what we might think of as its draft partial plan. However, as we discovered during our evaluation process, this difference limits the learning opportunities for Sys-REWRITE. Our approach to plan quality representation and the underlying learning algorithm may be briefly described as follows. We assume that complex quality tradeoffs among a number of competing factors can be mapped to a quantitative statement. Methodological work in operations research indicates that a large set of quality-tradeoffs (of the form "prefer to maximize X rather than 
Abstract-found: 1
Intro-found: 1
Reference: <author> Ambite, J., and Knoblock, C. </author> <year> 1996. </year> <title> Efficient planning by graph rewriting. </title> <booktitle> In Proc. of the Thirteenth National Conference on Artificial Intelligence. </booktitle> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Ambite, J., and Knoblock, C. </author> <year> 1997. </year> <title> Planning by rewriting: Efficiently generating high-quality plans. </title> <booktitle> In Proc. of the Fourteenth National Conference on Artificial Intelligence. </booktitle> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: These positive and negative examples are then passed to an inductive concept learner to induce a rule that covers all positive examples and none of the negative examples. PIPP learns using intra-solution learning and uses an algorithm very similar to ISL. Ambite and Knoblock <ref> (Ambite & Knoblock 1997) </ref> coined the term planning by rewriting. Their system, PBR, used a small number of hand-coded rewrite rules for the Block's world, Minton's manufacturing domain and the query planning domain to improve the quality of the plan produced by SAGE (Knoblock 1996), a partial order planner. <p> Conclusion Just by definition, the best that a planning by rewriting system can perform (with respect to planning efficiency) is as good as the base-planner it is using. Am-bite and Knoblock <ref> (Ambite & Knoblock 1997) </ref> argue that this is acceptable in domains in which generating an initial plan is easier than generating a high quality plan because the overhead for applying rewrite rules is not very great.
Reference: <author> Ambite, J., and Knoblock, C. </author> <year> 1998. </year> <title> Flexible and scalable querry planning in distributed and heterogeneous environments. </title> <booktitle> In Proc. of the Fourth International Conference on Artificial Intelligence Planning Systems, </booktitle> <pages> 3-10. </pages> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Estlin, T., and Mooney, R. </author> <year> 1997. </year> <title> Learning to improve both efficiency and quality of planning. </title> <booktitle> In Proc. of the IJCAI. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: However, analytic techniques can be used if we assume that the learning opportunities are identified with the help of a user who always supplies higher quality plans. (Zimmerman & Kambhampati 1996) and <ref> (Estlin & Mooney 1997) </ref> present two inductive learning techniques to learn while (Upal & Elio 1998) present an analytic technique in a system called PIPP to learn search control rules for partial order planners. SCOPE (Estlin & Mooney 1997) uses FOIL (Quinlan 1990), an inductive concept learner, whereas Zimmerman's system uses <p> help of a user who always supplies higher quality plans. (Zimmerman & Kambhampati 1996) and <ref> (Estlin & Mooney 1997) </ref> present two inductive learning techniques to learn while (Upal & Elio 1998) present an analytic technique in a system called PIPP to learn search control rules for partial order planners. SCOPE (Estlin & Mooney 1997) uses FOIL (Quinlan 1990), an inductive concept learner, whereas Zimmerman's system uses a neural network to to acquire search control rules for UCPOP.
Reference: <author> Etzioni, O. </author> <year> 1990. </year> <title> A structural theory of explanation based learning. </title> <type> Technical Report CMU-CS-90-185, PhD Thesis, </type> <institution> Carnegie Mellon University. </institution>
Reference: <author> Fishburn, P. </author> <year> 1970. </year> <title> Utility Theory for Decision Making. </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference: <author> Ihrig, L., and Kambhampati, S. </author> <year> 1997. </year> <title> Storing and indexing plan derivations through explanation-based analysis of retrieval failures. </title> <journal> Journal of Artificial Intelligence Research 7 </journal> <pages> 161-198. </pages>
Reference: <author> Iwamoto, M. </author> <year> 1994. </year> <title> A planner with quality goal and its speed up learning for optimization problems. </title> <booktitle> In Proc. of Second International Conference on AI Planning Systems, </booktitle> <address> Chicago, IL, </address> <pages> 281-286. </pages>
Reference: <author> Kambhampati, S.; Katukam, S.; and Qu, Y. </author> <year> 1996. </year> <title> Failure driven dynamic search control for partial order planners. </title> <booktitle> Artificial Intelligence 88 </booktitle> <pages> 253-316. </pages>
Reference-contexts: Minton's (Minton 1989) PRODIGY/EBL learned control rules by explaining why a search node lead to success or failure. Kamb-hampati et al. <ref> (Kambhampati, Katukam, & Qu 1996) </ref> propose a technique based on EBL to learn control rules for partial-order planners and apply it to SNLP and UCPOP to learn rejection-rules. Rejection-type rules are learned by generalizing the explanation of the planning failures. <p> However, analytic techniques can be used if we assume that the learning opportunities are identified with the help of a user who always supplies higher quality plans. <ref> (Zimmerman & Kambhampati 1996) </ref> and (Estlin & Mooney 1997) present two inductive learning techniques to learn while (Upal & Elio 1998) present an analytic technique in a system called PIPP to learn search control rules for partial order planners.
Reference: <author> Kautz, H., and Selman, B. </author> <year> 1998. </year> <title> BLACKBOX: a new approach to the application of theorem proving to problem solving. In AIPS-98 Workshop on Planning as Combinatorial Search. </title>
Reference: <author> Keeney, R., and Raiffa, H. </author> <year> 1993. </year> <title> Decisions With Multiple Objectives: Preferences and Value Tradeoffs. </title> <address> New York: </address> <publisher> Cambridge University Press, 2nd edition. </publisher>
Reference: <author> Knoblock, C. </author> <year> 1996. </year> <title> Building a planner for information gathering: a report from the trenches. </title> <booktitle> In Proc. of third International Conference on AI Planning Systems. </booktitle> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Ambite and Knoblock (Ambite & Knoblock 1997) coined the term planning by rewriting. Their system, PBR, used a small number of hand-coded rewrite rules for the Block's world, Minton's manufacturing domain and the query planning domain to improve the quality of the plan produced by SAGE <ref> (Knoblock 1996) </ref>, a partial order planner. However, their system was unable to automatically acquire rewrite rules. There are also slight differences in the syntax of PBR's rewrite rules and those of REWRITE.
Reference: <author> Laird, J.; Newell, A.; and Rosenbloom, P. </author> <year> 1987. </year> <title> Soar: An architecture for general intelligence. </title> <booktitle> Artificial Intelligence 33(3). </booktitle>
Reference: <author> McAllester, D., and Rosenblitt, D. </author> <year> 1991. </year> <title> Systematic nonlinear planning. </title> <booktitle> In Ninth National Conference on Artificial Intelligence, </booktitle> <pages> 634-639. </pages> <address> Menlo Park, CA: </address> <publisher> AAAI Press/MIT Press. </publisher>
Reference: <author> Minton, S. </author> <year> 1989. </year> <title> Expalantion-based learning. </title> <booktitle> Artificial Intelligence 40 </booktitle> <pages> 63-118. </pages>
Reference-contexts: However, at this stage we do not have a proof of this conjecture. Related Work The basic idea of learning search-control rules to speedup problem solving can be traced back to the early work on EBL (Mitchell, Keller, & Keddar-Cabelli 1986; Minton 1989; Etzioni 1990). Minton's <ref> (Minton 1989) </ref> PRODIGY/EBL learned control rules by explaining why a search node lead to success or failure. Kamb-hampati et al. (Kambhampati, Katukam, & Qu 1996) propose a technique based on EBL to learn control rules for partial-order planners and apply it to SNLP and UCPOP to learn rejection-rules.
Reference: <author> Mitchell, T.; Keller, R.; and Keddar-Cabelli, S. </author> <year> 1986. </year> <title> Explanation based learning: A unifying view. </title> <booktitle> Machine Learning 1 </booktitle> <pages> 47-80. </pages>
Reference: <author> Perez, A. </author> <year> 1996. </year> <title> Representing and learning quality-improving search control knowledge. </title> <editor> In Saitta, L., ed., </editor> <booktitle> Proc. of the Thirteenth International Conference on Machine Learning. </booktitle> <address> Los Altos, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Quinlan, R. </author> <year> 1990. </year> <title> Learning logical definitions from relations. </title> <booktitle> Machine Learning 5(3) </booktitle> <pages> 239-2666. </pages>
Reference-contexts: SCOPE (Estlin & Mooney 1997) uses FOIL <ref> (Quinlan 1990) </ref>, an inductive concept learner, whereas Zimmerman's system uses a neural network to to acquire search control rules for UCPOP.
Reference: <author> Upal, M. A., and Elio, R. </author> <year> 1998. </year> <title> Learning to improve quality of the plans produced by partial order planners. </title> <booktitle> In Proc. of AIPS-98 Workshop on Knowledge Engineering and Acquistion for Planning: Bridging theory and Practice, </booktitle> <pages> 94-103. </pages> <address> Menlo Park, CA: </address> <publisher> AAAI Press. AAAI Tech Report WS-98-03. </publisher>
Reference-contexts: However, analytic techniques can be used if we assume that the learning opportunities are identified with the help of a user who always supplies higher quality plans. (Zimmerman & Kambhampati 1996) and (Estlin & Mooney 1997) present two inductive learning techniques to learn while <ref> (Upal & Elio 1998) </ref> present an analytic technique in a system called PIPP to learn search control rules for partial order planners. SCOPE (Estlin & Mooney 1997) uses FOIL (Quinlan 1990), an inductive concept learner, whereas Zimmerman's system uses a neural network to to acquire search control rules for UCPOP.
Reference: <author> Williamson, M. </author> <year> 1996. </year> <title> A value-directed approach to planning. </title> <type> Technical Report TR-96-06-03, PhD thesis, </type> <institution> University of Washington. </institution>
Reference-contexts: That ends the learning about plan quality that can be accomplished from that single training problem. Experiments and Results Two domains were used for the experiments reported here: Softbot <ref> (Williamson 1996) </ref> and modified transportation logistics domain domain translated to R-STRIPS and with an action drive-truck-acities added as an alternative means of traveling between the cities. For the Softbot domain, plan quality is a function of the sum of all the resources consumed.
Reference: <author> Zimmerman, T., and Kambhampati, S. </author> <year> 1996. </year> <title> Neural network guided search control in partial order planning. </title> <booktitle> In Proc. of the Thirteenth National Conference on Artificial Intelligence. </booktitle> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: However, analytic techniques can be used if we assume that the learning opportunities are identified with the help of a user who always supplies higher quality plans. <ref> (Zimmerman & Kambhampati 1996) </ref> and (Estlin & Mooney 1997) present two inductive learning techniques to learn while (Upal & Elio 1998) present an analytic technique in a system called PIPP to learn search control rules for partial order planners.
References-found: 21

