URL: ftp://ftp.cc.gatech.edu/pub/ai/students/carlos/papers/adaptive-control.ps.gz
Refering-URL: http://www.cs.gatech.edu/aimosaic/robot-lab/mrl-online-publications.html
Root-URL: 
Title: Learning of Parameter-Adaptive Reactive Controllers for Robotic Navigation  
Author: Juan C. Santamara and Ashwin Ram 
Keyword: robotic navigation, reactive control, reinforcement learning, adaptive control.  
Address: Atlanta, GA 30332-0280  
Affiliation: Georgia Institute of Technology  
Note: To appear in the Proceedings of the World Multiconference on Systemics, Cybernetics, and Informatics (CSI'97), Caracas, Venezuela,  
Email: fcarlos ashwing@cc.gatech.edu  
Date: July 1997.  
Abstract: Reactive controllers are widely used in mobile robots because they are able to achieve successful performance in real-time. However, the configuration of a reactive controller depends highly on the operating conditions of the robot and the environment; thus, a reactive controller configured for one class of environments may not perform adequately in another. This paper presents a formulation of parameter-adaptive reactive controllers. Parameter-adaptive reactive controllers inherit all the advantages of traditional reactive controllers, but in addition they are able to adjust themselves to the current operating conditions of the robot and the environment in order to improve task performance. Additionally, the paper describes a multistrategy learning algorithm that combines ideas from case-based reasoning and reinforcement learning to construct a mapping between the operating conditions of the mobile robot and the appropriate controller configuration; this mapping is in turn used to adapt the controller configuration dynamically. The algorithm is implemented and evaluated in a robotic navigation system that controls a Denning MRV-III mobile robot. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. Agre and D. Chapman. Pengi: </author> <title> An implementation of a theory of activity. </title> <booktitle> In Proceedings of the American Association of Artificial Intelligence (AAAI-87), </booktitle> <volume> volume 1, </volume> <pages> pages 268-272. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1987. </year>
Reference-contexts: Reactive controllers has been widely used in mobile robots since they are able to achieve successful performance in real-time (e.g., <ref> [1, 2, 8, 9, 12, 16] </ref>). Reactive controllers typically rely on a combination of several task-achieving modules, behaviors, or schemas to perform a mobile robotic task (e.g., [8, 10, 13]). That is, a robotic task is decomposed into several subtasks that the robot must accomplish and execute concurrently.
Reference: [2] <author> R. C. Arkin. </author> <title> Motor schema-based mobile robot navigation. </title> <journal> International Journal of Robotics Research, </journal> <volume> 8(4) </volume> <pages> 92-112, </pages> <year> 1989. </year>
Reference-contexts: Reactive controllers has been widely used in mobile robots since they are able to achieve successful performance in real-time (e.g., <ref> [1, 2, 8, 9, 12, 16] </ref>). Reactive controllers typically rely on a combination of several task-achieving modules, behaviors, or schemas to perform a mobile robotic task (e.g., [8, 10, 13]). That is, a robotic task is decomposed into several subtasks that the robot must accomplish and execute concurrently. <p> Each module has a stimulus-response type of re lationship with the world. The response of the robot is the result of the interaction of all the responses in the system and can be computed according to different schemes, such as subsumption (e.g., [8]), weighted summation (e.g., <ref> [2] </ref>), or voting (e.g., [10]). There are many advantages of such controllers. Reactive controllers are able to execute actions in real-time since the modules act like quick "reflexes" to environmental inputs. This allows mobile robots to react to sudden changes in the environment. <p> For this task, an avoid-obstacle module only needs to know at what distance an obstacle is located from the robot to suggest an appropriate response (e.g., the "avoid-static-obstacle" motor schema of <ref> [2] </ref>). Thus, reactive controllers are characterized by having robust navigational capabilities and rapid, real-time response to the environment. Nevertheless, there is much room for improvement in reactive controllers. Like classical controllers (see, e.g., [15]), reactive controllers have several parameters that affect the performance of the controlled process. <p> These two characteristics allow reactive controllers to perform robustly in real-time since the mapping between sensors and actuators is implemented as quick reflexes without the reasoning necessary to update detailed world models (e.g., <ref> [2, 4, 8, 10, 13] </ref>). <p> Simple behaviors, such as wandering, obstacle avoidance, and goal following, can combine to produce complex emergent behaviors in a particular environment. Different emergent behaviors can be obtained by modifying the simple behaviors. A detailed description of schema-based reactive control methods can be found in <ref> [2] </ref>. The reactive controller in this system uses three motor schemas: Avoid-Static-Obstacle, Move-To-Goal, and Noise. Avoid-Static-Obstacle directs the system to move itself away from detected obstacles. Move-To--Goal schema directs the system to move towards a particular point in the terrain.
Reference: [3] <author> C. G. Atkeson. </author> <title> Local trajectory optimizers. </title> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> 6, </volume> <year> 1993. </year>
Reference: [4] <author> T. Balch, G. Boone, T. Collins, H. Forbes, D. MacKenzie, and J. C. Santamara. </author> <title> Io, ganymede, and callisto amultiagent robot janitorial team. </title> <journal> AI Magazine, </journal> <volume> 16(2) </volume> <pages> 39-51, </pages> <year> 1995. </year>
Reference-contexts: These two characteristics allow reactive controllers to perform robustly in real-time since the mapping between sensors and actuators is implemented as quick reflexes without the reasoning necessary to update detailed world models (e.g., <ref> [2, 4, 8, 10, 13] </ref>).
Reference: [5] <author> R. Bellman. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ, </address> <year> 1957. </year>
Reference-contexts: The ideal mapping fl () is such that when the robot is in situation s, then fl = fl (s) is the optimal parameterization of the reactive controller for that situation. The exact solution of the optimal adaptive policy can be found using dynamic programming <ref> [5] </ref>; however, its computation is very expensive and requires detailed knowledge of the environment and/or the dynamics of the robot. We proposed a heuristic method inspired by the derivation of the optimal solution that is computationally efficient and does not require detailed knowledge.
Reference: [6] <author> R. Bellman and R. Kabala. </author> <title> On adaptive control process. </title> <journal> IRE Transactions on Automatic Control, </journal> <volume> 4 </volume> <pages> 1-9, </pages> <year> 1959. </year>
Reference-contexts: Thus, the controller is designed in such a way that it improves its performance by observing the outputs of the process and choosing the appropriate configuration accordingly. As the process unfolds, additional information becomes available and improved configurations become possible <ref> [6] </ref>. During the design of adaptive controllers, the available knowledge about the dynamics of the system is used to design an adaptive policy that the controller can use at run-time to configure itself and improve task performance.
Reference: [7] <author> D. P. Bertsekas. </author> <title> Dynamic Programming and Optimal Control, volume 1. </title> <publisher> Athena Scientific, </publisher> <address> Belmont, MA, </address> <year> 1995. </year>
Reference-contexts: This approach has been extensively and successfully used in reinforcement learning [22, 19, 23, 18]. At any given stage, the parameter-value function follows a recursive relation (so called the Bellman's Equation, e.g. <ref> [7] </ref>) n t+1 (4) where s t and t are the environment situation and the control parameter vectors at stage t. Stochastic approximation procedures exploit this recursive relation to create an update formula the controller can use to adapt the weights of the function approximator after every experience.
Reference: [8] <author> R. Brooks. </author> <title> A robust layered control system for a mobile robot. </title> <journal> IEEE Journal on Robotics and Automation, </journal> <volume> RA-2(1):14-23, </volume> <year> 1986. </year>
Reference-contexts: Reactive controllers has been widely used in mobile robots since they are able to achieve successful performance in real-time (e.g., <ref> [1, 2, 8, 9, 12, 16] </ref>). Reactive controllers typically rely on a combination of several task-achieving modules, behaviors, or schemas to perform a mobile robotic task (e.g., [8, 10, 13]). That is, a robotic task is decomposed into several subtasks that the robot must accomplish and execute concurrently. <p> Reactive controllers has been widely used in mobile robots since they are able to achieve successful performance in real-time (e.g., [1, 2, 8, 9, 12, 16]). Reactive controllers typically rely on a combination of several task-achieving modules, behaviors, or schemas to perform a mobile robotic task (e.g., <ref> [8, 10, 13] </ref>). That is, a robotic task is decomposed into several subtasks that the robot must accomplish and execute concurrently. Typically, the system designer programs specific modules that accomplish each subtask by considering relevant information from the robot's sensors to control the robot's actuators. <p> Each module has a stimulus-response type of re lationship with the world. The response of the robot is the result of the interaction of all the responses in the system and can be computed according to different schemes, such as subsumption (e.g., <ref> [8] </ref>), weighted summation (e.g., [2]), or voting (e.g., [10]). There are many advantages of such controllers. Reactive controllers are able to execute actions in real-time since the modules act like quick "reflexes" to environmental inputs. This allows mobile robots to react to sudden changes in the environment. <p> These two characteristics allow reactive controllers to perform robustly in real-time since the mapping between sensors and actuators is implemented as quick reflexes without the reasoning necessary to update detailed world models (e.g., <ref> [2, 4, 8, 10, 13] </ref>).
Reference: [9] <author> L. P. Kaelbling. </author> <title> Learning in embedded Systems. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Stanford University, Stanford, </institution> <address> CA, </address> <year> 1990. </year> <note> Technical Report TR-90-04. </note>
Reference-contexts: Reactive controllers has been widely used in mobile robots since they are able to achieve successful performance in real-time (e.g., <ref> [1, 2, 8, 9, 12, 16] </ref>). Reactive controllers typically rely on a combination of several task-achieving modules, behaviors, or schemas to perform a mobile robotic task (e.g., [8, 10, 13]). That is, a robotic task is decomposed into several subtasks that the robot must accomplish and execute concurrently.
Reference: [10] <author> D. Langer, J. K. Rosenblatt, and M. Hebert. </author> <title> A behavior-based system for off-road navigation. </title> <journal> IEEE Journal on Robotics and Automation, </journal> <volume> 10(6) </volume> <pages> 776-783, </pages> <year> 1994. </year>
Reference-contexts: Reactive controllers has been widely used in mobile robots since they are able to achieve successful performance in real-time (e.g., [1, 2, 8, 9, 12, 16]). Reactive controllers typically rely on a combination of several task-achieving modules, behaviors, or schemas to perform a mobile robotic task (e.g., <ref> [8, 10, 13] </ref>). That is, a robotic task is decomposed into several subtasks that the robot must accomplish and execute concurrently. Typically, the system designer programs specific modules that accomplish each subtask by considering relevant information from the robot's sensors to control the robot's actuators. <p> The response of the robot is the result of the interaction of all the responses in the system and can be computed according to different schemes, such as subsumption (e.g., [8]), weighted summation (e.g., [2]), or voting (e.g., <ref> [10] </ref>). There are many advantages of such controllers. Reactive controllers are able to execute actions in real-time since the modules act like quick "reflexes" to environmental inputs. This allows mobile robots to react to sudden changes in the environment. <p> These two characteristics allow reactive controllers to perform robustly in real-time since the mapping between sensors and actuators is implemented as quick reflexes without the reasoning necessary to update detailed world models (e.g., <ref> [2, 4, 8, 10, 13] </ref>).
Reference: [11] <author> R. E. Larson. </author> <title> A survey of dynamic programming computational procedures. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> AC-12(6):767-774, </volume> <year> 1967. </year>
Reference-contexts: An explicit representation of the parameter-value function is important because it enables the controller to search and select the best control parameter at any given stage. However, a closed-form solution of the optimal parameter-value function is rarely available in most problems and its computation is very expensive (see <ref> [11] </ref>). A reasonable approach for explicitly representing the parameter-value function is to use a function ap-proximator. Function approximators have been used successfully in the past in other reinforcement learning tasks (e.g.,[19, 22, 3, 18]). They use finite resources to represent the value of continuous real-valued variables.
Reference: [12] <author> P. Maes. </author> <title> Situated agents can have goals. </title> <booktitle> Robotics and Autonomous Systems, </booktitle> <volume> 6 </volume> <pages> 49-70, </pages> <year> 1990. </year>
Reference-contexts: Reactive controllers has been widely used in mobile robots since they are able to achieve successful performance in real-time (e.g., <ref> [1, 2, 8, 9, 12, 16] </ref>). Reactive controllers typically rely on a combination of several task-achieving modules, behaviors, or schemas to perform a mobile robotic task (e.g., [8, 10, 13]). That is, a robotic task is decomposed into several subtasks that the robot must accomplish and execute concurrently.
Reference: [13] <author> S. Mahadevan and J. Connell. </author> <title> Scaling reinforcement learning to robotics by exploiting the subsumption architecture. </title> <booktitle> In Proceedings of the Eight International Workshop on Machine Learning, </booktitle> <volume> volume 1, </volume> <pages> pages 328-332. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: Reactive controllers has been widely used in mobile robots since they are able to achieve successful performance in real-time (e.g., [1, 2, 8, 9, 12, 16]). Reactive controllers typically rely on a combination of several task-achieving modules, behaviors, or schemas to perform a mobile robotic task (e.g., <ref> [8, 10, 13] </ref>). That is, a robotic task is decomposed into several subtasks that the robot must accomplish and execute concurrently. Typically, the system designer programs specific modules that accomplish each subtask by considering relevant information from the robot's sensors to control the robot's actuators. <p> We call this mapping the adaptive policy, which a mobile robot can use to continuously adapt the reactive controller to the particular situation at hand in order to improve performance. Our fundamental assumption is that learning the adaptive policy is simpler than learning the entire control policy (see <ref> [13] </ref>). This is usually the case because the learning task occurs at a higher level of abstraction than the operating level of the reactive controller. For example, consider the following two different configurations for a reactive controller: "hurried" and "patient". <p> These two characteristics allow reactive controllers to perform robustly in real-time since the mapping between sensors and actuators is implemented as quick reflexes without the reasoning necessary to update detailed world models (e.g., <ref> [2, 4, 8, 10, 13] </ref>).
Reference: [14] <author> K. S. Narendra and A. M. Annaswamy. </author> <title> Stable Adaptive Systems. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1989. </year>
Reference-contexts: The problem of designing classical controllers that adapt themselves dynamically has been addressed by researchers in the subarea of control theory known as adaptive control (see, e.g., <ref> [14] </ref>).
Reference: [15] <author> K. Ogata. </author> <title> Modern Control Engineering. </title> <publisher> Pretince Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1990. </year>
Reference-contexts: Thus, reactive controllers are characterized by having robust navigational capabilities and rapid, real-time response to the environment. Nevertheless, there is much room for improvement in reactive controllers. Like classical controllers (see, e.g., <ref> [15] </ref>), reactive controllers have several parameters that affect the performance of the controlled process. Thus, the performance of any given task executed by a controller will depend highly on the parameters of the controller and on the operating conditions of the robot (or plant).
Reference: [16] <author> D. W. Payton. </author> <title> An architecture for reflexive autonomous vehicle control. </title> <booktitle> In Proceedings of the IEEE Robotics and Automation Conference, </booktitle> <pages> pages 1838-1845, </pages> <year> 1986. </year>
Reference-contexts: Reactive controllers has been widely used in mobile robots since they are able to achieve successful performance in real-time (e.g., <ref> [1, 2, 8, 9, 12, 16] </ref>). Reactive controllers typically rely on a combination of several task-achieving modules, behaviors, or schemas to perform a mobile robotic task (e.g., [8, 10, 13]). That is, a robotic task is decomposed into several subtasks that the robot must accomplish and execute concurrently.
Reference: [17] <author> A. Ram, R. C. Arkin, K. Moorman, and R. J. Clark. </author> <title> Case-based reactive navigation: A case-based method for on-line selection and adaptation or reactive control parameters in autonomous robotic systems. </title> <type> Technical Report GIT-CC-92/57, </type> <institution> College of Computing, Georgia Institute of Technology, </institution> <address> Atlanta, GA, </address> <year> 1992. </year>
Reference-contexts: However, if the robot is to accomplish the task at high performance levels, then different controller parameters will be required for operating on areas with different numbers or configurations of obstacles (e.g., <ref> [17] </ref>). The problem of designing classical controllers that adapt themselves dynamically has been addressed by researchers in the subarea of control theory known as adaptive control (see, e.g., [14]). <p> Figure 1 shows this formulation of the adaptive reactive control problem. The adaptive policy can be hand-designed when enough knowledge about the robot, the environment, and their interaction is known in advance <ref> [17] </ref>. However, an alternative approach is to incorporate learning algorithms into the controller and let the robot learn an appropriate adaptive policy with its own experience. In this way, the system will be capable of dealing with situations not foreseen by its designers. This is discussed next.
Reference: [18] <author> G. A. Rummery and M. Niranjan. </author> <title> On-line q-learning using connectionist systems. </title> <type> Technical Report CUED/F-INFEG/TR66, </type> <institution> Cambridge University Department, </institution> <year> 1994. </year>
Reference-contexts: More specifically, the controller can incrementally learn the optimal long-term benefit values by continually exercising the current, non-optimal estimates of the parameter-value function and improving such estimates after every experience. This approach has been extensively and successfully used in reinforcement learning <ref> [22, 19, 23, 18] </ref>. At any given stage, the parameter-value function follows a recursive relation (so called the Bellman's Equation, e.g. [7]) n t+1 (4) where s t and t are the environment situation and the control parameter vectors at stage t.
Reference: [19] <author> J. C. Santamara, R. S. Sutton, and A. Ram. </author> <title> Experiments with reinforcement learning in problems with continuous state and actions spaces. </title> <type> Technical Report UM-CS-1996-088, </type> <institution> Department of Computer Science, University of Massachusetts, </institution> <address> Amherst, MA, </address> <year> 1996. </year>
Reference-contexts: More specifically, the controller can incrementally learn the optimal long-term benefit values by continually exercising the current, non-optimal estimates of the parameter-value function and improving such estimates after every experience. This approach has been extensively and successfully used in reinforcement learning <ref> [22, 19, 23, 18] </ref>. At any given stage, the parameter-value function follows a recursive relation (so called the Bellman's Equation, e.g. [7]) n t+1 (4) where s t and t are the environment situation and the control parameter vectors at stage t. <p> The function approximator used in the experiments is implemented using case-based methods. A description of the implementation of this type of function approximator can be found in <ref> [19] </ref>. The case-based function approxima-tor uses cases to represent the parameter-value function. Each case represents a point in the situation-parameter space and holds its associated Q-value. In the experiment, the density threshold and the smoothing parameters were set to t d = 0:1 and t k = 0:1 respectively. <p> This produces cases with spherical receptive fields and blending of the value function using a small number of cases. The values of each case were updated using temporal difference (Equation 5) and implemented using eligibility traces following <ref> [19] </ref>. The values for the free constants were fl = 0:99, = 0:7, ff = 0:4, and * = 0. The one-stage lookahead search was performed using 6 equally spaced values for each control parameter between its minimum and maximum values.
Reference: [20] <author> S. P. Singh and R. S. Sutton. </author> <title> Reinforcement learning with replacing eligibility traces. </title> <journal> Machine Learning, </journal> <volume> 22 </volume> <pages> 123-158, </pages> <year> 1996. </year>
Reference-contexts: The discounted sum of the previous gradients r w k are also credited for the current error although their influence decays exponentially with . An efficient on-line implementation of the update rule is possible using "eligibility traces" (for details, see, <ref> [20] </ref>). The idea is to maintain the value of the rightmost sum in Equation 5 in a variable or eligibility trace, which can be easily updated at every stage using a recursive relation. The controller performs the update every time it executes an action.
Reference: [21] <author> R. S. Sutton. </author> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44, </pages> <year> 1988. </year>
Reference-contexts: Thus, the reward function is a declarative description of the desired behavior of the mobile robot. The rewards defines "what" the controller should do but it is the controller's responsibility to find out "how". This formulation is similar to the one used in reinforcement learning problems <ref> [21, 23] </ref>; however, a fundamental difference is that in reinforcement learning the controller learns the actions it should execute given any sensory information (i.e., the control policy), whereas in here the controller is given a parameterized control policy and it must learn the control parameters it should use given any environment <p> The next subsection discusses the topic of improving the estimates of the parameter-value function approximator. Improving Estimates Finding the optimal parameter-value function is a compu-tationally expensive process. A reasonable approach is use some stochastic approximation procedure (e.g., temporal difference methods <ref> [21] </ref>) to asymptotically improve the estimates towards their optimal values. The main idea consists of using the controller's experience to progressively learn the optimal parameter-value function. <p> When the error is different from zero, the controller uses the update formula to adapt the weights w and improve the estimate of the parameter-value function at s t . The procedure above described can be efficiently implemented using temporal difference methods <ref> [21] </ref>. Sutton defines a whole family of update formulas for temporal difference methods called TD (), where 0 1 is a parameter used to measure the relevance of previous pre dictions in the current error.
Reference: [22] <author> R. S. Sutton. </author> <title> Generalization in reinforcement learning: Successful examples using sparse coarse coding. </title> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> 8, </volume> <year> 1996. </year>
Reference-contexts: More specifically, the controller can incrementally learn the optimal long-term benefit values by continually exercising the current, non-optimal estimates of the parameter-value function and improving such estimates after every experience. This approach has been extensively and successfully used in reinforcement learning <ref> [22, 19, 23, 18] </ref>. At any given stage, the parameter-value function follows a recursive relation (so called the Bellman's Equation, e.g. [7]) n t+1 (4) where s t and t are the environment situation and the control parameter vectors at stage t.
Reference: [23] <author> C. J. C. H. Watkins. </author> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> Univeristy of Cambridge, </institution> <address> England, </address> <year> 1989. </year>
Reference-contexts: Thus, the reward function is a declarative description of the desired behavior of the mobile robot. The rewards defines "what" the controller should do but it is the controller's responsibility to find out "how". This formulation is similar to the one used in reinforcement learning problems <ref> [21, 23] </ref>; however, a fundamental difference is that in reinforcement learning the controller learns the actions it should execute given any sensory information (i.e., the control policy), whereas in here the controller is given a parameterized control policy and it must learn the control parameters it should use given any environment <p> More specifically, the controller can incrementally learn the optimal long-term benefit values by continually exercising the current, non-optimal estimates of the parameter-value function and improving such estimates after every experience. This approach has been extensively and successfully used in reinforcement learning <ref> [22, 19, 23, 18] </ref>. At any given stage, the parameter-value function follows a recursive relation (so called the Bellman's Equation, e.g. [7]) n t+1 (4) where s t and t are the environment situation and the control parameter vectors at stage t.
References-found: 23

