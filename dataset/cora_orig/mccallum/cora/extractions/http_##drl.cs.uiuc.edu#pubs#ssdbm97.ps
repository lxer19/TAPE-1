URL: http://drl.cs.uiuc.edu/pubs/ssdbm97.ps
Refering-URL: http://drl.cs.uiuc.edu/pubs/ssdbm97.html
Root-URL: http://www.cs.uiuc.edu
Email: s-kuo,winslett,ying,ycho@uiuc.edu  masubram@us.oracle.com  seamons@transarc.com  
Title: Parallel Input/Output with Heterogeneous Disks  
Author: S. Kuo, M. Winslett, Y. Chen, Y. Cho M. Subramaniam K. Seamons 
Address: Urbana IL 61801  500 Oracle Parkway Redwood Shores, CA 94065  The Gulf Tower Pittsburgh, PA 15219  
Affiliation: Computer Science Department University of Illinois  Oracle Corporation  Transarc Corporation  
Abstract: Panda is a high-performance library for accessing large multidimensional array data on secondary storage of parallel platforms and networks of workstations. When using Panda as the I/O component of a scientific application, H3expresso, on the IBM SP2 at Cornell Theory Center, we found that some nodes are more powerful with respect to I/O than others, requiring the introduction of load balancing techniques to maintain high performance. We expect that heterogeneity will also be a big issue for DBMSs or parallel I/O libraries designed for scientific applications running on networks of workstations, and the methods of allocating data to servers in these environments will need to be upgraded to take heterogeneity into account, while still allowing users to exert control over data layout. We propose such an approach to load balancing, under which we respect the user's choice of high-level disk layout, but introduce automatic subchunking. The use of subchunks allows us to divide the very large chunks typically specified by the user's disk layout into more manageable-size units that can be allocated to I/O nodes in a manner that fairly distributes the load. We also present two techniques for allocating sub-chunks to nodes, static and dynamic, and evaluate their performance on the SP2. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. L. Bell, G. S. Patterson, Jr., </author> <title> Data Organization in Large Numerical Computations, </title> <journal> The Journal of Supercomputing, </journal> <volume> vol. 1, no. 1, </volume> <year> 1987. </year>
Reference-contexts: 1. Introduction Scientific applications often center around large multidimensional arrays <ref> [1] </ref> and require the use of some form of persistent data storage. <p> disk_distribution [ ] = the user's selected disk distribution, with each dimension either ``*'' or ``BLOCK'' distributed size = number of bytes in first disk chunk of the array (no other chunk can be larger in any dimension than the first chunk, with an HPF-style distribution) submesh [ ] = <ref> [1, ..., 1] </ref> (submesh has as many elements as the array has dimensions) threshold = maximum permissible number of bytes in a subchunk begin FIND_submesh (size, disk_distribution, submesh, threshold) if disk_distribution = [``*'', ..., ``*''] return submesh (we respect the user's decision to have only a single chunk) i = 1
Reference: [2] <author> J. L. Bell, </author> <title> A Specialized Data Management System for Parallel Execution of Particle Physics Codes, </title> <booktitle> ACM SIGMOD International Conference on Management of Data, </booktitle> <year> 1988. </year>
Reference-contexts: They argue that the traditional method of storing an array in row-major or column-major order will lead to disastrous performance when access patterns are different from storage patterns. Most of these works have been applied to real applications. For example, <ref> [2] </ref> designed a specialized data management system (DMS) for particle physics codes. [11] used a PLOP file structure for the array storage of radio astronomy applications at the NRAO. [16] enhanced the POSTGRES DBMS to support multidimensional arrays with chunked schemas (one chunk per disk block).
Reference: [3] <author> R. Bordawekar, A. Choudhary, K. Kennedy, C. Koel-bel and M. Paleczny, </author> <title> A Model and Compilation Strategy for Out-of-Core Data Parallel Programs, </title> <booktitle> ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <year> 1995. </year>
Reference-contexts: None of these research projects addressed problems of performance in a heterogeneous environment. 5.2. Parallel I/O Parallel I/O researchers have emphasized the design of parallel file systems or parallel I/O libraries suitable for large scale scientific applications. The collective I/O approach (e.g. <ref> [3, 12, 18] </ref>) was shown to produce much higher performance than that attainable if each processor independently performs I/O operations. Most of these works were applied to out-of-core applications. <p> Most of these works were applied to out-of-core applications. These are I/O-intensive because memory cannot hold the entire problem, so data needs to be moved to and from secondary storage and each processor's main memory periodically. [12] applied his disk-directed I/O approach to an out-of-core LU-decomposition program. <ref> [3] </ref> used the PASSION runtime library with three out-of-core applications: a Laplace equation solver using the Jacobi iteration method, LU factorization with pivoting, and three dimensional red-black relaxation. None of the parallel I/O approaches described in the literature addresses the load balancing issues that arise when heterogeneity is introduced.
Reference: [4] <author> C. H. Cao and V. Strumpen, </author> <title> Efficient Parallel Computing in Distributed Workstation Environments, </title> <journal> Parallel Computing, </journal> <volume> vol. 19, no. 11, </volume> <year> 1993. </year>
Reference-contexts: As processors' capabilities might not be the same and may change dynamically, load balancing is the key issue for providing high performance in such an environment. [7] designed a partitioning system for heterogeneous networked data-parallel processing. <ref> [4] </ref> proposed a heterogeneous partitioning strategy based on the load situation at start and dynamically balanced the load throughout the entire computation.
Reference: [5] <author> Y. Chen, M. Winslett, K. Seamons, S. Kuo, Y. Cho, M. Subramaniam, </author> <title> Scalable Message Passing in Panda, </title> <booktitle> Fourth Annual Workshop on I/O in Parallel and Distributed Systems, </booktitle> <year> 1996. </year>
Reference-contexts: Related works will be discussed in Section 5 and Section 6 concludes this paper. 2. System Architecture of Panda 2.1.1 Panda is designed to support SPMD-style application programs running on ordinary workstations, distributed memory parallel architectures, and networks of workstations. Panda 2.1 <ref> [5] </ref> is designed for an environment where processors are designated as compute nodes (Panda clients) or full-time I/O nodes (Panda servers). <p> After a chunk is gathered, it is written to disk. Read operations follow the same overall strategy, in reverse. We call this architecture server-directed I/O, and details about its use in Panda 2.1's system architecture, and the results of previous Panda performance studies, can be found in <ref> [5, 18] </ref>. While collaborating with H3expresso users, we found that application scientists often want to retain as many nodes as possible to act as compute nodes. This motivated us to alter Panda 2.1 (creating version 2.1.1) to support part-time I/O nodes.
Reference: [6] <author> Y. Chen, I. Foster, J. Nieplocha, M. Winslett, </author> <title> Optimizing Collective I/O Performance on Parallel Computers: A Multisystem Study, </title> <booktitle> ACM International Conference on Supercomputing, </booktitle> <year> 1997. </year>
Reference-contexts: The same load-imbalance problem was encountered in previous Panda out-of-core experiments, which read and write subarrays rather than entire arrays <ref> [6] </ref>. We found that 4 As each I/O node has two roles, it might happen that a server needs data from its local client. When this occurs, neither request messages nor data transfer are required. <p> Thus, the disk chunks corresponding to a subarray being read or written often belong to just a subset of the I/O nodes. Hence, the load is imbalanced and available I/O bandwidth is decreased. The approach taken in <ref> [6] </ref> was to change the I/O mesh by hand to decrease the granularity, and the paper concluded that choosing an optimal disk array distribution is a difficult task and performance would not be robust if users had to select the disk layout themselves.
Reference: [7] <author> P. E. Crandall and M. J. Quinn, </author> <title> A Partitioning Advisory System for Networked Data-Parallel Processing, </title> <journal> Concurrency: Practice and Experience, </journal> <volume> vol. 7, no. 5, </volume> <year> 1995. </year>
Reference-contexts: An application problem is decomposed into individual subtasks, which are distributed across available processors in order to be executed concurrently. As processors' capabilities might not be the same and may change dynamically, load balancing is the key issue for providing high performance in such an environment. <ref> [7] </ref> designed a partitioning system for heterogeneous networked data-parallel processing. [4] proposed a heterogeneous partitioning strategy based on the load situation at start and dynamically balanced the load throughout the entire computation.
Reference: [8] <author> D. DeWitt, N. Kabra, J. Luo, J. Patel and J. Yu, </author> <title> Client-Server Paradise, </title> <booktitle> Proceedings of the 29th VLDB Conference, </booktitle> <year> 1994. </year>
Reference-contexts: They chose an optimal chunk layout based on the actual access patterns of the arrays when used by global change scientists in the Sequoia project [20]. Paradise <ref> [8] </ref> used a client-server architecture and provided an extended-relational data model for modeling GIS applications, with support for 2D chunked arrays. [14] provided a query language which can directly manip-ulate scientific data stored in any format (e.g.
Reference: [9] <author> D. L. Eager, E. D. Lazowska and J. Zahorjan, </author> <title> Adaptive Load Sharing in Homogeneous Distributed Systems, </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> vol. 12, no. 5, </volume> <year> 1986. </year>
Reference-contexts: Load balancing is also an issue for job scheduling in distributed systems <ref> [9] </ref>.
Reference: [10] <author> M. Harry, J. Miguel, del Rosario and A. Choudhary, </author> <title> VIP-FS: A Virtual, Parallel File System for High Performance Parallel and Distributed Computing, </title> <booktitle> Proceedings of the Ninth International Parallel Processing Symposium, </booktitle> <year> 1995. </year>
Reference-contexts: None of the parallel I/O approaches described in the literature addresses the load balancing issues that arise when heterogeneity is introduced. PIOUS [15] and VIP-FS <ref> [10] </ref> are parallel file systems designed for accessing permanent storage in networked computing environments, where heterogeneity is a big issue. However, PIOUS focused on supporting concurrent and fault-tolerant access and did not address the load balancing issue.
Reference: [11] <author> J. F. Karpovich, J. C. French and A. S. Grimshaw, </author> <title> High Performance Access to Radio Astronomy Data: </title>
Reference-contexts: Most of these works have been applied to real applications. For example, [2] designed a specialized data management system (DMS) for particle physics codes. <ref> [11] </ref> used a PLOP file structure for the array storage of radio astronomy applications at the NRAO. [16] enhanced the POSTGRES DBMS to support multidimensional arrays with chunked schemas (one chunk per disk block).
References-found: 11

