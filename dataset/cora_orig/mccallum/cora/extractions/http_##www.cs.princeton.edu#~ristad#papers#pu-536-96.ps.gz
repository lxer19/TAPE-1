URL: http://www.cs.princeton.edu/~ristad/papers/pu-536-96.ps.gz
Refering-URL: http://www.cs.princeton.edu/~ristad/papers/pu-536-96.html
Root-URL: http://www.cs.princeton.edu
Title: Nonuniform Markov Models  
Author: Eric Sven Ristad Robert G. Thomas 
Keyword: nonuniform Markov model, interpolated Markov model, conditional independence, statistical language model, discrete time series.  
Abstract: Research Report CS-TR-536-96 November 1996 Abstract A statistical language model assigns probability to strings of arbitrary length. Unfortunately, it is not possible to gather reliable statistics on strings of arbitrary length from a finite corpus. Therefore, a statistical language model must decide that each symbol in a string depends on at most a small, finite number of other symbols in the string. In this report we propose a new way to model conditional independence in Markov models. The central feature of our nonuniform Markov model is that it makes predictions of varying lengths using contexts of varying lengths. Experiments on the Wall Street Journal reveal that the nonuniform model performs slightly better than the classic interpolated Markov model. This result is somewhat remarkable because both models contain identical numbers of parameters whose values are estimated in a similar manner. The only difference between the two models is how they combine the statistics of longer and shorter strings. 1 Thanks to Andrew Appel, Joe Kupin and Harry Printz for their critique. Our 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Bahl, L. R., Brown, P. F., de Souza, P. V., Mercer, R. L., and Nahamoo, D. </author> <title> A fast algorithm for deleted interpolation. </title> <booktitle> In Proc. </booktitle> <address> EU-ROSPEECH '91 (Genoa, </address> <year> 1991), </year> <pages> pp. 1209-1212. </pages>
Reference-contexts: as x i and the length t prefix of x T as x t . 2 2.1 Three Interpolated Models An interpolated Markov model = hn; A; ffi; i consists of a maximal string length n, a finite alphabet A, a set of string probabilities ffi : A n ! <ref> [0; 1] </ref>, and the interpolation parameters : A &lt;n ! [0; 1]. Given a string y l , l &lt; n, the string probabilities ffi (y l ) are typically their empirical probabilities in a training corpus. <p> T as x t . 2 2.1 Three Interpolated Models An interpolated Markov model = hn; A; ffi; i consists of a maximal string length n, a finite alphabet A, a set of string probabilities ffi : A n ! <ref> [0; 1] </ref>, and the interpolation parameters : A &lt;n ! [0; 1]. Given a string y l , l &lt; n, the string probabilities ffi (y l ) are typically their empirical probabilities in a training corpus. The only difference between our three models will be how the interpolation parameters are interpreted. <p> Until t = T 3. Pick context length i in [0; min (t; n 1)] p c (ijx t ) = (x t Q i+1 tl+1 )) ti+1 ; j max := max (n i; T t); 5. Pick prediction y j 1 of length j in <ref> [1; j max ] </ref> p v (y 1 jc) = (1 (cy 1 ))ffi (y j jcy 1 ; i + j) l=1 (cy l 1 ; l + i) where (cy j max : 6. <p> No parameter tying or parameter selection was performed. We report performance as test message perplexity. We set the ffi parameters to be the empirical probabilities in the training data and then optimized the parameters on the training data using deleted interpolation <ref> [8, 1] </ref>. We soon discovered that the initial values for the parameters had a noticeable effect on model performance as did the block size used for deleted interpolation.
Reference: [2] <author> Baum, L., and Eagon, J. </author> <title> An inequality with applications to statistical estimation for probabilistic functions of a Markov process and to models for ecology. Bull. </title> <booktitle> AMS 73 (1967), </booktitle> <pages> 360-363. </pages>
Reference-contexts: The most likely generation path is stored in the ^s variable. 10 3.3 Estimation In this section, we formulate an expectation maximization (EM) algorithm for the nonuniform Markov model. Our development follows the traditional lines established for the hidden Markov model <ref> [2, 3] </ref>. (See [16] for a tutorial.) Recall that we must first calculate the expected number of times that each hidden event occurred for a given training sequence. The hidden events for the nonuniform model are the choice of context and prediction lengths.
Reference: [3] <author> Baum, L., Petrie, T., Soules, G., and Weiss, N. </author> <title> A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains. </title> <journal> Ann. Math. Stat. </journal> <volume> 41 (1970), </volume> <pages> 164-171. </pages>
Reference-contexts: The most likely generation path is stored in the ^s variable. 10 3.3 Estimation In this section, we formulate an expectation maximization (EM) algorithm for the nonuniform Markov model. Our development follows the traditional lines established for the hidden Markov model <ref> [2, 3] </ref>. (See [16] for a tutorial.) Recall that we must first calculate the expected number of times that each hidden event occurred for a given training sequence. The hidden events for the nonuniform model are the choice of context and prediction lengths.
Reference: [4] <author> Cleary, J., and Witten, I. </author> <title> Data compression using adaptive coding and partial string matching. </title> <journal> IEEE Trans. Comm. COM-32, </journal> <volume> 4 (1984), </volume> <pages> 396-402. </pages>
Reference-contexts: Here the observables are the word sequences produced by language users. And so our goal is to assign accurate probabilities to word sequences. The interpolated Markov model [8] and its cousin the backoff model <ref> [4, 9, 18] </ref> have long been the workhorses of the statistical language modeling community. These traditional models rely only on the frequencies of strings up to a fixed length.
Reference: [5] <author> Jeanrenaud, P., Eide, E., Chaudhari, U., McDonough, J., Ng, K., Siu, M., and Gish, H. </author> <title> Reducing word error rate on conversational speech from the Switchboard corpus. </title> <booktitle> In ICASSP 95 (1995), </booktitle> <pages> pp. 53-56. </pages>
Reference-contexts: Nonuniform predictions is a principled way to perform alphabet extension, that is, to make a string become a symbol in the alphabet, an ad hoc technique that can improve model performance <ref> [5] </ref>. The remainder of this report is organized into four sections. In section 2 we motivate the nonuniform model as arising from the proper generative interpretation of our beliefs about conditional independence.
Reference: [6] <author> Jeffreys, H. </author> <title> An invariant form for the prior probability in estimation problems. </title> <journal> Proc. Roy. Soc. (London) A 186 (1946), </journal> <pages> 453-461. </pages>
Reference-contexts: Regardless of how the parameters were initialized or what block size was used, the nonuniform model performed slightly better than the uniform model under equivalent experimental conditions. We considered three initial estimates for the values: uniform, the Jeffreys-Perks rule of succession <ref> [6, 14, 10] </ref>, and the natural law of succession [17]. The uniform estimate sets all values to 0.5. The Jeffreys-Perks rule sets (x i ) to c (x i )=(c (x i )+k=2), for alphabet size k and string frequency c (x i ).
Reference: [7] <author> Jelinek, F., Lafferty, J., and Mercer, R. L. </author> <title> Basic methods of probabilistic context-free grammars. In Speech Recognition and Understanding: 16 Recent Advances, Trends, and Applications, </title> <editor> P. Laface and R. De Mori, Eds. </editor> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1992. </year>
Reference-contexts: These traditional models rely only on the frequencies of strings up to a fixed length. Recent research in statistical language modeling has focused primarily on developing more powerful model classes <ref> [7, 11] </ref> as well as on adding new sources of information to the traditional models [12, 15]. In contrast, the goal of this work is to find a more effective way to use the statistics of finite length strings.
Reference: [8] <author> Jelinek, F., and Mercer, R. L. </author> <title> Interpolated estimation of Markov source parameters from sparse data. </title> <booktitle> In Pattern Recognition in Practice (Amsterdam, </booktitle> <month> May 21-23 </month> <year> 1980), </year> <editor> E. S. Gelsema and L. N. Kanal, Eds., </editor> <publisher> North Holland, </publisher> <pages> pp. 381-397. </pages>
Reference-contexts: Here the observables are the word sequences produced by language users. And so our goal is to assign accurate probabilities to word sequences. The interpolated Markov model <ref> [8] </ref> and its cousin the backoff model [4, 9, 18] have long been the workhorses of the statistical language modeling community. These traditional models rely only on the frequencies of strings up to a fixed length. <p> The first contribution is our interpretation of the interpolation parameters as beliefs about conditional independence. Prior work on interpolated Markov models has interpreted the interpolation parameters as smoothing the "specific probabilities" with the "general probabilities" <ref> [8, 13] </ref>. Our interpretation gives rise to the second contribution of our work, namely, a class of nonuniform Markov models that make predictions of varying lengths using contexts of varying lengths. <p> be the probability that we make a prediction y j 1 of length j in the chosen context x t ti+1 . 2.1.1 Context Model In the interpolated context model, the interpolation parameters are understood as smoothing the conditional probabilities estimated from longer histories with those estimated from shorter histories <ref> [8, 13] </ref>. Longer histories support stronger predictions, while shorter histories have more accurate statistics. Interpolating the predictions from histories of different lengths results in more accurate predictions than can be obtained from any fixed history length. This interpretation of the interpolation parameters was originally proposed by Jelinek and Mercer [8]. <p> Longer histories support stronger predictions, while shorter histories have more accurate statistics. Interpolating the predictions from histories of different lengths results in more accurate predictions than can be obtained from any fixed history length. This interpretation of the interpolation parameters was originally proposed by Jelinek and Mercer <ref> [8] </ref>. It leads to the following generation algorithm, where the hidden transition from a longer context to a shorter context (line 3) is temporary, used only for the current prediction (line 4). context-generate (T ,) 1. Initialize t := 0; x 0 1 := *; 2. <p> Initialize ffi using B; 4 Experimental Results In this section we compare the performance of the interpolated context model and the nonuniform model on the Wall Street Journal. (Recall that the interpolated context model is the classic interpolated Markov model of Jelinek and Mercer <ref> [8] </ref>.) We performed two sets of experiments. The first set of experiments was with the 6.2 million word WSJ 1989 corpus. The goal of these initial experiments was to better understand how initial parameter values affect model performance. <p> No parameter tying or parameter selection was performed. We report performance as test message perplexity. We set the ffi parameters to be the empirical probabilities in the training data and then optimized the parameters on the training data using deleted interpolation <ref> [8, 1] </ref>. We soon discovered that the initial values for the parameters had a noticeable effect on model performance as did the block size used for deleted interpolation.
Reference: [9] <author> Katz, S. </author> <title> Estimation of probabilities from sparse data for the language model component of a speech recognizer. </title> <journal> IEEE Trans. ASSP 35 (1987), </journal> <pages> 400-401. </pages>
Reference-contexts: Here the observables are the word sequences produced by language users. And so our goal is to assign accurate probabilities to word sequences. The interpolated Markov model [8] and its cousin the backoff model <ref> [4, 9, 18] </ref> have long been the workhorses of the statistical language modeling community. These traditional models rely only on the frequencies of strings up to a fixed length.
Reference: [10] <author> Krichevskii, R. E., and Trofimov, V. K. </author> <title> The performance of universal coding. </title> <journal> IEEE Trans. Information Theory IT-27, </journal> <volume> 2 (1981), </volume> <pages> 199-207. </pages>
Reference-contexts: Regardless of how the parameters were initialized or what block size was used, the nonuniform model performed slightly better than the uniform model under equivalent experimental conditions. We considered three initial estimates for the values: uniform, the Jeffreys-Perks rule of succession <ref> [6, 14, 10] </ref>, and the natural law of succession [17]. The uniform estimate sets all values to 0.5. The Jeffreys-Perks rule sets (x i ) to c (x i )=(c (x i )+k=2), for alphabet size k and string frequency c (x i ).
Reference: [11] <author> Lafferty, J., Sleator, D., and Temperly, D. </author> <title> Grammatical trigrams: a probabilistic model of link grammar. </title> <type> Tech. Rep. </type> <address> CMU-CS-92-181, CMU, Pittsburgh, </address> <month> September </month> <year> 1992. </year>
Reference-contexts: These traditional models rely only on the frequencies of strings up to a fixed length. Recent research in statistical language modeling has focused primarily on developing more powerful model classes <ref> [7, 11] </ref> as well as on adding new sources of information to the traditional models [12, 15]. In contrast, the goal of this work is to find a more effective way to use the statistics of finite length strings.
Reference: [12] <author> Lau, R., Rosenfeld, R., and Roukos, S. </author> <title> Trigger-based language models: a maximum entropy approach. </title> <booktitle> In Proc. ICASSP-93 (1993), </booktitle> <pages> pp. 45-48. </pages> <month> vol.II. </month>
Reference-contexts: These traditional models rely only on the frequencies of strings up to a fixed length. Recent research in statistical language modeling has focused primarily on developing more powerful model classes [7, 11] as well as on adding new sources of information to the traditional models <ref> [12, 15] </ref>. In contrast, the goal of this work is to find a more effective way to use the statistics of finite length strings.
Reference: [13] <author> MacKay, D. J., and Peto, L. C. B. </author> <title> A hierarchical Dirichlet language model. </title> <booktitle> Natural Language Engineering 1, </booktitle> <month> 1 </month> <year> (1994). </year>
Reference-contexts: The first contribution is our interpretation of the interpolation parameters as beliefs about conditional independence. Prior work on interpolated Markov models has interpreted the interpolation parameters as smoothing the "specific probabilities" with the "general probabilities" <ref> [8, 13] </ref>. Our interpretation gives rise to the second contribution of our work, namely, a class of nonuniform Markov models that make predictions of varying lengths using contexts of varying lengths. <p> be the probability that we make a prediction y j 1 of length j in the chosen context x t ti+1 . 2.1.1 Context Model In the interpolated context model, the interpolation parameters are understood as smoothing the conditional probabilities estimated from longer histories with those estimated from shorter histories <ref> [8, 13] </ref>. Longer histories support stronger predictions, while shorter histories have more accurate statistics. Interpolating the predictions from histories of different lengths results in more accurate predictions than can be obtained from any fixed history length. This interpretation of the interpolation parameters was originally proposed by Jelinek and Mercer [8].
Reference: [14] <author> Perks, W. </author> <title> Some observations on inverse probability, including a new indifference rule. </title> <journal> J. Inst. Actuar. </journal> <volume> 73 (1947), </volume> <pages> 285-312. </pages>
Reference-contexts: Regardless of how the parameters were initialized or what block size was used, the nonuniform model performed slightly better than the uniform model under equivalent experimental conditions. We considered three initial estimates for the values: uniform, the Jeffreys-Perks rule of succession <ref> [6, 14, 10] </ref>, and the natural law of succession [17]. The uniform estimate sets all values to 0.5. The Jeffreys-Perks rule sets (x i ) to c (x i )=(c (x i )+k=2), for alphabet size k and string frequency c (x i ).
Reference: [15] <author> Pietra, S. D., Pietra, V. D., Gillett, J., Lafferty, J., Printz, H., and Ures, L. </author> <title> Inference and estimation of a long range trigram model. </title> <type> Tech. Rep. </type> <address> CMU-CS-94-188, CMU, Pittsburgh, </address> <month> September </month> <year> 1994. </year>
Reference-contexts: These traditional models rely only on the frequencies of strings up to a fixed length. Recent research in statistical language modeling has focused primarily on developing more powerful model classes [7, 11] as well as on adding new sources of information to the traditional models <ref> [12, 15] </ref>. In contrast, the goal of this work is to find a more effective way to use the statistics of finite length strings.
Reference: [16] <author> Rabiner, L., and Juang, B. </author> <title> An introduction to hidden Markov models. </title> <journal> IEEE ASSP Magazine (1986), </journal> <pages> 4-16. </pages>
Reference-contexts: The most likely generation path is stored in the ^s variable. 10 3.3 Estimation In this section, we formulate an expectation maximization (EM) algorithm for the nonuniform Markov model. Our development follows the traditional lines established for the hidden Markov model [2, 3]. (See <ref> [16] </ref> for a tutorial.) Recall that we must first calculate the expected number of times that each hidden event occurred for a given training sequence. The hidden events for the nonuniform model are the choice of context and prediction lengths. We begin by defining our forward and backward variables.
Reference: [17] <author> Ristad, E. S. </author> <title> A natural law of succession. </title> <type> Tech. Rep. 495-95, </type> <institution> Department of Computer Science, Princeton University, Princeton, NJ, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: We considered three initial estimates for the values: uniform, the Jeffreys-Perks rule of succession [6, 14, 10], and the natural law of succession <ref> [17] </ref>. The uniform estimate sets all values to 0.5. The Jeffreys-Perks rule sets (x i ) to c (x i )=(c (x i )+k=2), for alphabet size k and string frequency c (x i ). Jeffreys-Perks is a conservative estimate, that assigns relatively low probability to (x i ).
Reference: [18] <author> Ristad, E. S., and Thomas, R. G. </author> <title> New techniques for context modeling. </title> <booktitle> In Proc. 33rd Annual Meeting of the ACL (Cambridge, </booktitle> <address> MA, </address> <month> June </month> <year> 1995). </year>
Reference-contexts: Here the observables are the word sequences produced by language users. And so our goal is to assign accurate probabilities to word sequences. The interpolated Markov model [8] and its cousin the backoff model <ref> [4, 9, 18] </ref> have long been the workhorses of the statistical language modeling community. These traditional models rely only on the frequencies of strings up to a fixed length.
Reference: [19] <author> Ristad, E. S., and Yianilos, P. N. </author> <title> Library of practical abstractions. </title> <type> Tech. rep., </type> <institution> Department of Computer Science, Princeton University, Prince-ton, NJ, </institution> <year> 1996. </year> <note> ftp://ftp.cs.princeton.edu/pub/packages/libpa. 17 </note>
References-found: 19

