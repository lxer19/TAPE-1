URL: ftp://ftp.cs.toronto.edu/pub/mfalou/PODC95/podc.ps.Z
Refering-URL: http://www.cs.toronto.edu/~mfalou/papers.html
Root-URL: http://www.cs.toronto.edu
Title: Optimal Distributed Algorithm for Minimum Spanning Trees Revisited  
Author: Michalis Faloutsos and Mart Molle 
Abstract: In an earlier paper, Awerbuch presented an innovative distributed algorithm for solving minimum spanning tree problems that achieved optimal time and message complexity through the introduction of several advanced features. In this paper, we show that there are some cases where his algorithm can create cycles or fail to achieve optimal time complexity. We then show how to modify the algorithm to avoid these problems, and demonstrate both the correctness and optimality of the revised algorithm. 
Abstract-found: 1
Intro-found: 1
Reference: [Awe87] <author> B. Awerbuch. </author> <title> Optimal distributed algorithms for minimum weight spanning tree, counting, leader election and related problems. </title> <booktitle> Proc. 19th Symp. on Theory of Computing, </booktitle> <pages> pages 230-240, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: complexity independently of the way optimality will be defined. 1.1 Basic Algorithm of Gallager, Humblet and Spira In their pioneering paper [GHS83], Gallager, Humblet and Spira introduced the distributed MST problem and presented an algorithm that has formed the basis of subsequent work in the area, for example [CT85], [Gaf85], <ref> [Awe87] </ref> and [Fal95]. In their algorithm, each node is initially the root of its own fragment (a trivial connected subgraph of the MST) and all the edges are Unlabeled. Thereafter, adjacent fragments join to form larger fragments by labeling their intermediate edge as a Branch of the MST. <p> However, this is not guaranteed, since F could end up joining with some other fragment F x and get a very small level increase. 1.2 Awerbuch's Optimal Algorithm In <ref> [Awe87] </ref>, Awerbuch proposed an innovative three-phase distributed MST algorithm, which achieves optimal performance in terms of both message and time complexity. <p> Consequently, there are some cases where this algorithm, as it is described in <ref> [Awe87] </ref>, can fail by creating a cycle, or at least fail to achieve optimal time complexity. First, in the proof of optimal time complexity, it is assumed that after each minimum-level fragment has found its MOE, they can submit or equi-join in constant time. Since [Awe87] does not specify a new <p> as it is described in <ref> [Awe87] </ref>, can fail by creating a cycle, or at least fail to achieve optimal time complexity. First, in the proof of optimal time complexity, it is assumed that after each minimum-level fragment has found its MOE, they can submit or equi-join in constant time. Since [Awe87] does not specify a new policy, we must assume that the algorithm has inherited the fragment joining policy from the basic [GHS83] algorithm. Unfortunately, the joining policy in [GHS83] does not satisfy this assumption, even if the fragment (s) are of minimum level. <p> The algorithm incorporates several changes in order to eliminate the three problems we identified in the previous section. We have also tried to clarify certain fine details that were not obvious from reading <ref> [Awe87] </ref> but were needed for an implementation of the algorithm. 2.1 Node Counting Phase Any optimal Spanning Tree algorithm can be used. In the original paper [Awe87], an O (E + N log N ) messages and O (N ) time algorithm is proposed. <p> We have also tried to clarify certain fine details that were not obvious from reading <ref> [Awe87] </ref> but were needed for an implementation of the algorithm. 2.1 Node Counting Phase Any optimal Spanning Tree algorithm can be used. In the original paper [Awe87], an O (E + N log N ) messages and O (N ) time algorithm is proposed. The purpose of this phase is to determine N so that a fragment size threshold of N log (N) can be used to trigger the switch from phase II to phase III. <p> Therefore, we achieve fast level increases and the communication and time complexity of this phase are O (E + N log (N )) and O (N ) respectively (see <ref> [Awe87] </ref> for details). Given the Spanning Tree, the number of nodes in the network can easily be counted. In [FM95], a simple counting algorithm is presented. Its complexity is O (E) messages and O (N ) time. <p> In [FM95], a simple counting algorithm is presented. Its complexity is O (E) messages and O (N ) time. It assumes that one node can be authorized to initiate the algorithm. 2.2 Small Fragment MST Phase This second phase is unchanged from <ref> [Awe87] </ref>. It begins with each node acting as the root of a trivial one-node MST fragment executing the basic [GHS83] algorithm. <p> Each fragment switches to the more complicated phase III algorithm as soon as its size reaches N log (N) . 2.3 Large Fragment MST Phase This final phase keeps the same general structure as described in <ref> [Awe87] </ref>, i.e., it is the basic algorithm with the addition of the Root Distance and Leader Distance procedures. However, there are numerous detail changes to address the issues we identified above. Thus, we will give a detailed walk through of phase III, highlighting the new features in the revised algorithm. <p> As we already said, such a characteristic doesn't exist in the leader join protocol that was described in [GHS83], and seems to have been inherited by <ref> [Awe87] </ref>. Thus, unlike the joining protocol in [GHS83], ours offers a guarantee that the delay until the leader receives some answer from the adjacent fragment, is proportional to the other fragment's size. <p> NEW There are two ways the procedure can increase the level of the leader that started it: a) the message expires when the counter reaches 2 L msg +2 (this hop count trigger is twice as large as in <ref> [Awe87] </ref> and will be explained below); or b) the message arrives at a node whose level is greater than L msg (the second condition is entirely NEW). <p> Schematically, we can say that joining decisions and level increases take place at the root and thus errors are avoided. Let us consider the Leader Distance procedure. and recall the example that made <ref> [Awe87] </ref> create a cycle. As already discussed, our modified algorithm breaks the symmetry of the two procedures. <p> As we said, nodes start forwarding testDistance messages after they are decided guaranteeing that the final orientation of the fragment is established (final until the next Finding procedure). In <ref> [Awe87] </ref>, this problem of "path change" is not being discussed and lack of detailed description does not allow us to know whether it was taken under consideration. 4 Complexity This section will offer a brief description of the complexity issues for the revised algorithm. <p> It is optimal and it won't be discussed further (see [GHS83]). We will calculate the complexity of the algorithm by summing the complexities of its three phases. 4.1 Communication Complexity Phase I: The communication complexity of this phase was proven optimal <ref> [Awe87] </ref>. Phase II, III: We can see that each edge is rejected only once (if at all) and only two messages (two test messages or a test and a reject message) are required. Thus, edge rejection uses O (E) messages. <p> Therefore, the communication complexity of the algorithm is O (E + N log (N )) and optimal. 4.2 Time Complexity It is comparatively easy and considerably less exciting to see that the Counting phase and the first part of the MST phase are optimal with respect to time (see <ref> [Awe87] </ref>) and we will provide only short explanations. <p> Phase I: In the Counting phase adjacent fragments join in a way similar to that of [GHS83], but since it is trying to find just a Spanning Tree it is relatively easy to guarantee that fragments almost never wait for other trees (for a detailed proof see <ref> [Awe87] </ref>). Phase II: In the first part of the MST phase, the size of the fragments has at most N log (N) nodes and the maximum level is bounded by log (N ) log (log (N )) log (N ). <p> For this, fragment should switch to phase III after obtaining size N log (N) . Therefore, each fragment should be aware of N , and the Counting phase (I) is necessary. 5 Epilogue In this paper, we identified some problems with Awerbuch's distributed MST algorithm <ref> [Awe87] </ref>, involving both correctness and optimality issues. We then gave a revised algorithm, which introduces several new features to solve these problems. We also show that with these changes, our revised algorithm satisfies the desired correctness and optimality conditions.
Reference: [CT85] <author> F. Chin and H.F. Ting. </author> <title> An almost linear time and o(vlogv + e) messages distributed algorithm for minimum weight spanning trees. </title> <booktitle> Proceedings of Foundations Of Computer Science (FOCS) Conference Portland, </booktitle> <address> Oregon, </address> <month> October </month> <year> 1985. </year>
Reference-contexts: of competitive complexity independently of the way optimality will be defined. 1.1 Basic Algorithm of Gallager, Humblet and Spira In their pioneering paper [GHS83], Gallager, Humblet and Spira introduced the distributed MST problem and presented an algorithm that has formed the basis of subsequent work in the area, for example <ref> [CT85] </ref>, [Gaf85], [Awe87] and [Fal95]. In their algorithm, each node is initially the root of its own fragment (a trivial connected subgraph of the MST) and all the edges are Unlabeled. Thereafter, adjacent fragments join to form larger fragments by labeling their intermediate edge as a Branch of the MST.
Reference: [Fal95] <author> Michalis Faloutsos. </author> <title> Corrections, improvements, simulations and optimstic algorithms for the distributed minimum spanning tree problem. </title> <type> Technical Report CSRI-316, </type> <year> 1995. </year>
Reference-contexts: of the way optimality will be defined. 1.1 Basic Algorithm of Gallager, Humblet and Spira In their pioneering paper [GHS83], Gallager, Humblet and Spira introduced the distributed MST problem and presented an algorithm that has formed the basis of subsequent work in the area, for example [CT85], [Gaf85], [Awe87] and <ref> [Fal95] </ref>. In their algorithm, each node is initially the root of its own fragment (a trivial connected subgraph of the MST) and all the edges are Unlabeled. Thereafter, adjacent fragments join to form larger fragments by labeling their intermediate edge as a Branch of the MST. <p> We also show that with these changes, our revised algorithm satisfies the desired correctness and optimality conditions. This work arose from our pragmatic efforts to find a good MST algorithm for real applications reported in <ref> [Fal95] </ref>. In that work, apart from discussing theoretical issues, we tested the performance of several distributed MST algorithms after constructing detailed implementations of each one in a simulated communication network environment. Our results indicate that there is still a lot of room for improvement in this problem domain.
Reference: [FM95] <author> Michalis Faloutsos and Mart Molle. </author> <title> Creating optimal distributed algorithms for minimum spanning trees. </title> <note> Technical Report CSRI-327 (also submitted in WDAG '95), </note> <year> 1995. </year>
Reference-contexts: Therefore, we achieve fast level increases and the communication and time complexity of this phase are O (E + N log (N )) and O (N ) respectively (see [Awe87] for details). Given the Spanning Tree, the number of nodes in the network can easily be counted. In <ref> [FM95] </ref>, a simple counting algorithm is presented. Its complexity is O (E) messages and O (N ) time. It assumes that one node can be authorized to initiate the algorithm. 2.2 Small Fragment MST Phase This second phase is unchanged from [Awe87].
Reference: [Gaf85] <author> Eli Gafni. </author> <title> Improvements in the time complexity of two message-optimal election algorithms. </title> <booktitle> Proceedings of 1985 Principles Of Distributed Computing (PODC), Conference, </booktitle> <address> Minacki, Ontario, </address> <month> August, </month> <year> 1985. </year>
Reference-contexts: competitive complexity independently of the way optimality will be defined. 1.1 Basic Algorithm of Gallager, Humblet and Spira In their pioneering paper [GHS83], Gallager, Humblet and Spira introduced the distributed MST problem and presented an algorithm that has formed the basis of subsequent work in the area, for example [CT85], <ref> [Gaf85] </ref>, [Awe87] and [Fal95]. In their algorithm, each node is initially the root of its own fragment (a trivial connected subgraph of the MST) and all the edges are Unlabeled. Thereafter, adjacent fragments join to form larger fragments by labeling their intermediate edge as a Branch of the MST.
Reference: [GHS83] <author> R.G. Gallager, </author> <title> P.A. Humblet, and P.M. Spira. A distributed algorithm for minimum weight spanning trees. </title> <journal> ACM Trans. on Programming Languages and Systems, </journal> <volume> 5(1) </volume> <pages> 66-77, </pages> <month> January </month> <year> 1983. </year>
Reference-contexts: Recent research in the area suggests that the diameter of a network is a more accurate parameter for describing the time complexity [GKP93]. However, it was also proven [SB95] that a tighter bound for the termination time of the algorithm <ref> [GHS83] </ref>, presented below, is O ((D + d) log (N )), where d is the diameter of the resulting MST and D is the maximum degree of the nodes. <p> to be the optimal time bound and leave the resolution of this argument to future research, since we have reasons to believe that our algorithm will be of competitive complexity independently of the way optimality will be defined. 1.1 Basic Algorithm of Gallager, Humblet and Spira In their pioneering paper <ref> [GHS83] </ref>, Gallager, Humblet and Spira introduced the distributed MST problem and presented an algorithm that has formed the basis of subsequent work in the area, for example [CT85], [Gaf85], [Awe87] and [Fal95]. <p> Finally the root sends a changeRoot message to the node adjacent to the MOE, appointing it as the new leader of the fragment. The leader sends a connect message along that edge and joins with the other fragment. Even the basic algorithm presented in <ref> [GHS83] </ref> contains several subtleties. First, each fragment has a level, L, in addition to its unique fragment identifier, F . The fragment levels are used to make fragment joining less symmetric, so that certain types of "one-sided" joins can be permitted without the risk of forming cycles. <p> It can be shown (e.g., <ref> [GHS83] </ref>) that the message complexity of this algorithm is O (E + N log (N )), and hence optimal. However, its time complexity is O (N log (N )) and hence not optimal. <p> The second phase starts to build the MST by following the basic <ref> [GHS83] </ref> algorithm, described above. However, as the fragments grow larger, they switch to a more complicated phase III algorithm as soon as their sizes reach N log (N) . The estimation of the size is done trivially in the reporting procedure; all nodes that report are counted. <p> The estimation of the size is done trivially in the reporting procedure; all nodes that report are counted. Awerbuch's third phase differs from the basic <ref> [GHS83] </ref> algorithm through the addition of two new procedures designed to limit the time between level increases. <p> Since [Awe87] does not specify a new policy, we must assume that the algorithm has inherited the fragment joining policy from the basic <ref> [GHS83] </ref> algorithm. Unfortunately, the joining policy in [GHS83] does not satisfy this assumption, even if the fragment (s) are of minimum level. <p> Since [Awe87] does not specify a new policy, we must assume that the algorithm has inherited the fragment joining policy from the basic <ref> [GHS83] </ref> algorithm. Unfortunately, the joining policy in [GHS83] does not satisfy this assumption, even if the fragment (s) are of minimum level. As a counterexample, observe that the last fragment in a long chain of minimum level fragments, each submitting to its neighbor, could wait an arbitrarily long time before receiving an answer to its connect message. <p> It assumes that one node can be authorized to initiate the algorithm. 2.2 Small Fragment MST Phase This second phase is unchanged from [Awe87]. It begins with each node acting as the root of a trivial one-node MST fragment executing the basic <ref> [GHS83] </ref> algorithm. <p> If any smaller-level neighboring fragments try to submit before the node leaves the Testing state, it accepts the submission and then sends a copy of the initiate message. Edge Testing. The Testing policy is the same as in <ref> [GHS83] </ref>. Nodes query their Unvisited edges, one at a time in increasing order, by sending a test (F; L) message. Edges connecting nodes belonging to the same fragment are rejected using two messages, either a test answered by reject or two test messages sent concurrently. <p> As we already said, such a characteristic doesn't exist in the leader join protocol that was described in <ref> [GHS83] </ref>, and seems to have been inherited by [Awe87]. Thus, unlike the joining protocol in [GHS83], ours offers a guarantee that the delay until the leader receives some answer from the adjacent fragment, is proportional to the other fragment's size. <p> As we already said, such a characteristic doesn't exist in the leader join protocol that was described in <ref> [GHS83] </ref>, and seems to have been inherited by [Awe87]. Thus, unlike the joining protocol in [GHS83], ours offers a guarantee that the delay until the leader receives some answer from the adjacent fragment, is proportional to the other fragment's size. <p> For termination, the following theorem holds. Theorem 1 The revised algorithm is deadlock free. A proof can be found in <ref> [GHS83] </ref>. To prove that the algorithm finds the minimum of the spanning trees we can recall the fact that the MST problem can be solved by a "greedy" algorithm. In other words it is sufficient to verify that the algorithm makes fragments to try and join only along their MOE. <p> Processing time for each message within a node and queuing delays are considered negligible. The length of messages is O (log (N )), i.e., capable of representing N node identifiers. It is optimal and it won't be discussed further (see <ref> [GHS83] </ref>). We will calculate the complexity of the algorithm by summing the complexities of its three phases. 4.1 Communication Complexity Phase I: The communication complexity of this phase was proven optimal [Awe87]. <p> Phase I: In the Counting phase adjacent fragments join in a way similar to that of <ref> [GHS83] </ref>, but since it is trying to find just a Spanning Tree it is relatively easy to guarantee that fragments almost never wait for other trees (for a detailed proof see [Awe87]). <p> We can assume that for each of these subfragments, we run independently a <ref> [GHS83] </ref> algorithm for a graph of size N log (N) and thus the termination time, which is the product of the maximum level and the size of the graph, is O (N ). Phase III: we will provide an overview of the proof.
Reference: [GKP93] <author> J.A. Garay, S. Kutten, and D. Peleg. </author> <title> A sub-linear time distributed algorithm for minimum-weight spanning trees. </title> <booktitle> Proceedings of Foundations Of Computer Science (FOCS), </booktitle> <volume> 34, </volume> <year> 1993. </year>
Reference-contexts: In addition, there are graphs where the time complexity is at least (N ) assuming that each message delivery takes one time unit. Recent research in the area suggests that the diameter of a network is a more accurate parameter for describing the time complexity <ref> [GKP93] </ref>. However, it was also proven [SB95] that a tighter bound for the termination time of the algorithm [GHS83], presented below, is O ((D + d) log (N )), where d is the diameter of the resulting MST and D is the maximum degree of the nodes.
Reference: [SB95] <author> Gurdip Singh and Arthur J. Bernstein. </author> <title> A highly asynchronous minimum spanning tree protocol. </title> <note> to appear in Distributed Computing, Spinger Verlag 8(3), </note> <year> 1995. </year>
Reference-contexts: Recent research in the area suggests that the diameter of a network is a more accurate parameter for describing the time complexity [GKP93]. However, it was also proven <ref> [SB95] </ref> that a tighter bound for the termination time of the algorithm [GHS83], presented below, is O ((D + d) log (N )), where d is the diameter of the resulting MST and D is the maximum degree of the nodes.
References-found: 8

