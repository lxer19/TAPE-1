URL: http://www.isle.org/~langley/papers/grids.late.ps
Refering-URL: http://www.isle.org/~langley/grammar.html
Root-URL: 
Email: Langley@cs.stanford.edu  
Phone: Phone  
Title: Simplicity and Representation Change in Grammar Induction  
Author: Pat Langley 
Keyword: Representation change, feature construction, grammar induction, simplicity  
Note: Electronic mail:  number: (415) 725-8813 Also affiliated with the Institute for the Study of Learning and Expertise, 2164 Staunton  
Address: Stanford, CA 94305  Palo Alto, CA 94306.  
Affiliation: Robotics Laboratory, Computer Science Dept. Stanford University,  Court,  
Abstract: In this paper we examine the role of a bias toward simplicity in directing the process of representation change. We focus on the task of inducing context-free grammars from sample sentences, and we present a rational reconstruction of Wolff's SNPR the Grids system that incorporates the simplicity bias. The basic induction method alternates between merging existing nonterminal symbols and creating new symbols, using hill-climbing search to move from complex to simpler grammars. In the process, the algorithm creates word classes, phrases, and recursive rewrite rules that move beyond the training sentences, using the simplicity metric to direct search and negative training cases to eliminate overly general grammars. Experiments reveal that this approach can induce accurate grammars and that it scales reasonably to more difficult domains. Moreover, comparative studies show that both the simplicity bias and negative instances play an important role in constraining search, and that simplicity compares favorably with an alternative scheme that estimates accuracy using a separate evaluation set. We discuss Grids' relation to other work on grammar induction and representation change, and we draw some tentative conclusions about the role of simplicity in such tasks. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Anderson, J. R. </author> <year> (1977). </year> <title> Induction of augmented transition networks. </title> <booktitle> Cognitive Science, </booktitle> <pages> 1 , 125-157. </pages>
Reference: <author> Angluin, D. </author> <year> (1980). </year> <title> Inductive inference of formal languages from positive data. </title> <note> Information and Control , 45 , 117-135. </note>
Reference: <author> Berwick, R. C. </author> <year> (1980). </year> <title> Computational analogues of constraints on grammars: A model of syntactic acquisition. </title> <booktitle> Proceedings of the 18th Annual Meeting of the Association for Computational Linguistics (pp. </booktitle> <pages> 49-53). </pages> <address> Philadelphia: </address> <publisher> ACL. </publisher>
Reference: <author> Brieman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. </author> <year> (1984). </year> <title> Classification and regression trees. Belmont: </title> <publisher> Wadsworth. </publisher>
Reference-contexts: The literature on pruning and overfitting suggested one plausible alternative. Some work in this area (e.g., Quinlan & Rivest, 1989) has relied on minimum description length, another embodiment of the simplicity bias, to make pruning decisions. But other work <ref> (e.g., Brieman, Friedman, Olshen, & Stone, 1984) </ref> instead uses methods for cross validation, in which one holds back some training data to estimate the expected error on a separate test set, to prevent overfitting.
Reference: <author> Cook, C. M., Rosenfeld, A., & Aronson, A. </author> <year> (1976). </year> <title> Grammatical inference by hill climbing. </title> <booktitle> Informational Sciences, </booktitle> <pages> 10 , 59-80. </pages>
Reference: <author> Elman, J. L. </author> <year> (1991). </year> <title> Distributed representations, simple recurrent networks, and grammatical structure. </title> <booktitle> Machine Learning, </booktitle> <pages> 7 , 195-225. </pages>
Reference-contexts: Not all work on grammar induction relies on the simplicity bias <ref> (see Elman, 1991, for one counterexample) </ref>, but the idea does play a recurring role. The literature also contains many formal results on the learnability of languages under a variety of conditions, most of which deal with learning only from positive training sentences.
Reference: <author> Gold, E. M. </author> <year> (1967). </year> <title> Language identification in the limit. </title> <journal> Information and Control, </journal> <pages> 10 , 447-474. </pages>
Reference: <author> Hopcroft, J. E., & Ullman, J. D. </author> <year> (1979). </year> <title> Introduction to automata theory, </title> <booktitle> languages, and computation. </booktitle> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference: <author> Langley, P., & Zytkow, J. M. </author> <year> (1989). </year> <title> Data-driven approaches to empirical discovery. </title> <booktitle> Artificial Intelligence, </booktitle> <pages> 40 , 283-312. </pages>
Reference: <author> Ling, X. C., & Narayan, M. A. </author> <year> (1991). </year> <title> A critical comparison of various methods based on inverse resolution. </title> <booktitle> Proceedings of the Eighth International Workshop on Machine Learning (pp. </booktitle> <pages> 173-177). </pages> <address> Evanston, IL: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Matheus, C. J. </author> <year> (1991). </year> <title> The need for constructive induction. </title> <booktitle> Proceedings of the Eighth International Workshop on Machine Learning (pp. </booktitle> <pages> 173-177). </pages> <address> Evanston, IL: </address> <note> Morgan Kaufmann. Simplicity and Representation Change 18 Matheus, </note> <author> C. J., & Rendell, L. A. </author> <year> (1989). </year> <title> Constructive induction on decision trees. </title> <booktitle> Proceedings of the Eleventh International Conference on Artificial Intelligence (pp. </booktitle> <pages> 645-650). </pages> <address> Detroit: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Muggleton, S. </author> <year> (1987). </year> <title> Duce: An oracle-based approach to constructive induction. </title> <booktitle> Proceedings of the Tenth International Conference on Artificial Intelligence (pp. </booktitle> <pages> 287-292). </pages> <address> Milan: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Muggleton, S., & Buntine, W. </author> <year> (1988). </year> <title> Machine invention of first-order predicates by inverting resolution. </title> <booktitle> Proceedings of the Fifth International Conference on Machine Learning (pp. </booktitle> <pages> 339-352). </pages> <address> Ann Arbor, MI: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Research within this framework has addressed the induction of numeric laws (e.g., Nordhausen & Langley, 1990), the creation of terms for use in decision-tree induction (e.g., Matheus & Rendell, 1989; Pagallo, 1989), and the use of inverse resolution to induce Horn clause programs <ref> (e.g., Muggleton & Buntine, 1988) </ref>. We will not consider these areas in detail, but Langley and Zytkow (1989), Matheus (1991), and Ling and Narayan (1991) present overviews of each, respectively.
Reference: <author> Nordhausen, B., & Langley, P. </author> <year> (1990). </year> <title> A robust approach to numeric discovery. </title> <booktitle> Proceedings of the Seventh International Conference on Machine Learning (pp. </booktitle> <pages> 411-418). </pages> <address> Austin, TX: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This approach to learning goes by a variety of names, including constructive induction, representation change, and feature construction. Research within this framework has addressed the induction of numeric laws <ref> (e.g., Nordhausen & Langley, 1990) </ref>, the creation of terms for use in decision-tree induction (e.g., Matheus & Rendell, 1989; Pagallo, 1989), and the use of inverse resolution to induce Horn clause programs (e.g., Muggleton & Buntine, 1988).
Reference: <author> Pagallo, G. </author> <year> (1989). </year> <title> Learning DNF by decision trees. </title> <booktitle> Proceedings of the Eleventh International Conference on Artificial Intelligence (pp. </booktitle> <pages> 639-644). </pages> <address> Detroit: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Quinlan, J. R., & Rivest, R. L. </author> <year> (1989). </year> <title> Inferring decision trees using the minimum description length principle. </title> <booktitle> Information and Computation, </booktitle> <pages> 80 , 227-248. </pages>
Reference-contexts: Concern with this issue led us to consider other ways to direct search through a space of representational terms. The literature on pruning and overfitting suggested one plausible alternative. Some work in this area <ref> (e.g., Quinlan & Rivest, 1989) </ref> has relied on minimum description length, another embodiment of the simplicity bias, to make pruning decisions.
Reference: <author> Stolcke, A., & Omohundro, S. </author> <year> (1993). </year> <title> Hidden Markov model induction by Bayesian model merging. </title>
Reference: <editor> In C. L. Giles, S. J. Hanson, & J. D. Cowan (Eds.), </editor> <booktitle> Advances in neural information processing systems 5 . San Mateo, </booktitle> <address> CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Tomita, M. </author> <year> (1982). </year> <title> Dynamic construction of finite-state automata from examples using hill climbing. </title> <booktitle> Proceedings of the Fourth Conference of the Cognitive Science Society (pp. </booktitle> <pages> 105-108). </pages> <address> Ann Arbor, MI: </address> <publisher> Lawrence Erlbaum. </publisher>
Reference: <author> VanLehn, K., & Ball, W. </author> <year> (1987). </year> <title> A version space approach to learning context-free grammars. </title> <booktitle> Machine Learning, </booktitle> <pages> 2 , 39-74. </pages>
Reference: <author> Wharton, R. M. </author> <year> (1977). </year> <title> Grammar enumeration and inference. </title> <note> Information and Control , 33 , 253-272. </note>
Reference: <author> Wolff, J. G. </author> <year> (1978). </year> <title> Grammar discovery as data compression. </title> <booktitle> Proceedings of the AISB/GI Conference on Artificial Intelligence (pp. </booktitle> <pages> 375-379). </pages> <address> Hamburg, West Germany. </address>
Reference: <author> Wolff, J. G. </author> <year> (1987). </year> <title> Cognitive development as optimisation. </title> <editor> In L. Bolc (Ed.), </editor> <booktitle> Computational models of learning. </booktitle> <address> Berlin: </address> <publisher> Springer-Verlag. </publisher>
References-found: 23

