URL: ftp://ftp.idiap.ch/pub/papers/neural/saxena.icecs.ps.Z
Refering-URL: http://www.idiap.ch/~perry/allpubs.html
Root-URL: http://www.idiap.ch/~perry/allpubs.html
Email: fisaxena,efiesler,perryg@idiap.ch  
Title: A METHOD FOR ALL-POSITIVE OPTICAL MULTILAYER PERCEPTRONS  
Author: I. Saxena E. Fiesler P. Moerland 
Address: CP 592, CH-1920 Martigny, Switzerland,  
Affiliation: IDIAP,  
Date: 13-16 OCTOBER, 1996.  
Note: PUBLISHED IN THE PROCEEDINGS OF THE THIRD IEEE INTERNATIONAL CONFERENCE ON ELECTRONICS, CIRCUITS, AND SYSTEMS (ICECS'96) RODOS, GREECE,  
Abstract:  
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. M. Bishop, </author> <title> Neural Networks for Pattern Recognition. </title> <publisher> Oxford: Oxford University Press, </publisher> <year> 1995. </year>
Reference-contexts: A sketch of this topology and the associated terminology is given in Figure 1. It is well-known that these networks can approximate arbitrarily well any continuous functional mapping and can learn the weight values suitable for a given data set; see for example <ref> [1] </ref>. The virtually unlimited range of applications of neural networks and their massive parallelism has fostered the development of optoelectronic and optical neural network implementations. Most optical implementations are incoherent systems based on intensity encoding which are lacking optical subtraction. <p> Summary of the benchmarks used in the experiments. #Runs " learning initial rate weights XOR 50 0.1 0.3 <ref> [1; 1] </ref> Sonar 10 0.3 0.1 [1; 1] Wine 10 - 0.3 [0:5; 0:5] Cancer 10 - 0.3 [0:5; 0:5] Table 2. Summary of the parameters used in the experiments. <p> Summary of the benchmarks used in the experiments. #Runs " learning initial rate weights XOR 50 0.1 0.3 <ref> [1; 1] </ref> Sonar 10 0.3 0.1 [1; 1] Wine 10 - 0.3 [0:5; 0:5] Cancer 10 - 0.3 [0:5; 0:5] Table 2. Summary of the parameters used in the experiments.
Reference: [2] <author> M. Kranzdorf, B. J. Bigner, L. Zhang and K. M. </author> <title> Johnson,"Optical Connectionist Machine with Polarization-Based Bipolar Weight Values,"Optical Engineering, </title> <journal> vol. </journal> <volume> 28, no. 8, </volume> <pages> pp. 844-848, </pages> <publisher> Society of Photo-Optical Instrumentation Engineers, </publisher> <month> August </month> <year> 1989. </year>
Reference-contexts: Given these limitations, subtraction is usually realized as an electronic difference of two photode-tected quantities whose separation is based either on two states of a characteristic parameter of light (polarization <ref> [2] </ref>) or on spatial or temporal separation [3]. Such photo-electric conversion and subsequent regeneration at optical sources, is not an optimal solution as the light propagation gets interrupted, preventing all-optical neural processing at hidden layers.
Reference: [3] <author> N. Farhat and D. Psaltis, </author> <title> "New Approach to Optical Information Processing Based on Hop-field Model," </title> <journal> Journal of the Optical Society of America, </journal> <volume> vol. </volume> <editor> A1, p. </editor> <volume> 1337, </volume> <year> 1984. </year>
Reference-contexts: Given these limitations, subtraction is usually realized as an electronic difference of two photode-tected quantities whose separation is based either on two states of a characteristic parameter of light (polarization [2]) or on spatial or temporal separation <ref> [3] </ref>. Such photo-electric conversion and subsequent regeneration at optical sources, is not an optimal solution as the light propagation gets interrupted, preventing all-optical neural processing at hidden layers.
Reference: [4] <author> F. M. Dickey and J. M. DeLaurentis, </author> <title> "Optical Neural Networks with Unipolar Weights," </title> <journal> Optics Communications, </journal> <volume> vol. 101, no. 5/6, </volume> <pages> pp. 303-305, </pages> <address> North-Holland/Elsevier, </address> <year> 1993. </year>
Reference: [5] <author> J. M. DeLaurentis and F. M. </author> <title> Dickey,"A Convexity Based Analysis of Neural Networks," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 7, no. 1, </volume> <pages> pp. 141-146, </pages> <publisher> Pergamon Press, </publisher> <year> 1994. </year>
Reference: [6] <author> H. J. White and W. A. Wright, </author> <title> "Holographic Implementations of a Hopfield Model with Discrete Weights," </title> <journal> Applied Optics, </journal> <volume> vol. 27, no. 2, </volume> <pages> pp. 331-338, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: The use of both monotonically increasing and decreasing activation functions in the same network [4][5] has been proposed as a way to overcome the subtraction problem. However, the subtraction of biases is still required in this method. Another proposal is the variable thresholding approach <ref> [6] </ref> for Hopfield nets, which in principle overcomes the need for subtraction. However, to be more generally applicable, its realization would require optical devices with suitably matched characteristics. Recent work in this area [7] has also been motivated by the encumbrance of performing subtraction electronically in optical neural networks (ONNs).
Reference: [7] <author> Y. Hayasaki, I. Tohyama, M. Mori, and S. Ishi-hara, </author> <title> "Reversal-Input Superposing Technique for All-Optical Neural Networks," </title> <journal> Applied Optics, </journal> <volume> vol. 33, no. 8, </volume> <pages> pp. 1477-1484, </pages> <year> 1994. </year>
Reference-contexts: Another proposal is the variable thresholding approach [6] for Hopfield nets, which in principle overcomes the need for subtraction. However, to be more generally applicable, its realization would require optical devices with suitably matched characteristics. Recent work in this area <ref> [7] </ref> has also been motivated by the encumbrance of performing subtraction electronically in optical neural networks (ONNs). Their implementation still requires subtraction of problem-dependent biases and does not permit optical thresholding. The ideal solution would be to incorporate subtraction in incoherent ONNs in a practical way.
Reference: [8] <author> I. Saxena and E. Fiesler, </author> <title> "Adaptive Multilayer Optical Neural Network with Optical Thresholding," </title> <journal> Optical Engineering, </journal> <volume> vol. 34, no. 8, </volume> <pages> pp. 2435-2440, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: In the method described, the thresholding, S (), is done optically, where S (x) is defined for x 0 <ref> [8] </ref>. Figure 2 shows an example of such an all-positive non-linear response curve of a liquid crystal light valve. <p> The non-linear thresholding function used in the simulations is a sigmoid that has been translated by 2.75 along the positive x-axis. This sigmoid has a y-intercept of 0:06, which corresponds to the maximum y-intercept of the transfer curves of 4 different liquid crystal light valves available <ref> [8] </ref>. The experiments consist of a backpropagation 2 (BP) training of a three-layer 3 neural network, using this translated sigmoid with and without truncation at the origin. A set of benchmarks including three real-world problems and the eXclusive OR (XOR) problem has been used. <p> IMPLEMENTATION OF SUBTRACTION COMPENSATION An adaptive multilayer optical neural network which enables all-optical forward propagation and learning under the control of a computer has been described by Saxena and Fiesler <ref> [8] </ref>. The optical system uses liquid crystal televisions (LCTVs) to implement the matrix-vector multiplication of a weight matrix and an input vector a i , whereas liquid crystal light valves (LCLVs) are used to implement non-linear thresholding. See figure 2 for a typical LCLV response curve.
Reference: [9] <author> P. Moerland, E. Fiesler, and I. Saxena, </author> <title> "The Effects of Optical Thresholding in Backpropagation Neural Networks," </title> <booktitle> Proceedings of the International Conference on Artificial Neural Networks (ICANN'95), </booktitle> <volume> vol. 2, </volume> <pages> pp. 339-343, </pages> <year> 1995. </year>
Reference: [10] <author> D. Rumelhart, G. Hinton, and R. Williams, </author> <title> "Learning Internal Representations by Error Propagation," </title> <booktitle> in Parallel Distributed Processing: Explorations in the Microstructure of Cognition. </booktitle> <address> Cambridge, Massachusetts: </address> <publisher> MIT Press, vol. </publisher> <address> 1: </address> <booktitle> Foundations, </booktitle> <pages> pp. 318-362, </pages> <year> 1986. </year>
Reference-contexts: As was to be expected there are some differences between the results for the untruncated and truncated sigmoid. 1 # of neurons in the input-hidden-output layer. Figure 1 shows, for example, a 2-4-2 topology. 2 See, for example, <ref> [10] </ref>. 3 A layer is defined to be a layer of neurons [11]. #Iterations %Conv. %Miscl. XOR 1403.9 100.0 - Sonar 859.8 80.0 - Wine 816.5 - 2.95 Cancer 41.0 - 1.32 Table 3.
Reference: [11] <author> E. Fiesler. </author> <title> "Neural Network Topologies," in The Handbook of Neural Computation, </title> <editor> E. Fiesler and R. Beale (Editors-in-Chief), </editor> <publisher> Oxford University Press and IOP Publishing, </publisher> <year> 1996. </year> <note> (ISBN: 0-19-509138-8). </note>
Reference-contexts: Figure 1 shows, for example, a 2-4-2 topology. 2 See, for example, [10]. 3 A layer is defined to be a layer of neurons <ref> [11] </ref>. #Iterations %Conv. %Miscl. XOR 1403.9 100.0 - Sonar 859.8 80.0 - Wine 816.5 - 2.95 Cancer 41.0 - 1.32 Table 3. Results without truncation to zero. #Iterations: number of BP training iterations. %Conv.: percentage of converged runs. %Miscl.: percentage of misclassified test patterns. #Iterations %Conv. %Miscl.
Reference: [12] <author> P. Moerland, E. Fiesler, and I. Saxena, </author> <title> "Discrete Multilayer Perceptrons for All-Optical Implementation," in preparation for submission to Optical Engineering, </title> <booktitle> 1996. </booktitle> <pages> 4 </pages>
Reference-contexts: For the benchmarks that involve generalization, a cross-validation technique was used to decide on when to stop training. This technique involves a training, validation, and test set. A more detailed description of the benchmark problems and the simulation conditions can be found in <ref> [12] </ref>. The simulation results are outlined in Table 3 for the untruncated sigmoid and in Table 4 for the truncated sigmoid. All the results are averaged over a certain number of runs (#Runs in Table 2) with different random weight initializations.
References-found: 12

