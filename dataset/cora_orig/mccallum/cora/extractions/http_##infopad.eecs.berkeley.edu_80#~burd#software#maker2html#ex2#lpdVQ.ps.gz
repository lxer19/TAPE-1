URL: http://infopad.eecs.berkeley.edu:80/~burd/software/maker2html/ex2/lpdVQ.ps.gz
Refering-URL: http://infopad.eecs.berkeley.edu:80/~burd/software/maker2html/ex2/
Root-URL: http://www.cs.berkeley.edu
Title: Low Power Design of Memory Intensive Functions Case Study: Vector Quantization  
Author: David B. Lidsky, Jan M. Rabaey 
Address: Berkeley, California 94720 USA  
Affiliation: Department of Electrical Engineering and Computer Science University of California, Berkeley  
Abstract: This paper demonstrates techniques to optimize power consumption of memory intensive applications. A design example -a video, vector quantizer encoder-demonstrates how optimization at the algorithm, ar chitecture and circuit level can reduce power consumption by reducing both the effective switched capacitance and the required speed of the systems memory. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Rabaey, </author> <title> Digital Integrated Circuit: A Design Perspective, Prentice Hall, </title> <note> to be published in 1994. </note>
Reference: [2] <author> H.Veendrick, </author> <title> Short-Circuit Dissipation of Static CMOS Circuitry and Its Impact on the Design of Buffer Circuits, </title> <journal> IEEE JSSC, </journal> <volume> vol. sc-19, </volume> <pages> pp. 468-473, </pages> <month> August </month> <year> 1984. </year>
Reference: [3] <author> A. Chandrakasan, et al, </author> <title> Low-power CMOS Digital Design, </title> <journal> IEEE JSSC, Vol.27, </journal> <volume> No 4, </volume> <pages> pp. 473-484, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: Clock frequency may also be reduced by the judicial use of parallel or pipelined structures. Parallel architectures and dedicated hardware can also be used to reduce the ef fec-tive switching capacitance <ref> [3] </ref>. Low-power design must be approached at all levels. The high level decisions have profound effects on power consumption (section 4.1), because they set bounds on the amount of computation required.
Reference: [4] <author> R. Brodersen, et al, </author> <title> Design Considerations for Portable Systems, </title> <journal> IEEE JSSC, </journal> <volume> vol 27, </volume> <pages> no.4 pp. 473-484, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: VECTOR QUANTIZATION Vector Quantization (VQ) is a data compression method used in voice recognition and video systems. For example, the wireless InfoPad terminal being developed at UC Berkeley <ref> [4] </ref> requires low power video compression of grey scale images. The VQ encoder, whose design is discussed herein, is specifically customized for future use on such portable units. In video quantization, a video image is broken up into a sequence of 4x4 pixel images.
Reference: [5] <editor> A. Gersho, et al, </editor> <title> Vector Quantization and Signal Compression, </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, MA, </address> <year> 1992. </year>
Reference-contexts: This codebook is generated a priori with the intention of covering enough of the vector space to give a good representation of all probable vectors. The creation of the codebook is studied in greater detail in <ref> [5] </ref>. After compression, an 8 bit word is generated delineating the address of a codevec-tor which approximates the original 4x4 vector image. This corresponds to a compression ratio of 16:1, since 16 8-bit words are now represented by a single word. <p> The switched capacitance per vector is also comparatively large due to the amount of computation required. Tree Search Vector Quantization (TSVQ): Tree Search Vector Quantizer (TSVQ) encoding <ref> [5] </ref> requires far less computation. TSVQ performs a binary search, instead of a full search, of the vector space. As a result, the computational complexity is proportional to log 2 N, rather than N, where N is the number of vectors in the codebook. <p> There are many other vector quantization options that may be explored, such as a Pruned Tree Search Vector Quantizer (PTSVQ), which has the advantage of requiring fewer memory accesses. Another option, using adaptive codebooks for greater accuracy, can be used in both FSVQ and TSVQ <ref> [5] </ref>. 4. POWER OPTIMIZATIONS When designing for low power, decisions must be made on all levels including compression method, algorithmic choices and possible mathematical optimizations or transformations. <p> Computational intensity is summarized in table 1. While there is significant power savings in TSVQ as compared to FSVQ, there is a slight degradation in signal quality . This distortion, however, is not as great in most cases as to be prohibitive <ref> [5] </ref>, thus TSVQ is chosen for its greater power ef fi-ciency.
Reference: [6] <author> W. Fang, et al., </author> <title> A Systolic Tree-Searched Vector Quantizer for Real-Time Image Compression, </title> <booktitle> Proc.VLSI Signal Processing IV, </booktitle> <publisher> IEEE Press, </publisher> <address> NY, </address> <year> 1990. </year>
Reference-contexts: Mathematical Optimizations: In TSVQ, there is a lar ge computational reduction available by mathematically rearranging the computation of the difference between the original vector, X, and two codevectors C a and C b <ref> [6] </ref>: (2) Since in TSVQ the same two codevectors are always compared, the calculation of the errors can be combined under one summation.
Reference: [7] <author> R. Kolagotla, et al, </author> <title> VLSI Implementation of a Tree Searched Vector Quantizer, </title> <journal> IEEE Trans. on Signal Proc., </journal> <volume> vol. 41, no 2, </volume> <month> February </month> <year> 1993 </year>
Reference-contexts: Therefore methods to reduce the critical path, such as pipelining and parallelism, can not be implemented without duplicating the entire memory. If the memory is partitioned, however , and a distributed memory approach is used <ref> [7] </ref>, pipeline stages can be introduced, enabling reductions in voltage, clocking frequency, and switching capacitance. The distributive memory architecture is discussed in a later section. Memory Architecture: For each vector comparison, there are sixteen eight-bit words that must be accessed. <p> Distributive Memory: In TSVQ each level of a tree has specific codevectors associated with it, and the codevectors from each level are found only at that level. Therefore the memory can be partitioned into separate memories for each level of the tree <ref> [7] </ref>. Associated with each memory are identical processing elements and controllers. With this architecture, a pipelined structure may now be utilized and the critical path is reduced drastically (figure 5). The chip can now process eight vectors simultaneously with only a negligible increase in latency.
Reference: [8] <author> C. Svensson, et al, </author> <title> High-Speed CMOS Circuit Technique, </title> <journal> IEEE JSSC, </journal> <pages> pp. 62-70, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: Memory Architecture: For each vector comparison, there are sixteen eight-bit words that must be accessed. A technique often used in high throughput designs is to grab bytes of words rather than single words in order to reduce the critical path, allowing the reduction of clock speed and voltage <ref> [8] </ref>. In the case of the TSVQ, however, pre-fetching can not be used to lower the critical path since the location of the vector to be chosen is dependent on the operations at the previous node of the tree.
Reference: [9] <author> A. Chandrakasan, et al, </author> <title> A low Power Chipset for Portable Multimedia Applications, ISSCC, </title> <publisher> March1994. </publisher>
Reference-contexts: Memory is implemented with a low-power , on-chip SRAM. To reduce power dissipation, the bit-lines are pre-char ged to an NMOS threshold below the supply voltage to limit voltage swing. The memory is broken up into blocks with only one active at a time reducing spurious transitions <ref> [9] </ref>. 5. IMPLEMENTATION To illustrate the effectiveness of the multistage approach to power reduction, three architectures were implemented: the single memory with serial access, and the single and distributive memory TSVQ encoders with parallel memory access.
References-found: 9

