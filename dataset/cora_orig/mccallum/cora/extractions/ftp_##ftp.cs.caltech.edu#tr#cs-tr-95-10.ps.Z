URL: ftp://ftp.cs.caltech.edu/tr/cs-tr-95-10.ps.Z
Refering-URL: ftp://ftp.cs.caltech.edu/tr/INDEX.html
Root-URL: http://www.cs.caltech.edu
Email: john-t@cs.caltech.edu  
Title: Performance of a Class of Highly-Parallel Divide-and-Conquer Algorithms  
Author: John Thornley 
Date: October 1, 1995  
Web: http://www.cs.caltech.edu/~john-t/  
Address: Pasadena, California 91125, U.S.A.  
Affiliation: Computer Science Department California Institute of Technology  
Abstract: A wide range of important problems have efficient methods of solution based on the divide-and-conquer strategy. However, the traditional approach to parallel divide-and-conquer does not scale well due to the sequential component of the algorithms. We investigate the performance of a non-traditional, highly-parallel divide-and-conquer strategy that we refer to as "one-deep parallel divide-and-conquer". As representative examples, we measure and analyze the performance of highly-parallel variants of the quicksort and mergesort algorithms on a shared-memory multiprocessor. Both algorithms deliver impressive speedups. We present a systematic approach to modeling the performance of one-deep parallel divide-and-conquer algorithms and we demon strate this approach with the two parallel sorting algorithms.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. J. DeWitt, J. F. Naughton, and D. A. Schneider. </author> <title> Parallel sorting on a shared-nothing architecture using probabilistic splitting. </title> <booktitle> In International Conference of Parallel and Distributed Information Systems, </booktitle> <pages> pages 280-291, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: In comparison, traditional parallel quicksort gives speedups of only 6-fold on 16 processors and 7-fold on 32 processors. Neither of the two sorting algorithms is new. The parallel quicksort is also known as the PS (Probabilistic Splitting) algorithm <ref> [1] </ref>, and the parallel mergesort is also known as the PSRS (Parallel Sorting by Regular Sampling) algorithm [2].
Reference: [2] <author> Hanmao Shi and Jonathon Schaeffer. </author> <title> Parallel sorting by regular sampling. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 14(4) </volume> <pages> 361-372, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: Neither of the two sorting algorithms is new. The parallel quicksort is also known as the PS (Probabilistic Splitting) algorithm [1], and the parallel mergesort is also known as the PSRS (Parallel Sorting by Regular Sampling) algorithm <ref> [2] </ref>. However, these algorithms have not previously been recognized as instances of the much wider class of one-deep parallel divide-and-conquer algorithms, and their performance has not previously been analyzed in this more general context. We present a generic framework for modeling the performance of one-deep parallel divide-and-conquer algorithms.
Reference: [3] <author> Ada 95 Reference Manual. </author> <title> International Organization for Standardization, </title> <month> January </month> <year> 1995. </year> <note> International Standard ANSI/ISO/IEC-8652:1995. </note>
Reference-contexts: writes to a given variable and another thread reads or writes to the same variable in parallel. 3 Experimental Methods In this section, we describe the methods that we use in our performance experiments. 3.1 Programming Language The algorithms that we discuss and analyze have been implemented in Ada 95 <ref> [3] </ref>. Ada 95 was chosen primarily because it is a conventional imperative language that provides language-level support for parallelism.
Reference: [4] <author> John Thornley. </author> <title> Integrating parallel dataflow programming with the Ada tasking model. </title> <booktitle> In Proceedings of ACM TRI-Ada '94, </booktitle> <pages> pages 417-428, </pages> <address> Baltimore, Maryland, </address> <month> November 6-11 </month> <year> 1994. </year>
Reference-contexts: A parallel block is equivalent to a block containing a sequence of task declarations, and a parallel for-loop is equivalent to a block declaring an array of tasks. These simple transformations are given in appendix A and discussed in <ref> [4] </ref>. In our programs, parallel blocks and parallel for-loops were manually transformed into Ada tasking constructs. 3.2 Compiler The programs were compiled with the GNAT (GNU-NYU Ada Translator) compiler [5], release 2.08 for the SGI-IRIX operating system.
Reference: [5] <author> Edmond Schonberg and Bernard Banner. </author> <title> The GNAT project: A GNU-Ada 9X compiler. </title> <booktitle> In Proceedings of ACM TRI-Ada '94, </booktitle> <pages> pages 48-57, </pages> <address> Baltimore, Maryland, </address> <month> November 6-11 </month> <year> 1994. </year>
Reference-contexts: These simple transformations are given in appendix A and discussed in [4]. In our programs, parallel blocks and parallel for-loops were manually transformed into Ada tasking constructs. 3.2 Compiler The programs were compiled with the GNAT (GNU-NYU Ada Translator) compiler <ref> [5] </ref>, release 2.08 for the SGI-IRIX operating system. GNAT is an Ada 95 front-end and runtime system for the GCC (GNU C Compiler) family of compilers [6]. GNAT is part of the GNU software distributed by the Free Software Foundation.
Reference: [6] <author> Richard Stallman. </author> <title> Using and Porting GNU GCC. Free Software Foundation, </title> <address> Cambridge, Massachusetts, </address> <year> 1994. </year>
Reference-contexts: GNAT is an Ada 95 front-end and runtime system for the GCC (GNU C Compiler) family of compilers <ref> [6] </ref>. GNAT is part of the GNU software distributed by the Free Software Foundation. The efficiency of code produced by GNAT is comparable to that produced by GCC for C/C++ programs.
Reference: [7] <author> E. W. Giering, Frank Mueller, and T. P. Baker. </author> <title> Features of the GNU Ada runtime library. </title> <booktitle> In Proceedings of ACM TRI-Ada '94, </booktitle> <pages> pages 93-103, </pages> <address> Baltimore, Maryland, </address> <month> November 6-11 </month> <year> 1994. </year>
Reference-contexts: This to be expected, since Ada 95 is a conventional imperative language with similar constructs and capabilities to C/C++ and the GNAT compilation system uses the standard GCC code generator and optimizer as its back-end. Ada 95 tasking is implemented on top of Pthreads (POSIX threads) as described in <ref> [7] </ref>. For our performance experiments, the programs were compiled with the -O2 optimization option and with index and range checking suppressed. 3.3 Computer System Our experiments were performed on from 1 to 32 processors of a 36-processor SGI CHALLENGE system running the IRIX 6.1 operating system.
Reference: [8] <author> William H. Press, Saul A Teukolsky, William T. Vetterling, and Brian P. Flannery, </author> <title> editors. Numerical Recipes in C. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, Great Britain, </address> <note> second edition, </note> <year> 1992. </year>
Reference-contexts: For each experiment, five to ten trials were performed and averaged with outliers discarded to ensure consistent and accurate measurements. 3.4 Sorting Comparison The parallel sorting speedups that we report are all relative to a good sequential quicksort adapted from <ref> [8, section 8.2] </ref>. This quicksort also is used as the sequential base-case in our parallel sorting algorithms. Therefore, any improvements to the sequential quicksort would also improve the performance of the parallel algorithms. All our experiments involve sorting arrays of 32-bit integers randomly generated with a uniform distribution.
Reference: [9] <author> Alfred V. Aho, John E. Hopcroft, and Jeffrey D. Ullman. </author> <title> Data Structures and Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1983. </year>
Reference-contexts: Efficient divide-and-conquer algorithms have been devised to solve problems from a diverse range of application areas, e.g., sorting (quicksort and mergesort), searching (binary search), matrix multiplication (Strassen's algorithm), geometric algorithms (convex hull and closest pair), and signal processing (fast Fourier transform). Any comprehensive algorithms text, e.g., <ref> [9] </ref>, presents and discusses a large number of divide-and-conquer algorithms. One of the canonical examples of divide-and-conquer is quicksort [10][11]. In the quicksort algorithm, an array is sorted in place by partitioning the elements into two smaller subarrays and a pivot element.
Reference: [10] <author> C. A. R. Hoare. </author> <title> Algorithm 64: Quicksort. </title> <journal> Communications of the ACM, </journal> <volume> 4(7):321, </volume> <month> July </month> <year> 1961. </year>
Reference: [11] <author> Robert Sedgewick. </author> <title> Implementing quicksort programs. </title> <journal> Communications of the ACM, </journal> <volume> 21(10) </volume> <pages> 847-857, </pages> <month> October </month> <year> 1978. </year>
Reference: [12] <author> Ellis Horowitz and Alessandro Zorat. </author> <title> Divide-and-conquer for parallel processing. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-32(6):582-585, </volume> <month> June </month> <year> 1983. </year>
Reference: [13] <author> Tom Axford. </author> <title> The divide-and-conquer paradigm as a basis for parallel language design. </title> <editor> In Lydia Kronsjo and Dean Shumsheruddin, editors, </editor> <booktitle> Advances in Parallel Algorithms, </booktitle> <pages> pages 26-65. </pages> <publisher> Halsted Press, </publisher> <address> New York, </address> <year> 1992. </year>
Reference: [14] <author> D. J. Evans and N. Y. Yousif. </author> <title> Analysis of the performance of the parallel quicksort method. </title> <journal> BIT, </journal> <volume> 25 </volume> <pages> 106-112, </pages> <year> 1985. </year>
Reference-contexts: An algorithm template for traditional parallel divide-and-conquer is given in Figure 4. Based on the traditional parallel divide-and-conquer strategy, parallelism can be obtained from quicksort by sorting the two subarrays in parallel <ref> [14] </ref>. This parallel quicksort algorithm is given in Figure 5. In our experiments, we have found an appropriate threshold value to be T = N/(4P), where N is the length of the entire array and P is the number of processors.
Reference: [15] <author> G. M. </author> <title> Amdahl. Validity of the single-processor approach to achieving large-scale computing capabilities. </title> <booktitle> In Proceedings of AFIPS, </booktitle> <volume> volume 30, </volume> <pages> pages 483-485, </pages> <address> Atlantic City, New Jersey, </address> <month> April 18-20 </month> <year> 1967. </year> <month> 25 </month>
Reference-contexts: However, the parallel speedup that can be obtained with this approach is inherently limited by the sequential composition of the divide and combine operations with the parallel block. The essential problem is that parallel speedup is constrained by the sequential component of a program, as described by Amdahl's law <ref> [15] </ref>. For parallel quicksort of N elements on P processors, the linear partition operation is executed before any parallelism is obtained at each level of recursion.
Reference: [16] <author> K. Mani Chandy and Svetlana Kryukova. </author> <title> Patterns of specifications. </title> <note> Paper in preparation, </note> <institution> Computer Science Department, California Institute of Technology. </institution>
Reference-contexts: Quicksort demonstrates a non-trivial parallel divide operation and mergesort demonstrates a non-trivial parallel combine operation. Other work <ref> [16] </ref> presents one-deep parallel divide-and-conquer algorithms to solve several more complicated problems, including the convex 8 Number of Time Actual Speedup processors (seconds) speedup limit sequential 31.13 | | 1 31.17 1.00 1.00 4 9.67 3.22 3.39 16 4.90 6.35 7.38 1 | | 11.13 procedure Parallel Solve (K : in
Reference: [17] <author> Mark J. Clement and Michael J. Quinn. </author> <title> Overlapping computations, communications and I/O in parallel sorting. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 28(2) </volume> <pages> 162-172, </pages> <month> August </month> <year> 1995. </year>
Reference: [18] <author> G. E. Blelloch, C. E. Leiserson, B. M. Maggs, C. G. Plaxton, S. J. Smith, and M. Zagha. </author> <title> A comparison of sorting algorithms for the connection machine CM-2. </title> <booktitle> In 3rd Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <month> July </month> <year> 1991. </year> <month> 26 </month>
Reference-contexts: This is achieved by sampling and sorting a larger number of elements, then choosing evenly-spaced pivots from the sorted sample. Issues involved in choosing an appropriate sample size are discussed in <ref> [18] </ref>. In our algorithm, we choose a sample size such that the time sorting the sample is inversely proportional to the time per processor sorting the Result partitions.
References-found: 18

