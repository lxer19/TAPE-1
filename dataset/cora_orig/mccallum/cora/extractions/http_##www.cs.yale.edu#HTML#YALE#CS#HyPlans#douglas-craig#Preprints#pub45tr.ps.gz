URL: http://www.cs.yale.edu/HTML/YALE/CS/HyPlans/douglas-craig/Preprints/pub45tr.ps.gz
Refering-URL: http://www.cs.yale.edu/HTML/YALE/CS/HyPlans/douglas-craig/ccd-preprints.html
Root-URL: http://www.cs.yale.edu
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> J. Saltz, </author> <title> Parallel and Adaptive Algorithms for Problems in Scientific and Medical Computing, </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Duke University, Durham, NC, </institution> <year> 1985. </year>
Reference-contexts: In an effort to forestall this and to allow increasing numbers of processors to be used effectively, a number of researchers have suggested algorithms which introduce time domain parallelism as well as space domain parallelism <ref> [1, 4, 5, 6] </ref>. It is claimed in [1] and [6] that a parallel scheme based on time domain parallelism will converge in exactly the same number of iterations as the sequential scheme since the two schemes have iteration matrices with equal spectral radii. <p> In an effort to forestall this and to allow increasing numbers of processors to be used effectively, a number of researchers have suggested algorithms which introduce time domain parallelism as well as space domain parallelism [1, 4, 5, 6]. It is claimed in <ref> [1] </ref> and [6] that a parallel scheme based on time domain parallelism will converge in exactly the same number of iterations as the sequential scheme since the two schemes have iteration matrices with equal spectral radii. In this paper, we examine this claim. <p> In this paper, we provide an analysis of a class of these algorithms as well as computational experiments which demonstrate that this approach is not normally useful. 2. The Temporal Method. In [6] and <ref> [1] </ref>, an iterative scheme is described for introducing temporal domain parallelism by solving the linear systems at different time steps simultaneously. The central idea is to assemble n steps of (1) into the form Gu B B B B B A B . . . <p> Hence, both iterations appear to converge at the same rate. Based on the "proof" sketched above that iterative schemes (1) and (2) converge in exactly the same number of iterations, both <ref> [1] </ref> and [6] go on to demonstrate that the parallel algorithm involving both spatial and temporal dimension parallelism is far superior in terms of speedup and communication/computation ratio over algorithms employing only spatial dimension parallelism. However, in the following sections, we demonstrate that this is not valid. 3.
Reference: [2] <author> H. Schwarz, </author> <title> Uber einige abbildungsaufgaben, </title> <journal> Ges. Math. Abh., </journal> <volume> 11 (1869), </volume> <pages> pp. 65-83. </pages>
Reference-contexts: Usually, the entire process is spatially parallelized by splitting the domain into subdomains and distributing problems on the subdomains to multiple processors <ref> [2] </ref>. At each iteration, the processors need to exchange boundary information with processors holding adjacent subdomains. As the number of processors increases, the communication/computation ratio increases making the parallel efficiency decrease.
Reference: [3] <author> R. Varga, </author> <title> Matrix Iterative Analysis, </title> <publisher> Prentice-Hall Inc., </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1962. </year>
Reference-contexts: While the preceding result is true in general (see <ref> [3] </ref>), this argument holds only if the matrix T has m linearly independent eigenvectors. If T is defective, i.e., lacking eigenvectors, then we may not be able to express e 0 as a linear combination of the eigenvectors and (3) may not hold.
Reference: [4] <author> D. Womble, </author> <title> A time stepping algorithm for parallel computers, </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 11 (1990), </volume> <pages> pp. 824-837. </pages>
Reference-contexts: In an effort to forestall this and to allow increasing numbers of processors to be used effectively, a number of researchers have suggested algorithms which introduce time domain parallelism as well as space domain parallelism <ref> [1, 4, 5, 6] </ref>. It is claimed in [1] and [6] that a parallel scheme based on time domain parallelism will converge in exactly the same number of iterations as the sequential scheme since the two schemes have iteration matrices with equal spectral radii.
Reference: [5] <author> P. Worley, </author> <title> Parallelizing across time when solving time-dependent partial differential equations, </title> <booktitle> in Proc. 5th SIAM Conf. on Parallel Processing for Scientific Computing, </booktitle> <editor> D. Sorensen, ed., </editor> <publisher> SIAM, </publisher> <year> 1991. </year>
Reference-contexts: In an effort to forestall this and to allow increasing numbers of processors to be used effectively, a number of researchers have suggested algorithms which introduce time domain parallelism as well as space domain parallelism <ref> [1, 4, 5, 6] </ref>. It is claimed in [1] and [6] that a parallel scheme based on time domain parallelism will converge in exactly the same number of iterations as the sequential scheme since the two schemes have iteration matrices with equal spectral radii.
Reference: [6] <author> J. Zhu, </author> <title> A new parallel algorithm for the numerical solutions of time dependent partial differential equations, </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 40 (2001), </volume> <pages> pp. 1000-1001. 20 </pages>
Reference-contexts: In an effort to forestall this and to allow increasing numbers of processors to be used effectively, a number of researchers have suggested algorithms which introduce time domain parallelism as well as space domain parallelism <ref> [1, 4, 5, 6] </ref>. It is claimed in [1] and [6] that a parallel scheme based on time domain parallelism will converge in exactly the same number of iterations as the sequential scheme since the two schemes have iteration matrices with equal spectral radii. <p> In an effort to forestall this and to allow increasing numbers of processors to be used effectively, a number of researchers have suggested algorithms which introduce time domain parallelism as well as space domain parallelism [1, 4, 5, 6]. It is claimed in [1] and <ref> [6] </ref> that a parallel scheme based on time domain parallelism will converge in exactly the same number of iterations as the sequential scheme since the two schemes have iteration matrices with equal spectral radii. In this paper, we examine this claim. <p> In this paper, we provide an analysis of a class of these algorithms as well as computational experiments which demonstrate that this approach is not normally useful. 2. The Temporal Method. In <ref> [6] </ref> and [1], an iterative scheme is described for introducing temporal domain parallelism by solving the linear systems at different time steps simultaneously. The central idea is to assemble n steps of (1) into the form Gu B B B B B A B . . . <p> As was shown in <ref> [6] </ref>, T G is a block lower triangular matrix with diagonal blocks equal to (D L) 1 U . Hence, the eigenvalues of T G are the collections of all eigenvalues of all the block matrices on the main diagonal. <p> Hence, both iterations appear to converge at the same rate. Based on the "proof" sketched above that iterative schemes (1) and (2) converge in exactly the same number of iterations, both [1] and <ref> [6] </ref> go on to demonstrate that the parallel algorithm involving both spatial and temporal dimension parallelism is far superior in terms of speedup and communication/computation ratio over algorithms employing only spatial dimension parallelism. However, in the following sections, we demonstrate that this is not valid. 3. Convergence of Iterative Methods. <p> We also present numerical results to validate this claim. In Section 7 we consider the point Gauss-Seidel method. We describe numerical experiments which show that the parallel scheme using the point Gauss-Seidel method again exhibits very low efficiency contrary to the claims made in <ref> [6] </ref>. 4. Defectiveness of the Iteration Matrix. Consider solving (2) using a block Jacobi iteration scheme, where each block corresponds to one time step. The resulting iteration matrix is T BJ = B B B 0 . . . <p> The total wall clock times, assuming no communication overhead, for block Gauss-Seidel are given by t BE 1 * t BE and p 2 2 ln 2 ! s : Proof: We first consider the backward Euler difference approximation. The scheme suggested in <ref> [6] </ref> results in the following iteration matrix: T GB = B B B B B B B B B A . . . A . . . . . . . . . I . . . . . . . . . <p> This qualitative behavior is largely independent of m as can be seen from the graph in Figure 3, in which we show the efficiency of the parallel scheme for different values of m. Clearly, the efficiency does not stay constant as claimed in <ref> [6] </ref>. 16 Fig. 1. Predicted vs. Actual Efficiency for m = 60 r = 1 Fig. 2. Predicted vs. Actual Efficiency for m = 60 r = 100 7. Point Gauss-Seidel Iterative Schemes. In this section, we consider the point Gauss-Seidel scheme using the backward Euler method. <p> Efficiency for m = 60 and m = 1000 Fig. 4. Efficiency for m = 1000 r = 1 in <ref> [6] </ref> that it remains constant. 7.1. Numerical Results. In Figure 4 and Figures 5, we show the efficiency of the parallel scheme for two different values of r. As the graph shows, the efficiency decreases with increasing n. 18 Fig. 5.
References-found: 6

