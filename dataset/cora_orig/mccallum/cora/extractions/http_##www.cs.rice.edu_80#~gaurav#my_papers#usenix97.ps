URL: http://www.cs.rice.edu:80/~gaurav/my_papers/usenix97.ps
Refering-URL: http://www.cs.rice.edu:80/~gaurav/gaurav_research.html
Root-URL: 
Email: gaurav@cs.rice.edu,  douglis@research.att.com,  misha@research.att.com,  
Title: Optimistic Deltas for WWW Latency Reduction  
Author: Gaurav Banga Fred Douglis Michael Rabinovich 
Note: 1997 USENIX Technical Conference. Copyright to this work is retained by the authors. Permission is granted for the noncommercial reproduction of the complete work for educational or research purposes.  
Web: http://www.cs.rice.edu/gaurav/  http://www.research.att.com/douglis/  http://www.research.att.com/misha/  
Affiliation: Rice University  AT&T Labs Research  AT&T Labs Research  
Abstract: When a machine is connected to the Internet via a slow network, such as a 28.8 Kbps modem, the cumulative latency to communicate over the Internet to World Wide Web servers and then transfer documents over the slow network can be significant. We have built a system that optimistically transfers data that may be out of date, then sends either a subsequent confirmation that the data is current or a delta to change the older version to the current one. In addition, if both sides of the slow link already store the same older version, just the delta need be transferred to update it. Our mechanism is optimistic because it assumes that much of the time there will be sufficient idle time to transfer most or all of the older version before the newer version is available, and because it assumes that the changes between the two versions will be small relative to the actual document. Timings of retrievals of random URLs in the Internet support the former assumption, while experiments using a version repository of Web documents bear out the latter one. Performance measurements of the optimistic delta system demonstrate that deltas significantly reduce latency when both sides cache the old version, and optimistic deltas can reduce latency, to a lesser degree, when content-provider service times are in the range of seconds or longer. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Altavista. </author> <note> http://www.altavista.digital.com. Random URL selection at http://www.altavista.digital.com/cgi-bin/query?pg=s&target=0. </note>
Reference-contexts: We considered two sources of sample pages: the version archive of the AT&T Internet Difference Engine (AIDE) [8], which stores versions of pages for future visual comparison of their changes, and a set of random URLs obtained from AltaVista <ref> [1] </ref>. The random pages were tracked by AIDE as well, but they were considered separately from those pages that were actually registered explicitly, either by individuals or by inclusion in a list of popular URLs collected from a set of bookmark files within AT&T. <p> One might instead use a distillation technique to send a version of an image that is more appropriate for a low-bandwidth link [11]. Our study of 1000 random URLs from AltaVista <ref> [1] </ref> found that 861 URLs were actually accessible at the time we started tracking them, and the vast majority (79%) of those URLs were not modified in the next two months of daily checks. <p> a CGI script. (Determining when to compare one URL against a slightly different URL for differencing is an open question, but as long as both the client and server proxies agree on the versions being compared the system will act correctly.) For example, a query to the AltaVista search engine <ref> [1] </ref> might result in a page containing several links to content and several more links to other URLs within AltaVista. <p> the deltas from one page to the next, within a given search result, were much smaller even than the compressed pages. 3.2 HTTP Latency To get a sense for the likelihood that a request would take a long time to start receiving data, we collected 1000 random URLs from AltaVista <ref> [1] </ref> and timed their responses. This study differs somewhat from Viles and French [21], who studied the availability of random HTTP servers and the time to connect to them; here we are seeing how long it takes to collect the first data from a W 3 page.
Reference: [2] <author> Dave Belanger, David Korn, and Herman Rao. </author> <title> Infrastructure for wide-area software development. </title> <booktitle> In Proceedings of Sixth International Workshop on Software Configuration Management, </booktitle> <month> March </month> <year> 1996. </year>
Reference-contexts: The idea of trading off computation for I/O bandwidth has appeared numerous times in past systems. Examples include application-specific deltas and compression, such as Low-bandwidth X [15]; compressed network or disk I/O [3, 6, 7]; replicated file systems <ref> [2] </ref>; shared memory [16]; and checkpoint-ing [9, 19]. It seems that the same tradeoffs apply in the domain of the W 3 . We address the issue of latency from the perspective of sending the differences between versions of a page, or deltas, in order to avoid sending entire pages.
Reference: [3] <author> M. Burrows, C. Jerian, B. Lampson, and T. Mann. </author> <title> On-line data compression in a log-structured file system. </title> <booktitle> In The Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 2-9. </pages> <publisher> ACM, </publisher> <month> October </month> <year> 1992. </year>
Reference-contexts: We look at the problem of latency from another perspective: using computation to improve end-to-end network latency. The idea of trading off computation for I/O bandwidth has appeared numerous times in past systems. Examples include application-specific deltas and compression, such as Low-bandwidth X [15]; compressed network or disk I/O <ref> [3, 6, 7] </ref>; replicated file systems [2]; shared memory [16]; and checkpoint-ing [9, 19]. It seems that the same tradeoffs apply in the domain of the W 3 .
Reference: [4] <author> World Wide Web Consortium. </author> <title> Hypertext transfer protocol. </title> <address> http://www.w3.org/pub/WWW/Protocols/. </address>
Reference-contexts: This description provides background for the discussion in the following sections. Banga, Douglis, Rabinovich 1997 USENIX Technical Conference Optimistic Deltas for WWW Latency Reduction 3 2.1 Caching in the HTTP Protocol The HyperText Transfer Protocol (HTTP) <ref> [4] </ref> supports caching in clients (i.e. browsers) and intermediate servers known as proxy-caching servers. To display a page, a client without a cached copy will unconditionally send a page request to the content-provider or a proxy-caching server.
Reference: [5] <author> Adam Dingle and Tomas Partl. </author> <title> Web cache coherence. </title> <booktitle> In Proceedings of the Fifth International WWW Conference, </booktitle> <month> May </month> <year> 1996. </year> <note> Available as http://www5conf.inria.fr/fich html/papers/P2/Overview.html. </note>
Reference-contexts: then the latency for retrieving pages elsewhere in the Internet can be eliminated when someone else has retrieved those pages in the recent past. (Recency is a function of the size of the cache, any expiration dates in the pages, and any constraints passed from the browser to the cache <ref> [5, 12] </ref>. Also, some pages are flagged as uncacheable, and the proxy-caching server is obliged to pass those requests through to the content provider. <p> The limited bandwidth, high error rate, and contention in a wireless environment suggest that transferring data that may not be used could have more negative consequences than over a single-user modem. Our work also has some similarity to Dingle and Partl <ref> [5] </ref>, who proposed that a hierarchy of proxy-caching servers could be used to send stale data as a MIME multipart document, causing the browser to display the stale data immediately and to replace it with more recent versions as they become available. <p> In the case where the stale page is actually current, our system behaves like the proposal in <ref> [5] </ref>. <p> Table 1 summarizes the possible combinations of client/server proxy states and the procedures that are followed. 4.3 Detecting Non-cost-effective Trans fers As was mentioned in the introduction, one difference between our system and the proposal of Dingle and Partl <ref> [5] </ref> is the ability to abort the transfer of stale data. In fact, there are two cases when it is more appropriate to behave like standard proxies and just send the current version of a page to the client without delta processing.
Reference: [6] <author> Fred Douglis. </author> <title> On the role of compression in distributed systems. </title> <booktitle> In Proceedings of the Fifth ACM SIGOPS European Workshop, </booktitle> <address> Mont St.-Michel, France, </address> <month> September </month> <year> 1992. </year> <note> ACM. </note>
Reference-contexts: We look at the problem of latency from another perspective: using computation to improve end-to-end network latency. The idea of trading off computation for I/O bandwidth has appeared numerous times in past systems. Examples include application-specific deltas and compression, such as Low-bandwidth X [15]; compressed network or disk I/O <ref> [3, 6, 7] </ref>; replicated file systems [2]; shared memory [16]; and checkpoint-ing [9, 19]. It seems that the same tradeoffs apply in the domain of the W 3 .
Reference: [7] <author> Fred Douglis. </author> <title> The compression cache: Using on-line compression to extend physical memory. </title> <booktitle> In Proceedings of 1993 Winter USENIX Conference, </booktitle> <pages> pages 519-529, </pages> <address> San Diego, CA, </address> <month> January </month> <year> 1993. </year>
Reference-contexts: We look at the problem of latency from another perspective: using computation to improve end-to-end network latency. The idea of trading off computation for I/O bandwidth has appeared numerous times in past systems. Examples include application-specific deltas and compression, such as Low-bandwidth X [15]; compressed network or disk I/O <ref> [3, 6, 7] </ref>; replicated file systems [2]; shared memory [16]; and checkpoint-ing [9, 19]. It seems that the same tradeoffs apply in the domain of the W 3 .
Reference: [8] <author> Fred Douglis and Thomas Ball. </author> <title> Tracking and viewing changes on the web. </title> <booktitle> In Proceedings of 1996 USENIX Technical Conference, </booktitle> <pages> pages 165-176, </pages> <address> San Diego, CA, </address> <month> January </month> <year> 1996. </year>
Reference-contexts: We considered two sources of sample pages: the version archive of the AT&T Internet Difference Engine (AIDE) <ref> [8] </ref>, which stores versions of pages for future visual comparison of their changes, and a set of random URLs obtained from AltaVista [1].
Reference: [9] <author> E. N. Elnozahy, D. B. Johnson, and W. Zwaenepoel. </author> <title> The performance of consistent checkpointing. </title> <booktitle> In Proceedings of the 11th Symposium on Reliable and Distributed Systems, </booktitle> <pages> pages 39-47, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: The idea of trading off computation for I/O bandwidth has appeared numerous times in past systems. Examples include application-specific deltas and compression, such as Low-bandwidth X [15]; compressed network or disk I/O [3, 6, 7]; replicated file systems [2]; shared memory [16]; and checkpoint-ing <ref> [9, 19] </ref>. It seems that the same tradeoffs apply in the domain of the W 3 . We address the issue of latency from the perspective of sending the differences between versions of a page, or deltas, in order to avoid sending entire pages.
Reference: [10] <author> Glenn S. Fowler, David G. Korn, Steven C. North, Herman Rao, and K. Phong Vo. </author> <title> Libraries and file system architecture. </title> <editor> In B. Krishnamurthy, editor, </editor> <title> Practical Reusable UNIX Software, chapter 2. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <month> January </month> <year> 1995. </year>
Reference-contexts: Throughout our experiments, we computed deltas using vdelta, a program that generates compact deltas by essentially compressing the deltas in the process of computing them, and which can be used as a standalone compression program as well <ref> [10] </ref>. 3 We must consider the possibility that W 3 pages that are compressed in a stand-alone fashion will compress so well that the deltas between two versions of a page are not much smaller than the compressed page.
Reference: [11] <author> Armando Fox and Eric A. Brewer. </author> <title> Reducing www latency and bandwidth requirements by real-time distillation. </title> <booktitle> In Proceedings of the Fifth International WWW Conference, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: One might instead use a distillation technique to send a version of an image that is more appropriate for a low-bandwidth link <ref> [11] </ref>. Our study of 1000 random URLs from AltaVista [1] found that 861 URLs were actually accessible at the time we started tracking them, and the vast majority (79%) of those URLs were not modified in the next two months of daily checks.
Reference: [12] <author> James Gwertzman and Margo Seltzer. </author> <title> World-wide web cache consistency. </title> <booktitle> In Proceedings of 1996 USENIX Technical Conference, </booktitle> <pages> pages 141-151, </pages> <address> San Diego, CA, </address> <month> January </month> <year> 1996. </year> <note> Also available as http://www.eecs.harvard.edu/vino/web/usenix.196/. </note>
Reference-contexts: then the latency for retrieving pages elsewhere in the Internet can be eliminated when someone else has retrieved those pages in the recent past. (Recency is a function of the size of the cache, any expiration dates in the pages, and any constraints passed from the browser to the cache <ref> [5, 12] </ref>. Also, some pages are flagged as uncacheable, and the proxy-caching server is obliged to pass those requests through to the content provider.
Reference: [13] <author> Barron C. Housel and David B. Lindquist. WebExpress: </author> <title> A system for optimizing web browsing in a wireless environment. </title> <booktitle> In Proceedings of the Second Annual International Conference on Mobile Computing and Networking, </booktitle> <pages> pages 108-116, </pages> <address> Rye, New York, </address> <month> November </month> <year> 1996. </year> <note> ACM. </note>
Reference-contexts: The experiments described in this paper support this assumption. In contrast, we refer to the case where both sides share a cached version as a simple delta. As we were going to press, we found that the simple deltas case is similar to a system from IBM called WebExpress <ref> [13] </ref>. WebExpress is geared toward a low bandwidth wireless environment, where bandwidth is precious, so it has similar goals but makes different trade-offs.
Reference: [14] <author> Van Jacobson. </author> <title> Congestion avoidance and control. </title> <booktitle> In Proceedings of the ACM SIGCOMM Conference, </booktitle> <address> Stanford, CA, </address> <month> August </month> <year> 1988. </year>
Reference-contexts: In fact, the latency for optimistic deltas with 5s added delay was consistently somewhat less than that for simple deltas with the same delay. We attribute the better performance of optimistic deltas to TCP's slow-start algorithm <ref> [14] </ref>. In the case of the optimistic deltas the transfer of the stale data opened up the TCP congestion window, so the deltas were transferred 4 In the unmodified system, there were no proxies involved and the client talked directly to the content-provider.
Reference: [15] <author> Christopher A. Kantarjiev, Alan Demers, Ron Frederick, Robert T. Krivacic, and Mark Weiser. </author> <title> Experiences with X in a wireless environment. </title> <booktitle> In Proceedings USENIX Symposium on Mobile & Location-Independent Computing, </booktitle> <pages> pages 117-128. </pages> <publisher> USENIX, </publisher> <month> August </month> <year> 1993. </year>
Reference-contexts: We look at the problem of latency from another perspective: using computation to improve end-to-end network latency. The idea of trading off computation for I/O bandwidth has appeared numerous times in past systems. Examples include application-specific deltas and compression, such as Low-bandwidth X <ref> [15] </ref>; compressed network or disk I/O [3, 6, 7]; replicated file systems [2]; shared memory [16]; and checkpoint-ing [9, 19]. It seems that the same tradeoffs apply in the domain of the W 3 .
Reference: [16] <author> P. Keleher, S. Dwarkadas, A.L. Cox, , and W. Zwaenepoel. Treadmarks: </author> <title> Distributed shared memory on standard workstations and operating systems. </title> <booktitle> In Proceedings of 1994 Winter USENIX Conference, </booktitle> <pages> pages 115-131, </pages> <address> San Francisco, CA, </address> <month> January </month> <year> 1994. </year>
Reference-contexts: The idea of trading off computation for I/O bandwidth has appeared numerous times in past systems. Examples include application-specific deltas and compression, such as Low-bandwidth X [15]; compressed network or disk I/O [3, 6, 7]; replicated file systems [2]; shared memory <ref> [16] </ref>; and checkpoint-ing [9, 19]. It seems that the same tradeoffs apply in the domain of the W 3 . We address the issue of latency from the perspective of sending the differences between versions of a page, or deltas, in order to avoid sending entire pages.
Reference: [17] <author> Venkata N. Padmanabhan and Jeffrey C. Mogul. </author> <title> Improving http latency. Computer Networks and ISDN Systems, </title> <address> 28(1-2):25-35, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: Prefetching pages during periods when the modem would otherwise be idle can reduce or eliminate the latency of following a link, if the prefetch is accurate and the user thinks between clicks long enough for prefetching to complete [18, 22]. Pad-manabhan and Mogul's study of persistent HTTP connections <ref> [17] </ref> indicates that a persistent connection between a client and proxy, or between one of them and a content provider, will eliminate TCP connection setup and slow-start overhead. We look at the problem of latency from another perspective: using computation to improve end-to-end network latency. <p> In fact, it might be possible to hash the contents of pages to find other pages that are substantially similar and would generate small deltas. Currently, each communication between the browser and the client proxy, or between the client and server proxies, requires a TCP setup. Persistent HTTP <ref> [17] </ref> should improve performance further, but we have not yet implemented a persistent connection in our proxy. In the current system, deltas are generated only when the current version of a document has been received in its entirety.
Reference: [18] <author> Venkata N. Padmanabhan and Jeffrey C. Mogul. </author> <title> Using predictive prefetching to improve world wide web latency. </title> <journal> Computer Communication Review, </journal> <volume> 26(3) </volume> <pages> 22-36, </pages> <year> 1996. </year>
Reference-contexts: Prefetching pages during periods when the modem would otherwise be idle can reduce or eliminate the latency of following a link, if the prefetch is accurate and the user thinks between clicks long enough for prefetching to complete <ref> [18, 22] </ref>. Pad-manabhan and Mogul's study of persistent HTTP connections [17] indicates that a persistent connection between a client and proxy, or between one of them and a content provider, will eliminate TCP connection setup and slow-start overhead. <p> Finally, it should be useful to integrate prefetch-ing into the optimistic delta system. In addition to prefetching new pages through the server proxy to the client (similar to the studies mentioned above <ref> [18, 22] </ref>), we can prefetch deltas to keep the proxies' caches better synchronized. 7 Conclusion We have proposed an optimistic deltas approach to reduce the latency of accessing W 3 pages.
Reference: [19] <author> James S. Plank, Jian Xu, and Rob Netzer. </author> <title> Compressed differences: An algorithm for fast incremental checkpointing. </title> <type> Technical Report CS-95-302, </type> <institution> University of Tennessee, </institution> <month> August </month> <year> 1995. </year>
Reference-contexts: The idea of trading off computation for I/O bandwidth has appeared numerous times in past systems. Examples include application-specific deltas and compression, such as Low-bandwidth X [15]; compressed network or disk I/O [3, 6, 7]; replicated file systems [2]; shared memory [16]; and checkpoint-ing <ref> [9, 19] </ref>. It seems that the same tradeoffs apply in the domain of the W 3 . We address the issue of latency from the perspective of sending the differences between versions of a page, or deltas, in order to avoid sending entire pages.
Reference: [20] <author> Bill N. Schilit, Fred Douglis, David M. Kristol, Paul Krzyzanowski, James Sienicki, and John A. Trotter. Teleweb: </author> <title> Loosely connected access to the world wide web. </title> <booktitle> In Proceedings of the Fifth International World Wide Web Conference, </booktitle> <address> Paris, France, </address> <month> May </month> <year> 1996. </year> <note> Banga, Douglis, Rabinovich 1997 USENIX Technical Conference Optimistic Deltas for WWW Latency Reduction 15 </note>
Reference-contexts: Our approach attempts to reduce this latency by sending the the current version as a delta. In addition, we do not display the stale page: it is intercepted by a client proxy <ref> [20] </ref>, typically co-located with the browser on the same machine, and passed to the browser once it is updated or known to be current. In the case where the stale page is actually current, our system behaves like the proposal in [5].
Reference: [21] <author> Charles L. Viles and James C. </author> <title> French. Availability and latency of world wide web information servers. </title> <journal> Computing Systems, </journal> <volume> 8(1) </volume> <pages> 61-91, </pages> <month> Winter </month> <year> 1995. </year>
Reference-contexts: This study differs somewhat from Viles and French <ref> [21] </ref>, who studied the availability of random HTTP servers and the time to connect to them; here we are seeing how long it takes to collect the first data from a W 3 page.
Reference: [22] <author> Stuart Wachsberg, Thomas Kunz, and Johnny Wong. </author> <title> Fast world-wide web browsing over low-bandwidth links. </title> <note> Available as http://ccnga.uwaterloo.ca/sbwachsb/paper.html, June 1996. Banga, Douglis, Rabinovich 1997 USENIX Technical Conference </note>
Reference-contexts: Prefetching pages during periods when the modem would otherwise be idle can reduce or eliminate the latency of following a link, if the prefetch is accurate and the user thinks between clicks long enough for prefetching to complete <ref> [18, 22] </ref>. Pad-manabhan and Mogul's study of persistent HTTP connections [17] indicates that a persistent connection between a client and proxy, or between one of them and a content provider, will eliminate TCP connection setup and slow-start overhead. <p> Finally, it should be useful to integrate prefetch-ing into the optimistic delta system. In addition to prefetching new pages through the server proxy to the client (similar to the studies mentioned above <ref> [18, 22] </ref>), we can prefetch deltas to keep the proxies' caches better synchronized. 7 Conclusion We have proposed an optimistic deltas approach to reduce the latency of accessing W 3 pages.
References-found: 22

