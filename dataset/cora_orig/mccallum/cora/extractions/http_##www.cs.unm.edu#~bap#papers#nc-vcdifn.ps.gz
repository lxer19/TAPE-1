URL: http://www.cs.unm.edu/~bap/papers/nc-vcdifn.ps.gz
Refering-URL: http://www.cs.unm.edu/~bap/publications.html
Root-URL: http://www.cs.unm.edu
Title: VC Dimension of an Integrate-and-Fire Neuron Model  
Author: Anthony M. Zador Barak A. Pearlmutter 
Date: 8(3)  
Note: To appear (1996) in Neural Computation  
Abstract: We compute the VC dimension of a leaky integrate-and-fire neuron model. The VC dimension quantifies the ability of a function class to partition an input pattern space, and can be considered a measure of computational capacity. In this case, the function class is the class of integrate-and-fire models generated by varying the integration time constant t and the threshold q, the input space they partition is the space of continuous-time signals, and the binary partition is specified by whether or not the model reaches threshold at some specified time. We show that the VC dimension diverges only logarithmically with the input signal bandwidth N. We also extend this approach to arbitrary passive dendritic trees. The main contributions of this work are (1) it offers a novel treatment of the computational capacity of this class of dynamic system; and (2) it provides a framework for analyzing the computational capabilities of the dynamical systems defined by networks of spiking neurons. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Abu-Mostafa, Y. S. </author> <year> (1989). </year> <title> The Vapnik-Chervonenkis Dimension: Information versus Complexity in Learning. </title> <journal> Neural Computation, </journal> <volume> 1(3), </volume> <month> 312-317. </month> <title> 4 We speculate that local dendritic processing gives rise to a real increase in computational powerone that arises from a nonlinear partitioning of a space of fixed dimension. </title> <note> 12 Anthony, </note> <author> M. </author> <year> (1994). </year> <title> Probabilistic Analysis of Learning in Artificial Neural Net--works: The PAC Model and its Variants. Mathematics preprint series LSE-MPS-67, </title> <institution> Dept. of Mathematics, London School of Economics, Houghton St., London WC2A 2AE, UK. </institution> <note> Also available as NeuroCOLT technical report NC-TR-94-3, ftp://cscx.cs.rhbnc.ac.uk/pub/neurocolt/tech reports/. </note>
Reference: <author> Bartlett, P. L., Long, P. M., and Williamson, R. C. </author> <year> (1994). </year> <title> FAT-Shattering and the Learn-ability of Real-Valued Functions.. </title> <booktitle> In COLT (1994), </booktitle> <pages> pp. 299-310. </pages>
Reference: <author> Baum, E. and Haussler, D. </author> <year> (1989). </year> <title> What Size Net Gives Valid Generalization?. </title> <journal> Neural Computation, </journal> <volume> 1(1), </volume> <pages> 151-160. </pages>
Reference-contexts: This is true, for example, in feedforward linear threshold networkss, where the VC dimension is equal to the number of free weights, up to a logarithmic factor <ref> (Baum and Haussler 1989) </ref>. In our case, we expected the VC dimension to be about two, since there were two free parameters (q and t.) Furthermore, a small VC dimension for the integrate-and-fire model conforms to our intuitive notion of the simplicity of this model.
Reference: <author> Blumer, A., Ehrenfeucht, A., Haussler, D., and Warmuth, M. K. </author> <year> (1989). </year> <title> Learnability and the Vapnik-Chervonenkis Dimension. </title> <journal> Journal of the ACM, </journal> <volume> 36, </volume> <pages> 929-965. </pages>
Reference-contexts: It gives an upper bound on the number of exemplars required to guarantee that a set of parameters fit to data will provide a good fit for new data <ref> (Blumer et al. 1989) </ref>. It has been applied in the neural network literature to give a measure of the number of patterns needed to train a network of a given size.
Reference: <author> Brown, T. H., Zador, A. M., Mainen, Z. F., and Claiborne, B. J. </author> <year> (1992). </year> <booktitle> Hebbian Computations in Hippocampal Dendrites and Spines. In McKenna et al.(1992), </booktitle> <pages> pp. 81-116. </pages> <booktitle> COLT (1994). Seventh Annual ACM Workshop on Computational Learning Theory, </booktitle> <address> New Brunswick, NJ. </address>
Reference: <author> Haussler, D., Kearns, M., Seung, H. S., and Tishby, N. </author> <year> (1994). </year> <title> Rigorous Learning Curve Bounds from Statistical Mechanics.. </title> <booktitle> In COLT (1994), </booktitle> <pages> pp. 67-75. </pages>
Reference-contexts: In both cases, the apparent VC dimension in the presense of noise conformed much more closely with our intuitive notion that it should be rather small. It will be interesting to see whether related notions of computational capacity, such as those derived from work on average generalization <ref> (Haussler, Kearns, Seung, and Tishby 1994) </ref>, can be extended to dynamical systems in a similar way. Acknowledgments We are grateful to the reviewers for their constructive and thought-provoking comments, and Paul Zador for many useful discussions.
Reference: <author> Koch, C., Bernander, O., and Douglas, R. J. </author> <year> (1995). </year> <title> Do neurons have a voltage or a current threshold for action potential initiation?. </title> <journal> Journal of Computational Neuroscience, </journal> <volume> 2, </volume> <pages> 63-82. </pages>
Reference-contexts: McKenna et al. (1992).) The leaky integrate-and-fire model with reset is nevertheless a standard starting point for considering dynamical aspects of neuron behavior. A recent careful examination of its validity <ref> (Koch, Bernander, and Douglas 1995) </ref> supports the notion that for rapidly varying input signals of the kind considered here it offers a good first approximation. 3 In order to show that the VC dimension of a concept class is at least M, one must show that there exists some set of
Reference: <author> Koch, C., Poggio, T., and Torre, V. </author> <year> (1982). </year> <title> Retinal Ganglion Cells: a Functional Interpretation of Dendritic Morphology. </title> <journal> Proc. of the Royal Soc. of London B, </journal> <volume> 298, </volume> <pages> 227-264. </pages>
Reference: <author> Maass, W. </author> <year> (1995). </year> <title> Vapnik-Chervonenkis Dimension of Neural Networks. </title> <editor> In Arbib, M. A. (Ed.), </editor> <booktitle> Handbook of Brain Theory and Neural Networks, </booktitle> <pages> pp. 1000-1002. </pages> <publisher> MIT Press. </publisher>
Reference: <author> Maass, W. </author> <year> (1996). </year> <title> Lower bounds for the computational power of networks of spiking neurons. Neural Computation, 8(1). </title> <publisher> In press. </publisher>
Reference: <editor> McKenna, T., Davis, J., and Zornetzer, S. F. (Eds.). </editor> <year> (1992). </year> <title> Single Neuron Computation. </title> <publisher> Academic Press. </publisher>
Reference: <author> Mel, B. W. </author> <year> (1992). </year> <title> NMDA-Based Pattern Discrimination in a Modeled Cortical Neuron. </title> <journal> Neural Computation, </journal> <volume> 4(4), </volume> <pages> 502-516. </pages>
Reference: <author> Shepherd, G. and Brayton, R. </author> <year> (1987). </year> <title> Logic operations are properties of computer-simulated interactions between excitable dendritic spines. </title> <journal> Journal of Neuroscience, </journal> <volume> 21, </volume> <pages> 151-166. </pages>
Reference: <author> Valiant, L. G. </author> <year> (1984). </year> <title> A Theory of the Learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11), </volume> <pages> 1134-1142. </pages>
Reference-contexts: Given any set of 2 M concepts, we can find a set of M inputs that these concepts shatter. This has consequences in the application to PAC learning <ref> (Valiant 1984) </ref>, where it corresponds to generalizing one of the two worst-case assumptions of the PAC criterion.
Reference: <author> Vapnik, V. and Chervonenkis, A. </author> <year> (1971). </year> <title> On the Uniform Convergence of Relative Frequencies of Events to Their Probabilities. </title> <journal> Theory of Probability and Its Applications, </journal> <volume> 16, </volume> <pages> 264-280. </pages>
Reference-contexts: In the context of learning theory the VC dimension is useful because of a relation between the number of labeled exemplars in a training set and the probability of generating the correct output on a new exemplar <ref> (Vapnik and Chervonenkis 1971) </ref>.
Reference: <author> Zador, A. M., Claiborne, B. J., and Brown, T. H. </author> <year> (1992). </year> <title> Nonlinear Pattern Separation in Single Hippocampal Neurons with Active Dendritic Membrane. </title> <editor> In Moody, J. E., Hanson, S. J., and Lippmann, R. P. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pp. 51-58. </pages> <publisher> Morgan Kaufmann. </publisher> <pages> 14 </pages>
References-found: 16

