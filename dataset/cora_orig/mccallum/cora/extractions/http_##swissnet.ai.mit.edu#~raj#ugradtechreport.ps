URL: http://swissnet.ai.mit.edu/~raj/ugradtechreport.ps
Refering-URL: http://www-swiss.ai.mit.edu/~raj/index.html
Root-URL: 
Title: A Parallelizing Compiler Based on Partial Evaluation  
Author: Rajeev Surati 
Note: Copyright c Massachusetts Institute of Technology, 1993  
Date: 1377 July 1993  
Affiliation: MASSACHUSETTS INSTITUTE OF TECHNOLOGY ARTIFICIAL INTELLIGENCE LABORATORY  
Pubnum: A.I. Technical Report No.  
Abstract: This report describes research done at the Artificial Intelligence Laboratory of the Massachusetts Institute of Technology. Support for the laboratory's research is provided in part by the Advanced Research Projects Agency of the Department of Defense under Office of Naval Research contract N00014-92-J-4097, and by the National Science Foundation under grant number MIP-9001651. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. Abelson, A. Berlin, J. Katzenelson, W. McAllister, G. Rozas, G. Sussman, </author> <title> "The Supercomputer Toolkit and its Applications," </title> <institution> MIT Artificial Intelligence Laboratory Memo 1249, Cambridge, Massachusetts. </institution>
Reference-contexts: The Toolkit is an 8 processor MIMD machine. It is composed of eight separate VLIW processing nodes. A thorough explanation of the technical details of the Supercomputer Toolkit may be found in <ref> [1] </ref> A detailed explanation of the compiler's view of the toolkit processor boards , the interconnection network, and the synchronization mechanism follows. 3.1 The Toolkit Processing Nodes a 64-bit-floating-point chip set, a five-port 32x64-bit register file, two separately addressable data memories, two address generators for those memories, two I/O ports, a
Reference: [2] <author> J. Applegate, M. Douglas, Y. Gursel, P. Hunter, C. Seitz, G.J. Sussman, </author> <title> "A Digital Orrery," </title> <journal> IEEE Trans. on Computers, </journal> <month> Sept. </month> <year> 1985. </year>
Reference-contexts: Numerical simulation of the n-body problem is important for a number of research applications <ref> [2] </ref>. Though the two application represent simulations of the same problem, they represent signif icantly different numerical computations. This is because they utilize two different 25 26 CHAPTER 4. EXPERIMENTAL RESULTS numerical integrators. One integration method is known as Stormer and the other as Runge Kutta.
Reference: [3] <author> A.V. Aho, R. Sethi and J.D. Ullman,Compilers: </author> <booktitle> Principles, Techniques and Tools Addison Wesley, </booktitle> <year> 1988 </year>
Reference-contexts: Partial evaluation eliminates all of the data independent conditional branches in a program and thus produces huge sequences of easily parallelizable straight-line code <ref> [3] </ref>. A basic block is essentially a sequence of operations in a computation that must be executed once the sequence of instructions is entered. These huge sequences of straight line numerical code would be considered basic blocks. The large blocks produced by partial evaluation are 4 CHAPTER 1.
Reference: [4] <author> A. Berlin and D. Weise, </author> <title> "Compiling Scientific Code Using Partial Evaluation," </title> <note> to appear in IEEE Computer. Also see MIT Artificial Intelligence Laboratory Memo number 1145, </note> <month> July, </month> <year> 1989. </year>
Reference: [5] <author> A. </author> <title> Berlin, "Partial Evaluation Applied to Numerical Computation", in proceedings of the 1990 ACM Conference on Lisp and Functional Programming. Also see "A Compilation strategy for numerical programs based on partial evaluation," </title> <institution> MIT Artificial Intelligence Laboratory Technical Report TR-1144, </institution> <month> July, </month> <year> 1989. </year>
Reference-contexts: The compiler demonstrated here can achieve similar speedups automatically. By reconstructing the data dependencies of a computation expressed by a program, partial evaluation succeeds in "exposing the low level parallelism in a computation by eliminating inherently sequential data-structure references." <ref> [5] </ref> This is crucial for the the exploitation of parallelism across a multiprocessor. Partial evaluation eliminates all of the data independent conditional branches in a program and thus produces huge sequences of easily parallelizable straight-line code [3]. <p> This leaves only the numerical computation data dependency graph. It also results in an order of magnitude speedup of scientific codes <ref> [5] </ref>. The partial evaluator utilized by this compiler was written by Andrew Berlin. A more thorough discussion of the partial evaluator is contained in [5]. Berlin accomplishes partial evaluation through a technique that uses placeholders to propagate intermediate results. <p> This leaves only the numerical computation data dependency graph. It also results in an order of magnitude speedup of scientific codes <ref> [5] </ref>. The partial evaluator utilized by this compiler was written by Andrew Berlin. A more thorough discussion of the partial evaluator is contained in [5]. Berlin accomplishes partial evaluation through a technique that uses placeholders to propagate intermediate results. The placeholders are also used to represent data which is not known at compile time in the input data structures. <p> A data dependency graph of the computation is constructed by keeping track of all the operations which are performed on the data and the intermediate values. A simple example 1 to illustrate this follows: 1 This example appears in <ref> [5] </ref> 2.1. THE PARTIAL EVALUATOR 7 source code. 8 CHAPTER 2.
Reference: [6] <author> S. Borkar, R. Cohen, G. Cox, S. Gleason, T. Gross, H.T. Kung, M. Lam, B. Moore, C. Peterson, J. Pieper, L. Rankin, P.S. Tseng, J. Sutton, J. Urbanski, and J. Webb, </author> <title> "iWarp: An Integrated Solution to High-speed Parallel Computing," </title> <booktitle> Supercomputing '88, </booktitle> <address> Kissimmee, Florida, </address> <month> Nov., </month> <year> 1988. </year> <note> 43 44 BIBLIOGRAPHY </note>
Reference-contexts: The Supercomputer Toolkit is not a general purpose computing machine. It is optimized heavily for the static and data-independent nature of numerical problems. Thus, the Toolkit has no operating system and is a backend processor for a workstation, much like WARP <ref> [6] </ref>. The Toolkit is an 8 processor MIMD machine. It is composed of eight separate VLIW processing nodes.
Reference: [7] <author> G. Cybenko, J. Bruner, S. Ho, </author> <title> "Parallel Computing and the Perfect Benchmarks." Center for Supercomputing Research and Development Report 1191., </title> <month> November </month> <year> 1991 </year>
Reference-contexts: The latency is also quite high for a statically scheduled architecture. Each transmission has an ALU to ALU latency of 6 clock cycles. An example of typical speedups for manually restructured (hand optimized) code is given with the Perfect Benchmarks <ref> [7] </ref>. This set of benchmarks is provided by the Center for Supercomputing Research and Development at the University of Illinois at Urbana Champaign. <p> Remarkably there are now many utilities for profiling and analyzing parallelism that allow programmers to find bottle necks in their code. One such utility is known as Max-Par <ref> [7] </ref> which essentially deduces the data dependency graph after the computation is completed and shows the parallelism available and that being exploited in various portions of the programs.
Reference: [8] <author> J. Ellis, Bulldog: </author> <title> A Compiler for VLIW Architectures. </title> <type> PHD thesis, </type> <institution> Yale University, </institution> <year> 1985. </year>
Reference: [9] <author> J.A. Fisher, </author> <title> "Trace scheduling: A Technique for Global Microcode Compaction." </title> <journal> IEEE Transactions on Computers, </journal> <volume> Number 7, pp.478-490. </volume> <year> 1981. </year>
Reference-contexts: Other compilers are more general and are designed to optimize the execution of the program. In order to put this work into perspective, five different approaches including trace scheduling, software pipelining, vectorizing and iterative restructuring are all compared and contrasted with this compiler's methodology. 5.1 Trace Scheduling Trace scheduling <ref> [9] </ref> is a popular technique used by parallelizing compilers. The technique creates traces of the most frequently used path of basic blocks in the control structure of a program. The basic blocks are typically on the order of 10 to 20 instructions.
Reference: [10] <author> Kasahara, Hironori, Honda, Hiroki, Narita, Seinosuke, </author> <title> "Parallel Processing of Near Fine Grain Tasks Using Static Scheduling on OSCAR", </title> <booktitle> Supercomputing 90, </booktitle> <pages> pp 856-864, </pages> <year> 1990 </year>
Reference-contexts: Operations in the data dependency graph are collapsed into regions. A region is a computation which ends with a transmission. The only things that should be transmitted are 3 One attempt at addressing this issue is discussed in <ref> [10] </ref>. 10 CHAPTER 2. THE COMPILER values which are inputs to more than one operation. A simple algorithm creates a region dependency graph from a data dependency graph. A region ends in an operation whose result is used by more than one other operation. <p> The other way is to increase the level of optimization that is performed. A possible optimization is to find a better method of exploiting the fine grain parallelism than region division that will work well on larger architectures. Perhaps a method like task fusion <ref> [10] </ref> should be attempted. Another optimization that could be added involves computing values redundantly across processors because it is cheaper than transmitting these values in some cases.
Reference: [11] <author> Monica Lam, </author> <title> "A Systolic Array Optimizing Compiler." </title> <institution> Carnegie Mellon Computer Science Department Technical Report CMU-CS-87-187., </institution> <month> May, </month> <year> 1987. </year>
Reference-contexts: A good strategy would be to couple both techniques. Partial evaluation would do a good job optimizing data independent portions of the computations, whereas trace scheduling would do well with the data dependent portions. 5.2 Software Pipelining Software pipelining <ref> [11] </ref> optimizes a particular fixed size loop structure so that several iterations of the loop are started on different processors at constant intervals in time. This increases the throughput of the computation.
Reference: [12] <author> C. Heinzl, </author> <title> "Functional Diagnostics for the Supercomputer Toolkit MPCU Module", </title> <type> S.B. Thesis, </type> <institution> MIT, </institution> <year> 1990. </year>
Reference: [13] <author> P. Hut and G.J. Sussman, </author> <title> "Advanced Computing for Science," </title> <journal> Scientific American, </journal> <volume> vol. 255, no. 10, </volume> <month> October </month> <year> 1987. </year>
Reference: [14] <author> H. Printz, </author> <title> "Automatic Mapping of Large Signal Processing Systems to a Parallel Machine," </title> <institution> Carnegie Mellon Computer Science Department Technical Report CMU-CS-91-101., </institution> <month> May, </month> <year> 1991. </year>
Reference: [15] <author> G. J. Sussman and J. </author> <title> Wisdom, "Numerical Evidence that the Motion of Pluto is Chaotic," </title> <journal> Science, </journal> <volume> Volume 241, </volume> <month> 22 July </month> <year> 1988. </year>
Reference: [16] <author> J. D. Ullman, </author> <title> "NP-Complete Scheduling Problems", </title> <journal> Journal of Computer and System Sciences,vol. </journal> <month> 10 </month> <year> (1975),pp </year> <month> 384-393. </month>
Reference-contexts: This is the 2.3. REGION SCHEDULING 11 12 CHAPTER 2. THE COMPILER traditional multiprocessor scheduling problem of scheduling tasks on processors such that execution time is minimized. This is known to be a "strong" NP-hard problem <ref> [16] </ref>. Purely heuristic methods are justified on such a problem, as long as they do well on average. The heuristic used here relies on a critical path weighting Scheme and very akin to list scheduling. There are two steps to this heuristic: 1.
References-found: 16

