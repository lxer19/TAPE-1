URL: ftp://ftp.cs.ucsd.edu/pub/team/datumisca.ps.Z
Refering-URL: http://www.cs.ucsd.edu/users/flaviu/publications.html
Root-URL: http://www.cs.ucsd.edu
Email: fgalvarez, burkhard, flaviug@cs.ucsd.edu  
Title: Tolerating Multiple Failures in RAID Architectures with Optimal Storage and Uniform Declustering  
Author: Guillermo A. Alvarez Walter A. Burkhard Flaviu Cristian 
Address: La Jolla, CA 920930114, USA  
Affiliation: Department of Computer Science and Engineering University of California, San Diego  
Abstract: We present Datum, a novel method for tolerating multiple disk failures in disk arrays. Datum is the first known method that can mask any given number of failures, requires an optimal amount of redundant storage space, and spreads reconstruction accesses uniformly over disks in the presence of failures without needing large layout tables in controller memory. Our approach is based on information dispersal, a coding technique that admits an efficient hardware implementation. As the method does not restrict the configuration parameters of the disk array, many existing RAID organizations are particular cases of Datum. A detailed performance comparison with two other approaches shows that Datum's response times are similar to those of the best competitor when two or less disks fail, and that the performance degrades gracefully when more than two disks fail. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. Alvarez, W. Burkhard, and F. Cristian. </author> <title> Tolerating multiple failures in raid architectures with optimal storage and uniform declustering. </title> <type> Technical report CS96-500, </type> <institution> UCSD-CSE, </institution> <month> November </month> <year> 1996. </year> <note> Also available by anonymous ftp from elm.ucsd.edu/pub/galvarez/datum.ps.Z. </note>
Reference-contexts: Given a k-combination c =&lt; X 1 ; : : : ; X k &gt;, its position in the total order is given by (the reader interested in the proofs of the results mentioned in this paper is directed to <ref> [1] </ref>): loc (&lt; X 1 ; : : : ; X k &gt;) = X k k 1 + + X 1 Since the loc mapping is a bijection, we can compute its inverse invloc.
Reference: [2] <author> A. Bestavros. SETH: </author> <title> A VLSI chip for the real-time information dispersal and retrieval for security and fault-tolerance. </title> <booktitle> In Proc. Intl. Conf. on Parallel Processing, </booktitle> <year> 1990. </year>
Reference-contexts: Our approach is based on the information dispersal algorithm (IDA) [17], a coding technique for which a fast VLSI implementation exists <ref> [2] </ref>. We tested an implementation of Datum against implementations of parity declustering [10] and Evenodd [3], by utilizing a very accurate disk array simulator [8]. <p> Datum's configuration depends on the number of failures we want to tolerate. Bestavros reports in <ref> [2] </ref> times of approximately 100 nanoseconds/byte for the VLSI implementation of IDA. Since his design does not allow pipelining, this amounts to less than 0.5 milliseconds for a 4096-byte stripe. This has been added wherever necessary to the times spent by Datum in disk accesses. According to [2], it is possible <p> Bestavros reports in <ref> [2] </ref> times of approximately 100 nanoseconds/byte for the VLSI implementation of IDA. Since his design does not allow pipelining, this amounts to less than 0.5 milliseconds for a 4096-byte stripe. This has been added wherever necessary to the times spent by Datum in disk accesses. According to [2], it is possible to design a more sophisticated chip that can support pipelining and be driven by a clock ten times faster. 5.1 Failure-free Operation For these experiments, the Datum array was configured with k = 10 and m = 9, i.e. a single redundant unit per stripe.
Reference: [3] <author> M. Blaum, J. Brady, J. Bruck, and J. Menon. Even-odd: </author> <title> An efficient scheme for tolerating double disk failures in RAID architectures. </title> <booktitle> In Proc. of the Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 245-54, </pages> <year> 1994. </year>
Reference-contexts: Most existing approaches like RAID-5 [15] and parity declustering [10] rely on parity calculations to provide tolerance against a single disk failure. However, there are several reasons for considering architectures capable of masking multiple simultaneous disks failures <ref> [3, 4, 7, 9] </ref>: * Applications are placing stronger demands on storage subsystems as a result of manipulating large audio and video objects in real-time. <p> Our approach is based on the information dispersal algorithm (IDA) [17], a coding technique for which a fast VLSI implementation exists [2]. We tested an implementation of Datum against implementations of parity declustering [10] and Evenodd <ref> [3] </ref>, by utilizing a very accurate disk array simulator [8]. <p> It is convenient to store a few binomial coefficient values in memory for improving efficiency, as one or more of these functions are in the critical path of every request sent to the array. 5 Performance We report the results of a performance comparison made by running Datum, Evenodd <ref> [3] </ref>, and parity decluste-ring [10] under a highly-concurrent workload of small reads and writes. Our goal is to compare how these different methods perform under a spectrum of failure scenarios, from failure-free operation to three unavailable disks. <p> The redundant units are rotated cyclically over the dif ferent disks of the array in our implementation of Even-odd, as suggested in <ref> [3] </ref>. The experiments were run on RaidFrame [8], a tool where implementations of RAID architectures can be tested and evaluated. RaidFrame's synthetic workload generator generates concurrent sequences of requests for disk accesses, from multiple client threads. <p> The other codes also require an increasing number of redundant disks as a function of the reliability group size, unlike Datum where the size of a redundant unit is independent of the number of units per stripe. Evenodd <ref> [3] </ref> tolerates two disk failures, and can be efficiently implemented on existing controller hardware. Even though Evenodd does not inherently prevent declustering, the layout proposed in [3] makes every disk a member of every stripe; this limits the array to a maximum of 259 disks, and is likely to result in <p> Evenodd <ref> [3] </ref> tolerates two disk failures, and can be efficiently implemented on existing controller hardware. Even though Evenodd does not inherently prevent declustering, the layout proposed in [3] makes every disk a member of every stripe; this limits the array to a maximum of 259 disks, and is likely to result in performance degradation during on-line reconstruction [10].
Reference: [4] <author> W. Burkhard and J. Menon. </author> <title> Disk array storage system reliability. </title> <booktitle> In Proc. of the International Symposium on Fault-tolerant Computing, </booktitle> <pages> pages 432-41, </pages> <year> 1993. </year>
Reference-contexts: Most existing approaches like RAID-5 [15] and parity declustering [10] rely on parity calculations to provide tolerance against a single disk failure. However, there are several reasons for considering architectures capable of masking multiple simultaneous disks failures <ref> [3, 4, 7, 9] </ref>: * Applications are placing stronger demands on storage subsystems as a result of manipulating large audio and video objects in real-time. <p> However, there are several reasons for considering architectures capable of masking multiple simultaneous disks failures [3, 4, 7, 9]: * Applications are placing stronger demands on storage subsystems as a result of manipulating large audio and video objects in real-time. Existing analyses <ref> [4] </ref> show that increasing the number of check sectors per stripe is a better (i.e. more cost-effective) approach to improving reliability than increasing the number of reliability groups or relying on the likely improvements of each disk's MTBF. * Disks can exhibit latent sector failures, in which an unavailable group of <p> Even though Evenodd does not inherently prevent declustering, the layout proposed in [3] makes every disk a member of every stripe; this limits the array to a maximum of 259 disks, and is likely to result in performance degradation during on-line reconstruction [10]. Burkhard and Menon <ref> [4] </ref> analyze the additional dependability gained by using MDS codes to tolerate multiple failures; that paper concentrates on reliability calculations, and does not address the issues of run-time performance or array layout.
Reference: [5] <author> L. Cabrera and D. Long. Swift: </author> <title> Using distributed disk striping to provide high I/O data rates. </title> <journal> Computing Systems, </journal> <volume> 4(4) </volume> <pages> 405-39, </pages> <year> 1991. </year>
Reference-contexts: Data are irretrievably lost if the array tolerates a single failure and a latent sector failure is detected when trying to reconstruct a crashed disk. * Safety-critical applications can require large MTTDLs that can not be achieved by tolerating single failures. * Some approaches <ref> [5, 6] </ref> distribute the controller's functionality among loosely coupled nodes (to avoid the centralized controller as a single point of failure). In such scenarios, the contents of one or more correct disks may become inaccessible because some other component has failed, e.g. the communication network.
Reference: [6] <author> P. Cao, S. Lim, S. Venkataraman, and J. Wilkes. </author> <title> The TickerTAIP parallel RAID architecture. </title> <booktitle> In Proc. of the Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 52-63, </pages> <year> 1993. </year>
Reference-contexts: Data are irretrievably lost if the array tolerates a single failure and a latent sector failure is detected when trying to reconstruct a crashed disk. * Safety-critical applications can require large MTTDLs that can not be achieved by tolerating single failures. * Some approaches <ref> [5, 6] </ref> distribute the controller's functionality among loosely coupled nodes (to avoid the centralized controller as a single point of failure). In such scenarios, the contents of one or more correct disks may become inaccessible because some other component has failed, e.g. the communication network. <p> We assume the array controller can identify a failed disk. We do not discuss how to tolerate failures in other components of the system, such as interconnection buses or array controllers <ref> [6, 12] </ref>. Disk space is logically structured into stripe units; each stripe unit has a fixed number of sectors, and contains either user data or redundant data. Datum services accesses involving an integral number of stripe units at the client level.
Reference: [7] <author> A. Cohen and W. Burkhard. </author> <title> SID for efficient reconstruction in fault-tolerant video servers. </title> <booktitle> In Proc. of ACM Multimedia, </booktitle> <pages> pages 277-86, </pages> <month> November </month> <year> 1996. </year>
Reference-contexts: Most existing approaches like RAID-5 [15] and parity declustering [10] rely on parity calculations to provide tolerance against a single disk failure. However, there are several reasons for considering architectures capable of masking multiple simultaneous disks failures <ref> [3, 4, 7, 9] </ref>: * Applications are placing stronger demands on storage subsystems as a result of manipulating large audio and video objects in real-time. <p> RAID architectures were originally proposed by Pat-terson et al in [15]. From the solutions presented there, RAID-5 provides tolerance against a single failure and does not decluster the parity information. Multiple-erasure SID <ref> [7] </ref> is a RAID scheme for multimedia work-loads that can tolerate an arbitrary number of failures while performing contiguous accesses to the disks.
Reference: [8] <author> W. Courtright, G. Gibson, M. Holland, and J. Ze-lenka. </author> <title> A structured approach to redundant disk array implementation. </title> <booktitle> In Proc. of the International Symposium on Performance and Dependability, </booktitle> <year> 1996. </year>
Reference-contexts: Our approach is based on the information dispersal algorithm (IDA) [17], a coding technique for which a fast VLSI implementation exists [2]. We tested an implementation of Datum against implementations of parity declustering [10] and Evenodd [3], by utilizing a very accurate disk array simulator <ref> [8] </ref>. Our results show that despite its generality, Datum has the same performance as its best competitor method for the failure scenarios they have in common, and that response times continue to degrade gracefully beyond that point: the scenarios tolerated only by Datum having more than two failures. <p> The redundant units are rotated cyclically over the dif ferent disks of the array in our implementation of Even-odd, as suggested in [3]. The experiments were run on RaidFrame <ref> [8] </ref>, a tool where implementations of RAID architectures can be tested and evaluated. RaidFrame's synthetic workload generator generates concurrent sequences of requests for disk accesses, from multiple client threads. <p> We chose these workloads because small accesses (particularly writes) are typically most demanding for RAID architectures. The same disk array simulator and similar synthetic workloads have been used a number of times in the literature <ref> [8, 10, 22] </ref>. For larger accesses, there are a number of possible optimizations that can be implemented for any of the three schemes discussed in this section. Our implementation of the Datum array architecture took about 2,400 lines of code.
Reference: [9] <author> G. Gibson, L. Hellerstein, R. Karp, R. Katz, and D. Patterson. </author> <title> Coding techniques for handling failures in large disk arrays. </title> <booktitle> In Proc. of the International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 123-32, </pages> <year> 1989. </year>
Reference-contexts: Most existing approaches like RAID-5 [15] and parity declustering [10] rely on parity calculations to provide tolerance against a single disk failure. However, there are several reasons for considering architectures capable of masking multiple simultaneous disks failures <ref> [3, 4, 7, 9] </ref>: * Applications are placing stronger demands on storage subsystems as a result of manipulating large audio and video objects in real-time. <p> Coding methods for tolerating multiple failures have been proposed by Gibson et al <ref> [9] </ref>; their f -dimensional parity method can tolerate multiple failures, but it achieves the optimal redundancy only when k = i f for some integer i, and in that case it needs f i f1 separate disks for storing redundant information.
Reference: [10] <author> M. Holland and G. Gibson. </author> <title> Parity declustering for continuous operation on redundant disk arrays. </title> <booktitle> In Proc. of the International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 23-35, </pages> <year> 1992. </year>
Reference-contexts: Most existing approaches like RAID-5 [15] and parity declustering <ref> [10] </ref> rely on parity calculations to provide tolerance against a single disk failure. <p> Datum uses the theoretical minimum amount of storage space for storing redundant data in the array. Sectors containing redundancy are uniformly distributed over all disks, thereby eliminating bottlenecks when failures occur and easing the burden on surviving disks during on-line reconstruction <ref> [10] </ref>. Our approach can accommodate distributed sparing as well, with similar benefits. Datum also performs the minimal number of disk accesses to implement small writes. <p> Our approach is based on the information dispersal algorithm (IDA) [17], a coding technique for which a fast VLSI implementation exists [2]. We tested an implementation of Datum against implementations of parity declustering <ref> [10] </ref> and Evenodd [3], by utilizing a very accurate disk array simulator [8]. <p> In general, performance under failure is better when the units of each stripe are written in a subset of the n disks of the array. Our layout functions meet the first five layout goals set forth by Holland and Gibson <ref> [10] </ref>. When a disk fails and each stripe is distributed over all the disks of the array (like in RAID-5 [15]), an access to a random stripe unit has probability 1=n of hitting the failed disk, thus resulting in n 1 accesses to all the other stripe units. <p> This generalizes the function of <ref> [10] </ref> for f = 1, that takes in turn each of the disks of the tuple to hold the parity redundant unit. 4.3 Offset of Stripe Units within Disks Once we have found the set of disks c =&lt; X 1 ; : : : ; X k &gt; corresponding to <p> to store a few binomial coefficient values in memory for improving efficiency, as one or more of these functions are in the critical path of every request sent to the array. 5 Performance We report the results of a performance comparison made by running Datum, Evenodd [3], and parity decluste-ring <ref> [10] </ref> under a highly-concurrent workload of small reads and writes. Our goal is to compare how these different methods perform under a spectrum of failure scenarios, from failure-free operation to three unavailable disks. <p> We chose these workloads because small accesses (particularly writes) are typically most demanding for RAID architectures. The same disk array simulator and similar synthetic workloads have been used a number of times in the literature <ref> [8, 10, 22] </ref>. For larger accesses, there are a number of possible optimizations that can be implemented for any of the three schemes discussed in this section. Our implementation of the Datum array architecture took about 2,400 lines of code. <p> Even though Evenodd does not inherently prevent declustering, the layout proposed in [3] makes every disk a member of every stripe; this limits the array to a maximum of 259 disks, and is likely to result in performance degradation during on-line reconstruction <ref> [10] </ref>. Burkhard and Menon [4] analyze the additional dependability gained by using MDS codes to tolerate multiple failures; that paper concentrates on reliability calculations, and does not address the issues of run-time performance or array layout. <p> Several existing studies investigate parity decluste-ring, i.e. the uniform distribution of data and parity redundancy over the array when each stripe uses a subset of the disks. Muntz and Lui [14] suggest balanced incomplete block designs (BIBDs) to achieve uniform distribution; Holland and Gibson <ref> [10] </ref> implemented the scheme and evaluated its performance. BIBDs have two main drawbacks: they do not exist for every array configuration (thus only certain values of the array parameters can be implemented), and they can require substantial amounts of controller memory for table storage, of the order of several Mbytes.
Reference: [11] <author> F. MacWilliams and N. Sloane. </author> <title> The Theory of Error-Correcting Codes. </title> <publisher> North-Holland, </publisher> <address> The Netherlands, </address> <year> 1977. </year>
Reference-contexts: This can be done only if the whole user data portion of the stripe is also recorded in the array in the clear; that is, the encoding must be systematic <ref> [11] </ref>. The matrix T , shown in Equation (2), consists of the identity matrix and f more columns so that the set of m + f columns satisfies the linear independence condition. <p> We can always obtain a matrix T with the desired structure using elementary row operations on a candidate matrix obtained by any method. IDA makes no assumption regarding the amount of space required to represent the encoded values. Maximum distance separable (MDS) error-correcting encoding <ref> [11] </ref>, which is similar to IDA, ensures that the representation of m stripe units, each of size s bytes, as m + f stripe units will require exactly (m + f ) fi s bytes. This is accomplished by using finite fields to represent the values. <p> for each of the encoded stripe units. (d j;1 ; d j;2 ; : : : ; d j;m ) T = The two field operations can be implemented very efficiently: addition is the bitwise exclusive-or operation, and multiplication can be implemented using fast table lookups using a discrete logarithm <ref> [11] </ref> table of size 256 bytes. In general, for a finite field of elements we can store the logarithm table with elements rather than the structure with fi ( 2 ) elements that the full multiplication table requires. <p> As a consequence of the Singleton bound <ref> [11] </ref>, it is not possible to write less than f + 1 stripe units if the array is to tolerate up to f failures. Accordingly, Datum performs the optimal number f +1 disk writes per small write operation.
Reference: [12] <author> J. Menon and J. Cortney. </author> <title> The architecture of a fault-tolerant cached RAID controller. </title> <booktitle> In Proc. of the Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 76-86, </pages> <year> 1993. </year>
Reference-contexts: We assume the array controller can identify a failed disk. We do not discuss how to tolerate failures in other components of the system, such as interconnection buses or array controllers <ref> [6, 12] </ref>. Disk space is logically structured into stripe units; each stripe unit has a fixed number of sectors, and contains either user data or redundant data. Datum services accesses involving an integral number of stripe units at the client level. <p> The literature discusses system-level optimizations that do not require the user to wait until the f + 1 writes complete successfully. For instance, updates to redundant stripes can be deferred while logging them to disk [22], while writing them to non-volatile cache <ref> [12] </ref>, or by doing nothing at all [18].
Reference: [13] <author> A. Merchant and P. Yu. </author> <title> Performance analysis of a dual striping strategy for replicated disk arrays. </title> <booktitle> In Proc. of the Second International Conference on Parallel and Distributed Information Systems, </booktitle> <pages> pages 148-157, </pages> <address> San Diego, </address> <month> January </month> <year> 1993. </year>
Reference-contexts: Pseudorandom permutation declustering has been proposed by Schwarz and Burkhard [21] as well as Merchant and Yu <ref> [13] </ref>. ACATS declustering, proposed by Schwarz and Burkhard [20], works deterministically for any array configuration thus avoiding the need for a pseudorandom generator and explicit BIBD configuration storage. RAID architectures were originally proposed by Pat-terson et al in [15].
Reference: [14] <author> R. Muntz and J. Lui. </author> <title> Performance analysis of disk arrays under failure. </title> <booktitle> In Proceedings of the 16th VLDB Conference, </booktitle> <pages> pages 162-173, </pages> <address> Brisbane, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Their conclusions are applicable to Datum, as our IDA is an MDS code as well. Several existing studies investigate parity decluste-ring, i.e. the uniform distribution of data and parity redundancy over the array when each stripe uses a subset of the disks. Muntz and Lui <ref> [14] </ref> suggest balanced incomplete block designs (BIBDs) to achieve uniform distribution; Holland and Gibson [10] implemented the scheme and evaluated its performance.
Reference: [15] <author> D. Patterson, G. Gibson, and R. Katz. </author> <title> A case for redundant arrays of inexpensive disks (RAID). </title> <booktitle> In Proc. of the ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 109-116, </pages> <year> 1988. </year>
Reference-contexts: 1 Introduction Disk arrays <ref> [15] </ref> offer significant advantages over conventional disks. Fragmentation of the total storage space into multiple inexpensive disks allows cost-effective solutions that benefit from the aggregate bandwidth of the component disks, and from the smaller seek latencies associated with shorter arms. <p> It is therefore necessary to store, along with the data, redundant information in order to recover from disk fail This work was partially supported by grants from the Air Force Office of Scientific Research, California MICRO, Oakland, and Symbios Logic, Wichita. ures. Most existing approaches like RAID-5 <ref> [15] </ref> and parity declustering [10] rely on parity calculations to provide tolerance against a single disk failure. <p> Our layout functions meet the first five layout goals set forth by Holland and Gibson [10]. When a disk fails and each stripe is distributed over all the disks of the array (like in RAID-5 <ref> [15] </ref>), an access to a random stripe unit has probability 1=n of hitting the failed disk, thus resulting in n 1 accesses to all the other stripe units. <p> We chose parity declustering as a basis for comparison because it distributes data onto disks using balanced block designs, and it uses the same encoding (parity) as the RAID family <ref> [15] </ref>. Evenodd was considered because it is the only existing scheme that can tolerate two failures while using the same minimal amount of redundant storage, regardless of array size. <p> ACATS declustering, proposed by Schwarz and Burkhard [20], works deterministically for any array configuration thus avoiding the need for a pseudorandom generator and explicit BIBD configuration storage. RAID architectures were originally proposed by Pat-terson et al in <ref> [15] </ref>. From the solutions presented there, RAID-5 provides tolerance against a single failure and does not decluster the parity information. Multiple-erasure SID [7] is a RAID scheme for multimedia work-loads that can tolerate an arbitrary number of failures while performing contiguous accesses to the disks.
Reference: [16] <author> F. Preparata. </author> <title> Holographic dispersal and recovery of information. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 35(5) </volume> <pages> 1123-4, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: T = B @ 0 1 : : : 0 t 2;m+1 t 2;m+2 : : : t 2;m+f . . . . . . . . . . . . 1 C (2) Rabin [17] and Preparata <ref> [16] </ref> describe ways of constructing T that result in low computational costs; however, none achieves the systematic property yielding m clear stripe units. We can always obtain a matrix T with the desired structure using elementary row operations on a candidate matrix obtained by any method.
Reference: [17] <author> M. Rabin. </author> <title> Efficient dispersal of information for security, load balancing, and fault tolerance. </title> <journal> Journal of the ACM, </journal> <volume> 36(2) </volume> <pages> 335-48, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: Our approach is based on the information dispersal algorithm (IDA) <ref> [17] </ref>, a coding technique for which a fast VLSI implementation exists [2]. We tested an implementation of Datum against implementations of parity declustering [10] and Evenodd [3], by utilizing a very accurate disk array simulator [8]. <p> The existence of redundant units is transparent to the clients, that have access to a linear address space containing only user data. 3 Information Dispersal The information dispersal algorithm (IDA) <ref> [17] </ref> encodes a sequence F = (d 1 ; d 2 ; : : : ; d m ) of m integers into a sequence of m + f integers in such a way that any m of the m + f integers suffice to recover F . <p> T = B @ 0 1 : : : 0 t 2;m+1 t 2;m+2 : : : t 2;m+f . . . . . . . . . . . . 1 C (2) Rabin <ref> [17] </ref> and Preparata [16] describe ways of constructing T that result in low computational costs; however, none achieves the systematic property yielding m clear stripe units. We can always obtain a matrix T with the desired structure using elementary row operations on a candidate matrix obtained by any method.
Reference: [18] <author> S. Savage and J. Wilkes. </author> <title> AFRAID| a frequently redundant array of independent disks. </title> <booktitle> In Proc. of the USENIX 1996 Annual Technical Conference, </booktitle> <month> January </month> <year> 1996. </year>
Reference-contexts: The literature discusses system-level optimizations that do not require the user to wait until the f + 1 writes complete successfully. For instance, updates to redundant stripes can be deferred while logging them to disk [22], while writing them to non-volatile cache [12], or by doing nothing at all <ref> [18] </ref>.
Reference: [19] <author> E. Schwabe and I. Sutherland. </author> <title> Improved parity-declustered layouts for disk arrays. </title> <booktitle> In Proc. of the Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 76-84, </pages> <address> Cape May, N.J., </address> <month> June </month> <year> 1994. </year>
Reference-contexts: To address the first problem, Schwabe and Sutherland <ref> [19] </ref> have extended the BIBD generation techniques, and bounded the degradation in uniformity incurred when using approximate block designs.
Reference: [20] <author> T. Schwarz and W. Burkhard. </author> <title> Almost complete address translation (ACATS) disk array declustering. </title> <booktitle> In Proceedings of the Eighth IEEE Symposium on Parallel and Distributed Processing, </booktitle> <pages> pages 324-331, </pages> <address> New Orleans, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: Pseudorandom permutation declustering has been proposed by Schwarz and Burkhard [21] as well as Merchant and Yu [13]. ACATS declustering, proposed by Schwarz and Burkhard <ref> [20] </ref>, works deterministically for any array configuration thus avoiding the need for a pseudorandom generator and explicit BIBD configuration storage. RAID architectures were originally proposed by Pat-terson et al in [15]. From the solutions presented there, RAID-5 provides tolerance against a single failure and does not decluster the parity information.
Reference: [21] <author> T. Schwarz and W. Burkhard. </author> <title> Reliability and performance of RAIDs. </title> <journal> IEEE Transactions on Magnetics, </journal> <volume> 31 </volume> <pages> 1161-1166, </pages> <month> March </month> <year> 1995. </year>
Reference-contexts: Pseudorandom permutation declustering has been proposed by Schwarz and Burkhard <ref> [21] </ref> as well as Merchant and Yu [13]. ACATS declustering, proposed by Schwarz and Burkhard [20], works deterministically for any array configuration thus avoiding the need for a pseudorandom generator and explicit BIBD configuration storage. RAID architectures were originally proposed by Pat-terson et al in [15].
Reference: [22] <author> D. Stodolsky, G. Gibson, W. Courtright, and M. Holland. </author> <title> A redundant disk array architecture for efficient small writes. </title> <type> Technical report CMU-CS94-170, </type> <institution> Carnegie Mellon Univ., </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: Accordingly, Datum performs the optimal number f +1 disk writes per small write operation. The literature discusses system-level optimizations that do not require the user to wait until the f + 1 writes complete successfully. For instance, updates to redundant stripes can be deferred while logging them to disk <ref> [22] </ref>, while writing them to non-volatile cache [12], or by doing nothing at all [18]. <p> We chose these workloads because small accesses (particularly writes) are typically most demanding for RAID architectures. The same disk array simulator and similar synthetic workloads have been used a number of times in the literature <ref> [8, 10, 22] </ref>. For larger accesses, there are a number of possible optimizations that can be implemented for any of the three schemes discussed in this section. Our implementation of the Datum array architecture took about 2,400 lines of code.
References-found: 22

