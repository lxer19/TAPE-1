URL: ftp://ftp.cs.virginia.edu/pub/techreports/CS-93-18.ps.Z
Refering-URL: ftp://ftp.cs.virginia.edu/pub/techreports/README.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Access Ordering and Effective Memory Bandwidth  
Author: Steven A. Moyer 
Abstract: Technical Report CS-93-18 April 5, 1993 
Abstract-found: 1
Intro-found: 1
Reference: [AvCK87] <author> Aven-O, Coffman-E, Kogan-Y, </author> <title> Stochastic Analysis of Computer Storage, </title> <publisher> Kluwer Academic Publishers, Norwell, </publisher> <address> MA, </address> <year> 1987, </year> <pages> pp. 102-118. </pages>
Reference-contexts: While a number of stochastic models of program locality have been derived <ref> [AvCK87, ShTu72, SpDe72] </ref>, they are mainly applicable to the performance analysis of cache memories and virtual memory systems. For the purpose of deriving analytic results, most memory architectures are modeled under the assumption of a uniform random pattern of access, as discussed below.
Reference: [Bail87] <author> Bailey-D, </author> <title> Vector Computer Memory Bank Contention, </title> <journal> IEEE Trans. Com-put., </journal> <volume> 36, 3, </volume> <year> 1987, </year> <pages> pp. 293-298. </pages>
Reference-contexts: Cheung and Smith [ChSm86] perform a simulation study that characterizes reference stream interaction for up to three independent memory ports. Though no analytic results are presented, they too classify steady state conict situations and provide simulation performance data. Bailey <ref> [Bail87] </ref> takes a different approach in which performance of a vector computer memory system is modeled analytically under the assumption of uniform random accesses; while these results are not representative of the performance obtained in access 16 ing vectors, relationships between the number of processors, number of modules, and module access
Reference: [BeDa91] <author> Benitez-M, Davidson-J, </author> <title> Code Generation for Streaming: an Access/Execute Mechanism, </title> <booktitle> Proc. </booktitle> <address> ASPLOS-IV, </address> <year> 1991, </year> <pages> pp. 132-141. </pages>
Reference-contexts: Section 2.4 considers alternative parallel memory storage schemes for increasing the effective bandwidth of vector accesses. 2.1 Stream Detection Access ordering algorithms derived in this thesis presuppose the existence of compiler techniques to detect stream-oriented computations. Benitez and Davidson <ref> [BeDa91] </ref> describe a technique for explicitly detecting streaming opportunities, including those in recurrence relations. Furthermore, since stream-oriented computations reference vector operands, well known vectorization techniques are applicable, such as those described by Wolfe [Wolf89]. 2.2 Access Scheduling Techniques Access ordering is a compilation technique for maximizing effective memory bandwidth. <p> Loop-carried data dependence can not exist as a result of the stream interaction restriction. Note that loop-carried data dependence can often be removed, resulting in a computation that conforms to the stream interaction restriction. For example, restructuring the loop of be removed, as described in <ref> [BeDa91] </ref>. Though data dependence does not exist in the usual context, it is present in the data ow sense; that is, as right-hand-side values required in performing a computation. <p> Consider the finite difference approximation to the first derivative t i t j k k k k q k q k v i 1+ v i 1 -( ) = 30 Analysis techniques <ref> [BeDa91, CaCK90] </ref> can transform the natural pattern of access to vector to a simple stream requiring one access per iteration; two values of are preloaded prior to entering the loop, and each successive value accessed is carried in a register for two iterations.
Reference: [BeRo91] <author> Bernstein-D, Rodeh-M, </author> <title> Global Instruction Scheduling for Superscalar Machines, </title> <booktitle> Proc. SIGPLAN91 Conf. Prog. Lang. Design and Implementation, </booktitle> <year> 1991, </year> <pages> pp. 241-255. </pages>
Reference-contexts: Such techniques are referred to here collectively as access scheduling. Essentially, access scheduling techniques attempt to separate the execution of a load/store instruction from the execution of the instruction which consumes/produces its operand, reducing the time the processor spends delayed on memory requests. 12 Bernstein and Rodeh <ref> [BeRo91] </ref> present an algorithm for scheduling intra-loop instructions on superscalar architectures that accommodates load delay.
Reference: [BuCo70] <author> Burnett-G, Coffman-E, </author> <title> A Study of Interleaved Memory Systems, </title> <booktitle> 1970 Spring Joint Computer Conference, AFIPS Conf. Proc., </booktitle> <volume> 36, </volume> <year> 1970, </year> <pages> pp. 467-474. </pages>
Reference-contexts: Hellerman [Hell66] assumes a uniform random distribution of accesses and no buffering of conicting requests in deriving the well known formula that for m memory modules, an average of approximately are operating concurrently at any given time. Burnett and Coffman <ref> [BuCo70] </ref> derive an analytic model in which data and instruction requests are separated and serviced alternately; instruction accesses are assumed sequential with a fixed probability of branching, data requests access the next sequential module with probability and any other module with probability .
Reference: [BuKu71] <author> Budnik-P, Kuck-D, </author> <title> The Organization and Use of Parallel Memories, </title> <journal> IEEE Trans. Comput., </journal> <volume> 20, 12, </volume> <year> 1971, </year> <pages> pp. 1566-1569. </pages>
Reference-contexts: For parallel memory systems, the storage scheme limits the degree of concurrency achievable by a given computation. This research considers only two such storage schemes: sequentially interleaved and multicopy. However, a number of other schemes have been proposed and are discussed below. Budnik and Kuck <ref> [BuKu71] </ref> observed that for a sequentially interleaved storage scheme, only vectors with strides relatively prime to the number of modules m can be accessed without conict, i.e. at maximum system bandwidth. This result led Lawrie and Vora [LaVo82] to propose a memory system based on a prime number of modules. <p> This result led Lawrie and Vora [LaVo82] to propose a memory system based on a prime number of modules. Such a system was developed for the Burroughs Scientific Processor [KuSt82]; however, prime memory systems have proved impractical due to the computational complexity of the storage scheme. Budnik and Kuck <ref> [BuKu71] </ref> propose the use of skewed storage in which each successive set of m storage locations is assigned to m memory modules with a skew relative to the 17 previous set.
Reference: [CaCK90] <author> Callahan-D, Carr-S, Kennedy-K, </author> <title> Improving Register Allocation for Subscripted Variables, </title> <booktitle> Proc. SIGPLAN 90 Conf. Prog. Lang. Design and Implementation, </booktitle> <year> 1990, </year> <pages> pp. 53-65. </pages>
Reference-contexts: Consider the finite difference approximation to the first derivative t i t j k k k k q k q k v i 1+ v i 1 -( ) = 30 Analysis techniques <ref> [BeDa91, CaCK90] </ref> can transform the natural pattern of access to vector to a simple stream requiring one access per iteration; two values of are preloaded prior to entering the loop, and each successive value accessed is carried in a register for two iterations.
Reference: [CaKe89] <author> Carr-S, Kennedy-K, </author> <title> Blocking Linear Algebra Codes for Memory Hierarchies, </title> <booktitle> Proc. of the Fourth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <year> 1989. </year>
Reference-contexts: In general purpose scalar computing, the addition of cache memory is often a sufficient solution to the memory latency and bandwidth problems given the spatial and temporal locality of reference exhibited by most codes. For scientific computations, vectors are normally too large to cache. Iteration space tiling <ref> [CaKe89, Wolf89] </ref> can partition problems 1. Common superscalar and VLIW architectures incorporate concurrent pipelined functional units. 2 into cache-size blocks, however tiling often creates cache conicts [LaRW91] and the technique is difficult to automate. <p> However many codes generate multiple references to a subset of vector operands and hence can benefit from caching, particularly when implemented using strip-mining and tiling techniques <ref> [CaKe89, Wolf89] </ref>. Thus access ordering and caching should be used together to complement one another, exploiting the full memory hierarchy to maximize memory bandwidth.
Reference: [CaKP91] <author> Callahan-D, Kennedy-K, Porterfield-A, </author> <title> Software Prefetching, </title> <booktitle> Proc. </booktitle> <address> ASP-LOS-IV, </address> <year> 1991, </year> <pages> pp. 40-52. </pages>
Reference-contexts: Weiss and Smith [WeSm90] present a comprehensive study in which they classify and evaluate software pipelining techniques implemented in conjunction with loop unrolling. Klaiber and Levy [KlLe91] and Callahan et al <ref> [CaKP91] </ref> propose the use of fetch instructions to preload data into cache; compiler techniques are developed for inserting fetch instructions into the normal instruction stream. Access ordering and access scheduling are fundamentally different. Access scheduling techniques allow load/store architectures to better tolerate memory latency. <p> Few processors currently implement the non-caching load instruction required for access ordering. However, just as the demonstrable advantages of prefetching <ref> [CaKP91, KlLe91] </ref> led to processors with prefetch instructions, access ordering may provide the impetus for more manufacturers to implement a non-caching load. Similarly, access ordering demonstrates the true potential for page-mode memory components and provides incentive to further reduce page-hit times.
Reference: [ChKL77] <author> Chang-D, Kuck-D, Lawrie-D, </author> <title> On the Effective Bandwidth of Parallel Memories, </title> <journal> IEEE Trans. Comput., </journal> <volume> 26, 5, </volume> <year> 1977, </year> <pages> pp. 480-489. </pages>
Reference-contexts: Later work [CoBS71] extends this model to include buffering of access conicts. Ravi [Ravi72] develops an analytic model for the performance of a multiprocessor system accessing a sequentially interleaved memory, assuming a uniform random distribution of accesses. Chang, Kuck, and Lawrie <ref> [ChKL77] </ref> summarize the work of previous authors and introduce the notion of data dependence between accesses of a multiprocessor system; analytic models are derived for each dependence class, again assuming a uniform random distribution of requests. m a b 1 a () N= 15 Because previous performance studies of memory architectures
Reference: [ChSm86] <author> Cheung-T, Smith-J, </author> <title> A Simulation Study of the CRAY X-MP Memory System, </title> <journal> IEEE Trans. Comput., </journal> <volume> 35, 7, </volume> <year> 1986, </year> <pages> pp. 613-622. 145 </pages>
Reference-contexts: In the case of conict-free access, calculation of bandwidth is trivial and represents an upper bound on performance; for most other cases bandwidth calculation is intractable. Cheung and Smith <ref> [ChSm86] </ref> perform a simulation study that characterizes reference stream interaction for up to three independent memory ports. Though no analytic results are presented, they too classify steady state conict situations and provide simulation performance data.
Reference: [CoBS71] <author> Coffman-E, Burnett-G, Snowdon-R, </author> <title> On the Performance of Interleaved Memories with Multiple-Word Bandwidths, </title> <journal> IEEE Trans. Comput., </journal> <volume> 20, 12, </volume> <year> 1971, </year> <pages> pp. 1570-1573. </pages>
Reference-contexts: Burnett and Coffman [BuCo70] derive an analytic model in which data and instruction requests are separated and serviced alternately; instruction accesses are assumed sequential with a fixed probability of branching, data requests access the next sequential module with probability and any other module with probability . Later work <ref> [CoBS71] </ref> extends this model to include buffering of access conicts. Ravi [Ravi72] develops an analytic model for the performance of a multiprocessor system accessing a sequentially interleaved memory, assuming a uniform random distribution of accesses.
Reference: [GaJo79] <author> Garey-M, Johnson-D, </author> <title> Computers and Intractability: A Guide to the Theory of NP-Completeness, </title> <publisher> Freeman, </publisher> <address> New York N.Y., </address> <year> 1979. </year>
Reference-contexts: Unfortunately, in the presence of dependencies, determining an access sequence that maximizes concurrency is NP-complete with a time complexity exponential in the number of accesses; this result is obtained by restriction to precedence constrained scheduling <ref> [GaJo79] </ref>. As an optimal solution is intractable, a heuristic solution is presented below. In defining an access sequence for streams S, accesses are performed in two phases: a read phase and a write phase. By the stream interaction restriction, streams associated with each phase are independent. <p> Determining such a sequence is NP-complete with a time complexity exponential in the number of accesses; this result is obtained by restriction to precedence constrained scheduling <ref> [GaJo79] </ref>. As an optimal solution is intractable, a heuristic solution analogous to that derived in 5.5.2 is presented below. In the sections that follow, a base access sequence is first developed for computations that do not specify a read-modify-write. <p> Determining such a sequence is NP-complete with a time complexity exponential in the number of accesses; this result is obtained by restriction to multiprocessor scheduling <ref> [GaJo79] </ref>. As an optimal solution is intractable, a heuristic solution is presented below. In the sections that follow, a base access sequence and module reference model are developed. Intermixing and wrap-around adjacency are then discussed for computations implementing a read-modify-write.
Reference: [GoCh84] <author> Goodman-J, Chiang-M, </author> <title> The Use of Static Column RAM as a Memory Hierarchy, </title> <booktitle> Proc. 11th Annual Intl. Symp. on Comput. Architecture, </booktitle> <year> 1984, </year> <pages> pp. 167-174. </pages>
Reference-contexts: Peelen and Van de Goor [PeVa87] analyze the performance of page-mode DRAMs in a single module architecture by viewing page-mode as a cache with a single long cache line; a performance predictor is derived in the standard manner as a function of the miss ratio. Goodman and Chiang <ref> [GoCh84] </ref> consider an architecture in which parallel modules are accessed via a mid-order interleaving scheme such that sequential addresses proceed across a page, changing modules at page boundaries.
Reference: [GuSo88] <author> Gupta-R, Soffa-M, </author> <title> Compile-time Techniques for Efficient Utilization of Parallel Memories, </title> <journal> SIGPLAN Not., </journal> <volume> 23, 9, </volume> <year> 1988, </year> <pages> pp. 235-246. </pages>
Reference-contexts: Ramamoorthy and Wah [RaWa81] present an optimal algorithm for initiating queued requests on a sequentially interleaved memory to maximize module concurrency; however, this is an inherently dynamic technique that assumes independent random requests and knowledge of the modules to which requests are to be mapped. Gupta and Soffa <ref> [GuSo88] </ref> demonstrate compilation techniques for distributing scalars across parallel memory modules to allow for concurrent access by VLIW architectures; however, these techniques are not applicable to vector data. 13 2.3 Modeling Memory System Behavior Deriving an ordering algorithm and corresponding performance predictor requires analytic results that capture the interaction of a
Reference: [HaJu87] <author> Harper-D, Jump-J, </author> <title> Vector Access Performance in Parallel Memories Using a Skewed Storage Scheme, </title> <journal> IEEE Trans. Comput., </journal> <volume> 36, 12, </volume> <year> 1987, </year> <pages> pp. 1440-1449. </pages>
Reference-contexts: Budnik and Kuck [BuKu71] propose the use of skewed storage in which each successive set of m storage locations is assigned to m memory modules with a skew relative to the 17 previous set. Harper and Jump <ref> [HaJu87] </ref> present a comprehensive study of a skewed storage scheme that is shown to reduce conict over a wide range of strides.
Reference: [HaLi89] <author> Harper-D, Linebarger-D, </author> <title> A Dynamic Storage Scheme for Conict-Free Vector Access, </title> <booktitle> Proc. 16th Intl. Symp. Comput. Architecture, </booktitle> <year> 1989, </year> <pages> pp. 72-77. </pages>
Reference-contexts: Harper and Jump further demonstrate that with skewed storage, vector accesses can reference a sequence of modules with a periodicity that exceeds m, allowing queues placed at each module to buffer conicting requests and increase bandwidth. As an alternative to implementing a single storage scheme, Harper <ref> [HaLi89, Harp89] </ref> proposes a dynamic storage scheme in which each vector is stored so as to provide optimal bandwidth for a given stride of access; later work demonstrates that dynamic storage schemes can be devised that allow optimal access to a vector for a set of strides [Harp91].
Reference: [Harp89] <author> Harper-D, </author> <title> Address Transformations to Increase Memory Performance, </title> <booktitle> Proc. 1989 Intl. Conf. Parallel Processing, </booktitle> <year> 1989, </year> <pages> pp. 237-241. </pages>
Reference-contexts: Harper and Jump further demonstrate that with skewed storage, vector accesses can reference a sequence of modules with a periodicity that exceeds m, allowing queues placed at each module to buffer conicting requests and increase bandwidth. As an alternative to implementing a single storage scheme, Harper <ref> [HaLi89, Harp89] </ref> proposes a dynamic storage scheme in which each vector is stored so as to provide optimal bandwidth for a given stride of access; later work demonstrates that dynamic storage schemes can be devised that allow optimal access to a vector for a set of strides [Harp91].
Reference: [Harp91] <author> Harper-D, </author> <title> Block, Multistride Vector, and FFT Accesses in Parallel Memory Systems, </title> <journal> IEEE Trans. Parallel and Dist. Systems, </journal> <volume> 2, 1, </volume> <year> 1991, </year> <pages> pp. 43-51. </pages>
Reference-contexts: storage scheme, Harper [HaLi89, Harp89] proposes a dynamic storage scheme in which each vector is stored so as to provide optimal bandwidth for a given stride of access; later work demonstrates that dynamic storage schemes can be devised that allow optimal access to a vector for a set of strides <ref> [Harp91] </ref>. Rau [Rau91] analyzes a scheme that assigns storage locations to modules in a pseudo-random fashion, rendering memory performance nearly stride insensitive; such a memory system has been incorporated into Cydromes Cydra 5 Departmental Supercomputer [RaSY89]. As with skewed and dynamic schemes, pseudo-random storage schemes benefit from memory module queues.
Reference: [Hell66] <author> Hellerman-H, </author> <title> On the Average Speed of a Multiple-Module Storage System, </title> <journal> IEEE Trans. Comput., </journal> <volume> 15, 8, </volume> <year> 1966, </year> <note> p. 670. </note>
Reference-contexts: To date, no other studies involving page-mode memory components have been located. Concurrency in sequentially interleaved memory architectures has been the subject of numerous studies; analytic results are general based on stochastic access sequences. Hellerman <ref> [Hell66] </ref> assumes a uniform random distribution of accesses and no buffering of conicting requests in deriving the well known formula that for m memory modules, an average of approximately are operating concurrently at any given time.
Reference: [Inte89] <author> Intel Corporation, </author> <title> i860 64-Bit Microprocessor Hardware Reference Manual, </title> <address> ISBN 1-55512-106-3, </address> <year> 1989. </year>
Reference-contexts: This general system model is representative of uniprocessor systems and single-processor nodes of distributed memory parallel machines. The processor is presumed to implement a non-caching load instruction, ala Intels i860 <ref> [Inte89] </ref>, allowing the sequence of requests observed by the memory system to be controlled via software. For access ordering, all memory references are assumed to be non-caching.
Reference: [KlLe91] <author> Klaiber-A, Levy-H, </author> <title> An Architecture for Software-Controlled Data Prefetching, </title> <booktitle> Proc. 18th Annual Intl. Symp. Comput. Architecture, </booktitle> <year> 1991, </year> <pages> pp. 43-53. </pages>
Reference-contexts: Weiss and Smith [WeSm90] present a comprehensive study in which they classify and evaluate software pipelining techniques implemented in conjunction with loop unrolling. Klaiber and Levy <ref> [KlLe91] </ref> and Callahan et al [CaKP91] propose the use of fetch instructions to preload data into cache; compiler techniques are developed for inserting fetch instructions into the normal instruction stream. Access ordering and access scheduling are fundamentally different. Access scheduling techniques allow load/store architectures to better tolerate memory latency. <p> Few processors currently implement the non-caching load instruction required for access ordering. However, just as the demonstrable advantages of prefetching <ref> [CaKP91, KlLe91] </ref> led to processors with prefetch instructions, access ordering may provide the impetus for more manufacturers to implement a non-caching load. Similarly, access ordering demonstrates the true potential for page-mode memory components and provides incentive to further reduce page-hit times.
Reference: [KuSt82] <author> Kuck-D, Stokes-R, </author> <title> The Burroughs Scientific Processor (BSP), </title> <journal> IEEE Trans. Comput., </journal> <volume> 31, 5, </volume> <year> 1982, </year> <pages> pp. 363-375. 146 </pages>
Reference-contexts: This result led Lawrie and Vora [LaVo82] to propose a memory system based on a prime number of modules. Such a system was developed for the Burroughs Scientific Processor <ref> [KuSt82] </ref>; however, prime memory systems have proved impractical due to the computational complexity of the storage scheme.
Reference: [Lam88] <author> Lam-M, </author> <title> Software Pipelining: An Effective Scheduling Technique for VLIW Machines, </title> <booktitle> Proc. SIGPLAN88 Conf. Prog. Lang. Design and Implementation, </booktitle> <year> 1988, </year> <pages> pp. 318-328. </pages>
Reference-contexts: Lam <ref> [Lam88] </ref> presents a technique referred to as software pipelining that structures code such that a given loop iteration loads the data for a later iteration, stores results from a previous iteration, and performs computation for the current iteration.
Reference: [LaRW91] <author> Lam-M, Rothberg-E, Wolf-M, </author> <title> The Cache Performance and Optimizations of Blocked Algorithms, </title> <booktitle> Fourth International Conf. on Arch. Support for Prog. Langs. and Operating Systems, </booktitle> <year> 1991, </year> <pages> pp. 63-74. </pages>
Reference-contexts: For scientific computations, vectors are normally too large to cache. Iteration space tiling [CaKe89, Wolf89] can partition problems 1. Common superscalar and VLIW architectures incorporate concurrent pipelined functional units. 2 into cache-size blocks, however tiling often creates cache conicts <ref> [LaRW91] </ref> and the technique is difficult to automate. Furthermore, only a subset of the vectors accessed will generally be reused and hence benefit from caching. Finally, caching may actually reduce the effective bandwidth achieved by a computation by fetching extraneous data for non-unit strides. <p> Furthermore, only a subset of the vectors accessed will generally be reused and hence benefit from caching. Finally, caching may actually reduce the effective bandwidth achieved by a computation by fetching extraneous data for non-unit strides. Thus, as noted by Lam et al <ref> [LaRW91] </ref>, while data caches have been demonstrated to be effective for general-purpose applications..., their effectiveness for numerical code has not been established. Access ordering is a compiler technology developed in this thesis that addresses the memory bandwidth problem for scalar processors executing scientific codes. <p> Note that to insure pseudo vector register elements do not conict in cache, vector storage must not exceed cache capacity for a direct-mapped cache or cache capacity for an n-way set-associative cache <ref> [LaRW91] </ref>. double VectorRegister [2][64]; 1 n ( ) th 127 Within a loop, vector operands are loaded into the pseudo vector registers, arithmetic operations are performed on vector register data, and vector register results are stored back to the appropriate vector elements in memory. <p> To illustrate, consider implementing the matrix-vector multiply operation where A and B are matrices and and are vectors. y A B+( ) x= 130 size is dependent on cache size and structure <ref> [LaRW91] </ref>. Elements of are preloaded into cache memory at the appropriate loop level, and elements of A and B are referenced via non-caching loads; the reference to is a constant within the inner loop and is preloaded into a processor register. <p> Lam et al <ref> [LaRW91] </ref> analyze a technique that eliminates cache conicts by copying data to be cached into a contiguous address space. Note that in applying this copy optimization, non-caching loads, and hence access ordering, can be used to reduce cache conict and extraneous data movement.
Reference: [LaVo82] <author> Lawrie-D, Vora-C, </author> <title> The Prime Memory System for Array Access, </title> <journal> IEEE Trans. Comput., </journal> <volume> 31, 5, </volume> <year> 1982, </year> <pages> pp. 435-442. </pages>
Reference-contexts: Budnik and Kuck [BuKu71] observed that for a sequentially interleaved storage scheme, only vectors with strides relatively prime to the number of modules m can be accessed without conict, i.e. at maximum system bandwidth. This result led Lawrie and Vora <ref> [LaVo82] </ref> to propose a memory system based on a prime number of modules. Such a system was developed for the Burroughs Scientific Processor [KuSt82]; however, prime memory systems have proved impractical due to the computational complexity of the storage scheme.
Reference: [Lee90] <author> Lee-K, </author> <title> On the Floating-Point Performance of the i860 Microprocessor, </title> <institution> NASA Ames Research Center, NAS Systems Division, RNR-090-019, </institution> <year> 1990. </year>
Reference-contexts: Extensive tests of systems constructed from one such processor, Intels i860, show that as a result of insufficient bandwidth, the average performance of hand optimized scientific kernels is only 1/5 peak processor rate; for compiler generated code average performance is an order of magnitude below peak performance <ref> [Lee90, Moye91] </ref>. The majority of improvement in hand-coded routines over compiler generated code results from tailoring accesses to memory system performance characteristics.
Reference: [Lee91] <author> Lee-K, </author> <title> Achieving High Performance on the i860 Microprocessor with Naspack Subroutines, </title> <institution> NASA Ames Research Center, NAS Systems Division, RNR-091-029, </institution> <year> 1991. </year>
Reference-contexts: However, loop unrolling creates register pressure and has traditionally been limited by register resources. Lee <ref> [Lee91] </ref> presents a technique that employs cache memory to mimic a set of vector registers, effectively increasing register file size for vector computations. Storage is defined for a set of vectors, each of which represents a pseudo register; vector length corresponds to register size.
Reference: [Mcma90] <author> McMahon-F, </author> <title> FORTRAN Kernels: </title> <type> MFLOPS, </type> <institution> Lawrence Livermore National Laboratory, </institution> <note> Version MF443. </note>
Reference-contexts: The daxpy computation is the double-precision version of the axpy computation discussed earlier. Similarly dvaxpy is the double-precision version of the vaxpy (vector axpy) computation The remaining computations in Table 2 are selections from the Livermore Loops <ref> [Mcma90] </ref>, with all vectors defined as double-precision. This set of benchmark kernels is used in all subsequent performance evaluations.
Reference: [Moye91] <author> Moyer-S, </author> <title> Performance of the iPSC/860 Node Architecture, </title> <institution> University of Virginia, IPC-TR-91-007, </institution> <year> 1991. </year>
Reference-contexts: Extensive tests of systems constructed from one such processor, Intels i860, show that as a result of insufficient bandwidth, the average performance of hand optimized scientific kernels is only 1/5 peak processor rate; for compiler generated code average performance is an order of magnitude below peak performance <ref> [Lee90, Moye91] </ref>. The majority of improvement in hand-coded routines over compiler generated code results from tailoring accesses to memory system performance characteristics. <p> Thus, results represent maximum achievable bandwidth. The parameters of the single-module memory are defined in Table 1; sizes are in bytes and times are in nanoseconds. These parameters are representative of the node memory system for the Intel IPSC/860, as detailed in <ref> [Moye91] </ref>. Table 2 presents simulation results comparing effective bandwidth achieved by the natural versus ordered access sequence for a range of scientific kernels. For access ordering, the depth of loop unrolling is 4 in all cases. The daxpy computation is the double-precision version of the axpy computation discussed earlier.
Reference: [NEC92] <institution> NEC Corporation, 16Mb Synchronous DRAM, </institution> <note> Preliminary Data Sheet v3.1, </note> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: Note that LL-3 and LL-4 represent dot products and do not generate write streams, thus the substantial increase in performance. For next generation DRAMs the page miss-hit cycle time ratio will increase dramatically <ref> [NEC92] </ref>. This situation benefits a multicopy architecture as reduction in page overhead becomes even more critical to obtaining good performance, as illustrated below. m 2 = 122 Assume a 4 module multicopy system with buffer depth 1 and an equivalent interleaved system.
Reference: [OeLa85] <author> Oed-W, Lange-O, </author> <title> On the Effective Bandwidth of Interleaved Memories in Vector Processor Systems, </title> <journal> IEEE Trans. Comput., </journal> <volume> 34, 10, </volume> <year> 1985, </year> <pages> pp. 949-957. </pages>
Reference-contexts: While calculating memory bandwidth is trivial for access to a single vector, it becomes intractable for access to multiple vectors. The difficulty lies in characterizing the conicts which occur as the access streams contend for memory modules. Oed and Lange <ref> [OeLa85] </ref> derive analytic results, based on number of modules and strides of access, for determining when two access streams can proceed conict-free. In the case of conict-free access, calculation of bandwidth is trivial and represents an upper bound on performance; for most other cases bandwidth calculation is intractable.
Reference: [PeVa87] <author> Peelen-T, Van de Goor-A, </author> <title> Using the Page Mode of Dynamic RAMs to Obtain a Pseudo Cache, </title> <journal> Microprocessors and Microsystems, </journal> <volume> 11, 9, </volume> <year> 1987, </year> <pages> pp. 469-473. </pages>
Reference-contexts: Given a single module constructed from page-mode devices, as described in section 1.4.2, this is not the case. Surprisingly, few analytic results have previously been derived to capture the behavior of any memory 14 architecture constructed from page-mode components. Peelen and Van de Goor <ref> [PeVa87] </ref> analyze the performance of page-mode DRAMs in a single module architecture by viewing page-mode as a cache with a single long cache line; a performance predictor is derived in the standard manner as a function of the miss ratio.
Reference: [Quin91] <author> Quinnell-R, </author> <title> High-speed DRAMs, </title> <type> EDN, </type> <month> May 23, </month> <year> 1991, </year> <pages> pp. 106-116. </pages>
Reference-contexts: Thus, the effective bandwidth is sensitive to the sequence of requests. Nearly all DRAMs currently manufactured implement a form of page-mode operation <ref> [Quin91] </ref>. Address Source Data Sink Memory System 5 Note that a DRAM page should not be confused with a virtual memory page; this is an unfortunate overloading of terms. A DRAM page is a physical feature of the memory device.
Reference: [RaSY89] <author> Rau-B, Schlansker-M, Yen-D, </author> <title> The Cydra 5 Stride-Insensitive Memory System, </title> <booktitle> Proc. 1989 Intl. Conf. Parallel Processing, </booktitle> <year> 1989, </year> <pages> pp. 242-246. 147 </pages>
Reference-contexts: Rau [Rau91] analyzes a scheme that assigns storage locations to modules in a pseudo-random fashion, rendering memory performance nearly stride insensitive; such a memory system has been incorporated into Cydromes Cydra 5 Departmental Supercomputer <ref> [RaSY89] </ref>. As with skewed and dynamic schemes, pseudo-random storage schemes benefit from memory module queues. The above studies focus on increasing parallelism for accesses to a single vector beyond that achieved by sequentially interleaved storage.
Reference: [Rau91] <author> Rau-B, </author> <title> Pseudo-Randomly Interleaved Memory, </title> <booktitle> Proc. 18th Intl. Symp. Comput. Architecture, </booktitle> <year> 1991, </year> <pages> pp. 74-83. </pages>
Reference-contexts: Rau <ref> [Rau91] </ref> analyzes a scheme that assigns storage locations to modules in a pseudo-random fashion, rendering memory performance nearly stride insensitive; such a memory system has been incorporated into Cydromes Cydra 5 Departmental Supercomputer [RaSY89]. As with skewed and dynamic schemes, pseudo-random storage schemes benefit from memory module queues.
Reference: [Ravi72] <author> Ravi-C, </author> <title> On the Bandwidth and Interference in Interleaved Memory Systems, </title> <journal> IEEE Trans. Comput., </journal> <volume> 21, 8, </volume> <year> 1972, </year> <pages> pp. 899-901. </pages>
Reference-contexts: Later work [CoBS71] extends this model to include buffering of access conicts. Ravi <ref> [Ravi72] </ref> develops an analytic model for the performance of a multiprocessor system accessing a sequentially interleaved memory, assuming a uniform random distribution of accesses.
Reference: [RaWa81] <author> Ramamoorthy-C, Wah-B, </author> <title> An Optimal Algorithm for Scheduling Requests on Interleaved Memories for a Pipelined Processor, </title> <journal> IEEE Trans. Comput., </journal> <volume> 30, 10, </volume> <year> 1981, </year> <pages> pp. 787-799. </pages>
Reference-contexts: Access scheduling can then be applied to reduce interlock delay while maintaining the specified load/store instruction order. Access ordering as a compilation technique is an original concept; few references to similar ideas are found in the literature. Ramamoorthy and Wah <ref> [RaWa81] </ref> present an optimal algorithm for initiating queued requests on a sequentially interleaved memory to maximize module concurrency; however, this is an inherently dynamic technique that assumes independent random requests and knowledge of the modules to which requests are to be mapped.
Reference: [ShTu72] <author> Shedler-G, Tung-C, </author> <title> Locality in Page Reference Strings, </title> <journal> SIAM J. on Computing, </journal> <volume> 1, 3, </volume> <year> 1972, </year> <pages> pp. 218-241. </pages>
Reference-contexts: While a number of stochastic models of program locality have been derived <ref> [AvCK87, ShTu72, SpDe72] </ref>, they are mainly applicable to the performance analysis of cache memories and virtual memory systems. For the purpose of deriving analytic results, most memory architectures are modeled under the assumption of a uniform random pattern of access, as discussed below.
Reference: [Slat92] <author> Slater-M, </author> <title> Rambus Unveils Revolutionary Memory Interface, </title> <type> Microprocessor Report, </type> <month> March 4, </month> <year> 1992, </year> <pages> pp. 15-21. </pages>
Reference-contexts: However, the more complex stream interactions that result should be incorporated formally into both ordering algorithms and performance models. 138 Page-mode components are modeled as if implemented with a single on-chip cache line, reecting current DRAM technology. Future high-speed DRAMs will incorporate multiple pages, among other exploitable features. Rambus <ref> [Slat92] </ref> represents such a technology. As memory components evolve, access ordering algorithms and performance models must be developed that reect these changes. A number of implementation issues remain to be solved.
Reference: [SpDe72] <author> Spirn-J, Denning-P, </author> <title> Experiments with Program Locality, </title> <booktitle> AFIPS Conference Proc., Fall Joint Comput. Conf., </booktitle> <volume> 41, </volume> <year> 1972, </year> <pages> pp. 611-621. </pages>
Reference-contexts: While a number of stochastic models of program locality have been derived <ref> [AvCK87, ShTu72, SpDe72] </ref>, they are mainly applicable to the performance analysis of cache memories and virtual memory systems. For the purpose of deriving analytic results, most memory architectures are modeled under the assumption of a uniform random pattern of access, as discussed below.
Reference: [Stev92] <author> Stevens-R, </author> <title> Computational Science Experiences on the Intel Touchstone DELTA Supercomputer, </title> <booktitle> Compcon Spring 92 Digest of Papers, </booktitle> <year> 1992, </year> <pages> pp. 295-299. </pages>
Reference-contexts: Simulation results demonstrate that for a given computation, access ordering can significantly increase effective memory bandwidth over that achieved by the natural reference sequence. In a study of the Intel Touchstone Delta distributed memory parallel computer, Stevens <ref> [Stev92] </ref> notes that for many scientific codes per node performance is still the number one problem in obtaining good overall applications performance. The majority of [codes surveyed] are not communications bound.
Reference: [WeSm90] <author> Weiss-S, Smith-J, </author> <title> A Study of Scalar Compilation Techniques for Pipelined Supercomputers, </title> <journal> ACM Trans. Math. Soft., </journal> <volume> 16, 3, </volume> <year> 1990, </year> <pages> pp. 223-245. </pages>
Reference-contexts: Lam [Lam88] presents a technique referred to as software pipelining that structures code such that a given loop iteration loads the data for a later iteration, stores results from a previous iteration, and performs computation for the current iteration. Weiss and Smith <ref> [WeSm90] </ref> present a comprehensive study in which they classify and evaluate software pipelining techniques implemented in conjunction with loop unrolling.
Reference: [Wolf89] <author> Wolfe-M, </author> <title> Optimizing Supercompilers for Supercomputers, </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1989. </year>
Reference-contexts: In general purpose scalar computing, the addition of cache memory is often a sufficient solution to the memory latency and bandwidth problems given the spatial and temporal locality of reference exhibited by most codes. For scientific computations, vectors are normally too large to cache. Iteration space tiling <ref> [CaKe89, Wolf89] </ref> can partition problems 1. Common superscalar and VLIW architectures incorporate concurrent pipelined functional units. 2 into cache-size blocks, however tiling often creates cache conicts [LaRW91] and the technique is difficult to automate. <p> Benitez and Davidson [BeDa91] describe a technique for explicitly detecting streaming opportunities, including those in recurrence relations. Furthermore, since stream-oriented computations reference vector operands, well known vectorization techniques are applicable, such as those described by Wolfe <ref> [Wolf89] </ref>. 2.2 Access Scheduling Techniques Access ordering is a compilation technique for maximizing effective memory bandwidth. Previous work has focused on reducing load/store interlock delay by overlapping computation with memory latency. Such techniques are referred to here collectively as access scheduling. <p> A dependence relation between two accesses from the same instance of a loop iteration is said to be loop-independent, while a dependence between accesses from different instances is said to be loop-carried. A detailed treatment of dependence analysis can be found in <ref> [Wolf89] </ref>. 3.5.1 Output and Input Dependence Output and input dependence can not exist as a result of the stream interaction restriction; two streams of the same mode have a non-intersecting address space. <p> However many codes generate multiple references to a subset of vector operands and hence can benefit from caching, particularly when implemented using strip-mining and tiling techniques <ref> [CaKe89, Wolf89] </ref>. Thus access ordering and caching should be used together to complement one another, exploiting the full memory hierarchy to maximize memory bandwidth. <p> 7.4.3 Access Ordering and Vectorizable Computations A vectorizable loop is one with no multi-statement dependence cycles and only self-dependence cycles that are ignorable or represent known reduction or recurrence operations for which vector instructions exist; in testing if a loop is vectorizable, input dependence is ignored for non-volatile memory locations <ref> [Wolf89] </ref>.
References-found: 44

