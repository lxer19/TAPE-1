URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-96-1297/CS-TR-96-1297.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-96-1297/
Root-URL: http://www.cs.wisc.edu
Email: sunc@cs.wisc.edu, condon@cs.wisc.edu.  
Title: Parallel Implementation of Bor-uvka's Minimum Spanning Tree Algorithm  
Author: Sun Chung Anne Condon 
Note: Supported in part by NSF grant number CCR-9257241 and by matching awards from Thinking Machines Corporation and Digital Systems Corporation. E-mail addresses of the authors are:  
Address: 1210 West Dayton Street Madison, WI 53706 USA  
Affiliation: Computer Sciences Department University of Wisconsin  
Abstract: We study parallel algorithms for the minimum spanning tree problem, based on the sequential algorithm of Bor-uvka. The target architectures for our algorithm are asynchronous, distributed-memory machines. Analysis of our parallel algorithm, on a simple model that is reminiscent of the LogP model, shows that in principle a speedup proportional to the number of processors can be achieved, but that communication costs can be significant. To reduce these costs, we develop a new randomized linear work pointer jumping scheme that performs better than previous linear work algorithms. We also consider empirically the effects of data imbalance on the running time. For the graphs used in our experiments, load balancing schemes result in little improvement in running times. Our implementations on sparse graphs with 64,000 vertices on Thinking Machine's CM-5 achieve a speedup factor of about 4 on 16 processors. On this environment, packaging of messages turns out to be the most effective way to reduce communication costs. 
Abstract-found: 1
Intro-found: 1
Reference: [AISS95] <author> A. Alexandrov, M. F. Ionescu, K. E. Schauser, and C. Scheiman. LogGP: </author> <title> Incorporating long messages into the LogP model. </title> <booktitle> Proc. 7th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <year> 1995. </year>
Reference-contexts: Since we are not concerned with overlapping computation with communication, we do not find it necessary to differentiate latency and overhead. The LogGP model of Alexandrov et al. <ref> [AISS95] </ref> is an extension of the LogP model that uses another parameter, G (gap per byte), to account for long messages.
Reference: [AM91] <author> R. J. Anderson and G. L. Miller. </author> <title> Deterministic parallel list ranking. </title> <journal> Algorithmica, </journal> <volume> 6 </volume> <pages> 859-868, </pages> <year> 1991. </year>
Reference-contexts: For larger p, however, the advantage of packaging may be lost. 4.2 A New Randomized Pointer Jumping Algorithm Both deterministic and randomized list ranking algorithms that require only linear work are well known <ref> [Vis84, CV88, AM91] </ref>. However, they are not well suited to our application for the following reason: since they are list ranking algorithms, they require that the input data are linear lists, whereas we apply pointer jumping to rooted trees. By "linearizing" the trees, one could apply these techniques.
Reference: [AS87] <author> B. Awerbuch and Y. Shiloach. </author> <title> New connectivity and MSF algorithms for shu*e-exchange network and PRAM. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(10):1258-1263, </volume> <month> October </month> <year> 1987. </year>
Reference-contexts: However, these structures are 3 difficult to implement in a distributed memory parallel environment. (See Knuth [Knu93] and Moret and Shapiro [MS94] for empirical assessments of sequential minimum spanning tree algorithms.) Parallel variants of the classical algorithms have been designed for different machine models, primarily the PRAM model <ref> [HCS79, SJ81, CLC82, CV88, AS87, JM92, CKT94] </ref>. Because the PRAM model assumes a synchronous shared memory in which no communication cost is incurred, these algorithms are not well suited to our model. <p> Because the PRAM model assumes a synchronous shared memory in which no communication cost is incurred, these algorithms are not well suited to our model. Other parallel algorithms have been developed for fixed interconnection networks such as meshes, butterflies, hypercubes, and shu*e-exchange network <ref> [Ben80, DY81, KGGK94, AS87, DV87, Lei83, NMB83] </ref>. Although they are practical and efficient on a particular network, these algorithms owe their efficiency to the regular communication patterns made possible by the network or by the assumption that the graphs are dense.
Reference: [BHK89] <author> R. S. Barr, R. V. Helgaon and J. L. Kennington. </author> <title> Minimal spanning trees: an empirical investigation of parallel algorithms. </title> <journal> Parallel Computing, </journal> <volume> 12(1) </volume> <pages> 45-52, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: Of the three, Kruskal's algorithm is reported to be the best choice for many types of graphs, although Bor-uvka's algorithm is best for very sparse graphs <ref> [BHK89] </ref>. Also, Bor-uvka's algorithm is considered to be best suited for parallel computation [Tar83]. Other sequential variants of these algorithms use more elaborate data structures to improve the asymptotic running time. <p> Although they are practical and efficient on a particular network, these algorithms owe their efficiency to the regular communication patterns made possible by the network or by the assumption that the graphs are dense. Barr et al. <ref> [BHK89] </ref> give an empirical investigation of both sequential and parallel minimum spanning tree algorithms.
Reference: [Ben80] <author> J. L. Bentley. </author> <title> A parallel algorithm for constructing minimum spanning trees. </title> <journal> Journal of Algorithms, </journal> <volume> 1 </volume> <pages> 51-59, </pages> <year> 1980. </year>
Reference-contexts: Because the PRAM model assumes a synchronous shared memory in which no communication cost is incurred, these algorithms are not well suited to our model. Other parallel algorithms have been developed for fixed interconnection networks such as meshes, butterflies, hypercubes, and shu*e-exchange network <ref> [Ben80, DY81, KGGK94, AS87, DV87, Lei83, NMB83] </ref>. Although they are practical and efficient on a particular network, these algorithms owe their efficiency to the regular communication patterns made possible by the network or by the assumption that the graphs are dense.
Reference: [Bor26] <author> O. Bor-uvka. O jistem problemu minimalnm. Prace Mor. Prrodoved. </author> <title> Spol. </title> <journal> v Brne (Acta Societ. Scient. Natur. Moravicae), </journal> <volume> 3 </volume> <pages> 37-58, </pages> <year> 1926. </year>
Reference-contexts: We are interested in the case that the number of processors p is much less than the size of the graph, and the graph is distributed among the processors. Our algorithms are based on the classical sequential algorithm of Bor-uvka <ref> [Bor26] </ref>, which is considered to be "well suited for parallel computation" [Tar83]. Using a simple performance model, together with measurements of implementations on Thinking Machine's CM-5, we analyze and compare alternative implementations. Briefly, our conclusions are as follows. <p> On higher density graphs, the speedup factor improves. Communication costs account for most of the slowdown. 2 Background and Related Work 2.1 Minimum Spanning Tree Algorithms There are three classical O (m log n) minimum spanning tree algorithms: Bor-uvka's, Prim's, and Kruskal's <ref> [Bor26, Pri57, Kru56] </ref>. Of the three, Kruskal's algorithm is reported to be the best choice for many types of graphs, although Bor-uvka's algorithm is best for very sparse graphs [BHK89]. Also, Bor-uvka's algorithm is considered to be best suited for parallel computation [Tar83].
Reference: [CLC82] <author> F. Y. Chin, J. Lam, and I. Chen. </author> <title> Efficient parallel algorithms for some graph problems. </title> <journal> Communications of the ACM, </journal> <volume> 25(9) </volume> <pages> 659-665, </pages> <month> September </month> <year> 1982. </year>
Reference-contexts: However, these structures are 3 difficult to implement in a distributed memory parallel environment. (See Knuth [Knu93] and Moret and Shapiro [MS94] for empirical assessments of sequential minimum spanning tree algorithms.) Parallel variants of the classical algorithms have been designed for different machine models, primarily the PRAM model <ref> [HCS79, SJ81, CLC82, CV88, AS87, JM92, CKT94] </ref>. Because the PRAM model assumes a synchronous shared memory in which no communication cost is incurred, these algorithms are not well suited to our model.
Reference: [CKT94] <author> R. Cole, P. N. Klein, and R. E. Tarjan. </author> <title> A linear-work parallel algorithm for finding minimum spanning trees. </title> <booktitle> Proc. 6th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <year> 1994. </year>
Reference-contexts: However, these structures are 3 difficult to implement in a distributed memory parallel environment. (See Knuth [Knu93] and Moret and Shapiro [MS94] for empirical assessments of sequential minimum spanning tree algorithms.) Parallel variants of the classical algorithms have been designed for different machine models, primarily the PRAM model <ref> [HCS79, SJ81, CLC82, CV88, AS87, JM92, CKT94] </ref>. Because the PRAM model assumes a synchronous shared memory in which no communication cost is incurred, these algorithms are not well suited to our model.
Reference: [CV86] <author> R. Cole and U. Vishkin. </author> <title> Deterministic coin tossing with applications to optimal parallel list ranking. </title> <journal> Information and Control, </journal> <volume> 70(1) </volume> <pages> 32-53, </pages> <year> 1986. </year>
Reference-contexts: To address the first problem, we develop a new randomized linear work pointer jumping scheme that performs better than previous linear work algorithms on lists. Our algorithm is similar to the list ranking algorithms of Vishkin [Vis84] and Cole and Vishkin <ref> [CV86] </ref>, and shows how an idea developed for the PRAM model can be adapted to work effectively in practice. To address the second problem, since a precise analysis of the degree of imbalance of work seems difficult, we consider empirically the effects of data imbalance. <p> A completely asynchronous version of this algorithm could also be implemented, but we have not done this. Our supervertex algorithm is similar to one of the list ranking algorithms of Vishkin [Vis84] which uses the "random mate algorithm," and to that of Cole and Vishkin <ref> [CV86] </ref> which uses the "2-ruling set algorithm." Their algorithms (defined for lists only) can also be thought of as selecting supervertices that proceed to another iteration of pointer jumping while the remaining vertices drop out.
Reference: [CV88] <author> R. Cole and U. Vishkin. </author> <title> Approximate parallel scheduling, Part I: The basic technique with applications to optimal parallel list ranking in logarithmic time. </title> <journal> SIAM J. Computing, </journal> <volume> 17(1) </volume> <pages> 128-142, </pages> <year> 1988. </year>
Reference-contexts: However, these structures are 3 difficult to implement in a distributed memory parallel environment. (See Knuth [Knu93] and Moret and Shapiro [MS94] for empirical assessments of sequential minimum spanning tree algorithms.) Parallel variants of the classical algorithms have been designed for different machine models, primarily the PRAM model <ref> [HCS79, SJ81, CLC82, CV88, AS87, JM92, CKT94] </ref>. Because the PRAM model assumes a synchronous shared memory in which no communication cost is incurred, these algorithms are not well suited to our model. <p> For larger p, however, the advantage of packaging may be lost. 4.2 A New Randomized Pointer Jumping Algorithm Both deterministic and randomized list ranking algorithms that require only linear work are well known <ref> [Vis84, CV88, AM91] </ref>. However, they are not well suited to our application for the following reason: since they are list ranking algorithms, they require that the input data are linear lists, whereas we apply pointer jumping to rooted trees. By "linearizing" the trees, one could apply these techniques.
Reference: [CKP+93] <author> D. Culler, R. Karp, D. Patterson, A. Sahay, K. E. Schauser, E. Santos, R. Subramo-nian, T. von Eicken. </author> <title> LogP: Towards a realistic model of parallel computation. </title> <booktitle> 4th ACM SIGPLAN Symposium on Principles and Practices of Parallel Programming, </booktitle> <year> 1993. </year>
Reference-contexts: 1 Introduction A dominant emerging parallel architecture consists of a collection of fast processors, connected by a robust communication network <ref> [CKP+93, Sni93] </ref>. Properties of this type of architecture include a distributed memory, partitioned among processors whose interpro-cessor communication cost is rather high compared with the computation cost. Examples already available include the TMC CM-5 [Joh93], Meiko CS-2 [Row93], Cray T3D [OW93] and the IBM SP-1 [Sni93]. <p> The connected components problem is a special case of the minimum spanning tree problem. A similarity between our work and the work of Krishnamurthy et al. is the use of pointer jumping as a subroutine in the algorithm. Culler et al. <ref> [CKP+93] </ref> introduced the LogP model to be used in analyzing the performance of algorithms on the same architectures that we study. However, they focused on overlapping communication with computation, and the problems they studied broadcast, summation and FFT have the property that the communication pattern is simple and regular.
Reference: [DY81] <author> N. Deo and Y. B. Yoo. </author> <title> Parallel algorithms for the minimu spanning tree problem. </title> <booktitle> Proc. 1981 International Conference on Parallel Processing, </booktitle> <pages> 188-189, </pages> <year> 1981. </year>
Reference-contexts: Because the PRAM model assumes a synchronous shared memory in which no communication cost is incurred, these algorithms are not well suited to our model. Other parallel algorithms have been developed for fixed interconnection networks such as meshes, butterflies, hypercubes, and shu*e-exchange network <ref> [Ben80, DY81, KGGK94, AS87, DV87, Lei83, NMB83] </ref>. Although they are practical and efficient on a particular network, these algorithms owe their efficiency to the regular communication patterns made possible by the network or by the assumption that the graphs are dense.
Reference: [DV87] <author> K. A. Doshi and P. J. Varman. </author> <title> Optimal graph algorithms on a fixed-size linear array. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(4):460-470, </volume> <month> April </month> <year> 1987. </year>
Reference-contexts: Because the PRAM model assumes a synchronous shared memory in which no communication cost is incurred, these algorithms are not well suited to our model. Other parallel algorithms have been developed for fixed interconnection networks such as meshes, butterflies, hypercubes, and shu*e-exchange network <ref> [Ben80, DY81, KGGK94, AS87, DV87, Lei83, NMB83] </ref>. Although they are practical and efficient on a particular network, these algorithms owe their efficiency to the regular communication patterns made possible by the network or by the assumption that the graphs are dense.
Reference: [GH85] <author> R. L. Graham and P. Hell. </author> <title> On the history of the minimum spanning tree problem. </title> <journal> Annals of the History of Computing, </journal> <volume> 7(1) </volume> <pages> 43-57, </pages> <year> 1985. </year>
Reference-contexts: The output spanning tree is the union of the set of edges selected in step 1, taken over all iterations (with the original vertex names, as they were at the start of the algorithm). Using standard techniques (see <ref> [GH85] </ref>) the algorithm can be implemented so that an iteration in which the graph has n vertices and m edges takes O (n + m) sequential time.
Reference: [GPS87] <author> A. V. Goldberg, S. A. Plotkin and G. E. Shannon. </author> <title> Parallel symmetry-breaking in sparse graphs. </title> <booktitle> Proc. 19th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> 315-324, </pages> <year> 1987. </year>
Reference-contexts: By "linearizing" the trees, one could apply these techniques. But it is preferable not to do this, since the path from a vertex to the root of its component may be much shorter in the tree than in the linear list. (Goldberg et al. <ref> [GPS87] </ref> present a symmetry-breaking technique which works on rooted trees, but it is impractical for our application.) We developed a new pointer jumping scheme, which we call the supervertex algorithm. This randomized scheme can be applied to trees as well as lists and requires only expected linear work.
Reference: [HCS79] <author> D. S. Hirschberg, A. K. Chandra, and D. V. Sarwate. </author> <title> Computing connected components on parallel computers. </title> <journal> Communications of the ACM, </journal> <volume> 22(8) </volume> <pages> 461-464, </pages> <month> August </month> <year> 1979. </year>
Reference-contexts: However, these structures are 3 difficult to implement in a distributed memory parallel environment. (See Knuth [Knu93] and Moret and Shapiro [MS94] for empirical assessments of sequential minimum spanning tree algorithms.) Parallel variants of the classical algorithms have been designed for different machine models, primarily the PRAM model <ref> [HCS79, SJ81, CLC82, CV88, AS87, JM92, CKT94] </ref>. Because the PRAM model assumes a synchronous shared memory in which no communication cost is incurred, these algorithms are not well suited to our model.
Reference: [JM92] <author> D. B. Johnson and P. Metaxas. </author> <title> A parallel algorithm for computing minimum spanning trees. </title> <booktitle> Proc. 4th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> 363-372, </pages> <year> 1992. </year>
Reference-contexts: However, these structures are 3 difficult to implement in a distributed memory parallel environment. (See Knuth [Knu93] and Moret and Shapiro [MS94] for empirical assessments of sequential minimum spanning tree algorithms.) Parallel variants of the classical algorithms have been designed for different machine models, primarily the PRAM model <ref> [HCS79, SJ81, CLC82, CV88, AS87, JM92, CKT94] </ref>. Because the PRAM model assumes a synchronous shared memory in which no communication cost is incurred, these algorithms are not well suited to our model.
Reference: [Joh93] <author> L. Johnsson. </author> <title> The Connection Machine systems CM-5. </title> <booktitle> Proc. 5th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> 365-366, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Properties of this type of architecture include a distributed memory, partitioned among processors whose interpro-cessor communication cost is rather high compared with the computation cost. Examples already available include the TMC CM-5 <ref> [Joh93] </ref>, Meiko CS-2 [Row93], Cray T3D [OW93] and the IBM SP-1 [Sni93]. In this paper, we describe our experience with design and implementation of parallel algorithms for the minimum spanning tree problem.
Reference: [Knu93] <author> D. E. Knuth. </author> <title> The Stanford GraphBase: A Platform for Combinatorical Computing, </title> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1993. </year>
Reference-contexts: Also, Bor-uvka's algorithm is considered to be best suited for parallel computation [Tar83]. Other sequential variants of these algorithms use more elaborate data structures to improve the asymptotic running time. However, these structures are 3 difficult to implement in a distributed memory parallel environment. (See Knuth <ref> [Knu93] </ref> and Moret and Shapiro [MS94] for empirical assessments of sequential minimum spanning tree algorithms.) Parallel variants of the classical algorithms have been designed for different machine models, primarily the PRAM model [HCS79, SJ81, CLC82, CV88, AS87, JM92, CKT94].
Reference: [KR88] <author> R. M. Karp and V. Ramachandran. </author> <title> A survey of parallel algorithms for shared-memory machines. </title> <type> Technical Report 408, </type> <institution> University of California, Berkeley, </institution> <year> 1988. </year>
Reference-contexts: Step 2 (find root): Each vertex finds the root of the tree to which it belongs using the well-known technique of pointer jumping (see <ref> [KR88] </ref>). Initially, the root of each component sets its pointer to point at itself. Each remaining vertex initially points at the other endpoint of the lightest edge incident on it.
Reference: [KLCY94] <author> A. Krishnamurthy, S. Lumetta, D. E. Culler and K. Yelick. </author> <title> Connected components on distributed memory machines. DIMACS Implementational Challenge, </title> <year> 1994. </year>
Reference-contexts: For Bor-uvka's algorithm, they report a speedup factor of about 6 with 10 processors, for grid graphs, on the Sequent Symmetry S81 multicomputer. 2.2 Related Work on Parallel Models and Graph Algorithms Narendran et al. [NDT93] and Krishnamurthy et al. <ref> [KLCY94] </ref> reported on implementations of min cost flow and connected components algorithms on the CM-5, respectively. As we do in our implementation, they both distribute the vertices evenly among the processors, and store the complete edge list of one vertex at the same processor.
Reference: [Kru56] <author> J. B. Kruskal. </author> <title> On the shortest spanning subtree of a graph and the traveling salesman problem. </title> <journal> Proc. Amer. Math. Soc., </journal> <volume> 7(1) </volume> <pages> 48-50, </pages> <year> 1956. </year>
Reference-contexts: On higher density graphs, the speedup factor improves. Communication costs account for most of the slowdown. 2 Background and Related Work 2.1 Minimum Spanning Tree Algorithms There are three classical O (m log n) minimum spanning tree algorithms: Bor-uvka's, Prim's, and Kruskal's <ref> [Bor26, Pri57, Kru56] </ref>. Of the three, Kruskal's algorithm is reported to be the best choice for many types of graphs, although Bor-uvka's algorithm is best for very sparse graphs [BHK89]. Also, Bor-uvka's algorithm is considered to be best suited for parallel computation [Tar83].
Reference: [KGGK94] <author> V. Kumar, A. Grama, A. Gupta, and G. Karypis. </author> <title> Introduction to Parallel Computing, </title> <address> Benjamin/Cummings, Redwood City, CA, </address> <year> 1994. </year>
Reference-contexts: Because the PRAM model assumes a synchronous shared memory in which no communication cost is incurred, these algorithms are not well suited to our model. Other parallel algorithms have been developed for fixed interconnection networks such as meshes, butterflies, hypercubes, and shu*e-exchange network <ref> [Ben80, DY81, KGGK94, AS87, DV87, Lei83, NMB83] </ref>. Although they are practical and efficient on a particular network, these algorithms owe their efficiency to the regular communication patterns made possible by the network or by the assumption that the graphs are dense.
Reference: [Lei83] <author> F. T. Leighton. </author> <title> Parallel computations using meshes of trees. </title> <booktitle> Proc. 1983 International Workshop of Graph Theoretic Computer Science, </booktitle> <year> 1983. </year>
Reference-contexts: Because the PRAM model assumes a synchronous shared memory in which no communication cost is incurred, these algorithms are not well suited to our model. Other parallel algorithms have been developed for fixed interconnection networks such as meshes, butterflies, hypercubes, and shu*e-exchange network <ref> [Ben80, DY81, KGGK94, AS87, DV87, Lei83, NMB83] </ref>. Although they are practical and efficient on a particular network, these algorithms owe their efficiency to the regular communication patterns made possible by the network or by the assumption that the graphs are dense.
Reference: [MS94] <author> B. M. E. Moret and H. D. Shapiro. </author> <title> An empirical assessment of algorithms for constructing a minimum spanning tree. </title> <booktitle> DIMACS Series in Discrete Mathematics and Theoretical Computer Science, </booktitle> <volume> 15 </volume> <pages> 99-117, </pages> <publisher> American Mathematical Society, </publisher> <address> Providence, RI, </address> <year> 1994. </year>
Reference-contexts: Other sequential variants of these algorithms use more elaborate data structures to improve the asymptotic running time. However, these structures are 3 difficult to implement in a distributed memory parallel environment. (See Knuth [Knu93] and Moret and Shapiro <ref> [MS94] </ref> for empirical assessments of sequential minimum spanning tree algorithms.) Parallel variants of the classical algorithms have been designed for different machine models, primarily the PRAM model [HCS79, SJ81, CLC82, CV88, AS87, JM92, CKT94]. <p> The connections are determined as follows: n points corresponding to the vertices are chosen randomly and uniformly on the unit square in the Cartesian plane. Each vertex is then connected to its k nearest neighbors. These graphs were used by Moret and Shapiro <ref> [MS94] </ref> in their empirical study of sequential minimum spanning tree algorithms. The random graphs and the geometric graphs are not necessarily connected. We tested our algorithm on graphs with 32,000 and 64,000 vertices, with average degree ranging from 1.6 to 12.8.
Reference: [NDT93] <author> B. Narendran, R. De Leone and P. Tiwari. </author> <title> An implementation of the *-relaxation algorithm on CM-5. </title> <booktitle> Proc. 5th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> 183-192, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: For Bor-uvka's algorithm, they report a speedup factor of about 6 with 10 processors, for grid graphs, on the Sequent Symmetry S81 multicomputer. 2.2 Related Work on Parallel Models and Graph Algorithms Narendran et al. <ref> [NDT93] </ref> and Krishnamurthy et al. [KLCY94] reported on implementations of min cost flow and connected components algorithms on the CM-5, respectively. As we do in our implementation, they both distribute the vertices evenly among the processors, and store the complete edge list of one vertex at the same processor.
Reference: [NMB83] <author> D. Nath, S. N. Maheshwari, and P. C. P. Bhatt. </author> <title> Efficient VLSI networks for parallel processing based on orthogonal trees. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-32:21-23, </volume> <month> June </month> <year> 1983. </year>
Reference-contexts: Because the PRAM model assumes a synchronous shared memory in which no communication cost is incurred, these algorithms are not well suited to our model. Other parallel algorithms have been developed for fixed interconnection networks such as meshes, butterflies, hypercubes, and shu*e-exchange network <ref> [Ben80, DY81, KGGK94, AS87, DV87, Lei83, NMB83] </ref>. Although they are practical and efficient on a particular network, these algorithms owe their efficiency to the regular communication patterns made possible by the network or by the assumption that the graphs are dense.
Reference: [OW93] <author> W. Oed and M. Walker. </author> <title> An overview of Cray Research computers including the Y-MP/C90 and the new MPP T3D. </title> <booktitle> Proc. 5th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> 271-272, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Properties of this type of architecture include a distributed memory, partitioned among processors whose interpro-cessor communication cost is rather high compared with the computation cost. Examples already available include the TMC CM-5 [Joh93], Meiko CS-2 [Row93], Cray T3D <ref> [OW93] </ref> and the IBM SP-1 [Sni93]. In this paper, we describe our experience with design and implementation of parallel algorithms for the minimum spanning tree problem.
Reference: [Pri57] <author> R. C. Prim. </author> <title> Shortest connection networks and some generalizations. </title> <journal> Bell System Technical Journal, </journal> <volume> 36 </volume> <pages> 1389-1401, </pages> <month> November </month> <year> 1957. </year>
Reference-contexts: On higher density graphs, the speedup factor improves. Communication costs account for most of the slowdown. 2 Background and Related Work 2.1 Minimum Spanning Tree Algorithms There are three classical O (m log n) minimum spanning tree algorithms: Bor-uvka's, Prim's, and Kruskal's <ref> [Bor26, Pri57, Kru56] </ref>. Of the three, Kruskal's algorithm is reported to be the best choice for many types of graphs, although Bor-uvka's algorithm is best for very sparse graphs [BHK89]. Also, Bor-uvka's algorithm is considered to be best suited for parallel computation [Tar83].
Reference: [Row93] <author> D. Roweth. </author> <title> The Meiko CS-2 system architecture. </title> <booktitle> Proc. 5th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <address> p. 213, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: Properties of this type of architecture include a distributed memory, partitioned among processors whose interpro-cessor communication cost is rather high compared with the computation cost. Examples already available include the TMC CM-5 [Joh93], Meiko CS-2 <ref> [Row93] </ref>, Cray T3D [OW93] and the IBM SP-1 [Sni93]. In this paper, we describe our experience with design and implementation of parallel algorithms for the minimum spanning tree problem.
Reference: [SBR73] <author> R. G. Saltman, G. R. Bolotsky, and Z. G. Ruthberg. </author> <title> Heuristic cost optimization of the federal Telpak network. </title> <type> Tech. Note 787, </type> <institution> National Bureau of Standards, </institution> <address> Washington, D.C., </address> <month> June </month> <year> 1973. </year>
Reference: [SJ81] <author> C. Savage and J. Jaja. </author> <title> Fast, efficient parallel algorithms for some graph problems. </title> <journal> SIAM Journal of Computing, </journal> <volume> 10(4) </volume> <pages> 682-691, </pages> <month> November </month> <year> 1981. </year>
Reference-contexts: However, these structures are 3 difficult to implement in a distributed memory parallel environment. (See Knuth [Knu93] and Moret and Shapiro [MS94] for empirical assessments of sequential minimum spanning tree algorithms.) Parallel variants of the classical algorithms have been designed for different machine models, primarily the PRAM model <ref> [HCS79, SJ81, CLC82, CV88, AS87, JM92, CKT94] </ref>. Because the PRAM model assumes a synchronous shared memory in which no communication cost is incurred, these algorithms are not well suited to our model.
Reference: [Sni93] <author> M. Snir. </author> <title> Issues and directions in scalable parallel computing. </title> <type> Research Report Number RC 18940 (82749), </type> <institution> IBM Research Division, T.J. Watson Research Center, </institution> <address> Yorktown Heights, NY, </address> <year> 1993. </year>
Reference-contexts: 1 Introduction A dominant emerging parallel architecture consists of a collection of fast processors, connected by a robust communication network <ref> [CKP+93, Sni93] </ref>. Properties of this type of architecture include a distributed memory, partitioned among processors whose interpro-cessor communication cost is rather high compared with the computation cost. Examples already available include the TMC CM-5 [Joh93], Meiko CS-2 [Row93], Cray T3D [OW93] and the IBM SP-1 [Sni93]. <p> Properties of this type of architecture include a distributed memory, partitioned among processors whose interpro-cessor communication cost is rather high compared with the computation cost. Examples already available include the TMC CM-5 [Joh93], Meiko CS-2 [Row93], Cray T3D [OW93] and the IBM SP-1 <ref> [Sni93] </ref>. In this paper, we describe our experience with design and implementation of parallel algorithms for the minimum spanning tree problem. We are interested in the case that the number of processors p is much less than the size of the graph, and the graph is distributed among the processors.
Reference: [Sol77] <author> M. Sollin. </author> <title> An algorithm attributed to Sollin. In Introduction to the Design and Analysis of Algorithms, </title> <editor> S. E. Goodman and S. T. Hedetniemi, eds., sec. 5.5, </editor> <publisher> McGraw-Hill, </publisher> <address> Cambridge, MA, </address> <year> 1977. </year> <month> 16 </month>
Reference-contexts: Given a connected, undirected graph G with n vertices and m weighted edges, find a spanning tree of minimum weight. Our minimum spanning tree algorithms can easily be adapted to solve the case where the graph is not connected. Bor-uvka's minimum spanning tree algorithm, also known as Sollin's algorithm <ref> [Sol77] </ref>, constructs a spanning tree in iterations composed of the following steps (organized here to correspond to the phases of our parallel implementation).
Reference: [Sub92] <author> R. Subramonian. </author> <title> The influence of Limited Bandwidth on Algorithm Design and Im--plementation. </title> <booktitle> Proc. Dartmouth Institute for Advanced Graduate Studies (DAGS) 1992 Conference on CD-ROM, </booktitle> <address> P.A. </address> <publisher> Gloor, </publisher> <editor> F. Makedon, and J.W. Matthews, eds., TELOS, </editor> <address> Santa Clara, CA, </address> <year> 1994. </year>
Reference-contexts: However, their method of choosing supervertices is more complicated, requiring communication between each vertex and its parent. (Subramonian <ref> [Sub92] </ref> presents an implementation technique of the "random mate algorithm" which, by broadcasting pairs of random numbers, avoids interprocessor communication that would otherwise be required.) The differences between their algorithms and ours nicely illustrate how the choice of parallel model influences parallel algorithm design.
Reference: [Tar83] <author> R. E. Tarjan. </author> <title> Data Structures and Network Algorithms, </title> <institution> Society for Industrial and Applied Mathematics, </institution> <address> Philadelphia, PA, </address> <year> 1983. </year>
Reference-contexts: Our algorithms are based on the classical sequential algorithm of Bor-uvka [Bor26], which is considered to be "well suited for parallel computation" <ref> [Tar83] </ref>. Using a simple performance model, together with measurements of implementations on Thinking Machine's CM-5, we analyze and compare alternative implementations. Briefly, our conclusions are as follows. <p> Of the three, Kruskal's algorithm is reported to be the best choice for many types of graphs, although Bor-uvka's algorithm is best for very sparse graphs [BHK89]. Also, Bor-uvka's algorithm is considered to be best suited for parallel computation <ref> [Tar83] </ref>. Other sequential variants of these algorithms use more elaborate data structures to improve the asymptotic running time.

References-found: 36

