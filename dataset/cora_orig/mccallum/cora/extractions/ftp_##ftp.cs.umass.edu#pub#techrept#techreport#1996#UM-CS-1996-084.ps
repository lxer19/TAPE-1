URL: ftp://ftp.cs.umass.edu/pub/techrept/techreport/1996/UM-CS-1996-084.ps
Refering-URL: http://laser.cs.umass.edu/abstracts/96-084.html
Root-URL: 
Email: achamill@cs.usafa.af.mil clarke@cs.umass.edu avrunin@math.umass.edu  
Title: An Empirical Comparison of Static Concurrency Analysis Techniques  
Author: A.T. Chamillard Lori A. Clarke George S. Avrunin 
Address: USAFA, CO 80840 USA Amherst, MA 01003 USA Amherst, MA 01003 USA  
Affiliation: Computer Science Department Computer Science Department Department of Mathematics and Statistics HQ USAFA/DFCS University of Massachusetts University of Massachusetts  
Abstract: This paper reports the results of an empirical comparison of several static analysis tools for evaluating properties of concurrent software and also reports the results of our attempts to build predictive models for each of the tools based on program and property characteristics. Although this area seems well suited to empirical investigation, we encountered a number of significant issues that make designing a sound and unbiased study surprisingly difficult. These experiment design issues are also discussed in this paper. 
Abstract-found: 1
Intro-found: 1
Reference: [Agr84] <author> Alan Agresti. </author> <title> Analysis of Ordinal Categorical Data. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1984. </year>
Reference-contexts: We used linear regression to build our predictive models of analysis time based on the set of metrics selected for inclusion using the preprocessing discussed above. While linear regression is a widely used for predicting continuous response variables, it is not appropriate for predicting dichotomous response variables <ref> [Agr84] </ref>. Because linear regression assumes that the response variable has a continuous range of values, it can not be applied when the response variable can only have two values (true and false, for instance).
Reference: [ACD+94] <author> G.S. Avrunin, J.C. Corbett, L.K. Dillon, and J.C. Wileden. </author> <title> Automatic derivation of time bounds in uniprocessor concurrent systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 20(9) </volume> <pages> 708-719, </pages> <year> 1994. </year> <month> 27 </month>
Reference-contexts: Sizes: 4, 8, 12, 16, 20, and 24 tasks. Divide-and-conquer program - provides a set of solvers that can cooperatively solve a problem <ref> [ACD+94] </ref>. Sizes: 11, 21, 31, 41, 51, and 61 tasks. Standard Dining Philosophers program - includes a ring of philosophers with a single fork between a philosopher and its neighbor to the left. Sizes: 4, 8, 12, 16, 20, and 24 tasks.
Reference: [Bla70] <author> Hubert M. Blalock, Jr. </author> <title> Correlated Independent Variables: The Problem of Multicollinearity. </title> <editor> In Edward R. Tufte, editor, </editor> <title> The Quantitative Analysis of Social Problems. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Massachusetts, </address> <year> 1970. </year>
Reference-contexts: Logistic regression is the proper technique for these variables, so we used logistic regression to build our predictive models for failure and presence of spurious results. It has been noted in the literature that high linear correlations between several (or many) of our predictor variables can cause problems <ref> [Bla70] </ref> in both of the regression techniques that we use.
Reference: [BCM+90] <author> J.R. Burch, E.M. Clarke, K.L. McMillan, D.L. Dill, and L.J. Hwang. </author> <title> Symbolic model checking: 10 20 states and beyond. </title> <booktitle> In Proceedings of the Fifth Annual IEEE Symposium on Logic in Computer Science, </booktitle> <pages> pages 428-439, </pages> <year> 1990. </year>
Reference-contexts: We note that another partial order addition [Pel94] to SPIN has been implemented, but since it does not currently support the use of rendezvous, we did not consider it for this experiment. SMV The Symbolic Model Verifier (SMV) [McM93] performs symbolic model checking <ref> [BCM+90] </ref>, in which the program state space is represented symbolically rather than explicitly. Although SMV was originally designed as a hardware verification tool, it can also be used for analysis of concurrent software. The program is described in the form of a transition relation for the program states.
Reference: [Cha96] <author> A.T. Chamillard. </author> <title> An empirical comparison of static concurrency analysis techniques. </title> <type> Ph.D. Dissertation, </type> <institution> University of Massachusetts, Amherst, </institution> <year> 1996. </year>
Reference-contexts: Therefore, any experiment using these tools should incorporate a means of checking for any identified areas of potential bias. 3 COMPARISON RESULTS In this section, we describe the result of our empirical comparison, with a more complete discussion provided in <ref> [Cha96] </ref>. We found that there was no single best tool for the programs and properties in our experiment, but our comparison results require some discussion.
Reference: [Coh95] <author> Paul R. Cohen. </author> <title> Empirical Methods for Artificial Intelligence. </title> <publisher> The MIT Press, </publisher> <address> Massachusetts, </address> <year> 1995. </year>
Reference-contexts: Standard statistical tests are not applicable, since our concern is about distributions of the correlation coefficients rather than distributions of the mean. However, we can use randomization tests, in conjunction with correlation, to test the hypothesis that two samples are linearly dependent <ref> [Coh95] </ref>. To conduct the randomization test, we randomly paired up values of the first and second variables and calculated the correlation coefficient.
Reference: [CA95] <author> James C. Corbett and George S. Avrunin. </author> <title> Using integer programming to verify general safety and liveness properties. </title> <booktitle> Formal Methods in System Design, </booktitle> <volume> 6(1) </volume> <pages> 97-123, </pages> <year> 1995. </year>
Reference-contexts: A variety of static concurrency analysis techniques have been proposed, including reachability analysis (e.g., [Hol91, GW91, Pel94]), symbolic model checking (e.g., [McM93]), inequality necessary condition analysis (e.g., <ref> [CA95] </ref>), and data flow analysis (e.g., [DC94]). We are interested in understanding the relative strengths and weaknesses of these techniques in terms of their ability to prove properties of concurrent programs, such as freedom from deadlock and the mutually exclusive use of resources. <p> The property of interest is specified in the temporal logic Computation Tree Logic (CTL). If the property is ever false, SMV reports the violation and terminates. 3 INCA The INCA tool implements the Inequality Necessary Condition Analysis technique <ref> [CA95] </ref>, in which necessary conditions for an execution of the system to violate a property are formulated as a system of inequalities. The program is specified in an Ada-like language or in the SExpression Design Language (SEDL). Properties are given as w star-less expressions, which specify sequences of event symbols.
Reference: [Cor94] <author> James C. Corbett. </author> <title> An empirical evaluation of three methods for deadlock analysis of Ada tasking programs. </title> <booktitle> In Proceedings of the 1994 International Symposium on Software Testing and Analysis (ISSTA), </booktitle> <pages> pages 228-239, </pages> <address> Seattle WA, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: Analysis accuracy is therefore also an important measurement to consider. 1 Previously, Corbett <ref> [Cor94, Cor96] </ref> reported on empirical studies comparing the performance of several tools in detecting deadlock in Ada tasking programs and we have built upon that work. <p> Sizes: 11, 21, 31, 41, 51, and 61 tasks. Memory management program - based on a set of conservative release and allocate memory management algorithms [For88]. Sizes: 5, 6, 7, 8, 9, and 10 tasks. Ring program - based on a simulation of token ring access to a resource <ref> [Cor94] </ref>. Sizes: 4, 8, 12, 16, 20, and 24 tasks. <p> One approach would be to add accuracy to the program representations incrementally. With this approach, we would start without modeling variables, and would incrementally model additional variables until the analysis results meets the accuracy requirements; this is the approach we took in our experiment. Another approach <ref> [Cor94, Cor96] </ref> would model all variables initially, or at least all variables that directly impact the events in the property. If we discovered that this variable modeling led to intractable analyses, we would incrementally remove variable modeling until the analysis was tractable.
Reference: [Cor96] <author> James C. Corbett. </author> <title> Evaluating deadlock detection methods for concurrent software. </title> <journal> Transactions on Software Engineering, </journal> <volume> 22(3) </volume> <pages> 161-180, </pages> <year> 1996. </year>
Reference-contexts: Analysis accuracy is therefore also an important measurement to consider. 1 Previously, Corbett <ref> [Cor94, Cor96] </ref> reported on empirical studies comparing the performance of several tools in detecting deadlock in Ada tasking programs and we have built upon that work. <p> Some of the programs had already been coded in Ada by members of the Arcadia project [Kad92] to demonstrate testing and analysis tools, while for others we acquired the INCA inputs used by Corbett <ref> [Cor96] </ref> and converted them to Ada programs. Since the size of the programs included in the experiment can be increased by including more tasks into the system, it is also interesting to consider how the performance of the tools changes as the problem size is increased. <p> For example, we can run all the tools on an arithmetic progression of sizes, choosing our maximum size as the size on which at least one of the tools fails to complete the analysis. This was the approach taken by Corbett <ref> [Cor96] </ref> and in our experiment. <p> The INCA tool was then used to generate a set of FSAs that were then converted to the program representations for SPIN, SPIN+PO, and SMV using a slightly modified version of Corbett's conversion tool <ref> [Cor96] </ref>. <p> One approach would be to add accuracy to the program representations incrementally. With this approach, we would start without modeling variables, and would incrementally model additional variables until the analysis results meets the accuracy requirements; this is the approach we took in our experiment. Another approach <ref> [Cor94, Cor96] </ref> would model all variables initially, or at least all variables that directly impact the events in the property. If we discovered that this variable modeling led to intractable analyses, we would incrementally remove variable modeling until the analysis was tractable. <p> It would also be interesting to measure the rates at which analysis time and consumption of other resources, such as memory, grow as the size of the programs being analyzed increases. Corbett proposed a measure of such growth for individual programs for a single property <ref> [Cor96] </ref>. Although Corbett provides some justification for his calculation of growth rate, it is a relatively crude measure. Moreover, it is not clear how we could extend this calculation to combine data from multiple programs and properties. Another issue is how to compare tool failures.
Reference: [DS92] <author> Srinivasarao Damerla and Sol M. Shatz. </author> <title> Software complexity and Ada rendezvous: Metrics based on nondeterminism. </title> <journal> Journal of Systems and Software, </journal> <volume> 17(2) </volume> <pages> 119-127, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: None of the metrics above try to account for nondeterminism in the 17 program being analyzed. Damerla and Shatz <ref> [DS92] </ref> propose several metrics that we also included in our experiment that are intended to quantify the nondeterminism in Ada programs. A metric called Alpha is used to account for the nondeterminism in entries when several tasks can make entry calls on those entries (entry nondeterminism).
Reference: [DC94] <author> Matthew B. Dwyer and Lori A. Clarke. </author> <title> Data flow analysis for verifying properties of concurrent programs. </title> <booktitle> In Proceedings of the Second ACM SIGSOFT Symposium on Foundations of Software Engineering, </booktitle> <pages> pages 62-75, </pages> <address> New Orleans LA, </address> <month> December </month> <year> 1994. </year>
Reference-contexts: A variety of static concurrency analysis techniques have been proposed, including reachability analysis (e.g., [Hol91, GW91, Pel94]), symbolic model checking (e.g., [McM93]), inequality necessary condition analysis (e.g., [CA95]), and data flow analysis (e.g., <ref> [DC94] </ref>). We are interested in understanding the relative strengths and weaknesses of these techniques in terms of their ability to prove properties of concurrent programs, such as freedom from deadlock and the mutually exclusive use of resources. <p> If there is no integer solution to the system of inequalities there can be no execution that violates the property, and therefore the property must hold for all executions. FLAVERS The FLow Analysis VERifier for Software (FLAVERS) tool <ref> [DC94] </ref> performs data flow analysis to check properties of concurrent programs. The tool accepts a set of Control Flow Graphs (CFGs), annotated with the events of interest, as the specification of the program to be analyzed.
Reference: [For88] <author> Ray Ford. </author> <title> Concurrent algorithms for real-time memory management. </title> <journal> IEEE Software, </journal> <pages> pages 10-23, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: Hartstone program based on the hartstone benchmark program, which iteratively starts and stops a series of tasks. Sizes: 11, 21, 31, 41, 51, and 61 tasks. Memory management program - based on a set of conservative release and allocate memory management algorithms <ref> [For88] </ref>. Sizes: 5, 6, 7, 8, 9, and 10 tasks. Ring program - based on a simulation of token ring access to a resource [Cor94]. Sizes: 4, 8, 12, 16, 20, and 24 tasks.
Reference: [GW91] <author> Patrice Godefroid and Pierre Wolper. </author> <title> Using partial orders for the efficient verification of deadlock freedom and safety properties. </title> <booktitle> In Proceedings of the Third Workshop on Computer Aided Verification, </booktitle> <volume> LNCS vol. 575, </volume> <pages> pages 417-428, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: One class of techniques used for analyzing concurrent software is static analysis, where compile-time information is employed to prove properties about a system. A variety of static concurrency analysis techniques have been proposed, including reachability analysis (e.g., <ref> [Hol91, GW91, Pel94] </ref>), symbolic model checking (e.g., [McM93]), inequality necessary condition analysis (e.g., [CA95]), and data flow analysis (e.g., [DC94]). <p> The partial orders approach of Godefroid and Wolper attempts to reduce the size of the reachable state space through the use of sleep sets <ref> [GW91] </ref>. This method has been implemented as an addition to SPIN, and thus we refer to the resulting tool as SPIN+PO. Like SPIN, the SPIN+PO tool takes input in the form of PROMELA, converts that input into a C program, and checks for deadlock automatically.
Reference: [HL85] <author> D. Helmbold and D.C. Luckham. </author> <title> Debugging Ada tasking programs. </title> <journal> IEEE Software, </journal> <pages> pages 47-57, </pages> <month> March </month> <year> 1985. </year> <month> 28 </month>
Reference-contexts: Dining Philosophers with Host program - includes a host that only lets a certain number of philosophers sit in the ring. Sizes: 5, 7, 9, 11, 13, and 15 tasks. Gas Station program - provides a simulation of a self-service gas station <ref> [HL85] </ref>. Sizes: 4, 5, 6, 7, 8, and 9 tasks. Hartstone program based on the hartstone benchmark program, which iteratively starts and stops a series of tasks. Sizes: 11, 21, 31, 41, 51, and 61 tasks.
Reference: [Hol91] <author> Gerard J. Holzmann. </author> <title> Design and Validation of Computer Protocols. </title> <publisher> Prentice--Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1991. </year>
Reference-contexts: One class of techniques used for analyzing concurrent software is static analysis, where compile-time information is employed to prove properties about a system. A variety of static concurrency analysis techniques have been proposed, including reachability analysis (e.g., <ref> [Hol91, GW91, Pel94] </ref>), symbolic model checking (e.g., [McM93]), inequality necessary condition analysis (e.g., [CA95]), and data flow analysis (e.g., [DC94]). <p> Although there are a number of other concurrency analysis tools, the selected tools represent most of the main approaches to static analysis of concurrent software. SPIN The Simple Promela INterpreter (SPIN) <ref> [Hol91] </ref> performs reachability analysis, in which the reachable states of the program being analyzed are enumerated and the property of interest is checked on the reachable state space. The program is described in the PROMELA language [Hol91], a language that was developed for the specification of communication protocols. <p> SPIN The Simple Promela INterpreter (SPIN) <ref> [Hol91] </ref> performs reachability analysis, in which the reachable states of the program being analyzed are enumerated and the property of interest is checked on the reachable state space. The program is described in the PROMELA language [Hol91], a language that was developed for the specification of communication protocols. Given a PROMELA description of the program and property, SPIN constructs a C program which, when compiled and executed, performs the actual analysis. SPIN automatically checks for deadlock.
Reference: [Kad92] <author> R. Kadia. </author> <title> Issues encountered in building a flexible software development environment: Lessons from the Arcadia project. </title> <booktitle> In Proceedings of the Fifth ACM SIGSOFT Symposium on Software Development Environments (SDE5), </booktitle> <pages> pages 169-180, </pages> <address> Tyson's Corner VA, </address> <month> December </month> <year> 1992. </year>
Reference-contexts: For this study, we selected 11 scalable programs from the concurrency analysis literature for our experiment. Some of the programs had already been coded in Ada by members of the Arcadia project <ref> [Kad92] </ref> to demonstrate testing and analysis tools, while for others we acquired the INCA inputs used by Corbett [Cor96] and converted them to Ada programs.
Reference: [LT93] <author> David L. Levine and Richard N. Taylor. </author> <title> Metric-driven reengineering for static concurrency analysis. </title> <booktitle> In Proceedings of the 1993 International Symposium on Software Testing and Analysis (ISSTA), </booktitle> <pages> pages 40-50, </pages> <address> Cambridge MA, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: Beta' is given by a a ' Beta = 1 . The metrics Gamma (Alpha + Beta) and Gamma' (Alpha' + Beta') are used to account for total nondeterminism. Levine and Taylor <ref> [LT93] </ref> propose a metric similar to Gamma called Cnd to account for nondeterminism. <p> We note that the elision of variable information tends to yield TIGs that are smaller, in some cases much smaller, than the FSAs for the same tasks. Levine and Taylor <ref> [LT93] </ref> propose a metric, called Cgt, intended to capture the graph theoretic complexity of the program.
Reference: [LC89] <author> Douglas L. Long and Lori A. Clarke. </author> <title> Task interaction graphs for concurrency analysis. </title> <booktitle> In Proceedings of the 11th International Conference on Software Engineering, </booktitle> <pages> pages 44-52, </pages> <address> Pittsburgh PA, </address> <month> May </month> <year> 1989. </year>
Reference-contexts: The program metrics are used to capture characteristics of the Ada programs being analyzed. The internal representation metrics are used to capture characteristics of the set of FSAs for that program, the set of Task Interaction Graphs (TIGs, <ref> [LC89] </ref>) for that program, and the state space and transition relation for SMV. The property metrics are used to capture characteristics of the SPIN never claim and assertions, INCA query, and FLAVERS Property Automaton for each property.
Reference: [McM93] <author> Kenneth L. McMillan. </author> <title> Symbolic Model Checking. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, MA, </address> <year> 1993. </year>
Reference-contexts: One class of techniques used for analyzing concurrent software is static analysis, where compile-time information is employed to prove properties about a system. A variety of static concurrency analysis techniques have been proposed, including reachability analysis (e.g., [Hol91, GW91, Pel94]), symbolic model checking (e.g., <ref> [McM93] </ref>), inequality necessary condition analysis (e.g., [CA95]), and data flow analysis (e.g., [DC94]). We are interested in understanding the relative strengths and weaknesses of these techniques in terms of their ability to prove properties of concurrent programs, such as freedom from deadlock and the mutually exclusive use of resources. <p> We note that another partial order addition [Pel94] to SPIN has been implemented, but since it does not currently support the use of rendezvous, we did not consider it for this experiment. SMV The Symbolic Model Verifier (SMV) <ref> [McM93] </ref> performs symbolic model checking [BCM+90], in which the program state space is represented symbolically rather than explicitly. Although SMV was originally designed as a hardware verification tool, it can also be used for analysis of concurrent software.
Reference: [Mil80] <author> R. Milner, </author> <title> A Calculus of Communicating Systems, </title> <booktitle> volume 92 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1980. </year>
Reference-contexts: The programs and sizes selected for the experiment were: Cyclic program - provides a loosely synchronized ring of processes, where the processes start in order as the ring is traversed, but each process can complete its task at any time <ref> [Mil80] </ref>. Sizes: 4, 8, 12, 16, 20, and 24 tasks. Divide-and-conquer program - provides a set of solvers that can cooperatively solve a problem [ACD+94]. Sizes: 11, 21, 31, 41, 51, and 61 tasks.
Reference: [MP82] <author> Douglas C. Montgomery and Elizabeth A. Peck. </author> <title> Introduction to Linear Regression Analysis. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1982. </year>
Reference-contexts: assertions and the number of assignments to variables used in the assertions as measures of the amount of information needed to check the property. 4.2 Building Predictive Models Linear regression models can be used as approximations of the functional relationship between a response variable and a set of predictor variables <ref> [MP82] </ref>. We used linear regression to build our predictive models of analysis time based on the set of metrics selected for inclusion using the preprocessing discussed above. While linear regression is a widely used for predicting continuous response variables, it is not appropriate for predicting dichotomous response variables [Agr84].
Reference: [NWK85] <author> John Neter, William Wasserman, and Michael H. Kutner. </author> <title> Applied Linear Statistical Models. </title> <editor> Richard D. Irwin, </editor> <publisher> Inc, Illinois, </publisher> <year> 1985. </year>
Reference-contexts: It was therefore necessary for us to preprocess our experimental data, removing predictor variables that are highly correlated to other predictors. 20 One relatively straightforward way to detect multicollinearity is to consider the pairwise Pearson correlation coefficients for the predictor variables <ref> [NWK85] </ref>. Pearson's correlation coefficient provides an estimate of the linear relationship between two variables x and y. The coefficient ranges from -1.0 to 1.0, with a coefficient magnitude close to 1.0 indicating a strong relationship and a magnitude close to 0.0 indicating no linear relationship.
Reference: [Pel94] <author> Doron Peled. </author> <title> Combining partial order reductions with on-the-fly model checking. </title> <booktitle> In Proceedings of the 6th International Conference on Computer Aided Verification, </booktitle> <pages> pages 377-390, </pages> <address> Stanford CA, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: One class of techniques used for analyzing concurrent software is static analysis, where compile-time information is employed to prove properties about a system. A variety of static concurrency analysis techniques have been proposed, including reachability analysis (e.g., <ref> [Hol91, GW91, Pel94] </ref>), symbolic model checking (e.g., [McM93]), inequality necessary condition analysis (e.g., [CA95]), and data flow analysis (e.g., [DC94]). <p> SPIN+PO checks those assertions, just as SPIN does during state space generation, and reports a violation and terminates if an assertion evaluates to false. We note that another partial order addition <ref> [Pel94] </ref> to SPIN has been implemented, but since it does not currently support the use of rendezvous, we did not consider it for this experiment. SMV The Symbolic Model Verifier (SMV) [McM93] performs symbolic model checking [BCM+90], in which the program state space is represented symbolically rather than explicitly.
Reference: [Spr93] <author> P. Sprent. </author> <title> Applied Nonparametric Statistical Methods. </title> <publisher> Chapman & Hall, </publisher> <address> London, </address> <year> 1993. </year>
Reference-contexts: For our alternative hypothesis, we checked whether the analysis times were different. Hypothesis formulation for checking the other biases is similar. To perform the actual hypothesis testing we used the sign test, a nonparametric test for comparing the means of two distributions <ref> [Spr93] </ref>. While the paired-sample t-test is a more common way to compare means when cases need to be paired in the calculation, the paired-sample t-test assumes that the differences between the cases are normally distributed.
Reference: [TLP+95] <author> Walter F. Tichy, Paul Lukowitz, Lutz Prechelt, and Ernst A. Heinz. </author> <title> Experimental evaluation in computer science; A quantitative study. </title> <journal> The Journal of Systems and Software, </journal> <volume> 28(1) </volume> <pages> 9-18, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: The final section presents our conclusions and a discussion of future work. 2 EXPERIMENT DESIGN ISSUES AND TRADEOFFS Researchers have pointed out the limited number of empirical studies in software engineering <ref> [TLP+95] </ref>. Such studies can be extremely difficult to carry out, however, especially if human subjects are involved and the measures are subjective (e.g., code understanding and effectiveness of design methodologies).
Reference: [Wam85] <author> Gordon Kent Wampler. </author> <title> Static concurrency analysis of Ada programs. </title> <type> Master's thesis, </type> <institution> University of California, Irvine, </institution> <year> 1985. </year> <month> 29 </month>
Reference-contexts: Wampler has proposed the metric N T/2 as a good predictor of reachability graph size, at least for some programs <ref> [Wam85] </ref> and we included the WFSA (for Wampler, FSAs) metric in our experiment as well.
References-found: 26

