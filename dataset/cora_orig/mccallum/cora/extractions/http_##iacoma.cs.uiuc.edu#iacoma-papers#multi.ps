URL: http://iacoma.cs.uiuc.edu/iacoma-papers/multi.ps
Refering-URL: http://iacoma.cs.uiuc.edu/papers.html
Root-URL: http://www.cs.uiuc.edu
Email: venkat,torrella@cs.uiuc.edu  
Title: Efficient Use of Processing Transistors for Larger On-Chip Storage: Multithreading 1  
Author: Venkata Krishnan and Josep Torrellas 
Date: May 1997  
Web: http://iacoma.cs.uiuc.edu/iacoma/  
Address: IL 61801  
Affiliation: Center for Supercomputing Research and Development University of Illinois at Urbana-Champaign,  
Abstract: Recent proposals have included multiple conventional superscalars on a chip (which we call superchip) and multithreaded superscalars, also called simultaneous multithreaded (SMT) processors. In the past, these systems have been compared in the context of sequential applications. In this paper, we focus on a wide range of parallel applications. We show that, in low-end machines with only one processor chip on which to run the multiple threads, in-order issue SMTs achieve a performance that is more stable and, on average, higher than out-of-order issue superchips. In high-end machines with several processor chips working on the same application, however, the higher demands on the processors often make out-of-order issue a requirement. While a fully dynamic SMT design is too expensive, a very cost-effective organization is several low-issue dynamic SMT processors sharing a chip. Overall, SMTs deliver high performance because they adapt to the specific thread- and instruction-level parallelism of the application. They utilize the processing transistors more effectively, thereby freeing up space to be used for more on-chip storage. 
Abstract-found: 1
Intro-found: 1
Reference: [BEF + 95] <author> W. Blume, R. Eigenmann, K. Faigin, J. Grout, J. Hoeflinger, D. Padua, P. Pe-tersen, W. Pottenger, L. Rauchwerger, P. Tu, and S. Weatherford. </author> <title> Effective Automatic Parallelization with Polaris. </title> <journal> International Journal of Parallel Programming, </journal> <month> May </month> <year> 1995. </year>
Reference-contexts: The two Splash applications are explicitly-parallel programs written in C and use ANL m4 macros for parallel constructs. However, the SPEC95 benchmarks and the NASA7 kernels are sequential programs written in FOR-TRAN77. We used the Polaris automatic parallelizing compiler <ref> [BEF + 95] </ref> to identify parallel sections of the code. Polaris uses several techniques, such as inter-procedural symbolic program analysis, scalar and array privatization, symbolic dependence analysis, advanced induction and reduction variable recognition and elimination, for parallelizing a FORTRAN application.
Reference: [DEC96] <author> The 21264: </author> <title> A Superscalar Alpha Processor with Out-of-order Execution. Microprocessor Forum, </title> <month> October </month> <year> 1996. </year>
Reference-contexts: There are many solutions to this problem. For example, since it is rare that a thread is able to fill the entire issue bandwidth, we could reduce the number of read/write ports and use an arbitration mechanism. Alternatively, the Alpha 21264 <ref> [DEC96] </ref>, which has 112 integer registers, solves the problem by having two ver Table 1: Characteristics of the functional units of the SMT processor. sions of the register file and partitioning the functional units between them. Consistency is maintained with a 1-cycle penalty between the two clusters.
Reference: [DGH91] <author> H. Davis, S.R. Goldschmidt, and J. Hen-nessy. </author> <title> Multiprocessor Simulation and Tracing using Tango. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <year> 1991. </year>
Reference-contexts: The remote latencies, specified in Table 3, are low because we only model a 4-node machine. 4 Evaluation Environment We have built a detailed processor simulator that accurately models the two architectures described in the previous section. The simulator uses the front-end of the execution driven TangoLite simulation environment <ref> [DGH91] </ref>. TangoLite is a software-based multiprocessor environment that runs a parallel application, using ANL m4 macros for parallel constructs, on a MIPS-based uniprocessor. It provides a multiprocessor environment through the multiplexing of lightweight threads onto this sequential machine.
Reference: [LLG + 90] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> The Directory-based Cache Coherence Protocol for the DASH Multiprocessor. </title> <booktitle> In 17th International Symposium on Computer Architecture, </booktitle> <year> 1990. </year>
Reference-contexts: All latencies correspond to a contention-free round trip. For the low-end machine, we model a simple workstation. For the high-end machine, we model a scalable shared-memory multiprocessor similar to DASH <ref> [LLG + 90] </ref> as shown in Figure 4. Each node has a portion of the global shared memory and directory.
Reference: [MIP94] <institution> MIPS Technologies, Inc. R10000 Microprocessor Chipset, Product Overview, </institution> <year> 1994. </year>
Reference-contexts: Finally, in their SMT processor, Tullsen et al [TEE + 96] add an extra cycle for register file access. Their simulations show that the performance impact is negligible. 3.2 Out-of-Order Issue Superchip Ar chitecture The out-of-order mechanism of a processor in the su-perchip is based on the MIPS R10K <ref> [MIP94] </ref>. It has an associative queue for instruction issue, a mapping table for register renaming, and a reorder buffer (called active list in R10K) to perform instruction retirement in order. Multiple branch predictions are allowed, even when there are pending unresolved branches.
Reference: [ONH + 96] <author> K. Olukotun, B. Nayfeh, L. Hammond, K. Wilson, and K. Chang. </author> <title> The Case for a Single-Chip Multiprocessor. </title> <booktitle> In 7th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <year> 1996. </year>
Reference-contexts: There are two major proposals for architectures to exploit ILP across multiple control flows. The first one is to put several simpler superscalars on the same chip <ref> [ONH + 96] </ref>. We call this the superchip approach. In this approach, each processor in the chip handles one thread and exploits ILP within that thread. A major advantage of this approach is the simplicity of the design, which enables high clock frequencies.
Reference: [SBV95] <author> G.S. Sohi, </author> <title> S.E. Breach, </title> <booktitle> and T.N. Vijayaku-mar. Multiscalar Processors. In 22nd International Symposium on Computer Architecture, </booktitle> <year> 1995. </year>
Reference-contexts: Therefore, it runs slower than on a plain superscalar. The absence of parallel threads in much of the existing executables is a major handicap for super-chips. An organization related to the superchip is the Mul-tiscalar architecture <ref> [SBV95] </ref>. This architecture allows binary translation of object code to convert existing sequential applications into multithreaded ones. Unfortunately, in Multiscalars, like in superchips and similar architectures like the Superthreaded one [TY96], resources can be easily wasted.
Reference: [TEE + 96] <author> D.M. Tullsen, S.J. Eggers, J.S. Emer, H.M. Levy, J.L. Lo, and R.L. Stamm. </author> <title> Exploiting Choice: Instruction Fetch and Issue on an implementable Simultaneous Multithread-ing Processor. </title> <booktitle> In 23rd International Symposium on Computer Architecture, </booktitle> <year> 1996. </year>
Reference-contexts: Consistency is maintained with a 1-cycle penalty between the two clusters. Finally, in their SMT processor, Tullsen et al <ref> [TEE + 96] </ref> add an extra cycle for register file access. Their simulations show that the performance impact is negligible. 3.2 Out-of-Order Issue Superchip Ar chitecture The out-of-order mechanism of a processor in the su-perchip is based on the MIPS R10K [MIP94]. <p> For the static data points, we see that tomcatv, mgrid and swim have moved to the left inducing poor SMT performance. 5.3 Dynamic SMTs and SMT-Based Su perchips To deliver high, stable performance, we now examine SMT processors with out-of-order issue. Tullsen et al <ref> [TEE + 96] </ref> have described support for dynamic SMT processors. However, this approach inherits all the complexities of conventional superscalars and adds some more. For example, the instruction issue in a SMT processor with eight threads requires a large associative queue.
Reference: [TEL95] <author> D.M. Tullsen, S.J. Eggers, and H.M. Levy. </author> <title> Simultaneous Multithreading: Maximizing on-chip Parallelism. </title> <booktitle> In 22nd International Symposium on Computer Architecture, </booktitle> <year> 1995. </year>
Reference-contexts: The second proposed architecture supports multiple threads on a single superscalar processor such that instructions from different threads can be issued in the same cycle. The different threads share some or most of the resources. This approach is called simultaneous multithreading (SMT) <ref> [TEL95] </ref>. An advantage of this approach is that the resources tend to be highly utilized. If, in a given cycle, a thread is not using a resource, that resource can, typically, be utilized by another thread. <p> However, one drawback of this approach is that it inherits the complexity of existing superscalars and, in addition, adds extra hardware. In the past, superchips and SMT processors have been compared for throughput by running multipro-grammed loads of sequential applications <ref> [TEL95] </ref>. In this paper, we show that a wide range of parallel applications run more cost-effectively on SMT-based processors than on superchips.
Reference: [TY96] <author> J.Y. Tsai and P.C. Yew. </author> <title> The Superthreaded Architecture: Thread Pipelining with Run-Time Data Dependence Checking and Control Speculation. </title> <booktitle> In Proceedings of International Conference on Parallel Architectures and Compilation Techniques (PACT '96), </booktitle> <year> 1996. </year>
Reference-contexts: An organization related to the superchip is the Mul-tiscalar architecture [SBV95]. This architecture allows binary translation of object code to convert existing sequential applications into multithreaded ones. Unfortunately, in Multiscalars, like in superchips and similar architectures like the Superthreaded one <ref> [TY96] </ref>, resources can be easily wasted. This is due to the strict partitioning of resources among the various processors constituting the superchip. A thread running on one processor cannot utilize a resource belonging to a sec ond processor even if the resource is unused.
Reference: [WOT + 95] <author> S.C. Woo, M. Ohara, E. Torrie, J.P. Singh, and A. Gupta. </author> <title> The SPLASH-2 programs: Characterization and methodological considerations. </title> <booktitle> In 22nd International Symposium on Computer Architecture, </booktitle> <year> 1995. </year>
Reference-contexts: We choose seven applications. Three applications are SPEC95 benchmarks, namely, swim, tomcatv and mgrid ; two are NASA7 kernels, emit and vpenta, and the remaining two are Splash applications, fmm and ocean from the SPLASH-2 suite <ref> [WOT + 95] </ref>. The two Splash applications are explicitly-parallel programs written in C and use ANL m4 macros for parallel constructs. However, the SPEC95 benchmarks and the NASA7 kernels are sequential programs written in FOR-TRAN77.
References-found: 11

