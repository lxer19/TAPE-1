URL: ftp://ftp.cs.columbia.edu/pub/CAVE/papers/nayar/nayar-murase-nene-track_icra-94.ps.gz
Refering-URL: http://www.cs.columbia.edu/CAVE/video-demos.html
Root-URL: http://www.cs.columbia.edu
Title: Learning, Positioning, and Tracking Visual Appearance  
Author: Shree K. Nayar, Hiroshi Murase, and Sameer A. Nene 
Address: New York, N.Y. 10027  
Affiliation: Department of Computer Science Columbia University  
Abstract: The problem of vision-based robot positioning and tracking is addressed. A general learning algorithm is presented for determining the mapping between robot position and object appearance. The robot is first moved through several displacements with respect to its desired position, and a large set of object images is acquired. This image set is compressed using principal component analysis to obtain a low-dimensional subspace. Variations in object images due to robot displacements are represented as a compact parametrized manifold in the subspace. While positioning or tracking, errors in end-effector coordinates are efficiently computed from a single brightness image using the parametric manifold representation. The learning component enables accurate visual control without any prior hand-eye calibration. Several experiments have been conducted to demonstrate the practical feasibility of the proposed positioning/tracking approach and its relevance to industrial applications. 
Abstract-found: 1
Intro-found: 1
Reference: [Weiss et al. 87] <author> L. Weiss, A. Sanderson, and C. Neuman, </author> <title> Dynamic sensor-based control of robots with visual feedback, </title> <journal> IEEE Journal of Robotics and Automation, </journal> <volume> Vol. RA-3, No. 5, </volume> <pages> pp. 404-417, </pages> <month> Oct. </month> <year> 1987. </year>
Reference-contexts: The goal is to find the rotation and translation that must be applied to the end-effector to bring the features back to their desired positions in the image. Image features used vary from geometric primitives such as edges, lines, vertices, and circles <ref> [Weiss et al. 87] </ref> [Feddema et al. 91], [Koivo and Houshangi 91] [Hashimoto et al. 91] to optical fl This research was conducted at the Center for Research in Intelligent Systems, Department of Computer Science, Columbia University.
Reference: [Feddema et al. 91] <author> J. Feddema, C.S.G. Lee, and O. Mitchell, </author> <title> Weighted selection of image features for resolved rate visual feedback control, </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> Vol. 7, No. 1, </volume> <pages> pp. 31-47, </pages> <month> Feb. </month> <year> 1991. </year>
Reference-contexts: The goal is to find the rotation and translation that must be applied to the end-effector to bring the features back to their desired positions in the image. Image features used vary from geometric primitives such as edges, lines, vertices, and circles [Weiss et al. 87] <ref> [Feddema et al. 91] </ref>, [Koivo and Houshangi 91] [Hashimoto et al. 91] to optical fl This research was conducted at the Center for Research in Intelligent Systems, Department of Computer Science, Columbia University.
Reference: [Koivo and Houshangi 91] <author> A. Koivo and N. Houshangi, </author> <title> Real-time vision feedback for servoing robotics manipulator with self-tuning controller, </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> Vol. 21, No. 1, </volume> <pages> pp. 134-142, </pages> <month> Feb. </month> <year> 1991. </year>
Reference-contexts: Image features used vary from geometric primitives such as edges, lines, vertices, and circles [Weiss et al. 87] [Feddema et al. 91], <ref> [Koivo and Houshangi 91] </ref> [Hashimoto et al. 91] to optical fl This research was conducted at the Center for Research in Intelligent Systems, Department of Computer Science, Columbia University. It was supported in part by the David and Lucile Packard Fellowship and in part by ARPA Contract No. DACA 76-92-C-0007.
Reference: [Hashimoto et al. 91] <author> K. Hashimoto, T. Kimoto, and H. Kimura, </author> <title> Manipulator Control with Image-Based Visual Servo, </title> <booktitle> Proceedings of IEEE International Conference on Robotics and Automation, </booktitle> <year> 1991, </year> <pages> pp. 2267-2271. </pages> <editor> [Papanikolopoulos et al. 91] N. Papanikolopoulos, P. Khosla, and T. </editor> <title> Kanade, Adaptive robotic visual tracking, </title> <booktitle> Proceedings of Automatic Control Conference, </booktitle> <year> 1991. </year>
Reference-contexts: Image features used vary from geometric primitives such as edges, lines, vertices, and circles [Weiss et al. 87] [Feddema et al. 91], [Koivo and Houshangi 91] <ref> [Hashimoto et al. 91] </ref> to optical fl This research was conducted at the Center for Research in Intelligent Systems, Department of Computer Science, Columbia University. It was supported in part by the David and Lucile Packard Fellowship and in part by ARPA Contract No. DACA 76-92-C-0007.
Reference: [Luo et al. 88] <author> R. Luo, R. Mullen, and D. Wessel, </author> <title> An Adaptive Robotic Tracking System using Optical Flow, </title> <booktitle> Proceedings of IEEE International Conference on Robotics and Automation, </booktitle> <year> 1988, </year> <pages> pp. 568-573. </pages>
Reference-contexts: It was supported in part by the David and Lucile Packard Fellowship and in part by ARPA Contract No. DACA 76-92-C-0007. Hiroshi Murase is with the NTT Basic Research Lab., Tokyo, Japan. flow estimates [Papanikolopoulos et al. 91] <ref> [Luo et al. 88] </ref> and object location estimates obtained using stereo [Allen et al. 92].
Reference: [Allen et al. 92] <author> P. K. Allen, A. Timcenko, B. Yoshimi, and P. Michelman, </author> <title> Trajectory Filtering and Prediction for Automated Tracking and Grasping of a Moving Object, </title> <booktitle> Proceedings of IEEE International Conference on Robotics and Automation, </booktitle> <address> Nice, </address> <year> 1992, </year> <pages> pp. 1850-1856. </pages>
Reference-contexts: It was supported in part by the David and Lucile Packard Fellowship and in part by ARPA Contract No. DACA 76-92-C-0007. Hiroshi Murase is with the NTT Basic Research Lab., Tokyo, Japan. flow estimates [Papanikolopoulos et al. 91] [Luo et al. 88] and object location estimates obtained using stereo <ref> [Allen et al. 92] </ref>. The control schemes used to drive the robot to its desired position vary from simple prediction algorithms employed to achieve computational efficiency, to more sophisticated adaptive self-tuning controllers that account for the dynamics of the manipulator.
Reference: [Kuperstien 87] <author> M. Kuperstien, </author> <title> Adaptive visual-motor coordination in multijoint robots using parallel architecture, </title> <booktitle> Proceedings of IEEE International Conference on Robotics and Automation, </booktitle> <address> Raleigh, N.C., </address> <year> 1987, </year> <pages> pp. 1595-1602. </pages>
Reference-contexts: In addition, calibration of the vision sensor is not required as long as the sensor-robot configuration remains unaltered between learning and tracking. These methods differ from each other primarily in the type of learning algorithm used. The learning strategies vary from neural-like networks <ref> [Kuperstien 87] </ref> [Mel 87] [Miller 89] [Walter et al. 90] to table lookup mechanisms such as the cerebellar model articulation controller (CMAC) [Albus 75] [Miller 87]. Here, we propose a new framework for learning-based visual positioning/tracking.
Reference: [Mel 87] <author> B. W. Mel, MURPHY: </author> <title> A robot that learns by doing, </title> <booktitle> AIP Proceedings of Neural Information Processing System Conference, </booktitle> <address> Denver, CO, </address> <year> 1987. </year>
Reference-contexts: In addition, calibration of the vision sensor is not required as long as the sensor-robot configuration remains unaltered between learning and tracking. These methods differ from each other primarily in the type of learning algorithm used. The learning strategies vary from neural-like networks [Kuperstien 87] <ref> [Mel 87] </ref> [Miller 89] [Walter et al. 90] to table lookup mechanisms such as the cerebellar model articulation controller (CMAC) [Albus 75] [Miller 87]. Here, we propose a new framework for learning-based visual positioning/tracking.
Reference: [Miller 89] <author> W. T. Miller, </author> <title> Real-time application of neural networks for sensor-based control of robots with vision, </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> Vol. 19, No. 4, </volume> <pages> pp. 825-831, </pages> <address> July/August, </address> <year> 1989. </year>
Reference-contexts: In addition, calibration of the vision sensor is not required as long as the sensor-robot configuration remains unaltered between learning and tracking. These methods differ from each other primarily in the type of learning algorithm used. The learning strategies vary from neural-like networks [Kuperstien 87] [Mel 87] <ref> [Miller 89] </ref> [Walter et al. 90] to table lookup mechanisms such as the cerebellar model articulation controller (CMAC) [Albus 75] [Miller 87]. Here, we propose a new framework for learning-based visual positioning/tracking.
Reference: [Walter et al. 90] <author> J. Walter, T. Martinez, and K. Schul-ten, </author> <title> Industrial robot learns visuo-motor coordination by means of neural-gas network, </title> <booktitle> Proceedings of International Joint Conference on Neural Networks, </booktitle> <month> June, </month> <year> 1990. </year>
Reference-contexts: In addition, calibration of the vision sensor is not required as long as the sensor-robot configuration remains unaltered between learning and tracking. These methods differ from each other primarily in the type of learning algorithm used. The learning strategies vary from neural-like networks [Kuperstien 87] [Mel 87] [Miller 89] <ref> [Walter et al. 90] </ref> to table lookup mechanisms such as the cerebellar model articulation controller (CMAC) [Albus 75] [Miller 87]. Here, we propose a new framework for learning-based visual positioning/tracking.
Reference: [Albus 75] <author> J. S. Albus, </author> <title> A new approach to manipulator control: The cerebellar model, </title> <journal> Transactions of ASME, Journal of Dynamic Systems Measurement and Control, </journal> <volume> Vol. 97, </volume> <pages> pp. 220-227, </pages> <month> Sept. </month> <year> 1975. </year>
Reference-contexts: These methods differ from each other primarily in the type of learning algorithm used. The learning strategies vary from neural-like networks [Kuperstien 87] [Mel 87] [Miller 89] [Walter et al. 90] to table lookup mechanisms such as the cerebellar model articulation controller (CMAC) <ref> [Albus 75] </ref> [Miller 87]. Here, we propose a new framework for learning-based visual positioning/tracking.
Reference: [Miller 87] <author> W. T. Miller, </author> <title> Sensor-based control of robotic manipulators using a general learning algorithm, </title> <journal> IEEE Journal of Robotics and Automation, </journal> <volume> Vol. RA-3, No. 2, </volume> <pages> pp. 157-165, </pages> <month> April, </month> <year> 1987. </year>
Reference-contexts: These methods differ from each other primarily in the type of learning algorithm used. The learning strategies vary from neural-like networks [Kuperstien 87] [Mel 87] [Miller 89] [Walter et al. 90] to table lookup mechanisms such as the cerebellar model articulation controller (CMAC) [Albus 75] <ref> [Miller 87] </ref>. Here, we propose a new framework for learning-based visual positioning/tracking.
Reference: [Oja 83] <author> E. Oja, </author> <title> Subspace methods of Pattern Recognition, </title> <publisher> Research Studies Press, </publisher> <address> Hertfordshire, </address> <year> 1983. </year>
Reference-contexts: A large set of object images is then obtained by incrementally perturbing the robot's end-effector (hand-eye system). The image set is compressed using principal component analysis <ref> [Oja 83] </ref> to obtain a low-dimensional subspace, called eigenspace. Variations in object images due to robot displacements are represented in the form of a parametrized manifold in this eigenspace. The advantages of using this representation are discussed in the paper. <p> Our first step is to take advantage of this correlation and compress the large set to a low-dimensional representation that captures the key appearance characteristics of the object. A suitable compression technique is based on principal component analysis <ref> [Oja 83] </ref>, where the eigen-vectors of the image set are computed and used as orthogonal bases for representing individual images. Though, in general, all the eigenvectors of an image set are required for perfect reconstruction of any particular image, only a few are sufficient for positioning or tracking applications. <p> Fast algorithms for solving this problem have been a topic of active research in the area of image coding/compression and pattern recognition (see <ref> [Oja 83] </ref>). A reasonably efficient algorithm is based on the conjugate gradient method. The problem is formulated as one of finding the eigenvalues and eigenvectors that maximize a scalar function. <p> Consider two images ^ i r and ^ i s that belong to the image set used to compute an eigenspace. Let the points f r and f s be the projections of the two images in eigenspace. It is well-known in pattern recognition theory <ref> [Oja 83] </ref> [Murase and Nayar 93] that the distance between the two points in eigenspace is an approximation to the cor relation between the two brightness images: jj ^ i r ^ i s jj jj f r f s jj (13) The closer the projections are in eigenspace, the more
Reference: [Murakami and Kumar 82] <author> H. Murakami and V. Kumar, </author> <title> "Efficient Calculation of Primary Images from a Set of Images," </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> Vol. 4, No. 5, </volume> <pages> pp. 511-515, </pages> <month> September </month> <year> 1982. </year>
Reference-contexts: If the number of images M in the set is much smaller than the number of pixels N in each image, a substantially more efficient algorithm may be used. Developed by Murakami and Kumar <ref> [Murakami and Kumar 82] </ref>, this algorithm uses the implicit covariance matrix ~ Q, where: ~ Q = P P (9) Note that ~ Q is a M fi M matrix and hence much smaller than Q when the number of images in P is smaller than the number of pixels in <p> Using the conjugate gradient algorithm, the M eigenvectors of ~ Q can be computed. These can be computed much faster than the first M eigenvectors of Q due to the disparity in the sizes of the two matrices. Using singular value decomposition (SVD), Murakami and Kumar <ref> [Murakami and Kumar 82] </ref> show that the M largest eigenvalues and corresponding eigenvectors of Q can be determined from the M eigenvalues and eigenvectors of ~ Q as: k = ~ k 1 Here, ~ k and ~ e k are the k th eigenvalue and eigenvector of ~ Q.
Reference: [Murase and Nayar 93] <author> H. Murase and S. K. Nayar, </author> <title> "Learning and Recognition of 3D Objects from Appearance," </title> <booktitle> Proc. of IEEE Workshop on Qualitative Vision, </booktitle> <pages> pp. 39-50, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: In practice, the number of end-effector DOFs used for positioning and track ing can vary. 3 The parametric eigenspace representation was introduced in <ref> [Murase and Nayar 93] </ref> for object recognition and pose estimation. The above eigenspace representation has an important property. Consider two images ^ i r and ^ i s that belong to the image set used to compute an eigenspace. <p> Consider two images ^ i r and ^ i s that belong to the image set used to compute an eigenspace. Let the points f r and f s be the projections of the two images in eigenspace. It is well-known in pattern recognition theory [Oja 83] <ref> [Murase and Nayar 93] </ref> that the distance between the two points in eigenspace is an approximation to the cor relation between the two brightness images: jj ^ i r ^ i s jj jj f r f s jj (13) The closer the projections are in eigenspace, the more similar are
Reference: [Rogers 90] <author> D. F. Rogers, </author> <title> Mathematical Elements for Computer Graphics, 2nd ed., </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: The discrete points are interpolated to obtain this manifold. In our implementation, we have used a standard quadratic B-spline interpolation algorithm <ref> [Rogers 90] </ref>. The resulting manifold can be expressed as: f (q) = f ( q 1 ; q 2 ; :::::::;q m ) (12) This manifold is in a low-dimensional space and therefore is a compact continuous representation of object appearance as a function of manipulator coordinates q.
Reference: [Nene and Nayar 93] <author> S. A. Nene and S. K. Nayar, </author> <title> "Binary Search Through Multiple Dimensions," </title> <type> Technical Report CUCS-26-93, </type> <institution> Department of Computer Science, Columbia University, </institution> <month> August, </month> <year> 1993. </year>
Reference-contexts: In practice, the manifold is stored in memory as a list of K-dimensional points obtained by densely re-sampling f (q). The closest point to f c on f (q) can be determined either by exhaustive search (if the list of manifold points is small), binary search, or indexing. In <ref> [Nene and Nayar 93] </ref> we have developed an algorithm that results in near-constant search time of approximately 20 msec on a Sun IPX workstation. Alternatively, q c can be determined from f c by training a regularization network of the type described in [Poggio and Girosi 90].

References-found: 17

