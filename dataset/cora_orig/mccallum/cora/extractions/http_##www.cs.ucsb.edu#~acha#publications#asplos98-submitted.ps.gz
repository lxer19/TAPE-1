URL: http://www.cs.ucsb.edu/~acha/publications/asplos98-submitted.ps.gz
Refering-URL: http://www.cs.ucsb.edu/~acha/sdi.html
Root-URL: http://www.cs.ucsb.edu
Title: Active Disks: Programming Model, Algorithms and Evaluation  
Author: Anurag Acharya Mustafa Uysal Joel Saltz 
Address: Santa Barbara College Park College Park  
Affiliation: Dept. of Computer Science Dept. of Computer Science Dept. of Computer Science University of California University of Maryland University of Maryland  
Abstract: Several application and technology trends indicate that it might be both profitable and feasible to move computation closer to the data that it processes. In this paper, we evaluate Active Disk architectures which integrate significant processing power and memory into a disk drive and allow application-specific code to be downloaded and executed on the data that is being read from (written to) disk. The key idea is to o*oad bulk of the processing to the disk-resident processors and to use the host processor primarily for coordination, scheduling and combination of results from individual disks. To program Active Disks, we propose a stream-based programming model which allows disklets to be executed efficiently and safely. Simulation results for a suite of six algorithms from three application domains (commercial data warehouses, image processing and satellite data processing) indicate that for these algorithms, Active Disks outperform conventional-disk architectures. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Acharya, M. Uysal, R. Bennett, A. Mendelson, M. Beynon, J. Hollingsworth, J. Saltz, and A. Suss-man. </author> <title> Tuning the performance of I/O-intensive parallel applications. </title> <booktitle> In Proceedings of the Fourth ACM Workshop on I/O in Parallel and Distributed Systems, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: Given the volume of data processed and the cost of fetching data from disk, optimizing I/O-intensive algorithms often is matter of setting up efficient pipelines where each stage performs some processing on the data being read from disk and passes it on to the next stage <ref> [1, 4] </ref>. The SQL standard already supports a simpler version of the stream-based model proposed in this paper via the cursor interface [20]. This interface allows a client application to ship a query to the server and receive the results of the query one tuple at-a-time.
Reference: [2] <author> R. Agarwal. </author> <title> A super scalar sort algorithm for RISC processors. </title> <booktitle> In Proceedings of 1996 ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 240-6, </pages> <year> 1996. </year>
Reference-contexts: The host accumulates the partial results forwarded by all disklets. External sort: We used NOWsort [4] as the starting point for both versions of external sort. NOWsort is based on a long history of external sorting research in the database community (e.g. <ref> [2] </ref> and [22]) and currently holds the record for the fastest external sort (the Indy MinuteSort record [16]). We used the pipelined version of the two-pass single-node sort [4] for the conventional-disk version. <p> into one of eight 3 sets of merge buffers; a merger selects the lowest-valued key from the current block of each partition and copies it to one of eight write buffers; a writer writes buffers to disk. 2 Making two passes over the keys with a radix size of 11-bits <ref> [2] </ref> plus a cleanup. 3 Two buffers in the original algorithm. The active-disk algorithm uses two disklets for the first phase, the partitioner and the sorter. The partitioner uses its scratch-space to form as many buckets as the number of disks.
Reference: [3] <author> S. Agarwal, R. Agrawal, P. Deshpande, A. Gupta, J. Naughton, R. Ramakrishnan, and S. Sarawagi. </author> <title> On the computation of multidimensional aggregates. </title> <booktitle> In Proceedings of the 22nd International Conference on Very Large Databases, </booktitle> <pages> pages 506-21, </pages> <year> 1996. </year>
Reference-contexts: The first four algorithms are used in relational databases, the remaining two are used in image databases and satellite data repositories respectively. For each application, we started with well-known algorithms from the literature <ref> [3, 4, 11] </ref> and tried to keep the modifications to the minimum. Third, we compare the performance of the partitioned algorithms running on Active Disks to the performance of the original algorithms running on conventional disks. <p> In each pipeline, the data placed in a box at the bottom is read from disk. gates [14]. In effect, a datacube computes group-bys for all possible combinations of a list of attributes. Several efficient methods for computing a datacube are presented in <ref> [3] </ref>. We use one of these algorithms, called PipeHash, as the conventional-disk algorithm. PipeHash represents the datacube as a lattice of related group-bys. A directed edge connects group-by i to group-by j if j can be generated from i and has exactly one less attribute.
Reference: [4] <author> A.C. Arpaci-Dusseau, R.H. Arpaci-Dusseau, D.E. Culler, J.M. Hellerstein, and D.A. Patterson. </author> <title> High-performance sorting on networks of workstations. </title> <booktitle> In Proceedings of 1997 ACM SIGMOD International Conference on Management of Data, </booktitle> <address> Tucson, AZ, </address> <year> 1997. </year>
Reference-contexts: The first four algorithms are used in relational databases, the remaining two are used in image databases and satellite data repositories respectively. For each application, we started with well-known algorithms from the literature <ref> [3, 4, 11] </ref> and tried to keep the modifications to the minimum. Third, we compare the performance of the partitioned algorithms running on Active Disks to the performance of the original algorithms running on conventional disks. <p> When it runs out of space, it ships the partial results to the host and reinitializes the disk-memory. The host accumulates the partial results forwarded by all disklets. External sort: We used NOWsort <ref> [4] </ref> as the starting point for both versions of external sort. NOWsort is based on a long history of external sorting research in the database community (e.g. [2] and [22]) and currently holds the record for the fastest external sort (the Indy MinuteSort record [16]). <p> NOWsort is based on a long history of external sorting research in the database community (e.g. [2] and [22]) and currently holds the record for the fastest external sort (the Indy MinuteSort record [16]). We used the pipelined version of the two-pass single-node sort <ref> [4] </ref> for the conventional-disk version. The first phase uses a reader-thread to read data and move tuple pointers to buckets and a writer-thread to sort each bucket with partial-radix sort 2 and write the bucket. <p> For group-by, the smaller dataset had 3.35 million distinct values and the larger dataset had 6.7 million distinct values. For select, we assumed a selectivity of 1%. Sort: we used two datasets with 100-byte tuples and 10-byte uniformly distributed keys. We created these datasets based on the description in <ref> [4] </ref>. The smaller dataset was 2 GB (21.5 million tuples) and the larger dataset was 4 GB (43 million tuples). Datacube: we used two datasets with 32-byte tuples. The smaller dataset was 4 GB (134 million tuples) and the larger dataset was 8 GB (268 million tuples). <p> Given the volume of data processed and the cost of fetching data from disk, optimizing I/O-intensive algorithms often is matter of setting up efficient pipelines where each stage performs some processing on the data being read from disk and passes it on to the next stage <ref> [1, 4] </ref>. The SQL standard already supports a simpler version of the stream-based model proposed in this paper via the cursor interface [20]. This interface allows a client application to ship a query to the server and receive the results of the query one tuple at-a-time.
Reference: [5] <author> E. Borowsky, R. Golding, A. Merchant, L. Schrier, E. Shriver, M. Spasojevic, and J. Wilkes. </author> <title> Using attribute-managed storage to achieve QoS. </title> <booktitle> In Proceedings of the 5th International Workshop on Quality of Service, </booktitle> <year> 1997. </year>
Reference-contexts: As a part of the Network-attached Storage Devices effort, 5 several researchers are exploring the use of processors that are integrated with disk drives. Borowsky et al <ref> [5] </ref> are investigating the use of disk-processors to implement quality-of-service guarantees for data retrieval. Gibson et al [9] are investigating the use of disk-processors for performing filesystem and security-related processing on network-attached disks.
Reference: [6] <editor> Cheetah Specifications. </editor> <address> http://www.seagate.com/disc/- cheetah/cheetah.shtml, </address> <month> Feb </month> <year> 1998. </year>
Reference-contexts: At the technology end, the disk transfer rate has been increasing rapidly. The Cheetah 18 drives from Seagate are capable of delivering up to 21 MB/s <ref> [6] </ref>; faster drives are on the horizon. In addition, the power of cheap processors and the size of cheap memory is increasing rapidly. Currently, a 200 MHz Cyrix/IBM 6x86 processor and 16 MB of SDRAM can be purchased for about $100 [7, 28]. <p> It is important to note that disk drives already have embedded processors (for servo control) and memory (for disk cache). Current trends indicate that both these components are already scaling up - e.g. disk caches are already up to 4 MB <ref> [6] </ref>. In this paper, we evaluate Active Disk architectures which integrate significant processing power and memory into a disk drive and allow application-specific code to be downloaded and executed on the data that is being read from (written to) disk. <p> First, since the number of processors scales with the number of disks, active-disk architectures are better equipped to keep up with the processing requirements for rapidly growing datasets. Second, since the processing 1 The 9.1 GB, 16 MB/s Cheetah 9 <ref> [6] </ref> is currently available for about $1100 [21], the 18.2 GB, 21 MB/s Cheetah 18 [6] is available for about $1750 [17]. components are integrated with the drives, the processing capacity will evolve as the disk drives evolve. <p> Second, since the processing 1 The 9.1 GB, 16 MB/s Cheetah 9 <ref> [6] </ref> is currently available for about $1100 [21], the 18.2 GB, 21 MB/s Cheetah 18 [6] is available for about $1750 [17]. components are integrated with the drives, the processing capacity will evolve as the disk drives evolve. This is similar to the evolution of disk caches as the drives get faster, the disk cache becomes larger. The introduction of Active Disks raises several questions.
Reference: [7] <institution> Custom Network Technologies Product Catalog. </institution> <note> http://www.cntwv.com/catalog.htm, Feb 1998. Following link from http://www.lowerprices.com. </note>
Reference-contexts: In addition, the power of cheap processors and the size of cheap memory is increasing rapidly. Currently, a 200 MHz Cyrix/IBM 6x86 processor and 16 MB of SDRAM can be purchased for about $100 <ref> [7, 28] </ref>. If current trends continue, by the end of the decade, the same $100 will be able to buy a 266-300 MHz processor with 32 MB of memory.
Reference: [8] <author> Jeff Eidenshink and Jim Fenno. </author> <title> Source code for LAS, </title> <editor> ADAPS and XID, </editor> <year> 1995. </year> <institution> Eros Data Center, Sioux Falls. </institution>
Reference-contexts: Sensor values are preprocessed to correct the effects of various distortions. The conventional-disk algorithm is based on the technique used in several programs used by NASA <ref> [8, 10, 31] </ref>. It processes sensor values in large chunks, mapping each value to the output grid and performing the composition operation using an accumulator for every output pixel. The active-disk algorithm performs pre-processing and mapping at the disk.
Reference: [9] <author> G. Gibson et al. </author> <title> File server scaling with network-attached secure disks. </title> <booktitle> In Proceedings of the ACM International Conference on Measurement and Modeling of Computer Systems (Sigmetrics '97), </booktitle> <year> 1997. </year>
Reference-contexts: As a part of the Network-attached Storage Devices effort, 5 several researchers are exploring the use of processors that are integrated with disk drives. Borowsky et al [5] are investigating the use of disk-processors to implement quality-of-service guarantees for data retrieval. Gibson et al <ref> [9] </ref> are investigating the use of disk-processors for performing filesystem and security-related processing on network-attached disks. This paper presents one of several independent proposals for architectures whose goal is to scale processing power with dataset size by embedding programmable processors into disk units.
Reference: [10] <author> Gene Feldman. </author> <title> Source code for the SeaWIFS ocean data processing system, 1995. </title> <institution> SeaWIFS group (NASA Goddard). </institution>
Reference-contexts: Sensor values are preprocessed to correct the effects of various distortions. The conventional-disk algorithm is based on the technique used in several programs used by NASA <ref> [8, 10, 31] </ref>. It processes sensor values in large chunks, mapping each value to the output grid and performing the composition operation using an accumulator for every output pixel. The active-disk algorithm performs pre-processing and mapping at the disk.
Reference: [11] <author> G. Graefe. </author> <title> Query evaluation techniques for large databases. </title> <journal> ACM Computing Surveys, </journal> <volume> 25(2) </volume> <pages> 73-170, </pages> <month> Jun </month> <year> 1993. </year>
Reference-contexts: The first four algorithms are used in relational databases, the remaining two are used in image databases and satellite data repositories respectively. For each application, we started with well-known algorithms from the literature <ref> [3, 4, 11] </ref> and tried to keep the modifications to the minimum. Third, we compare the performance of the partitioned algorithms running on Active Disks to the performance of the original algorithms running on conventional disks. <p> The SQL standard provides five aggregation functions: MIN MAX, SUM, AVG and COUNT. Aggregation functions are order-independent and need only a small amount of intermediate storage. Graefe <ref> [11] </ref> shows that hashing-based techniques out-perform sort-based and nested-loop-based techniques for implementing group-bys. Accordingly, we used the hashing-based algorithm from [11] as our conventional-disk algorithm. The active-disk algorithm performs the group-by in two steps. <p> The SQL standard provides five aggregation functions: MIN MAX, SUM, AVG and COUNT. Aggregation functions are order-independent and need only a small amount of intermediate storage. Graefe <ref> [11] </ref> shows that hashing-based techniques out-perform sort-based and nested-loop-based techniques for implementing group-bys. Accordingly, we used the hashing-based algorithm from [11] as our conventional-disk algorithm. The active-disk algorithm performs the group-by in two steps.
Reference: [12] <author> J. Gray. </author> <title> Some Challenges in Building Petabyte Data Stores. </title> <institution> Distinguished Lecture, University of California, Santa Barbara, </institution> <month> Oct </month> <year> 1997. </year>
Reference-contexts: Jim Gray argues that satellite data repositories will grow to petabyte size over the next few years and will require a variety of processing ranging from reprocessing the entire dataset to take advantage of new algorithms to re-projection and composition to suit different display requirements <ref> [12, 13] </ref>. These trends have two implications: first, large data warehouses will always have a large number of disks and, second, architectures that do not scale the processing power as the dataset grows may not be able to keep up with the processing requirements.
Reference: [13] <author> J. Gray. </author> <title> What Happens When Processors Are Infinitely Fast and Storage Is Free? Keynote Speech at the Fifth Workshop on I/O in Parallel and Distributed Systems, </title> <month> Nov </month> <year> 1997. </year>
Reference-contexts: Jim Gray argues that satellite data repositories will grow to petabyte size over the next few years and will require a variety of processing ranging from reprocessing the entire dataset to take advantage of new algorithms to re-projection and composition to suit different display requirements <ref> [12, 13] </ref>. These trends have two implications: first, large data warehouses will always have a large number of disks and, second, architectures that do not scale the processing power as the dataset grows may not be able to keep up with the processing requirements.
Reference: [14] <author> J. Gray, A. Bosworth, A. Layman, and H. Pirahesh. </author> <title> Data cube: A relational aggregation operator generalizing group-by, </title> <booktitle> cross-tab, and sub-totals. In Proceedings of the 12th International Conference on Data Engineering, </booktitle> <pages> pages 152-9, </pages> <address> New Orleans, </address> <month> February </month> <year> 1996. </year>
Reference-contexts: They also simplify the operating system support required on disk-processors. Second, we present partitioned versions of a suite of algorithms that process the datasets of interest. We present six algorithms: SQL select,SQL group-by, external sort, the datacube operation for decision support <ref> [14] </ref>, image convolution and generation of earth images from raw satellite datax. The first four algorithms are used in relational databases, the remaining two are used in image databases and satellite data repositories respectively. <p> It computes multi-dimensional aggregates that are indexed by values of multiple aggre minimum spanning tree computed by the algorithm using estimated sizes of individual group-bys. The right hand side shows four pipelines. In each pipeline, the data placed in a box at the bottom is read from disk. gates <ref> [14] </ref>. In effect, a datacube computes group-bys for all possible combinations of a list of attributes. Several efficient methods for computing a datacube are presented in [3]. We use one of these algorithms, called PipeHash, as the conventional-disk algorithm. PipeHash represents the datacube as a lattice of related group-bys.
Reference: [15] <author> Jim Gray. </author> <title> Put EVERYTHING in the Storage Device. Talk at NASD workshop on storage embedded computing 6 , June 1998. </title>
Reference-contexts: Their results sug-gest that for a set of database operations, an IDISK-based architecture can be significantly faster than a high-end SMP-based server. Jim Gray <ref> [15] </ref> proposes an architecture which contains no front-end host and which integrates a state-of-the-art processor, a large memory and a network interface into the disk unit. We plan to evaluate the proposed architectural alternatives and compare their performance in near future. <p> Second, we plan to extend our suite to include index-based algorithms (such as indexed joins and nearest-neighbor search). Third, we plan to evaluate alternative architectures including a network of cheap PCs, IDISKs and the architecture proposed by Jim Gray <ref> [15] </ref>. Finally, we plan to investigate ways to allow concurrent execution of multiple disklet-groups.
Reference: [16] <author> Jim Gray. </author> <title> The Sort Benchmark Home Page. </title> <note> Available at http://research.microsoft.com/research/barc/- SortBenchmark/, </note> <year> 1998. </year>
Reference-contexts: External sort: We used NOWsort [4] as the starting point for both versions of external sort. NOWsort is based on a long history of external sorting research in the database community (e.g. [2] and [22]) and currently holds the record for the fastest external sort (the Indy MinuteSort record <ref> [16] </ref>). We used the pipelined version of the two-pass single-node sort [4] for the conventional-disk version. The first phase uses a reader-thread to read data and move tuple pointers to buckets and a writer-thread to sort each bucket with partial-radix sort 2 and write the bucket.
Reference: [17] <institution> HyperMedia Communications Inc., </institution> <address> 901 Mariner's Island Blvd, San Mateo CA 94404. The 1998 New Media Hyper Awards. http://newmedia.com/NewMedia/98/- 03/feature/storage.html/, </address> <month> March </month> <year> 1998. </year>
Reference-contexts: Second, since the processing 1 The 9.1 GB, 16 MB/s Cheetah 9 [6] is currently available for about $1100 [21], the 18.2 GB, 21 MB/s Cheetah 18 [6] is available for about $1750 <ref> [17] </ref>. components are integrated with the drives, the processing capacity will evolve as the disk drives evolve. This is similar to the evolution of disk caches as the drives get faster, the disk cache becomes larger. The introduction of Active Disks raises several questions.
Reference: [18] <author> K. Keeton, D. Patterson, and J. Hellerstein. </author> <title> The intelligent disk (IDISK): A revolutionary approach to database computing infrastucture. </title> <type> Unpublished White paper. </type> <month> 7 , Feb </month> <year> 1998. </year>
Reference-contexts: Riedel et al [29] propose a model much similar to ours and evaluate its performance for data-mining and image-processing algorithms. They show that these algorithms can achieve significant gains from the use of Active Disks. Keeton et al <ref> [18] </ref> propose an architecture (IDISK) in which a processor-in-memory chip (IRAM [25]) is integrated into the disk unit and the disk units are connected by a crossbar. They compare this architecture with a conventional 5 http://www.nsic.org/nasd/ (a) Conventional disks (b) Active disks smaller datasets.
Reference: [19] <author> D. Kotz, S. Toh, and S. Radhakrishnan. </author> <title> A detailed simulation model of the HP97560 disk drive. </title> <type> Technical Report PCS-TR94-220, </type> <institution> Dartmouth College, </institution> <year> 1994. </year> <note> 6 http://www.nsic.org/nasd/1998-jun/gray.pdf 7 http://www.cs.berkeley.edu/ kkeeton/Papers/idisk98-draft.ps </note>
Reference-contexts: ADsim contains a detailed disk model, a preliminary implementation of DiskOS, and relatively coarse-grain models of the processor and the I/O interconnect. The disk model is based on the Ruemmler&Wilkes' disk model [30]. We used the implementation by David Kotz <ref> [19] </ref> as the initial codebase. The disk model includes a fixed cost for the controller overhead, a seek model that models short seeks and long seeks separately, 4 a rotational model and a model for disk-geometry that includes multiple zones, inline track-sparing, track skew and cylinder skew.
Reference: [20] <author> J. Melton and A. Simon. </author> <title> Understanding the New SQL: </title>
Reference-contexts: The combination of large requests and deep request-queues allows these algorithms to take full advantage of the aggressive I/O subsystem. SQL SELECT: SELECT filters tuples from a relation based on a user-specified predicate <ref> [20] </ref>. Database administrators can build indices on one or more attributes to speed up SELECTs; SELECTs on non-index attributes, however, are fairly frequent and require scanning the entire relation. We focus on such operations. SELECT applies the filtering predicate independently to each tuple and, therefore, is amenable to coarse-grain parallelization. <p> When the outbound buffer is full, it is shipped to the host. At the host, data from different disks is concatenated in preparation for transfer to the requesting client. SQL group-by: The group-by operation allows users to compute a one-dimensional vector of aggregates indexed by a list of attributes <ref> [20] </ref>. It partitions a relation into disjoint sets of tuples based on the value (s) of index attribute (s) and computes an aggregate value for each set of tuples. The SQL standard provides five aggregation functions: MIN MAX, SUM, AVG and COUNT. <p> The SQL standard already supports a simpler version of the stream-based model proposed in this paper via the cursor interface <ref> [20] </ref>. This interface allows a client application to ship a query to the server and receive the results of the query one tuple at-a-time.
References-found: 20

