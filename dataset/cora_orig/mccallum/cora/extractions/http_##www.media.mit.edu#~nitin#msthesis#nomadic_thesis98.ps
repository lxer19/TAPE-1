URL: http://www.media.mit.edu/~nitin/msthesis/nomadic_thesis98.ps
Refering-URL: http://www.media.mit.edu/~nitin/msthesis/index.html
Root-URL: http://www.media.mit.edu
Title: Contextual Awareness, Messaging and Communication in Nomadic Audio Environments  Architecture and Planning,  
Author: Nitin Sawhney Christopher M. Schmandt Stephen A. Benton 
Degree: in partial fulfillment of the requirements of the degree of Master of Science in Media Arts and Sciences at the  All Rights Reserved Author: Program in Media Arts and Sciences  Certified by:  Accepted by:  Chair, Departmental Committee on Graduate Students  
Date: June 1998  1998  19 May 1998  
Note: 1996 B.E.,  1993 Submitted to the Program in  Copyright Massachusetts Institute of Technology,  Program in  
Affiliation: M.S., Information Design and Technology Georgia Institute of Technology,  Industrial Engineering Georgia Institute of Technology,  Media Arts and Sciences, School of  Massachusetts Institute of Technology  Principal Research Scientist, MIT Media Laboratory  Media Arts and Sciences  
Abstract-found: 0
Intro-found: 1
Reference: [Ahlberg94] <author> Ahlberg, C. and B. Schneiderman. </author> <title> Visual Information Seeking: Tight Coupling of Dynamic Query Filters with Starfield Display. </title> <booktitle> Proceedings of CHI `94. </booktitle> <pages> pp. 313-317. </pages>
Reference-contexts: As messages arrive the system plots the notification level on graphs depicting priority vs. usage level and Chapter 5: Scaleable and Contextual Notification 98 priority vs. conversation level. The graph is similar to dynamic Starfield display used in several interactive visual querying applications <ref> [Ahlberg94] </ref>. When no messages are shown, the graphs display predicted notification levels as slopes based on the two graph axis. See figure 5.7 for two example displays showing the notification graph with "Medium" and "High" weights assigned.
Reference: [Arnaud95] <author> Arnaud, Nicolas. </author> <title> Classification of Sound Textures. M.S. </title> <type> Thesis, </type> <institution> Media Arts and Sciences, MIT Media Lab, </institution> <month> September </month> <year> 1995. </year>
Reference-contexts: Two different approaches were developed for extracting contextual cues from environmental sounds. An earlier approach based on prior work at the Machine Listening Group at the MIT Media Lab <ref> [Arnaud95] </ref> was designed to extract and classify features from 5 pre-defined classes of sounds in the environment [Sawhney97]. The focus of the work was on distinguishing longer-duration environmental sounds into pre-defined classes using near real-time classification techniques. Environmental sounds were recorded via DAT and segmented into training and test data-sets.
Reference: [Arons92] <author> Arons, Barry. </author> <title> A Review of the Cocktail Party Effect. </title> <journal> Journal of American Voice I/O Society, </journal> <volume> Vol. 12, </volume> <month> July </month> <year> 1992. </year>
Reference-contexts: Arons suggests that the effect of spatialization can be improved by allowing listeners to easily switch between channels and pull an audio stream into focus, as well as by allowing sufficient time to fully fuse the audio streams <ref> [Arons92] </ref>. <p> On a hand-held or wearable audio system, lack of a visual display restricts the amount of information that can be easily conveyed at any time. "Spatial and perceptual streaming cues can help in presenting a high bandwidth information to the user by displaying multiple streams of information simultaneously." <ref> [Arons92] </ref>. Nomadic Radio allows simultaneous audio streams. While browsing audio messages, such as voice mail and news summaries, listeners need to find the messages (or segments of the audio) of interest in a time-efficient manner. <p> available, using spatial audio can allow listeners to focus on a primary stream, while listening to others in parallel in the background. "The goal is to keep the speech signals identifiable and differentiable, so that the user can shift attention between Chapter 4: Interface Design 66 the various sound streams." <ref> [Arons92] </ref>. The user can easily switch between "overheard" audio streams to focus on a relevant message. In a future extension of the system, audio-based communication with multiple participants will be provided (spatialized audio-conferencing). <p> As the message is pulled in, its spatial direction is maintained allowing the listener to retain a sense of message arrival time. This spatial continuity is important for discriminating and holding the auditory streams together <ref> [Arons92] </ref>. Spatial Scanning Sometimes listeners want to get a preview of all their messages quickly without manually selecting and playing each one. This is similar to the scan feature on modern radio tuners that allows users to alternatively hear each station for a short duration. <p> In Nomadic Radio, a maximum pitch of 1.3x times was used to maintain intelligibility. There are a range of techniques for time-compressing speech without modifying the pitch, based on SOLA (synchronized overlap add method) <ref> [Arons92] </ref>. Research suggests that presenting the audio at over twice its original playback rate, provides too little of the signal to be accurately comprehensible [Heiman86]. Comprehension of time-compressed speech does increase as listeners hear it more frequently since they tend to adapt to it quickly [Voor65]. <p> In Nomadic Radio, messages can be played at faster rates by increasing the pitch, but the resulting audio is less comprehensible. There are a range of techniques for time-compressing speech without modifying the pitch, based on SOLA (synchronized overlap add method) <ref> [Arons92] </ref>. In future versions of Nomadic Radio, such time-compression would provide better message previews. A mechanism for replying to email and voice messages requires voice recording, speech based dictation, or use of pre-composed email replies triggered by tactile or spoken commands.
Reference: [Arons93] <author> Arons, Barry. SpeechSkimmer: </author> <title> Interactively Skimming Recorded Speech. </title> <booktitle> Proceedings of UIST 93, </booktitle> <month> November </month> <year> 1993. </year>
Reference-contexts: Speech is fast for the author but slow and tedious for the listener. Speech is sequential and exists only temporally; the ear cannot browse around a set of recordings the way the eye can scan a screen of text and images. Hence, techniques such as interactive skimming <ref> [Arons93] </ref>, non-linear access and indexing [Stifleman93, Wilcox97] and audio spatialization [Mullins96] must be considered for browsing audio. Design for wearable audio computing requires (1) attention to the affordances and constraints of speech and audio in the interface (2) coupled with the physical form of the wearable itself.
Reference: [AT&T97] <institution> WATSON for Windows, </institution> <note> Version 2.1, System Developers Guide. AT&T, 1997. http://www.att.com/aspg/blasr.html </note>
Reference-contexts: Independent-speaker recognizer allows the wearable application to be used easily by others as needed. Selecting a Speech Engine for Nomadic Radio We chose to use the Watson SDK (software development kit) from AT&T corporation <ref> [AT&T97] </ref>. At the time, the SDK was in Beta (version 2.0) whereas recently it has been released as a full product (Watson version 2.1). The Watson product is an integrated, automatic speech recognition (ASR) and text-to-speech (TTS) synthesis system that complies with the Microsoft Speech API specification (SAPI).
Reference: [Bederson96] <author> Bederson, Benjamin B. </author> <title> Audio Augmented Reality: A Prototype Automated Tour Guide. </title> <booktitle> Proceedings of CHI 95, </booktitle> <month> May </month> <year> 1996, </year> <pages> pp. 210-211. </pages>
Reference-contexts: It provides the user information related to a recognized physical object via a display and synthesized voice. The system also accepted queries through speech input. 2.3.5 Augmented Audio Museum Guide A prototype audio augmented reality-based tour guide <ref> [Bederson96] </ref> presented digital audio recordings indexed by the spatial location of visitors in a museum. This is a early implementation of a wearable audio system which stores only pre-defined audio information to augment a physical environment. <p> The services in Audio Aura were designed to be easy to author, customize and lightweight to run. Hence, Audio Aura represents a light-weight version of <ref> [Bederson96] </ref>, where local audio storage is not required and a user's location is transmitted via the active badge infrastructure. The design of sound environments and techniques were explored in a simulated aural environment (using VRML).
Reference: [Blauert83] <author> Blauert, J. </author> <title> Spatial Hearing, trans. </title> <editor> John S. Allen. </editor> <publisher> MIT Press, </publisher> <year> 1983. </year>
Reference-contexts: This reduces the spatial separation between messages arriving within a short time of each other. A minimum spatial separation is needed to discriminate between two sound sources. Early experiments found that localization blur ranged from approximately 1.5-5 degrees for various types of sounds presented sequentially <ref> [Blauert83] </ref>. For simultaneous presentation, the minimum recommended distance for accurate localization is 60 degrees spatial audio message 12:00 PM 6:00 PM fl30 horizontal plane Chapter 4: Interface Design 70 [Divenyi89].
Reference: [Bregman90] <author> Bregman, Albert S. </author> <title> Auditory Scene Analysis: The Perceptual Organization of Sound. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Yet the cognitive load of listening to simultaneous channels increases with the number of channels. Experiments show that increasing the number of channels beyond three causes a degradation in comprehension [Stifelman94]. Bregman claims that stream segregation is better when frequency separation is greater between sound streams <ref> [Bregman90] </ref>. Arons suggests that the effect of spatialization can be improved by allowing listeners to easily switch between channels and pull an audio stream into focus, as well as by allowing sufficient time to fully fuse the audio streams [Arons92]. <p> This technique does not scale well as the number of categories increase, and any mechanism to position messages within each category would be arbitrary. 4. Segregation via Pitch: Bregman claims that stream segregation is better when frequency separation is greater between sound streams <ref> [Bregman90] </ref>. As an experiment, different sounds could be played at successively changing pitches (where a sound is highlighted briefly by normalizing the pitch), which could be used to permit rapid browsing of multiple audio streams simultaneously.
Reference: [Cahn90] <author> Cahn, Janet E. </author> <title> The Generation of Affect in Synthesized Speech. </title> <journal> Journal of the American Voice I/O Society , July 1990. </journal>
Reference-contexts: For text messages, synthesized via synthetic speech, the volume of spoken text increases to foreground the message. Other features of the synthetic voice such as pitch (changes mood and expressiveness of the voice) and gender of the synthetic speaker <ref> [Cahn90] </ref> can be used in the future to indicate higher priority messages. The scale of auditory presentation is set while browsing by manually activating/deactivating speech feedback, auditory cues or the ambient sound.
Reference: [Chalfonte91] <author> Chalfonte, B.L., Fish, </author> <title> R.S. and Kraut, R.E. Expressive richness: A comparison of speech and text as media for revision. </title> <booktitle> Proceedings of CHI '91, </booktitle> <pages> pp. 21-26. </pages> <publisher> ACM, </publisher> <year> 1991. </year>
Reference-contexts: used in Nomadic Radio and we will consider design solutions for two different wearable configurations. 1.3.3 Expressive and Efficient Interaction Voice is more expressive and efficient than text because it places less cognitive demands on the speaker and permits more attention to be devoted to the content of the message <ref> [Chalfonte91] </ref>. The intonation in human voice also provides many implicit hints about the message content. VoiceCues (short audio signatures), played as notifications in Nomadic Radio, indicate the identity of the sender of an email message in a quick and unobtrusive manner.
Reference: [Clarkson98] <author> Clarkson, Brian and Alex Pentland. </author> <title> Extracting Context from Environmental Audio. </title> <booktitle> Submitted to the International Symposium on Wearable Computing, IEEE, </booktitle> <year> 1998. </year>
Reference-contexts: Recent collaboration with Brian Clarkson at the vision and modeling group (Vismod) at the Media Lab, allowed integration of a real-time audio classifier in Nomadic Radio that extracts the level of speech from environmental sound <ref> [Clarkson98] </ref>. The system is based on Hidden Markov Models (HMMs) [Rabiner89], a statistical/pattern recognition framework. Audio data sampled at 16 kHz and quantized to 16 bits, is analyzed with a 256 point FFT giving a vector of frequencies extracted at each time step.
Reference: [Cohen94] <author> Cohen, J. </author> <title> Monitoring background activities. Auditory Display: Sonification, Audification, and Auditory Interfaces . Reading, </title> <address> MA: </address> <publisher> Addison-Wesley, </publisher> <year> 1994. </year>
Reference-contexts: Speech and music in the background and peripheral auditory cues can provide an awareness of messages or signify events, without requiring ones full attention or disrupting their foreground activity. Audio easily fades into the background, but users are alerted when it changes <ref> [Cohen94] </ref>. In Nomadic Radio, ambient auditory cues are used to convey events and changes in background activity. 1.3.5 Simultaneous Listening One can attend to multiple background processes via the auditory channel as long as the sounds representing each process are distinguishable. <p> The various auditory cues (as many as 12 sounds play simultaneously) merged as an auditory texture allowed people to hear the plant as a complex integrated process. Background sounds were also explored in ShareMon <ref> [Cohen94] </ref>, a prototype application that notified users of file sharing activity. Cohen found that pink noise used to indicate %CPU time was considered obnoxious, even though users understood the pitch correlation. However, preliminary reactions to wave sounds were considered positive and even soothing.
Reference: [Conaill95] <author> Conaill, O' Brid and David Frohlich. </author> <title> Timespace in the Workplace: Dealing with Interruptions. </title> <booktitle> Proceedings of CHI `95, </booktitle> <year> 1995. </year>
Reference-contexts: A recent observational study <ref> [Conaill95] </ref> evaluated the effect of interruptions on the activity of mobile professionals in their workplace. The study analyzed 29 hours of video data to extract 125 naturally occurring interruptions. An interruption, defined as asynchronous and unscheduled interactions, not initiated by the user, results in the recipient discontinuing the current activity.
Reference: [Gardner95] <author> Gardner, W. G., and Martin, K. D. </author> <title> HRTF measurements of a KEMAR. </title> <journal> Journal of the Acoustical Society of America , 97 (6), </journal> <year> 1995, </year> <pages> pp. 3907-3908. </pages>
Reference-contexts: The perceptual audio models used in RSX 3D are based on a set of HRTF measurements of a KEMAR (electronic mannequin) by Bill Gardner at the MIT Media Lab <ref> [Gardner95] </ref>. The measurements consist of the left and right ear impulse responses from a loudspeaker mounted 1.4 meters from the KEMAR. The HRTF model allows real-time rendering of several monophonic sound sources positioned arbitrarily around the head and permits control of their elevation, azimuth, and distance cues.
Reference: [Gaver89] <author> Gaver, </author> <title> W.W. The Sonic Finder: An interface that uses auditory icons. </title> <booktitle> Human Computer Interaction, </booktitle> <volume> 4 </volume> <pages> 67-94, </pages> <year> 1989. </year>
Reference-contexts: Synthetic speech can be distracting to listeners while they are performing other tasks or having conversation. On the other hand, non-speech audio in the form of auditory icons <ref> [Gaver89] </ref> or cues provide such feedback via short everyday sounds. Auditory cues are a crucial means for conveying awareness, notification and providing necessary assurances in a non-visual interface.
Reference: [Gaver91] <author> Gaver, W.W., R. B. Smith, T. O'Shea. </author> <title> Effective Sounds in Complex Systems: </title> <booktitle> The ARKola Simulation. Proceedings of CHI '91, </booktitle> <address> April 28-May 2, </address> <year> 1991. </year>
Reference-contexts: This reduces the need for synthetic speech feedback which can be distracting. In the future, the inferred priority or urgency of the message could be depicted by increasing the pitch of the VoiceCue. 4.6.4 Ambient Cues for Background Awareness In ARKola <ref> [Gaver91] </ref>, a audio/visual simulation of a bottling factory, repetitive streams of sounds allowed people to keep track of activity, rate, and functioning of running machines. Without sounds people often overlooked problems; with sounds these were indicated by the machine's sound ceasing (often ineffective) or various alert sounds.
Reference: [Handel89] <author> Handel, S. </author> <title> Listening: An Introduction to the Perception of Auditory Events. </title> <publisher> MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: This well known cognitive phenomenon, known as the "Cocktail Party Effect" <ref> [Handel89] </ref> provides justification that humans can monitor several audio streams simultaneously, selectively focusing on any one and placing the rest in the background. A good model of the head-related transfer functions (HRTF) permits effective localization and externalization of sound sources [Wenzel92].
Reference: [Hanson96] <author> Hansen, Brian, David G. Novick, Stephen Sutton. </author> <title> Systematic Design of Spoken Prompts. </title> <booktitle> Proceedings of CHI '96, April1996, </booktitle> <pages> pp. 157-164. References 120 </pages>
Reference-contexts: Many of the prompts use a conversational style rather than being too abrupt. This provides natural sounding feedback that is easier for users to listen to. There is a clear trade-off between conversational vs. short prompts <ref> [Hanson96] </ref>. In general, the user should be allowed to set the level of preferred feedback (discussed in the next section).
Reference: [Hayes83] <author> Hayes, P. and D. Reddy. </author> <title> Steps towards graceful interaction in spoken and written man-machine communication. </title> <journal> International Journal of Man Machine Studies, </journal> <volume> 19, </volume> <pages> pp. 231-284, </pages> <year> 1983. </year>
Reference-contexts: We will consider techniques for contextual notification and scaleable feedback. While the user is navigating the interface, the feedback must be concise yet natural to hear <ref> [Hayes83] </ref>. The synchronization, foregrounding and backgrounding of audio cues, speech feedback and auditory streams is critical for providing a coherent and pleasing presentation.
Reference: [Hindus96] <author> Hindus, Debby, Mark S. Ackerman, Scott Mainwaring, and Brian Starr. Thunderwire: </author> <title> A Field Study of an Audio-Only Media Space. </title> <booktitle> Proceedings of CSCW96, </booktitle> <pages> pp. 238-247. </pages> <month> November </month> <year> 1996. </year>
Reference: [Horvitz95] <author> Horvitz, Eric and Jed Lengyel. </author> <title> Perception, Attention, and Resources: A Decision-Theoretic Approach to Graphics Rendering. </title> <booktitle> Proceedings of the Thirteenth Conference on Uncertainty in Artificial Intelligence (UAI97), </booktitle> <address> Providence, RI, </address> <month> Aug. </month> <pages> 1-3, </pages> <year> 1997, </year> <pages> pp. 238-249. </pages>
Reference: [Horvitz97] <author> Horvitz, Eric and Matthew Barry. </author> <title> Display of Information for Time-Critical Decision Making. </title> <booktitle> Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence (UAI95), </booktitle> <address> Montreal, </address> <month> August </month> <year> 1995. </year>
Reference: [Hudson96] <author> Hudson, Scott E. and Ian Smith. </author> <title> Electronic Mail Previews Using NonSpeech Audio. </title> <booktitle> Proceedings of CHI '96, </booktitle> <month> April </month> <year> 1996, </year> <pages> pp. 237-238. </pages>
Reference-contexts: confirmations - button pressed, speech understood, connected to servers, finished playing or loaded/deleted messages. (2) Mode transitions - switching categories, going to non-speech or ambient mode. (3) Exceptional conditions - message not found, lost connection with servers, and errors. 4.6.2 Priority Cues for Notification In a related project, "email glances" <ref> [Hudson96] </ref> were formulated as a stream of short sounds indicating category, sender and content flags (from keywords in the message).
Reference: [Intel97] <institution> Intel Realistic Sound Experience, 3D RSX SDK Documentation, Intel Corporation, </institution> <year> 1997. </year> <note> http://developer.intel.com/ial/rsx/doc.htm </note>
Reference-contexts: with their own HRTFs, however using a generalized set of superior HRTFs can sometimes improve their localization. 4.7.3 Integrating Spatial Audio in Nomadic Radio In our implementation, the audio sources are rendered in the spatial environment of the listener using a Java interface to the RSX 3D audio API 16 <ref> [Intel97] </ref> developed by Intel. The perceptual audio models used in RSX 3D are based on a set of HRTF measurements of a KEMAR (electronic mannequin) by Bill Gardner at the MIT Media Lab [Gardner95].
Reference: [Jebara98] <author> Jebara, Tony, Bernt Schiele, Nuria Oliver, and Alex Pentland. DyPERS: </author> <title> A Dynamic and Personal Enhanced Reality System. </title> <booktitle> Submitted to the International Symposium on Wearable Computing, IEEE, </booktitle> <year> 1998. </year>
Reference-contexts: Such physical hypertext links used location context to provide instructions on use, history or information left by previous users. A recent approach called DyPERS <ref> [Jebara98] </ref>, utilizes vision techniques to provide audio/video memories of a user's experience on a museum visit. A key innovation is the use of real-time image recognition techniques to trigger the user's own recordings related to physical objects in the environment.
Reference: [Kaelbling96] <author> Kaelbling, Leslie Pack and Michael L. Littman. </author> <title> Reinforcement Learning: A Survey. </title> <journal> Journal of Artificial Intelligence Research , vol. </journal> <volume> 4, </volume> <year> 1996, </year> <pages> pp. 237-285. </pages>
Reference-contexts: Hence, future messages arriving in a similar contextual situation should be assigned an appropriate notification level, based on prior listening patterns of the user. There are several problems and issues in formal reinforcement learning techniques that must be taken into account <ref> [Kaelbling96] </ref> in future work.
Reference: [Kendall95] <author> Kendall, Gary S. </author> <title> A 3-D Sound Primer: Directional Hearing and Stereo Reproduction. Computer Music Journal, </title> <booktitle> 19:4, pp.23-46, Winter 1995, </booktitle> <publisher> MIT Press. </publisher>
Reference-contexts: The composite of these properties are measured and captured as a head-related transfer function (HRTF). When the sound source is not equidistant form the ears, the signal arrives at each ear from a different direction and the HRTFs at each ear differ <ref> [Kendall95] </ref>. <p> The complex acoustic profiles of HRTFs provides a more accurate means for explaining localization and hence modeling synthetic spatial sound. Individual HRTFs will generally be similar for most people despite slight differences in their head size and pinnae (this difference will vary considerably for children) <ref> [Kendall95] </ref>. Elevation judgments and front-back differentiation are more likely to degrade with non-individualized HRTFs [Wenzel93], yet it is also clear that head movement plays a dominant role in resolving front/back confusions [Wallach40].
Reference: [Klatt87] <author> Klatt, D.H. </author> <title> Review of text-to-speech conversion for English. </title> <journal> Journal of the Acoustic Society of America , 82(3): </journal> <pages> 737-793, </pages> <year> 1987. </year>
Reference-contexts: Synthetic Speech Pre-recorded voice prompts provide a natural means for conveying feedback, but this constrains the spoken vocabulary of the interface. Synthetic speech is necessary for applications where prompts must be dynamically generated and textual information such as email or calendar events is conveyed to the listener <ref> [Klatt87] </ref>. Synthetic speech also provides easier development and maintenance, when new prompts are added to the application without any need to explicitly pre-record prompts (and find the original speaker). The perception of synthetic speech does impose greater demand on short-term memory more than recorded speech [Luce83].
Reference: [Kobayashi97] <author> Kobayashi, Minoru and Chris Schmandt. </author> <title> Dynamic Soundscape: Mapping Time to Space for Audio Browsing. </title> <booktitle> Proceedings of CHI 97, </booktitle> <month> March </month> <year> 1997. </year>
Reference-contexts: Kobayashi introduced a technique for browsing audio by allowing listeners to switch their attention between moving sound sources that play multiple portions of a single audio recording <ref> [Kobayashi97] </ref>. <p> This technique is explored for previewing messages as time-compressed spatial audio streams (see section 4.7.6). 5. Segregation via Dynamic Movement: As new audio streams are introduced to a listener's soundspace, they could be placed in "orbit" around the listener at varying or constant speeds <ref> [Kobayashi97] </ref>. The sense of motion could allow segregation of a new stream, especially if the user has a tactile handle on the stream, i.e. the user can grab a stream as it moves, and place it in a specific location, while the others remain fixed.
Reference: [Luce83] <author> Luce, </author> <title> P.A., T.C. Feustel and D.B. Pisoni. Capacity demands in short-term memory for synthetic and natural speech. </title> <booktitle> Human Factors, </booktitle> <volume> 25(1) </volume> <pages> 17-32, </pages> <year> 1983. </year>
Reference-contexts: Synthetic speech also provides easier development and maintenance, when new prompts are added to the application without any need to explicitly pre-record prompts (and find the original speaker). The perception of synthetic speech does impose greater demand on short-term memory more than recorded speech <ref> [Luce83] </ref>. Difficulty in listening to synthetic speech may interfere with the listener's tasks and even remembering the information being spoken [Schmandt94b].
Reference: [Ly94] <author> Ly, Eric and Chris Schmandt. Chatter: </author> <title> A Conversational Learning Speech Interface. </title> <booktitle> Proceedings of AAAI'94 Spring Symposium on MultiMedia MultiModal Systems, </booktitle> <address> Stanford, CA, </address> <month> March </month> <year> 1994. </year>
Reference-contexts: However, once key combinations were learned, experts can execute actions easily using robust DTMF recognition. Nomadic Radio utilizes much of the infrastructure developed for Phoneshell, complementing the remote phone-based access of Phoneshell with spatial audio display and voice-controlled interaction for wearable inter-office access. 2.1.3 Chatter Chatter <ref> [Ly94] </ref> is an attempt to extend the functionality of Phoneshell using continuous, speaker-independent speech recognition. Chatter focuses on communication within a workgroup, i.e. reading email, sending voice messages to workgroup members, looking up people in a rolodex, and placing outgoing calls.
Reference: [Maes94] <editor> Maes, Pattie. </editor> <title> Agents that Reduce Work and Information Overload. </title> <journal> Communications of the ACM, </journal> <month> July </month> <year> 1994, </year> <month> Vol.34, No.7, </month> <pages> pp. 31-41. </pages>
Reference-contexts: A partial solution to information overload is to give people timely and filtered information, most relevant to the context of their current tasks <ref> [Maes94] </ref>. <p> However, hand-tuning such weights requires changes to a complex set of inter-related parameters. In addition, users will not feel comfortable to trust delegating message filtering and notification to a system that does not show its notification strategy clearly <ref> [Maes94] </ref>. Hence, Nomadic Radio provides a visualization of the notification levels based on the current weights and allows users to change these weights dynamically.
Reference: [Maher97] <author> Maher, Brenden. </author> <title> Navigating a Spatialized Speech Environment through Simultaneous Listening and Tangible Interactions. M.S. </title> <type> Thesis, </type> <institution> Media Arts and Sciences, MIT Media Lab, </institution> <month> Fall </month> <year> 1997. </year>
Reference-contexts: An audio landscape with directional sound sources and overlapping auditory streams (audio-braiding) provides a listening environment for browsing multiple audio sources easily <ref> [Maher97] </ref>. 4.7.1 Why use Spatial Audio in Nomadic Radio? We will first consider some key reasons why spatial and simultaneous listening can be beneficial for notification and browsing of messages on an audio-only wearable system. <p> Spatialized Audio Braiding: Several audio streams can be spatialized around a listener's head by making one stream prominent and letting it decay as another stream begins to play. Multiple audio streams such as news articles or voice mail could be braided alternatively <ref> [Maher97] </ref> at pre-defined intervals based on the urgency of the information, content attributes or via user-controlled intervals. This technique was implemented in Nomadic Radio so listeners can scan all audio messages sequentially with braided playback (discussed in section 4.7.6). http://developer.intel.com/ial/rsx/index.htm Chapter 4: Interface Design 68 2.
Reference: [Manandhar91] <author> Manandhar, </author> <title> Sanjay Activity Server: A Model for Everyday Office Activities. M.S. </title> <type> Thesis, </type> <institution> Media Arts and Sciences, MIT Media Lab, </institution> <address> June1991. References 121 </address>
Reference-contexts: Location awareness requires integration of IR-based receivers on the user's wearable PC. The Position Server also polls user activity data (on desktop machines) from the Activity Server <ref> [Manandhar91] </ref>. In the future, this data will be used to provide a location context for message notifications and asynchronous communication between nomadic users (see the future work section). 6.2 Wired Wearable vs. <p> The Position Server keeps track of the user's location throughout the day and polls user activity data from the Activity Server <ref> [Manandhar91] </ref>. RadioSpace is a Java-based client, developed by Natalia Marmasse in the Speech Interface Group, that communicates with the Position Server to provide a visual display of user activity as well as auditory cues. Nomadic Radio will utilize the same protocols as RadioSpace to communicate with the Position Server.
Reference: [Martin89] <author> Martin, G.L. </author> <title> The utility of speech input in user interfaces. </title> <journal> International Journal of Man Machine Studies, </journal> <volume> 30 </volume> <pages> 355-375, </pages> <year> 1989. </year>
Reference-contexts: An eyes-free approach, using audio-based augmentation of the physical environment, can be used to express information directly. Spoken commands provide a natural and hands-free operation. This allows the user to simultaneously perform other tasks while listening or speaking <ref> [Martin89] </ref>. Such an interface can be unobtrusive and perhaps even designed to be discreet.
Reference: [Marx95] <author> Marx, Matthew. </author> <title> Toward Effective Conversational Messaging. M.S. </title> <type> Thesis, </type> <institution> Media Arts and Sciences, MIT Media Lab, </institution> <month> June </month> <year> 1995. </year>
Reference-contexts: See figure 4.9. for examples of how attributes are spoken when messages are summarized. It is interesting to consider the effect of using synthetic speech with increased pitch or volume to convey urgency or decreased pitch to aid comprehension and emphasize the subject <ref> [Marx95] </ref>. (YHQWflfl*DUGHQfl/XQFKflDWflfl30flIRUflflPLQXWHV 0RVWflLPSRUWDQWflPHVVDJHflflIURPfl*HHNflDERXWfl5HSO"flWRflWKHVLVflGUDIW 6KRUWfl0HVVDJHflfl9RLFHflPHVVDJHflIURPfl~- /RQJfl0HVVDJHfl-fl1HZVfl6XPPDU"flDWflfl30 Chapter 4: Interface Design 62 4.5.4 Synchronizing Synthetic Speech with Audio Cues and Spatial Audio Finally, to provide effective feedback to the listener, the timing of the spoken speech feedback is paced with adequate periods of silence.
Reference: [Marx96a] <author> Marx, Matthew and Chris Schmandt. MailCall: </author> <title> Message Presentation and Navigation in a Non-visual Environment. </title> <booktitle> Proceedings of CHI 96, </booktitle> <pages> pp. 165-172, </pages> <month> April </month> <year> 1996. </year>
Reference-contexts: WildFire screens and routes incoming calls, schedules reminders and retrieves voice mail (but not email). It allows users to sort messages, yet the navigation is sequential. The discrete-word speech recognition in WildFire limits conversational style for interaction. 2.1.6 MailCall MailCall <ref> [Marx96a] </ref> is a bold attempt towards effective telephone-based conversational messaging. The system provides remote access to email and voice messages, using speech recognition and a combination of intelligent message categorization, efficient presentation, and random access navigation.
Reference: [Marx96b] <author> Marx, Matthew and Chris Schmandt. </author> <title> CLUES: Dynamic Personalized Message Filtering. </title> <booktitle> Proceedings of CSCW 96, </booktitle> <pages> pp. 113-121, </pages> <month> November </month> <year> 1996. </year>
Reference: [Muller90] <author> Muller, M. and J. Daniel. </author> <title> Toward a definition of voice documents. </title> <booktitle> Proceedings of COIS '90, </booktitle> <pages> pp. 174-182, </pages> <publisher> ACM, </publisher> <year> 1990. </year>
Reference-contexts: Generally, the user's command follows (lags behind) the playback of the target message. This can cause the wrong message to be selected for playback. Muller and Daniel <ref> [Muller90] </ref> suggest a "partially overlapping temporal window" to select the correct target.
Reference: [Mullins96] <author> Mullins, Atty T. AudioStreamer: </author> <title> Leveraging The Cocktail Party Effect for Efficient Listening. M.S. </title> <type> Thesis, </type> <institution> Media Arts and Sciences, MIT Media Lab, </institution> <month> February </month> <year> 1996. </year>
Reference-contexts: Speech is sequential and exists only temporally; the ear cannot browse around a set of recordings the way the eye can scan a screen of text and images. Hence, techniques such as interactive skimming [Arons93], non-linear access and indexing [Stifleman93, Wilcox97] and audio spatialization <ref> [Mullins96] </ref> must be considered for browsing audio. Design for wearable audio computing requires (1) attention to the affordances and constraints of speech and audio in the interface (2) coupled with the physical form of the wearable itself. <p> The effective use of spatial layout can be used to aid auditory memory. The AudioStreamer [Schmandt95] <ref> [Mullins96] </ref> detects the gesture of head movement towards spatialized audio-based news sources to increase the relative gain of the source allowing simultaneous browsing and listening of several news articles. <p> It should be noted that this is an experimental technique and has not been readily evaluated with listeners. increases as it zooms, reaching 1.3x speed before slowing back down as it fades out. Chapter 4: Interface Design 76 Time compression was incorporated in AudioStreamer <ref> [Mullins96] </ref> as an experiment where a selected stream could be played gradually up to 2x times faster until the end of audio was reached. AudioStreamer did not foreground the audio stream during time-compression, and there was no gradual slow down to the original playback rate.
Reference: [Mynatt98] <author> Mynatt, E.D., Back, M., Want, R. Baer, M., and Ellis J.B. </author> <title> Designing Audio Aura. </title> <booktitle> Proceedings of CHI 98, </booktitle> <month> April </month> <year> 1998. </year>
Reference-contexts: Wearable auditory interfaces can be used to enhance an environment with timely information and provide a sense of peripheral awareness <ref> [Mynatt98] </ref> of people and background events. http://www.pitneybowes.com/pbi/whatsnew/releases/workers_overwhelmed.htm Chapter 1: Introduction 18 This thesis demonstrates interface techniques developed for wearable access and awareness of timely information in a nomadic physical environment. <p> A speech-enabled web browser allows users to access local and remote documents through a wireless link. 2.3.7 Audio Aura An augmented audio reality project at Xerox PARC, Audio Aura <ref> [Mynatt98] </ref>, explored the use of background auditory cues to provide serendipitous information coupled with people's physical actions in the workplace. The project leveraged an existing infrastructure of active badges and distributed IR sensors along with wireless headphones to deliver audio cues to people in the workplace.
Reference: [Norman94] <author> Norman, Donald A. </author> <title> "The Power of Representation in Things That Make Us Smart: Defending Human Attributes in the Age of the Machine. </title> <publisher> Addison-Wesley Pub Co. </publisher> <month> May </month> <year> 1994, </year> <pages> pp. 43-75. </pages>
Reference-contexts: The Message Swatch has a simple visual display which allows users to understand its chronological representation at a glance. The spatial display of the watch dial has additive affordances <ref> [Norman94] </ref> which allows representation of different message attributes using geometric and color attributes of the display (see figure 4.21). Messages are displayed based on their time of arrival, importance and duration. Concentric lines overlaid on the dial represent messages with varying attributes such as line thickness and brightness.
Reference: [Nortel94] <institution> SoundBeam Design Qualification. Proprietary Report TL940041, Nortel, </institution> <month> September </month> <year> 1994. </year>
Reference-contexts: For Nomadic Radio, several solutions were considered before adopting a design based on research at Nortel. We will first discuss results from their design evaluations. 3.3.1 Nortel's Evaluation of Personal Listening Devices Nortel conducted an evaluation 12 <ref> [Nortel94] </ref> of listening devices such as headsets (corded and cordless), speaker phones, cordless phones, and single-ear earphones (the Jabra). These were compared with a variety of solutions for hands-free devices worn around the neck, based on what would come to be called the SoundBeam Neckset.
Reference: [Papp96] <author> Papp, Albert and Meera M. Blattner, </author> <title> Dynamic Presentation of Asynchronous Auditory Output, </title> <booktitle> Proceedings of ACM Multimedia96 , November 1996, </booktitle> <address> pp.109-116. </address>
Reference-contexts: These parameters are weighted for high, medium or low interruption levels set by the user. As messages arrive, an appropriate notification level is selected based on a computed average of these weighted parameters <ref> [Papp96] </ref>. The model also computes a notification latency i.e. a period of time to wait before playing the message, after a notification is delivered. The message notifications are shown on a visual graph where they can be interactively manipulated by the user.
Reference: [Pashler98] <editor> Pashler, Harold E. </editor> <publisher> The Psychology of Attention . MIT Press, </publisher> <year> 1998. </year>
Reference-contexts: This approach implicitly reduces transaction time and cognitive overhead in using such devices over long periods of time. Chapter 5: Scaleable and Contextual Notification 84 5.3.4 Pre-cueing via Notification Latency Pre-cueing is a phenomenon that cognitively prepares listeners to open a channel of attention <ref> [Pashler98] </ref>, based on a prior warning of an event.
Reference: [Rekimoto95] <author> Rekimoto, Jun and Katashi Nagao. </author> <title> The World through the Computer: Computer Augmented Interaction with Real World Environments. </title> <booktitle> Proceedings of UIST ''95, </booktitle> <month> November 14-17, </month> <year> 1995, </year> <pages> pp. 29-38. </pages>
Reference-contexts: Benot's work served as an inspiration for the Radio Vest designed for Nomadic Radio. http://www.inx.de/~maubrey/ 11 Personal discussions with Benot Maubrey at the International Symposium of Electronic Arts (ISEA) in Chicago, September 1997. Chapter 2: Related Work 32 2.3.4 Ubiquitous Talker Ubiquitous Talker <ref> [Rekimoto95] </ref> is a camera-enabled system developed at Sony Research Labs. It provides the user information related to a recognized physical object via a display and synthesized voice.
Reference: [Rhodes97] <author> Rhodes, Bradley J. </author> <title> The Wearable Remembrance Agent: a system for augmented memory. </title> <booktitle> Proceedings of the International Symposium on Wearable Computing, IEEE, </booktitle> <month> October </month> <year> 1997, </year> <pages> pp. 123-128. </pages>
Reference-contexts: We consider visual approaches for providing timely information and recorded memories on wearables based on the user's context. We will then focus on audio-only wearable interfaces that augment the physical environment with active audio information as well as peripheral auditory cues. 2.3.1 Remembrance Agent The Remembrance Agent (RA) <ref> [Rhodes97] </ref> is a program that continuously analyzes the user's typed responses on a wearable (via a chording keyboard) to query the user's database of email and textual notes.
Reference: [Roy95] <author> Roy, Deb K. NewsComm: </author> <title> A HandHeld Device for Interactive Access to Structured Audio. M.S. </title> <type> Thesis, </type> <institution> Media Arts and Sciences, MIT Media Lab, </institution> <month> June </month> <year> 1995. </year>
Reference-contexts: A preview for an audio source such as a voice message or news broadcast plays a fifth of the message in the foreground (described in section 4.7.6). A better representation requires a structural description of the audio, based on annotated or automatically determined pauses in speech, speaker changes <ref> [Roy95] </ref> and topic changes [Stifleman97]. Such a representation is considered an auditory thumbnail similar in function to its visual counterpart. A preview for a structured voice message allows a listener to hear pertinent aspects such as name of caller and phone number.
Reference: [Roy96] <author> Roy, Deb K. and Chris Schmandt. NewsComm: </author> <title> A HandHeld Interface for Interactive Access to Structured Audio. </title> <booktitle> Proceedings of CHI 96, </booktitle> <month> April </month> <year> 1996, </year> <pages> pp. 173-180. </pages>
Reference-contexts: Personal Digital Assistants (PDAs) offer the benefit of personal applications in a smaller size. However, they generally utilize pen-based graphical user interfaces which are not ideal when the users hands and eyes are busy. Hand-held <ref> [Stifelman93, Roy96] </ref> and mobile [Stifelman96, Wilcox97] audio devices with localized computing and richer interaction mechanisms certainly point towards audio interfaces and networked applications for a new personal information platform, Wearable Audio Computing (WAC) [Roy97].
Reference: [Roy97] <author> Roy, Deb K., Nitin Sawhney, Chris Schmandt, and Alex Pentland. </author> <title> Wearable Audio Computing: A Survey of Interaction Techniques Perceptual Computing Technical Report No. </title> <type> 434, </type> <institution> MIT Media Lab, </institution> <month> April </month> <year> 1997. </year>
Reference-contexts: Hand-held [Stifelman93, Roy96] and mobile [Stifelman96, Wilcox97] audio devices with localized computing and richer interaction mechanisms certainly point towards audio interfaces and networked applications for a new personal information platform, Wearable Audio Computing (WAC) <ref> [Roy97] </ref>.
Reference: [Rudnicky96] <author> Rudnicky, Alexander, Stephen Reed and Eric Thayer. SpeechWear: </author> <title> A mobile speech system. </title> <booktitle> Proceedings of ICSLP '96, </booktitle> <year> 1996. </year>
Reference-contexts: This is a early implementation of a wearable audio system which stores only pre-defined audio information to augment a physical environment. This system did not utilize the listener's context or prior listening patterns to selectively filter or present information. 2.3.6 SpeechWear SpeechWear <ref> [Rudnicky96] </ref> is a mobile speech system developed at CMU which enabled users to perform data entry and retrieval using an interface based on speech recognition and synthesis.
Reference: [Sawhney97] <author> Sawhney, Nitin. </author> <title> Situational Awareness from Environmental Sounds. Project Report for Pattie Maes, </title> <publisher> MIT Media Lab, </publisher> <month> June </month> <year> 1997. </year> <note> http://www.media.mit.edu/~nitin/papers/Env_Snds/EnvSnds.html References 122 </note>
Reference-contexts: Two different approaches were developed for extracting contextual cues from environmental sounds. An earlier approach based on prior work at the Machine Listening Group at the MIT Media Lab [Arnaud95] was designed to extract and classify features from 5 pre-defined classes of sounds in the environment <ref> [Sawhney97] </ref>. The focus of the work was on distinguishing longer-duration environmental sounds into pre-defined classes using near real-time classification techniques. Environmental sounds were recorded via DAT and segmented into training and test data-sets.
Reference: [Schmandt93] <author> Schmandt, Chris. Phoneshell: </author> <title> The Telephone as a Computer Terminal. </title> <booktitle> Proceedings of ACM Multimedia 93, </booktitle> <pages> pp. 373-382, </pages> <address> New York, </address> <month> Aug </month> <year> 1993. </year>
Reference-contexts: These devices offer an extremely low-bandwidth for communication, and subsequently the interface does not afford rich delivery of information content. Telephones are ubiquitous and cellular services offer mobility. Computer-based telephony services, such as Phoneshell <ref> [Schmandt93] </ref>, offer subscribers integrated access to information such as email, voice mail, news and scheduling, using digitized audio and synthesized speech. However, telephones primarily operate on a synchronous model of communication, requiring availability of both parties. <p> A key aspect of the system was easy interruptability by the user during speech-only interaction. 2.1.2 Phoneshell A nomadic computing environment was developed at the Speech Interface Group to provide remote access to a set of desktop applications via telephony. Phoneshell <ref> [Schmandt93] </ref> allows subscribers to access email, voice mail, calendar, rolodex and a variety of news, weather and traffic reports. Interaction is provided via touch-tones (DTMF). Users hear messages and feedback via synthetic speech and audio playback. <p> In the Speech Interface group at the MIT Media Lab, we have developed an environment <ref> [Schmandt93] </ref> which allows subscribers to access information such as email, voice messages, weather, hourly news and calendar events using a variety of interfaces such as desktops, telephones, pagers, fax, etc.
Reference: [Schmandt94a] <author> Schmandt, Chris. </author> <title> Multimedia Nomadic Services on Todays Hardware. </title> <journal> IEEE Network, </journal> <note> September/October 1994, pp12-21. </note>
Reference-contexts: A partial solution to information overload is to give people timely and filtered information, most relevant to the context of their current tasks [Maes94]. Seamless nomadic access to personal information and communication services <ref> [Schmandt94a] </ref> should be made available to users in a passive and unobtrusive manner based on their level of attention and interruptability. 1.1 Nomadic Access to Timely Information Simple devices, such as pagers, provide a convenient form of alerting users to remote information.
Reference: [Schmandt94b] <author> Schmandt, Chris. </author> <title> Voice Communication with Computers: Conversational Systems, </title> <publisher> Van Nostrand Reinhold, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: Yet, it can provide an alternative channel of interaction when the user's hands and eyes are busy elsewhere, without having to shift their gaze to the screen <ref> [Schmandt94b] </ref>. For nomadic access to information and interface control, speech provides a natural and convenient mechanism. Nomadic access includes use of telephony, access while driving, and on portable or wearable devices such as Nomadic Radio. <p> Template Matching vs. Sub-Word Analysis Most systems today rely on template matching. Here, a set of template or models describing each word to be recognized, are created using a representation of speech used by the recognized <ref> [Schmandt94b] </ref>. These templates form the recognizer's vocabulary, acting as reference models with which to compare spoken input. Pre-processing on spoken words determines word boundaries, and the most similar template is selected if the difference is minor enough to accept the word. <p> Avoiding Coarticulation Short phrases require users to speak quickly without pausing and this can cause coarticulation. Coarticulation is the process whereby the pronunciation of a phoneme changes as a function of its surrounding phonemes <ref> [Schmandt94b] </ref>. Interaction between coarticulation and lexical stress in a speaker's voice causes unstressed syllables to change more easily. This leads to vowel reduction where the vowel of the unstressed syllable is shortened and turned into a schwa. <p> The perception of synthetic speech does impose greater demand on short-term memory more than recorded speech [Luce83]. Difficulty in listening to synthetic speech may interfere with the listener's tasks and even remembering the information being spoken <ref> [Schmandt94b] </ref>. Such prompts must be designed carefully to convey important information using both concise and comprehensible utterances. 4.5.2 Design of Speech Prompts in Nomadic Radio Synthetic speech is most appropriate if relatively short prompts are used.
Reference: [Schmandt95] <author> Schmandt, Chris and Atty Mullins. AudioStreamer: </author> <title> Exploiting Simultaneity for Listening. </title> <booktitle> Proceedings of CHI 95, </booktitle> <pages> pp. 218-219, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: The effective use of spatial layout can be used to aid auditory memory. The AudioStreamer <ref> [Schmandt95] </ref> [Mullins96] detects the gesture of head movement towards spatialized audio-based news sources to increase the relative gain of the source allowing simultaneous browsing and listening of several news articles.
Reference: [Starner97a] <author> Starner, Thad, Steve Mann, Bradley Rhodes, Jeffery Levine, Jennifer Healey, Dana Kirsch, Rosalind Picard, and Alex Pentland, </author> <title> Augmented Reality through Wearable Computing. </title> <journal> Presence, </journal> <volume> Vol. 6, No. 4, </volume> <month> August </month> <year> 1997, </year> <pages> pp. 386-398. </pages>
Reference-contexts: Hence, the RA shows the benefits of a wearable context-driven augmented reality. 2.3.2 Augmented Hyper-links and Audio/Video Memories in DyPERS In one wearable approach at the MIT Media Lab <ref> [Starner97a] </ref>, visual tags in the wearer's physical environment identified objects and rendered the associated text, graphics or video content on the user's visual field. Such physical hypertext links used location context to provide instructions on use, history or information left by previous users. <p> A recent approach called DyPERS [Jebara98], utilizes vision techniques to provide audio/video memories of a user's experience on a museum visit. A key innovation is the use of real-time image recognition techniques to trigger the user's own recordings related to physical objects in the environment. In a recent paper <ref> [Starner97a] </ref>, researchers suggest the use of sensors and user modeling to allow wearables to infer when users would not wish to be disturbed by incoming messages. However, the system should understand enough information context to alert the user of emergency messages immediately.
Reference: [Starner97b] <author> Starner, Thad and Dana Kirsch. </author> <title> The locust swarm: An environmentally-powered, networkless location and messaging system. </title> <booktitle> Proceedings of the International Symposium on Wearable Computing, IEEE, </booktitle> <month> October </month> <year> 1997. </year>
Reference-contexts: It keeps track of the user's position throughout the day using a distributed IR location system at the Media Lab, called the Locust Swarm <ref> [Starner97b] </ref>. Location awareness requires integration of IR-based receivers on the user's wearable PC. The Position Server also polls user activity data (on desktop machines) from the Activity Server [Manandhar91]. <p> Location awareness can be added to Nomadic Radio by integrating IR (Infrared) receivers on the wearable device. These IR receivers can provide positioning data to a Position Server, being developed by John Holmes in the Speech Group, using the Locust Swarm <ref> [Starner97b] </ref>, a distributed IR location system at the MIT Media Lab. The Position Server keeps track of the user's location throughout the day and polls user activity data from the Activity Server [Manandhar91].
Reference: [Starner97c] <author> Starner, T. </author> <year> (1997). </year> <title> Lizzy Wearable Computer Assembly Instructions. </title> <publisher> MIT Media Laboratory. </publisher> <address> http://wearables.www.media.mit.edu/projects/wearables/ </address>
Reference-contexts: Chapter 6: System Architecture 110 6.2.1 Wired Wearable Audio Several alternatives were considered as a wearable computing platform for Nomadic Radio, based on in-house hardware and recently introduced commercial systems. An effort by lead by Thad Starner at the Media Lab created the Lizzy 18 platform <ref> [Starner97c] </ref>, which is predominantly used by most wearable users here. The Lizzy is generally assembled with a 486 or 586-based processor, 16 MB RAM and 1GB harddrive (figure 6.4). Specialized PC-104 cards can be installed for audio and video I/O.
Reference: [Stifelman92] <author> Stifelman, Lisa J. VoiceNotes: </author> <title> An Application for a Voice-Controlled HandHeld Computer. M.S. </title> <type> Thesis, </type> <institution> Media Arts and Sciences, MIT Media Lab, </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: Dynomite utilizes user-defined ink properties and keywords to dynamically generate new structured views of the recorded information. Synchronized audio is selectively captured and edited via user's highlighting of the audio for minimal storage on mobile devices. 2.2.4 VoiceNotes VoiceNotes <ref> [Stifelman92, 93] </ref> was designed as an application for a voice-controlled hand-held computer which allows creation, management and retrieval of user-authored voice notes. The user interacts with the system using a combination of button and speech input, and feedback is provided via recorded speech and non-speech audio cues. <p> For example parameters for spatial audio playback or synthetic speech feedback are changed in a similar manner. Nomadic Radio adopted modeless interaction from its inception. This is based on related work by Lisa Stifelman on the moded vs. modeless interface in VoiceNotes <ref> [Stifelman92] </ref>. In Nomadic Radio, this ensures a scaleable approach as new information services or categories are added by developers or subscribed to by users in the future. <p> Hence, commands like "move back", "play message preview" or "remove this message" can apply to email, voice mail, news or calendar events. The following are some of the considerations taken for designing and selecting a vocabulary for Nomadic Radio: Selecting Single Word vs. Phrase Commands In VoiceNotes <ref> [Stifelman92] </ref>, single word commands are used due to the nature of the speech recognizer. The discrete word recognizer requires users to pause between words. Commands such as "Play -listname-" are eliminated in preference to simply "-listname-". <p> Speech input is more suitable for "coarse" navigation commands <ref> [Stifelman92] </ref> such as "Go to my voice mail," whereas tactile input is better suited for "fine" grained commands such as "move back" or "speed-up audio". Tactile input for Nomadic Radio is implemented via a numeric keypad connected to a networked Toshiba Libretto 50CT mini-notebook PC (see in figure 4.5). <p> Finally, an audio cue is also heard if the application loses its connection with the Nomadic Client. Audio cues can be turned off by the user, if desired. 4.4.4 Hybrid Interaction The addition of tactile input provides a hybrid speech-button interface <ref> [Stifelman92] </ref> that allows both modalities to work well in conjunction with one another. Similar commands issued by either speech or tactile input provides the same feedback to the user to maintain consistency.
Reference: [Stifelman93] <author> Stifelman, Lisa J., Barry Arons, Chris Schmandt, and Eric A. Hulteen. VoiceNotes: </author> <title> A Speech Interface for Hand Held Voice Notetaker. </title> <booktitle> Proceedings of INTERCHI 93, </booktitle> <address> New York, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: Personal Digital Assistants (PDAs) offer the benefit of personal applications in a smaller size. However, they generally utilize pen-based graphical user interfaces which are not ideal when the users hands and eyes are busy. Hand-held <ref> [Stifelman93, Roy96] </ref> and mobile [Stifelman96, Wilcox97] audio devices with localized computing and richer interaction mechanisms certainly point towards audio interfaces and networked applications for a new personal information platform, Wearable Audio Computing (WAC) [Roy97].
Reference: [Stifelman94] <author> Stifelman, Lisa J. </author> <title> The Cocktail Party Effect in Auditory Interfaces: A Study of Simultaneous Presentation. </title> <type> MIT Media Lab Technical Report, </type> <month> September </month> <year> 1994. </year>
Reference-contexts: A good model of the head-related transfer functions (HRTF) permits effective localization and externalization of sound sources [Wenzel92]. Yet the cognitive load of listening to simultaneous channels increases with the number of channels. Experiments show that increasing the number of channels beyond three causes a degradation in comprehension <ref> [Stifelman94] </ref>. Bregman claims that stream segregation is better when frequency separation is greater between sound streams [Bregman90].
Reference: [Stifelman96] <author> Stifelman, Lisa J. </author> <title> Augmenting Real-World Objects: A Paper-Based Audio Notebook. </title> <booktitle> Proceedings of CHI 96, </booktitle> <month> April </month> <year> 1996. </year>
Reference-contexts: Personal Digital Assistants (PDAs) offer the benefit of personal applications in a smaller size. However, they generally utilize pen-based graphical user interfaces which are not ideal when the users hands and eyes are busy. Hand-held [Stifelman93, Roy96] and mobile <ref> [Stifelman96, Wilcox97] </ref> audio devices with localized computing and richer interaction mechanisms certainly point towards audio interfaces and networked applications for a new personal information platform, Wearable Audio Computing (WAC) [Roy97]. <p> Users can access a portion of audio data by gesturing at an associated note. 2.2.2 Audio Notebook The Audio Notebook <ref> [Stifelman96] </ref> is a paper-based augmented audio system (see figure 2.2) that allows a user to capture and access an audio recording synchronized with handwritten notes and page turns. The Audio Notebook also structures the audio via topic-changes and phrase-breaks, which is particularly helpful for browsing audio from classroom lectures.
Reference: [Voor65] <author> Voor, J.B. and J.M. Miller. </author> <title> The effect of practice upon the comprehension of time-compressed speech. </title> <booktitle> Speech Monographs, </booktitle> <pages> 32(452-455), </pages> <year> 1965. </year>
Reference-contexts: Research suggests that presenting the audio at over twice its original playback rate, provides too little of the signal to be accurately comprehensible [Heiman86]. Comprehension of time-compressed speech does increase as listeners hear it more frequently since they tend to adapt to it quickly <ref> [Voor65] </ref>. Novice users quickly adapt to a compression rate of 50%. But with more training, much higher compression is possible. In future versions of Nomadic Radio, SOLA-based compression will provide more effective time compression for previewing messages.
Reference: [Wallach40] <author> Wallach, H. </author> <year> 1940. </year> <title> The Role of Head Movements and Vestibular and Visual Cues in Sound Localization. </title> <journal> Journal of Experimental Psychology , 27(4) </journal> <pages> 339-368. </pages>
Reference-contexts: Elevation judgments and front-back differentiation are more likely to degrade with non-individualized HRTFs [Wenzel93], yet it is also clear that head movement plays a dominant role in resolving front/back confusions <ref> [Wallach40] </ref>.
Reference: [Watts96] <author> Watts, Jennifer C., David D. Woods, James M. Corban, and Emily S. Patterson. </author> <title> Voice Loops as Cooperative Aids in Space Shuttle Mission Control. </title> <booktitle> Proceedings of CSCW96, </booktitle> <pages> pp. 48-247. </pages> <month> November </month> <year> 1996. </year>
Reference: [Wenzel92] <author> Wenzel, </author> <title> E.M. Localization in virtual acoustic displays, </title> <journal> Presence, </journal> <volume> 1, 80, </volume> <year> 1992. </year>
Reference-contexts: A good model of the head-related transfer functions (HRTF) permits effective localization and externalization of sound sources <ref> [Wenzel92] </ref>. Yet the cognitive load of listening to simultaneous channels increases with the number of channels. Experiments show that increasing the number of channels beyond three causes a degradation in comprehension [Stifelman94]. Bregman claims that stream segregation is better when frequency separation is greater between sound streams [Bregman90].
Reference: [Whittaker94] <author> Whittaker, S. P. Hyland, and M. Wiley. Filochat: </author> <title> Handwritten Notes Provide Access to Recorded Conversations. </title> <booktitle> Proceedings of CHI 94, </booktitle> <pages> pp. 271-279, </pages> <year> 1994. </year>
Reference-contexts: Hand-held audio devices allow voice recordings to serve as memory aids and also deliver structured and personalized audio programs. Several efforts have recently been initiated to provide information and communication services to drivers in automobiles. 2.2.1 FiloChat FiloChat <ref> [Whittaker94] </ref> indexes speech recording to pen-strokes of handwritten notes taken with a digital notebook.
Reference: [Wilcox97] <author> Wilcox, Lynn D., Bill N. Schilit, and Nitin Sawhney. Dynomite: </author> <title> A Dynamically Organized Ink and Audio Notebook. </title> <booktitle> Proceedings of CHI 97, </booktitle> <month> March </month> <year> 1997, </year> <pages> pp. 186-193. </pages>
Reference-contexts: Personal Digital Assistants (PDAs) offer the benefit of personal applications in a smaller size. However, they generally utilize pen-based graphical user interfaces which are not ideal when the users hands and eyes are busy. Hand-held [Stifelman93, Roy96] and mobile <ref> [Stifelman96, Wilcox97] </ref> audio devices with localized computing and richer interaction mechanisms certainly point towards audio interfaces and networked applications for a new personal information platform, Wearable Audio Computing (WAC) [Roy97]. <p> Speech is sequential and exists only temporally; the ear cannot browse around a set of recordings the way the eye can scan a screen of text and images. Hence, techniques such as interactive skimming [Arons93], non-linear access and indexing <ref> [Stifleman93, Wilcox97] </ref> and audio spatialization [Mullins96] must be considered for browsing audio. Design for wearable audio computing requires (1) attention to the affordances and constraints of speech and audio in the interface (2) coupled with the physical form of the wearable itself. <p> The Audio Notebook also structures the audio via topic-changes and phrase-breaks, which is particularly helpful for browsing audio from classroom lectures. Poticio was previously code-named "Serengeti" by General Magic. http://www.genmagic.com/portico/portico.html Chapter 2: Related Work 27 browse an audio recording in conjunction with notes written on paper. 2.2.3 Dynomite Dynomite <ref> [Wilcox97] </ref> is a portable electronic notebook which captures, searches and organizes handwritten and digital audio notes. Dynomite utilizes user-defined ink properties and keywords to dynamically generate new structured views of the recorded information.
Reference: [Yankelovich94] <author> Yankelovich, Nicole. </author> <title> Talking vs. Taking: Speech Access to Remote Computers. </title> <booktitle> Proceedings of CHI 94, </booktitle> <address> Boston, MA, </address> <month> April 24-28, </month> <year> 1994. </year>
Reference-contexts: However, since Chatter does not verify the output of the recognizer, it is prone to erroneous actions such as deleting messages by mistake or unpredictably hanging up on the user during a session. 2.1.4 SpeechActs SpeechActs <ref> [Yankelovich94] </ref> combines the conversational style of Chatter with the broad functionality of Phoneshell, providing a speech interface to email, calendar, stock quotes and weather forecasts. It allows users to access messages by number ("read message 13") and verifies irreversible requests such as "delete message".
References-found: 70

