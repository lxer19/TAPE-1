URL: http://www.cs.rice.edu:80/~andras/Papers/icassp97.ps
Refering-URL: http://www.cs.rice.edu:80/~andras/pub.html
Root-URL: 
Email: kornai@almaden.ibm.com  
Title: AN EXPERIMENTAL HMM-BASED POSTAL OCR SYSTEM  
Author: Andras Kornai 
Address: 650 Harry Road, San Jose, CA 95120  
Affiliation: IBM Almaden Research Center  
Abstract: It is almost universally accepted in speech recognition that phone- or word-level segmentation prior to recognition is neither feasible nor desirable, and in the dynamic (pen-based) handwriting recognition domain the success of segmentation-free techniques points to the same conclusion. But in image-based handwriting recognition, this conclusion is far from being firmly established, and the results presented in this paper show that systems employing character-level presegmenta-tion can be more effective, even within the same HMM paradigm, than systems relying on sliding window feature extraction. We describe two variants of a Hidden Markov system recognizing handwritten addresses on US mail, one with presegmentation and one without, and report results on the CEDAR data set. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J.K. Baker, </author> <title> "Stochastic modeling for automatic speech understanding" Reprinted in A. Waibel and Kai-Fu Lee (eds) Readings in Speech recognition, </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo CA, </address> <year> 1990, </year> <pages> 297-307 </pages>
Reference-contexts: The realization that prior to recognition it is virtually impossible to segment the signal into phone-size units led to the broad acceptance of Hidden Markov techniques in the speech domain, since HMMs effectively delay segmentation decisions until the context checking stage <ref> [1] </ref>. In the OCR domain the issue is more complex. Historically, commercial products were first deployed for machine print and typewritten material, where segmentation is considerably easier than in speech.
Reference: [2] <author> J.R. Bellegarda, D. Nahamoo, K.S. Nathan and E.J. Bellegarda, </author> <title> "Supervised Hidden Markov Modeling for On-line Handwriting Recognition" IEEE Proc. </title> <booktitle> ICASSP, Adelaide 1994, </booktitle> <volume> Vol 5, </volume> <pages> 149-152 </pages>
Reference-contexts: This yields a sequence of 24-dimensional vectors which are reduced by principal component analysis to 12-16 dimensions <ref> [2] </ref>. The main additions to our earlier feature extraction method which was successful in a bank check recognition system [10] are the initial global height normalization and the lowpass filtering of local height. 3.
Reference: [3] <author> M.R. Bokser, </author> <title> "State of the Art in a Commercial OCR System: A Retrospective View" Paper presented at the IS&T/SPIE Symposium on Electronic Imaging, </title> <address> San Jose CA 1996 </address>
Reference-contexts: Historically, commercial products were first deployed for machine print and typewritten material, where segmentation is considerably easier than in speech. Only in the last few years has the reliability of isolated letter recognition reached the level where errors of segmentation became the dominant source of error <ref> [3] </ref>.
Reference: [4] <author> T.H. </author> <title> Crystal and A.S. House, "Segmental durations in connected speech signals: </title> <booktitle> current results" JASA 83 1988, </booktitle> <pages> 1553-1573 </pages>
Reference-contexts: For most characters we use Bakis models, with a self-loop for each state, a step transition to the next state, and a jump transition to the second state following it. If we enrich the model with a silent input state with transitions to any subsequent state <ref> [4] </ref>, it becomes possible to fully absorb arbitrary width variation [7], and in certain classes, such as u or s; width variation was so extreme that we found it advantageous to do so.
Reference: [5] <author> A.J. Elms, </author> <title> The representation and recognition of text using Hidden Markov Models. </title> <type> University of Surrey PhD Thesis, </type> <year> 1996 </year>
Reference-contexts: These results, though 5% better than the only comparable HMM results reported in the literature <ref> [5] </ref>, are not competitive with ANN results on similar data. Therefore we proceeded on the assumption that zipcodes will actually be recognized by a traditional OCR system, and HMMs will only be used for city, street, state, and addressee recognition.
Reference: [6] <author> J.J. Hull, </author> <booktitle> "Database for handwritten word recognition research" IEEE PAMI 16 1994, </booktitle> <pages> 550-554 </pages>
Reference-contexts: For the sliding window method, the line image is first height-normalized to 64 pixels. Since the standard algorithms for scaling a bilevel image yield greyscale output, some form of binarization becomes necessary even for those images that start out as binary on the CEDAR CD-ROM <ref> [6] </ref>. We investigated two scaling algorithms, bilinear and bicu-bic resampling. Linear resampling was faster, but only marginally, and the results were indistinguishable as far as later stages and overall recognition accuracy are concerned. We also investigated two different binariza-tion methods, global thresholding and local (adaptive) thresholding [11]. <p> This bitrate, being an order of magnitude larger than that required for pen- or tablet-based recognition, offers a rough measure of the difficulty of extracting dynamic information from an image. 4. TRAINING AND TESTING The CEDAR CD-ROM <ref> [6] </ref> is organized with the requirements of the classical presegmentation-based OCR systems in mind: there are plenty of isolated character training data, often isolated and truthed by hand, and most files embody presegmentation at least at the word level.
Reference: [7] <author> A. </author> <title> Kornai Formal Phonology. </title> <publisher> Garland Publishing, </publisher> <address> New York, </address> <year> 1995 </year>
Reference-contexts: If we enrich the model with a silent input state with transitions to any subsequent state [4], it becomes possible to fully absorb arbitrary width variation <ref> [7] </ref>, and in certain classes, such as u or s; width variation was so extreme that we found it advantageous to do so.
Reference: [8] <author> A. Kornai and S.D. Connell, </author> <title> "Statistical Zone Finding" IEEE Proc. </title> <booktitle> 13th ICPR, Vienna 1996, </booktitle> <volume> Vol III, </volume> <pages> 818-822 </pages>
Reference-contexts: Such selective systems, typically rejecting over a third of their input, have been deployed by large postal service throughout the world. In this paper we will ignore earlier stages of processing up to and including the modules that find the address block and separate the lines <ref> [8] </ref>, noting only that (1) these steps can be performed on a drastically subsampled image, containing no more than 1 pixel for every 8 by 8 region of the original image given at 200-240 ppi resolution and that (2) establishing the baseline of writing, including line-by-line skew, is more important than
Reference: [9] <author> A. Kornai, K.M. Mohiuddin and S.D. Connell, </author> <title> "An HMM-based legal amount field OCR system for checks" IEEE Proc. </title> <journal> SMC, Vancouver, </journal> <volume> BC 1995 Vol 3, </volume> <pages> 2800-2805 </pages>
Reference-contexts: In conventional systems, the remaining stages of word and character-level segmentation, recognition, and postprocessing are disjoint modules arranged in a cascade, which makes late rejection and sending the data upstream for re-evaluation by earlier modules a major architectural challenge. Our earlier experiments on isolated character data <ref> [9] </ref> demonstrated that given perfect segmentation Artificial Neural Nets (ANNs) are more efficient recognition devices than HMMs. On the other hand, ANNs are very inefficient as segmentation devices. <p> In the bank check domain, where most of the characters in the legal amount field are connected, we found the opposite result: heuristic segmentation schemes based on connected components and vertical projections were considerably worse than sliding window systems <ref> [9] </ref>. Taken together, the zipcode and the bank check results imply that being isolated vs. being connected is a fundamental property of character image data, and OCR systems need to be tuned to this property just as they need to be tuned to other characteristics of the data domain.
Reference: [10] <author> A. Kornai, K.M. Mohiuddin and S.D. Con-nell, </author> <title> "Recognition of cursive writing on personal checks" Proc. </title> <booktitle> 5th IWFHR, Essex 1996, </booktitle> <pages> 373-378 </pages>
Reference-contexts: In Section 2 we describe the segmentation-free (sliding window) feature extraction method used in our experiments and else-where <ref> [10] </ref>, and in Section 3 we describe another, connected component based method. The results obtained by the two are compared in Section 4 and conclusions are offered in Section 5. 2. <p> This yields a sequence of 24-dimensional vectors which are reduced by principal component analysis to 12-16 dimensions [2]. The main additions to our earlier feature extraction method which was successful in a bank check recognition system <ref> [10] </ref> are the initial global height normalization and the lowpass filtering of local height. 3. FEATURE EXTRACTION BY PRESEGMENTATION In the zipcode field, numbers are rarely touching, and one field, such as state, rarely touches another, such as city or zip.
Reference: [11] <author> K.M. Mohiuddin and J. Mao, </author> <title> "A Comparative Study of Different Classifiers for Handprinted Character Recognition" Pattern Recognition in Practice IV, </title> <booktitle> 1994, </booktitle> <pages> 437-448 </pages>
Reference-contexts: We investigated two scaling algorithms, bilinear and bicu-bic resampling. Linear resampling was faster, but only marginally, and the results were indistinguishable as far as later stages and overall recognition accuracy are concerned. We also investigated two different binariza-tion methods, global thresholding and local (adaptive) thresholding <ref> [11] </ref>. Here the impact was marked: local thresholding was considerably better than global.
Reference: [12] <author> H. Takahashi, </author> <title> "A Neural Net OCR Using Geometrical and Zonal-pattern Features" Proc. </title> <booktitle> 1st IC-DAR, </booktitle> <year> 1991, </year> <pages> 821-828 </pages>
Reference-contexts: A feature vector is computed for each frag using the contour direction features described in <ref> [12] </ref>. Given that the goal of the algorithm is to preseg-ment characters, it is not surprising that for the majority of characters it creates a single frag, and therefore a single feature vector.
Reference: [13] <institution> W.A. Woods et al "Speech understanding systems: final technical progress report" Bolt Beranek and Newman Inc. </institution> <type> Report 3438, </type> <address> Cambridge MA, </address> <year> 1976 </year>
Reference-contexts: 1. INTRODUCTION Any approach to speech and handwriting recognition must take into account that the signal is composed from a succession of alphabetic units (phonemes or graphemes). In the early work on speech recognition <ref> [13] </ref> as well as in character recognition this understanding led to systems that divided the overall recognition task into three separate tasks: segmentation, recognition, and postprocessing (context checking), performed in a cascade fashion.
References-found: 13

