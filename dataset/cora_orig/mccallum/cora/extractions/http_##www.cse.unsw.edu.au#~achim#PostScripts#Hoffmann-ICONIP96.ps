URL: http://www.cse.unsw.edu.au/~achim/PostScripts/Hoffmann-ICONIP96.ps
Refering-URL: http://www.cse.unsw.edu.au/~achim/index.html
Root-URL: http://www.cse.unsw.edu.au
Title: On Integrating Domain Knowledge into Reinforcement Learning  
Author: Achim Hoffmanny, Bernd Freiery and 
Note: Student at the Australian Graduate  
Address: Sydney, Australia  Sydney, Australia  
Affiliation: School of Computer Science and Engineering, University of New South Wales  School of Management, University of New South Wales  
Abstract: Reinforcement learning attracted increasing interest in recent years. While reinforcement learners proved successful for certain problems with comparably small system state spaces, they tend to have severe problems when the system state space is larger. This paper introduces a method for integrating domain knowledge into the process of Q-learning, in order to allow significantly faster learning and good scale-up behavior for larger state spaces. We present experimental results in the domain of a simulated pole-and-cart system indicating the applicability and usefulness of the approach. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. W. Anderson. </author> <title> Strategy learning with multilayer connectionist representations. </title> <booktitle> In Proceedings of the 4 th International Conference on Machine Learning, </booktitle> <pages> pages 103-114. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1987. </year>
Reference-contexts: To overcome this shortcoming, different methods for coping with nonlinear systems have been proposed. Among them are Reinforcement Learning approaches, see e.g. [7, 11, 15] or recent NIPS, ICML or IJCAI proceedings for a number of papers, as well as Neural Networks, e.g. <ref> [1] </ref>, and Genetic Algorithms, e.g. [14]. All these approaches face the problem of credit assignment: A control strategy can only be evaluated as a whole. If a strategy proves unsatisfactory, it is not clear how to alter the strategy to obtain improved performance. <p> Usually time intervals of a 1 50 sec. are considered. A system state is described by the current position x of the cart on the track, its velocity _x along the track, the angle of the pole and the pole's angle velocity _ . In many papers, e.g. <ref> [1, 9, 8] </ref>, this problem was used for experiments with the a state space representation of the system based on the division of the ranges of x, _x, into 3 intervals and 6 intervals for _ , resulting in 162 states.
Reference: [2] <author> A. Barto, R. Sutton, and C. Anderson. </author> <title> Neuronlike adaptive elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal>
Reference-contexts: There has been a variety of different reinforcement learning techniques proposed in the literature; a major class of such algorithms considered today is the class of temporal difference learners. Temporal difference learners Early work on temporal difference learners are found in <ref> [16, 2] </ref>. These temporal difference learners were all based on an evaluation of system states, where for each system state an estimate of the sum of all future rewards was determined. Q-learning, as it will be extended in this paper, was introduced in [15].
Reference: [3] <author> I. Bratko. </author> <title> Deriving qualitative control for dynamic systems. </title> <editor> In K. Furukawa and S. Muggleton, editors, </editor> <booktitle> Machine Intelligence and Inductive Learning. </booktitle> <institution> Oxford University Press. (new series of Machine Intelligence), </institution> <note> to appear. </note>
Reference-contexts: Recently, the problem of forming the class of considered control strategies by qualitative knowledge has been addressed, e.g. <ref> [13, 3, 14, 4] </ref>. In these investigations, not exclusively but mainly qualitative models about the system to be controlled were used. In the following, qualitative knowledge about possible control strategies is exploited without stating anything about the physics of the system.
Reference: [4] <author> A. G. Hoffmann. </author> <title> Exploiting causal domain knowledge for learning to control dynamic systems. </title> <booktitle> In Proceedings of the 11 th European Conference on Artificial Intelligence, </booktitle> <pages> pages 433-437, </pages> <address> Amsterdam, The Netherlands, August 1994. </address> <publisher> Wiley & Sons. </publisher>
Reference-contexts: If a strategy proves unsatisfactory, it is not clear how to alter the strategy to obtain improved performance. The mentioned learning approaches basically consider the system to be controlled as a black box. This appears in many cases unnecessarily restrictive. In <ref> [4] </ref> it has been shown, how learning can significantly be improved by employing qualitative together with causal domain knowledge. The learning algorithm employed, however, was not a reinforcement learner but rather tailored to exploit certain types of domain knowledge. <p> Such domain knowledge addresses both, constraining the set of admissible control strategies as well as guiding the order in which strategies are chosen for testing. These types of knowledge has been used initially in <ref> [4] </ref> for the pole balancing problem. In [5] it has been applied to the domain of trailer truck control. In this paper, it is used for the first time to support the process of reinforcement learning. Thus, the following recalls parts of [4]. 2 A state can also be considered to <p> types of knowledge has been used initially in <ref> [4] </ref> for the pole balancing problem. In [5] it has been applied to the domain of trailer truck control. In this paper, it is used for the first time to support the process of reinforcement learning. Thus, the following recalls parts of [4]. 2 A state can also be considered to be defined by the currently perceived sensations, denoted by s. 3 Of course, discounted future rewards assume that the optimal policy action will be taken in order to receive the respective rewards at a later stage. 1 2 3 4 5 60 <p> Recently, the problem of forming the class of considered control strategies by qualitative knowledge has been addressed, e.g. <ref> [13, 3, 14, 4] </ref>. In these investigations, not exclusively but mainly qualitative models about the system to be controlled were used. In the following, qualitative knowledge about possible control strategies is exploited without stating anything about the physics of the system. <p> In fact, it would be absurd attempting to take a curve of radius r 0 , if it is already clear that a curve of radius r is too narrow for the current speed. The mentioned type of monotonicity restrictions are treated in detail in <ref> [4] </ref>. <p> The dashed lines show the Q-Learning results. The dotted lines give the results for Q-learning with the small state space of 162 states for comparison. 5.1 The domain knowledge used The specified qualitative knowledge and causal knowledge is the same as in the experiments in <ref> [4] </ref> where a non-reinforcement approach was taken. * Causal knowledge: If the pole falls to the left side, then at least one control action which applied the force to accelerate the cart to the right has to be replaced by accelerating the cart to the left and vice versa.
Reference: [5] <author> A. Hoffmann and S. Matsushima. </author> <title> Using easy-to-provide domain knowled ge for learning to control dynamic systems. </title> <booktitle> In Proceedings of the Australian Conference on Artificial Intelligence, </booktitle> <address> Canberra, Australia, </address> <month> November </month> <year> 1995. </year>
Reference-contexts: Such domain knowledge addresses both, constraining the set of admissible control strategies as well as guiding the order in which strategies are chosen for testing. These types of knowledge has been used initially in [4] for the pole balancing problem. In <ref> [5] </ref> it has been applied to the domain of trailer truck control. In this paper, it is used for the first time to support the process of reinforcement learning.
Reference: [6] <author> M. Mataric. </author> <title> Reward functions for accelerated learning. </title> <editor> In W.Cohen and H.Hirsh, editors, </editor> <booktitle> Proc. of Eleventh Int. Conf. on Machine Learning. </booktitle> <address> New Brunswick, New Jersey: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: Subsequently, these algorithms proved useful for solving problems of prediction and control in stochastic environments. Applications of reinforcement learning techniques currently being investigated range from noise-free game playing environments such as backgammon, 1 to possibly very noisy robotic environments <ref> [6] </ref>. There has been a variety of different reinforcement learning techniques proposed in the literature; a major class of such algorithms considered today is the class of temporal difference learners. Temporal difference learners Early work on temporal difference learners are found in [16, 2].
Reference: [7] <author> D. Michie and R. A. Chambers. </author> <title> BOXES: An experiment in adaptive control. </title> <editor> In E. Dale and D. Michie, editors, </editor> <booktitle> Machine Intelligence, </booktitle> <pages> pages 137-152. </pages> <publisher> Edinburgh: Oliver and Boyd, </publisher> <year> 1968. </year>
Reference-contexts: An example is the classical pole balancing problem. To overcome this shortcoming, different methods for coping with nonlinear systems have been proposed. Among them are Reinforcement Learning approaches, see e.g. <ref> [7, 11, 15] </ref> or recent NIPS, ICML or IJCAI proceedings for a number of papers, as well as Neural Networks, e.g. [1], and Genetic Algorithms, e.g. [14]. All these approaches face the problem of credit assignment: A control strategy can only be evaluated as a whole.
Reference: [8] <author> M. Pendrith, M. Ryan, and A. G. Hoffmann. </author> <title> Reinforcement learning for cybernetic control. </title> <booktitle> In Proceedings of the 13 th European Meeting on Cybernetics and Systems Research, page to appear, </booktitle> <address> Vienna, Austria, </address> <month> April </month> <year> 1996. </year>
Reference-contexts: Usually time intervals of a 1 50 sec. are considered. A system state is described by the current position x of the cart on the track, its velocity _x along the track, the angle of the pole and the pole's angle velocity _ . In many papers, e.g. <ref> [1, 9, 8] </ref>, this problem was used for experiments with the a state space representation of the system based on the division of the ranges of x, _x, into 3 intervals and 6 intervals for _ , resulting in 162 states.
Reference: [9] <author> C. Sammut. </author> <title> Experimental results from an evaluation of algorithms that learn to control dynamic systems. </title> <booktitle> In Proceedings of the 5 th International Conference on Machine Learning, </booktitle> <pages> pages 437-443. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: Usually time intervals of a 1 50 sec. are considered. A system state is described by the current position x of the cart on the track, its velocity _x along the track, the angle of the pole and the pole's angle velocity _ . In many papers, e.g. <ref> [1, 9, 8] </ref>, this problem was used for experiments with the a state space representation of the system based on the division of the ranges of x, _x, into 3 intervals and 6 intervals for _ , resulting in 162 states.
Reference: [10] <author> C. Sammut and D. Michie. </author> <title> Controlling a "black box" simulation of a space craft. </title> <journal> AI Magazine, </journal> (12):56-63, 1991. 
Reference-contexts: The pole balancing problem is a popular domain for case studies on controlling nonlinear dynamic systems, as well as for reinforcement learning approaches. It is not only an attractive benchmark. It shows also similarities with control problems of practical importance, such as satellite altitude control <ref> [10] </ref>. A recent survey on approaches for learning to balance a pole can be found in [14]. The pole balancing system, see Figure 1 (right), consists of a pole which is hinged on a cart. The pole can swing in the vertical plane.
Reference: [11] <author> R. S. Sutton. </author> <title> Learning to predict by the methods of temporal difference. </title> <journal> Machine Learning, </journal> (3):9-44, 1988. 
Reference-contexts: An example is the classical pole balancing problem. To overcome this shortcoming, different methods for coping with nonlinear systems have been proposed. Among them are Reinforcement Learning approaches, see e.g. <ref> [7, 11, 15] </ref> or recent NIPS, ICML or IJCAI proceedings for a number of papers, as well as Neural Networks, e.g. [1], and Genetic Algorithms, e.g. [14]. All these approaches face the problem of credit assignment: A control strategy can only be evaluated as a whole. <p> Section 4 presents the novel approach of introducing domain knowledge into Q-learning. Section 5 presents the experimental results with the new approach. The conclusions are given in section 6. 2 Reinforcement learning Reinforcement learning algorithms, such as the method of temporal differences (TD) <ref> [11] </ref> and Q-learning [15], were originally motivated by models of animal learning being inspired by the behavioral paradigms of classical and instrumental conditioning. Subsequently, these algorithms proved useful for solving problems of prediction and control in stochastic environments.
Reference: [12] <author> G. Tesauro. </author> <title> TD-Gammon, a self-teaching backgammon program, achieves master-level play. </title> <journal> Neural Computation, </journal> <volume> 6 </volume> <pages> 215-219, </pages> <year> 1994. </year>
Reference-contexts: Q-learning, as it will be extended in this paper, was introduced in [15]. The key advance here was to have an evaluation function of state/action pairs. This led to convergence and 1 In Backgammon reinforcement learning techniques have been successfully applied to develop systems capable of master level play <ref> [12] </ref>. International Conference on Neural Information Processing 1996 pp. 954-959 optimality proofs in Markov domains, which has the consequence, that the result of Q-learning becomes experimentation insensitive in such domains [15].
Reference: [13] <author> T. Urbancic and I. Bratko. </author> <title> Learning to control dynamic systems. </title> <editor> In D. Michie and D. Spiegelhalter, editors, </editor> <title> Machine Learning, Neural and Statistical Classification. </title> <publisher> Ellis Horwood, </publisher> <year> 1994. </year>
Reference-contexts: Recently, the problem of forming the class of considered control strategies by qualitative knowledge has been addressed, e.g. <ref> [13, 3, 14, 4] </ref>. In these investigations, not exclusively but mainly qualitative models about the system to be controlled were used. In the following, qualitative knowledge about possible control strategies is exploited without stating anything about the physics of the system.
Reference: [14] <author> A. Varsek, T. Urbancic, and B. Filipic. </author> <title> Genetic algorithms in controller design and tuning. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> SMC-23(6):1330-1339, </volume> <year> 1993. </year>
Reference-contexts: To overcome this shortcoming, different methods for coping with nonlinear systems have been proposed. Among them are Reinforcement Learning approaches, see e.g. [7, 11, 15] or recent NIPS, ICML or IJCAI proceedings for a number of papers, as well as Neural Networks, e.g. [1], and Genetic Algorithms, e.g. <ref> [14] </ref>. All these approaches face the problem of credit assignment: A control strategy can only be evaluated as a whole. If a strategy proves unsatisfactory, it is not clear how to alter the strategy to obtain improved performance. <p> Recently, the problem of forming the class of considered control strategies by qualitative knowledge has been addressed, e.g. <ref> [13, 3, 14, 4] </ref>. In these investigations, not exclusively but mainly qualitative models about the system to be controlled were used. In the following, qualitative knowledge about possible control strategies is exploited without stating anything about the physics of the system. <p> It is not only an attractive benchmark. It shows also similarities with control problems of practical importance, such as satellite altitude control [10]. A recent survey on approaches for learning to balance a pole can be found in <ref> [14] </ref>. The pole balancing system, see Figure 1 (right), consists of a pole which is hinged on a cart. The pole can swing in the vertical plane. The cart can be pushed to the right and left on a bounded track.
Reference: [15] <author> C. J. Watkins. </author> <title> Learning from delayed rewards. </title> <type> PhD thesis, </type> <institution> King's College, </institution> <address> Cambridge, UK, </address> <year> 1989. </year>
Reference-contexts: An example is the classical pole balancing problem. To overcome this shortcoming, different methods for coping with nonlinear systems have been proposed. Among them are Reinforcement Learning approaches, see e.g. <ref> [7, 11, 15] </ref> or recent NIPS, ICML or IJCAI proceedings for a number of papers, as well as Neural Networks, e.g. [1], and Genetic Algorithms, e.g. [14]. All these approaches face the problem of credit assignment: A control strategy can only be evaluated as a whole. <p> Section 4 presents the novel approach of introducing domain knowledge into Q-learning. Section 5 presents the experimental results with the new approach. The conclusions are given in section 6. 2 Reinforcement learning Reinforcement learning algorithms, such as the method of temporal differences (TD) [11] and Q-learning <ref> [15] </ref>, were originally motivated by models of animal learning being inspired by the behavioral paradigms of classical and instrumental conditioning. Subsequently, these algorithms proved useful for solving problems of prediction and control in stochastic environments. <p> These temporal difference learners were all based on an evaluation of system states, where for each system state an estimate of the sum of all future rewards was determined. Q-learning, as it will be extended in this paper, was introduced in <ref> [15] </ref>. The key advance here was to have an evaluation function of state/action pairs. This led to convergence and 1 In Backgammon reinforcement learning techniques have been successfully applied to develop systems capable of master level play [12]. <p> International Conference on Neural Information Processing 1996 pp. 954-959 optimality proofs in Markov domains, which has the consequence, that the result of Q-learning becomes experimentation insensitive in such domains <ref> [15] </ref>.
Reference: [16] <author> I. Witten. </author> <title> An adaptive optimal controller for discrete-time Markov environments. </title> <journal> Information and Control, </journal> <volume> 34 </volume> <pages> 286-295, </pages> <year> 1977. </year>
Reference-contexts: There has been a variety of different reinforcement learning techniques proposed in the literature; a major class of such algorithms considered today is the class of temporal difference learners. Temporal difference learners Early work on temporal difference learners are found in <ref> [16, 2] </ref>. These temporal difference learners were all based on an evaluation of system states, where for each system state an estimate of the sum of all future rewards was determined. Q-learning, as it will be extended in this paper, was introduced in [15].
References-found: 16

