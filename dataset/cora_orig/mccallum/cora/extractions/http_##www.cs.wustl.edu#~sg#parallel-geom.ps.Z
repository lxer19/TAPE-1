URL: http://www.cs.wustl.edu/~sg/parallel-geom.ps.Z
Refering-URL: http://www.cs.wustl.edu/~sg/
Root-URL: 
Email: bshouty@cpsc.ucalgary.ca  sg@cs.wustl.edu  dmath@cs.wustl.edu  
Title: Noise-Tolerant Parallel Learning of Geometric Concepts  
Author: Nader H. Bshouty Sally A. Goldman H. David Mathias 
Address: Calgary, Alberta, Canada T2N 1N4  St. Louis, MO 63130  St. Louis, MO 63130  
Affiliation: Department of Computer Science The University of Calgary  Dept. of Computer Science Washington University  Dept. of Computer Science Washington University  
Abstract: We present several efficient parallel algorithms for PAC-learning geometric concepts in a constant-dimensional space that are robust even against malicious misclassification noise of any rate less than 1=2. In particular we consider the class of geometric concepts defined by a polynomial number of (d 1)-dimensional hyperplanes against an arbitrary distribution where each hyperplane has a slope from a set of known slopes, and the class of geometric concepts defined by a polynomial number of (d 1)-dimensional hyperplanes (of unrestricted slopes) against a product distribution. Next we define a complexity measure of any set S of (d1)-dimensional surfaces that we call the variant of S and prove that the class of geometric concepts defined by surfaces of polynomial variant can be efficiently learned in parallel under a product distribution (even under malicious misclassifi-cation noise). Finally, we describe how boosting techniques can be used so that our algorithms' de pendence on * and ffi does not depend on d.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Dana Angluin. </author> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 319-342, </pages> <year> 1988. </year>
Reference-contexts: 1 Introduction We present several efficient parallel algorithms for PAC-learning geometric concepts over <ref> [0; 1] </ref> d (for d any constant) that are robust even against malicious misclassification noise of any rate less than 1=2. <p> labelled data, they may have practical value. 2 Preliminaries We use the column vectors ~y = (y 1 ; : : : ; y d ) to denote the d dimension variables, and ~x = (x 1 ; : : : ; x d ) to denote an element of <ref> [0; 1] </ref> d . Let ~a = (a 1 ; : : : ; a d ) be a row vector where each a i is a constant. A d-dimensional hyperplane is ~a ~y = b for b a constant. <p> A d-dimensional hyperplane is ~a ~y = b for b a constant. A d-dimensional halfspace is ~a ~y b where 2 f&gt;; ; &lt;; g and b is a constant. We call G <ref> [0; 1] </ref> d , a subspace. D (G) denotes the weight of the points in G under distribution D. We assume the reader is familiar with Valiant's distribution-free, or probably approximately correct (PAC) model [34] as well as the statistical query model of Kearns [25]. <p> More formally, we associate a Boolean variable v i for 1 i s with each of the s halfspaces. For ~x 2 <ref> [0; 1] </ref> d , we define v i (~x) = 1 if and only if ~x is in the half space associated with v i . <p> of a first stage in which an unlabelled sample is used to gather information about the unknown distribution, and a second stage in which a random sample is used to compute the conditional probability of the form Pr [random point ~x is positive j ~x is in a subspace of <ref> [0; 1] </ref> d ]. Thus it follows from known results about the noise tolerance of statistical query (SQ) algorithms [25, 16, 2] that our algorithms can tolerate random misclassification noise of any noise rate bounded above by 1=2. <p> Bshouty, et al. [10] give PAC algorithm to learn the discretized version of R-linear geometric concepts with random misclassification noise. A number of results [29, 30, 31, 26, 3, 28, 15, 13, 24, 14, 22, 12] have been obtained for geometric classes in Angluin's query learning model <ref> [1] </ref> as well. There has also been work on learning in parallel [36, 7, 37, 11, 4]. Of particular relevance is the work of Vit-ter and Lin [36, 37]. <p> Next we parallelize the algorithm, and analyze the parallel time complexity. 4.1 A Sequential Algorithm The algorithm we present runs in two stages. First it draws an unlabeled sample, S 1 , of size m 1 , that is used to partition <ref> [0; 1] </ref> d into a set of subspaces by passing through each point of S 1 a hyperplane with each of the possible r slopes, where each of these hyperplanes defines three regions (the hyperplane itself and the two open halfspaces it defines). <p> We define A 1;i = f~x 2 <ref> [0; 1] </ref> d j b i ~a i ~x b i + w 1;i g and A 2;i = fx 2 [0; 1] d j b i w 2;i ~a i ~x b i g as the sets of points contained between the target hyperplane and the bounding hyperplanes. <p> We define A 1;i = f~x 2 <ref> [0; 1] </ref> d j b i ~a i ~x b i + w 1;i g and A 2;i = fx 2 [0; 1] d j b i w 2;i ~a i ~x b i g as the sets of points contained between the target hyperplane and the bounding hyperplanes. Note that by the definition of w 1;i and w 2;i , D (A 1;i ) * (4s) . <p> A key component of the algorithms we present in the remainder of this paper is to partition <ref> [0; 1] </ref> d into t d subspaces by partitioning each dimension of [0; 1] d into t pieces. We use I i;1 ; : : : ; I i;t to denote the intervals used to partition [0; 1] in the ith dimension where each I i;j is the interval such that <p> A key component of the algorithms we present in the remainder of this paper is to partition <ref> [0; 1] </ref> d into t d subspaces by partitioning each dimension of [0; 1] d into t pieces. We use I i;1 ; : : : ; I i;t to denote the intervals used to partition [0; 1] in the ith dimension where each I i;j is the interval such that c 1 y i &lt; c 2 for constants c 1 ; <p> of the algorithms we present in the remainder of this paper is to partition <ref> [0; 1] </ref> d into t d subspaces by partitioning each dimension of [0; 1] d into t pieces. We use I i;1 ; : : : ; I i;t to denote the intervals used to partition [0; 1] in the ith dimension where each I i;j is the interval such that c 1 y i &lt; c 2 for constants c 1 ; c 2 . <p> Lemma 7 There exist at most d t d1 subspaces in the uniform grid G d t that are intersected by a d 1 dimensional hyperplane through G d t . Proof: A d 1-dimensional hyperplane in <ref> [0; 1] </ref> d can be written as y 1 = a 1 a 2 y 2 a 1 where a 1 = maxfa 1 ; : : : ; a d g. <p> Then we can proceed with a second phase like that used when learning this class under the uniform distribution (with the only change being that our choice for t must be adjusted slightly). Theorem 11 Consider the interval <ref> [0; 1] </ref> and let D be an unknown distribution over [0; 1]. Using a sample of size O (tlog (t=ffi)) we can partition [0; 1] into t intervals such that with probability at least 1 ffi each interval has weight at least 1=(4t) and at most 4=t. <p> Then we can proceed with a second phase like that used when learning this class under the uniform distribution (with the only change being that our choice for t must be adjusted slightly). Theorem 11 Consider the interval <ref> [0; 1] </ref> and let D be an unknown distribution over [0; 1]. Using a sample of size O (tlog (t=ffi)) we can partition [0; 1] into t intervals such that with probability at least 1 ffi each interval has weight at least 1=(4t) and at most 4=t. <p> Theorem 11 Consider the interval <ref> [0; 1] </ref> and let D be an unknown distribution over [0; 1]. Using a sample of size O (tlog (t=ffi)) we can partition [0; 1] into t intervals such that with probability at least 1 ffi each interval has weight at least 1=(4t) and at most 4=t. The proof for this theorem follows directly from the Vapnik-Chervonenkis theory [35] and the fact that intervals have a VC-dimension of 2.
Reference: [2] <author> J. A. Aslam and S. E. Decatur. </author> <title> General bounds on statistical query learning and PAC learning with noise via hypothesis boosting. </title> <booktitle> In 34th Ann. Symp. on Foundations of Comp. Sci., </booktitle> <pages> pages 282-291, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Thus it follows from known results about the noise tolerance of statistical query (SQ) algorithms <ref> [25, 16, 2] </ref> that our algorithms can tolerate random misclassification noise of any noise rate bounded above by 1=2. Because of the simplicity of the statistical queries we make, rather than using this general technique, we can directly compute the additional sample complexity needed to obtain significantly better bounds.
Reference: [3] <author> Peter Auer. </author> <title> On-line learning of rectangles in noisy environments. </title> <booktitle> In Proc. of the 6th Ann. ACM Conf. on Comput. Learning Theory, </booktitle> <pages> pages 253-261, </pages> <month> July </month> <year> 1993. </year>
Reference: [4] <author> J. L. Balcazar, J. Diaz, R. Gavalda, and O. Watanabe. </author> <title> An optimal parallel algorithm for learning DFA. </title> <booktitle> In Proc. of the 7th Ann. ACM Conf. on Comput. Learning Theory, </booktitle> <pages> pages 208-217. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference-contexts: A number of results [29, 30, 31, 26, 3, 28, 15, 13, 24, 14, 22, 12] have been obtained for geometric classes in Angluin's query learning model [1] as well. There has also been work on learning in parallel <ref> [36, 7, 37, 11, 4] </ref>. Of particular relevance is the work of Vit-ter and Lin [36, 37].
Reference: [5] <author> E. B. Baum. </author> <title> On learning a union of half spaces. </title> <journal> Journal of Complexity, </journal> <volume> 6(1) </volume> <pages> 67-101, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: In particular, unions and in tersections of halfspaces have been considered. Blum and Rivest [8] show that there does not exist an efficient proper 4 learning algorithm for unions of s halfspaces, unless P = N P . Baum <ref> [5] </ref> gives an algorithm that efficiently learns a union of s halfspaces in a constant number of dimensions. 4 A learning algorithm is proper if all hypotheses come from the concept class. Blumer et al. [9] give a similar result.
Reference: [6] <author> E. B. Baum. </author> <title> The perceptron algorithm is fast for nonmalicious distributions. </title> <journal> Neural Computation, </journal> <volume> 2 </volume> <pages> 248-260, </pages> <year> 1990. </year>
Reference-contexts: Considering this class under the product distribution is of particular interest since it is well known that even surfaces with variant 1 have infinite VC-dimension (see, for example, Baum <ref> [6] </ref>) and thus, by the results of Ehrenfeucht, et.al. [18], this class is not efficiently learnable under an arbitrary distribution. Next we give an efficient parallel noise-tolerant algorithm to learn 2-dimensional geometric concepts defined by a set of 1-dimensional surfaces (or curves) of polynomial length under the uniform distribution. <p> Blumer et al. [9] give a similar result. Both algorithms re-turn hypotheses containing O (s lg m) halfspaces where m is the size of the sample. Baum gives efficient algorithms for learning several classes with infinite VC-dimension (such as convex polyhedral sets) under uniform distributions <ref> [6] </ref>. Haussler [23] also gives distribution specific algorithms for several classes of functions. Research has also been done on the learnability of unions of axis-parallel boxes.
Reference: [7] <author> B. Berger, J. Rompel, and P. W. Shor. </author> <title> Efficient NC algorithms for set cover with applications to learning and geometry. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 49(2) </volume> <pages> 454-477, </pages> <year> 1994. </year>
Reference-contexts: A number of results [29, 30, 31, 26, 3, 28, 15, 13, 24, 14, 22, 12] have been obtained for geometric classes in Angluin's query learning model [1] as well. There has also been work on learning in parallel <ref> [36, 7, 37, 11, 4] </ref>. Of particular relevance is the work of Vit-ter and Lin [36, 37]. <p> Furthermore they prove that the class of axis-parallel rectangles, linear separators and simple k-gons are N C MC -learnablefor &lt; 1=2. Berger, Rompel and Shor <ref> [7] </ref> gave N C approximation algorithms for the unweighted and weighted set cover problems.
Reference: [8] <author> A. Blum and R. L. Rivest. </author> <title> Training a 3-node neural net is NP-Complete. </title> <booktitle> In Advances in Neural Information Processing Systems I, </booktitle> <pages> pages 494-501. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1989. </year>
Reference-contexts: In particular, unions and in tersections of halfspaces have been considered. Blum and Rivest <ref> [8] </ref> show that there does not exist an efficient proper 4 learning algorithm for unions of s halfspaces, unless P = N P .
Reference: [9] <author> Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth. </author> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 36(4) </volume> <pages> 929-965, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: Baum [5] gives an algorithm that efficiently learns a union of s halfspaces in a constant number of dimensions. 4 A learning algorithm is proper if all hypotheses come from the concept class. Blumer et al. <ref> [9] </ref> give a similar result. Both algorithms re-turn hypotheses containing O (s lg m) halfspaces where m is the size of the sample. Baum gives efficient algorithms for learning several classes with infinite VC-dimension (such as convex polyhedral sets) under uniform distributions [6].
Reference: [10] <author> Nader H. Bshouty, Zhixiang Chen, and Steve Homer. </author> <title> On learning discretized geometric concepts. </title> <booktitle> In 35th Ann. Symp. on Foundations of Comp. Sci., </booktitle> <pages> pages 54-63, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: For any arbitrary set S of (d 1)-dimensional surfaces, we define the complexity or variant of S, V (S), as the minimum 2 There is a known sequential PAC algorithm to learn the dis-cretized version of R-linear geometric concepts with random mis-classification noise <ref> [10] </ref>. 3 A function f is monotone if y 1 &gt; y 2 ) f (y 1 ) &gt; f (y 2 ). v such that S can be expressed as a union of v 1-variant surfaces. <p> Their algorithm learns this subclass of general unions of boxes in time polynomial in both s and d. Bshouty, et al. <ref> [10] </ref> give PAC algorithm to learn the discretized version of R-linear geometric concepts with random misclassification noise. A number of results [29, 30, 31, 26, 3, 28, 15, 13, 24, 14, 22, 12] have been obtained for geometric classes in Angluin's query learning model [1] as well.
Reference: [11] <author> Nader H. Bshouty and Richard Cleve. </author> <title> On the exact learning of formulas in parallel. </title> <booktitle> In 33rd Ann. Symp. on Foundations of Comp. Sci., </booktitle> <pages> pages 1-15, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: So for example, the adversary can give each point a different noise rate as long as each noise rate is at most .) We also note that it follows from the work of Bshouty and Cleve <ref> [11] </ref> and Maass and Warmuth [32] that no efficient parallel algorithm exists to exactly learn the union of s axis-parallel boxes over f1; : : : ; ng d (which is the discretized version of a subclass of C linear s ). <p> A number of results [29, 30, 31, 26, 3, 28, 15, 13, 24, 14, 22, 12] have been obtained for geometric classes in Angluin's query learning model [1] as well. There has also been work on learning in parallel <ref> [36, 7, 37, 11, 4] </ref>. Of particular relevance is the work of Vit-ter and Lin [36, 37].
Reference: [12] <author> Nader H. Bshouty, Paul W. Goldberg, Sally A. Goldman, and H. David Mathias. </author> <title> Exact learning of discretized concepts. </title> <type> Tech. Report WUCS-94-19, </type> <institution> Washington University, </institution> <year> 1994. </year>
Reference: [13] <author> Zhixiang Chen. </author> <title> Learning unions of two rectangles in the plane with equivalence queries. </title> <booktitle> In Proc. of the 6th Ann. ACM Conf. on Comput. Learning Theory, </booktitle> <pages> pages 243-252. </pages> <publisher> ACM Press, </publisher> <month> July </month> <year> 1993. </year>
Reference: [14] <author> Zhixiang Chen and Steven Homer. </author> <title> The bounded injury priority method and the learnability of unions of rectangles. </title> <type> Unpublished manuscript, </type> <month> May </month> <year> 1994. </year>
Reference: [15] <author> Zhixiang Chen and Wolfgang Maass. </author> <title> On-line learning of rectangles. </title> <booktitle> In Proc. of the 5th Ann. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 16-27. </pages> <publisher> ACM Press, </publisher> <month> July </month> <year> 1992. </year>
Reference: [16] <author> S. E. Decatur. </author> <title> Statistical queries and faulty PAC oracles. </title> <booktitle> In Proc. of the 6th Ann. ACM Conf. on Comput. Learning Theory, </booktitle> <pages> pages 262-268. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1993. </year>
Reference-contexts: Thus it follows from known results about the noise tolerance of statistical query (SQ) algorithms <ref> [25, 16, 2] </ref> that our algorithms can tolerate random misclassification noise of any noise rate bounded above by 1=2. Because of the simplicity of the statistical queries we make, rather than using this general technique, we can directly compute the additional sample complexity needed to obtain significantly better bounds. <p> Since all of our algorithms are easily formulated as statistical query algorithms, in addition to handling labelling noise, known results allow us to handle small amounts of malicious noise, and various types of noise effecting the distribution of the random examples (e.g. see Decatur <ref> [16] </ref>). In addition, they can all be efficiently implemented in parallel. Another nice feature of our algorithms is that they can take advantage of unlabelled as well as labelled data which may be of value for some real-life applications, especially when combined with the simplicity and robustness of the algorithms.
Reference: [17] <author> Herbert Edelsbrunner. </author> <title> Algorithms in Combinatorial Geometry. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: setting 2s 4s ffi 2 , yields the desired result. 2 The total number of subspaces created (where each of the m 1 r hyperplanes divides each region it intersects into three parts) is t = ((m 1 r) d ) = * log ffi : (See, for example, Edelsbrunner <ref> [17] </ref>.) In the second stage of the algorithm we must determine the classification of all of the subspaces that contain at least t = * 2t of the distribution.
Reference: [18] <author> Andrzej Ehrenfeucht, David Haussler, Michael Kearns, and Leslie Valiant. </author> <title> A general lower bound on the number of examples needed for learning. </title> <journal> Information and Computation, </journal> <volume> 82(3) </volume> <pages> 247-251, </pages> <year> 1989. </year>
Reference-contexts: Considering this class under the product distribution is of particular interest since it is well known that even surfaces with variant 1 have infinite VC-dimension (see, for example, Baum [6]) and thus, by the results of Ehrenfeucht, et.al. <ref> [18] </ref>, this class is not efficiently learnable under an arbitrary distribution. Next we give an efficient parallel noise-tolerant algorithm to learn 2-dimensional geometric concepts defined by a set of 1-dimensional surfaces (or curves) of polynomial length under the uniform distribution. <p> Finally, observe that the resulting surface has variant 1 and shatters the m points. 2 Thus by the results of Ehrenfeucht et al. <ref> [18] </ref>, there is no efficient algorithm (even if computation time is unbounded) to PAC-learn C nonlinear V , even when V (S) = 1, against an arbitrary distribution.
Reference: [19] <author> Mike Frazier, Sally Goldman, Nina Mishra, and Leonard Pitt. </author> <title> Learning from a consistently ignorant teacher. </title> <booktitle> In Proc. of the 7th Ann. ACM Conf. on Comput. Learning Theory, </booktitle> <month> July </month> <year> 1994. </year> <note> To appear, Journal of Computer and System Sciences. </note>
Reference-contexts: Finally, under a variation of the PAC model in which membership queries can be made, Frazier et al. <ref> [19] </ref> have given an algorithm to PAC-learn the s-fold union of boxes in E d for which each box is entirely contained within the positive quadrant and contains the origin. Their algorithm learns this subclass of general unions of boxes in time polynomial in both s and d.
Reference: [20] <author> Yoav Freund. </author> <title> Boosting a weak learning algorithm by majority. </title> <booktitle> In Proc. of the 3rd Ann. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 202-216. </pages> <publisher> Morgan Kaufmann, </publisher> <month> August </month> <year> 1990. </year>
Reference-contexts: Let B be the resulting algorithm. We now run B for * = 1=4 to get a weak approximation of the target. Using boosting techniques we need to generate log (1=*) weak hypotheses to get an *-good approximation <ref> [33, 20, 21] </ref>. The blow up in ffi is poly (log (1=ffi)).
Reference: [21] <author> Yoav Freund. </author> <title> An improved boosting algorithm and its implications on learning complexity. </title> <booktitle> In Proc. of the 5th Ann. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 391-398, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: Let B be the resulting algorithm. We now run B for * = 1=4 to get a weak approximation of the target. Using boosting techniques we need to generate log (1=*) weak hypotheses to get an *-good approximation <ref> [33, 20, 21] </ref>. The blow up in ffi is poly (log (1=ffi)).
Reference: [22] <author> Paul W. Goldberg, Sally A. Goldman, and H. David Mathias. </author> <title> Learning unions of boxes with membership and equivalence queries. </title> <booktitle> In Proc. of the 7th Ann. ACM Conf. on Comput. Learning Theory, </booktitle> <month> July </month> <year> 1993. </year>
Reference: [23] <author> David Haussler. </author> <title> Generalizing the PAC model: sample size bounds from metric dimension-based uniform convergence results. </title> <booktitle> In 30th Ann. Symp. on Foundations of Comp. Sci., </booktitle> <pages> pages 40-45, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: Blumer et al. [9] give a similar result. Both algorithms re-turn hypotheses containing O (s lg m) halfspaces where m is the size of the sample. Baum gives efficient algorithms for learning several classes with infinite VC-dimension (such as convex polyhedral sets) under uniform distributions [6]. Haussler <ref> [23] </ref> also gives distribution specific algorithms for several classes of functions. Research has also been done on the learnability of unions of axis-parallel boxes.
Reference: [24] <author> Steven Homer and Zhixiang Chen. </author> <title> Fast learning unions of rectangles with queries. </title> <type> Unpublished manuscript, </type> <month> July </month> <year> 1993. </year>
Reference: [25] <author> M. Kearns. </author> <title> Efficient noise-tolerant learning from statistical queries. </title> <booktitle> In Proc. 25th Annu. ACM Sympos. Theory Comput., </booktitle> <pages> pages 392-401. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1993. </year>
Reference-contexts: We call G [0; 1] d , a subspace. D (G) denotes the weight of the points in G under distribution D. We assume the reader is familiar with Valiant's distribution-free, or probably approximately correct (PAC) model [34] as well as the statistical query model of Kearns <ref> [25] </ref>. We now define the concept classes that we study. <p> Thus it follows from known results about the noise tolerance of statistical query (SQ) algorithms <ref> [25, 16, 2] </ref> that our algorithms can tolerate random misclassification noise of any noise rate bounded above by 1=2. Because of the simplicity of the statistical queries we make, rather than using this general technique, we can directly compute the additional sample complexity needed to obtain significantly better bounds.
Reference: [26] <author> Nick Littlestone. </author> <title> Learning when irrelevant attributes abound: A new linear-threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 285-318, </pages> <year> 1988. </year>
Reference: [27] <author> Philip M. Long and Manfred K. Warmuth. </author> <title> Composite geometric concepts and polynomial predictability. </title> <booktitle> In Proc. of the 3rd Ann. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 273-287. </pages> <publisher> Morgan Kaufmann, </publisher> <month> August </month> <year> 1990. </year>
Reference-contexts: Long and Warmuth <ref> [27] </ref> present an algorithm to PAC-learn this same class by again drawing a sufficiently large sample and constructing a hypothesis that consists of at most s (2d) s boxes consistent with the sample.
Reference: [28] <author> Wolfgang Maass and Gyorgy Turan. </author> <title> On the complexity of learning from counterexamples. </title> <booktitle> In 30th Ann. Symp. on Foundations of Comp. Sci., </booktitle> <pages> pages 262-267, </pages> <month> October </month> <year> 1989. </year>
Reference: [29] <author> Wolfgang Maass and Gyorgy Turan. </author> <title> On the complexity of learning from counterexamples and membership queries. </title> <booktitle> In 31st Ann. Symp. on Foundations of Comp. Sci., </booktitle> <pages> pages 203-210, </pages> <month> October </month> <year> 1990. </year>
Reference: [30] <author> Wolfgang Maass and Gyorgy Turan. </author> <title> Algorithms and lower bounds for on-line learning of geometrical concepts. </title> <type> Tech. Report IIG-Report 316, </type> <institution> Technische Universitat Graz, TU Graz, Austria, </institution> <month> October </month> <year> 1991. </year>
Reference: [31] <author> Wolfgang Maass and Gyorgy Turan. </author> <title> Lower bound methods and separation results for on-line learning models. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 107-145, </pages> <year> 1992. </year>
Reference: [32] <author> Wolfgang Maass and Manfred Warmuth. </author> <title> Efficient learning with virtual threshold gates. </title> <type> Unpublished Manuscript, </type> <year> 1994. </year>
Reference-contexts: So for example, the adversary can give each point a different noise rate as long as each noise rate is at most .) We also note that it follows from the work of Bshouty and Cleve [11] and Maass and Warmuth <ref> [32] </ref> that no efficient parallel algorithm exists to exactly learn the union of s axis-parallel boxes over f1; : : : ; ng d (which is the discretized version of a subclass of C linear s ).
Reference: [33] <author> Robert E. Schapire. </author> <title> The strength of weak learnability. </title> <journal> Machine Learning, </journal> <volume> 5(2) </volume> <pages> 197-227, </pages> <year> 1990. </year>
Reference-contexts: Let B be the resulting algorithm. We now run B for * = 1=4 to get a weak approximation of the target. Using boosting techniques we need to generate log (1=*) weak hypotheses to get an *-good approximation <ref> [33, 20, 21] </ref>. The blow up in ffi is poly (log (1=ffi)).
Reference: [34] <author> Leslie Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <month> November </month> <year> 1984. </year>
Reference-contexts: We call G [0; 1] d , a subspace. D (G) denotes the weight of the points in G under distribution D. We assume the reader is familiar with Valiant's distribution-free, or probably approximately correct (PAC) model <ref> [34] </ref> as well as the statistical query model of Kearns [25]. We now define the concept classes that we study.
Reference: [35] <author> V. N. Vapnik and A. Ya. Chervonenkis. </author> <title> On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability and Its Applications, </title> <address> XVI(2):264-280, </address> <year> 1971. </year>
Reference-contexts: Using a sample of size O (tlog (t=ffi)) we can partition [0; 1] into t intervals such that with probability at least 1 ffi each interval has weight at least 1=(4t) and at most 4=t. The proof for this theorem follows directly from the Vapnik-Chervonenkis theory <ref> [35] </ref> and the fact that intervals have a VC-dimension of 2. We now apply this theorem to obtain an algorithm to PAC learn C linear s under a product distribution.
Reference: [36] <author> Jeffrey S. Vitter and Jyh-Han Lin. </author> <title> Learning in parallel. </title> <booktitle> In Proc. of the 1988 Workshop on Comput. Learning Theory, </booktitle> <pages> pages 106-124. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: A number of results [29, 30, 31, 26, 3, 28, 15, 13, 24, 14, 22, 12] have been obtained for geometric classes in Angluin's query learning model [1] as well. There has also been work on learning in parallel <ref> [36, 7, 37, 11, 4] </ref>. Of particular relevance is the work of Vit-ter and Lin [36, 37]. <p> There has also been work on learning in parallel [36, 7, 37, 11, 4]. Of particular relevance is the work of Vit-ter and Lin <ref> [36, 37] </ref>.
Reference: [37] <author> Jeffrey S. Vitter and Jyh-Han Lin. </author> <title> Learning in parallel. </title> <booktitle> Information and Computation, </booktitle> <pages> pages 179-202, </pages> <year> 1992. </year>
Reference-contexts: A number of results [29, 30, 31, 26, 3, 28, 15, 13, 24, 14, 22, 12] have been obtained for geometric classes in Angluin's query learning model [1] as well. There has also been work on learning in parallel <ref> [36, 7, 37, 11, 4] </ref>. Of particular relevance is the work of Vit-ter and Lin [36, 37]. <p> There has also been work on learning in parallel [36, 7, 37, 11, 4]. Of particular relevance is the work of Vit-ter and Lin <ref> [36, 37] </ref>.
References-found: 37

