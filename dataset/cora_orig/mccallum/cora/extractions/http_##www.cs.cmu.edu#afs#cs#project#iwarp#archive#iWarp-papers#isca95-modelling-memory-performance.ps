URL: http://www.cs.cmu.edu/afs/cs/project/iwarp/archive/iWarp-papers/isca95-modelling-memory-performance.ps
Refering-URL: 
Root-URL: 
Title: Optimizing Memory System Performance for Communication in Parallel Computers NX) force the parallelizing compiler (or
Author: T. Stricker and T. Gross ; 
Keyword: Most standard message passing libraries (like MPI, PVM  
Address: Pittsburgh, PA 15213 CH 8092 Zuerich, Switzerland  
Affiliation: Santa Marguerita di Ligure, Italy  1 School of Computer Science 2 Institut fuer Computer Systeme Carnegie Mellon University ETH Zuerich  
Date: June 21-25, 1995,  
Note: Appears in: Proceedings of the 22nd International Symposium on Computer Architecture,  or  
Abstract: Communication in a parallel system frequently involves moving data from the memory of one node to the memory of another; this is the standard communication model employed in message passing systems. Depending on the application, we observe a variety of patterns as part of communication steps, e.g., regular (i.e. blocks of data), strided, or irregular (indexed) memory accesses. The effective speed of these communication steps is determined by the network bandwidth and the memory bandwidth, and measurements on current parallel supercomputers indicate that the performance is limited by the memory bandwidth rather than the network bandwidth. Current systems provide a wealth of options to perform communication, and a compiler or user is faced with the difficulty of finding the communication operations that best use the available memory and network bandwidth. This paper provides a framework to evaluate different solutions for inter-node communication and presents the copy-transfer model; this model captures the contributions of the memory system to inter-node communication. We demonstrate the usefulness of this simple model by applying it to two commercial parallel systems, the Cray T3D and the Intel Paragon. In particular we identify two methods to transfer data between nodes in these two machines. In buffer-packing transfers, a contiguous block of data is transferred across the network. If the data are not stored contiguously, they are copied to (gathering) or from (scattering) buffers in local memory before and after the transfer. Chained transfers perform gathering, transfer and scattering in one step, reading the data elements with some non-sequential pattern and immediately transferring them on to the destination. Our model and measurements indicate that chaining of the gather, transfer, and scatter operations results in better performance than buffer packing for many important access patterns. This research was sponsored in part by the Advanced Research Projects Agency/CSTO monitoredby SPAWAR under contract N00039-93-C-0152. Computational resources were provided in part by the Pittsburgh Supercomputing Center (PSC). The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Government. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Adams. </author> <title> Cray T3D System Architecture Overview. </title> <type> Technical report, </type> <institution> Cray Research Inc., </institution> <month> September </month> <year> 1993. </year> <note> Revision 1.C. </note>
Reference-contexts: The relative advantages of both machines have been discussed in numerous papers, and there exist a number of machines for either style. This paper concerns itself solely with message passing communication, because (1) any improvement to message passing communication helps current <ref> [1, 13, 3] </ref> and future machines [7] that provide this communication model, even if these machine support other models as well, (2) a number of commercial systems are based on message passing (including all systems with a large number of nodes), and (3) the hardware/software solutions offered for communication on these <p> We refer the interested reader to the reference literature about these machines for further technical details <ref> [1, 3, 12] </ref>. The compiler demands communication with transfers x Q y for all access patterns x and y, including strided and indexed.
Reference: [2] <author> G. Blelloch and J. </author> <title> Sipelstein. </title> <journal> Collection-Oriented Languages. Proc. IEEE, </journal> <volume> 79(4) </volume> <pages> 504-523, </pages> <month> Apr </month> <year> 1991. </year>
Reference-contexts: The importance of throughput is not surprising given the properties of communication related memory accesses. In data parallel programs, parallelism is exploited by operation on large collections, with the data distributed over a large number of processors <ref> [2] </ref>. In practice, these collections can be quite large and a compiler cannot assume that the local data structure on any node fits entirely into the local cache of a node.
Reference: [3] <author> Intel Corp. </author> <title> Paragon X/PS Product Overview. </title> <institution> Intel Corp., </institution> <month> March </month> <year> 1991. </year>
Reference-contexts: The relative advantages of both machines have been discussed in numerous papers, and there exist a number of machines for either style. This paper concerns itself solely with message passing communication, because (1) any improvement to message passing communication helps current <ref> [1, 13, 3] </ref> and future machines [7] that provide this communication model, even if these machine support other models as well, (2) a number of commercial systems are based on message passing (including all systems with a large number of nodes), and (3) the hardware/software solutions offered for communication on these <p> We refer the interested reader to the reference literature about these machines for further technical details <ref> [1, 3, 12] </ref>. The compiler demands communication with transfers x Q y for all access patterns x and y, including strided and indexed.
Reference: [4] <institution> Cray Research Inc. </institution> <note> CRAY T3D Applications Programming Course, </note> <month> Nov </month> <year> 1993. </year> <month> TR-T3DAPPL. </month>
Reference-contexts: For writes, the default configuration of the cache is write-around, and support for writes consists of the write back queue (WBQ) provided by the microprocessor. The documentation of the Cray T3D Application Programmers Course <ref> [4] </ref> specifies the local read bandwidth at 55 MB/s for non-contiguous single word transfers, and up to 320 MB/s for contiguous reading of cache lines with read-ahead. The latency of a load from main memory is around 150ns.
Reference: [5] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran language specification version 1.0 draft, </title> <month> January </month> <year> 1993. </year>
Reference-contexts: Recently, the High Performance Fortran (HPF) effort has resulted in a set of user directives that assist the compiler in performing its tasks <ref> [5] </ref> 1 . HPF focuses on block-cyclic distribution of arrays, where the two variants, the block and cyclic are the most common [15]. The distributions included in standard HPF are well-suited to describe regular data layouts.
Reference: [6] <author> T. Gross, D. O'Hallaron, and J. Subhlok. </author> <title> Task Parallelism in a High Performance Fortran Framework. </title> <journal> IEEE Parallel and Distributed Technology, </journal> <volume> 2(3) </volume> <pages> 16-26, </pages> <month> Fall </month> <year> 1994. </year>
Reference-contexts: From a compiler's point of view, data are moved between the address spaces of nodes, and these data can be contiguous blocks, slices, intersections of slices [15], or irregular blocks of 1 Our work is done in collaboration with the implementation of an HPF compiler <ref> [6] </ref>, but the details of HPF are irrelevant to this study. Our results apply to any system that moves data from the local memory of one node to the remote memory of another. 2 vendor specific or third party libraries that offer best throughput. data described by an index array.
Reference: [7] <author> K. Hayashi, T. Doi, T. Horie, Y. Koyanagi, O. Shiraki, N. Ima-mura, T. Shimizu, H. Ishihata, and T. Shindo. </author> <title> Ap1000+: Architectural Support of a put/get Interface for Parallelizing Compilers. </title> <booktitle> In Proc. of ASPLOS IV, </booktitle> <pages> pages 196-207. </pages> <publisher> ACM, </publisher> <month> Oct </month> <year> 1994. </year>
Reference-contexts: The relative advantages of both machines have been discussed in numerous papers, and there exist a number of machines for either style. This paper concerns itself solely with message passing communication, because (1) any improvement to message passing communication helps current [1, 13, 3] and future machines <ref> [7] </ref> that provide this communication model, even if these machine support other models as well, (2) a number of commercial systems are based on message passing (including all systems with a large number of nodes), and (3) the hardware/software solutions offered for communication on these systems are far from satisfactory.
Reference: [8] <author> S. Hinrichs, C. Kosak, D. O'Hallaron, T. Stricker, and R. </author> <title> Take. An Architecture for Optimal All-to-All Personalized Communication. </title> <booktitle> In ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 310-319, </pages> <address> Cape May, New Jersey, </address> <month> June </month> <year> 1994. </year> <note> A revised version is available as Tech. Report CMU-CS-94-140. </note>
Reference-contexts: In general, next neighbor patterns like cyclic shifts cause just a small congestion of one or two, and even dense patterns like the complete exchange or personalized all-to-all communication can be scheduled with minimal congestion on T3D tori of up to 1024 (2x8x8x8) compute nodes <ref> [8] </ref>. Because of these two problems in the T3D and Paragon networks, communication runs at a congestion of two in many cases, and we use the measured throughput for this congestion, when using our model to compute overall throughputs.
Reference: [9] <author> C. Leiserson, A. Abuhamdeh, D. Douglas, C. Feynman, M. Gan-mukhi, J. Hill, D. Hillis, B. Kuszmaul, M. St.Pierre, D. Wells, M. Wong, S. Yang, and R. Zak. </author> <title> The Network Architecture of the Connection Machine CM-5. </title> <booktitle> In Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 272-285, </pages> <address> San Diego, </address> <month> June </month> <year> 1992. </year> <note> ACM. </note>
Reference-contexts: 0 D 1 j j 0 R 64 j j 0 D 64 j j 0 R ! j j 0 D ! j Paragon 82 160 38 - 42 - Table 3: Throughput figures for receiving network transfers (MB/s). provides a fully scalable bisection bandwidth as e.g. the CM-5 <ref> [9] </ref>. Both machines use a simple mesh topology with fast links for their communication networks. In our experience, the raw link speed in the network significantly exceeds the effective throughput achievable in useful data transfers.
Reference: [10] <author> A. B. Maccabe, K. S. McCurley, R. Riesen, and S. R. Wheat. </author> <title> SUNMOS for the Intel Paragon: A Brief User's Guide. </title> <booktitle> In Proceedings of the Intel SupercomputerUsers' Group. 1994 Annual North America Users' Conference., </booktitle> <pages> pages 245-251, </pages> <month> June </month> <year> 1994. </year> <note> ftp.cs.sandia.gov /pub/sunmos/papers/published/ISUG94-1.ps. </note>
Reference-contexts: The processors of a Paragon node are two Intel i860XP processors. Both processors have their own primary on-chip data cache and are connected to the local memory system over a 400 MB/s high speed bus. The data cache is 16 KB, organized 4-way associative, write-back or write-through. Under SUNMOS <ref> [10] </ref> (the operating system of choice for low-latency communi cation) the caches are write through. The i860XP processors contain support for higher bandwidth through pipelined loads (using the PFQ) that bypass the caches.
Reference: [11] <author> G. McRae, W. Goodin, and J. Seinfeld. </author> <title> Development of a Second-Generation Mathematical Model for Urban Air Pollution Model Formulation. </title> <journal> Atmospheric Environment, </journal> <volume> 16(4) </volume> <pages> 679-696, </pages> <year> 1982. </year>
Reference-contexts: The transposes are necessary to provide locality for the column FFTs after the row FFTs are completed. We encountered a transpose of similar size as the performance critical communication step of a grand challenge application in air-shed modeling <ref> [11] </ref>. This code redistributes a 3500 fi (35 fi 5) array 10 for either loads or stores. The buffer-packing implementations (left) result in a lower throughput than the chained implementations (right).
Reference: [12] <author> R. Numrich, P. Springer, and J. Peterson. </author> <title> Measurement of Communication Rates on the Cray T3D Interprocessor Network. </title> <booktitle> In Proc. HPCN Europe '94, </booktitle> <volume> Vol. II, </volume> <pages> pages 150-157, </pages> <address> Munich, </address> <month> April </month> <year> 1994. </year> <title> Springer Verlag. </title> <booktitle> Lecture Notes in Computer Science, </booktitle> <volume> Vol. </volume> <pages> 797. </pages>
Reference-contexts: We refer the interested reader to the reference literature about these machines for further technical details <ref> [1, 3, 12] </ref>. The compiler demands communication with transfers x Q y for all access patterns x and y, including strided and indexed.
Reference: [13] <author> W. Oed. </author> <title> The Cray Research Massively Parallel Processor System Cray T3D, </title> <note> 1993. Available from via ftp from cray.com. </note>
Reference-contexts: The relative advantages of both machines have been discussed in numerous papers, and there exist a number of machines for either style. This paper concerns itself solely with message passing communication, because (1) any improvement to message passing communication helps current <ref> [1, 13, 3] </ref> and future machines [7] that provide this communication model, even if these machine support other models as well, (2) a number of commercial systems are based on message passing (including all systems with a large number of nodes), and (3) the hardware/software solutions offered for communication on these
Reference: [14] <author> E. J. Schwabe, G. E. Blelloch, A. Feldmann, O. Ghattas, J. R. Gilbert, G. L. Miller, D. R. O'Hallaron, J. R. Shewchuk, and S. Teng. </author> <title> A Separator-Based Framework for Automated Partitioning and Mapping of Parallel Algorithms for Numerical Solution of PDEs. </title> <booktitle> In Proceedings of the 1992 DAGS/PC Symposium, </booktitle> <pages> pages 48-62, </pages> <month> June </month> <year> 1992. </year> <note> Revised version accepted for Comm. ACM. </note>
Reference-contexts: Reading the indices is overhead; reading the index is considered to be part of the memory access operation and does not count towards what we report as the effective memory access bandwidth for an application. Indexed patterns are common for irregular distributions and sparse matrix representations <ref> [14] </ref>. <p> This graph is used by our colleagues to study earthquakes <ref> [14] </ref>. Since the structure is an irregular well partitioned grid, only a fraction of the local data elements is exchanged between nodes, and the communication involves indexed accesses with arbitrary strides. 6.1.3 Successive over-relaxation solver Not all applications require the transfer of strided or indexed data.
Reference: [15] <author> J. Stichnoth, D. O'Hallaron, and T. Gross. </author> <title> Generating Communication for Array Statements: Design, Implementation, and Evaluation. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 21(1) </volume> <pages> 150-159, </pages> <year> 1994. </year>
Reference-contexts: Recently, the High Performance Fortran (HPF) effort has resulted in a set of user directives that assist the compiler in performing its tasks [5] 1 . HPF focuses on block-cyclic distribution of arrays, where the two variants, the block and cyclic are the most common <ref> [15] </ref>. The distributions included in standard HPF are well-suited to describe regular data layouts. However, many applications are irregular in that the access pattern cannot be described with a few parameters. Instead, the access pattern is contained in another data structure, usually referred to as an index array. <p> From a compiler's point of view, data are moved between the address spaces of nodes, and these data can be contiguous blocks, slices, intersections of slices <ref> [15] </ref>, or irregular blocks of 1 Our work is done in collaboration with the implementation of an HPF compiler [6], but the details of HPF are irrelevant to this study.
Reference: [16] <author> T. Stricker, J. Stichnoth, D. O'Hallaron, S. Hinrichs, and T. Gross. </author> <booktitle> The Performance Impact of Fast Synchronization in Parallel Computers To appear in Proceedings of International Conference of Supercomputing, </booktitle> <address> Barcelona, Spain, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: The compiler generates synchronization (or control) instructions separately (e.g., before and after a complete array redistribution) <ref> [16] </ref>. This organization allows us to focus in this paper on speeding up the data transfers. There are two principal approaches to organizing the data transfers.

References-found: 16

