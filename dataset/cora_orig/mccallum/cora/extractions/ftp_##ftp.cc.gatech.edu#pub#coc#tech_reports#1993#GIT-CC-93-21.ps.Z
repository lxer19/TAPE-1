URL: ftp://ftp.cc.gatech.edu/pub/coc/tech_reports/1993/GIT-CC-93-21.ps.Z
Refering-URL: http://www.cs.gatech.edu/tech_reports/index.93.html
Root-URL: 
Title: LU Logging An Efficient Transaction Recovery Method  
Author: Sreenivas Gukal Edward Omiecinski Umakishore Ramachandran 
Address: Atlanta, Georgia 30332-0280  
Affiliation: College of Computing Georgia Institute of Technology  
Date: July 20, 1993  
Pubnum: GIT-CC-93/21  
Abstract: In this paper, we present LU-Logging, an efficient transaction recovery method. The method is based on flexible-redo/minimal-undo algorithm. The paper describes an implementation which avoids the overheads of deferred updating used in previous no-undo implementations. An update by a transaction to a data record does not immediately update the data record. Instead, it generates a redo log record and associates it with the data page. Each page in the data base has an associated log page, which contains the still-uncommitted log records of the updates for the data page. The log page is read from and written to the disk along with the corresponding data page. This gives the flexibility of applying the redo log records any time after the transaction commits, in particular when the data page is read by another transaction. We call this updating as lazy. For aborted transactions the redo log records are just discarded. Simulation studies show that the overhead during normal transaction processing for LU-Logging is comparable to that of traditional Logging. The crash recovery time is shown to be an order of magnitude faster than that for traditional Logging. 
Abstract-found: 1
Intro-found: 1
Reference: [AgDe 85] <author> Agarwal, R., Dewitt, </author> <title> D.J. Integrated Concurrency Control and Recovery Mechanisms: Design and Performance Evaluation, </title> <journal> ACM Transactions on Database Systems, </journal> <month> December </month> <year> 1985. </year>
Reference-contexts: Media recovery is discussed in section 6. The paper ends with conclusions and suggestions for future work. 2 Logging & Shadowing The traditional Logging mechanism <ref> [Gray 78, HaRe 83, AgDe 85] </ref> is an excellent example for the redo/undo algorithm. Here, every update operation, besides updating the data record, also creates an "undo" and a "redo" log record. These log records are written to an append-only log on the disk. <p> The actions during checkpointing and the contents of the checkpoint record depend on the particular implementation. The crash recovery algorithm consists of using the log on the disk to un-do the effects of aborted transactions and to re-do the lost updates of committed transactions. The Shadowing method <ref> [Lori 77, AgDe 85] </ref> is based on the redo/no-undo algorithm. Here, when a transaction wants to modify a page, a copy of the page is made and the modifications are done to the copy. The original unmodified copy is called the "Shadow Page". <p> Whatever the mechanism, the recovery process in Logging is much more expensive than that using Shadowing. Several earlier studies have compared the transaction processing overheads of Logging and Shadowing, e.g. <ref> [AgDe 85] </ref>. It has been shown that Logging incurs much lower transaction processing overheads than Shadowing for the reasons mentioned above.
Reference: [BeHG 87] <author> Bernstein, P.A., Hadzilacos, V., Goodman, N. </author> <title> Concurrency Control and Recovery in Database Systems, </title> <publisher> Addison-Wesley Pub. Co., </publisher> <year> 1987. </year>
Reference-contexts: On the other hand, if uncommitted updates are allowed in the stable database, the recovery algorithm may have to perform "undo". Redo/undo (e.g. traditional Logging) and redo/no-undo (e.g. Shadowing) are two widely used recovery algorithms <ref> [BeHG 87] </ref>. Redo/undo has low overheads during normal transaction processing, but takes a long time to process failures during crash recovery. On the other hand, redo/no-undo has higher overheads for normal processing, and considerably lower overheads for crash recovery due to no-undo, compared to redo/undo algorithm.
Reference: [Borr 84] <author> Borr, A. </author> <title> Robustness to Crash in a Distributed Database: A Non Shared-Memory Multiprocessor Approach, </title> <booktitle> Proc. 10th International Conference on Very Large Data Bases, </booktitle> <month> August </month> <year> 1984. </year>
Reference-contexts: Methods like [GMLB 81, Crus 84, MHLPS 92] partially recover the state of the database first, and then, while continuing with the recovery, allow normal processing on that part of the database which has been recovered. Some others use a hot standby <ref> [Borr 84] </ref> or non-volatile memory [CKKS 89] for increased data availability for new transactions during crash recovery.
Reference: [CKKS 89] <author> Copeland, G., Keller, T., Krishnamurthy, R., Smith, M. </author> <title> The Case for Safe RAM, </title> <booktitle> Proc. 15th International Conference on Very Large Data Bases, </booktitle> <month> August </month> <year> 1989. </year>
Reference-contexts: Methods like [GMLB 81, Crus 84, MHLPS 92] partially recover the state of the database first, and then, while continuing with the recovery, allow normal processing on that part of the database which has been recovered. Some others use a hot standby [Borr 84] or non-volatile memory <ref> [CKKS 89] </ref> for increased data availability for new transactions during crash recovery. Hence the performance of any recovery mechanism, during crash recovery, is based on how fast the system can be brought up and how much of the database is made available subsequent to a crash for normal transaction processing.
Reference: [Crus 84] <author> Crus, R. </author> <title> Data Recovery in IBM Database 2, </title> <journal> IBM Systems Journal, </journal> <volume> Vol. 23, No. 2, </volume> <year> 1984. </year>
Reference-contexts: On the other hand, system crashes do not occur frequently (typically, on the order of twice a week). Usually, new transactions are allowed after the system is brought up after failure, even before the recovery is completed. Methods like <ref> [GMLB 81, Crus 84, MHLPS 92] </ref> partially recover the state of the database first, and then, while continuing with the recovery, allow normal processing on that part of the database which has been recovered. <p> Using this information, the log is scanned forward applying the redo log records and then scanned backwards applying the undo log records for aborted transactions. Here again, the actual process depends on the particular implementation. IMS combines the analysis and redo phases into one. Some methods <ref> [GMLB 81, Crus 84] </ref> perform redo only for the lost updates of committed transactions. ARIES [MHLPS 92] does a redo for all the missing updates first, and then does an undo for the aborted updates.
Reference: [Curt 88] <author> Curtis, R. Informix-Turbo, </author> <booktitle> Proc. IEEE Compcon, February-March 1988. </booktitle> <pages> 22 </pages>
Reference-contexts: Several implementations have tried the idea of making use of no-undo property in atleast a restricted form in implementing redo/undo algorithm. The no-undo helps in limiting the amount of work to be undone during crash recovery. In <ref> [Ong 84, Curt 88] </ref> two log files are maintained. The first one, called the logical log file, records logical changes in the form of undo and redo logs. The second one, called the physical log file, is used to log the before images of the objects to be updated.
Reference: [GMLB 81] <author> Gray, J.N., McJones, P., Lindsay, B.G., Blasgen, M.W., Lorie, R.A., Price, T.G., Putzolu, F., Traiger, </author> <title> I.L. The Recovery Manager of System R Database Manager, </title> <journal> ACM Computing Surveys, </journal> <month> June </month> <year> 1981. </year>
Reference-contexts: On the other hand, system crashes do not occur frequently (typically, on the order of twice a week). Usually, new transactions are allowed after the system is brought up after failure, even before the recovery is completed. Methods like <ref> [GMLB 81, Crus 84, MHLPS 92] </ref> partially recover the state of the database first, and then, while continuing with the recovery, allow normal processing on that part of the database which has been recovered. <p> Thus all transactions that started after the "consistent point" and aborted need not be undone. The logical log now is used 1 2 to redo committed transactions and to undo aborted ones (which started before the "consistent point"), bringing the database to a logically consistent state. System R <ref> [GMLB 81] </ref> periodically saves an action-consistent state of the data file. An incremental log is used for the undo and redo log records to save changes to shared files. <p> Using this information, the log is scanned forward applying the redo log records and then scanned backwards applying the undo log records for aborted transactions. Here again, the actual process depends on the particular implementation. IMS combines the analysis and redo phases into one. Some methods <ref> [GMLB 81, Crus 84] </ref> perform redo only for the lost updates of committed transactions. ARIES [MHLPS 92] does a redo for all the missing updates first, and then does an undo for the aborted updates.
Reference: [Gray 78] <author> Gray, J.N. </author> <title> Notes on Database Operating Systems, </title> <editor> In R.Bayer, R.M.Graham and G.Seegmuller (Eds.) </editor> <booktitle> Lecture Notes in Computer Science, </booktitle> <publisher> Springer Ver-lag, </publisher> <address> New York, </address> <year> 1978. </year>
Reference-contexts: Media recovery is discussed in section 6. The paper ends with conclusions and suggestions for future work. 2 Logging & Shadowing The traditional Logging mechanism <ref> [Gray 78, HaRe 83, AgDe 85] </ref> is an excellent example for the redo/undo algorithm. Here, every update operation, besides updating the data record, also creates an "undo" and a "redo" log record. These log records are written to an append-only log on the disk.
Reference: [Gray 81] <author> Gray, J. </author> <title> The Transaction Concept: Virtues and Limitations , Proc. </title> <booktitle> 7th International Conference on Very Large Data Bases, </booktitle> <month> September </month> <year> 1981. </year>
Reference-contexts: The expected aborts percentage in an actual data base environment seems to be about 3 to 5% <ref> [HaRe 83, Gray 81] </ref>. The references are so designed as not to get trans actions into deadlocks. The third variable parameter needs some explanation. Suppose (n) is the number of transactions that can be active at any given instant (i.e. degree of multiprogramming) and each transaction has (r) data references.
Reference: [HaRe 83] <author> Haerder, T., Reuter, A. </author> <title> Principles of Transaction- Oriented Database Recovery, </title> <journal> ACM Computing Surveys, </journal> <month> December </month> <year> 1983. </year>
Reference-contexts: Media recovery is discussed in section 6. The paper ends with conclusions and suggestions for future work. 2 Logging & Shadowing The traditional Logging mechanism <ref> [Gray 78, HaRe 83, AgDe 85] </ref> is an excellent example for the redo/undo algorithm. Here, every update operation, besides updating the data record, also creates an "undo" and a "redo" log record. These log records are written to an append-only log on the disk. <p> The expected aborts percentage in an actual data base environment seems to be about 3 to 5% <ref> [HaRe 83, Gray 81] </ref>. The references are so designed as not to get trans actions into deadlocks. The third variable parameter needs some explanation. Suppose (n) is the number of transactions that can be active at any given instant (i.e. degree of multiprogramming) and each transaction has (r) data references.
Reference: [JhKh 92] <author> Jhingran, A., Khedkar, P. </author> <title> Analysis of Recovery in a Database System Using a Write-Ahead Log Protocol, </title> <type> ACM-SIGMOD, </type> <month> June </month> <year> 1992. </year>
Reference-contexts: The (LPP) accesses only those pages not accessed by any transaction. It requires at most one page of main memory, holds at most one page latch and operates only when the system is lightly loaded. We refer to <ref> [JhKh 92] </ref> for an excellent quantitative analysis of the crash recovery process. That paper examines the recovery time in a database system which uses a write-ahead log protocol, specifically ARIES. The analytical equations for log scan time, data I/O, log application and undo processing time are presented there. <p> The analytical equations for log scan time, data I/O, log application and undo processing time are presented there. The overhead during the first phase in LU-Logging scheme is the same as the log scan time 20 in ARIES. A few of the important conclusions of <ref> [JhKh 92] </ref> are presented below to give an idea of the difference in overheads. 1.
Reference: [Lori 77] <author> Lorie, R.A. </author> <title> Physical Integrity in a Large Segmented Database, </title> <journal> ACM Transactions on Database Systems, </journal> <month> March </month> <year> 1977. </year>
Reference-contexts: The actions during checkpointing and the contents of the checkpoint record depend on the particular implementation. The crash recovery algorithm consists of using the log on the disk to un-do the effects of aborted transactions and to re-do the lost updates of committed transactions. The Shadowing method <ref> [Lori 77, AgDe 85] </ref> is based on the redo/no-undo algorithm. Here, when a transaction wants to modify a page, a copy of the page is made and the modifications are done to the copy. The original unmodified copy is called the "Shadow Page".
Reference: [MHLPS 92] <author> Mohan, C., Haderle, D., Lindsay, B., Pirahesh, H., Schwarz, P. </author> <title> ARIES: A Transaction Recovery Method Supporting Fine-Granularity Locking and Partial Rollbacks Using Write-Ahead Logging, </title> <booktitle> ACM Transaction on Database Systems, </booktitle> <month> March </month> <year> 1992. </year>
Reference-contexts: On the other hand, system crashes do not occur frequently (typically, on the order of twice a week). Usually, new transactions are allowed after the system is brought up after failure, even before the recovery is completed. Methods like <ref> [GMLB 81, Crus 84, MHLPS 92] </ref> partially recover the state of the database first, and then, while continuing with the recovery, allow normal processing on that part of the database which has been recovered. <p> Section 2 briefly describes, compares and contrasts Logging and Shadowing recovery methods as examples of redo/undo and redo/no-undo algorithms. Section 3 explains in detail the LU-Logging recovery method and its implementation. Sections 4 and 5 present the transaction processing and crash recovery performance of LU-Logging as compared to ARIES <ref> [MHLPS 92] </ref>, a well-known implementation of redo/undo algorithm. Media recovery is discussed in section 6. The paper ends with conclusions and suggestions for future work. 2 Logging & Shadowing The traditional Logging mechanism [Gray 78, HaRe 83, AgDe 85] is an excellent example for the redo/undo algorithm. <p> Here again, the actual process depends on the particular implementation. IMS combines the analysis and redo phases into one. Some methods [GMLB 81, Crus 84] perform redo only for the lost updates of committed transactions. ARIES <ref> [MHLPS 92] </ref> does a redo for all the missing updates first, and then does an undo for the aborted updates. Whatever the mechanism, the recovery process in Logging is much more expensive than that using Shadowing.
Reference: [Moha 91] <author> Mohan, C. </author> <title> A Cost-Effective Method for Providing Improved Data Availability During DBMS Restart Recovery After a Failure, </title> <booktitle> Proc. 4th International Workshop on High Performance Transaction Systems, </booktitle> <month> September </month> <year> 1991. </year>
Reference-contexts: In LU-Logging, the entire database is accessible for normal transaction processing after the first phase. In contrast, in ARIES <ref> [Moha 91] </ref>, after the analysis phase, all the data pages that may potentially be affected by redo or undo phases are not made available for normal processing. After the redo phase, all pages that may potentially contain updates to be un-done are not released for transactions.
Reference: [MoPL 92] <author> Mohan, c., Pirahesh, H., Lorie, R. </author> <title> Efficient and Flexible Methods for Transient Versioning of Records to Avoid Locking by Read-Only Transactions, </title> <booktitle> ACM SIGMOD, </booktitle> <month> June </month> <year> 1992. </year>
Reference-contexts: The technique here is to create a new entry (based on the updated value) without actually deleting the previous entry. If the transaction commits, the old entry is deleted. If the transaction aborts, the new entry is removed. As pointed out in <ref> [MoPL 92] </ref>, this approach avoids the next key locking during key delete. Since index entries of deleted keys are maintained atleast until the end of transaction, logical key deletion capability comes automatically. The LU-Logging method seems to be suitable for multi-processor data base environments.
Reference: [Ong 84] <author> Ong, K. </author> <title> SYNAPSE Approach to Database Recovery, </title> <booktitle> Proc. 3rd ACM SIGACT-SIGMOD Symposium, </booktitle> <month> April </month> <year> 1984. </year>
Reference-contexts: Several implementations have tried the idea of making use of no-undo property in atleast a restricted form in implementing redo/undo algorithm. The no-undo helps in limiting the amount of work to be undone during crash recovery. In <ref> [Ong 84, Curt 88] </ref> two log files are maintained. The first one, called the logical log file, records logical changes in the form of undo and redo logs. The second one, called the physical log file, is used to log the before images of the objects to be updated.
Reference: [Reut 80] <author> Reuter, A. </author> <title> A Fast Transaction-Oriented Logging Scheme for UNDO Recovery, </title> <journal> IEEE Transactions on Software Engineering, </journal> <month> July </month> <year> 1980. </year>
Reference-contexts: Crash recovery starts with the action-consistent copy of the data file and applies the redo log records to restore the lost updates. Here again, only the aborted transactions, which started before the checkpoint, need to be undone. System R does action-consistent backups at the file level. <ref> [Reut 80] </ref> uses transaction-consistent backups, called shadow pages, at the page level. In this method, the shadows and the modified data pages are stored adjacent to each other on the disk. Locking is done at the page level. Each page-read brings in both the shadow page and the modified page.
References-found: 17

