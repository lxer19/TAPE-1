URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P543.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts95.htm
Root-URL: http://www.mcs.anl.gov
Title: High Performance Computational Chemistry: (I) Scalable Fock Matrix Construction Algorithms  
Author: Ian T. Foster, Jeffrey L. Tilson, Albert F. Wagner, Ron Shepard Robert J. Harrison, Rick A. Kendall, Rik J. Littlefield, 
Address: Argonne, IL 60439  Richland,WA 99352  
Affiliation: Argonne National Laboratory  Molecular Science Research Center Pacific Northwest Laboratory  
Abstract: Several parallel algorithms for Fock matrix construction are described. The algorithms calculate only the unique integrals, distribute the Fock and density matrices over the processors of a massively parallel computer, use blocking techniques to construct the distributed data structures, and use clustering techniques on each processor to maximize data reuse. Algorithms based on both square and row blocked distributions of the Fock and density matrices are described and evaluated. Variants of the algorithms are discussed that use either triple-sort or canonical ordering of integrals, and dynamic or static task clustering schemes. The algorithms are shown to adapt to screening, with communication volume scaling down with computation costs. Modeling techniques are used to characterize algorithm performance. Given the characteristics of existing massively parallel computers, all the algorithms are shown to be highly efficient on problems of moderate size. The algorithms using the row blocked data distribution are the most efficient. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Harrison, R. J., Guest, M. F., Kendall, R. A., Bernholdt, D. E., Wong, A. T., Stave, M., Anchell, J., Hess, A. C., Littlefield, R. L., Fann, G. L., Nieplocha, J., Thomas, G. S., Elwood, D., Tilson, J., Shepard, R. L., Wagner, A. F., Foster, I., Lusk, E., and Stevens, R. </author> <note> submitted J. Comp. </note> <institution> Chem., </institution> <year> 1994. </year>
Reference-contexts: 1 Introduction The high computational power and large aggregate memory of massively parallel processing (MPP) supercomputers gives these machines the potential to solve Grand Challenge-class problems in computational chemistry. In this and a companion paper <ref> [1] </ref>, we report 1 our initial efforts to develop effective ab initio electronic structure codes for MPP comput-ers that are capable of solving problems with O (10 23 ) atoms and O (10 34 ) basis functions. <p> In addition, analysis shows that communication costs are significantly less than computation costs on most reasonable computer systems. Hence, the algorithms are expected to be efficient and scalable. The results of this analysis are confirmed in the companion paper <ref> [1] </ref>. The rest of the paper is organized as follows. In Section 2, we describe the basic features of the algorithm in the absence of screening. In Section 3, we describe variations 3 that can make the basic algorithm more versatile and efficient. <p> We show that static clustering is superior from the point of view of communication requirements. However, it tends to suffer from load imbalances in the presence of screening. Hence, we recommend the more flexible dynamic clustering technique. In a companion paper <ref> [1] </ref>, a fully scalable code is presented that exploits the algorithms discussed here and, in addition, addresses other practical issues such as the distribution of all data structures of order N 2 (e.g., the overlap integrals used in screening), the development of a communications library for efficient data transfer, coding modifications
Reference: [2] <author> Roothaan, </author> <title> C., </title> <journal> Reviews of Modern Physics, </journal> <volume> 23, 69, </volume> <year> 1951. </year>
Reference-contexts: Hence, a sound methodology when developing parallel algorithms is to begin by examining algorithmic alternatives at a theoretical level. Only once scalability has been established should effort be devoted to implementations on parallel computers. In this paper, we apply this methodology to the direct closed-shell self-consistent field (SCF) method <ref> [2, 3] </ref>. See [4] (and references therein) for a recent review of current parallel SCF development. SCF is an important method in its own right and, in addition, is typical of other more sophisticated methods in its use of large data structures and irregular data access patterns.
Reference: [3] <author> Almlof, J., Faegri, K. Jr., Korsell, K., J. </author> <title> Comp. </title> <journal> Chem, </journal> <volume> 3, 385, </volume> <year> 1982. </year>
Reference-contexts: Hence, a sound methodology when developing parallel algorithms is to begin by examining algorithmic alternatives at a theoretical level. Only once scalability has been established should effort be devoted to implementations on parallel computers. In this paper, we apply this methodology to the direct closed-shell self-consistent field (SCF) method <ref> [2, 3] </ref>. See [4] (and references therein) for a recent review of current parallel SCF development. SCF is an important method in its own right and, in addition, is typical of other more sophisticated methods in its use of large data structures and irregular data access patterns. <p> Screening may be accomplished by the use of a tolerance limit on the integrals, specified in Figs 2 and 3. In large molecules, this screening criterion can reduce the cost of constructing F from a nominal O (N 4 ) integrals to, in the limit, O (N 2 ) <ref> [3, 5] </ref>. The effect of screening on communication requirements cannot be analytically determined without some approximate representation of which index values will pass the screening tests. Hence, we make two simplifying assumptions: 15 1.
Reference: [4] <author> Harrison, R. J., Shepard, R, </author> <note> For publication in Annual Review of Physical Chemistry, </note> <year> 1994. </year>
Reference-contexts: Only once scalability has been established should effort be devoted to implementations on parallel computers. In this paper, we apply this methodology to the direct closed-shell self-consistent field (SCF) method [2, 3]. See <ref> [4] </ref> (and references therein) for a recent review of current parallel SCF development. SCF is an important method in its own right and, in addition, is typical of other more sophisticated methods in its use of large data structures and irregular data access patterns.
Reference: [5] <author> Haser, M., Ahlrichs, R., J. </author> <title> Comp. </title> <journal> Chem, </journal> <volume> 10, 104, </volume> <year> 1989. </year>
Reference-contexts: Screening may be accomplished by the use of a tolerance limit on the integrals, specified in Figs 2 and 3. In large molecules, this screening criterion can reduce the cost of constructing F from a nominal O (N 4 ) integrals to, in the limit, O (N 2 ) <ref> [3, 5] </ref>. The effect of screening on communication requirements cannot be analytically determined without some approximate representation of which index values will pass the screening tests. Hence, we make two simplifying assumptions: 15 1.
Reference: [6] <author> Littlefield, R., and Maschhoff, K.,Theor. </author> <title> Chim. </title> <journal> Acta, </journal> <volume> 84, 457, </volume> <year> 1993. </year>
Reference-contexts: First, a two-dimensional Fock matrix F is constructed from the current estimate of the wavefunction. Second, F is diagonalized to obtain an improved estimate of the wavefunction. The second step is computationally trivial on sequential computers but can become a rate limiting step on large numbers of processors <ref> [6] </ref>. In future work, we will explore an alternative scheme proposed by Shepard [7] that avoids the need for diagonalization. We focus on the Fock matrix construction problem in this paper.
Reference: [7] <author> Shepard, R., </author> <title> Theor. </title> <journal> Chim. Acta, </journal> <volume> 84, 343, </volume> <year> 1993. </year>
Reference-contexts: Second, F is diagonalized to obtain an improved estimate of the wavefunction. The second step is computationally trivial on sequential computers but can become a rate limiting step on large numbers of processors [6]. In future work, we will explore an alternative scheme proposed by Shepard <ref> [7] </ref> that avoids the need for diagonalization. We focus on the Fock matrix construction problem in this paper.
Reference: [8] <author> Clementi, E., Corongiu, G., Detrich, J., Chin, S., Domingo, L., </author> <title> Int. </title> <journal> J. Quant. Chem.: Quantum Chem. Symp., </journal> <volume> 18, 601, </volume> <year> 1984. </year>
Reference: [9] <author> Dupuis, M., Watts, J. D., </author> <title> Theor. </title> <journal> Chim. Acta, </journal> <volume> 71, 91, </volume> <year> 1987. </year> <month> 23 </month>
Reference: [10] <author> Feyereisen, M., Kendall, R., Nichols, J., Dame, D., Golab, J., J. </author> <title> Comp. </title> <journal> Chem., </journal> <volume> 14, 818, </volume> <year> 1993. </year>
Reference: [11] <author> Brode, S., Horn, H., Ehrig, M., Moldrup, D., Rice, J., Ahlrichs, R., J. </author> <title> Comp. </title> <journal> Chem., </journal> <volume> 14, 1142, </volume> <year> 1993. </year>
Reference: [12] <author> Burkhardt, A., Wedig, U., Schnering, H. G. v., </author> <title> Theor. </title> <journal> Chim. Acta., </journal> <volume> 86, 497, </volume> <year> 1993. </year>
Reference: [13] <author> Shirsat, R., Limaye, A., Gadre, S., J. </author> <title> Comp. </title> <journal> Chem., </journal> <volume> 14, 445, </volume> <year> 1993. </year>
Reference: [14] <author> Luthi, H., Mertz, M., Feyereisen, M., </author> <title> and Almlof, </title> <journal> J., Intl J. Quant. Chem., </journal> <volume> 13, 160, </volume> <year> 1992. </year>
Reference: [15] <author> Feyereisen, M., and Kendall, R., Theo. </author> <title> Chim. </title> <journal> Acta., </journal> <volume> 84, 289, </volume> <year> 1993. </year>
Reference-contexts: This approach simplified implementation and achieved high performance. However, the replicated data restricted scalability: the maximum problem size that could be solved was limited by the amount of memory on a single processor. For example, Feyereisen and Kendall's parallel DISCO code <ref> [15] </ref> is limited to approximately 400 basis functions (without symmetry) on the 512-processor Intel Touchstone Delta computer, which has 16 MB of memory per processor. Nevertheless, these studies provided much useful information on the distribution of computational tasks, load balancing and task scheduling, etc. <p> A consequence of this variation in task cost is that an allocation strategy that places an equal number of tasks on each processor may suffer from load imbalances. One solution to this problem is to use a centralized scheduler to allocate tasks to processors in a demand-driven manner <ref> [15, 19] </ref>. Scheduler-based techniques can achieve excellent load balance, but increase communication requirements and are not truly scalable. Alternatively, a random allocation strategy can be used. This relies on the law of large numbers to balance the computational load.
Reference: [16] <author> Luthi, H. P., </author> <title> Almlof, </title> <journal> Theor. Chim. Acta., </journal> <volume> 84, 443, </volume> <year> 1993. </year>
Reference: [17] <author> Guest, M., Sherwood, P., van Lenthe, J., </author> <title> Theor. </title> <journal> Chim. Acta., </journal> <volume> 84, 423, </volume> <year> 1993. </year>
Reference: [18] <author> Colvin, M., Janssen, C., Whiteside, R., and Tong, C., Theo. </author> <title> Chim. </title> <journal> Acta., </journal> <volume> 84, 301, </volume> <year> 1993. </year>
Reference-contexts: A scalable parallel Fock matrix construction algorithm must distribute the D and F matrices over available processors, so that the maximum problem size is limited only by the aggregate memory available on the MPP computer. In Colvin et al.'s systolic algorithm <ref> [18] </ref>, Fock and density submatrices are circulated among processors. However, this approach requires the computation of 3N 4 =8 integrals and suffers from an overly synchronous computational model. Furlani and King [19] describe an algorithm that avoids these deficiencies.
Reference: [19] <author> Furlani, T. R., King, H. F., J. </author> <note> Comp. Chem., submitted for publication., </note> <year> 1994. </year>
Reference-contexts: In Colvin et al.'s systolic algorithm [18], Fock and density submatrices are circulated among processors. However, this approach requires the computation of 3N 4 =8 integrals and suffers from an overly synchronous computational model. Furlani and King <ref> [19] </ref> describe an algorithm that avoids these deficiencies. <p> A consequence of this variation in task cost is that an allocation strategy that places an equal number of tasks on each processor may suffer from load imbalances. One solution to this problem is to use a centralized scheduler to allocate tasks to processors in a demand-driven manner <ref> [15, 19] </ref>. Scheduler-based techniques can achieve excellent load balance, but increase communication requirements and are not truly scalable. Alternatively, a random allocation strategy can be used. This relies on the law of large numbers to balance the computational load. <p> This should also be true of the blocked algorithm where the random allocation would be at the head of the l loop in the innerloop of Figs 2 and 3. Hybrid schemes that use both a scheduler and random mapping are also possible <ref> [19] </ref>. 10 2.4 Blocked Algorithm Summary Blocking is equivalent to the use of a non-unit stride I c in the nested loops executed when constructing the Fock matrix. <p> Empirical studies performed with this code confirm the general conclusions of this paper as does the work of Furlani and King <ref> [19] </ref>. Future work is being directed in three broad areas.
Reference: [20] <author> Harrison, R., Theo. </author> <title> Chim. </title> <journal> Acta., </journal> <volume> 84, 363, </volume> <year> 1993. </year>
Reference: [21] <author> A. Edelman, J. </author> <booktitle> Parallel and Distributed Computing, </booktitle> <volume> 11, 328, </volume> <year> 1991. </year>
Reference-contexts: As illustrated in Fig. 4, the canonical ordering produces an F that must be symmetrized. The actual arithmetic operations involved in the symmetrization, namely summing the symmetrically related elements of F , will normally be insignificant. However, the operation has the same communication requirements as a parallel matrix transpose <ref> [21, 22] </ref>. The usual algorithm requires that each processor exchange data with every other processor, for a total of P 2 communications on a computer with P processors. Assuming O (N ) processors, the total communication costs are O (N 2 ) data in O (N 2 ) messages.
Reference: [22] <author> S. L. Johnsson and C-T. </author> <title> Ho, </title> <journal> SIAM J. Matrix Anal. Appl. </journal> <volume> 9, 419, </volume> <year> 1988. </year> <month> 24 </month>
Reference-contexts: As illustrated in Fig. 4, the canonical ordering produces an F that must be symmetrized. The actual arithmetic operations involved in the symmetrization, namely summing the symmetrically related elements of F , will normally be insignificant. However, the operation has the same communication requirements as a parallel matrix transpose <ref> [21, 22] </ref>. The usual algorithm requires that each processor exchange data with every other processor, for a total of P 2 communications on a computer with P processors. Assuming O (N ) processors, the total communication costs are O (N 2 ) data in O (N 2 ) messages.
References-found: 22

