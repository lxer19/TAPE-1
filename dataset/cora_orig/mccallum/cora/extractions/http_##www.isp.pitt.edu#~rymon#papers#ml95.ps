URL: http://www.isp.pitt.edu/~rymon/papers/ml95.ps
Refering-URL: http://www.kdnuggets.com/sift/se-learn.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: E-mail: Rymon@ISP.Pitt.edu  
Title: Where Do SE-trees Perform? (Part I)  
Author: Ron Rymon 
Date: March 5, 1995  
Address: Pittsburgh Pittsburgh, PA 15260  
Affiliation: Intelligent Systems Program 901 Cathedral of Learning University of  
Abstract: As a classifier, a Set Enumeration (SE) tree can be viewed as a generalization of decision trees. We empirically characterize domains in which SE-trees are particularly advantageous relative to decision trees. Specifically, we show that: 
Abstract-found: 1
Intro-found: 1
Reference: [Breiman et al., 1984] <author> Breiman, L., Friedman, J., Olshen, R., and Stone, C., </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth, </publisher> <address> Belmont, </address> <year> 1984. </year>
Reference: [Buntine, 90] <author> Buntine, W., </author> <title> Myths and Legends in Learning Classification Rules. </title> <booktitle> Proc. AAAI-90, </booktitle> <address> Boston, MA, </address> <pages> pp. 736-742, </pages> <year> 1990. </year>
Reference: [Mitchell, 80] <author> Mitchell, T. M., </author> <title> The Need for Biases in Learning Generalizations. </title> <type> Technical Report 5-110, </type> <institution> Rutgers University, </institution> <year> 1980. </year>
Reference-contexts: A related advantage of the new framework is in the way in which it handles the need for an external preference criterion, or bias <ref> [Mitchell, 80, Utgoff, 86] </ref>. Buntine (1990) distinguishes a search bias, the result of a program's inability to search the entire space, from the hypothesis-space bias, an preference criterion inherent to the domain. The decision tree framework often suffers from its search bias, stemming from premature attribute selection decisions.
Reference: [Murphy, 94] <author> Murphy, P. M., </author> <title> An Empirical Analysis of the Benefit of Decision Tree Size Biases as a Function of Concept Distribution. </title> <type> Unpublished manuscript. </type>
Reference: [Quinlan, 86] <author> Quinlan, J. R., </author> <title> Induction of Decision Trees. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: More than a handful of frameworks for decision tree induction have been proposed throughout the years. The experiments reported here used decision trees that would have been generated using the ID3 framework <ref> [Quinlan, 86] </ref>, i.e. using the Information Gain attribute selection measure. For verification, however, we have separately experimented with attribute selection based on the GINI, Gain Ratio, and Chi-Square measures, and found no significant differences. Another decision was to use unpruned decision trees.
Reference: [Rymon, 1992] <author> Rymon, R., </author> <title> Search through Systematic Set Enumeration. </title> <booktitle> In Proceedings Third International Conference on Principles of Knowledge Representation and Reasoning, </booktitle> <address> Cambridge MA, </address> <pages> pp. 539-550, </pages> <year> 1992. </year>
Reference-contexts: Sections 5, and 6 present the details, and results, of the corresponding studies. Finally, Section 7 compares SE-trees and decision trees for problems of varying complexity. 2 SE-tree-based Induction Set Enumeration trees were first proposed simply as a way to systematically search a space of sets <ref> [Rymon, 1992] </ref>. Given a set of attributes, or features, a complete SE-tree is a tree representation of all sets of attribute-value pairs. A fixed ordering of the underlying set of attributes is used to do so systematically, i.e. to uniquely represent all such sets.
Reference: [Rymon, 1993] <author> Rymon, R., </author> <title> An SE-tree-based Characterization of the Induction Problem. </title> <booktitle> In Proceedings Tenth International Conference on Machine Learning, </booktitle> <pages> pp. 268-275, </pages> <address> Amherst MA, </address> <year> 1993. </year>
Reference-contexts: A wealth of research which followed their work has resulted in a variety of improved techniques for decision tree induction. In <ref> [Rymon, 1993] </ref>, we have proposed a generalization of decision tree-based induction which uses Set Enumeration (SE) trees. We have shown that the new framework can utilize many of the sub-techniques developed for decision trees. <p> We also examine the standard deviation of each classifier's performance, as well as its complexity (size). Finally, we develop a simple complexity measure for tree-inducibility, and show that the SE-tree-based framework is particularly advantageous in more complex domains. Section 2 reviews SE-tree-based induction; the interested reader is referred to <ref> [Rymon, 1993] </ref> for a more in-depth presentation. Section 3 discusses the two main hypotheses, and Section 4 outlines the experimentation methodology. Sections 5, and 6 present the details, and results, of the corresponding studies. <p> Algorithmically, an SE-tree can be custom-built to embed anyone's favorite decision tree. A hill-climbing variation of SE-Learn starts by first exploring nodes of this favorite tree (this requires dynamic re-ordering of the attributes; see <ref> [Rymon, 1993] </ref>). Other parts of the SE-tree are then added if they result in an improved model. This procedure is important since complete SE-trees tend to grow very quickly and become quite large. <p> In general, the SE-tree is more sensitive to the problem size because it may be impossible for much of it to be completely explored. On the other hand, as shown in <ref> [Rymon, 1993] </ref>, a partially explored SE-tree will typically suffice to outperform a given decision tree. We chose to experiment with domains of a fixed size: 10 binary attributes and a binary class. <p> Exploration policy and extent of exploration. In the reported experiments, we have explored whole SE-trees. This was a comfortable choice, in part because it renders the choice of an exploration policy unimportant. As noted in <ref> [Rymon, 1993] </ref>, whole SE-trees are typically not as good classifiers as some par tially explored sub-trees. Resolution criterion. In general, a domain-specific resolution criterion would be ideal. Lacking such, we used a simple weight-based resolution criterion.
Reference: [Rymon, 1994] <author> Rymon, R., </author> <title> On Kernel Rules and Prime Implicants. </title> <booktitle> In Proceedings Twelfth National Conference on Artificial Intelligence, </booktitle> <address> Seattle WA, </address> <year> 1994, </year> <pages> pp. 181-186. </pages>
Reference-contexts: A kernel rule is a minimal rule, i.e. such that none of its subsets is also a rule. Kernel rules are thus sets of attribute-value pairs, making SE-tree-based search a natural choice. In <ref> [Rymon, 1994] </ref>, we show that kernel rules are close relatives of prime implicants (PIs). As a result of this relationship, we show that any of a number of PI generation algorithms can be used to compute kernel rules.
Reference: [Schaffer, 94] <author> Schaffer, C., </author> <title> A Conservation Law for Generalization Performance. </title> <booktitle> In Proceedings Eleventh International Conference on Machine Learning, </booktitle> <pages> pp. 259-265, </pages> <address> New Brunswick, NJ, </address> <year> 1994. </year>
Reference-contexts: In the experiments reported in this paper, we used an implementation of SE-Learn in which an ID3 decision tree is explored first, and then expanded upon. 3 The Hypotheses Schaffer's Conservation Law for Generalization Performance <ref> [Schaffer, 94] </ref> states that it is impossible for one classifier to outperform another on a class of learning problems without paying for it by underperforming its counterpart on another subset of such problems.
Reference: [Schrag, 94] <author> Schrag, R. C., </author> <title> Private communication. </title>
Reference-contexts: In the learning-phase, this estimate can serve to tune the bias of further exploration. More importantly, this measure can help tune classification-phase bias. Some very preliminary results suggest that a resolution criterion which prefers more specific rules does better on problems which are more complex a la Murphy's measure <ref> [Schrag, 94] </ref>. Algorithmically, there is of course much more work to be done in identifying a good in-operation estimate, and tying its values to a range of biases. 8 Conclusion and Future Research We take first steps in characterizing domains in which SE-tree-based learning can be useful.
Reference: [Utgoff, 86] <author> Utgoff, P. E., </author> <title> Machine Learning of Inductive Bias. </title> <publisher> Kluwer Academic, </publisher> <address> Boston MA, </address> <year> 1986. </year>
Reference-contexts: A related advantage of the new framework is in the way in which it handles the need for an external preference criterion, or bias <ref> [Mitchell, 80, Utgoff, 86] </ref>. Buntine (1990) distinguishes a search bias, the result of a program's inability to search the entire space, from the hypothesis-space bias, an preference criterion inherent to the domain. The decision tree framework often suffers from its search bias, stemming from premature attribute selection decisions.
References-found: 11

