URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3420/3420.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Email: subrata@cs.umd.edu  
Phone: Telephone: (301) 405-2717 Fax: (301) 405-6707  Calcutta 700 027  Telephone: (301) 405-2684 Fax: (301) 405-6707  
Title: Improving the Efficiency of Limited-Memory Heuristic Search  INDIA  
Author: Subrata Ghosh Ambuj Mahanti Dana S. Nau Dana S. Nau 
Note: Address all correspondence to  Supported in part by NSF Grants NSFD CDR-88003012, IRI-9306580, and CMDS project (work order no. 019/7-148/CMDS-1039/90-91).  
Address: College Park, MD 20742 USA  Diamond Harbour Road P.O. Box No. 16757  College Park, MD 20742 USA  
Affiliation: Department of Computer Science University of Maryland  Indian Institute of Management Calcutta  Department of Computer Science and Institute for Systems Research University of Maryland  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> A. Bagchi and A. Mahanti. </author> <title> Search algorithms under different kinds of heuristics-a comparative study. </title> <journal> JACM, </journal> <volume> 30(1) </volume> <pages> 1-21, </pages> <year> 1983. </year> <month> 35 </month>
Reference-contexts: The cost of an arc (m; n) in G is denoted by c (m; n). Let P be any path from the start node s. Then the function pathmax (P ) <ref> [1] </ref> is defined as follows: pathmax (P ) = max n2P where c (P; s; n) is the cost of the subpath of P that goes from s to n.
Reference: [2] <author> P. P. Chakrabarti, S. Ghosh, A. Acharya, and S. C. De Sarkar. </author> <title> Heuristic search in restricted memory. </title> <journal> Artificial Intelligence, </journal> <volume> 47 </volume> <pages> 197-221, </pages> <year> 1989. </year>
Reference-contexts: MA* does top-down propagation of h-values. In particular, if MA* generates a node m as a child of some other node n, then for each child q of m MA* computes heuristic value with respect to the heuristic value at node n (see <ref> [2] </ref>, pp. 199). Thus, the f -values of generated nodes are strictly nondecreasing, regardless of whether the heuristic function h is monotone. In contrast, ITS does not do downward propagation of h-values. 17 2.
Reference: [3] <author> R. Dechter and J. Pearl. </author> <title> Generalized best-first search strategies and the optimality of A*. </title> <journal> JACM, </journal> <volume> 32(3) </volume> <pages> 505-536, </pages> <year> 1985. </year>
Reference-contexts: 1 Introduction Although A* is usually very efficient in terms of number of node expansions <ref> [3] </ref>, it requires an exponential amount of memory, and thus runs out of memory even on problem instances of moderate size. This problem led to the development of algorithm IDA* [8]. <p> The process of node selection and expansion continues until a goal node is selected for expansion. One major problem with A* is the amount of memory required to store nodes in OPEN and CLOSED. As shown in <ref> [3] </ref>, every admissible search algorithm must expand all surely expandable nodes before finding a solution.
Reference: [4] <author> M. Evett, J. Hendler, A. Mahanti, and D. Nau. PRA*: </author> <title> A memory-limited heuristic search procedure for the connection machine. </title> <booktitle> In Frontiers'90: Frontiers of Massively Parallel Computation, </booktitle> <year> 1990. </year>
Reference-contexts: These algorithms can be categorized into two classes: (1) the first class uses additional memory to store more nodes than IDA*, and thereby reduce regeneration of some nodes. The algorithms which belong to this class are MREC, MA*, RA* <ref> [4] </ref>, SMA* [13], and ITS, and (2) the second class of algorithms attempts to reduce node regenerations by 5 reducing the number of iterations, by increasing the threshold more liberally than IDA*. IDA* CR [14], DFS* [12], and MIDA* [16] belong to this class.
Reference: [5] <author> S. Ghosh, A. Mahanti, and D. S. Nau. </author> <title> ITS: An efficient limited-memory heuristic tree search algorithm. </title> <booktitle> In AAAI 1994, </booktitle> <pages> pages 1353-1358, </pages> <year> 1994. </year>
Reference-contexts: Our results show that if the heuristic branching factor is low and the node-generation time is high (which is 1 Some of these results have also been summarized briefly in <ref> [5] </ref>. 2 the case for most practical problems), ITS can provide significant savings in both number of node generations and running time. 4.
Reference: [6] <author> P. E. Hart, N. J. Nilsson, and B. Raphael. </author> <title> A formal basis for the heuristic determination of minimum cost paths. </title> <journal> IEEE Transactions on Systems Sciences and Cybernetics, </journal> <pages> pages 1556-1562, </pages> <year> 1968. </year>
Reference-contexts: Since the number of nodes on any path is one more than the length of the path, 2 L = 1 + maxflength (P ) : pathmax (P ) h fl (s)g: 2.1 Algorithm A* The best known admissible algorithm is A* <ref> [6, 11] </ref>. A* works in a best-first manner. It maintains two lists: OPEN, which contains nodes that are to be expanded, and CLOSED, which contains nodes that have already been expanded.
Reference: [7] <author> E. Ignall and L. </author> <title> Schrage. Applications of the branch and bound technique to some flow-shop scheduling problems. </title> <journal> Operations Research, </journal> <volume> 13(3) </volume> <pages> 400-412, </pages> <year> 1965. </year>
Reference-contexts: In our experiments, we selected the number of machines to be 3. We used a search-space representation and admissible node evaluation function of Ignall and Schrage <ref> [7] </ref>. For ITS (0), there is a special case to consider. In the flow-shop scheduling problem, it is very easy to generate the successor n 0 of a node n.
Reference: [8] <author> R. E. Korf. </author> <title> Depth first iterative deepening: An optimal admissible tree search. </title> <journal> Artificial Intelligence, </journal> <volume> 27 </volume> <pages> 97-109, </pages> <year> 1985. </year>
Reference-contexts: 1 Introduction Although A* is usually very efficient in terms of number of node expansions [3], it requires an exponential amount of memory, and thus runs out of memory even on problem instances of moderate size. This problem led to the development of algorithm IDA* <ref> [8] </ref>. IDA*'s memory requirement is only linear in the depth of the search, enabling it to solve larger problems than A* can solve in practice. However, when additional memory is available, IDA* does not make use of this memory to reduce the number of node expansions. <p> with some measure of the problem size|and since A* keeps track of all of these nodes, it needs an exponential amount of memory in which to store these nodes. 2.2 Algorithm IDA* To overcome the storage problem, a variant of A* called IDA* (Iterative Deepening A*) was introduced by Korf <ref> [8, 9] </ref>. Basically, IDA* performs a depth-first search, backtracking whenever 2 The length of a path P is the number of arcs in P . 4 procedure IDA*: Let z; z 0 be global variables. Set z := h (s), where s is the start node.
Reference: [9] <author> R. E. Korf. </author> <title> Optimal path finding algorithms. </title> <editor> In L. Kanal and V. Kumar, editors, </editor> <booktitle> Search in Artificial Intelligence, </booktitle> <pages> pages 200-222. </pages> <publisher> Springer Verlag, </publisher> <year> 1988. </year>
Reference-contexts: with some measure of the problem size|and since A* keeps track of all of these nodes, it needs an exponential amount of memory in which to store these nodes. 2.2 Algorithm IDA* To overcome the storage problem, a variant of A* called IDA* (Iterative Deepening A*) was introduced by Korf <ref> [8, 9] </ref>. Basically, IDA* performs a depth-first search, backtracking whenever 2 The length of a path P is the number of arcs in P . 4 procedure IDA*: Let z; z 0 be global variables. Set z := h (s), where s is the start node. <p> In other words, IDA*'s memory requirement grows only linearly with the depth of the search. This enables IDA* to solve much larger problems than A* can solve, such as the 15-puzzle <ref> [9] </ref>. Thus IDA* has drawn significant attention from the AI research community. 2.3 Other Limited-Memory Algorithms Following IDA*, several other limited-memory algorithms have been designed to reduce the number of node generations compared to IDA*. <p> average, over j = 2; : : : ; I, of the quantity x new (j) : Intuitively, this is the average ratio of the number of nodes of each f -value (assuming that the heuristic is monotone) to the the number of nodes at the next smaller f -value <ref> [9] </ref>. 4.2 Basic Properties For i = 1; 2; : : :, the i'th instant in the operation of ITS is the i'th time that Step 2 (b)i is executed, i.e., the i'th time that ITS selects a tip branch for expansion.
Reference: [10] <author> J. D. Little, K. G. Murty, D. W. Sweeney, and C. Karel. </author> <title> An algorithm for the traveling salesman problem. </title> <journal> Operations Research, </journal> <volume> 11 </volume> <pages> 972-989, </pages> <year> 1963. </year>
Reference-contexts: A tour is a path that starting at some initial city visits every city once and only once, and returns to the initial city. We chose the well known method of Little et al. <ref> [10] </ref> to represent the search space and the lower bound heuristic for the Traveling Salesman Problem. The search space in this formulation is a binary tree.
Reference: [11] <author> N. J. Nilsson. </author> <booktitle> Principles of Artificial Intelligence. </booktitle> <publisher> Tioga, </publisher> <address> Palo Alto, </address> <year> 1980. </year> <month> 36 </month>
Reference-contexts: Since the number of nodes on any path is one more than the length of the path, 2 L = 1 + maxflength (P ) : pathmax (P ) h fl (s)g: 2.1 Algorithm A* The best known admissible algorithm is A* <ref> [6, 11] </ref>. A* works in a best-first manner. It maintains two lists: OPEN, which contains nodes that are to be expanded, and CLOSED, which contains nodes that have already been expanded. <p> Every time MA* generates a node p such that f (p) exceeds the current threshold z, MA* recursively propagates the f -values upwards in a manner similar to the upward propagation done in the AO* algorithm <ref> [11] </ref>. In contrast, ITS does not do upward propagation of this kind. The closest that it comes to anything similar to this is a one-level back-up of B-values when it retracts a node. 6 Comparison of ITS with IDA* 6.1 Theoretical Results In this section we compare ITS with IDA*.
Reference: [12] <author> V. N. Rao, V. Kumar, and R. E. Korf. </author> <title> Depth-first vs. best-first search. </title> <booktitle> In AAAI-1991, </booktitle> <pages> pages 434-440, </pages> <address> Anaheim, California, </address> <year> 1991. </year>
Reference-contexts: The algorithms which belong to this class are MREC, MA*, RA* [4], SMA* [13], and ITS, and (2) the second class of algorithms attempts to reduce node regenerations by 5 reducing the number of iterations, by increasing the threshold more liberally than IDA*. IDA* CR [14], DFS* <ref> [12] </ref>, and MIDA* [16] belong to this class. Like IDA*, MREC is a recursive search algorithm. The difference between MREC and other algorithms in its class is that MREC allocates its memory statically, in the order in which nodes are generated.
Reference: [13] <author> S. Russell. </author> <title> Efficient memory-bounded search methods. </title> <booktitle> In ECAI-1992, </booktitle> <address> Vienna, Austria, </address> <year> 1992. </year>
Reference-contexts: These algorithms can be categorized into two classes: (1) the first class uses additional memory to store more nodes than IDA*, and thereby reduce regeneration of some nodes. The algorithms which belong to this class are MREC, MA*, RA* [4], SMA* <ref> [13] </ref>, and ITS, and (2) the second class of algorithms attempts to reduce node regenerations by 5 reducing the number of iterations, by increasing the threshold more liberally than IDA*. IDA* CR [14], DFS* [12], and MIDA* [16] belong to this class. Like IDA*, MREC is a recursive search algorithm.
Reference: [14] <author> U. K. Sarkar, P. P. Chakrabarti, S. Ghose, and S. C. De Sarkar. </author> <title> Reducing reexpansions in iterative deepening search by controlling cutoff bounds. </title> <journal> Artificial Intelligence, </journal> <volume> 50(2) </volume> <pages> 207-221, </pages> <year> 1991. </year>
Reference-contexts: The algorithms which belong to this class are MREC, MA*, RA* [4], SMA* [13], and ITS, and (2) the second class of algorithms attempts to reduce node regenerations by 5 reducing the number of iterations, by increasing the threshold more liberally than IDA*. IDA* CR <ref> [14] </ref>, DFS* [12], and MIDA* [16] belong to this class. Like IDA*, MREC is a recursive search algorithm. The difference between MREC and other algorithms in its class is that MREC allocates its memory statically, in the order in which nodes are generated.
Reference: [15] <author> A. Sen and A. Bagchi. </author> <title> Fast recursive formulations for best-first search that allow controlled use of memory. </title> <booktitle> In IJCAI-89, </booktitle> <pages> pages 274-277, </pages> <year> 1989. </year>
Reference: [16] <author> B. W. Wah. MIDA*, </author> <title> an IDA* search with dynamic control. </title> <type> Technical Report UILU-ENG-91-2216, </type> <institution> University of Illinois at Urbana, Champaign-Urbana, IL, </institution> <year> 1991. </year> <month> 37 </month>
Reference-contexts: IDA* CR [14], DFS* [12], and MIDA* <ref> [16] </ref> belong to this class. Like IDA*, MREC is a recursive search algorithm. The difference between MREC and other algorithms in its class is that MREC allocates its memory statically, in the order in which nodes are generated.
References-found: 16

