URL: ftp://ftp.cs.colorado.edu/users/baveja/Papers/ML92.ps.gz
Refering-URL: http://www.cs.colorado.edu/~baveja/papers.html
Root-URL: 
Title: Scaling Reinforcement Learning Algorithms by Learning Variable Temporal Resolution Models  
Author: Satinder P. Singh 
Address: Amherst, MA 01003  
Affiliation: Department of Computer Science University of Massachusetts  
Note: As appeared in the Proceedings of the Ninth Machine Learning Conference, pp. 406-415. 1992.  
Abstract: The close connection between reinforcement learning (RL) algorithms and dynamic programming algorithms has fueled research on RL within the machine learning community. Yet, despite increased theoretical understanding, RL algorithms remain applicable to simple tasks only. In this paper I use the abstract framework afforded by the connection to dynamic programming to discuss the scaling issues faced by RL researchers. I focus on learning agents that have to learn to solve multiple structured RL tasks in the same environment. I propose learning abstract environment models where the abstract actions represent "intentions" of achieving a particular state. Such models are variable temporal resolution models because in different parts of the state space the abstract actions span different number of time steps. The operational definitions of abstract actions can be learned incrementally using repeated experience at solving RL tasks. I prove that under certain conditions solutions to new RL tasks can be found by using simu lated experience with abstract actions alone.
Abstract-found: 1
Intro-found: 1
Reference: <author> Barto, A. G., Bradtke, S., & Singh, S. P. </author> <year> (1993). </year> <title> Learning to act using real-time dynamic programming. </title> <type> Technical Report 93-02, </type> <institution> University of Mas-sachusetts, </institution> <address> Amherst, MA. </address> <note> Submitted to: AI Journal. </note>
Reference: <author> Barto, A. G. & Singh, S. P. </author> <month> (Nov. </month> <year> 1990). </year> <title> On the computational economics of reinforcement learning. </title> <booktitle> In Proceedings of the 1990 Connectionist Models Summer School, </booktitle> <address> San Mateo, CA. </address> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: For learning tasks where a model of the environment is not available at the beginning, indirect learning algorithms <ref> (see Barto and Singh 1990) </ref> use system identification techniques to learn a model on-line. Equation 1 can then be used by substituting the estimated transition probabilities for the real transition probabilities. <p> Indeed, for some MDTs it is possible to directly determine an optimal policy by using actual experience at controlling the environment at the highest temporal resolution <ref> (Barto and Singh 1990) </ref>, before the environment model becomes accurate enough to be useful. However, if the learning agent has to learn to solve multiple MDTs (Singh 1992c) in the same environment, the cost of constructing a VTRM can be amortized across the tasks.
Reference: <author> Barto, A. G., Sutton, R. S., & Watkins, C. </author> <year> (1990). </year> <title> Sequential decision problems and neural networks. </title>
Reference-contexts: For learning tasks where a model of the environment is not available at the beginning, indirect learning algorithms <ref> (see Barto and Singh 1990) </ref> use system identification techniques to learn a model on-line. Equation 1 can then be used by substituting the estimated transition probabilities for the real transition probabilities. <p> Indeed, for some MDTs it is possible to directly determine an optimal policy by using actual experience at controlling the environment at the highest temporal resolution <ref> (Barto and Singh 1990) </ref>, before the environment model becomes accurate enough to be useful. However, if the learning agent has to learn to solve multiple MDTs (Singh 1992c) in the same environment, the cost of constructing a VTRM can be amortized across the tasks.
Reference: <editor> In Touretzky, D. S. (Ed.), </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <pages> pages 686-693, </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Bertsekas, D. P. </author> <year> (1987). </year> <title> Dynamic Programming: Deterministic and Stochastic Models. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall. </publisher>
Reference-contexts: Figure 1 shows a block diagram representation of an MDT. At each time step the agent observes the state of the environment, executes an action, and receives a payoff in return. In MDTs <ref> (Bertsekas 1987) </ref> the objective function to be maximized is often of the form, J n (i) = P n where i is the starting state, R (t) is the expected payoff received at time step t, and n is the horizon of the task.
Reference: <author> Booth, T. L. </author> <year> (1967). </year> <title> Sequential Machines and Automata Theory. </title> <publisher> John Wiley. </publisher>
Reference-contexts: Thus, temporal abstraction is achieved by learning abstract actions that span many time steps in the real environment. 4 A HIERARCHY OF ENVIRONMENT MODELS Consider 2 levels of a hierarchy of VTRMs for solving the set of compositionally structured MDTs. Such VTR models are stochastic sequential machines <ref> (Booth 1967) </ref> of the Mealy-type (when the payoffs are considered to be the outputs of the machines), and when convenient I shall treat them as such. Let M-1 be the highest resolution model or machine with action set A 1 consisting of primitive actions executable in one time step.
Reference: <author> Chapman, D. & Kaelbling, L. P. </author> <year> (1991). </year> <title> Input generalization in delayed reinforcement learning: An algorithm and performance comparisons. </title> <booktitle> In Proceedings of the 1991 International Joint Conference on Artificial Intelligence. </booktitle>
Reference: <author> Gullapalli, V. </author> <year> (1992). </year> <title> Reinforcement Learning and its application to control. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, </institution> <address> Amherst, MA 01003. </address>
Reference: <author> Iba, G. A. </author> <year> (1989). </year> <title> A heuristic approach to the discovery of macro-operators. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 285-317. </pages>
Reference-contexts: Doing DP in abstract models may then quickly lead to a near-optimal value function. The quick sub-optimal solutions could then be optimized over time using experience in the real environment. Further research is needed to test these intuitions. Learning abstract actions shares some drawbacks with macro-learning systems <ref> (e.g., Iba 1989) </ref>; they increase the total size of the action set available to the agent (increased branching factor), and redundant and useless abstract actions can offset the advantages gained from reducing the potential number of backups.
Reference: <author> Kaelbling, L. P. </author> <year> (1990). </year> <title> Learning in Embedded Systems. </title> <type> PhD thesis, </type> <institution> Stanford University, Department of Computer Science, Stanford, CA. </institution> <note> Technical Report TR-90-04. </note>
Reference: <author> Korf, R. E. </author> <year> (1985). </year> <title> Learning to Solve Problems by Searching for Macro-Operators. </title> <address> Massachusetts: </address> <publisher> Pitman Publishers. </publisher>
Reference-contexts: The abstract action A is shown via solid lines and the abstract action B via dashed lines. The payoffs to be assigned to these arcs will depend on the control policies associated with these abstract actions. Abstract actions are similar to macro-operators <ref> (Korf 1985) </ref> in that they allow the agent to ignore irrelevant temporal detail in determining solutions. However, macros are generally open loop sequences of actions that would, if executed, transform the environment from a fixed initial state to a goal state.
Reference: <author> Lin, L. J. </author> <year> (1991). </year> <title> Self-improvement based on reinforcement learning, planning and teaching. </title> <editor> In Birn-baum, L. & Collins, G. (Eds.), </editor> <booktitle> Maching Learning: Proceedings of the Eighth International Workshop, </booktitle> <pages> pages 323-327, </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Numerous researchers have demonstrated accelerated learning in both model-based and model-free approaches by using heuristics and domain knowledge to change the order in which the backups are done. State-preference predicates (Utgoff and Clouse 1991), external critics (Whitehead 1991), external teachers <ref> (Lin 1991) </ref>, and nominal controllers, are some methods of utilizing prior domain knowledge.
Reference: <author> Moore, A. W. </author> <year> (1991). </year> <title> Variable resolution dynamic programming: Efficiently learning action maps in multivariate real-valued state-spaces. </title> <editor> In Birn-baum, L. A. & Collins, G. C. (Eds.), </editor> <booktitle> Maching Learning: Proceedings of the Eighth International Workshop, </booktitle> <pages> pages 333-337, </pages> <address> San Mateo, CA. </address> <publisher> Mor-gan Kaufmann. </publisher>
Reference: <author> Ross, S. </author> <year> (1983). </year> <title> Introduction to Stochastic Dynamic Programming. </title> <address> New York: </address> <publisher> Academic Press. </publisher>
Reference-contexts: I do not consider non-stationary control policies because it is known that the optimal policies for MDTs are stationary <ref> (Ross 1983) </ref>. 4 The policy iteration algorithm is an exception because it searches for an optimal policy directly in policy space. Nevertheless, policies are evaluated by determining their value function.
Reference: <author> Samuel, A. L. </author> <year> (1967). </year> <title> Some studies in machine learning using the game of checkers. </title> <journal> II|Recent progress. IBM Journal on Research and Development, </journal> <pages> 601-617. </pages>
Reference: <author> Schmidhuber, J. H. </author> <year> (1992). </year> <title> Learning complex, extended sequences using the principle of history compression. </title> <journal> Neural Computation, </journal> <volume> 4, </volume> <pages> 234-243. </pages>
Reference-contexts: Of the three types of learning tasks mentioned above, the need to abstract temporal detail arises only for researchers studying multi-stage RL tasks <ref> (but see Schmidhuber 1992) </ref>. One of the contributions of this paper is to demonstrate that temporal detail can be abstracted independently of abstracting structural detail, and that the two are really orthogonal issues.
Reference: <author> Simon, H. A. </author> <year> (1990). </year> <title> Prediction and prescription in systems modeling. </title> <journal> Operations Research, </journal> <volume> 38 (1), </volume> <pages> 7-14. </pages>
Reference-contexts: The central issue addressed in this paper is the nature of the abstractions appropriate for accelerating learning of the value function for MDTs. In particular, I study the abstractions necessary to mitigate the high temporal resolution problem. To that end, I focus on using abstract models for prescription <ref> (see Simon 1990) </ref>, i.e., on using models to determine the effects of control policies via simulation or temporal projection. However, the models that I will describe could be put to other uses, e.g., for deriving structural explanations to deal with the state generalization issue.
Reference: <author> Singh, S. P. </author> <year> (1992a). </year> <title> The efficient learning of multiple task sequences. </title> <editor> In Moody, J. E., Hanson, S. J., & Lippman, R. P. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 251-258, </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kauffman. </publisher>
Reference-contexts: Given the long sequences of actions required to solve a composite MDT, the agent faces very difficult temporal credit assignment problems, and conventional RL architectures may be unable to learn the value function for composite MDTs <ref> (Singh 1992a) </ref>.
Reference: <author> Singh, S. P. </author> <year> (1992b). </year> <title> Reinforcement learning with a hierarchy of abstract models. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 202-207, </pages> <address> San Jose,CA. </address> <publisher> AAAI Press/MIT Press. </publisher>
Reference-contexts: To that end, I assumed that the desired abstract actions had already been learned in M-2. In a separate paper <ref> (Singh 1992b) </ref>, I address the issue of learning the useful abstract actions for a set of compositionally-structured MDTs. Note that the learning agent does not have access to the structure of the composite tasks, else the discovery would be trivial.
Reference: <author> Singh, S. P. </author> <year> (1992c). </year> <title> Transfer of learning by composing solutions for elemental sequential tasks. </title> <journal> Machine Learning, </journal> <volume> 8 (3/4), </volume> <pages> 323-339. </pages>
Reference-contexts: However, if the learning agent has to learn to solve multiple MDTs <ref> (Singh 1992c) </ref> in the same environment, the cost of constructing a VTRM can be amortized across the tasks.
Reference: <author> Sutton, R. S. </author> <year> (1984). </year> <title> Temporal Credit Assignment in Reinforcement Learning. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, </institution> <address> Amherst, MA. </address>
Reference-contexts: MDTs with a horizon greater than one, or multi-stage MDTs, face the difficult temporal credit assignment <ref> (Sutton 1984) </ref> problem. Hence, the search for an optimal action in a state cannot be conducted locally because it may be necessary to examine the consequences of all action sequences of length equal to the horizon of the MDT.
Reference: <author> Sutton, R. S. </author> <year> (1988). </year> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 9-44. </pages>
Reference-contexts: Asymptotic convergence results have been obtained under certain conditions for both TD <ref> (Sutton 1988) </ref> and Q-learning (Watkins 1989; Watkins and Dayan 1992). Sutton (1990) demonstrated that both TD and Q-learning could approximate the optimal value function as well by using simulated experience with a model of the environment. The essential operation shared by all DP-based learning algorithms is that of a "backup".
Reference: <author> Sutton, R. S. </author> <year> (1990). </year> <title> Integrating architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> In Proc. of the Seventh International Conference on Machine Learning, </booktitle> <pages> pages 216-224, </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Tesauro, G. J. </author> <year> (1992). </year> <title> Practical issues in temporal difference learning. </title> <journal> Machine Learning, </journal> <volume> 8 (3/4), </volume> <pages> 257-277. </pages>
Reference-contexts: Yet, despite the consequent increase in theoretical understanding, the inability of RL algorithms to scale well to complex tasks has limited their application to simple tasks <ref> (but see Tesauro 1992 for an exception) </ref>. In this paper I use the general and abstract framework afforded by DP to discuss some of the scaling issues faced by RL researchers.
Reference: <author> Thrun, S. B. & Moller, K. </author> <year> (1992). </year> <title> Active exploration in dynamic environments. </title> <editor> In Moody, J. E., Han-son, S. J., & Lippman, R. P. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <address> San Mateo, CA. </address> <publisher> Morgan Kauffman. </publisher>
Reference: <author> Utgoff, P. E. & Clouse, J. A. </author> <year> (1991). </year> <title> Two kinds of training information for evaluation function learning. </title> <booktitle> In Proceedings of the Ninth Annual Conference on Artificial Intelligence, </booktitle> <pages> pages 596-600, </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Equation 1 can then be used by substituting the estimated transition probabilities for the real transition probabilities. Numerous researchers have demonstrated accelerated learning in both model-based and model-free approaches by using heuristics and domain knowledge to change the order in which the backups are done. State-preference predicates <ref> (Utgoff and Clouse 1991) </ref>, external critics (Whitehead 1991), external teachers (Lin 1991), and nominal controllers, are some methods of utilizing prior domain knowledge.
Reference: <author> Watkins, C. J. C. H. </author> <year> (1989). </year> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> Cambridge Univ., </institution> <address> Cam-bridge, England. </address>
Reference: <author> Watkins, C. J. C. H. & Dayan, P. </author> <year> (1992). </year> <title> Q-learning. </title> <journal> Machine Learning, </journal> <volume> 8 (3/4), </volume> <pages> 279-292. </pages>
Reference: <author> Werbos, P. J. </author> <year> (1990). </year> <title> Consistency of HDP applied to a simple reinforcement learning problem. </title> <booktitle> Neural Networks, </booktitle> <volume> 3 (2), </volume> <pages> 179-189. </pages>
Reference: <author> Whitehead, S. D. </author> <year> (1991). </year> <title> Complexity and cooperation in Q-learning. </title> <editor> In Birnbaum, L. & Collins, G. (Eds.), </editor> <booktitle> Maching Learning: Proceedings of the Eighth International Workshop, </booktitle> <pages> pages 363-367, </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Numerous researchers have demonstrated accelerated learning in both model-based and model-free approaches by using heuristics and domain knowledge to change the order in which the backups are done. State-preference predicates (Utgoff and Clouse 1991), external critics <ref> (Whitehead 1991) </ref>, external teachers (Lin 1991), and nominal controllers, are some methods of utilizing prior domain knowledge.
Reference: <author> Whitehead, S. D. & Ballard, D. H. </author> <year> (1990). </year> <title> Active perception and reinforcement learning. </title> <booktitle> In Proc. of the Seventh International Conference on Machine Learning, </booktitle> <address> Austin, TX. </address> <publisher> M. </publisher>
Reference-contexts: In this paper I do not consider such tasks. 2 By choosing Markovian decision tasks I ignore the complex issues arising from learning with incomplete state information <ref> (e.g., Whitehead et al. 1990) </ref>, and learning in non-stationary environments. control tasks with the property that the current state and future actions determine the expected future sequence of states independently of the state trajectory prior to the current state. Figure 1 shows a block diagram representation of an MDT.
Reference: <author> Yee, R. C. </author> <year> (1992). </year> <title> Abstraction in control learning. </title> <type> Technical Report COINS Technical Report 92-16, </type> <institution> Department of Computer and Information Science, University of Massachusetts, </institution> <address> Amherst, MA 01003. </address> <note> A dissertation proposal. </note>
References-found: 32

