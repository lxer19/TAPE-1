URL: http://www-psrg.lcs.mit.edu/ftpdir/papers/sigir97.ps
Refering-URL: http://www.psrg.lcs.mit.edu/~bvelez/sigir97.abstract.html
Root-URL: 
Title: Fast and Effective Query Refinement  
Author: Bienvenido Velez Ron Weiss Mark A. Sheldon David K. Gifford 
Affiliation: 1 Programming Systems Research Group MIT Laboratory for Computer Science 2 Lotus Development Corporation  
Abstract: Query Refinement is an essential information retrieval tool that interactively recommends new terms related to a particular query. This paper introduces concept recall, an experimental measure of an algorithm's ability to suggest terms humans have judged to be semantically related to an information need. This study uses precision improvement experiments to measure the ability of an algorithm to produce single term query modifications that predict a user's information need as partially encoded by the query. An oracle algorithm produces ideal query modifications, providing a meaningful context for interpreting precision improvement results. This study also introduces RMAP, a fast and practical query refinement algorithm that refines multiple term queries by dynamically combining precomputed suggestions for single term queries. RMAP achieves accuracy comparable to a much slower algorithm, although both RMAP and the slower algorithm lag behind the best possible term suggestions offered by the oracle. We believe RMAP is fast enough to be integrated into present day Internet search engines: RMAP computes 100 term suggestions for a 160,000 document collection in 15 ms on a low-end PC. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> IJsbrand Jan Aalbersberg. </author> <title> Incremental relevance feedback. </title> <booktitle> In Proceedings of the 15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 11-22, </pages> <address> Copenhagen, Denmark, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: We are not aware of any work using concept recall to evaluate query refinement algorithms. The techniques here are also unique in their focus on single-term query modifications and on measurements that take into account initial query precision. Aalbersberg <ref> [1] </ref> proposes a new user interface for incremental relevance feedback and measures the effectiveness of competing techniques in several standard collections. This study focuses on precision. Qiu and Frei [17] measure recall-precision and usefulness of query expansions based on a similarity thesaurus constructed from the corpus.
Reference: [2] <author> R. C. Barrett and E. J. Selker. </author> <title> Finding what i am looking for: An information retrieval agent. </title> <type> Technical Report RJ 9816, </type> <institution> IBM Research Division, Almaden Research Center, </institution> <address> San Jose, California, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: HyPursuit also offers a browsable set of more general and more specific terms, though the browsing tools were limited and the hierarchy was a simple two-level thesaurus. A group at IBM has also focused on term suggestions based on relevance feedback <ref> [2] </ref>. This system allows users to annotate result documents as relevant or `trash' and then tries to form a boolean query that distinguishes the two sets. The selection algorithm uses a unique scoring method for determining how much terms improve a query.
Reference: [3] <author> Chris Buckley, Gerard Salton, James Allan, and Amit Sing-hal. </author> <title> Automatic query expansion using SMART:TREC3. </title> <editor> In Donna Harman, editor, </editor> <booktitle> Proceedings of the Third Text Retrieval Conference (TREC-3), </booktitle> <pages> pages 69-80, </pages> <address> Gaithersburg, MD, </address> <month> November </month> <year> 1994. </year> <title> NIST and ARPA. </title>
Reference-contexts: Discover and HyPursuit use a variant of the DM query refinement algorithm discussed in Section 3. Related work can be broken down into two broad categories: mechanisms for query modifications and query evaluation techniques. Query Modifications Most prior work on query modifications has focused on automatic query expansion <ref> [3, 7, 17, 18] </ref>, the addition of terms to a query to enhance recall. Query expansion has been done using global (i.e., thesauri [20, 8]) and local document analysis (i.e., result sets [27]) as well as relevance feedback [19, 12, 7].
Reference: [4] <author> W. B. Croft, R. Cook, and D. Wilder. </author> <title> Providing government information on the internet: Experiences with THOMAS. </title> <booktitle> In Digital Libraries Conference DL95, </booktitle> <pages> pages 19-24, </pages> <address> Austin, Texas, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: However, we chose to use the topic field because it better represents queries made by naive users in large information systems. For example, <ref> [4] </ref> determined that searches on the World-Wide Web have on average only two terms. Using the topic field results in an average query length of 2:08 for the set of 150 queries and 2:84 for the set of 200 queries.
Reference: [5] <author> Douglass R. Cutting, David R. Karger, Jan O. Pedersen, and John W. Tukey. Scatter/gather: </author> <title> A cluster-based approach to browsing large document collections. </title> <booktitle> In 15th Annual International SIGIR, </booktitle> <pages> pages 318-329, </pages> <address> Denmark, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: The notion of browsing a hierarchical representation of a query result set is common to Scatter/Gather <ref> [5] </ref>, the Content Routing System and HyPursuit. HyPursuit also offers a browsable set of more general and more specific terms, though the browsing tools were limited and the hierarchy was a simple two-level thesaurus. A group at IBM has also focused on term suggestions based on relevance feedback [2].
Reference: [6] <author> Andrzej Duda and Mark A. Sheldon. </author> <title> Content routing in networks of WAIS servers. </title> <booktitle> In Proceedings of the 14th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 124-132, </pages> <address> Poznan, Poland, </address> <month> June </month> <year> 1994. </year> <note> IEEE. </note>
Reference-contexts: the object of this study (Section 3), describe our experimental results (Section 4) and offer conclusions and possible avenues for future work (Section 5). 2 Related Work Our interest in query refinement is based on past experience with the Community Information System [10, 9] as well as with the Discover <ref> [22, 6, 21, 24] </ref> and HyPursuit [26] network search engines. In the Community Information System, a simple theorem prover assists the user in choosing query terms that guarantee the query can be satisfied at available news wire databases. <p> Users evaluated which terms might be useful for augmenting the query. 3 Query Refinement Algorithms This section introduces two query refinement algorithms that we have implemented and evaluated. Section 3.1 presents a generic query refinement algorithm, DM, based on our prior work <ref> [23, 6, 26] </ref> that can be instantiated by selecting a term weighting scheme. Section 3.2 describes and analyzes RMAP, a new and faster query refinement algorithm. <p> The first function, w df assigns to each term t a weight equal to the document frequency of the term in the set of documents matching the query. The Discover system <ref> [6] </ref> uses w df and provided anecdotal evidence that this approach generates useful query suggestions based on a collection of WAIS [16] document headlines. The second weight function, w tf , is based on term frequencies in the matching documents.
Reference: [7] <author> Efthimis N. Efthimiadis. </author> <title> A user-centered evaluation of ranking algorithms for interactive query expansion. </title> <booktitle> In Proceedings of the 16th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 146-159, </pages> <address> Pittsburgh, PA USA, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: Discover and HyPursuit use a variant of the DM query refinement algorithm discussed in Section 3. Related work can be broken down into two broad categories: mechanisms for query modifications and query evaluation techniques. Query Modifications Most prior work on query modifications has focused on automatic query expansion <ref> [3, 7, 17, 18] </ref>, the addition of terms to a query to enhance recall. Query expansion has been done using global (i.e., thesauri [20, 8]) and local document analysis (i.e., result sets [27]) as well as relevance feedback [19, 12, 7]. <p> Query expansion has been done using global (i.e., thesauri [20, 8]) and local document analysis (i.e., result sets [27]) as well as relevance feedback <ref> [19, 12, 7] </ref>. These techniques represent points on a spectrum: fully automatic query expansion adds related and subsuming terms to the query according to a thesaurus or terms in highly ranked documents from the result set, with no intervention on the part of the user. <p> This study focuses on precision. Qiu and Frei [17] measure recall-precision and usefulness of query expansions based on a similarity thesaurus constructed from the corpus. This work uses fully automatic query expansion. Efthimiadis <ref> [7] </ref> describes a user study in which six algorithms were used to generate query expansion terms. Users evaluated which terms might be useful for augmenting the query. 3 Query Refinement Algorithms This section introduces two query refinement algorithms that we have implemented and evaluated.
Reference: [8] <author> William B. Frakes and Ricardo Baeza-Yates, </author> <title> editors. Information Retrieval: Data Structures & Algorithms. </title> <publisher> Prentice Hall, </publisher> <address> Englwood Cliffs, New Jersey, </address> <year> 1992. </year>
Reference-contexts: Query Modifications Most prior work on query modifications has focused on automatic query expansion [3, 7, 17, 18], the addition of terms to a query to enhance recall. Query expansion has been done using global (i.e., thesauri <ref> [20, 8] </ref>) and local document analysis (i.e., result sets [27]) as well as relevance feedback [19, 12, 7].
Reference: [9] <author> David K. Gifford. </author> <title> Polychannel systems for mass digital communication. </title> <journal> Comm. ACM, </journal> <volume> 33(2), </volume> <month> February </month> <year> 1990. </year>
Reference-contexts: present the two query refinement algorithms that are the object of this study (Section 3), describe our experimental results (Section 4) and offer conclusions and possible avenues for future work (Section 5). 2 Related Work Our interest in query refinement is based on past experience with the Community Information System <ref> [10, 9] </ref> as well as with the Discover [22, 6, 21, 24] and HyPursuit [26] network search engines. In the Community Information System, a simple theorem prover assists the user in choosing query terms that guarantee the query can be satisfied at available news wire databases.
Reference: [10] <author> David K. Gifford, John M. Lucassen, and Stephen T. </author> <title> Berlin. An architecture for large scale information systems. </title> <booktitle> In 10th Symposium on Operating System Principles, </booktitle> <pages> pages 161-170. </pages> <publisher> ACM, </publisher> <month> December </month> <year> 1985. </year>
Reference-contexts: present the two query refinement algorithms that are the object of this study (Section 3), describe our experimental results (Section 4) and offer conclusions and possible avenues for future work (Section 5). 2 Related Work Our interest in query refinement is based on past experience with the Community Information System <ref> [10, 9] </ref> as well as with the Discover [22, 6, 21, 24] and HyPursuit [26] network search engines. In the Community Information System, a simple theorem prover assists the user in choosing query terms that guarantee the query can be satisfied at available news wire databases.
Reference: [11] <author> Donna Harman. </author> <title> Towards interactive query expansion. </title> <booktitle> In Proceedings of the 11th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 321-331, </pages> <address> Grenoble, France, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: We have elsewhere described our use of automatically generated thesauri for suggesting both broader and narrower search terms [26]. Harman <ref> [11] </ref> introduces a system that offers interactive query expansion by generating term suggestions using relevance feedback, nearest neighbors, and term variants of original query terms.
Reference: [12] <author> Donna Harman. </author> <title> Chapter 14: Ranking algorithms. </title> <editor> In William B. Frakes and Ricardo Baeza-Yates, editors, </editor> <booktitle> Information Retrieval: Data Structures & Algorithms, </booktitle> <pages> pages 363-392. </pages> <publisher> Prentice Hall, </publisher> <address> Englwood Cliffs, New Jersey, </address> <year> 1992. </year>
Reference-contexts: Query expansion has been done using global (i.e., thesauri [20, 8]) and local document analysis (i.e., result sets [27]) as well as relevance feedback <ref> [19, 12, 7] </ref>. These techniques represent points on a spectrum: fully automatic query expansion adds related and subsuming terms to the query according to a thesaurus or terms in highly ranked documents from the result set, with no intervention on the part of the user. <p> It concludes by examining sample suggestions from our prototype. The experiments were conducted using an internally developed information retrieval system that supports boolean queries and document ranking. Our ranking algorithm differs from classical ranking algorithms such as <ref> [12] </ref> by ignoring document size and by giving primary consideration to the number of query terms a document contains. 4.1 Collections and Query Sets The experimental framework uses a standard collection that consists of documents, sample queries, and human generated relevance judgments.
Reference: [13] <author> Donna Harman. </author> <title> Relevance feedback revisited. </title> <booktitle> In Proceedings of the 15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 1-10, </pages> <address> Copenhagen, Denmark, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: This scheme also requires the user to look through the result set and find positive and negative example documents. Query Evaluation Techniques Recall and precision are standard evaluation metrics in information retrieval [20]. They have consequently been used in evaluating relevance feedback <ref> [13] </ref> and thesaurus-based query expansion techniques [27]. We are not aware of any work using concept recall to evaluate query refinement algorithms. The techniques here are also unique in their focus on single-term query modifications and on measurements that take into account initial query precision.
Reference: [14] <author> Donna Harman. </author> <title> TIPSTER information retrieval text research collection. CDROM set, Linguistic Data Consortium, </title> <year> 1994. </year>
Reference-contexts: This paper introduces concept recall, an experimental measure of an algorithm's ability to suggest terms that are semantically related to the user's information need. The experimental results reported in this study are based on the proportion of human selected keywords appearing in Tipster topics <ref> [14] </ref> that were suggested by the algorithms under investigation. An algorithm that automatically generates many of these concepts is more likely to be accepted by users because it is able to produce semantically related terms. <p> It presents two experimental designs, accompanied with results, for evaluating the accuracy of query refinement algorithms. Both methods for accuracy evaluation measure the effectiveness of the suggested terms within the context of a subset of the Tipster collection <ref> [14] </ref>. The first evaluation method, concept recall, measures the proportion of suggested terms that appear in a list of human generated relevant terms. This measures an algorithm's ability to suggest terms related to a given query.
Reference: [15] <author> Renato Iannella, Nigel Ward, Andrew Wood, Hoylen Sue, and Peter Bruza. </author> <title> Digital libraries and the open information locator project. </title> <type> Technical Report DSTC 34, </type> <institution> Research Data Network Cooperative Research Centre, Resource Discover Unit, </institution> <address> Queesland, Australia, </address> <year> 1996. </year>
Reference-contexts: This evalua tion mechanism does not consider the effects of single term (or short) query modifications, which should be the focus of query refinement. Smeaton and van Rijsbergen [25] also use relevance feedback and nearest neighbor to generate term suggestions. The Open Information Locator project <ref> [15] </ref> has more recently taken up query refinement. They use two techniques: non-monotonic reasoning which combines user-directed relevance feedback and term co-occurrence analysis, and query by navigation which allows the user to browse a hierarchy of more general and more specific terms.
Reference: [16] <author> Brewster Kahle and Art Medlar. </author> <title> An information system for corporate users: Wide Area Information Servers. </title> <type> Technical Report TMC-199, </type> <institution> Thinking Machines, Inc., </institution> <month> April </month> <year> 1991. </year> <note> Version 3. </note>
Reference-contexts: The Discover system [6] uses w df and provided anecdotal evidence that this approach generates useful query suggestions based on a collection of WAIS <ref> [16] </ref> document headlines. The second weight function, w tf , is based on term frequencies in the matching documents.
Reference: [17] <author> Yonggang Qiu and H. P. Frei. </author> <title> Concept based query expansion. </title> <booktitle> In Proceedings of the 16th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 160-169, </pages> <address> Pittsburgh, PA USA, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: Discover and HyPursuit use a variant of the DM query refinement algorithm discussed in Section 3. Related work can be broken down into two broad categories: mechanisms for query modifications and query evaluation techniques. Query Modifications Most prior work on query modifications has focused on automatic query expansion <ref> [3, 7, 17, 18] </ref>, the addition of terms to a query to enhance recall. Query expansion has been done using global (i.e., thesauri [20, 8]) and local document analysis (i.e., result sets [27]) as well as relevance feedback [19, 12, 7]. <p> Aalbersberg [1] proposes a new user interface for incremental relevance feedback and measures the effectiveness of competing techniques in several standard collections. This study focuses on precision. Qiu and Frei <ref> [17] </ref> measure recall-precision and usefulness of query expansions based on a similarity thesaurus constructed from the corpus. This work uses fully automatic query expansion. Efthimiadis [7] describes a user study in which six algorithms were used to generate query expansion terms.
Reference: [18] <author> Salton, E. A. Fox, and E. Voorhees. </author> <title> Advanced feedback methods in information retrieval. </title> <journal> Journal of the American Society for Informaiton Science, </journal> <volume> 36(3) </volume> <pages> 200-210, </pages> <year> 1985. </year>
Reference-contexts: Discover and HyPursuit use a variant of the DM query refinement algorithm discussed in Section 3. Related work can be broken down into two broad categories: mechanisms for query modifications and query evaluation techniques. Query Modifications Most prior work on query modifications has focused on automatic query expansion <ref> [3, 7, 17, 18] </ref>, the addition of terms to a query to enhance recall. Query expansion has been done using global (i.e., thesauri [20, 8]) and local document analysis (i.e., result sets [27]) as well as relevance feedback [19, 12, 7].
Reference: [19] <author> G. Salton, E. A. Fox, C. Buckley, and E. Voorhees. </author> <title> Boolean query formulation with relevance feedback. </title> <type> Technical Report TR 83-539, </type> <institution> Department of Computer Science, Cornell University, </institution> <address> Ithaca, NY, </address> <month> January </month> <year> 1983. </year>
Reference-contexts: Query expansion has been done using global (i.e., thesauri [20, 8]) and local document analysis (i.e., result sets [27]) as well as relevance feedback <ref> [19, 12, 7] </ref>. These techniques represent points on a spectrum: fully automatic query expansion adds related and subsuming terms to the query according to a thesaurus or terms in highly ranked documents from the result set, with no intervention on the part of the user.
Reference: [20] <author> Gerard Salton. </author> <title> Another look at automatic text-retrieval systems. </title> <journal> Comm. ACM, </journal> <volume> 29(7) </volume> <pages> 648-656, </pages> <month> July </month> <year> 1986. </year>
Reference-contexts: Query Modifications Most prior work on query modifications has focused on automatic query expansion [3, 7, 17, 18], the addition of terms to a query to enhance recall. Query expansion has been done using global (i.e., thesauri <ref> [20, 8] </ref>) and local document analysis (i.e., result sets [27]) as well as relevance feedback [19, 12, 7]. <p> The selection algorithm uses a unique scoring method for determining how much terms improve a query. This scheme also requires the user to look through the result set and find positive and negative example documents. Query Evaluation Techniques Recall and precision are standard evaluation metrics in information retrieval <ref> [20] </ref>. They have consequently been used in evaluating relevance feedback [13] and thesaurus-based query expansion techniques [27]. We are not aware of any work using concept recall to evaluate query refinement algorithms.
Reference: [21] <author> Mark A. Sheldon. </author> <title> Content Routing: A Scalable Architecture for Network-Based Information Discovery. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <address> Cambridge, Mas-sachusetts, </address> <month> October </month> <year> 1995. </year>
Reference-contexts: the object of this study (Section 3), describe our experimental results (Section 4) and offer conclusions and possible avenues for future work (Section 5). 2 Related Work Our interest in query refinement is based on past experience with the Community Information System [10, 9] as well as with the Discover <ref> [22, 6, 21, 24] </ref> and HyPursuit [26] network search engines. In the Community Information System, a simple theorem prover assists the user in choosing query terms that guarantee the query can be satisfied at available news wire databases.
Reference: [22] <author> Mark A. Sheldon, Andrzej Duda, Ron Weiss, and David K. Gifford. </author> <title> Discover: A resource discovery system based on content routing. </title> <booktitle> In Proceedings of The Third International World Wide Web Conference, </booktitle> <address> Darmstadt, Germany, </address> <month> April </month> <year> 1995. </year> <title> Also in Computer Networks and ISDN Systems, </title> <publisher> Else-vier North Holland, </publisher> <pages> 27(1995), pp. 953-972. </pages>
Reference-contexts: the object of this study (Section 3), describe our experimental results (Section 4) and offer conclusions and possible avenues for future work (Section 5). 2 Related Work Our interest in query refinement is based on past experience with the Community Information System [10, 9] as well as with the Discover <ref> [22, 6, 21, 24] </ref> and HyPursuit [26] network search engines. In the Community Information System, a simple theorem prover assists the user in choosing query terms that guarantee the query can be satisfied at available news wire databases.
Reference: [23] <author> Mark A. Sheldon, Andrzej Duda, Ron Weiss, James W. O'Toole, Jr., and David K. Gifford. </author> <title> A content routing system for distributed information servers. </title> <type> Technical Report MIT/LCS/TR-578, </type> <institution> M.I.T. Laboratory for Computer Science, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: Users evaluated which terms might be useful for augmenting the query. 3 Query Refinement Algorithms This section introduces two query refinement algorithms that we have implemented and evaluated. Section 3.1 presents a generic query refinement algorithm, DM, based on our prior work <ref> [23, 6, 26] </ref> that can be instantiated by selecting a term weighting scheme. Section 3.2 describes and analyzes RMAP, a new and faster query refinement algorithm.
Reference: [24] <author> Mark A. Sheldon, Andrzej Duda, Ron Weiss, James W. O'Toole, Jr., and David K. Gifford. </author> <title> Content routing for distributed information servers. </title> <booktitle> In Fourth International Conference on Extending Database Technology, </booktitle> <pages> pages 109-122, </pages> <address> Cambridge, England, </address> <month> March </month> <year> 1994. </year> <note> Available as Springer-Verlag LNCS Number 779. </note>
Reference-contexts: the object of this study (Section 3), describe our experimental results (Section 4) and offer conclusions and possible avenues for future work (Section 5). 2 Related Work Our interest in query refinement is based on past experience with the Community Information System [10, 9] as well as with the Discover <ref> [22, 6, 21, 24] </ref> and HyPursuit [26] network search engines. In the Community Information System, a simple theorem prover assists the user in choosing query terms that guarantee the query can be satisfied at available news wire databases.
Reference: [25] <author> A. F. Smeaton and C. J. van Rijsbergen. </author> <title> The retrieval effects of query expansion on a feedback document retrieval system. </title> <journal> The Computer Journal, </journal> <volume> 26(3) </volume> <pages> 239-246, </pages> <month> August </month> <year> 1983. </year>
Reference-contexts: This evalua tion mechanism does not consider the effects of single term (or short) query modifications, which should be the focus of query refinement. Smeaton and van Rijsbergen <ref> [25] </ref> also use relevance feedback and nearest neighbor to generate term suggestions. The Open Information Locator project [15] has more recently taken up query refinement.
Reference: [26] <author> Ron Weiss, Bienvenido Velez, Mark A. Sheldon, Chanathip Namprempre, Peter Szilagyi, and David K. Gifford. Hy-Pursuit: </author> <title> A hierarchical network search engine that exploits content-link hypertext clustering. </title> <booktitle> In Proceedings of the Seventh ACM Conference on Hypertext, </booktitle> <address> Washington, DC, </address> <month> March </month> <year> 1996. </year>
Reference-contexts: Figure 2 shows refinement suggestions for the query scheme (a programming language developed at MIT) generated by the HyPursuit prototype search tool <ref> [26] </ref>. This paper introduces concept recall, an experimental measure of an algorithm's ability to suggest terms that are semantically related to the user's information need. <p> 3), describe our experimental results (Section 4) and offer conclusions and possible avenues for future work (Section 5). 2 Related Work Our interest in query refinement is based on past experience with the Community Information System [10, 9] as well as with the Discover [22, 6, 21, 24] and HyPursuit <ref> [26] </ref> network search engines. In the Community Information System, a simple theorem prover assists the user in choosing query terms that guarantee the query can be satisfied at available news wire databases. Discover and HyPursuit use a variant of the DM query refinement algorithm discussed in Section 3. <p> We have elsewhere described our use of automatically generated thesauri for suggesting both broader and narrower search terms <ref> [26] </ref>. Harman [11] introduces a system that offers interactive query expansion by generating term suggestions using relevance feedback, nearest neighbors, and term variants of original query terms. <p> Users evaluated which terms might be useful for augmenting the query. 3 Query Refinement Algorithms This section introduces two query refinement algorithms that we have implemented and evaluated. Section 3.1 presents a generic query refinement algorithm, DM, based on our prior work <ref> [23, 6, 26] </ref> that can be instantiated by selecting a term weighting scheme. Section 3.2 describes and analyzes RMAP, a new and faster query refinement algorithm.
Reference: [27] <author> Jinxi Xu and W. Bruce Croft. </author> <title> Query expansion using local and global document analysis. </title> <booktitle> In Proceedings of the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 4-11, </pages> <address> Zurich, Switzerland, </address> <month> August </month> <year> 1996. </year>
Reference-contexts: Query Modifications Most prior work on query modifications has focused on automatic query expansion [3, 7, 17, 18], the addition of terms to a query to enhance recall. Query expansion has been done using global (i.e., thesauri [20, 8]) and local document analysis (i.e., result sets <ref> [27] </ref>) as well as relevance feedback [19, 12, 7]. <p> This scheme also requires the user to look through the result set and find positive and negative example documents. Query Evaluation Techniques Recall and precision are standard evaluation metrics in information retrieval [20]. They have consequently been used in evaluating relevance feedback [13] and thesaurus-based query expansion techniques <ref> [27] </ref>. We are not aware of any work using concept recall to evaluate query refinement algorithms. The techniques here are also unique in their focus on single-term query modifications and on measurements that take into account initial query precision. <p> The average performance of DM nfx over all queries is 20%; the average for RMAP is 16%. We suspect that since RMAP is a global algorithm and DM is a local algorithm (in the sense of <ref> [27] </ref>), DM is better able to take advantage of information rich queries. precision, shows the number of new relevant documents out of 100 added to the result set versus initial query precision.
References-found: 27

