URL: http://www.pdl.cs.cmu.edu/PDL-FTP/TIP/SOSP15.ps
Refering-URL: http://www.cs.cmu.edu/~rhp/
Root-URL: 
Email: jimz-@cs.cmu.edu  
Title: 79 Informed Prefetching and Caching  
Author: R. Hugo Patterson*, Garth A. Gibson, Eka Ginting, Daniel Stodolsky, Jim Zelenka -rhp, garth, eginting, danner, 
Web: http://www.cs.cmu.edu/Web/Groups/PDL/  
Address: Pittsburgh, PA 15213-3890  
Affiliation: *Department of Electrical and Computer Engineering School of Computer Science Carnegie Mellon University  
Abstract: In this paper, we present aggressive, proactive mechanisms that tailor file system resource management to the needs of I/O-intensive applications. In particular, we show how to use application-disclosed access patterns (hints) to expose and exploit I/O parallelism, and to dynamically allocate file buffers among three competing demands: prefetching hinted blocks, caching hinted blocks for reuse, and caching recently used data for unhinted accesses. Our approach estimates the impact of alternative buffer allocations on application execution time and applies cost-benefit analysis to allocate buffers where they will have the greatest impact. We have implemented informed prefetching and caching in Digitals OSF/1 operating system and measured its performance on a 150 MHz Alpha equipped with 15 disks running a range of applications. Informed prefetching reduces the execution time of text search, scientific visualization, relational database queries, speech recognition, and object linking by 20-83%. Informed caching reduces the execution time of computational physics by up to 42% and contributes to the performance improvement of the object linker and the database. Moreover, applied to multiprogrammed, I/O-intensive workloads, informed prefetching and caching increase overall throughput. 
Abstract-found: 1
Intro-found: 1
Reference: [Baker91] <author> Baker, M.G., Hartman, J.H., Kupfer, M.D., Shirriff, K.W., Ousterhout, J.K., </author> <title> Measurements of a Distributed File System, </title> <booktitle> Proc. of the 13th Symp. on Operating System Principles, </booktitle> <address> Pacific Grove, CA, </address> <month> Oct. </month> <year> 1991, </year> <pages> pp. 198-212. </pages>
Reference-contexts: For example, the Sprite groups 1985 caching study led them to predict higher hit ratios for larger caches. But in 1991, after larger caches had been installed, hit ratios were not much changed files had grown just as fast as the caches <ref> [Ousterhout85, Baker91] </ref>. This suggests that new techniques are needed to boost I/O performance. The problem is especially acute for read-intensive applications. Write performance is less critical because the writing application generally does not wait for the disk to be written.
Reference: [Cao94] <author> Cao, P., Felten, E.W., Li, K., </author> <title> Implementation and Performance of Application-Controlled File Caching, </title> <booktitle> Proc. of the First USENIX Symp. on Operating Systems Design and Implementation, </booktitle> <address> Monterey, CA, </address> <month> Nov., </month> <year> 1994, </year> <month> pp.165-178. </month>
Reference-contexts: Perhaps the most familiar of these occurs in the form of policy advice from an application to the virtual-memory or file-cache modules. In these hints, the application recommends a resource management policy that has been statically or dynamically determined to improve performance for this application <ref> [Trivedi79, Sun88, Cao94] </ref>. In large integrated applications, more detailed knowledge may be available. The database community has long taken advantage of this for buffer management. The buffer manager can use the access plan for a query to help determine the number of buffers to allocate [Sacco82, Chou85, Cornell89, Ng91, Chen93].
Reference: [Cao95a] <author> Cao, P., Felten, E.W., Karlin, A., Li, K., </author> <title> A Study of Integrated Prefetching and Caching Strategies, </title> <booktitle> Proc. of the Joint Int. Conf. on Measurement & Modeling of Computer Systems (SIGMETRICS), </booktitle> <address> Ottawa, Canada, </address> <month> May, </month> <year> 1995, </year> <pages> pp. 188-197. </pages>
Reference-contexts: Relatively little work has been done on the combination of caching and prefetching. In one notable example, however, Cao, Felton, Karlin and Li derive an aggressive prefetching policy with excellent competitive performance characteristics in the context of complete knowledge of future accesses <ref> [Cao95a] </ref>. These same authors go on to show how to integrate prefetching according to hints with application-supplied cache management advice [Cao95b]. In contrast, we use the same hints, described in the next section, for both caching and prefetching.
Reference: [Cao95b] <author> Cao, P., Felten, E.W., Karlin, A., Li, K., </author> <title> Implementation and Performance of Integrated Application-Controlled Caching, Prefetching and Disk Scheduling, </title> <institution> Computer Science Technical Report No. TR-CS-95-493, Princeton University, </institution> <year> 1995. </year>
Reference-contexts: In one notable example, however, Cao, Felton, Karlin and Li derive an aggressive prefetching policy with excellent competitive performance characteristics in the context of complete knowledge of future accesses [Cao95a]. These same authors go on to show how to integrate prefetching according to hints with application-supplied cache management advice <ref> [Cao95b] </ref>. In contrast, we use the same hints, described in the next section, for both caching and prefetching.
Reference: [Chen93] <author> Chen, C-M.M., Roussopoulos, N., </author> <title> Adaptive Database Buffer Allocation Using Query Feedback, </title> <booktitle> Proc. of the 19th Int. Conf. on Very Large Data Bases, </booktitle> <address> Dublin, Ireland, </address> <year> 1993, </year> <pages> pp. 342-353. </pages>
Reference-contexts: In large integrated applications, more detailed knowledge may be available. The database community has long taken advantage of this for buffer management. The buffer manager can use the access plan for a query to help determine the number of buffers to allocate <ref> [Sacco82, Chou85, Cornell89, Ng91, Chen93] </ref>. Ng, Faloutsos and Selliss work on marginal gains considered the question of how much benefit a query would derive from an additional buffer. Their work stimulated the development of our approach to cache management.
Reference: [Chou85] <author> Chou, H. T., DeWitt, D. J., </author> <title> An Evaluation of Buffer Management Strategies for Relational Database Systems, </title> <booktitle> Proc. of the 11th Int. Conf. on Very Large Data Bases, </booktitle> <address> Stockholm, </address> <year> 1985, </year> <pages> pp. 127-141. </pages>
Reference-contexts: In large integrated applications, more detailed knowledge may be available. The database community has long taken advantage of this for buffer management. The buffer manager can use the access plan for a query to help determine the number of buffers to allocate <ref> [Sacco82, Chou85, Cornell89, Ng91, Chen93] </ref>. Ng, Faloutsos and Selliss work on marginal gains considered the question of how much benefit a query would derive from an additional buffer. Their work stimulated the development of our approach to cache management.
Reference: [Cornell89] <author> Cornell, D. W., Yu, P. S., </author> <title> Integration of Buffer Management and Query Optimization in Relational Database Environment, </title> <booktitle> Proc. of the 15th Int. Conf. on Very Large Data Bases, </booktitle> <address> Amsterdam, </address> <month> Aug. </month> <year> 1989, </year> <pages> pp. 247-255. </pages>
Reference-contexts: In large integrated applications, more detailed knowledge may be available. The database community has long taken advantage of this for buffer management. The buffer manager can use the access plan for a query to help determine the number of buffers to allocate <ref> [Sacco82, Chou85, Cornell89, Ng91, Chen93] </ref>. Ng, Faloutsos and Selliss work on marginal gains considered the question of how much benefit a query would derive from an additional buffer. Their work stimulated the development of our approach to cache management.
Reference: [Curewitz93] <author> Curewitz, K.M., Krishnan, P., Vitter, J.S., </author> <title> Practical Prefetching via Data Compression, </title> <booktitle> Proc. of the 1993 ACM Conf. on Management of Data (SIGMOD), </booktitle> <address> Washington, DC, </address> <month> May </month> <year> 1993, </year> <pages> pp. 257-66. 95 </pages>
Reference-contexts: Others, notably Kotz, have looked at detecting more complex access patterns and prefetching non-sequentially within a file [Kotz91]. At the level of whole files or database objects, a number of researchers have looked at inferring future accesses based on past accesses <ref> [Korner90, Kotz91, Tait91, Palmer91, Curewitz93, Griffioen94] </ref>. The danger in speculative prefetching based on historical access patterns is that it risks hurting, rather than helping, performance [Smith85]. As a result of this danger, speculative prefetching is usually conservative, waiting until its theories are confirmed by some number of demand accesses.
Reference: [Ebling94] <author> Ebling, </author> <title> M.R., Mummert, L.B., Steere, D.C., Overcoming the Network Bottleneck in Mobile Computing, </title> <booktitle> Proc. of the Workshop on Mobile Computing Systems and Applications, </booktitle> <month> Dec. </month> <year> 1994. </year>
Reference-contexts: Thus, to determine H (n) for caches larger than the current size, TIP uses ghost buffers. Ghost buffers are dataless buffer headers which serve as placeholders to record when an access would have been a hit had there been more buffers in the cache <ref> [Ebling94] </ref>. The length of the LRU queue, including ghosts, is limited to the total number of buffers in the cache. To reduce overhead costs and estimate variation, hit counts are recorded not by individual stack depths, but by disjoint intervals of stack depths, called segments.
Reference: [Feiertag71] <author> Feiertag, R. J., Organisk, E. I., </author> <title> The Multics Input/Output System, </title> <booktitle> Proc. of the 3rd Symp. on Operating System Principles, </booktitle> <year> 1971, </year> <pages> pp. 35-41. </pages>
Reference-contexts: In the context of file systems, historical information is often used for both file caching and prefetching. The ubiquitous LRU cache replacement algorithm relies on the history of recent accesses to choose a buffer for replacement. For history-based prefetching, the most successful approach is sequential readahead <ref> [Feiertag71, McKusick84] </ref>. Digitals OSF/1 is an aggressive example, prefetching up to 64 blocks ahead when it detects long sequential runs. Others, notably Kotz, have looked at detecting more complex access patterns and prefetching non-sequentially within a file [Kotz91].
Reference: [Griffioen94] <author> Griffioen, J., Appleton, R., </author> <title> Reducing File System Latency using a Predictive Approach, </title> <booktitle> Proc. of the 1994 Summer USENIX Conference, </booktitle> <address> Boston, MA, </address> <year> 1994. </year>
Reference-contexts: Others, notably Kotz, have looked at detecting more complex access patterns and prefetching non-sequentially within a file [Kotz91]. At the level of whole files or database objects, a number of researchers have looked at inferring future accesses based on past accesses <ref> [Korner90, Kotz91, Tait91, Palmer91, Curewitz93, Griffioen94] </ref>. The danger in speculative prefetching based on historical access patterns is that it risks hurting, rather than helping, performance [Smith85]. As a result of this danger, speculative prefetching is usually conservative, waiting until its theories are confirmed by some number of demand accesses.
Reference: [Grimshaw91] <author> Grimshaw, A.S., Loyot Jr., </author> <title> E.C., ELFS: ObjectOriented Extensible File Systems, </title> <institution> Computer Science Technical Report No. TR-91-14, University of Virginia, </institution> <year> 1991. </year>
Reference-contexts: In contrast, we use the same hints, described in the next section, for both caching and prefetching. Much richer languages for expressing and exploiting disclosure include collective I/O calls [Kotz94] and operations on structured files <ref> [Grimshaw91] </ref> or dynamic sets [Steere95]. 3 Hints that disclose The proactive management strategy described in this paper depends on a reliable picture of future demands. We advocate a form of hints based on advance knowledge which we call disclosure [Patterson93].
Reference: [Korner90] <author> Korner, K., </author> <title> Intelligent Caching for Remote File Service, </title> <booktitle> Proc. of the 10th Int. Conf. on Distributed Computing Systems, </booktitle> <year> 1990, </year> <month> pp.220-226. </month>
Reference-contexts: Others, notably Kotz, have looked at detecting more complex access patterns and prefetching non-sequentially within a file [Kotz91]. At the level of whole files or database objects, a number of researchers have looked at inferring future accesses based on past accesses <ref> [Korner90, Kotz91, Tait91, Palmer91, Curewitz93, Griffioen94] </ref>. The danger in speculative prefetching based on historical access patterns is that it risks hurting, rather than helping, performance [Smith85]. As a result of this danger, speculative prefetching is usually conservative, waiting until its theories are confirmed by some number of demand accesses.
Reference: [Kotz91] <author> Kotz, D., Ellis, C.S., </author> <title> Practical Prefetching Techniques for Parallel File Systems, </title> <booktitle> Proc. First International Conf. on Parallel and Distributed Information Systems, </booktitle> <address> Miami Beach, Florida, </address> <month> Dec. </month> <pages> 4-6, </pages> <year> 1991, </year> <pages> pp. 182-189. </pages>
Reference-contexts: For history-based prefetching, the most successful approach is sequential readahead [Feiertag71, McKusick84]. Digitals OSF/1 is an aggressive example, prefetching up to 64 blocks ahead when it detects long sequential runs. Others, notably Kotz, have looked at detecting more complex access patterns and prefetching non-sequentially within a file <ref> [Kotz91] </ref>. At the level of whole files or database objects, a number of researchers have looked at inferring future accesses based on past accesses [Korner90, Kotz91, Tait91, Palmer91, Curewitz93, Griffioen94]. The danger in speculative prefetching based on historical access patterns is that it risks hurting, rather than helping, performance [Smith85]. <p> Others, notably Kotz, have looked at detecting more complex access patterns and prefetching non-sequentially within a file [Kotz91]. At the level of whole files or database objects, a number of researchers have looked at inferring future accesses based on past accesses <ref> [Korner90, Kotz91, Tait91, Palmer91, Curewitz93, Griffioen94] </ref>. The danger in speculative prefetching based on historical access patterns is that it risks hurting, rather than helping, performance [Smith85]. As a result of this danger, speculative prefetching is usually conservative, waiting until its theories are confirmed by some number of demand accesses.
Reference: [Kotz94] <author> Kotz, D., </author> <title> Disk-directed I/O for MIMD Multiprocessors, </title> <booktitle> Proc. of the 1st USENIX Symp. on Operating Systems Design and Implementation, </booktitle> <address> Monterey, CA, </address> <month> Nov. </month> <year> 1994, </year> <pages> pp. 61-74. </pages>
Reference-contexts: These same authors go on to show how to integrate prefetching according to hints with application-supplied cache management advice [Cao95b]. In contrast, we use the same hints, described in the next section, for both caching and prefetching. Much richer languages for expressing and exploiting disclosure include collective I/O calls <ref> [Kotz94] </ref> and operations on structured files [Grimshaw91] or dynamic sets [Steere95]. 3 Hints that disclose The proactive management strategy described in this paper depends on a reliable picture of future demands. We advocate a form of hints based on advance knowledge which we call disclosure [Patterson93].
Reference: [Lampson83] <author> Lampson, B.W., </author> <title> Hints for Computer System Design, </title> <booktitle> Proc. of the 9th Symp. on Operating System Principles, </booktitle> <address> Bretton Woods, N.H., </address> <year> 1983, </year> <pages> pp. 33-48. </pages>
Reference-contexts: Finally, Sections 9 and 10 provide directions for future research and conclusions. 2 Related work Hints are a well established, broadly applicable technique for improving system performance. Lampson reports their use in operating systems (Alto, Pilot), networking (Arpanet, Ethernet), and language implementation (Smalltalk) <ref> [Lampson83] </ref>. Broadly, these examples consult a possibly out-of-date cache as a hint to short-circuit some expensive computation or blocking event. In the context of file systems, historical information is often used for both file caching and prefetching.
Reference: [Lee90] <author> Lee, K.-F., Hon, H.-W., Reddy, </author> <title> R.An Overview of the SPHINX Speech Recognition System, </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, (USA), </journal> <volume> V 38 (1), </volume> <month> Jan. </month> <year> 1990, </year> <pages> pp. 35-45. </pages>
Reference-contexts: hints to coalesce into one disk read blocks that are contiguous on disk but widely separated in the access sequence, TIP-2 reduces the number of distinct disk reads from 18,700 to 15,000. 90 This improved I/O efficiency contributes to the slight performance advantage of TIP-2 over TIP-1. 7.3 Sphinx Sphinx <ref> [Lee90] </ref> is a high-quality, speaker-independent, continuous-voice, speech-recognition system. In our experiments, Sphinx is recognizing an 18-second recording commonly used in Sphinx regression testing. Sphinx represents acoustics with Hidden Markov Models and uses a Viterbi beam search to prune unpromising word combinations from these models.
Reference: [McKusick84] <author> McKusick, M. K., Joy, W. J., Lefer, S. J., Fabry, R. S., </author> <title> A Fast File System for Unix, </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> V 2 (3), </volume> <month> Aug. </month> <year> 1984, </year> <pages> pp. 181-197. </pages>
Reference-contexts: In the context of file systems, historical information is often used for both file caching and prefetching. The ubiquitous LRU cache replacement algorithm relies on the history of recent accesses to choose a buffer for replacement. For history-based prefetching, the most successful approach is sequential readahead <ref> [Feiertag71, McKusick84] </ref>. Digitals OSF/1 is an aggressive example, prefetching up to 64 blocks ahead when it detects long sequential runs. Others, notably Kotz, have looked at detecting more complex access patterns and prefetching non-sequentially within a file [Kotz91].
Reference: [NCSA89] <institution> National Center for Supercomputing Applications. XDataSlice for the X Window System, http://www. ncsa.uiuc.edu/, Univ. of Illinois at Urbana-Champaign, </institution> <year> 1989. </year>
Reference-contexts: TIP-2 effectively balances the allocation of cache buffers between prefetching and caching. 7.2 XDataSlice XDataSlice (XDS) is an interactive scientific visualization tool developed at the National Center for Supercomputer Applica- tions at the University of Illinois <ref> [NCSA89] </ref>. Among other features, XDS lets scientists view arbitrary planar slices through their 3-dimensional data with a false color mapping. The datasets may originate from a broad range of applications such as airflow simulations, pollution modelling, or magnetic resonance imaging, and tend to be very large.
Reference: [Ng91] <author> Ng, R., Faloutsos, C., Sellis, T., </author> <title> Flexible Buffer Allocation Based on Marginal Gains, </title> <booktitle> Proc. of the 1991 ACM Conf. on Management of Data (SIGMOD), </booktitle> <pages> pp. 387-396. </pages>
Reference-contexts: In large integrated applications, more detailed knowledge may be available. The database community has long taken advantage of this for buffer management. The buffer manager can use the access plan for a query to help determine the number of buffers to allocate <ref> [Sacco82, Chou85, Cornell89, Ng91, Chen93] </ref>. Ng, Faloutsos and Selliss work on marginal gains considered the question of how much benefit a query would derive from an additional buffer. Their work stimulated the development of our approach to cache management.
Reference: [Ousterhout85] <author> Ousterhout, J.K., Da Costa, H., Harrison, D., Kunze, J.A., Kupfer, M., Thompson, J.G., </author> <title> A Trace-Driven Analysis of the UNIX 4.2 BSD File System, </title> <booktitle> Proc. of the 10th Symp. on Operating System Principles, </booktitle> <address> Orcas Island, WA, </address> <month> Dec. </month> <year> 1985, </year> <pages> pp. 15-24. </pages>
Reference-contexts: For example, the Sprite groups 1985 caching study led them to predict higher hit ratios for larger caches. But in 1991, after larger caches had been installed, hit ratios were not much changed files had grown just as fast as the caches <ref> [Ousterhout85, Baker91] </ref>. This suggests that new techniques are needed to boost I/O performance. The problem is especially acute for read-intensive applications. Write performance is less critical because the writing application generally does not wait for the disk to be written.
Reference: [Palmer91] <author> Palmer, </author> <title> M.L., Zdonik, S.B., FIDO: A Cache that Learns to Fetch, </title> <institution> Brown University Technical Report CS-9015, </institution> <year> 1991. </year>
Reference-contexts: Others, notably Kotz, have looked at detecting more complex access patterns and prefetching non-sequentially within a file [Kotz91]. At the level of whole files or database objects, a number of researchers have looked at inferring future accesses based on past accesses <ref> [Korner90, Kotz91, Tait91, Palmer91, Curewitz93, Griffioen94] </ref>. The danger in speculative prefetching based on historical access patterns is that it risks hurting, rather than helping, performance [Smith85]. As a result of this danger, speculative prefetching is usually conservative, waiting until its theories are confirmed by some number of demand accesses.
Reference: [Patterson88] <author> Patterson, D., Gibson, G., Katz, R., </author> <title> A, A Case for Redundant Arrays of Inexpensive Disks (RAID), </title> <booktitle> Proc. of the 1988 ACM Conf. on Management of Data (SIGMOD), </booktitle> <address> Chicago, IL, </address> <month> Jun. </month> <year> 1988, </year> <pages> pp. 109-116. </pages>
Reference-contexts: The second factor encouraging our proactive I/O management is that ever-faster CPUs are processing data more quickly and encouraging the use of ever-larger data objects. Unless file-cache miss ratios decrease in proportion to processor performance, Amdahls law tells us that overall system performance will increasingly depend on I/O-subsystem performance <ref> [Patterson88] </ref>. Unfortunately, simply growing the cache does not decrease cache- miss ratios as much as one might expect. For example, the Sprite groups 1985 caching study led them to predict higher hit ratios for larger caches.
Reference: [Patterson93] <author> Patterson, R.H., Gibson, G., Satyanarayanan, M., </author> <title> A Status Report on Research in Transparent Informed Prefetching, </title> <journal> ACM Operating Systems Review, </journal> <volume> V 27 (2), </volume> <month> Apr. </month> <year> 1993, </year> <pages> pp. 21-34. </pages>
Reference-contexts: We advocate a form of hints based on advance knowledge which we call disclosure <ref> [Patterson93] </ref>. An application discloses its future resource requirements when its hints describe its future requests in terms of the existing request interface. For example, a disclosing hint might indicate that a particular file is going to be read sequentially four times in succession.
Reference: [Patterson94] <author> Patterson, R.H., Gibson, G., </author> <title> Exposing I/O Concurrency with Informed Prefetching, </title> <booktitle> Proc. of the 3rd Int. Conf. on Parallel and Distributed Information Systems, </booktitle> <address> Austin, TX, </address> <month> Sept. </month> <pages> 28-30, </pages> <year> 1994, </year> <pages> pp. 7-16. </pages>
Reference-contexts: Soon thereafter, it was ported to the UX server in a Mach 3.0 system on a DECstation 5000/200. Equipped with four disks and a user-level striper, this system was able to reduce the elapsed time of a seek-intensive data visualization tool (XDataSlice) by up to 70% <ref> [Patterson94] </ref>. During the summer of 1994 we ported TIP-1 to the current Alpha testbed to exploit its greater CPU and disk performance. During 1994, we designed and began implementation of a second test system, TIP-2, which exploits hints for both informed prefetching and informed caching. <p> In the second half of the split loop, XDS reads the cached pixel mappings, reads the corresponding data from the cached blocks, and applies the false coloring <ref> [Patterson94] </ref>. Our test dataset consists of 512 3 32-bit floating point values requiring 512 MB of disk storage. The dataset is organized into 8 KB blocks of 16x16x8 data points and is stored on the disk in Z- major order. Our test renders 25 random slices through the dataset.
Reference: [Rosenblum91] <author> Rosenblum, M., Ousterhout, J.K., </author> <title> The Design and Implementation of a Log-Structured File System, </title> <booktitle> Proc. of the 13th Symp. on Operating System Principles, </booktitle> <address> Pacific Grove, CA, </address> <month> Oct. </month> <year> 1991, </year> <pages> pp. 1-15. </pages>
Reference-contexts: Request Permissions from Publications Dept, ACM Inc., Fax +1 (212) 869-0481, or &lt;permissions@acm.org&gt;. Appears in Proc. of the 15th ACM Symp. on Operating System Principles, Copper Mountain Resort, CO, Dec. 3-6, 1995, pp. 79-95. 80 when the applications writes are serial and non-sequential <ref> [Rosenblum91, Solworth90] </ref>. Examples of read-intensive applications include: text search, 3D scientific visualization, relational database queries, speech recognition, object code linkers, and computational physics.
Reference: [Sacco82] <author> Sacco, G.M., Schkolnick, M., </author> <title> A Mechanism for Managing the Buffer Pool in a Relational Database System Using the Hot Set Model, </title> <booktitle> Proc. of the 8th Int. Conf. on Very Large Data Bases, </booktitle> <month> Sep. </month> <year> 1982, </year> <pages> pp. 257-262. </pages>
Reference-contexts: In large integrated applications, more detailed knowledge may be available. The database community has long taken advantage of this for buffer management. The buffer manager can use the access plan for a query to help determine the number of buffers to allocate <ref> [Sacco82, Chou85, Cornell89, Ng91, Chen93] </ref>. Ng, Faloutsos and Selliss work on marginal gains considered the question of how much benefit a query would derive from an additional buffer. Their work stimulated the development of our approach to cache management.
Reference: [Salem86] <author> Salem, K. Garcia-Molina, H., </author> <title> Disk Striping, </title> <booktitle> Proc. of the 2nd IEEE Int. Conf. on Data Engineering, </booktitle> <year> 1986. </year>
Reference-contexts: Storage parallelism is increasingly available in the form of disk arrays and striping device drivers. These hardware and software arrays promise the I/O throughput needed to balance ever- faster CPUs by distributing the data of a single file system over many disk arms <ref> [Salem86] </ref>. Trivially parallel I/O workloads benefit immediately; very large accesses benefit from parallel transfer, and multiple concurrent accesses benefit from independent disk actuators. Unfortunately, many I/O workloads are not at all parallel, but instead consist of serial streams of non-sequential accesses.
Reference: [Smith85] <author> Smith, A.J., </author> <title> Disk Cache Miss Ratio Analysis and Design Considerations, </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> V 3 (3), </volume> <month> Aug. </month> <year> 1985, </year> <pages> pp. 161-203. </pages>
Reference-contexts: At the level of whole files or database objects, a number of researchers have looked at inferring future accesses based on past accesses [Korner90, Kotz91, Tait91, Palmer91, Curewitz93, Griffioen94]. The danger in speculative prefetching based on historical access patterns is that it risks hurting, rather than helping, performance <ref> [Smith85] </ref>. As a result of this danger, speculative prefetching is usually conservative, waiting until its theories are confirmed by some number of demand accesses. An alternate class of hints are those that express one system components advance knowledge of its impact on another.
Reference: [Solworth90] <author> Solworth, J.A., Orji, C.U., </author> <title> Write-Only Disk Caches, </title> <booktitle> Proc. of the 1990 ACM Int. Conf. on Management of Data (SIGMOD), </booktitle> <pages> pp. 123-132. </pages>
Reference-contexts: Request Permissions from Publications Dept, ACM Inc., Fax +1 (212) 869-0481, or &lt;permissions@acm.org&gt;. Appears in Proc. of the 15th ACM Symp. on Operating System Principles, Copper Mountain Resort, CO, Dec. 3-6, 1995, pp. 79-95. 80 when the applications writes are serial and non-sequential <ref> [Rosenblum91, Solworth90] </ref>. Examples of read-intensive applications include: text search, 3D scientific visualization, relational database queries, speech recognition, object code linkers, and computational physics.
Reference: [Stathopoulos94] <author> Stathopoulos, A., Fischer, C. F., </author> <title> A Davidson program for finding a few selected extreme eigenpairs of a large, sparse, real, symmetric matrix, </title> <journal> Computer Physics Communications, </journal> <volume> vol. 79, </volume> <year> 1994, </year> <pages> pp. 268-290. </pages> <note> [Steere95]Steere, </note> <author> D., Satyanarayanan, M., </author> <title> Using Dynamic Sets to Overcome High I/O Latencies during Search, </title> <booktitle> Proc. of the 5th Workshop on Hot Topics in Operating Systems, </booktitle> <address> Orcas Island, WA, </address> <month> May 4-5, </month> <year> 1995, </year> <pages> pp. 136-140. </pages>
Reference-contexts: The Davidson algorithm <ref> [Stathopoulos94] </ref> is an element of the suite that computes, by successive refinement, the extreme eigenvalue-eigenvector pairs of a large, sparse, real, symmetric matrix stored on disk. In our test, the size of this matrix is 16.3 MB.
Reference: [Stonebraker86] <author> Stonebraker, M., Rowe, </author> <title> L, The Design of Postgres, </title> <booktitle> Proc. of 1986 ACM Int. Conf. on Management of Data (SIGMOD), </booktitle> <address> Washington, DC, USA, </address> <month> 28-30 May </month> <year> 1986. </year>
Reference-contexts: With hints, Gnuld eliminates 77% of its stall time with 4 disks and 87% with 10 disks. The remaining stall time is mostly due to the remaining unhinted accesses that Gnuld performs. 7.6 Postgres Postgres version 4.2 <ref> [Stonebraker86, Stonebraker90] </ref> is an extensible, object-oriented relational database system from the University of California at Berkeley. In our test, Postgres executes a join of two relations. The outer relation contains 20,000 unindexed tuples (3.2 MB) while the inner relation has 200,000 tuples (32 MB) and is indexed (5 MB).
Reference: [Stonebraker90] <author> Stonebraker, M., Rowe, L.A., Hirohama, M., </author> <title> The implementation of POSTGRES, </title> <journal> IEEE Trans. on Knowledge and Data Engineering, </journal> <volume> V 2 (1), </volume> <month> Mar. </month> <year> 1990, </year> <pages> pp. 125-42 </pages>
Reference-contexts: With hints, Gnuld eliminates 77% of its stall time with 4 disks and 87% with 10 disks. The remaining stall time is mostly due to the remaining unhinted accesses that Gnuld performs. 7.6 Postgres Postgres version 4.2 <ref> [Stonebraker86, Stonebraker90] </ref> is an extensible, object-oriented relational database system from the University of California at Berkeley. In our test, Postgres executes a join of two relations. The outer relation contains 20,000 unindexed tuples (3.2 MB) while the inner relation has 200,000 tuples (32 MB) and is indexed (5 MB).
Reference: [Sun88] <author> Sun Microsystems, Inc., </author> <title> Sun OS Reference Manual, Part Number 800-1751-10, Revision A, </title> <month> May 9, </month> <year> 1988. </year>
Reference-contexts: Perhaps the most familiar of these occurs in the form of policy advice from an application to the virtual-memory or file-cache modules. In these hints, the application recommends a resource management policy that has been statically or dynamically determined to improve performance for this application <ref> [Trivedi79, Sun88, Cao94] </ref>. In large integrated applications, more detailed knowledge may be available. The database community has long taken advantage of this for buffer management. The buffer manager can use the access plan for a query to help determine the number of buffers to allocate [Sacco82, Chou85, Cornell89, Ng91, Chen93].
Reference: [Tait91] <author> Tait, C.D., Duchamp, D., </author> <title> Detection and Exploitation of File Working Sets, </title> <booktitle> Proc. of the 11th Int. Conf. on Distributed Computing Systems, </booktitle> <address> Arlington, TX, </address> <month> May, </month> <year> 1991, </year> <pages> pp. 2-9. </pages>
Reference-contexts: Others, notably Kotz, have looked at detecting more complex access patterns and prefetching non-sequentially within a file [Kotz91]. At the level of whole files or database objects, a number of researchers have looked at inferring future accesses based on past accesses <ref> [Korner90, Kotz91, Tait91, Palmer91, Curewitz93, Griffioen94] </ref>. The danger in speculative prefetching based on historical access patterns is that it risks hurting, rather than helping, performance [Smith85]. As a result of this danger, speculative prefetching is usually conservative, waiting until its theories are confirmed by some number of demand accesses.
Reference: [Trivedi79] <author> Trivedi, </author> <title> K.S., An Analysis of Prepaging, </title> <journal> Computing, </journal> <volume> V 22 (3), </volume> <year> 1979, </year> <pages> pp. 191-210. </pages>
Reference-contexts: Perhaps the most familiar of these occurs in the form of policy advice from an application to the virtual-memory or file-cache modules. In these hints, the application recommends a resource management policy that has been statically or dynamically determined to improve performance for this application <ref> [Trivedi79, Sun88, Cao94] </ref>. In large integrated applications, more detailed knowledge may be available. The database community has long taken advantage of this for buffer management. The buffer manager can use the access plan for a query to help determine the number of buffers to allocate [Sacco82, Chou85, Cornell89, Ng91, Chen93].
Reference: [Wu92] <author> Wu, S. and Manber, U. </author> <title> AGREP-a fast approximate pat-tern-matching tool, </title> <booktitle> Proc. of the 1992 Winter USENIX Conference, </booktitle> <address> San Francisco, CA, </address> <month> Jan. </month> <year> 1992, </year> <pages> pp. 20-24. </pages>
Reference-contexts: Thus, the informed caching in TIP-2 does not improve upon the performance of simple informed prefetching in TIP-1. 7.4 Agrep Agrep, a variant of grep, was written by Wu and Manber at the University of Arizona <ref> [Wu92] </ref>. It is a full-text pattern matching program that allows errors. Invoked in its simplest form, it opens the files specified on its command line one at a time, in argument order, and reads each sequentially.
References-found: 37

