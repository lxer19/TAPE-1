URL: ftp://ftp.mbfys.kun.nl/snn/pub/reports/Kappen.LR_NIPS.ps.Z
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00349.html
Root-URL: 
Title: Boltzmann Machine learning using mean field theory and linear response correction  
Author: H.J. Kappen F. B. Rodrguez 
Address: NL 6525 EZ Nijmegen, The Netherlands  Madrid, Canto Blanco,28049 Madrid, Spain  
Affiliation: Department of Biophysics University of Nijmegen, Geert Grooteplein 21  Instituto de Ingeniera del Conocimiento Departamento de Ingeniera Informatica, Universidad Autonoma de  
Abstract: We present a new approximate learning algorithm for Boltzmann Machines, using a systematic expansion of the Gibbs free energy to second order in the weights. The linear response correction to the correlations is given by the Hessian of the Gibbs free energy. The computational complexity of the algorithm is cubic in the number of neurons. We compare the performance of the exact BM learning algorithm with first order (Weiss) mean field theory and second order (TAP) mean field theory. The learning task consists of a fully connected Ising spin glass model on 10 neurons. We conclude that 1) the method works well for paramagnetic problems 2) the TAP correction gives a significant improvement over the Weiss mean field theory, both for paramagnetic and spin glass problems and 3) that the inclusion of diagonal weights improves the Weiss approximation for paramagnetic problems, but not for spin glass problems.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Ackley, G. Hinton, and T. Sejnowski. </author> <title> A learning algorithm for Boltzmann Machines. </title> <journal> Cognitive Science, </journal> <volume> 9 </volume> <pages> 147-169, </pages> <year> 1985. </year>
Reference-contexts: 1 Introduction Boltzmann Machines (BMs) <ref> [1] </ref>, are networks of binary neurons with a stochastic neuron dynamics, known as Glauber dynamics. Assuming symmetric connections between neurons, the probability distribution over neuron states ~s will become stationary and is given by the Boltzmann-Gibbs distribution P (~s). <p> This is due to the fact that P (~s) contains a normalization term Z, which involves a sum over all states in the network, of which there are exponentially many. This problem is particularly important for BM learning. Using statistical sampling techiques [2], learning can be significantly improved <ref> [1] </ref>. However, the method has rather poor convergence and can only be applied to small networks. In [3, 4], an acceleration method for learning in BMs is proposed using mean field theory by replacing hs i s j i by m i m j in the learning rule. <p> Learning <ref> [1] </ref> consists of adjusting the weights and thresholds in such a way that the Boltzmann distribution approximates a target distribution q (~s) as closely as possible. <p> A suitable measure of the difference between the distributions p (~s) and q (~s) is the Kullback divergence [9] K = ~s q (~s) : (2) Learning consists of minimizing K using gradient descent <ref> [1] </ref> w ij = hs i s j i c hs i s j i ; i = hs i i c hs i i : The parameter is the learning rate. The brackets hi and hi c denote the 'free' and 'clamped' expectation values, respectively.
Reference: [2] <author> C. Itzykson and J-M. Drouffe. </author> <title> Statistical Field Theory. Cambridge monographs on mathematical physics. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, UK, </address> <year> 1989. </year>
Reference-contexts: This is due to the fact that P (~s) contains a normalization term Z, which involves a sum over all states in the network, of which there are exponentially many. This problem is particularly important for BM learning. Using statistical sampling techiques <ref> [2] </ref>, learning can be significantly improved [1]. However, the method has rather poor convergence and can only be applied to small networks.
Reference: [3] <author> C. Peterson and J.R. Anderson. </author> <title> A mean field theory learning algorithm for neural networks. </title> <journal> Complex Systems, </journal> <volume> 1 </volume> <pages> 995-1019, </pages> <year> 1987. </year>
Reference-contexts: This problem is particularly important for BM learning. Using statistical sampling techiques [2], learning can be significantly improved [1]. However, the method has rather poor convergence and can only be applied to small networks. In <ref> [3, 4] </ref>, an acceleration method for learning in BMs is proposed using mean field theory by replacing hs i s j i by m i m j in the learning rule.
Reference: [4] <author> G.E. Hinton. </author> <title> Deterministic Boltzmann learning performs steepest descent in weight-space. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 143-150, </pages> <year> 1989. </year>
Reference-contexts: This problem is particularly important for BM learning. Using statistical sampling techiques [2], learning can be significantly improved [1]. However, the method has rather poor convergence and can only be applied to small networks. In <ref> [3, 4] </ref>, an acceleration method for learning in BMs is proposed using mean field theory by replacing hs i s j i by m i m j in the learning rule.
Reference: [5] <author> H.J. Kappen and F.B. Rodrguez. </author> <title> Efficient learning in Boltzmann Machines using linear response theory. </title> <booktitle> Neural Computation, </booktitle> <year> 1997. </year> <note> In press. </note>
Reference-contexts: In [3, 4], an acceleration method for learning in BMs is proposed using mean field theory by replacing hs i s j i by m i m j in the learning rule. It can be shown <ref> [5] </ref> that such a naive mean field approximation of the learning rules does not converge in general. Furthermore, we argue that the correlations can be computed using the linear response theorem [6]. <p> It can be shown [5] that such a naive mean field approximation of the learning rules does not converge in general. Furthermore, we argue that the correlations can be computed using the linear response theorem [6]. In <ref> [7, 5] </ref> the mean field approximation is derived by making use of the properties of convex functions (Jensen's inequality and tangential bounds). In this paper we present an alternative derivation which uses a Legendre transformation and a small coupling expansion [8]. <p> The inclusion of hidden units is straigthforward. One applies the above approximations in the free and the clamped phase separately <ref> [5] </ref>. The complexity of the method is O (n 3 ), due to the matrix inversion. 4 Learning without hidden units We will assess the accuracy of the above method for networks without hidden units. <p> Since this is the standard Weiss mean field expression, we refer to this method as the Weiss approximation. The fixed point equations are only imposed for the off-diagonal elements of w ij because the Boltzmann distribution Eq. 1 does not depend on the diagonal elements w ii . In <ref> [5] </ref>, we explored a variant of the Weiss approximation, where we included diagonal weight terms. As is discussed there, if we were to impose Eq. 7 for i = j as well, we have A = C. If C is invertible, we therefore have A 1 = C 1 . <p> Despite these large fluctuations, the quality of the TAP solution is consistently better than the Weiss solution. In Fig. 1c, we plot the difference between the TAP and Weiss solution, averaged over the 10 problem instances. In <ref> [5] </ref> we concluded that the Weiss solution with diagonal weights is better than the standard Weiss solution when learning a finite number of randomly generated patterns. In Fig. 1d we plot the difference between the Weiss solution with and without diagonal weights.
Reference: [6] <author> G. Parisi. </author> <title> Statistical Field Theory. </title> <booktitle> Frontiers in Physics. </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1988. </year>
Reference-contexts: It can be shown [5] that such a naive mean field approximation of the learning rules does not converge in general. Furthermore, we argue that the correlations can be computed using the linear response theorem <ref> [6] </ref>. In [7, 5] the mean field approximation is derived by making use of the properties of convex functions (Jensen's inequality and tangential bounds). In this paper we present an alternative derivation which uses a Legendre transformation and a small coupling expansion [8].
Reference: [7] <author> L.K. Saul, T. Jaakkola, and M.I. Jordan. </author> <title> Mean field theory for sigmoid belief networks. </title> <journal> Journal of artificial intelligence research, </journal> <volume> 4 </volume> <pages> 61-76, </pages> <year> 1996. </year>
Reference-contexts: It can be shown [5] that such a naive mean field approximation of the learning rules does not converge in general. Furthermore, we argue that the correlations can be computed using the linear response theorem [6]. In <ref> [7, 5] </ref> the mean field approximation is derived by making use of the properties of convex functions (Jensen's inequality and tangential bounds). In this paper we present an alternative derivation which uses a Legendre transformation and a small coupling expansion [8].
Reference: [8] <author> T. Plefka. </author> <title> Convergence condition of the TAP equation for the infinite-range Ising spin glass model. </title> <journal> Journal of Physics A, </journal> <volume> 15 </volume> <pages> 1971-1978, </pages> <year> 1982. </year>
Reference-contexts: In [7, 5] the mean field approximation is derived by making use of the properties of convex functions (Jensen's inequality and tangential bounds). In this paper we present an alternative derivation which uses a Legendre transformation and a small coupling expansion <ref> [8] </ref>. It has the advantage that higher order contributions (TAP and higher) can be computed in a systematic manner and that it may be applicable to arbitrary graphical models. 2 Boltzmann Machine learning The Boltzmann Machine is defined as follows. <p> As a result, the BM learning algorithm can not be applied to practical problems. 3 The mean field approximation We derive the mean field free energy using the small fl expansion as introduced by Plefka <ref> [8] </ref>. The energy of the network is given by E (s; w; h; fl) = flE int i E int = 2 ij for fl = 1. <p> The expectation hi fl is with respect to the full model with interaction fl. We expand G (fl) = G (0) + flG 0 (0) + 2 We directly obtain from <ref> [8] </ref> G 0 (fl) = hE int i fl 2 int fl + E int i @fl + For fl = 0 the expectation values hi fl become the mean field expectations which we can directly compute: G (0) = 2 i (1 + m i ) log 2 1 (1
Reference: [9] <author> S. Kullback. </author> <title> Information Theory and Statistics. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1959. </year>
Reference-contexts: Learning [1] consists of adjusting the weights and thresholds in such a way that the Boltzmann distribution approximates a target distribution q (~s) as closely as possible. A suitable measure of the difference between the distributions p (~s) and q (~s) is the Kullback divergence <ref> [9] </ref> K = ~s q (~s) : (2) Learning consists of minimizing K using gradient descent [1] w ij = hs i s j i c hs i s j i ; i = hs i i c hs i i : The parameter is the learning rate.
Reference: [10] <author> D. Sherrington and S. Kirkpatrick. </author> <title> Solvable model of Spin-Glass. </title> <journal> Physical review letters, </journal> <volume> 35 </volume> <pages> 1792-1796, </pages> <year> 1975. </year>
Reference-contexts: Gaussian variables with mean J 0 n1 and variance J 2 n1 . This model is known as the Sherrington-Kirkpatrick (SK) model <ref> [10] </ref>. Depending on the values of J and J 0 , the model displays a para-magnetic (unordered), ferro-magnetic (ordered) and a spin-glass (frustrated) phase. For J 0 = 0, the para-magnetic (spin-glass) phase is obtained for J &lt; 1 (J &gt; 1).
References-found: 10

