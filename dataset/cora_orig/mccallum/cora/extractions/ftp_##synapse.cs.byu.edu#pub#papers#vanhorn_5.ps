URL: ftp://synapse.cs.byu.edu/pub/papers/vanhorn_5.ps
Refering-URL: ftp://synapse.cs.byu.edu/pub/papers/details.html
Root-URL: 
Email: email: kevin@bert.cs.byu.edu, martinez@cs.byu.edu  
Title: Extending Occam's Razor  
Author: Kevin S. Van Horn Tony R. Martinez TMCB 
Address: Provo, UT 84602  
Affiliation: Computer Science Department Brigham Young University  
Abstract: This paper will appear in the Proceedings of the Third Golden West International Conference on Intelligent Systems, Las Vegas, Nevada, 6 June 1994. An extended version of this paper that also examines "loose Occam Algorithms" appears as Chapter 3 of K. S. Van Horn Learning as Optimization Ph.D. dissertation, Computer Science Dept. Brigham Young University, Provo, UT July 1994 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Blumer, A., et al. </author> <year> (1989.) </year> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of the ACM 36, </journal> <pages> 929-965. </pages>
Reference-contexts: Occam's Razor can be applied once we have a measure of the complexity of a hypothesis. Such a measure is obtained by choosing some means of representing hypotheses, and defining the complexity of a hypothesis in terms of the size of its smallest representation. Blumer et al. <ref> [1] </ref> have formalized this intuition for a restricted model of learning. <p> That is, one tries to minimize the empirical risk (a measure of error on the training sample) with different bounds on the complexity of the hypotheses to be considered, then uses a separate hold-out set to choose the best complexity bound. As in <ref> [1] </ref>, attention is paid to avoiding exact minimization and its attendant intractability | a limited increase in hypothesis complexity is allowed over what is strictly needed to attain a given level of empirical risk. We obtain sample complexity bounds similar to those in [1], but for Haussler's more general learning model. <p> As in <ref> [1] </ref>, attention is paid to avoiding exact minimization and its attendant intractability | a limited increase in hypothesis complexity is allowed over what is strictly needed to attain a given level of empirical risk. We obtain sample complexity bounds similar to those in [1], but for Haussler's more general learning model. 2. Background 2.1. The PAC model and Occam's Razor The PAC (probably approximately correct) model of learning [1, 4-6] is a simplified learning model first introduced by Valiant [6]. <p> We obtain sample complexity bounds similar to those in [1], but for Haussler's more general learning model. 2. Background 2.1. The PAC model and Occam's Razor The PAC (probably approximately correct) model of learning <ref> [1, 4-6] </ref> is a simplified learning model first introduced by Valiant [6]. The elements of the model are an instance space X and a "stratified" hypothesis space H = (H i ) i1 , where H i H i+1 and we write H for S i H i . <p> Blumer et al. <ref> [1] </ref> discuss the Occam's Razor approach to learning: find a near-smallest consistent hypothesis (a consistent hypothesis correctly classifies all m training examples). <p> Their result is framed in terms of the VC dimension of H i , a combinatorial parameter that is generally related to the number of bits or parameters required to specify an arbitrary hypothesis in H i (see <ref> [1] </ref>).
Reference: [2] <author> Devroye, L. </author> <year> (1988.) </year> <title> Automatic pattern recognition: a study of the probability of error. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence 10, </journal> <pages> 530-543. </pages>
Reference-contexts: Let m def 3. For all 1 i blog m= log (1=c)c, compute h i def 4. Output that h i minimizing ^r (h i ; z 2 ). The above is essentially the "hold-out" method often used in applied statistics (see <ref> [2] </ref>, for example).
Reference: [3] <author> Haussler, D. </author> <year> (1990.) </year> <title> Decision theoretic generalizations of the PAC model for neural net and other learning applications. </title> <type> Technical report UCSC-CRL-91-02, </type> <institution> Baskin Center for Computer Engineering and Information Sciences, UC Santa Cruz. </institution>
Reference-contexts: This rules out problems such as learning real-valued functions, probability distributions, class probability distributions as a function of the instance to be classified, and the Bayes-optimal classifier in a stochastic setting. Haussler <ref> [3] </ref> has generalized the PAC model to deal with these situations and others. In this paper we prove, for Haussler's model, an analog of the Occam algorithm result. The approach we analyze is essentially the "hold-out" method often used in applied statistics. <p> This is important, because exact mini mization is often NP-hard. 2.2. Haussler's Extension of the PAC Model Haussler <ref> [3] </ref> has extended the PAC model to handle more general learning situations. <p> and l (y 0 ; y) = (y 0 6= y). (Note: in this paper we abuse terminology by writing pdim (H) when we really mean pdimff h : h 2 Hg, where f h (x; y) = l (h (x); y).) Details on the pseudodimension may be found in <ref> [3] </ref>. <p> We write ^r (h) when z is understood. A learning algorithm is said to use empirical risk minimization <ref> [3, 7, 8] </ref> if it returns a hypothesis whose empirical risk is the minimum possible for hypotheses from H. <p> We write ^r (h) when z is understood. A learning algorithm is said to use empirical risk minimization [3, 7, 8] if it returns a hypothesis whose empirical risk is the minimum possible for hypotheses from H. Combining Lemma 1 and Theorem 7 of Haussler <ref> [3] </ref> gives this result: any learning algorithm that uses empirical risk minimization has a sample complexity that is O ((ff 2 -) 1 (pdim (H) log (ff-) 1 + log ffi 1 )): (1) 3. Summary of Results We begin with some definitions. <p> We will assume throughout that M is a bound on the loss function for whatever hypothesis class H is being discussed. The theorems from Haussler give sample complexity bounds for hypothesis classes of finite cardinality or finite pseudodimension. Theorem 1 (Theorem 1 of <ref> [3] </ref>.) Let H be a finite set of hypotheses; let a sequence z of m examples be drawn randomly and independently from the distribution D; and let - &gt; 0 and 0 &lt; ff &lt; 1. <p> Then Pr [9h 2 H: d -(^r (h; z); r (h)) &gt; ff] 2jHj exp (ff 2 -m=M ): For ffi &gt; 0 and m M (ff 2 -) 1 (ln jHj + ln (2=ffi)) this probability is at most ffi. Theorem 2 (Theorem 7 of <ref> [3] </ref>.) Let H be a set of hypotheses with pdim (H) = n for some 1 n &lt; 1; let a sequence z of m 1 examples be drawn randomly and independently from the distribution D; and let 0 &lt; - 8M and 0 &lt; ff &lt; 1.
Reference: [4] <author> Kearns, M. </author> <year> (1990.) </year> <title> The Computational Complexity of Machine Learning. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: [5] <author> Natarajan, B. K. </author> <year> (1991.) </year> <title> Machine Learning: A Theoretical Approach. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [6] <author> Valiant, L. G. </author> <year> (1984.) </year> <title> A theory of the learnable. </title> <journal> Communications of the ACM 27, </journal> <pages> 1134-1142. </pages>
Reference-contexts: We obtain sample complexity bounds similar to those in [1], but for Haussler's more general learning model. 2. Background 2.1. The PAC model and Occam's Razor The PAC (probably approximately correct) model of learning [1, 4-6] is a simplified learning model first introduced by Valiant <ref> [6] </ref>. The elements of the model are an instance space X and a "stratified" hypothesis space H = (H i ) i1 , where H i H i+1 and we write H for S i H i . The elements of H are functions from X to f0; 1g.
Reference: [7] <author> Vapnik, V. N. </author> <year> (1982.) </year> <title> Estimation of Dependences Based on Empirical Data. </title> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: We write ^r (h) when z is understood. A learning algorithm is said to use empirical risk minimization <ref> [3, 7, 8] </ref> if it returns a hypothesis whose empirical risk is the minimum possible for hypotheses from H.
Reference: [8] <author> Vapnik, V. N. </author> <year> (1989.) </year> <title> Inductive principles of the search for empirical dependences. </title> <booktitle> In Proceedings of the 2nd Annual Workshop on Computational Learning Theory. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: We write ^r (h) when z is understood. A learning algorithm is said to use empirical risk minimization <ref> [3, 7, 8] </ref> if it returns a hypothesis whose empirical risk is the minimum possible for hypotheses from H.
References-found: 8

