URL: ftp://ftp.cs.buffalo.edu/users/jagota/HKP.ps.Z
Refering-URL: http://neural-server.aston.ac.uk/NN/books.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: email: jagota@next1.msci.memst.edu  
Title: An Exercises Supplement to the Introduction To The Theory Of Neural Computation  
Author: J. Hertz, A. Krogh, and R.G. Palmer Arun Jagota 
Note: Includes Contributions from Connectionists 25 May 1995, Version 2  
Address: Memphis, TN, 38152  
Affiliation: Department of Mathematical Sciences University of Memphis  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> J. Hertz, A. Krogh, and R.G. Palmer. </author> <title> Introduction to the Theory of Neural Computation. </title> <publisher> Addison-Wesley, </publisher> <year> 1991. </year>
Reference: [2] <author> C.B. Miller and C.L. Giles. </author> <title> Experimental comparison of the effect of order in recurrent neural networks. </title> <journal> International Journal of Pattern Recognition and Artifical Intelligence, </journal> <volume> 7(4) </volume> <pages> 849-872, </pages> <year> 1993. </year> <title> Special Issue on Neural Networks and Pattern Recognition, </title> <editor> editors: I. Guyon , P.S.P. </editor> <publisher> Wang. </publisher>
Reference-contexts: Discuss how the strings of 0s and 1s will be encoded and how the network will do the training. Discuss the minimum training set needed. How well will the trained network perform on the unseen strings? Is the network stable? (See for example <ref> [2] </ref>) 3.5 Geoff Goodhill, h geoff@salk.edu i Tutorial exercise: Linsker's model Background Linsker's model for the development of certain types of receptive fields in the visual system has generated much interest in the neural networks literature.
Reference: [3] <author> G.V. Puskorius and L.A. Feldkamp. </author> <title> Neurocontrol of nonlinear dynamical systems with kalman filter-trained recurrent networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 5(2) </volume> <pages> 279-297, </pages> <year> 1994. </year>
Reference-contexts: Make sure the complexity considers whether or not the networks are fully or partially connected. (Solution: see [6]). 2. Derive how a recurrent network can be trained with an extended Kalman estimator. What is the space and time complexity of this training algorithm? (Solution: see <ref> [5, 3] </ref>) 3. How do recurrent networks compare to FIR and IIR filters? Show architectural examples. Derive appropriate training algorithms. (Solution: see [4]) 3.5. GEOFF GOODHILL, h GEOFF@SALK.EDU i 19 4.
Reference: [4] <author> A.C. Tsoi and A. </author> <title> Back. Locally recurrent globally feedforward networks, a critical review of architectures. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 5(2) </volume> <pages> 229-239, </pages> <year> 1994. </year>
Reference-contexts: Derive how a recurrent network can be trained with an extended Kalman estimator. What is the space and time complexity of this training algorithm? (Solution: see [5, 3]) 3. How do recurrent networks compare to FIR and IIR filters? Show architectural examples. Derive appropriate training algorithms. (Solution: see <ref> [4] </ref>) 3.5. GEOFF GOODHILL, h GEOFF@SALK.EDU i 19 4. Learn the parity grammar (all strings with an odd number of ones are accepted, all others rejected) with a recurrent network and an appropriate training algorithm.
Reference: [5] <author> R.J. Williams. </author> <title> Some observations on the use of the extended kalman filter as a recurrent network learning algorithm. </title> <type> Technical Report NU-CCS-92-1, </type> <institution> Computer Science, Northeastern University, </institution> <address> Boston, MA, </address> <year> 1992. </year>
Reference-contexts: Make sure the complexity considers whether or not the networks are fully or partially connected. (Solution: see [6]). 2. Derive how a recurrent network can be trained with an extended Kalman estimator. What is the space and time complexity of this training algorithm? (Solution: see <ref> [5, 3] </ref>) 3. How do recurrent networks compare to FIR and IIR filters? Show architectural examples. Derive appropriate training algorithms. (Solution: see [4]) 3.5. GEOFF GOODHILL, h GEOFF@SALK.EDU i 19 4.
Reference: [6] <author> R.J. Williams and D. Zipser. </author> <title> Gradient-based learning algorithms for recurrent connectionist networks. </title> <type> Technical Report NU-CCS-90-9, </type> <institution> Computer Science, Northeastern University, </institution> <address> Boston, MA, </address> <year> 1990. </year>
Reference-contexts: Derive and contrast the complexity both in space and time for back-propagation through time and real-time recurrent learning training algorithms. Make sure the complexity considers whether or not the networks are fully or partially connected. (Solution: see <ref> [6] </ref>). 2. Derive how a recurrent network can be trained with an extended Kalman estimator. What is the space and time complexity of this training algorithm? (Solution: see [5, 3]) 3. How do recurrent networks compare to FIR and IIR filters? Show architectural examples.
Reference: [7] <author> Linsker, R. </author> <year> (1986). </year> <title> From basic network principles to neural architecture (series). </title> <booktitle> Proc. </booktitle> <institution> Nat. Acad. Sci., USA, </institution> <month> 83, </month> <pages> 7508-7512, 8390-8394, 8779-8783. </pages>
Reference-contexts: This is because it shows how a series of layers of simple units, evolving with a simple learning rule, can generate progressively more complicated sets of feature detectors given entirely random activity in the first layer. The model was originally presented in a series of rather technical papers <ref> [7] </ref>. A later article [8] reviewed this work, and went on to discuss optimization and information-theoretic properties of the model, as well as a number of more general issues. <p> This is reasonable because there are no interactions between units in the same layer, and all units in a layer evolve simliar receptive fields. The Program Here are some hints about how to write the program. See <ref> [7, 8] </ref> if anything is unclear. Write the program with the following main data structures: * A weight array w containing the weights from each input unit to the single output unit.
Reference: [8] <author> Linsker, R. </author> <year> (1988). </year> <title> Self-organization in a perceptual network. </title> <journal> Computer , March, </journal> <pages> 105-117. </pages>
Reference-contexts: The model was originally presented in a series of rather technical papers [7]. A later article <ref> [8] </ref> reviewed this work, and went on to discuss optimization and information-theoretic properties of the model, as well as a number of more general issues. <p> This is reasonable because there are no interactions between units in the same layer, and all units in a layer evolve simliar receptive fields. The Program Here are some hints about how to write the program. See <ref> [7, 8] </ref> if anything is unclear. Write the program with the following main data structures: * A weight array w containing the weights from each input unit to the single output unit.
Reference: [9] <author> MacKay, D.J.C. & Miller, K.D. </author> <year> (1990). </year> <title> Analysis of Linsker's simulations of Hebbian rules. </title> <journal> Neural Computation, </journal> <volume> 2, </volume> <pages> 169-182. </pages>
Reference-contexts: 37500 -0.50 -0.50 -0.50 -0.50 -0.50 -0.50 -0.50 -0.50 -0.50 0.50 0.50 0.50 0.50 -0.50 -0.50 0.50 0.50 0.50 0.50 0.50 -0.50 -0.50 -0.50 -0.50 0.09 -0.50 -0.50 -0.50 A detailed theoretical analysis of the effects of k 1 and k 2 on the final results can be found in <ref> [9, 10] </ref>: read these if you want an explanation in terms of competing eigenvectors. Linsker's system has sometimes been criticised for being very sensitive to it's parameters.
Reference: [10] <author> MacKay, D.J.C. & Miller, K.D. </author> <year> (1990). </year> <title> Analysis of Linsker's application of Hebbian rules to linear networks. </title> <booktitle> Network , 1, </booktitle> <pages> 257-297. 27 </pages>
Reference-contexts: 37500 -0.50 -0.50 -0.50 -0.50 -0.50 -0.50 -0.50 -0.50 -0.50 0.50 0.50 0.50 0.50 -0.50 -0.50 0.50 0.50 0.50 0.50 0.50 -0.50 -0.50 -0.50 -0.50 0.09 -0.50 -0.50 -0.50 A detailed theoretical analysis of the effects of k 1 and k 2 on the final results can be found in <ref> [9, 10] </ref>: read these if you want an explanation in terms of competing eigenvectors. Linsker's system has sometimes been criticised for being very sensitive to it's parameters.
References-found: 10

