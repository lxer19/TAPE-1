URL: ftp://ftp.cs.rochester.edu/pub/papers/systems/94.High_performance_synchronization_algorithms.ps.gz
Refering-URL: http://www.cs.rochester.edu/trs/systems-trs.html
Root-URL: 
Email: fbob,kthanasi,scottg@cs.rochester.edu  
Title: High Performance Synchronization Algorithms for Multiprogrammed Multiprocessors  
Author: Robert W. Wisniewski, Leonidas I. Kontothanassis, and Michael L. Scott 
Date: December 1994  
Address: Rochester, NY 14627-0226  
Affiliation: Department of Computer Science University of Rochester  
Abstract: Scalable busy-wait synchronization algorithms are essential for achieving good parallel program performance on large scale multiprocessors. Such algorithms include mutual exclusion locks, reader-writer locks, and barrier synchronization. Unfortunately, scalable synchronization algorithms are particularly sensitive to the effects of multiprogramming: their performance degrades sharply when processors are shared among different applications, or even among processes of the same application. In this paper we describe the design and evaluation of scalable scheduler-conscious mutual exclusion locks, reader-writer locks, and barriers, and show that by sharing information across the kernel/application interface we can improve the performance of scheduler-oblivious implementations by more than an order of magnitude. fl This work was supported in part by National Science Foundation grants numbers CCR-9319445 and CDA-8822724, by ONR contract number N00014-92-J-1801 (in conjunction with the ARPA Research in Information Science and Technology-High Performance Computing, Software Science and Technical program, ARPA Order no. 8930), and by ARPA research grant no. MDA972-92-J-1012. Robert Wisniewski was supported in part by an ARPA Fellowship in High Performance Computing administered by the Institute for Advanced Computer Studies, University of Maryland. Experimental results were obtained in part through use of resources at the Cornell Theory Center, which receives major funding from NSF and New York State; additional funding comes from ARPA, the NIH, IBM Corporation, and other members of the Center's Corporate Research Institute. The government has certain rights in this material. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. E. Anderson. </author> <title> The Performance of Spin Lock Alternatives for Shared-Memory Multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(1) </volume> <pages> 6-16, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: We describe our scheduler-conscious algorithms in section 3. Section 4 discusses performance results. Conclusions appear in section 5. 2 Related Work Mutual exclusion algorithms in which many processes spin on the same location can incur large amounts of contention on scalable multiprocessors, degrading parallel program performance. Several researchers <ref> [1, 7, 15] </ref> have observed that the key to good performance is to minimize active sharing by spinning on local locations. Similar approaches have been adopted for more complex synchronization primitives, including barriers [15] and reader-writer locks [11, 16].
Reference: [2] <author> T. E. Anderson, B. N. Bershad, E. D. Lazowska, and H. M. Levy. </author> <title> Scheduler Activations: Effective Kernel Support for the User-Level Management of Parallelism. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 10(1) </volume> <pages> 53-79, </pages> <month> February </month> <year> 1992. </year> <booktitle> Originally presented at the Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <month> October </month> <year> 1991. </year>
Reference-contexts: Performance degrades in the presence of multiprogramming under the following circumstances: * A process is preempted while holding a lock. This situation arises in both mutual exclusion and reader-writer locks when a process is preempted in its critical section. It has been addressed by several researchers <ref> [2, 4, 6, 14] </ref>. * A process is preempted while waiting for a lock and then is handed the lock while still preempted. <p> Several researchers have shown how to avoid preempting a process that holds a lock [6, 14], or to recover from such preemption if it occurs <ref> [2, 4] </ref>. Others have shown how to guess whether a lock is going to be held long enough that it makes sense to yield the processor, rather than busy-wait for access [8, 17, 21].
Reference: [3] <author> T. S. Axelrod. </author> <title> Effects of Synchronization Barriers on Multiprocessor Performance. </title> <journal> Parallel Computing, </journal> <volume> 3 </volume> <pages> 129-140, </pages> <year> 1986. </year>
Reference-contexts: It incorporates our counter-based barrier [9] into a two-level barrier scheme, and adjusts dynamically to the available number of processors in an application's partition. Two-level barriers employ a single counter on each processor, and a scalable barrier between processors. They were originally proposed by Axelrod <ref> [3] </ref> to minimize requirements for locks; Markatos et al. [13] first suggested their use to minimize overhead on multiprogrammed systems. The ability to perform well in the presence of multiprogramming is a combination of intelligent algorithms and extensions to the kernel interface.
Reference: [4] <author> D. L. Black. </author> <title> Scheduling Support for Concurrency and Parallelism in the Mach Operating System. </title> <journal> Computer, </journal> <volume> 23(5) </volume> <pages> 35-43, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Performance degrades in the presence of multiprogramming under the following circumstances: * A process is preempted while holding a lock. This situation arises in both mutual exclusion and reader-writer locks when a process is preempted in its critical section. It has been addressed by several researchers <ref> [2, 4, 6, 14] </ref>. * A process is preempted while waiting for a lock and then is handed the lock while still preempted. <p> Several researchers have shown how to avoid preempting a process that holds a lock [6, 14], or to recover from such preemption if it occurs <ref> [2, 4] </ref>. Others have shown how to guess whether a lock is going to be held long enough that it makes sense to yield the processor, rather than busy-wait for access [8, 17, 21].
Reference: [5] <author> Mark Crovella, Prakash Das, Cesary Dubnicki, Thomas LeBlanc, and Evangelos Markatos. </author> <title> Multiprogramming on Multiprocessors. </title> <booktitle> In Proceedings of the Third IEEE Symposium on Parallel and Distributed Processing, </booktitle> <pages> pages 590-597, </pages> <month> December </month> <year> 1991. </year> <month> 9 </month>
Reference-contexts: It 1 Our barrier algorithm assumes that processors are partitioned among applications (i.e. that each processor is dedicated to a particular application), as suggested by several recent studies <ref> [5, 12, 20, 18] </ref>. <p> Similar approaches have been adopted for more complex synchronization primitives, including barriers [15] and reader-writer locks [11, 16]. The efficiency of synchronization primitives depends in large part on the scheduling discipline used by the operating system. A growing body of evidence <ref> [5, 12, 18, 20] </ref> suggests that throughput is maximized by partitioning processors among applications. Unfortunately, if an application receives fewer processors than it has processes, the resulting multiprogramming can degrade performance by allowing processes to spin while their peers could be doing useful work.
Reference: [6] <author> J. Edler, J. Lipkis, and E. Schonberg. </author> <title> Process Management for Highly Parallel UNIX Systems. </title> <booktitle> In Proceedings of the USENIX Workshop on Unix and Supercomputers, </booktitle> <address> Pittsburgh, PA, </address> <month> September </month> <year> 1988. </year>
Reference-contexts: Performance degrades in the presence of multiprogramming under the following circumstances: * A process is preempted while holding a lock. This situation arises in both mutual exclusion and reader-writer locks when a process is preempted in its critical section. It has been addressed by several researchers <ref> [2, 4, 6, 14] </ref>. * A process is preempted while waiting for a lock and then is handed the lock while still preempted. <p> Unfortunately, if an application receives fewer processors than it has processes, the resulting multiprogramming can degrade performance by allowing processes to spin while their peers could be doing useful work. Several researchers have shown how to avoid preempting a process that holds a lock <ref> [6, 14] </ref>, or to recover from such preemption if it occurs [2, 4]. Others have shown how to guess whether a lock is going to be held long enough that it makes sense to yield the processor, rather than busy-wait for access [8, 17, 21]. <p> It returns a status code indicating whether the overwrite occurred. 3 for the application's partition. The kernel increments this count each time it changes the allocation of processes to processors. Extensions (1) and (2) are based in part on ideas introduced in Symunix <ref> [6] </ref>, and described in our work on queued locks [19]. Extension (3) is a generalization of the interface described in our work on small-scale scheduler-conscious barriers [9]. None of these extensions requires the kernel to maintain information that it does not already have available in its internal data structures.
Reference: [7] <author> G. Graunke and S. Thakkar. </author> <title> Synchronization Algorithms for Shared-Memory Multiprocessors. </title> <journal> Computer, </journal> <volume> 23(6) </volume> <pages> 60-69, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: We describe our scheduler-conscious algorithms in section 3. Section 4 discusses performance results. Conclusions appear in section 5. 2 Related Work Mutual exclusion algorithms in which many processes spin on the same location can incur large amounts of contention on scalable multiprocessors, degrading parallel program performance. Several researchers <ref> [1, 7, 15] </ref> have observed that the key to good performance is to minimize active sharing by spinning on local locations. Similar approaches have been adopted for more complex synchronization primitives, including barriers [15] and reader-writer locks [11, 16].
Reference: [8] <author> A. R. Karlin, K. Li, M. S. Manasse, and S. Owicki. </author> <title> Empirical Studies of Competitive Spinning for a Shared-Memory Multiprocessor. </title> <booktitle> In Proceedings of the Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 41-55, </pages> <address> Pacific Grove, CA, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: This situation arises with locks, but is satisfactorily addressed by techniques that choose dynamically between spinning and yielding, based on observed lengths of critical sections <ref> [8] </ref>. A more severe version of the problem occurs with barriers, where the decision between spinning and yielding needs to be based not on critical section lengths, but on whether there are preempted processes that have not yet reached the barrier [9]. <p> Others have shown how to guess whether a lock is going to be held long enough that it makes sense to yield the processor, rather than busy-wait for access <ref> [8, 17, 21] </ref>. In previous work we have shown how to make an optimal decision between spinning and yielding in a small-scale centralized barrier [9]. As noted in the introduction, scalable synchronization algorithms are particularly susceptible to scheduler-induced performance problems on multiprogrammed machines.
Reference: [9] <author> L. Kontothanassis and R. Wisniewski. </author> <title> Using Scheduler Information to Achieve Optimal Barrier Synchronization Performance. </title> <booktitle> In Proceedings of the Fourth ACM Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: A more severe version of the problem occurs with barriers, where the decision between spinning and yielding needs to be based not on critical section lengths, but on whether there are preempted processes that have not yet reached the barrier <ref> [9] </ref>. Scalable barriers exacerbate the problem by requiring portions of the barrier code in different processes to be interleaved in a deterministic order|an order that may conflict with the scheduling policy on a multiprogrammed processor, leading to an unreasonable number of context switches [13]. <p> Placing limits on when the kernel honors a request for disabling preemption allows the kernel to maintain control of the processor. In previous work we used the concept of kernel/application information sharing to design small- scale scheduler-conscious barriers <ref> [9] </ref> and scalable scheduler-conscious locks [19] for multiprogrammed machines. In this paper we extend this work to encompass three important busy-wait synchronization algorithms that have not previously been addressed. Specifically we provide algorithms for: 1. A mutual-exclusion ticket lock. <p> In previous work we have shown how to make an optimal decision between spinning and yielding in a small-scale centralized barrier <ref> [9] </ref>. As noted in the introduction, scalable synchronization algorithms are particularly susceptible to scheduler-induced performance problems on multiprogrammed machines. They may give locks to preempted processes, spin at a barrier that has not yet been reached by a preempted peer, or force the scheduler through unnecessary context switches. <p> The second is a scheduler-conscious fair reader-writer lock based on the scheduler-oblivious code of Krieger, Stumm, and Unrau [11]. The third is a scheduler-conscious tree barrier. It incorporates our counter-based barrier <ref> [9] </ref> into a two-level barrier scheme, and adjusts dynamically to the available number of processors in an application's partition. Two-level barriers employ a single counter on each processor, and a scalable barrier between processors. <p> Extensions (1) and (2) are based in part on ideas introduced in Symunix [6], and described in our work on queued locks [19]. Extension (3) is a generalization of the interface described in our work on small-scale scheduler-conscious barriers <ref> [9] </ref>. None of these extensions requires the kernel to maintain information that it does not already have available in its internal data structures. <p> Processes running on the same processor use the small-scale scheduler-conscious barrier described in previous work <ref> [9] </ref> to coordinate among themselves, spinning or yielding as appropriate. The last process arriving at the barrier on a given processor becomes the unique representative of that processor. In steady state (no re-partitioning), representative processes then participate in a scalable tree barrier [15].
Reference: [10] <author> Leonidas I. Kontothanassis, Robert W. Wisniewski, and Michael L. Scott. </author> <title> Scheduler Conscious Synchronization. </title> <type> TR 550, </type> <institution> Computer Science Department, University of Rochester, </institution> <month> December </month> <year> 1994. </year> <note> Submitted for publication. </note>
Reference-contexts: The rest of this section describes the scheduler-conscious synchronization algorithms in more detail. Pseudocode for these algorithms can be found in a technical report <ref> [10] </ref> (it is omitted here to save space). <p> A more thorough description of all the results can be found in a technical report <ref> [10] </ref>.
Reference: [11] <author> O. Krieger, M. Stumm, and R. Unrau. </author> <title> A Fair Fast Scalable Reader-Writer Lock. </title> <booktitle> In Proceedings of the 1993 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: Several researchers [1, 7, 15] have observed that the key to good performance is to minimize active sharing by spinning on local locations. Similar approaches have been adopted for more complex synchronization primitives, including barriers [15] and reader-writer locks <ref> [11, 16] </ref>. The efficiency of synchronization primitives depends in large part on the scheduling discipline used by the operating system. A growing body of evidence [5, 12, 18, 20] suggests that throughput is maximized by partitioning processors among applications. <p> The first is a mutual-exclusion ticket lock that uses handshaking to detect preempted waiting processes and avoid giving them the lock. The second is a scheduler-conscious fair reader-writer lock based on the scheduler-oblivious code of Krieger, Stumm, and Unrau <ref> [11] </ref>. The third is a scheduler-conscious tree barrier. It incorporates our counter-based barrier [9] into a two-level barrier scheme, and adjusts dynamically to the available number of processors in an application's partition. Two-level barriers employ a single counter on each processor, and a scalable barrier between processors.
Reference: [12] <author> S. T. Leutenegger and M. K. Vernon. </author> <title> Performance of Multiprogrammed Multiprocessor Scheduling Algorithms. </title> <booktitle> In Proceedings of the 1990 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems, </booktitle> <address> Boulder, CO, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: It 1 Our barrier algorithm assumes that processors are partitioned among applications (i.e. that each processor is dedicated to a particular application), as suggested by several recent studies <ref> [5, 12, 20, 18] </ref>. <p> Similar approaches have been adopted for more complex synchronization primitives, including barriers [15] and reader-writer locks [11, 16]. The efficiency of synchronization primitives depends in large part on the scheduling discipline used by the operating system. A growing body of evidence <ref> [5, 12, 18, 20] </ref> suggests that throughput is maximized by partitioning processors among applications. Unfortunately, if an application receives fewer processors than it has processes, the resulting multiprogramming can degrade performance by allowing processes to spin while their peers could be doing useful work.
Reference: [13] <author> Evangelos Markatos, Mark Crovella, Prakash Das, Cesary Dubnicki, and Thomas LeBlanc. </author> <title> The Effects of Multiprogramming on Barrier Synchronization. </title> <booktitle> In Proceedings of the Third IEEE Symposium on Parallel and Distributed Processing, </booktitle> <pages> pages 662-669, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: Scalable barriers exacerbate the problem by requiring portions of the barrier code in different processes to be interleaved in a deterministic order|an order that may conflict with the scheduling policy on a multiprogrammed processor, leading to an unreasonable number of context switches <ref> [13] </ref>. We have developed a simple set of mechanisms to handle synchronization difficulties arising from multiprogramming. The key idea is to share information across the application-kernel interface in order to eliminate the sources of overhead mentioned above. <p> Two-level barriers employ a single counter on each processor, and a scalable barrier between processors. They were originally proposed by Axelrod [3] to minimize requirements for locks; Markatos et al. <ref> [13] </ref> first suggested their use to minimize overhead on multiprogrammed systems. The ability to perform well in the presence of multiprogramming is a combination of intelligent algorithms and extensions to the kernel interface. We have extended the kernel interface in three ways: 1.
Reference: [14] <author> Brian D. Marsh, Michael L. Scott, Thomas J. LeBlanc, and Evangelos P. Markatos. </author> <title> First-Class UserLevel Threads. </title> <booktitle> In Proceedings of the Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 110-121, </pages> <address> Pacific Grove, CA, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: Performance degrades in the presence of multiprogramming under the following circumstances: * A process is preempted while holding a lock. This situation arises in both mutual exclusion and reader-writer locks when a process is preempted in its critical section. It has been addressed by several researchers <ref> [2, 4, 6, 14] </ref>. * A process is preempted while waiting for a lock and then is handed the lock while still preempted. <p> Unfortunately, if an application receives fewer processors than it has processes, the resulting multiprogramming can degrade performance by allowing processes to spin while their peers could be doing useful work. Several researchers have shown how to avoid preempting a process that holds a lock <ref> [6, 14] </ref>, or to recover from such preemption if it occurs [2, 4]. Others have shown how to guess whether a lock is going to be held long enough that it makes sense to yield the processor, rather than busy-wait for access [8, 17, 21].
Reference: [15] <author> J. M. Mellor-Crummey and M. L. Scott. </author> <title> Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(1) </volume> <pages> 21-65, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: We describe our scheduler-conscious algorithms in section 3. Section 4 discusses performance results. Conclusions appear in section 5. 2 Related Work Mutual exclusion algorithms in which many processes spin on the same location can incur large amounts of contention on scalable multiprocessors, degrading parallel program performance. Several researchers <ref> [1, 7, 15] </ref> have observed that the key to good performance is to minimize active sharing by spinning on local locations. Similar approaches have been adopted for more complex synchronization primitives, including barriers [15] and reader-writer locks [11, 16]. <p> Several researchers [1, 7, 15] have observed that the key to good performance is to minimize active sharing by spinning on local locations. Similar approaches have been adopted for more complex synchronization primitives, including barriers <ref> [15] </ref> and reader-writer locks [11, 16]. The efficiency of synchronization primitives depends in large part on the scheduling discipline used by the operating system. A growing body of evidence [5, 12, 18, 20] suggests that throughput is maximized by partitioning processors among applications. <p> The last process arriving at the barrier on a given processor becomes the unique representative of that processor. In steady state (no re-partitioning), representative processes then participate in a scalable tree barrier <ref> [15] </ref>. In response to re-partioning in a dynamic multiprogrammed environment, the barrier goes through a data structure reorganization phase that allows it to maintain a single representative process per processor. This is accomplished using the partition generation counter provided by the kernel. <p> The first of these outperforms the others in the absence of multiprogramming <ref> [15] </ref>, but performs terribly with multiprogramming, because a process can spin away the bulk of a scheduling quantum waiting for a peer to reach the barrier, when that peer has been preempted.
Reference: [16] <author> J. M. Mellor-Crummey and M. L. Scott. </author> <title> Scalable Reader-Writer Synchronization for Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the Third ACM Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 106-113, </pages> <address> Williamsburg, VA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Several researchers [1, 7, 15] have observed that the key to good performance is to minimize active sharing by spinning on local locations. Similar approaches have been adopted for more complex synchronization primitives, including barriers [15] and reader-writer locks <ref> [11, 16] </ref>. The efficiency of synchronization primitives depends in large part on the scheduling discipline used by the operating system. A growing body of evidence [5, 12, 18, 20] suggests that throughput is maximized by partitioning processors among applications.
Reference: [17] <author> B. Mukherjee and K. Schwan. </author> <title> Improving Performance by Use of Adaptive Objects: Experimentation with a Configurable Multiprocessor Thread Package. </title> <booktitle> In Proceedings of the Second International Symposium on High Performance Distributed Computing, </booktitle> <address> Spokane, WA, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: Others have shown how to guess whether a lock is going to be held long enough that it makes sense to yield the processor, rather than busy-wait for access <ref> [8, 17, 21] </ref>. In previous work we have shown how to make an optimal decision between spinning and yielding in a small-scale centralized barrier [9]. As noted in the introduction, scalable synchronization algorithms are particularly susceptible to scheduler-induced performance problems on multiprogrammed machines.
Reference: [18] <author> A. Tucker and A. Gupta. </author> <title> Process Control and Scheduling Issues for Multiprogrammed Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the Twelfth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 159-166, </pages> <address> Litchfield Park, AZ, </address> <month> December </month> <year> 1989. </year>
Reference-contexts: It 1 Our barrier algorithm assumes that processors are partitioned among applications (i.e. that each processor is dedicated to a particular application), as suggested by several recent studies <ref> [5, 12, 20, 18] </ref>. <p> Similar approaches have been adopted for more complex synchronization primitives, including barriers [15] and reader-writer locks [11, 16]. The efficiency of synchronization primitives depends in large part on the scheduling discipline used by the operating system. A growing body of evidence <ref> [5, 12, 18, 20] </ref> suggests that throughput is maximized by partitioning processors among applications. Unfortunately, if an application receives fewer processors than it has processes, the resulting multiprogramming can degrade performance by allowing processes to spin while their peers could be doing useful work.
Reference: [19] <author> Robert W. Wisniewski, Leonidas Kontothanassis, and Michael L. Scott. </author> <title> Scalable Spin Locks for Multiprogrammed Systems. </title> <booktitle> In Proceedings of the Eighth International Parallel Processing Symposium, </booktitle> <pages> pages 583-589, </pages> <address> Cancun, Mexico, </address> <month> April </month> <year> 1994. </year> <note> Earlier but expanded version available as TR 454, </note> <institution> Computer Science Department, University of Rochester, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: This situation can arise in locks that enforce a predetermined ordering, either for the sake of fairness or to minimize contention on large-scale machines <ref> [19] </ref>. * A process spins waiting for its peers when some of them are preempted. This situation arises with locks, but is satisfactorily addressed by techniques that choose dynamically between spinning and yielding, based on observed lengths of critical sections [8]. <p> Placing limits on when the kernel honors a request for disabling preemption allows the kernel to maintain control of the processor. In previous work we used the concept of kernel/application information sharing to design small- scale scheduler-conscious barriers [9] and scalable scheduler-conscious locks <ref> [19] </ref> for multiprogrammed machines. In this paper we extend this work to encompass three important busy-wait synchronization algorithms that have not previously been addressed. Specifically we provide algorithms for: 1. A mutual-exclusion ticket lock. <p> The kernel increments this count each time it changes the allocation of processes to processors. Extensions (1) and (2) are based in part on ideas introduced in Symunix [6], and described in our work on queued locks <ref> [19] </ref>. Extension (3) is a generalization of the interface described in our work on small-scale scheduler-conscious barriers [9]. None of these extensions requires the kernel to maintain information that it does not already have available in its internal data structures. <p> Scheduling quantum was set to 50 ms on the KSR 1. processor KSR 1 as the multiprogramming level increases. Figure 2 presents the analogous results for varying numbers of processors. The other locks include a plain (scheduler-oblivious) ticket lock (with proportional backoff), our scheduler-conscious "Smart-Q" lock <ref> [19] </ref>, and both scheduler- conscious and scheduler-oblivious versions of a test and set lock with exponential backoff and the native (hardware-implemented) spinlock. The scheduler-conscious ticket lock provides performance improvements of more than an order of magnitude over the scheduler-oblivious version.
Reference: [20] <author> J. Zahorjan and C. McCann. </author> <title> Processor Scheduling in Shared Memory Multiprocessors. </title> <booktitle> In Proceedings of the 1990 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 214-225, </pages> <address> Boulder, CO, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: It 1 Our barrier algorithm assumes that processors are partitioned among applications (i.e. that each processor is dedicated to a particular application), as suggested by several recent studies <ref> [5, 12, 20, 18] </ref>. <p> Similar approaches have been adopted for more complex synchronization primitives, including barriers [15] and reader-writer locks [11, 16]. The efficiency of synchronization primitives depends in large part on the scheduling discipline used by the operating system. A growing body of evidence <ref> [5, 12, 18, 20] </ref> suggests that throughput is maximized by partitioning processors among applications. Unfortunately, if an application receives fewer processors than it has processes, the resulting multiprogramming can degrade performance by allowing processes to spin while their peers could be doing useful work.
Reference: [21] <author> J. Zahorjan, E. D. Lazowska, and D. L. Eager. </author> <title> The Effect of Scheduling Discipline on Spin Overhead in Shared Memory Parallel Systems. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(2):180198, </volume> <month> April </month> <year> 1991. </year> <month> 10 </month>
Reference-contexts: Others have shown how to guess whether a lock is going to be held long enough that it makes sense to yield the processor, rather than busy-wait for access <ref> [8, 17, 21] </ref>. In previous work we have shown how to make an optimal decision between spinning and yielding in a small-scale centralized barrier [9]. As noted in the introduction, scalable synchronization algorithms are particularly susceptible to scheduler-induced performance problems on multiprogrammed machines.
References-found: 21

