URL: http://www.cs.nyu.edu/phd_students/lindaoi/Thesis/Thesis.ps.gz
Refering-URL: http://www.cs.nyu.edu/phd_students/lindaoi/index.html
Root-URL: http://www.cs.nyu.edu
Title: Fast Algorithms for Discovering the Maximum Frequent Set  
Author: Zvi M. Kedem 
Degree: by A dissertation submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy  Approved:  
Date: September 1998  
Address: New York University  
Affiliation: Department of Computer Science  
Abstract-found: 0
Intro-found: 1
Reference: [AABMSS96] <author> R. Agrawal, A. Arning, T. Bollinger, M. Mehta, J. Shafer, R. Srikant. </author> <title> The Quest Data Mining System. </title> <booktitle> In Proc. 2nd International Conference on Knowledge Discovery in Databases and Data Mining (KDD), </booktitle> <month> Aug. </month> <year> 1996. </year>
Reference-contexts: Therefore, our algorithm does have the potential to benefit from this nonmonotonicity. 57 4.2 Experiments The test databases are generated synthetically by an algorithm designed by the IBM Quest project <ref> [AABMSS96] </ref>. The synthetic data generation procedure is described in detail in [AS94], whose parameter settings we follow. The number of items N is set to 1000. jDj is the number of transactions. jT j is the average size of transactions. jIj is the average size of maximal frequent itemsets.
Reference: [AIS93a] <author> R. Agrawal, T. Imielinski, and A. Swami. </author> <title> Mining association rules between sets of items in large databases. </title> <booktitle> In Proc. SIGMOD, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: Typical algorithms for finding the frequent set, i.e., the set of all frequent item-sets, operate in a bottom-up breadth-first fashion (e.g., <ref> [AIS93a] </ref>) [AS94] [AS96] [BMUT97] [HF95] [MT97] [MTV94] [ORS98] [PCY95] [SON95]). The computation starts from frequent 1-itemsets (minimal length frequent itemsets at the bottom) and then extends one level up in every pass until all maximal (length) frequent itemsets are discovered. <p> Chapter 5 concludes this thesis. 5 Chapter 2 Association Rule Mining 2.1 The Setting of the Problem This section briefly introduces the association rule mining problem. To the extent feasible, we follow the terminology of <ref> [AIS93a] </ref>. Let I = fi 1 ; i 2 ; : : : ; i m g be a set of m distinct items. The itemsets could represent different items in a supermarket or different alarm signals in telecommunication networks [HKMRT96]. <p> This is the technique used in [ZPOL97] and [MT96a]. 2.4 Typical Algorithms for Frequent Set Discovery We briefly discuss existing frequent set discovery algorithms in a roughly chronological order. 2.4.1 AIS and SETM Algorithms The problem of association rule mining was first introduced in <ref> [AIS93a] </ref>. An algorithm called AIS was given for discovering the frequent set. To find the frequent itemsets, AIS creates candidates while reading the database. During each pass, the entire database is read.
Reference: [AIS93b] <author> R. Agrawal, T. Imielinski, and A. Swami. </author> <title> Database mining: A performance perspective. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> Vol. 5, No. 6, </volume> <month> Dec. </month> <year> 1993. </year>
Reference: [AS94] <author> R. Agrawal and R. Srikant. </author> <title> Fast algorithms for mining association rules in large databases. </title> <booktitle> In Proc. 20th VLDB, </booktitle> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: Depending on the semantics attached to the input database, the frequent itemsets, and the term "occurs," we get the key components of different data mining problems such as the discovery of association rules (e.g., <ref> [AS94] </ref> [HF95] [MTV94]), theories (e.g., [GMS97]), strong rule (e.g., [P91]), episodes (e.g., [MT95] [MT96c]), and minimal keys (e.g., [GMS97]). Typical algorithms for finding the frequent set, i.e., the set of all frequent item-sets, operate in a bottom-up breadth-first fashion (e.g., [AIS93a]) [AS94] [AS96] [BMUT97] [HF95] [MT97] [MTV94] [ORS98] [PCY95] [SON95]). <p> problems such as the discovery of association rules (e.g., <ref> [AS94] </ref> [HF95] [MTV94]), theories (e.g., [GMS97]), strong rule (e.g., [P91]), episodes (e.g., [MT95] [MT96c]), and minimal keys (e.g., [GMS97]). Typical algorithms for finding the frequent set, i.e., the set of all frequent item-sets, operate in a bottom-up breadth-first fashion (e.g., [AIS93a]) [AS94] [AS96] [BMUT97] [HF95] [MT97] [MTV94] [ORS98] [PCY95] [SON95]). The computation starts from frequent 1-itemsets (minimal length frequent itemsets at the bottom) and then extends one level up in every pass until all maximal (length) frequent itemsets are discovered. All frequent itemsets are explicitly examined and discovered by these algorithms. <p> We present a novel Pincer-Search algorithm, which searches for the MFS from both bottom-up and top-down directions. It performs well even when the maximal frequent itemsets are long. The bottom-up search is similar to Apriori <ref> [AS94] </ref> and OCD [MTV94] algorithms. However, the top-down search is novel. It is implemented efficiently by introducing an auxiliary data structure, the maximum frequent candidate set (or MFCS), as explained later. <p> In fact, the MFCS concept can be applied in solving other data mining problems as long as the problem has the closure properties to be discussed later. The monotone specialization relation discussed in [MT97] and [M82] was addressing the same properties. Popular benchmark databases designed by Agrawal and Srikant <ref> [AS94] </ref> have been used in [AS96], [ORS98], [PCY95], [SON95], and [ZPOL97]. We use these same benchmarks to evaluate the performance of our algorithm. <p> The goal of association rule mining is to discover all rules that have support and confidence greater than some user-defined minimum support and minimum confidence thresholds, respectively. 8 The normally followed scheme for mining association rules consists of two stages <ref> [AS94] </ref>: 1. the discovery of frequent itemsets, followed by 2. the generation of association rules. Example Consider the database as in Fig. 2.1. Suppose the minimum support is set to 75% and the minimum confidence is set to 100%. <p> As shown in Fig. 2.3, every infrequent itemset (itemset f5g and its supersets) needs to be visited before the maximal frequent itemsets are obtained. 15 Note that, in a "pure" bottom-up approach, only Property 1 above is used to prune candidates. This is the technique that many algorithms (e.g., <ref> [AS94] </ref> [AS96] [BMUT97] [HF95] [MT97] [MTV94] [ORS98] [PCY95] [SON95]) use to decrease the number of candidates. In a "pure" top-down approach, only Property 2 is used to prune candidates. <p> However, like AIS, SETM also creates candidates on-the-fly while reading the database. Both algorithms are not efficient, since they generate and count too many unnecessary candidates. 2.4.2 Apriori and OCD Algorithms The Apriori algorithm <ref> [AS94] </ref> is a typical bottom-up approach algorithm, which perform much better than AIS and SETM. It repeatedly uses Apriori-gen algorithm to generate candidates and then count their supports by reading the entire database once. The algorithm is described in Fig. 2.4. Apriori-gen is a candidate generation algorithm. <p> This tree can be viewed as a special case of the hash-tree <ref> [AS94] </ref> in the Apriori algorithm with the leaf nodes containing only one itemset. Unlike their hash-tree which stores only the itemsets with same length, this complete hash-tree store all the subsets of the maximal frequent itemsets (of different length). <p> However, according to both <ref> [AS94] </ref> and our experiments, a large fraction of the 2-itemsets will usually be infrequent. These infrequent itemsets will cause the MFCS to go down the levels very fast, allowing it to reach some maximal frequent itemsets after only a few passes. <p> the characteristics of the new algorithm's performance. 4.1 Preliminary Discussion 4.1.1 Auxiliary Data Structures Used Since we are interested in studying the effect of using the MFCS to reduce the number of candidates and the number of passes, we didn't use more efficient data structures, such as hash tables (e.g., <ref> [AS94] </ref> [PCY95]), to store the itemsets. We simply used a link-list data structure to store the frequent set and the candidate set in each pass. The databases used in performance evaluation, are the synthetic databases used in [AS94], the census databases similar to [BMUT97], and the stock transaction databases from New <p> passes, we didn't use more efficient data structures, such as hash tables (e.g., <ref> [AS94] </ref> [PCY95]), to store the itemsets. We simply used a link-list data structure to store the frequent set and the candidate set in each pass. The databases used in performance evaluation, are the synthetic databases used in [AS94], the census databases similar to [BMUT97], and the stock transaction databases from New York Stock Exchange, Inc. [NYSE97]. Also, as done in [ORS98] and [SA95a], we used a one-dimensional array and a two-dimensional array to speed up the process of the first and the second pass correspondingly. <p> In concentrated distribution, the frequent itemsets, having the same length, contain many common items: the frequent items tend to cluster. If the frequent itemsets do not have many common elements, the distribution is scattered. By using the synthetic data generation program as in <ref> [AS94] </ref>, we can generate databases with different distributions by adjusting various parameters. We will present experiments to examine the impact of the distribution type on the performance of the two algorithms. <p> We will present experiments to examine the impact of the distribution type on the performance of the two algorithms. In the first set of experiments, the number of the maximal frequent itemsets jLj is set to 2000, as in <ref> [AS94] </ref>. The frequent itemsets found in this set of experiments are rather scattered. To produce databases having a concentrated distribution 56 of the frequent itemsets, we decrease the parameter jLj to 50 in the second set of experiments. <p> Therefore, our algorithm does have the potential to benefit from this nonmonotonicity. 57 4.2 Experiments The test databases are generated synthetically by an algorithm designed by the IBM Quest project [AABMSS96]. The synthetic data generation procedure is described in detail in <ref> [AS94] </ref>, whose parameter settings we follow. The number of items N is set to 1000. jDj is the number of transactions. jT j is the average size of transactions. jIj is the average size of maximal frequent itemsets.
Reference: [AS96] <author> R. Agrawal and J. Shafer. </author> <title> Parallel mining of association rules. </title> <journal> IEEE Trans. on Knowledge and Data Engineering, </journal> <month> Jan. </month> <year> 1996. </year> <month> 89 </month>
Reference-contexts: Typical algorithms for finding the frequent set, i.e., the set of all frequent item-sets, operate in a bottom-up breadth-first fashion (e.g., [AIS93a]) [AS94] <ref> [AS96] </ref> [BMUT97] [HF95] [MT97] [MTV94] [ORS98] [PCY95] [SON95]). The computation starts from frequent 1-itemsets (minimal length frequent itemsets at the bottom) and then extends one level up in every pass until all maximal (length) frequent itemsets are discovered. All frequent itemsets are explicitly examined and discovered by these algorithms. <p> The monotone specialization relation discussed in [MT97] and [M82] was addressing the same properties. Popular benchmark databases designed by Agrawal and Srikant [AS94] have been used in <ref> [AS96] </ref>, [ORS98], [PCY95], [SON95], and [ZPOL97]. We use these same benchmarks to evaluate the performance of our algorithm. In most cases, our algorithm not only reduces the number of passes of reading the database but also reduces the number of candidates (for whom support is counted). <p> This is the technique that many algorithms (e.g., [AS94] <ref> [AS96] </ref> [BMUT97] [HF95] [MT97] [MTV94] [ORS98] [PCY95] [SON95]) use to decrease the number of candidates. In a "pure" top-down approach, only Property 2 is used to prune candidates. <p> By using this hash filter, some candidates can be pruned before reading the database in the next pass. However, according to the investigations done in <ref> [AS96] </ref>, this optimization may not be as good as using a two-dimensional array as discussed in [SA95a]. Furthermore, like Apriori, DHP considers every frequent itemset. Savasere et al. [SON95] proposed a Partition algorithm. This paper addresses two issues in previous algorithms.
Reference: [B97] <author> R. J. Bayardo. </author> <title> Brute-force mining of high-confidence classification rules. </title> <booktitle> In Proc. 3rd International Conference on Knowledge Discovery and Data Mining (KDD), </booktitle> <month> Aug. </month> <year> 1997. </year>
Reference-contexts: The maximal frequent itemsets in many instances of such applications are likely to be long. Therefore we expect the algorithm to provide dramatic performance improvements. Many classification problems as discussed in <ref> [B97] </ref> tend to have long patterns. We will study the performance of the Pincer-Search algorithm on these problems. In general, if some maximal frequent itemsets are long and the maximal frequent itemsets are distributed scatteredly, then the problem of discovering the MFS can be very hard.
Reference: [BMS97] <author> S. Brin, R. Motwani, and C. Silverstein. </author> <title> Beyond market baskets: generalizing association rules to correlations. </title> <booktitle> In Proc. SIGMOD, </booktitle> <month> May </month> <year> 1997. </year>
Reference-contexts: In addition to the algorithms discussed so far, there has been extensive research relating to the problem of association rule mining such as <ref> [BMS97] </ref>, [GKMT97], [HCC92], [HF95], [MT96b], [ORS98], [S96], [SA95b], [SA96b], [SVA97], [T96a], and [KMRTB94]. Similar candidate pruning techniques has been applied to discover sequential patterns (e.g., [MT95] [MT96c] [SA95a] [SA96a] [Z97]) and episodes (e.g., 30 [MT95] [MT96c]).
Reference: [BMUT97] <author> S. Brin, R. Motwani, J. Ullman, and S. Tsur. </author> <title> Dynamic itemset counting and implication rules for market basket data. </title> <booktitle> In Proc. SIGMOD, </booktitle> <month> May </month> <year> 1997. </year>
Reference-contexts: Typical algorithms for finding the frequent set, i.e., the set of all frequent item-sets, operate in a bottom-up breadth-first fashion (e.g., [AIS93a]) [AS94] [AS96] <ref> [BMUT97] </ref> [HF95] [MT97] [MTV94] [ORS98] [PCY95] [SON95]). The computation starts from frequent 1-itemsets (minimal length frequent itemsets at the bottom) and then extends one level up in every pass until all maximal (length) frequent itemsets are discovered. All frequent itemsets are explicitly examined and discovered by these algorithms. <p> In data mining applications where items are correlated, maximum frequent itemsets could be long <ref> [BMUT97] </ref>. 2 Therefore, instead of examining and "assembling" all the frequent itemsets, an alternative approach might be to "shortcut" the process and attempt to search for maximal frequent itemsets "more directly," as they immediately specify all frequent itemsets. <p> This is the technique that many algorithms (e.g., [AS94] [AS96] <ref> [BMUT97] </ref> [HF95] [MT97] [MTV94] [ORS98] [PCY95] [SON95]) use to decrease the number of candidates. In a "pure" top-down approach, only Property 2 is used to prune candidates. <p> The Apriori-gen algorithm has been very successful in reducing the number of candidates and has been used in many subsequent algorithms, such as Partition 20 [SON95], DHP [PCY95], Sampling [T96b], DIC <ref> [BMUT97] </ref>, and Clique [ZPOL97]. 2.4.3 DHP and Partition Algorithm DHP [PCY95] tried to improve Apriori by using a hash filter to count the upper-bound of the support for the next pass. The upper-bound can be used to eliminate candidates. <p> When the database cannot fit in the main memory, it is expensive to determine whether the extended itemset is frequent or not, since extending the length by one require reading the database once. Brin et al. <ref> [BMUT97] </ref> proposed a dynamic itemset counting (DIC) algorithm which combines candidates with different lengths into one pass. This algorithm focuses on reducing the number of passes of reading the database. <p> We simply used a link-list data structure to store the frequent set and the candidate set in each pass. The databases used in performance evaluation, are the synthetic databases used in [AS94], the census databases similar to <ref> [BMUT97] </ref>, and the stock transaction databases from New York Stock Exchange, Inc. [NYSE97]. Also, as done in [ORS98] and [SA95a], we used a one-dimensional array and a two-dimensional array to speed up the process of the first and the second pass correspondingly. <p> This file contains actual census entries which constitute a five percent sample of a state that the file represents. This database is similar to the database looked at by Brin et al. <ref> [BMUT97] </ref>. As discussed in that paper, this PUMS database is very hard. That is because there are many more items than in the previous synthetic supermarket databases: there are about 7000 items in this PUMS database, in comparison with 1000 items in the synthetic databases.
Reference: [FPS96] <author> U. Fayyad, G. Piatetsky-Shapiro, and P. Smyth. </author> <title> Knowledge discovery and data mining: toward a unifying framework. </title> <booktitle> In Proc. 2nd International Conference on Knowledge Discovery and Data Mining (KDD), </booktitle> <month> Aug. </month> <year> 1996. </year>
Reference-contexts: It updates the MFCS only when a new infrequent itemset is discovered. In fact, MaxClique can be viewed as a special case of our Pincer-Search algorithm, if we discard the inplementation details. 2.5 Other Related Work General survey papers regarding data mining problems can be found in, e.g., <ref> [FPS96] </ref> [FPSU96] [M97] [PBKKS97]. In addition to the algorithms discussed so far, there has been extensive research relating to the problem of association rule mining such as [BMS97], [GKMT97], [HCC92], [HF95], [MT96b], [ORS98], [S96], [SA95b], [SA96b], [SVA97], [T96a], and [KMRTB94].
Reference: [FPSU96] <editor> U. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthrusamy (Eds.). </editor> <booktitle> Advances in Knowledge Discovery and Data Mining. </booktitle> <publisher> AAAI Press, </publisher> <address> Menlo Park, CA, </address> <year> 1996. </year>
Reference-contexts: Introduction Knowledge discovery in databases (KDD) has received increasing attention and has been recognized as a promising new field of database research. It is defined by Fayyad et al. <ref> [FPSU96] </ref> as "the non-trivial process of identifying valid, novel, potentially useful, and ultimately understandable patterns in data". <p> The key step in the knowledge discovery process is the data mining step, which is "consisting of applying data analysis and discovery algorithms that, under acceptable computational efficiency limitations, produce a particular enumeration of patterns over the data" <ref> [FPSU96] </ref>. This thesis addresses an important pattern discovery algorithm, which can be applied to many data mining applications. A key component of many data mining problems is formulated as follows. <p> It updates the MFCS only when a new infrequent itemset is discovered. In fact, MaxClique can be viewed as a special case of our Pincer-Search algorithm, if we discard the inplementation details. 2.5 Other Related Work General survey papers regarding data mining problems can be found in, e.g., [FPS96] <ref> [FPSU96] </ref> [M97] [PBKKS97]. In addition to the algorithms discussed so far, there has been extensive research relating to the problem of association rule mining such as [BMS97], [GKMT97], [HCC92], [HF95], [MT96b], [ORS98], [S96], [SA95b], [SA96b], [SVA97], [T96a], and [KMRTB94].
Reference: [GKMT97] <author> D. Gunopulos, R. Khardon, H. Mannila, and H. Toivonen. </author> <title> Data mining, hypergraph traversal, </title> <booktitle> and machine leaning. In Proc. PODS, </booktitle> <year> 1997. </year>
Reference-contexts: In addition to the algorithms discussed so far, there has been extensive research relating to the problem of association rule mining such as [BMS97], <ref> [GKMT97] </ref>, [HCC92], [HF95], [MT96b], [ORS98], [S96], [SA95b], [SA96b], [SVA97], [T96a], and [KMRTB94]. Similar candidate pruning techniques has been applied to discover sequential patterns (e.g., [MT95] [MT96c] [SA95a] [SA96a] [Z97]) and episodes (e.g., 30 [MT95] [MT96c]). <p> The discovery of frequent set is a key process in solving these problems. A good algorithm for discovering frequent set could be applied in solving these problems. Mannila and Toivonen [MT97] <ref> [GKMT97] </ref> analyze the complexity of the (level-wise) bottom-up breadth-first search style algorithms. As our algorithm does not fit in this model, their complexity low bound does not apply to it. Our work was inspired by the notion of version space in Mitchell's machine learning paper [M82].
Reference: [GMS97] <author> D. Gunopulos, H. Mannila, and S. Saluja. </author> <title> Discovering all most specific sentences by randomized algorithm. </title> <booktitle> In Proc. International Conference of Database Theory (ICDT), </booktitle> <month> Jan. </month> <year> 1997. </year> <month> 90 </month>
Reference-contexts: Depending on the semantics attached to the input database, the frequent itemsets, and the term "occurs," we get the key components of different data mining problems such as the discovery of association rules (e.g., [AS94] [HF95] [MTV94]), theories (e.g., <ref> [GMS97] </ref>), strong rule (e.g., [P91]), episodes (e.g., [MT95] [MT96c]), and minimal keys (e.g., [GMS97]). Typical algorithms for finding the frequent set, i.e., the set of all frequent item-sets, operate in a bottom-up breadth-first fashion (e.g., [AIS93a]) [AS94] [AS96] [BMUT97] [HF95] [MT97] [MTV94] [ORS98] [PCY95] [SON95]). <p> the semantics attached to the input database, the frequent itemsets, and the term "occurs," we get the key components of different data mining problems such as the discovery of association rules (e.g., [AS94] [HF95] [MTV94]), theories (e.g., <ref> [GMS97] </ref>), strong rule (e.g., [P91]), episodes (e.g., [MT95] [MT96c]), and minimal keys (e.g., [GMS97]). Typical algorithms for finding the frequent set, i.e., the set of all frequent item-sets, operate in a bottom-up breadth-first fashion (e.g., [AIS93a]) [AS94] [AS96] [BMUT97] [HF95] [MT97] [MTV94] [ORS98] [PCY95] [SON95]). <p> Therefore, it is still inefficient when the frequent itemsets are long. 25 2.4.5 A-Random-MFS, DIC, Eclat, MaxEclat, TopDown, Clique, and MaxClique Algorithms A randomized algorithm called A-Random-MFS for discovering the maximum frequent set was presented by Gunopulos et al. <ref> [GMS97] </ref>. The randomized algorithm alone cannot guarantee completeness. Therefore, a complete algorithm that can discover all maximal frequent itemsets requires repeatedly calling the randomized algorithm until no new maximal frequent itemset can be found.
Reference: [HCC92] <author> J. Han, Y. Cai, and N. Cercone. </author> <title> Knowledge discovery in database: an attribute-oriented approach. </title> <booktitle> In Proc. 18th VLDB. </booktitle> <month> Sept. </month> <year> 1992. </year>
Reference-contexts: In addition to the algorithms discussed so far, there has been extensive research relating to the problem of association rule mining such as [BMS97], [GKMT97], <ref> [HCC92] </ref>, [HF95], [MT96b], [ORS98], [S96], [SA95b], [SA96b], [SVA97], [T96a], and [KMRTB94]. Similar candidate pruning techniques has been applied to discover sequential patterns (e.g., [MT95] [MT96c] [SA95a] [SA96a] [Z97]) and episodes (e.g., 30 [MT95] [MT96c]).
Reference: [HF95] <author> J. Han and Y. Fu. </author> <title> Discovery of multiple-level association rules from large databases. </title> <booktitle> In Proc. 21st VLDB, </booktitle> <month> Sept. </month> <year> 1995. </year>
Reference-contexts: Depending on the semantics attached to the input database, the frequent itemsets, and the term "occurs," we get the key components of different data mining problems such as the discovery of association rules (e.g., [AS94] <ref> [HF95] </ref> [MTV94]), theories (e.g., [GMS97]), strong rule (e.g., [P91]), episodes (e.g., [MT95] [MT96c]), and minimal keys (e.g., [GMS97]). Typical algorithms for finding the frequent set, i.e., the set of all frequent item-sets, operate in a bottom-up breadth-first fashion (e.g., [AIS93a]) [AS94] [AS96] [BMUT97] [HF95] [MT97] [MTV94] [ORS98] [PCY95] [SON95]). <p> the discovery of association rules (e.g., [AS94] <ref> [HF95] </ref> [MTV94]), theories (e.g., [GMS97]), strong rule (e.g., [P91]), episodes (e.g., [MT95] [MT96c]), and minimal keys (e.g., [GMS97]). Typical algorithms for finding the frequent set, i.e., the set of all frequent item-sets, operate in a bottom-up breadth-first fashion (e.g., [AIS93a]) [AS94] [AS96] [BMUT97] [HF95] [MT97] [MTV94] [ORS98] [PCY95] [SON95]). The computation starts from frequent 1-itemsets (minimal length frequent itemsets at the bottom) and then extends one level up in every pass until all maximal (length) frequent itemsets are discovered. All frequent itemsets are explicitly examined and discovered by these algorithms. <p> This is the technique that many algorithms (e.g., [AS94] [AS96] [BMUT97] <ref> [HF95] </ref> [MT97] [MTV94] [ORS98] [PCY95] [SON95]) use to decrease the number of candidates. In a "pure" top-down approach, only Property 2 is used to prune candidates. <p> In addition to the algorithms discussed so far, there has been extensive research relating to the problem of association rule mining such as [BMS97], [GKMT97], [HCC92], <ref> [HF95] </ref>, [MT96b], [ORS98], [S96], [SA95b], [SA96b], [SVA97], [T96a], and [KMRTB94]. Similar candidate pruning techniques has been applied to discover sequential patterns (e.g., [MT95] [MT96c] [SA95a] [SA96a] [Z97]) and episodes (e.g., 30 [MT95] [MT96c]).
Reference: [HKK97] <author> E. Han, G. Karypis, and Vipin Kumar. </author> <title> Scalable parallel data mining for association rules. </title> <booktitle> In Proc. SIGMOD, </booktitle> <month> May </month> <year> 1997. </year>
Reference-contexts: Similar candidate pruning techniques has been applied to discover sequential patterns (e.g., [MT95] [MT96c] [SA95a] [SA96a] [Z97]) and episodes (e.g., 30 [MT95] [MT96c]). Some other papers concentrate on designing parallel algorithms on share-nothing parallel environment (e.g., ([AS96] <ref> [HKK97] </ref>) and share-memory parallel environment (e.g., [ZPOL96]). The discovery of frequent set is a key process in solving these problems. A good algorithm for discovering frequent set could be applied in solving these problems. Mannila and Toivonen [MT97] [GKMT97] analyze the complexity of the (level-wise) bottom-up breadth-first search style algorithms.
Reference: [HKMRT96] <author> K. Hatonen, M. Klemettinen, H. Mannila, P. Ronkainen, and H. Toivonen. </author> <title> Knowledge Discovery from Telecommunication Network Alarm Databases. </title> <booktitle> In Proc. 12th International Conference on Data Engineering (ICDE), </booktitle> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: To the extent feasible, we follow the terminology of [AIS93a]. Let I = fi 1 ; i 2 ; : : : ; i m g be a set of m distinct items. The itemsets could represent different items in a supermarket or different alarm signals in telecommunication networks <ref> [HKMRT96] </ref>. A transaction T is a set of items in I. A transaction could represent some customer purchases of some items from a supermarket or the set of alarm signals occurring within a time interval. A database D is just a set of transactions.
Reference: [HS93] <author> M. Houtsma and A. Swami. </author> <title> Set-oriented mining of association rules. </title> <type> Research Report RJ 9567, </type> <institution> IBM Almaden Research Center, </institution> <month> Oct. </month> <year> 1993. </year>
Reference-contexts: Itemsets f1,2,3,5g and f1,2,5g were not considered, since the item 5 was not in the transaction. 17 Two complicated heuristics, remaining tuples optimization and pruning func-tion optimization, were used to prune candidates. Unfortunately, this algorithm still generates too many candidates. SETM <ref> [HS93] </ref> algorithm was later designed to use only standard SQL commands to find the frequent set. However, like AIS, SETM also creates candidates on-the-fly while reading the database.
Reference: [KMRTB94] <author> M. Klemettinen, H. Mannila, P. Ronkainen, H. Toivonen, and A. I. Berkamo. </author> <title> Finding interesting rules from large sets of discovered association rules. </title> <booktitle> In Proc. 3rd International Conference on Information and Knowledge Management, </booktitle> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: In addition to the algorithms discussed so far, there has been extensive research relating to the problem of association rule mining such as [BMS97], [GKMT97], [HCC92], [HF95], [MT96b], [ORS98], [S96], [SA95b], [SA96b], [SVA97], [T96a], and <ref> [KMRTB94] </ref>. Similar candidate pruning techniques has been applied to discover sequential patterns (e.g., [MT95] [MT96c] [SA95a] [SA96a] [Z97]) and episodes (e.g., 30 [MT95] [MT96c]). Some other papers concentrate on designing parallel algorithms on share-nothing parallel environment (e.g., ([AS96] [HKK97]) and share-memory parallel environment (e.g., [ZPOL96]).
Reference: [L96] <author> D. Lin. </author> <title> Mining association rules with longest frequent candidates. </title> <type> Unpublished manuscript. </type> <month> July </month> <year> 1996. </year> <month> 91 </month>
Reference-contexts: This "two-way search approach" can fully make use of both properties and thus speed up the search for the maximum frequent set. We call this "two-way search approach" as Pincer-Search method. This two-way search approach was first introduced in <ref> [L96] </ref> [L97] and formalized in [LK97] [LK98]. Let us use an example to explain the concept of this two-way search approach. 33 Considering the same database as in Fig. 2.1, Fig. 3.1 shows the process of the Pincer-Search algorithm.
Reference: [L97] <author> D. Lin. </author> <title> Mining association rules for long frequent itemsets. </title> <type> Unpublished manuscript. </type> <month> March </month> <year> 1997. </year>
Reference-contexts: This "two-way search approach" can fully make use of both properties and thus speed up the search for the maximum frequent set. We call this "two-way search approach" as Pincer-Search method. This two-way search approach was first introduced in [L96] <ref> [L97] </ref> and formalized in [LK97] [LK98]. Let us use an example to explain the concept of this two-way search approach. 33 Considering the same database as in Fig. 2.1, Fig. 3.1 shows the process of the Pincer-Search algorithm.
Reference: [LK97] <author> D. Lin and Z. Kedem. Pincer-Search: </author> <title> A new algorithm for discovering the maximum frequent set. </title> <type> Technical Report TR1997-742, </type> <institution> Dept. of Computer Science, </institution> <address> New York University, </address> <month> Sept. </month> <year> 1997. </year>
Reference-contexts: This "two-way search approach" can fully make use of both properties and thus speed up the search for the maximum frequent set. We call this "two-way search approach" as Pincer-Search method. This two-way search approach was first introduced in [L96] [L97] and formalized in <ref> [LK97] </ref> [LK98]. Let us use an example to explain the concept of this two-way search approach. 33 Considering the same database as in Fig. 2.1, Fig. 3.1 shows the process of the Pincer-Search algorithm.
Reference: [LK98] <author> D. Lin and Z. Kedem. Pincer-Search: </author> <title> A new algorithm for discovering the maximum frequent set. </title> <booktitle> In Proc. of the 6th International Conference on Extending Database Technology (EDBT). </booktitle> <month> Mar. </month> <year> 1998. </year>
Reference-contexts: This "two-way search approach" can fully make use of both properties and thus speed up the search for the maximum frequent set. We call this "two-way search approach" as Pincer-Search method. This two-way search approach was first introduced in [L96] [L97] and formalized in [LK97] <ref> [LK98] </ref>. Let us use an example to explain the concept of this two-way search approach. 33 Considering the same database as in Fig. 2.1, Fig. 3.1 shows the process of the Pincer-Search algorithm.
Reference: [M82] <author> T. Mitchell. </author> <title> Generalization as search. </title> <journal> Artificial Intelligence, </journal> <volume> Vol. 18, </volume> <year> 1982. </year>
Reference-contexts: In this thesis, we apply the MFCS concept to the association rule mining. In fact, the MFCS concept can be applied in solving other data mining problems as long as the problem has the closure properties to be discussed later. The monotone specialization relation discussed in [MT97] and <ref> [M82] </ref> was addressing the same properties. Popular benchmark databases designed by Agrawal and Srikant [AS94] have been used in [AS96], [ORS98], [PCY95], [SON95], and [ZPOL97]. We use these same benchmarks to evaluate the performance of our algorithm. <p> Mannila and Toivonen [MT97] [GKMT97] analyze the complexity of the (level-wise) bottom-up breadth-first search style algorithms. As our algorithm does not fit in this model, their complexity low bound does not apply to it. Our work was inspired by the notion of version space in Mitchell's machine learning paper <ref> [M82] </ref>.
Reference: [M95] <author> A. Mueller. </author> <title> Fast sequential and parallel algorithms for association rule mining: A comparison. </title> <type> Technical Report No. </type> <institution> CS-TR-3515 of CS Department, University of Maryland-College Park. </institution>
Reference: [M97] <author> H. Mannila. </author> <title> Methods and problems in data mining (a tutorial). </title> <booktitle> In Proc. of International Conference on Database Theory (ICDT), </booktitle> <month> Jan. </month> <year> 1997. </year>
Reference-contexts: In fact, MaxClique can be viewed as a special case of our Pincer-Search algorithm, if we discard the inplementation details. 2.5 Other Related Work General survey papers regarding data mining problems can be found in, e.g., [FPS96] [FPSU96] <ref> [M97] </ref> [PBKKS97]. In addition to the algorithms discussed so far, there has been extensive research relating to the problem of association rule mining such as [BMS97], [GKMT97], [HCC92], [HF95], [MT96b], [ORS98], [S96], [SA95b], [SA96b], [SVA97], [T96a], and [KMRTB94].
Reference: [MT95] <author> H. Mannila and H. Toivonen. </author> <title> Discovering frequent episodes in sequences. </title> <booktitle> In Proc. 1st International Conference on Knowledge Discovery and Data Mining (KDD), </booktitle> <month> Aug. </month> <year> 1995. </year>
Reference-contexts: Depending on the semantics attached to the input database, the frequent itemsets, and the term "occurs," we get the key components of different data mining problems such as the discovery of association rules (e.g., [AS94] [HF95] [MTV94]), theories (e.g., [GMS97]), strong rule (e.g., [P91]), episodes (e.g., <ref> [MT95] </ref> [MT96c]), and minimal keys (e.g., [GMS97]). Typical algorithms for finding the frequent set, i.e., the set of all frequent item-sets, operate in a bottom-up breadth-first fashion (e.g., [AIS93a]) [AS94] [AS96] [BMUT97] [HF95] [MT97] [MTV94] [ORS98] [PCY95] [SON95]). <p> In addition to the algorithms discussed so far, there has been extensive research relating to the problem of association rule mining such as [BMS97], [GKMT97], [HCC92], [HF95], [MT96b], [ORS98], [S96], [SA95b], [SA96b], [SVA97], [T96a], and [KMRTB94]. Similar candidate pruning techniques has been applied to discover sequential patterns (e.g., <ref> [MT95] </ref> [MT96c] [SA95a] [SA96a] [Z97]) and episodes (e.g., 30 [MT95] [MT96c]). Some other papers concentrate on designing parallel algorithms on share-nothing parallel environment (e.g., ([AS96] [HKK97]) and share-memory parallel environment (e.g., [ZPOL96]). The discovery of frequent set is a key process in solving these problems. <p> Similar candidate pruning techniques has been applied to discover sequential patterns (e.g., <ref> [MT95] </ref> [MT96c] [SA95a] [SA96a] [Z97]) and episodes (e.g., 30 [MT95] [MT96c]). Some other papers concentrate on designing parallel algorithms on share-nothing parallel environment (e.g., ([AS96] [HKK97]) and share-memory parallel environment (e.g., [ZPOL96]). The discovery of frequent set is a key process in solving these problems. A good algorithm for discovering frequent set could be applied in solving these problems.
Reference: [MT96a] <author> H. Mannila and H. Toivonen. </author> <title> On an algorithm for finding all interesting 92 sentences. </title> <booktitle> In 13th European Meeting on Cybernetics and Systems Research, </booktitle> <month> Apr. </month> <year> 1996. </year>
Reference-contexts: This is the technique that many algorithms (e.g., [AS94] [AS96] [BMUT97] [HF95] [MT97] [MTV94] [ORS98] [PCY95] [SON95]) use to decrease the number of candidates. In a "pure" top-down approach, only Property 2 is used to prune candidates. This is the technique used in [ZPOL97] and <ref> [MT96a] </ref>. 2.4 Typical Algorithms for Frequent Set Discovery We briefly discuss existing frequent set discovery algorithms in a roughly chronological order. 2.4.1 AIS and SETM Algorithms The problem of association rule mining was first introduced in [AIS93a]. An algorithm called AIS was given for discovering the frequent set. <p> This random sampling approach can solve the data skew problem in the Partition algorithm. Sampling algorithm is a guess-and-correct algorithm <ref> [MT96a] </ref> [MT97]. It guesses an answer in the first pass, then corrects the answer in subsequent passes. Unlike Partition, which look at the entire database, Sampling looks only at a part of the database in the first pass.
Reference: [MT96b] <author> H. Mannila and H. Toivonen. </author> <title> Multiple uses of frequent sets and condensed representations. </title> <booktitle> In Proc. 2nd International Conference on Knowledge Discovery and Data Mining (KDD), </booktitle> <month> Aug. </month> <year> 1996. </year>
Reference-contexts: In addition to the algorithms discussed so far, there has been extensive research relating to the problem of association rule mining such as [BMS97], [GKMT97], [HCC92], [HF95], <ref> [MT96b] </ref>, [ORS98], [S96], [SA95b], [SA96b], [SVA97], [T96a], and [KMRTB94]. Similar candidate pruning techniques has been applied to discover sequential patterns (e.g., [MT95] [MT96c] [SA95a] [SA96a] [Z97]) and episodes (e.g., 30 [MT95] [MT96c]).
Reference: [MT96c] <author> H. Mannila and H. Toivonen. </author> <title> Discovering generalized episodes using minimal occurrences. </title> <booktitle> In Proc. 2nd International Conference on Knowledge Discovery and Data Mining (KDD), </booktitle> <month> Aug. </month> <year> 1996. </year>
Reference-contexts: Depending on the semantics attached to the input database, the frequent itemsets, and the term "occurs," we get the key components of different data mining problems such as the discovery of association rules (e.g., [AS94] [HF95] [MTV94]), theories (e.g., [GMS97]), strong rule (e.g., [P91]), episodes (e.g., [MT95] <ref> [MT96c] </ref>), and minimal keys (e.g., [GMS97]). Typical algorithms for finding the frequent set, i.e., the set of all frequent item-sets, operate in a bottom-up breadth-first fashion (e.g., [AIS93a]) [AS94] [AS96] [BMUT97] [HF95] [MT97] [MTV94] [ORS98] [PCY95] [SON95]). <p> In addition to the algorithms discussed so far, there has been extensive research relating to the problem of association rule mining such as [BMS97], [GKMT97], [HCC92], [HF95], [MT96b], [ORS98], [S96], [SA95b], [SA96b], [SVA97], [T96a], and [KMRTB94]. Similar candidate pruning techniques has been applied to discover sequential patterns (e.g., [MT95] <ref> [MT96c] </ref> [SA95a] [SA96a] [Z97]) and episodes (e.g., 30 [MT95] [MT96c]). Some other papers concentrate on designing parallel algorithms on share-nothing parallel environment (e.g., ([AS96] [HKK97]) and share-memory parallel environment (e.g., [ZPOL96]). The discovery of frequent set is a key process in solving these problems. <p> Similar candidate pruning techniques has been applied to discover sequential patterns (e.g., [MT95] <ref> [MT96c] </ref> [SA95a] [SA96a] [Z97]) and episodes (e.g., 30 [MT95] [MT96c]). Some other papers concentrate on designing parallel algorithms on share-nothing parallel environment (e.g., ([AS96] [HKK97]) and share-memory parallel environment (e.g., [ZPOL96]). The discovery of frequent set is a key process in solving these problems. A good algorithm for discovering frequent set could be applied in solving these problems.
Reference: [MT97] <author> H. Mannila and H. Toivonen. </author> <title> Levelwise search and borders of theories in knowledge discovery. </title> <type> Technical Report TR C-1997-8, </type> <institution> Dept. of Computer Science, U. of Helsinki, </institution> <month> Jan. </month> <year> 1997. </year>
Reference-contexts: Typical algorithms for finding the frequent set, i.e., the set of all frequent item-sets, operate in a bottom-up breadth-first fashion (e.g., [AIS93a]) [AS94] [AS96] [BMUT97] [HF95] <ref> [MT97] </ref> [MTV94] [ORS98] [PCY95] [SON95]). The computation starts from frequent 1-itemsets (minimal length frequent itemsets at the bottom) and then extends one level up in every pass until all maximal (length) frequent itemsets are discovered. All frequent itemsets are explicitly examined and discovered by these algorithms. <p> In this thesis, we apply the MFCS concept to the association rule mining. In fact, the MFCS concept can be applied in solving other data mining problems as long as the problem has the closure properties to be discussed later. The monotone specialization relation discussed in <ref> [MT97] </ref> and [M82] was addressing the same properties. Popular benchmark databases designed by Agrawal and Srikant [AS94] have been used in [AS96], [ORS98], [PCY95], [SON95], and [ZPOL97]. We use these same benchmarks to evaluate the performance of our algorithm. <p> This is the technique that many algorithms (e.g., [AS94] [AS96] [BMUT97] [HF95] <ref> [MT97] </ref> [MTV94] [ORS98] [PCY95] [SON95]) use to decrease the number of candidates. In a "pure" top-down approach, only Property 2 is used to prune candidates. <p> This random sampling approach can solve the data skew problem in the Partition algorithm. Sampling algorithm is a guess-and-correct algorithm [MT96a] <ref> [MT97] </ref>. It guesses an answer in the first pass, then corrects the answer in subsequent passes. Unlike Partition, which look at the entire database, Sampling looks only at a part of the database in the first pass. <p> From subset lattice point of view, this negative border contains the neighbor itemsets of the local maximal frequent itemsets. The problem of computing the negative border can be transformed into a hypergraph traversal problem <ref> [MT97] </ref>. If no itemset in the negative border turns out to be frequent, then there is no false negative itemset. Otherwise, some false negative itemsets may need to be recovered. They can be recovered by further extending the negative border. <p> The discovery of frequent set is a key process in solving these problems. A good algorithm for discovering frequent set could be applied in solving these problems. Mannila and Toivonen <ref> [MT97] </ref> [GKMT97] analyze the complexity of the (level-wise) bottom-up breadth-first search style algorithms. As our algorithm does not fit in this model, their complexity low bound does not apply to it. Our work was inspired by the notion of version space in Mitchell's machine learning paper [M82].
Reference: [MTV94] <author> H. Mannila, H. Toivonen, and A. Verkamo. </author> <title> Improved methods for finding association rules. </title> <booktitle> In Proc. AAAI Workshop on Knowledge Discovery, </booktitle> <month> July </month> <year> 1994. </year>
Reference-contexts: Depending on the semantics attached to the input database, the frequent itemsets, and the term "occurs," we get the key components of different data mining problems such as the discovery of association rules (e.g., [AS94] [HF95] <ref> [MTV94] </ref>), theories (e.g., [GMS97]), strong rule (e.g., [P91]), episodes (e.g., [MT95] [MT96c]), and minimal keys (e.g., [GMS97]). Typical algorithms for finding the frequent set, i.e., the set of all frequent item-sets, operate in a bottom-up breadth-first fashion (e.g., [AIS93a]) [AS94] [AS96] [BMUT97] [HF95] [MT97] [MTV94] [ORS98] [PCY95] [SON95]). <p> of association rules (e.g., [AS94] [HF95] <ref> [MTV94] </ref>), theories (e.g., [GMS97]), strong rule (e.g., [P91]), episodes (e.g., [MT95] [MT96c]), and minimal keys (e.g., [GMS97]). Typical algorithms for finding the frequent set, i.e., the set of all frequent item-sets, operate in a bottom-up breadth-first fashion (e.g., [AIS93a]) [AS94] [AS96] [BMUT97] [HF95] [MT97] [MTV94] [ORS98] [PCY95] [SON95]). The computation starts from frequent 1-itemsets (minimal length frequent itemsets at the bottom) and then extends one level up in every pass until all maximal (length) frequent itemsets are discovered. All frequent itemsets are explicitly examined and discovered by these algorithms. <p> We present a novel Pincer-Search algorithm, which searches for the MFS from both bottom-up and top-down directions. It performs well even when the maximal frequent itemsets are long. The bottom-up search is similar to Apriori [AS94] and OCD <ref> [MTV94] </ref> algorithms. However, the top-down search is novel. It is implemented efficiently by introducing an auxiliary data structure, the maximum frequent candidate set (or MFCS), as explained later. <p> This is the technique that many algorithms (e.g., [AS94] [AS96] [BMUT97] [HF95] [MT97] <ref> [MTV94] </ref> [ORS98] [PCY95] [SON95]) use to decrease the number of candidates. In a "pure" top-down approach, only Property 2 is used to prune candidates. <p> In other words, supersets of an infrequent itemset are pruned. Concurrently with the Apriori algorithm, Mannila et al. <ref> [MTV94] </ref> proposed an OCD algorithm, which used the same closure property to eliminate candidates. The candidate generation process is also divided into two steps.
Reference: [NYSE97] <institution> The TAQ Database Release 1.0 in CD-ROM, New York Stock Exchange, Inc., </institution> <month> June </month> <year> 1997. </year>
Reference-contexts: We simply used a link-list data structure to store the frequent set and the candidate set in each pass. The databases used in performance evaluation, are the synthetic databases used in [AS94], the census databases similar to [BMUT97], and the stock transaction databases from New York Stock Exchange, Inc. <ref> [NYSE97] </ref>. Also, as done in [ORS98] and [SA95a], we used a one-dimensional array and a two-dimensional array to speed up the process of the first and the second pass correspondingly. The support counting phase runs very fast by using an array, since no searching is needed.
Reference: [ORS98] <author> B. Ozden, S. Ramaswamy. and A. Silberschatz. </author> <title> Cyclic Association Rules. </title> <booktitle> In Proc. 14th International Conference on Data Engineering (ICDE), </booktitle> <month> Feb. </month> <year> 1998. </year>
Reference-contexts: Typical algorithms for finding the frequent set, i.e., the set of all frequent item-sets, operate in a bottom-up breadth-first fashion (e.g., [AIS93a]) [AS94] [AS96] [BMUT97] [HF95] [MT97] [MTV94] <ref> [ORS98] </ref> [PCY95] [SON95]). The computation starts from frequent 1-itemsets (minimal length frequent itemsets at the bottom) and then extends one level up in every pass until all maximal (length) frequent itemsets are discovered. All frequent itemsets are explicitly examined and discovered by these algorithms. <p> The monotone specialization relation discussed in [MT97] and [M82] was addressing the same properties. Popular benchmark databases designed by Agrawal and Srikant [AS94] have been used in [AS96], <ref> [ORS98] </ref>, [PCY95], [SON95], and [ZPOL97]. We use these same benchmarks to evaluate the performance of our algorithm. In most cases, our algorithm not only reduces the number of passes of reading the database but also reduces the number of candidates (for whom support is counted). <p> This is the technique that many algorithms (e.g., [AS94] [AS96] [BMUT97] [HF95] [MT97] [MTV94] <ref> [ORS98] </ref> [PCY95] [SON95]) use to decrease the number of candidates. In a "pure" top-down approach, only Property 2 is used to prune candidates. <p> In addition to the algorithms discussed so far, there has been extensive research relating to the problem of association rule mining such as [BMS97], [GKMT97], [HCC92], [HF95], [MT96b], <ref> [ORS98] </ref>, [S96], [SA95b], [SA96b], [SVA97], [T96a], and [KMRTB94]. Similar candidate pruning techniques has been applied to discover sequential patterns (e.g., [MT95] [MT96c] [SA95a] [SA96a] [Z97]) and episodes (e.g., 30 [MT95] [MT96c]). <p> The databases used in performance evaluation, are the synthetic databases used in [AS94], the census databases similar to [BMUT97], and the stock transaction databases from New York Stock Exchange, Inc. [NYSE97]. Also, as done in <ref> [ORS98] </ref> and [SA95a], we used a one-dimensional array and a two-dimensional array to speed up the process of the first and the second pass correspondingly. The support counting phase runs very fast by using an array, since no searching is needed.
Reference: [P91] <author> G. Piatetsky-Shapiro. </author> <title> Discovery, analysis, and presentation of strong rules. Knowledge Discovery in Databases, </title> <publisher> AAAI Press, </publisher> <year> 1991. </year>
Reference-contexts: Depending on the semantics attached to the input database, the frequent itemsets, and the term "occurs," we get the key components of different data mining problems such as the discovery of association rules (e.g., [AS94] [HF95] [MTV94]), theories (e.g., [GMS97]), strong rule (e.g., <ref> [P91] </ref>), episodes (e.g., [MT95] [MT96c]), and minimal keys (e.g., [GMS97]). Typical algorithms for finding the frequent set, i.e., the set of all frequent item-sets, operate in a bottom-up breadth-first fashion (e.g., [AIS93a]) [AS94] [AS96] [BMUT97] [HF95] [MT97] [MTV94] [ORS98] [PCY95] [SON95]).
Reference: [PBKKS97] <author> G. Piatetsky-Shapiro, R. Brachman, T. Khabaza, W. Kloesgen, and E. Simoudis. </author> <title> An overview of issues in developing industrial data mining and knowledge discovery applications. </title> <booktitle> In Proc. 2nd International Conference on Knowledge Discovery and Data Mining (KDD), </booktitle> <month> Aug. </month> <year> 1996. </year>
Reference-contexts: In fact, MaxClique can be viewed as a special case of our Pincer-Search algorithm, if we discard the inplementation details. 2.5 Other Related Work General survey papers regarding data mining problems can be found in, e.g., [FPS96] [FPSU96] [M97] <ref> [PBKKS97] </ref>. In addition to the algorithms discussed so far, there has been extensive research relating to the problem of association rule mining such as [BMS97], [GKMT97], [HCC92], [HF95], [MT96b], [ORS98], [S96], [SA95b], [SA96b], [SVA97], [T96a], and [KMRTB94].
Reference: [PCY95] <author> J. Park, M. Chen, and P. Yu. </author> <title> An effective hash-based algorithm for mining association rules. </title> <booktitle> In Proc. ACM-SIGMOD, </booktitle> <month> May </month> <year> 1995. </year>
Reference-contexts: Typical algorithms for finding the frequent set, i.e., the set of all frequent item-sets, operate in a bottom-up breadth-first fashion (e.g., [AIS93a]) [AS94] [AS96] [BMUT97] [HF95] [MT97] [MTV94] [ORS98] <ref> [PCY95] </ref> [SON95]). The computation starts from frequent 1-itemsets (minimal length frequent itemsets at the bottom) and then extends one level up in every pass until all maximal (length) frequent itemsets are discovered. All frequent itemsets are explicitly examined and discovered by these algorithms. <p> The monotone specialization relation discussed in [MT97] and [M82] was addressing the same properties. Popular benchmark databases designed by Agrawal and Srikant [AS94] have been used in [AS96], [ORS98], <ref> [PCY95] </ref>, [SON95], and [ZPOL97]. We use these same benchmarks to evaluate the performance of our algorithm. In most cases, our algorithm not only reduces the number of passes of reading the database but also reduces the number of candidates (for whom support is counted). <p> This is the technique that many algorithms (e.g., [AS94] [AS96] [BMUT97] [HF95] [MT97] [MTV94] [ORS98] <ref> [PCY95] </ref> [SON95]) use to decrease the number of candidates. In a "pure" top-down approach, only Property 2 is used to prune candidates. <p> But, in general, the first step might produce a superset of the candidates produced in the join procedure of the Apriori algorithm. The Apriori-gen algorithm has been very successful in reducing the number of candidates and has been used in many subsequent algorithms, such as Partition 20 [SON95], DHP <ref> [PCY95] </ref>, Sampling [T96b], DIC [BMUT97], and Clique [ZPOL97]. 2.4.3 DHP and Partition Algorithm DHP [PCY95] tried to improve Apriori by using a hash filter to count the upper-bound of the support for the next pass. The upper-bound can be used to eliminate candidates. <p> The Apriori-gen algorithm has been very successful in reducing the number of candidates and has been used in many subsequent algorithms, such as Partition 20 [SON95], DHP <ref> [PCY95] </ref>, Sampling [T96b], DIC [BMUT97], and Clique [ZPOL97]. 2.4.3 DHP and Partition Algorithm DHP [PCY95] tried to improve Apriori by using a hash filter to count the upper-bound of the support for the next pass. The upper-bound can be used to eliminate candidates. This algorithm is targeting on reducing the number of candidates in the second pass, which could be very large. <p> characteristics of the new algorithm's performance. 4.1 Preliminary Discussion 4.1.1 Auxiliary Data Structures Used Since we are interested in studying the effect of using the MFCS to reduce the number of candidates and the number of passes, we didn't use more efficient data structures, such as hash tables (e.g., [AS94] <ref> [PCY95] </ref>), to store the itemsets. We simply used a link-list data structure to store the frequent set and the candidate set in each pass.
Reference: [S96] <author> R. Srikant. </author> <title> Fast algorithm for mining association rules and sequential patterns. </title> <type> Ph.D. Thesis, </type> <institution> University of Wisconsin, Madison, </institution> <year> 1996. </year>
Reference-contexts: In addition to the algorithms discussed so far, there has been extensive research relating to the problem of association rule mining such as [BMS97], [GKMT97], [HCC92], [HF95], [MT96b], [ORS98], <ref> [S96] </ref>, [SA95b], [SA96b], [SVA97], [T96a], and [KMRTB94]. Similar candidate pruning techniques has been applied to discover sequential patterns (e.g., [MT95] [MT96c] [SA95a] [SA96a] [Z97]) and episodes (e.g., 30 [MT95] [MT96c]).
Reference: [SA95a] <author> R. Agrawal and R. Srikant. </author> <title> Mining Sequential Patterns. </title> <booktitle> In Proc. 11th Int'l Conference on Data Engineering (ICDE), </booktitle> <month> Mar. </month> <year> 1995. </year>
Reference-contexts: By using this hash filter, some candidates can be pruned before reading the database in the next pass. However, according to the investigations done in [AS96], this optimization may not be as good as using a two-dimensional array as discussed in <ref> [SA95a] </ref>. Furthermore, like Apriori, DHP considers every frequent itemset. Savasere et al. [SON95] proposed a Partition algorithm. This paper addresses two issues in previous algorithms. <p> Similar candidate pruning techniques has been applied to discover sequential patterns (e.g., [MT95] [MT96c] <ref> [SA95a] </ref> [SA96a] [Z97]) and episodes (e.g., 30 [MT95] [MT96c]). Some other papers concentrate on designing parallel algorithms on share-nothing parallel environment (e.g., ([AS96] [HKK97]) and share-memory parallel environment (e.g., [ZPOL96]). The discovery of frequent set is a key process in solving these problems. <p> The databases used in performance evaluation, are the synthetic databases used in [AS94], the census databases similar to [BMUT97], and the stock transaction databases from New York Stock Exchange, Inc. [NYSE97]. Also, as done in [ORS98] and <ref> [SA95a] </ref>, we used a one-dimensional array and a two-dimensional array to speed up the process of the first and the second pass correspondingly. The support counting phase runs very fast by using an array, since no searching is needed.
Reference: [SA95b] <author> R. Srikant and R. Agrawal. </author> <title> Mining generalized association rules. </title> <booktitle> In Proc. 21st VLDB. </booktitle> <month> Sep. </month> <year> 1995. </year>
Reference-contexts: In addition to the algorithms discussed so far, there has been extensive research relating to the problem of association rule mining such as [BMS97], [GKMT97], [HCC92], [HF95], [MT96b], [ORS98], [S96], <ref> [SA95b] </ref>, [SA96b], [SVA97], [T96a], and [KMRTB94]. Similar candidate pruning techniques has been applied to discover sequential patterns (e.g., [MT95] [MT96c] [SA95a] [SA96a] [Z97]) and episodes (e.g., 30 [MT95] [MT96c]). Some other papers concentrate on designing parallel algorithms on share-nothing parallel environment (e.g., ([AS96] [HKK97]) and share-memory parallel environment (e.g., [ZPOL96]).
Reference: [SA96a] <author> R. Srikant and R. Agrawal. </author> <title> Mining Sequential Patterns: Generalizations and Performance Improvements. </title> <booktitle> In Proc. of the 5th International Conference on Extending Database Technology (EDBT). </booktitle> <month> Mar. </month> <year> 1996. </year> <month> 94 </month>
Reference-contexts: Similar candidate pruning techniques has been applied to discover sequential patterns (e.g., [MT95] [MT96c] [SA95a] <ref> [SA96a] </ref> [Z97]) and episodes (e.g., 30 [MT95] [MT96c]). Some other papers concentrate on designing parallel algorithms on share-nothing parallel environment (e.g., ([AS96] [HKK97]) and share-memory parallel environment (e.g., [ZPOL96]). The discovery of frequent set is a key process in solving these problems.
Reference: [SA96b] <author> R. Srikant and R. Agrawal. </author> <title> Mining Quantitative Association Rules in Large Relational Tables. </title> <booktitle> In Proc. SIGMOD, </booktitle> <month> June </month> <year> 1996. </year>
Reference-contexts: In addition to the algorithms discussed so far, there has been extensive research relating to the problem of association rule mining such as [BMS97], [GKMT97], [HCC92], [HF95], [MT96b], [ORS98], [S96], [SA95b], <ref> [SA96b] </ref>, [SVA97], [T96a], and [KMRTB94]. Similar candidate pruning techniques has been applied to discover sequential patterns (e.g., [MT95] [MT96c] [SA95a] [SA96a] [Z97]) and episodes (e.g., 30 [MT95] [MT96c]). Some other papers concentrate on designing parallel algorithms on share-nothing parallel environment (e.g., ([AS96] [HKK97]) and share-memory parallel environment (e.g., [ZPOL96]).
Reference: [SON95] <author> A. Sarasere, E. Omiecinsky, and S. Navathe. </author> <title> An efficient algorithm for mining association rules in large databases. </title> <booktitle> In Proc. 21st VLDB, </booktitle> <month> Sept. </month> <year> 1995. </year>
Reference-contexts: Typical algorithms for finding the frequent set, i.e., the set of all frequent item-sets, operate in a bottom-up breadth-first fashion (e.g., [AIS93a]) [AS94] [AS96] [BMUT97] [HF95] [MT97] [MTV94] [ORS98] [PCY95] <ref> [SON95] </ref>). The computation starts from frequent 1-itemsets (minimal length frequent itemsets at the bottom) and then extends one level up in every pass until all maximal (length) frequent itemsets are discovered. All frequent itemsets are explicitly examined and discovered by these algorithms. <p> The monotone specialization relation discussed in [MT97] and [M82] was addressing the same properties. Popular benchmark databases designed by Agrawal and Srikant [AS94] have been used in [AS96], [ORS98], [PCY95], <ref> [SON95] </ref>, and [ZPOL97]. We use these same benchmarks to evaluate the performance of our algorithm. In most cases, our algorithm not only reduces the number of passes of reading the database but also reduces the number of candidates (for whom support is counted). <p> This is the technique that many algorithms (e.g., [AS94] [AS96] [BMUT97] [HF95] [MT97] [MTV94] [ORS98] [PCY95] <ref> [SON95] </ref>) use to decrease the number of candidates. In a "pure" top-down approach, only Property 2 is used to prune candidates. <p> But, in general, the first step might produce a superset of the candidates produced in the join procedure of the Apriori algorithm. The Apriori-gen algorithm has been very successful in reducing the number of candidates and has been used in many subsequent algorithms, such as Partition 20 <ref> [SON95] </ref>, DHP [PCY95], Sampling [T96b], DIC [BMUT97], and Clique [ZPOL97]. 2.4.3 DHP and Partition Algorithm DHP [PCY95] tried to improve Apriori by using a hash filter to count the upper-bound of the support for the next pass. The upper-bound can be used to eliminate candidates. <p> However, according to the investigations done in [AS96], this optimization may not be as good as using a two-dimensional array as discussed in [SA95a]. Furthermore, like Apriori, DHP considers every frequent itemset. Savasere et al. <ref> [SON95] </ref> proposed a Partition algorithm. This paper addresses two issues in previous algorithms. The first issue is that the algorithms discussed so far require reading the database many times (as many times as the length of the longest frequent itemset).
Reference: [SVA97] <author> R. Srikant, Q. Vu, and R. Agrawal. </author> <title> Mining Association Rules with Item Constraints. </title> <booktitle> In Proc. of the 3rd International Conference on Knowledge Discovery in Databases and Data Mining (KDD), </booktitle> <month> Aug. </month> <year> 1997. </year>
Reference-contexts: In addition to the algorithms discussed so far, there has been extensive research relating to the problem of association rule mining such as [BMS97], [GKMT97], [HCC92], [HF95], [MT96b], [ORS98], [S96], [SA95b], [SA96b], <ref> [SVA97] </ref>, [T96a], and [KMRTB94]. Similar candidate pruning techniques has been applied to discover sequential patterns (e.g., [MT95] [MT96c] [SA95a] [SA96a] [Z97]) and episodes (e.g., 30 [MT95] [MT96c]). Some other papers concentrate on designing parallel algorithms on share-nothing parallel environment (e.g., ([AS96] [HKK97]) and share-memory parallel environment (e.g., [ZPOL96]).
Reference: [T96a] <author> H. Toivonen. </author> <title> Discovery of frequent patterns in large data collections. </title> <institution> Technical Report A-1996-5 of the Department of Computer Science, University of Helsinki, Finland, </institution> <year> 1996. </year>
Reference-contexts: In addition to the algorithms discussed so far, there has been extensive research relating to the problem of association rule mining such as [BMS97], [GKMT97], [HCC92], [HF95], [MT96b], [ORS98], [S96], [SA95b], [SA96b], [SVA97], <ref> [T96a] </ref>, and [KMRTB94]. Similar candidate pruning techniques has been applied to discover sequential patterns (e.g., [MT95] [MT96c] [SA95a] [SA96a] [Z97]) and episodes (e.g., 30 [MT95] [MT96c]). Some other papers concentrate on designing parallel algorithms on share-nothing parallel environment (e.g., ([AS96] [HKK97]) and share-memory parallel environment (e.g., [ZPOL96]).
Reference: [T96b] <author> H. Toivonen. </author> <title> Sampling large databases for association rules. </title> <booktitle> In Proc. 22nd VLDB, </booktitle> <month> Sept. </month> <year> 1996. </year>
Reference-contexts: The Apriori-gen algorithm has been very successful in reducing the number of candidates and has been used in many subsequent algorithms, such as Partition 20 [SON95], DHP [PCY95], Sampling <ref> [T96b] </ref>, DIC [BMUT97], and Clique [ZPOL97]. 2.4.3 DHP and Partition Algorithm DHP [PCY95] tried to improve Apriori by using a hash filter to count the upper-bound of the support for the next pass. The upper-bound can be used to eliminate candidates. <p> Then, the global candidate set will be very large. Third, this algorithm will consider more candidates than Apriori. If there are long maximal frequent itemsets, this algorithm is infeasible. 2.4.4 Sampling Algorithm Toivonen <ref> [T96b] </ref> proposed to consider only some samples of the database and discover an approximate frequent set by using a standard bottom-up approach algorithm, such as the OCD algorithm or the Apriori algorithm. This random sampling approach can solve the data skew problem in the Partition algorithm.
Reference: [Z97] <author> M. J. Zaki. </author> <title> Fast mining of sequential patterns in very large databases. </title> <institution> Technical Report 668 of the Department of Computer Science, University of Rochester. </institution> <month> Nov. </month> <year> 1997. </year>
Reference-contexts: Similar candidate pruning techniques has been applied to discover sequential patterns (e.g., [MT95] [MT96c] [SA95a] [SA96a] <ref> [Z97] </ref>) and episodes (e.g., 30 [MT95] [MT96c]). Some other papers concentrate on designing parallel algorithms on share-nothing parallel environment (e.g., ([AS96] [HKK97]) and share-memory parallel environment (e.g., [ZPOL96]). The discovery of frequent set is a key process in solving these problems.
Reference: [ZPOL96] <author> M. J. Zaki, S. Parthasarathy, M. Ogihara, and W. Li. </author> <title> Parallel data min-nig for association rules on shared-memory multi-processors. </title> <institution> Technical Re 95 port 618 of the Department of Computer Science, University of Rochester. </institution> <month> May </month> <year> 1996. </year>
Reference-contexts: Similar candidate pruning techniques has been applied to discover sequential patterns (e.g., [MT95] [MT96c] [SA95a] [SA96a] [Z97]) and episodes (e.g., 30 [MT95] [MT96c]). Some other papers concentrate on designing parallel algorithms on share-nothing parallel environment (e.g., ([AS96] [HKK97]) and share-memory parallel environment (e.g., <ref> [ZPOL96] </ref>). The discovery of frequent set is a key process in solving these problems. A good algorithm for discovering frequent set could be applied in solving these problems. Mannila and Toivonen [MT97] [GKMT97] analyze the complexity of the (level-wise) bottom-up breadth-first search style algorithms.

References-found: 47

