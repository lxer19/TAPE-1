URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/academic/class/11761-s97/WWW/tex/segment.ps
Refering-URL: 
Root-URL: 
Title: Text Segmentation Using Exponential Models  
Author: Doug Beeferman, Adam Berger, and John Lafferty 
Address: 5000 Forbes Avenue Pittsburgh, PA 15213  
Affiliation: School of Computer Science Carnegie Mellon University  
Abstract: This paper introduces a new statistical approach to automatically partitioning text into coherent segments. Our proposed model enlists both long-range and short-range language models to help it sniff out likely sites of topic changes in text. To aid its search, the model consults a set of simple lexical hints it has learned to associate with the presence of boundaries through inspection of a large corpus of annotated data. We also propose a new probabilistically motivated error metric for use by the natural language processing community, intended to supersede precision and recall for appraising segmentation algorithms. Qualitative assessment of our algorithm as well as evaluation using this new metric demonstrate the effectiveness of our approach in two very different domains, Wall Street Journal articles and broadcast news transcripts.
Abstract-found: 1
Intro-found: 1
Reference: [BBL97] <author> D. Beeferman, A. Berger, and J. Lafferty. </author> <title> Accounting for distance in a long-range language model. </title> <booktitle> In Proceedings of the ACL, </booktitle> <address> Madrid, Spain, </address> <year> 1997. </year> <note> Submitted. </note>
Reference-contexts: Another approach, using maximum entropy methods, introduces a parameter for trigger pairs of mutually informative words, so that the occurrence of certain words in recent context boosts the probability of the words that they trigger [LRR93]. The method we use here, described in <ref> [BBL97] </ref>, employs a static trigram model as a "prior," or default distribution, and adds certain features to a family of conditional exponential models to capture some of the nonstationary features of text. The features are simple trigger pairs of words chosen on the basis of mutual information. <p> In general, the cache consists of content words s which promote the probability of their mate t, and correspondingly demote the probability of other words. As described in <ref> [BBL97] </ref>, for each (s; t) trigger pair there corresponds a real-valued parameter ; the probability of t is boosted by a factor of e for W words following the occurrence of s i .
Reference: [BDD96] <author> A. Berger, S. Della Pietra, and V. Della Pietra. </author> <title> A maximum entropy approach to natural language processing. </title> <journal> Computational Linguistics, </journal> <volume> 22(1) </volume> <pages> 39-71, </pages> <year> 1996. </year>
Reference-contexts: and it is hard to predict how well the idea would apply to cases in which just the text is available. 3 An exponential model of segmentation Our attack on the segmentation problem is based on a statistical framework that we call feature induction for random fields and exponential models <ref> [BDD96, DDL97] </ref>. The basic idea behind this approach is to assign a probability distribution to a particular position in the data stream by combining different "features" of the data and assigning each of them a weight in an exponential model. For simplicity, we assume that the features are binary questions. <p> Similar effects are seen to be at play in the feature induction results that we present for the segmentation problem in the following sections. For the details on feature induction and other examples of it in action, we refer to the papers <ref> [BDD96, DDL97] </ref>. The discussion in [DDL97] also explains how the feature induction algorithm generalizes decision trees. While decision trees recursively partition the training data, the features in an exponential model can be overlapping, so that the scheme is much less prone to overfitting.
Reference: [CKM + 95] <author> M. Christel, T. Kanade, M. Mauldin, R. Reddy, M. Sirbu, S. Stevens, and H. Wactlar. </author> <title> Informedia digital video library. </title> <journal> Communications of the ACM, </journal> <volume> 38(4) </volume> <pages> 57-58, </pages> <year> 1995. </year>
Reference-contexts: This can manifest itself in quite unfortunate ways. For example, a video-on-demand application (such as the one described in <ref> [CKM + 95] </ref>) responding to a query about a recent news event may provide the user with a news clip related to the event, followed by or preceded by (or both) an unrelated story or even a commercial. 1 Document summarization is another fertile area for an automatic segmenter. <p> Qualitative assessment as well as the evaluation of our algorithm with this new metric demonstrates its effectiveness in two very different domains, Wall Street Journal articles and broadcast news transcripts. Our immediate application of this model will be to the video-on-demand application called Informedia <ref> [CKM + 95] </ref>. We intend to mix simple audio and video features such as statistics from pauses, black frames, and color histograms with our lexical features in order to segment news broadcasts into component stories.
Reference: [DDL97] <author> S. Della Pietra, V. Della Pietra, and J. Lafferty. </author> <title> Inducing features of random fields. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 19(3), </volume> <year> 1997. </year> <note> In press. </note>
Reference-contexts: and it is hard to predict how well the idea would apply to cases in which just the text is available. 3 An exponential model of segmentation Our attack on the segmentation problem is based on a statistical framework that we call feature induction for random fields and exponential models <ref> [BDD96, DDL97] </ref>. The basic idea behind this approach is to assign a probability distribution to a particular position in the data stream by combining different "features" of the data and assigning each of them a weight in an exponential model. For simplicity, we assume that the features are binary questions. <p> The training algorithm we use for estimating values is the Improved Iterative Scaling algorithm of Della Pietra et al. <ref> [DDL97] </ref>, which is a maximum-likelihood scheme. Assuming robust estimates for the parameters, the resulting model is essentially guaranteed to be superior to the trigram model. <p> Under certain mild regularity 10 conditions, the maximum likelihood solution q ? = arg min D (p k q) (10) exists and is unique. To find this solution, we use the iterative scaling algorithm presented in <ref> [DDL97] </ref>. This explains how a model is chosen once we know the features f 1 ; : : : ; f n , but how are these features to be found? The procedure that we follow is a greedy algorithm akin to growing a decision tree. <p> Similar effects are seen to be at play in the feature induction results that we present for the segmentation problem in the following sections. For the details on feature induction and other examples of it in action, we refer to the papers <ref> [BDD96, DDL97] </ref>. The discussion in [DDL97] also explains how the feature induction algorithm generalizes decision trees. While decision trees recursively partition the training data, the features in an exponential model can be overlapping, so that the scheme is much less prone to overfitting. <p> Similar effects are seen to be at play in the feature induction results that we present for the segmentation problem in the following sections. For the details on feature induction and other examples of it in action, we refer to the papers [BDD96, DDL97]. The discussion in <ref> [DDL97] </ref> also explains how the feature induction algorithm generalizes decision trees. While decision trees recursively partition the training data, the features in an exponential model can be overlapping, so that the scheme is much less prone to overfitting.
Reference: [Hea94] <author> M.A. Hearst. </author> <title> Multi-paragraph segmentation of expository text. </title> <booktitle> In Proc. ACL, </booktitle> <year> 1994. </year> <month> 21 </month>
Reference-contexts: demonstrate our model's effectiveness on two distinct domains. 2 Previous work In this section we consider past approaches to the text segmentation problem, including data-driven algorithms like ours and an approach that requires a pre-existing knowledge base of word similarity information. 2.1 Text tiling The TextTiling algorithm, introduced by Hearst <ref> [Hea94] </ref>, segments expository texts into multiple paragraphs of coherent discourse units. Hearst's work emphasizes subtopic structure, although nothing excludes it from more objectively defined applications like article segmentation. A cosine measure is used to gauge the similarity between constant-size blocks of morphologically analyzed tokens.
Reference: [JMRS91] <author> F. Jelinek, B. Merialdo, S. Roukos, and M. Strauss. </author> <title> A dynamic language model for speech recognition. </title> <booktitle> In Proceedings of the DARPA Speech and Natural Language Workshop, </booktitle> <pages> pages 293-295, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: One means of injecting long-range awareness into a language model is by retaining a cache of the most recently seen n-grams which is smoothed together (typically by linear interpolation) with the static model; see for example <ref> [JMRS91, Kd90] </ref>. Another approach, using maximum entropy methods, introduces a parameter for trigger pairs of mutually informative words, so that the occurrence of certain words in recent context boosts the probability of the words that they trigger [LRR93].
Reference: [Kat87] <author> S. Katz. </author> <title> Estimation of probabilities from sparse data for the langauge model component of a speech recognizer. </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> ASSP-35(3):400-401, </volume> <month> March </month> <year> 1987. </year>
Reference-contexts: In this section and the next, we describe the short- and long-range models and explain their utility in identifying segments. The trigram models p tri (w j w 2 ; w 1 ) we employ use the Katz backof scheme <ref> [Kat87] </ref> for smoothing. We relied on two different corpora for training these models: * WSJ: A 38-million word collection of Wall Street Journal articles.
Reference: [Kd90] <author> R. Kuhn and R. de Mori. </author> <title> A cache-based natural language model for speech recognition. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 12 </volume> <pages> 570-583, </pages> <year> 1990. </year>
Reference-contexts: One means of injecting long-range awareness into a language model is by retaining a cache of the most recently seen n-grams which is smoothed together (typically by linear interpolation) with the static model; see for example <ref> [JMRS91, Kd90] </ref>. Another approach, using maximum entropy methods, introduces a parameter for trigger pairs of mutually informative words, so that the occurrence of certain words in recent context boosts the probability of the words that they trigger [LRR93].
Reference: [KF94] <author> H. Kozima and T. Furugori. </author> <title> Segmenting narrative text into coherent scenes. </title> <journal> Literary and Linguistic Computing, </journal> <volume> 9 </volume> <pages> 13-19, </pages> <year> 1994. </year>
Reference-contexts: Word pairs other than "self-triggers", for example, can be discovered automatically from training data using the techniques of mutual information employed by our language model. (As observed in <ref> [KF94] </ref> in regard to Youmans' [You91] Vocabulary Management Profile, words are often not simply repeated, but they are restated in different ways, particularly in vocabulary-rich domains.) Furthermore, Hearst's approach segments at the paragraph level, which is too coarse for applications like information retrieval on transcribed or automatically recognized spoken documents, in
Reference: [Koz93] <author> H. Kozima. </author> <title> Text segmentation based on similarity between words. </title> <booktitle> In Proc. ACL, </booktitle> <year> 1993. </year>
Reference-contexts: often not simply repeated, but they are restated in different ways, particularly in vocabulary-rich domains.) Furthermore, Hearst's approach segments at the paragraph level, which is too coarse for applications like information retrieval on transcribed or automatically recognized spoken documents, in which paragraph boundaries are not known. 2.2 Lexical cohesion Kozima <ref> [Koz93] </ref> employs a "lexical cohesion profile" to keep track of the cohesiveness of words in a text within a fixed-length window. In contrast to Hearst's focus on strict repetition, Kozima uses a semantic network to provide knowledge about word pairs that are semantically related.
Reference: [LP95] <author> D. J. Litman and R. J. Passonneau. </author> <title> Combining multiple knowledge sources for discourse segmentation. </title> <booktitle> In Proc. ACL, </booktitle> <year> 1995. </year>
Reference-contexts: A graphically motivated segmentation technique called dotplotting is offered by Reynar [Rey94]. This technique uses a simplified notion of lexical cohesion, depending exclusively on word repetition to find tight regions of topic similarity. 2.3 Other feature-based approaches Litman and Passonneau <ref> [LP95] </ref> offer an algorithm that uses an existing machine learning program to combine multiple linguistic features extracted from corpora of spoken text, including prosodic and lexical cues. The program outputs a decision tree that can be used to decide whether boundaries exist between prosodic phrases in a test corpus.
Reference: [LRR93] <author> R. Lau, R. Rosenfeld, and S. Roukos. </author> <title> Adaptive language modeling using the maximum entropy principle. </title> <booktitle> In Proceedings of the ARPA Human Language Technology Workshop, </booktitle> <pages> pages 108-113. </pages> <publisher> Morgan Kaufman Publishers, </publisher> <year> 1993. </year>
Reference-contexts: Another approach, using maximum entropy methods, introduces a parameter for trigger pairs of mutually informative words, so that the occurrence of certain words in recent context boosts the probability of the words that they trigger <ref> [LRR93] </ref>. The method we use here, described in [BBL97], employs a static trigram model as a "prior," or default distribution, and adds certain features to a family of conditional exponential models to capture some of the nonstationary features of text.
Reference: [Nea92] <author> R. Neal. </author> <title> Connectionist learning of belief networks. </title> <journal> Artificial Intelligence, </journal> <volume> 56 </volume> <pages> 71-113, </pages> <year> 1992. </year>
Reference-contexts: (!) + 2 f 2 (!) + n f n (!) : (7) The normalization constants Z (!) = 1 + e f (!) (8) insure that this is indeed a family of conditional probability distributions. (This family of models is closely related to the class of sigmoidal belief networks <ref> [Nea92] </ref>.) Our judgement of the merit of a model q 2 Q (f; q 0 ) relative to a reference distribution p 62 Q (f; q 0 ) during training is made in terms of the Kullback-Leibler divergence D (p k q) = !2 X b2fyes;nog p (b j !) log
Reference: [Rey94] <author> J. C. Reynar. </author> <title> An automatic method of finding topic boundaries. </title> <booktitle> In Proc. ACL, </booktitle> <year> 1994. </year>
Reference-contexts: A weakness of the approach, however, is that it relies on knowledge that is not focused on any particular domain, and that may not exist in all languages. A graphically motivated segmentation technique called dotplotting is offered by Reynar <ref> [Rey94] </ref>. <p> We believe that in a segmenter, close should count for something. A useful metric should also be robust with respect to the scale (words, sentences, paragraphs, for instance) at which boundaries are determined. However, precision and recall are scale-dependent quantities. <ref> [Rey94] </ref> uses an error window that redefines "correct" to mean hypothesized within some constant window of units away from a reference boundary, but this approach still suffers from overdiscretizing error, drawing all-or-nothing lines insensitive to gradations of correctness. Finally, a useful metric should be a single number.
Reference: [You91] <author> G. Youmans. </author> <title> A new tool for discourse analysis: The vocabulary-management profile. </title> <booktitle> Language, </booktitle> <volume> 67 </volume> <pages> 763-789, </pages> <year> 1991. </year> <month> 22 </month>
Reference-contexts: Word pairs other than "self-triggers", for example, can be discovered automatically from training data using the techniques of mutual information employed by our language model. (As observed in [KF94] in regard to Youmans' <ref> [You91] </ref> Vocabulary Management Profile, words are often not simply repeated, but they are restated in different ways, particularly in vocabulary-rich domains.) Furthermore, Hearst's approach segments at the paragraph level, which is too coarse for applications like information retrieval on transcribed or automatically recognized spoken documents, in which paragraph boundaries are not
References-found: 15

