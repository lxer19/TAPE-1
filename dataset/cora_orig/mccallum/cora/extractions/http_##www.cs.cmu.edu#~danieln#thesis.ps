URL: http://www.cs.cmu.edu/~danieln/thesis.ps
Refering-URL: http://www.cs.cmu.edu/~danieln/publications.html
Root-URL: 
Title: Adaptive Computation Techniques for Time Series Analysis  
Author: by Daniel Nikolaev Nikovski 
Degree: A Thesis Submitted in Partial Fulfillment of the Requirements for the Master of Science degree Department of Computer Science in the Graduate School  
Date: July 1995  
Affiliation: Engineer of Computer Systems and Control Technical University Sofia, Bulgaria  Southern Illinois University at Carbondale  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Bauer, R.J., Jr. </author> <year> (1994). </year> <title> Genetic Algorithms and Investment Strategies. </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference-contexts: Yet another AI-related method for function approximation comes from the field of Genetic Programming (GP). The respective machine learning procedure there is called symbolic regression [25]. Basic genetic algorithms have also been extensively used for financial modeling <ref> [1, 6] </ref>. 12 All these methods are examples of adaptive computation, or soft computing | that is, techniques that rely on self-modifying learning rules to approximate complex dependencies. These methods are to a great extent complementary to each other.
Reference: [2] <author> Box, G.E.P., and G.M. </author> <title> Jenkins (1970). Time Series Analysis: Forecasting and Control. </title> <address> San Francisco: </address> <publisher> Holden-Day. </publisher>
Reference-contexts: First, it is not easy to establish from the time series data what the dimensionality of the system is. This issue arises with linear modeling too, but there the autocorrelation function of the time-series can be used to this end <ref> [2] </ref>. Among the techniques used in nonlinear modeling are estimation of the correlation integral, the fractal dimension and various box-counting methods [34, 14]. The second major issue, which is also the topic of this thesis, is the discovery of the actual mapping between the lagged vector and the system state. <p> A natural approach is to combine the two models by allowing both an autoregressive and a moving average component: 8 M X a m y (tm) + p=1 This model is called accordingly an ARMA (M,P) model. Box and Jenkins <ref> [2] </ref> proposed an efficient algorithm for calculating the coefficients a m and b p from the spectrum of the signal. The Box-Jenkins algorithm is a batch algorithm | it needs all samples of the time-series in order to estimate the spectrum and find the coefficients.
Reference: [3] <author> Brock, W. A. </author> <year> (1988). </year> <title> Nonlinearity and complex dynamics in economics and finance, </title> <editor> in Andersen, P. W., K.J. Arrow, and D. Pines, (Eds.) </editor> <title> The Economy as an Evolving Complex System, </title> <address> Reading, MA: </address> <publisher> Addison-Wesley, </publisher> <pages> 77-97. </pages>
Reference: [4] <author> Casdagli, M. </author> <year> (1989). </year> <title> Nonlinear prediction of chaotic time series. </title> <journal> Physica D, </journal> <volume> vol. 35, </volume> <pages> 335-356. </pages>
Reference-contexts: We can use to this end techniques from the area of nonsymbolic artificial intelligence (AI), which are also called adaptive computation techniques. The computational models most often used in function approximation are multilayer perceptrons (MLP) [35] and radial basis functions (RBF) <ref> [4] </ref>. The utility of these techniques is that they can infer nonlinear dependencies between data variables that take on real values. Their predecessors are the traditional symbolic learning methods in AI that work with Boolean variables only. <p> Other less known, but arguably more powerful and faster learning architectures are Radial Basis Functions (RBF), and Hierarchical Mixtures of Experts (HME). RBFs have been used for time series analysis by Casdagly <ref> [4] </ref> and Hutchinson [20]. An application of HME to this task has been done by Waterhouse and Robinson [36]. These two methods will be described in greater detail in Chapters 3 and 4 respectively. Another successful extension of the classical AI methods are Fuzzy Logic Systems (FLS). <p> The linear RBF h (r) = r was used in all experiments, based on Casdagli's observation that various RBFs give comparable performance <ref> [4] </ref>. The only free parameter in a generalized RBF network is the number of knots. The in-sample and out-of-sample NRMSE were plotted against the number of knots.
Reference: [5] <author> Casdagli, M., and S. Eubank (Eds.) </author> <year> (1992). </year> <title> Nonlinear Modeling and Forecasting. </title> <address> Reading, MA: Addidson-Wesley. </address>
Reference: [6] <author> Colin, A.M. </author> <year> (1994). </year> <title> Genetic algorithms for financial modeling, </title> <editor> In Deboeck, G.J. (Ed.) </editor> <year> (1994). </year> <title> Trading on the Edge. </title> <address> New York: </address> <publisher> Wiley., </publisher> <pages> 148-174. </pages>
Reference-contexts: Yet another AI-related method for function approximation comes from the field of Genetic Programming (GP). The respective machine learning procedure there is called symbolic regression [25]. Basic genetic algorithms have also been extensively used for financial modeling <ref> [1, 6] </ref>. 12 All these methods are examples of adaptive computation, or soft computing | that is, techniques that rely on self-modifying learning rules to approximate complex dependencies. These methods are to a great extent complementary to each other.
Reference: [7] <author> Deboeck, G.J. (Ed.) </author> <year> (1994). </year> <title> Trading on the Edge. </title> <address> New York: Wiley.. </address>
Reference-contexts: In fact, there is no evidence that disproves the conjecture that the fluctuations in the stock market are simply driven by random noise and no meaningful dynamics are present. This last hypothesis is known as the Efficient Market, or Random Walk Hypothesis <ref> [7] </ref>. This hypothesis formally means that the next value of a stock price is its current value plus a displacement that is a zero-mean normally distributed random variable, uncorrelated with the displacements at previous moments in time.
Reference: [8] <author> Deboeck. G. J., and M. </author> <month> Cader </month> <year> (1994). </year> <title> Pre- and postprocessing of financial data, </title> <editor> In Deboeck, G.J. (Ed.) </editor> <year> (1994). </year> <title> Trading on the Edge. </title> <address> New York: </address> <publisher> Wiley., </publisher> <pages> 27-44. </pages>
Reference: [9] <author> Elbert, </author> <title> Theodore (1984). Estimation and Control of Systems. </title> <address> New York: </address> <publisher> Van Nostrand Reinhold Company. </publisher>
Reference: [10] <author> Embrechts, M. </author> <year> (1994). </year> <title> Basic Concepts of Nonlinear Dynamics and Chaos Theory, </title> <editor> In Deboeck, G.J. (Ed.) </editor> <year> (1994). </year> <title> Trading on the Edge. </title> <address> New York: </address> <publisher> Wiley., </publisher> <pages> 265-279. </pages>
Reference: [11] <author> Frison, T. </author> <year> (1994). </year> <title> Nonlinear data analysis techniques. </title> <editor> In Deboeck, G.J. (Ed.) </editor> <year> (1994). </year> <title> Trading on the Edge. </title> <address> New York: </address> <publisher> Wiley., </publisher> <pages> 280-296. </pages>
Reference-contexts: Estimating the true nonlinear dimension of the attractor (and determining if the system is indeed nonlinear of low dimensionality) has been an intractable task until the 1980s. In 1985 Grassberger and Procaccia proposed an algorithm for discovering the dimensionality of a general manifold <ref> [11] </ref>. The more computationally efficient class of box counting algorithms was introduced by Theiler to the same end in 1987 [34].
Reference: [12] <author> Gershenfeld, N.A., </author> <title> and A.S. Weigend (1994). The Future of Time Series: Learning and Understanding. In Weigend, A.S, </title> <editor> and N.A. Gershenfeld (Eds.) </editor> <title> (1994) Time Series Prediction. </title> <address> Reading, MA: </address> <publisher> Addison-Wesley, </publisher> <pages> 1-70. </pages>
Reference-contexts: Thus it follows from the theorem of Takens that an embedding dimension of 7 or more is sufficient. Second, Sauer was able to predict well this time series using a locally linear model with embedding dimension 8 <ref> [12] </ref>. As can be seen from Figure 16, the laser output is a form of oscillation, and it can be expected that a linear predictor might be at least partly successful in one-step predictions.
Reference: [13] <author> Girosi, F., and T. </author> <title> Poggio (1989). Networks and the Best Approximation Property. </title> <type> AI Memo 1164, </type> <institution> MIT AI Lab. </institution> <note> Available at ftp://ftp.ai.mit.edu/ai-publications/1000-1499/AIM-1164.ps.Z 53 </note>
Reference: [14] <author> Granger, C.W.J., and P. </author> <month> Newbold </month> <year> (1986). </year> <title> Forecasting Economic Time Series. </title> <publisher> Academic Press. </publisher>
Reference-contexts: This issue arises with linear modeling too, but there the autocorrelation function of the time-series can be used to this end [2]. Among the techniques used in nonlinear modeling are estimation of the correlation integral, the fractal dimension and various box-counting methods <ref> [34, 14] </ref>. The second major issue, which is also the topic of this thesis, is the discovery of the actual mapping between the lagged vector and the system state. In 4 practice, we are more interested in another mapping N which includes M.
Reference: [15] <author> Green, H., </author> <title> and M.Pearson (1994). Neural nets for foreign exchange trading. </title> <editor> In Deboeck, G.J. (Ed.) </editor> <year> (1994). </year> <title> Trading on the Edge. </title> <address> New York: </address> <publisher> Wiley., </publisher> <pages> pp. 123-129. </pages>
Reference: [16] <author> Hall, J. W. </author> <year> (1994). </year> <title> Adaptive selection of US stocks with neural nets, </title> <editor> In Deboeck, G.J. (Ed.) </editor> <year> (1994). </year> <title> Trading on the Edge. </title> <address> New York: </address> <publisher> Wiley., </publisher> <pages> 45-65. </pages>
Reference-contexts: The embedding dimension was set to 8 following the results of Hall, who reported that neural networks having between 4 and 8 inputs were very successful at predicting monthly and weekly stock prices <ref> [16] </ref>. This choice of embedding dimension results in an embedded time series of 232 elements, 192 of which were used for training, and 40 for testing.
Reference: [17] <author> Harvey, A.C. </author> <year> (1993). </year> <title> Time Series Models. </title> <address> Hemel Hempstead, UK: </address> <publisher> Harvester Wheatsheaf. </publisher>
Reference: [18] <author> Hertz, J.A., Anders S. Krogh, and Richard G. </author> <title> Palmer (1991). Introduction to the Theory of Neural Computation, </title> <institution> Santa Fe Institute Studies in the Sciences of Complexity, </institution> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: As the figure shows, the best generalization is achieved when the number of RBFs is about one tenth to one third of the number of training examples. This agrees well with results from MLP learning <ref> [18] </ref>. some generalization. Here the noise amplitude is 54%, which corresponds to an S/N ratio of about 2.
Reference: [19] <author> Hubner, U., C.-O. Weiss, N.B. Abraham, and D. </author> <title> Tang (1994). Lorenz-like chaos in N H 3 F IR lasers, In Weigend, A.S, </title> <editor> and N.A. Gershenfeld (Eds.) </editor> <title> (1994) Time Series Prediction. </title> <address> Reading, MA: </address> <publisher> Addison-Wesley, </publisher> <pages> 73-104. </pages>
Reference-contexts: rates is scanned and the progress of the learning is monitored so that it can be stopped before overfitting occurs. 5.3 Prediction of N H 3 F IR Laser Output The second test case was chosen to be a real physical system | a N H 3 F IR laser <ref> [19] </ref>. Part of the time series is shown in Figure 15. As it is visible, the output is a train of pulses with varying amplitude. Based on the provided time series data ([12]), one training and nine testing data sets were prepared.
Reference: [20] <author> Hutchinson, J.M. </author> <year> (1994). </year> <title> A Radial Basis Function Approach to Financial Time Series Analysis. </title> <type> Ph.D. Dissertation, </type> <institution> Department of Computer Science and Electrical Engineering, Massachusetts Institute of Technology. </institution>
Reference-contexts: Other less known, but arguably more powerful and faster learning architectures are Radial Basis Functions (RBF), and Hierarchical Mixtures of Experts (HME). RBFs have been used for time series analysis by Casdagly [4] and Hutchinson <ref> [20] </ref>. An application of HME to this task has been done by Waterhouse and Robinson [36]. These two methods will be described in greater detail in Chapters 3 and 4 respectively. Another successful extension of the classical AI methods are Fuzzy Logic Systems (FLS). <p> The chosen examples are called knots. Another approach is to perform clustering on the training set, and take as knots for the RBFs the centers of the formed clusters. This is the approach taken by Hutchison <ref> [20] </ref> for time series analysis. Whatever the approach is, let C ff are the chosen knots. The expansion will then have the form F (X) = ff=1 where K is the number of knots, K &lt; T .
Reference: [21] <author> IBM (1994). </author> <title> IBM Microelectronics ZISC zero instruction set computer: preliminary information, Supplement to Proc. </title> <booktitle> of World Congress on Neural Networks, </booktitle> <publisher> INNS Press. </publisher>
Reference-contexts: The NI1000 chip from Nestor/Intel and the ZISC chip from IBM are examples <ref> [21] </ref>. A further extension of the model, proposed by Poggio and Girosi ([28]) is to replace the L 2 norm by a weighted norm.
Reference: [22] <author> Jordan, M.I., and Jacobs, R.A. </author> <year> (1993). </year> <title> Hierarchical Mixtures of Experts and the EM Algorithm. </title> <type> AI Memo 1440, </type> <institution> MIT AI Lab. </institution> <note> Available at ftp://ftp.ai.mit.edu/ai-publications/1000-1499/AIM-1440.ps.Z </note>
Reference: [23] <author> Jordan, M.I., and L. </author> <title> Xu (1993). Convergence results for the EM approach to mixtures of experts architectures. </title> <type> AI Memo 1458, </type> <institution> MIT AI Lab. </institution> <note> Available at ftp://ftp.ai.mit.edu/ai-publications/1000-1499/AIM-1458.ps.Z </note>
Reference: [24] <author> Jordan, M.I., and Jacobs, R.A. </author> <year> (1994). </year> <title> Hierarchical mixtures of experts and the EM algorithm. </title> <journal> Neural Computation, </journal> <volume> vol. 6, </volume> <pages> 181-214. </pages>
Reference-contexts: It is obvious that a very comprehensive investigation, comprising all known adaptive computation methods is not feasible. Instead, the two most promising techniques are put to test | radial basis functions (RBF) [28, 29] and hierarchical mixtures of experts <ref> [24] </ref>. RBFs are known to possess good approximating abilities and fast learning. <p> CHAPTER 4 HIERARCHICAL MIXTURES OF EXPERTS 4.1 Computational Model In this chapter an alternative method for approximation of nonlinear functions is considered | the Hierarchical Mixture of Experts (HME) model <ref> [24] </ref>. In addition to being able to perform good nonlinear approximation, this method is able to learn time-varying functions too. The HME model, like RBFs, also extends the linear learning algorithm, described in Chapter 2, but in a way different than the one taken by the RBF approach. <p> Jordan and Jacobs subsequently refined the architecture, adding a hierarchy in the processing and called it accordingly Hierarchical Mixture of Experts (HME) <ref> [24] </ref>. 23 Network ExpertExpert Network Network ExpertExpert Network Gating Network Network Gating Gating Network X XXXX 2221 1211 yy yy 2 2 1 2|2 1|1 g g y An HME has two types of processing elements | experts and gates (Figure 6). <p> The trees do not have to be binary | any branching factor can be used. 4.2 Learning Rules Jordan and Jacobs <ref> [24] </ref> derived learning rules for the experts and the gates on the basis of a probability model and a maximum-likelihood approach. A probability model for each expert has to be defined in order to use maximum likelihood estimation. <p> (yjX; ) j g jji P ij (yjX; ) h ij = P P (4:18) Note that h ij is the product of h i and h jji . 27 Jordan and Jacobs have recently derived another set of learning rules, based on the Expectation-Maximization (EM) algorithm used in statistics <ref> [24] </ref>. These EM learning rules learn even faster than the ones based on the maximum-likelihood approach. 4.3 A Numerical Example To illustrate the operation of the learning algorithm, a two-dimensional numerical example will be developed in this section. The network consists of two experts and one gating node.
Reference: [25] <author> Koza, J. </author> <year> (1992). </year> <title> Genetic Programming: On the Programming of Computers by Means of Natural Selection. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: These systems learn fast and their operation can be explained in terms of firing rules in a fuzzy production system. Yet another AI-related method for function approximation comes from the field of Genetic Programming (GP). The respective machine learning procedure there is called symbolic regression <ref> [25] </ref>. Basic genetic algorithms have also been extensively used for financial modeling [1, 6]. 12 All these methods are examples of adaptive computation, or soft computing | that is, techniques that rely on self-modifying learning rules to approximate complex dependencies.
Reference: [26] <author> Ljung, L., and T. </author> <title> Soderstrom (1983). Theory and Practice of Recursive Identification. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: The correction of the parameter vector A is proportional to this error via a learning rate factor . For this case, the optimal value for can be determined analytically <ref> [26] </ref> but practically any small positive value will result in successful approximation of the model. Similar, although not so simple learning rules can be derived for the case of general ARMA models [26]. <p> For this case, the optimal value for can be determined analytically <ref> [26] </ref> but practically any small positive value will result in successful approximation of the model. Similar, although not so simple learning rules can be derived for the case of general ARMA models [26]. The use of such on-line algorithms is known under the name recursive system identification. 2.2 Nonlinear Systems For nearly 50 years linear models dominated entirely the field of time series prediction and modeling.
Reference: [27] <author> Mendenhall, W., D.D. Wackerly, and R.L. </author> <month> Scheaffer </month> <year> (1990). </year> <title> Mathematical Statistics with Applications. </title> <address> Boston: PWS-KENT. </address> <month> 54 </month>
Reference: [28] <author> Poggio, T., and F. </author> <title> Girosi (1989). A Theory of Networks for Approximation and Learning. </title> <type> AI Memo 1140, </type> <institution> MIT AI Lab. </institution> <note> Available at ftp://ftp.ai.mit.edu/ai-publications/1000-1499/AIM-1140.ps.Z </note>
Reference-contexts: It is obvious that a very comprehensive investigation, comprising all known adaptive computation methods is not feasible. Instead, the two most promising techniques are put to test | radial basis functions (RBF) <ref> [28, 29] </ref> and hierarchical mixtures of experts [24]. RBFs are known to possess good approximating abilities and fast learning. <p> With the development of the connectionist approach to computation, it was noticed that the computation performed by these functions can be carried out by parallel and distributed processing elements similar to MLP <ref> [28] </ref>. RBFs solve the real multivariate interpolation problem, defined as follows [28]: Given T different points fX (t) 2 R n jt = 1; T g and T real numbers fy (t) 2 Rjt = 1; T g find a function F from R n to R satisfying the interpolation conditions: <p> With the development of the connectionist approach to computation, it was noticed that the computation performed by these functions can be carried out by parallel and distributed processing elements similar to MLP <ref> [28] </ref>. RBFs solve the real multivariate interpolation problem, defined as follows [28]: Given T different points fX (t) 2 R n jt = 1; T g and T real numbers fy (t) 2 Rjt = 1; T g find a function F from R n to R satisfying the interpolation conditions: F (X (t) ) = y (t) ; t = 1; <p> Note that even if h is a linear function with respect to its argument (the distance r), it is nonetheless nonlinear with respect to X itself. Thus the RBFs provide nonlinear processing of the raw inputs. Possible and popular RBFs are the following <ref> [28] </ref>: h (r) = e ( r (gaussian) (3:3) h (r) = (c 2 + r 2 ) ff ff &gt; 0 (3:4) h (r) = r 2 + c 2 (multiquadric) (3:6) h (r) = r (linear) (3:7) where c, ff, and fi are suitable constants. <p> The solution of the interpolation problem has the following form <ref> [28] </ref>: F (X) = t=1 A polynomial term can be added to the RBF expansion too [28]. <p> The solution of the interpolation problem has the following form <ref> [28] </ref>: F (X) = t=1 A polynomial term can be added to the RBF expansion too [28]. Since the expression above is linear in the coefficients ! t , the RBF approach can be considered to be a generalized least squares algorithm, and solved as a 15 system of linear equations. <p> The most we can hope for is to solve it in a least-squares sense. This can be done in many ways, for example with Singular Value Decomposition ([30]) or by using the Moore-Pensrose pseudoinverse <ref> [28] </ref>. With the latter approach, the solution for the expansion coefficients ! ff is given by ([28]) = H + Y (3:13) where (H) iff = h (k X (i) C ff k) and the pseudoinverse of H is given by ([28]) H + = (H T H) 1 H T <p> In the case when the centers are less than the number of training examples, the network is called a generalized RBF network. It should be noted that this computing architecture bears much more resemblance to natural neural circuits than the MLP architecture <ref> [28] </ref>. Thus, if any parallel distributed architecture has to be called "neural network", RBF networks deserve to be called so much more than MLPs. 17 n3 2 1 h hh C RBF networks can be readily implemented in VLSI hardware.
Reference: [29] <author> Poggio, T., </author> <title> and F.Girosi (1990). Regularization algorithms for learning that are equivalent to multilayer networks. </title> <journal> Science, </journal> <volume> vol. 247, </volume> <pages> 978-982. </pages>
Reference-contexts: It is obvious that a very comprehensive investigation, comprising all known adaptive computation methods is not feasible. Instead, the two most promising techniques are put to test | radial basis functions (RBF) <ref> [28, 29] </ref> and hierarchical mixtures of experts [24]. RBFs are known to possess good approximating abilities and fast learning.
Reference: [30] <editor> Press, W.H et al. </editor> <year> (1992). </year> <title> Numerical Recipes in C, </title> <publisher> Cambridge University Press. </publisher>
Reference: [31] <editor> David Rumelhart, Jay MacClelland, (eds.) </editor> <booktitle> (1986), Parallel Distributed Processing: Explorations in the Microstructure of Cognition, vol. </booktitle> <address> 1,2, Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: learning methods of the classical symbolic AI that can discover Boolean functions from examples, do not extend to real-valued functions either. 11 The new field of connectionism was born in the mid-1980s from the merging of ideas from classical AI, neural science, very large scale integration (VLSI) circuits, and statistics <ref> [31] </ref>. The central tool in connectionism is the Universal Function Approximator (UFA) in the form of parallel and distributed parametrized computational architectures. The best known connectionist architectures are the Multi-Layer Perceptrons (MLP), popularly known as "neural networks".
Reference: [32] <author> Sauer, T. </author> <year> (1994). </year> <title> Time series prediction by using delay coordinate embedding. In Weigend, A.S, </title> <editor> and N.A. Gershenfeld (Eds.) </editor> <title> (1994) Time Series Prediction. </title> <address> Reading, MA: </address> <publisher> Addison-Wesley, </publisher> <pages> 175-193. </pages>
Reference: [33] <author> Takens, F. </author> <year> (1981). </year> <title> Detecting strange attractors in turbulence. In D.A. </title> <editor> Rand and L.-S. Young (eds.), </editor> <booktitle> Dynamical Systems and Turbulence, Lecture Notes in Mathematics, </booktitle> <volume> Vol. 898, </volume> <pages> 336-381. </pages> <address> Berlin: </address> <publisher> Springer. </publisher>
Reference-contexts: These techniques convincingly outperform linear modeling for many complex applications [37]. In 1981 F.Takens proved an important theorem which justifies the use of lagged (delayed) values of the time series variable to reconstruct the state space of the underlying dynamic system <ref> [33] </ref>. According to the theorem, if the dimensionality of the system is d, a vector Y consisting of the most recent 2d + 1 values of the measured variable y can be used to identify unambiguously the true state of the system X at that moment of time.
Reference: [34] <author> Theiler, J. </author> <year> (1987). </year> <title> Efficient algorithm for estimating the correlation dimension from a set of discrete points. </title> <journal> Physical Review A, </journal> <volume> vol. 36, </volume> <pages> 4456-4462. </pages>
Reference-contexts: This issue arises with linear modeling too, but there the autocorrelation function of the time-series can be used to this end [2]. Among the techniques used in nonlinear modeling are estimation of the correlation integral, the fractal dimension and various box-counting methods <ref> [34, 14] </ref>. The second major issue, which is also the topic of this thesis, is the discovery of the actual mapping between the lagged vector and the system state. In 4 practice, we are more interested in another mapping N which includes M. <p> In 1985 Grassberger and Procaccia proposed an algorithm for discovering the dimensionality of a general manifold [11]. The more computationally efficient class of box counting algorithms was introduced by Theiler to the same end in 1987 <ref> [34] </ref>.
Reference: [35] <author> Wan, E.A. </author> <year> (1994). </year> <title> Time series prediction by using a connectionist network with internal delay lines. In Weigend, A.S, </title> <editor> and N.A. Gershenfeld (Eds.) </editor> <title> (1994) Time Series Prediction. </title> <address> Reading, MA: </address> <publisher> Addison-Wesley, </publisher> <pages> 195-217. </pages>
Reference-contexts: We can use to this end techniques from the area of nonsymbolic artificial intelligence (AI), which are also called adaptive computation techniques. The computational models most often used in function approximation are multilayer perceptrons (MLP) <ref> [35] </ref> and radial basis functions (RBF) [4]. The utility of these techniques is that they can infer nonlinear dependencies between data variables that take on real values. Their predecessors are the traditional symbolic learning methods in AI that work with Boolean variables only. <p> The best known connectionist architectures are the Multi-Layer Perceptrons (MLP), popularly known as "neural networks". A type of MLP, called Time-Delay Neural Network (TDNN) has been very successfully used for time series analysis and prediction <ref> [35] </ref>. The problem with MLP is that the learning is very slow | on the order of hundreds of thousands of iterations through the training set, while linear algorithms need typically one to ten iterations to converge.
Reference: [36] <author> Waterhouse, S.R., and A.J. </author> <title> Robinson (1995). Non-linear prediction of acoustic vectors using hierarchical mixtures of experts, </title> <editor> in G. Tesauro, D.S. Touretzky, and T.K. Leen, (Eds.) </editor> <booktitle> Neural Information Processing Systems 7, </booktitle> <address> Cambridge, Massachusetts: </address> <publisher> MIT Press. </publisher>
Reference-contexts: RBFs have been used for time series analysis by Casdagly [4] and Hutchinson [20]. An application of HME to this task has been done by Waterhouse and Robinson <ref> [36] </ref>. These two methods will be described in greater detail in Chapters 3 and 4 respectively. Another successful extension of the classical AI methods are Fuzzy Logic Systems (FLS). These systems learn fast and their operation can be explained in terms of firing rules in a fuzzy production system.
Reference: [37] <author> Weigend, </author> <title> A.S, </title> <editor> and N.A. Gershenfeld (Eds.) </editor> <title> (1994) Time Series Prediction. </title> <address> Reading, MA: </address> <publisher> Addison-Wesley </publisher>
Reference-contexts: It has been only recently that notable progress in nonlinear modeling has been possible. New developments in two areas | nonlinear dynamics and nonsymbolic artificial intelligence (AI) have made possible the creation of general nonlinear modeling techniques. These techniques convincingly outperform linear modeling for many complex applications <ref> [37] </ref>. In 1981 F.Takens proved an important theorem which justifies the use of lagged (delayed) values of the time series variable to reconstruct the state space of the underlying dynamic system [33].
Reference: [38] <author> White, A. </author> <year> (1993). </year> <title> Economic prediction using neural networks: the case of IBM daily stock returns. In R.R. </title> <editor> Trippi and E. Turban (Eds.), </editor> <booktitle> Neural Networks in Finance and Investing, </booktitle> <address> Chicago: Phobus. </address>
Reference: [39] <author> Yoda, M. </author> <year> (1994). </year> <title> Predicting the Tokyo stock market. </title> <editor> In Deboeck, G.J. (Ed.) </editor> <year> (1994). </year> <title> Trading on the Edge. </title> <address> New York: </address> <publisher> Wiley., </publisher> <pages> 66-79. </pages> <institution> VITA Graduate School Southern Illinois University Daniel Nikolaev Nikovski Date of Birth: </institution> <address> April 13, 1969 606 West College Street, Apt.4, Rm. 6, Carbondale, Illinois 62901 150 Hristo Botev Boulevard, Apt. </address> <month> 54, </month> <title> 4004 Plovdiv, Bulgaria Technical University - Sofia, Bulgaria Engineer of Computer Systems and Control Thesis Title: Adaptive Computation Techniques for Time Series Analysis Major Professor: </title> <journal> Dr. Mehdi Zargham </journal>
References-found: 39

