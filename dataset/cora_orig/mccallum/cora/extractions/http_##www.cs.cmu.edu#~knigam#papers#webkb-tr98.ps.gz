URL: http://www.cs.cmu.edu/~knigam/papers/webkb-tr98.ps.gz
Refering-URL: http://www.cs.cmu.edu/~knigam/
Root-URL: http://www.cs.cmu.edu/~jr6b
Title: Learning to Extract Symbolic Knowledge from the World Wide Web  
Author: Mark Craven Dan DiPasquo Dayne Freitag Andrew McCallum Tom Mitchell Kamal Nigam Sean Slattery 
Note: This research has been supported in part by the DARPA HPKB program under research contract F30602-97-1-0215.  
Address: Pittsburgh, PA 15213  
Affiliation: School of Computer Science Carnegie Mellon University  
Date: September 1, 1998  
Pubnum: CMU-CS-98-122  
Abstract: The World Wide Web is a vast source of information accessible to computers, but understandable only to humans. The goal of the research described here is to automatically create a computer understandable knowledge base whose content mirrors that of the World Wide Web. Such a knowledge base would enable much more effective retrieval of Web information, and promote new uses of the Web to support knowledge-based inference and problem solving. Our approach is to develop a trainable information extraction system that takes two inputs. The first is an ontology that defines the classes (e.g., Company, Person, Employee, Product) and relations (e.g., Employed.By, Produced.By) of interest when creating the knowledge base. The second is a set of training data consisting of labeled regions of hypertext that represent instances of these classes and relations. Given these inputs, the system learns to extract information from other pages and hyperlinks on the Web. This paper describes our general approach, several machine learning algorithms for this task, and promising initial results with a prototype system that has created a knowledge base describing university people, courses, and research projects. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> Proceedings of the Fourth Message Understanding Conference (MUC-4), </institution> <address> McLean, Vir-ginia, June 1992. </address> <publisher> Morgan Kaufmann Publisher, Inc., </publisher> <address> San Francisco. </address>
Reference-contexts: These studies indicate that these algorithms are quite competitive with statistical-based methods. 8.2. Information Extraction The problem that we are addressing is related to the traditional information extraction task, such as the research done in the Message Understanding (MUC) <ref> [1, 2] </ref> community. The work in the MUC community has considered problems such as extracting symbolic descriptions of terrorist attacks from news articles, constructing case frames that indicate fields such as the Perpetrator, Victim, etc.
Reference: [2] <institution> Proceedings of the Fifth Message Understanding Conference (MUC-5), </institution> <address> Baltimore, Mary-land, August 1993. </address> <publisher> Morgan Kaufmann Publisher, Inc., </publisher> <address> San Francisco. </address>
Reference-contexts: These studies indicate that these algorithms are quite competitive with statistical-based methods. 8.2. Information Extraction The problem that we are addressing is related to the traditional information extraction task, such as the research done in the Message Understanding (MUC) <ref> [1, 2] </ref> community. The work in the MUC community has considered problems such as extracting symbolic descriptions of terrorist attacks from news articles, constructing case frames that indicate fields such as the Perpetrator, Victim, etc.
Reference: [3] <institution> The Fourth Text Retrieval Cconference. National Technical Information Services, Springfield, VA, </institution> <year> 1995. </year> <note> http://www-nlpir.nist.gov/TREC/t4"_proceedings.html. </note>
Reference-contexts: In this section we briefly review the main areas of related work. 8.1. Document Classification Our work is related to research in document classification, such as that reported at recent Text REtrieval Conferences (TREC) <ref> [3, 4] </ref>. A wide variety of methods have been applied to the document-classification task. The TFIDF approach to information retrieval is the basis for the Rocchio classification algorithm which has become a standard baseline algorithm for text classification [8, 15, 32].
Reference: [4] <institution> The Fifth Text Retrieval Cconference. National Technical Information Services, Spring-field, VA, </institution> <year> 1996. </year> <note> http://www-nlpir.nist.gov/TREC/t5"_proceedings.html. </note>
Reference-contexts: In this section we briefly review the main areas of related work. 8.1. Document Classification Our work is related to research in document classification, such as that reported at recent Text REtrieval Conferences (TREC) <ref> [3, 4] </ref>. A wide variety of methods have been applied to the document-classification task. The TFIDF approach to information retrieval is the basis for the Rocchio classification algorithm which has become a standard baseline algorithm for text classification [8, 15, 32].
Reference: [5] <author> C. Apte, F. Damerau, and S. M. Weiss. </author> <title> Automated learning of decision rules for text categorization. </title> <journal> ACM Transactions on Information Systems, </journal> <volume> 12(3) </volume> <pages> 233-251, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: Another useful line of research in text classification comes from basic ideas in probability and information theory. Bayes Rule has been the starting point for a number of classification algorithms <ref> [5, 6, 33, 35, 43, 46] </ref>, and the Minimum Description Length principle has been used as the basis of an algorithm as well [32]. 36 Another line of research has been to use symbolic learning methods for text classification. <p> Numerous studies have used algorithms such as decision trees, Swap-1, Ripper and Charade can be found in <ref> [5, 6, 8, 13, 34, 35, 43, 44, 46, 61] </ref>. These studies indicate that these algorithms are quite competitive with statistical-based methods. 8.2.
Reference: [6] <author> C. Apte, F. Damerau, and S. M. Weiss. </author> <title> Towards language independent automated learning of text categorization models. </title> <type> Technical report, </type> <institution> IBM, </institution> <year> 1994. </year> <note> (TR 19481) (also appeared in SIGIR94). </note>
Reference-contexts: Another useful line of research in text classification comes from basic ideas in probability and information theory. Bayes Rule has been the starting point for a number of classification algorithms <ref> [5, 6, 33, 35, 43, 46] </ref>, and the Minimum Description Length principle has been used as the basis of an algorithm as well [32]. 36 Another line of research has been to use symbolic learning methods for text classification. <p> Numerous studies have used algorithms such as decision trees, Swap-1, Ripper and Charade can be found in <ref> [5, 6, 8, 13, 34, 35, 43, 44, 46, 61] </ref>. These studies indicate that these algorithms are quite competitive with statistical-based methods. 8.2.
Reference: [7] <author> M. Balabanovic and Y. Shoham. </author> <title> Learning information retrieval agents: Experiments with automated web browsing. </title> <booktitle> In AAAI Spring Symposium on Information Gathering from Heterogeneous, Distributed Environments, </booktitle> <year> 1995. </year>
Reference-contexts: The system uses the ratings to learn a user specific topic profile that can be used to suggest unexplored hyperlinks on the page. Syskill and Webert can also use search engines like LYCOS to retrieve pages by turning the topic profile into a query. Lira <ref> [7] </ref> works in an off-line setting. A general model of one user's interest is learned by asking the user to rate pages. Lira uses the model to browse the Web off-line and returns a set of pages that match the user's interest.
Reference: [8] <author> E. Bloedorn, I. Mani, and T. R. </author> <title> MacMillan. Machine learning of user profiles: Representational issues. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 433-438. </pages> <publisher> AAAI/MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: A wide variety of methods have been applied to the document-classification task. The TFIDF approach to information retrieval is the basis for the Rocchio classification algorithm which has become a standard baseline algorithm for text classification <ref> [8, 15, 32] </ref>. Its "word-vector" approach involves describing classes with a vector of weights, where each weight indicates how important the corresponding word is to the class. <p> Numerous studies have used algorithms such as decision trees, Swap-1, Ripper and Charade can be found in <ref> [5, 6, 8, 13, 34, 35, 43, 44, 46, 61] </ref>. These studies indicate that these algorithms are quite competitive with statistical-based methods. 8.2.
Reference: [9] <author> A. Blum and T. Mitchell. </author> <title> Combining labeled and unlabeled data with co-training. </title> <booktitle> In Proceedings of the 11th Annual Conference on Computational Learning Theory. ACM, </booktitle> <year> 1998. </year>
Reference-contexts: In recent work, we have proposed a method by which each classifier acts as a trainer for the other, and we have provided initial experiments and theoretical analysis showing the promise of this approach <ref> [9] </ref>. * Exploit more linguistic structure. We plan to explore ways in which noun, verb, and prepositional phrases extracted from the text can be used as features for information extraction.
Reference: [10] <author> T. Bray, J. Paoli, and C. M. Sperberg-McQueen. </author> <title> Extensible markup language (XML) 1.0. Technical Report REC-xml-19980210, World Wide Web Consortium, </title> <note> 1998. http://www.w3.org/TR/REC-xml. </note>
Reference-contexts: START builds its knowledge base by discovering mostly manually added natural language annotations on Web pages. The most significant recent development in this area is the advent of Extensible Markup 38 Language (XML) <ref> [10] </ref>. Whereas HTML is designed to describe the layout of information in a page, XML can be used to describe information about the contents of the page. As with SHOE, Web page authors can use XML to encode ontological information about their pages.
Reference: [11] <author> M. E. Califf and R. J. Mooney. </author> <title> Relational learning of pattern-match rules for information extraction. </title> <booktitle> In Working Papers of ACL-97 Workshop on Natural Language Learning, </booktitle> <year> 1997. </year>
Reference-contexts: These patterns are then reviewed and manually installed into a larger information extraction system. AutoSlog-TS [53] removes the requirement that documents be annotated. CRYSTAL [58] and RAPIER <ref> [11] </ref> both demonstrate that machine learning techniques can be used to learn rules that perform extraction autonomously. CRYSTAL is a covering algorithm which takes parsed, annotated sentences as input and produces rules for extracting from novel sentences.
Reference: [12] <author> B. Cestnik. </author> <title> Estimating probabilities: A crucial task in machine learning. </title> <editor> In L. Aiello, editor, </editor> <booktitle> Proceedings of the Ninth European Conference on Artificial Intelligence (ECAI-90), </booktitle> <pages> pages 147-150, </pages> <address> Stockholm, Sweden, 1990. </address> <publisher> Pitman. </publisher>
Reference-contexts: We use the following procedure to calculate the confidence of each of our predictions. First, we estimate the error rate of each of our learned clauses by calculating an m-estimate <ref> [12] </ref> (with m = 2) of the rule's error over the training examples.
Reference: [13] <author> W. W. Cohen. </author> <title> Fast effective rule induction. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: Numerous studies have used algorithms such as decision trees, Swap-1, Ripper and Charade can be found in <ref> [5, 6, 8, 13, 34, 35, 43, 44, 46, 61] </ref>. These studies indicate that these algorithms are quite competitive with statistical-based methods. 8.2.
Reference: [14] <author> T. H. Cover and J. A. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> John Wiley and Sons, Inc., </publisher> <year> 1991. </year> <month> 45 </month>
Reference-contexts: i jc) N (w i ;d) (12) T X N (w i ; d) log (Pr (w i jc)) (13) log (Pr (c)) + i=1 n If we interpret N (w i ; d)=n as Pr (w i jd), the right-hand term of this expression is the negative Cross Entropy <ref> [14] </ref> between the distribution of words induced by the document with the distribution of words induced by the class: log (Pr (c)) + i=1 Thus, the second term specifies that the class c with the highest score will be the one with the lowest Cross Entropy|the class that could "compress" the
Reference: [15] <editor> H.C.M. de Kroon, T.M. Mitchell, and E.J.H Kerckhoffs. </editor> <title> Improving learning accuracy in information filtering. </title> <booktitle> In International Conference on Machine Learning Workshop on Machine Learning Meets HCI (ICML-96), </booktitle> <year> 1996. </year>
Reference-contexts: A wide variety of methods have been applied to the document-classification task. The TFIDF approach to information retrieval is the basis for the Rocchio classification algorithm which has become a standard baseline algorithm for text classification <ref> [8, 15, 32] </ref>. Its "word-vector" approach involves describing classes with a vector of weights, where each weight indicates how important the corresponding word is to the class.
Reference: [16] <author> D. DiPasquo. </author> <title> Using HTML formatting to aid in natural language processing on the World Wide Web, </title> <month> June </month> <year> 1998. </year> <type> Senior thesis, </type> <institution> Computer Science Department, Carnegie Mellon University (http://www.cs.cmu.edu/ WebKB/danthesis.ps.gz). </institution>
Reference-contexts: We plan to investigate the utility of representing the HTML structure of pages when learning rules for relation classification and information extraction. We have investigated one approach to representing HTML structure and exploiting it for learning tasks <ref> [16] </ref>. * Learn regularities over the growing knowledge base. We plan to use learning methods to discover interesting regularities over the facts that have been extracted from the Web, and to use these learned facts to improve future fact extraction.
Reference: [17] <author> P. Domingos and M. Pazzani. </author> <title> On the optimality of the simple Bayesian classifier under zero-one loss. </title> <journal> Machine Learning, </journal> <volume> 29 </volume> <pages> 103-130, </pages> <year> 1997. </year>
Reference-contexts: The independence assumption is clearly false, and it produces obviously incorrect class-membership probabilities. However, it is often the case that even when the assumption does not hold and Naive Bayes produces inaccurate probability estimates, it still is able to classify test examples with high accuracy <ref> [17] </ref>. There are two common approaches to naive Bayes text classification. One, the multi-variate Bernoulli model, is a Bayesian Network with no dependencies between words and binary word counts; the document is considered to be the "event" and the words are features 12 of that event.
Reference: [18] <author> R. Doorenbos, O. Etzioni, and D. S. Weld. </author> <title> A scalable comparison-shopping agent for the world-wide web. </title> <booktitle> In Proceedings of the 1st International Conference on Autonomous Agents. ACM, </booktitle> <year> 1997. </year>
Reference-contexts: ILA exploits prior expectations about the probable contents of a database to learn how records are formatted for output by a particular system. Similar, but specifically designed for use with HTML is Shopbot <ref> [18] </ref>, a bargain hunting agent. These approaches are related to the general problem of "wrapper induction" [31], learning extraction patterns for highly regular sources. At the same time, ideas that have proven useful for general text have also been shown to work well for Web pages.
Reference: [19] <author> S. Dzeroski and I. Bratko. </author> <title> Handling noise in inductive logic programming. </title> <editor> In S.H. Mug-gleton and K. Furukawa, editors, </editor> <booktitle> Proceedings of the Second International Workshop on Inductive Logic Programming (ILP-92), number TM-1182 in ICOT Technical Memorandum, </booktitle> <pages> pages 109-125, </pages> <address> Tokyo, Japan, </address> <year> 1992. </year> <institution> Institute for New Generation Computer Technology. </institution>
Reference-contexts: This bias takes advantage of domain knowledge which is present in the ontology given to the WebKB system. The second difference between our relation-learning algorithm and Foil is that whereas Foil uses an information-theoretic measure to guide its hill-climbing search, our method, like Dzeroski and Bratko's m-Foil <ref> [19] </ref>, uses m-estimates of a clause's error to guide its construction. We have found that using this evaluation function causes the algorithm to learn fewer, more general clauses than when Foil's information gain measure is used. 6.3.
Reference: [20] <author> D. Freitag. </author> <title> Using grammatical inference to improve precision in information extraction. In ICML-97 Workshop on Automation Induction, Grammatical Inference, and Language Acquisition, </title> <address> Nashville, TN, 1997. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: We conjecture that such linguistic features will be even more useful for tasks with few words, such as classifying individual hyperlinks. * Explore multiple strategies for learning to extract text fields from Web pages. We have developed a number of approaches to this task <ref> [20, 21, 23] </ref>, including multi-strategy learning [22]. * Integrate statistical bag-of-words methods into first-order learning tasks.
Reference: [21] <author> D. Freitag. </author> <title> Information extraction from HTML: Application of a general learning approach. </title> <booktitle> In Proceedings of the Fifteenth National Conference on Artificial Intelligence, </booktitle> <address> Madison, WI, 1998. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: We conjecture that such linguistic features will be even more useful for tasks with few words, such as classifying individual hyperlinks. * Explore multiple strategies for learning to extract text fields from Web pages. We have developed a number of approaches to this task <ref> [20, 21, 23] </ref>, including multi-strategy learning [22]. * Integrate statistical bag-of-words methods into first-order learning tasks.
Reference: [22] <author> D. Freitag. </author> <title> Multistrategy learning for information extraction. </title> <booktitle> In Proceedings of the 15th International Conference on Machine Learning, </booktitle> <pages> pages 161-169. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1998. </year>
Reference-contexts: We have developed a number of approaches to this task [20, 21, 23], including multi-strategy learning <ref> [22] </ref>. * Integrate statistical bag-of-words methods into first-order learning tasks. We have begun developing methods that augment first-order learning with the ability to use bag 41 of-words classifiers to invent new predicates for characterizing the pages and hyperlinks referenced in learned rules [57]. * Exploit more HTML structure.
Reference: [23] <author> D. Freitag. </author> <title> Toward general-purpose learning for information extraction. </title> <booktitle> In Proceedings of COLING/ACL-98, </booktitle> <year> 1998. </year>
Reference-contexts: We conjecture that such linguistic features will be even more useful for tasks with few words, such as classifying individual hyperlinks. * Explore multiple strategies for learning to extract text fields from Web pages. We have developed a number of approaches to this task <ref> [20, 21, 23] </ref>, including multi-strategy learning [22]. * Integrate statistical bag-of-words methods into first-order learning tasks.
Reference: [24] <author> J. Fuernkranz, T. Mitchell, and E. Riloff. </author> <title> A case study in using linguistic phrases for text categorization of the www. In Working Notes of the AAAI/ICML Workshop on Learning for Text Categorization, </title> <note> 1998. http://www.cs.cmu.edu/ WebKB/aaai-ws-aslog.ps.gz. </note>
Reference-contexts: We plan to explore ways in which noun, verb, and prepositional phrases extracted from the text can be used as features for information extraction. We have conducted preliminary experiments that show improved accuracy in some cases when our bag of words representation is augmented by these extracted phrases <ref> [24] </ref>. We conjecture that such linguistic features will be even more useful for tasks with few words, such as classifying individual hyperlinks. * Explore multiple strategies for learning to extract text fields from Web pages.
Reference: [25] <author> T. Joachims. </author> <title> A probabilistic analysis of the Rocchio algorithm with TFIDF for text categorization. </title> <booktitle> In Proceedings of the Fourteenth International Conference on Machine Learning, </booktitle> <pages> pages 143-151, </pages> <address> Nashville, TN, 1997. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: (c) log (Pr (c)) (8) X Pr (v i ) c2C = v i 2fw i ;:w i g c2C Pr (c) Pr (v i ) (10) This feature selection method has been found to perform best among several alternatives [63], and has been used in many text classification studies <ref> [25, 29, 30, 45, 40] </ref>. 5.1.2. Experimental Evaluation We evaluate our method using the cross-validation methodology described in Section 4. On each iteration of the cross-validation run, we train a classifier for each of the page representations described at the beginning of this section: full-Text, title/heading, and hyperlink.
Reference: [26] <author> T. Joachims, D. Freitag, and T. Mitchell. Webwatcher: </author> <title> A tour guide for the World Wide Web. </title> <booktitle> In Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 770-775, </pages> <address> Nagoya, Japan, 1997. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 46 </pages>
Reference-contexts: Many other Web agents have been developed over the past few years, including several that involve some form of learning. However, the vast majority of these systems use learning to improve their ability to retrieve text information, rather that to extract computer-understandable information. For example, Joachims et al. <ref> [26] </ref> describe a Web agent called WebWatcher that serves as a tour guide for users browsing the Web. WebWatcher learns to suggest appropriate hyperlinks given users' interests, based on the hyperlinks followed by previous users with similar interests.
Reference: [27] <author> B. Katz. </author> <title> From sentence processing to information access on the World Wide Web. </title> <booktitle> In Proceedings of the AAAI Spring Symposium on Natural Language Processing for the World Wide Web, </booktitle> <year> 1997. </year>
Reference-contexts: Their hope is that a library of standard ontologies will come into common usage, enabling agents such as Expose to learn the information encoded on the Web. The START Information Server <ref> [27] </ref> provides a natural language interface to a knowledge base collected from the Web. The knowledge base contains meta-information about the content of the Web, so that a query to START returns relevant hypertext segments.
Reference: [28] <author> J.-T. Kim and D. I. Moldovan. </author> <title> Acquisition of linguistic patterns for knowledge-based information extraction. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <year> 1995. </year>
Reference-contexts: In addition, our approach relies heavily on machine learning methods that can be trained to extract information, whereas most early work in the MUC community relied on hand-crafted methods for extracting information. Recently, the problem of using machine-learning methods to induce information-extraction routines has received more attention. PALKA <ref> [28] </ref> and AutoSlog [54] are machine learning systems which learn extraction patterns from collections of parsed documents that have been annotated to identify fragments of interest. These patterns are then reviewed and manually installed into a larger information extraction system. AutoSlog-TS [53] removes the requirement that documents be annotated.
Reference: [29] <author> D. Koller and M. Sahami. </author> <title> Toward optimal feature selection. </title> <booktitle> In Proceedings of Thirteenth International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1996. </year>
Reference-contexts: (c) log (Pr (c)) (8) X Pr (v i ) c2C = v i 2fw i ;:w i g c2C Pr (c) Pr (v i ) (10) This feature selection method has been found to perform best among several alternatives [63], and has been used in many text classification studies <ref> [25, 29, 30, 45, 40] </ref>. 5.1.2. Experimental Evaluation We evaluate our method using the cross-validation methodology described in Section 4. On each iteration of the cross-validation run, we train a classifier for each of the page representations described at the beginning of this section: full-Text, title/heading, and hyperlink.
Reference: [30] <author> D. Koller and M. Sahami. </author> <title> Hierarchically classifying documents using very few words. </title> <booktitle> In Proceedings of the Fourteenth International Conference on Machine Learning, </booktitle> <pages> pages 170-178, </pages> <address> Nashville, TN, 1997. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: (c) log (Pr (c)) (8) X Pr (v i ) c2C = v i 2fw i ;:w i g c2C Pr (c) Pr (v i ) (10) This feature selection method has been found to perform best among several alternatives [63], and has been used in many text classification studies <ref> [25, 29, 30, 45, 40] </ref>. 5.1.2. Experimental Evaluation We evaluate our method using the cross-validation methodology described in Section 4. On each iteration of the cross-validation run, we train a classifier for each of the page representations described at the beginning of this section: full-Text, title/heading, and hyperlink.
Reference: [31] <author> N. Kushmerick. </author> <title> Wrapper Induction for Information Extraction. </title> <type> PhD thesis, </type> <institution> University of Washington, </institution> <year> 1997. </year> <note> Tech Report UW-CSE-97-11-04. </note>
Reference-contexts: ILA exploits prior expectations about the probable contents of a database to learn how records are formatted for output by a particular system. Similar, but specifically designed for use with HTML is Shopbot [18], a bargain hunting agent. These approaches are related to the general problem of "wrapper induction" <ref> [31] </ref>, learning extraction patterns for highly regular sources. At the same time, ideas that have proven useful for general text have also been shown to work well for Web pages. Webfoot [59] is a modification of CRYSTAL in which parsed sentence fragments are replaced by segments of HTML. 37 8.3.
Reference: [32] <author> K. Lang. NewsWeeder: </author> <title> Learning to filter Netnews. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: A wide variety of methods have been applied to the document-classification task. The TFIDF approach to information retrieval is the basis for the Rocchio classification algorithm which has become a standard baseline algorithm for text classification <ref> [8, 15, 32] </ref>. Its "word-vector" approach involves describing classes with a vector of weights, where each weight indicates how important the corresponding word is to the class. <p> Bayes Rule has been the starting point for a number of classification algorithms [5, 6, 33, 35, 43, 46], and the Minimum Description Length principle has been used as the basis of an algorithm as well <ref> [32] </ref>. 36 Another line of research has been to use symbolic learning methods for text classification. Numerous studies have used algorithms such as decision trees, Swap-1, Ripper and Charade can be found in [5, 6, 8, 13, 34, 35, 43, 44, 46, 61].
Reference: [33] <author> D. Lewis and W. A. Gale. </author> <title> A sequential algorithm for training text classifiers. </title> <booktitle> In Proceedings of the Seventeenth Annual International ACM SIGIR Conference on Research and Development in Inforamtion Retrieval (SIGIR94), </booktitle> <year> 1994. </year>
Reference-contexts: Another useful line of research in text classification comes from basic ideas in probability and information theory. Bayes Rule has been the starting point for a number of classification algorithms <ref> [5, 6, 33, 35, 43, 46] </ref>, and the Minimum Description Length principle has been used as the basis of an algorithm as well [32]. 36 Another line of research has been to use symbolic learning methods for text classification.
Reference: [34] <author> D. Lewis, R. E. Shapire, J. P. Callan, and R. Papka. </author> <title> Training algorithms for linear text classifiers. </title> <booktitle> In Proceedings of the Nineteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 298-306, </pages> <year> 1996. </year>
Reference-contexts: This representation has been used with many different learning algorithms, including memory based reasoning [38], neural networks [46, 55], linear discriminant analysis [55], logistic regression [55], Widrow-Hoff and the exponentiated gradient (EG) algorithm <ref> [34] </ref>. Another useful line of research in text classification comes from basic ideas in probability and information theory. <p> Numerous studies have used algorithms such as decision trees, Swap-1, Ripper and Charade can be found in <ref> [5, 6, 8, 13, 34, 35, 43, 44, 46, 61] </ref>. These studies indicate that these algorithms are quite competitive with statistical-based methods. 8.2.
Reference: [35] <author> D. D. Lewis and M. Ringuette. </author> <title> A comparison of two learning algorithms for text categorization. </title> <booktitle> In Third Annual Symposium on Document Analysis and Information Retrieval, </booktitle> <pages> pages 81-93, </pages> <year> 1994. </year>
Reference-contexts: Another useful line of research in text classification comes from basic ideas in probability and information theory. Bayes Rule has been the starting point for a number of classification algorithms <ref> [5, 6, 33, 35, 43, 46] </ref>, and the Minimum Description Length principle has been used as the basis of an algorithm as well [32]. 36 Another line of research has been to use symbolic learning methods for text classification. <p> Numerous studies have used algorithms such as decision trees, Swap-1, Ripper and Charade can be found in <ref> [5, 6, 8, 13, 34, 35, 43, 44, 46, 61] </ref>. These studies indicate that these algorithms are quite competitive with statistical-based methods. 8.2.
Reference: [36] <author> H. Lieberman. Letizia: </author> <title> An agent that assists Web browsing. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 924-929. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: As such, it involves learning to classify hyperlinks a task similar to the work reported here on learning to extract relational information. A system with a similar goal is Letizia <ref> [36] </ref>, which learns the interests of a single user, in contrast to WebWatcher which learns from a community of users. Syskill and Webert [46] offers a more restricted way of browsing than WebWatcher and Letizia.
Reference: [37] <author> S. Luke, L. Spector, D. Rager, and J. Hendler. </author> <title> Ontology-based Web agents. </title> <editor> In W. L. Johnson, editor, </editor> <booktitle> Proceedings of the 1st International Conference on Autonomous Agents, </booktitle> <pages> pages 59-66. </pages> <publisher> ACM, </publisher> <year> 1997. </year>
Reference-contexts: Extracting Knowledge Bases from the Web Other groups have worked on extracting propositional knowledge-base information from the Web. Luke et al. <ref> [37] </ref> have proposed an extension to HTML called SHOE whereby Web page authors can encode ontological information on their pages. The have also developed a system, Expose, that extracts SHOE-encoded information from Web pages, and stored it in a local knowledge base.
Reference: [38] <author> B. Masand, G. Linoff, and D. Waltz. </author> <title> Classifying news stories using memory based reasoning. </title> <booktitle> In Proceedings of the Fifteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR92), </booktitle> <pages> pages 59-65, </pages> <year> 1992. </year>
Reference-contexts: Its "word-vector" approach involves describing classes with a vector of weights, where each weight indicates how important the corresponding word is to the class. This representation has been used with many different learning algorithms, including memory based reasoning <ref> [38] </ref>, neural networks [46, 55], linear discriminant analysis [55], logistic regression [55], Widrow-Hoff and the exponentiated gradient (EG) algorithm [34]. Another useful line of research in text classification comes from basic ideas in probability and information theory.
Reference: [39] <author> A. McCallum and K. Nigam. </author> <title> Employing EM in pool-based active learning for text classification. </title> <booktitle> In Proceedings of the 15th International Conference on Machine Learning, </booktitle> <pages> pages 350-358. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1998. </year> <month> 47 </month>
Reference-contexts: We are also exploring the combination of EM with pool-based training for active learning in which the learner requests labels for specific Web pages whose label will be especially helpful <ref> [39] </ref>. * Co-training multiple classifiers. For example, consider a problem setting in which one Web page classifier examines the words on the page, and a second classifier examines instead the words on the incoming hyperlinks to that page.
Reference: [40] <author> A. McCallum, R. Rosenfeld, T. Mitchell, and A. Ng. </author> <title> Improving text clasification by shrinkage in a hierarchy of classes. </title> <booktitle> In Proceedings of the 15th International Conference on Machine Learning, </booktitle> <pages> pages 359-367. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1998. </year>
Reference-contexts: (c) log (Pr (c)) (8) X Pr (v i ) c2C = v i 2fw i ;:w i g c2C Pr (c) Pr (v i ) (10) This feature selection method has been found to perform best among several alternatives [63], and has been used in many text classification studies <ref> [25, 29, 30, 45, 40] </ref>. 5.1.2. Experimental Evaluation We evaluate our method using the cross-validation methodology described in Section 4. On each iteration of the cross-validation run, we train a classifier for each of the page representations described at the beginning of this section: full-Text, title/heading, and hyperlink. <p> For example, in recent work we have shown that the accuracy of our Bayesian bag of words classifier can be improved by using the class hierarchy to obtain more accurate estimates of class conditional word probabilities <ref> [40] </ref>. * Use the vast pool of unlabeled Web pages to supplement the available hand-labeled data to improve learning accuracy. Recently we have shown that the EM algorithm can be used to combine labeled and unlabeled data to boost accuracy [45].
Reference: [41] <author> A. K. McCallum and K. Nigam. </author> <title> A comparison of event models for naive bayes text classification. </title> <booktitle> In Working Notes of the ICML/AAAI Workshop on Learning for Text Categorization, </booktitle> <year> 1998. </year> <note> http://www.cs.cmu.edu/ mccallum. </note>
Reference-contexts: We use the second approach, since it has been found to out-perform the first on several data sets <ref> [41] </ref>. We formulate Naive Bayes for text classification as follows.
Reference: [42] <author> A. E. Monge and C. P. Elkan. </author> <title> The webfind tool for finding scientific papers over the Worldwide Web. </title> <booktitle> In Proceedings of the Third International Congress on Computer Science Research, </booktitle> <address> Tijuana, Mexico, </address> <year> 1996. </year>
Reference-contexts: Spertus [60] presents a set of heuristics that relate hypertext conventions to semantic relationships. Specifically, she considers relationships that can often be inferred from hyperlink structure, file system organization, and HTML page structure. Monge and Elkan <ref> [42] </ref> have developed a system that finds the Web page for a paper given a bibliographic citation to it. Part of the task performed by this system is to find the personal home page and the publications page of an author starting from the home page of the person's institution.
Reference: [43] <author> I. Moulinier and J.-G. Ganascia. </author> <title> Applying an existing machine learning algorithm to text categorization. </title> <editor> In S. Wermter, E. Riloff, and G. Scheler, editors, </editor> <title> Connectionist, Statistical, and Symbolic Approaches to Learning for Natural Language Processing. </title> <publisher> Springer-Verlag, </publisher> <month> March </month> <year> 1996. </year>
Reference-contexts: Another useful line of research in text classification comes from basic ideas in probability and information theory. Bayes Rule has been the starting point for a number of classification algorithms <ref> [5, 6, 33, 35, 43, 46] </ref>, and the Minimum Description Length principle has been used as the basis of an algorithm as well [32]. 36 Another line of research has been to use symbolic learning methods for text classification. <p> Numerous studies have used algorithms such as decision trees, Swap-1, Ripper and Charade can be found in <ref> [5, 6, 8, 13, 34, 35, 43, 44, 46, 61] </ref>. These studies indicate that these algorithms are quite competitive with statistical-based methods. 8.2.
Reference: [44] <author> I. Moulinier, G. Raskinis, and J.-G. Ganascia. </author> <title> Text categorization: a symbolic approach. </title> <booktitle> In SDAIR, </booktitle> <year> 1996. </year>
Reference-contexts: Numerous studies have used algorithms such as decision trees, Swap-1, Ripper and Charade can be found in <ref> [5, 6, 8, 13, 34, 35, 43, 44, 46, 61] </ref>. These studies indicate that these algorithms are quite competitive with statistical-based methods. 8.2.
Reference: [45] <author> K. Nigam, A. McCallum, S. Thrun, and T. Mitchell. </author> <title> Learning to classify text from labeled and unlabeled documents. </title> <booktitle> In Proceedings of the Fifteenth National Conference on Artificial Intelligence. </booktitle> <publisher> AAAI Press, </publisher> <year> 1998. </year>
Reference-contexts: (c) log (Pr (c)) (8) X Pr (v i ) c2C = v i 2fw i ;:w i g c2C Pr (c) Pr (v i ) (10) This feature selection method has been found to perform best among several alternatives [63], and has been used in many text classification studies <ref> [25, 29, 30, 45, 40] </ref>. 5.1.2. Experimental Evaluation We evaluate our method using the cross-validation methodology described in Section 4. On each iteration of the cross-validation run, we train a classifier for each of the page representations described at the beginning of this section: full-Text, title/heading, and hyperlink. <p> Recently we have shown that the EM algorithm can be used to combine labeled and unlabeled data to boost accuracy <ref> [45] </ref>. We are also exploring the combination of EM with pool-based training for active learning in which the learner requests labels for specific Web pages whose label will be especially helpful [39]. * Co-training multiple classifiers.
Reference: [46] <author> M. J. Pazzani, J. Muramatsu, and D. Billsus. Syskill & Webert: </author> <title> Identifying interesting Web sites. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 54-59, </pages> <address> Portland, OR, 1996. </address> <publisher> AAAI/MIT Press. </publisher>
Reference-contexts: Its "word-vector" approach involves describing classes with a vector of weights, where each weight indicates how important the corresponding word is to the class. This representation has been used with many different learning algorithms, including memory based reasoning [38], neural networks <ref> [46, 55] </ref>, linear discriminant analysis [55], logistic regression [55], Widrow-Hoff and the exponentiated gradient (EG) algorithm [34]. Another useful line of research in text classification comes from basic ideas in probability and information theory. <p> Another useful line of research in text classification comes from basic ideas in probability and information theory. Bayes Rule has been the starting point for a number of classification algorithms <ref> [5, 6, 33, 35, 43, 46] </ref>, and the Minimum Description Length principle has been used as the basis of an algorithm as well [32]. 36 Another line of research has been to use symbolic learning methods for text classification. <p> Numerous studies have used algorithms such as decision trees, Swap-1, Ripper and Charade can be found in <ref> [5, 6, 8, 13, 34, 35, 43, 44, 46, 61] </ref>. These studies indicate that these algorithms are quite competitive with statistical-based methods. 8.2. <p> A system with a similar goal is Letizia [36], which learns the interests of a single user, in contrast to WebWatcher which learns from a community of users. Syskill and Webert <ref> [46] </ref> offers a more restricted way of browsing than WebWatcher and Letizia. Starting from a manually constructed index page for a particular topic, the user can rate hyperlinks off this page.
Reference: [47] <author> M. Perkowitz and O. Etzioni. </author> <title> Category translation: learning to understand information on the Internet. </title> <booktitle> In Proceedings of the 15th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 930-936. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: Starting with maximally specific extraction patterns, both systems learn by dropping constraints and merging patterns. This contrasts with the general-to-specific approach introduced here. Several researchers have explored the problem of text extraction from the Web and other Internet sources. One example is ILA <ref> [47] </ref>, a system designed to learn extraction patterns over the human-readable output of online databases. ILA exploits prior expectations about the probable contents of a database to learn how records are formatted for output by a particular system.
Reference: [48] <author> P. Pirolli, J. Pitkow, and R. Rao. </author> <title> Silk from a sow's ear: Extracting usable structures from the Web. </title> <booktitle> In Human Factors in Computing Systems: CHI '96 Conference Proceedings, </booktitle> <pages> pages 118-125, </pages> <address> New York, NY, </address> <year> 1996. </year>
Reference-contexts: Their rules look for certain keywords in hyperlinks to decide which ones to follow in the search. Whereas their rules are hand-coded for a specific task, our work considers the problem of learning such rules for arbitrary relations. Pirolli et al. <ref> [48] </ref> consider the task of classifying pages into functional categories such as head, index and reference. They characterize the classes using features such as file size, number of incoming and outgoing hyperlinks, average depth of children pages in the hyperlink graph, etc.
Reference: [49] <author> J. R. Quinlan. </author> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 239-2666, </pages> <year> 1990. </year>
Reference-contexts: In this section, we consider the task of learning to classify pages using a learner that is able to induce first-order rules. 5.2.1. Approach The learning algorithm that we use in this section is Quinlan's Foil algorithm <ref> [49, 50] </ref>. Foil is a greedy covering algorithm for learning function-free Horn clauses 3 . Foil induces each Horn clause by beginning with an empty tail and using a hill-climbing search to add literals to the tail until the clause covers only (mostly) positive instances. <p> Problem Representation Because this task involves discovering hyperlink paths of unknown and variable size, we employ a learning method that uses a first-order representation for its learned rules. Specifically, the algorithm we have developed for this task is based on the Foil algorithm <ref> [49, 50] </ref> which we used for page classification in Section 5.2. We discuss our algorithm in more detail below.
Reference: [50] <author> J. R. Quinlan and R. M. Cameron-Jones. </author> <title> FOIL: A midterm report. </title> <booktitle> In Proceedings of the European Conference on Machine Learning, </booktitle> <pages> pages 3-20, </pages> <address> Vienna, Austria, </address> <year> 1993. </year>
Reference-contexts: In this section, we consider the task of learning to classify pages using a learner that is able to induce first-order rules. 5.2.1. Approach The learning algorithm that we use in this section is Quinlan's Foil algorithm <ref> [49, 50] </ref>. Foil is a greedy covering algorithm for learning function-free Horn clauses 3 . Foil induces each Horn clause by beginning with an empty tail and using a hill-climbing search to add literals to the tail until the clause covers only (mostly) positive instances. <p> Problem Representation Because this task involves discovering hyperlink paths of unknown and variable size, we employ a learning method that uses a first-order representation for its learned rules. Specifically, the algorithm we have developed for this task is based on the Foil algorithm <ref> [49, 50] </ref> which we used for page classification in Section 5.2. We discuss our algorithm in more detail below.
Reference: [51] <author> L. Rabiner and B.-H. Juang. </author> <title> Fundamentals of Speech Recognition. </title> <publisher> Prentice Hall Signal Processing Series, </publisher> <year> 1993. </year>
Reference-contexts: First, instead of using the product of the word likelihoods, we use the geometric mean of the likelihoods. This approach is closely related to the concept of perplexity in language modeling for speech recognition <ref> [51] </ref>. Perplexity is a measure of the likelihood of some data given a model, where the likelihood is normalized for the length of the data.
Reference: [52] <author> B. L. Richards and R. J. Mooney. </author> <title> Learning relations by pathfinding. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 50-55, </pages> <address> San Jose, CA, 1992. </address> <publisher> AAAI/MIT Press. </publisher> <pages> 48 </pages>
Reference-contexts: In the first phase, the "path" part of the clause is learned, and in the second phase, additional literals are added to the clause using a hill-climbing search. Our algorithm for constructing the path part of a clause is a variant of Richards and Mooney's relational pathfinding method <ref> [52] </ref>.
Reference: [53] <author> E. Riloff. </author> <title> Automatically generating extraction patterns from untagged text. </title> <booktitle> In Proceed--ings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 1044-1049. </pages> <publisher> AAAI/MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: PALKA [28] and AutoSlog [54] are machine learning systems which learn extraction patterns from collections of parsed documents that have been annotated to identify fragments of interest. These patterns are then reviewed and manually installed into a larger information extraction system. AutoSlog-TS <ref> [53] </ref> removes the requirement that documents be annotated. CRYSTAL [58] and RAPIER [11] both demonstrate that machine learning techniques can be used to learn rules that perform extraction autonomously. CRYSTAL is a covering algorithm which takes parsed, annotated sentences as input and produces rules for extracting from novel sentences.
Reference: [54] <author> E. Riloff. </author> <title> An empirical study of automated dictionary construction for information extraction in three domains. </title> <journal> Arificial Intelligence, </journal> <volume> 85 </volume> <pages> 101-134, </pages> <year> 1996. </year>
Reference-contexts: Recently, the problem of using machine-learning methods to induce information-extraction routines has received more attention. PALKA [28] and AutoSlog <ref> [54] </ref> are machine learning systems which learn extraction patterns from collections of parsed documents that have been annotated to identify fragments of interest. These patterns are then reviewed and manually installed into a larger information extraction system. AutoSlog-TS [53] removes the requirement that documents be annotated.
Reference: [55] <author> H. Schutze, D. A. Hull, and J. O. Pedersen. </author> <title> A comparison of classifiers and document representations for the routing problem. </title> <booktitle> In Proceedings of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR95), </booktitle> <pages> pages 229-237, </pages> <year> 1995. </year>
Reference-contexts: Its "word-vector" approach involves describing classes with a vector of weights, where each weight indicates how important the corresponding word is to the class. This representation has been used with many different learning algorithms, including memory based reasoning [38], neural networks <ref> [46, 55] </ref>, linear discriminant analysis [55], logistic regression [55], Widrow-Hoff and the exponentiated gradient (EG) algorithm [34]. Another useful line of research in text classification comes from basic ideas in probability and information theory. <p> Its "word-vector" approach involves describing classes with a vector of weights, where each weight indicates how important the corresponding word is to the class. This representation has been used with many different learning algorithms, including memory based reasoning [38], neural networks [46, 55], linear discriminant analysis <ref> [55] </ref>, logistic regression [55], Widrow-Hoff and the exponentiated gradient (EG) algorithm [34]. Another useful line of research in text classification comes from basic ideas in probability and information theory. <p> Its "word-vector" approach involves describing classes with a vector of weights, where each weight indicates how important the corresponding word is to the class. This representation has been used with many different learning algorithms, including memory based reasoning [38], neural networks [46, 55], linear discriminant analysis <ref> [55] </ref>, logistic regression [55], Widrow-Hoff and the exponentiated gradient (EG) algorithm [34]. Another useful line of research in text classification comes from basic ideas in probability and information theory.
Reference: [56] <author> M. Shakes, J. Langheinrich and O. Etzioni. </author> <title> Dynamic reference sifting: a case study in the homepage domain. </title> <booktitle> In Proceedings of Sixth International World Wide Web Conference, </booktitle> <address> Santa Clara, CA, </address> <year> 1996. </year>
Reference-contexts: Lira uses the model to browse the Web off-line and returns a set of pages that match the user's interest. One related system that is closer in spirit to our work is Shakes et al.'s <ref> [56] </ref> Ahoy system, which attempts to locate the home page of a person, given information such as the person's name, organizational affiliation etc. Ahoy uses knowledge of home page placement conventions to search for personal home pages, and in fact learns these conventions from experience. 39 9.
Reference: [57] <author> S. Slattery and M. Craven. </author> <title> Combining statistical and relational methods for learning in hypertext domains. </title> <booktitle> In Proceedings of the 8th International Conference on Inductive Logic Programming. </booktitle> <publisher> Springer Verlag, </publisher> <year> 1998. </year>
Reference-contexts: We have begun developing methods that augment first-order learning with the ability to use bag 41 of-words classifiers to invent new predicates for characterizing the pages and hyperlinks referenced in learned rules <ref> [57] </ref>. * Exploit more HTML structure. We plan to investigate the utility of representing the HTML structure of pages when learning rules for relation classification and information extraction.
Reference: [58] <author> S. Soderland. </author> <title> Learning Text Analysis Rules for Domain-specific Natural Language Processing. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, </institution> <year> 1996. </year> <note> Available as Department of Computer Science Technical Report 96-087. </note>
Reference-contexts: These patterns are then reviewed and manually installed into a larger information extraction system. AutoSlog-TS [53] removes the requirement that documents be annotated. CRYSTAL <ref> [58] </ref> and RAPIER [11] both demonstrate that machine learning techniques can be used to learn rules that perform extraction autonomously. CRYSTAL is a covering algorithm which takes parsed, annotated sentences as input and produces rules for extracting from novel sentences.
Reference: [59] <author> S. Soderland. </author> <title> Learning to extract text-based information from the World Wide Web. </title> <booktitle> In Proceedings of the 3rd International Conference on Knowledge Discovery and Data Mining, </booktitle> <year> 1997. </year>
Reference-contexts: These approaches are related to the general problem of "wrapper induction" [31], learning extraction patterns for highly regular sources. At the same time, ideas that have proven useful for general text have also been shown to work well for Web pages. Webfoot <ref> [59] </ref> is a modification of CRYSTAL in which parsed sentence fragments are replaced by segments of HTML. 37 8.3. Extracting Semantic Information from Hypertext Several other research groups have considered the semantic information that can be automatically inferred and extracted from hypertext.
Reference: [60] <author> E. Spertus. ParaSite: </author> <title> Mining structural information on the Web. </title> <booktitle> In Proceedings of the Sixth International World Wide Web Conference, </booktitle> <address> Santa Clara, CA, </address> <year> 1997. </year>
Reference-contexts: We accomplish this by solving two subtasks: grouping related pages together, and identifying the most representative page of a group. Spertus <ref> [60] </ref> identifies regularities in URL structure and naming, and presents several heuristics for discovering page groupings and identifying representative home page. We use a similar, slightly expanded, approach. In an ideal scenario, one could imagine trying to learn these heuristics from examples. <p> Webfoot [59] is a modification of CRYSTAL in which parsed sentence fragments are replaced by segments of HTML. 37 8.3. Extracting Semantic Information from Hypertext Several other research groups have considered the semantic information that can be automatically inferred and extracted from hypertext. Spertus <ref> [60] </ref> presents a set of heuristics that relate hypertext conventions to semantic relationships. Specifically, she considers relationships that can often be inferred from hyperlink structure, file system organization, and HTML page structure.
Reference: [61] <author> S. M. Weiss and N. Indurkhya. </author> <title> Optimized rule induction. </title> <journal> IEEE Expert, </journal> <pages> pages 61-69, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: Numerous studies have used algorithms such as decision trees, Swap-1, Ripper and Charade can be found in <ref> [5, 6, 8, 13, 34, 35, 43, 44, 46, 61] </ref>. These studies indicate that these algorithms are quite competitive with statistical-based methods. 8.2.
Reference: [62] <author> I. H. Witten and T. C. Bell. </author> <title> The zero-frequence problem: Estimating the probabilities of novel events in adaptive text compression. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 37(4), </volume> <month> July </month> <year> 1991. </year>
Reference-contexts: Rather than smoothing with the common Laplace Estimates (i.e., adding one to all the word counts for a class), we use Witten-Bell smoothing <ref> [62] </ref>, which we have found to perform better on several data sets.
Reference: [63] <author> Y. Yang and J. Pederson. </author> <title> Feature selection in statistical learning of text categorization. </title> <booktitle> In Proceedings of the Fourteenth International Conference on Machine Learning, </booktitle> <pages> pages 412-420, </pages> <address> Nashville, TN, 1997. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 49 </pages>
Reference-contexts: = H (C) H (CjW i ) (7) X Pr (c) log (Pr (c)) (8) X Pr (v i ) c2C = v i 2fw i ;:w i g c2C Pr (c) Pr (v i ) (10) This feature selection method has been found to perform best among several alternatives <ref> [63] </ref>, and has been used in many text classification studies [25, 29, 30, 45, 40]. 5.1.2. Experimental Evaluation We evaluate our method using the cross-validation methodology described in Section 4.
References-found: 63

