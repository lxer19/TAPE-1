URL: ftp://ftp.cs.virginia.edu/pub/techreports/CS-95-17.ps.Z
Refering-URL: ftp://ftp.cs.virginia.edu/pub/techreports/README.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Multicasting in DIS: A Unified Solution  
Author: Sudhir Srinivasan Bronis R. de Supinski 
Abstract: Computer Science Report No. CS-95-17 March 21, 1995 
Abstract-found: 1
Intro-found: 1
Reference: [Bea92] <author> Beaver, R., etal., </author> <title> Strawman Distributed Interactive Simulation Architecture Description Document Volume I, </title> <institution> Loral Systems Company, </institution> <month> March </month> <year> 1992. </year>
Reference-contexts: This multi-tiered implementation is consistent with the two-tiered implementation of the DIS Strawman document <ref> [Bea92] </ref>. We assume geographically distributed sites are interconnected 8 by some system of WANs. This forms the top level of the network hierarchy. At each site, we may have some hierarchical organization of local networks. Each local network connects to its higher level network through a gateway.
Reference: [DIS93] <institution> DIS Steering Committee, The DIS Vision, Institute for Simulation and Training, </institution> <month> October </month> <year> 1993. </year>
Reference-contexts: We study the particular requirements placed on the implementation by a large-scale distributed system such as DIS and suggest a simple implementation that meets these requirements. 3. Multicast Groups Multicast groups (MCGs) may be organized based on different criteria. Several excellent criteria are suggested in <ref> [MZP95, DIS93] </ref>, for example, physical proximity, functional relationships, fidelity requirements, temporal behavior. In most of these cases, the design of the MCGs is fairly straightforward. However, groups based on spatial proximity among entities in the VE (called the spatial class in [MZP95]) present some interesting challenges.
Reference: [Eri94] <author> Eriksson, H., MBONE: </author> <title> The Multicast Backbone, </title> <journal> CACM, </journal> <volume> Vol. 37, No. 8, </volume> <month> August, </month> <year> 1994, </year> <pages> pp. 54-60 </pages>
Reference-contexts: In most distributed computations, the requirement of quick dynamic response to MCG-membership changes may not be very stringent. However, it is particularly important in DIS since loss of messages can detract from the fidelity of the simulation. 5.1 Current technology One implementation of multicasts is in the MBONE <ref> [Eri94] </ref> which operates over the Internet. Agents located around the network communicate with each other periodically, exchanging information about their locations and MCG sets. Thus, each agent has sufficient information locally to route multicasts to appropriate neighbors.
Reference: [JoM92] <author> Johnson M. and Myers, S., </author> <title> Allocation of Multicast Message Addresses for Distributed Interactive Simulation, </title> <booktitle> 6th Workshop on Standards for the Interoperability of Defense Simulations, </booktitle> <volume> Vol. II, </volume> <pages> pp. 109-114. </pages>
Reference-contexts: In its simplest form, the two grids are identical so that each cell in the grid corresponds to a single MCG, as suggested by <ref> [JoM92] </ref> (Local Effects category) and [MZP95] (example hexagonal MCGs).
Reference: [MZP95] <author> Macedonia, M.R., Zyda, M.J, Pratt, D.R. and Barham, P.T., </author> <title> Exploiting Reality with Multicast Groups: A Network Architecture for Large Scale Virtual Environments, </title> <note> submitted to the 1995 VRAIS Conference. </note>
Reference-contexts: While this scheme works for the small-scale systems for which SIMNET was intended, it will not scale to the systems being considered for DIS which are expected to have millions of entities. For a more detailed analysis of this scalability problem of DIS, see <ref> [MZP95] </ref>. The reason broadcasts were used in earlier systems is that the VEs were small enough that each entity needed to be aware of most other entities. Large scale DIS exercises are expected to simulate much larger VEs than before. <p> We study the particular requirements placed on the implementation by a large-scale distributed system such as DIS and suggest a simple implementation that meets these requirements. 3. Multicast Groups Multicast groups (MCGs) may be organized based on different criteria. Several excellent criteria are suggested in <ref> [MZP95, DIS93] </ref>, for example, physical proximity, functional relationships, fidelity requirements, temporal behavior. In most of these cases, the design of the MCGs is fairly straightforward. However, groups based on spatial proximity among entities in the VE (called the spatial class in [MZP95]) present some interesting challenges. <p> Several excellent criteria are suggested in [MZP95, DIS93], for example, physical proximity, functional relationships, fidelity requirements, temporal behavior. In most of these cases, the design of the MCGs is fairly straightforward. However, groups based on spatial proximity among entities in the VE (called the spatial class in <ref> [MZP95] </ref>) present some interesting challenges. Intuitively, spatial MCGs seem to be the most obvious way of exploiting multicasting in DIS. If entities are close to each other in the VE, they are likely to perceive each other, requiring the corresponding simulators to communicate. <p> In its simplest form, the two grids are identical so that each cell in the grid corresponds to a single MCG, as suggested by [JoM92] (Local Effects category) and <ref> [MZP95] </ref> (example hexagonal MCGs). <p> Irregular MCGs have some significant drawbacks as well. As we shall see later, entities change the set of MCGs they are listening to incrementally as they move. As pointed out in <ref> [MZP95] </ref>, computing the change is trivial with regular MCGs such as hexes (because the change is also regular). However, with irregular MCGs, this task may require relatively more computation 5 in spite of the regular grids. <p> In a sense, this unification goes beyond the sum of its parts by capturing only the best of previous schemes and rejecting the rest. For example, our scheme is completely distributed. We have eliminated the need for centralizing agents such as Area of Interest Managers <ref> [MZP95] </ref>, 9 EOMANs [StW94] and group leaders [MZP95]. This reduces not only the complexity of the system but also communication latencies (the scheme in [StW94] requires three messages). <p> For example, our scheme is completely distributed. We have eliminated the need for centralizing agents such as Area of Interest Managers <ref> [MZP95] </ref>, 9 EOMANs [StW94] and group leaders [MZP95]. This reduces not only the complexity of the system but also communication latencies (the scheme in [StW94] requires three messages). Further, there is no physical act of registering in and out of multicast groups (also checking in/out or join/ leave) on the part of the entities.
Reference: [Mil92] <author> Miller, J.T., </author> <title> Strawman Static Multi-state Objects, 7th Workshop on Standards for the Interoperability of Defense Simulations, </title> <journal> Vol. </journal> <volume> I, </volume> <pages> pp. </pages> <address> A-183 - A-191. </address>
Reference-contexts: As the simulation proceeds, these static objects may be affected (buildings may be destroyed, craters may form, etc.). These changes must be made known to participating entities because the changes may affect the course of the rest of the simulation. Since these objects are usually multi-state <ref> [Mil92] </ref> and can be affected by multiple entities at the same time in the simulation, the main problem is to provide all entities with a consistent view of the state of these objects. There are basically two ways of solving this problem: distributed consensus or centralized arbitration. <p> There are basically two ways of solving this problem: distributed consensus or centralized arbitration. The former has obvious time costs which make it infeasible for DIS. Miller <ref> [Mil92] </ref> proposes the use of special Object Management Processors (OMPs) whose sole responsibility is to maintain the states of static objects. These processors listen to the DIS network for messages that could cause changes in the states of any static objects for which they are responsible.
Reference: [ShB92] <author> Sherman, R. and Butler, B., </author> <title> Segmenting the Battlefield, 7th Workshop on Standards for the Interoperability of Defense Simulations, </title> <journal> Vol. </journal> <volume> I, </volume> <pages> pp. </pages> <address> A-27 - A-35. </address>
Reference-contexts: In reality, it may not be possible to predict exactly where the action will take place. In other words, incorrect analyses can lead to poor allocation of MCGs and consequent performance penalties. Finally, as mentioned in <ref> [ShB92] </ref>, changes in terrain databases may change the allocation of MCGs requiring a re-analysis.
Reference: [StW94] <author> Steinman, J.S. and Wieland, F., </author> <title> Parallel Proximity Detection and the Distribution List Algorithm, ELECSIM 1994, </title> <editor> R. Smith, Editor, </editor> <booktitle> smithr@mystech.com, </booktitle> <pages> pp. 1 - 11. </pages>
Reference-contexts: Then M L - P N is the set of MCGs that the entity need no longer listen to and M J - P O is the set of MCGs it must begin listening to. Steinman and Wieland <ref> [StW94] </ref> point out an important potential source of error due to the fact that an entity computes its current grid location only periodically. <p> A similar problem may occur when an entity crosses into and out of a grid cell in between successive grid location computations. The authors <ref> [StW94] </ref> present a satisfactory solution using fuzzy grids which is incorporated into our scheme very easily. Their solution is to inate the AOI of sensors by fixed amounts to compensate for the fact that other entities as well as the sensors themselves are moving. <p> In a sense, this unification goes beyond the sum of its parts by capturing only the best of previous schemes and rejecting the rest. For example, our scheme is completely distributed. We have eliminated the need for centralizing agents such as Area of Interest Managers [MZP95], 9 EOMANs <ref> [StW94] </ref> and group leaders [MZP95]. This reduces not only the complexity of the system but also communication latencies (the scheme in [StW94] requires three messages). <p> For example, our scheme is completely distributed. We have eliminated the need for centralizing agents such as Area of Interest Managers [MZP95], 9 EOMANs <ref> [StW94] </ref> and group leaders [MZP95]. This reduces not only the complexity of the system but also communication latencies (the scheme in [StW94] requires three messages). Further, there is no physical act of registering in and out of multicast groups (also checking in/out or join/ leave) on the part of the entities.
References-found: 8

