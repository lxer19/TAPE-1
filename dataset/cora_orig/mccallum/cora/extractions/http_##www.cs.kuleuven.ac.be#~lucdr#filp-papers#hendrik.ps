URL: http://www.cs.kuleuven.ac.be/~lucdr/filp-papers/hendrik.ps
Refering-URL: http://www.cs.kuleuven.ac.be/~lucdr/filp.html
Root-URL: 
Email: email: fLuc.DeRaedt,Hendrik.Blockeelg@cs.kuleuven.ac.be  
Phone: Tel: 32 16 32 76 43 Fax 32 16 32 79 96  
Title: Using Logical Decision Trees for Clustering  
Author: Luc De Raedt and Hendrik Blockeel 
Date: April 2, 1997  
Address: Celestijnenlaan 200A, B-3001 Heverlee, Belgium  
Affiliation: Department of Computer Science, Katholieke Universiteit Leuven  
Abstract: A novel first order clustering system, called C 0.5, is presented. It inherits its logical decision tree formalism from the TILDE system, but instead of using class information to guide the search, it employs the principles of instance based learning in order to perform clustering. Various experiments are discussed, which show the promise of the approach.
Abstract-found: 1
Intro-found: 1
Reference: [ Bisson, 1992a ] <author> G. Bisson. </author> <title> Conceptual clustering in a first order logic representation. </title> <booktitle> In Proceedings of the 10th European Conference on Artificial Intelligence, </booktitle> <pages> pages 458-462. </pages> <publisher> John Wiley & Sons, </publisher> <year> 1992. </year>
Reference-contexts: d 2 between the points in the three-dimensional space corresponding to the lumo; logp and activity values of the two compounds, or it could be the distance d 3 as measure by a first order distance measure such as used in RIBL [ Emde and Wettschereck, 1996 ] or KBG <ref> [ Bisson, 1992a ] </ref> or [ Hutchinson, 1997 ] . 4 Given the distance at the level of the examples, the principles of instance based learning can be used as to compute the prototypes. <p> to formally specify the clustering problem: Given * a set of examples E (each example is a definite clause theory), * a background theory B in the form of a definite clause theory, 2 Using Plotkin's [ Plotkin, 1970 ] notion of -subsumption or the variants corresponding to structural matching <ref> [ Bisson, 1992a; Raedt et al., 1997 ] </ref> . 5 * a distance measure d that computes the distance between two examples or prototypes, * a prototype function p that computes the prototype of a set of examples, Find: clusters in the form of a logical decision tree. <p> As far as related work is concerned, our work is related to KBG by <ref> [ Bisson, 1992a ] </ref> , which also performs first order clustering. In contrast to the current version of C 0.5., KBG does use a first order similarity measure, which could also be used within C 0.5.
Reference: [ Bisson, 1992b ] <author> G. Bisson. </author> <title> Learning in fol with a similarity measure. </title> <booktitle> In Proceedings of the 9th National Conference on Artificial Intelligence (AAAI-92). </booktitle> <publisher> AAAI Press, </publisher> <year> 1992. </year>
Reference-contexts: Because there are no relations between different examples, the definition of first order distances can be simplified. Indeed, some of the complications in the definition of distance metrics that are caused by possible relationships between examples are avoided in this way. For instance, in Bisson's framework <ref> [ Bisson, 1992b ] </ref> the distance between two examples may depend on the distance between two other examples to which these are somehow related; but the latter may depend again on the distance of these two examples.
Reference: [ Blockeel and De Raedt, 1997 ] <author> H. Blockeel and L. De Raedt. </author> <title> Experiments with top-down induction of logical decision trees. </title> <type> Technical Report CW 247, </type> <institution> Dept. of Computer Science, K.U.Leuven, </institution> <month> January </month> <year> 1997. </year> <title> Also in Periodic Progress Report ESPRIT Project ILP2, </title> <month> January </month> <year> 1997. </year>
Reference-contexts: Decision trees are an example of the former, while typical clustering algorithms are unsupervised. Regression trees, as discussed by e.g. Kramer [ Kramer, 1996 ] should be regarded as supervised clustering. fl Also submitted to ILP '97 1 The ILP system TILDE <ref> [ Blockeel and De Raedt, 1997 ] </ref> induces first order log-ical decision trees from classified examples. In this paper, we show how to adapt the TILDE system to perform clustering, resulting in the C 0.5 system. To realize this, principles from instance based learning are employed. <p> The root test, for instance, tests whether there occurs an atom of type 14 in the molecule. This view is in correspondence with Langley's viewpoint that a test in a node is not just a decision criterion, but also a description of the subclusters formed in this node. In <ref> [ Blockeel and De Raedt, 1997 ] </ref> , we have shown how a logical decision tree can be transformed into an equivalent logic program. Each node in the tree then corresponds to a unique (propositional) predicate that is defined in the logic program. <p> So, conceptually, the clustering process learns from interpretations. This is similar to the Claudien [ De Raedt and Dehaspe, 1997 ] , ICL [ De Raedt and Van Laer, 1995 ] and TILDE <ref> [ Blockeel and De Raedt, 1997 ] </ref> systems. 3 C 0.5 : Clustering of order 0.5 3.1 The language of logical decision trees C 0.5 employs the basic TDIDT framework as it is also incorporated in TILDE. <p> When a node N is to be refined, the set of all the conjunctions that can possibly be put in that node is computed as fCj ( Q; C) 2 ( Q)g where Q is the query associated with the node, as defined in <ref> [ Blockeel and De Raedt, 1997 ] </ref> . Roughly speaking, the query associated with a node is the set of accumulated conditions that hold on the path from root to the present node. <p> Roughly speaking, the query associated with a node is the set of accumulated conditions that hold on the path from root to the present node. These queries are also the basis of the transformation from decision trees to logical theories, this is formally worked out in <ref> [ Blockeel and De Raedt, 1997 ] </ref> . Declarations defining the refinement operator are an input parameter of the algorithm. Since determines the language bias, a separate language bias specification is not needed.
Reference: [ De Raedt and Dehaspe, 1997 ] <author> L. De Raedt and L. Dehaspe. </author> <title> Clausal discovery. </title> <journal> Machine Learning, </journal> <volume> 26 </volume> <pages> 99-146, </pages> <year> 1997. </year>
Reference-contexts: Notice that in this problem-setting, the interpretation i corresponding to an example e is the least Herbrand model of B ^ e. So, conceptually, the clustering process learns from interpretations. This is similar to the Claudien <ref> [ De Raedt and Dehaspe, 1997 ] </ref> , ICL [ De Raedt and Van Laer, 1995 ] and TILDE [ Blockeel and De Raedt, 1997 ] systems. 3 C 0.5 : Clustering of order 0.5 3.1 The language of logical decision trees C 0.5 employs the basic TDIDT framework as it <p> The tree typically gives the smallest possible description of clusters that allows to discriminate between them. From the point of view of abduction, this is not very interesting. However, a much richer form of prediction is possible if one applies a discovery system such as Claudien <ref> [ De Raedt and Dehaspe, 1997 ] </ref> to each of the clusters. One then obtains a maximally specific description characterizing each cluster. As pointed out in [ De Raedt and Dehaspe, 1997 ] such theories can be used for predicting missing information about an example. <p> However, a much richer form of prediction is possible if one applies a discovery system such as Claudien <ref> [ De Raedt and Dehaspe, 1997 ] </ref> to each of the clusters. One then obtains a maximally specific description characterizing each cluster. As pointed out in [ De Raedt and Dehaspe, 1997 ] such theories can be used for predicting missing information about an example.
Reference: [ De Raedt and Dzeroski, 1994 ] <author> L. De Raedt and S. Dzeroski. </author> <title> First order jk-clausal theories are PAC-learnable. </title> <journal> Artificial Intelligence, </journal> <volume> 70 </volume> <pages> 375-392, </pages> <year> 1994. </year>
Reference-contexts: In this setting, each example is a Prolog program encoding the specific properties of the example. One may also specify background knowledge in the form of a Prolog program. 1 See <ref> [ De Raedt and Dzeroski, 1994; De Raedt, 1996 ] </ref> for more details on learning from interpretations. For instance, examples for the well-known mutagenesis problem [ Srinivasan et al., 1994 ] can be described by interpretations.
Reference: [ De Raedt and Van Laer, 1995 ] <author> L. De Raedt and W. Van Laer. </author> <title> Inductive constraint logic. </title> <booktitle> In Proceedings of the 5th Workshop on Algorithmic Learning Theory, volume 997 of Lecture Notes in Artificial Intelligence. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1995. </year>
Reference-contexts: Notice that in this problem-setting, the interpretation i corresponding to an example e is the least Herbrand model of B ^ e. So, conceptually, the clustering process learns from interpretations. This is similar to the Claudien [ De Raedt and Dehaspe, 1997 ] , ICL <ref> [ De Raedt and Van Laer, 1995 ] </ref> and TILDE [ Blockeel and De Raedt, 1997 ] systems. 3 C 0.5 : Clustering of order 0.5 3.1 The language of logical decision trees C 0.5 employs the basic TDIDT framework as it is also incorporated in TILDE.
Reference: [ De Raedt, 1996 ] <author> L. De Raedt. </author> <title> Induction in logic. In R.S. </title> <editor> Michalski and Wnek J., editors, </editor> <booktitle> Proceedings of the 3rd International Workshop on Multistrategy Learning, </booktitle> <pages> pages 29-38, </pages> <year> 1996. </year> <month> 15 </month>
Reference-contexts: In this setting, each example is a Prolog program encoding the specific properties of the example. One may also specify background knowledge in the form of a Prolog program. 1 See <ref> [ De Raedt and Dzeroski, 1994; De Raedt, 1996 ] </ref> for more details on learning from interpretations. For instance, examples for the well-known mutagenesis problem [ Srinivasan et al., 1994 ] can be described by interpretations.
Reference: [ Emde and Wettschereck, 1996 ] <author> W. Emde and D. Wettschereck. </author> <title> Relational instance-based learning. </title> <editor> In L. Saitta, editor, </editor> <booktitle> Proceedings of the 13th International Conference on Machine Learning, </booktitle> <pages> pages 122-130. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1996. </year>
Reference-contexts: of the two compounds, or the Euclidean distance d 2 between the points in the three-dimensional space corresponding to the lumo; logp and activity values of the two compounds, or it could be the distance d 3 as measure by a first order distance measure such as used in RIBL <ref> [ Emde and Wettschereck, 1996 ] </ref> or KBG [ Bisson, 1992a ] or [ Hutchinson, 1997 ] . 4 Given the distance at the level of the examples, the principles of instance based learning can be used as to compute the prototypes. <p> For KBG, these descriptions have to be derived in a separate step because the clustering process only produces the clusters (i.e. sets of examples) and not their description. With respect to first order distance functions, we want to mention the instance-based learner RIBL <ref> [ Emde and Wettschereck, 1996 ] </ref> . This system uses an advanced first order distance metric that might be a good candidate for incorporation in C 0.5. Our work also exploits many of the ideas contained in Langley's book, as he makes the link between TDIDT and clustering.
Reference: [ Emde, 1994 ] <author> W. Emde. </author> <title> Inductive learning of characteristic concept descriptions. </title> <editor> In S. Wrobel, editor, </editor> <booktitle> Proceedings of the 4th International Workshop on Inductive Logic Programming, volume 237 of GMD-Studien, </booktitle> <pages> pages 51-70, </pages> <address> Sankt Au-gustin, Germany, </address> <year> 1994. </year> <institution> Gesellschaft fur Mathematik und Datenverarbeitung MBH. </institution>
Reference-contexts: If the leaves are coherent with respect to classes, this method would yield relatively high classification accuracy with a minimum of class information available. This is quite similar in spirit to Emde's method for learning from few classified examples, implemented in the COLA system <ref> [ Emde, 1994 ] </ref> . 4.3 Regression The above shows that first order clustering can be used for characterisation of clusters in the data, as well as for classification. An application that has a flavour of both, is predicting numerical values.
Reference: [ Fisher, 1987 ] <author> D. H. Fisher. </author> <title> Knowledge acquisition via incremental conceptual clustering. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 139-172, </pages> <year> 1987. </year>
Reference-contexts: A concept is associated with each node of the tree, and as such the tree represents a kind of taxonomy, a hierarchy of many concepts. This is very similar to what many clustering algorithms do (e.g. COBWEB, <ref> [ Fisher, 1987 ] </ref> ). Indeed, Langley views both techniques as instantiations of the same general technique, namely induction of concept hierarchies. Concept hierarchies can be induced in a supervised or unsupervised manner. Decision trees are an example of the former, while typical clustering algorithms are unsupervised. <p> The situation is therefore different from that of concept-learning, where the quality of the output of a system is measured in terms of accuracy. Therefore, we cannot directly evaluate the result of the clustering task. Instead, as frequently done in conceptual clustering (cf. <ref> [ Fisher, 1987 ] </ref> ), we will measure the quality of the obtained clusters using the accuracy with which these clusters can be used for prediction. Because of this, when reading this section, the reader might get the impression that C 0.5 is a classification system.
Reference: [ Hutchinson, 1997 ] <author> A. Hutchinson. </author> <title> Metrics on terms and clauses. </title> <booktitle> In Proceedings of the 9th European Conference on Machine Learning, </booktitle> <year> 1997. </year>
Reference-contexts: in the three-dimensional space corresponding to the lumo; logp and activity values of the two compounds, or it could be the distance d 3 as measure by a first order distance measure such as used in RIBL [ Emde and Wettschereck, 1996 ] or KBG [ Bisson, 1992a ] or <ref> [ Hutchinson, 1997 ] </ref> . 4 Given the distance at the level of the examples, the principles of instance based learning can be used as to compute the prototypes.
Reference: [ Ketterlin et al., 1995 ] <author> A. Ketterlin, P. Gancarski, and J.J. Korczak. </author> <title> Conceptual clustering in structured databases : a practical approach. </title> <booktitle> In Proceedings of KDD-95, </booktitle> <year> 1995. </year>
Reference-contexts: Finally, we should also refer to a number of other approaches to first order clustering, which include Kietz's and Morik's Kluster [ Kietz and Morik, 1994 ] , [ Thompson and Langley, 1991 ] and <ref> [ Ketterlin et al., 1995 ] </ref> . 7 Acknowledgements Luc De Raedt is supported by the Fund for Scientific Research, Flanders. Hendrik Blockeel is supported by the Flemish Institute for the Promotion of Scientific and Technological Research in the Industry (IWT).
Reference: [ Kietz and Morik, 1994 ] <author> J.U. Kietz and K.. Morik. </author> <title> A polynomial approach to the constructive induction of structural knowledge. </title> <journal> Machine Learning, </journal> <volume> 14 </volume> <pages> 193-217, </pages> <year> 1994. </year>
Reference-contexts: Finally, we should also refer to a number of other approaches to first order clustering, which include Kietz's and Morik's Kluster <ref> [ Kietz and Morik, 1994 ] </ref> , [ Thompson and Langley, 1991 ] and [ Ketterlin et al., 1995 ] . 7 Acknowledgements Luc De Raedt is supported by the Fund for Scientific Research, Flanders.
Reference: [ Kramer, 1996 ] <author> S. Kramer. </author> <title> Structural regression trees. </title> <booktitle> In Proceedings of the 13th National Conference on Artificial Intelligence (AAAI-96), </booktitle> <year> 1996. </year>
Reference-contexts: Indeed, Langley views both techniques as instantiations of the same general technique, namely induction of concept hierarchies. Concept hierarchies can be induced in a supervised or unsupervised manner. Decision trees are an example of the former, while typical clustering algorithms are unsupervised. Regression trees, as discussed by e.g. Kramer <ref> [ Kramer, 1996 ] </ref> should be regarded as supervised clustering. fl Also submitted to ILP '97 1 The ILP system TILDE [ Blockeel and De Raedt, 1997 ] induces first order log-ical decision trees from classified examples. <p> A second argument is based on the distance metric. Without performing any tests in the tree, one can still say that an example belongs to a cluster because it is close to the examples in that cluster. Both arguments are quite independent from one another (cf. also <ref> [ Kramer, 1996 ] </ref> ). This means that information can flow in two opposite directions. <p> If clusters are coherent with respect to some numerical attribute of the examples, one can compute the average value for a cluster of examples and use this to predict the value of unseen examples. This is basically what Kramer's SRT system <ref> [ Kramer, 1996 ] </ref> does. SRT builds so-called structural regression trees: these are decision trees in which each leaf predicts a numerical value instead of a symbolic class. The "structural" refers to the fact that the tests put in nodes are conjunctions of literals.
Reference: [ Langley, 1996 ] <author> P. Langley. </author> <title> Elements of Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1996. </year>
Reference-contexts: In this preliminary paper on first order clustering we employ only propositional distance measures and the prototype functions that correspond to the instance averaging methods along the lines of <ref> [ Langley, 1996 ] </ref> . However, we wish to stress that in principle we could use any distance measure. In this respect, it is worth mentioning a potential advantage of learning from interpretations. Because there are no relations between different examples, the definition of first order distances can be simplified.
Reference: [ Michalski, 1987 ] <author> R.S. Michalski. </author> <title> Clustering. </title> <editor> In S. Shapiro, editor, </editor> <booktitle> Encyclopedia of artificial intelligence. </booktitle> <publisher> Wiley, </publisher> <year> 1987. </year>
Reference-contexts: In C 0.5, we consider two criteria. The first criterion is the simplest (and is used in the present experiments), and requires that each leaf of the tree should contain a minimal number of examples. The second criterion is derived from classical principles of clustering as formulated by <ref> [ Michalski, 1987 ] </ref> . It states 7 that subclusters C 1 and C 2 of cluster C are not allowed if w fi e2C X d (e 1 ; p (C 1 )) + e 2 2C 2 where w is weight between 0 and 1.
Reference: [ Muggleton and De Raedt, 1994 ] <author> S. Muggleton and L. De Raedt. </author> <title> Inductive logic programming : Theory and methods. </title> <journal> Journal of Logic Programming, </journal> <volume> 19,20:629-679, </volume> <year> 1994. </year>
Reference-contexts: The only point where our algorithms C 0.5 and TILDE differ from the propositional TDIDT algorithm is in the computation of the tests to be placed in a node. To this aim, we employ a classical refinement operator under -subsumption <ref> [ Plotkin, 1970; Muggleton and De Raedt, 1994 ] </ref> . A clause c 1 -subsumes another clause c 2 if and only if there is a variable substitution such that c 1 c 2 .
Reference: [ Muggleton, 1995 ] <author> S. Muggleton. </author> <title> Inverse entailment and progol. </title> <journal> New Generation Computing, </journal> <volume> 13, </volume> <year> 1995. </year>
Reference-contexts: When we refer to the language bias specification, we mean the definition of . 6 The implementation of C 0.5 and TILDE assumes that the predicate rmode is available that indicates which conjunctions are eligible for addition to a query (this is similar to mode declarations as in Muggleton's Progol <ref> [ Muggleton, 1995 ] </ref> ).
Reference: [ Plotkin, 1970 ] <author> G. Plotkin. </author> <title> A note on inductive generalization. </title> <booktitle> In Machine Intelligence, </booktitle> <volume> volume 5, </volume> <pages> pages 153-163. </pages> <publisher> Edinburgh University Press, </publisher> <year> 1970. </year>
Reference-contexts: the name of our system C 0.5. 2.4 Problem-specification By now we are able to formally specify the clustering problem: Given * a set of examples E (each example is a definite clause theory), * a background theory B in the form of a definite clause theory, 2 Using Plotkin's <ref> [ Plotkin, 1970 ] </ref> notion of -subsumption or the variants corresponding to structural matching [ Bisson, 1992a; Raedt et al., 1997 ] . 5 * a distance measure d that computes the distance between two examples or prototypes, * a prototype function p that computes the prototype of a set of <p> The only point where our algorithms C 0.5 and TILDE differ from the propositional TDIDT algorithm is in the computation of the tests to be placed in a node. To this aim, we employ a classical refinement operator under -subsumption <ref> [ Plotkin, 1970; Muggleton and De Raedt, 1994 ] </ref> . A clause c 1 -subsumes another clause c 2 if and only if there is a variable substitution such that c 1 c 2 .
Reference: [ Raedt et al., 1997 ] <author> L. De Raedt, P. Idestam-Almquist, and G. Sablon. </author> <title> Theta-subsumption for structural matching. </title> <booktitle> In Proceedings of the 9th European Conference on Machine Learning, </booktitle> <year> 1997. </year> <month> 16 </month>
Reference-contexts: to formally specify the clustering problem: Given * a set of examples E (each example is a definite clause theory), * a background theory B in the form of a definite clause theory, 2 Using Plotkin's [ Plotkin, 1970 ] notion of -subsumption or the variants corresponding to structural matching <ref> [ Bisson, 1992a; Raedt et al., 1997 ] </ref> . 5 * a distance measure d that computes the distance between two examples or prototypes, * a prototype function p that computes the prototype of a set of examples, Find: clusters in the form of a logical decision tree.
Reference: [ Srinivasan et al., 1994 ] <author> A. Srinivasan, S.H. Muggleton, R.D. King, and M.J.E. Sternberg. Mutagenesis: </author> <title> ILP experiments in a non-determinate biological domain. </title> <editor> In S. Wrobel, editor, </editor> <booktitle> Proceedings of the 4th International Workshop on Inductive Logic Programming, volume 237 of GMD-Studien, </booktitle> <pages> pages 217-232. </pages> <institution> Gesellschaft fur Mathematik und Datenverarbeitung MBH, </institution> <year> 1994. </year>
Reference-contexts: One may also specify background knowledge in the form of a Prolog program. 1 See [ De Raedt and Dzeroski, 1994; De Raedt, 1996 ] for more details on learning from interpretations. For instance, examples for the well-known mutagenesis problem <ref> [ Srinivasan et al., 1994 ] </ref> can be described by interpretations.
Reference: [ Srinivasan et al., 1995 ] <author> A. Srinivasan, S.H. Muggleton, and R.D. King. </author> <title> Comparing the use of background knowledge by inductive logic programming systems. </title> <editor> In L. De Raedt, editor, </editor> <booktitle> Proceedings of the 5th International Workshop on Inductive Logic Programming, </booktitle> <year> 1995. </year>
Reference-contexts: Several distance metrics have been tried out. First of all, supervised learning (using logm for distance computation) was compared to unsupervised learning (using logp and lumo). This was done for three different hypothesis languages corresponding to the background knowledges BG1-BG3 as defined in <ref> [ Srinivasan et al., 1995 ] </ref> . BG1 contains only structural information (atoms and bonds), BG2 adds to this the charges of each individual atom, BG3 adds to BG2 the lumo and logp values of each molecule. <p> This explains in part why FOIL, on the Mutagenesis dataset 12 with BG3, finds a theory simply consisting of a large number of intervals for logp and lumo (see <ref> [ Srinivasan et al., 1995 ] </ref> ). In order to check this out in more detail, we have additionally experimented (on BG2) with distances based on logp only, on lumo only, and on the three numerical values together. Results are shown in Table 2 (first row).
Reference: [ Thompson and Langley, 1991 ] <author> K. Thompson and P. Langley. </author> <title> Concept formation in structured domains. </title> <editor> In D. Fisher, M. Pazzani, and P. Langley, editors, </editor> <title> Concept formation: knowledge and experience in unsupervised learning. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: Finally, we should also refer to a number of other approaches to first order clustering, which include Kietz's and Morik's Kluster [ Kietz and Morik, 1994 ] , <ref> [ Thompson and Langley, 1991 ] </ref> and [ Ketterlin et al., 1995 ] . 7 Acknowledgements Luc De Raedt is supported by the Fund for Scientific Research, Flanders. Hendrik Blockeel is supported by the Flemish Institute for the Promotion of Scientific and Technological Research in the Industry (IWT).
Reference: [ Van Laer et al., 1996 ] <author> W. Van Laer, S. Dzeroski, and L. De Raedt. </author> <title> Multi-class problems and discretization in ICL (extended abstract). </title> <booktitle> In Proceedings of the MLnet Familiarization Workshop on Data Mining with Inductive Logic Programming (ILP for KDD), </booktitle> <year> 1996. </year> <month> 17 </month>
Reference-contexts: Finally, C 0.5 also inherits a discretization procedure from TILDE and ICL (see <ref> [ Van Laer et al., 1996 ] </ref> for a description). This discretization procedure makes it possible to derive theories that can use numerical information. 3.2 The heuristics The other point where C 0.5 deviates from the classical TDIDT algorithm is in the heuristics used.
References-found: 24

