URL: ftp://ftp.cs.toronto.edu/pub/zoubin/nips95.ps.Z
Refering-URL: http://www.cs.cmu.edu/Web/Groups/NIPS/NIPS95/Papers.html
Root-URL: 
Email: zoubin@psyche.mit.edu  jordan@psyche.mit.edu  
Title: Factorial Hidden Markov Models  
Author: Zoubin Ghahramani Michael I. Jordan In D. S. Touretzky, M. C. Mozer and M. E. Hasselmo 
Note: (eds.), Advances in Neural Information Processing Systems 8. pp. 472-478. MIT Press,  
Address: Toronto, ON M5S 1A4 Canada  Cambridge, MA 02139 USA  Cambridge, MA, 1996.  
Affiliation: Department of Computer Science University of Toronto  Department of Brain Cognitive Sciences Massachusetts Institute of Technology  
Abstract: We present a framework for learning in hidden Markov models with distributed state representations. Within this framework, we derive a learning algorithm based on the Expectation-Maximization (EM) procedure for maximum likelihood estimation. Analogous to the standard Baum-Welch update rules, the M-step of our algorithm is exact and can be solved analytically. However, due to the combinatorial nature of the hidden state representation, the exact E-step is intractable. A simple and tractable mean field approximation is derived. Empirical results on a set of problems suggest that both the mean field approximation and Gibbs sampling are viable alternatives to the computationally expensive exact algorithm.
Abstract-found: 1
Intro-found: 1
Reference: <author> Baldi, P., Chauvin, Y., Hunkapiller, T., and McClure, M. </author> <year> (1994). </year> <title> Hidden Markov models of biological primary sequence information. </title> <booktitle> Proc. </booktitle> <institution> Nat. Acad. Sci. (USA), </institution> <month> 91(3) </month> <pages> 1059-1063. </pages>
Reference-contexts: Due to the simplicity and efficiency of its parameter estimation algorithm, the hidden Markov model (HMM) has emerged as one of the basic statistical tools for modeling discrete time series, finding widespread application in the areas of speech recognition (Rabiner and Juang, 1986) and computational molecular biology <ref> (Baldi et al., 1994) </ref>. An HMM is essentially a mixture model, encoding information about the history of a time series in the value of a single multinomial variable (the hidden state). This multinomial assumption allows an efficient parameter estimation algorithm to be derived (the Baum-Welch algorithm).
Reference: <author> Ghahramani, Z. </author> <year> (1995). </year> <title> Factorial learning and the EM algorithm. </title> <editor> In Tesauro, G., Touret-zky, D., and Leen, T., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: The probability model is defined from this energy by the Boltzmann distribution P (fs; yg) = Z expfH (fs; yg)g: (2) Note that like in the CVQ <ref> (Ghahramani, 1995) </ref>, the unclamped partition function Z = dfyg X expfH (fs; yg)g; evaluates to a constant, independent of the parameters.
Reference: <author> Hinton, G. and Zemel, R. </author> <year> (1994). </year> <title> Autoencoders, minimum description length, and Helmholtz free energy. </title> <editor> In Cowan, J., Tesauro, G., and Alspector, J., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6. </booktitle> <publisher> Morgan Kaufmanm Publishers, </publisher> <address> San Francisco, CA. </address>
Reference: <author> Neal, R. </author> <year> (1992). </year> <title> Connectionist learning of belief networks. </title> <journal> Artificial Intelligence, </journal> <volume> 56 </volume> <pages> 71-113. </pages>
Reference-contexts: The constant partition function allowed us to drop the second term in (3). Therefore, unlike the Boltzmann machine, the expected log likelihood does not depend on statistics collected in an unclamped phase of learning, resulting in much faster learning than the traditional Boltzmann machine <ref> (Neal, 1992) </ref>. <p> The learning algorithm derived in this paper assumed real-valued observables. The algorithm can also be derived for HMMs with discrete observables, an architecture closely related to sigmoid belief networks <ref> (Neal, 1992) </ref>. However, the nonlinearities induced by discrete observables make both the E-step and M-step of the algorithm more difficult. In conclusion, we have presented Gibbs sampling and mean field learning algorithms for factorial hidden Markov models.
Reference: <author> Rabiner, L. and Juang, B. </author> <year> (1986). </year> <title> An Introduction to hidden Markov models. </title> <journal> IEEE Acoustics, Speech & Signal Processing Magazine, </journal> <volume> 3 </volume> <pages> 4-16. </pages>
Reference-contexts: Due to the simplicity and efficiency of its parameter estimation algorithm, the hidden Markov model (HMM) has emerged as one of the basic statistical tools for modeling discrete time series, finding widespread application in the areas of speech recognition <ref> (Rabiner and Juang, 1986) </ref> and computational molecular biology (Baldi et al., 1994). An HMM is essentially a mixture model, encoding information about the history of a time series in the value of a single multinomial variable (the hidden state). <p> Note that in standard HMM notation <ref> (Rabiner and Juang, 1986) </ref>, hs t i i c corresponds to fl t and hs t i i c corresponds to ~ t , whereas hs t i s t 0 j i c has no analogue when there is only a single underlying Markov model.
Reference: <author> Saul, L. and Jordan, M. </author> <year> (1995). </year> <title> Boltzmann chains and hidden Markov models. </title> <editor> In Tesauro, G., Touretzky, D., and Leen, T., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Saul, L. and Jordan, M. </author> <year> (1996). </year> <title> Exploiting tractable substructures in Intractable networks. </title> <editor> In Touretzky, D., Mozer, M., and Hasselmo, M., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 8. </booktitle> <publisher> MIT Press. </publisher>
Reference-contexts: Future work will concentrate on a more efficient mean field approximation in which the forward-backward algorithm is used to compute the E-step exactly within each Markov chain, and mean field theory is used to handle interactions between chains <ref> (Saul and Jordan, 1996) </ref>. Acknowledgements This project was supported in part by a grant from the McDonnell-Pew Foundation, by a grant from ATR Human Information Processing Research Laboratories, by a grant from Siemens Corporation, and by grant N00014-94-1-0777 from the Office of Naval Research.
Reference: <author> Williams, C. and Hinton, G. </author> <year> (1991). </year> <title> Mean field networks that learn to discriminate temporally distorted strings. </title> <editor> In Touretzky, D., Elman, J., Sejnowski, T., and Hinton, G., editors, </editor> <title> Connectionist Models: </title> <booktitle> Proceedings of the 1990 Summer School, </booktitle> <pages> pages 18-22. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> Man Mateo, CA. </address>
Reference-contexts: For example, to represent 30 bits of information about the history of a time sequence, an HMM would need 2 30 distinct states. On the other hand an HMM with a distributed state representation could achieve the same task with 30 binary units <ref> (Williams and Hinton, 1991) </ref>. This paper addresses the problem of deriving efficient learning algorithms for hidden Markov models with distributed state representations. The need for distributed state representations in HMMs can be motivated in two ways.
References-found: 8

