URL: http://www.cs.jhu.edu/~sheppard/cs.605.755/papers/paper8-2.ps
Refering-URL: http://www.cs.jhu.edu/~sheppard/cs.605.755/sched.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: Email: ibrahim@cogs.susx.ac.uk  
Title: Evolution of Learning Rules for Supervised Tasks II: Hard Learning Problems  
Author: Ibrahim KUSCU 
Keyword: Supervised Learning, Genetic Programming, Monk's Prob lems  
Date: November 10, 1995  
Address: Brighton BN1 9QH  
Affiliation: Cognitive and Computing Sciences University of Sussex  
Abstract: Recent experiments with a genetic based encoding schema are presented as a potentially powerful tool to discover learning rules by means of evolution. The representation used is similar to the one used in Genetic Programming (GP) but it employs only a fixed set of functions to solve a variety of problems. In this paper three Monks' and parity problems are tested. The results indicate the usefulness of the encoding schema in discovering learning rules for hard learning problems. The problems and future research directions are discussed within the context of GP practices. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D.J. Chalmers. </author> <title> Evolution of learning: an experiment in genetic connectionism. </title> <editor> In Touretzky et al, editor, </editor> <title> Connectionist Models. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference: [2] <author> A. Clark and C. Thornton. </author> <title> Trading spaces: Computation, representation and the limits of learning. </title> <type> Technical Report 291, </type> <institution> COGS, University of Sussex, </institution> <year> 1993. </year>
Reference: [3] <author> D. Goldberg. </author> <title> Genetic Algorithms in Search, Optimization and Machine Learning. </title> <publisher> Addison-Wesley, </publisher> <address> Massacheusettes, </address> <year> 1989. </year>
Reference-contexts: to achieve, potentially an equal chance of producing negative and positive values when generating the expressions in the initial population. 3.2 Genetic Algorithms The Schema Theorem developed by Holland [4] based on genetic search has been proven to be useful in many applications involving large, complex and deceptive search spaces <ref> [3] </ref>. So genetic search is most likely to allow fast, robust evolution of genotypes encoding for potential learning rules as mathematical expressions. Using Genetic Algorithms (GA) the model is implemented in LISP. The top level structure of the system exhibits the following: 1. Initialize the population of expressions 2.
Reference: [4] <author> J. Holland. </author> <title> Adaptation in Natural and Artificial Systems. </title> <publisher> University of Michigan Press, </publisher> <address> Ann Arbor, USA, </address> <year> 1975. </year>
Reference-contexts: This is to achieve, potentially an equal chance of producing negative and positive values when generating the expressions in the initial population. 3.2 Genetic Algorithms The Schema Theorem developed by Holland <ref> [4] </ref> based on genetic search has been proven to be useful in many applications involving large, complex and deceptive search spaces [3]. So genetic search is most likely to allow fast, robust evolution of genotypes encoding for potential learning rules as mathematical expressions.
Reference: [5] <author> J. Koza. </author> <title> Genetic Programming:On the programming of computers by means of natural selection. </title> <publisher> MIT press, </publisher> <year> 1992. </year>
Reference: [6] <author> Ibrahim Kuscu. </author> <title> Evolution of learning rules for supervised tasks i: Simple learning problems. </title> <type> Technical Report CSRP-394, Uni. </type> <institution> of Sussex, COGS, </institution> <year> 1995. </year>
Reference-contexts: The results of the experiments in <ref> [6] </ref> and being able to discover or re-represent solutions to Monk 1 and Monk 3 problems in this paper provided evidence in support of the first hypothesis. Failing to find a successful solution for Monk 2 seems a poor support for the second hypothesis.
Reference: [7] <author> Ibrahim Kuscu. </author> <title> Incrementally learning the rules for supervised tasks: Monk's problems. </title> <type> Technical Report CSRP-396, Uni. </type> <institution> of Sussex, COGS, </institution> <year> 1995. </year>
Reference-contexts: The performance on MONK 1 and MONK 3 is at the level of competing with most of the algorithms. Although the performance on MONK 2 is very low, this is not surprising and similar to the results obtained by Thrun. Moreover, in a recent extension of the experiments <ref> [7] </ref> where the representation is improved and the performance in learning, especially on the MONK 2 problem, is increased. 11 The results emphasise how the encoding can enable us to evolve learning rules for these problems with fixed, general and non-problem-specific set of functions. <p> The second issue is that more complex and larger problems might require solutions which are represented hierarchically. In fact, this is exactly what I have found through my recent experiments <ref> [7] </ref>. The new representation provides a direct hierarchical coding for the possible solutions to Monks and parity problems and improves the possibility of finding solution as well as the speed of it. <p> In the new experiment, the representation is allowed to be random expressions organised in layers so that it can code for larger and more complex solutions <ref> [7] </ref>.
Reference: [8] <author> Una-May O'Reilly and Franz Oppacher. </author> <title> An experimental perspective on genetic programming. </title> <editor> In R. Manner and B. Manderick, editors, </editor> <booktitle> Proc of 2nd Intl Conf on Parallel Problem Solving from Nature, </booktitle> <pages> pages 331-340, </pages> <year> 1992. </year>
Reference-contexts: Although Koza claims that GP is most proper for those problems which require hierarchical representation, there is a strong evidence in my experiments and in <ref> [8] </ref> that this aspect of the GP might be quite limited. In the new experiment, the representation is allowed to be random expressions organised in layers so that it can code for larger and more complex solutions [7].
Reference: [9] <author> D. Rumelhart, G. Hinton, and R. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In D. Rumelhart, J. McClelland, </editor> <title> and the PDP Research Group, editors, Parallel Distributed Processing: Explorations in the Micro-structures of Cognition. Vols I and II. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1986a. </year>
Reference-contexts: Since the target outputs are in the range of 0 to 1, the values, once obtained after the evaluation of the expressions, are mapped to values between 0 and 1 by using a squashing function. Several functions have been tested in this mapping including logistic activation function used by <ref> [9] </ref>.
Reference: [10] <author> C. Thornton. </author> <title> Supervised learning of conditional approach: a case study. </title> <type> Technical Report 291, </type> <institution> COGS, University of Sussex, </institution> <year> 1993. </year> <month> 17 </month>
Reference: [11] <author> S. Bala et al Thrun. </author> <title> The monk's problems a performance comparison of different learning algorithms. </title> <type> Technical Report CMU-CS-91-197, </type> <institution> School of Computer Science, Carnegie-Mellon University., USA, </institution> <year> 1991. </year>
Reference-contexts: BEST RESULTS Original Coding Binary Coding Training Testing Training Testing MONK 1 91 88 - MONK 2 74 68 79 69 MONK 3 93 98 93.5 97 The results obtained are better than some of the learning algorithms used in the comparison experiment of Thrun <ref> [11] </ref>. The performance on MONK 1 and MONK 3 is at the level of competing with most of the algorithms. Although the performance on MONK 2 is very low, this is not surprising and similar to the results obtained by Thrun.
Reference: [12] <author> D. Whitley. </author> <title> The genitor algorithm and why rank based-based allocation of reproductive trials is best. </title> <editor> In J.D. Schaffer, editor, </editor> <booktitle> Proceedings of Third iInternational Conference on Genetic Algorithms, </booktitle> <pages> pages 116-123, </pages> <year> 1989. </year> <month> 18 </month>
Reference-contexts: For the model this means to select those expressions with higher scores (beginning part of the rank) and give them more chance to reproduce. In the model, parent selection technique for reproduction is normalizing by using an exponential function taken from Whitley's <ref> [12] </ref> rank-based selection technique. The function generates integer numbers from 1 to population size. The generation of numbers exhibits characteristics of a non-linear function where there is more tendency to produce smaller numbers (since higher scoring expressions are on top of the rank).
References-found: 12

