URL: http://www.aic.nrl.navy.mil/~spears/papers/mlc95.ps.gz
Refering-URL: http://www.aic.nrl.navy.mil/~spears/pubs.html
Root-URL: 
Email: gordon@aic.nrl.navy.mil  spears@aic.nrl.navy.mil  
Title: For Every Generalization Action, Is There Really an Equal and Opposite Reaction? Analysis of the
Author: R. Bharat Rao Diana Gordon William Spears 
Address: Urbana, IL 61801  Washington, DC 20375-5337  Washington, DC 20375-5337  
Affiliation: Electrical Engineering University of Illinois  Naval Research Laboratory  Naval Research Laboratory  
Abstract: The "Conservation Law for Generalization Performance" [ Schaffer, 1994 ] states that for any learning algorithm and bias, "generalization is a zero-sum enterprise." In this paper we study the law and show that while the law is true, the manner in which the Conservation Law adds up generalization performance over all target concepts, without regard to the probability with which each concept occurs, is relevant only in a uniformly random universe. We then introduce a more meaningful measure of generalization, expected generalization performance. Unlike the Conservation Law's measure of generalization performance (which is, in essence, defined to be zero), expected generalization performance is conserved only when certain symmetric properties hold in our universe. There is no reason to believe, a priori, that such symmetries exist; learning algorithms may well exhibit non-zero (expected) generalization per formance.
Abstract-found: 1
Intro-found: 1
Reference: [ Ade et al., 1995 ] <author> Ade, H.; Raedt, L. De; and Bruynooghe, M. </author> <year> 1995. </year> <title> Declarative bias for specific-to-general ilp systems. </title> <journal> Machine Learning. </journal> <note> (To appear). </note>
Reference: [ Brodley, 1995 ] <author> Brodley, C. </author> <year> 1995. </year> <title> Recursive automatic bias selection for classifier construction. </title> <journal> Machine Learning. </journal> <note> (To appear). </note>
Reference: [ Buntine, 1991 ] <author> Buntine, W. </author> <year> 1991. </year> <title> Theory refinement of Bayesian networks. </title> <editor> In D'Ambrosio, B.; Snets, P.; and Bonissone, P., editors 1991, </editor> <booktitle> Uncertainity in Artificial Intelligence: Proceedings of the Seventh Conference. </booktitle>
Reference: [ Buntine, 1993 ] <author> Buntine, W. </author> <year> 1993. </year> <title> Prior probabilities: A tutorial and unifying view. </title> <type> Technical report, </type> <institution> RI-ACS/NASA Ames Research Center. </institution>
Reference: [ Dietterich, 1989 ] <author> Dietterich, T. </author> <year> 1989. </year> <title> Limitations on inductive learning. </title> <booktitle> In Proceedings of the Sixth International Workshop on Machine Learning. </booktitle> <pages> 124-128. </pages>
Reference: [ Fisher and Schlimmer, 1988 ] <author> Fisher, D. and Schlimmer, J. </author> <year> 1988. </year> <title> Concept simplification and prediction accuracy. </title> <booktitle> In Proceedings of the Fifth International Conference on Machine Learning. </booktitle> <pages> 22-28. </pages>
Reference: [ Haussler et al., 1994 ] <author> Haussler, D.; Kearns, M.; and Schapire, R.E. </author> <year> 1994. </year> <title> Bounds on the sample complexity of Bayesian learning using informations theory and the VC dimension. </title> <booktitle> Machine Learning 14 </booktitle> <pages> 83-113. </pages>
Reference: [ Holte, 1993 ] <author> Holte, R. </author> <year> 1993. </year> <title> Very simple classification rules perform well on most commonly used datasets. </title> <booktitle> Machine Learning 11(1) </booktitle> <pages> 63-90. </pages>
Reference: [ Mitchell, 1980 ] <author> Mitchell, T.M. </author> <year> 1980. </year> <title> The need for biases in learning generalizations. </title> <type> Technical Report CBM-TR-117, </type> <institution> Computer Science Department, Rutgers University, </institution> <address> New Brunswick, NJ. </address>
Reference-contexts: Throughout this paper, the term "learner" (or learning algorithm) is equivalent to the bias <ref> [ Mitchell, 1980; Utgoff, 1986 ] </ref> used by L to generalize from to unseen cases, and "8L" should be read as "for all learning biases." The clgp states that in a classification problem, for any learning algorithm the total generalization performance over all learning situations is zero. 8L S X [GA
Reference: [ Provost and Buchanan, 1995 ] <author> Provost, F. and Buchanan, B. </author> <year> 1995. </year> <title> Inductive policy: The pragmatics of bias selection. </title> <journal> Machine Learning. </journal> <note> (To appear). </note>
Reference: [ Rao et al., 1995 ] <author> Rao, R.B.; Gordon, D.; and Spears, W. </author> <year> 1995. </year> <title> On the conservation of generalization and expected generalization. </title> <type> Technical Report AIC-95-006, </type> <institution> Naval Research Laboratory, </institution> <address> S.W. Washington DC. </address> <booktitle> (In progress). </booktitle>
Reference: [ Schaffer, 1993 ] <author> Schaffer, C. </author> <year> 1993. </year> <title> Overfitting avoidance as bias. </title> <booktitle> Machine Learning 10 </booktitle> <pages> 153-178. </pages>
Reference-contexts: 1 INTRODUCTION The theoretical analysis of inductive learning algorithms over all learning situations has been the subject of some recent research <ref> [ Wolpert, 1992; Schaffer, 1993; Wolpert, 1994 ] </ref> . This paper begins by focusing on a recent result for concept learning, the "Conservation Law for Generalization Performance" [ Schaffer, 1994 ] .
Reference: [ Schaffer, 1994 ] <author> Schaffer, C. </author> <year> 1994. </year> <title> A conservation law for generalization performance. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning. </booktitle> <pages> 259-265. </pages>
Reference-contexts: 1 INTRODUCTION The theoretical analysis of inductive learning algorithms over all learning situations has been the subject of some recent research [ Wolpert, 1992; Schaffer, 1993; Wolpert, 1994 ] . This paper begins by focusing on a recent result for concept learning, the "Conservation Law for Generalization Performance" <ref> [ Schaffer, 1994 ] </ref> . This law states that for any learning algorithm and bias, "positive performance in some learning situations must be balanced by negative performance in others." The Conservation Law (henceforth, clgp) has been fl This paper will appear in the proceedings of the 1995 Machine Learning Conference.
Reference: [ Utgoff, 1986 ] <author> Utgoff, P. E. </author> <year> 1986. </year> <title> Shift of bias of inductive concept learning. </title> <editor> In Michalski, R.S.; Carbonell, J.G.; and Mitchell, T.M., editors 1986, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, </booktitle> <volume> Vol II. </volume> <publisher> Morgan Kaufmann Publishers. </publisher> <pages> 107-148. </pages>
Reference-contexts: Throughout this paper, the term "learner" (or learning algorithm) is equivalent to the bias <ref> [ Mitchell, 1980; Utgoff, 1986 ] </ref> used by L to generalize from to unseen cases, and "8L" should be read as "for all learning biases." The clgp states that in a classification problem, for any learning algorithm the total generalization performance over all learning situations is zero. 8L S X [GA
Reference: [ Valiant, 1984 ] <author> Valiant, L.G. </author> <year> 1984. </year> <title> A theory of the learnable. </title> <journal> Communications of the ACM 27 </journal> <pages> 1134-1142. </pages>
Reference-contexts: In addition to the focus on off-training set error, Wolpert [1994] also points out other ways in which his results differ from prior learning theory, in particular, pac <ref> [ Valiant, 1984 ] </ref> . First, pac analyses typically assume the concept class is known a priori; the clgp does not.
Reference: [ Wolpert, 1992 ] <author> Wolpert, D.H. </author> <year> 1992. </year> <title> On the connection between in-sample testing and generalization error. </title> <booktitle> Complex Systems 6 </booktitle> <pages> 47-94. </pages>
Reference-contexts: 1 INTRODUCTION The theoretical analysis of inductive learning algorithms over all learning situations has been the subject of some recent research <ref> [ Wolpert, 1992; Schaffer, 1993; Wolpert, 1994 ] </ref> . This paper begins by focusing on a recent result for concept learning, the "Conservation Law for Generalization Performance" [ Schaffer, 1994 ] . <p> [ Fisher and Schlimmer, 1988; Holte, 1993; Ade et al., 1995; Brodley, 1995; Provost and Buchanan, 1995 ] ), that it is imperative to consider priors (see [ Dietterich, 1989; Buntine, 1991; Bun-tine, 1993; Haussler et al., 1994 ] ), and that we should focus on off-training set error (see <ref> [ Wolpert, 1992; Wolpert, 1994 ] </ref> ). In addition to the focus on off-training set error, Wolpert [1994] also points out other ways in which his results differ from prior learning theory, in particular, pac [ Valiant, 1984 ] .
Reference: [ Wolpert, 1994 ] <author> Wolpert, D. H. </author> <year> 1994. </year> <title> Off-training set error and a priori distinctions between learning algorithms. </title> <type> Technical report, </type> <institution> Santa Fe Institute, </institution> <address> Santa Fe, NM. </address>
Reference-contexts: 1 INTRODUCTION The theoretical analysis of inductive learning algorithms over all learning situations has been the subject of some recent research <ref> [ Wolpert, 1992; Schaffer, 1993; Wolpert, 1994 ] </ref> . This paper begins by focusing on a recent result for concept learning, the "Conservation Law for Generalization Performance" [ Schaffer, 1994 ] . <p> [ Fisher and Schlimmer, 1988; Holte, 1993; Ade et al., 1995; Brodley, 1995; Provost and Buchanan, 1995 ] ), that it is imperative to consider priors (see [ Dietterich, 1989; Buntine, 1991; Bun-tine, 1993; Haussler et al., 1994 ] ), and that we should focus on off-training set error (see <ref> [ Wolpert, 1992; Wolpert, 1994 ] </ref> ). In addition to the focus on off-training set error, Wolpert [1994] also points out other ways in which his results differ from prior learning theory, in particular, pac [ Valiant, 1984 ] .
References-found: 17

