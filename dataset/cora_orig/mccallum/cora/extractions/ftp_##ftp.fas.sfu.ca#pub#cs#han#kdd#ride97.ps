URL: ftp://ftp.fas.sfu.ca/pub/cs/han/kdd/ride97.ps
Refering-URL: http://fas.sfu.ca/cs/research/groups/DB/sections/publication/kdd/kdd.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: hang@cs.sfu.ca  
Title: Generalization and Decision Tree Induction: Efficient Classification in Data Mining  
Author: Micheline Kamber Lara Winstone Wan Gong Shan Cheng Jiawei Han fkamber, winstone, wgong, shanc, 
Address: B.C., Canada V5A 1S6  
Affiliation: Database Systems Research Laboratory School of Computing Science Simon Fraser University,  
Abstract: Efficiency and scalability are fundamental issues concerning data mining in large databases. Although classification has been studied extensively, few of the known methods take serious consideration of efficient induction in large databases and the analysis of data at multiple abstraction levels. This paper addresses the efficiency and scalability issues by proposing a data classification method which integrates attribute-oriented induction, relevance analysis, and the induction of decision trees. Such an integration leads to efficient, high-quality, multiple-level classification of large amounts of data, the relaxation of the requirement of perfect training sets, and the elegant handling of continuous and noisy data. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Agrawal, S. Ghosh, T. Imielinski, B. Iyer, and A. Swami. </author> <title> An interval classifier for database mining applications. </title> <booktitle> In Proc. 18th Intl. Conf. Very Large Data Bases (VLDB), </booktitle> <pages> pages 560573, </pages> <address> Vancouver, Canada, </address> <year> 1992. </year>
Reference-contexts: Further partitioning of the data subset at a given node is terminated if the percentage of samples belonging to any given class at that node exceeds the classification threshold. Our classification threshold is similar to the precision threshold introduced by <ref> [1] </ref>. Use of such thresholds for dynamic pruning may be more efficient than traditional tree pruning strategies [21] which first fully grow a tree and then prune it. The MedGen algorithm is outlined below.
Reference: [2] <author> H. Almuallim and T. G. Dietterich. </author> <title> Learning with many irrelevant features. </title> <booktitle> In Proc. 9th Nat. Conf. on Artificial Intelligence, </booktitle> <volume> volume 2, </volume> <pages> pages 547552, </pages> <address> Menlo Park, CA, July 1991. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Relevance analysis In the second step of our approach, we apply attribute relevance analysis to the generalized data obtained from attribute-oriented induction. This further reduces the size of the training data. A number of statistical and machine learning-based techniques for relevance analysis have been proposed in the literature <ref> [2, 10, 17] </ref>. We employ an information-theoretic asymmetric measure of relevance known as the uncertainty coefficient [19]. This coefficient is based on the information gain attribute selection measure used in C4.5 [26] for building decision trees. The relevance analysis is performed as follows. <p> Relevance analysis: Perform relevance analysis on the generalized data relation, R 1 . The information theoretic-based uncertainty coefficient [19] is used for this purpose, although other methods, such as those suggested in <ref> [2, 17] </ref> could be used. The resulting data relation is R 2 . 4. Decision-tree generation (a) Given the generalized and relevant data relation, R 2 , compute the information gain for each candidate attribute using the equations (2.1) (2.3).
Reference: [3] <author> L. Breiman, J. Friedman, R. Olshen, and C. Stone. </author> <title> Classification of Regression Trees. </title> <publisher> Wadsworth, </publisher> <year> 1984. </year>
Reference-contexts: Classification has numerous applications including credit approval, product marketing, and medical diagnosis. A number of classification techniques from the statistics and machine learning communities have been proposed [7, 25, 26, 30]. A well-accepted method of classification is the induction of decision trees <ref> [3, 25] </ref>. A decision tree is a flow-chart-like structure consisting of internal nodes, leaf nodes, and branches. Each internal node represents a decision, or test, on a data attribute, and each outgoing branch corresponds to a possible outcome of the test. Each leaf node represents a class. <p> A path is traced from the root to a leaf node which holds the class predication for that sample. Decision trees can easily be converted into IF-THEN rules [26] and used for decision-making. The efficiency of existing decision tree algorithms, such as ID3 [25] and CART <ref> [3] </ref>, has been well established for relatively small data sets [16, 22, 29]. Efficiency and scalability become issues of concern when these algorithms are applied to the mining of very large, real-world databases. Most decision tree algorithms have the restriction that the training tuples should reside in main memory. <p> MedGen with dynamic pruning (using an empirically set classification threshold of 85% to control the growth of branches during construction of the tree) was compared with MedGen with post-pruning (employing a classification threshold of 100% to avoid pre-pruning). The error-cost complexity post-pruning algorithm <ref> [3] </ref> was used since it has 8 been shown to produce small and accurate decision trees [21].
Reference: [4] <author> W. Buntine and T. Niblett. </author> <title> A further comparison of splitting rules for decision-tree induction. </title> <journal> Machine Learning, </journal> <volume> 8:75 85, </volume> <year> 1992. </year>
Reference-contexts: Furthermore, C4.5 allows the use of an attribute selection measure known as information gain [25]. In a comparative study of selection measures, information gain was found to produce accurate and small trees <ref> [4] </ref>. C4.5 has been repeatedly shown to perform well on relatively small data sets [16, 22, 29]. Section 3.1 outlines the C4.5 algorithm. MedGen and MedGenAdjust are described in Sections 3.2 and 3.3, respectively. 3.1.
Reference: [5] <author> U. Fayyad, G. Piatetsky-Shapiro, and P. Smyth. </author> <title> Knowledge discovery and data mining: Towards a unifying framework. </title> <booktitle> In Proc. 2nd Intl. Conf. on Knowledge Discovery and Data Mining (KDD'96), </booktitle> <pages> pages 8288, </pages> <address> Portland, Oregon, </address> <year> 1996. </year>
Reference-contexts: Large scale data mining applications involving complex decision-making can access billionsof bytes of data. Hence, the efficiency of such applications is paramount. Classification is a key data mining technique whereby database tuples, acting as training samples, are analyzed in order to produce a model of the given data <ref> [5, 8, 23] </ref>. Each tuple is assumed to belong to a predefined class, as determined by one of the attributes, called the classifying attribute. Once derived, the classification model can be used to categorize future data samples, as well as provide a better understanding of the database contents.
Reference: [6] <author> U. M. Fayyad. </author> <title> Branching on attribute values in decision tree generation. </title> <booktitle> In Proc. 1994 AAAI Conf., </booktitle> <pages> pages 601606, </pages> <publisher> AAAI Press, </publisher> <year> 1994. </year>
Reference-contexts: By creating a branch for each decision attribute value, C4.5 encounters the overbranching problem caused by unnecessary partitioning of the data. Various solutions have been proposed for this problem <ref> [6, 20, 27] </ref>. MedGenAdjust also addresses this problem since it allows node merging, thereby discouraging overpartitioning of the data. 6. Conclusions and future work We have proposed a simple but promising method for data classification which addresses efficiency and scalability issues regarding data mining in large databases.
Reference: [7] <author> U. M. Fayyad, S. G. Djorgovski, and N. Weir. </author> <title> Automating the analysis and cataloging of sky surveys. </title> <editor> In U. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 471493. </pages> <publisher> AAAI/MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: Classification has numerous applications including credit approval, product marketing, and medical diagnosis. A number of classification techniques from the statistics and machine learning communities have been proposed <ref> [7, 25, 26, 30] </ref>. A well-accepted method of classification is the induction of decision trees [3, 25]. A decision tree is a flow-chart-like structure consisting of internal nodes, leaf nodes, and branches.
Reference: [8] <author> U. M. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthu-rusamy. </author> <title> Advances in Knowledge Discovery and Data Mining. </title> <publisher> AAAI/MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: Large scale data mining applications involving complex decision-making can access billionsof bytes of data. Hence, the efficiency of such applications is paramount. Classification is a key data mining technique whereby database tuples, acting as training samples, are analyzed in order to produce a model of the given data <ref> [5, 8, 23] </ref>. Each tuple is assumed to belong to a predefined class, as determined by one of the attributes, called the classifying attribute. Once derived, the classification model can be used to categorize future data samples, as well as provide a better understanding of the database contents.
Reference: [9] <author> W. J. Frawley, G. Piatetsky-Shapiro, and C. J. Matheus. </author> <title> Knowledge discovery in databases: An overview. </title> <editor> In G. Piatetsky-Shapiro and W. J. Frawley, editors, </editor> <booktitle> Knowledge Discoveryin Databases, </booktitle> <pages> pages 127. </pages> <publisher> AAAI/MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: 1. Introduction Computational efficiency and scalability are two important and challenging issues in data mining. Data mining is the automated discovery of non-trivial, previously unknown, and potentially useful patterns embedded in databases <ref> [9] </ref>. The increasing computerization of all aspects of life has led to the storage of massive amounts of data. Large scale data mining applications involving complex decision-making can access billionsof bytes of data. Hence, the efficiency of such applications is paramount.
Reference: [10] <author> D. H. Freeman, Jr. </author> <title> Applied Categorical Data Analysis. </title> <publisher> Mar-cel Dekker, Inc., </publisher> <address> New York, NY, </address> <year> 1987. </year>
Reference-contexts: Unlike SLIQ and SPRINT, which operate on the raw, low-level data, we address the efficiency and scalability issues by proposing a different approach, consisting of three steps: 1) attribute-oriented induction [11, 13], where concept hierarchies are used to generalize low-level data to higher level concepts, 2) relevance analysis <ref> [10] </ref>, and 3) multi-level mining, whereby decision trees can be induced at different levels of abstraction. Attribute-oriented induction, a knowledge discovery tool which allows the generalization of data, offers two major advantages for the mining of large databases. <p> As we will illustrate in section 4, the efficiency of the resulting decision tree induction will be greatly improved. The second step of our approach aids scalability by performing attribute relevance analysis <ref> [10] </ref> on the generalized data, prior to decision tree induction. This process identifies attributes that are either irrelevant or redundant. Including such attributes in the generalized data would slow down, and possibly confuse, the classification process. <p> Relevance analysis In the second step of our approach, we apply attribute relevance analysis to the generalized data obtained from attribute-oriented induction. This further reduces the size of the training data. A number of statistical and machine learning-based techniques for relevance analysis have been proposed in the literature <ref> [2, 10, 17] </ref>. We employ an information-theoretic asymmetric measure of relevance known as the uncertainty coefficient [19]. This coefficient is based on the information gain attribute selection measure used in C4.5 [26] for building decision trees. The relevance analysis is performed as follows.
Reference: [11] <author> J. Han, Y. Cai, and N. Cercone. </author> <title> Data-driven discovery of quantitative rules in relational databases. </title> <journal> IEEE Trans. Knowledge and Data Engineering, </journal> <volume> 5:2940, </volume> <year> 1993. </year>
Reference-contexts: Unlike SLIQ and SPRINT, which operate on the raw, low-level data, we address the efficiency and scalability issues by proposing a different approach, consisting of three steps: 1) attribute-oriented induction <ref> [11, 13] </ref>, where concept hierarchies are used to generalize low-level data to higher level concepts, 2) relevance analysis [10], and 3) multi-level mining, whereby decision trees can be induced at different levels of abstraction. <p> Applying attribute-oriented induction prior to classification substantially reduces the computational complexity of this data-intensive process [14]. Data are compressed with a concept tree ascension technique which replaces attribute values by generalized concepts from corresponding attribute concept hierarchies <ref> [11] </ref>. Each generalized tuple has a count associated with it. This registers the number of tuples in the original training data that the generalized tuple now represents. The count information represents statistical data and is used later in the classification process, and enables the handling of noisy and exceptional data. <p> The count information represents statistical data and is used later in the classification process, and enables the handling of noisy and exceptional data. Concept hierarchies may be provided by domain experts or database administrators, or may be defined using the database schema <ref> [11] </ref>. Concept hierarchies for numeric attributes can be generated automatically [12]. In addition to allowing the substantial reduction in size of the training set, concept hierarchies allow the representation of data in the user's vocabulary. <p> The degree of generalization is controlled by an empirically-set generalization threshold. If the number of distinct values of an attribute is less than or equal to this threshold, then further generalization of the attribute is halted. 2 Attribute-oriented induction also performs generalization by attribute removal <ref> [11] </ref>. In this technique, an attribute having a large number of distinct values is removed if there is no higher level concept for it. Attribute removal further compacts the training data and reduces the bushiness of resulting trees. <p> First, generalization may proceed to the minimally generalized concept level, based on the attribute generalization thresholds described above. A relation is minimally generalized if each attribute satisfies its generalization threshold, and if specialization of any attribute would cause the attribute to exceed the generalization threshold <ref> [11] </ref>. A disadvantage of this approach is that, if each database attribute has many discrete values, the decision tree induced will likely be quite bushy and large. On the other extreme, generalization may proceed to very high concept levels. <p> Classification-rule generation: Generate rules according to the decision tree so derived (as in [26]). 2 Rationale of Algorithm 3.1. Step 1 is relational query processing. Step 2 has been verified in <ref> [11, 13] </ref>. Step 3 eliminates irrelevant attributes from the generalized data. Step 4 is essentially the C4.5 algorithm whose correctness has been shown in [25, 26].
Reference: [12] <author> J. Han and Y. Fu. </author> <title> Dynamic generation and refinement of concept hierarchies for knowledge discovery in databases. </title> <booktitle> In Proc. AAAI'94 Workshop on Knowledge Discovery in Databases (KDD'94), </booktitle> <pages> pages 157168, </pages> <address> Seattle, WA, </address> <year> 1994. </year>
Reference-contexts: Concept hierarchies may be provided by domain experts or database administrators, or may be defined using the database schema [11]. Concept hierarchies for numeric attributes can be generated automatically <ref> [12] </ref>. In addition to allowing the substantial reduction in size of the training set, concept hierarchies allow the representation of data in the user's vocabulary.
Reference: [13] <author> J. Han and Y. Fu. </author> <title> Exploration of the power of attribute-oriented induction in data mining. </title> <editor> In U. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 399421. </pages> <publisher> AAAI/MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: Unlike SLIQ and SPRINT, which operate on the raw, low-level data, we address the efficiency and scalability issues by proposing a different approach, consisting of three steps: 1) attribute-oriented induction <ref> [11, 13] </ref>, where concept hierarchies are used to generalize low-level data to higher level concepts, 2) relevance analysis [10], and 3) multi-level mining, whereby decision trees can be induced at different levels of abstraction. <p> First, it allows the raw data to be handled at higher conceptual levels. Generalization is performed with the use of attribute concept hierarchies, where the leaves of a given attribute's concept hierarchy correspond to the attribute's values in the data (referred to as primitive level data) <ref> [13] </ref>. Generalization of the training data is achieved by replacing primitive level data (such as numerical values for a GPA, or grade point average attribute) by higher level concepts (such as ranges 3:6 3:8, 3:8 4:0, or categories good or excellent). <p> Classification-rule generation: Generate rules according to the decision tree so derived (as in [26]). 2 Rationale of Algorithm 3.1. Step 1 is relational query processing. Step 2 has been verified in <ref> [11, 13] </ref>. Step 3 eliminates irrelevant attributes from the generalized data. Step 4 is essentially the C4.5 algorithm whose correctness has been shown in [25, 26].
Reference: [14] <author> J. Han, Y. Fu, W. Wang, J. Chiang, W. Gong, K. Koperski, D. Li, Y. Lu, A. Rajan, N. Stefanovic, B. Xia, and O. R. Zaiane. </author> <title> DBMiner: A system for mining knowledge in large relational databases. </title> <booktitle> In Proc. 1996 Intl. Conf. on Data Mining and Knowledge Discovery (KDD'96), </booktitle> <pages> pages 250255, </pages> <address> Portland, Oregon, </address> <month> August </month> <year> 1996. </year>
Reference-contexts: Furthermore, once a decision tree has been derived, the concept hierarchies can be used to generalize or specialize individual nodes in the tree, allowing attribute rolling-up or drilling-down, and reclassification of the data for the newly specified abstraction level. This interactive feature executes in our DBMiner data mining system <ref> [14] </ref> with fast response. This paper is organized as follows. Section 2 describes the attribute-oriented induction, relevance analysis, and multi-level mining components of our proposed approach. Each component has been implemented in DBMiner [14]. Section 3 presents MedGen and MedGenAdjust, our proposed multi-level decision tree induction algorithms. <p> This interactive feature executes in our DBMiner data mining system <ref> [14] </ref> with fast response. This paper is organized as follows. Section 2 describes the attribute-oriented induction, relevance analysis, and multi-level mining components of our proposed approach. Each component has been implemented in DBMiner [14]. Section 3 presents MedGen and MedGenAdjust, our proposed multi-level decision tree induction algorithms. A performance evaluation is given in Section 4. Section 5 addresses related issues, such as classification accuracy. Conclusions and future work are discussed in Section 6. 2. <p> This section describes each step in detail. 2.1. Attribute-oriented induction In real-world applications, data mining tasks such as classification are applied to training data consisting of thousands or millions of tuples. Applying attribute-oriented induction prior to classification substantially reduces the computational complexity of this data-intensive process <ref> [14] </ref>. Data are compressed with a concept tree ascension technique which replaces attribute values by generalized concepts from corresponding attribute concept hierarchies [11]. Each generalized tuple has a count associated with it. This registers the number of tuples in the original training data that the generalized tuple now represents. <p> Attribute removal further compacts the training data and reduces the bushiness of resulting trees. In our approach, the generalized data are stored in a multi-dimensional data cube <ref> [14] </ref>, similar to that used in data warehousing [15]. This is a multi-dimensional array structure. Each dimension represents a generalized attribute, and each cell stores the value of some aggregate attribute, such as the count information described earlier. <p> The data mining task is to classify the data according to the database attribute median income (median family income), based on the attributes city area (in square miles), state, population 92, and pub incomep. This task is specified in our data mining query language, DMQL <ref> [14] </ref>, as shown below in Example 2.1. <p> This results in heavy computation from the calculation of memory addresses and subsequent accessing of the raw data. Fast accessing and quick calculation of addresses are a feature of the data cube's storage and indexing system <ref> [14, 15] </ref>. Hence, our experiments focus on comparing the efficiency of MinGen, MedGen, and MedGenAdjust. Classification accuracy is discussed in Section 5. All experiments were conducted on a Pentium-166 PC with 64 MB RAM, running Windows/NT. <p> Hence, our experiments focus on comparing the efficiency of MinGen, MedGen, and MedGenAdjust. Classification accuracy is discussed in Section 5. All experiments were conducted on a Pentium-166 PC with 64 MB RAM, running Windows/NT. The algorithms were implemented in DBMiner <ref> [14] </ref> using Mi-croSoft Visual C ++ . (DBMiner is accessible on the web at http://db.cs.sfu.ca/DBMiner). The results were obtained from the execution of a query from the CITYDATA database. The query was executed ten times, and the average CPU time in milliseconds was calculated.
Reference: [15] <author> V. Harinarayan, A. Rajaraman, and J. D. Ullman. </author> <title> Implementing data cubes efficiently. </title> <booktitle> In Proc. 1996 ACM-SIGMOD Int. Conf. Management of Data, pages 205216, </booktitle> <address> Montreal, Canada, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: Attribute removal further compacts the training data and reduces the bushiness of resulting trees. In our approach, the generalized data are stored in a multi-dimensional data cube [14], similar to that used in data warehousing <ref> [15] </ref>. This is a multi-dimensional array structure. Each dimension represents a generalized attribute, and each cell stores the value of some aggregate attribute, such as the count information described earlier. Consider a CITYDATA database consisting of statistics describing incomes and populations, collected for cities and counties in the United States. <p> This results in heavy computation from the calculation of memory addresses and subsequent accessing of the raw data. Fast accessing and quick calculation of addresses are a feature of the data cube's storage and indexing system <ref> [14, 15] </ref>. Hence, our experiments focus on comparing the efficiency of MinGen, MedGen, and MedGenAdjust. Classification accuracy is discussed in Section 5. All experiments were conducted on a Pentium-166 PC with 64 MB RAM, running Windows/NT.
Reference: [16] <author> L. B. Holder. </author> <title> Intermediate decision trees. </title> <booktitle> In Proc. 14th Intl. Joint Conf. on Artificial Intelligence, </booktitle> <pages> pages 10561062, </pages> <address> Montreal, Canada, </address> <month> Aug </month> <year> 1995. </year>
Reference-contexts: Decision trees can easily be converted into IF-THEN rules [26] and used for decision-making. The efficiency of existing decision tree algorithms, such as ID3 [25] and CART [3], has been well established for relatively small data sets <ref> [16, 22, 29] </ref>. Efficiency and scalability become issues of concern when these algorithms are applied to the mining of very large, real-world databases. Most decision tree algorithms have the restriction that the training tuples should reside in main memory. <p> Furthermore, C4.5 allows the use of an attribute selection measure known as information gain [25]. In a comparative study of selection measures, information gain was found to produce accurate and small trees [4]. C4.5 has been repeatedly shown to perform well on relatively small data sets <ref> [16, 22, 29] </ref>. Section 3.1 outlines the C4.5 algorithm. MedGen and MedGenAdjust are described in Sections 3.2 and 3.3, respectively. 3.1. C4.5 overview The C4.5 method [25, 26] is a greedy tree growing algorithm which constructs decision trees in a top-down recursive divide-and-conquer strategy.
Reference: [17] <author> G. H. John. </author> <title> Irrelevant features and the subset selection problem. </title> <editor> In W. W. Cohen and H. Hirsh, editors, </editor> <booktitle> Proc. 11th Intl. Conf. on Machine Learning, </booktitle> <pages> pages 121129, </pages> <address> San Fran-sisco, CA, July 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Relevance analysis In the second step of our approach, we apply attribute relevance analysis to the generalized data obtained from attribute-oriented induction. This further reduces the size of the training data. A number of statistical and machine learning-based techniques for relevance analysis have been proposed in the literature <ref> [2, 10, 17] </ref>. We employ an information-theoretic asymmetric measure of relevance known as the uncertainty coefficient [19]. This coefficient is based on the information gain attribute selection measure used in C4.5 [26] for building decision trees. The relevance analysis is performed as follows. <p> Relevance analysis: Perform relevance analysis on the generalized data relation, R 1 . The information theoretic-based uncertainty coefficient [19] is used for this purpose, although other methods, such as those suggested in <ref> [2, 17] </ref> could be used. The resulting data relation is R 2 . 4. Decision-tree generation (a) Given the generalized and relevant data relation, R 2 , compute the information gain for each candidate attribute using the equations (2.1) (2.3).
Reference: [18] <author> M. Kamber, R. Shinghal, D. L. Collins, G. S. Francis, and A. C. Evans. </author> <title> Model-based 3D segmentation of multiple sclerosis lesions in magnetic resonance brain images. </title> <journal> IEEE Transactions on Medical Imaging, </journal> <volume> 14(3):442453, </volume> <year> 1995. </year>
Reference-contexts: C4.5 was chosen because it is generally accepted as a standard for decision tree algorithms, and has been extensively tested. It has been used in many application areas ranging from medicine <ref> [18] </ref> to game theory [24], and is the basis of several commercial rule-induction systems [25]. Furthermore, C4.5 allows the use of an attribute selection measure known as information gain [25]. In a comparative study of selection measures, information gain was found to produce accurate and small trees [4].
Reference: [19] <author> H. J. Loether and D. G. McTavish. </author> <title> Descriptive and Inferential Statistics: An Introduction. </title> <publisher> Allyn and Bacon, </publisher> <year> 1993. </year>
Reference-contexts: This further reduces the size of the training data. A number of statistical and machine learning-based techniques for relevance analysis have been proposed in the literature [2, 10, 17]. We employ an information-theoretic asymmetric measure of relevance known as the uncertainty coefficient <ref> [19] </ref>. This coefficient is based on the information gain attribute selection measure used in C4.5 [26] for building decision trees. The relevance analysis is performed as follows. Let the generalized data be a set P of p data samples. <p> The resultant relation is R 1 . The generalization level is controlled by t i , a set of attribute thresholds associated with attributes A i . 3. Relevance analysis: Perform relevance analysis on the generalized data relation, R 1 . The information theoretic-based uncertainty coefficient <ref> [19] </ref> is used for this purpose, although other methods, such as those suggested in [2, 17] could be used. The resulting data relation is R 2 . 4.
Reference: [20] <author> M. Mehta, R. Agrawal, and J. Rissanen. SLIQ: </author> <title> A fast scalable classifier for data mining. </title> <booktitle> In Proc. 1996 Intl. Conf. on Extending Database Technology (EDBT'96), </booktitle> <address> Avignon, France, </address> <month> March </month> <year> 1996. </year>
Reference-contexts: Hence, this restriction limits the scalability of such algorithms, where the decision tree construction can become inefficient due to swapping of the training samples in and out of main and cache memories. The induction of decision trees from very large training sets has been previously addressed by the SLIQ <ref> [20] </ref> and SPRINT [27] decision tree algorithms. These propose pre-sorting techniques on disk-resident data sets that are too large to fit in memory. <p> Hence, aside from increasing efficiency, attribute-oriented induction may result in classification trees that are more understandable, smaller, and therefore easier to interpret than trees obtained from methods operating on un-generalized (larger) sets of low-level data (such as <ref> [20, 27] </ref>). The degree of generalization is controlled by an empirically-set generalization threshold. If the number of distinct values of an attribute is less than or equal to this threshold, then further generalization of the attribute is halted. 2 Attribute-oriented induction also performs generalization by attribute removal [11]. <p> Select the candidate attribute which gives the maximum information gain as the decision or test" attribute at this current level, and partition the current set of objects accordingly. Although information gain is used here, as in C4.5 [25], alternatively other selection measurements (such as the gini-index <ref> [20, 30] </ref>) could be used. (b) For each subset created by the partitioning, repeat Step 4a to further classify data until either (1) all or a substantial proportion (no less than , the classification threshold) of the objects are in one class, (2) no more attributes can be used for further <p> By creating a branch for each decision attribute value, C4.5 encounters the overbranching problem caused by unnecessary partitioning of the data. Various solutions have been proposed for this problem <ref> [6, 20, 27] </ref>. MedGenAdjust also addresses this problem since it allows node merging, thereby discouraging overpartitioning of the data. 6. Conclusions and future work We have proposed a simple but promising method for data classification which addresses efficiency and scalability issues regarding data mining in large databases.
Reference: [21] <author> J. Mingers. </author> <title> An empirical comparison of pruning methods for decision-tree induction. </title> <booktitle> Machine Learning, </booktitle> <address> 4(3):227243, </address> <year> 1989. </year>
Reference-contexts: Our classification threshold is similar to the precision threshold introduced by [1]. Use of such thresholds for dynamic pruning may be more efficient than traditional tree pruning strategies <ref> [21] </ref> which first fully grow a tree and then prune it. The MedGen algorithm is outlined below. Algorithm 3.1 (MedGen) Perform data classification using a prespecified classifying attribute on a relational database by integration of attribute-oriented induction and relevance analysis with a decision tree induction method. <p> The error-cost complexity post-pruning algorithm [3] was used since it has 8 been shown to produce small and accurate decision trees <ref> [21] </ref>.
Reference: [22] <author> R. Mooney, J. Shavlik, G. Towell, and A. Grove. </author> <title> An experimental comparison of symbolic and connectionist learning algorithms. </title> <booktitle> In Proc. 11th Intl. Joint Conf. on Artificial Intelligence, </booktitle> <pages> pages 775787, </pages> <address> Detroit, MI, Aug 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Decision trees can easily be converted into IF-THEN rules [26] and used for decision-making. The efficiency of existing decision tree algorithms, such as ID3 [25] and CART [3], has been well established for relatively small data sets <ref> [16, 22, 29] </ref>. Efficiency and scalability become issues of concern when these algorithms are applied to the mining of very large, real-world databases. Most decision tree algorithms have the restriction that the training tuples should reside in main memory. <p> Furthermore, C4.5 allows the use of an attribute selection measure known as information gain [25]. In a comparative study of selection measures, information gain was found to produce accurate and small trees [4]. C4.5 has been repeatedly shown to perform well on relatively small data sets <ref> [16, 22, 29] </ref>. Section 3.1 outlines the C4.5 algorithm. MedGen and MedGenAdjust are described in Sections 3.2 and 3.3, respectively. 3.1. C4.5 overview The C4.5 method [25, 26] is a greedy tree growing algorithm which constructs decision trees in a top-down recursive divide-and-conquer strategy.
Reference: [23] <author> G. Piatetsky-Shapiro and W. J. Frawley. </author> <title> Knowledge Discovery in Databases. </title> <publisher> AAAI/MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: Large scale data mining applications involving complex decision-making can access billionsof bytes of data. Hence, the efficiency of such applications is paramount. Classification is a key data mining technique whereby database tuples, acting as training samples, are analyzed in order to produce a model of the given data <ref> [5, 8, 23] </ref>. Each tuple is assumed to belong to a predefined class, as determined by one of the attributes, called the classifying attribute. Once derived, the classification model can be used to categorize future data samples, as well as provide a better understanding of the database contents.
Reference: [24] <author> J. R. Quinlan. </author> <title> Learning efficient classification procedures and their application to chess end-games. </title> <editor> In M. et al., editor, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, </booktitle> <volume> Vol. 1, </volume> <pages> pages 463482. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1983. </year>
Reference-contexts: C4.5 was chosen because it is generally accepted as a standard for decision tree algorithms, and has been extensively tested. It has been used in many application areas ranging from medicine [18] to game theory <ref> [24] </ref>, and is the basis of several commercial rule-induction systems [25]. Furthermore, C4.5 allows the use of an attribute selection measure known as information gain [25]. In a comparative study of selection measures, information gain was found to produce accurate and small trees [4].
Reference: [25] <author> J. R. Quinlan. </author> <title> Induction of decision trees. </title> <booktitle> Machine Learning, </booktitle> <address> 1:81106, </address> <year> 1986. </year>
Reference-contexts: Classification has numerous applications including credit approval, product marketing, and medical diagnosis. A number of classification techniques from the statistics and machine learning communities have been proposed <ref> [7, 25, 26, 30] </ref>. A well-accepted method of classification is the induction of decision trees [3, 25]. A decision tree is a flow-chart-like structure consisting of internal nodes, leaf nodes, and branches. <p> Classification has numerous applications including credit approval, product marketing, and medical diagnosis. A number of classification techniques from the statistics and machine learning communities have been proposed [7, 25, 26, 30]. A well-accepted method of classification is the induction of decision trees <ref> [3, 25] </ref>. A decision tree is a flow-chart-like structure consisting of internal nodes, leaf nodes, and branches. Each internal node represents a decision, or test, on a data attribute, and each outgoing branch corresponds to a possible outcome of the test. Each leaf node represents a class. <p> A path is traced from the root to a leaf node which holds the class predication for that sample. Decision trees can easily be converted into IF-THEN rules [26] and used for decision-making. The efficiency of existing decision tree algorithms, such as ID3 <ref> [25] </ref> and CART [3], has been well established for relatively small data sets [16, 22, 29]. Efficiency and scalability become issues of concern when these algorithms are applied to the mining of very large, real-world databases. <p> Hence, attribute-oriented induction allows the user to view the data at more meaningful abstractions. In many decision tree induction examples, such as in <ref> [25] </ref>, the data to be classified are presented at a relatively high concept level, such as mild" (temperature) and high" (humidity). This implies that some preprocessing may have been performed on the primitive data to bring it to a higher concept level. <p> The decision tree induction algorithm on which Med-Gen and MedGenAdjust are based is C4.5 [26] (an earlier version of which is known as ID3 <ref> [25] </ref>). C4.5 was chosen because it is generally accepted as a standard for decision tree algorithms, and has been extensively tested. It has been used in many application areas ranging from medicine [18] to game theory [24], and is the basis of several commercial rule-induction systems [25]. <p> is known as ID3 <ref> [25] </ref>). C4.5 was chosen because it is generally accepted as a standard for decision tree algorithms, and has been extensively tested. It has been used in many application areas ranging from medicine [18] to game theory [24], and is the basis of several commercial rule-induction systems [25]. Furthermore, C4.5 allows the use of an attribute selection measure known as information gain [25]. In a comparative study of selection measures, information gain was found to produce accurate and small trees [4]. C4.5 has been repeatedly shown to perform well on relatively small data sets [16, 22, 29]. <p> It has been used in many application areas ranging from medicine [18] to game theory [24], and is the basis of several commercial rule-induction systems <ref> [25] </ref>. Furthermore, C4.5 allows the use of an attribute selection measure known as information gain [25]. In a comparative study of selection measures, information gain was found to produce accurate and small trees [4]. C4.5 has been repeatedly shown to perform well on relatively small data sets [16, 22, 29]. Section 3.1 outlines the C4.5 algorithm. <p> C4.5 has been repeatedly shown to perform well on relatively small data sets [16, 22, 29]. Section 3.1 outlines the C4.5 algorithm. MedGen and MedGenAdjust are described in Sections 3.2 and 3.3, respectively. 3.1. C4.5 overview The C4.5 method <ref> [25, 26] </ref> is a greedy tree growing algorithm which constructs decision trees in a top-down recursive divide-and-conquer strategy. The tree starts as a single node containing the training samples. If the samples are all of the same class, then the node becomes a leaf and is labeled with that class. <p> Select the candidate attribute which gives the maximum information gain as the decision or test" attribute at this current level, and partition the current set of objects accordingly. Although information gain is used here, as in C4.5 <ref> [25] </ref>, alternatively other selection measurements (such as the gini-index [20, 30]) could be used. (b) For each subset created by the partitioning, repeat Step 4a to further classify data until either (1) all or a substantial proportion (no less than , the classification threshold) of the objects are in one class, <p> Step 1 is relational query processing. Step 2 has been verified in [11, 13]. Step 3 eliminates irrelevant attributes from the generalized data. Step 4 is essentially the C4.5 algorithm whose correctness has been shown in <ref> [25, 26] </ref>. Our modification uses quantitative information collected in Step 1 to terminate classification if the frequency of the majority class in a given subset is greater than the classification threshold, or if the percentage of training objects represented by the subset is less than the exception threshold. <p> Presently, we are investigating some form of weighted accuracy measure which considers the correctness of objects belonging to multiple classes. Additional issues. An inherent weakness of C4.5 is that the information gain attribute selection criterion has a tendency to favor many-valued attributes <ref> [25] </ref>. Quinlan offers a solution to this problem in [26]. Attribute-oriented induction is yet another solution since it results in the generalization of many values into a small set of distinct values.
Reference: [26] <author> J. R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Mor-gan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: Classification has numerous applications including credit approval, product marketing, and medical diagnosis. A number of classification techniques from the statistics and machine learning communities have been proposed <ref> [7, 25, 26, 30] </ref>. A well-accepted method of classification is the induction of decision trees [3, 25]. A decision tree is a flow-chart-like structure consisting of internal nodes, leaf nodes, and branches. <p> In order to classify an unlabeled data sample, the classifier tests the attribute values of the sample against the decision tree. A path is traced from the root to a leaf node which holds the class predication for that sample. Decision trees can easily be converted into IF-THEN rules <ref> [26] </ref> and used for decision-making. The efficiency of existing decision tree algorithms, such as ID3 [25] and CART [3], has been well established for relatively small data sets [16, 22, 29]. <p> A number of statistical and machine learning-based techniques for relevance analysis have been proposed in the literature [2, 10, 17]. We employ an information-theoretic asymmetric measure of relevance known as the uncertainty coefficient [19]. This coefficient is based on the information gain attribute selection measure used in C4.5 <ref> [26] </ref> for building decision trees. The relevance analysis is performed as follows. Let the generalized data be a set P of p data samples. Suppose the classifying attribute has m distinct values defining m distinct classes P i (for i = 1; : : : ; m). <p> While MedGen directly applies a decision tree algorithm to the generalized data, MedGenAdjust allows for dynamic adjustment between different levels of abstraction during the tree building process. The decision tree induction algorithm on which Med-Gen and MedGenAdjust are based is C4.5 <ref> [26] </ref> (an earlier version of which is known as ID3 [25]). C4.5 was chosen because it is generally accepted as a standard for decision tree algorithms, and has been extensively tested. <p> C4.5 has been repeatedly shown to perform well on relatively small data sets [16, 22, 29]. Section 3.1 outlines the C4.5 algorithm. MedGen and MedGenAdjust are described in Sections 3.2 and 3.3, respectively. 3.1. C4.5 overview The C4.5 method <ref> [25, 26] </ref> is a greedy tree growing algorithm which constructs decision trees in a top-down recursive divide-and-conquer strategy. The tree starts as a single node containing the training samples. If the samples are all of the same class, then the node becomes a leaf and is labeled with that class. <p> The MedGen algorithm The MedGen algorithm integrates attribute-oriented induction and relevance analysis with a slightly modified ver sion of the C4.5 decision tree algorithm <ref> [26] </ref>. 5 MedGen introduces two thresholds which are not part of C4.5. These are an exception threshold and a classification threshold. A criticism of C4.5 is that, because of the recursive partitioning, some resulting data subsets may become so small that partitioning them further would have no statistically significant basis. <p> Classification-rule generation: Generate rules according to the decision tree so derived (as in <ref> [26] </ref>). 2 Rationale of Algorithm 3.1. Step 1 is relational query processing. Step 2 has been verified in [11, 13]. Step 3 eliminates irrelevant attributes from the generalized data. Step 4 is essentially the C4.5 algorithm whose correctness has been shown in [25, 26]. <p> Step 1 is relational query processing. Step 2 has been verified in [11, 13]. Step 3 eliminates irrelevant attributes from the generalized data. Step 4 is essentially the C4.5 algorithm whose correctness has been shown in <ref> [25, 26] </ref>. Our modification uses quantitative information collected in Step 1 to terminate classification if the frequency of the majority class in a given subset is greater than the classification threshold, or if the percentage of training objects represented by the subset is less than the exception threshold. <p> Recall that prior to decision tree induction, the task relevant data can be generalized to a number of different degrees, e.g., 7 1. No generalization: The training data are not general-ized at all. This is the case for typical applications of decision tree classifiers, such as C4.5 <ref> [26] </ref>. We call such an algorithm NoGen. 2. Minimal-level generalization: The training data are generalized to a minimally generalized concept level (see Section 2.1). Subsequently, decision tree induc tion is applied. We call such an algorithm MinGen. 3. <p> However, the subsequent advantage of stored data in a data cube is that it allows fast indexing to cells (or slices) of the cube. Recall that C4.5 requires the calculation of information gain for each attribute, considering each attribute value, at each node <ref> [26] </ref>. This results in heavy computation from the calculation of memory addresses and subsequent accessing of the raw data. Fast accessing and quick calculation of addresses are a feature of the data cube's storage and indexing system [14, 15]. <p> Additional issues. An inherent weakness of C4.5 is that the information gain attribute selection criterion has a tendency to favor many-valued attributes [25]. Quinlan offers a solution to this problem in <ref> [26] </ref>. Attribute-oriented induction is yet another solution since it results in the generalization of many values into a small set of distinct values. By creating a branch for each decision attribute value, C4.5 encounters the overbranching problem caused by unnecessary partitioning of the data. <p> Relevance analysis is then applied to remove irrelevant data attributes, thereby further compacting the generalized data. The resulting data are much smaller than the original training set. MedGen then applies a modified version of the C4.5 algorithm <ref> [26] </ref> to extract classification rules from the generalized relation by inducing a decision tree. MedGenAdjust is a refinement of MedGen since it allows for level-adjustment across multiple abstraction levels during decision tree construction.
Reference: [27] <author> J. Shafer, R. Agrawal, and M. Mehta. SPRINT: </author> <title> a scalable parallel classifier for data mining. </title> <booktitle> In Proc. 22nd Intl. Conf. Very Large Data Bases (VLDB), </booktitle> <pages> pages 544555, </pages> <address> Mumbai (Bombay), India, </address> <year> 1996. </year>
Reference-contexts: The induction of decision trees from very large training sets has been previously addressed by the SLIQ [20] and SPRINT <ref> [27] </ref> decision tree algorithms. These propose pre-sorting techniques on disk-resident data sets that are too large to fit in memory. <p> These propose pre-sorting techniques on disk-resident data sets that are too large to fit in memory. While SLIQ's scalability, however, is limited by the use of a memory-resident data structure, SPRINT removes all memory restrictions and hence can handle data sets that are too large for SLIQ <ref> [27] </ref>. <p> Hence, aside from increasing efficiency, attribute-oriented induction may result in classification trees that are more understandable, smaller, and therefore easier to interpret than trees obtained from methods operating on un-generalized (larger) sets of low-level data (such as <ref> [20, 27] </ref>). The degree of generalization is controlled by an empirically-set generalization threshold. If the number of distinct values of an attribute is less than or equal to this threshold, then further generalization of the attribute is halted. 2 Attribute-oriented induction also performs generalization by attribute removal [11]. <p> By creating a branch for each decision attribute value, C4.5 encounters the overbranching problem caused by unnecessary partitioning of the data. Various solutions have been proposed for this problem <ref> [6, 20, 27] </ref>. MedGenAdjust also addresses this problem since it allows node merging, thereby discouraging overpartitioning of the data. 6. Conclusions and future work We have proposed a simple but promising method for data classification which addresses efficiency and scalability issues regarding data mining in large databases.
Reference: [28] <author> R. Uthurusamy, U. Fayyad, and S. Spangler. </author> <title> Learning useful rules from inconclusive data. </title> <editor> In G. Piatetsky-Shapiro and W. J. Frawley, editors, </editor> <booktitle> Knowledge Discovery in Databases, </booktitle> <pages> pages 141157, </pages> <address> Menlo Park, CA, 1991. </address> <publisher> AAAI/MIT Press. </publisher>
Reference-contexts: This problem has not yet been adequately solved in the literature. A second guess heuristic has been used to address this situation <ref> [28] </ref> whereby a classification prediction is counted as correct if it agrees with the majority class of the leaf at which the object falls", or if it agrees with the second class in majority at that leaf.
Reference: [29] <author> S. M. Weiss and I. Kapouleas. </author> <title> An empirical comparison of pattern recognition, neural nets, and machine learning classification methods. </title> <booktitle> In Proc. 11th Intl. Joint Conf. on Artificial Intelligence, </booktitle> <pages> pages 781787, </pages> <address> Detroit, MI, Aug 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Decision trees can easily be converted into IF-THEN rules [26] and used for decision-making. The efficiency of existing decision tree algorithms, such as ID3 [25] and CART [3], has been well established for relatively small data sets <ref> [16, 22, 29] </ref>. Efficiency and scalability become issues of concern when these algorithms are applied to the mining of very large, real-world databases. Most decision tree algorithms have the restriction that the training tuples should reside in main memory. <p> Furthermore, C4.5 allows the use of an attribute selection measure known as information gain [25]. In a comparative study of selection measures, information gain was found to produce accurate and small trees [4]. C4.5 has been repeatedly shown to perform well on relatively small data sets <ref> [16, 22, 29] </ref>. Section 3.1 outlines the C4.5 algorithm. MedGen and MedGenAdjust are described in Sections 3.2 and 3.3, respectively. 3.1. C4.5 overview The C4.5 method [25, 26] is a greedy tree growing algorithm which constructs decision trees in a top-down recursive divide-and-conquer strategy.
Reference: [30] <author> S. M. Weiss and C. A. </author> <title> Kulikowski. Computer Systems that Learn: Classification and Prediction Methods from Statistics, Neural Nets, Machine Learning, and Expert Systems. </title> <publisher> Morgan Kaufman, </publisher> <year> 1991. </year> <month> 10 </month>
Reference-contexts: Classification has numerous applications including credit approval, product marketing, and medical diagnosis. A number of classification techniques from the statistics and machine learning communities have been proposed <ref> [7, 25, 26, 30] </ref>. A well-accepted method of classification is the induction of decision trees [3, 25]. A decision tree is a flow-chart-like structure consisting of internal nodes, leaf nodes, and branches. <p> Select the candidate attribute which gives the maximum information gain as the decision or test" attribute at this current level, and partition the current set of objects accordingly. Although information gain is used here, as in C4.5 [25], alternatively other selection measurements (such as the gini-index <ref> [20, 30] </ref>) could be used. (b) For each subset created by the partitioning, repeat Step 4a to further classify data until either (1) all or a substantial proportion (no less than , the classification threshold) of the objects are in one class, (2) no more attributes can be used for further
References-found: 30

