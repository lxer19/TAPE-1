URL: http://www.cs.wisc.edu/~dewitt/papers/paralleldb/nestedloops.ps
Refering-URL: http://www.cs.wisc.edu/~dewitt/paralleldb.html
Root-URL: 
Title: Nested Loops Revisited  
Author: David J. DeWitt Jeffrey F. Naughton Joseph Burger 
Abstract: The research community has considered hash-based parallel join algorithms the algorithms of choice for almost a decade. However, almost none of the commercial parallel database systems use hashing-based join algorithms, using instead nested-loops with index or sort-merge. While the research literature abounds with comparisons between the various hash-based and sort-merge join algorithms, to our knowledge there is no published comparison between the parallel hash-based algorithms and a parallel nested loops algorithm with index. In this paper we present a comparison of four variants of parallel index nested loops algorithms with the parallel hybrid hash algorithm. The conclusions of our experiments both with an analytic model and with an implementation in the Gamma parallel database system are that (1) overall, parallel hybrid hash is the method of choice, but (2) there are cases where nested-loops with index wins big enough that systems could profit from implementing both algorithms. Furthermore, our experiments show that among the nested loop algorithms, one of them, subset nested loops with sorting, clearly dominates. 
Abstract-found: 1
Intro-found: 1
Reference: [BE77] <author> M. W. Blasgen and K. P. Eswaran. </author> <title> Storage and access in relational databases. </title> <journal> IBM Systems Journal, </journal> <volume> 16(4), </volume> <year> 1977. </year>
Reference-contexts: 1 Introduction The research community has long considered hash-based joins to be the method of choice for performing joins in multiprocessor database systems. One line of reasoning behind this goes roughly like this: it has long been known that in uniprocessor systems, sort-merge beats nested loops with index almost always <ref> [BE77] </ref>; also, hybrid-hash beats sort-merge just about everywhere in both the uniprocessor and multiprocessor case [DKO + 84, SD89]; so, transitively, one can expect fl Department of Computer Sciences, University of Wisconsin-Madison. <p> Wolf et al. [WDYT90, WDY90] consider the performance of parallel hashing and sort-merge algorithms in the presence of skew, but do not consider parallel nested loop algorithms. In early work, Blasgen and Eswaran <ref> [BE77] </ref> compared sort-merge with nested-loops with index on uniproces-sors, and concluded that sort merge almost always wins unless there is an appropriate clustered index on one of the join operands. <p> Furthermore, we will assume that there is an index on the attribute S:B. Then the uniprocessor nested loops with index join algorithm is just the following <ref> [BE77] </ref>: for each tuple r in R do lookup the value r.A in the index on s.B; for each S tuple s returned by the lookup output answer tuple (r,s); endFor; endFor; We now consider how to implement a version of this algorithm in a shared-nothing parallel database system [Sto86].
Reference: [BGMP79] <author> M. W. Blasgen, J. Gray, M. Mitoma, and T. Price. </author> <title> The convoy phenomenon. </title> <journal> Operating System Review, </journal> <volume> 13(2), </volume> <year> 1979. </year>
Reference-contexts: Each module supports eight full-duplex, serial, reliable communication channels operating at 2.8 megabytes/sec. Gamma is built on top of an operating system designed specifically for supporting database management systems. NOSE provides multiple, lightweight processes with shared memory. A non-preemptive scheduling policy is used to help prevent convoys <ref> [BGMP79] </ref> from occurring. NOSE provides communications between NOSE processes using the reliable message passing hardware of the Intel iPSC/2 hypercube. File services in NOSE are based on the Wisconsin Storage System (WiSS) [CDKK85].
Reference: [CDKK85] <author> H-T. Chou, D. J. Dewitt, R. H. Katz, and A. C. Klug. </author> <title> Design and implementation of the Wisconsin Storage System. </title> <journal> Software| Practice and Experience, </journal> <volume> 15(10) </volume> <pages> 943-962, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: A non-preemptive scheduling policy is used to help prevent convoys [BGMP79] from occurring. NOSE provides communications between NOSE processes using the reliable message passing hardware of the Intel iPSC/2 hypercube. File services in NOSE are based on the Wisconsin Storage System (WiSS) <ref> [CDKK85] </ref>. The services provided by WiSS include sequential files, byte-stream files as in UNIX, B + tree indices, long data items, an external sort utility, and a scan mechanism.
Reference: [CK85] <author> G. Copeland and S. Khoshafian. </author> <title> A decomposition storage model. </title> <booktitle> In Proc. of the ACM SIGMOD Conference, </booktitle> <pages> pages 268 - 279, </pages> <address> Austin, Texas, </address> <month> May </month> <year> 1985. </year>
Reference-contexts: For this reason MapS is declustered by hashing throughout the system on the attribute MapS:B. The concept of such a mapping relation from attributes to surrogates was introduced by Copeland and Khoshafian <ref> [CK85] </ref> in their decomposition storage model. In the remainder of this paper, for simplicity we will assume that P 0 is itself a key for S, so we can use P = P 0 . Note that MapS is not a join index [Val87].
Reference: [DG85] <author> D. M. DeWitt and R. Gerber. </author> <title> Multiprocessor hash-based join algorithms. </title> <booktitle> In Proc. of the Twelfth VLDB, </booktitle> <pages> pages 151-164, </pages> <address> Stock-holm, Sweden, </address> <year> 1985. </year>
Reference-contexts: Furthermore, the results show that one version of the parallel index nested loops algorithms, subset nested loops with sorting, clearly dominates the others. In related work, DeWitt and Gerber investigated the performance of four parallel hashing join algorithms <ref> [DG85] </ref>, while Schneider and DeWitt compared several parallel hashing algorithms with parallel sort-merge [SD89]. Kitsuregawa [KTMo83] proposed an algorithm based on hashing for redistribution followed by sort-merge, intended to take advantage of special-purpose sorting hardware. None of these papers compared the hashing join algorithms with nested loop with index algorithms. <p> An optimization of this technique is discussed in Subsection 4.2. 2.4 Hybrid Hash The hybrid hash join algorithm has been described in detail elsewhere in the literature <ref> [DKO + 84, DG85, SD89] </ref>; here we give a top-level overview of the important aspects of parallel hash join algorithms. In the following we will denote hybrid hash by HH.
Reference: [DGS + 90] <author> D. DeWitt, S. Ghandeharizadeh, D. Schnei-der, A. Bricker, H.-I Hsiao, and R. Ras-mussen. </author> <title> The Gamma database machine project. </title> <journal> IEEE TKDE, </journal> <volume> 2(1), </volume> <month> March </month> <year> 1990. </year>
Reference-contexts: However, in our opinion an analytic model alone can never provide the detailed insight and confidence that is engendered by an actual implementation. Accordingly, we also implemented the algorithms on the Gamma parallel database system <ref> [DGS + 90] </ref>. <p> Our solution is to build an additional relation, which we will call MapS, that maps from S join attribute values to S partitioning attribute values. In more detail, suppose that S is range or hash partitioned <ref> [DGS + 90] </ref> on attribute S:P 0 , where P 0 6= B. Furthermore, let P be a key for S that contains P 0 .
Reference: [DKO + 84] <author> D. J. DeWitt, R. H. Katz, F. Olken, L. D. Shapiro, M. R. Stonebraker, and D. Wood. </author> <title> Implementation techniques for main memory database systems. </title> <booktitle> In Proc. of the ACM SIGMOD Conference, </booktitle> <pages> pages 1-8, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: One line of reasoning behind this goes roughly like this: it has long been known that in uniprocessor systems, sort-merge beats nested loops with index almost always [BE77]; also, hybrid-hash beats sort-merge just about everywhere in both the uniprocessor and multiprocessor case <ref> [DKO + 84, SD89] </ref>; so, transitively, one can expect fl Department of Computer Sciences, University of Wisconsin-Madison. <p> An optimization of this technique is discussed in Subsection 4.2. 2.4 Hybrid Hash The hybrid hash join algorithm has been described in detail elsewhere in the literature <ref> [DKO + 84, DG85, SD89] </ref>; here we give a top-level overview of the important aspects of parallel hash join algorithms. In the following we will denote hybrid hash by HH. <p> The CPU cost for the replicate nested loops is just that of the replicate nested loops, plus additional CPU for the sort. In our analytical model, we used the formula from in <ref> [DKO + 84] </ref>: kRk fl (log kRk fl keyswap + move) 3.4 Subsetting Nested Loops (SNL) Recall that in SNL, instead of broadcasting R to all processors, each R tuple r is first sent to an intermediate processor for a lookup on the MapS relation, then forwarded to all processors that
Reference: [DNSS92] <author> D. J. DeWitt, J. F. Naughton, D. A. Schnei-der, and S. Seshadri. </author> <title> Practical skew handling in parallel joins. </title> <booktitle> In Proc. of the Nineteenth VLDB, </booktitle> <address> Vancouver, British Columbia, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: Item one was straightforward. Item two was necessary because the tuple redistribution mechanisms used by the parallel hybrid-hash algorithm assume that each tuple is sent to exactly one destination. Fortunately, we had already added code to do the broadcast and subset redistributions in our work on skew-handling join algorithms <ref> [DNSS92] </ref>, so no new code needed to be written. Items one and two were all that was necessary to implement the RNL and RNL-S algorithms. Item three was less straightforward.
Reference: [EGKS90] <author> S. Englert, J. Gray, T. Kocher, and P. Shah. </author> <title> A benchmark of NonStop SQL Release 2 demonstrating near-linear speedup and scaleup on large database. </title> <booktitle> In Proc. of the SIGMETRICS Conference, </booktitle> <pages> pages 245-247, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: None of these papers compared the hashing join algorithms with nested loop with index algorithms. Valduriez and Gardarin [VG84] compared parallel join and semijoin algorithms based on hashing, sort-merge, and nested loops, but did not consider nested-loops with index. The Tandem group <ref> [EGKS90] </ref> state that their NonStop SQL system uses a variant of parallel nested loops with index algorithm if the appropriate indices exist and one of the relations is small, and hashing followed by sort-merge otherwise, but did not compare the two algorithms. <p> RNL is also the algorithm used by Tandem if the appropriate index exists, one of the relations is small, and the join is on a key for the small relation <ref> [EGKS90] </ref>. Recently Stamos and Young have proposed an improvement on the full fragment-replicate algorithm [SY89] that partially replicates and redistributes both input relations.
Reference: [ESW78] <author> R. Epstein, M. Stonebraker, and E. Wong. </author> <title> Distributed query processing in a relational database system. </title> <booktitle> In Proc. of the ACM-SIGMOD Conference, </booktitle> <year> 1978. </year>
Reference-contexts: RNL is similar to the distributed database join algorithm known as fragment-replicate, originally proposed in <ref> [ESW78] </ref>, with the "fragment" phase a no-op since this algorithm begins with S fragmented about the sites of the system.
Reference: [KTMo83] <author> M. Kitsuregawa, H. Tanaka, and T. Moto-oka. </author> <title> Application of hash to data base machine and its architecture. </title> <journal> New Generation Computing, </journal> <volume> 1(1), </volume> <year> 1983. </year>
Reference-contexts: In related work, DeWitt and Gerber investigated the performance of four parallel hashing join algorithms [DG85], while Schneider and DeWitt compared several parallel hashing algorithms with parallel sort-merge [SD89]. Kitsuregawa <ref> [KTMo83] </ref> proposed an algorithm based on hashing for redistribution followed by sort-merge, intended to take advantage of special-purpose sorting hardware. None of these papers compared the hashing join algorithms with nested loop with index algorithms.
Reference: [OL89] <author> R. Omiecinski and E. T. Lin. </author> <title> Hash-based and index-based join algorithms for cube and ring connected multicomputers. </title> <journal> IEEE TKDE, </journal> <volume> 1(3) </volume> <pages> 329-343, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: A join index essentially precomputes the join by storing pairs of tuple id's, one pair of tuple id's for each tuple that would appear in the join result. The indices used in nested-loop with index are just the usual B-tree indexes built by current relational DBMS. Omiecinski and Lin <ref> [OL89] </ref> consider the use of join indices in a parallel environment. The remainder of the paper is organized as follows. We describe the hybrid-hash algorithm and four variants of nested loops with index in Section 2.
Reference: [SC90] <author> E. J. Shekita and M. J. Carey. </author> <title> A performance evaluation of pointer-based joins. </title> <booktitle> In Proc. of the ACM-SIGMOD Conference, </booktitle> <pages> pages 300-312, </pages> <address> Atlantic City, New Jersey, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: In early work, Blasgen and Eswaran [BE77] compared sort-merge with nested-loops with index on uniproces-sors, and concluded that sort merge almost always wins unless there is an appropriate clustered index on one of the join operands. Recently, Shekita and Carey <ref> [SC90] </ref> compared a number of uniprocessor join algorithms, including pointer based join algorithms, nested-loops with index, sort-merge, and hybrid hash. In the portion of that work relevant to this paper, they found that index nested loops works well if one of the relations is small.
Reference: [SD89] <author> D. A. Schneider and D. J. DeWitt. </author> <title> A performance evaluation of four parallel join algorithms in a shared-nothing multiprocessor environment. </title> <booktitle> In Proc. of the ACM-SIGMOD Conference, </booktitle> <pages> pages 110-121, </pages> <address> Port-land, Oregon, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: One line of reasoning behind this goes roughly like this: it has long been known that in uniprocessor systems, sort-merge beats nested loops with index almost always [BE77]; also, hybrid-hash beats sort-merge just about everywhere in both the uniprocessor and multiprocessor case <ref> [DKO + 84, SD89] </ref>; so, transitively, one can expect fl Department of Computer Sciences, University of Wisconsin-Madison. <p> In related work, DeWitt and Gerber investigated the performance of four parallel hashing join algorithms [DG85], while Schneider and DeWitt compared several parallel hashing algorithms with parallel sort-merge <ref> [SD89] </ref>. Kitsuregawa [KTMo83] proposed an algorithm based on hashing for redistribution followed by sort-merge, intended to take advantage of special-purpose sorting hardware. None of these papers compared the hashing join algorithms with nested loop with index algorithms. <p> An optimization of this technique is discussed in Subsection 4.2. 2.4 Hybrid Hash The hybrid hash join algorithm has been described in detail elsewhere in the literature <ref> [DKO + 84, DG85, SD89] </ref>; here we give a top-level overview of the important aspects of parallel hash join algorithms. In the following we will denote hybrid hash by HH.
Reference: [STG + 90] <author> B. Salzberg, A. Tsukerman, J. Gray, S. Uern, and B. Vaughan. FastSort: </author> <title> A distributed single-input single-output external sort. </title> <booktitle> In Proc. of the ACM-SIGMOD Conference, </booktitle> <pages> pages 94-101, </pages> <address> Atlantic City, New Jersey, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: In our analytic model we assume that R can be sorted in two passes (given modern memory sizes, even a 100GByte relation can be sorted in two passes <ref> [STG + 90] </ref>).
Reference: [Sto86] <author> M. Stonebraker. </author> <title> The case for shared nothing. </title> <journal> Database Engineering, </journal> <volume> 9(1), </volume> <year> 1986. </year>
Reference-contexts: following [BE77]: for each tuple r in R do lookup the value r.A in the index on s.B; for each S tuple s returned by the lookup output answer tuple (r,s); endFor; endFor; We now consider how to implement a version of this algorithm in a shared-nothing parallel database system <ref> [Sto86] </ref>. When mapping this algorithm to a shared-nothing parallel system, one must first specify how the relations R and S are stored in the system. We will assume that both R and S are declustered throughout the processors of the system on attributes other than R:A and S:B. <p> Our goal was to explore the performance of the algorithms in an implementation, and also to investigate how naturally the nested loops algorithms could be implemented in a system that already has indices and the hash-join algorithms available. 5.1 Implementation Gamma falls into the class of shared-nothing <ref> [Sto86] </ref> architectures. The hardware consists of a 32 processor Intel iPSC/2 hypercube. Each processor is configured with a 80386 CPU, 8 megabytes of memory, and a 330 megabyte MAXTOR 4380 (5 1/4 in.) disk drive.
Reference: [SY89] <author> J. W. Stamos and H. C. Young. </author> <title> A symmetric fragment and replicate algorithm for distributed joins. </title> <type> TR RJ 7118 (67667), </type> <institution> IBM Research Division, Almaden Research Center, </institution> <address> San Jose, California, </address> <month> December </month> <year> 1989. </year>
Reference-contexts: RNL is also the algorithm used by Tandem if the appropriate index exists, one of the relations is small, and the join is on a key for the small relation [EGKS90]. Recently Stamos and Young have proposed an improvement on the full fragment-replicate algorithm <ref> [SY89] </ref> that partially replicates and redistributes both input relations.
Reference: [Val87] <author> P. Valduriez. </author> <title> Join indices. </title> <journal> ACM TODS, </journal> <volume> 12(2):218 - 246, </volume> <month> June </month> <year> 1987. </year>
Reference-contexts: In the portion of that work relevant to this paper, they found that index nested loops works well if one of the relations is small. Valduriez <ref> [Val87] </ref> proposed the use of an auxiliary data structure called a join index in join processing, and showed that in many cases a join algorithm using a join index can out-perform hybrid hashing. <p> In the remainder of this paper, for simplicity we will assume that P 0 is itself a key for S, so we can use P = P 0 . Note that MapS is not a join index <ref> [Val87] </ref>. A join index contains an entry for every pair of tuples (r; s) such that r is from R and s is from S and r and s join. MapS merely pairs S join attribute values with S partitioning attribute values, independent of R.
Reference: [VG84] <author> P. Valduriez and G. Gardarin. </author> <title> Join and semijoin algorithms for a multiprocessor database machine. </title> <journal> ACM TODS, </journal> <volume> 9(1) </volume> <pages> 133-161, </pages> <month> March </month> <year> 1984. </year>
Reference-contexts: Kitsuregawa [KTMo83] proposed an algorithm based on hashing for redistribution followed by sort-merge, intended to take advantage of special-purpose sorting hardware. None of these papers compared the hashing join algorithms with nested loop with index algorithms. Valduriez and Gardarin <ref> [VG84] </ref> compared parallel join and semijoin algorithms based on hashing, sort-merge, and nested loops, but did not consider nested-loops with index.
Reference: [WDY90] <author> J. L. Wolf, D. M. Dias, and P. S. Yu. </author> <title> An effective algorithm for parallelizing sort merge joins in the presence of data skew. </title> <booktitle> In Proc. of the Second ISDPDS, </booktitle> <pages> pages 103-115, </pages> <address> Dublin, Ireland, </address> <month> July </month> <year> 1990. </year>
Reference-contexts: The Tandem group [EGKS90] state that their NonStop SQL system uses a variant of parallel nested loops with index algorithm if the appropriate indices exist and one of the relations is small, and hashing followed by sort-merge otherwise, but did not compare the two algorithms. Wolf et al. <ref> [WDYT90, WDY90] </ref> consider the performance of parallel hashing and sort-merge algorithms in the presence of skew, but do not consider parallel nested loop algorithms.
Reference: [WDYT90] <author> J. L. Wolf, D. M. Dias, P. S. Yu, and J. J. Turek. </author> <title> An effective algorithm for paralleliz-ing hash joins in the presence of data skew. </title> <institution> IBM T. J. Watson Research Center Tech Report RC 15510, </institution> <year> 1990. </year>
Reference-contexts: The Tandem group [EGKS90] state that their NonStop SQL system uses a variant of parallel nested loops with index algorithm if the appropriate indices exist and one of the relations is small, and hashing followed by sort-merge otherwise, but did not compare the two algorithms. Wolf et al. <ref> [WDYT90, WDY90] </ref> consider the performance of parallel hashing and sort-merge algorithms in the presence of skew, but do not consider parallel nested loop algorithms.
References-found: 21

