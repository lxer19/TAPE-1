URL: http://www.math.jyu.fi/~orponen/papers/noisyac.ps
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00214.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: On the Effect of Analog Noise in Discrete-Time Analog Computations  
Author: Wolfgang Maass Pekka Orponen 
Keyword: VC-dimension of computational models with analog noise.  
Date: May 28, 1997  
Affiliation: Institute for Theoretical Computer Science Technische Universitat Graz  Department of Mathematics University of Jyvaskyla  
Abstract: We introduce a model for analog computation with discrete time in the presence of analog noise that is flexible enough to cover the most important concrete cases, such as noisy analog neural nets and networks of spiking neurons. This model subsumes the classical model for digital computation in the presence of noise. We show that the presence of arbitrarily small amounts of analog noise reduces the power of analog computational models to that of finite automata, and we also prove a new type of upper bound for the 
Abstract-found: 1
Intro-found: 1
Reference: [Alon et al., 1993] <author> N. Alon, N. Cesa-Bianchi, S. Ben-David, D. Haussler, </author> <title> Scale-sensitive dimensions, uniform convergence, and learnability. </title> <booktitle> Proceedings of the 34th Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> 292-301. </pages> <publisher> IEEE Computer Science Press, </publisher> <address> New York, NY, </address> <year> 1993. </year>
Reference-contexts: G and any x; ~x 2 S the condition 8q 2 G (j fl (x; q) ~ fl (~x; q)j 1) implies that R One can get an upper bound for the "complexity" of G fl S by applying to G fl S a generalization of "Sauer's Lemma" due to <ref> [Alon et al., 1993] </ref>. Given integers m, b, and , define fi (m; b; ) := log 2 i=1 i b i . Lemma 15 of [Alon et al., 1993] states that if A fl is any class of functions obtained as the discretizations of the functions in a class A <p> get an upper bound for the "complexity" of G fl S by applying to G fl S a generalization of "Sauer's Lemma" due to <ref> [Alon et al., 1993] </ref>. Given integers m, b, and , define fi (m; b; ) := log 2 i=1 i b i . Lemma 15 of [Alon et al., 1993] states that if A fl is any class of functions obtained as the discretizations of the functions in a class A of pseudo-dimension , such that the functions in A fl have a domain D of size m and range f0; : : : ; b 1g, <p> Thus we have verified the preceding claim, and the proof of Theorem 5.1 is now complete. Remark 5.2 It follows from <ref> [Alon et al., 1993] </ref> that instead of a finite upper bound for the pseudo-dimension of G it suffices for Theorem 5.1 to assume a finite upper bound for the fl-dimension P fl -dim (G) of G for fl = =20 ().
Reference: [Alon, Dewdney, Ott, 1991] <author> N. Alon, A. K. Dewdney, T. J. Ott, </author> <title> Efficient simulation of finite automata by neural nets. </title> <journal> J. Assoc. Comput. Mach. </journal> <volume> 38 (1991), </volume> <pages> 495-514. </pages>
Reference: [Anderson et al., 1977] <author> J. A. Anderson, J. W. Silverstein, S. A. Ritz, R. S. Jones, </author> <title> Distinctive features, categorical perception, and probability learning: some applications of a neural model. </title> <note> Psychological Review 84 (1977), 413-451. Reprinted in [Anderson, </note> <editor> Rosenfeld, </editor> <year> 1988], </year> <pages> pp. 287-325. </pages>
Reference-contexts: considers bounded noise processes (where the analog noise can move an internal state at most over a distance , for a sufficiently small value of ), then any finite automaton can be simulated with perfect ( = 1=2) reliability by a recurrent analog neural net of the type discussed in <ref> [Anderson et al., 1977] </ref>, [Siegelmann, Sontag, 1991]. Other embeddings of finite automata in recurrent sigmoidal networks include [Frasconi et al., 1996] and [Omlin, Giles, 1996] which discuss, respectively, implementations of automata in noise-free radial basis function networks, and in second-order networks with synaptic noise. <p> the class F at some point after the input has finished. (We shall give a more precise definition of the dynamics, including the effects of noise, later.) For instance, the recurrent analog neural net model of [Siegelmann, Sontag, 1991] (also known as the "Brain State in a Box" model of <ref> [Anderson et al., 1977] </ref>) is obtained from this general framework as follows. For a network N with d neurons and activation values between 1 and 1, the state space is = [1; 1] d . <p> For any p 2 and a 2 , we define s (p; a) = p + , where for each i = 1; : : : ; d, p + X d w ij p j + h i + c i a) Both <ref> [Anderson et al., 1977] </ref> and [Siegelmann, Sontag, 1991] use the saturated-linear sigmoid activation function (u) = &gt; &lt; 1; if u &lt; 1; 1; if u &gt; 1; but one may obviously also define the model with respect to other activation functions, notably the "standard sigmoid" (u) = tanh u, or
Reference: [Anderson, Rosenfeld, 1988] <editor> J. A. Anderson and E. Rosenfeld (eds.), Neurocomputing: </editor> <booktitle> Foundations of Research. </booktitle> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1988. </year>
Reference: [Asarin, Maler, 1994] <author> E. Asarin, O. Maler, </author> <title> On some relations between dynamical systems and transition systems. </title> <booktitle> Proceedings of the 21st International Colloquium on Automata, Languages, and Programming, </booktitle> <pages> 59-72. </pages> <booktitle> Lecture Notes in Computer Science 820, </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1994. </year>
Reference-contexts: applied to completely different types of models for discrete time analog computation than neural nets, such as arithmetical circuits [Turan, Vatan, 1994], the random access machine (RAM) with analog inputs, the parallel random access machine (PRAM) with analog inputs, various computational discrete-time dynamical systems ([Moore, 1990], [Koiran et al., 1994], <ref> [Asarin, Maler, 1994] </ref>, [Orponen, Matamala, 1996]) and (with some minor adjustments) also the BSS model [Blum, Shub, Smale, 1989], [Koiran, 1993].
Reference: [Blum, Shub, Smale, 1989] <author> L. Blum, M. Shub, S. Smale, </author> <title> On a theory of computation over the real numbers: NP-completeness, recursive functions and universal machines. </title> <journal> Bulletin of the Amer. Math. Soc. </journal> <volume> 21 (1989), </volume> <pages> 1-46. </pages>
Reference-contexts: such as arithmetical circuits [Turan, Vatan, 1994], the random access machine (RAM) with analog inputs, the parallel random access machine (PRAM) with analog inputs, various computational discrete-time dynamical systems ([Moore, 1990], [Koiran et al., 1994], [Asarin, Maler, 1994], [Orponen, Matamala, 1996]) and (with some minor adjustments) also the BSS model <ref> [Blum, Shub, Smale, 1989] </ref>, [Koiran, 1993]. Our framework provides for each of these models an adequate definition of noise-robust computation in the presence of analog noise, and our results provide upper bounds for their computational power and VC-dimension in terms of characteristics of their analog noise.
Reference: [Blumer et al., 1989] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, M. K. Warmuth, </author> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> J. Assoc. Comput. Mach. </journal> <volume> 36 (1989), </volume> <pages> 929-965. </pages>
Reference-contexts: parameter for estimating the number of randomly chosen examples that are needed to "learn" arbitrary target functions g : R n ! f0; 1g from randomly chosen examples hx; g (x)i for g by a learning algorithm that uses functions from F as hypotheses (see [Haussler, 1992], [Vapnik, Chervonenkis, 1971], <ref> [Blumer et al., 1989] </ref>, [Maass, 1995]).
Reference: [Casey, 1996] <author> M. Casey, </author> <title> The dynamics of discrete-time computation, with application to recurrent neural networks and finite state machine extraction. </title> <booktitle> Neural Computation 8 (1996), </booktitle> <pages> 1135-1178. </pages>
Reference-contexts: More specific hardware-oriented models for analog noise in analog neural nets have been discussed in [Phatak, Koren 1995]. Another related work is <ref> [Casey, 1996] </ref>, which addresses the special case of analog computations on recurrent neural nets, where the analog noise can move an internal state at most over some bounded distance , and the digital output is required to be perfectly reliable (i.e. = 1=2 in the present notation). <p> <ref> [Casey, 1996] </ref>, which addresses the special case of analog computations on recurrent neural nets, where the analog noise can move an internal state at most over some bounded distance , and the digital output is required to be perfectly reliable (i.e. = 1=2 in the present notation). Corollary 3.1 of [Casey, 1996] states a special case of our Theorem 3.1 for the model considered in that article. The proof of Corollary 3.1 of [Casey, 1996] is incorrect. 1 A correct proof is contained as a special case in the proof of Theorem 3.1 in Section 3 of this article. 2 Apart <p> Corollary 3.1 of <ref> [Casey, 1996] </ref> states a special case of our Theorem 3.1 for the model considered in that article. The proof of Corollary 3.1 of [Casey, 1996] is incorrect. 1 A correct proof is contained as a special case in the proof of Theorem 3.1 in Section 3 of this article. 2 Apart from Corollary 3.1 there is no further overlap between [Casey, 1996] and this article. <p> The proof of Corollary 3.1 of <ref> [Casey, 1996] </ref> is incorrect. 1 A correct proof is contained as a special case in the proof of Theorem 3.1 in Section 3 of this article. 2 Apart from Corollary 3.1 there is no further overlap between [Casey, 1996] and this article. There are relatively few examples of nontrivial computations on common digital or analog computational models that can achieve perfect reliability of the output in spite of noisy internal components. <p> In addition, if one wants to investigate computations with common noise distributions such as Gaussian noise which may in principle move a state to any other state it is 1 Corollary 3.1 is derived as a corollary of Theorem 3.1 in <ref> [Casey, 1996] </ref>, whose proof relies on the assumption that the recognized language is regular. <p> intervals [1=(2i + 1); 1=2i] for i = 1; 2; ::: are infinitely many disjoint sets with nonempty interior, which are all contained in the compact set [0; 1]. 2 Actually, since there is no need to analyze probability distributions for this special case, one can prove Corollary 3.1. of <ref> [Casey, 1996] </ref> more directly by considering the equivalence relation defined at the beginning of Section 3, and by deriving a lower bound for the volume of the set of states that correspond to an equivalence class. 2 necessary to move to a computational model with less than perfect reliability of the
Reference: [Cowan, 1966] <author> J. D. Cowan, </author> <title> Synthesis of reliable automata from unreliable components. Automata Theory (E. </title> <editor> R. Caianiello, ed.), </editor> <address> 131-145. </address> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1966. </year>
Reference-contexts: We propose and investigate such model in this article. The investigation of noise-tolerant digital computations in the presence of stochastic failures of gates or wires was initiated by [von Neumann, 1956]. We refer to <ref> [Cowan, 1966] </ref>, [Pippenger, 1989] and [Gal, 1991] for a small sample of the numerous results that have been achieved in this direction. <p> There are relatively few examples of nontrivial computations on common digital or analog computational models that can achieve perfect reliability of the output in spite of noisy internal components. Most constructions of noise-robust computational models rely on the replication of noisy computational units (see [von Neumann, 1956], <ref> [Cowan, 1966] </ref>). The idea of this method is that the average of the outputs of k identical noisy networks (with stochastically independent noise processes) is more reliable than the output of a single network.
Reference: [Feller, 1971] <author> W. Feller, </author> <title> An Introduction to Probability Theory and Its Applications, </title> <note> Vol. 2, Second Edition. </note> <editor> J. </editor> <publisher> Wiley & Sons, </publisher> <address> New York, NY, </address> <year> 1971. </year>
Reference-contexts: The function Z is called the noise process affecting M , and it should satisfy the mild conditions of being a stochastic kernel <ref> [Feller, 1971, p. 205] </ref>, i.e., for each p 2 , Z (p; ) should be a probability distribution, and for each Borel set B, Z (; B) should be a measurable function. <p> We assume that there is some measure over so that Z (p; ) is absolutely continuous with respect to for each p 2 , i.e. (B) = 0 implies Z (p; B) = 0 for every measurable B . By the Radon-Nikodym theorem <ref> [Feller, 1971, p. 140] </ref>, Z then possesses a density kernel with respect to , i.e. there exists a function z (; ) such that for any state p 2 and Borel set B , Z (p; B) = q2B We assume that this function z (; ) has values in [0;
Reference: [Frasconi et al., 1996] <author> P. Frasconi, M. Gori, M. Maggini, G. </author> <title> Soda, Representation of finite state automata in recurrent radial basis function networks. </title> <booktitle> Machine Learning 23 (1996), </booktitle> <pages> 5-32. </pages>
Reference-contexts: Other embeddings of finite automata in recurrent sigmoidal networks include <ref> [Frasconi et al., 1996] </ref> and [Omlin, Giles, 1996] which discuss, respectively, implementations of automata in noise-free radial basis function networks, and in second-order networks with synaptic noise. In Section 5 we establish a new type of upper bound for the VC-dimension of computational models with analog noise.
Reference: [Gal, 1991] <author> A. Gal, </author> <title> Lower bounds for the complexity of reliable Boolean circuits with noisy gates. </title> <booktitle> Proceedings of the 32th Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> 594-601. </pages> <publisher> IEEE Computer Science Press, </publisher> <address> New York, NY, </address> <year> 1991. </year>
Reference-contexts: We propose and investigate such model in this article. The investigation of noise-tolerant digital computations in the presence of stochastic failures of gates or wires was initiated by [von Neumann, 1956]. We refer to [Cowan, 1966], [Pippenger, 1989] and <ref> [Gal, 1991] </ref> for a small sample of the numerous results that have been achieved in this direction. In all these articles one considers computations which produce a correct output not with perfect reliability, but with probability 1 2 + (for some parameter 2 (0; 1 2 ]).
Reference: [Gerstner, van Hemmen, 1994] <author> W. Gerstner, J. L. van Hemmen, </author> <title> How to describe neuronal activity: </title> <booktitle> spikes, rates or assemblies? Advances in Neural Information Processing Systems 6, </booktitle> <pages> 463-470. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1994. </year>
Reference-contexts: The completely different model of a network of m stochastic spiking neurons (see e.g. <ref> [Gerstner, van Hemmen, 1994] </ref> or [Maass, 1997]) is also a special case of our general framework.
Reference: [Goldberg, Jerrum, 1995] <author> P. W. Goldberg, M. R. Jerrum, </author> <title> Bounding the Vapnik-Chervonenkis dimension of concept classes parameterized by real numbers. </title> <booktitle> Machine Learning 18 (1995), </booktitle> <pages> 131-148. </pages>
Reference-contexts: In the case where the activation functions and density kernels are piecewise polynomial one can apply the results of <ref> [Goldberg, Jerrum, 1995] </ref> to get a slightly better finite upper bound for the pseudo-dimension of G .
Reference: [Haussler, 1992] <author> D. Haussler, </author> <title> Decision theoretic generalizations of the PAC-model for neural nets and other learning applications. </title> <booktitle> Information and Computation 100 (1992), </booktitle> <pages> 78-150. </pages>
Reference-contexts: The resulting upper bounds for the required sample size of a noisy multi-layer sigmoidal neural net extend a preceding result by Haussler. He had shown in Corollary 3 in section 7 of <ref> [Haussler, 1992] </ref> that even in the noise-free case an upper bound for the VC-dimension can be given that only depends on the maximal absolute value of weights for gates on layers 2 and on their maximal fan-in. <p> of F is the key parameter for estimating the number of randomly chosen examples that are needed to "learn" arbitrary target functions g : R n ! f0; 1g from randomly chosen examples hx; g (x)i for g by a learning algorithm that uses functions from F as hypotheses (see <ref> [Haussler, 1992] </ref>, [Vapnik, Chervonenkis, 1971], [Blumer et al., 1989], [Maass, 1995]). It should be noted that this does not only hold for the "classical" PAC-learning model where the target function g is required to belong to the class F , but according to [Haussler, 1992] also in the general case of <p> uses functions from F as hypotheses (see <ref> [Haussler, 1992] </ref>, [Vapnik, Chervonenkis, 1971], [Blumer et al., 1989], [Maass, 1995]). It should be noted that this does not only hold for the "classical" PAC-learning model where the target function g is required to belong to the class F , but according to [Haussler, 1992] also in the general case of agnostic PAC-learning where g : R n ! f0; 1g can be any function.
Reference: [Hopcroft, Ullman, 1979] <author> J. E. Hopcroft, J. D. Ullman, </author> <title> Introduction to Automata Theory, Languages, and Computation. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1979. </year>
Reference-contexts: Sequences x; y 2 fl 0 are equivalent with respect to L if one has xw 2 L , yw 2 L for all w 2 fl 0 . The set L is regular if this equivalence relation has only finitely many equivalence classes. By the Myhill-Nerode theorem <ref> [Hopcroft, Ullman, 1979, pp. 65-67] </ref>, for finite alphabets 0 this definition coincides with the usual definition of regular sets via finite automata. From the point of view of computational complexity theory, machine models that only accept regular sets belong to the most "primitive" class of models.
Reference: [Horne, Hush, 1996] <author> B. G. Horne, D. R. Hush, </author> <title> Bounds on the complexity of recurrent neural network implementations of finite state machines. </title> <booktitle> Neural Networks 9 (1996), </booktitle> <pages> 243-252. </pages>
Reference-contexts: The resulting network then has integer weights, with w max = 2, and recognizes the language L with delay t: Remark 4.4 One can obtain different noise-tolerance vs. delay tradeoffs using the recent more advanced simulations of finite automata by threshold logic networks ([Alon, Dewdney, Ott, 1991], <ref> [Horne, Hush, 1996] </ref>, [Indyk, 1995]). For instance, [Horne, Hush, 1996] presents a simulation of m-state finite automata by threshold logic networks with O ( p m log m) units, connection weights 1, and delay 4. <p> has integer weights, with w max = 2, and recognizes the language L with delay t: Remark 4.4 One can obtain different noise-tolerance vs. delay tradeoffs using the recent more advanced simulations of finite automata by threshold logic networks ([Alon, Dewdney, Ott, 1991], <ref> [Horne, Hush, 1996] </ref>, [Indyk, 1995]). For instance, [Horne, Hush, 1996] presents a simulation of m-state finite automata by threshold logic networks with O ( p m log m) units, connection weights 1, and delay 4.
Reference: [Indyk, 1995] <author> P. Indyk, </author> <title> Optimal simulation of automata by neural nets. </title> <booktitle> Proceedings of the 12th Annual Symposium on Theoretical Aspects of Computer Science, </booktitle> <pages> 337-347. </pages> <booktitle> Lecture Notes in Computer Science 900, </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1995. </year>
Reference-contexts: The resulting network then has integer weights, with w max = 2, and recognizes the language L with delay t: Remark 4.4 One can obtain different noise-tolerance vs. delay tradeoffs using the recent more advanced simulations of finite automata by threshold logic networks ([Alon, Dewdney, Ott, 1991], [Horne, Hush, 1996], <ref> [Indyk, 1995] </ref>). For instance, [Horne, Hush, 1996] presents a simulation of m-state finite automata by threshold logic networks with O ( p m log m) units, connection weights 1, and delay 4.
Reference: [Karpinski, Macintyre, 1995] <author> M. Karpinski, A. Macintyre, </author> <title> Polynomial bounds for VC-dimension of sigmoidal and general Pfaffian neural networks. </title> <journal> J. of Computer and System Sciences, </journal> <note> to appear. </note>
Reference-contexts: We take as the class G of functions considered in the proof of Theorem 5.1 all functions of the form (x; q) = z (F (x; w); q) for arbitrary parameters w 2 R m . The results presented in <ref> [Karpinski, Macintyre, 1995] </ref> imply that the pseudo-dimension of this class G of functions is 17 bounded by a polynomial in m, for all common choices of activation functions of the sigmoidal units and all practically relevant density kernels z for the noise process (even involving the exponential function).
Reference: [Kifer, 1988] <author> Y. Kifer, </author> <title> Random Perturbations of Dynamical Systems. </title> <publisher> Birkhauser, </publisher> <address> Boston, MA, </address> <year> 1988. </year>
Reference-contexts: Continuous-space noise models similar to ours have been used in general studies of the stability of dynamical systems affected by random perturbations (e.g., <ref> [Kifer, 1988] </ref>), but our work is to our knowledge the first to consider the computational aspects of systems of this type. More specific hardware-oriented models for analog noise in analog neural nets have been discussed in [Phatak, Koren 1995].
Reference: [Koiran, 1993] <author> P. Koiran, </author> <title> A weak version of the Blum, Shub and Smale model. </title> <booktitle> Proceedings of the 34th Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> 486-495. </pages> <publisher> IEEE Computer Science Press, </publisher> <address> New York, NY, </address> <year> 1993. </year>
Reference-contexts: [Turan, Vatan, 1994], the random access machine (RAM) with analog inputs, the parallel random access machine (PRAM) with analog inputs, various computational discrete-time dynamical systems ([Moore, 1990], [Koiran et al., 1994], [Asarin, Maler, 1994], [Orponen, Matamala, 1996]) and (with some minor adjustments) also the BSS model [Blum, Shub, Smale, 1989], <ref> [Koiran, 1993] </ref>. Our framework provides for each of these models an adequate definition of noise-robust computation in the presence of analog noise, and our results provide upper bounds for their computational power and VC-dimension in terms of characteristics of their analog noise.
Reference: [Koiran et al., 1994] <author> P. Koiran, M. Cosnard, M. Garzon, </author> <title> Computability with low-dimensional dynamical systems. </title> <type> Theoret. </type> <institution> Comput. Sci. </institution> <month> 132 </month> <year> (1994), </year> <pages> 113-128. </pages>
Reference-contexts: computations can also be applied to completely different types of models for discrete time analog computation than neural nets, such as arithmetical circuits [Turan, Vatan, 1994], the random access machine (RAM) with analog inputs, the parallel random access machine (PRAM) with analog inputs, various computational discrete-time dynamical systems ([Moore, 1990], <ref> [Koiran et al., 1994] </ref>, [Asarin, Maler, 1994], [Orponen, Matamala, 1996]) and (with some minor adjustments) also the BSS model [Blum, Shub, Smale, 1989], [Koiran, 1993].
Reference: [Maass, 1995] <author> W. Maass, </author> <title> Vapnik-Chervonenkis dimension of neural nets. The Handbook of Brain Theory and Neural Networks (M. </title> <editor> A. Arbib, ed.), </editor> <address> 1000-1003. </address> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1995. </year> <month> 20 </month>
Reference-contexts: number of randomly chosen examples that are needed to "learn" arbitrary target functions g : R n ! f0; 1g from randomly chosen examples hx; g (x)i for g by a learning algorithm that uses functions from F as hypotheses (see [Haussler, 1992], [Vapnik, Chervonenkis, 1971], [Blumer et al., 1989], <ref> [Maass, 1995] </ref>).
Reference: [Maass, 1996] <author> W. Maass, </author> <title> Lower bounds for the computational power of networks of spiking neurons. </title> <booktitle> Neural Computation 8 (1996), </booktitle> <pages> 1-40. </pages>
Reference-contexts: This contrasts with the anomaly that in the noise-free setting the classes of, e.g. finite recurrent analog neural nets [Siegelmann, Sontag, 1991] and finite recurrent networks of spiking neurons <ref> [Maass, 1996] </ref> have infinite VC-dimension, and are thus strongly "unlearnable" from the point of view of learning theory. <p> case of analog neural nets. 7 3 An Upper Bound for the Computational Power of Systems with Analog Noise It has been shown for various concrete models of analog computation without noise (such as generalized shift maps [Moore, 1990], recurrent neural nets [Siegelmann, Sontag, 1991] and networks of spiking neurons <ref> [Maass, 1996] </ref>) that they can simulate a universal Turing machine, and hence have immense computational power. It has long been conjectured that their computational power collapses to that of a finite automaton as soon as one assumes that they are subject to even small amounts of analog noise. <p> Then for any two functions x (), y () in the same class C it is the case that for any r 2 j , j = 1; : : :; k, and thus R R 9 Remark 3.2 In stark contrast to the results of [Siegelmann, Sontag, 1991] and <ref> [Maass, 1996] </ref> for the noise-free case, the preceding Theorem implies that both recurrent analog neural nets and recurrent networks of spiking neurons with online input from fl 0 can only recognize regular languages in the presence of any reasonable type of analog noise, even if their computation time is unlimited and <p> It is obvious from the results of [Siegelmann, Sontag, 1991] and <ref> [Maass, 1996] </ref> that there exist finite recurrent analog neural nets and finite recurrent networks of spiking neurons with batch-input and parameters from Q that have infinite VC-dimension (consider networks that can simulate a universal Turing machine, with each input bit-string encoded into a rational number).
Reference: [Maass, 1997] <author> W. Maass, </author> <title> Fast sigmoidal networks via spiking neurons, </title> <booktitle> Neural Computation 9, </booktitle> <year> (1997), </year> <pages> 279-304. </pages>
Reference-contexts: The completely different model of a network of m stochastic spiking neurons (see e.g. [Gerstner, van Hemmen, 1994] or <ref> [Maass, 1997] </ref>) is also a special case of our general framework.
Reference: [Minsky, 1972] <author> M. L. Minsky, </author> <title> Computation: Finite and Infinite Machines. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1972. </year>
Reference-contexts: Proof: Let L be recognized by some finite automaton with m states. As presented in e.g. <ref> [Minsky, 1972, pp. 55-57] </ref>, one can easily construct from this automaton a threshold logic network T with 2m + 1 units that recognizes L.
Reference: [Moore, 1990] <author> C. Moore, </author> <title> Unpredictability and undecidability in physical systems. </title> <journal> Phys. Review Letters 64 (1990), </journal> <pages> 2354-2357. </pages>
Reference-contexts: with a measure over defined via a decomposition of similarly as in the case of analog neural nets. 7 3 An Upper Bound for the Computational Power of Systems with Analog Noise It has been shown for various concrete models of analog computation without noise (such as generalized shift maps <ref> [Moore, 1990] </ref>, recurrent neural nets [Siegelmann, Sontag, 1991] and networks of spiking neurons [Maass, 1996]) that they can simulate a universal Turing machine, and hence have immense computational power.
Reference: [Omlin, Giles, 1996] <author> C. W. Omlin, C. L Giles, </author> <title> Constructing deterministic finite-state automata in recurrent neural networks. </title> <journal> J. Assoc. Comput. Mach. </journal> <volume> 43 (1996), </volume> <pages> 937-972. </pages>
Reference-contexts: Other embeddings of finite automata in recurrent sigmoidal networks include [Frasconi et al., 1996] and <ref> [Omlin, Giles, 1996] </ref> which discuss, respectively, implementations of automata in noise-free radial basis function networks, and in second-order networks with synaptic noise. In Section 5 we establish a new type of upper bound for the VC-dimension of computational models with analog noise.
Reference: [Orponen, Matamala, 1996] <author> P. Orponen, M. Matamala, </author> <title> Universal computation by finite two-dimensional coupled map lattices. </title> <booktitle> Proceedings of the Workshop on Physics and Computation, PhysComp'96, </booktitle> <pages> 243-247. </pages> <institution> New England Complex Systems Institute, </institution> <address> Boston, MA, </address> <year> 1996. </year>
Reference-contexts: different types of models for discrete time analog computation than neural nets, such as arithmetical circuits [Turan, Vatan, 1994], the random access machine (RAM) with analog inputs, the parallel random access machine (PRAM) with analog inputs, various computational discrete-time dynamical systems ([Moore, 1990], [Koiran et al., 1994], [Asarin, Maler, 1994], <ref> [Orponen, Matamala, 1996] </ref>) and (with some minor adjustments) also the BSS model [Blum, Shub, Smale, 1989], [Koiran, 1993].
Reference: [Paz, 1971] <author> A. Paz, </author> <title> Introduction to Probabilistic Automata. </title> <publisher> Academic Press, </publisher> <address> New York, NY, </address> <year> 1971. </year>
Reference: [Phatak, Koren 1995] <author> D. S. Phatak, I. Koren, </author> <title> Complete and partial fault tolerance of feed-forward neural nets. </title> <booktitle> IEEE Transactions on Neural Networks 6 (1995), </booktitle> <pages> 446-456. </pages>
Reference-contexts: More specific hardware-oriented models for analog noise in analog neural nets have been discussed in <ref> [Phatak, Koren 1995] </ref>.
Reference: [Pippenger, 1989] <author> N. Pippenger, </author> <title> Invariance of complexity measures for networks with unreliable gates. </title> <journal> J. Assoc. Comput. Mach. </journal> <volume> 36 (1989), </volume> <pages> 531-539. </pages>
Reference-contexts: We propose and investigate such model in this article. The investigation of noise-tolerant digital computations in the presence of stochastic failures of gates or wires was initiated by [von Neumann, 1956]. We refer to [Cowan, 1966], <ref> [Pippenger, 1989] </ref> and [Gal, 1991] for a small sample of the numerous results that have been achieved in this direction. In all these articles one considers computations which produce a correct output not with perfect reliability, but with probability 1 2 + (for some parameter 2 (0; 1 2 ]).
Reference: [Rabin, 1963] <author> M. Rabin, </author> <title> Probabilistic automata. </title> <booktitle> Information and Control 6 (1963), </booktitle> <pages> 230-245. </pages>
Reference-contexts: We show in Theorem 3.1 of this article that any language recognized by such noisy analog computational system is regular. Our model and Theorem 3.1 are somewhat related to the analysis of probabilistic finite automata in <ref> [Rabin, 1963] </ref>, although in Rabin's case the finiteness of the state space simplifies the setup considerably.
Reference: [Siegelmann, 1994] <author> H. T. Siegelmann, </author> <title> On the computational power of probabilistic and faulty networks. </title> <booktitle> Proceedings of the 21st International Colloquium on Automata, Languages, and Programming, </booktitle> <pages> 23-34. </pages> <booktitle> Lecture Notes in Computer Science 820, </booktitle> <publisher> Springer-Verlag, </publisher> <address> Ber-lin, </address> <year> 1994. </year>
Reference-contexts: In all these articles one considers computations which produce a correct output not with perfect reliability, but with probability 1 2 + (for some parameter 2 (0; 1 2 ]). The same framework (with stochastic failures of gates or wires) has been applied to analog neural nets in <ref> [Siegelmann, 1994] </ref>.
Reference: [Siegelmann, Sontag, 1991] <author> H. T. Siegelmann, E. D. Sontag, </author> <title> Turing computability with neural nets. </title> <journal> Appl. Math. </journal> <note> Letters 4(6) (1991), 77-80. Cf. also: </note> <author> H. T. Siegelmann, E. D. Sontag, </author> <title> On the computational power of neural nets. </title> <editor> J. </editor> <booktitle> of Computer and System Sciences 50 (1995), </booktitle> <pages> 132-150. </pages>
Reference-contexts: (where the analog noise can move an internal state at most over a distance , for a sufficiently small value of ), then any finite automaton can be simulated with perfect ( = 1=2) reliability by a recurrent analog neural net of the type discussed in [Anderson et al., 1977], <ref> [Siegelmann, Sontag, 1991] </ref>. Other embeddings of finite automata in recurrent sigmoidal networks include [Frasconi et al., 1996] and [Omlin, Giles, 1996] which discuss, respectively, implementations of automata in noise-free radial basis function networks, and in second-order networks with synaptic noise. <p> This contrasts with the anomaly that in the noise-free setting the classes of, e.g. finite recurrent analog neural nets <ref> [Siegelmann, Sontag, 1991] </ref> and finite recurrent networks of spiking neurons [Maass, 1996] have infinite VC-dimension, and are thus strongly "unlearnable" from the point of view of learning theory. <p> The system accepts its input if it enters a state in the class F at some point after the input has finished. (We shall give a more precise definition of the dynamics, including the effects of noise, later.) For instance, the recurrent analog neural net model of <ref> [Siegelmann, Sontag, 1991] </ref> (also known as the "Brain State in a Box" model of [Anderson et al., 1977]) is obtained from this general framework as follows. For a network N with d neurons and activation values between 1 and 1, the state space is = [1; 1] d . <p> For any p 2 and a 2 , we define s (p; a) = p + , where for each i = 1; : : : ; d, p + X d w ij p j + h i + c i a) Both [Anderson et al., 1977] and <ref> [Siegelmann, Sontag, 1991] </ref> use the saturated-linear sigmoid activation function (u) = &gt; &lt; 1; if u &lt; 1; 1; if u &gt; 1; but one may obviously also define the model with respect to other activation functions, notably the "standard sigmoid" (u) = tanh u, or the discontinuous signum function sgn <p> via a decomposition of similarly as in the case of analog neural nets. 7 3 An Upper Bound for the Computational Power of Systems with Analog Noise It has been shown for various concrete models of analog computation without noise (such as generalized shift maps [Moore, 1990], recurrent neural nets <ref> [Siegelmann, Sontag, 1991] </ref> and networks of spiking neurons [Maass, 1996]) that they can simulate a universal Turing machine, and hence have immense computational power. <p> Then for any two functions x (), y () in the same class C it is the case that for any r 2 j , j = 1; : : :; k, and thus R R 9 Remark 3.2 In stark contrast to the results of <ref> [Siegelmann, Sontag, 1991] </ref> and [Maass, 1996] for the noise-free case, the preceding Theorem implies that both recurrent analog neural nets and recurrent networks of spiking neurons with online input from fl 0 can only recognize regular languages in the presence of any reasonable type of analog noise, even if their computation <p> The results are here formulated using the interval [1; 1], and changes in this interval would have the pro portional effects on the values. For instance, if the interval [0; 1] were used (as in e.g. <ref> [Siegelmann, Sontag, 1991] </ref>), the bound in Corollary 4.3 would decrease from 1 2 to 1 5 A Novel Upper Bound for the VC-Dimension of Various Types of Neural Nets with Analog Noise In this section we provide an example for the effect of analog noise on discrete time analog computations with <p> It is obvious from the results of <ref> [Siegelmann, Sontag, 1991] </ref> and [Maass, 1996] that there exist finite recurrent analog neural nets and finite recurrent networks of spiking neurons with batch-input and parameters from Q that have infinite VC-dimension (consider networks that can simulate a universal Turing machine, with each input bit-string encoded into a rational number).
Reference: [Turan, Vatan, 1994] <author> G. Turan, F. Vatan, </author> <title> On the computation of Boolean functions by analog circuits of bounded fan-in. </title> <booktitle> Proceedings of the 35th Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> 553-564. </pages> <publisher> IEEE Computer Science Press, </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference-contexts: neural nets and networks of spiking neurons in the presence of analog noise. 18 Finally we would like to point out that our model for noisy analog computations can also be applied to completely different types of models for discrete time analog computation than neural nets, such as arithmetical circuits <ref> [Turan, Vatan, 1994] </ref>, the random access machine (RAM) with analog inputs, the parallel random access machine (PRAM) with analog inputs, various computational discrete-time dynamical systems ([Moore, 1990], [Koiran et al., 1994], [Asarin, Maler, 1994], [Orponen, Matamala, 1996]) and (with some minor adjustments) also the BSS model [Blum, Shub, Smale, 1989], [Koiran,
Reference: [Vapnik, Chervonenkis, 1971] <author> V. N. Vapnik, A. Y. Chervonenkis, </author> <title> On the uniform convergence of relative frequencies of events to their probabilities. </title> <booktitle> Theory of Probability and its Applications 16 (1971), </booktitle> <pages> 264-280. 21 </pages>
Reference-contexts: is the key parameter for estimating the number of randomly chosen examples that are needed to "learn" arbitrary target functions g : R n ! f0; 1g from randomly chosen examples hx; g (x)i for g by a learning algorithm that uses functions from F as hypotheses (see [Haussler, 1992], <ref> [Vapnik, Chervonenkis, 1971] </ref>, [Blumer et al., 1989], [Maass, 1995]).
Reference: [von Neumann, 1956] <author> J. von Neumann, </author> <title> Probabilistic logics and the synthesis of reliable or-ganisms from unreliable components. Automata Studies (C. </title> <editor> E. Shannon, J. E. McCarthy, eds.), </editor> <booktitle> 329-378. Annals of Mathematics Studies 34, </booktitle> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ, </address> <year> 1956. </year>
Reference-contexts: We propose and investigate such model in this article. The investigation of noise-tolerant digital computations in the presence of stochastic failures of gates or wires was initiated by <ref> [von Neumann, 1956] </ref>. We refer to [Cowan, 1966], [Pippenger, 1989] and [Gal, 1991] for a small sample of the numerous results that have been achieved in this direction. <p> There are relatively few examples of nontrivial computations on common digital or analog computational models that can achieve perfect reliability of the output in spite of noisy internal components. Most constructions of noise-robust computational models rely on the replication of noisy computational units (see <ref> [von Neumann, 1956] </ref>, [Cowan, 1966]). The idea of this method is that the average of the outputs of k identical noisy networks (with stochastically independent noise processes) is more reliable than the output of a single network.
References-found: 38

