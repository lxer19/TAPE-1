URL: http://cm.bell-labs.com/cm/cs/who/lorenz/papers/gp97.ps
Refering-URL: http://www.cs.bham.ac.uk/~wbl/biblio/gp-bibliography.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: lorenz@research.bell-labs.com  
Title: Learning Recursive Sequences via Evolution of Machine-Language Programs  
Author: Lorenz Huelsbergen 
Note: Appears in 1997 Conference on Genetic Programming  
Address: 600 Mountain Avenue, Murray Hill, NJ 07974  
Affiliation: Bell Laboratories, Lucent Technologies  
Abstract: We use directed search techniques in the space of computer programs to learn recursive sequences of positive integers. Specifically, the integer sequences of squares, x 2 ; cubes, x 3 ; factorial, x!; and Fibonacci numbers are studied. Given a small finite prefix of a sequence, we show that three directed searches|machine-language genetic programming with crossover, exhaustive iterative hill climbing, and a hybrid (crossover and hill climbing)|can automatically discover programs that exactly reproduce the finite target prefix and, moreover, that correctly produce the remaining sequence up to the underlying machine's precision. Our machine-language representation is generic|it contains instructions for arithmetic, register manipulation and comparison, and control flow. We also introduce an output instruction that allows variable-length sequences as result values. Importantly, this representation does not contain recursive operators; recursion, when needed, is automatically synthesized from primitive instructions. For a fixed set of search parameters (e.g., instruction set, program size, fitness criteria), we compare the efficiencies of the three directed search techniques on the four sequence problems. For this parameter set, an evolutionary-based search always outperforms exhaustive hill climbing as well as undirected random search. Since only the prefix of the target sequence is variable in our experiments, we posit that this approach to sequence induction is potentially quite general. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. W. Appel and D. B. MacQueen. </author> <title> A Standard ML compiler. </title> <booktitle> Functional Programming Languages and Computer Architecture, </booktitle> <volume> 274 </volume> <pages> 301-324, </pages> <year> 1987. </year>
Reference-contexts: It is written in the Standard ML (SML) programming language [10] and implemented using the Standard ML of New Jersey (SML/NJ) compiler <ref> [1] </ref>.
Reference: [2] <author> N. L. Cramer. </author> <title> A representation for the adaptive generation of simple sequential programs. </title> <booktitle> In Proceeding of the International Conference on Genetic Algorithms and their Applications, </booktitle> <pages> pages 183-187. </pages> <institution> Texas Instruments, </institution> <month> July </month> <year> 1985. </year>
Reference-contexts: We then consider a hybrid search composed of machine-language genetic programming and hill climbing; it also performs well on this paper's set of sequence problems. Random search, on the other hand, is found to be ineffective. Machine-language genetic programming (MLGP) <ref> [11, 7, 2] </ref> is a form of genetic algorithm (GA) [6] closely related to Koza's genetic programming (GP) [8] of Lisp expressions. GAs use principles from evolutionary theory (populations, fitness criteria, recombination) to search large non-linear spaces.
Reference: [3] <author> R. M. Friedberg. </author> <title> A learning machine: Part I. </title> <journal> 8 IBM Journal of Research and Development, </journal> <volume> 2 </volume> <pages> 2-13, </pages> <year> 1958. </year>
Reference-contexts: MLGP has roots in work by Friedberg et al <ref> [4, 3] </ref> on evolving programs; such early approaches however did not improve upon random search (i.e. guessing).
Reference: [4] <author> R. M. Friedberg, B. Dunham, and J. H. </author> <title> North. A learning machine: Part II. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 3 </volume> <pages> 282-287, </pages> <year> 1959. </year>
Reference-contexts: MLGP has roots in work by Friedberg et al <ref> [4, 3] </ref> on evolving programs; such early approaches however did not improve upon random search (i.e. guessing).
Reference: [5] <author> S. Handley. </author> <title> A new class of function sets for solving sequence problems. </title> <booktitle> In Proceedings of the Conference on Genetic-Programming, </booktitle> <pages> pages 301-308, </pages> <month> July </month> <year> 1996. </year>
Reference-contexts: That is, when evaluating the test for Fibonacci (x n ), p's results of prior tests (Fibonacci (x i ) for i &lt; n) are available to p through SRF. Arguably, inclusion of SRF requires a prior knowledge of the structure of the solution. Other researchers (e.g. <ref> [5] </ref>) introduce explicit domain-specific sequence instructions 2 into the representation (instruction set) defining the search space. They thus sidestep (as does SRF) the task of synthesizing recursion from non-recursive instructions by requiring a human to provide recursion within the set of supplied instructions.
Reference: [6] <author> J. Holland. </author> <title> Adapation in Natural and Artifical Systems. </title> <publisher> University of Michigan Press, </publisher> <year> 1975. </year>
Reference-contexts: Random search, on the other hand, is found to be ineffective. Machine-language genetic programming (MLGP) [11, 7, 2] is a form of genetic algorithm (GA) <ref> [6] </ref> closely related to Koza's genetic programming (GP) [8] of Lisp expressions. GAs use principles from evolutionary theory (populations, fitness criteria, recombination) to search large non-linear spaces. GP and MLGP apply such search to the space of computer programs primarily via the crossover (XO) genetic operator.
Reference: [7] <author> L. Huelsbergen. </author> <title> Toward simulated evolution of machine-language iteration. </title> <booktitle> In Proceedings of the Conference on Genetic-Programming, </booktitle> <pages> pages 315-320, </pages> <month> July </month> <year> 1996. </year>
Reference-contexts: We then consider a hybrid search composed of machine-language genetic programming and hill climbing; it also performs well on this paper's set of sequence problems. Random search, on the other hand, is found to be ineffective. Machine-language genetic programming (MLGP) <ref> [11, 7, 2] </ref> is a form of genetic algorithm (GA) [6] closely related to Koza's genetic programming (GP) [8] of Lisp expressions. GAs use principles from evolutionary theory (populations, fitness criteria, recombination) to search large non-linear spaces. <p> In particular, we build on prior work <ref> [7] </ref> that evolved iteration and control flow using primitive machine instructions. <p> Our line of inquiry is to demonstrate that GAs (as MLGP) can utilize primitive machine instructions as building blocks for complex programs. Our approach is general in that a single instruction set|embodied as a virtual register machine (VRM, cf. <ref> [7] </ref>) for sequences, called VRM-S|suffices for the four problems under consideration. VRM-S consists of instructions for basic register manipulation (move, set, clear, increment, decrement), arithmetic (add, subtract, multiply, divide, negate), control flow (conditional and unconditional branches) and output. <p> Section 4 describes the experimental setup in general and the four search techniques (XO, EIHC, XO-EIHC, Random) in particular. Results and conclusions of using the searches to learn the four sequences defined by the functions of 2 Finnegan System Finnegan (cf. <ref> [7] </ref>) is a framework for experimenting with simulated evolution of machine-language programs. It is written in the Standard ML (SML) programming language [10] and implemented using the Standard ML of New Jersey (SML/NJ) compiler [1]. <p> The trapping arithmetic operators (, , , and ff) denote the respective integer operation, but yield zero on exceptional cases (overflow, underflow, divide-by-zero). 4 VRM-S is a proper superset of VRM-M <ref> [7] </ref> previously used to evolve machine-language iteration. The Out instruction is, however, novel to VRM-S and to MLGP in general. Programs can use Out to place an integer on an output stream.
Reference: [8] <author> J. Koza. </author> <title> Genetic Programming: On the Programming of Computers by the Means of Natural Selection. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1992. </year>
Reference-contexts: Random search, on the other hand, is found to be ineffective. Machine-language genetic programming (MLGP) [11, 7, 2] is a form of genetic algorithm (GA) [6] closely related to Koza's genetic programming (GP) <ref> [8] </ref> of Lisp expressions. GAs use principles from evolutionary theory (populations, fitness criteria, recombination) to search large non-linear spaces. GP and MLGP apply such search to the space of computer programs primarily via the crossover (XO) genetic operator. <p> MLGP has roots in work by Friedberg et al [4, 3] on evolving programs; such early approaches however did not improve upon random search (i.e. guessing). Koza has used GP to learn the Fibonacci function <ref> [8] </ref> (among many other, albeit non-recursive, functions and programs). 1 To do so, he added a recursive function (called SRF) to the set of GP functions. For a program p, SRF retains the values produced by p on previous tests. <p> The ability to synthesize arbitrary control flow, coupled with readable and writable memory locations, enables the juxtaposition of non-recursive 1 The square and cube functions are similar to the simple regressions commonly solved via GP (e.g. <ref> [8] </ref>); their recursive solution by MLGP is novel as is their treatment here as sequences. <p> Population Selection Population selection, for the construction of successive generations, is performed via proportional selection (see, e.g., <ref> [8, 9] </ref>). Let P be a population (set) of N programs.
Reference: [9] <author> Z. Michalewicz. </author> <title> Genetic Algorithms + Data Structures = Evolution Programs. </title> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1992. </year>
Reference-contexts: Population Selection Population selection, for the construction of successive generations, is performed via proportional selection (see, e.g., <ref> [8, 9] </ref>). Let P be a population (set) of N programs.
Reference: [10] <author> R. Milner, M. Tofte, and R. Harper. </author> <title> The Definition of Standard ML. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Results and conclusions of using the searches to learn the four sequences defined by the functions of 2 Finnegan System Finnegan (cf. [7]) is a framework for experimenting with simulated evolution of machine-language programs. It is written in the Standard ML (SML) programming language <ref> [10] </ref> and implemented using the Standard ML of New Jersey (SML/NJ) compiler [1].
Reference: [11] <author> P. Nordin. </author> <title> A compiling genetic programming system that directly manipulates the machine-code. </title> <editor> In K. Kinnear Jr., editor, </editor> <booktitle> Advances in Genetic Programming, chapter 14, </booktitle> <pages> pages 311-331. </pages> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: We then consider a hybrid search composed of machine-language genetic programming and hill climbing; it also performs well on this paper's set of sequence problems. Random search, on the other hand, is found to be ineffective. Machine-language genetic programming (MLGP) <ref> [11, 7, 2] </ref> is a form of genetic algorithm (GA) [6] closely related to Koza's genetic programming (GP) [8] of Lisp expressions. GAs use principles from evolutionary theory (populations, fitness criteria, recombination) to search large non-linear spaces.
Reference: [12] <author> U.-M. O'Reilly and F. Oppacher. </author> <title> A comparative analysis of genetic programming. </title> <editor> In K. Kinnear and P. J. Angeline, editors, </editor> <booktitle> Advances in Genetic Programming II, chapter 2, </booktitle> <pages> pages 23-44. </pages> <publisher> MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: In comparing the efficiencies of the various search techniques by counting program evaluations, we find that an evolutionary search (XO or XO-EIHC) outperforms exhaustive hill climbing (EIHC) on the problems considered. The work of O'Reilly and Oppacher <ref> [12] </ref> similarly contrasts GP, stochastic iterated hill climbing, and hybrids thereof; their problem set and representation, however, differ qualitatively from ours, making direct comparisons of quantitative conclusions unenlightening. <p> Since it moves in the direction with the greatest improvement in fitness, it is a "steepest gradient" search. Many variations on hill climbing are possible: movement in the direction of the first improvement (instead of the best), stochastic selection of some single-point changes (cf. <ref> [12] </ref>), examination of multi-point changes. We have not explored the impact on performance that such variations may have. 4.3.3 Hybrid Search ( XO-EIHC) We combine XO with EIHC to form a hybrid search method (XO-EIHC) as follows. Recall that MLGP with XO proceeds in generations.
Reference: [13] <editor> E. Rich. </editor> <booktitle> Artificial Intelligence. </booktitle> <publisher> McGraw-Hill, </publisher> <year> 1983. </year> <month> 9 </month>
Reference-contexts: In this paper, we consider only machine-language programs of fixed size and composed of such instructions. Hill climbing is a well-known search technique (see, for example, <ref> [13] </ref>) that, given a candidate individual, examines near neighbors for an improvement. Iterative hill climbers continue this process with a "better" neighbor as a replacement for the candidate individual until a (global or local) optimum is reached.
References-found: 13

