URL: ftp://robotics.stanford.edu/pub/gjohn/papers/c45ap.ps
Refering-URL: http://www.robotics.stanford.edu/~gjohn/pubs.html
Root-URL: http://www.robotics.stanford.edu
Email: ronnyk@CS.Stanford.EDU  gjohn@CS.Stanford.EDU  
Title: Automatic Parameter Selection by Minimizing Estimated Error  
Author: Ron Kohavi George H. John 
Note: In Armand Prieditis Stuart Russell, eds., Machine Learning: Proceedings of the Twelfth International Conference, Morgan Kaufmann Publishers,  
Web: http://robotics.stanford.edu/~ronnyk  http://robotics.stanford.edu/~gjohn  
Address: Stanford, CA 94305  Stanford, CA 94305  San Francisco, 1995.  
Affiliation: Computer Science Dept. Stanford University  Computer Science Dept. Stanford University  
Abstract: We address the problem of finding the parameter settings that will result in optimal performance of a given learning algorithm using a particular dataset as training data. We describe a "wrapper" method, considering determination of the best parameters as a discrete function optimization problem. The method uses best-first search and cross-validation to wrap around the basic induction algorithm: the search explores the space of parameter values, running the basic algorithm many times on training and holdout sets produced by cross-validation to get an estimate of the expected error of each parameter setting. Thus, the final selected parameter settings are tuned for the specific induction algorithm and dataset being studied. We report experiments with this method on 33 datasets selected from the UCI and StatLog collections using C4.5 as the basic induction algorithm. At a 90% confidence level, our method improves the performance of C4.5 on nine domains, degrades performance on one, and is statistically indistinguishable from C4.5 on the rest. On the sample of datasets used for comparison, our method yields an average 13% relative decrease in error rate. We expect to see similar performance improvements when using our method with other machine learning al gorithms.
Abstract-found: 1
Intro-found: 1
Reference: <author> Brazdil, P., Gama, J. & Henery, B. </author> <year> (1994), </year> <title> Characterizing the applicability of classification algorithms using meta-level learning, </title> <editor> in F. Bergadano & L. D. Raedt, eds, </editor> <booktitle> "Machine Learning: ECML-94. European Conference on Machine Learning", Lectures Notes in Artificial Intelligence, </booktitle> <publisher> Springer-Verlag, Catania,Italy, </publisher> <pages> pp. 83-102. </pages>
Reference-contexts: The user wants to choose the algorithm and parameters that result in the best future performance. Although the former problem of selecting a learning algorithm for a particular task is recognized as an important issue in machine learning <ref> (Brazdil, Gama & Henery 1994, Schaffer 1993) </ref>, the latter problem of finding the best parameter values has not been systematically studied.
Reference: <author> Breiman, L., Friedman, J., Olshen, R. & Stone, C. </author> <year> (1984), </year> <title> Classification and Regression Trees, </title> <publisher> Chapman & Hall, </publisher> <address> New York. </address>
Reference: <author> Caruana, R. & Freitag, D. </author> <year> (1994), </year> <title> Greedy attribute selection, </title> <booktitle> in Hirsh & Cohen (1994), </booktitle> <pages> pp. 28-37. </pages>
Reference: <author> Casella, G. & Berger, R. L. </author> <year> (1990), </year> <title> Statistical Inference, </title> <publisher> Wadsworth & Brooks/Cole. </publisher>
Reference-contexts: We believe that best first search conducts a reasonably thorough search of the space, and therefore we conjecture that the results of C4.5* cannot be significantly improved upon by any settings of C4.5 parameters. For each dataset in Table 4, we used a one-tailed paired t-test <ref> (Casella & Berger 1990) </ref> to test the hypothesis that the accuracy of the C4.5-AP algorithm is higher than the C4.5 algorithm (versus the null hypothesis that the algorithms perform equally). When we run 10-fold cross-validation, we get ten accuracy estimates that we average to give the final estimated accuracy.
Reference: <author> Craven, M. W. & Shavlik, J. W. </author> <year> (1993), </year> <title> Learning symbolic rules using artificial neural networks, </title> <editor> in P. Utgoff, ed., </editor> <booktitle> "Proceedings of the Tenth International Conference on Machine Learning", </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 73-80. </pages>
Reference: <author> Finnoff, W., Hergert, F. & Zimmerman, H. G. </author> <year> (1993), </year> <title> "Improving model selection by nonconvergent methods", </title> <booktitle> Neural Networks 6(6), </booktitle> <pages> 771-783. </pages>
Reference: <author> Gasser, T., Kneip, A. & Kohler, W. </author> <year> (1991), </year> <title> "A flexible and fast method for automatic smoothing", </title> <journal> Journal of the American Statistical Association 86(415), </journal> <pages> 643-652. </pages>
Reference: <author> Ginsberg, M. L. </author> <year> (1993), </year> <booktitle> Essentials of Artificial Intelligence, </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: We chose to view the problem as state-space search (Kohavi 1994). We chose the best-first search algorithm <ref> (Ginsberg 1993, Nilsson 1980) </ref>, which works by repeatedly expanding the most promising unexpanded state (Table 2). (Note that this is not simply hill-climbing.) Search problems can be characterized by five distinct components.
Reference: <author> Hirsh, H. & Cohen, W., </author> <booktitle> eds (1994), Machine Learning: Proceedings of the Eleventh International Conference, </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Holte, R. C. </author> <year> (1993), </year> <title> "Very simple classification rules perform well on most commonly used datasets", </title> <booktitle> Machine Learning 11(1), </booktitle> <pages> 63-90. </pages>
Reference: <author> John, G. H. </author> <year> (1994), </year> <title> Cross-validated C4.5: Using error estimation for automatic parameter selection, </title> <type> Technical Report STAN-CS-TN-94-12, </type> <institution> Computer Science Department, Stanford University. </institution> <note> Available by anonymous ftp from starry.Stanford. EDU:pub/gjohn/papers/cvc45.ps. </note>
Reference-contexts: When the estimated standard deviation of err cv is larger than 1%, we run cross-validation again, up to a maximum of three times. We used a 10% trimmed mean <ref> (John 1994) </ref>; that is, if we run 10-fold CV three times, then out of the 30 resulting estimates we remove the lowest and highest 3 and take the average of the rest. * Operators For the binary parameters, we try the opposite values.
Reference: <author> John, G., Kohavi, R. & Pfleger, K. </author> <year> (1994), </year> <title> Irrelevant features and the subset selection problem, </title>
Reference-contexts: When the estimated standard deviation of err cv is larger than 1%, we run cross-validation again, up to a maximum of three times. We used a 10% trimmed mean <ref> (John 1994) </ref>; that is, if we run 10-fold CV three times, then out of the 30 resulting estimates we remove the lowest and highest 3 and take the average of the rest. * Operators For the binary parameters, we try the opposite values.
References-found: 12

