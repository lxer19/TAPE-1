URL: ftp://ftp.cs.virginia.edu/pub/techreports/IPC-92-12.ps.Z
Refering-URL: ftp://ftp.cs.virginia.edu/pub/techreports/README.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Access Ordering Algorithms for an Interleaved Memory  
Author: Steven A. Moyer 
Date: December 18, 1992  
Pubnum: IPC-TR-92-012  
Abstract-found: 0
Intro-found: 1
Reference: [BeDa91] <author> Benitez-M, Davidson-J, </author> <title> Code Generation for Streaming: an Access/Execute Mechanism, </title> <booktitle> Proc. </booktitle> <address> ASPLOS-IV, </address> <year> 1991, </year> <pages> pp. 132-141. </pages>
Reference-contexts: Since vectorizable loops contain no loop-carried dependencies, excepting ignorable input dependence and self-antidependence cycles [Wolf89], reordering accesses within an unrolled loop is simplified. Note that recurrence relations can often be eliminated through streaming optimizations <ref> [BeDa91] </ref>, so that algorithms developed here are actually applicable to a superset of the vectorizable loops. 1.5 Memory Device Types For stream-oriented computations, access ordering reorders references within an unrolled loop to exploit features of the underlying memory system. <p> Benitez and Davidson <ref> [BeDa91] </ref> describe a technique for detecting streaming opportunities, including those in recurrence relations. Callahan et al [CaCK90] present a technique called scalar replacement that detects redundant accesses to subscripted variables in a loop, often transforming a more complex sequence of references to a vector into a single access stream. <p> Loop-carried input dependence can result from the transformation of a more complex sequence of read accesses to a single read stream. Consider the finite difference approximation to the first derivative Analysis techniques <ref> [BeDa91, CaCK90] </ref> can transform the natural pattern of access to vector to a simple stream requiring one access per iteration; two values of are preloaded prior to entering the loop, and each successive value accessed is carried in a register for two iterations.
Reference: [BeRo91] <author> Bernstein-D, Rodeh-M, </author> <title> Global Instruction Scheduling for Superscalar Machines, </title> <booktitle> Proc. SIGPLAN91 Conf. Prog. Lang. Design and Implementation, </booktitle> <year> 1991, </year> <pages> pp. 241-255. </pages>
Reference-contexts: Essentially, access scheduling techniques attempt to separate the execution of a load/store instruction from the execution of the instruction which consumes/produces its operand, reducing the time the processor spends delayed on memory requests. Bernstein and Rodeh <ref> [BeRo91] </ref> present an algorithm for scheduling intra-loop instructions on superscalar architectures that accommodates load delay.
Reference: [BuKu71] <author> Budnik-P, Kuck-D, </author> <title> The Organization and Use of Parallel Memories, </title> <journal> IEEE Trans. Com-put., </journal> <volume> 20, 12, </volume> <year> 1971, </year> <pages> pp. 1566-1569. </pages>
Reference-contexts: Research into parallel memory systems is generally directed towards developing storage schemes, i.e. mappings of addresses to memory locations, that reduce module conict and hence increase concur-rency. Proposed storage schemes include the use of a prime number of modules [LaVo82], skewed storage <ref> [BuKu71, HaJu87] </ref>, and dynamic address transformations [Harp89, Rau91]. Note that these techniques are dependent on a relatively long sequence of references to a single vector. Scalar processors executing scientific codes generate an interleaved sequence of references to a set of vector operands.
Reference: [CaCK90] <author> Callahan-D, Carr-S, Kennedy-K, </author> <title> Improving Register Allocation for Subscripted Variables, </title> <booktitle> Proc. SIGPLAN 90 Conf. Prog. Lang. Design and Implementation, </booktitle> <year> 1990, </year> <pages> pp. 53-65. </pages>
Reference-contexts: Benitez and Davidson [BeDa91] describe a technique for detecting streaming opportunities, including those in recurrence relations. Callahan et al <ref> [CaCK90] </ref> present a technique called scalar replacement that detects redundant accesses to subscripted variables in a loop, often transforming a more complex sequence of references to a vector into a single access stream. <p> Loop-carried input dependence can result from the transformation of a more complex sequence of read accesses to a single read stream. Consider the finite difference approximation to the first derivative Analysis techniques <ref> [BeDa91, CaCK90] </ref> can transform the natural pattern of access to vector to a simple stream requiring one access per iteration; two values of are preloaded prior to entering the loop, and each successive value accessed is carried in a register for two iterations.
Reference: [CaKe89] <author> Carr-S, Kennedy-K, </author> <title> Blocking Linear Algebra Codes for Memory Hierarchies, </title> <booktitle> Proc. of the Fourth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <year> 1989. </year>
Reference-contexts: In general purpose scalar computing, the addition of cache memory is often a sufficient solution to the memory latency and bandwidth problems given the spatial and temporal locality of reference exhibited by most codes. For scientific computations, vectors are normally too large to cache. Iteration space tiling <ref> [CaKe89, Wolf89] </ref> can partition problems into cache-size blocks, however tiling often creates cache conicts [LaRW91] and the technique is difficult to automate. Furthermore, only a subset of the vectors accessed will generally be reused and hence benefit from caching.
Reference: [CaKP91] <author> Callahan-D, Kennedy-K, Porterfield-A, </author> <title> Software Prefetching, </title> <booktitle> Proc. </booktitle> <address> ASPLOS-IV, </address> <year> 1991, </year> <pages> pp. 40-52. </pages>
Reference-contexts: Weiss and Smith [WeSm90] present a comprehensive study in which they classify and evaluate software pipelining techniques implemented in conjunction with loop unrolling. Klaiber and Levy [KlLe91] and Callahan et al <ref> [CaKP91] </ref> propose the use of fetch instructions to preload data into cache; compiler techniques are developed for inserting fetch instructions into the normal instruction stream. 8 Access ordering and access scheduling are fundamentally different.
Reference: [HaJu87] <author> Harper-D, Jump-J, </author> <title> Vector Access Performance in Parallel Memories Using a Skewed Storage Scheme, </title> <journal> IEEE Trans. Comput., </journal> <volume> 36, 12, </volume> <year> 1987, </year> <pages> pp. 1440-1449. </pages>
Reference-contexts: Research into parallel memory systems is generally directed towards developing storage schemes, i.e. mappings of addresses to memory locations, that reduce module conict and hence increase concur-rency. Proposed storage schemes include the use of a prime number of modules [LaVo82], skewed storage <ref> [BuKu71, HaJu87] </ref>, and dynamic address transformations [Harp89, Rau91]. Note that these techniques are dependent on a relatively long sequence of references to a single vector. Scalar processors executing scientific codes generate an interleaved sequence of references to a set of vector operands.
Reference: [Harp89] <author> Harper-D, </author> <title> Address Transformations to Increase Memory Performance, </title> <booktitle> Proc. 1989 Intl. Conf. Parallel Processing, </booktitle> <year> 1989, </year> <pages> pp. 237-241. </pages>
Reference-contexts: Research into parallel memory systems is generally directed towards developing storage schemes, i.e. mappings of addresses to memory locations, that reduce module conict and hence increase concur-rency. Proposed storage schemes include the use of a prime number of modules [LaVo82], skewed storage [BuKu71, HaJu87], and dynamic address transformations <ref> [Harp89, Rau91] </ref>. Note that these techniques are dependent on a relatively long sequence of references to a single vector. Scalar processors executing scientific codes generate an interleaved sequence of references to a set of vector operands.
Reference: [Inte89] <author> Intel Corporation, </author> <title> i860 64-Bit Microprocessor Hardware Reference Manual, </title> <address> ISBN 1-55512-106-3, </address> <year> 1989. </year>
Reference-contexts: This general system model is representative of uniprocessors and single-processor nodes of distributed memory parallel machines. The processor is presumed to implement a non-caching load instruction, ala Intels i860 <ref> [Inte89] </ref>, allowing the sequence of requests observed by the memory system to be con amod m ( ) 3 trolled via software. For access ordering, all memory references are assumed to be non-caching.
Reference: [KlLe91] <author> Klaiber-A, Levy-H, </author> <title> An Architecture for Software-Controlled Data Prefetching, </title> <booktitle> Proc. 18th Annual Intl. Symp. Comput. Architecture, </booktitle> <year> 1991, </year> <pages> pp. 43-53. </pages>
Reference-contexts: Weiss and Smith [WeSm90] present a comprehensive study in which they classify and evaluate software pipelining techniques implemented in conjunction with loop unrolling. Klaiber and Levy <ref> [KlLe91] </ref> and Callahan et al [CaKP91] propose the use of fetch instructions to preload data into cache; compiler techniques are developed for inserting fetch instructions into the normal instruction stream. 8 Access ordering and access scheduling are fundamentally different.
Reference: [Lam88] <author> Lam-M, </author> <title> Software Pipelining: An Effective Scheduling Technique for VLIW Machines, </title> <booktitle> Proc. SIGPLAN88 Conf. Prog. Lang. Design and Implementation, </booktitle> <year> 1988, </year> <pages> pp. 318-328. </pages>
Reference-contexts: Bernstein and Rodeh [BeRo91] present an algorithm for scheduling intra-loop instructions on superscalar architectures that accommodates load delay. Lam <ref> [Lam88] </ref> presents a technique referred to as software pipelining that structures code such that a given loop iteration loads the data for a later iteration, stores results from a previous iteration, and performs computation for the current iteration.
Reference: [LaRW91] <author> Lam-M, Rothberg-E, Wolf-M, </author> <title> The Cache Performance and Optimizations of Blocked Algorithms, </title> <booktitle> Fourth International Conf. on Arch. Support for Prog. Langs. and Operating Systems, </booktitle> <year> 1991, </year> <pages> pp. 63-74. </pages>
Reference-contexts: For scientific computations, vectors are normally too large to cache. Iteration space tiling [CaKe89, Wolf89] can partition problems into cache-size blocks, however tiling often creates cache conicts <ref> [LaRW91] </ref> and the technique is difficult to automate. Furthermore, only a subset of the vectors accessed will generally be reused and hence benefit from caching. Finally, caching may actually reduce effective memory bandwidth by fetching extraneous data for non-unit strides. Thus, as . <p> Finally, caching may actually reduce effective memory bandwidth by fetching extraneous data for non-unit strides. Thus, as . Both superscalar and VLIW architectures are suited for scientific applications and place similar demands on the memory system. 2 noted by Lam et al <ref> [LaRW91] </ref>, while data caches have been demonstrated to be effective for general-purpose applications..., their effectiveness for numerical code has not been established. Access ordering [Moye92b] is a compiler technology that addresses the memory bandwidth problem for scalar processors executing scientific codes.
Reference: [LaVo82] <author> Lawrie-D, Vora-C, </author> <title> The Prime Memory System for Array Access, </title> <journal> IEEE Trans. Comput., </journal> <volume> 31, 5, </volume> <year> 1982, </year> <pages> pp. 435-442. </pages>
Reference-contexts: Research into parallel memory systems is generally directed towards developing storage schemes, i.e. mappings of addresses to memory locations, that reduce module conict and hence increase concur-rency. Proposed storage schemes include the use of a prime number of modules <ref> [LaVo82] </ref>, skewed storage [BuKu71, HaJu87], and dynamic address transformations [Harp89, Rau91]. Note that these techniques are dependent on a relatively long sequence of references to a single vector. Scalar processors executing scientific codes generate an interleaved sequence of references to a set of vector operands.
Reference: [Lee90] <author> Lee-K, </author> <title> On the Floating-Point Performance of the i860 Microprocessor, </title> <institution> NASA Ames Research Center, NAS Systems Division, RNR-090-019, </institution> <year> 1990. </year>
Reference-contexts: 1 Introduction Superscalar pipelined processors are well suited for meeting the demands of scientific computing, singly and as components of parallel machines. However, studies demonstrate that for such applications, performance is limited by the processor-memory bandwidth <ref> [Lee90, Moye91] </ref>. For vector computers, parallel memory modules are employed to increase effective bandwidth through concurrent processing of memory requests. Research into parallel memory systems is generally directed towards developing storage schemes, i.e. mappings of addresses to memory locations, that reduce module conict and hence increase concur-rency.
Reference: [Lee91] <author> Lee-K, </author> <title> Achieving High Performance on the i860 Microprocessor with Naspack Subroutines, </title> <institution> NASA Ames Research Center, NAS Systems Division, RNR-091-029, </institution> <year> 1991. </year>
Reference-contexts: However, several important topics are briey discussed below; a more complete treatment of these issues can be found in [Moye92b]. Access ordering employs loop unrolling which creates register pressure and has traditionally been limited by register resources. Lee <ref> [Lee91] </ref> presents a technique that employs cache memory to mimic a set of vectors registers, effectively increasing register file size for vector computations. Essentially, storage is defined for a set of pseudo vector registers and placed in cache via a standard (caching) load instruction.
Reference: [Mcma90] <author> McMahon-F, </author> <title> FORTRAN Kernels: </title> <type> MFLOPS, </type> <institution> Lawrence Livermore National Laboratory, </institution> <note> Version MF443. </note>
Reference-contexts: For all computations the depth of loop unrolling is 4, data is double-precision, and vectors are aligned to module . The daxpy and dvaxpy computations are double-precision versions of the axpy and vaxpy computations, respectively, discussed earlier. The remaining computations are selections from the Livermore Loops <ref> [Mcma90] </ref>. This set of scientific kernels serves as the benchmark suite for all subsequent simulations. Table 1 Module Parameters (Uniform) Parameter Value 8 40 T u/ r M 0 40 Access ordering improves performance over the natural access sequence for the given computations from 78% to 300%.
Reference: [Moye91] <author> Moyer-S, </author> <title> Performance of the iPSC/860 Node Architecture, </title> <institution> University of Virginia, IPC-TR-91-007, </institution> <year> 1991. </year> <month> 69 </month>
Reference-contexts: 1 Introduction Superscalar pipelined processors are well suited for meeting the demands of scientific computing, singly and as components of parallel machines. However, studies demonstrate that for such applications, performance is limited by the processor-memory bandwidth <ref> [Lee90, Moye91] </ref>. For vector computers, parallel memory modules are employed to increase effective bandwidth through concurrent processing of memory requests. Research into parallel memory systems is generally directed towards developing storage schemes, i.e. mappings of addresses to memory locations, that reduce module conict and hence increase concur-rency. <p> Module parameters are defined in Table 4 and are representative of the IPSC/860 node memory system <ref> [Moye91] </ref>. Table 5 compares effective memory bandwidth achieved by the natural versus ordered access sequence for the benchmark kernels. The depth of loop unrolling is 4, data is double-precision, and all vectors are aligned to module .
Reference: [Moye92a] <author> Moyer-S, </author> <title> Access Ordering Algorithms for a Single Module Memory, </title> <institution> University of Vir-ginia, IPC-TR-92-002, </institution> <year> 1992. </year>
Reference-contexts: Interleaving is the most prevalent parallel memory storage scheme whereby for an m module system, word a maps to module . 1.1 Background This work builds on previous analytic results derived for a single module memory system <ref> [Moye92a] </ref>. To make this document self-contained, the necessary analysis from that report is repeated here.
Reference: [Moye92b] <author> Moyer-S, </author> <title> Access Ordering and Effective Memory Bandwidth, </title> <type> Ph.D. </type> <institution> Dissertation in progress, Computer Science Department, University of Virginia. </institution>
Reference-contexts: Both superscalar and VLIW architectures are suited for scientific applications and place similar demands on the memory system. 2 noted by Lam et al [LaRW91], while data caches have been demonstrated to be effective for general-purpose applications..., their effectiveness for numerical code has not been established. Access ordering <ref> [Moye92b] </ref> is a compiler technology that addresses the memory bandwidth problem for scalar processors executing scientific codes. Access ordering is a loop optimization that reorders non-caching accesses to better utilize memory system resources. <p> The following sections provide the minimal level of context neces T p/ r T p/ m 7 sary to characterize the contributions of this work; a more complete survey of all relevant topics can be found in <ref> [Moye92b] </ref>. 2.1 Stream Detection Access ordering algorithms derived in this report presuppose the existence of compiler techniques to detect stream-oriented computations. Benitez and Davidson [BeDa91] describe a technique for detecting streaming opportunities, including those in recurrence relations. <p> However, several important topics are briey discussed below; a more complete treatment of these issues can be found in <ref> [Moye92b] </ref>. Access ordering employs loop unrolling which creates register pressure and has traditionally been limited by register resources. Lee [Lee91] presents a technique that employs cache memory to mimic a set of vectors registers, effectively increasing register file size for vector computations.
Reference: [Quin91] <author> Quinnell-R, </author> <title> High-speed DRAMs, </title> <type> EDN, </type> <month> May 23, </month> <year> 1991, </year> <pages> pp. 106-116. </pages>
Reference-contexts: Thus, the effective bandwidth is sensitive to the sequence of requests. Nearly all DRAMs currently manufactured implement a form of page-mode operation <ref> [Quin91] </ref>. the vaxpy, vector axpy, computation . Note that a DRAM page should not be confused with a virtual memory page; this is an unfortunate overloading of terms.
Reference: [Rau91] <author> Rau-B, </author> <title> Pseudo-Randomly Interleaved Memory, </title> <booktitle> Proc. 18th Intl. Symp. Comput. Architecture, </booktitle> <year> 1991, </year> <pages> pp. 74-83. </pages>
Reference-contexts: Research into parallel memory systems is generally directed towards developing storage schemes, i.e. mappings of addresses to memory locations, that reduce module conict and hence increase concur-rency. Proposed storage schemes include the use of a prime number of modules [LaVo82], skewed storage [BuKu71, HaJu87], and dynamic address transformations <ref> [Harp89, Rau91] </ref>. Note that these techniques are dependent on a relatively long sequence of references to a single vector. Scalar processors executing scientific codes generate an interleaved sequence of references to a set of vector operands.
Reference: [WeSm90] <author> Weiss-S, Smith-J, </author> <title> A Study of Scalar Compilation Techniques for Pipelined Supercomputers, </title> <journal> ACM Trans. Math. Soft., </journal> <volume> 16, 3, </volume> <year> 1990, </year> <pages> pp. 223-245. </pages>
Reference-contexts: Lam [Lam88] presents a technique referred to as software pipelining that structures code such that a given loop iteration loads the data for a later iteration, stores results from a previous iteration, and performs computation for the current iteration. Weiss and Smith <ref> [WeSm90] </ref> present a comprehensive study in which they classify and evaluate software pipelining techniques implemented in conjunction with loop unrolling.
Reference: [Wolf89] <author> Wolfe-M, </author> <title> Optimizing Supercompilers for Supercomputers, </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1989. </year>
Reference-contexts: In general purpose scalar computing, the addition of cache memory is often a sufficient solution to the memory latency and bandwidth problems given the spatial and temporal locality of reference exhibited by most codes. For scientific computations, vectors are normally too large to cache. Iteration space tiling <ref> [CaKe89, Wolf89] </ref> can partition problems into cache-size blocks, however tiling often creates cache conicts [LaRW91] and the technique is difficult to automate. Furthermore, only a subset of the vectors accessed will generally be reused and hence benefit from caching. <p> In this report, the computation domain for which access ordering algorithms are developed is further restricted to the class of vectorizable loops. Since vectorizable loops contain no loop-carried dependencies, excepting ignorable input dependence and self-antidependence cycles <ref> [Wolf89] </ref>, reordering accesses within an unrolled loop is simplified. <p> Finally, as stream-oriented computations reference vector operands, well known vectorization techniques are applicable, such as those described by Wolfe <ref> [Wolf89] </ref>. 2.2 Access Scheduling Techniques Access ordering is a compilation technique for maximizing effective memory bandwidth. Previous work has focused on reducing load/store interlock delay by overlapping computation with memory latency, referred to here as access scheduling. <p> A dependence relation between two accesses from the same instance of a loop iteration is said to be loop-independent, while a dependence between accesses from different instances is said to be loop-carried. A detailed treatment of dependence analysis can be found in <ref> [Wolf89] </ref>. 3.5.1 Output and Input Dependence Output and input dependence can not exist as a result of the stream interaction restriction; two streams of the same mode have a non-intersecting address space.
References-found: 23

