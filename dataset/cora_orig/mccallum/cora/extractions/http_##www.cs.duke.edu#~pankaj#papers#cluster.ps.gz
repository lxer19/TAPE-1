URL: http://www.cs.duke.edu/~pankaj/papers/cluster.ps.gz
Refering-URL: http://www.cs.duke.edu/CGC/papers.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Exact and Approximation Algortihms for Clustering  
Author: Pankaj K. Agarwal Cecilia M. Procopiuc 
Date: July 8, 1997  
Abstract: In this paper we present a n O(k 11=d ) time algorithm for solving the k-center problem in R d , under L 1 and L 2 metrics. The algorithm extends to other metrics, and can be used to solve the discrete k-center problem, as well. We also describe a simple (1 + *)-approximation algorithm for the k-center problem, with running time O(n log k) + (k=*) O(k 11=d ) . Finally, we present a n O(k 11=d ) time algorithm for solving the L-capacitated k-center problem, provided that L = (n=k 11=d ) or L = O(1). We conclude with a simple approximation algorithm for the L-capacitated k-center problem. fl The work on this paper was partially supported by a National Science Foundation Grant CCR-93-01259, by an Army Research Office MURI grant DAAH04-96-1-0013, by a Sloan fellowship, by an NYI award and matching funds from Xerox Corporation, and by a grant from the U.S.-Israeli Binational Science Foundation. y Department of Computer Science, Box 90129, Duke University, Durham, NC 27708-0129, USA z Department of Computer Science, Box 90129, Duke University, Durham, NC 27708-0129, USA 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. K. Agarwal and M. Sharir, </author> <title> Efficient algorithms for geometric optimization, </title> <type> Tech. Rep. </type> <institution> CS-1996-19, Dept. Computer Science, Duke University, </institution> <year> 1996. </year>
Reference-contexts: The capacitated k-center problem is to compute a k-clustering of the smallest size so that each cluster has at most L points. Previous results. There is a vast literature on clustering problems, see, for example, the books [3, 11, 18], the survey paper <ref> [1] </ref>, and the references there in. Even the simplest clustering problems are known to be NP-Hard, including the Euclidean k-center problem in the plane [13, 24]. In fact, it is NP-Hard to approximate the two-dimensional k-center problem within a factor of &lt; 2 even under the L 1 -metric [12]. <p> The running time was improved by Feder and Greene [12] to O (n log k) for any L p -metric. Several efficient algorithms have been developed for Euclidean and rectilinear k-center problems when k is small; see the survey by Agarwal and Sharir <ref> [1] </ref> for a summary of such results. Gonzalez [15] presented an n O (l) -time algorithm for the k-center problem in R d under the L 1 metric, when the points lie in a horizontal strip of height l; it can be extended to the L 2 -metric as well.
Reference: [2] <author> R. Agrawal, A. Ghosh, T. Imielinski, B. Iyer, and A. Swami, </author> <title> An interval classifier for database mining applications, </title> <booktitle> Proceedings of the 18th Conference on Very Large Databases, </booktitle> <publisher> Morgan Kauffman, </publisher> <month> August </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Clustering a set of points into a few groups is frequently used for statistical analysis and classification in numerous applications, including information retrieval [6, 7, 8, 26], facility location [11, 30], data mining <ref> [2, 28] </ref>, spatial data bases [9, 20, 25], data compression [23], image processing [19, 27], astrophysics [22], and scientific computing [5]. Because of such a diversity of applications, several variants of clustering problems have been proposed and widely studied.
Reference: [3] <author> M. R. Anderberg, </author> <title> Cluster Analysis for Applications, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: The capacitated k-center problem is to compute a k-clustering of the smallest size so that each cluster has at most L points. Previous results. There is a vast literature on clustering problems, see, for example, the books <ref> [3, 11, 18] </ref>, the survey paper [1], and the references there in. Even the simplest clustering problems are known to be NP-Hard, including the Euclidean k-center problem in the plane [13, 24].
Reference: [4] <author> J. Bar-Ilan, G. Kortsarz, and D. Peleg, </author> <title> How to allocate network centers, </title> <journal> J. Algorithms, </journal> <volume> 15 (1993), </volume> <pages> 385-415. </pages>
Reference-contexts: In another paper, Hwang et al. [16] gave a n O ( p k) time algorithm that also works for the Euclidean discrete k-center problem. Not much is known about the capacitated k-center problem. Bar-Ilan et al. <ref> [4] </ref> gave the first polynomial time approximation algorithm for the capacitated k-center problem, achieving an approximation factor of 10. They also proposed an approximation algorithm for the capacitated problem with fixed cluster size. The algorithm approximates the minimum number of clusters by a factor of dln ne.
Reference: [5] <author> M. Berger and I. Rigoutsos, </author> <title> An algorithm for point clustering and grid generation, </title> <journal> IEEE Trans. Systems, Man and Cybernetics., </journal> <volume> 21 (1991), </volume> <pages> 1278-1286. </pages>
Reference-contexts: of points into a few groups is frequently used for statistical analysis and classification in numerous applications, including information retrieval [6, 7, 8, 26], facility location [11, 30], data mining [2, 28], spatial data bases [9, 20, 25], data compression [23], image processing [19, 27], astrophysics [22], and scientific computing <ref> [5] </ref>. Because of such a diversity of applications, several variants of clustering problems have been proposed and widely studied. However, they all require a partition of a given set of points into clusters that optimizes a given objective function.
Reference: [6] <author> M. Charikar, C. Chekuri, T. Feder, and R. Motwani, </author> <title> Incremental clustering and dynamic information retrieval, </title> <booktitle> Proceedings of the 29th Annual ACM Symposium on Theory of Computing, </booktitle> <year> 1997, </year> <pages> pp. 626-634. </pages>
Reference-contexts: 1 Introduction Clustering a set of points into a few groups is frequently used for statistical analysis and classification in numerous applications, including information retrieval <ref> [6, 7, 8, 26] </ref>, facility location [11, 30], data mining [2, 28], spatial data bases [9, 20, 25], data compression [23], image processing [19, 27], astrophysics [22], and scientific computing [5]. Because of such a diversity of applications, several variants of clustering problems have been proposed and widely studied.
Reference: [7] <author> D. R. Cutting, D. R. Karger, and J. O. Pedersen, </author> <title> Constant interaction-time scatter/gather browsing of very large document collections, </title> <booktitle> Proceedings of the Sixteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <year> 1993, </year> <pages> pp. 126-134. </pages>
Reference-contexts: 1 Introduction Clustering a set of points into a few groups is frequently used for statistical analysis and classification in numerous applications, including information retrieval <ref> [6, 7, 8, 26] </ref>, facility location [11, 30], data mining [2, 28], spatial data bases [9, 20, 25], data compression [23], image processing [19, 27], astrophysics [22], and scientific computing [5]. Because of such a diversity of applications, several variants of clustering problems have been proposed and widely studied.
Reference: [8] <author> D. R. Cutting, D. R. Karger, J. O. Pedersen, and J. W. Tukey, Scatter/gather: </author> <title> A cluster-based approach to browsing large document collections, </title> <booktitle> Proceedings of the Fifteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <year> 1992, </year> <pages> pp. 318-329. </pages>
Reference-contexts: 1 Introduction Clustering a set of points into a few groups is frequently used for statistical analysis and classification in numerous applications, including information retrieval <ref> [6, 7, 8, 26] </ref>, facility location [11, 30], data mining [2, 28], spatial data bases [9, 20, 25], data compression [23], image processing [19, 27], astrophysics [22], and scientific computing [5]. Because of such a diversity of applications, several variants of clustering problems have been proposed and widely studied.
Reference: [9] <author> A. A. Diwan, S. Rane, S. Seshadri, and S. Sudarshan, </author> <title> Clustering techniques for minimizing external path length., </title> <booktitle> Proceedings of the International Conference on Very Large Databases, </booktitle> <publisher> Morgan Kauffman, </publisher> <year> 1996. </year>
Reference-contexts: 1 Introduction Clustering a set of points into a few groups is frequently used for statistical analysis and classification in numerous applications, including information retrieval [6, 7, 8, 26], facility location [11, 30], data mining [2, 28], spatial data bases <ref> [9, 20, 25] </ref>, data compression [23], image processing [19, 27], astrophysics [22], and scientific computing [5]. Because of such a diversity of applications, several variants of clustering problems have been proposed and widely studied.
Reference: [10] <author> Z. Drezner, </author> <title> The p-centre problems | Heuristic and optimal algorithms, </title> <journal> J. Oper. Res. Soc., </journal> <volume> 35 (1984), </volume> <pages> 741-748. </pages>
Reference-contexts: A similar approach can be employed when the metric is L 2 . In this case, the clusters are balls, and there exists a k-clustering of S of smallest size so that each ball in the cluster is defined by at most d + 1 points of S <ref> [10] </ref>. Then, fl is the radius of one of the balls defined by d + 1 input points. There are O (n d+1 ) radii of balls determined by d + 1 points of S, and we can compute each one in time O (d 3 ).
Reference: [11] <author> Z. Drezner, ed., </author> <title> Facility Location, </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1995. </year>
Reference-contexts: 1 Introduction Clustering a set of points into a few groups is frequently used for statistical analysis and classification in numerous applications, including information retrieval [6, 7, 8, 26], facility location <ref> [11, 30] </ref>, data mining [2, 28], spatial data bases [9, 20, 25], data compression [23], image processing [19, 27], astrophysics [22], and scientific computing [5]. Because of such a diversity of applications, several variants of clustering problems have been proposed and widely studied. <p> If the centers of the clusters are required to be a subset of the input points, the problem is called the discrete k-center problem. In some applications (e.g. facility location <ref> [11, 30] </ref>, astrophysics [22]), not only the size of a cluster is important but also the number of points in the cluster. We can generalize the notion of k-clustering as follows. <p> The capacitated k-center problem is to compute a k-clustering of the smallest size so that each cluster has at most L points. Previous results. There is a vast literature on clustering problems, see, for example, the books <ref> [3, 11, 18] </ref>, the survey paper [1], and the references there in. Even the simplest clustering problems are known to be NP-Hard, including the Euclidean k-center problem in the plane [13, 24].
Reference: [12] <author> T. Feder and D. H. Greene, </author> <title> Optimal algorithms for approximate clustering, </title> <booktitle> Proc. 20th Annu. ACM Sympos. Theory Comput., </booktitle> <year> 1988, </year> <pages> pp. 434-444. </pages>
Reference-contexts: Even the simplest clustering problems are known to be NP-Hard, including the Euclidean k-center problem in the plane [13, 24]. In fact, it is NP-Hard to approximate the two-dimensional k-center problem within a factor of &lt; 2 even under the L 1 -metric <ref> [12] </ref>. The greedy algorithm by Gonzalez [14] gives a 2-approximation algorithm for the k-center problem in any metric space. This algorithm requires O (kn) distance computations. The running time was improved by Feder and Greene [12] to O (n log k) for any L p -metric. <p> two-dimensional k-center problem within a factor of &lt; 2 even under the L 1 -metric <ref> [12] </ref>. The greedy algorithm by Gonzalez [14] gives a 2-approximation algorithm for the k-center problem in any metric space. This algorithm requires O (kn) distance computations. The running time was improved by Feder and Greene [12] to O (n log k) for any L p -metric. Several efficient algorithms have been developed for Euclidean and rectilinear k-center problems when k is small; see the survey by Agarwal and Sharir [1] for a summary of such results. <p> We present the algorithm for the L 1 metric; it can also be adapted for any L p -metric. First, compute a real number 0 , so that fl 0 2 fl , using the algorithm by Feder and Greene <ref> [12] </ref>, in time O (n log k). Let Z be a grid of size ffi = 0 *=3, i.e. Z = f (iffi; jffi) j i; j 2 Zg.
Reference: [13] <author> R. J. Fowler, M. S. Paterson, and S. L. Tanimoto, </author> <title> Optimal packing and covering in the plane are NP-complete, </title> <journal> Inform. Process. Lett., </journal> <volume> 12 (1981), </volume> <pages> 133-137. </pages>
Reference-contexts: Previous results. There is a vast literature on clustering problems, see, for example, the books [3, 11, 18], the survey paper [1], and the references there in. Even the simplest clustering problems are known to be NP-Hard, including the Euclidean k-center problem in the plane <ref> [13, 24] </ref>. In fact, it is NP-Hard to approximate the two-dimensional k-center problem within a factor of &lt; 2 even under the L 1 -metric [12]. The greedy algorithm by Gonzalez [14] gives a 2-approximation algorithm for the k-center problem in any metric space.
Reference: [14] <author> T. Gonzalez, </author> <title> Clustering to minimize the maximum intercluster distance, </title> <type> Theoret. </type> <institution> Comput. Sci., </institution> <month> 38 </month> <year> (1985), </year> <pages> 293-306. </pages>
Reference-contexts: Even the simplest clustering problems are known to be NP-Hard, including the Euclidean k-center problem in the plane [13, 24]. In fact, it is NP-Hard to approximate the two-dimensional k-center problem within a factor of &lt; 2 even under the L 1 -metric [12]. The greedy algorithm by Gonzalez <ref> [14] </ref> gives a 2-approximation algorithm for the k-center problem in any metric space. This algorithm requires O (kn) distance computations. The running time was improved by Feder and Greene [12] to O (n log k) for any L p -metric.
Reference: [15] <author> T. Gonzalez, </author> <title> Covering a set of points in multidimensional space, </title> <journal> Inform. Process. Lett., </journal> <volume> 40 (1991), </volume> <pages> 181-188. </pages>
Reference-contexts: Several efficient algorithms have been developed for Euclidean and rectilinear k-center problems when k is small; see the survey by Agarwal and Sharir [1] for a summary of such results. Gonzalez <ref> [15] </ref> presented an n O (l) -time algorithm for the k-center problem in R d under the L 1 metric, when the points lie in a horizontal strip of height l; it can be extended to the L 2 -metric as well. <p> Using this observation and the algorithm by Gonzalez <ref> [15] </ref>, we develop a dynamic-programming based algorithm for finding an optimal solution. Our approach also yields an n O (k 11=d ) -time algorithm for the discrete k-center problem in R d . <p> We denote by the set of all anchored unit squares. Following a similar argument to the one Clustering Algorithms July 8, 1997 The Exact Algorithm 4 in <ref> [15] </ref>, we can prove the following lemma. Lemma 2.3 The optimal cover of S is an anchored cover. <p> We show that the lines can be chosen out of a finite set of lines, and we use dynamic programming. To compute the optimal cover on a "thin" strip, we use a slightly modified version of the algorithm by Gonzalez <ref> [15] </ref>, which we refer to as the Strip Algorithm. We present the main ideas of this algorithm below. Strip Algorithm: Let S be a set of n points lying in a horizontal strip of height l. <p> Condition (iii) and Lemma 2.4 imply u = O (n 4l2 ). Whenever the sweep line passes through a point of S, the algorithm updates the set F so that conditions (i)-(iii) are satisfied. See the original paper <ref> [15] </ref> for details. We can modify the algorithm so that it returns the optimal cover of S in the sense of our definition.
Reference: [16] <author> R. Z. Hwang, R. C. Chang, and R. C. T. Lee, </author> <title> The generalized searching over separators strategy to solve some NP-Hard problems in subexponential time, </title> <journal> Algorithmica, </journal> <volume> 9 (1993), </volume> <pages> 398-423. </pages>
Reference-contexts: By doing a binary search, they compute an optimal k-clustering. This approach can be extended to the L 1 -metric as well [29]. In another paper, Hwang et al. <ref> [16] </ref> gave a n O ( p k) time algorithm that also works for the Euclidean discrete k-center problem. Not much is known about the capacitated k-center problem. Bar-Ilan et al. [4] gave the first polynomial time approximation algorithm for the capacitated k-center problem, achieving an approximation factor of 10.
Reference: [17] <author> R. Z. Hwang, R. C. T. Lee, and R. C. Chang, </author> <title> The slab dividing approach to solve the Euclidean p-center problem, </title> <journal> Algorithmica, </journal> <volume> 9 (1993), </volume> <pages> 1-22. </pages> <note> Clustering Algorithms July 8, 1997 References 12 </note>
Reference-contexts: Gonzalez [15] presented an n O (l) -time algorithm for the k-center problem in R d under the L 1 metric, when the points lie in a horizontal strip of height l; it can be extended to the L 2 -metric as well. Hwang et al. <ref> [17] </ref> gave an n O ( p k) -time algorithm for the Euclidean k-center problem in the plane. <p> Using this observation and the algorithm by Gonzalez [15], we develop a dynamic-programming based algorithm for finding an optimal solution. Our approach also yields an n O (k 11=d ) -time algorithm for the discrete k-center problem in R d . Recall that the algorithm by Hwang et al. <ref> [17] </ref> also runs in time n O ( p k) for d = 2. Our algorithm is however not only considerably simpler than theirs, but the constant hidden in the big-Oh notation is smaller. Moreover, our algorithm is straightforward to extend to higher dimensions.
Reference: [18] <author> A. K. Jain and R. C. Dubes, </author> <title> Algorithms for Clustering Data, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1988. </year>
Reference-contexts: The capacitated k-center problem is to compute a k-clustering of the smallest size so that each cluster has at most L points. Previous results. There is a vast literature on clustering problems, see, for example, the books <ref> [3, 11, 18] </ref>, the survey paper [1], and the references there in. Even the simplest clustering problems are known to be NP-Hard, including the Euclidean k-center problem in the plane [13, 24].
Reference: [19] <author> J. Jolion, P. Meer, and S. Batauche, </author> <title> Robust clustering with applications in computer vision, </title> <journal> IEEE Trans. Pattern Analysis Mach. Intell., </journal> <volume> 13 (1991), </volume> <pages> 791-802. </pages>
Reference-contexts: 1 Introduction Clustering a set of points into a few groups is frequently used for statistical analysis and classification in numerous applications, including information retrieval [6, 7, 8, 26], facility location [11, 30], data mining [2, 28], spatial data bases [9, 20, 25], data compression [23], image processing <ref> [19, 27] </ref>, astrophysics [22], and scientific computing [5]. Because of such a diversity of applications, several variants of clustering problems have been proposed and widely studied. However, they all require a partition of a given set of points into clusters that optimizes a given objective function.
Reference: [20] <author> L. Kaufman and P. J. Rousseeuw, </author> <title> Finding Groups in Data: An Introduction to Cluster Analysis, </title> <publisher> John Wiley & Sons, </publisher> <year> 1990. </year>
Reference-contexts: 1 Introduction Clustering a set of points into a few groups is frequently used for statistical analysis and classification in numerous applications, including information retrieval [6, 7, 8, 26], facility location [11, 30], data mining [2, 28], spatial data bases <ref> [9, 20, 25] </ref>, data compression [23], image processing [19, 27], astrophysics [22], and scientific computing [5]. Because of such a diversity of applications, several variants of clustering problems have been proposed and widely studied.
Reference: [21] <author> S. Khuller and Y. J. Sussmann, </author> <title> The capacitated k-center problem, </title> <booktitle> Proceedings of the 4th Annual European Symposium on Algorithms, </booktitle> <year> 1996, </year> <pages> pp. 152-166. </pages>
Reference-contexts: They also proposed an approximation algorithm for the capacitated problem with fixed cluster size. The algorithm approximates the minimum number of clusters by a factor of dln ne. Khuller and Sussmann <ref> [21] </ref> improved the approximation factor for capaci-tated k-center to 6 (5 for a slightly different version of the problem). Recently Shmoys et al. [30] presented approximation algorithms for some generalizations of the capacitated k-center problem, using relaxation techniques. Our results. <p> Khuller and Sussmann <ref> [21] </ref> gave a polynomial algorithm for computing a ((2=c)k; cL; 2 fl ) cover, for any c &gt; 1.
Reference: [22] <author> R. Lupton, F. M. Maley, and N. Young, </author> <title> Data collection for the sloan digital sky survey: A network-flow heuristic, </title> <booktitle> Proceedings of the Seventh Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <address> ACM/SIAM, </address> <month> January </month> <year> 1996, </year> <pages> pp. 296-303. </pages>
Reference-contexts: Introduction Clustering a set of points into a few groups is frequently used for statistical analysis and classification in numerous applications, including information retrieval [6, 7, 8, 26], facility location [11, 30], data mining [2, 28], spatial data bases [9, 20, 25], data compression [23], image processing [19, 27], astrophysics <ref> [22] </ref>, and scientific computing [5]. Because of such a diversity of applications, several variants of clustering problems have been proposed and widely studied. However, they all require a partition of a given set of points into clusters that optimizes a given objective function. <p> If the centers of the clusters are required to be a subset of the input points, the problem is called the discrete k-center problem. In some applications (e.g. facility location [11, 30], astrophysics <ref> [22] </ref>), not only the size of a cluster is important but also the number of points in the cluster. We can generalize the notion of k-clustering as follows.
Reference: [23] <author> J. Makhoul, S. Roucos, and H. Gish, </author> <title> Vector quantization in speech coding, </title> <booktitle> Proceedings of the IEEE, 73 (1985), </booktitle> <pages> 1551-1588. </pages>
Reference-contexts: 1 Introduction Clustering a set of points into a few groups is frequently used for statistical analysis and classification in numerous applications, including information retrieval [6, 7, 8, 26], facility location [11, 30], data mining [2, 28], spatial data bases [9, 20, 25], data compression <ref> [23] </ref>, image processing [19, 27], astrophysics [22], and scientific computing [5]. Because of such a diversity of applications, several variants of clustering problems have been proposed and widely studied. However, they all require a partition of a given set of points into clusters that optimizes a given objective function.
Reference: [24] <author> N. Megiddo and K. J. Supowit, </author> <title> On the complexity of some common geometric location problems, </title> <journal> SIAM J. Comput., </journal> <volume> 13 (1984), </volume> <pages> 182-196. </pages>
Reference-contexts: Previous results. There is a vast literature on clustering problems, see, for example, the books [3, 11, 18], the survey paper [1], and the references there in. Even the simplest clustering problems are known to be NP-Hard, including the Euclidean k-center problem in the plane <ref> [13, 24] </ref>. In fact, it is NP-Hard to approximate the two-dimensional k-center problem within a factor of &lt; 2 even under the L 1 -metric [12]. The greedy algorithm by Gonzalez [14] gives a 2-approximation algorithm for the k-center problem in any metric space.
Reference: [25] <author> R. T. Ng and J. Han, </author> <title> Efficient and Effective Clustering Methods for Spatial Data Mining, </title> <booktitle> Proceedings of the Twentieth International Conference on Very Large Databases, </booktitle> <year> 1994, </year> <pages> pp. 144-155. </pages>
Reference-contexts: 1 Introduction Clustering a set of points into a few groups is frequently used for statistical analysis and classification in numerous applications, including information retrieval [6, 7, 8, 26], facility location [11, 30], data mining [2, 28], spatial data bases <ref> [9, 20, 25] </ref>, data compression [23], image processing [19, 27], astrophysics [22], and scientific computing [5]. Because of such a diversity of applications, several variants of clustering problems have been proposed and widely studied.
Reference: [26] <author> P. Raghavan, </author> <title> Information retrieval algorithms: A survey, </title> <booktitle> Proceedings of the Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <month> January </month> <year> 1997, </year> <pages> pp. 11-18. </pages>
Reference-contexts: 1 Introduction Clustering a set of points into a few groups is frequently used for statistical analysis and classification in numerous applications, including information retrieval <ref> [6, 7, 8, 26] </ref>, facility location [11, 30], data mining [2, 28], spatial data bases [9, 20, 25], data compression [23], image processing [19, 27], astrophysics [22], and scientific computing [5]. Because of such a diversity of applications, several variants of clustering problems have been proposed and widely studied.
Reference: [27] <author> P. Schroeter and J. Bigun, </author> <title> Hierarchical image segmentation by multi-dimensional clustering and orientation-adaptive boundary refinement, </title> <journal> Pattern Recognition., </journal> <volume> 28 (1995), </volume> <pages> 695-709. </pages>
Reference-contexts: 1 Introduction Clustering a set of points into a few groups is frequently used for statistical analysis and classification in numerous applications, including information retrieval [6, 7, 8, 26], facility location [11, 30], data mining [2, 28], spatial data bases [9, 20, 25], data compression [23], image processing <ref> [19, 27] </ref>, astrophysics [22], and scientific computing [5]. Because of such a diversity of applications, several variants of clustering problems have been proposed and widely studied. However, they all require a partition of a given set of points into clusters that optimizes a given objective function.
Reference: [28] <author> J. Shafer, R. Agrawal, and M. Mehta, Sprint: </author> <title> A scalable parallel classifier for data mining., </title> <booktitle> Proceedings of the International Conference on Very Large Databases, </booktitle> <publisher> Morgan Kauffman, </publisher> <year> 1996. </year>
Reference-contexts: 1 Introduction Clustering a set of points into a few groups is frequently used for statistical analysis and classification in numerous applications, including information retrieval [6, 7, 8, 26], facility location [11, 30], data mining <ref> [2, 28] </ref>, spatial data bases [9, 20, 25], data compression [23], image processing [19, 27], astrophysics [22], and scientific computing [5]. Because of such a diversity of applications, several variants of clustering problems have been proposed and widely studied.
Reference: [29] <author> M. Sharir, </author> <title> Private communication, </title> <year> (1997). </year>
Reference-contexts: By doing a binary search, they compute an optimal k-clustering. This approach can be extended to the L 1 -metric as well <ref> [29] </ref>. In another paper, Hwang et al. [16] gave a n O ( p k) time algorithm that also works for the Euclidean discrete k-center problem. Not much is known about the capacitated k-center problem.
Reference: [30] <author> D. Shmoys, E. Tardos, and K. Aardal, </author> <title> Approximation algorithms for facility location problems, </title> <booktitle> Proceedings of the 29th Annual ACM Symposium on Theory of Computing, </booktitle> <year> 1997, </year> <pages> pp. 265-274. </pages>
Reference-contexts: 1 Introduction Clustering a set of points into a few groups is frequently used for statistical analysis and classification in numerous applications, including information retrieval [6, 7, 8, 26], facility location <ref> [11, 30] </ref>, data mining [2, 28], spatial data bases [9, 20, 25], data compression [23], image processing [19, 27], astrophysics [22], and scientific computing [5]. Because of such a diversity of applications, several variants of clustering problems have been proposed and widely studied. <p> If the centers of the clusters are required to be a subset of the input points, the problem is called the discrete k-center problem. In some applications (e.g. facility location <ref> [11, 30] </ref>, astrophysics [22]), not only the size of a cluster is important but also the number of points in the cluster. We can generalize the notion of k-clustering as follows. <p> The algorithm approximates the minimum number of clusters by a factor of dln ne. Khuller and Sussmann [21] improved the approximation factor for capaci-tated k-center to 6 (5 for a slightly different version of the problem). Recently Shmoys et al. <ref> [30] </ref> presented approximation algorithms for some generalizations of the capacitated k-center problem, using relaxation techniques. Our results. One of the main results of this paper is an n O (k 11=d ) -time algorithm for the k-center problem in R d , under any L p -metric (Section 2).
References-found: 30

