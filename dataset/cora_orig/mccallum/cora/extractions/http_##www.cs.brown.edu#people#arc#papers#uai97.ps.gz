URL: http://www.cs.brown.edu/people/arc/papers/uai97.ps.gz
Refering-URL: http://www.cs.brown.edu/people/arc/papers/papers.html
Root-URL: 
Email: arc@cs.brown.edu  mlittman@cs.duke.edu  lzhang@cs.ust.hk  
Title: Incremental Pruning: A Simple, Fast, Exact Algorithm for Partially Observable Markov Decision Processes  
Author: Anthony Cassandra, Michael L. Littman, and Nevin L. Zhang 
Date: February 22, 1997  
Abstract: Most exact algorithms for general pomdps use a form of dynamic programming in which a piecewise-linear and convex representation of one value function is transformed into another. We examine variations of the "incremental pruning" approach for solving this problem and compare them to earlier algorithms from theoretical and empirical perspectives. We find that incremental pruning is presently the most efficient algorithm for solving pomdps.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Craig Boutilier and David Poole. </author> <title> Computing optimal policies for partially observable decision processes using compact representations. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 1168-1175. </pages> <publisher> AAAI Press/The MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: After taking action a 2 A from state s 2 S, the agent receives immediate reward r (s; a) 2 R and the agent's state becomes some state s 0 with the probability given by the transition function Pr (s 0 js; a) 2 <ref> [0; 1] </ref>. The agent is not aware of its current state, and instead only knows its information state x, which is a probability distribution over possible states (x (s) is the probability that the agent is in state s). <p> After each transition, the agent makes an observation z of its current state from a finite set of possible observations Z. The function Pr (zjs 0 ; a) 2 <ref> [0; 1] </ref> gives the probability that observation z will be made after the agent takes action a and makes a transition to state s 0 . <p> Most exact algorithms for general pomdps use a form of dynamic programming in which a piecewise-linear and convex representation of one value function is transformed into another. This includes algorithms that solve pomdps via value iteration [13, 2], policy iteration [16], accelerated value iteration [18], structured representations <ref> [1] </ref>, and approximation [20]. Because dynamic-programming updates are critical to such a wide array of pomdp algorithms, identifying fast algorithms is crucial. Several algorithms for dynamic-programming updates have been proposed, such as one pass [15], exhaustive [9], linear support [3], and witness [7].
Reference: [2] <author> Anthony R. Cassandra, Leslie Pack Kaelbling, and Michael L. Littman. </author> <title> Acting optimally in partially observable stochastic domains. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pages 1023-1028, </pages> <address> Seattle, WA, </address> <year> 1994. </year>
Reference-contexts: 1 Introduction Partially observable Markov decision processes (pomdps) model problems in which an agent must make a sequence of decisions to maximize its utility given uncertainty in the effects of its actions and its current state <ref> [2, 19] </ref>. At any moment in time, the agent is in one of a finite set of possible states S and must choose one of a finite set of possible actions A. <p> Most exact algorithms for general pomdps use a form of dynamic programming in which a piecewise-linear and convex representation of one value function is transformed into another. This includes algorithms that solve pomdps via value iteration <ref> [13, 2] </ref>, policy iteration [16], accelerated value iteration [18], structured representations [1], and approximation [20]. Because dynamic-programming updates are critical to such a wide array of pomdp algorithms, identifying fast algorithms is crucial. <p> Obs. Stages jV f j Ref. 1D maze 4 2 2 70 4 4x3 11 4 6 8 436 [10] 4x4 16 4 2 374 20 <ref> [2] </ref> Cheese 11 4 7 373 14 [8] Part painting 4 4 2 371 9 [6] Network 7 4 2 14 438 Shuttle 8 3 5 7 481 [4] Aircraft ID 12 6 5 4 258 Table 1: Test problem parameter sizes.

Reference: [4] <author> Lonnie Chrisman. </author> <title> Reinforcement learning with perceptual aliasing: The perceptual distinctions approach. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 183-188, </pages> <address> San Jose, California, 1992. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: j Ref. 1D maze 4 2 2 70 4 4x3 11 4 6 8 436 [10] 4x4 16 4 2 374 20 [2] Cheese 11 4 7 373 14 [8] Part painting 4 4 2 371 9 [6] Network 7 4 2 14 438 Shuttle 8 3 5 7 481 <ref> [4] </ref> Aircraft ID 12 6 5 4 258 Table 1: Test problem parameter sizes. In principle, it is also possible to choose a D set that is the smallest set satisfying conditions 1 and 2.
Reference: [5] <author> Eric A. Hansen. </author> <title> Cost-effective sensing during plan execution. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pages 1029-1035. </pages> <publisher> AAAI Press/The MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: There 1 are many ways to approach this problem based on checking which information states can be reached <ref> [17, 5] </ref>, searching for good controllers [11], and using dynamic programming [14, 3, 9, 7]. Most exact algorithms for general pomdps use a form of dynamic programming in which a piecewise-linear and convex representation of one value function is transformed into another.
Reference: [6] <author> Nicholas Kushmerick, Steve Hanks, and Daniel S. Weld. </author> <title> An algorithm for probabilistic planning. </title> <journal> Artificial Intelligence, </journal> <volume> 76(1-2):239-286, </volume> <month> September </month> <year> 1995. </year>
Reference-contexts: Obs. Stages jV f j Ref. 1D maze 4 2 2 70 4 4x3 11 4 6 8 436 [10] 4x4 16 4 2 374 20 [2] Cheese 11 4 7 373 14 [8] Part painting 4 4 2 371 9 <ref> [6] </ref> Network 7 4 2 14 438 Shuttle 8 3 5 7 481 [4] Aircraft ID 12 6 5 4 258 Table 1: Test problem parameter sizes. In principle, it is also possible to choose a D set that is the smallest set satisfying conditions 1 and 2.
Reference: [7] <author> Michael L. Littman, Anthony R. Cassandra, and Leslie Pack Kaelbling. </author> <title> Efficient dynamic-programming updates in partially observable Markov decision processes. </title> <type> Technical Report CS-95-19, </type> <institution> Brown University, Providence, RI, </institution> <year> 1996. </year>
Reference-contexts: There 1 are many ways to approach this problem based on checking which information states can be reached [17, 5], searching for good controllers [11], and using dynamic programming <ref> [14, 3, 9, 7] </ref>. Most exact algorithms for general pomdps use a form of dynamic programming in which a piecewise-linear and convex representation of one value function is transformed into another. <p> Because dynamic-programming updates are critical to such a wide array of pomdp algorithms, identifying fast algorithms is crucial. Several algorithms for dynamic-programming updates have been proposed, such as one pass [15], exhaustive [9], linear support [3], and witness <ref> [7] </ref>. Cheng [3] gave experimental evidence that the linear support algorithm is more efficient than the one-pass algorithm. Littman, Cassandra and Kaelbling [7] compared the exhaustive algorithm, the linear support algorithm, and the witness algorithm and found that, except for tiny problems with approximately 2 observations or 2 states, witness was <p> Several algorithms for dynamic-programming updates have been proposed, such as one pass [15], exhaustive [9], linear support [3], and witness <ref> [7] </ref>. Cheng [3] gave experimental evidence that the linear support algorithm is more efficient than the one-pass algorithm. Littman, Cassandra and Kaelbling [7] compared the exhaustive algorithm, the linear support algorithm, and the witness algorithm and found that, except for tiny problems with approximately 2 observations or 2 states, witness was the fastest. Recently, Zhang and Liu [20] proposed a new algorithm for dynamic-programming updates in pomdps called incremental pruning. <p> Each V a and V a z function is a value function mapping information states to value and is defined in terms of relatively simple transformations of other value functions. The transformations preserve piecewise linearity and convexity <ref> [14, 7] </ref>. <p> These sets have a unique representation of minimum size <ref> [7] </ref>, and we assume that the symbols S a z , S a , and S 0 refer to the minimum-size sets. Here is a brief description of the set and vector notation we will be using. <p> Using the definition of R, we can define purge (A) = fffjff 2 A; R (ff; A) 6= ;g: The set purge (A) is the set of vectors in A that have non-empty witness regions; it is precisely the minimum-size set for representing the piecewise-linear convex function given by A <ref> [7] </ref> 1 . or "pruning" out the unnecessary vectors. The algorithm is due to Lark [19]; Littman et al. [7] analyze the algorithm and describe the way that the argmax operators need to be implemented for the analysis to hold (ties must be broken lexicographically). <p> The set purge (A) is the set of vectors in A that have non-empty witness regions; it is precisely the minimum-size set for representing the piecewise-linear convex function given by A <ref> [7] </ref> 1 . or "pruning" out the unnecessary vectors. The algorithm is due to Lark [19]; Littman et al. [7] analyze the algorithm and describe the way that the argmax operators need to be implemented for the analysis to hold (ties must be broken lexicographically). <p> It is implemented by solving a simple linear program <ref> [7] </ref>, illustrated in Figure 2. The Filter algorithm forms the basis of all the incremental pruning algorithms we examine, so it deserves some additional explanation. <p> For the remainder of this paper, we assume that jW j &gt; 1, since the case of jW j = 1 is trivial. The witness algorithm has been analyzed previously <ref> [7] </ref>, and we repeat the basic results here for easy comparison. The total number of linear programs solved by witness is ( P z j jZj)jS a j + jS a j 1; asymptotically, this is fi (jS a j P z j).
Reference: [8] <author> R. Andrew McCallum. </author> <title> Overcoming incomplete perception with utile distinction memory. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pages 190-196, </pages> <address> Amherst, Massachusetts, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Obs. Stages jV f j Ref. 1D maze 4 2 2 70 4 4x3 11 4 6 8 436 [10] 4x4 16 4 2 374 20 [2] Cheese 11 4 7 373 14 <ref> [8] </ref> Part painting 4 4 2 371 9 [6] Network 7 4 2 14 438 Shuttle 8 3 5 7 481 [4] Aircraft ID 12 6 5 4 258 Table 1: Test problem parameter sizes.
Reference: [9] <author> George E. Monahan. </author> <title> A survey of partially observable Markov decision processes: Theory, models, and algorithms. </title> <journal> Management Science, </journal> <volume> 28(1) </volume> <pages> 1-16, </pages> <month> January </month> <year> 1982. </year>
Reference-contexts: There 1 are many ways to approach this problem based on checking which information states can be reached [17, 5], searching for good controllers [11], and using dynamic programming <ref> [14, 3, 9, 7] </ref>. Most exact algorithms for general pomdps use a form of dynamic programming in which a piecewise-linear and convex representation of one value function is transformed into another. <p> Because dynamic-programming updates are critical to such a wide array of pomdp algorithms, identifying fast algorithms is crucial. Several algorithms for dynamic-programming updates have been proposed, such as one pass [15], exhaustive <ref> [9] </ref>, linear support [3], and witness [7]. Cheng [3] gave experimental evidence that the linear support algorithm is more efficient than the one-pass algorithm. <p> This can be a large number of vectors even when the S a sets are relatively small. This approach to computing the S a sets from the S a z sets was essentially proposed by Monahan (under the name of "Sondik's algorithm") <ref> [9] </ref>. 3.2 Complexity Analysis We seek to express the running time of algorithms in terms of the number of linear programs they solve and the size of these linear programs. <p> Hence x! &gt; x! 0 . The lemma follows. Different choices of D result in different incremental pruning algorithms. In general, the smaller the D set, the more efficient the incremental pruning algorithm. Equation 12 is equivalent to using Monahan's filtering algorithm <ref> [9] </ref> in IncPrune, Equation 14 is equivalent to using Lark's filtering algorithm [19] in IncPrune. The analysis in Section 4 assumes that incremental pruning is implemented using D defined by Equation 14. We refer to variations of incremental pruning using Equations 15 and 16 as "restricted region" variations.
Reference: [10] <author> Ronald Parr and Stuart Russell. </author> <title> Approximating optimal policies for partially observable stochastic domains. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <year> 1995. </year>
Reference-contexts: Obs. Stages jV f j Ref. 1D maze 4 2 2 70 4 4x3 11 4 6 8 436 <ref> [10] </ref> 4x4 16 4 2 374 20 [2] Cheese 11 4 7 373 14 [8] Part painting 4 4 2 371 9 [6] Network 7 4 2 14 438 Shuttle 8 3 5 7 481 [4] Aircraft ID 12 6 5 4 258 Table 1: Test problem parameter sizes.
Reference: [11] <author> Loren K. Platzman. </author> <title> A feasible computational approach to infinite-horizon partially-observed Markov decision problems. </title> <type> Technical report, </type> <institution> Georgia Institute of Technology, </institution> <address> Atlanta, GA, </address> <month> January </month> <year> 1981. </year>
Reference-contexts: There 1 are many ways to approach this problem based on checking which information states can be reached [17, 5], searching for good controllers <ref> [11] </ref>, and using dynamic programming [14, 3, 9, 7]. Most exact algorithms for general pomdps use a form of dynamic programming in which a piecewise-linear and convex representation of one value function is transformed into another.
Reference: [12] <author> Stuart J. Russell and Peter Norvig. </author> <title> Artificial Intelligence: A Modern Approach. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1994. </year>
Reference: [13] <author> Katsushige Sawaki and Akira Ichikawa. </author> <title> Optimal control for partially observable Markov decision processes over an infinite horizon. </title> <journal> Journal of the Operations Research Society of Japan, </journal> <volume> 21(1) </volume> <pages> 1-14, </pages> <month> March </month> <year> 1978. </year>
Reference-contexts: Most exact algorithms for general pomdps use a form of dynamic programming in which a piecewise-linear and convex representation of one value function is transformed into another. This includes algorithms that solve pomdps via value iteration <ref> [13, 2] </ref>, policy iteration [16], accelerated value iteration [18], structured representations [1], and approximation [20]. Because dynamic-programming updates are critical to such a wide array of pomdp algorithms, identifying fast algorithms is crucial.
Reference: [14] <author> Richard D. Smallwood and Edward J. Sondik. </author> <title> The optimal control of partially observable Markov processes over a finite horizon. </title> <journal> Operations Research, </journal> <volume> 21 </volume> <pages> 1071-1088, </pages> <year> 1973. </year>
Reference-contexts: There 1 are many ways to approach this problem based on checking which information states can be reached [17, 5], searching for good controllers [11], and using dynamic programming <ref> [14, 3, 9, 7] </ref>. Most exact algorithms for general pomdps use a form of dynamic programming in which a piecewise-linear and convex representation of one value function is transformed into another. <p> Each V a and V a z function is a value function mapping information states to value and is defined in terms of relatively simple transformations of other value functions. The transformations preserve piecewise linearity and convexity <ref> [14, 7] </ref>.
Reference: [15] <author> Edward Sondik. </author> <title> The Optimal Control of Partially Observable Markov Processes. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1971. </year>
Reference-contexts: Because dynamic-programming updates are critical to such a wide array of pomdp algorithms, identifying fast algorithms is crucial. Several algorithms for dynamic-programming updates have been proposed, such as one pass <ref> [15] </ref>, exhaustive [9], linear support [3], and witness [7]. Cheng [3] gave experimental evidence that the linear support algorithm is more efficient than the one-pass algorithm.
Reference: [16] <author> Edward J. Sondik. </author> <title> The optimal control of partially observable Markov processes over the infinite horizon: Discounted costs. </title> <journal> Operations Research, </journal> <volume> 26(2) </volume> <pages> 282-304, </pages> <year> 1978. </year>
Reference-contexts: Most exact algorithms for general pomdps use a form of dynamic programming in which a piecewise-linear and convex representation of one value function is transformed into another. This includes algorithms that solve pomdps via value iteration [13, 2], policy iteration <ref> [16] </ref>, accelerated value iteration [18], structured representations [1], and approximation [20]. Because dynamic-programming updates are critical to such a wide array of pomdp algorithms, identifying fast algorithms is crucial. Several algorithms for dynamic-programming updates have been proposed, such as one pass [15], exhaustive [9], linear support [3], and witness [7].
Reference: [17] <author> R. </author> <title> Washington. Incremental Markov-model planning. </title> <booktitle> In Proceedings of TAI-96, Eighth IEEE International Conference on Tools With Artificial Intelligence, </booktitle> <pages> pages 41-47, </pages> <year> 1996. </year> <month> 14 </month>
Reference-contexts: There 1 are many ways to approach this problem based on checking which information states can be reached <ref> [17, 5] </ref>, searching for good controllers [11], and using dynamic programming [14, 3, 9, 7]. Most exact algorithms for general pomdps use a form of dynamic programming in which a piecewise-linear and convex representation of one value function is transformed into another.
Reference: [18] <author> C. C. White and W. T. Scherer. </author> <title> Accelerated successive approximation algorithms for partially observed Markov decision processes. </title> <year> 1986. </year>
Reference-contexts: Most exact algorithms for general pomdps use a form of dynamic programming in which a piecewise-linear and convex representation of one value function is transformed into another. This includes algorithms that solve pomdps via value iteration [13, 2], policy iteration [16], accelerated value iteration <ref> [18] </ref>, structured representations [1], and approximation [20]. Because dynamic-programming updates are critical to such a wide array of pomdp algorithms, identifying fast algorithms is crucial. Several algorithms for dynamic-programming updates have been proposed, such as one pass [15], exhaustive [9], linear support [3], and witness [7].
Reference: [19] <author> Chelsea C. White, III. </author> <title> Partially observed Markov decision processes: A survey. </title> <journal> Annals of Operations Research, </journal> <volume> 32, </volume> <year> 1991. </year>
Reference-contexts: 1 Introduction Partially observable Markov decision processes (pomdps) model problems in which an agent must make a sequence of decisions to maximize its utility given uncertainty in the effects of its actions and its current state <ref> [2, 19] </ref>. At any moment in time, the agent is in one of a finite set of possible states S and must choose one of a finite set of possible actions A. <p> Equations 6 and 8 can be implemented efficiently using an efficient implementation of the purge function, described in the next section. 3 Purging Sets of Vectors This section describes and analyzes the purge function due to Lark <ref> [19] </ref>. <p> The algorithm is due to Lark <ref> [19] </ref>; Littman et al. [7] analyze the algorithm and describe the way that the argmax operators need to be implemented for the analysis to hold (ties must be broken lexicographically). <p> The lemma follows. Different choices of D result in different incremental pruning algorithms. In general, the smaller the D set, the more efficient the incremental pruning algorithm. Equation 12 is equivalent to using Monahan's filtering algorithm [9] in IncPrune, Equation 14 is equivalent to using Lark's filtering algorithm <ref> [19] </ref> in IncPrune. The analysis in Section 4 assumes that incremental pruning is implemented using D defined by Equation 14. We refer to variations of incremental pruning using Equations 15 and 16 as "restricted region" variations.
Reference: [20] <author> Nevin L. Zhang and Wenju Liu. </author> <title> Planning in stochastic domains: Problem characteristics and approximation. </title> <type> Technical Report HKUST-CS96-31, </type> <institution> Department of Computer Science, Hong Kong University of Science and Technology, </institution> <year> 1996. </year> <month> 15 </month>
Reference-contexts: This includes algorithms that solve pomdps via value iteration [13, 2], policy iteration [16], accelerated value iteration [18], structured representations [1], and approximation <ref> [20] </ref>. Because dynamic-programming updates are critical to such a wide array of pomdp algorithms, identifying fast algorithms is crucial. Several algorithms for dynamic-programming updates have been proposed, such as one pass [15], exhaustive [9], linear support [3], and witness [7]. <p> Littman, Cassandra and Kaelbling [7] compared the exhaustive algorithm, the linear support algorithm, and the witness algorithm and found that, except for tiny problems with approximately 2 observations or 2 states, witness was the fastest. Recently, Zhang and Liu <ref> [20] </ref> proposed a new algorithm for dynamic-programming updates in pomdps called incremental pruning. In this paper, we analyze the algorithm and a novel variation and compare them to the witness algorithm. <p> The total number of constraints over all the linear programs is fi (jS a j 2 P z jS a z j) asymptotically 2 . 4 Incremental Pruning This section describes the incremental pruning algorithm <ref> [20] </ref>, which computes S a efficiently from the S a z sets. Recall the definition for S a in Equation 7: S a = purge z2Z z = purge (S a z 1 S a z k ); here, k = jZj.
References-found: 19

