URL: http://www.cs.ucla.edu/~inki/iccad97.ps
Refering-URL: http://www.cs.ucla.edu/~inki/publications.html
Root-URL: http://www.cs.ucla.edu
Title: Power Optimization using Divide-and-Conquer Techniques for Minimization of the Number of Operations  
Author: Inki Hong, Miodrag Potkonjak and Ramesh Karriy 
Address: Los Angeles, CA 90095  Amherst, MA 01003  
Affiliation: Computer Science Department, University of California,  yDept. of Electrical and Computer Engineering, University of Massachusetts,  
Abstract: We develop an approach to minimizing power consumption of portable wireless DSP applications using a set of compilation and architectural techniques. The key technical innovation is a novel divide-and-conquer compilation technique to minimize the number of operations for general DSP computations. Our technique optimizes not only a significantly wider set of computations than the previously published techniques, but also outperforms (or performs at least as well as other techniques) on all examples. Along the architectural dimension, we investigate coordinated impact of compilation techniques on the number of processors which provide optimal trade-off between cost and power. We demonstrate that proper compilation techniques can significantly reduce power with bounded hardware cost. The effectiveness of all techniques and algorithms is documented on numerous real-life designs. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A.P. Chandrakasan, S. Sheng, and R.W. Broderson. </author> <title> Low-power CMOS digital design. </title> <journal> IEEE J. of Solid-State Circuits, </journal> <volume> 27(4):473484, </volume> <year> 1992. </year>
Reference-contexts: Secondly, the power consumption in programmable processors is directly proportional to the number of operations, regardless of what the mix of operations being executed is [13]. Finally, our model follows the power consumption and timing models in digital CMOS circuits presented in <ref> [1] </ref>. Based on these three facts, we conclude that if the targeted implementa-tion platform is a single CMOS processor, reduction in the number of operations is the key to power minimization. 3 Related Work Power minimization efforts across all levels of design abstraction process are surveyed in [10].
Reference: [2] <author> P.D. Hoang and J.M. Rabaey. </author> <title> Scheduling of DSP programs onto multiprocessors for maximum throughput. </title> <journal> IEEE Trans. on Signal Processing, </journal> <volume> 41(6):2225 2235, </volume> <year> 1993. </year>
Reference-contexts: The power model used in this research is built on three established facts. First, the number of operations at the machine code-level is proportional to the number of operations at high-level language <ref> [2] </ref>. Secondly, the power consumption in programmable processors is directly proportional to the number of operations, regardless of what the mix of operations being executed is [13]. Finally, our model follows the power consumption and timing models in digital CMOS circuits presented in [1].
Reference: [3] <author> I. Hong, M. Potkonjak, and R. Karri. </author> <title> Power optimization using divide-and-conquer techniques for minimization of the number of operations. </title> <type> Technical report, </type> <institution> Computer Science Department, UCLA, </institution> <year> 1997. </year>
Reference-contexts: Tables 2 and 3 present the results of our technique for minimizing power on single processor for various technologies. Our method results in power reduction by an average factor of 3.43. Due to space limitation, the experimental results for multi-processors were omitted. For the results, we refer to <ref> [3] </ref>. Our method achieves cost-effective solutions with very low power penalty compared to the solutions which only optimize power without considering hardware cost.
Reference: [4] <author> E.A. Lee and D.G. Messerschmitt. </author> <title> Synchronous dataflow. </title> <booktitle> Proc. of the IEEE, </booktitle> <address> 75(9):12351245, </address> <year> 1987. </year>
Reference-contexts: The second technical highlight is the quantitative analysis of cost vs power trade-off on multiple programmable processors. We derive a condition under which the optimization of the cost-power product using parallelization is beneficial. 2 Preliminaries We selected as a computational model synchronous data flow (SDF) <ref> [4] </ref>. The syntax of a targeted computation is defined as a hierarchical control-data flow graph (CDFG) [8]. The only relevant speed metric is throughput. We assume that all types of operations take one clock cycle for their execution, as it is the case in many modern DSP processors.
Reference: [5] <author> C.E. Leiserson and J.B. Saxe. </author> <title> Retiming synchronous circuitry. </title> <journal> Algorithmica, </journal> <volume> 6(1):535, </volume> <year> 1991. </year>
Reference-contexts: Thus, any adjacent trivial SCCs are merged together before the isolation step, to reduce the number of pipeline delays used. The number of delays in each sub part is minimized using retiming by the Leiserson-Saxe algorithm <ref> [5] </ref>. Note that smaller number of delays will require smaller number of operations since both the next states and outputs depend on the previous states. SCCs are further classified as either linear or nonlinear. Minimization of the number of operations for linear computations is NP-complete [9].
Reference: [6] <author> K.K. Parhi and D.G. Messerschmitt. </author> <title> Static rate-optimal scheduling of iterative data-flow programs via optimum unfolding. </title> <journal> IEEE Trans. on Computers, </journal> <volume> 40(2):178195, </volume> <year> 1991. </year>
Reference-contexts: Parhi and Messer-schmitt <ref> [6] </ref> presented optimal unfolding of linear DSP computations. Potkonjak and Rabaey [7] addressed the minimization of the number of multiplications and additions in linear computations in their maximally fast form so that the throughput is preserved.
Reference: [7] <author> M. Potkonjak and J. Rabaey. </author> <title> Maximally fast and arbitrarily fast implementation of linear computations. </title> <booktitle> ICCAD, </booktitle> <pages> pages 304308, </pages> <year> 1992. </year>
Reference-contexts: Parhi and Messer-schmitt [6] presented optimal unfolding of linear DSP computations. Potkonjak and Rabaey <ref> [7] </ref> addressed the minimization of the number of multiplications and additions in linear computations in their maximally fast form so that the throughput is preserved. Sheliga and Sha [9] presented an approach to minimizing the number of operations in linear computations. <p> SCCs are further classified as either linear or nonlinear. Minimization of the number of operations for linear computations is NP-complete [9]. We have adopted an approach of [11] for the optimization of linear sub parts, which uses unfolding and the maximally fast procedure <ref> [7] </ref>.
Reference: [8] <author> J. Rabaey, C. Chu, P. Hoang, and M. Potkonjak. </author> <title> Fast prototyping of data path intensive architectures. </title> <booktitle> IEEE Design & Test of Computers, </booktitle> <address> 8(2):4051, </address> <year> 1991. </year>
Reference-contexts: We derive a condition under which the optimization of the cost-power product using parallelization is beneficial. 2 Preliminaries We selected as a computational model synchronous data flow (SDF) [4]. The syntax of a targeted computation is defined as a hierarchical control-data flow graph (CDFG) <ref> [8] </ref>. The only relevant speed metric is throughput. We assume that all types of operations take one clock cycle for their execution, as it is the case in many modern DSP processors.
Reference: [9] <author> M. Sheliga and E.H.-M. Sha. </author> <title> Global node reduction of linear systems using ratio analysis. </title> <booktitle> International Symposium on High-Level Synthesis, </booktitle> <pages> pages 140145, </pages> <year> 1994. </year>
Reference-contexts: Parhi and Messer-schmitt [6] presented optimal unfolding of linear DSP computations. Potkonjak and Rabaey [7] addressed the minimization of the number of multiplications and additions in linear computations in their maximally fast form so that the throughput is preserved. Sheliga and Sha <ref> [9] </ref> presented an approach to minimizing the number of operations in linear computations. Srivastava and Potkonjak [11] developed an approach to minimizing power for linear computations, based on the minimization of the number of operations. A variant of their technique is used in conquer phase of our approach. <p> Note that smaller number of delays will require smaller number of operations since both the next states and outputs depend on the previous states. SCCs are further classified as either linear or nonlinear. Minimization of the number of operations for linear computations is NP-complete <ref> [9] </ref>. We have adopted an approach of [11] for the optimization of linear sub parts, which uses unfolding and the maximally fast procedure [7]. We note that instead of maximally fast procedure, the ratio analysis by [9] can be used. [11] has provided the closed-form formula for the optimal unfolding factor <p> Minimization of the number of operations for linear computations is NP-complete <ref> [9] </ref>. We have adopted an approach of [11] for the optimization of linear sub parts, which uses unfolding and the maximally fast procedure [7]. We note that instead of maximally fast procedure, the ratio analysis by [9] can be used. [11] has provided the closed-form formula for the optimal unfolding factor with the assumption of dense linear computations, which means that every output and state are linear combinations of all inputs and states with no 0, 1, or -1 coefficients.
Reference: [10] <author> D. Singh, J. Rabaey, M. Pedram, F. Catthoor, S. Raj-gopal, N. Sehgal, and T. Mozdzen. </author> <title> Power conscious cad tools and methodologies: A perspective. </title> <journal> Proc. of the IEEE, </journal> <volume> 83(4), </volume> <year> 1995. </year>
Reference-contexts: Based on these three facts, we conclude that if the targeted implementa-tion platform is a single CMOS processor, reduction in the number of operations is the key to power minimization. 3 Related Work Power minimization efforts across all levels of design abstraction process are surveyed in <ref> [10] </ref>. Parhi and Messer-schmitt [6] presented optimal unfolding of linear DSP computations. Potkonjak and Rabaey [7] addressed the minimization of the number of multiplications and additions in linear computations in their maximally fast form so that the throughput is preserved.
Reference: [11] <author> M. Srivastava and M. Potkonjak. </author> <title> Power optimization in programmable processors and ASIC implementations of linear systems: Transformation-based approach. </title> <booktitle> Design Automation Conference, </booktitle> <pages> pages 343 348, </pages> <year> 1996. </year>
Reference-contexts: Furthermore, we study achievable power-cost tradeoffs when parallelism is traded for power reduction. The main technical innovation of the research is the first approach for minimizing the number of operations in general computations. The approach optimizes not only significantly wider set of computations than the other previously published techniques <ref> [11] </ref>, but also outperforms or performs at least as well as other techniques on all examples. To the best of our knowledge this is the first optimization-intensive approach for minimizing the number of operations in general computations. <p> Potkonjak and Rabaey [7] addressed the minimization of the number of multiplications and additions in linear computations in their maximally fast form so that the throughput is preserved. Sheliga and Sha [9] presented an approach to minimizing the number of operations in linear computations. Srivastava and Potkonjak <ref> [11] </ref> developed an approach to minimizing power for linear computations, based on the minimization of the number of operations. A variant of their technique is used in conquer phase of our approach. Our approach is different from theirs in two respects. <p> SCCs are further classified as either linear or nonlinear. Minimization of the number of operations for linear computations is NP-complete [9]. We have adopted an approach of <ref> [11] </ref> for the optimization of linear sub parts, which uses unfolding and the maximally fast procedure [7]. We note that instead of maximally fast procedure, the ratio analysis by [9] can be used. [11] has provided the closed-form formula for the optimal unfolding factor with the assumption of dense linear computations, <p> We have adopted an approach of <ref> [11] </ref> for the optimization of linear sub parts, which uses unfolding and the maximally fast procedure [7]. We note that instead of maximally fast procedure, the ratio analysis by [9] can be used. [11] has provided the closed-form formula for the optimal unfolding factor with the assumption of dense linear computations, which means that every output and state are linear combinations of all inputs and states with no 0, 1, or -1 coefficients. <p> For sparse linear computations, they have proposed a heuristic which continues to unfold until there is no improvement. We have made the simple heuristic more efficient with binary search, based on the unimodality property of the number of operations on unfolding factor <ref> [11] </ref>. When a sub part is classified as nonlinear, we apply unfolding after the isolation of nonlinear operations. All nonlinear operations are isolated from the sub part so that the remaining linear sub parts can be optimized. <p> We want to find the number of processors which minimizes power cost-effectively. Reduction % Init. New Reduction % From Design Ops <ref> [11] </ref> Method From [11] Init. <p> We want to find the number of processors which minimizes power cost-effectively. Reduction % Init. New Reduction % From Design Ops <ref> [11] </ref> Method From [11] Init. <p> The maximum value is determined based on the power requirement and the cost budget for the implementation. The strategy produces solutions with only a few processors, in many cases single processor. 6 Experimental Results Our set of benchmark designs include all the examples used in <ref> [11] </ref> as well as the following typical portable DSP applications: DAC - 4 stage NEC digital to analog converter for audio signals; modem - 2 stage NEC modem; GE controller - 5-state GE linear controller; APCM receiver - Motorola's adaptive pulse code modulation receiver; Audio Filter - ADC followed by 18 <p> DAC, modem, and GE controller are linear and the rest are nonlinear. The examples from <ref> [11] </ref> are all lin ear, which include ellip, iir5, wdf5, iir6, iir10, iir12, steam, dist, and chemical. Table 1 presents the results of our technique for minimizing the number of operations. The fifth and sixth columns of Table 1 provide the reduction percentage of our method from [11] and from the <p> The examples from <ref> [11] </ref> are all lin ear, which include ellip, iir5, wdf5, iir6, iir10, iir12, steam, dist, and chemical. Table 1 presents the results of our technique for minimizing the number of operations. The fifth and sixth columns of Table 1 provide the reduction percentage of our method from [11] and from the initial number of operations, respectively. Our method achieves the same number of operations as [11] for ellip, iir5, wdf5, iir6, iir10, iir12, and steam while it reduces the number of operations by 23 and 10.3% for dist and chemical, respectively. All the examples from [11] are small <p> Table 1 presents the results of our technique for minimizing the number of operations. The fifth and sixth columns of Table 1 provide the reduction percentage of our method from <ref> [11] </ref> and from the initial number of operations, respectively. Our method achieves the same number of operations as [11] for ellip, iir5, wdf5, iir6, iir10, iir12, and steam while it reduces the number of operations by 23 and 10.3% for dist and chemical, respectively. All the examples from [11] are small single-input single-output (SISO) linear computations, except dist and chemical which are two-inputs single-output linear computations, which results in <p> method from <ref> [11] </ref> and from the initial number of operations, respectively. Our method achieves the same number of operations as [11] for ellip, iir5, wdf5, iir6, iir10, iir12, and steam while it reduces the number of operations by 23 and 10.3% for dist and chemical, respectively. All the examples from [11] are small single-input single-output (SISO) linear computations, except dist and chemical which are two-inputs single-output linear computations, which results in no room for further improvement from [11]. Our method reduces the number of operations by an average 39.7% for the examples that previous techniques are either ineffective or inapplicable. <p> All the examples from <ref> [11] </ref> are small single-input single-output (SISO) linear computations, except dist and chemical which are two-inputs single-output linear computations, which results in no room for further improvement from [11]. Our method reduces the number of operations by an average 39.7% for the examples that previous techniques are either ineffective or inapplicable. Tables 2 and 3 present the results of our technique for minimizing power on single processor for various technologies.
Reference: [12] <author> R.E. Tarjan. </author> <title> Depth first search and linear graph algorithms. </title> <journal> SIAM J. on Computing, </journal> <volume> 1(2):146160, </volume> <year> 1972. </year>
Reference-contexts: linear) Apply optimal unfolding; Else Apply unfolding after isolating nonlinear operations; Merge linear sub parts to further optimize; Schedule merged sub parts to minimize memory usage; for general DSP computations The first step of the approach is to identify the computation's strongly connected components (SCCs), using the depth-first search-based algorithm <ref> [12] </ref>. For any pair of operations A and B within an SCC, there exist both a path from A to B and a path from B to A. The graph formed by all SCCs is acyclic. <p> To compute the gain, COST (i; j) must be computed, which requires constant coefficient matrices A; B; C; and D for the merged sub part of i and j. It is easy to construct the matrices using the depth-first search <ref> [12] </ref>. Sub part merging is performed by a greedy optimization approach. The algorithm is straightforward. Until there is no improvement, merge the pair of sub parts which produces the highest gain. 5 Multiple Programmable Processors When multi-processors are used, potentially more savings in power can be obtained.
Reference: [13] <author> V. Tiwari, S. Malik, and A. Wolfe. </author> <title> Power analysis of embedded software: a first step towards software power minimization. </title> <journal> IEEE Trans. on VLSI Systems, </journal> <volume> 2(4):437445, </volume> <year> 1994. </year>
Reference-contexts: First, the number of operations at the machine code-level is proportional to the number of operations at high-level language [2]. Secondly, the power consumption in programmable processors is directly proportional to the number of operations, regardless of what the mix of operations being executed is <ref> [13] </ref>. Finally, our model follows the power consumption and timing models in digital CMOS circuits presented in [1].
References-found: 13

