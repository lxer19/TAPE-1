URL: ftp://ftp.cs.wisc.edu/markhill/Papers/isca91_race_detection.ps
Refering-URL: http://www.cs.wisc.edu/~markhill/
Root-URL: 
Email: sarita@cs.wisc.edu  
Title: Detecting Data Races on Weak Memory Systems  
Author: Sarita V. Adve, Mark D. Hill, Barton P. Miller, Robert H.B. Netzer 
Keyword: data races, sequential consistency, weak ordering, release consistency, data-race-free-0, data-race-free-1.  
Address: Madison, Wisconsin 53706  
Affiliation: Computer Sciences Department University of Wisconsin  
Note: To Appear in the Proceedings of the 18th Annual International Symposium on Computer Architecture  Research supported in part by National Science Foundation grants CCR-8815928, CCR-8902536 and MIPS-8957278, Office of Naval Research grant N00014-89-J-1222, and external research grants from A. T. T. Bell Laboratories, Cray Research, Digital Equipment Corporation and Texas Instruments.  
Abstract: For shared-memory systems, the most commonly assumed programmer's model of memory is sequential consistency. The weaker models of weak ordering, release consistency with sequentially consistent synchronization operations, data-race-free-0, and data-race-free-1 provide higher performance by guaranteeing sequential consistency to only a restricted class of programs mainly programs that do not exhibit data races. To allow programmers to use the intuition and algorithms already developed for sequentially consistent systems, it is important to determine when a program written for a weak system exhibits no data races. In this paper, we investigate the extension of dynamic data race detection techniques developed for sequentially consistent systems to weak systems. A potential problem is that in the presence of a data race, weak systems fail to guarantee sequential consistency and therefore dynamic techniques may not give meaningful results. However, we reason that in practice a weak system will preserve sequential consistency at least until the ``first'' data races since it cannot predict if a data race will occur. We formalize this condition and show that it allows data races to be dynamically detected. Further, since this condition is already obeyed by all proposed implementations of weak systems, the full performance of weak systems can be exploited. 
Abstract-found: 1
Intro-found: 1
Reference: [AdH90] <author> S. V. ADVE and M. D. HILL, </author> <title> Weak Ordering ANew Definition, </title> <booktitle> Proc. 17th Annual Intl. Symp. on Computer Architecture, </booktitle> <month> May </month> <year> 1990, </year> <pages> 2-14. </pages>
Reference-contexts: Implementing sequential consistency in shared-memory multiprocessors, however, restricts the use of many performance enhancing features of uniprocessor systems, such as out-of-order instruction issue and completion, write buffers, and lockup-free caches [DSB86]. For higher performance, researchers have proposed alternative memory models <ref> [AdH90, AdH91, DSB86, GLL90, Goo91] </ref>. These models, however, do not provide sequential consistency to all programs. To allow programmers to use the intuition and algorithms already developed for sequential consistency, it is important to characterize the programs for which the models do provide sequential consistency. <p> To allow programmers to use the intuition and algorithms already developed for sequential consistency, it is important to characterize the programs for which the models do provide sequential consistency. The models of weak ordering (WO) [DSB86], release consistency with sequentially consistent synchronization operations (RC sc ) [GLL90], data-race-free-0 (DRF0) <ref> [AdH90] </ref>, and data-race-free-1 (DRF1) [AdH91] are similar in the respect that they provide sequential consistency to all programs that exhibit no data races in any execution on sequentially consistent hardware (i.e., to data-race-free programs). <p> We now discuss how the four weak models differ from each other. The model of WO [DSB86] is specified as a set of explicit conditions on hardware. It has been shown that these conditions ensure sequential consistency for all data-race-free programs <ref> [AdH90, AdH91] </ref>. The model of RC sc [GLL90] is also specified as a set of conditions on hardware. The main difference between the conditions of WO and RC sc is that the latter exploit the distinction between acquire and release synchronization operations. <p> Such programs have been called properly labeled with respect to sequential consistency, and are the same as data-race-free programs [AdH91]. There do exist some programs that exhibit certain restricted kinds of data races for which WO and RC sc hardware ensure sequential consistency <ref> [AdH90] </ref>. These programs, however, are few and are not formally characterized. Hence for all practical purposes, we assume that for sequential consistency to be guaranteed by these models, programs should be data-race-free. DRF0 [AdH90] and DRF1 [AdH91] are defined to include all hardware that guarantees sequential consistency to programs that are <p> exhibit certain restricted kinds of data races for which WO and RC sc hardware ensure sequential consistency <ref> [AdH90] </ref>. These programs, however, are few and are not formally characterized. Hence for all practical purposes, we assume that for sequential consistency to be guaranteed by these models, programs should be data-race-free. DRF0 [AdH90] and DRF1 [AdH91] are defined to include all hardware that guarantees sequential consistency to programs that are data-race-free. DRF0 differs from DRF1 in that it does not distinguish between acquire and release operations. 3. <p> All weak implementations guarantee sequential consistency to data-race-free programs. They achieve this by guaranteeing sequential consistency to every data-race-free execution on the weak sys - 6 - - -- tem <ref> [AdH90, AdH91, GLL90] </ref>, thereby obeying Condition 3.4 (1). Condition 3.4 (2) requires that every data race in a weak execution that is not affected by any other data race should also occur in a specific sequentially consistent execution of the same program.
Reference: [AHM91] <author> S. V. ADVE, M. D. HILL, B. P. MILLER and R. H. B. NETZER, </author> <title> Detecting Data Races on Weak Memory Systems, </title> <note> Computer Sciences Technical Report, To be Published, </note> <institution> University of Wisconsin, Madison, </institution> <year> 1991. </year>
Reference-contexts: Due to space constraints, in this paper, we only give an informal intuition for this result. A formal proof of the theorem appears in <ref> [AHM91] </ref>. For brevity, we use the term all weak implementations to imply all possible implementations of WO and RC sc and all implementations of DRF0 and DRF1 that have been proposed to date. <p> For the execution of Figure 2b, the partitions and their ordering are shown in Figure 3. We now state the following theorems regarding the first partitions. The proofs of these theorems are relatively straightforward and appear in <ref> [AHM91] </ref>. Theorem 4.1: There are no first partitions containing data races iff no data races were exhibited in the execution. Theorem 4.2: In each first partition containing data races, at least one data race belongs to an SCP.
Reference: [AdH91] <author> S. V. ADVE and M. D. HILL, </author> <title> An Approach for Specifying Shared Memory Models, </title> <note> Computer Sciences Technical Report, To be Published, </note> <institution> University of Wisconsin, Madison, </institution> <year> 1991. </year>
Reference-contexts: Implementing sequential consistency in shared-memory multiprocessors, however, restricts the use of many performance enhancing features of uniprocessor systems, such as out-of-order instruction issue and completion, write buffers, and lockup-free caches [DSB86]. For higher performance, researchers have proposed alternative memory models <ref> [AdH90, AdH91, DSB86, GLL90, Goo91] </ref>. These models, however, do not provide sequential consistency to all programs. To allow programmers to use the intuition and algorithms already developed for sequential consistency, it is important to characterize the programs for which the models do provide sequential consistency. <p> The models of weak ordering (WO) [DSB86], release consistency with sequentially consistent synchronization operations (RC sc ) [GLL90], data-race-free-0 (DRF0) [AdH90], and data-race-free-1 (DRF1) <ref> [AdH91] </ref> are similar in the respect that they provide sequential consistency to all programs that exhibit no data races in any execution on sequentially consistent hardware (i.e., to data-race-free programs). <p> We now discuss how the four weak models differ from each other. The model of WO [DSB86] is specified as a set of explicit conditions on hardware. It has been shown that these conditions ensure sequential consistency for all data-race-free programs <ref> [AdH90, AdH91] </ref>. The model of RC sc [GLL90] is also specified as a set of conditions on hardware. The main difference between the conditions of WO and RC sc is that the latter exploit the distinction between acquire and release synchronization operations. <p> The hardware conditions of RC sc are accompanied by a formalization of the programs for which they ensure sequential consistency. Such programs have been called properly labeled with respect to sequential consistency, and are the same as data-race-free programs <ref> [AdH91] </ref>. There do exist some programs that exhibit certain restricted kinds of data races for which WO and RC sc hardware ensure sequential consistency [AdH90]. These programs, however, are few and are not formally characterized. <p> These programs, however, are few and are not formally characterized. Hence for all practical purposes, we assume that for sequential consistency to be guaranteed by these models, programs should be data-race-free. DRF0 [AdH90] and DRF1 <ref> [AdH91] </ref> are defined to include all hardware that guarantees sequential consistency to programs that are data-race-free. DRF0 differs from DRF1 in that it does not distinguish between acquire and release operations. 3. <p> All weak implementations guarantee sequential consistency to data-race-free programs. They achieve this by guaranteeing sequential consistency to every data-race-free execution on the weak sys - 6 - - -- tem <ref> [AdH90, AdH91, GLL90] </ref>, thereby obeying Condition 3.4 (1). Condition 3.4 (2) requires that every data race in a weak execution that is not affected by any other data race should also occur in a specific sequentially consistent execution of the same program.
Reference: [AlP87] <author> T. R. ALLEN and D. A. PADUA, </author> <title> Debugging Fortran on a Shared Memory Machine, </title> <booktitle> Proc. Intl. Conf. on Parallel Processing, </booktitle> <month> August </month> <year> 1987, </year> <pages> 721-727. </pages>
Reference-contexts: Dynamic techniques, on the other hand, use a tracing mechanism to detect whether a particular sequentially consistent execution of a program actually exhibited a data race <ref> [AlP87, ChM91, DiS90, HKM90, NeM90, NeM91] </ref>. While dynamic techniques provide precise information about a single execution, they provide little information about other executions, a serious disadvantage. <p> We then give a hardware condition that addresses these limitations, and explain why this condition is already obeyed by all proposed implementations of the weak systems. 3.1. Problems in Applying Dynamic Techniques to Weak Systems Existing dynamic data race detection techniques for sequentially consistent systems <ref> [AlP87, ChM91, DiS90, HKM90, NeM90, NeM91] </ref> instrument the program to record information about the memory operations of its exe cution. This information allows the hb1 relation to be constructed, thereby allowing the detection of data races. There are two approaches for recording and using this information. <p> Instead, higher-level information about the execution can be recorded and used for data race detection. The execution of each processor can be viewed as a sequence of events that represent groups of memory operations. We adopt the approach used by previous data race detection methods <ref> [AlP87, ChM91, DiS90, HKM90, NeM90, NeM91] </ref> and define an event to be either a single synchronization operation (a synchronization event), or a group of consecutively executed data operations (a computation event).
Reference: [BaK89] <author> V. BALASUNDARAM and K. KENNEDY, </author> <title> Compile-time Detection of Race Conditions in a Parallel Program, </title> <booktitle> 3rd Intl. Conf. on Supercomputing, </booktitle> <month> June </month> <year> 1989, </year> <pages> 175-185. </pages>
Reference-contexts: Static techniques perform a compile-time analysis of the program text to detect a superset of all possible data races that could potentially occur in all possible sequentially consistent executions of the program <ref> [BaK89, Tay83a] </ref>. In general, static analysis must be conservative and slow, because detecting data races is undecidable for arbitrary programs [Ber66] and is NP-complete for even very restricted classes of programs (e.g., those containing no branches) [Tay83b].
Reference: [Ber66] <author> A. J. BERNSTEIN, </author> <title> Analysis of Programs for Parallel Processing, </title> <journal> IEEE Trans. on Electronic Computers EC-15, </journal> <month> 5 (October </month> <year> 1966), </year> <pages> 757-763. </pages>
Reference-contexts: In general, static analysis must be conservative and slow, because detecting data races is undecidable for arbitrary programs <ref> [Ber66] </ref> and is NP-complete for even very restricted classes of programs (e.g., those containing no branches) [Tay83b]. Dynamic techniques, on the other hand, use a tracing mechanism to detect whether a particular sequentially consistent execution of a program actually exhibited a data race [AlP87, ChM91, DiS90, HKM90, NeM90, NeM91].
Reference: [ChM91] <author> J. CHOI and S. L. MIN, </author> <title> Race Frontier: Reproducing Data Races in Parallel Program Debugging, </title> <booktitle> Proc. 3rd ACM Symp. on Principles and Practice of Parallel Programming, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: Dynamic techniques, on the other hand, use a tracing mechanism to detect whether a particular sequentially consistent execution of a program actually exhibited a data race <ref> [AlP87, ChM91, DiS90, HKM90, NeM90, NeM91] </ref>. While dynamic techniques provide precise information about a single execution, they provide little information about other executions, a serious disadvantage. <p> We then give a hardware condition that addresses these limitations, and explain why this condition is already obeyed by all proposed implementations of the weak systems. 3.1. Problems in Applying Dynamic Techniques to Weak Systems Existing dynamic data race detection techniques for sequentially consistent systems <ref> [AlP87, ChM91, DiS90, HKM90, NeM90, NeM91] </ref> instrument the program to record information about the memory operations of its exe cution. This information allows the hb1 relation to be constructed, thereby allowing the detection of data races. There are two approaches for recording and using this information. <p> Instead, higher-level information about the execution can be recorded and used for data race detection. The execution of each processor can be viewed as a sequence of events that represent groups of memory operations. We adopt the approach used by previous data race detection methods <ref> [AlP87, ChM91, DiS90, HKM90, NeM90, NeM91] </ref> and define an event to be either a single synchronization operation (a synchronization event), or a group of consecutively executed data operations (a computation event). <p> Another approach for locating data races is on-the-fly detection. On-the-fly approaches have the advantage of not consuming secondary storage <ref> [ChM91, DiS90, HKM90] </ref>, but existing methods are typically less accurate and have higher run-time overhead than post-mortem techniques. The loss of accuracy is a result of attempts to keep space overhead low by only buffering limited trace information in memory.
Reference: [DiS90] <author> A. DINNING and E. SCHONBERG, </author> <title> An Empirical Comparison of Monitoring Algorithms for Access Anomaly Detection, </title> <booktitle> Proc. ACM SIGPLAN Notices Symp. on Principles and Practice of Parallel Programming, </booktitle> <month> March </month> <year> 1990, </year> <pages> 1-10. </pages>
Reference-contexts: Dynamic techniques, on the other hand, use a tracing mechanism to detect whether a particular sequentially consistent execution of a program actually exhibited a data race <ref> [AlP87, ChM91, DiS90, HKM90, NeM90, NeM91] </ref>. While dynamic techniques provide precise information about a single execution, they provide little information about other executions, a serious disadvantage. <p> We then give a hardware condition that addresses these limitations, and explain why this condition is already obeyed by all proposed implementations of the weak systems. 3.1. Problems in Applying Dynamic Techniques to Weak Systems Existing dynamic data race detection techniques for sequentially consistent systems <ref> [AlP87, ChM91, DiS90, HKM90, NeM90, NeM91] </ref> instrument the program to record information about the memory operations of its exe cution. This information allows the hb1 relation to be constructed, thereby allowing the detection of data races. There are two approaches for recording and using this information. <p> Instead, higher-level information about the execution can be recorded and used for data race detection. The execution of each processor can be viewed as a sequence of events that represent groups of memory operations. We adopt the approach used by previous data race detection methods <ref> [AlP87, ChM91, DiS90, HKM90, NeM90, NeM91] </ref> and define an event to be either a single synchronization operation (a synchronization event), or a group of consecutively executed data operations (a computation event). <p> Another approach for locating data races is on-the-fly detection. On-the-fly approaches have the advantage of not consuming secondary storage <ref> [ChM91, DiS90, HKM90] </ref>, but existing methods are typically less accurate and have higher run-time overhead than post-mortem techniques. The loss of accuracy is a result of attempts to keep space overhead low by only buffering limited trace information in memory.
Reference: [DSB86] <author> M. DUBOIS, C. SCHEURICH and F. A. BRIGGS, </author> <title> Memory Access Buffering in Multiprocessors, </title> <booktitle> Proc. 13th Annual Intl. Symp. on Computer Architecture 14, </booktitle> <month> 2 (June </month> <year> 1986), </year> <pages> 434-442. </pages>
Reference-contexts: A system provides sequential consistency if it only allows sequentially consistent executions. Implementing sequential consistency in shared-memory multiprocessors, however, restricts the use of many performance enhancing features of uniprocessor systems, such as out-of-order instruction issue and completion, write buffers, and lockup-free caches <ref> [DSB86] </ref>. For higher performance, researchers have proposed alternative memory models [AdH90, AdH91, DSB86, GLL90, Goo91]. These models, however, do not provide sequential consistency to all programs. <p> Implementing sequential consistency in shared-memory multiprocessors, however, restricts the use of many performance enhancing features of uniprocessor systems, such as out-of-order instruction issue and completion, write buffers, and lockup-free caches [DSB86]. For higher performance, researchers have proposed alternative memory models <ref> [AdH90, AdH91, DSB86, GLL90, Goo91] </ref>. These models, however, do not provide sequential consistency to all programs. To allow programmers to use the intuition and algorithms already developed for sequential consistency, it is important to characterize the programs for which the models do provide sequential consistency. <p> These models, however, do not provide sequential consistency to all programs. To allow programmers to use the intuition and algorithms already developed for sequential consistency, it is important to characterize the programs for which the models do provide sequential consistency. The models of weak ordering (WO) <ref> [DSB86] </ref>, release consistency with sequentially consistent synchronization operations (RC sc ) [GLL90], data-race-free-0 (DRF0) [AdH90], and data-race-free-1 (DRF1) [AdH91] are similar in the respect that they provide sequential consistency to all programs that exhibit no data races in any execution on sequentially consistent hardware (i.e., to data-race-free programs). <p> Thus, it is possible for P2 to read the new value for y but the old value for x, thereby violating sequential consistency. We now discuss how the four weak models differ from each other. The model of WO <ref> [DSB86] </ref> is specified as a set of explicit conditions on hardware. It has been shown that these conditions ensure sequential consistency for all data-race-free programs [AdH90, AdH91]. The model of RC sc [GLL90] is also specified as a set of conditions on hardware.
Reference: [EmP88] <author> P. A. EMRATH and D. A. PADUA, </author> <title> Automatic Detection of Nondeterminacy in Parallel Programs , Proc. </title> <booktitle> SIGOPS Workshop on Parallel and Distributed Debugging, </booktitle> <month> May </month> <year> 1988, </year> <pages> 89-99. </pages> <note> Also appears in SIGPLAN Notices 24(1) (January 1989). </note>
Reference-contexts: While dynamic techniques provide precise information about a single execution, they provide little information about other executions, a serious disadvantage. For these reasons, the general consensus among researchers investigating data race detection is that tools should support both static and dynamic techniques in a complementary fashion <ref> [EmP88] </ref>. Rather than start from scratch, we seek to apply the data race detection techniques from sequentially consistent systems to weak systems. Static techniques can be applied to programs for weak systems unchanged, because they do not rely on executing the program. Dynamic techniques, however, depend on executing a program.
Reference: [GLL90] <author> K. GHARACHORLOO, D. LENOSKI, J. LAUDON, P. GIBBONS, A. GUPTA and J. HENNESSY, </author> <title> Memory Consistency and Event Ordering in Scalable Shared-Memory Multiprocessors, </title> <booktitle> Proc. 17th Annual Intl. Symp. on Computer Architecture, </booktitle> <month> May </month> <year> 1990, </year> <pages> 15-26. </pages>
Reference-contexts: Implementing sequential consistency in shared-memory multiprocessors, however, restricts the use of many performance enhancing features of uniprocessor systems, such as out-of-order instruction issue and completion, write buffers, and lockup-free caches [DSB86]. For higher performance, researchers have proposed alternative memory models <ref> [AdH90, AdH91, DSB86, GLL90, Goo91] </ref>. These models, however, do not provide sequential consistency to all programs. To allow programmers to use the intuition and algorithms already developed for sequential consistency, it is important to characterize the programs for which the models do provide sequential consistency. <p> To allow programmers to use the intuition and algorithms already developed for sequential consistency, it is important to characterize the programs for which the models do provide sequential consistency. The models of weak ordering (WO) [DSB86], release consistency with sequentially consistent synchronization operations (RC sc ) <ref> [GLL90] </ref>, data-race-free-0 (DRF0) [AdH90], and data-race-free-1 (DRF1) [AdH91] are similar in the respect that they provide sequential consistency to all programs that exhibit no data races in any execution on sequentially consistent hardware (i.e., to data-race-free programs). <p> Further, with the synchronization operations that are allowed to be used for ordering, there should be associated certain semantics. We encapsulate these semantics by using the classification of release and acquire synchronization operations proposed in <ref> [GLL90] </ref> and a pairing relation as follows: Definition 2.1: Two synchronization operations, s 1 and s 2 , are paired iff the following are satisfied. (1) Synchronization operation s 1 is a write and can be used by a processor to communicate the completion of all its previous memory operations (in <p> We now discuss how the four weak models differ from each other. The model of WO [DSB86] is specified as a set of explicit conditions on hardware. It has been shown that these conditions ensure sequential consistency for all data-race-free programs [AdH90, AdH91]. The model of RC sc <ref> [GLL90] </ref> is also specified as a set of conditions on hardware. The main difference between the conditions of WO and RC sc is that the latter exploit the distinction between acquire and release synchronization operations. <p> All weak implementations guarantee sequential consistency to data-race-free programs. They achieve this by guaranteeing sequential consistency to every data-race-free execution on the weak sys - 6 - - -- tem <ref> [AdH90, AdH91, GLL90] </ref>, thereby obeying Condition 3.4 (1). Condition 3.4 (2) requires that every data race in a weak execution that is not affected by any other data race should also occur in a specific sequentially consistent execution of the same program.
Reference: [Goo91] <author> J. R. GOODMAN, </author> <title> Cache Consistency and Sequential Consistency, </title> <type> Computer Sciences Technical Report #1006, </type> <institution> Univ. of Wisconsin, Madison, </institution> <month> February </month> <year> 1991. </year>
Reference-contexts: Implementing sequential consistency in shared-memory multiprocessors, however, restricts the use of many performance enhancing features of uniprocessor systems, such as out-of-order instruction issue and completion, write buffers, and lockup-free caches [DSB86]. For higher performance, researchers have proposed alternative memory models <ref> [AdH90, AdH91, DSB86, GLL90, Goo91] </ref>. These models, however, do not provide sequential consistency to all programs. To allow programmers to use the intuition and algorithms already developed for sequential consistency, it is important to characterize the programs for which the models do provide sequential consistency.
Reference: [HKM90] <author> R. HOOD, K. KENNEDY and J. MELLOR-CRUMMEY, </author> <title> Parallel Program Debugging with On-the-fly Anomaly Detection, </title> <booktitle> Supercomputing '90, </booktitle> <month> November </month> <year> 1990, </year> <pages> 74-81. </pages>
Reference-contexts: Dynamic techniques, on the other hand, use a tracing mechanism to detect whether a particular sequentially consistent execution of a program actually exhibited a data race <ref> [AlP87, ChM91, DiS90, HKM90, NeM90, NeM91] </ref>. While dynamic techniques provide precise information about a single execution, they provide little information about other executions, a serious disadvantage. <p> We then give a hardware condition that addresses these limitations, and explain why this condition is already obeyed by all proposed implementations of the weak systems. 3.1. Problems in Applying Dynamic Techniques to Weak Systems Existing dynamic data race detection techniques for sequentially consistent systems <ref> [AlP87, ChM91, DiS90, HKM90, NeM90, NeM91] </ref> instrument the program to record information about the memory operations of its exe cution. This information allows the hb1 relation to be constructed, thereby allowing the detection of data races. There are two approaches for recording and using this information. <p> Instead, higher-level information about the execution can be recorded and used for data race detection. The execution of each processor can be viewed as a sequence of events that represent groups of memory operations. We adopt the approach used by previous data race detection methods <ref> [AlP87, ChM91, DiS90, HKM90, NeM90, NeM91] </ref> and define an event to be either a single synchronization operation (a synchronization event), or a group of consecutively executed data operations (a computation event). <p> Another approach for locating data races is on-the-fly detection. On-the-fly approaches have the advantage of not consuming secondary storage <ref> [ChM91, DiS90, HKM90] </ref>, but existing methods are typically less accurate and have higher run-time overhead than post-mortem techniques. The loss of accuracy is a result of attempts to keep space overhead low by only buffering limited trace information in memory.
Reference: [Lam78] <author> L. LAMPORT, </author> <title> Time, Clocks, and the Ordering of Events in a Distributed System, </title> <journal> Communications of the ACM 21, </journal> <month> 7 (July </month> <year> 1978), </year> <pages> 558-565. </pages>
Reference-contexts: For a formal definition of a data race, we define a happens-before-1 or hb1 relation for every sequentially consistent execution of a program. Our definition is closely related to the ``happened-before'' relation defined by Lam port <ref> [Lam78] </ref> for message-passing systems. The hb1 lation for a sequentially consistent execution is a partial order on its memory operations. Two operations initiated by the same processor are ordered by hb1 according to program order.
Reference: [Lam79] <author> L. LAMPORT, </author> <title> How to Make a Multiprocessor Computer That Correctly Executes Multiprocess Programs, </title> <journal> IEEE Trans. on Computers C-28, </journal> <month> 9 (September </month> <year> 1979), </year> <pages> 690-691. </pages>
Reference-contexts: A specification of shared memory semantics, called a memory model or memory consistency model, is necessary to determine exactly which writes could be ``last''. The most commonly and often implicitly assumed memory model is sequential consistency <ref> [Lam79] </ref>, a direct extension of the way memory is viewed in a multipro-grammed uniprocessor.
Reference: [NeM90] <author> R. H. B. NETZER and B. P. MILLER, </author> <title> Detecting Data Races in Parallel Program Executions, </title> <booktitle> To appear in Research Monographs in Parallel and Distributed Computing, </booktitle> <publisher> MIT Press, </publisher> <year> 1991. </year> <note> Also available as Proc. 3rd Workshop on Programming Languages and Compilers for Parallel Computing, </note> <month> August </month> <year> 1990. </year>
Reference-contexts: Dynamic techniques, on the other hand, use a tracing mechanism to detect whether a particular sequentially consistent execution of a program actually exhibited a data race <ref> [AlP87, ChM91, DiS90, HKM90, NeM90, NeM91] </ref>. While dynamic techniques provide precise information about a single execution, they provide little information about other executions, a serious disadvantage. <p> We then give a hardware condition that addresses these limitations, and explain why this condition is already obeyed by all proposed implementations of the weak systems. 3.1. Problems in Applying Dynamic Techniques to Weak Systems Existing dynamic data race detection techniques for sequentially consistent systems <ref> [AlP87, ChM91, DiS90, HKM90, NeM90, NeM91] </ref> instrument the program to record information about the memory operations of its exe cution. This information allows the hb1 relation to be constructed, thereby allowing the detection of data races. There are two approaches for recording and using this information. <p> Instead, higher-level information about the execution can be recorded and used for data race detection. The execution of each processor can be viewed as a sequence of events that represent groups of memory operations. We adopt the approach used by previous data race detection methods <ref> [AlP87, ChM91, DiS90, HKM90, NeM90, NeM91] </ref> and define an event to be either a single synchronization operation (a synchronization event), or a group of consecutively executed data operations (a computation event). <p> A data race is an artifact if it occurs only because a previous data race left the program's data in an inconsistent state unexpected by the programmer, and hence is not a direct manifestation of a program bug. Methods for accurately locating data races on sequentially consistent systems <ref> [NeM90, NeM91] </ref> also order partitions of data races to enable detection of the non-artifact races. <p> However, the overhead of our method is no worse than post-mortem methods on sequentially consistent systems: we require no more execution-time information that these methods, and our analysis requires computation similar to the more accurate techniques for sequentially consistent systems <ref> [NeM90, NeM91] </ref>. Another approach for locating data races is on-the-fly detection. On-the-fly approaches have the advantage of not consuming secondary storage [ChM91, DiS90, HKM90], but existing methods are typically less accurate and have higher run-time overhead than post-mortem techniques.
Reference: [NeM91] <author> R. H. B. NETZER and B. P. MILLER, </author> <title> Improving the Accuracy of Data Race Detection, </title> <booktitle> Proc. 3rd ACM Symp. on Principles and Practice of Parallel Programming, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: Dynamic techniques, on the other hand, use a tracing mechanism to detect whether a particular sequentially consistent execution of a program actually exhibited a data race <ref> [AlP87, ChM91, DiS90, HKM90, NeM90, NeM91] </ref>. While dynamic techniques provide precise information about a single execution, they provide little information about other executions, a serious disadvantage. <p> We then give a hardware condition that addresses these limitations, and explain why this condition is already obeyed by all proposed implementations of the weak systems. 3.1. Problems in Applying Dynamic Techniques to Weak Systems Existing dynamic data race detection techniques for sequentially consistent systems <ref> [AlP87, ChM91, DiS90, HKM90, NeM90, NeM91] </ref> instrument the program to record information about the memory operations of its exe cution. This information allows the hb1 relation to be constructed, thereby allowing the detection of data races. There are two approaches for recording and using this information. <p> Instead, higher-level information about the execution can be recorded and used for data race detection. The execution of each processor can be viewed as a sequence of events that represent groups of memory operations. We adopt the approach used by previous data race detection methods <ref> [AlP87, ChM91, DiS90, HKM90, NeM90, NeM91] </ref> and define an event to be either a single synchronization operation (a synchronization event), or a group of consecutively executed data operations (a computation event). <p> A data race is an artifact if it occurs only because a previous data race left the program's data in an inconsistent state unexpected by the programmer, and hence is not a direct manifestation of a program bug. Methods for accurately locating data races on sequentially consistent systems <ref> [NeM90, NeM91] </ref> also order partitions of data races to enable detection of the non-artifact races. <p> These methods also suffer from the above limitations: (1) they identify partitions of data races, each of which is guaranteed to contain at least one non-artifact data race, but cannot determine precisely which races within each partition are not artifacts <ref> [NeM91] </ref>, and (2) they cannot identify non-artifact data races that happen to belong to non-first partitions. <p> However, the overhead of our method is no worse than post-mortem methods on sequentially consistent systems: we require no more execution-time information that these methods, and our analysis requires computation similar to the more accurate techniques for sequentially consistent systems <ref> [NeM90, NeM91] </ref>. Another approach for locating data races is on-the-fly detection. On-the-fly approaches have the advantage of not consuming secondary storage [ChM91, DiS90, HKM90], but existing methods are typically less accurate and have higher run-time overhead than post-mortem techniques.
Reference: [Tay83a] <author> R. N. TAYLOR, </author> <title> A General-Purpose Algorithm for Analyzing Concurrent Programs, </title> <journal> Communications of the ACM 26, </journal> <month> 5 (May </month> <year> 1983), </year> <pages> 362-376. </pages>
Reference-contexts: Static techniques perform a compile-time analysis of the program text to detect a superset of all possible data races that could potentially occur in all possible sequentially consistent executions of the program <ref> [BaK89, Tay83a] </ref>. In general, static analysis must be conservative and slow, because detecting data races is undecidable for arbitrary programs [Ber66] and is NP-complete for even very restricted classes of programs (e.g., those containing no branches) [Tay83b].
Reference: [Tay83b] <author> R. N. TAYLOR, </author> <title> Complexity of Analyzing the Synchronization Structure of Concurrent Programs, </title> <journal> Acta Informatica 19(1983), </journal> <pages> 57-84. - 10 </pages> - 
Reference-contexts: In general, static analysis must be conservative and slow, because detecting data races is undecidable for arbitrary programs [Ber66] and is NP-complete for even very restricted classes of programs (e.g., those containing no branches) <ref> [Tay83b] </ref>. Dynamic techniques, on the other hand, use a tracing mechanism to detect whether a particular sequentially consistent execution of a program actually exhibited a data race [AlP87, ChM91, DiS90, HKM90, NeM90, NeM91].
References-found: 19

