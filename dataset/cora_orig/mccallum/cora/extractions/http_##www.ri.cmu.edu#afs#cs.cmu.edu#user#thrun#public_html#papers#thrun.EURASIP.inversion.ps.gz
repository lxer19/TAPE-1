URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/thrun/public_html/papers/thrun.EURASIP.inversion.ps.gz
Refering-URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/thrun/public_html/papers/full.html
Root-URL: 
Title: INVERSION IN TIME  
Author: Sebastian Thrun Alexander Linden 
Keyword: connectionist systems, backpropagation, inversion, recurrent neural networks, digit recognition  
Address: D-5205 St. Augustin, West Germany  
Affiliation: Gesellschaft fur Mathematik und Datenverarbeitung mbH  
Date: February 15-17, 1990, Portugal  
Note: In: Proceedings of EURASIP Workshop,  
Abstract: Inversion of multilayer synchronous networks is a method which tries to answer questions like "What kind of input will give a desired output?" or "Is it possible to get a desired output (under special input/output constraints)?". We will describe two methods of inverting a connectionist network. Firstly, we extend inversion via backpropagation (Linden/Kindermann [4], Williams [11]) to recurrent (El-man [1], Jordan [3], Mozer [5], Williams/Zipser [10]), time-delayed (Waibel at al. [9]) and discrete versions of continuous networks (Pineda [7], Pearlmutter [6]). The result of inversion is an input vector. The corresponding output vector is equal to the target vector except a small remainder. The knowledge of those attractors may help to understand the function and the generalization qualities of connectionist systems of this kind. Secondly, we introduce a new inversion method for proving the non-existence of an input combination under special constraints, e.g. in a subspace of the input space. This method works by iterative exclusion of invalid activation values. It might be a helpful way to judge the properties of a trained network. We conclude with simulation results of three different tasks: XOR, morse signal decoding and handwritten digit recognition. 
Abstract-found: 1
Intro-found: 1
Reference: [1 ] <author> J. L. Elman, </author> <title> Finding Structure in Time. </title> <type> Technical Report CRL Technical Report 8801, </type> <institution> Center for Research in Language, University of California, </institution> <address> San Diego, </address> <year> 1988 </year>
Reference-contexts: As a remark, working with the activation gradient (equation (5)) leads to unbounded activations which violate the usual constraint that activities are bounded (by <ref> [0; 1] </ref>, for example). When using a pseudo net-input and its gradient ffi i = @net i instead of equation (5), the activations are always in the correct interval. In this case, the computation works slower. Sometimes it is useful to approximate a certain input vector ~ I simultaneously. <p> On the whole, there are two directions to propagate constraints through the network. So the validity intervals can be decreased iteratively. We derive the algorithm in the appendix. Since each sequence of intervals is monotonous and bounded by <ref> [0; 1] </ref>, it has to converge. There are two criteria for finishing the iteration: a) the accumulated changes are smaller than a small *, no significant changes are expected here and b) a contradiction comes forward: ff i &gt; fi i . <p> Knowing this, inversion via backpropagation seemed to be less interesting in this context. 6.2 Morse-Decoding A simple sequential approach is the decoding of morse signals. We constructed a network consisting of one input, five hidden, five context and 26 output units (Elman <ref> [1] </ref>, see figure 2). One signal, either long or short, was fed into the net in one time step. Short signals were represented by low activation values (2 [0:5; 0:3]) and long signals by high activation values (2 [0:3; 0:5]). <p> Now we applied inversion with validity intervals to the trained network. We could prove that there is no attractor of false length for those four target patterns at all. 6.3 Handwritten Digit Recognition Finally, 490 handwritten digits were trained with a recurrent network similar to those proposed by Elman <ref> [1] </ref> and Jordan [3]. Every picture of a handwritten digit was coded by an (11 fi 8)-pixel matrix. <p> At the beginning of the algorithm each interval of a restricted unit will be initialized by its restriction interval the free units' intervals will be set to the largest allowed interval <ref> [0; 1] </ref>. Loop: Compute the new interval serially/in parallel for every unit i 2 ^ N : 1. The validity output interval of unit i depends on the maximal net-input. Every predecessor k 2 P (i) of unit i has got a validity interval itself.
Reference: [2 ] <author> G. E. Hinton, </author> <title> Connectionist Learning Procedures. </title> <type> Technical Report CMU-CS-87-115, </type> <institution> Pitts-burgh, </institution> <year> 1987 </year>
Reference: [3 ] <author> M. I. Jordan, </author> <title> Serial Order: A Parallel Distributed Processing Approach. </title> <type> Technical Report ICS Report 8604, </type> <institution> Institute for Cognitive Science, University of California, B1986 </institution>
Reference-contexts: We could prove that there is no attractor of false length for those four target patterns at all. 6.3 Handwritten Digit Recognition Finally, 490 handwritten digits were trained with a recurrent network similar to those proposed by Elman [1] and Jordan <ref> [3] </ref>. Every picture of a handwritten digit was coded by an (11 fi 8)-pixel matrix.
Reference: [4 ] <author> A. Linden, J. Kindermann, </author> <title> Inversion of Multilayer Nets. </title> <booktitle> Proceedings of the First Interna--tional Joint Conference on Neural Networks, </booktitle> <address> Washington, </address> <year> 1989 </year>
Reference-contexts: Since the new corresponding graph ^ G = ( ^ N ; ^ E) is free of cycles and free of time delays, the inversion method by Linden/Kindermann <ref> [4] </ref> can be used.
Reference: [5 ] <author> M. C. Mozer, </author> <title> A Focused Back/Propagation Algorithm for Temporal Pattern Recognition. </title> <type> Technical Report CRG-TR-88-3, </type> <year> 1988 </year>
Reference-contexts: Usually the set of output nodes, denoted by O N , is disjunct from the set of input nodes. It includes all nodes without any successor. For unfolding the network in time (Rumelhart/McClelland [8], Mozer <ref> [5] </ref>), the graph G has to be transformed into its spatial representation. Therefore it is necessary to know the maximum length over all input sequences ^ T 2 IN [ f+1g. Note that in the case of ^ T = +1 the spatial representation will be of infinite length.
Reference: [6 ] <author> B. A. Pearlmutter, </author> <title> Learning State Space Trajectories in Recurrent Neural Networks. </title> <type> Technical Report CMU-CS-88-191, </type> <year> 1988 </year>
Reference-contexts: In this paper we have shown how to use the two inversion methods to get an impression of the generalization concept of a trained discrete recurrent network. On the same way one might apply our methods to continuous networks (Pearlmutter <ref> [6] </ref>, Pineda [7]). In general, the method of inversion with validity intervals is more useful for smaller networks and binary tasks; inversion via backpropagation provides good results at large networks which have to learn fuzzy classifications.
Reference: [7 ] <author> F. J. Pineda, </author> <title> Generalization of backpropagation to recurrent neural networks. </title> <journal> Physical Review Letters, </journal> 59/19:2229-2232, 1987 
Reference-contexts: In this paper we have shown how to use the two inversion methods to get an impression of the generalization concept of a trained discrete recurrent network. On the same way one might apply our methods to continuous networks (Pearlmutter [6], Pineda <ref> [7] </ref>). In general, the method of inversion with validity intervals is more useful for smaller networks and binary tasks; inversion via backpropagation provides good results at large networks which have to learn fuzzy classifications.
Reference: [8 ] <author> D. E. Rumelhart, J. L. McClelland, </author> <title> Parallel Distributed Processing. </title> <publisher> The MIT Press, </publisher> <address> University of California San Diego, </address> <year> 1986 </year>
Reference-contexts: Usually the set of output nodes, denoted by O N , is disjunct from the set of input nodes. It includes all nodes without any successor. For unfolding the network in time (Rumelhart/McClelland <ref> [8] </ref>, Mozer [5]), the graph G has to be transformed into its spatial representation. Therefore it is necessary to know the maximum length over all input sequences ^ T 2 IN [ f+1g.
Reference: [9 ] <author> A. Waibel, H. Sawai, K. Shikano, </author> <title> Modularity and Scaling in Large Phonemic Networks. </title> <type> Technical Report TR-I-0034, </type> <institution> ATR Interpreting Telephony Research Laboratories, </institution> <year> 1988 </year>
Reference: [10 ] <author> R. J. Williams, D. Zipser, </author> <title> A Learning Algorithm for Continually Running Fully Recurrent Networks. </title> <type> ICS Report 8805, </type> <year> 1988 </year>
Reference: [11 ] <author> R. J. Williams, </author> <title> Inverting a Connectionist Network Mapping by Backpropagation of Error, </title> <booktitle> Proceedings 8th Annual Conference of the Cognitive Science Society, </booktitle> <month> Lawrence-Erlbaum </month> <year> 1986 </year>
References-found: 11

