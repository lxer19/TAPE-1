URL: http://www.ececs.uc.edu/~mpolycar/publ/tr91-09-01.ps
Refering-URL: http://www.ececs.uc.edu/~mpolycar/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Identification and Control of Nonlinear Systems Using Neural Network Models: Design and Stability Analysis  
Author: by Marios M. Polycarpou and Petros A. Ioannou 
Abstract: Report 91-09-01 September 1991 (revised) May 1994 
Abstract-found: 1
Intro-found: 1
Reference: <author> Antsaklis, P., and Passino, K., Eds, </author> <year> 1992, </year> <title> An Introduction to Intelligent and Autonomous Control, </title> <address> Boston, MA: </address> <publisher> Kluwer Academic Publishers. </publisher>
Reference: <author> Arapostathis, A., Jakubczyk, B., Lee, H.G., Marcus, S.I., and Sontag, E.D., </author> <year> 1989, </year> <title> The effect of sampling on linear equivalence and feedback linearization. </title> <journal> Systems & Control Letters, </journal> <volume> 13, </volume> <pages> 373-381. </pages>
Reference-contexts: The approach usually taken by nonlinear control theorists is to design a control system based on continuous-time plant model and controller and then study the effect of sampling on the overall system <ref> (Arapostathis et al., 1989) </ref>. Furthermore, from a practical perspective, if the neural network approach to control is to achieve its full potential, then it will be through parallel hardware realizations of neural network architectures implemented for real-time control systems.
Reference: <author> Astrom, K.J., and Wittenmark, B., </author> <year> 1989, </year> <title> Adaptive Control, </title> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: A straightforward choice for the control law is the certainty equivalence controller <ref> (Astrom & Wittenmark, 1989) </ref>, obtained by replacing the unknown functions f (x) and g (x) in (4.3) by their estimates ^ f (x; f ) and ^g (x; g ) respectively.
Reference: <author> Barto, A.G., </author> <year> 1990, </year> <title> Connectionist learning for control: an overview, in Neural Networks for Control, </title> <editor> T.W. Miller, R.S. Sutton III, and P.J. Werbos, </editor> <booktitle> Eds, </booktitle> <pages> (pp. 5-58), </pages> <address> Cambridge, MA: </address> <publisher> The MIT Press. </publisher>
Reference: <editor> Bekey, G., and Goldberg, K., Eds, </editor> <booktitle> 1992, Neural Networks in Robotics, </booktitle> <address> Boston, MA: </address> <publisher> Kluwer Academic Publishers. </publisher>
Reference: <author> Broomhead, D.S., and Lowe, D., </author> <year> 1988, </year> <title> Multivariable functional interpolation and adaptive networks. </title> <journal> Complex Systems, </journal> <volume> 2, </volume> <pages> 321-355. </pages> <note> 41 Coddington, E.A., </note> <author> and Levinson, N., </author> <year> 1955, </year> <title> Theory of Ordinary Differential Equations, </title> <address> New York, NY: </address> <publisher> McGraw-Hill. </publisher>
Reference: <author> Cohen, M.A., and Grossberg, S., </author> <year> 1983, </year> <title> Absolute stability of global pattern formation and parallel memory storage by competitive neural networks. </title> <journal> IEEE Trans. System, Man, Cybernetics, </journal> <volume> 13, </volume> <pages> 815-826. </pages>
Reference-contexts: Remark 2.1 The interconnection of static neural networks and dynamic components shown in Figure 1 includes as special cases many of the dynamical models for neuronal activations that have been studied, as for example, the Cohen-Grossberg model <ref> (Cohen & Grossberg, 1983) </ref>, and continuous-time Hopfield model (Hopfield, 1984). For a detailed description of various neuronal activation dynamics the reader is referred to the book by Kosko (1991).
Reference: <author> Cruz, J.B., Jr., Ed., </author> <year> 1973, </year> <title> System Sensitivity Analysis, </title> <address> Stroudsburg, PA: Dowden, </address> <publisher> Hutchinson and Ross. </publisher>
Reference-contexts: Although the above training methods have been successfully used in many investigations, problems arise in attempting to prove stability of the overall system, or convergence of the output error to zero. Interestingly, methods based on straightforward application of optimization techniques, such as sensitivity models <ref> (Cruz, 1973) </ref> and the M.I.T. rule (Whitaker, Yamron, & Kezer, 1958) that dominated the early adaptive linear control literature, exhibited similar stability problems.
Reference: <author> Cybenko, G., </author> <year> 1989, </year> <title> Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals, </title> <journal> and Systems, </journal> <volume> 2, </volume> <pages> 303-314. </pages>
Reference: <author> Depree, J.D., and Swartz, C.W., </author> <year> 1988, </year> <title> Introduction to Real Analysis, </title> <publisher> New-York: John Wiley & Sons. </publisher>
Reference: <author> Golub, G.H., and van Loan, C.F., </author> <year> 1989, </year> <title> Matrix Computations, </title> <booktitle> 2nd Edition, </booktitle> <address> Baltimore: </address> <publisher> The John Hopkins Univ. Press. </publisher>
Reference-contexts: The norm k k F denotes the Frobenius matrix norm <ref> (Golub & van Loan, 1989) </ref>, defined as kAk 2 F := ij ja ij j 2 = tr AA T , where trfg denotes the trace of a matrix.
Reference: <author> Goodwin, G.C., and Mayne, D.Q., </author> <year> 1987, </year> <title> A parameter estimation perspective of continuous time model reference adaptive control. </title> <journal> Automatica, </journal> <volume> 23, </volume> <pages> 57-70. </pages>
Reference-contexts: In order to avoid parameter drift, W 1 (t), W 2 (t) are confined to the sets fW 1 : kW 1 k F M 1 g and fW 2 : kW 2 k F M 2 g respectively, through the use of a projection algorithm <ref> (Goodwin & Mayne, 1987) </ref>.
Reference: <author> Goodwin, </author> <title> G.C., and Sin, </title> <address> K.S., </address> <year> 1984, </year> <title> Adaptive Filtering, Prediction and Control, </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall. </publisher>
Reference: <author> Hale, J.K., </author> <year> 1969, </year> <title> Ordinary Differential Equations, </title> <address> New York, NY: </address> <publisher> Wiley-InterScience. </publisher>
Reference-contexts: It is worth noting that the projection modification causes the adaptive law to be discontinuous. However, the trajectory behavior on the discontinuity hypersurface is "smooth" and hence existence of a solution, in the sense of Caratheodory <ref> (Hale, 1969) </ref>, is assured. The issue of existence and uniqueness of solutions in adaptive systems is treated in detail by Polycarpou & Ioannou (1993).
Reference: <author> Hartman, E.J., Keeler, J.D., and Kowalski, J.M., </author> <year> 1990, </year> <title> Layered neural networks with guassian hidden units as universal approximations. </title> <journal> Neural Computation, </journal> <volume> 2, </volume> <pages> 210-215. </pages>
Reference: <author> Hertz, J., Krogh, A., and Palmer, R.G., </author> <year> 1991, </year> <title> Introduction to the Theory of Neural Computation, </title> <address> Redwood City, CA: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: Multilayer neural networks (Rumelhart & McClelland, 1986) are by far the most widely used neural network models. They have been applied successfully to a wide range of problems including pattern and speech recognition, image compression, signal prediction and classification <ref> (Hertz, Krogh, & Palmer, 1991) </ref>. Due to some desirable features such as local adjustment of the weights and mathematical tractability, radial basis function networks have recently also attracted considerable attention especially in applications dealing with prediction and classification (Moody & Darken, 1989; Niranjan & Fallside, 1990).
Reference: <author> Holcomb T., and Morari, M., </author> <year> 1991, </year> <title> Local training of radial basis function networks: towards solving the hidden unit problem. </title> <booktitle> Proceedings of the 1991 American Control Conference, </booktitle> <pages> pp. 2331-2336. </pages>
Reference: <author> Hopfield, J.J., </author> <year> 1984, </year> <title> Neurons with graded response have collective computational properties like those of two-state neurons. </title> <booktitle> Proc. National Acad. Science, </booktitle> <volume> 81, </volume> <pages> 3088-3092. </pages>
Reference-contexts: Remark 2.1 The interconnection of static neural networks and dynamic components shown in Figure 1 includes as special cases many of the dynamical models for neuronal activations that have been studied, as for example, the Cohen-Grossberg model (Cohen & Grossberg, 1983), and continuous-time Hopfield model <ref> (Hopfield, 1984) </ref>. For a detailed description of various neuronal activation dynamics the reader is referred to the book by Kosko (1991).
Reference: <author> Hornik, K., Stinchcombe, M., and White, H., </author> <year> 1989, </year> <title> Multilayer feedforward networks are universal approximators. </title> <booktitle> Neural Networks, </booktitle> <volume> 2, </volume> <pages> 359-366. </pages>
Reference: <author> Hunt, K.J., Sbarbaro, D., Zbikowski, R., and Gawthrop, P.J., </author> <year> 1992, </year> <title> Neural Networks for control systems: A Survey. </title> <journal> Automatica, </journal> <volume> 28, </volume> <pages> 1083-1122. </pages>
Reference: <author> Ioannou, P. A., and Datta, A., </author> <year> 1991, </year> <title> Robust adaptive control: a unified approach. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 79, </volume> <pages> 1736-1768. </pages>
Reference: <institution> Ioannou, P.A., and Kokotovic, P.V., </institution> <year> 1983, </year> <title> Adaptive Systems with Reduced Models, </title> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: This problem, which is referred to as parameter drift, is well known in the adaptive control literature <ref> (Ioannou & Kokotovic, 1983) </ref>. Parameter drift has also been encountered in empirical studies of neural network learning, where it is usually referred to as weight saturation.
Reference: <institution> Ioannou, P.A., and Tsakalis, K.S., </institution> <year> 1986, </year> <title> A robust direct adaptive controller. </title> <journal> IEEE Trans. Automatic Control, </journal> <volume> 31, </volume> <pages> pp. 1033-1043. </pages>
Reference: <author> Isidori, A., </author> <year> 1989, </year> <title> Nonlinear Control Systems, </title> <publisher> Berlin: Springer-Verlag. </publisher>
Reference-contexts: The requirement g (x) 6= 0, 8x 2 N c (0) is a controllability condition <ref> (Isidori, 1989) </ref>; i.e., for g (x (t)) = 0 the system is no longer controllable. <p> In the spirit of the feedback linearization literature <ref> (see, for example, Isidori, 1989) </ref>, these systems are in normal form and have no zero dynamics.
Reference: <author> James, D.J., </author> <year> 1971, </year> <title> Stability of a model reference control system. </title> <journal> AIAA Journal, </journal> <volume> 9 (5). </volume>
Reference: <author> Kailath, T., </author> <year> 1980, </year> <title> Linear Systems, </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall. </publisher> <address> 42 Kanellakopoulos, I., Kokotovic, P.V., </address> <note> and Morse, A.S., </note> <year> 1991, </year> <title> Systematic design of adaptive controllers for feedback linearizable systems. </title> <journal> IEEE Trans. Automatic Control, </journal> <volume> 36, </volume> <pages> 1241-1253. </pages>
Reference-contexts: In order to have a well-posed problem, it is assumed that the relative 36 degree of the reference model is equal to n. 6 This implies <ref> (Kailath, 1980) </ref> c T b = c T Ab = = c T A n2 b = 0 (4.25) If we let X = [x 1 ; x 2 ; x n ] T , then using (4.22), (4.25), it can be readily verified that the tracking error e = y <p> + b c ^ f (X; f ) f (X) + (^g (X; g ) g (X)) u c (4.30) where det (sI fl c ) = h (s), b c := [ 0 0 1 ] T and (fl c ; b c ) is in controllable canonical form <ref> (Kailath, 1980) </ref>. Once expressed in the form (4.30), then it is clear that we have the same error equation as (4.5), with the exception that in (4.5) the error e was a scalar, while in (4.30) the error " is a vector.
Reference: <editor> Kokotovic, P.V., Ed., </editor> <booktitle> 1991, Foundations of Adaptive Control, </booktitle> <address> New York, NY: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: This problem is closely related to the areas of adaptive linear control (Narendra & Annaswamy, 1989), and adaptive nonlinear control <ref> (Kokotovic, 1991) </ref>, where the uncertainty in the system is due to some unknown parameters. <p> These difficulties are also present (to a lesser degree) in adaptive nonlinear control, where two types of control schemes have been considered <ref> (Kanellakopoulos, Kokotovic & Morse, 1991) </ref> : uncertainty-constrained schemes im pose restrictions (matching conditions) on the location of the unknown parameters, while nonlinearity constrained schemes impose restrictions on the type of nonlinearities in the original system.
Reference: <author> Kosko, B., </author> <year> 1991, </year> <title> Neural Networks and Fuzzy Systems: A Dynamical Systems Approach to Machine Intelligence, </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall. </publisher>
Reference: <author> Kudva, P., and Narendra, K.S., </author> <year> 1973, </year> <title> Synthesis of an adaptive observer using Lyapunov's direct method. </title> <journal> International Journal of Control, </journal> <volume> 18, </volume> <pages> 1201-1210. </pages>
Reference: <author> Luenberger, D.G., </author> <year> 1984, </year> <title> Linear and Nonlinear programming, </title> <publisher> New-York: Addison-Wesley. </publisher>
Reference-contexts: Hence by filtering the regressors, an algebraic relationship between the output error e x and the weight estimation errors 1 , 2 is obtained. In the framework of the optimization approach <ref> (Luenberger, 1984) </ref>, adaptive laws for W 1 , W 2 are obtained by minimizing an appropriate cost functional with respect to each element of W 1 , W 2 . <p> Note that if x 0 6= 0 then the initial condition will appear in the identification model so that it gets cancelled in the error equation; this is possible because x is available for measurement. 19 RBF networks. Using the gradient projection method <ref> (Luenberger, 1984) </ref>, the following adaptive laws for continuous adjustment of the weights W 1 , W 2 are obtained: _ W 1 = &lt; fl 1 e x ~ T x W 1 ~ f 0g n f if fkW 1 k F = M 1 and e T (3.27) 8
Reference: <author> Mareels, I.M.Y., Anderson, B.D.O., Bitmead, R.R., Bodson, M., and Sastry, S.S., </author> <year> 1986, </year> <title> Revisiting the MIT rule for adaptive control. </title> <booktitle> Proc. of the 2nd IFAC Workshop on Adaptive Systems in Control and Signal Processing, </booktitle> <institution> Lund, Sweden. </institution>
Reference: <author> Miller, T.W., Sutton, S.T., III, and Werbos, P.J., Eds., </author> <year> 1990, </year> <title> Neural Networks for Control, </title> <address> Cambridge, MA: </address> <publisher> The MIT Press. </publisher>
Reference: <author> Moody, J., and Darken, C.J., </author> <year> 1989, </year> <title> Fast learning in networks of locally-tuned processing units. </title> <journal> Neural Computation, </journal> <volume> 1, </volume> <pages> 281-294. </pages>
Reference: <author> Narendra, K.S., and Annaswamy, A.M., </author> <year> 1987, </year> <title> A new adaptive law for robust adaptation with persistent excitation. </title> <journal> IEEE Trans. Automatic Control, </journal> <volume> 32, </volume> <pages> pp. 134-145. </pages>
Reference-contexts: The stability of the proposed identification scheme in the presence of modeling errors can also be achieved, under certain assumptions, by other modifications to the standard adaptive laws, such as the fixed, or switching -modification (Ioannou & Kokotovic, (1983; Ioannou & Tsakalis, 1986), "-modification <ref> (Narendra & An-naswamy, 1987) </ref>, and the dead-zone (Narendra & Annaswamy, 1989; Sastry & Bodson 1989). A comprehensive exposition to robust adaptive control theory for linear systems is given by Ioannou & Datta (1991).
Reference: <author> Narendra, K.S., and Annaswamy, A.M., </author> <year> 1989, </year> <title> Stable Adaptive Systems, </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall. </publisher>
Reference-contexts: In this paper we consider identification schemes that are based on the setting shown in Figure 4, which is known as the series-parallel configuration <ref> ( Narendra & Annaswamy, 1989) </ref>. As is common in identification procedures, we will assume that the state x (t) is bounded for all admissible bounded inputs u (t). <p> This problem is closely related to the areas of adaptive linear control <ref> (Narendra & Annaswamy, 1989) </ref>, and adaptive nonlinear control (Kokotovic, 1991), where the uncertainty in the system is due to some unknown parameters.
Reference: <author> Narendra, K.S., and Parthasarathy, K., </author> <year> 1990, </year> <title> Identification and control of dynamical systems using neural networks. </title> <journal> IEEE Trans. on Neural Networks, </journal> <volume> 1, </volume> <pages> 4-27. </pages>
Reference: <author> Narendra, K.S., and Parthasarathy, K., </author> <year> 1991, </year> <title> Gradient methods for the optimization of dynamical systems containing neural networks. </title> <journal> IEEE Trans. on Neural Networks, </journal> <volume> 2, </volume> <pages> 252-262. </pages>
Reference-contexts: Examples of such learning algorithms include the recurrent backpropagation (Pineda, 1988), the backpropagation-through-time algorithms (Werbos, 1990), the real-time recurrent learning algorithm (Williams & Zipser 1989), and the dynamic backpropagation <ref> (Narendra and Parthasarathy 1991) </ref>. The last approach, is based on the computation of sensitivity models for generalized neural networks, which combine feedforward neural networks and dynamical components in the form of stable rational transfer functions.
Reference: <author> Nijmeijer, H., and van der Schaft, A.J., </author> <year> 1990, </year> <title> Nonlinear Dynamical Control Systems, </title> <address> New York, NY: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: However, as also pointed out by Sanner & Slotine (1992), studying the problem in discrete-time has several drawbacks. It is known that discretization of physical continuous-time nonlinear systems yields highly complex discrete-time models <ref> (Nijmeijer & van der Schaft, 1990) </ref>. More precisely, discretization of affine (in the input) continuous-time systems results in non-affine discrete-time models which are almost impossible to analyze. <p> First, most of the systems encountered in engineering are, by nature or design, affine systems. Secondly, most of the nonlinear control techniques, including feedback linearization, are developed for affine systems. Finally, we note that non-affine systems can be converted to affine systems by passing the input through integrators <ref> (Nijmeijer & van der Schaft, 1990) </ref>. This procedure is known as dynamic extension.
Reference: <author> Niranjan, M., and Fallside, F., </author> <year> 1990, </year> <title> Neural networks and radial basis functions for classifying static speech patterns. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 4, </volume> <pages> 275-289. </pages>
Reference: <author> Park, J., and Sandberg, I.W., </author> <year> 1990, </year> <title> Universal approximation using radial-basis-function networks. </title> <journal> Neural Computation, </journal> <volume> 3, </volume> <pages> 246-257. </pages>
Reference: <author> Parks, </author> <title> P.C., 1966, Lyapunov redesign of model reference adaptive control systems. </title> <journal> IEEE Trans. on Automatic Control, </journal> <volume> 11, </volume> <pages> 362-367. </pages>
Reference: <author> Pinenda, F.J., </author> <year> 1988, </year> <title> Generalization of back propagation to recurrent networks. </title> <journal> Phys. Rev. Lett., </journal> <volume> 59, </volume> <pages> 2229-2232. </pages>
Reference: <author> Poggio, T., and Girosi, F., </author> <year> 1990, </year> <title> Networks for approximation and learning. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 78, </volume> <pages> 1481-1497. </pages>
Reference: <author> Polycarpou, M.M., and Ioannou, P. A., </author> <year> 1992, </year> <title> Learning and convergence analysis of neural-type structured networks. </title> <journal> IEEE Trans. on Neural Networks, </journal> <volume> 3, </volume> <pages> 39-50. </pages>
Reference: <author> Polycarpou, M.M., and Ioannou, P. A., </author> <year> 1993, </year> <title> On the existence and uniqueness of solutions in adaptive control systems. </title> <journal> IEEE Trans. on Automatic Control, </journal> <volume> 38, </volume> <pages> pp. 474-479. </pages> <note> 43 Praly, </note> <author> L., Bastin, G., Pomet, J.-B., and Jiang, Z.P., </author> <year> 1991, </year> <title> Adaptive stabilization of nonlinear systems, </title> <booktitle> in Foundations of Adaptive Control, </booktitle> <address> P.V. Kokotovic, </address> <publisher> Ed., </publisher> <pages> (pp. 347-433), </pages> <address> Berlin: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: One potential problem is the fact that u s , as given by (4.14), is discontinuous with respect to x. Discontinuous control laws not only create problems of existence and uniqueness of solutions <ref> (Polycarpou & Ioannou, 1993) </ref>, but are also known to display chattering phenomena (Utkin, 1978), and excite high frequency unmodeled dynamics (Slotine & Li, 1991). One way of avoiding these problems is to approximate the discontinuous terms in (4.14) by sigmoidal functions.
Reference: <editor> Rumelhart, D., McClelland, J.L., </editor> <booktitle> and the PDP Research Group, 1986, Parallel Distributed Processing: Exploration in the Microstructure of Cognition. Volume 1: Foundations, </booktitle> <address> Cambridge, MA: </address> <publisher> The MIT Press. </publisher>
Reference-contexts: In our analysis we consider two types of neural network architectures: 1) multilayer neural network models with sigmoidal nonlinearities and 2) radial basis function networks with Gaussian activation functions. Multilayer neural networks <ref> (Rumelhart & McClelland, 1986) </ref> are by far the most widely used neural network models. They have been applied successfully to a wide range of problems including pattern and speech recognition, image compression, signal prediction and classification (Hertz, Krogh, & Palmer, 1991).
Reference: <author> Sanner, R.M., and Slotine, J.-J.E., </author> <year> 1992, </year> <title> Gaussian networks for direct adaptive control. </title> <journal> IEEE Trans. on Neural Networks, </journal> <volume> 3, </volume> <pages> 837-863. </pages>
Reference: <author> Sastry, S., and Bodson, M., </author> <year> 1989, </year> <title> Adaptive Control: Stability, Convergence and Robustness, </title> <address> Engle-wood Cliffs, NJ: </address> <publisher> Prentice-Hall. </publisher>
Reference-contexts: Hence from (3.10) we have that _e x 2 L 1 . Since e x 2 L 2 " L 1 and _e x 2 L 1 , using Barbalat's Lemma <ref> (Sastry & Bodson, 1989) </ref> we conclude that lim t!1 e x (t) = 0. Now, using the boundedness of ~(t) and the convergence of e x (t) to zero, we have that _ 1 = _ W 1 also converges to zero. <p> In general, this is far 29 from being a trivial task; adaptive control theorists are aware, from the linear case, that this is one of the major problems with indirect-type of adaptive control schemes <ref> (Sastry & Bodson, 1989) </ref>.
Reference: <author> Sastry, S., and Isidori, A., </author> <year> 1989, </year> <title> Adaptive control of linearizable systems. </title> <journal> IEEE Trans. Automatic Control, </journal> <volume> 34, </volume> <pages> 1123-1131. </pages>
Reference-contexts: Hence from (3.10) we have that _e x 2 L 1 . Since e x 2 L 2 " L 1 and _e x 2 L 1 , using Barbalat's Lemma <ref> (Sastry & Bodson, 1989) </ref> we conclude that lim t!1 e x (t) = 0. Now, using the boundedness of ~(t) and the convergence of e x (t) to zero, we have that _ 1 = _ W 1 also converges to zero. <p> In general, this is far 29 from being a trivial task; adaptive control theorists are aware, from the linear case, that this is one of the major problems with indirect-type of adaptive control schemes <ref> (Sastry & Bodson, 1989) </ref>.
Reference: <author> Slotine, J.-J.E., and Li, W., </author> <year> 1991, </year> <title> Applied Nonlinear Control, </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall. </publisher>
Reference-contexts: The sliding control methodology (Utkin, 1978), which is sometimes used as an alternative to adaptive control in systems with parametric uncertainty, represents a rather simple approach to robust control with applications in robot manipulators, power systems, and others <ref> (Slotine & Li, 1991) </ref>. The principal idea behind using sliding control in the context of this paper, is very similar to that of the projection algorithm, which was introduced earlier; in both cases the objective is to restrict a signal or a parameter value from entering an undesired region. <p> Another drawback of the sliding control design is the extremely high control effort that it requires, in general. Typically, high control activity is not only expensive but it may also excite any unmodeled dynamics of the system <ref> (Slotine & Li, 1991) </ref>. In fact, sliding control in this context, is analogous to high-gain feedback, which is a classical control tool for reducing the effects of unknown disturbances (Young, Kokotovic & Utkin, 1977). <p> Discontinuous control laws not only create problems of existence and uniqueness of solutions (Polycarpou & Ioannou, 1993), but are also known to display chattering phenomena (Utkin, 1978), and excite high frequency unmodeled dynamics <ref> (Slotine & Li, 1991) </ref>. One way of avoiding these problems is to approximate the discontinuous terms in (4.14) by sigmoidal functions.
Reference: <author> Sontag, E.D., </author> <year> 1991, </year> <title> Feedback stabilization using two-hidden-layer nets. </title> <booktitle> Proc. American Control Conference, </booktitle> <pages> pp. 815-820. </pages>
Reference-contexts: Therefore multilayer neural networks with as few as two layers satisfy (N1), (N2), and hence can be used to model continuous nonlinearities in dynamical systems. Furthermore, it has been shown that three-layer neural networks (i.e., with two hidden layers), are capable of approximating discontinuous functions <ref> (Sontag, 1991) </ref>.
Reference: <author> Sontag, E.D., </author> <year> 1993, </year> <title> Some topics in neural networks and control. </title> <type> Tech. Report LS93-02, </type> <institution> Siemens Corporate Research, Inc., Princeton, NJ. </institution>
Reference: <author> Taylor, D.G., Kokotovic, P.V., Marino, R., and Kanellakopoulos, I., </author> <year> 1989, </year> <title> Adaptive regulation of nonlinear systems with unmodelled dynamics. </title> <journal> IEEE Trans. Automatic Control, </journal> <volume> 34, </volume> <pages> 405-412. </pages>
Reference: <author> Utkin, V.I., </author> <year> 1978, </year> <title> Sliding Models and their Application to Variable Structure Systems, </title> <address> Moscow: </address> <publisher> MIR Publishers. </publisher>
Reference-contexts: According to this approach, an auxiliary component is added in the control law, whose purpose is to introduce a sliding control term for confining x (t) to the region X . The sliding control methodology <ref> (Utkin, 1978) </ref>, which is sometimes used as an alternative to adaptive control in systems with parametric uncertainty, represents a rather simple approach to robust control with applications in robot manipulators, power systems, and others (Slotine & Li, 1991). <p> One potential problem is the fact that u s , as given by (4.14), is discontinuous with respect to x. Discontinuous control laws not only create problems of existence and uniqueness of solutions (Polycarpou & Ioannou, 1993), but are also known to display chattering phenomena <ref> (Utkin, 1978) </ref>, and excite high frequency unmodeled dynamics (Slotine & Li, 1991). One way of avoiding these problems is to approximate the discontinuous terms in (4.14) by sigmoidal functions.
Reference: <author> Werbos, P.J., </author> <year> 1990, </year> <title> Backpropagation through time: what it does and how to do it. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 78, </volume> <pages> 1550-1560. </pages>
Reference-contexts: Most of the proposed neural control methods rely on the gradient methodology and involve the computation of partial derivatives, or sensitivity functions. Examples of such learning algorithms include the recurrent backpropagation (Pineda, 1988), the backpropagation-through-time algorithms <ref> (Werbos, 1990) </ref>, the real-time recurrent learning algorithm (Williams & Zipser 1989), and the dynamic backpropagation (Narendra and Parthasarathy 1991). The last approach, is based on the computation of sensitivity models for generalized neural networks, which combine feedforward neural networks and dynamical components in the form of stable rational transfer functions.
Reference: <author> Whitaker, H.P., Yamron, J., and Kezer, A., </author> <year> 1958, </year> <title> Design of model-reference adaptive control systems for aircraft. </title> <type> Tech. Report R-164, </type> <institution> Instrumentation Lab., MIT, </institution> <address> Cambridge, MA. </address>
Reference-contexts: Interestingly, methods based on straightforward application of optimization techniques, such as sensitivity models (Cruz, 1973) and the M.I.T. rule <ref> (Whitaker, Yamron, & Kezer, 1958) </ref> that dominated the early adaptive linear control literature, exhibited similar stability problems. The fact that, even for linear 3 systems, such methods can lead to instability was shown by Parks (1966) and James (1971); see also Mareels et al. (1986).
Reference: <author> White, D.A., and Sofge, D.A., Eds., </author> <year> 1992, </year> <title> Handbook of Intelligent Control, </title> <address> New York, NY: </address> <publisher> Van Nostrand Reinhold. </publisher>
Reference: <author> Williams, R.J., and Zipser, D., </author> <year> 1989, </year> <title> A learning algorithm for continually running fully recurrent neural networks. </title> <journal> Neural Computation, </journal> <volume> 1, </volume> <pages> 270-280. </pages>
Reference-contexts: Most of the proposed neural control methods rely on the gradient methodology and involve the computation of partial derivatives, or sensitivity functions. Examples of such learning algorithms include the recurrent backpropagation (Pineda, 1988), the backpropagation-through-time algorithms (Werbos, 1990), the real-time recurrent learning algorithm <ref> (Williams & Zipser 1989) </ref>, and the dynamic backpropagation (Narendra and Parthasarathy 1991). The last approach, is based on the computation of sensitivity models for generalized neural networks, which combine feedforward neural networks and dynamical components in the form of stable rational transfer functions.
Reference: <author> Young, K.D., Kokotovic, P.V., and Utkin, V.I., </author> <year> 1977, </year> <title> A singular perturbation analysis of high-gain feedback systems. </title> <journal> IEEE Trans. Automatic Control, </journal> <volume> 22, </volume> <pages> pp. 931-938. 44 </pages>
Reference-contexts: Typically, high control activity is not only expensive but it may also excite any unmodeled dynamics of the system (Slotine & Li, 1991). In fact, sliding control in this context, is analogous to high-gain feedback, which is a classical control tool for reducing the effects of unknown disturbances <ref> (Young, Kokotovic & Utkin, 1977) </ref>.
References-found: 59

