URL: ftp://ftp.cs.rochester.edu/pub/u/jag/cvs95.ps.gz
Refering-URL: http://www.cs.rochester.edu/u/jag/publications.html
Root-URL: 
Email: fjag,nelsong@cs.rochester.edu  
Title: Visual Space Task Specification, Planning and Control  
Author: Martin Jagersand, Randal Nelson 
Web: http://www.cs.rochester.edu/u/fjag,nelsong/  
Address: Rochester, Rochester, NY 14627  
Affiliation: Department of Computer Science, University of  
Note: In Proc of IEEE Int. Symp. on Computer Vision, 1995, p. 521-526  
Abstract: Robot manipulators, some thirty years after their commercial introduction, have found widespread application in structured industrial environments, performing, for instance, repetitive tasks in an assembly line. Successful application in unstructured environments however has proven much harder. Yet there are many such tasks where robots would be useful. We present a promising approach to visual (and more general sensory) robot control, that does not require modeling of robot transfer functions or the use of absolute world coordinate systems, and thus is suitable for use in unstructured environments. Our approach codes actions and tasks in terms of desired general perceptions rather than motor sequences. We argue that our vision space approach is particularly suited for easy teaching/programming of a robot. For instance a task can be taught by supplying an image sequence illustrating it. The resulting robot behavior is robust to changes in the environment, dynamically adjusting the motor control rules in response to environmental variation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Weiss L. E. Sanderson A. C. Neumann C. P. </author> <title> "Dynamic Sensor-Based Control of Robots with Visual Feedback" J. of Robotics and Aut. </title> <publisher> v. </publisher> <month> RA-3 </month> <year> 1987 </year>
Reference-contexts: 1 Introduction Using visual input to control robot manipulators allows more flexible and robust robot behaviors than traditional position based control. Recently "uncalibrated" visual servoing control has been demonstrated in a variety of settings e.g. <ref> [1, 4, 2, 6, 7, 10, 11, 14, 15, 18] </ref> 1 . Visual models suitable for specifying simple visual alignments have also been studied [19, 9, 8]. In [3] Feddema asks why visual servoing has not yet been adopted in industry. <p> The changes in visual appearance are recorded in a perception or feature vector y = (y 1 : : : y m ). Visual features can be drawn from a large class of visual measurements <ref> [1, 10] </ref>, but we have found that the ones which can be represented as points or point vectors in camera space are suitable, since they yield smooth transfer functions f [12]. We track features such as boundary discontinuities (lines,corners) and surface markings. <p> In our active framework the agent also knows along which direction x the system changes. This leaves only a one dimensional search space along y predicted = Jx + y k ; 2 <ref> [0; 1] </ref> in feature space. Note however that we cannot boldly constrain the tracker output to this space. That would take away the innovation term in our model updating, and the system would no longer adapt its model to a changing environment.
Reference: [2] <author> Feddema J. T. Lee G. C. S. </author> <title> "Adaptive Image Feature Prediction and Control for Visual Tracking with a Hand-Eye Coordinated Camera" IEEE Tr. </title> <journal> on Systems, Man and Cyber., </journal> <volume> v 20, no 5 1990 </volume>
Reference-contexts: 1 Introduction Using visual input to control robot manipulators allows more flexible and robust robot behaviors than traditional position based control. Recently "uncalibrated" visual servoing control has been demonstrated in a variety of settings e.g. <ref> [1, 4, 2, 6, 7, 10, 11, 14, 15, 18] </ref> 1 . Visual models suitable for specifying simple visual alignments have also been studied [19, 9, 8]. In [3] Feddema asks why visual servoing has not yet been adopted in industry. <p> x k ), valid around the current system configuration x k , and described by the "image"[14] or visual-motor Jacobian defined as (J j;i )(x k ) = @x i The image Jacobian not only relates visual changes to motor changes, as has been previously exploited in visual feedback control <ref> [2] </ref> but highly constrains the possible visual changes to the set of possible solutions y k+1 3 Vectors written bold, scalars plain and matrices capitalized. 1 In Proc of IEEE Int. Symp. on Computer Vision, 1995, p. 521-526 of y k+1 = J x + y k . <p> Partial modeling of the viewing geometry using an AR-MAX model and estimating only one or a few parameters (e.g. depth) has also been tried <ref> [2, 7] </ref>. This however restricts the camera-robot configurations and environments to structured, easy to model settings. Estimation of the full Jacobian was solved mathematically by Broyden in the 60's [20], and later rediscovered in robotics by [11, 10, 15].
Reference: [3] <author> Feddema J. T. </author> <title> "Visual Servoing: A Technology In Search of an Application" Visual Servoing: Achievements, Applications and Open problems, </title> <editor> eds Hager, Hutchinson, </editor> <booktitle> Workshop M5, </booktitle> <month> ICRA </month> <year> 1994. </year>
Reference-contexts: Recently "uncalibrated" visual servoing control has been demonstrated in a variety of settings e.g. [1, 4, 2, 6, 7, 10, 11, 14, 15, 18] 1 . Visual models suitable for specifying simple visual alignments have also been studied [19, 9, 8]. In <ref> [3] </ref> Feddema asks why visual servoing has not yet been adopted in industry.
Reference: [4] <author> Conkie A. Chongstitvatana P. </author> <title> "An Uncalibrated Stereo Visual Servo System" DAI TR #475, </title> <institution> U of Edinburgh 1990 </institution>
Reference-contexts: 1 Introduction Using visual input to control robot manipulators allows more flexible and robust robot behaviors than traditional position based control. Recently "uncalibrated" visual servoing control has been demonstrated in a variety of settings e.g. <ref> [1, 4, 2, 6, 7, 10, 11, 14, 15, 18] </ref> 1 . Visual models suitable for specifying simple visual alignments have also been studied [19, 9, 8]. In [3] Feddema asks why visual servoing has not yet been adopted in industry. <p> More details can be found in [10]. An estimate to the image Jacobian can be obtained by physically executing a set of calibration movements <ref> [4, 9, 19] </ref> along the basis direction of motor space e i as ^ J (x; d) = (f (x+d 1 e 1 )f (x); : : : ; f (x+d n e n )f (x))D 1 , where D = diag (d); d 2 &lt; n . <p> Control The active agent specifies its actions in terms of desired perceptions y fl . We need a control system capable of turning these goal perceptions into motor actions; in our system joint movements. A simple control law, in some form occuring in most visual servoing research (e.g. <ref> [4, 19, 15] </ref>) is where K is a gain matrix.
Reference: [5] <author> Curwen R. Blake A. </author> <title> "Dynamic Contours: Real time active splines" In Active Vision Ed Blake, </title> <publisher> Yuille MIT Press 1992. </publisher>
Reference-contexts: Real time visual feature trackers of three different kinds are used to obtain visual information. The Oxford snakes <ref> [5] </ref> are used to track surface discontinuities.
Reference: [6] <author> Wijesoma S. W. Wolfe D. F. H. Richards R. J. </author> <title> "Eye-to-Hand Coordination for vision guided Robot Control Applications" Int. </title> <journal> J. of Robotics Research, </journal> <volume> v 12 No 1 1993 </volume>
Reference-contexts: 1 Introduction Using visual input to control robot manipulators allows more flexible and robust robot behaviors than traditional position based control. Recently "uncalibrated" visual servoing control has been demonstrated in a variety of settings e.g. <ref> [1, 4, 2, 6, 7, 10, 11, 14, 15, 18] </ref> 1 . Visual models suitable for specifying simple visual alignments have also been studied [19, 9, 8]. In [3] Feddema asks why visual servoing has not yet been adopted in industry.
Reference: [7] <author> Papanikolopoulos N. P. Khosla P. K. </author> <title> "Adaptive Robotic Visual Tracking: </title> <journal> Theory and Experiments" IEEE Tr. on Aut. </journal> <note> Control Vol 38 no 3 1993 </note>
Reference-contexts: 1 Introduction Using visual input to control robot manipulators allows more flexible and robust robot behaviors than traditional position based control. Recently "uncalibrated" visual servoing control has been demonstrated in a variety of settings e.g. <ref> [1, 4, 2, 6, 7, 10, 11, 14, 15, 18] </ref> 1 . Visual models suitable for specifying simple visual alignments have also been studied [19, 9, 8]. In [3] Feddema asks why visual servoing has not yet been adopted in industry. <p> Partial modeling of the viewing geometry using an AR-MAX model and estimating only one or a few parameters (e.g. depth) has also been tried <ref> [2, 7] </ref>. This however restricts the camera-robot configurations and environments to structured, easy to model settings. Estimation of the full Jacobian was solved mathematically by Broyden in the 60's [20], and later rediscovered in robotics by [11, 10, 15].
Reference: [8] <author> Harris M. </author> <title> "Vision Guided Part Alignment with Degraded Data" DAI TR #615, </title> <address> Edinburgh 1993 </address>
Reference-contexts: Recently "uncalibrated" visual servoing control has been demonstrated in a variety of settings e.g. [1, 4, 2, 6, 7, 10, 11, 14, 15, 18] 1 . Visual models suitable for specifying simple visual alignments have also been studied <ref> [19, 9, 8] </ref>. In [3] Feddema asks why visual servoing has not yet been adopted in industry.
Reference: [9] <author> Hollinghurst N. Cipolla R. </author> <title> "Uncalibrated Stereo Hand-Eye Coordination" Brit. </title> <booktitle> Machine Vision Conf 1993 </booktitle>
Reference-contexts: Recently "uncalibrated" visual servoing control has been demonstrated in a variety of settings e.g. [1, 4, 2, 6, 7, 10, 11, 14, 15, 18] 1 . Visual models suitable for specifying simple visual alignments have also been studied <ref> [19, 9, 8] </ref>. In [3] Feddema asks why visual servoing has not yet been adopted in industry. <p> More details can be found in [10]. An estimate to the image Jacobian can be obtained by physically executing a set of calibration movements <ref> [4, 9, 19] </ref> along the basis direction of motor space e i as ^ J (x; d) = (f (x+d 1 e 1 )f (x); : : : ; f (x+d n e n )f (x))D 1 , where D = diag (d); d 2 &lt; n .
Reference: [10] <author> Jagersand M. Nelson R. </author> <title> Adaptive Differential Visual Feedback for uncalibrated hand-eye coordination and motor control TR# 579, </title> <type> U. </type> <institution> of Rochester 1994. </institution>
Reference-contexts: 1 Introduction Using visual input to control robot manipulators allows more flexible and robust robot behaviors than traditional position based control. Recently "uncalibrated" visual servoing control has been demonstrated in a variety of settings e.g. <ref> [1, 4, 2, 6, 7, 10, 11, 14, 15, 18] </ref> 1 . Visual models suitable for specifying simple visual alignments have also been studied [19, 9, 8]. In [3] Feddema asks why visual servoing has not yet been adopted in industry. <p> The changes in visual appearance are recorded in a perception or feature vector y = (y 1 : : : y m ). Visual features can be drawn from a large class of visual measurements <ref> [1, 10] </ref>, but we have found that the ones which can be represented as points or point vectors in camera space are suitable, since they yield smooth transfer functions f [12]. We track features such as boundary discontinuities (lines,corners) and surface markings. <p> More details can be found in <ref> [10] </ref>. <p> This however restricts the camera-robot configurations and environments to structured, easy to model settings. Estimation of the full Jacobian was solved mathematically by Broyden in the 60's [20], and later rediscovered in robotics by <ref> [11, 10, 15] </ref>. <p> Intuitively both these techniques serve to synchronize actions with model acquisition, so that the actions never run ahead too far before the local model has been adapted to the new environment. In an extensive evaluation of this adaptive visual control and servoing schema <ref> [10] </ref> we have found that positioning of a 6 axis PUMA 762 arm is 5 times more precise under visual control, than under joint control. We also found that the adaptive controller is very robust. <p> The low level trajectory generator takes this coarse sequence of goals and for convergence and error control reasons <ref> [10] </ref> breaks it down further along straight lines in &lt; m connecting the subgoals y fl k . Real time visual feature trackers of three different kinds are used to obtain visual information. The Oxford snakes [5] are used to track surface discontinuities.
Reference: [11] <author> Jagersand M. </author> <title> "Perception level control for uncalibrated hand-eye coordination and motor actions" Areapaper, </title> <note> University of Rochester May 1994. Forthcoming as TR. </note>
Reference-contexts: 1 Introduction Using visual input to control robot manipulators allows more flexible and robust robot behaviors than traditional position based control. Recently "uncalibrated" visual servoing control has been demonstrated in a variety of settings e.g. <ref> [1, 4, 2, 6, 7, 10, 11, 14, 15, 18] </ref> 1 . Visual models suitable for specifying simple visual alignments have also been studied [19, 9, 8]. In [3] Feddema asks why visual servoing has not yet been adopted in industry. <p> Task coding and planning is done in a visual 2 space, rather than in the robot motor or 3D world space. fl Support was provided by ONR grant N00014-93-I-0221, Sverige-Amerika Stiftelsen and the Fulbright Commission. 1 For a review of this work we direct the reader to <ref> [11] </ref> or [14]. 2 Think camera space, although more general visual features are also used. 2. When composing tasks we switch between different types of low level servoing, controlling different DOF's, depending on the nature of the subtask be ing solved. 3. <p> This however restricts the camera-robot configurations and environments to structured, easy to model settings. Estimation of the full Jacobian was solved mathematically by Broyden in the 60's [20], and later rediscovered in robotics by <ref> [11, 10, 15] </ref>.
Reference: [12] <author> Jagersand M. </author> <title> "Perception level control for uncalibrated hand-eye coordination and motor actions" Thesis proposal, </title> <institution> University of Rochester, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: Visual features can be drawn from a large class of visual measurements [1, 10], but we have found that the ones which can be represented as points or point vectors in camera space are suitable, since they yield smooth transfer functions f <ref> [12] </ref>. We track features such as boundary discontinuities (lines,corners) and surface markings. Redundant visual perceptions (m n) are desirable as they are used to constrain the raw visual sensory information.
Reference: [13] <author> Kutulakos K. Jagersand M. </author> <title> "Exploring objects by purposive viewpoint control and invariant-based hand-eye coordination" Workshop on vision for robots In conjunction with IROS 1995. </title> <note> To appear. </note>
Reference-contexts: For instance identifying the three lines forming a corner on a rectangular box in two cameras, or two poses, gives an Euclidean base P . Using more views improves the accuracy of the base <ref> [13] </ref>. Often an incomplete base is enough (i.e. to move up we only need to identify a vertical line near the robot in each of the cameras).
Reference: [14] <author> Corke P. I. </author> <title> High-Performance Visual Closed-Loop Robot Control PhD thesis U of Melbourne 1994. </title>
Reference-contexts: 1 Introduction Using visual input to control robot manipulators allows more flexible and robust robot behaviors than traditional position based control. Recently "uncalibrated" visual servoing control has been demonstrated in a variety of settings e.g. <ref> [1, 4, 2, 6, 7, 10, 11, 14, 15, 18] </ref> 1 . Visual models suitable for specifying simple visual alignments have also been studied [19, 9, 8]. In [3] Feddema asks why visual servoing has not yet been adopted in industry. <p> Task coding and planning is done in a visual 2 space, rather than in the robot motor or 3D world space. fl Support was provided by ONR grant N00014-93-I-0221, Sverige-Amerika Stiftelsen and the Fulbright Commission. 1 For a review of this work we direct the reader to [11] or <ref> [14] </ref>. 2 Think camera space, although more general visual features are also used. 2. When composing tasks we switch between different types of low level servoing, controlling different DOF's, depending on the nature of the subtask be ing solved. 3.
Reference: [15] <author> Hosoda K. Asada M. </author> <title> "Versatile Visual Servoing without Knowledge of True Jacobian" Proc of IROS Aug 1994. </title>
Reference-contexts: 1 Introduction Using visual input to control robot manipulators allows more flexible and robust robot behaviors than traditional position based control. Recently "uncalibrated" visual servoing control has been demonstrated in a variety of settings e.g. <ref> [1, 4, 2, 6, 7, 10, 11, 14, 15, 18] </ref> 1 . Visual models suitable for specifying simple visual alignments have also been studied [19, 9, 8]. In [3] Feddema asks why visual servoing has not yet been adopted in industry. <p> This however restricts the camera-robot configurations and environments to structured, easy to model settings. Estimation of the full Jacobian was solved mathematically by Broyden in the 60's [20], and later rediscovered in robotics by <ref> [11, 10, 15] </ref>. <p> Control The active agent specifies its actions in terms of desired perceptions y fl . We need a control system capable of turning these goal perceptions into motor actions; in our system joint movements. A simple control law, in some form occuring in most visual servoing research (e.g. <ref> [4, 19, 15] </ref>) is where K is a gain matrix. <p> Furthermore highly redundant systems allow us to detect outliers in y, and deal with partial occlusion. 4 Visual space task representation, planning and control To date work in image/feature space visual control has demonstrated low level servoing behaviors, achieving a single visual alignment, eg. <ref> [15, 19] </ref>. A remaining principal challenge is how to specify complex tasks in visual space, divide them up into subtasks, plan trajectories in visual space, and select different primitive visual servoing behaviors and visual goals.
Reference: [16] <author> Pook P., Ballard D. H., "Teleassistance: </author> <booktitle> Contextual guidance for autonomous manipulation" Proc. of AAAI, </booktitle> <month> Aug </month> <year> 1994. </year>
Reference-contexts: Local viewing geometry models, obtained during the course of a manipulation, are used to specify parts of the task not describable directly in terms of visual alignments. 4. The product of our research is a vision based interface for uncalibrated teleassistance <ref> [16] </ref> and vision based robot programming. 5. We demonstrate this system by solving several complex real world tasks in unstructured environ ments. 2 Viewing model An active vision agent has control over its actions, and can watch the results of an action by observing the changes in visual appearance. <p> The user specifies the changes he wishes to bring about in the world by clicking on the objects, and pointing out their desired locations, and alignment features. If this is done interactively we have a very low bandwidth telemanipulation system, isolating the user from the difficult <ref> [16] </ref> low level control prob In Proc of IEEE Int. Symp. on Computer Vision, 1995, p. 521-526 lems. When it is done off line, we have a user friendly programming interface.
Reference: [17] <author> Fuentes O. Nelson R. </author> <title> Morphing hands and virtual tools TR# 551, </title> <institution> Dept of CS, U. of Rochester 1994. </institution>
Reference-contexts: The latter position remains fixed in the eye-in-hand setup and thus need not be tracked. We perform the hand manipulations as a canned motion sequence, specified in hand space, without visual feedback. We are currently experimenting with hand manipulations under visual guidance, using hand primitives provided in <ref> [17] </ref>. 5.3 12 DOF control of a non-rigid foam beam High DOF control problems involving manipulation of non rigid objects are very hard to solve with traditional model based robot control paradigms.
Reference: [18] <author> B. H. Yoshimi P. K. Allen "Active, Uncalibrated Visual Servoing" ICRA, </author> <year> 1995, </year> <note> submitted. </note>
Reference-contexts: 1 Introduction Using visual input to control robot manipulators allows more flexible and robust robot behaviors than traditional position based control. Recently "uncalibrated" visual servoing control has been demonstrated in a variety of settings e.g. <ref> [1, 4, 2, 6, 7, 10, 11, 14, 15, 18] </ref> 1 . Visual models suitable for specifying simple visual alignments have also been studied [19, 9, 8]. In [3] Feddema asks why visual servoing has not yet been adopted in industry.
Reference: [19] <author> Hager G. </author> <title> "Calibration-Free Visual Control Using Projective Invariance" In Proc. </title> <booktitle> of 5:th ICCV 1995. </booktitle>
Reference-contexts: Recently "uncalibrated" visual servoing control has been demonstrated in a variety of settings e.g. [1, 4, 2, 6, 7, 10, 11, 14, 15, 18] 1 . Visual models suitable for specifying simple visual alignments have also been studied <ref> [19, 9, 8] </ref>. In [3] Feddema asks why visual servoing has not yet been adopted in industry. <p> More details can be found in [10]. An estimate to the image Jacobian can be obtained by physically executing a set of calibration movements <ref> [4, 9, 19] </ref> along the basis direction of motor space e i as ^ J (x; d) = (f (x+d 1 e 1 )f (x); : : : ; f (x+d n e n )f (x))D 1 , where D = diag (d); d 2 &lt; n . <p> Control The active agent specifies its actions in terms of desired perceptions y fl . We need a control system capable of turning these goal perceptions into motor actions; in our system joint movements. A simple control law, in some form occuring in most visual servoing research (e.g. <ref> [4, 19, 15] </ref>) is where K is a gain matrix. <p> Furthermore highly redundant systems allow us to detect outliers in y, and deal with partial occlusion. 4 Visual space task representation, planning and control To date work in image/feature space visual control has demonstrated low level servoing behaviors, achieving a single visual alignment, eg. <ref> [15, 19] </ref>. A remaining principal challenge is how to specify complex tasks in visual space, divide them up into subtasks, plan trajectories in visual space, and select different primitive visual servoing behaviors and visual goals.
Reference: [20] <author> Broyden C. G. </author> <title> Mathematics of Computation, v 19 p 577-593, </title> <year> 1965. </year>
Reference-contexts: This however restricts the camera-robot configurations and environments to structured, easy to model settings. Estimation of the full Jacobian was solved mathematically by Broyden in the 60's <ref> [20] </ref>, and later rediscovered in robotics by [11, 10, 15].
Reference: [21] <author> Fletcher R. </author> <title> Practical Methods of Optimization Chichester, second ed. </title> <year> 1987 </year>
Reference: [22] <author> Gustafsson I. </author> <note> Tillampad Optimeringslara Komp., </note> <institution> Inst. for Inf. Beh., </institution> <note> Chalmers 1991. </note>
Reference: [23] <author> Dahlquist G. Bjorck A. </author> <title> Numerical Methods Second Ed, </title> <publisher> Prentice Hall, </publisher> <year> 1991, </year> <type> preprint. </type>
Reference-contexts: Dynamic stability of the robot at this low sampling frequency is achieved by a secondary set of high bandwidth joint feedback controllers. This popular controller however has two major de ficiencies. Even for a convex problem (f T f convex) it is not guaranteed to be convergent <ref> [23] </ref>, and in the case of a non convex problem it often does not converge at all [23]. Previous work has overcome this problem by only making single, small distance moves within a relatively smooth and well scaled region of f. <p> This popular controller however has two major de ficiencies. Even for a convex problem (f T f convex) it is not guaranteed to be convergent <ref> [23] </ref>, and in the case of a non convex problem it often does not converge at all [23]. Previous work has overcome this problem by only making single, small distance moves within a relatively smooth and well scaled region of f.
References-found: 23

