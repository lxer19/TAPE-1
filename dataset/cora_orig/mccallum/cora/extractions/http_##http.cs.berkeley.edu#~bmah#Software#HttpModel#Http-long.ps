URL: http://http.cs.berkeley.edu/~bmah/Software/HttpModel/Http-long.ps
Refering-URL: http://http.cs.berkeley.edu/~bmah/Software/HttpModel/
Root-URL: http://www.cs.berkeley.edu
Email: bmah@CS.Berkeley.EDU  
Phone: Tel: (510) 642-8905  
Title: An Empirical Model of HTTP Network Traffic 1 An Empirical Model of HTTP Network Traffic  
Author: Bruce A. Mah 
Keyword: World Wide Web, HTTP, traffic model, traffic measurements, workload, Internet.  
Address: Berkeley, CA 94720-1776  
Affiliation: The Tenet Group Computer Science Division University of California at Berkeley  
Abstract: The workload of the global Internet is dominated by the Hypertext Transfer Protocol (HTTP), an application protocol used by World Wide Web clients and servers. Simulation studies of this environment will require a model of the traffic patterns of the World Wide Web, in order to investigate the performance aspects of this increasingly popular application. We have developed an empirical model of network traffic produced by HTTP. Instead of relying on server or client logs, our approach is based on gathering packet traces of HTTP network conversations. Through traffic analysis, we have determined statistics and distributions for higher-level quantities such as the size of HTTP items retrieved, the number of items per Web page, think time, and user browsing behavior. These quantities form a model can then be used by simulations to mimic World Wide Web network applications in wide-area IP internetworks. 
Abstract-found: 1
Intro-found: 1
Reference: [Arlitt96] <author> Martin F. Arlitt and Carey L. Williamson. </author> <title> Web server workload characterization: The search for invariants. </title> <booktitle> In Proceedings of the ACM SIGMETRICS Conference on Measure ment & Modeling of Computer Systems, </booktitle> <pages> pages 126137, </pages> <address> Philadelphia, PA, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: In some sense this approach is the easiest to take, because the machinery for collecting model data already exists and, in fact, the data is very likely being collected anyway. Indeed, for some studies, such as [Mogul95] and <ref> [Arlitt96] </ref>, it is appropriate to use a characterization of a stream of HTTP requests arriving at a Web server. However, there are two principal drawbacks to this approach. One important disadvantage of using server logs is that they cannot easily capture user access patterns across multiple Web servers. <p> Originally, it was used to describe the frequency of words in texts, as well as other human related phenomena [Zipf49]. More recently, this distribution has been applied to the frequency that WWW docu ments are accessed <ref> [Crovella96, Arlitt96] </ref>. It would seem reasonable to apply Zipfs Law, or some other heavy-tailed distribution to the access patterns of servers as well, but confirmation of this assertion requires a larger data sample than we have available.
Reference: [Berners-Lee95] <author> Tim Berners-Lee and Daniel W. Connolly. </author> <title> Hypertext Markup Language 2.0. Internet Request for Comments 1886, </title> <month> November </month> <year> 1995. </year>
Reference-contexts: Servers furnish these documents on request to clients (also known as browsers). Each document (sometimes referred to as a page) may consist of a number of files. For example, a multipart document may consist of text represented using the Hypertext Markup Language (HTML) <ref> [Berners-Lee95] </ref>, along with some number of images to be displayed inline with the text. The Hypertext Transfer Protocol (HTTP) [Berners-Lee96] is a request-response protocol designed to transfer the files making up the parts of Web documents. <p> Web documents employ a model in which a document can consist of multiple files. Thus, a server and client may need to employ multiple HTTP transactions, each of which requires a distinct TCP connection, to transfer a single document. For example, a document could consist of HTML text <ref> [Berners-Lee95] </ref>, which in turn could specify three images to be displayed inline in the body of the document. Such a document would require four TCP connections, each serving one HTTP request and reply.
Reference: [Berners-Lee96] <author> Tim Berners-Lee, Roy T. Fielding, and Henrik Frystyk Nielsen. </author> <title> Hypertext Transfer Proto col HTTP/1.0. Internet Draft draft-ietf-http-v10-spec-05, </title> <month> February </month> <year> 1996. </year> <title> This draft is a work in progress which is valid for a maximum of six months from its publication date. </title>
Reference-contexts: For example, a multipart document may consist of text represented using the Hypertext Markup Language (HTML) [Berners-Lee95], along with some number of images to be displayed inline with the text. The Hypertext Transfer Protocol (HTTP) <ref> [Berners-Lee96] </ref> is a request-response protocol designed to transfer the files making up the parts of Web documents. Each transfer consists of the client requesting a file from the server, then the server replying with the requested file (or an error notification).
Reference: [Bray96] <author> Tim Bray. </author> <title> Measuring the Web. </title> <booktitle> In Proceedings of the Fifth International World Wide Web Conference, </booktitle> <address> Paris, France, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: Given our choice of an idle threshold, we can characterize the number of files per document, as shown in Table 7. We note that in the survey of HTML documents in <ref> [Bray96] </ref>, slightly more than half of all pages contained either zero or one inlined image, corresponding to either one or two connections per document.
Reference: [Cceres91] <author> Ramn Cceres, Peter B. Danzig, Sugih Jamin, and Danny Mitzel. </author> <title> Characteristics of wide-area TCP/IP conversations. </title> <booktitle> In Proceedings of ACM SIGCOMM 91, </booktitle> <address> Zurich, Swit zerland, </address> <month> September </month> <year> 1991. </year> <title> An Empirical Model of HTTP Network Traffic 26 </title>
Reference-contexts: This approach has been used in a number of other traffic studies, such as <ref> [Cceres91] </ref> and [Paxson91], that predate the Web. [Stevens96] analyzes the packets arriving at an HTTP server and presents some interesting statistics and observations. [Danzig91] describes a library of traffic models for common (circa 1991) Internet applications, which is designed for inclusion in network simulators requiring synthetic workloads. [Paxson94] additionally describes analytic
Reference: [Catledge95] <author> Lara D. Catledge and James E. Pitkow. </author> <title> Characterizing browsing strategies in the World Wide Web. </title> <booktitle> In Proceedings of the Third International World Wide Web Conference, </booktitle> <address> Darmstadt, Germany, </address> <month> April </month> <year> 1995. </year>
Reference-contexts: Another shortcoming is that current server logs do not capture any aspects of HTTP overheads, such as protocol headers. An Empirical Model of HTTP Network Traffic 4 3.2 Client Logs [Crovella96], [Cunha95], and <ref> [Catledge95] </ref> relied on data gathered by instrumenting the NCSA Mosaic Web browser [Mosaic95] to log all retrievals made during Web user sessions. The instrumented systems were in public computing laboratories in academic environments. These studies were primarily concerned with investigating various characteristics of Web accesses. <p> This fact may be important, for example, in network systems that rely on locality of references in allocating virtual circuits or other network resources [Mah95]. Table 12 summarizes the number of consecutive document retrievals from HTTP servers during our network traces. By contrast, <ref> [Catledge95] </ref> noted that users accessed an average of ten consecutive pages per server, considerably more than the average of four to five document retrievals we observed. We believe that the difference is attributable to the interaction between user browsing strategies and client caching in Web browsers.
Reference: [Crovella96] <author> Mark E. Crovella and Azer Bestavros. </author> <title> Self-similarity in World Wide Web traffic: Evi dence and possible causes. </title> <booktitle> In Proceedings of the ACM SIGMETRICS Conference on Measurement & Modeling of Computer Systems, </booktitle> <pages> pages 160169, </pages> <address> Philadelphia, PA, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: Another shortcoming is that current server logs do not capture any aspects of HTTP overheads, such as protocol headers. An Empirical Model of HTTP Network Traffic 4 3.2 Client Logs <ref> [Crovella96] </ref>, [Cunha95], and [Catledge95] relied on data gathered by instrumenting the NCSA Mosaic Web browser [Mosaic95] to log all retrievals made during Web user sessions. The instrumented systems were in public computing laboratories in academic environments. These studies were primarily concerned with investigating various characteristics of Web accesses. <p> These characteristics are consistent with distributions of reply sizes that are heavy-tailed (with a large amount of the probability mass in the tail of the distribution). It has been in fact demonstrated that WWW file sizes are heavy tailed <ref> [Crovella96] </ref>. 19 Sep 1995 11 Oct 1995 1 Nov 1995 20 Nov 1995 Number 5030 5699 3659 18034 Minimum Size 62 81 57 30 Maximum Size 8146976 3270319 1740705 8146976 Mean Size 10664 8899 8319 8812 Median 2035 1532 2179 2127 TABLE 5. Summary of HTTP Reply Lengths (in Bytes). <p> We repeated the analysis of <ref> [Crovella96] </ref> on our data, and found that the distributions of reply sizes above 1KB are reasonably well-modeled by Pareto distributions with estimates ranging from to . 7 Further details are given in Table 6. By comparison, [Crovella96] arrived at an estimate of . 6.4 Page Length Determining the number of files <p> We repeated the analysis of <ref> [Crovella96] </ref> on our data, and found that the distributions of reply sizes above 1KB are reasonably well-modeled by Pareto distributions with estimates ranging from to . 7 Further details are given in Table 6. By comparison, [Crovella96] arrived at an estimate of . 6.4 Page Length Determining the number of files per page is less straightforward. There is no way to determine exactly which TCP connections were transferred as parts of a single document. <p> the two are judged to belong to the same document. c 2 T thresh c 1 c 1 c 2 c 1 c 2 T thresh c 2 c 1 Time Time Time T thresh T thresh T thresh An Empirical Model of HTTP Network Traffic 14 The analysis in <ref> [Crovella96] </ref> required a similar classification in order to analyze the distribution of idle times between connections. This analysis classified files separated by less than one second of idle time as belonging to the same document, due to the limitations of the users reaction time. <p> Originally, it was used to describe the frequency of words in texts, as well as other human related phenomena [Zipf49]. More recently, this distribution has been applied to the frequency that WWW docu ments are accessed <ref> [Crovella96, Arlitt96] </ref>. It would seem reasonable to apply Zipfs Law, or some other heavy-tailed distribution to the access patterns of servers as well, but confirmation of this assertion requires a larger data sample than we have available.
Reference: [Cunha95] <author> Carlos R. Cunha, Azer Bestavros, and Mark E. Crovella. </author> <title> Characteristics of WWW cli ent-based traces. </title> <type> Technical Report BU-CS-95-010, </type> <institution> Computer Science Department, Bos ton University, </institution> <month> July </month> <year> 1995. </year>
Reference-contexts: Another shortcoming is that current server logs do not capture any aspects of HTTP overheads, such as protocol headers. An Empirical Model of HTTP Network Traffic 4 3.2 Client Logs [Crovella96], <ref> [Cunha95] </ref>, and [Catledge95] relied on data gathered by instrumenting the NCSA Mosaic Web browser [Mosaic95] to log all retrievals made during Web user sessions. The instrumented systems were in public computing laboratories in academic environments. These studies were primarily concerned with investigating various characteristics of Web accesses.
Reference: [Danzig91] <author> Peter B. Danzig and Sugih Jamin. tcplib: </author> <title> A library of TCP internetwork traffic character istics. </title> <type> Technical Report USC-CS-91-495, </type> <institution> Computer Science Department, University of Southern California, </institution> <address> Los Angeles, CA, </address> <year> 1991. </year>
Reference-contexts: This approach has been used in a number of other traffic studies, such as [Cceres91] and [Paxson91], that predate the Web. [Stevens96] analyzes the packets arriving at an HTTP server and presents some interesting statistics and observations. <ref> [Danzig91] </ref> describes a library of traffic models for common (circa 1991) Internet applications, which is designed for inclusion in network simulators requiring synthetic workloads. [Paxson94] additionally describes analytic models derived from traffic traces, which have a more compact representation than purely empirical models and can be parameterized to more accurately reect <p> Since this information cannot be known a priori, we conclude that an accurate packet-level network simulation will depend on a simulation of the actual TCP algorithms. This is in fact the approach taken for other types of TCP bulk transfers in the traffic model described in <ref> [Danzig91] </ref>. Web documents employ a model in which a document can consist of multiple files. Thus, a server and client may need to employ multiple HTTP transactions, each of which requires a distinct TCP connection, to transfer a single document. <p> The alternative is to represent probability distributions by their CDFs, and to use the inverse transformation method (for example, as described in [Jain91] and applied in <ref> [Danzig91] </ref>). While requiring more storage and perhaps being slower at generating random values, this approach does have the virtue of being able to represent arbitrary probability distributions.
Reference: [Fielding96] <author> Roy T. Fielding, Jim Gettys, Jeffrey C. Mogul, Henrik Frystyk Nielsen, and Tim Berners-Lee. </author> <title> Hypertext Transfer Protocol HTTP/1.1. Internet Draft draft-ietf-http-v1.1-spec-05, </title> <month> June </month> <year> 1996. </year> <title> This draft is a work in progress which is valid for a maximum of six months from its publication date. </title>
Reference-contexts: HTTP uses the services of TCP [Postel81] for reliable transport across the unreliable global Internet. In current versions of HTTP, each TCP connection can be used for at most one HTTP retrieval. Future An Empirical Model of HTTP Network Traffic 3 versions of HTTP, as described in <ref> [Fielding96] </ref>, incorporate the work of [Padmanabhan94] and [Mogul95], which propose the reuse of TCP connections for multiple retrievals between the same client and server. In this paper, we will occasionally take several liberties with terminology. Strictly speaking, Web documents can be transferred by means other than HTTP.
Reference: [Jacobson95] <author> Van Jacobson, Craig Leres, and Steven McCanne. </author> <note> tcpdump software, version 3.0.2, 1995. This software is available at ftp://ftp.ee.lbl.gov/tcpdump.tar.Z. </note>
Reference-contexts: While this approach does lose higher-level information such as the actual files accessed, we felt that such a characterization is not essential to a network workload model. We used the freely-available tcpdump packet capture utility <ref> [Jacobson95] </ref> running on a DEC Alpha 3000/300 to record packet headers on a shared 10 Mbps Ethernet in the Computer Science Division at the University of California at Berkeley, during four periods in late 1995.
Reference: [Jain91] <author> Raj Jain. </author> <title> The Art of Computer Systems Performance Analysis. </title> <publisher> John Wiley & Sons, Inc., </publisher> <address> New York, NY, </address> <year> 1991. </year>
Reference-contexts: The alternative is to represent probability distributions by their CDFs, and to use the inverse transformation method (for example, as described in <ref> [Jain91] </ref> and applied in [Danzig91]). While requiring more storage and perhaps being slower at generating random values, this approach does have the virtue of being able to represent arbitrary probability distributions.
Reference: [Katz94] <author> Eric Dean Katz, Michelle Butler, and Robert McGrath. </author> <title> A scalable HTTP server: The NCSA prototype. </title> <booktitle> In Proceedings of the First International WWW Conference, </booktitle> <address> Geneva, Switzerland, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: This particular situation may arise in the case of rep licated HTTP servers, which rely on randomization in the Domain Name System to spread accesses to a single Web server across multiple machines, as described in <ref> [Katz94] </ref>. Rank Frequency Type 1 43 Local 2 11 Local 3 8 Remote 4 7 Remote 5 6 Local 6 6 Remote 7 6 Remote 8 6 Local 9 5 Remote 10 5 Remote TABLE 13. Top Ten Servers Observed, 19 September 1995.
Reference: [Mah95] <author> Bruce A. Mah. </author> <title> On the use of quality of service in IP over ATM. </title> <type> Technical Report CSD 95-884, </type> <institution> University of California at Berkeley, </institution> <month> September </month> <year> 1995. </year>
Reference-contexts: This fact may be important, for example, in network systems that rely on locality of references in allocating virtual circuits or other network resources <ref> [Mah95] </ref>. Table 12 summarizes the number of consecutive document retrievals from HTTP servers during our network traces. By contrast, [Catledge95] noted that users accessed an average of ten consecutive pages per server, considerably more than the average of four to five document retrievals we observed.
Reference: [Mah96] <author> Bruce A. Mah. </author> <title> INSANE Users Manual. </title> <institution> Computer Science Division, University of Cali fornia at Berkeley, </institution> <month> May </month> <year> 1996. </year>
Reference-contexts: An earlier version of this model has been implemented and incorporated into the INSANE network simulator, a discrete-event simulator for investigating the performance of IP-over-ATM designs <ref> [Mah96] </ref>. The simulation of more complex HTTP applications, such as Web browsers that perform multiple, concurrent retrievals, or multi-threaded Web servers, is analogous.
Reference: [Mogul95] <author> Jeffrey C. </author> <title> Mogul. </title> <booktitle> The case for persistent-connection HTTP. In Proceedings of ACM SIGCOMM 95, </booktitle> <pages> pages 299313, </pages> <address> Cambridge, MA, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: In current versions of HTTP, each TCP connection can be used for at most one HTTP retrieval. Future An Empirical Model of HTTP Network Traffic 3 versions of HTTP, as described in [Fielding96], incorporate the work of [Padmanabhan94] and <ref> [Mogul95] </ref>, which propose the reuse of TCP connections for multiple retrievals between the same client and server. In this paper, we will occasionally take several liberties with terminology. Strictly speaking, Web documents can be transferred by means other than HTTP. <p> In some sense this approach is the easiest to take, because the machinery for collecting model data already exists and, in fact, the data is very likely being collected anyway. Indeed, for some studies, such as <ref> [Mogul95] </ref> and [Arlitt96], it is appropriate to use a characterization of a stream of HTTP requests arriving at a Web server. However, there are two principal drawbacks to this approach. One important disadvantage of using server logs is that they cannot easily capture user access patterns across multiple Web servers.
Reference: [Mosaic95] <institution> NCSA Mosaic software, </institution> <note> version 2.6, 1995. This software is available at http:// www.ncsa.uiuc.edu/SDG/Software/XMosaic/. </note>
Reference-contexts: Another shortcoming is that current server logs do not capture any aspects of HTTP overheads, such as protocol headers. An Empirical Model of HTTP Network Traffic 4 3.2 Client Logs [Crovella96], [Cunha95], and [Catledge95] relied on data gathered by instrumenting the NCSA Mosaic Web browser <ref> [Mosaic95] </ref> to log all retrievals made during Web user sessions. The instrumented systems were in public computing laboratories in academic environments. These studies were primarily concerned with investigating various characteristics of Web accesses.
Reference: [Netscape96] <institution> Netscape Navigator software, </institution> <note> version 2.02, 1996. This software is available at http:// home.netscape.com/. </note>
Reference-contexts: However, this technique requires that browsers be able to log their requests, or more likely, the availability of source code for the Web browser so that such logging can be added. Source for newer Web browsers, including the popular Netscape Navigator <ref> [Netscape96] </ref>, is generally not available. <p> The user community consists primarily of Computer Science graduate students. While no statistics are available on the relative popularity of different Web clients used in this environment, operational experience suggests that the most prevalent is Netscape Navigator <ref> [Netscape96] </ref>. There are also several WWW servers on this subnet, associated with various research groups. <p> The page used an extension to HTML which caused the client to automatically reload the document at regular intervals, thus updating the picture every five minutes <ref> [Netscape96] </ref>. As these periodic HTTP retrievals were skewing our data, we removed them from our traces prior to further analysis. 6 6.2 Request Length HTTP requests are sent from a client to a server.
Reference: [NSFNET95] <institution> NSFNET backbone traffic distribution by service, </institution> <month> April </month> <year> 1995. </year> <note> This document is avail able at ftp://nic.merit.edu/nsfnet/statistics/1995/nsf-9504-ports.gz. </note>
Reference-contexts: Just before the NSFNET backbone was transitioned to a new architecture in April 1995, HTTP was the leading source of network traffic across the backbone network, measured both by number of bytes and number of packets transferred <ref> [NSFNET95] </ref>. Submitted to INFOCOM 97. An Empirical Model of HTTP Network Traffic 2 contemporary Internet traffic workloads, it is therefore necessary to be able to describe the network usage of this rapidly-growing application.
Reference: [Padmanabhan94] <author> Venkata N. Padmanabhan and Jeffrey C. Mogul. </author> <title> Improving HTTP latency. </title> <booktitle> In Proceed ings of the Second International World Wide Web Conference, </booktitle> <address> Chicago, IL, </address> <month> October </month> <year> 1994. </year> <title> An Empirical Model of HTTP Network Traffic 27 </title>
Reference-contexts: In current versions of HTTP, each TCP connection can be used for at most one HTTP retrieval. Future An Empirical Model of HTTP Network Traffic 3 versions of HTTP, as described in [Fielding96], incorporate the work of <ref> [Padmanabhan94] </ref> and [Mogul95], which propose the reuse of TCP connections for multiple retrievals between the same client and server. In this paper, we will occasionally take several liberties with terminology. Strictly speaking, Web documents can be transferred by means other than HTTP.
Reference: [Paxson91] <author> Vern Paxson. </author> <title> Measurements of wide area TCP conversations. </title> <type> Masters report, </type> <institution> Univer sity of California at Berkeley, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: This approach has been used in a number of other traffic studies, such as [Cceres91] and <ref> [Paxson91] </ref>, that predate the Web. [Stevens96] analyzes the packets arriving at an HTTP server and presents some interesting statistics and observations. [Danzig91] describes a library of traffic models for common (circa 1991) Internet applications, which is designed for inclusion in network simulators requiring synthetic workloads. [Paxson94] additionally describes analytic models derived
Reference: [Paxson94] <author> Vern Paxson. </author> <title> Emprically derived analytic models of wide-area TCP connections. </title> <journal> IEEE/ ACM Transactions on Networking, </journal> <volume> 2(4):316336, </volume> <month> August </month> <year> 1994. </year>
Reference-contexts: studies, such as [Cceres91] and [Paxson91], that predate the Web. [Stevens96] analyzes the packets arriving at an HTTP server and presents some interesting statistics and observations. [Danzig91] describes a library of traffic models for common (circa 1991) Internet applications, which is designed for inclusion in network simulators requiring synthetic workloads. <ref> [Paxson94] </ref> additionally describes analytic models derived from traffic traces, which have a more compact representation than purely empirical models and can be parameterized to more accurately reect particular networks. 2. <p> One is to attempt to fit the observed data to probability distributions that are easily described analytically. A simple analytic representation has the advantage of being compact and (perhaps) easier to use in analysis. This approach was discussed in <ref> [Paxson94] </ref>; in fact, we performed some rudimentary curve-fitting when analyzing the tails of some of the data samples discussed in Section 6.3 and Section 6.4.
Reference: [Postel81] <author> Jon Postel. </author> <title> Transmission Control Protocol. Internet Request for Comments 793, Septem ber 1981. </title>
Reference-contexts: Each transfer consists of the client requesting a file from the server, then the server replying with the requested file (or an error notification). Both the request and reply contain identification and control information in headers. HTTP uses the services of TCP <ref> [Postel81] </ref> for reliable transport across the unreliable global Internet. In current versions of HTTP, each TCP connection can be used for at most one HTTP retrieval.
Reference: [Postel85] <author> Jon Postel and Joyce Reynolds. </author> <title> File Transfer Protocol (FTP). Internet Request for Com ments 959, </title> <month> October </month> <year> 1985. </year>
Reference-contexts: In this paper, we will occasionally take several liberties with terminology. Strictly speaking, Web documents can be transferred by means other than HTTP. In particular, the File Transfer Protocol FTP <ref> [Postel85] </ref> is used for some portions of the Web, for example document archives where HTTP cannot be deployed for administrative reasons and FTP servers exist already. Thus, the terms Web server and HTTP server are not strictly synonymous, though we will frequently use them interchangeably.
Reference: [Stevens96] <editor> W. Richard Stevens. TCP/IP Illustrated, </editor> <volume> Volume 3. </volume> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, MA, </address> <year> 1996. </year>
Reference-contexts: This approach has been used in a number of other traffic studies, such as [Cceres91] and [Paxson91], that predate the Web. <ref> [Stevens96] </ref> analyzes the packets arriving at an HTTP server and presents some interesting statistics and observations. [Danzig91] describes a library of traffic models for common (circa 1991) Internet applications, which is designed for inclusion in network simulators requiring synthetic workloads. [Paxson94] additionally describes analytic models derived from traffic traces, which have
Reference: [Woodruff96] <author> Allison Woodruff, Paul M. Aoki, Eric Brewer, Paul Gauthier, and Lawrence A. Rowe. </author> <title> An investigation of documents from the World Wide Web. </title> <booktitle> In Proceedings of the Fifth International World Wide Web Conference, </booktitle> <address> Paris, France, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: In the most common case, the client application sends a request for some data; the 3. In a recent study of the characteristics of HTML documents indexed by the Inktomi web crawler, approximately 94% of the documents surveyed were accessed via the standard HTTP port <ref> [Woodruff96] </ref>. Start Time End Time Number of HTTP Packets Tue Sep 19 16:12:33 1995 Thu Sep 21 07:53:22 1995 186068 Wed Nov 1 11:22:47 1995 Thu Nov 2 10:53:12 1995 369671 TABLE 1. Summary of Traffic Traces.
Reference: [Zipf49] <author> George Kingsley Zipf. </author> <title> Human Behavior and the Principle of Least Effort. </title> <publisher> Hafner Publish ing Company, </publisher> <address> New York, NY, </address> <year> 1949. </year>
Reference-contexts: Cumulative Distribution Functions for Consecutive Document Retrievals. ith An Empirical Model of HTTP Network Traffic 21 set is proportional to . Originally, it was used to describe the frequency of words in texts, as well as other human related phenomena <ref> [Zipf49] </ref>. More recently, this distribution has been applied to the frequency that WWW docu ments are accessed [Crovella96, Arlitt96].
References-found: 27

