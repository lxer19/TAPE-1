URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR94631.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: An Algebraic Schwarz Theory  
Author: Michael Holst, 
Note: This work was supported in part by the NSF under Cooperative Agreement No. CCR-9120008. The government has certain rights in this material. iii  
Date: December 17, 1994  
Address: Pasadena, CA 91125  
Affiliation: Caltech Applied Math 217-50  
Abstract: This report contains a collection of notes on abstract additive and multiplicative Schwarz methods for self-adjoint positive linear operator equations. We examine closely one of the most elegant and useful modern convergence theories for these methods, following the recent work of Dryja and Widlund, Xu, and their colleagues. Our motivation is to fully understand this recent theory, and then to take a step toward developing a variation of the theory in a more general setting. It is hoped that such a generalized theory will be useful in the analysis of algebraic multigrid and domain decomposition methods (as well as other methods), when no finite element structure is available. Using this approach some initial convergence results can be shown for broad classes of fully algebraic domain decomposition and multigrid methods, although rate estimation currently requires additional finite element or other structure as in existing theories. Although we cannot at this time use the framework to provide accurate convergence rate and complexity estimates, we believe that with additional analysis it may be possible to do so, as well as to use the framework to guide the design of the coarse problems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. E. Alcouffe, A. Brandt, J. E. Dendy, Jr., and J. W. Painter, </author> <title> The multi-grid method for the diffusion equation with strongly discontinuous coefficients, </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> 2 (1981), </volume> <pages> pp. 430-454. </pages>
Reference-contexts: )] 2 (A j=1 n X ff j v j ) = max [1 j p i ( j )] 2 ke 0 k 2 Thus, we have that ke i+1 k A min j 2 (BA) The scaled and shifted Chebyshev polynomials T i+1 (), extended outside the interval <ref> [1; 1] </ref> as in the Appendix A of [4], yield a solution to this mini-max problem. <p> Multigrid methods typically perform badly, and even the regularity-free multigrid convergence theory [7] is invalid. Possible approaches include coefficient averaging methods (cf. <ref> [1] </ref>) and the explicit enforcement of the conditions (27) (cf. [1, 11, 33]). <p> Multigrid methods typically perform badly, and even the regularity-free multigrid convergence theory [7] is invalid. Possible approaches include coefficient averaging methods (cf. [1]) and the explicit enforcement of the conditions (27) (cf. <ref> [1, 11, 33] </ref>). By introducing a symbolic stencil calculus and employing MAPLE or MATHEMATICA, the conditions (27) can be enforced algebraically in an efficient way for certain types of sparse matrices; details may be found for example in the appendix of [20].
Reference: [2] <author> S. Ashby, M. Holst, T. Manteuffel, and P. </author> <title> Saylor, The role of the inner product in stopping criteria for conjugate gradient iterations, </title> <type> Tech. Rep. </type> <institution> UCRL-JC-112586, Lawrence Livermore National Laboratory, </institution> <year> 1992. </year>
Reference-contexts: This property can also be shown to hold for A-self-adjoint operators. The following lemma can be found in <ref> [2] </ref> (as Lemma 4.1), although the proof there is for A-normal matrices rather than A-self-adjoint operators. Lemma 2.1 If A is SPD and M is A-self-adjoint, then kM k A = (M ). Proof. <p> The ratio of the extreme eigenvalues of BA appearing in the bound is often mistakenly called the (spectral) condition number (BA); in fact, since BA is not self-adjoint (it is A-self-adjoint), this ratio is not in general equal to the condition number (this point is discussed in great detail in <ref> [2] </ref>). However, the ratio does yield a condition number in a different norm. The following lemma is a special case of Corollary 4.2 in [2]. <p> BA is not self-adjoint (it is A-self-adjoint), this ratio is not in general equal to the condition number (this point is discussed in great detail in <ref> [2] </ref>). However, the ratio does yield a condition number in a different norm. The following lemma is a special case of Corollary 4.2 in [2]. Lemma 2.12 If A and B are SPD, then A (BA) = kBAk A k (BA) 1 k A = max (BA) : (9) 10 LINEAR OPERATOR EQUATIONS Proof. <p> A proof of this fact is given in Corollary 4.2 of <ref> [2] </ref>, along with a detailed discussion of this and other relationships for more general conjugate gradient methods.
Reference: [3] <author> S. F. Ashby, T. A. Manteuffel, and P. E. </author> <title> Saylor, A taxonomy for conjugate gradient methods, </title> <type> Tech. Rep. </type> <institution> UCRL-98508, Lawrence Livermore National Laboratory, </institution> <month> March </month> <year> 1988. </year> <note> To appear in SIAM J. Numer. Anal. </note>
Reference-contexts: These relationships are commonly used, but some of the short proofs seem unavailable. In <ref> [3] </ref>, a general class of conjugate gradient methods obeying three-term recursions is studied, and it is shown that each instance of the class can be characterized by three operators: an inner product operator X, a preconditioning operator Y , and the system operator Z. <p> As such, these methods are denoted as CG (X,Y ,Z). We are interested in the special case that X = A, Y = B, and Z = A, when both B and A are SPD. Choosing the Omin <ref> [3] </ref> algorithm to implement the method CG (A,B,A), the preconditioned conjugate gradient method results: Algorithm 2.2 (Preconditioned Conjugate Gradient Algorithm) Let u 0 2 H be given. r 0 = f Au 0 , s 0 = Br 0 , p 0 = s 0 . <p> LINEAR OPERATOR EQUATIONS 9 If the dimension of H is n, then the algorithm can be shown to converge in n steps since the preconditioned operator BA is A-SPD <ref> [3] </ref>. Note that if B = I, then this algorithm is exactly the Hestenes and Stiefel algorithm.
Reference: [4] <author> O. Axelsson and V. Barker, </author> <title> Finite Element Solution of Boundary Value Problems, </title> <publisher> Academic Press, </publisher> <address> Orlando, FL, </address> <year> 1984. </year>
Reference-contexts: v j ) = max [1 j p i ( j )] 2 ke 0 k 2 Thus, we have that ke i+1 k A min j 2 (BA) The scaled and shifted Chebyshev polynomials T i+1 (), extended outside the interval [1; 1] as in the Appendix A of <ref> [4] </ref>, yield a solution to this mini-max problem.
Reference: [5] <author> R. E. Bank and T. F. Dupont, </author> <title> An optimal order process for solving finite element equations, </title> <journal> Math. Comp., </journal> <volume> 36 (1981), </volume> <pages> pp. 35-51. </pages>
Reference-contexts: In addition to the references cited directly in the text below, the material here owes much to the following sources: <ref> [5, 6, 7, 12, 14, 17, 26, 27, 28, 38, 39] </ref>. 2. Linear operator equations In this section, we first review the theory of self-adjoint linear operators on a Hilbert space. The results required for the analysis of linear methods, as well as conjugate gradient methods, are summarized.
Reference: [6] <author> J. H. Bramble and J. E. Pasciak, </author> <title> New convergence estimates for multigrid algorithms, </title> <journal> Math. Comp., </journal> <volume> 49 (1987), </volume> <pages> pp. 311-329. </pages>
Reference-contexts: In addition to the references cited directly in the text below, the material here owes much to the following sources: <ref> [5, 6, 7, 12, 14, 17, 26, 27, 28, 38, 39] </ref>. 2. Linear operator equations In this section, we first review the theory of self-adjoint linear operators on a Hilbert space. The results required for the analysis of linear methods, as well as conjugate gradient methods, are summarized.
Reference: [7] <author> J. H. Bramble, J. E. Pasciak, J. Wang, and J. Xu, </author> <title> Convergence estimates for multigrid algorithms without regularity assumptions, </title> <journal> Math. Comp., </journal> <volume> 57 (1991), </volume> <pages> pp. 23-45. </pages>
Reference-contexts: In addition to the references cited directly in the text below, the material here owes much to the following sources: <ref> [5, 6, 7, 12, 14, 17, 26, 27, 28, 38, 39] </ref>. 2. Linear operator equations In this section, we first review the theory of self-adjoint linear operators on a Hilbert space. The results required for the analysis of linear methods, as well as conjugate gradient methods, are summarized. <p> Remark 4.13. Although our language and notation is quite different, the proof we have given above for Lemma 4.10 is similar to results in [41] and [17]. Similar ideas and results appear [35]. The main ideas and techniques underlying proofs of this type were originally developed in <ref> [7, 8, 39] </ref>. 4.3 Product and sum splitting theory for non-nested Schwarz methods The main theory for Schwarz methods based on non-nested subspaces, as in the case of overlapping domain decomposition-like methods, may be summarized in the following way. <p> In particular, it can be shown <ref> [7, 20, 29] </ref> that if the variational conditions (27) hold, then the multigrid error propagator can be factored as: E = I BA = (I T J )(I T J1 ) (I T 1 ); (29) where: I J = I; I k = I J J2 I k+2 k ; <p> Multigrid methods typically perform badly, and even the regularity-free multigrid convergence theory <ref> [7] </ref> is invalid. Possible approaches include coefficient averaging methods (cf. [1]) and the explicit enforcement of the conditions (27) (cf. [1, 11, 33]). <p> This defines the splitting v = k=1 which is central to the BPWX theory <ref> [7] </ref>. Employing this splitting along with results from finite element approximation theory, it is shown in [7], using a similar Schwarz theory framework, that kEk 2 C This result holds even in the presence of coefficient discontinuities (the constants being independent of the jumps in the coefficients). <p> This defines the splitting v = k=1 which is central to the BPWX theory <ref> [7] </ref>. Employing this splitting along with results from finite element approximation theory, it is shown in [7], using a similar Schwarz theory framework, that kEk 2 C This result holds even in the presence of coefficient discontinuities (the constants being independent of the jumps in the coefficients). The restriction is that all discontinuities lie along all element boundaries on all levels.
Reference: [8] <author> J. H. Bramble, J. E. Pasciak, J. Wang, and J. Xu, </author> <title> Convergence estimates for product iterative methods with applications to domain decomposition and multigrid, </title> <journal> Math. Comp., </journal> <volume> 57 (1991), </volume> <pages> pp. 1-21. </pages>
Reference-contexts: In x3, we present an approach for bounding the norms and condition numbers of products and sums of self-adjoint operators on a Hilbert space, derived from work due to Dryja and Widlund [14], Bramble et al. <ref> [8] </ref>, and Xu [39]. This particular approach is quite general in that we establish the main norm and condition number bounds without reference to subspaces; each of the three required assumptions for the theory involve only the operators on the original Hilbert space. <p> The theory of products and sums of operators In this section, we present an approach for bounding the norms and condition numbers of products and sums of self-adjoint operators on a Hilbert space, derived from work due to Dryja and Widlund [14], Bramble et al. <ref> [8] </ref>, and Xu [39]. This particular approach is quite general in that we establish the main norm and condition number bounds without reference to subspaces; each of the three required assumptions for the theory involve only the operators on the original Hilbert space. <p> The following simple lemma, first appearing in <ref> [8] </ref>, will often be useful at various points in the analysis of the product and sum operators. Lemma 3.2 Under Assumption 3.1, it holds that (AT k u; T k u) !(AT k u; u); 8u 2 H: THE THEORY OF PRODUCTS AND SUMS OF OPERATORS 15 Proof. <p> But this gives (AT k u; T k u) = (AT 1=2 1=2 1=2 1=2 = (AT k v; v) !(Av; v) = !(AT k u; T k u) = !(AT k u; u); 8u 2 H. The next lemma, also appearing first in <ref> [8] </ref>, will be a key tool in the analysis of the product operator. Lemma 3.3 Under Assumption 3.1, it holds that (2 !) k=1 A kE J vk 2 Proof. <p> In certain cases it will still be necessary to verify Assumption 3.3 directly. The following theorems provide a fundamental framework for analyzing product and sum operators, employing only the five assumptions previously stated. A version of the product theorem similar to the one below first appeared in <ref> [8] </ref>. Theorems for sum operators were established early by Dryja and Widlund [12]. Theorem 3.5 Under Assumptions 3.1 and 3.3, the product operator (11) satisfies: kEk 2 2 ! : Proof. <p> Both of these default or worst case results appear essentially in <ref> [8] </ref>. First, we recall the Cauchy-Schwarz inequality in R n , and state a useful corollary. Lemma 3.8 If a k ; b k 2 R, k = 1; : : : ; n, then it holds that n X a k b k k=1 k k=1 k : Proof. <p> An early approach employing an interaction matrix appears in <ref> [8] </ref>; the form appearing below is most closely related to that used in [17] and [39]. The idea of employing a strictly upper-triangular interaction matrix to improve the bound for the weak interaction property is due to Hackbusch [17]. <p> Remark 4.13. Although our language and notation is quite different, the proof we have given above for Lemma 4.10 is similar to results in [41] and [17]. Similar ideas and results appear [35]. The main ideas and techniques underlying proofs of this type were originally developed in <ref> [7, 8, 39] </ref>. 4.3 Product and sum splitting theory for non-nested Schwarz methods The main theory for Schwarz methods based on non-nested subspaces, as in the case of overlapping domain decomposition-like methods, may be summarized in the following way.
Reference: [9] <author> X.-C. Cai and O. B. Widlund, </author> <title> Multiplicative Schwarz algorithms for some nonsymmetric and indefinite problems, </title> <type> Tech. Rep. 595, </type> <institution> Courant Institute of Mathematical Science, </institution> <address> New York University, New York, NY, </address> <year> 1992. </year>
Reference-contexts: From Lemmas 3.20 and 3.16 it follows that C 3 = ! + C 5 ! + !(fi) = !((fi) + 1): Remark 3.9. It is apparently possible to establish a sharper bound <ref> [9, 14] </ref> than the one given above in Lemma 3.21, the improved bound having the form C 2 = 1 + 2! 2 (fi) 2 : This result is stated and used in several recent papers on domain decomposition, e.g., in [14], but the proof of the result has apparently not <p> A proof of a similar result is established for some related nonsymmetric problems in <ref> [9] </ref>. 3.4 Main results of the theory The main theory may be summarized in the following way.
Reference: [10] <author> P. Concus, G. H. Golub, and D. P. O'Leary, </author> <title> A generalized conjugate gradient method for the numerical solution of elliptic partial differential equations, in Sparse Matrix Computations, </title> <editor> J. R. Bunch and D. J. Rose, eds., </editor> <publisher> Academic Press, </publisher> <address> New York, NY, </address> <year> 1976, </year> <pages> pp. 309-332. </pages>
Reference-contexts: The conjugate gradient method was developed by Hestenes and Stiefel [19] for linear systems with symmetric positive definite operators A. It is common to precondition the linear system by the SPD preconditioning operator B A 1 , in which case the generalized or preconditioned conjugate gradient method <ref> [10] </ref> results. Our purpose in this section is to briefly examine the algorithm, its contraction properties, and establish some simple relationships between the contraction number of a basic linear preconditioner and that of the resulting preconditioned conjugate gradient algorithm.
Reference: [11] <author> J. E. Dendy, Jr., </author> <title> Two multigrid methods for three-dimensional problems with discontinuous and anisotropic coefficients, </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> 8 (1987), </volume> <pages> pp. 673-685. </pages>
Reference-contexts: Multigrid methods typically perform badly, and even the regularity-free multigrid convergence theory [7] is invalid. Possible approaches include coefficient averaging methods (cf. [1]) and the explicit enforcement of the conditions (27) (cf. <ref> [1, 11, 33] </ref>). By introducing a symbolic stencil calculus and employing MAPLE or MATHEMATICA, the conditions (27) can be enforced algebraically in an efficient way for certain types of sparse matrices; details may be found for example in the appendix of [20].
Reference: [12] <author> M. Dryja and O. B. Widlund, </author> <title> Towards a unified theory of domain decomposition algorithms for elliptic problems, in Third International Symposium on Domain Decomposition Methods for Partial Differential Equations, </title> <editor> T. F. Chan, R. Glowinski, J. Periaux, and O. B. Widlund, eds., </editor> <address> Philadelphia, PA, 1989, </address> <publisher> SIAM, </publisher> <pages> pp. 3-21. </pages>
Reference-contexts: In addition to the references cited directly in the text below, the material here owes much to the following sources: <ref> [5, 6, 7, 12, 14, 17, 26, 27, 28, 38, 39] </ref>. 2. Linear operator equations In this section, we first review the theory of self-adjoint linear operators on a Hilbert space. The results required for the analysis of linear methods, as well as conjugate gradient methods, are summarized. <p> The following theorems provide a fundamental framework for analyzing product and sum operators, employing only the five assumptions previously stated. A version of the product theorem similar to the one below first appeared in [8]. Theorems for sum operators were established early by Dryja and Widlund <ref> [12] </ref>. Theorem 3.5 Under Assumptions 3.1 and 3.3, the product operator (11) satisfies: kEk 2 2 ! : Proof.
Reference: [13] <author> M. Dryja and O. B. Widlund, </author> <title> Multilevel additive methods for elliptic finite element problems, </title> <type> Tech. Rep. 507, </type> <institution> Courant Institute of Mathematical Science, </institution> <address> New York University, New York, NY, </address> <year> 1990. </year>
Reference: [14] <author> M. Dryja and O. B. Widlund, </author> <title> Domain decomposition algorithms with small overlap, </title> <type> Tech. Rep. 606, </type> <institution> Courant Institute of Mathematical Science, </institution> <address> New York University, New York, NY, </address> <year> 1992. </year>
Reference-contexts: In x3, we present an approach for bounding the norms and condition numbers of products and sums of self-adjoint operators on a Hilbert space, derived from work due to Dryja and Widlund <ref> [14] </ref>, Bramble et al. [8], and Xu [39]. This particular approach is quite general in that we establish the main norm and condition number bounds without reference to subspaces; each of the three required assumptions for the theory involve only the operators on the original Hilbert space. <p> In x4, we consider abstract Schwarz methods based on subspaces, and apply the general product and sum operator theory to these methods. The resulting theory, which is a variation of that presented in [39] and <ref> [14] </ref>, rests on the notion of a stable subspace splitting of the original Hilbert space (cf. [31, 32]). <p> In addition to the references cited directly in the text below, the material here owes much to the following sources: <ref> [5, 6, 7, 12, 14, 17, 26, 27, 28, 38, 39] </ref>. 2. Linear operator equations In this section, we first review the theory of self-adjoint linear operators on a Hilbert space. The results required for the analysis of linear methods, as well as conjugate gradient methods, are summarized. <p> The theory of products and sums of operators In this section, we present an approach for bounding the norms and condition numbers of products and sums of self-adjoint operators on a Hilbert space, derived from work due to Dryja and Widlund <ref> [14] </ref>, Bramble et al. [8], and Xu [39]. This particular approach is quite general in that we establish the main norm and condition number bounds without reference to subspaces; each of the three required assumptions for the theory involve only the operators on the original Hilbert space. <p> k v; v) [1 + ! 1=2 C 5 + C 4 ] 2 k=0 Therefore, Assumption 3.4 holds, where: C 2 = [1 + ! 1=2 C 5 + C 4 ] 2 : Results similar to the next lemma are used in several recent papers on domain decomposition <ref> [14] </ref>; the proof is quite simple once the proof of Lemma 3.13 is available. Lemma 3.20 Under Assumptions 3.1 (including T 0 ) and 3.7 (excluding T 0 ), we have that Assumption 3.5 (including T 0 ) holds, where: C 3 = ! + C 5 : Proof. <p> From Lemmas 3.20 and 3.16 it follows that C 3 = ! + C 5 ! + !(fi) = !((fi) + 1): Remark 3.9. It is apparently possible to establish a sharper bound <ref> [9, 14] </ref> than the one given above in Lemma 3.21, the improved bound having the form C 2 = 1 + 2! 2 (fi) 2 : This result is stated and used in several recent papers on domain decomposition, e.g., in [14], but the proof of the result has apparently not <p> It is apparently possible to establish a sharper bound [9, 14] than the one given above in Lemma 3.21, the improved bound having the form C 2 = 1 + 2! 2 (fi) 2 : This result is stated and used in several recent papers on domain decomposition, e.g., in <ref> [14] </ref>, but the proof of the result has apparently not been published. A proof of a similar result is established for some related nonsymmetric problems in [9]. 3.4 Main results of the theory The main theory may be summarized in the following way. <p> Abstract Schwarz theory In this section, we consider abstract Schwarz methods based on subspaces, and apply the general product and sum operator theory to these methods. The resulting theory, which is a variation of that presented in [39] and <ref> [14] </ref>, rests on the notion of a stable subspace splitting of the original Hilbert space (cf. [31, 32]). <p> Remark 4.17. The theory in this section was derived mainly from work in the domain decomposition community, due chiefly to Widlund and his co-workers. In particular, our presentation owes much to [39] and <ref> [14] </ref>. ABSTRACT SCHWARZ THEORY 35 4.4 Product and sum splitting theory for nested Schwarz methods The main theory for Schwarz methods based on nested subspaces, as in the case of multigrid-like methods, is summarized in this section.
Reference: [15] <author> M. Dryja and O. B. Widlund, </author> <title> Some recent results on schwarz type domain decomposition algorithms, </title> <type> Tech. Rep. 615, </type> <institution> Courant Institute of Mathematical Science, </institution> <address> New York University, New York, NY, </address> <year> 1992. </year>
Reference: [16] <author> M. Dryja and O. B. Widlund, </author> <title> Schwarz methods of neumann-neumann type for three-dimensional elliptic finite element problems, </title> <type> tech. rep., </type> <institution> Courant Institute of Mathematical Science, </institution> <address> New York University, New York, NY, </address> <year> 1993. </year> <note> (To appear). </note>
Reference: [17] <author> W. Hackbusch, </author> <title> Iterative Solution of Large Sparse Systems of Equations, </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, Germany, </address> <year> 1994. </year>
Reference-contexts: In addition, we develop the Schwarz theory allowing for completely general prolongation and restriction operators, so that the theory is not restricted to the use of inclusion and projection as the transfer operators (a similar Schwarz framework with general transfer operators was constructed recently by Hackbusch <ref> [17] </ref>). The resulting theoretical framework is useful for analyzing specific algebraic methods, such as algebraic multigrid and algebraic domain decomposition, without requiring the use of finite element spaces (and their associated transfer operators of inclusions and projection). <p> In addition to the references cited directly in the text below, the material here owes much to the following sources: <ref> [5, 6, 7, 12, 14, 17, 26, 27, 28, 38, 39] </ref>. 2. Linear operator equations In this section, we first review the theory of self-adjoint linear operators on a Hilbert space. The results required for the analysis of linear methods, as well as conjugate gradient methods, are summarized. <p> An early approach employing an interaction matrix appears in [8]; the form appearing below is most closely related to that used in <ref> [17] </ref> and [39]. The idea of employing a strictly upper-triangular interaction matrix to improve the bound for the weak interaction property is due to Hackbusch [17]. The default bound for the strictly upper-triangular matrix is also due to Hackbusch [17]. <p> An early approach employing an interaction matrix appears in [8]; the form appearing below is most closely related to that used in <ref> [17] </ref> and [39]. The idea of employing a strictly upper-triangular interaction matrix to improve the bound for the weak interaction property is due to Hackbusch [17]. The default bound for the strictly upper-triangular matrix is also due to Hackbusch [17]. <p> form appearing below is most closely related to that used in <ref> [17] </ref> and [39]. The idea of employing a strictly upper-triangular interaction matrix to improve the bound for the weak interaction property is due to Hackbusch [17]. The default bound for the strictly upper-triangular matrix is also due to Hackbusch [17]. <p> Remark 4.13. Although our language and notation is quite different, the proof we have given above for Lemma 4.10 is similar to results in [41] and <ref> [17] </ref>. Similar ideas and results appear [35]. <p> Remark 4.19. The theory in this section was derived from several sources; in particular, our presentation owes much to [39], <ref> [17] </ref>, and to [41]. 5. Applications to domain decomposition Domain decomposition methods were first proposed by H.A. Schwarz as a theoretical tool for studying elliptic problems on complicated domains, constructed as the union of simple domains.
Reference: [18] <author> P. R. Halmos, </author> <title> Finite-Dimensional Vector Spaces, </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, Germany, </address> <year> 1958. </year>
Reference-contexts: For SPD (or A-SPD) operators M , the eigenvalues of M are real and positive, and the powers M s for real s are well-defined through the spectral decomposition; see for example x79 and x82 in <ref> [18] </ref>. Finally, recall that a matrix representing the operator M with respect to any basis for H has the same eigenvalues as the operator M .
Reference: [19] <author> M. R. Hestenes and E. </author> <title> Stiefel, Methods of conjugate gradients for solving linear systems, </title> <journal> J. Research of NBS, </journal> <volume> 49 (1952), </volume> <pages> pp. 409-435. 45 46 BIBLIOGRAPHY </pages>
Reference-contexts: The conjugate gradient method was developed by Hestenes and Stiefel <ref> [19] </ref> for linear systems with symmetric positive definite operators A. It is common to precondition the linear system by the SPD preconditioning operator B A 1 , in which case the generalized or preconditioned conjugate gradient method [10] results.
Reference: [20] <author> M. Holst, </author> <title> The Poisson-Boltzmann equation: Analysis and multilevel numerical solution, </title> <type> tech. rep., </type> <institution> Applied Mathematics and CRPC, California Institute of Technology, </institution> <year> 1994. </year>
Reference-contexts: In particular, it can be shown <ref> [7, 20, 29] </ref> that if the variational conditions (27) hold, then the multigrid error propagator can be factored as: E = I BA = (I T J )(I T J1 ) (I T 1 ); (29) where: I J = I; I k = I J J2 I k+2 k ; <p> By introducing a symbolic stencil calculus and employing MAPLE or MATHEMATICA, the conditions (27) can be enforced algebraically in an efficient way for certain types of sparse matrices; details may be found for example in the appendix of <ref> [20] </ref>.
Reference: [21] <author> L. V. Kantorovich and V. I. </author> <title> Krylov, Approximate Methods of Higher Analysis, </title> <editor> P. Noordhoff, </editor> <publisher> Ltd, </publisher> <address> Groningen, The Netherlands, </address> <year> 1958. </year>
Reference-contexts: Applications to domain decomposition Domain decomposition methods were first proposed by H.A. Schwarz as a theoretical tool for studying elliptic problems on complicated domains, constructed as the union of simple domains. An interesting early reference not often mentioned is <ref> [21] </ref>, containing both analysis and numerical examples, and references to the original work by Schwarz.
Reference: [22] <author> A. N. Kolmogorov and S. V. Fomin, </author> <title> Introductory Real Analysis, </title> <publisher> Dover Publications, </publisher> <address> New York, NY, </address> <year> 1970. </year>
Reference-contexts: Lemma 3.8 If a k ; b k 2 R, k = 1; : : : ; n, then it holds that n X a k b k k=1 k k=1 k : Proof. See for example <ref> [22] </ref>. Corollary 3.9 If a k 2 R, k = 1; : : :; n, then it holds that n X a k n k=1 k : Proof. This follows easily from Lemma 3.8 by taking b k = 1 for all k.
Reference: [23] <author> R. Kress, </author> <title> Linear Integral Equations, </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, Germany, </address> <year> 1989. </year>
Reference-contexts: Theorem 2.2 The condition (I BA) &lt; 1 is necessary and sufficient for convergence of Algorithm 2.1. Proof. See for example Theorem 10.11 in <ref> [23] </ref> or Theorem 7.1.1 in [30]. Since jjkuk = kuk = kM uk kM k kuk for any norm kk, it follows that (M ) kM k for all norms kk.
Reference: [24] <author> E. </author> <title> Kreyszig, Introductory Functional Analysis with Applications, </title> <publisher> John Wiley & Sons, Inc., </publisher> <address> New York, NY, </address> <year> 1990. </year>
Reference-contexts: As noted on page 95 in [34], since (E) = lim m!1 kE m k 1=m for all bounded linear operators E and norms k k (Theorem 7.5-5 in <ref> [24] </ref>), it follows that lim m!1 R (E m ) = R 1 (E). <p> Lemma 4.1 Let A 2 L (H; H) be SPD. Then the operator P 2 L (H; H) is an A-orthogonal projector if and only if P is A-self-adjoint and idempotent (P 2 = P ). Proof. See <ref> [24] </ref>, Theorem 9.5-1, page 481. Lemma 4.2 Assume dim (H k ) dim (H), I k : H k 7! H, null (I k ) = f0g, and that A is SPD. <p> Now, let us define: ^ P 1 = P 1 ; ^ P k = P k P k1 ; k = 2; : : : ; J: By Theorem 9.6-2 in <ref> [24] </ref> we have that each ^ P k is a projection. (It is easily verified that ^ P k is idempotent and A-self-adjoint.) Define now I k V k = ^ P k H = (P k P k1 )H = (I k A 1 k A I k1 A 1
Reference: [25] <author> P. L. Lions, </author> <title> On the Schwarz Alternating Method. I, in First International Symposium on Domain Decomposition Methods for Partial Differential Equations, </title> <editor> R. Glowinski, G. H. Golub, G. A. Meurant, and J. Periaux, eds., </editor> <address> Philadelphia, PA, 1988, </address> <publisher> SIAM, </publisher> <pages> pp. 1-42. </pages>
Reference-contexts: The following key lemma (in the case of inclusion and projection as prolongation and restriction) is sometimes referred to as Lions' Lemma <ref> [25] </ref>, although the multiple-subspace case is essentially due to Wid lund [36]. Lemma 4.6 Under Assumption 4.2 it holds that S 0 kvk 2 J X (AP k v; v); 8v 2 H: Proof.
Reference: [26] <author> J. Mandel, </author> <title> Some recent advances in multigrid methods, </title> <booktitle> Advances in Electronics and Electron Physics, 82 (1991), </booktitle> <pages> pp. 327-377. </pages>
Reference-contexts: In addition to the references cited directly in the text below, the material here owes much to the following sources: <ref> [5, 6, 7, 12, 14, 17, 26, 27, 28, 38, 39] </ref>. 2. Linear operator equations In this section, we first review the theory of self-adjoint linear operators on a Hilbert space. The results required for the analysis of linear methods, as well as conjugate gradient methods, are summarized.
Reference: [27] <author> J. Mandel, S. McCormick, and R. Bank, </author> <title> Variational multigrid theory, in Multigrid Methods, </title> <editor> S. McCormick, ed., </editor> <publisher> SIAM, </publisher> <year> 1987, </year> <pages> pp. 131-177. </pages>
Reference-contexts: In addition to the references cited directly in the text below, the material here owes much to the following sources: <ref> [5, 6, 7, 12, 14, 17, 26, 27, 28, 38, 39] </ref>. 2. Linear operator equations In this section, we first review the theory of self-adjoint linear operators on a Hilbert space. The results required for the analysis of linear methods, as well as conjugate gradient methods, are summarized.
Reference: [28] <author> S. F. McCormick, </author> <title> An algebraic interpretation of multigrid methods, </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 19 (1982), </volume> <pages> pp. 548-560. </pages>
Reference-contexts: In addition to the references cited directly in the text below, the material here owes much to the following sources: <ref> [5, 6, 7, 12, 14, 17, 26, 27, 28, 38, 39] </ref>. 2. Linear operator equations In this section, we first review the theory of self-adjoint linear operators on a Hilbert space. The results required for the analysis of linear methods, as well as conjugate gradient methods, are summarized.
Reference: [29] <author> S. F. McCormick and J. W. Ruge, </author> <title> Unigrid for multigrid simulation, </title> <journal> Math. Comp., </journal> <volume> 41 (1983), </volume> <pages> pp. 43-62. </pages>
Reference-contexts: In particular, it can be shown <ref> [7, 20, 29] </ref> that if the variational conditions (27) hold, then the multigrid error propagator can be factored as: E = I BA = (I T J )(I T J1 ) (I T 1 ); (29) where: I J = I; I k = I J J2 I k+2 k ;
Reference: [30] <author> J. M. Ortega, </author> <title> Numerical Analysis: A Second Course, </title> <publisher> Academic Press, </publisher> <address> New York, NY, </address> <year> 1972. </year>
Reference-contexts: Theorem 2.2 The condition (I BA) &lt; 1 is necessary and sufficient for convergence of Algorithm 2.1. Proof. See for example Theorem 10.11 in [23] or Theorem 7.1.1 in <ref> [30] </ref>. Since jjkuk = kuk = kM uk kM k kuk for any norm kk, it follows that (M ) kM k for all norms kk. Therefore, kI BAk &lt; 1 and kI BAk A &lt; 1 are both sufficient conditions for convergence of Algorithm 2.1. <p> An alternate sufficient condition for convergence of the basic linear method is given in the following lemma, which is similar to Stein's Theorem (Theorem 7.1.8 in <ref> [30] </ref>, or Theorem 6.1, page 80 in [40]). Lemma 2.3 If E fl is the A-adjoint of E, and I E fl E is A-positive, then it holds that (E) kEk A &lt; 1. Proof. By hypothesis, (A (I E fl E)u; u) &gt; 0 8u 2 H.
Reference: [31] <author> P. Oswald, </author> <title> Stable subspace splittings for Sobolev spaces and their applications, </title> <type> Tech. Rep. </type> <institution> MATH-93-7, Institut fur Angewandte Mathematik, </institution> <address> Friedrich-Schiller-Universitatat Jena, D-07740 Jena, FRG, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: The resulting theory, which is a variation of that presented in [39] and [14], rests on the notion of a stable subspace splitting of the original Hilbert space (cf. <ref> [31, 32] </ref>). Although the derivation here is presented in a somewhat different, algebraic language, many of the intermediate results 1 2 INTRODUCTION we use have appeared previously in the literature in other forms (we provide references at the appropriate points). <p> This lemma has appeared many times in the literature in one form or another; cf. <ref> [31] </ref>. LINEAR OPERATOR EQUATIONS 7 Lemma 2.11 If A and B are SPD, then (I ffBA) = kI ffBAk A &lt; 1: if and only if ff 2 (0; 2=(BA)). <p> A discussion of these ideas appears in <ref> [31] </ref>. 3. <p> The resulting theory, which is a variation of that presented in [39] and [14], rests on the notion of a stable subspace splitting of the original Hilbert space (cf. <ref> [31, 32] </ref>). Although the derivation here is presented in a somewhat different, algebraic language, many of the intermediate results we use have appeared previously in the literature in other forms (we provide references at the appropriate points).
Reference: [32] <author> U. R ude, </author> <title> Mathematical and Computational Techniques for Multilevel Adaptive Methods, </title> <booktitle> vol. 13 of SIAM Frontiers Series, </booktitle> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1993. </year>
Reference-contexts: The resulting theory, which is a variation of that presented in [39] and [14], rests on the notion of a stable subspace splitting of the original Hilbert space (cf. <ref> [31, 32] </ref>). Although the derivation here is presented in a somewhat different, algebraic language, many of the intermediate results 1 2 INTRODUCTION we use have appeared previously in the literature in other forms (we provide references at the appropriate points). <p> The resulting theory, which is a variation of that presented in [39] and [14], rests on the notion of a stable subspace splitting of the original Hilbert space (cf. <ref> [31, 32] </ref>). Although the derivation here is presented in a somewhat different, algebraic language, many of the intermediate results we use have appeared previously in the literature in other forms (we provide references at the appropriate points).
Reference: [33] <author> J. W. Ruge and K. St uben, </author> <title> Algebraic multigrid, in Multigrid Methods, </title> <editor> S. McCormick, ed., </editor> <publisher> SIAM, </publisher> <year> 1987, </year> <pages> pp. 73-130. </pages>
Reference-contexts: Multigrid methods typically perform badly, and even the regularity-free multigrid convergence theory [7] is invalid. Possible approaches include coefficient averaging methods (cf. [1]) and the explicit enforcement of the conditions (27) (cf. <ref> [1, 11, 33] </ref>). By introducing a symbolic stencil calculus and employing MAPLE or MATHEMATICA, the conditions (27) can be enforced algebraically in an efficient way for certain types of sparse matrices; details may be found for example in the appendix of [20].
Reference: [34] <author> R. S. Varga, </author> <title> Matrix Iterative Analysis, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1962. </year>
Reference-contexts: The particular framework we construct here for analyzing linear methods is based on the recent work of Xu [39], on the recent papers on multigrid and domain decomposition methods referenced therein, and on the text by Varga <ref> [34] </ref>. An alternate sufficient condition for convergence of the basic linear method is given in the following lemma, which is similar to Stein's Theorem (Theorem 7.1.8 in [30], or Theorem 6.1, page 80 in [40]). <p> Even when this is not the case, (E) is often used above in place of kEk A to obtain an estimate, and the quantity R 1 (E) = ln (E) is referred to as the asymptotic convergence rate (see page 67 of <ref> [34] </ref>, or page 88 of [40]). In [34], the average rate of convergence of m iterations is defined as the quantity R (E m ) = ln (kE m k 1=m ), the meaning of which is intuitively clear from equation (5). As noted on page 95 in [34], since (E) <p> Even when this is not the case, (E) is often used above in place of kEk A to obtain an estimate, and the quantity R 1 (E) = ln (E) is referred to as the asymptotic convergence rate (see page 67 of <ref> [34] </ref>, or page 88 of [40]). In [34], the average rate of convergence of m iterations is defined as the quantity R (E m ) = ln (kE m k 1=m ), the meaning of which is intuitively clear from equation (5). As noted on page 95 in [34], since (E) = lim m!1 kE m k 1=m <p> 67 of <ref> [34] </ref>, or page 88 of [40]). In [34], the average rate of convergence of m iterations is defined as the quantity R (E m ) = ln (kE m k 1=m ), the meaning of which is intuitively clear from equation (5). As noted on page 95 in [34], since (E) = lim m!1 kE m k 1=m for all bounded linear operators E and norms k k (Theorem 7.5-5 in [24]), it follows that lim m!1 R (E m ) = R 1 (E). <p> of linear iterations (it is called the "convergence rate" in [40], page 88), this is really an asymptotic measure, and the convergence behavior for the early iterations may be better monitored by using the norm of the propagator E directly in (6); an example is given on page 67 of <ref> [34] </ref> for which R 1 (E) gives a poor estimate of the number of iterations required. 2.4 Conjugate gradient acceleration of linear methods Consider now the linear equation Au = f in the space H.
Reference: [35] <author> J. Wang, </author> <title> Convergence analysis without regularity assumptions for multigrid algorithms based on SOR smoothing, </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 29 (1992), </volume> <pages> pp. 987-1001. </pages>
Reference-contexts: Remark 4.13. Although our language and notation is quite different, the proof we have given above for Lemma 4.10 is similar to results in [41] and [17]. Similar ideas and results appear <ref> [35] </ref>.
Reference: [36] <author> O. B. Widlund, </author> <title> Optimal iterative refinement methods, in Third International Symposium on Domain Decomposition Methods for Partial Differential Equations, </title> <editor> T. F. Chan, R. Glowinski, J. Periaux, and O. B. Widlund, eds., </editor> <address> Philadelphia, PA, 1989, </address> <publisher> SIAM, </publisher> <pages> pp. 114-125. </pages>
Reference-contexts: The following key lemma (in the case of inclusion and projection as prolongation and restriction) is sometimes referred to as Lions' Lemma [25], although the multiple-subspace case is essentially due to Wid lund <ref> [36] </ref>. Lemma 4.6 Under Assumption 4.2 it holds that S 0 kvk 2 J X (AP k v; v); 8v 2 H: Proof.
Reference: [37] <author> O. B. Widlund, </author> <title> Some schwarz methods for symmetric and nonsymmetric elliptic problems, </title> <type> Tech. Rep. 581, </type> <institution> Courant Institute of Mathematical Science, </institution> <address> New York University, New York, NY, </address> <year> 1991. </year>
Reference: [38] <author> J. Xu, </author> <title> Theory of Multilevel Methods, </title> <type> PhD thesis, </type> <institution> Department of Mathematics, Penn State University, University Park, </institution> <address> PA, </address> <month> July </month> <year> 1989. </year> <type> Technical Report AM 48. </type>
Reference-contexts: In addition to the references cited directly in the text below, the material here owes much to the following sources: <ref> [5, 6, 7, 12, 14, 17, 26, 27, 28, 38, 39] </ref>. 2. Linear operator equations In this section, we first review the theory of self-adjoint linear operators on a Hilbert space. The results required for the analysis of linear methods, as well as conjugate gradient methods, are summarized. <p> The conjugate gradient contraction number ffi cg can now be written as: ffi cg = A (BA) 1 A (BA) + 1 2 p : The following lemma is used in the analysis of multigrid and other linear preconditioners (it appears for example as Proposition 5.1 in <ref> [38] </ref>) to bound the condition number of the operator BA in terms of the extreme eigenvalues of the linear preconditioner error propagator E = I BA. We have given our own short proof of this result for completeness. <p> From above, we see that the interval is given by [(1 C 2 ); (1 + C 1 )], and by Lemma 2.12 the result follows. The next corollary appears for example as Theorem 5.1 in <ref> [38] </ref>. <p> This follows immediately from Lemma 2.13 with ffi = maxfC 1 ; C 2 g. We comment briefly on an interesting implication of Lemma 2.13, which was apparently first noticed in <ref> [38] </ref>. It seems that even if a linear method is not convergent, for example if C 1 &gt; 1 so that (E) &gt; 1, it may still be a good preconditioner. For example, if A and B are SPD, then by Corollary 2.10 we always have C 2 &lt; 1.
Reference: [39] <author> J. Xu, </author> <title> Iterative methods by space decomposition and subspace correction, </title> <journal> SIAM Review, </journal> <volume> 34 (1992), </volume> <pages> pp. 581-613. </pages>
Reference-contexts: The approach in this paper is quite similar (and owes much) to <ref> [39] </ref>, with the following exceptions. We first develop a separate and complete theory for products and sums of operators, without reference to subspaces, and then use this theory to formulate a Schwarz theory based on subspaces. <p> In x3, we present an approach for bounding the norms and condition numbers of products and sums of self-adjoint operators on a Hilbert space, derived from work due to Dryja and Widlund [14], Bramble et al. [8], and Xu <ref> [39] </ref>. This particular approach is quite general in that we establish the main norm and condition number bounds without reference to subspaces; each of the three required assumptions for the theory involve only the operators on the original Hilbert space. <p> In x4, we consider abstract Schwarz methods based on subspaces, and apply the general product and sum operator theory to these methods. The resulting theory, which is a variation of that presented in <ref> [39] </ref> and [14], rests on the notion of a stable subspace splitting of the original Hilbert space (cf. [31, 32]). <p> In addition to the references cited directly in the text below, the material here owes much to the following sources: <ref> [5, 6, 7, 12, 14, 17, 26, 27, 28, 38, 39] </ref>. 2. Linear operator equations In this section, we first review the theory of self-adjoint linear operators on a Hilbert space. The results required for the analysis of linear methods, as well as conjugate gradient methods, are summarized. <p> We note that several of these properties are commonly used, especially in the multigrid literature, although the short proofs of the results seem difficult to locate. The particular framework we construct here for analyzing linear methods is based on the recent work of Xu <ref> [39] </ref>, on the recent papers on multigrid and domain decomposition methods referenced therein, and on the text by Varga [34]. <p> This follows easily from the argument used in the proof of Lemma 2.13. The following corollary, which relates the contraction property of a linear method to the condition number of the operator BA, appears without proof as Proposition 2.2 in <ref> [39] </ref>. Corollary 2.15 If A and B are SPD, and kI BAk A ffi &lt; 1, then A (BA) 1 ffi Proof. This follows immediately from Lemma 2.13 with ffi = maxfC 1 ; C 2 g. <p> Simply using conjugate gradient acceleration in conjunction with the multigrid method often yields a convergent (even rapidly convergent) method without employing any of the special techniques that have been developed for these problems; Lemma 2.13 may be the explanation for this behavior. The following result from <ref> [39] </ref> connects the contraction number of the linear method used as the pre-conditioner to the contraction number of the resulting conjugate gradient method, and it shows that the conjugate gradient method always accelerates a linear method. <p> Theorem 2.16 If A and B are SPD, and kI BAk A ffi &lt; 1, then ffi cg &lt; ffi. Proof. An abbreviated proof appears in <ref> [39] </ref>; we fill in the details here for completeness. Assume that the given linear method has contraction number bounded as kI BAk A &lt; ffi. <p> The theory of products and sums of operators In this section, we present an approach for bounding the norms and condition numbers of products and sums of self-adjoint operators on a Hilbert space, derived from work due to Dryja and Widlund [14], Bramble et al. [8], and Xu <ref> [39] </ref>. This particular approach is quite general in that we establish the main norm and condition number bounds without reference to subspaces; each of the three required assumptions for the theory involve only the operators on the original Hilbert space. <p> We will further investigate the interaction properties more precisely in a moment. This approach to quantifying the interaction of the operators T k is similar to that taken in <ref> [39] </ref>. <p> An early approach employing an interaction matrix appears in [8]; the form appearing below is most closely related to that used in [17] and <ref> [39] </ref>. The idea of employing a strictly upper-triangular interaction matrix to improve the bound for the weak interaction property is due to Hackbusch [17]. The default bound for the strictly upper-triangular matrix is also due to Hackbusch [17]. <p> Abstract Schwarz theory In this section, we consider abstract Schwarz methods based on subspaces, and apply the general product and sum operator theory to these methods. The resulting theory, which is a variation of that presented in <ref> [39] </ref> and [14], rests on the notion of a stable subspace splitting of the original Hilbert space (cf. [31, 32]). <p> Remark 4.13. Although our language and notation is quite different, the proof we have given above for Lemma 4.10 is similar to results in [41] and [17]. Similar ideas and results appear [35]. The main ideas and techniques underlying proofs of this type were originally developed in <ref> [7, 8, 39] </ref>. 4.3 Product and sum splitting theory for non-nested Schwarz methods The main theory for Schwarz methods based on non-nested subspaces, as in the case of overlapping domain decomposition-like methods, may be summarized in the following way. <p> Remark 4.17. The theory in this section was derived mainly from work in the domain decomposition community, due chiefly to Widlund and his co-workers. In particular, our presentation owes much to <ref> [39] </ref> and [14]. ABSTRACT SCHWARZ THEORY 35 4.4 Product and sum splitting theory for nested Schwarz methods The main theory for Schwarz methods based on nested subspaces, as in the case of multigrid-like methods, is summarized in this section. <p> Remark 4.19. The theory in this section was derived from several sources; in particular, our presentation owes much to <ref> [39] </ref>, [17], and to [41]. 5. Applications to domain decomposition Domain decomposition methods were first proposed by H.A. Schwarz as a theoretical tool for studying elliptic problems on complicated domains, constructed as the union of simple domains. <p> Proof. Refer for example to the proof in <ref> [39] </ref> and the references therein to related results. 6. Applications to multigrid Multigrid methods were first developed by Federenko in the early 1960's, and have been extensively studied and developed since they became widely known in the late 1970's.
Reference: [40] <author> D. M. Young, </author> <title> Iterative Solution of Large Linear Systems, </title> <publisher> Academic Press, </publisher> <address> New York, NY, </address> <year> 1971. </year>
Reference-contexts: An alternate sufficient condition for convergence of the basic linear method is given in the following lemma, which is similar to Stein's Theorem (Theorem 7.1.8 in [30], or Theorem 6.1, page 80 in <ref> [40] </ref>). Lemma 2.3 If E fl is the A-adjoint of E, and I E fl E is A-positive, then it holds that (E) kEk A &lt; 1. Proof. By hypothesis, (A (I E fl E)u; u) &gt; 0 8u 2 H. <p> Even when this is not the case, (E) is often used above in place of kEk A to obtain an estimate, and the quantity R 1 (E) = ln (E) is referred to as the asymptotic convergence rate (see page 67 of [34], or page 88 of <ref> [40] </ref>). In [34], the average rate of convergence of m iterations is defined as the quantity R (E m ) = ln (kE m k 1=m ), the meaning of which is intuitively clear from equation (5). <p> While R 1 (E) is considered the standard measure of convergence of linear iterations (it is called the "convergence rate" in <ref> [40] </ref>, page 88), this is really an asymptotic measure, and the convergence behavior for the early iterations may be better monitored by using the norm of the propagator E directly in (6); an example is given on page 67 of [34] for which R 1 (E) gives a poor estimate of
Reference: [41] <author> H. Yserentant, </author> <title> Old and new convergence proofs for multigrid methods, </title> <journal> Acta Numerica, (1993), pp. </journal> <volume> 285-326. BIBLIOGRAPHY 47 [12, 13, 14, 15, 16, </volume> <pages> 37] </pages>
Reference-contexts: Remark 4.13. Although our language and notation is quite different, the proof we have given above for Lemma 4.10 is similar to results in <ref> [41] </ref> and [17]. Similar ideas and results appear [35]. <p> Remark 4.19. The theory in this section was derived from several sources; in particular, our presentation owes much to [39], [17], and to <ref> [41] </ref>. 5. Applications to domain decomposition Domain decomposition methods were first proposed by H.A. Schwarz as a theoretical tool for studying elliptic problems on complicated domains, constructed as the union of simple domains. <p> See for example the proof in <ref> [41] </ref>.
References-found: 41

