URL: http://www.cs.colorado.edu/~zorn/oopsla95/bonner.ps
Refering-URL: http://www.cs.colorado.edu/~zorn/oopsla95/papers.html
Root-URL: http://www.cs.colorado.edu
Email: bonner@db.toronto.edu  shrufi@db.toronto.edu  steve@genome.wi.mit.edu  
Phone: 1  
Title: Benchmarking Object-Oriented DBMSs for Workflow Management  
Author: Anthony J. Bonner Adel Shrufi Steve Rozen 
Date: 2 Whitehead/MIT  
Address: 10 King's College Rd Toronto, ON, Canada  One Kendall Square Building 300, Floor 5 Cambridge, MA 02139, USA  
Affiliation: University of Toronto Department of Computer Science  Center for Genome Research  
Pubnum: M5S 1A4  
Abstract-found: 0
Intro-found: 1
Reference: [1] <institution> Standard Guide for Laboratory Information Management Systems (LIMS). American Society for Testing and Materials, </institution> <address> 1916 Race St., Philadelphia PA 19103, U.S.A, </address> <year> 1993. </year>
Reference-contexts: Examples of central materials include insurance claims, loan applications, and laboratory samples. As a central material is processed, workflow activities gather information about it. Production workflow systems include the class of Laboratory Information Management Systems, or LIMS <ref> [26, 1, 24] </ref>. LIMS are found in analytical laboratories in a wide range of industries, including pharmaceuticals, health care, environmental monitoring, food and drug testing, and water and soil management.
Reference: [2] <author> T.L. Anderson, A.J. Berre, M. Mallison, H.H. Porter, and B. Schneider. </author> <title> The hypermodel benchmark. </title> <booktitle> In Proceedings of the International Conference on Extending Database Technology (EDBT), </booktitle> <pages> pages 317-331, </pages> <address> Venice, Italy, </address> <month> March </month> <year> 1990. </year>
Reference-contexts: A quick glance at several recent benchmarks illustrates their diversity of characteristics and requirements. For instance, the OO1, OO7 and HyperModel benchmarks <ref> [6, 5, 2] </ref> are concerned with the traversal of large graphs, which is a requirement of engineering and hypertext applications. In contrast, the SEQUOIA 2000 benchmark [36] is concerned with the manipulation of large sets of spatial and image data, such as those found in geographic information systems (GIS).
Reference: [3] <author> A. Bonner, A. Shrufi, and S. Rozen. LabFlow-1: </author> <title> a database benchmark for high-throughput workflow management. </title> <type> Technical report, </type> <institution> CSRI, University of Toronto, </institution> <year> 1995. </year> <pages> 53 pages. </pages> <note> Available at ftp://db.toronto.edu/pub/bonner/papers/workflow/report.ps.gz. </note>
Reference-contexts: These tests revealed substantial differences between these two systems and highlighted the critical importance of being able to control locality of reference to persistent data. Details of the benchmark and the test results can be found in <ref> [3] </ref>, and to a lesser extent in [4]. 1 1.1 Workflow Management Examples of workflow management are found in a wide range of industries, from banking and insurance, to telecommunications and manufacturing, to pharmaceuticals and health care [13, 33, 24, 11]. <p> Besides providing support for workflow, this approach also provides portability, since different object storage managers can be "plugged into" the DBMS. In this way, we can test a wide range of existing storage managers. Our implementation of the LabFlow-1 benchmark is based on this idea <ref> [3] </ref>. We emphasize, however, that LabFlow-1 does not depend on LabBase, which is an implementation detail. <p> Although LabFlow-1 is intended to be a general benchmark for DBMSs, we have so far used it to compare storage managers only. This is achieved by running the benchmark on versions of LabBase implemented on top of different object storage managers, as described above. In <ref> [3, 4] </ref>, we compare ObjectStore (version 3.0) and Texas (version 0.3) [34, 37]. Compared to relational systems, these storage managers have been used in few production applications, so this analysis is interesting in its own right. <p> An overview of the benchmark can be found in [4], and a detailed description can be found in <ref> [3] </ref>, which is available by anonymous ftp. Benchmark software will soon be available at the same site. 2 Programs use ObjectStore data by mapping it into virtual memory, which is at most 4Gbyte on a machine with a 32-bit address space.
Reference: [4] <author> A. Bonner, A. Shrufi, and S. Rozen. LabFlow-1: </author> <title> a database benchmark for high-throughput workflow management. </title> <booktitle> In Proceedings of the International Conference on Extending Database Technology (EDBT), </booktitle> <address> Avignon, France, </address> <month> March 25-29 </month> <year> 1996. </year> <note> Springer-Verlag, Lecture Notes in Computer Science. To appear. </note>
Reference-contexts: These tests revealed substantial differences between these two systems and highlighted the critical importance of being able to control locality of reference to persistent data. Details of the benchmark and the test results can be found in [3], and to a lesser extent in <ref> [4] </ref>. 1 1.1 Workflow Management Examples of workflow management are found in a wide range of industries, from banking and insurance, to telecommunications and manufacturing, to pharmaceuticals and health care [13, 33, 24, 11]. The task is to coordinate the various activities involved in running an enterprise. <p> Although LabFlow-1 is intended to be a general benchmark for DBMSs, we have so far used it to compare storage managers only. This is achieved by running the benchmark on versions of LabBase implemented on top of different object storage managers, as described above. In <ref> [3, 4] </ref>, we compare ObjectStore (version 3.0) and Texas (version 0.3) [34, 37]. Compared to relational systems, these storage managers have been used in few production applications, so this analysis is interesting in its own right. <p> It is designed for applications with the following characteristics and requirements: * high volume, mission critical workflows; * frequent workflow change and process re-engineering; * an audit trail of workflow activity; * complex-structured data. An overview of the benchmark can be found in <ref> [4] </ref>, and a detailed description can be found in [3], which is available by anonymous ftp. Benchmark software will soon be available at the same site. 2 Programs use ObjectStore data by mapping it into virtual memory, which is at most 4Gbyte on a machine with a 32-bit address space.
Reference: [5] <author> M.J. Carey, D.J. DeWitt, and J.F. Naughton. </author> <title> The OO7 benchmark. </title> <type> Technical report, </type> <institution> Computer Sciences Department, University of Wisconsin-Madison, </institution> <month> January </month> <year> 1994. </year> <note> Available at ftp://ftp.cs.wisc.edu/oo7/techreport.ps. </note>
Reference-contexts: Existing database benchmarks do not capture the above requirements. This should not be surprising, as it has been observed by researchers working on OODBMS benchmarks that advanced applications are too complex and diverse to be captured by a single benchmark <ref> [5, 7] </ref>. A quick glance at several recent benchmarks illustrates their diversity of characteristics and requirements. For instance, the OO1, OO7 and HyperModel benchmarks [6, 5, 2] are concerned with the traversal of large graphs, which is a requirement of engineering and hypertext applications. <p> A quick glance at several recent benchmarks illustrates their diversity of characteristics and requirements. For instance, the OO1, OO7 and HyperModel benchmarks <ref> [6, 5, 2] </ref> are concerned with the traversal of large graphs, which is a requirement of engineering and hypertext applications. In contrast, the SEQUOIA 2000 benchmark [36] is concerned with the manipulation of large sets of spatial and image data, such as those found in geographic information systems (GIS).
Reference: [6] <author> R.G.G. Cattell. </author> <title> An engineering database benchmark. </title> <booktitle> In [16], chapter 6, </booktitle> <pages> pages 247-281. </pages>
Reference-contexts: A quick glance at several recent benchmarks illustrates their diversity of characteristics and requirements. For instance, the OO1, OO7 and HyperModel benchmarks <ref> [6, 5, 2] </ref> are concerned with the traversal of large graphs, which is a requirement of engineering and hypertext applications. In contrast, the SEQUOIA 2000 benchmark [36] is concerned with the manipulation of large sets of spatial and image data, such as those found in geographic information systems (GIS).
Reference: [7] <author> A. Chaudhri. </author> <title> An Annotated Bibliography of Benchmarks for Object Databases. </title> <booktitle> SIGMOD Record, </booktitle> <volume> 24(1) </volume> <pages> 50-57, </pages> <month> March </month> <year> 1995. </year>
Reference-contexts: Existing database benchmarks do not capture the above requirements. This should not be surprising, as it has been observed by researchers working on OODBMS benchmarks that advanced applications are too complex and diverse to be captured by a single benchmark <ref> [5, 7] </ref>. A quick glance at several recent benchmarks illustrates their diversity of characteristics and requirements. For instance, the OO1, OO7 and HyperModel benchmarks [6, 5, 2] are concerned with the traversal of large graphs, which is a requirement of engineering and hypertext applications.
Reference: [8] <author> I-Min A. Chen and Victor M. Markowitz. </author> <title> The Object-Protocol Model, version 3.0. </title> <type> Technical Report LBL-32738, </type> <institution> Lawrence Berkeley Laboratory, </institution> <address> 1 Cyclotron Road, Berkeley, CA, 94720, USA, </address> <month> December </month> <year> 1994. </year> <note> This document and others on OPM available via http://gizmo.lbl.gov/ DM TOOLS/OPM/opm.html. </note>
Reference-contexts: For instance, workflow in genome laboratories requires database support for object-oriented features, such as complex data types, class hierarchies, and user-defined methods [15]. In fact, object-oriented data models have been specifically developed with laboratory workflow in mind <ref> [8] </ref>. Much of the research on workflow management in computer science has focussed on developing extended transaction models for specifying dependencies between workflow activities, especially in a heterogeneous environment [13, 25, 10, 18, 17, 12, 31]. However, the performance of WFMSs has so far received little attention.
Reference: [9] <institution> Communications of the ACM, </institution> <month> 34(11), November </month> <year> 1991. </year> <title> Special issue on the Human Genome Project. </title>
Reference-contexts: Note that each workflow may involve many transactions. High throughput workflows are also characteristic of large genome laboratories, like the Whitehead Institute/MIT Center for Genome Research (hereafter called "the Genome Center"). Workflow management is needed to support the Genome Center's large-scale genome-mapping projects <ref> [9] </ref>. Because of automation in instrumentation, data capture and workflow management, transaction rates at the Genome Center have increased dramatically in the last three years, from processing under 1,000 queries and updates per day in 1992 [14], to over 15,000 on many days in 1995. <p> These rates are expected to increase by another order of magnitude in the near future if the Genome Center begins large scale sequencing of the Human genome <ref> [9] </ref>. Moreover, unlike the simple banking debit/credit transactions of some TPC benchmarks [32], these transactions involve complex queries, and operations on complex objects, such as arrays, sequences, and nested sets. For these reasons, the Genome Center utilizes an OODBMS in tracking workflow activity [15].
Reference: [10] <author> U. Dayal, H. Garcia-Molina, M. Hsu, B. Kao, and M.-C. Shan. </author> <title> Third generation TP monitors: A database challenge. </title> <booktitle> In Proceedings of the ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 393-397, </pages> <address> Washington, DD, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: In fact, object-oriented data models have been specifically developed with laboratory workflow in mind [8]. Much of the research on workflow management in computer science has focussed on developing extended transaction models for specifying dependencies between workflow activities, especially in a heterogeneous environment <ref> [13, 25, 10, 18, 17, 12, 31] </ref>. However, the performance of WFMSs has so far received little attention. The need to study performance arises because commercial products cannot support applications with high-throughput workflows.
Reference: [11] <author> E. Dyson. </author> <title> Workflow. </title> <booktitle> In Forbes, </booktitle> <pages> page 192. </pages> <month> November 23 </month> <year> 1992. </year>
Reference-contexts: Details of the benchmark and the test results can be found in [3], and to a lesser extent in [4]. 1 1.1 Workflow Management Examples of workflow management are found in a wide range of industries, from banking and insurance, to telecommunications and manufacturing, to pharmaceuticals and health care <ref> [13, 33, 24, 11] </ref>. The task is to coordinate the various activities involved in running an enterprise. <p> Salient examples include clinical drug trials and environmental testing. Dynamic Schema Evolution. A hallmark of modern workflow management is that workflows change frequently, in response to rapidly changing business needs and circumstances <ref> [11, 33, 13] </ref>. Typically, a workflow will acquire new activities and existing activities will evolve. In both cases, the 3 changed workflow generates new kinds of information, which must be recorded in the database.
Reference: [12] <editor> A. K. Elmagarmid, editor. </editor> <title> Database Transaction Models for Advanced Applications. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1992. </year>
Reference-contexts: In fact, object-oriented data models have been specifically developed with laboratory workflow in mind [8]. Much of the research on workflow management in computer science has focussed on developing extended transaction models for specifying dependencies between workflow activities, especially in a heterogeneous environment <ref> [13, 25, 10, 18, 17, 12, 31] </ref>. However, the performance of WFMSs has so far received little attention. The need to study performance arises because commercial products cannot support applications with high-throughput workflows.
Reference: [13] <author> D. Georgakopoulos, M. Hornick, and A. Sheth. </author> <title> An overview of workflow management: From process modeling to infrastructure for automation. </title> <journal> Journal on Distributed and Parallel Database Systems, </journal> <volume> 3(2) </volume> <pages> 119-153, </pages> <month> April </month> <year> 1995. </year> <month> 6 </month>
Reference-contexts: Details of the benchmark and the test results can be found in [3], and to a lesser extent in [4]. 1 1.1 Workflow Management Examples of workflow management are found in a wide range of industries, from banking and insurance, to telecommunications and manufacturing, to pharmaceuticals and health care <ref> [13, 33, 24, 11] </ref>. The task is to coordinate the various activities involved in running an enterprise. <p> In fact, object-oriented data models have been specifically developed with laboratory workflow in mind [8]. Much of the research on workflow management in computer science has focussed on developing extended transaction models for specifying dependencies between workflow activities, especially in a heterogeneous environment <ref> [13, 25, 10, 18, 17, 12, 31] </ref>. However, the performance of WFMSs has so far received little attention. The need to study performance arises because commercial products cannot support applications with high-throughput workflows. <p> However, the performance of WFMSs has so far received little attention. The need to study performance arises because commercial products cannot support applications with high-throughput workflows. As stated in <ref> [13] </ref>, Commercial workflow management systems typically support no more than a few hundred 2 workflows a day. Some processes require handling a larger number of workflows; perhaps a number comparable to the number of transactions TP systems are capable of handling. <p> Salient examples include clinical drug trials and environmental testing. Dynamic Schema Evolution. A hallmark of modern workflow management is that workflows change frequently, in response to rapidly changing business needs and circumstances <ref> [11, 33, 13] </ref>. Typically, a workflow will acquire new activities and existing activities will evolve. In both cases, the 3 changed workflow generates new kinds of information, which must be recorded in the database.
Reference: [14] <author> Nathan Goodman. </author> <title> An object oriented DBMS war story: Developing a genome mapping database in C++. </title> <editor> In Won Kim, editor, </editor> <title> Modern Database Management: Object-Oriented and Multidatabase Technologies. </title> <publisher> ACM Press, </publisher> <year> 1994. </year>
Reference-contexts: Workflow management is needed to support the Genome Center's large-scale genome-mapping projects [9]. Because of automation in instrumentation, data capture and workflow management, transaction rates at the Genome Center have increased dramatically in the last three years, from processing under 1,000 queries and updates per day in 1992 <ref> [14] </ref>, to over 15,000 on many days in 1995. Of course, peak rates can be much higher, with a rate of 22.5 updates and queries per second recently observed over a 5-minute period.
Reference: [15] <author> Nathan Goodman, Steve Rozen, and Lincoln Stein. </author> <title> Building a laboratory information system around a C++-based object-oriented DBMS. </title> <booktitle> In Proceedings of the International Conference on Very Large Data Bases (VLDB), </booktitle> <month> September </month> <year> 1994. </year> <note> Available at ftp://genome.wi.mit.edu/ pub/papers/Y1994/building.ps.Z. </note>
Reference-contexts: Workflow management is needed to maintain throughput and control quality [23]. In addition, LIMS, like many scientific information systems, must frequently deal with complex-structured data. For instance, workflow in genome laboratories requires database support for object-oriented features, such as complex data types, class hierarchies, and user-defined methods <ref> [15] </ref>. In fact, object-oriented data models have been specifically developed with laboratory workflow in mind [8]. <p> Moreover, unlike the simple banking debit/credit transactions of some TPC benchmarks [32], these transactions involve complex queries, and operations on complex objects, such as arrays, sequences, and nested sets. For these reasons, the Genome Center utilizes an OODBMS in tracking workflow activity <ref> [15] </ref>. LabFlow-1 is motivated by their experience with this system. 2 DBMS Requirements Workflow management has numerous DBMS requirements. First, it requires standard database features, such as concurrency control, crash recovery, consistency maintenance, a high-level query language, and query optimization.
Reference: [16] <author> Jim Gray, </author> <title> editor. The Benchmark Handbook for Database and Transaction Processing Systems. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference: [17] <author> M. Hsu, Ed. </author> <title> Special issue on workflow and extended transaction systems. </title> <journal> Bulletin of the Technical Committee on Data Engineering (IEEE Computer Society), </journal> <volume> 16(2), </volume> <month> June </month> <year> 1993. </year>
Reference-contexts: In fact, object-oriented data models have been specifically developed with laboratory workflow in mind [8]. Much of the research on workflow management in computer science has focussed on developing extended transaction models for specifying dependencies between workflow activities, especially in a heterogeneous environment <ref> [13, 25, 10, 18, 17, 12, 31] </ref>. However, the performance of WFMSs has so far received little attention. The need to study performance arises because commercial products cannot support applications with high-throughput workflows.
Reference: [18] <author> M. Hsu, </author> <title> Ed. </title> <journal> Special issue on workflow systems. Bulletin of the Technical Committee on Data Engineering (IEEE Computer Society), </journal> <volume> 18(1), </volume> <month> March </month> <year> 1995. </year>
Reference-contexts: In fact, object-oriented data models have been specifically developed with laboratory workflow in mind [8]. Much of the research on workflow management in computer science has focussed on developing extended transaction models for specifying dependencies between workflow activities, especially in a heterogeneous environment <ref> [13, 25, 10, 18, 17, 12, 31] </ref>. However, the performance of WFMSs has so far received little attention. The need to study performance arises because commercial products cannot support applications with high-throughput workflows.
Reference: [19] <author> Setrag Khoshafian and Marek Buckiewicz. </author> <title> Introduction to Groupware, Workflow, and Workgroup Computing. </title> <publisher> John Wiley & Sons, Inc., </publisher> <year> 1995. </year>
Reference-contexts: The LabFlow-1 benchmark concisely describes the database requirements of a WFMS in a high-throughput genome laboratory. Although based on genome-laboratory workflow, we believe that LabFlow-1 captures the database requirements of a common class workflow management applications: those that require a production workflow system <ref> [19] </ref>. In a production workflow system, workflow activities are organized into a kind of production line, involving a mix of human and computer activities. Examples in business include insurance-claim or loan-application processing.
Reference: [20] <author> Charles Lamb, Gordon Landis, Jack Orenstein, and Dan Weinreb. </author> <title> The ObjectStore database system. </title> <journal> Communications of the ACM, </journal> <volume> 34(10) </volume> <pages> 50-63, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: One goal is to provide a tool for the Genome Center to use in analyzing object storage managers for LabBase. LabBase is currently implemented on top of ObjectStore <ref> [20, 27] </ref>. The Genome Center is evaluating alternatives to ObjectStore because of impending address-space limitations 2 and difficulties in obtaining the granularity of concurrent access required of workflow-management databases.
Reference: [21] <author> S. Leutenegger and D. Diaz. </author> <title> A Modeling Study of the TPC-C Benchmark. </title> <booktitle> In Proceedings of the ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 22-31, </pages> <address> Washington, D.C., </address> <month> May </month> <year> 1993. </year>
Reference-contexts: This application is characterized by a demand for flexible management of a stream of queries and updates, and of historical data and schema. On the surface, LabFlow-1 might appear similar to some TPC benchmarks <ref> [32, 21] </ref>, which are also based on a stream of transactions that construct a history. However, the way in which the stream is generated (the workload) is very different.
Reference: [22] <author> D.C. Mattes. </author> <title> LIMS and good laboratory practice. </title> <booktitle> In [24], </booktitle> <pages> pages 332-345. </pages> <year> 1985. </year>
Reference-contexts: The DBMS must therefore support queries and views on an historical database.We note that many commercial laboratories are legally bound to record event histories, since "Accountability is critical in tracking who is responsible for data and its approval for release" <ref> [22] </ref>. Salient examples include clinical drug trials and environmental testing. Dynamic Schema Evolution. A hallmark of modern workflow management is that workflows change frequently, in response to rapidly changing business needs and circumstances [11, 33, 13]. Typically, a workflow will acquire new activities and existing activities will evolve.
Reference: [23] <author> R.D. McDowall. </author> <title> Introduction to laboratory information management systems. </title> <booktitle> In [24], </booktitle> <pages> pages 1-16. </pages> <year> 1985. </year>
Reference-contexts: In all cases, the laboratory receives a continual stream of samples, each of which is subjected to a battery of tests and analyses. Workflow management is needed to maintain throughput and control quality <ref> [23] </ref>. In addition, LIMS, like many scientific information systems, must frequently deal with complex-structured data. For instance, workflow in genome laboratories requires database support for object-oriented features, such as complex data types, class hierarchies, and user-defined methods [15].
Reference: [24] <author> R.D. McDowell, </author> <title> editor. Laboratory Information Management Systems: Concepts, Integration, Implementation. </title> <publisher> Sigma Press, </publisher> <address> Wilmslow, U.K., </address> <year> 1985. </year>
Reference-contexts: Details of the benchmark and the test results can be found in [3], and to a lesser extent in [4]. 1 1.1 Workflow Management Examples of workflow management are found in a wide range of industries, from banking and insurance, to telecommunications and manufacturing, to pharmaceuticals and health care <ref> [13, 33, 24, 11] </ref>. The task is to coordinate the various activities involved in running an enterprise. <p> Examples of central materials include insurance claims, loan applications, and laboratory samples. As a central material is processed, workflow activities gather information about it. Production workflow systems include the class of Laboratory Information Management Systems, or LIMS <ref> [26, 1, 24] </ref>. LIMS are found in analytical laboratories in a wide range of industries, including pharmaceuticals, health care, environmental monitoring, food and drug testing, and water and soil management.
Reference: [25] <author> C. Mohan. </author> <title> Tutorial: A survey and critique of advanced transaction models. </title> <booktitle> In Proceedings of the ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> page 521, </pages> <address> Minneapolis, MN, </address> <month> May </month> <year> 1994. </year> <title> Tutorial. </title>
Reference-contexts: In fact, object-oriented data models have been specifically developed with laboratory workflow in mind [8]. Much of the research on workflow management in computer science has focussed on developing extended transaction models for specifying dependencies between workflow activities, especially in a heterogeneous environment <ref> [13, 25, 10, 18, 17, 12, 31] </ref>. However, the performance of WFMSs has so far received little attention. The need to study performance arises because commercial products cannot support applications with high-throughput workflows.
Reference: [26] <author> Allen S. Nakagawa. LIMS: </author> <title> Implementation and Management. </title> <institution> Royal Society of Chemistry, Thomas Granham House, The Science Park, </institution> <address> Cambridge CB4 4WF, England, </address> <year> 1994. </year>
Reference-contexts: Examples of central materials include insurance claims, loan applications, and laboratory samples. As a central material is processed, workflow activities gather information about it. Production workflow systems include the class of Laboratory Information Management Systems, or LIMS <ref> [26, 1, 24] </ref>. LIMS are found in analytical laboratories in a wide range of industries, including pharmaceuticals, health care, environmental monitoring, food and drug testing, and water and soil management.
Reference: [27] <institution> Object Design, Inc., </institution> <address> 25 Burlington Mall Rd., Burlington MA 01803-4194, USA. </address> <note> Manual set for ObjectStore Release 3.0 for UNIX Systems, </note> <month> December </month> <year> 1993. </year>
Reference-contexts: One goal is to provide a tool for the Genome Center to use in analyzing object storage managers for LabBase. LabBase is currently implemented on top of ObjectStore <ref> [20, 27] </ref>. The Genome Center is evaluating alternatives to ObjectStore because of impending address-space limitations 2 and difficulties in obtaining the granularity of concurrent access required of workflow-management databases.
Reference: [28] <author> P. O'Neal. </author> <title> The set query benchmark. </title> <booktitle> In [16], chapter 5, </booktitle> <pages> pages 209-245. </pages>
Reference-contexts: In contrast, the SEQUOIA 2000 benchmark [36] is concerned with the manipulation of large sets of spatial and image data, such as those found in geographic information systems (GIS). The Set Query benchmark <ref> [28] </ref> is concerned with queries for decision support, including aggregation, multiple joins and report generation. (Such queries also arise in workflow management|for process re-engineering|but they are only part of the story.) Like these benchmarks, LabFlow-1 specifically targets a broad application area: workflow management.
Reference: [29] <author> S. Rozen and L. Stein and N. Goodman. </author> <title> Labbase User Manual. </title> <note> Available at ftp:// genome.wi.mit.edu/pub/papers/Y1994/labbase-manual.ps. 7 </note>
Reference-contexts: Fortunately, one can build a specialized DBMS that supports workflow on top of a storage manager that does not. This approach is taken at the Genome Center. Their specialized DBMS (called LabBase <ref> [30, 35, 29] </ref>) provides the needed support for event histories and schema evolution on top of an object storage manager. LabBase provides a historical query language, as well as structures for rapid access into history lists.
Reference: [30] <author> Steve Rozen, Lincoln Stein, and Nathan Goodman. </author> <title> Constructing a domain-specific DBMS using a persistent object system. In M.P. </title> <editor> Atkinson, V. Benzaken, and D. Maier, editors, </editor> <booktitle> Persistent Object Systems, Workshops in Computing. </booktitle> <publisher> Springer-Verlag and British Computer Society, </publisher> <year> 1995. </year> <note> Presented at POS-VI, Sep. 1994. Available at ftp://genome.wi.mit.edu/pub/papers/Y1994/ labbase-design.ps.Z. </note>
Reference-contexts: Fortunately, one can build a specialized DBMS that supports workflow on top of a storage manager that does not. This approach is taken at the Genome Center. Their specialized DBMS (called LabBase <ref> [30, 35, 29] </ref>) provides the needed support for event histories and schema evolution on top of an object storage manager. LabBase provides a historical query language, as well as structures for rapid access into history lists.
Reference: [31] <author> M. Rusinkiewicz and A. Sheth. </author> <title> Specification and execution of transactional workflows. </title> <editor> In W. Kim, editor, </editor> <title> Modern Database Systems: The Object Model, Interoperability, and Beyond. </title> <publisher> Addison-Wesley, </publisher> <year> 1994. </year>
Reference-contexts: In fact, object-oriented data models have been specifically developed with laboratory workflow in mind [8]. Much of the research on workflow management in computer science has focussed on developing extended transaction models for specifying dependencies between workflow activities, especially in a heterogeneous environment <ref> [13, 25, 10, 18, 17, 12, 31] </ref>. However, the performance of WFMSs has so far received little attention. The need to study performance arises because commercial products cannot support applications with high-throughput workflows.
Reference: [32] <author> O. </author> <title> Serlin. The history of debit credit and the TPC. </title> <booktitle> In [16], chapter 2, </booktitle> <pages> pages 19-117. </pages>
Reference-contexts: These rates are expected to increase by another order of magnitude in the near future if the Genome Center begins large scale sequencing of the Human genome [9]. Moreover, unlike the simple banking debit/credit transactions of some TPC benchmarks <ref> [32] </ref>, these transactions involve complex queries, and operations on complex objects, such as arrays, sequences, and nested sets. For these reasons, the Genome Center utilizes an OODBMS in tracking workflow activity [15]. LabFlow-1 is motivated by their experience with this system. 2 DBMS Requirements Workflow management has numerous DBMS requirements. <p> This application is characterized by a demand for flexible management of a stream of queries and updates, and of historical data and schema. On the surface, LabFlow-1 might appear similar to some TPC benchmarks <ref> [32, 21] </ref>, which are also based on a stream of transactions that construct a history. However, the way in which the stream is generated (the workload) is very different.
Reference: [33] <author> A. Sheth. </author> <title> Workflow automation: </title> <booktitle> Applications technology and research. In Proceedings of the ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> page 469, </pages> <address> San Jose, CA, </address> <month> May </month> <year> 1995. </year> <note> Tutorial. Slides available at http:/www.cs.uga.edu/LSDIS. </note>
Reference-contexts: Details of the benchmark and the test results can be found in [3], and to a lesser extent in [4]. 1 1.1 Workflow Management Examples of workflow management are found in a wide range of industries, from banking and insurance, to telecommunications and manufacturing, to pharmaceuticals and health care <ref> [13, 33, 24, 11] </ref>. The task is to coordinate the various activities involved in running an enterprise. <p> Salient examples include clinical drug trials and environmental testing. Dynamic Schema Evolution. A hallmark of modern workflow management is that workflows change frequently, in response to rapidly changing business needs and circumstances <ref> [11, 33, 13] </ref>. Typically, a workflow will acquire new activities and existing activities will evolve. In both cases, the 3 changed workflow generates new kinds of information, which must be recorded in the database.
Reference: [34] <author> Vivek Singhal, Sheetal V. Kakkad, and Paul R. Wilson. </author> <title> Texas: an efficient, portable persistent store. </title> <booktitle> In Proceedings of the Fifth International Workshop on Persistent Object Systems (POS-V), </booktitle> <address> San Minato, Italy, </address> <month> September </month> <year> 1992. </year> <note> Available at ftp://cs.utexas.edu/ pub/garbage/texaspstore.ps. </note>
Reference-contexts: This is achieved by running the benchmark on versions of LabBase implemented on top of different object storage managers, as described above. In [3, 4], we compare ObjectStore (version 3.0) and Texas (version 0.3) <ref> [34, 37] </ref>. Compared to relational systems, these storage managers have been used in few production applications, so this analysis is interesting in its own right.
Reference: [35] <author> Lincoln Stein, Steve Rozen, and Nathan Goodman. </author> <title> Managing laboratory workflow with LabBase. </title> <booktitle> In Proceedings of the 1994 Conference on Computers in Medicine (CompMed94). </booktitle> <publisher> World Scientific Publishing Company, </publisher> <year> 1995. </year> <note> In press. Available at ftp://genome.wi.mit.edu /pub/papers/Y1995/workflow.ps.Z. </note>
Reference-contexts: Fortunately, one can build a specialized DBMS that supports workflow on top of a storage manager that does not. This approach is taken at the Genome Center. Their specialized DBMS (called LabBase <ref> [30, 35, 29] </ref>) provides the needed support for event histories and schema evolution on top of an object storage manager. LabBase provides a historical query language, as well as structures for rapid access into history lists.
Reference: [36] <author> M. Stonebraker, J. Frew, K. Gardels, and J. Meredith. </author> <title> The Sequoia 2000 storage benchmark. </title> <booktitle> In Proceedings of the ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 2-11, </pages> <address> Minneapolis, MN, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: A quick glance at several recent benchmarks illustrates their diversity of characteristics and requirements. For instance, the OO1, OO7 and HyperModel benchmarks [6, 5, 2] are concerned with the traversal of large graphs, which is a requirement of engineering and hypertext applications. In contrast, the SEQUOIA 2000 benchmark <ref> [36] </ref> is concerned with the manipulation of large sets of spatial and image data, such as those found in geographic information systems (GIS).
Reference: [37] <author> Paul R. Wilson and Sheetal V. Kakkad. </author> <title> Pointer swizzling at page fault time: Efficiently and compatibly supporting huge address spaces on standard hardware. </title> <booktitle> In International Workshop on Object Orientation in Operating Systems, </booktitle> <year> 1992. </year> <note> Available at ftp://cs.utexas.edu/ pub/garbage/swizz.ps. </note>
Reference-contexts: This is achieved by running the benchmark on versions of LabBase implemented on top of different object storage managers, as described above. In [3, 4], we compare ObjectStore (version 3.0) and Texas (version 0.3) <ref> [34, 37] </ref>. Compared to relational systems, these storage managers have been used in few production applications, so this analysis is interesting in its own right.
References-found: 37

