URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P569.ps.Z
Refering-URL: http://www.mcs.anl.gov/~gropp/papers.html
Root-URL: http://www.mcs.anl.gov
Email: @mcs.anl.gov  
Title: with special emphasis on Parallel Databases and Parallel I/O, Sept. 1996 An Experimental Evaluation of
Author: Rajeev Thakur, William Gropp, and Ewing Lusk fthakur, gropp, luskg 
Address: 9700 S. Cass Avenue Argonne, IL 60439, USA  
Affiliation: Mathematics and Computer Science Division Argonne National Laboratory  
Note: To appear in Proc. of the 3rd Int'l Conf. of the Austrian Center for Parallel Computation  
Abstract: We present the results of an experimental evaluation of the parallel I/O systems of the IBM SP and Intel Paragon using a real three-dimensional parallel application code. This application, developed by scientists at the University of Chicago, simulates the gravitational collapse of self-gravitating gaseous clouds. It performs parallel I/O by using library routines that we developed and optimized separately for the SP and Paragon. The I/O routines perform two-phase I/O and use the parallel file systems PIOFS on the SP and PFS on the Paragon. We studied the I/O performance for two different sizes of the application. In the small case, we found that I/O was much faster on the SP. In the large case, open, close, and read operations were only slightly faster, and seeks were significantly faster, on the SP; whereas, writes were slightly faster on the Paragon. The communication required within our I/O routines was faster on the Paragon in both cases. The highest read bandwidth obtained was 48 Mbytes/sec., and the highest write bandwidth obtained was 31.6 Mbytes/sec., both on the SP. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> A. Acharya, M. Uysal, R. Bennett, A. Mendelson, M. Beynon, J. Hollingsworth, J. Saltz, and A. Sussman. </author> <title> Tuning the Performance of I/O Intensive Parallel Applications. </title> <booktitle> In Proceedings of Fourth Workshop on Input/Output in Parallel and Distributed Systems, </booktitle> <pages> pages 15-27, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Baylor and Wu [3] studied the I/O characteristics of four parallel applications on an IBM SP using the Vesta parallel file system. They found I/O request rates on the order of hundreds of requests per second, mainly small request sizes, and strong temporal and spatial locality. Acharya et al. <ref> [1] </ref> report their experience in tuning the performance of four applications on an IBM SP. Del Rosario and Choudhary [9] provide an informal summary of the I/O requirements of several Grand Challenge applications. Researchers have also studied the performance of parallel file systems.
Reference: 2. <author> R. Aydt. </author> <title> A User's Guide to Pablo I/O Instrumentation. </title> <type> Technical report, </type> <institution> Dept. of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> December </month> <year> 1994. </year>
Reference-contexts: We read the variables by reading all of them in a single operation from processor 0 only and then broadcasting them to other processors. 4 Details of Experiments To study the I/O behavior of the application, we instrumented the I/O routines by using the Pablo instrumentation library <ref> [2, 19] </ref>. We instrumented all open, close, read, write, and seek calls, and also all communication required within the I/O routines. We ran the instrumented code on both the SP and Paragon and collected trace files.
Reference: 3. <author> S. Baylor and C. Wu. </author> <title> Parallel I/O Workload Characteristics Using Vesta. </title> <editor> In R. Jain, J. Werth, and J. Browne, editors, </editor> <booktitle> Input/Output in Parallel and Distributed Computer Systems, chapter 7, </booktitle> <pages> pages 167-185. </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1996. </year>
Reference-contexts: Crandall et al. [7] analyzed the I/O characteristics of three parallel applications on the Intel Paragon at Caltech. They found a wide variety of access patterns, including both read-intensive and write-intensive phases, large as well as small request sizes, and both sequential and irregular access patterns. Baylor and Wu <ref> [3] </ref> studied the I/O characteristics of four parallel applications on an IBM SP using the Vesta parallel file system. They found I/O request rates on the order of hundreds of requests per second, mainly small request sizes, and strong temporal and spatial locality.
Reference: 4. <author> R. Bordawekar, A. Choudhary, and J. del Rosario. </author> <title> An Experimental Performance Evaluation of Touchstone Delta Concurrent File System. </title> <booktitle> In Proceedings of the 7th ACM International Conference on Supercomputing, </booktitle> <pages> pages 367-376, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: Acharya et al. [1] report their experience in tuning the performance of four applications on an IBM SP. Del Rosario and Choudhary [9] provide an informal summary of the I/O requirements of several Grand Challenge applications. Researchers have also studied the performance of parallel file systems. Bor-dawekar et al. <ref> [4] </ref> performed a detailed performance evaluation of the Concurrent 2 See http://www.cacr.caltech.edu/SIO/ for information on the Scalable I/O Initiative File System (CFS) on the Intel Touchstone Delta. Kwan and Reed [15] measured the performance of the CM-5 Scalable File System.
Reference: 5. <author> D. Bradley and D. Reed. </author> <title> Performance of the Intel iPSC/2 Input/Output System. </title> <booktitle> In Fourth Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 141-144, </pages> <year> 1989. </year>
Reference-contexts: Feitelson et al. [10] studied the performance of the Vesta file system. Nieuwejaar and Kotz [16] present performance results for the Galley parallel file system. Several researchers have measured the performance of the Concurrent File System (CFS) on the Intel iPSC/2 and iPSC/860 hypercubes <ref> [5, 11, 18] </ref>. In an earlier work, we studied the I/O characteristics of a different application on the SP and Paragon [20].
Reference: 6. <author> P. Colella and P. Woodward. </author> <title> The Piecewise Parabolic Method (PPM) for Gas-Dynamical Simulations. </title> <journal> Journal of Computational Physics, </journal> <volume> 54(1) </volume> <pages> 174-201, </pages> <month> April </month> <year> 1984. </year>
Reference-contexts: This process is the fundamental mechanism through which intergalactic gases condense to form stars. The application solves the equations of compressible hydrodynamics with the inclusion of self-gravity. It uses the piecewise parabolic method <ref> [6] </ref> to solve the compressible Euler equations and a multigrid elliptic solver to compute the gravitational potential. The application uses the Chameleon library for communication [13], which is portable.
Reference: 7. <author> P. Crandall, R. Aydt, A. Chien, and D. Reed. </author> <title> Input-Output Characteristics of Scalable Parallel Applications. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: They found that file sizes were large, I/O request sizes were fairly small, data was accessed in sequence but with strides, and I/O was dominated by writes. Crandall et al. <ref> [7] </ref> analyzed the I/O characteristics of three parallel applications on the Intel Paragon at Caltech. They found a wide variety of access patterns, including both read-intensive and write-intensive phases, large as well as small request sizes, and both sequential and irregular access patterns.
Reference: 8. <author> J. del Rosario, R. Bordawekar, and A. Choudhary. </author> <title> Improved Parallel I/O via a Two-Phase Runtime Access Strategy. </title> <booktitle> In Proceedings of the Workshop on I/O in Parallel Computer Systems at IPPS '93, </booktitle> <pages> pages 56-70, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: In this application, the local array of each processor is not located contiguously in the file. Therefore, an attempt by any processor to read/write its local array directly would result in too many small read/write requests. We eliminated this problem by using two-phase I/O <ref> [8] </ref>, a technique for reading/writing distributed arrays efficiently. In two-phase I/O, as the name suggests, a distributed array is read or written in two phases.
Reference: 9. <author> J. del Rosario and A. Choudhary. </author> <title> High Performance I/O for Parallel Computers: Problems and Prospects. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 59-68, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: They found I/O request rates on the order of hundreds of requests per second, mainly small request sizes, and strong temporal and spatial locality. Acharya et al. [1] report their experience in tuning the performance of four applications on an IBM SP. Del Rosario and Choudhary <ref> [9] </ref> provide an informal summary of the I/O requirements of several Grand Challenge applications. Researchers have also studied the performance of parallel file systems.
Reference: 10. <author> D. Feitelson, P. Corbett, and J. Prost. </author> <title> Performance of the Vesta Parallel File System. </title> <booktitle> In Proceedings of the Ninth International Parallel Processing Symposium, </booktitle> <pages> pages 150-158, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: Bor-dawekar et al. [4] performed a detailed performance evaluation of the Concurrent 2 See http://www.cacr.caltech.edu/SIO/ for information on the Scalable I/O Initiative File System (CFS) on the Intel Touchstone Delta. Kwan and Reed [15] measured the performance of the CM-5 Scalable File System. Feitelson et al. <ref> [10] </ref> studied the performance of the Vesta file system. Nieuwejaar and Kotz [16] present performance results for the Galley parallel file system. Several researchers have measured the performance of the Concurrent File System (CFS) on the Intel iPSC/2 and iPSC/860 hypercubes [5, 11, 18].
Reference: 11. <author> J. French, T. Pratt, and M. Das. </author> <title> Performance Measurement of the Concurrent File System of the Intel iPSC/2 Hypercube. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> 17(1-2):115-121, January and February 1993. </note>
Reference-contexts: Feitelson et al. [10] studied the performance of the Vesta file system. Nieuwejaar and Kotz [16] present performance results for the Galley parallel file system. Several researchers have measured the performance of the Concurrent File System (CFS) on the Intel iPSC/2 and iPSC/860 hypercubes <ref> [5, 11, 18] </ref>. In an earlier work, we studied the I/O characteristics of a different application on the SP and Paragon [20].
Reference: 12. <author> N. Galbreath, W. Gropp, and D. Levine. </author> <title> Applications-Driven Parallel I/O. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 462-471, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: It uses the piecewise parabolic method [6] to solve the compressible Euler equations and a multigrid elliptic solver to compute the gravitational potential. The application uses the Chameleon library for communication [13], which is portable. Originally, the application also used the Chameleon library for I/O <ref> [12] </ref>, but we found that the Chameleon I/O routines were not well optimized for parallel I/O on the SP and Paragon. We therefore wrote special I/O routines, described below, with the same interface as the Chameleon I/O library, but separately optimized for the SP and Paragon.
Reference: 13. <author> W. Gropp and B. Smith. </author> <title> Chameleon Parallel Programming Tools User's Manual. </title> <type> Technical Report ANL-93/23, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <month> March </month> <year> 1993. </year>
Reference-contexts: The application solves the equations of compressible hydrodynamics with the inclusion of self-gravity. It uses the piecewise parabolic method [6] to solve the compressible Euler equations and a multigrid elliptic solver to compute the gravitational potential. The application uses the Chameleon library for communication <ref> [13] </ref>, which is portable. Originally, the application also used the Chameleon library for I/O [12], but we found that the Chameleon I/O routines were not well optimized for parallel I/O on the SP and Paragon.
Reference: 14. <author> V. Herrarte and E. Lusk. </author> <title> Studying Parallel Program Behavior with Upshot. </title> <type> Technical Report ANL-91/15, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <month> August </month> <year> 1991. </year>
Reference-contexts: We instrumented all open, close, read, write, and seek calls, and also all communication required within the I/O routines. We ran the instrumented code on both the SP and Paragon and collected trace files. The traces were visualized and analyzed by using Upshot <ref> [14] </ref>, a tool for studying parallel program behavior. The application only performs writes except when restarting from a checkpoint. To be able to measure the read performance as well, we restarted the code from a checkpoint each time.
Reference: 15. <author> T. Kwan and D. Reed. </author> <title> Performance of the CM-5 Scalable File System. </title> <booktitle> In Proceedings of the 8th ACM International Conference on Supercomputing, </booktitle> <pages> pages 156-165, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: Researchers have also studied the performance of parallel file systems. Bor-dawekar et al. [4] performed a detailed performance evaluation of the Concurrent 2 See http://www.cacr.caltech.edu/SIO/ for information on the Scalable I/O Initiative File System (CFS) on the Intel Touchstone Delta. Kwan and Reed <ref> [15] </ref> measured the performance of the CM-5 Scalable File System. Feitelson et al. [10] studied the performance of the Vesta file system. Nieuwejaar and Kotz [16] present performance results for the Galley parallel file system.
Reference: 16. <author> N. Nieuwejaar and D. Kotz. </author> <title> Performance of the Galley Parallel File System. </title> <booktitle> In Proceedings of Fourth Workshop on Input/Output in Parallel and Distributed Systems, </booktitle> <pages> pages 83-94, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Kwan and Reed [15] measured the performance of the CM-5 Scalable File System. Feitelson et al. [10] studied the performance of the Vesta file system. Nieuwejaar and Kotz <ref> [16] </ref> present performance results for the Galley parallel file system. Several researchers have measured the performance of the Concurrent File System (CFS) on the Intel iPSC/2 and iPSC/860 hypercubes [5, 11, 18].
Reference: 17. <author> N. Nieuwejaar, D. Kotz, A. Purakayastha, C. Ellis, and M. </author> <title> Best. File-Access Characteristics of Parallel Scientific Workloads. </title> <type> Technical Report PCS-TR95-263, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> August </month> <year> 1995. </year>
Reference-contexts: Section 4 provides details of the experiments performed. We present performance results in Section 5 and draw overall conclusions in Section 6. 2 Related Work We discuss related work in the area of I/O characterization of parallel applications and performance evaluation of parallel file systems. Nieuwejaar et al. <ref> [17] </ref> performed a tracing study of all file-related activity on the Intel iPSC/860 at NASA Ames Research Center and the Thinking Machines CM-5 at the National Center for Supercomputing Applications.
Reference: 18. <author> B. Nitzberg. </author> <title> Performance of the iPSC/860 Concurrent File System. </title> <type> Technical Report RND-92-020, </type> <institution> NAS Systems Division, NASA Ames, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: Feitelson et al. [10] studied the performance of the Vesta file system. Nieuwejaar and Kotz [16] present performance results for the Galley parallel file system. Several researchers have measured the performance of the Concurrent File System (CFS) on the Intel iPSC/2 and iPSC/860 hypercubes <ref> [5, 11, 18] </ref>. In an earlier work, we studied the I/O characteristics of a different application on the SP and Paragon [20].
Reference: 19. <author> D. Reed, R. Aydt, R. Noe, P. Roth, K. Shields, B. Schwartz, and L. Tavera. </author> <title> Scalable Performance Analysis: The Pablo Performance Analysis Environment. </title> <booktitle> In Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <pages> pages 104-113, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: We read the variables by reading all of them in a single operation from processor 0 only and then broadcasting them to other processors. 4 Details of Experiments To study the I/O behavior of the application, we instrumented the I/O routines by using the Pablo instrumentation library <ref> [2, 19] </ref>. We instrumented all open, close, read, write, and seek calls, and also all communication required within the I/O routines. We ran the instrumented code on both the SP and Paragon and collected trace files.
Reference: 20. <author> R. Thakur, E. Lusk, and W. Gropp. </author> <title> I/O Characterization of a Portable Astrophysics Application on the IBM SP and Intel Paragon. </title> <type> Technical Report MCS-P534-0895, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <month> Revised October </month> <year> 1995. </year> <title> This article was processed using the L a T E X macro package with LLNCS style </title>
Reference-contexts: Several researchers have measured the performance of the Concurrent File System (CFS) on the Intel iPSC/2 and iPSC/860 hypercubes [5, 11, 18]. In an earlier work, we studied the I/O characteristics of a different application on the SP and Paragon <ref> [20] </ref>. For that study, we used a two-dimensional astrophysics application that performs sequential I/O (only processor 0 performs all I/O) using the Unitree file system on the SP and the PFS file system on the Paragon.
References-found: 20

