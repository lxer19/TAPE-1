URL: http://www.cs.umd.edu/users/cml/work/pubs/1995-tr-nocode.ps.gz
Refering-URL: http://www.cs.umd.edu/users/cml/work/exp/
Root-URL: 
Email: fkamsties, lottg@informatik.uni-kl.de  
Title: An Empirical Evaluation of Three Defect-Detection Techniques  
Author: Erik Kamsties and Christopher M. Lott flfl 
Date: May 30, 1995  
Address: 67653 Kaiserslautern, Germany  
Affiliation: Department of Computer Science Software Technology Transfer Initiative flfl University of Kaiserslautern  
Abstract-found: 0
Intro-found: 1
Reference: [BHH78] <author> G. E. P. Box, W. G. Hunter, and J. S. Hunter. </author> <title> Statistics for Experimenters. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1978. </year>
Reference-contexts: 4 groups 5, 6 pgm. 2 (day 2) groups 3, 5 groups 1, 6 groups 2, 4 pgm. 3 (day 3) groups 4, 6 groups 2, 5 groups 1, 3 Table 3.1: Summary of experimental design 3.1 A fractional-factorial design We use a 3 2 fi 6, randomized, fractional-factorial design <ref> [BS87, BHH78, JSK91] </ref> that is summarized in Table 3.1. Subjects apply three defect-detection techniques (first independent variable) to three programs (second independent variable) in six different orders (third independent variable). The experiment requires three days, but all subjects see the same program on the same day to prevent cheating. <p> Based on our randomized approach for matching subjects, instruments, and treatments, we primarily use parametric statistics to analyze the results (see <ref> [BHH78] </ref>, pp. 46ff. and p. 104). <p> The influences of the independent variables on effectiveness can be tested in isolation from each other by partitioning the data appropriately, and then applying a hypothesis test that analyzes the variance in the data <ref> [BHH78] </ref>. This analysis allows us to determine what variance in the results is due to experimental procedures (i.e., threats to internal validity) and what variance is due to the defect-detection techniques. <p> Results We reformulate each of the questions posed in Section 6.3 as a null hypothesis, test the null hypothesis by applying an analysis of variance technique (ANOVA) to the data, and reject the null hypothesis if we attain a probability value below the generally accepted cutoff of 0.05 (see also <ref> [BHH78] </ref>, pp. 165ff). This refers to a 5% probability of mistakenly rejecting the null hypothesis, or in other words, of making a type I error. We include intermediate ANOVA results to make our analyses transparent and understandable.
Reference: [Bro80] <author> Ruven E. Brooks. </author> <title> Studying programmer behavior experimentally: The problems of proper methodology. </title> <journal> Communications of the ACM, </journal> <volume> 23(4) </volume> <pages> 207-213, </pages> <month> April </month> <year> 1980. </year>
Reference-contexts: Because we used inexperienced subjects, it is not safe to generalize our results <ref> [Bro80] </ref>. 7.3.1 Analysis of effectiveness (question 1) Question 1 is refined into the two null hypotheses that none of the independent variables (technique, program, group, or subject) affects the percentage of total possible failures observed (null 1.1) or the percentage of total faults isolated (null 1.2).
Reference: [BS87] <author> Victor R. Basili and Richard W. Selby. </author> <title> Comparing the effectiveness of software testing techniques. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 13(12) </volume> <pages> 1278-1296, </pages> <month> December </month> <year> 1987. </year>
Reference-contexts: To contribute to this base of evidence, we replicated and extended an experiment that evaluates three defect-detection techniques <ref> [BS87] </ref>, specifically code reading by stepwise abstraction [LMW79], functional (black-box) testing [How80, Mye79], and structural (white-box) testing [How78, Mar94, Mye79]. <p> We use defect as a general term that encompasses both failures and faults. See also [IEE83]. An Empirical Evaluation of Three Defect-Detection Techniques 1 ISERN 95-02 LIST OF FIGURES extending an experiment first performed by Basili and Selby (the B&S experiment) in the early 1980s <ref> [BS87, Sel85, SBPM84] </ref>. Replication helps validate empirical results published by other software-engineering researchers [DBM + 94]. Our extension consisted of adding the step of isolating faults after revealing and observing failures. Second, we have packaged and made available all materials required to perform the experiment. <p> 4 groups 5, 6 pgm. 2 (day 2) groups 3, 5 groups 1, 6 groups 2, 4 pgm. 3 (day 3) groups 4, 6 groups 2, 5 groups 1, 3 Table 3.1: Summary of experimental design 3.1 A fractional-factorial design We use a 3 2 fi 6, randomized, fractional-factorial design <ref> [BS87, BHH78, JSK91] </ref> that is summarized in Table 3.1. Subjects apply three defect-detection techniques (first independent variable) to three programs (second independent variable) in six different orders (third independent variable). The experiment requires three days, but all subjects see the same program on the same day to prevent cheating. <p> The techniques are in wide use in industry, and the procedures used in the experiment were designed by Basili and Selby to be representative of the state of the practice, although somewhat modified for experimental purposes <ref> [BS87] </ref>. We further believe that extending the procedures to include fault isolation makes them more representative of industrial practice.
Reference: [BSH86] <author> Victor R. Basili, Richard W. Selby, and David H. Hutchens. </author> <title> Experimentation in software engineering. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-12(7):733-743, </volume> <month> July </month> <year> 1986. </year>
Reference-contexts: a computer to reveal and observe failures, or use their sharp eyes to isolate faults by reading the source code. 1 For software engineering to move from a craft towards an engineering discipline, software developers need empirical evidence to help them decide what defect-detection technique to apply under various conditions <ref> [BSH86, Cur80, RBS93] </ref>. To contribute to this base of evidence, we replicated and extended an experiment that evaluates three defect-detection techniques [BS87], specifically code reading by stepwise abstraction [LMW79], functional (black-box) testing [How80, Mye79], and structural (white-box) testing [How78, Mar94, Mye79].
Reference: [BW84] <author> Victor R. Basili and David M. Weiss. </author> <title> A methodology for collecting valid software engineering data. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-10(6):728-738, </volume> <month> November </month> <year> 1984. </year>
Reference-contexts: We conducted follow-up interviews with some of the subjects to resolve cases of illegible or incomprehensible entries on the data-collection forms and to validate some of the subjective data points <ref> [BW84] </ref>. Only a few subjective measures were collected, namely experience with the C programming language, estimated mastery of the technique, and motivation for participating in the exercise.
Reference: [Cur80] <author> Bill Curtis. </author> <booktitle> Measurement and experimentation in software engineering. Proceedings of the IEEE, </booktitle> <volume> 68(9) </volume> <pages> 1144-1157, </pages> <month> September </month> <year> 1980. </year>
Reference-contexts: a computer to reveal and observe failures, or use their sharp eyes to isolate faults by reading the source code. 1 For software engineering to move from a craft towards an engineering discipline, software developers need empirical evidence to help them decide what defect-detection technique to apply under various conditions <ref> [BSH86, Cur80, RBS93] </ref>. To contribute to this base of evidence, we replicated and extended an experiment that evaluates three defect-detection techniques [BS87], specifically code reading by stepwise abstraction [LMW79], functional (black-box) testing [How80, Mye79], and structural (white-box) testing [How78, Mar94, Mye79].
Reference: [DBM + 94] <author> J. Daly, A. Brooks, J. Miller, M. Roper, and M. Wood. </author> <title> Verification of results in software maintenance through external replication. </title> <editor> In Hausi A. M uller and Mari Georges, editors, </editor> <booktitle> Proceedings of the International Conference on Software Maintenance, </booktitle> <pages> pages 50-57. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> September </month> <year> 1994. </year>
Reference-contexts: See also [IEE83]. An Empirical Evaluation of Three Defect-Detection Techniques 1 ISERN 95-02 LIST OF FIGURES extending an experiment first performed by Basili and Selby (the B&S experiment) in the early 1980s [BS87, Sel85, SBPM84]. Replication helps validate empirical results published by other software-engineering researchers <ref> [DBM + 94] </ref>. Our extension consisted of adding the step of isolating faults after revealing and observing failures. Second, we have packaged and made available all materials required to perform the experiment.
Reference: [Het76] <author> W. H. Hetzel. </author> <title> An Experimental Analysis of Program Verification Methods. </title> <type> PhD thesis, </type> <institution> University of North Carolina at Chapel Hill, </institution> <year> 1976. </year>
Reference-contexts: The result for accuracy of classifications was that 67% of the answers given by the subjects did not match our classifications, which is strong evidence that the subjects were not able to classify faults using the classification scheme. 7.5 Comparison with related work Hetzel <ref> [Het76] </ref> performed a controlled experiment with 39 student subjects to compare black-box testing, white-box testing, and individual code reading. The testing-based techniques were found to be equally effective, while code reading was found to be inferior. His subjects only observed about 50% of the revealed failures.
Reference: [How78] <author> William E. Howden. </author> <title> An evaluation of the effectiveness of symbolic testing. </title> <journal> Software-Practice and Experience, </journal> <volume> 8(4) </volume> <pages> 381-398, </pages> <month> July/August </month> <year> 1978. </year>
Reference-contexts: To contribute to this base of evidence, we replicated and extended an experiment that evaluates three defect-detection techniques [BS87], specifically code reading by stepwise abstraction [LMW79], functional (black-box) testing [How80, Mye79], and structural (white-box) testing <ref> [How78, Mar94, Mye79] </ref>. There is, of course, no silver-bullet technique, but we would like to identify conditions under which a technique helps a developer detect the most defects (i.e., maximum effectiveness) and conditions under which the technique helps a developer detect defects most rapidly (i.e., maximum efficiency).
Reference: [How80] <author> William E. Howden. </author> <title> Functional program testing. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-6:162-169, </volume> <month> March </month> <year> 1980. </year>
Reference-contexts: To contribute to this base of evidence, we replicated and extended an experiment that evaluates three defect-detection techniques [BS87], specifically code reading by stepwise abstraction [LMW79], functional (black-box) testing <ref> [How80, Mye79] </ref>, and structural (white-box) testing [How78, Mar94, Mye79].
Reference: [Hum95] <author> Watts H. Humphrey. </author> <title> A Discipline for Software Engineering. </title> <publisher> Addison-Wesley, </publisher> <year> 1995. </year>
Reference-contexts: Second, we have packaged and made available all materials required to perform the experiment. The experiment offers a chance for prospective and current software developers to evaluate their performance quantitatively, an educational experience in the spirit of Watts Humphrey's Personal Software Process <ref> [Hum95] </ref>. We hope that our efforts will make it possible for the experiment to become a standard exercise that developers will use to evaluate and sharpen their defect-detection skills. This report describes the experiment and gives the results from two internal replications of the extended experiment (the K&L experiment).
Reference: [IEE83] <institution> Institute of Electrical and Electronics Engineers. Standard Glossary of Software Engineering Terminology, </institution> <year> 1983. </year>
Reference-contexts: A fault is any problem in the program's source code that may manifest itself in a failure during execution. We use defect as a general term that encompasses both failures and faults. See also <ref> [IEE83] </ref>. An Empirical Evaluation of Three Defect-Detection Techniques 1 ISERN 95-02 LIST OF FIGURES extending an experiment first performed by Basili and Selby (the B&S experiment) in the early 1980s [BS87, Sel85, SBPM84]. Replication helps validate empirical results published by other software-engineering researchers [DBM + 94].
Reference: [JSK91] <author> Charles M. Judd, Eliot R. Smith, and Louise H. Kidder. </author> <title> Research Methods in Social Relations. </title> <publisher> Holt, Rinehart and Winston, </publisher> <address> 6 th edition, </address> <year> 1991. </year> <title> An Empirical Evaluation of Three Defect-Detection Techniques 155 ISERN 95-02 BIBLIOGRAPHY </title>
Reference-contexts: 4 groups 5, 6 pgm. 2 (day 2) groups 3, 5 groups 1, 6 groups 2, 4 pgm. 3 (day 3) groups 4, 6 groups 2, 5 groups 1, 3 Table 3.1: Summary of experimental design 3.1 A fractional-factorial design We use a 3 2 fi 6, randomized, fractional-factorial design <ref> [BS87, BHH78, JSK91] </ref> that is summarized in Table 3.1. Subjects apply three defect-detection techniques (first independent variable) to three programs (second independent variable) in six different orders (third independent variable). The experiment requires three days, but all subjects see the same program on the same day to prevent cheating. <p> As discussed in Section 6.4, our dependent variables focus on counts of failures and faults as well as the time spent to apply the techniques. 3.2 Threats to validity First we consider the problem of unknown factors that may influence the results without our knowledge, called threats to internal validity <ref> [JSK91] </ref>. Instrumentation effects (i.e., effects caused by differences among the programs) are spread equally over all techniques, and can be measured to some extent. <p> Because we only observed three of the six possible orderings, we weakened our ability to measure maturation effects. Table 7.1 presents the raw data. Subjects. The 27 students enrolled in the laboratory course Software Engineering I were required to participate as subjects, an accidental sample <ref> [JSK91] </ref> of the population of future computer-science professionals. An accidental sample means that our subjects were available due to another reason.
Reference: [KL95] <author> Erik Kamsties and Christopher M. Lott. </author> <title> An empirical evaluation of three defect-detection techniques. </title> <type> Technical Report ISERN 95-02, </type> <institution> Department of Computer Science, University of Kaiserslautern, </institution> <address> 67653 Kaiserslautern, Germany, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: We have made all materials available for software-engineering empiricists who may be interested in replicating or further extending the K&L experiment, or those who might like to use the experiment as a teaching tool. The materials in <ref> [KL95] </ref> include the instruments, lists of faults, data-collection forms, and a description of all processes involved in running the experiment, extracting the data, and performing the analyses.
Reference: [LMW79] <author> Richard C. Linger, Harlan D. Mills, and Bernard I. </author> <title> Witt. Structured Programming: Theory and Practice. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1979. </year>
Reference-contexts: To contribute to this base of evidence, we replicated and extended an experiment that evaluates three defect-detection techniques [BS87], specifically code reading by stepwise abstraction <ref> [LMW79] </ref>, functional (black-box) testing [How80, Mye79], and structural (white-box) testing [How78, Mar94, Mye79]. <p> No information is ever taken away after the subjects receive it. 4.1 Code reading own specification of the code based on the technique of reading by stepwise abstraction <ref> [LMW79] </ref>. Subjects identify prime subprograms (consecutive lines of code), write a specification for the subprogram as formally as possible, group subprograms and their specifications together, and repeat the process until they have abstracted all of the source code.
Reference: [Mar94] <author> Brian Marick. </author> <title> The Craft of Software Testing. </title> <publisher> Prentice Hall, </publisher> <year> 1994. </year>
Reference-contexts: To contribute to this base of evidence, we replicated and extended an experiment that evaluates three defect-detection techniques [BS87], specifically code reading by stepwise abstraction [LMW79], functional (black-box) testing [How80, Mye79], and structural (white-box) testing <ref> [How78, Mar94, Mye79] </ref>. There is, of course, no silver-bullet technique, but we would like to identify conditions under which a technique helps a developer detect the most defects (i.e., maximum effectiveness) and conditions under which the technique helps a developer detect defects most rapidly (i.e., maximum efficiency).
Reference: [Mye78] <author> Glenford J. Myers. </author> <title> A controlled experiment in program testing and code walkthroughs / inspections. </title> <journal> Communications of the ACM, </journal> <volume> 21(9) </volume> <pages> 760-768, </pages> <month> September </month> <year> 1978. </year>
Reference-contexts: The testing-based techniques were found to be equally effective, while code reading was found to be inferior. His subjects only observed about 50% of the revealed failures. Myers <ref> [Mye78] </ref> performed an experiment with 59 professional programmers who tested a single program using three techniques. The techniques are very similar to those used in our experiment with the exception that his code readers worked in 3-person teams.
Reference: [Mye79] <author> Glenford J. Myers. </author> <title> The Art of Software Testing. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1979. </year>
Reference-contexts: To contribute to this base of evidence, we replicated and extended an experiment that evaluates three defect-detection techniques [BS87], specifically code reading by stepwise abstraction [LMW79], functional (black-box) testing <ref> [How80, Mye79] </ref>, and structural (white-box) testing [How78, Mar94, Mye79]. <p> To contribute to this base of evidence, we replicated and extended an experiment that evaluates three defect-detection techniques [BS87], specifically code reading by stepwise abstraction [LMW79], functional (black-box) testing [How80, Mye79], and structural (white-box) testing <ref> [How78, Mar94, Mye79] </ref>. There is, of course, no silver-bullet technique, but we would like to identify conditions under which a technique helps a developer detect the most defects (i.e., maximum effectiveness) and conditions under which the technique helps a developer detect defects most rapidly (i.e., maximum efficiency). <p> However, we cannot support hypothesis H3; the self-reported measures of motivation and skill did not reliably predict effectiveness and efficiency values. When choosing and running test cases, our subjects may have been trying to falsify the model (make the program fail) or attempting to confirm the program's correct operation <ref> [Mye79, Pop92] </ref>. In brief, we might expect a developer to test his or her own code with the goal of confirming its correct operation.
Reference: [Par74] <author> H. M. Parsons. </author> <title> What happened at Hawthorne? Science, </title> <booktitle> 183(4128) </booktitle> <pages> 922-932, </pages> <month> March </month> <year> 1974. </year>
Reference-contexts: Selection effects (i.e., natural differences in human performance) cannot be measured with precision but the randomization spreads the effect across all techniques. Finally, 6 An Empirical Evaluation of Three Defect-Detection Techniques 3.2 Threats to validity ISERN 95-02 we cannot know whether there is an observer effect <ref> [Par74] </ref>; i.e., whether the subject's behavior is influenced by experimental procedures such as filling out forms. Next we consider the problems that prevent generalizing the results of the K&L experiment, called threats to external validity.
Reference: [Pop92] <author> Karl R. </author> <title> Popper. Conjectures and Refutations: The Growth of Scientific Knowledge. </title> <editor> Routledge and Keegan Paul, </editor> <address> London, 5th edition, </address> <year> 1992. </year>
Reference-contexts: However, we cannot support hypothesis H3; the self-reported measures of motivation and skill did not reliably predict effectiveness and efficiency values. When choosing and running test cases, our subjects may have been trying to falsify the model (make the program fail) or attempting to confirm the program's correct operation <ref> [Mye79, Pop92] </ref>. In brief, we might expect a developer to test his or her own code with the goal of confirming its correct operation.
Reference: [PV94] <author> Adam A. Porter and Lawrence G. Votta. </author> <title> An experiment to assess different defect detection methods for software requirements inspections. </title> <booktitle> In Proceedings of the 16 th International Conference on Software Engineering, </booktitle> <pages> pages 103-112, </pages> <year> 1994. </year>
Reference-contexts: Next we consider the problems that prevent generalizing the results of the K&L experiment, called threats to external validity. We see all of the threats described in <ref> [PV94] </ref> and then some, namely that any one of the subjects, programs, faults, fault densities, or techniques are not representative of typical software-engineering practice. The first four threats are real and can only be minimized through repeated experimentation using different subjects, programs, faults, and fault densities.
Reference: [RBS93] <author> H. Dieter Rombach, Victor R. Basili, and Richard W. Selby, </author> <title> editors. Experimental Software Engineering Issues: A critical assessment and future directions. </title> <booktitle> Lecture Notes in Computer Science Nr. </booktitle> <volume> 706, </volume> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: a computer to reveal and observe failures, or use their sharp eyes to isolate faults by reading the source code. 1 For software engineering to move from a craft towards an engineering discipline, software developers need empirical evidence to help them decide what defect-detection technique to apply under various conditions <ref> [BSH86, Cur80, RBS93] </ref>. To contribute to this base of evidence, we replicated and extended an experiment that evaluates three defect-detection techniques [BS87], specifically code reading by stepwise abstraction [LMW79], functional (black-box) testing [How80, Mye79], and structural (white-box) testing [How78, Mar94, Mye79].
Reference: [SBPM84] <author> Richard W. Selby, Victor R. Basili, Jerry Page, and Frank E. McGarry. </author> <title> Evaluating software testing strategies. </title> <booktitle> In Proceedings of the 9 th Annual Software Engineering Workshop, </booktitle> <pages> pages 42-53. </pages> <institution> NASA Goddard Space Flight Center, Greenbelt MD 20771, </institution> <year> 1984. </year>
Reference-contexts: We use defect as a general term that encompasses both failures and faults. See also [IEE83]. An Empirical Evaluation of Three Defect-Detection Techniques 1 ISERN 95-02 LIST OF FIGURES extending an experiment first performed by Basili and Selby (the B&S experiment) in the early 1980s <ref> [BS87, Sel85, SBPM84] </ref>. Replication helps validate empirical results published by other software-engineering researchers [DBM + 94]. Our extension consisted of adding the step of isolating faults after revealing and observing failures. Second, we have packaged and made available all materials required to perform the experiment.
Reference: [SCML79] <author> Sylvia B. Sheppard, Bill Curtis, Phil Milliman, and Tom Love. </author> <title> Modern coding practices and programmer performance. </title> <journal> IEEE Computer, </journal> <volume> 12 </volume> <pages> 41-49, </pages> <month> December </month> <year> 1979. </year>
Reference-contexts: Were our subjects predisposed to confirm or falsify the programs? We gave them no guidance, but believe that future replications should include explicit directions to take an approach of falsification. No explicit directions were given for the fault-isolation step. Sheppard <ref> [SCML79] </ref> points out that one of two approaches may be taken when a subject is first confronted with source code and output with visible failures.
Reference: [Sel85] <author> Richard W. Selby. </author> <title> Evaluations of Software Technologies: Testing, CLEANROOM, and Metrics. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Maryland, College Park, MD 20742, </institution> <month> May </month> <year> 1985. </year>
Reference-contexts: We use defect as a general term that encompasses both failures and faults. See also [IEE83]. An Empirical Evaluation of Three Defect-Detection Techniques 1 ISERN 95-02 LIST OF FIGURES extending an experiment first performed by Basili and Selby (the B&S experiment) in the early 1980s <ref> [BS87, Sel85, SBPM84] </ref>. Replication helps validate empirical results published by other software-engineering researchers [DBM + 94]. Our extension consisted of adding the step of isolating faults after revealing and observing failures. Second, we have packaged and made available all materials required to perform the experiment.
Reference: [Sel86] <author> Richard W. Selby. </author> <title> Combining software testing strategies: An empirical evaluation. </title> <booktitle> In Proceedings of the Workshop on Software Testing, </booktitle> <address> 15-17 July, Banff, Canada, </address> <pages> pages 82-90. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1986. </year>
Reference-contexts: However, if efficiency is a concern, the results suggest applying the functional-testing technique. The observed differences in effectiveness by fault class among the techniques suggest that a combination of the techniques might surpass the performance of any single technique <ref> [Sel86] </ref>. These types of empirical studies are one part of the software engineering profession's efforts to provide empirical evidence that will help practitioners decide which technique to apply under a given condition. The long-term goal of such work is to move software engineering from a craft towards an engineering discipline.
Reference: [VC93] <author> Jon D. Valett and Steven E. Condon. </author> <title> The (mis)use of subjective process measures in software engineering. </title> <booktitle> In Proceedings of the 18 th Annual Software Engineering Workshop, </booktitle> <pages> pages 161-175. </pages> <institution> NASA Goddard Space Flight Center, Greenbelt MD 20771, </institution> <year> 1993. </year>
Reference-contexts: We discussed the questions with the subjects before and after the experiment, and they appeared to understand the spirit behind An Empirical Evaluation of Three Defect-Detection Techniques 23 ISERN 95-02 7.2 Uncontrolled influences the questions, but subjective data is highly prone to various problems of misunderstanding <ref> [VC93] </ref>. Finally, we closely examined all output generated by the subjects in the course of the experiment to gather data about failures and faults. Some subjects reported having difficulty reporting fault-isolation data, especially those who received the source code in the first step (code readers and structural testers).
Reference: [Wey82] <author> Elaine J. Weyuker. </author> <title> On testing non-testable programs. </title> <journal> Computer Journal, </journal> <volume> 25(4) </volume> <pages> 465-470, </pages> <month> November </month> <year> 1982. </year> <title> 156 An Empirical Evaluation of Three Defect-Detection Techniques </title>
Reference-contexts: Step 2 concludes when the subjects print out their results and log off the computer. In step 3, the subjects use the specification to observe failures that were revealed in their output; they do not have a test oracle <ref> [Wey82] </ref>. After recording the failures, the subjects hand in a copy of their printed output and receive the printed source code in exchange, and begin step 4.
References-found: 28

