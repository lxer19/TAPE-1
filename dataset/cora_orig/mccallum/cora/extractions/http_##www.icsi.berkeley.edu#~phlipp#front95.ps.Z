URL: http://www.icsi.berkeley.edu/~phlipp/front95.ps.Z
Refering-URL: http://www.icsi.berkeley.edu/~phlipp/phlipp.publ.html
Root-URL: http://www.icsi.berkeley.edu
Title: Automatic Synchronization Elimination in Synchronous FORALLs  
Author: Michael Philippsen and Ernst A. Heinz 
Abstract: IPD, University of Karlsruhe, Germany, email: (phlipp j heinze) @ ira.uka.de This paper appeared in: Frontiers'95: The Fifth Symposium on the Frontiers of Massively Parallel Computation, pages 350-357, McLean, Virginia, February 6-9, 1995 Abstract This paper investigates a promising optimization technique that automatically eliminates redundant synchronization barriers in synchronous FORALLs. We present complete algorithms for the necessary program restructurings and subsequent code generation. Furthermore, we discuss the correctness, complexity, and performance of our restructuring algorithm before we finally evaluate its practical usefulness by quantitative experimentation. The experimental evaluation results are very encouraging. An implementation of the optimization algorithms in our Modula-2* compiler eliminated more than 50% of the originally present synchronization barriers in a set of seven parallel benchmarks. This barrier reduction improved the execution times of the generated programs by over 40% on a MasPar MP-1 with 16384 processors and by over 100% on a sequential workstation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S.G. Akl. </author> <title> The Design and Analysis of Parallel Algorithms. </title> <publisher> Prentice Hall, </publisher> <year> 1989. </year>
Reference-contexts: Update the interval 6 [a 0 ; b 0 ] := [x p1 ; x p ]. Iterate until the error b 0 a 0 &lt; *. Note: This problem occurs in science and engineering <ref> [1] </ref>. Comment: The solution requires access to neighboring data elements. Currently this is implemented on the Mas-Par with global communication primitives. Since relative overhead of the work incurred by unnecessary virtualiza-tion loops will increase when faster grid communication can be used instead, we expect better results in future.
Reference: [2] <author> R. Allen, D. Callahan, and K. Kennedy. </author> <title> Automatic decomposition of scientific programs for parallel execution. </title> <booktitle> In Proc. of ACM POPL'87, </booktitle> <pages> pages 63-76, </pages> <address> Munich, </address> <month> January </month> <year> 1987, </year> <note> ACM Press. </note>
Reference-contexts: When representing constraints as edges in a program dependence graph, the above example will have two edges of this type; each connects the RHS with the LHS of one assignment. The other type of constraint is induced by data dependences as defined in <ref> [2, 3, 6, 19] </ref>. Data dependences may exist within one thread or between different threads; intra-thread dependences are similar to def-use chains (no synchronization necessary) whereas inter-thread dependences require synchronization between the threads.
Reference: [3] <author> R. Allen and K. Kennedy. </author> <title> Automatic translation of Fortran programs to vector form. </title> <journal> ACM TOPLAS, </journal> <volume> 9(4) </volume> <pages> 491-592, </pages> <address> April 1987, </address> <publisher> ACM Press. </publisher>
Reference-contexts: When representing constraints as edges in a program dependence graph, the above example will have two edges of this type; each connects the RHS with the LHS of one assignment. The other type of constraint is induced by data dependences as defined in <ref> [2, 3, 6, 19] </ref>. Data dependences may exist within one thread or between different threads; intra-thread dependences are similar to def-use chains (no synchronization necessary) whereas inter-thread dependences require synchronization between the threads.
Reference: [4] <author> R. Allen and K. Kennedy. </author> <title> Vector register allocation. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 41(10) </volume> <pages> 1290-1317, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: Hence, we do not see any necessity to exclude synchronous parallelism from high-performance programming languages. As for future work, we hope to be able to improve the overall optimization results by completely integrating processor virtualization, synchronization elimination, and data prefetching. Moreover, Allen's and Kennedy's work on vector register allocation <ref> [4] </ref> contains ideas on how to reduce temporary storage consumption that seem to apply and fit well into our framework, too. The IPD Modula-2* system is freely available by anonymous ftp from ftp.ira.uka.de in pub/programming/modula2star.
Reference: [5] <author> A. Apostoli, M.J. Atallah, L.L. Larmore, and S. Mc-Faddin. </author> <title> Efficient parallel algorithms for string editing and related problems. </title> <institution> CSD-TR-724, Purdue University, Dept. of Computer Science, </institution> <month> May </month> <year> 1990. </year>
Reference-contexts: It causes intensive access to neighboring data elements. Note: The parallel solution is based on <ref> [5] </ref>. Comment: see 5.1.2. Reduction of synchronization barriers: 62 ! 26 5.1.4 Transitive Closure Problem: The adjacency matrix of a directed graph with n nodes is given. Find its transitive closure.
Reference: [6] <author> U. Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1988. </year>
Reference-contexts: When representing constraints as edges in a program dependence graph, the above example will have two edges of this type; each connects the RHS with the LHS of one assignment. The other type of constraint is induced by data dependences as defined in <ref> [2, 3, 6, 19] </ref>. Data dependences may exist within one thread or between different threads; intra-thread dependences are similar to def-use chains (no synchronization necessary) whereas inter-thread dependences require synchronization between the threads.
Reference: [7] <author> S. Chatterjee. </author> <title> Compiling nested data-parallel programs for shared memory multiprocessors. </title> <journal> ACM TOPLAS, </journal> <volume> 15(3) </volume> <pages> 400-462, </pages> <month> March </month> <year> 1993. </year>
Reference-contexts: Section 4 formulates the restructuring algorithm and discusses its properties. Finally, section 5 describes the setup and results of the experiments evaluating the effectiveness of our techniques. 2 Related Work Several researchers have studied different variations of the synchronization elimination problem in the context of compiling data-parallel programs <ref> [7, 10, 13, 14, 16, 17] </ref>. Our approach shares some similarities with the work of Hatcher and Quinn [10]. They use a data-parallel language that assigns a private address space to each virtual processor. Data from other virtual processors can only be accessed by explicit communication. <p> Here, synchronization barriers are not explicitly visible; compilers need sophisticated data dependence analysis to capture access interferences. Furthermore, our solution is more general than the work of Hatcher and Quinn because our restructuring algorithm works on (sub-) expressions and is extremely fine grained. The article <ref> [7] </ref> by Chatterjee focuses on the compilation of VCODE for shared memory multiprocessors. VCODE is a low-level, data-parallel vector language intended to serve as the target for optimizing compilers of higher level languages. It is based on the shared address space paradigm and allows for nested parallelism.
Reference: [8] <author> J. Ferrante, K.J. Ottenstein, and J.D. Warren. </author> <title> The program dependence graph and its use in optimizations. </title> <journal> ACM TOPLAS, </journal> <volume> 9(3) </volume> <pages> 319-349, </pages> <month> March </month> <year> 1987. </year>
Reference-contexts: We show that keeping them in P leads to further optimization. 3 The definition of P resembles both the program dependence graph PDG of <ref> [8] </ref> and the dependence flow graph DPG of [12].
Reference: [9] <author> S. Hangen, E.A. Heinz, P. Lukowicz, M. Philippsen, and W.F. Tichy. </author> <title> The Modula-2* environment for parallel programming. </title> <booktitle> In Proc. of the Conf. on Programming Models for Massively Parallel Computers, </booktitle> <pages> pages 43-52, </pages> <address> Berlin, September 1993, </address> <publisher> IEEE Society Press. </publisher>
Reference-contexts: Therefore we present and discuss complete algorithms automizing program restructuring and code generation. Moreover, we evaluate the practical usefulness of the proposed optimization techniques as implemented in our Modula-2* compiler <ref> [9, 14] </ref> by conducting quantitative experiments measuring the performance of Modula-2* programs on sequential workstations and a distributed memory MasPar MP-1. 3 Synchronous FORALLs When speaking of synchronous FORALLs we mean high-level language constructs that allow for problem-oriented expression of synchronous parallelism. <p> Here, we only consider those seven problems whose Modula-2* solutions involve synchronous FORALL statements. Using the PowerTest [20] to check subscripts during dependence analysis, the programs were compiled for a 16K processor MasPar MP-1 (SIMD) and a sequential SUN SparcStation-1 (SISD) by our Modula-2* compiler <ref> [9, 14] </ref>. Automatic application of the synchronization elimination scheme improved the execution times of the programs by over 40% for the MasPar and by over 100% for the Sparc on average.
Reference: [10] <author> P.J. Hatcher and M.J. Quinn. </author> <title> Data-Parallel Programming on MIMD Computers. </title> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: Section 4 formulates the restructuring algorithm and discusses its properties. Finally, section 5 describes the setup and results of the experiments evaluating the effectiveness of our techniques. 2 Related Work Several researchers have studied different variations of the synchronization elimination problem in the context of compiling data-parallel programs <ref> [7, 10, 13, 14, 16, 17] </ref>. Our approach shares some similarities with the work of Hatcher and Quinn [10]. They use a data-parallel language that assigns a private address space to each virtual processor. Data from other virtual processors can only be accessed by explicit communication. <p> Our approach shares some similarities with the work of Hatcher and Quinn <ref> [10] </ref>. They use a data-parallel language that assigns a private address space to each virtual processor. Data from other virtual processors can only be accessed by explicit communication. Hence, synchronizations are only necessary where communications occur. The number of barriers is reduced by grouping communication operations together. <p> Find its transitive closure. Approach: Process the adjacency matrix according to the property that if nodes x and m as well as nodes m and y are (transitively) adjacent, then x and y are (transitively) adjacent. Note: The problem was suggested by Hatcher <ref> [10] </ref>. Comment: see 5.1.1. Reduction of synchronization barriers: 12 ! 8 5.1.5 Game of Life Problem: Apply Conway's rules of life to a given matrix. Approach: The value of a grid point depends on the sum of the values of its neighbors. Comment: see 5.1.2.
Reference: [11] <author> J. JaJa. </author> <title> An Introduction to Parallel Algorithms. </title> <publisher> Addison-Wesley, </publisher> <year> 1992. </year>
Reference-contexts: Compute for each element its rank in the list. Approach: This problem is solved by pointer jumping. Note: Ranking the elements of a list is one of the elementary list processing tasks <ref> [11] </ref>. Comment: This problem heavily relies on the general communication mechanism of the MasPar programming language (mpl). Since the cost of communication dominates the total work, the elimination of synchronization barriers can only be effective when just a few packets are sent, which is the case for smaller problem sizes.
Reference: [12] <author> R. Johnson and K. Pingali. </author> <title> Dependence-based program analysis. </title> <booktitle> In Proc. of ACM PLDI'93, </booktitle> <pages> pages 78-89, </pages> <address> June 1993, </address> <publisher> ACM Press. </publisher>
Reference-contexts: We show that keeping them in P leads to further optimization. 3 The definition of P resembles both the program dependence graph PDG of [8] and the dependence flow graph DPG of <ref> [12] </ref>.
Reference: [13] <author> S.P. Midkiff and D.A. Padua. </author> <title> Compiler algorithms for synchronization. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 36(12) </volume> <pages> 1485-1495, </pages> <month> December </month> <year> 1987. </year>
Reference-contexts: Section 4 formulates the restructuring algorithm and discusses its properties. Finally, section 5 describes the setup and results of the experiments evaluating the effectiveness of our techniques. 2 Related Work Several researchers have studied different variations of the synchronization elimination problem in the context of compiling data-parallel programs <ref> [7, 10, 13, 14, 16, 17] </ref>. Our approach shares some similarities with the work of Hatcher and Quinn [10]. They use a data-parallel language that assigns a private address space to each virtual processor. Data from other virtual processors can only be accessed by explicit communication.
Reference: [14] <author> M. Philippsen. </author> <title> Optimierungstechniken zur Uberset-zung paralleler Programmiersprachen. </title> <type> Ph.D. Thesis, </type> <institution> University of Karlsruhe, Dept. of Informatics, </institution> <year> 1993. </year>
Reference-contexts: Section 4 formulates the restructuring algorithm and discusses its properties. Finally, section 5 describes the setup and results of the experiments evaluating the effectiveness of our techniques. 2 Related Work Several researchers have studied different variations of the synchronization elimination problem in the context of compiling data-parallel programs <ref> [7, 10, 13, 14, 16, 17] </ref>. Our approach shares some similarities with the work of Hatcher and Quinn [10]. They use a data-parallel language that assigns a private address space to each virtual processor. Data from other virtual processors can only be accessed by explicit communication. <p> Therefore we present and discuss complete algorithms automizing program restructuring and code generation. Moreover, we evaluate the practical usefulness of the proposed optimization techniques as implemented in our Modula-2* compiler <ref> [9, 14] </ref> by conducting quantitative experiments measuring the performance of Modula-2* programs on sequential workstations and a distributed memory MasPar MP-1. 3 Synchronous FORALLs When speaking of synchronous FORALLs we mean high-level language constructs that allow for problem-oriented expression of synchronous parallelism. <p> Our technique covers all these cases but due to space limita tions we must refer the interested reader to <ref> [14] </ref>. * Prakash et al. [17] eliminate intra-thread dependences from their graphs. We show that keeping them in P leads to further optimization. 3 The definition of P resembles both the program dependence graph PDG of [8] and the dependence flow graph DPG of [12]. <p> Whereas nodes of PDG and DPG are complete statements, in P evaluation ordering is expressed on a subexpression basis. 4.2.2 Evaluation Ordering Edges Although our restructuring and code generation techniques include the handling of branches, loops, and procedure calls <ref> [14] </ref>, the current presentation is restricted to flat sequences of assignments for the sake of clarity. <p> Thus, the overall complexity amounts to O (n (n + m)). 6 Code generation for nested synchronous FORALLs, branches, and loops is more complicated. Due to space limitations we kindly refer the interested reader to <ref> [14] </ref>. 4.6 Heuristics The number of temporary variables can be reduced if the freezing phase of the restructuring algorithms is more sophisticated. Instead of the "arbitrary selection" in step II.1 the successor and predecessor locality rules can be applied. <p> Here, we only consider those seven problems whose Modula-2* solutions involve synchronous FORALL statements. Using the PowerTest [20] to check subscripts during dependence analysis, the programs were compiled for a 16K processor MasPar MP-1 (SIMD) and a sequential SUN SparcStation-1 (SISD) by our Modula-2* compiler <ref> [9, 14] </ref>. Automatic application of the synchronization elimination scheme improved the execution times of the programs by over 40% for the MasPar and by over 100% for the Sparc on average.
Reference: [15] <author> M. Philippsen, E.A. Heinz, and P. Lukowicz. </author> <title> Compiling machine-independent parallel programs. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 28(8) </volume> <pages> 99-108, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: From this graph, the best/last code as given in section 4.1 is generated. Although there are only anti dependences in the example, the algorithm works for output and flow dependences as well. 5 Performance Results At the moment, our benchmark suite consists of 17 problems collected from literature <ref> [15] </ref>. Here, we only consider those seven problems whose Modula-2* solutions involve synchronous FORALL statements. Using the PowerTest [20] to check subscripts during dependence analysis, the programs were compiled for a 16K processor MasPar MP-1 (SIMD) and a sequential SUN SparcStation-1 (SISD) by our Modula-2* compiler [9, 14]. <p> The resulting general, relative performances, averaged arithmetically over all test programs per problem size, are shown below. (Only results with at least three measurement points per problem size are included in this 7 Comparisons with hand-coded programs are given in <ref> [15] </ref>. average graph.) Originally, the programs had 278 synchronization barriers with only 109 remaining after application of the elimination technique presented in the paper. On sequential machines the asynchronous FORALLs are implemented as for loops. The number of synchronization barriers is equal to the number of loops.
Reference: [16] <author> M. Philippsen and W.F. Tichy. </author> <title> Modula-2* and its compilation. </title> <booktitle> In Proc. of the First Intl. Conf. of the Austrian Center for Parallel Computation, </booktitle> <pages> pages 169-183, </pages> <month> October </month> <year> 1991, </year> <title> LNCS 591, </title> <publisher> Springer Verlag. </publisher>
Reference-contexts: Section 4 formulates the restructuring algorithm and discusses its properties. Finally, section 5 describes the setup and results of the experiments evaluating the effectiveness of our techniques. 2 Related Work Several researchers have studied different variations of the synchronization elimination problem in the context of compiling data-parallel programs <ref> [7, 10, 13, 14, 16, 17] </ref>. Our approach shares some similarities with the work of Hatcher and Quinn [10]. They use a data-parallel language that assigns a private address space to each virtual processor. Data from other virtual processors can only be accessed by explicit communication. <p> As for language framework and optimization goals, the recent work of Prakash et al. [17] closely follows our direction which originally stems from 1991 <ref> [16] </ref>. Prakash investigates synchronization elimination in the UC programming language, a data-parallel extension of C featuring a shared address space, built-in data-parallel operations like reduc 1 tions, and two new statements that introduce parallelism. UC's par and arb statements are effectively equivalent to synchronous and asynchronous FORALLs, resp.
Reference: [17] <author> S. Prakash, M. Dhagat, and R. Bagrodia. </author> <title> Synchronization issues in data-parallel languages. </title> <booktitle> In Proc. of the 6th Intl. Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 76-95, </pages> <month> August </month> <year> 1993, </year> <title> LNCS 768, </title> <publisher> Springer Verlag. </publisher>
Reference-contexts: Section 4 formulates the restructuring algorithm and discusses its properties. Finally, section 5 describes the setup and results of the experiments evaluating the effectiveness of our techniques. 2 Related Work Several researchers have studied different variations of the synchronization elimination problem in the context of compiling data-parallel programs <ref> [7, 10, 13, 14, 16, 17] </ref>. Our approach shares some similarities with the work of Hatcher and Quinn [10]. They use a data-parallel language that assigns a private address space to each virtual processor. Data from other virtual processors can only be accessed by explicit communication. <p> His experiments and performance measurements conducted for several parallel programs on 12 processors of a 16 processor shared memory Encore Multimax further confirm the effectiveness of synchronization barrier elimination. As for language framework and optimization goals, the recent work of Prakash et al. <ref> [17] </ref> closely follows our direction which originally stems from 1991 [16]. Prakash investigates synchronization elimination in the UC programming language, a data-parallel extension of C featuring a shared address space, built-in data-parallel operations like reduc 1 tions, and two new statements that introduce parallelism. <p> Furthermore, a simple cost model which may serve as the foundation of future static performance estimation is introduced. But for none of the above <ref> [17] </ref> gives any concrete algorithms or implementation schemes. Hence, it remains unclear how to successfully au-tomize the inter-play of the proposed optimizations. However, the implementation of an optimizing UC compiler is claimed to be in progress. <p> Our technique covers all these cases but due to space limita tions we must refer the interested reader to [14]. * Prakash et al. <ref> [17] </ref> eliminate intra-thread dependences from their graphs. We show that keeping them in P leads to further optimization. 3 The definition of P resembles both the program dependence graph PDG of [8] and the dependence flow graph DPG of [12].
Reference: [18] <author> W.F. Tichy and C.G. Herter. </author> <title> Modula-2*: An extension of Modula-2 for highly parallel, portable programs. </title> <institution> TR-4/90, University of Karlsruhe, Dept. of Informatics, </institution> <month> January </month> <year> 1990. </year>
Reference-contexts: Hence, the primary optimization goal is to cover all detected dependences with as few synchronizations as possible. We tackle this optimization problem by means of a restructuring technique based on source-to-source transformations in the framework of Modula-2* <ref> [18] </ref>. Our restructuring algorithm covers all language features of Modula-2*, including branches, loops, and procedure calls inside synchronous FORALLs as well as arbitrary nestings thereof. In general, reduction of synchronization barriers increases the amount of temporary storage. Thus, we face the secondary optimization problem of minimizing this increase. <p> The behavior of branches inside synchronous FORALLs is defined as follows: Modula-2* allows branches of synchronous CASE or IF statements to be executed concurrently without any synchronization. The exact synchronous semantics of nested statements are defined in <ref> [18] </ref>. 4 Synchronization Barrier Elimination 4.1 Example FORALL i : AnySimpleType IN SYNC A [i] := A [i+1] + A [i-1] + B [i]; END A number of parallel threads is created by this FORALL statement. <p> In loops inside synchronous FORALLs we do not have to consider loop carried (i.e., lexically negative) dependences, since synchronous loop semantics <ref> [18] </ref> prescribe a barrier after each iteration. * Hence, the resulting graph is acyclic. * If two references to an array are both inside the same asynchronous FORALL, no data dependence edge is required, since the programmer explicitly allows the parallel threads to proceed with arbitrary speed. * For branching statements,
Reference: [19] <author> M. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> Pitman, </publisher> <year> 1989. </year>
Reference-contexts: When representing constraints as edges in a program dependence graph, the above example will have two edges of this type; each connects the RHS with the LHS of one assignment. The other type of constraint is induced by data dependences as defined in <ref> [2, 3, 6, 19] </ref>. Data dependences may exist within one thread or between different threads; intra-thread dependences are similar to def-use chains (no synchronization necessary) whereas inter-thread dependences require synchronization between the threads.
Reference: [20] <author> M. Wolfe and C.-W. Tseng. </author> <title> The power test for data dependence. </title> <institution> TR-CSE-90-015, Oregon Graduate Institute, </institution> <month> August </month> <year> 1990. </year> <month> 8 </month>
Reference-contexts: Here, we only consider those seven problems whose Modula-2* solutions involve synchronous FORALL statements. Using the PowerTest <ref> [20] </ref> to check subscripts during dependence analysis, the programs were compiled for a 16K processor MasPar MP-1 (SIMD) and a sequential SUN SparcStation-1 (SISD) by our Modula-2* compiler [9, 14].
References-found: 20

