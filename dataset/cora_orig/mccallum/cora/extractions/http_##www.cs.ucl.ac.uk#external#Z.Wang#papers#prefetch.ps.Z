URL: http://www.cs.ucl.ac.uk/external/Z.Wang/papers/prefetch.ps.Z
Refering-URL: http://www.cs.ucl.ac.uk/external/Z.Wang/pub.html
Root-URL: http://www.cs.ucl.ac.uk
Title: Prefetching in World Wide Web  
Author: Zheng Wang and Jon Crowcroft 
Address: Gower Street, London WC1E 6BT, United Kingdom  
Affiliation: Department of Computer Science, University College London  
Abstract: This paper considers the use of prefetching in WWW for reducing perceived latency. We first look at a number of basic issues and tradeoff in prefetching. Our analysis shows that, unless the traffic is very light or the prefetching efficiency is very high, statistical prefetching may not necessarily reduce perceived delay. We then present an implementation of deterministic prefetching in a hotlist manager. 
Abstract-found: 1
Intro-found: 1
Reference: <institution> References </institution>
Reference: [1] <author> Jeffrey Mogul, </author> <booktitle> The Case for Persistent-Connection HTTP, in Proceedings of ACM SIGCOMM'95, </booktitle> <month> Oct </month> <year> 1995. </year>
Reference-contexts: In current HTTP implementation, each object in a Web page requires a new TCP connection, hence at least 2 RTTs. The new persistent HTTP can reuse TCP connections so it requires only one TCP setup for each interaction <ref> [1] </ref>. The latency experienced by users varies considerably depending on the bandwidth available, the propagation delay to the server, the size of the Web page and the server load.
Reference: [2] <author> Carlos Cunha, Azer Bestavros, and Mark Crovella, </author> <title> Characteristics of WWW Client-based Traces, </title> <type> Technical Report TR-95-010, </type> <institution> Boston University, </institution> <address> Boston, MA 02215, </address> <month> April, </month> <year> 1995. </year>
Reference-contexts: To illustrate the delay Web users currently experience, we calculated the delay distribution with 79,430 samples from the client-based traces collected at Boston University during November 1994 to February 1995 <ref> [2] </ref>. Figure 1 shows the distribution of delay for retrieving an object from non-local servers (i.e., the servers other than the ones on users' LANs). For most objects, the retrieving time is between 0.4-4 seconds.
Reference: [3] <author> Azer Bestavros, </author> <title> Using speculation to reduce server load and service time on the WWW, in Proceedings of CIKM'95: </title> <booktitle> The Fourth ACM International Conference on Information and Knowledge Management, </booktitle> <address> Baltimore, Maryland, </address> <month> November </month> <year> 1995. </year>
Reference-contexts: The information on access patterns may be derived from servers' access statistics or from clients' config 2 uration. Recent studies on the WWW traffic show that there are considerable interdependencies among consecutive accesses to some Web pages <ref> [3] </ref>. Such results are not surprising. As WWW is a hypertext based information system, it is natural that many Web pages are closely linked together.
Reference: [4] <author> J.D. </author> <title> Touch and D.J. Farber, An Experiment in Latency Reduction, </title> <booktitle> in Proceedings of IEEE INFO-COM'94, </booktitle> <address> Toronto, </address> <month> June </month> <year> 1994. </year> <month> 11 </month>
Reference-contexts: There is a general tradeoff between bandwidth and latency here. As we reduce the threshold for statistical prefetching, the latency may improve, but at at the price of increased bandwidth consumption. A study on FTP shows that the latency can be reduced by 67% for 7-fold increase in bandwidth <ref> [4] </ref>. Unfortunately, bandwidth is still scarce resources in most networks, particularly over long distance paths. Thus, statistical prefetching has be used with great care to avoid any waste of bandwidth.
Reference: [5] <author> Tom Magliery, Briand Sanderson, </author> <title> Writing Web software through client APIs, </title> <booktitle> In the Tutorial Notes of 4th World Wide Web Conference, </booktitle> <address> Boston USA, </address> <month> Nov </month> <year> 1995 </year>
Reference-contexts: Design Basics Coolist is a standalone hostlist manager based on TCL/TK, and can be used in conjunction with Netscape, Mosaic and any Web browsers which support client APIs and caching proxy. caching proxy, and controls the browser through client APIs <ref> [5] </ref>. When a user clicks an entry in the Coolist, a client API command is issued to the browser to open the web site selected.
Reference: [6] <author> Lynx, </author> <title> Lynx On-Line Documents, </title> <note> (http://ftp2.cc.ukans.edu/pub/lynx/), 1995. 12 </note>
Reference-contexts: The Coolist also acts as a simple proxy gateway for the browser passing messages to and from the browser. This allows Coolist to see all the requests and documents fetched by the browser. The prefetching is carried out by Coolist with Lynx <ref> [6] </ref> in a non-interactive mode. If a caching proxy exists on local net, it is desirable to use the caching proxy to store prefetched documents. However, it is also possible to store them on the local machine by Coolist. User Interface Entries in Coolist are organized into folders.
References-found: 7

