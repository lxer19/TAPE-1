URL: http://www.cs.nyu.edu/cs/projects/proteus/sekine/papers/muc7.ps
Refering-URL: http://www.cs.nyu.edu/cs/projects/proteus/sekine/index.html
Root-URL: http://www.cs.nyu.edu
Email: sekine@cs.nyu.edu  
Title: NYU: DESCRIPTION OF THE JAPANESE NE SYSTEM USED FOR MET-2  
Author: Satoshi Sekine 
Address: New York University 715 Broadway, 7th floor New York, NY 10003, USA  
Affiliation: Computer Science Department  
Abstract-found: 0
Intro-found: 1
Reference: [1] <institution> Defense Advanced Research Projects Agency, </institution> <note> "Proceedings of Workshop on Tipster Program Phase II" Morgan Kaufmann Publishers (1996) </note>
Reference: [2] <author> Bennett, S., Aone, C. and Lovell, C., </author> <title> "Learning to Tag Multilingual Texts Through Observation" Conference on Empirical Methods in Natural Language Processing (1997) </title>
Reference-contexts: For example, one token could be tagged as the opening of an organization, while the next token might be tagged as the closing of person name. We can think of several strategies to solve this problem (for example, the method by <ref> [2] </ref> will be described in a later section), but we used a probabilistic method. The instances in the training corpus corresponding to a leaf of the decision tree may not all have the same tag. <p> Experiment F-measure 3) 75/25 experiment 80.46 4) All training + 75/25 82.73 5) Add planet names 86.34 Table 5: Comparative Results RELATED WORK There have been several efforts to apply machine learning techniques to the same task [4] [3] [5] <ref> [2] </ref>. In this section, we will discuss a system which is one of the most advanced and which closely resembles our own [2]. A good review of most of the other systems can be found in their paper. Their system uses the decision tree algorithm and almost the same features. <p> 5) Add planet names 86.34 Table 5: Comparative Results RELATED WORK There have been several efforts to apply machine learning techniques to the same task [4] [3] [5] <ref> [2] </ref>. In this section, we will discuss a system which is one of the most advanced and which closely resembles our own [2]. A good review of most of the other systems can be found in their paper. Their system uses the decision tree algorithm and almost the same features. However, there are significant differences between the systems. <p> In this regard, we are similar to [3], which also uses a probabilistic method in their N-gram based system. This is a crucial difference which also has important consequences. Because the system of <ref> [2] </ref> makes multiple decisions at each token, they could assign multiple, possibly inconsistent tags. They solved the problem by introducing two somewhat idiosyncratic methods. One of them is the distance score, which is used to find an opening and closing pair for each named entity mainly based on distance information.
Reference: [3] <author> Bikel, D., Miller, S., Schwartz, R. and Weischedel, R., "Nymble: </author> <booktitle> a High-Performance Learning Name-finder" Proceedings of the Fifth Conference on Applied Natural Language Processing (1997) </booktitle>
Reference-contexts: Experiment F-measure 3) 75/25 experiment 80.46 4) All training + 75/25 82.73 5) Add planet names 86.34 Table 5: Comparative Results RELATED WORK There have been several efforts to apply machine learning techniques to the same task [4] <ref> [3] </ref> [5] [2]. In this section, we will discuss a system which is one of the most advanced and which closely resembles our own [2]. A good review of most of the other systems can be found in their paper. <p> In contrast, our system has only one decision tree which produces probabilities of information about the named entity. In this regard, we are similar to <ref> [3] </ref>, which also uses a probabilistic method in their N-gram based system. This is a crucial difference which also has important consequences. Because the system of [2] makes multiple decisions at each token, they could assign multiple, possibly inconsistent tags. They solved the problem by introducing two somewhat idiosyncratic methods.
Reference: [4] <author> Cowie, J., </author> <booktitle> "Description of the CRL/NMSU Systems Used for MUC-6" Proceedings of Sixth Message Understanding Conference (MUC-6) (1995) </booktitle>
Reference-contexts: Experiment F-measure 3) 75/25 experiment 80.46 4) All training + 75/25 82.73 5) Add planet names 86.34 Table 5: Comparative Results RELATED WORK There have been several efforts to apply machine learning techniques to the same task <ref> [4] </ref> [3] [5] [2]. In this section, we will discuss a system which is one of the most advanced and which closely resembles our own [2]. A good review of most of the other systems can be found in their paper.
Reference: [5] <author> Gallippi, A., </author> <booktitle> "Learning to Recognize Names Across Languages" Proceedings of the 16th International Conference on Computational Linguistics (COLING-96) (1996) </booktitle>
Reference-contexts: Experiment F-measure 3) 75/25 experiment 80.46 4) All training + 75/25 82.73 5) Add planet names 86.34 Table 5: Comparative Results RELATED WORK There have been several efforts to apply machine learning techniques to the same task [4] [3] <ref> [5] </ref> [2]. In this section, we will discuss a system which is one of the most advanced and which closely resembles our own [2]. A good review of most of the other systems can be found in their paper.
Reference: [6] <author> Matsumoto, Y., Kurohashi, S., Yamaji, O., Taeki, Y. and Nagao, M., </author> <title> "Japanese morphological analyzing System: </title> <institution> JUMAN" Kyoto University and Nara Institute of Science and Technology (1997) </institution>
Reference-contexts: ALGORITHM In this section, the algorithm of the system will be presented. There are two phases, one for creating the decision tree from training data (training phase) and the other for generating the tagged output based on the decision tree (running phase). We use a Japanese morphological analyzer, JUMAN <ref> [6] </ref> and a program package for decision trees, C4.5 [7].
Reference: [7] <author> Quinlan, R., "C4.5: </author> <title> Program for Machine Learning" Morgan Kaufmann Publishers (1993) </title>
Reference-contexts: There are two phases, one for creating the decision tree from training data (training phase) and the other for generating the tagged output based on the decision tree (running phase). We use a Japanese morphological analyzer, JUMAN [6] and a program package for decision trees, C4.5 <ref> [7] </ref>.
Reference: [8] <author> Sekine, S., </author> <title> "Homepage of data related Japanese Named Entity" http://cs.nyu.edu/cs/projects/proteus/met2j (1997) </title>
Reference-contexts: Also, we have a special dictionary which contains words written in Roman alphabet but most likely these are not an organization (e.g. TEL, FAX). We made a list of 93 such words. 1 Some of the lists are available at <ref> [8] </ref> Entity prefix name suffix Org. 14 10076/49 175 Person 0 17672 82 Loc. 0 14903 60 Date 24 199 29 Time 2 24 5 Money 15 0 39 Percent 0 99 3 Table 1: Special Dictionary Entries Creating the special dictionaries is not very easy, but it is not very <p> Here, "Training data", "Dry run data" and "Formal run data" are the data distributed by SAIC, and "seefu data" is the data created by Oki, NTT data and NYU (available through <ref> [8] </ref>).
References-found: 8

