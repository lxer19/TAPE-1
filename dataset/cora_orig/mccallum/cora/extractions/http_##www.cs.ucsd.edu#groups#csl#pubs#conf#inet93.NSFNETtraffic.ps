URL: http://www.cs.ucsd.edu/groups/csl/pubs/conf/inet93.NSFNETtraffic.ps
Refering-URL: http://www.cs.ucsd.edu/groups/csl/pubs/author.html
Root-URL: http://www.cs.ucsd.edu
Email: kc@cs.ucsd.edu  polyzos@cs.ucsd.edu  hwb@sdsc.edu  
Title: Long-term traffic aspects of the NSFNET  
Author: Kimberly C. Claffy George C. Polyzos Hans-Werner Braun 
Address: La Jolla, CA 92093-0114  San Diego, CA 92186-9784  
Affiliation: Computer Systems Laboratory University of California, San Diego  San Diego Supercomputer Center  
Abstract: We present the architecture for data collection for the T3 NSFNET backbone service, and difficulties with using the collected statistics for long-term network forecasting of certain traffic aspects. We describe relevant aspects of the T3 backbone architecture, describe the instrumentation for the statistics collection process, and how it differs from that on the T1 backbone. We then present long-term NSFNET data to elucidate long term trends in both the reachability of Internet components via the NSFNET as well as the growing cross-section of traffic. We focus on the difficulties of forecasting and planning for these two traffic aspects in an infrastructure whose protocol architecture and instrumentation for data collection was not designed to support such objectives. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> ANS. ARTS: ANSnet Router Statistics software, </institution> <year> 1992. </year>
Reference-contexts: Interface statistics derive from programs using the Simple Network Management Protocol (SNMP) [4]. Specialized software packages perform packet categorization: the T1 backbone utilized the NNStat [3] package for collection; the T3 backbone utilizes the ARTS (ANSnet Router Traffic Statistics) <ref> [1] </ref> package, which encompasses similar functionality. III.A. Interface performance The mechanism for collecting interface performance statistics did not change from the T1 to the T3 backbone.
Reference: [2] <author> J. Becker. </author> <type> personal communication, </type> <month> January </month> <year> 1993. </year> <note> e-mail about T3 backbone. </note>
Reference-contexts: an IBM RS/6000 at the ANS NOC; as an example of the memory requirements, this machine collected approximately 9 The sampling microcode in the subsystem does not send the whole packet, but rather the first min (packetsize; 128) bytes of the packet, starting from the beginning of the IP header <ref> [2] </ref>. CBA-4 Tracking NSFNET traffic aspects Proc.
Reference: [3] <author> R.T. Braden and A. DeSchon. NNStat: </author> <title> Internet statistics collection package. Introduction and User Guide. </title> <type> Technical Report RR-88-206, ISI, </type> <institution> USC, </institution> <year> 1988. </year> <note> Available for a-ftp from isi.edu. </note>
Reference-contexts: Interface statistics derive from programs using the Simple Network Management Protocol (SNMP) [4]. Specialized software packages perform packet categorization: the T1 backbone utilized the NNStat <ref> [3] </ref> package for collection; the T3 backbone utilizes the ARTS (ANSnet Router Traffic Statistics) [1] package, which encompasses similar functionality. III.A. Interface performance The mechanism for collecting interface performance statistics did not change from the T1 to the T3 backbone. <p> To categorize IP packets entering the T1 backbone based on information contained in packet headers, this processor would examine the header of every packet traversing the intra-NSS processor intercommunication facility, and use a modified version of the NNStat package <ref> [3] </ref> to build statistical objects based on the collected information. Because all packets traveled across the interconnection facility on their way through the node, the collection processor could passively collect data without affecting switching throughput.
Reference: [4] <author> J.D. Case, M. Fedor, M.L. Schoffstall, and C. Davin. </author> <title> Simple Network Management Protocol (SNMP). Internet Request for Comments Series RFC 1157, </title> <year> 1987. </year>
Reference-contexts: The principal sources of information about the T3 backbone come from routine collection of three classes of network statistics: interface statistics; packet categorization; and internodal delays. Interface statistics derive from programs using the Simple Network Management Protocol (SNMP) <ref> [4] </ref>. Specialized software packages perform packet categorization: the T1 backbone utilized the NNStat [3] package for collection; the T3 backbone utilizes the ARTS (ANSnet Router Traffic Statistics) [1] package, which encompasses similar functionality. III.A.
Reference: [5] <author> B. Chinoy and H.-W. Braun. </author> <title> The National Science Foundation Network. </title> <type> Technical Report GA-A21029, </type> <institution> SDSC, </institution> <year> 1992. </year>
Reference-contexts: 1 This research is supported by a grant of the National Science Foundation (NCR-9119473), and a joint study agreement with the International Business Machines, Inc. 2 Chinoy and Braun <ref> [5] </ref> offer a more detailed description of the NSFNET. mid-level networks themselves, and campus networks. The hierarchical structure includes a large fraction of the research and educational community, and even extends into a global arena via international connections. Figure 1 shows the logical topology of the backbone.
Reference: [6] <author> B. Chinoy and P. Smith. </author> <note> Final version of Aborted T3. ANS Update, </note> <month> November </month> <year> 1992. </year>
Reference-contexts: However, no guarantee is given for the completeness of the data or its accuracy. II. Architecture of the T3 backbone We review a few of the network parameters that affect traffic flow in the current T3 backbone. Chi-noy and Smith <ref> [6] </ref> present details of the T3 network architecture, which evolved from the experience of managing the T1 network. Backbone nodes, the core packet switches in the T3 infrastructure, are designated as either Exterior Nodal Switching Subsystems (ENSSs) or Core Nodal Switching Subsystems (CNSSs). <p> The physical and electrical interfacing to these lines is handled by a Data Service Unit (DSU). Nearly all of the DS3 circuits are terrestrial fiber-optic lines; other possible media are microwave and copper <ref> [6] </ref>. To access external client networks, the T3 backbone nodes currently use Ethernet and FDDI interfaces, with packet size limits of 1.5 and 4 kilobytes, respectively. Each packet is also embedded within an Ethernet or FDDI frame, which the LAN drivers at the endpoints append and remove.
Reference: [7] <author> K. Claffy, H.-W. Braun, and G. Polyzos. </author> <title> Application of sampling methodologies to network traffic characterization. </title> <note> to appear in SIGCOMM '93, </note> <month> September </month> <year> 1993. </year>
Reference-contexts: Nonetheless, the nodal transmission rate did eventually surpass the capability to keep up with the statistics collection in parallel, and this processor had to eventually revert to sampling <ref> [7] </ref>. The design of the T3 backbone required significant modification to this data collection mecha 7 Error conditions on the interface include HDLC checksum errors, invalid packet length, and queue overflows resulting in discards.
Reference: [8] <author> K. Claffy, G. C. Polyzos, and H.-W. Braun. </author> <title> Traffic characteristics of the T1 NSFNET backbone. </title> <booktitle> In Proc. INFOCOM '93, </booktitle> <address> San Fran-cisco, CA, </address> <month> April </month> <year> 1993. </year>
Reference: [9] <author> P. Ford, Y. Rekhter, and H.-W. Braun. </author> <title> Improving the ROuting and Addressing of the Internet protocol. </title> <journal> IEEE Network, </journal> <month> May </month> <year> 1993. </year>
Reference-contexts: This methodology, called Classless Inter Domain Routing (CIDR), uses clustered Class C numbers as an alternative to Class B numbers, with network masks allowing for a number aggregation <ref> [9] </ref>. Although these graphs give some sense of the increasing geographic and administrative scope of the NSFNET, a discussion of the significance of the IP addresses structure to the infrastructure will allow clearer understanding of the growth in service reachability of the NSFNET. <p> We present IP addresses as they are registered with the NSF Inter-nic at the top level, and then delegated to other authorities from there. Our primary focus here is the distribution and registration of IP network numbers, and not issues such as network classes, multicasting, or CIDR <ref> [9] </ref>. CBA-6 Tracking NSFNET traffic aspects Proc. INET '93 k claffy and C IP network numbers (Data source: Internic, personal communication) The IP address space architecture originated with RFC 791 [18], the initial Internet Protocol specification that defined a pool of available network numbers.
Reference: [10] <author> E. Gerich. </author> <title> Guidelines for management of ip address space. Obsoleted by RFC 1466, </title> <month> Octo-ber </month> <year> 1992. </year>
Reference-contexts: The In-ternic Registrar, on behalf of the Internet community, now formally registers these assigned network numbers in a data base that also includes address information of the institution responsible for the network, and other attributes. With the advent of RFC1366 <ref> [10] </ref> [11] in October 1992, the Internic began to assign addresses according to the geographic location of the requestor, with a strong preference for assigning single or multiple class C addresses rather than a Class B address.
Reference: [11] <author> E. Gerich. </author> <title> Guidelines for management of ip address space. </title> <booktitle> Obsoletes RFC 1366, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: The In-ternic Registrar, on behalf of the Internet community, now formally registers these assigned network numbers in a data base that also includes address information of the institution responsible for the network, and other attributes. With the advent of RFC1366 [10] <ref> [11] </ref> in October 1992, the Internic began to assign addresses according to the geographic location of the requestor, with a strong preference for assigning single or multiple class C addresses rather than a Class B address.
Reference: [12] <editor> J. Postel J. Reynolds. </editor> <booktitle> Assigned numbers. </booktitle> <pages> 138 pages, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: Originally, the Internet Assigned Numbers Authority, 13 on behalf of DARPA, administered a space of 1 to 255 as the group of well known or assigned port numbers, reserved for specific applications. For example, telnet received port assignment 23 <ref> [12] </ref>. <p> Eventually network users began to use numbers even above 1024 to specify further services, extending the lack of community coordination further. Examples include XDR/NFS (port 2049), and X-Windows (port 6000+), and port 4444 for (some) MBONE video multicasts. In July 1992 <ref> [12] </ref> the IANA extended the range of ports for which it manages assignments to the 0-1023 range. At this time the IANA also began to track, to the best of its ability, a set of registered ports within the full range of 1024-65535. <p> IANA does not attempt to control the assignments of these ports; but only registers uses of which is is aware as a convenience to the community <ref> [12] </ref>. These port numbers are the only mechanism via which the NSFNET can monitor statistics on the aggregated distribution of applications on the backbone. Thus the proliferation of uncoordinated number assignments imposes ambiguity into this categorization of packets by application. of port numbers we have discussed.
Reference: [13] <author> S. Kirkpatrick, M. Stahl, and M. Recher. </author> <title> Internet numbers. </title> <address> RFC1166, </address> <month> July </month> <year> 1990. </year>
Reference-contexts: The number of allocatable class A, B and C network numbers is 2 7 , 2 14 , and 2 21 , respectively <ref> [13] </ref>. the NSFNET backbone [16]. These configured NSFNET numbers are the only destinations to which the NSFNET backbone will route packets, and also exhibits quadratic growth over the last few years, including substantial increases in the international area.
Reference: [14] <author> K. McCloghrie and ed. M. T. Rose. </author> <title> Management Information Base for network management of TCP/IP-based internets, mib-ii. Internet Request for Comments Series RFC 1213, </title> <year> 1991. </year>
Reference-contexts: The single counter does not distin guish among these error conditions. 8 Object definitions found in McCloghrie and Rose <ref> [14] </ref> [15]. nism. This modification actually occurred in two phases. In the first statistics collection design, all forwarded packets had to traverse the main RS/6000 processor itself, imposing a burden on the single packet forwarding engine and impeding comprehensive statistics collection.
Reference: [15] <author> K. McCloghrie and M. T. Rose. </author> <title> Management Information Base for network management of TCP/IP-based internets. Internet Request for Comments Series RFC 1156, </title> <year> 1990. </year>
Reference-contexts: The single counter does not distin guish among these error conditions. 8 Object definitions found in McCloghrie and Rose [14] <ref> [15] </ref>. nism. This modification actually occurred in two phases. In the first statistics collection design, all forwarded packets had to traverse the main RS/6000 processor itself, imposing a burden on the single packet forwarding engine and impeding comprehensive statistics collection.
Reference: [16] <institution> Network information services, </institution> <month> February </month> <year> 1993. </year> <note> Data available on nis.nsf.net: /nsf/statistics. </note>
Reference-contexts: The number of allocatable class A, B and C network numbers is 2 7 , 2 14 , and 2 21 , respectively [13]. the NSFNET backbone <ref> [16] </ref>. These configured NSFNET numbers are the only destinations to which the NSFNET backbone will route packets, and also exhibits quadratic growth over the last few years, including substantial increases in the international area. <p> Merit/ANS categorizes packets into these ports if either the source or destination port in a given packet matched one of these numbers. However even within this range not all ports have a generally known assignment, so packets using such undefined ports go into an unknown port category <ref> [16] </ref>. Figures 9 and 10 use this collected data to categorize the proportion of traffic on the network by category since August 1989.
Reference: [17] <author> J. B. Postel. </author> <title> Internet Control Message Protocol. </title> <type> RFC 792, </type> <year> 1981. </year>
Reference-contexts: III.C. Internodal latency On the T1 backbone, Merit used the ping utility to perform internodal latency assessments. Ping probes from one endpoint of the network to another using the ICMP Echo functionality <ref> [17] </ref> to record the round-trip times (RTT) between the two endpoints. As of 1 February 1993, ANS collects delay data between nodes using the yet-another-ping (yap) utility, which runs on each backbone node and can measure delay to the microsecond level using the AIX system clock.
Reference: [18] <author> J. B. Postel. </author> <title> Internet Protocol. Internet Request for Comments Series RFC 791, </title> <year> 1981. </year>
Reference-contexts: CBA-6 Tracking NSFNET traffic aspects Proc. INET '93 k claffy and C IP network numbers (Data source: Internic, personal communication) The IP address space architecture originated with RFC 791 <ref> [18] </ref>, the initial Internet Protocol specification that defined a pool of available network numbers. Ignoring some special cases, such as multicast addresses, every network number on the Internet came from this pool of available network numbers.
Reference: [19] <author> Y. Rekhter. </author> <title> NSFNET backbone SPF based Interior Gateway Protocol. Internet Request for Comments Series RFC 1074, </title> <year> 1990. </year> <month> CBA-12 </month>
Reference-contexts: The T3 routing technology and architecture is functionally equivalent to that on the T1 network. Packets travel through the network individually and are passed from node to node aided by an adaptive, distributed routing procedure based on the standard IS-IS protocol <ref> [19] </ref>. Buffering on the output queues of the nodes contributes to the latency of the delivery of packets to the destination. On the T3 backbone, the number and size of buffers in each node depends on the interface type and operating system version.
References-found: 19

