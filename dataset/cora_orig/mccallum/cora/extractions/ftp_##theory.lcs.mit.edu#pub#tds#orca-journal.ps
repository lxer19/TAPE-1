URL: ftp://theory.lcs.mit.edu/pub/tds/orca-journal.ps
Refering-URL: http://theory.lcs.mit.edu/tds/orca.html
Root-URL: 
Title: Implementing Sequentially Consistent Shared Objects using Broadcast and Point-To-Point Communication  
Author: Alan Feketey M. Frans Kaashoekz Nancy Lynchz 
Address: Sydney 2006, Australia. Cambridge MA 02139, U.S.A.  
Affiliation: yDepartment of Computer Science F09 MIT Laboratory for Computer Science University of  
Abstract: This paper presents and proves correct a distributed algorithm that implements a sequentially consistent collection of shared read/update objects. This algorithm is a generalization of one used in the Orca shared object system. The algorithm caches objects in the local memory of processors according to application needs; each read operation accesses a single copy of the object, while each update accesses all copies. The algorithm uses broadcast communication when it sends messages to replicated copies of an object, and it uses point-to-point communication when a message is sent to a single copy, and when a reply is returned. Copies of all the objects are kept consistent using a strategy based on sequence numbers for broadcasts. The algorithm is presented in two layers. The lower layer uses the given broadcast and point-to-point communication services, plus sequence numbers, to provide a new communication service called a context multicast channel. The higher layer uses a context multicast channel to manage the object replication in a consistent fashion. Both layers and their combination are described and verified formally, using the I/O automaton model for asynchronous concurrent systems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Adve and K. Gharachorloo. </author> <title> Shared memory consistency models: A tutorial. </title> <journal> IEEE Computer, </journal> <volume> 29(12) </volume> <pages> 66-76, </pages> <year> 1996. </year>
Reference-contexts: Other papers exploring correctness conditions for shared memory and algorithms that implement them include [2, 3, 5, 11, 12, 15, 17, 18, 19, 20, 21, 28, 33]. A valuable survey of these ideas is given by Adve and Gharachorloo <ref> [1] </ref>. 4 In most of previous work, memory is modelled as a collection of items that are accessed through read and write operations. The study of correctness for shared memory with more general data types was initiated by Herlihy and Wing [22].
Reference: [2] <author> S. Adve and M. Hill. </author> <title> Weak ordering anew definition. </title> <booktitle> In Proc. Seventeenth Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-14, </pages> <month> May </month> <year> 1990. </year>
Reference: [3] <author> S. V. Adve. </author> <title> Designing memory consistency models for shared memory multiprocessors. </title> <type> Technical Report 1198, </type> <institution> University of Wisconsin, Madison, </institution> <year> 1993. </year>
Reference: [4] <author> Y. Afek, G. Brown, and M. Merritt. </author> <title> Lazy caching. </title> <journal> ACM. Trans. on Programming Languages and Systems, </journal> <volume> 15(1) </volume> <pages> 182-205, </pages> <month> Jan. </month> <year> 1989. </year>
Reference-contexts: There are several formalizations of the notion of sequentially consistent memory, differing in subtle ways. We use the state machine definition of Afek, Brown and Merritt <ref> [4] </ref>. This research was supported in part by ARPA contracts N00014-92-J-4033 and F19628-95-C-0118, by NSF grant 9225124-CCR and CCR-9520298, by ONR-AFOSR contract F49620-94-1-0199 and by AFOSR contract F49620-97-1-0337 also by ONR contract N00014-94-1-0985 and an NSF Young Investigator Award. <p> Sequential consistency was first defined by Lamport [26]; in this paper, we use an alternative formulation proposed by Afek et al. <ref> [4] </ref>, based on I/O automata. Other papers exploring correctness conditions for shared memory and algorithms that implement them include [2, 3, 5, 11, 12, 15, 17, 18, 19, 20, 21, 28, 33]. <p> Specifically, we let AM, the atomic memory automaton, be just like the serial object automaton M serial defined by Afek, Brown and Merritt <ref> [4] </ref> for the given collection of objects, except that we generalize it to allow updates that apply functions rather than just blind writes. The code appears in Figure 3. <p> There is a supportive partial order for the sequences fijc. Then A is a sequentially consistent shared object system. Proof: Immediate by Lemma 3.1. The literature contains other definitions of sequential consistency, besides the automaton-based one we have adopted from Afek, Brown and Merritt <ref> [4] </ref>.
Reference: [5] <author> M. Ahamad, P.W. Hutto, and R. John. </author> <title> Implementing and programming causal distributed shared memory. </title> <booktitle> In Proc. Eleventh International Conference on Distributed Computing Systems, </booktitle> <pages> pages 274-281, </pages> <address> Arlington, TX, </address> <month> May </month> <year> 1991. </year>
Reference: [6] <author> Y. Amir, D. Dolev, S. Kramer, and D. Malki. Transis: </author> <title> A communication subsystem for high availability. </title> <booktitle> In Proc. Twenty-Second International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 76-84, </pages> <address> Boston, MA, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: which every fair trace fi: * satisfies the basic reliability requirements, and * has a well-founded partial order Q such that fi and Q are receive consistent, context safe, and total. 4.2 Other Types of Multicast Channels Many researchers have proposed the development of distributed applications based on multicast services <ref> [6, 13, 16, 32, 35] </ref>. The proposed services make different guarantees on the ordering of message deliveries. In this subsection, we define two more conditions similar to those used to define context multicast channels. <p> This idea has been widely adopted in systems for group communication <ref> [6, 32] </ref>. The Isis CBCAST primitive does not guarantee consistent order of receipt of all messages at different sites, and so it does not provide a context multicast channel.
Reference: [7] <author> H. Attiya and R. Friedman. </author> <title> A correctness condition for high-performance multiprocessors. </title> <booktitle> In Proc. 24th ACM Symp. on Theory of Computing, </booktitle> <pages> pages 679-691, </pages> <year> 1992. </year>
Reference-contexts: The study of correctness for shared memory with more general data types was initiated by Herlihy and Wing [22]. Sequential consistency and other consistency conditions for general data types has been studied by Attiya and Welch [8] and Attiya and Friedman <ref> [7] </ref>. The algorithms that provide each layer in this paper are closely related to some in the literature. In the lower layer, context multicast is provided by placing sequence numbers in messages, and delaying processing until after the receipt of messages that should be ordered ahead. <p> Attiya and Welch [8] proved that this algorithm provides sequential consistency when run over a totally ordered broadcast communication service. Our proof technique for sequential consistency based on a partial order is similar to a method used by Attiya and Friedman <ref> [7] </ref> to prove hybrid consistency. 1.2 Overview of this paper The rest of the paper is organized as follows. Section 2 introduces basic terminology that is used in the rest of the paper.
Reference: [8] <author> H. Attiya and J. Welch. </author> <title> Sequential consistency versus linearizability. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 12(2) </volume> <pages> 91-122, </pages> <year> 1994. </year>
Reference-contexts: The study of correctness for shared memory with more general data types was initiated by Herlihy and Wing [22]. Sequential consistency and other consistency conditions for general data types has been studied by Attiya and Welch <ref> [8] </ref> and Attiya and Friedman [7]. The algorithms that provide each layer in this paper are closely related to some in the literature. <p> The algorithm in the upper layer updates all copies, and reads any copy; this is folklore from the database community, where operation ordering is managed by locking. Attiya and Welch <ref> [8] </ref> proved that this algorithm provides sequential consistency when run over a totally ordered broadcast communication service. <p> It remains to describe and verify existing schemes using our framework, and to develop and verify new schemes that preserve the liveness condition. Another direction for further work is to consider different algorithms that trade off between the latency of different operations, as explored in <ref> [8] </ref>. For example, in a shared update, our replica management algorithm delays reporting the completion of the update to the client until the multicast message is received by the client's processor.
Reference: [9] <author> H.E. Bal and M.F. Kaashoek. </author> <title> Object distribution in orca using compile-time and run-time techniques. </title> <booktitle> In Proc. Eigth Annual Conf. on Object-Oriented Programming Systems, Languages, and Applications, </booktitle> <pages> pages 162-177, </pages> <address> Washington, DC, </address> <month> Sept. </month> <year> 1993. </year>
Reference-contexts: The decision about whether or not to replicate an object is made at run time using information generated by the Orca compiler. The details of this decision process, and also performance measurements to show the benefits of not replicating all objects, can be found in <ref> [9] </ref>. The naive strategy of allowing each read operation to access any copy of the object and each update operation to access all copies is not by itself sufficient to implement a sequentially consistent shared memory.
Reference: [10] <author> H.E. Bal, M.F. Kaashoek, </author> <title> and A.S. Tanenbaum. Orca: A language for parallel programming of distributed systems. </title> <journal> IEEE Trans. on Soft. Eng., </journal> <volume> 18(3) </volume> <pages> 190-205, </pages> <month> March </month> <year> 1992. </year> <month> 31 </month>
Reference-contexts: This algorithm is a generalization of one used in the implementation of the Orca distributed programming language <ref> [10] </ref> over the Amoeba distributed operating system [35]. Orca is a language for writing parallel and distributed application programs to run on clusters of workstations, processor pools and massively parallel computers [10, 34]. <p> This algorithm is a generalization of one used in the implementation of the Orca distributed programming language [10] over the Amoeba distributed operating system [35]. Orca is a language for writing parallel and distributed application programs to run on clusters of workstations, processor pools and massively parallel computers <ref> [10, 34] </ref>. It provides a simple shared object model in which each object has a state and a set of operations, classified as either read operations or update operations. Read operations do not modify the object state, while update operations may do so.
Reference: [11] <author> J.K. Bennett, J.B. Carter, and W. Zwaenepoel. Munin: </author> <title> Distributed shared memory based on type-specific memory coherence. </title> <booktitle> In Proc. Second Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 168-176, </pages> <address> Seattle, WA, </address> <month> March </month> <year> 1990. </year>
Reference: [12] <author> B.N. Bershad and M.J. Zekauskas. Midway: </author> <title> Shared memory parallel programming with entry consistency for distributed memory multiprocessors. </title> <type> Technical Report CMU-CS-91-170, </type> <address> CMU, Pittsburgh, PA, </address> <month> Sept. </month> <year> 1991. </year>
Reference: [13] <author> K. Birman and R. van Renesse. </author> <title> Reliable Distributed Computing with the Isis Toolkit. </title> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1994. </year>
Reference-contexts: The guarantees provided by a context multicast channel are weaker than those that are provided by totally ordered causal multicast channels, as provided by systems such as early versions of Isis <ref> [13, 14] </ref>. However, the properties of a context multicast channel are sufficiently strong to support the replica management of the Orca algorithm. <p> which every fair trace fi: * satisfies the basic reliability requirements, and * has a well-founded partial order Q such that fi and Q are receive consistent, context safe, and total. 4.2 Other Types of Multicast Channels Many researchers have proposed the development of distributed applications based on multicast services <ref> [6, 13, 16, 32, 35] </ref>. The proposed services make different guarantees on the ordering of message deliveries. In this subsection, we define two more conditions similar to those used to define context multicast channels. <p> The CBCAST multicast primitive of the Isis system <ref> [14, 13] </ref> guarantees that message delivery respects Lamport causality [25]. Informally speaking, this ensures that when a message is delivered, the recipient has already seen any other message whose contents could have been known to the sender at the time of sending.
Reference: [14] <author> K.P. Birman and T.A. Joseph. </author> <title> Reliable communication in the presence of failures. </title> <journal> ACM Trans. Comp. Syst., </journal> <volume> 5(1) </volume> <pages> 47-76, </pages> <month> Feb. </month> <year> 1987. </year>
Reference-contexts: The guarantees provided by a context multicast channel are weaker than those that are provided by totally ordered causal multicast channels, as provided by systems such as early versions of Isis <ref> [13, 14] </ref>. However, the properties of a context multicast channel are sufficiently strong to support the replica management of the Orca algorithm. <p> The CBCAST multicast primitive of the Isis system <ref> [14, 13] </ref> guarantees that message delivery respects Lamport causality [25]. Informally speaking, this ensures that when a message is delivered, the recipient has already seen any other message whose contents could have been known to the sender at the time of sending. <p> For some distributed applications, especially those based on replicated data, it is necessary not only to ensure causality but also to ensure that non-causally-related messages are received in the same order at different destinations. The ABCAST multicast primitive in the early versions of ISIS <ref> [14] </ref> guarantees these properties.
Reference: [15] <author> L.M. Censier and P. Feautrier. </author> <title> A new solution to cache coherence problems in multicache systems. </title> <journal> IEEE Trans. on Computers, </journal> <pages> pages 1112-1118, </pages> <month> Dec. </month> <year> 1978. </year>
Reference: [16] <author> D. Cheriton and W. Zwaenepoel. </author> <title> Distributed process groups in the v kernel. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 3(2) </volume> <pages> 77-107, </pages> <year> 1985. </year>
Reference-contexts: which every fair trace fi: * satisfies the basic reliability requirements, and * has a well-founded partial order Q such that fi and Q are receive consistent, context safe, and total. 4.2 Other Types of Multicast Channels Many researchers have proposed the development of distributed applications based on multicast services <ref> [6, 13, 16, 32, 35] </ref>. The proposed services make different guarantees on the ordering of message deliveries. In this subsection, we define two more conditions similar to those used to define context multicast channels.
Reference: [17] <author> Collier. </author> <title> Reasoning about Parallel Architectures. </title> <publisher> Prentice Hall Publishers, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1992. </year>
Reference: [18] <author> M. Dubois, C. Scheurich, and F.A. Briggs. </author> <title> Synchronization, coherence, and event ordering in multiprocessors. </title> <journal> IEEE Computer, </journal> <volume> 21(2) </volume> <pages> 9-21, </pages> <month> Feb. </month> <year> 1988. </year>
Reference: [19] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proc. Seventeenth Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <address> Seattle, WA, </address> <month> May </month> <year> 1990. </year>
Reference: [20] <author> P. Gibbons and M. Merritt. </author> <title> Specifying non-blocking shared memories. </title> <booktitle> In Proc. Fourth ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pages 306-315, </pages> <year> 1992. </year>
Reference-contexts: This work opens up many avenues for future research. First, some simple extensions to our results can be made. For example, we could allow concurrent invocations of operations by the same client, as in <ref> [20] </ref>, instead of requiring clients to block. In order to handle this case, we need to adjust our definition of sequential consistency to eliminate the client-well-formedness condition, to modify the algorithm to maintain sets of active operations, and to make minor changes in our proofs.
Reference: [21] <author> P. Gibbons, M. Merritt, and K. Gharachorloo. </author> <title> Proving sequential consistency of high-performance shared memories. </title> <booktitle> In Proc. Third ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pages 292-303, </pages> <year> 1991. </year>
Reference: [22] <author> M.P. Herlihy and J.M. Wing. </author> <title> Linearizability: A correctness condition for concurrent objects. </title> <journal> ACM Trans. on Programming Languages and Systems, </journal> <volume> 12(3) </volume> <pages> 463-492, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: The study of correctness for shared memory with more general data types was initiated by Herlihy and Wing <ref> [22] </ref>. Sequential consistency and other consistency conditions for general data types has been studied by Attiya and Welch [8] and Attiya and Friedman [7]. The algorithms that provide each layer in this paper are closely related to some in the literature. <p> Notice that for each client c, totally-precedes fijc totally orders the operations that occur in fijc. 3.2 Definition of Sequential Consistency Our definition of sequential consistency is based on an atomic object [27, 29], also known as a linearizable object <ref> [22] </ref>, whose underlying data type is the entire collection of data objects to be shared. In an atomic object, the operations appear to the clients as if they happened in some sequential order, and furthermore, that order must be consistent with the totally-precedes order. <p> It is also worth pointing out that although the partial replication algorithm does provide sequential consistency when run over a context multicast layer, it does not provide the stronger linearizability condition <ref> [22] </ref>. This is shown in Figure 8, in which an object x is replicated on three processors P 1 , P 2 and P 3 .
Reference: [23] <author> Donald E. Knuth. </author> <title> Fundamental Algorithms, </title> <booktitle> volume 1 of The Art of Computer Programming. </booktitle> <publisher> Addison-Wesley Publishing Company, Inc., </publisher> <address> Reading, MA, </address> <note> second edition, </note> <year> 1973. </year>
Reference-contexts: We first show that P is well-founded, that is, that each operation has finitely many predecessors in P . Note that in each constituent relation, each operation has finitely many predecessors. So if this property does not hold in P , K onig's lemma <ref> [24, 23] </ref> implies the existence of an infinite chain of direct predecessors. If infinitely many operations in this chain are global, then Lemma 5.10 gives an infinite chain of predecessors in mcast-order, contradicting the well-founded property of the multicast service.
Reference: [24] <editor> Denes K onig. </editor> <title> Sur les correspondances multivoques des ensembles. </title> <journal> Fundamenta Mathematicae, </journal> <volume> 8 </volume> <pages> 114-134, </pages> <year> 1926. </year>
Reference-contexts: We first show that P is well-founded, that is, that each operation has finitely many predecessors in P . Note that in each constituent relation, each operation has finitely many predecessors. So if this property does not hold in P , K onig's lemma <ref> [24, 23] </ref> implies the existence of an infinite chain of direct predecessors. If infinitely many operations in this chain are global, then Lemma 5.10 gives an infinite chain of predecessors in mcast-order, contradicting the well-founded property of the multicast service.
Reference: [25] <author> L. Lamport. </author> <title> Time, clocks, and the ordering of events in a distributed system. </title> <journal> Commun. ACM, </journal> <volume> 21(7) </volume> <pages> 558-565, </pages> <month> July </month> <year> 1978. </year>
Reference-contexts: The CBCAST multicast primitive of the Isis system [14, 13] guarantees that message delivery respects Lamport causality <ref> [25] </ref>. Informally speaking, this ensures that when a message is delivered, the recipient has already seen any other message whose contents could have been known to the sender at the time of sending.
Reference: [26] <author> L. Lamport. </author> <title> How to make a multiprocessor computer that correctly executes multiprocess programs. </title> <journal> IEEE Trans. on Computers, </journal> <volume> 28(9) </volume> <pages> 690-691, </pages> <month> Sept. </month> <year> 1979. </year>
Reference-contexts: Read operations do not modify the object state, while update operations may do so. Each operation involves only a single object and appears to be indivisible. More precisely, Orca provides a sequentially consistent memory model <ref> [26] </ref>. Informally speaking, a sequentially consistent memory appears to its users as if it were centralized (even though it may be implemented in a distributed fashion). There are several formalizations of the notion of sequentially consistent memory, differing in subtle ways. <p> Sequential consistency is widely used because it appears to be closest to what programmers expect from a shared memory system; non-sequentially-consistent shared memory systems typically trade programmability for performance. Sequential consistency was first defined by Lamport <ref> [26] </ref>; in this paper, we use an alternative formulation proposed by Afek et al. [4], based on I/O automata. Other papers exploring correctness conditions for shared memory and algorithms that implement them include [2, 3, 5, 11, 12, 15, 17, 18, 19, 20, 21, 28, 33]. <p> Then A is a sequentially consistent shared object system. Proof: Immediate by Lemma 3.1. The literature contains other definitions of sequential consistency, besides the automaton-based one we have adopted from Afek, Brown and Merritt [4]. The original definition of Lamport <ref> [26] </ref> says that a multiprocessor is sequentially consistent if the result of every operation is the same as if the operations of all the processors were executed in some sequential order, and the operations of each individual processor appear in this sequence in the order specified by its program.
Reference: [27] <author> L. Lamport. </author> <title> On interprocess communication, parts i and ii. </title> <journal> Distributed Computing, </journal> <volume> 1(2) </volume> <pages> 77-101, </pages> <year> 1986. </year>
Reference-contexts: Notice that for each client c, totally-precedes fijc totally orders the operations that occur in fijc. 3.2 Definition of Sequential Consistency Our definition of sequential consistency is based on an atomic object <ref> [27, 29] </ref>, also known as a linearizable object [22], whose underlying data type is the entire collection of data objects to be shared.
Reference: [28] <author> R.J. Lipton and J.S. Sandberg. </author> <title> Pram: a scalable shared memory. </title> <type> Technical Report CS-TR-180-88, </type> <institution> Princeton University, Princeton, NJ, </institution> <month> Sept. </month> <year> 1988. </year> <month> 32 </month>
Reference: [29] <author> N. Lynch. </author> <title> Distributed Algorithms. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: We prove that this algorithm, combined with any context multicast system, provides a sequentially consistent memory. Our proof uses a new method based on partial orders. All our specifications and proofs are presented in terms of the I/O automaton model for asynchronous concurrent systems [30]; see <ref> [29] </ref> for a tutorial presentation of the model. <p> I/O automata can be composed, by identifying actions with the same name. The fair trace semantics is compositional. Output actions of an I/O automaton can also be hidden, which means that they are reclassified as internal actions. See [30] or <ref> [29] </ref> for more details. 5 3 Sequentially Consistent Shared Object Systems In this section, we define a sequentially consistent shared object system and give a new method for proving that a system is sequentially consistent. <p> Notice that for each client c, totally-precedes fijc totally orders the operations that occur in fijc. 3.2 Definition of Sequential Consistency Our definition of sequential consistency is based on an atomic object <ref> [27, 29] </ref>, also known as a linearizable object [22], whose underlying data type is the entire collection of data objects to be shared.
Reference: [30] <author> N. Lynch and M. Tuttle. </author> <title> An introduction to input/output automata. </title> <journal> CWI Quarterly, </journal> <volume> 2(3) </volume> <pages> 219-246, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: We prove that this algorithm, combined with any context multicast system, provides a sequentially consistent memory. Our proof uses a new method based on partial orders. All our specifications and proofs are presented in terms of the I/O automaton model for asynchronous concurrent systems <ref> [30] </ref>; see [29] for a tutorial presentation of the model. <p> I/O automata can be composed, by identifying actions with the same name. The fair trace semantics is compositional. Output actions of an I/O automaton can also be hidden, which means that they are reclassified as internal actions. See <ref> [30] </ref> or [29] for more details. 5 3 Sequentially Consistent Shared Object Systems In this section, we define a sequentially consistent shared object system and give a new method for proving that a system is sequentially consistent.
Reference: [31] <author> G. Neiger and S. Toueg. </author> <title> Simulating synchronized clocks and common knowledge in distributed systems. </title> <journal> Journal of the ACM, </journal> <volume> 40(2) </volume> <pages> 334-367, </pages> <year> 1993. </year>
Reference-contexts: In the lower layer, context multicast is provided by placing sequence numbers in messages, and delaying processing until after the receipt of messages that should be ordered ahead. This is similar to techniques used by Welch [36] and Neiger and Toueg <ref> [31] </ref>, which delay point-to-point messages based on sequence numbers. The algorithm in the upper layer updates all copies, and reads any copy; this is folklore from the database community, where operation ordering is managed by locking.
Reference: [32] <author> L. Peterson, N. Bucholz, and R. Schlichting. </author> <title> Preserving and using context information in interprocess communication. </title> <journal> ACM Trans. Comp. Syst., </journal> <volume> 7(3) </volume> <pages> 217-246, </pages> <month> Aug. </month> <year> 1989. </year>
Reference-contexts: which every fair trace fi: * satisfies the basic reliability requirements, and * has a well-founded partial order Q such that fi and Q are receive consistent, context safe, and total. 4.2 Other Types of Multicast Channels Many researchers have proposed the development of distributed applications based on multicast services <ref> [6, 13, 16, 32, 35] </ref>. The proposed services make different guarantees on the ordering of message deliveries. In this subsection, we define two more conditions similar to those used to define context multicast channels. <p> This idea has been widely adopted in systems for group communication <ref> [6, 32] </ref>. The Isis CBCAST primitive does not guarantee consistent order of receipt of all messages at different sites, and so it does not provide a context multicast channel.
Reference: [33] <author> C. Scheurich and M. Dubois. </author> <title> Correct memory operation of cache-based multiprocessors. </title> <booktitle> In Proc. Fourteenth Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 234-243, </pages> <address> Pittsburg, PA, </address> <month> June </month> <year> 1987. </year>
Reference: [34] <author> A.S. Tanenbaum, M.F. Kaashoek, and H.E. Bal. </author> <title> Parallel programming using shared objects and broadcasting. </title> <journal> IEEE Computer, </journal> <volume> 25(8) </volume> <pages> 10-19, </pages> <month> Aug. </month> <year> 1992. </year>
Reference-contexts: This algorithm is a generalization of one used in the implementation of the Orca distributed programming language [10] over the Amoeba distributed operating system [35]. Orca is a language for writing parallel and distributed application programs to run on clusters of workstations, processor pools and massively parallel computers <ref> [10, 34] </ref>. It provides a simple shared object model in which each object has a state and a set of operations, classified as either read operations or update operations. Read operations do not modify the object state, while update operations may do so.
Reference: [35] <author> A.S. Tanenbaum, R. van Renesse, H. van Staveren, G. Sharp, S. Mullender, A. Jansen, and G. van Rossum. </author> <title> Experiences with the Amoeba distributed operating system. </title> <journal> Communications of the ACM, </journal> <volume> 33(12) </volume> <pages> 46-63, </pages> <month> Dec. </month> <year> 1990. </year>
Reference-contexts: This algorithm is a generalization of one used in the implementation of the Orca distributed programming language [10] over the Amoeba distributed operating system <ref> [35] </ref>. Orca is a language for writing parallel and distributed application programs to run on clusters of workstations, processor pools and massively parallel computers [10, 34]. <p> A preliminary version of this paper appeared in Proceedings of the International Conference on Distributed Computing Systems, 1995. Orca runs over the Amoeba operating system <ref> [35] </ref>, which provides two communication services: broadcast and point-to-point communication. Both services provide reliable communication, even in the presence of communication failures. No guarantees are made by Orca if processors fail; therefore, we do not consider processor failures either. <p> which every fair trace fi: * satisfies the basic reliability requirements, and * has a well-founded partial order Q such that fi and Q are receive consistent, context safe, and total. 4.2 Other Types of Multicast Channels Many researchers have proposed the development of distributed applications based on multicast services <ref> [6, 13, 16, 32, 35] </ref>. The proposed services make different guarantees on the ordering of message deliveries. In this subsection, we define two more conditions similar to those used to define context multicast channels.
Reference: [36] <author> J. Welch. </author> <title> Simulating synchronous processors. </title> <journal> Information and Computation, </journal> <volume> 74(2) </volume> <pages> 159-171, </pages> <year> 1987. </year> <month> 33 </month>
Reference-contexts: In the lower layer, context multicast is provided by placing sequence numbers in messages, and delaying processing until after the receipt of messages that should be ordered ahead. This is similar to techniques used by Welch <ref> [36] </ref> and Neiger and Toueg [31], which delay point-to-point messages based on sequence numbers. The algorithm in the upper layer updates all copies, and reads any copy; this is folklore from the database community, where operation ordering is managed by locking.
References-found: 36

